<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2309.04902] Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art</title><meta property="og:description" content="Transformers have rapidly gained popularity in computer vision, especially in the field of object recognition and detection. Upon examining the outcomes of state-of-the-art object detection methods, we noticed that tra…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2309.04902">

<!--Generated on Wed Feb 28 07:23:47 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Object recognition,  small object detection,  vision transformers,  object localization,  deep learning,  attention,  MS COCO dataset.
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Aref Miri Rekavandi, <span id="id1.1.id1" class="ltx_text ltx_font_italic">Member, IEEE,</span> Shima Rashidi, Farid Boussaid, Stephen Hoefs, Emre Akbas, and Mohammed Bennamoun, <span id="id2.2.id2" class="ltx_text ltx_font_italic">Senior Member, IEEE
<br class="ltx_break"></span>
</span><span class="ltx_author_notes">Aref Miri Rekavandi and Mohammed Bennamoun are with the Department of Computer Science and Software Engineering, The University of Western Australia (Emails: aref.mirirekavandi@uwa.edu.au, mohammed.bennamoun@uwa.edu.au). Shima Rashidi is an independent researcher (Email: shima.rashidi7@gmail.com). Farid Boussaid is with the Department of Electrical, Electronics and Computer Engineering, The University of Western Australia (Email: farid.boussaid@uwa.edu.au). Stephen Hoefs is a discipline leader at Defence Science and Technology Group, Australia (Email: stephen.hoefs@defence.gov.au). Emre Akbas is with the Department of Computer Engineering, Middle East Technical University, Turkey. (Email: emre@ceng.metu.edu.tr).</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id3.id1" class="ltx_p">Transformers have rapidly gained popularity in computer vision, especially in the field of object recognition and detection. Upon examining the outcomes of state-of-the-art object detection methods, we noticed that transformers consistently outperformed well-established CNN-based detectors in almost every video or image dataset. While transformer-based approaches remain at the forefront of small object detection (SOD) techniques, this paper aims to explore the performance benefits offered by such extensive networks and identify potential reasons for their SOD superiority. Small objects have been identified as one of the most challenging object types in detection frameworks due to their low visibility. We aim to investigate potential strategies that could enhance transformers’ performance in SOD. This survey presents a taxonomy of over 60 research studies on developed transformers for the task of SOD, spanning the years 2020 to 2023. These studies encompass a variety of detection applications, including small object detection in generic images, aerial images, medical images, active millimeter images, underwater images, and videos. We also compile and present a list of 12 large-scale datasets suitable for SOD that were overlooked in previous studies and compare the performance of the reviewed studies using popular metrics such as mean Average Precision (mAP), Frames Per Second (FPS), number of parameters, and more. Researchers can keep track of newer studies on our web page, which is available at: <a target="_blank" href="https://github.com/arekavandi/Transformer-SOD" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/arekavandi/Transformer-SOD</a>.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Object recognition, small object detection, vision transformers, object localization, deep learning, attention, MS COCO dataset.

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.3" class="ltx_p">Small Object Detection (SOD) has been recognized as a significant challenge for State-Of-The-Art (SOTA) object detection methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. The term “small object” refers to objects that occupy a small fraction of the input image. For example, in the widely used MS COCO dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, it defines objects whose bounding box is <math id="S1.p1.1.m1.1" class="ltx_Math" alttext="32\times 32" display="inline"><semantics id="S1.p1.1.m1.1a"><mrow id="S1.p1.1.m1.1.1" xref="S1.p1.1.m1.1.1.cmml"><mn id="S1.p1.1.m1.1.1.2" xref="S1.p1.1.m1.1.1.2.cmml">32</mn><mo lspace="0.222em" rspace="0.222em" id="S1.p1.1.m1.1.1.1" xref="S1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S1.p1.1.m1.1.1.3" xref="S1.p1.1.m1.1.1.3.cmml">32</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.p1.1.m1.1b"><apply id="S1.p1.1.m1.1.1.cmml" xref="S1.p1.1.m1.1.1"><times id="S1.p1.1.m1.1.1.1.cmml" xref="S1.p1.1.m1.1.1.1"></times><cn type="integer" id="S1.p1.1.m1.1.1.2.cmml" xref="S1.p1.1.m1.1.1.2">32</cn><cn type="integer" id="S1.p1.1.m1.1.1.3.cmml" xref="S1.p1.1.m1.1.1.3">32</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p1.1.m1.1c">32\times 32</annotation></semantics></math> pixels or less, in a typical <math id="S1.p1.2.m2.1" class="ltx_Math" alttext="480\times 640" display="inline"><semantics id="S1.p1.2.m2.1a"><mrow id="S1.p1.2.m2.1.1" xref="S1.p1.2.m2.1.1.cmml"><mn id="S1.p1.2.m2.1.1.2" xref="S1.p1.2.m2.1.1.2.cmml">480</mn><mo lspace="0.222em" rspace="0.222em" id="S1.p1.2.m2.1.1.1" xref="S1.p1.2.m2.1.1.1.cmml">×</mo><mn id="S1.p1.2.m2.1.1.3" xref="S1.p1.2.m2.1.1.3.cmml">640</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.p1.2.m2.1b"><apply id="S1.p1.2.m2.1.1.cmml" xref="S1.p1.2.m2.1.1"><times id="S1.p1.2.m2.1.1.1.cmml" xref="S1.p1.2.m2.1.1.1"></times><cn type="integer" id="S1.p1.2.m2.1.1.2.cmml" xref="S1.p1.2.m2.1.1.2">480</cn><cn type="integer" id="S1.p1.2.m2.1.1.3.cmml" xref="S1.p1.2.m2.1.1.3">640</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p1.2.m2.1c">480\times 640</annotation></semantics></math> image (Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). Other datasets have their own definitions, e.g. objects that occupy <math id="S1.p1.3.m3.1" class="ltx_Math" alttext="10\%" display="inline"><semantics id="S1.p1.3.m3.1a"><mrow id="S1.p1.3.m3.1.1" xref="S1.p1.3.m3.1.1.cmml"><mn id="S1.p1.3.m3.1.1.2" xref="S1.p1.3.m3.1.1.2.cmml">10</mn><mo id="S1.p1.3.m3.1.1.1" xref="S1.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.p1.3.m3.1b"><apply id="S1.p1.3.m3.1.1.cmml" xref="S1.p1.3.m3.1.1"><csymbol cd="latexml" id="S1.p1.3.m3.1.1.1.cmml" xref="S1.p1.3.m3.1.1.1">percent</csymbol><cn type="integer" id="S1.p1.3.m3.1.1.2.cmml" xref="S1.p1.3.m3.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p1.3.m3.1c">10\%</annotation></semantics></math> of the image. Small objects are often missed or detected with incorrectly localized bounding boxes, and sometimes with incorrect labels.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2309.04902/assets/Figures/air2.jpg" id="S1.F1.1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="275" height="275" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2309.04902/assets/Figures/per4.jpg" id="S1.F1.2.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" width="183" height="275" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2309.04902/assets/Figures/test1.jpg" id="S1.F1.3.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" width="138" height="207" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2309.04902/assets/Figures/test2.jpg" id="S1.F1.4.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="329" height="214" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.6.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.7.2" class="ltx_text" style="font-size:90%;">Examples of small size objects from MS COCO dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. The objects are highlighted with color segments.</span></figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The main reason for the deficient localization in SOD stems from the limited information provided in the input image or video frame, compounded by the subsequent spatial degradation experienced as they pass through multiple layers in deep networks. Since small objects frequently appear in various application domains, such as pedestrian detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, medical image analysis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, face recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, traffic sign detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, traffic light detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, ship detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, Synthetic Aperture Radar (SAR)-based object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, it is worth examining the performance of modern deep learning SOD techniques. In this paper, we compare transformer-based detectors with Convolutional Neural Networks (CNNs) based detectors in terms of their small object detection performance. In the case of outperforming CNNs with a clear margin, we then attempt to uncover the reasons behind the transformer’s strong performance. One immediate explanation could be that transformers model the interactions between pairwise locations in the input image. This is effectively a way of encoding the context. And, it is well established that context is a major source of information to detect and recognize small objects both in humans and computational models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. However, this might not be the only factor to explain transformers’ success. Specifically, we aim to analyze this success along several dimensions including object representation, fast attention for high-resolution or multiscale
feature maps, fully transformer-based detection, architecture
and block modification, auxiliary techniques, improved feature
representation, and spatio-temporal information. Furthermore, we point out approaches that could potentially enhance the performance of transformers for SOD.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In our previous work, we surveyed numerous strategies
employed in deep learning to enhance the performance of small
object detection in optical images and videos up to the year 2022 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. We showed that beyond the adaptation of newer deep learning structures such as transformers, prevalent approaches include data augmentation, super-resolution, multi-scale feature learning, context learning, attention-based learning, region proposal, loss function regularization, leveraging auxiliary tasks, and spatiotemporal feature aggregation. Additionally,
we observed that transformers are among the leading methods in localizing
small objects across most datasets. However, given that <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> predominantly evaluated over 160 papers focusing
on CNN-based networks, an in-depth exploration of transformer-centric methods was not undertaken. Recognizing the growth and
exploration pace in the field, there is a timely window now to
delve into the current transformer models geared towards small
object detection.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this paper, our goal is to comprehensively
understand the factors contributing to the impressive performance
of transformers when applied to small object detection and their distinction with strategies used for generic object detection. To lay
the groundwork, we first highlight renowned transformer-based
object detectors for SOD, juxtaposing their advancements against
established CNN-based methodologies.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Since 2017, the field has seen the publication of numerous
review articles. An extensive discussion and listing of these reviews
are presented in our previous survey <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. Another recent survey article <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> mostly focuses on the CNN-based
techniques, too. The narrative of this current survey stands distinct
from its predecessors. Our focus in this paper narrows down
specifically to transformers — an aspect not explored previously — positioning them as the dominant network architecture for image and video SOD. This entails a unique taxonomy tailored to this innovative architecture, consciously sidelining CNN-based methods. Given the novelty and intricacy of this topic, our review prioritizes works primarily brought forth post-2022. Additionally, we shed light on newer datasets employed for the localization and detection of small objects across a broader spectrum of applications.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">The studies examined in this survey primarily presented methods
tailored for small object localization and classification or
indirectly tackled SOD challenges. What drove our analysis were
the detection outcomes specified for small objects in these papers.
However, earlier research that noted SOD outcomes but either
demonstrated subpar performance or overlooked SOD-specific
parameters in their development approach were not considered
for inclusion in this review. In this survey, we assume the reader is already familiar with
generic object detection techniques, their architectures, and relevant
performance measures. If the reader requires foundational
insight into these areas, we refer the reader to our previous work
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">The structure of this paper is as follows: Section <a href="#S2" title="2 Background ‣ Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> offers an overview of CNN-based object detectors, transformers, and their components, including the encoder and decoder. This section also touches upon two initial iterations of transformer-based object detectors: DETR and ViT-FRCNN. In Section <a href="#S3" title="3 Transfomers For Small Object Detection ‣ Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we present a classification for transformer-based SOD techniques and delve into each category comprehensively. Section <a href="#S4" title="4 Results and Benchmarks ‣ Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> showcases the different datasets used for SOD and evaluates them across a range of applications. In Section <a href="#S5" title="5 Discussion ‣ Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we analyze and contrast these outcomes with earlier results derived from CNN networks. The paper wraps up with conclusions in Section <a href="#S6" title="6 Conclusion ‣ Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Background</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Object detection and in particular SOD, has long relied on CNN-based deep learning models. Several single-stage and two-stage detectors have emerged over time, such as You Only Look Once (YOLO) variants <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, Single Shot multi-box Detector (SSD) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, RetinaNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, Spatial Pyramid Pooling Network (SPP-Net) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, Fast R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, Region-Based Fully Convolutional Networks (R-FCN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, Feature Pyramid Networks (FPN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, cascade R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, and Libra R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. Various strategies have been used in conjunction with these techniques to improve their detection performance for SOD, with multi-scale feature learning being the most commonly used approach.
<br class="ltx_break"></p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2309.04902/assets/Figures/transformer.jpg" id="S2.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="538" height="723" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.3.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S2.F2.4.2" class="ltx_text" style="font-size:90%;">Transformer architecture containing encoder (left module) and decoder (right module) used in sequence to sequence translation (figure from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>).</span></figcaption>
</figure>
<figure id="S2.F3" class="ltx_figure">
<table id="S2.F3.2" class="ltx_tabular ltx_align_middle">
<tr id="S2.F3.1.1" class="ltx_tr">
<td id="S2.F3.1.1.1" class="ltx_td ltx_align_center"><img src="/html/2309.04902/assets/Figures/DETRn.jpg" id="S2.F3.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="568" height="150" alt="Refer to caption"></td>
</tr>
<tr id="S2.F3.2.2" class="ltx_tr">
<td id="S2.F3.2.2.1" class="ltx_td ltx_align_center ltx_border_t"><img src="/html/2309.04902/assets/Figures/ViT-FRCNNn.jpg" id="S2.F3.2.2.1.g1" class="ltx_graphics ltx_img_landscape" width="568" height="151" alt="Refer to caption"></td>
</tr>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.4.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S2.F3.5.2" class="ltx_text" style="font-size:90%;">Top: DETR (figure from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>). Bottom: ViT-FRCNN (figure from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>).</span></figcaption>
</figure>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S2.T1.2.1.1" class="ltx_text" style="font-size:90%;">TABLE I</span>: </span><span id="S2.T1.3.2" class="ltx_text" style="font-size:90%;">A list of terminologies used in this paper with their meanings.</span></figcaption>
<div id="S2.T1.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:558.5pt;height:393.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<div id="S2.T1.4.1" class="ltx_inline-block ltx_transformed_outer" style="width:552.9pt;height:389.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-2.8pt,2.0pt) scale(0.99,0.99) ;">
<p id="S2.T1.4.1.1" class="ltx_p"><span id="S2.T1.4.1.1.1" class="ltx_text">
<span id="S2.T1.4.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S2.T1.4.1.1.1.1.1" class="ltx_tr">
<span id="S2.T1.4.1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_t">Full Term</span>
<span id="S2.T1.4.1.1.1.1.1.2" class="ltx_td ltx_align_left ltx_border_t">Description</span></span>
<span id="S2.T1.4.1.1.1.1.2" class="ltx_tr">
<span id="S2.T1.4.1.1.1.1.2.1" class="ltx_td ltx_align_left ltx_border_t">Encoder</span>
<span id="S2.T1.4.1.1.1.1.2.2" class="ltx_td ltx_align_left ltx_border_t">Encoder in transformers consists of multiple layers of self-attention modules and feed-forward neural networks to extract local</span></span>
<span id="S2.T1.4.1.1.1.1.3" class="ltx_tr">
<span id="S2.T1.4.1.1.1.1.3.1" class="ltx_td"></span>
<span id="S2.T1.4.1.1.1.1.3.2" class="ltx_td ltx_align_left">and global semantic information from the input data.</span></span>
<span id="S2.T1.4.1.1.1.1.4" class="ltx_tr">
<span id="S2.T1.4.1.1.1.1.4.1" class="ltx_td ltx_align_left">Decoder</span>
<span id="S2.T1.4.1.1.1.1.4.2" class="ltx_td ltx_align_left">Decoder module is responsible to generate the output (either sequence or independent) based on the concept of self and cross</span></span>
<span id="S2.T1.4.1.1.1.1.5" class="ltx_tr">
<span id="S2.T1.4.1.1.1.1.5.1" class="ltx_td"></span>
<span id="S2.T1.4.1.1.1.1.5.2" class="ltx_td ltx_align_left">attention applied to the object queries and encoder’s output.</span></span>
<span id="S2.T1.4.1.1.1.1.6" class="ltx_tr">
<span id="S2.T1.4.1.1.1.1.6.1" class="ltx_td ltx_align_left">Token</span>
<span id="S2.T1.4.1.1.1.1.6.2" class="ltx_td ltx_align_left">Token refers to the most basic unit of data input into the transformers. It can be image pixels, patches, or video clips.</span></span>
<span id="S2.T1.4.1.1.1.1.7" class="ltx_tr">
<span id="S2.T1.4.1.1.1.1.7.1" class="ltx_td ltx_align_left">Multi-Head Attention</span>
<span id="S2.T1.4.1.1.1.1.7.2" class="ltx_td ltx_align_left">Multi-Head Attention is a mechanism in transformers that enhances the learning capacity and representational power of</span></span>
<span id="S2.T1.4.1.1.1.1.8" class="ltx_tr">
<span id="S2.T1.4.1.1.1.1.8.1" class="ltx_td"></span>
<span id="S2.T1.4.1.1.1.1.8.2" class="ltx_td ltx_align_left">self-attention. It divides the input into multiple subspaces and performs attention computations independently on each subspace,</span></span>
<span id="S2.T1.4.1.1.1.1.9" class="ltx_tr">
<span id="S2.T1.4.1.1.1.1.9.1" class="ltx_td"></span>
<span id="S2.T1.4.1.1.1.1.9.2" class="ltx_td ltx_align_left">known as attention heads.</span></span>
<span id="S2.T1.4.1.1.1.1.10" class="ltx_tr">
<span id="S2.T1.4.1.1.1.1.10.1" class="ltx_td ltx_align_left">Spatial Attention</span>
<span id="S2.T1.4.1.1.1.1.10.2" class="ltx_td ltx_align_left">Spatial attention in transformers refers to a type of attention mechanism that attends to the spatial positions of tokens within a</span></span>
<span id="S2.T1.4.1.1.1.1.11" class="ltx_tr">
<span id="S2.T1.4.1.1.1.1.11.1" class="ltx_td"></span>
<span id="S2.T1.4.1.1.1.1.11.2" class="ltx_td ltx_align_left">sequence. It allows the model to focus on the relative positions of tokens and capture spatial relationships.</span></span>
<span id="S2.T1.4.1.1.1.1.12" class="ltx_tr">
<span id="S2.T1.4.1.1.1.1.12.1" class="ltx_td ltx_align_left">Channel Attention</span>
<span id="S2.T1.4.1.1.1.1.12.2" class="ltx_td ltx_align_left">Channel attention in transformers refers to an attention mechanism that operates across different channels or feature dimensions of</span></span>
<span id="S2.T1.4.1.1.1.1.13" class="ltx_tr">
<span id="S2.T1.4.1.1.1.1.13.1" class="ltx_td"></span>
<span id="S2.T1.4.1.1.1.1.13.2" class="ltx_td ltx_align_left">the input. It allows the model to dynamically adjust the importance of different channels, enhancing the representation and modeling</span></span>
<span id="S2.T1.4.1.1.1.1.14" class="ltx_tr">
<span id="S2.T1.4.1.1.1.1.14.1" class="ltx_td"></span>
<span id="S2.T1.4.1.1.1.1.14.2" class="ltx_td ltx_align_left">of channel-specific information in tasks</span></span>
<span id="S2.T1.4.1.1.1.1.15" class="ltx_tr">
<span id="S2.T1.4.1.1.1.1.15.1" class="ltx_td ltx_align_left">Object Query</span>
<span id="S2.T1.4.1.1.1.1.15.2" class="ltx_td ltx_align_left">It refers to a learned vector representation that is used to query and attend to specific objects or entities within a scene.</span></span>
<span id="S2.T1.4.1.1.1.1.16" class="ltx_tr">
<span id="S2.T1.4.1.1.1.1.16.1" class="ltx_td ltx_align_left">Positional Embedding</span>
<span id="S2.T1.4.1.1.1.1.16.2" class="ltx_td ltx_align_left">It refers to a learned representation that encodes the positional information of tokens in an input sequence, enabling the model to</span></span>
<span id="S2.T1.4.1.1.1.1.17" class="ltx_tr">
<span id="S2.T1.4.1.1.1.1.17.1" class="ltx_td ltx_border_bb"></span>
<span id="S2.T1.4.1.1.1.1.17.2" class="ltx_td ltx_align_left ltx_border_bb">capture sequential dependencies.</span></span>
</span></span></p>
</span></div>
</span></div>
</figure>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.6" class="ltx_p">The transformer model was first introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> as a novel technique for machine translation. This model aimed to advance beyond traditional recurrent networks and CNNs by introducing a new network architecture solely based on attention mechanisms, thereby eliminating the need for recurrence and convolutions. The Transformer model consists of two main modules: the encoder and the decoder. Figure <a href="#S2.F2" title="Figure 2 ‣ 2 Background ‣ Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> provides a visual representation of the processing blocks within each module. The description of terminologies commonly used in Transformers for computer vision is provided in Table <a href="#S2.T1" title="TABLE I ‣ 2 Background ‣ Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> for readers who are not familiar with the topic.
Within the context of SOD, the encoder module ingests input tokens, which can refer to image patches or video clips, and employs various feature embedding approaches, such as utilizing pre-trained CNNs to extract suitable representations. The positional encoding block embeds positional information into the feature representations of each token. Positional encoding has demonstrated significant performance improvements in various applications. The encoded representations are then passed through a Multi-Head Attention block, which is parameterized with three main matrices, namely <math id="S2.p2.1.m1.1" class="ltx_Math" alttext="\textbf{W}_{q}\in\mathbf{R}^{d_{q}\times d}" display="inline"><semantics id="S2.p2.1.m1.1a"><mrow id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml"><msub id="S2.p2.1.m1.1.1.2" xref="S2.p2.1.m1.1.1.2.cmml"><mtext class="ltx_mathvariant_bold" id="S2.p2.1.m1.1.1.2.2" xref="S2.p2.1.m1.1.1.2.2a.cmml">W</mtext><mi id="S2.p2.1.m1.1.1.2.3" xref="S2.p2.1.m1.1.1.2.3.cmml">q</mi></msub><mo id="S2.p2.1.m1.1.1.1" xref="S2.p2.1.m1.1.1.1.cmml">∈</mo><msup id="S2.p2.1.m1.1.1.3" xref="S2.p2.1.m1.1.1.3.cmml"><mi id="S2.p2.1.m1.1.1.3.2" xref="S2.p2.1.m1.1.1.3.2.cmml">𝐑</mi><mrow id="S2.p2.1.m1.1.1.3.3" xref="S2.p2.1.m1.1.1.3.3.cmml"><msub id="S2.p2.1.m1.1.1.3.3.2" xref="S2.p2.1.m1.1.1.3.3.2.cmml"><mi id="S2.p2.1.m1.1.1.3.3.2.2" xref="S2.p2.1.m1.1.1.3.3.2.2.cmml">d</mi><mi id="S2.p2.1.m1.1.1.3.3.2.3" xref="S2.p2.1.m1.1.1.3.3.2.3.cmml">q</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S2.p2.1.m1.1.1.3.3.1" xref="S2.p2.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S2.p2.1.m1.1.1.3.3.3" xref="S2.p2.1.m1.1.1.3.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.1b"><apply id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1"><in id="S2.p2.1.m1.1.1.1.cmml" xref="S2.p2.1.m1.1.1.1"></in><apply id="S2.p2.1.m1.1.1.2.cmml" xref="S2.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.p2.1.m1.1.1.2.1.cmml" xref="S2.p2.1.m1.1.1.2">subscript</csymbol><ci id="S2.p2.1.m1.1.1.2.2a.cmml" xref="S2.p2.1.m1.1.1.2.2"><mtext class="ltx_mathvariant_bold" id="S2.p2.1.m1.1.1.2.2.cmml" xref="S2.p2.1.m1.1.1.2.2">W</mtext></ci><ci id="S2.p2.1.m1.1.1.2.3.cmml" xref="S2.p2.1.m1.1.1.2.3">𝑞</ci></apply><apply id="S2.p2.1.m1.1.1.3.cmml" xref="S2.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.p2.1.m1.1.1.3.1.cmml" xref="S2.p2.1.m1.1.1.3">superscript</csymbol><ci id="S2.p2.1.m1.1.1.3.2.cmml" xref="S2.p2.1.m1.1.1.3.2">𝐑</ci><apply id="S2.p2.1.m1.1.1.3.3.cmml" xref="S2.p2.1.m1.1.1.3.3"><times id="S2.p2.1.m1.1.1.3.3.1.cmml" xref="S2.p2.1.m1.1.1.3.3.1"></times><apply id="S2.p2.1.m1.1.1.3.3.2.cmml" xref="S2.p2.1.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="S2.p2.1.m1.1.1.3.3.2.1.cmml" xref="S2.p2.1.m1.1.1.3.3.2">subscript</csymbol><ci id="S2.p2.1.m1.1.1.3.3.2.2.cmml" xref="S2.p2.1.m1.1.1.3.3.2.2">𝑑</ci><ci id="S2.p2.1.m1.1.1.3.3.2.3.cmml" xref="S2.p2.1.m1.1.1.3.3.2.3">𝑞</ci></apply><ci id="S2.p2.1.m1.1.1.3.3.3.cmml" xref="S2.p2.1.m1.1.1.3.3.3">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.1c">\textbf{W}_{q}\in\mathbf{R}^{d_{q}\times d}</annotation></semantics></math>, <math id="S2.p2.2.m2.1" class="ltx_Math" alttext="\textbf{W}_{k}\in\mathbf{R}^{d_{k}\times d}" display="inline"><semantics id="S2.p2.2.m2.1a"><mrow id="S2.p2.2.m2.1.1" xref="S2.p2.2.m2.1.1.cmml"><msub id="S2.p2.2.m2.1.1.2" xref="S2.p2.2.m2.1.1.2.cmml"><mtext class="ltx_mathvariant_bold" id="S2.p2.2.m2.1.1.2.2" xref="S2.p2.2.m2.1.1.2.2a.cmml">W</mtext><mi id="S2.p2.2.m2.1.1.2.3" xref="S2.p2.2.m2.1.1.2.3.cmml">k</mi></msub><mo id="S2.p2.2.m2.1.1.1" xref="S2.p2.2.m2.1.1.1.cmml">∈</mo><msup id="S2.p2.2.m2.1.1.3" xref="S2.p2.2.m2.1.1.3.cmml"><mi id="S2.p2.2.m2.1.1.3.2" xref="S2.p2.2.m2.1.1.3.2.cmml">𝐑</mi><mrow id="S2.p2.2.m2.1.1.3.3" xref="S2.p2.2.m2.1.1.3.3.cmml"><msub id="S2.p2.2.m2.1.1.3.3.2" xref="S2.p2.2.m2.1.1.3.3.2.cmml"><mi id="S2.p2.2.m2.1.1.3.3.2.2" xref="S2.p2.2.m2.1.1.3.3.2.2.cmml">d</mi><mi id="S2.p2.2.m2.1.1.3.3.2.3" xref="S2.p2.2.m2.1.1.3.3.2.3.cmml">k</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S2.p2.2.m2.1.1.3.3.1" xref="S2.p2.2.m2.1.1.3.3.1.cmml">×</mo><mi id="S2.p2.2.m2.1.1.3.3.3" xref="S2.p2.2.m2.1.1.3.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.2.m2.1b"><apply id="S2.p2.2.m2.1.1.cmml" xref="S2.p2.2.m2.1.1"><in id="S2.p2.2.m2.1.1.1.cmml" xref="S2.p2.2.m2.1.1.1"></in><apply id="S2.p2.2.m2.1.1.2.cmml" xref="S2.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S2.p2.2.m2.1.1.2.1.cmml" xref="S2.p2.2.m2.1.1.2">subscript</csymbol><ci id="S2.p2.2.m2.1.1.2.2a.cmml" xref="S2.p2.2.m2.1.1.2.2"><mtext class="ltx_mathvariant_bold" id="S2.p2.2.m2.1.1.2.2.cmml" xref="S2.p2.2.m2.1.1.2.2">W</mtext></ci><ci id="S2.p2.2.m2.1.1.2.3.cmml" xref="S2.p2.2.m2.1.1.2.3">𝑘</ci></apply><apply id="S2.p2.2.m2.1.1.3.cmml" xref="S2.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S2.p2.2.m2.1.1.3.1.cmml" xref="S2.p2.2.m2.1.1.3">superscript</csymbol><ci id="S2.p2.2.m2.1.1.3.2.cmml" xref="S2.p2.2.m2.1.1.3.2">𝐑</ci><apply id="S2.p2.2.m2.1.1.3.3.cmml" xref="S2.p2.2.m2.1.1.3.3"><times id="S2.p2.2.m2.1.1.3.3.1.cmml" xref="S2.p2.2.m2.1.1.3.3.1"></times><apply id="S2.p2.2.m2.1.1.3.3.2.cmml" xref="S2.p2.2.m2.1.1.3.3.2"><csymbol cd="ambiguous" id="S2.p2.2.m2.1.1.3.3.2.1.cmml" xref="S2.p2.2.m2.1.1.3.3.2">subscript</csymbol><ci id="S2.p2.2.m2.1.1.3.3.2.2.cmml" xref="S2.p2.2.m2.1.1.3.3.2.2">𝑑</ci><ci id="S2.p2.2.m2.1.1.3.3.2.3.cmml" xref="S2.p2.2.m2.1.1.3.3.2.3">𝑘</ci></apply><ci id="S2.p2.2.m2.1.1.3.3.3.cmml" xref="S2.p2.2.m2.1.1.3.3.3">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.2.m2.1c">\textbf{W}_{k}\in\mathbf{R}^{d_{k}\times d}</annotation></semantics></math>, and <math id="S2.p2.3.m3.1" class="ltx_Math" alttext="\textbf{W}_{v}\in\mathbf{R}^{d_{v}\times d}" display="inline"><semantics id="S2.p2.3.m3.1a"><mrow id="S2.p2.3.m3.1.1" xref="S2.p2.3.m3.1.1.cmml"><msub id="S2.p2.3.m3.1.1.2" xref="S2.p2.3.m3.1.1.2.cmml"><mtext class="ltx_mathvariant_bold" id="S2.p2.3.m3.1.1.2.2" xref="S2.p2.3.m3.1.1.2.2a.cmml">W</mtext><mi id="S2.p2.3.m3.1.1.2.3" xref="S2.p2.3.m3.1.1.2.3.cmml">v</mi></msub><mo id="S2.p2.3.m3.1.1.1" xref="S2.p2.3.m3.1.1.1.cmml">∈</mo><msup id="S2.p2.3.m3.1.1.3" xref="S2.p2.3.m3.1.1.3.cmml"><mi id="S2.p2.3.m3.1.1.3.2" xref="S2.p2.3.m3.1.1.3.2.cmml">𝐑</mi><mrow id="S2.p2.3.m3.1.1.3.3" xref="S2.p2.3.m3.1.1.3.3.cmml"><msub id="S2.p2.3.m3.1.1.3.3.2" xref="S2.p2.3.m3.1.1.3.3.2.cmml"><mi id="S2.p2.3.m3.1.1.3.3.2.2" xref="S2.p2.3.m3.1.1.3.3.2.2.cmml">d</mi><mi id="S2.p2.3.m3.1.1.3.3.2.3" xref="S2.p2.3.m3.1.1.3.3.2.3.cmml">v</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S2.p2.3.m3.1.1.3.3.1" xref="S2.p2.3.m3.1.1.3.3.1.cmml">×</mo><mi id="S2.p2.3.m3.1.1.3.3.3" xref="S2.p2.3.m3.1.1.3.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.3.m3.1b"><apply id="S2.p2.3.m3.1.1.cmml" xref="S2.p2.3.m3.1.1"><in id="S2.p2.3.m3.1.1.1.cmml" xref="S2.p2.3.m3.1.1.1"></in><apply id="S2.p2.3.m3.1.1.2.cmml" xref="S2.p2.3.m3.1.1.2"><csymbol cd="ambiguous" id="S2.p2.3.m3.1.1.2.1.cmml" xref="S2.p2.3.m3.1.1.2">subscript</csymbol><ci id="S2.p2.3.m3.1.1.2.2a.cmml" xref="S2.p2.3.m3.1.1.2.2"><mtext class="ltx_mathvariant_bold" id="S2.p2.3.m3.1.1.2.2.cmml" xref="S2.p2.3.m3.1.1.2.2">W</mtext></ci><ci id="S2.p2.3.m3.1.1.2.3.cmml" xref="S2.p2.3.m3.1.1.2.3">𝑣</ci></apply><apply id="S2.p2.3.m3.1.1.3.cmml" xref="S2.p2.3.m3.1.1.3"><csymbol cd="ambiguous" id="S2.p2.3.m3.1.1.3.1.cmml" xref="S2.p2.3.m3.1.1.3">superscript</csymbol><ci id="S2.p2.3.m3.1.1.3.2.cmml" xref="S2.p2.3.m3.1.1.3.2">𝐑</ci><apply id="S2.p2.3.m3.1.1.3.3.cmml" xref="S2.p2.3.m3.1.1.3.3"><times id="S2.p2.3.m3.1.1.3.3.1.cmml" xref="S2.p2.3.m3.1.1.3.3.1"></times><apply id="S2.p2.3.m3.1.1.3.3.2.cmml" xref="S2.p2.3.m3.1.1.3.3.2"><csymbol cd="ambiguous" id="S2.p2.3.m3.1.1.3.3.2.1.cmml" xref="S2.p2.3.m3.1.1.3.3.2">subscript</csymbol><ci id="S2.p2.3.m3.1.1.3.3.2.2.cmml" xref="S2.p2.3.m3.1.1.3.3.2.2">𝑑</ci><ci id="S2.p2.3.m3.1.1.3.3.2.3.cmml" xref="S2.p2.3.m3.1.1.3.3.2.3">𝑣</ci></apply><ci id="S2.p2.3.m3.1.1.3.3.3.cmml" xref="S2.p2.3.m3.1.1.3.3.3">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.3.m3.1c">\textbf{W}_{v}\in\mathbf{R}^{d_{v}\times d}</annotation></semantics></math> to obtain query, key and value vectors, shown by <span id="S2.p2.6.1" class="ltx_text ltx_markedasmath ltx_font_bold">q</span>, <span id="S2.p2.6.2" class="ltx_text ltx_markedasmath ltx_font_bold">k</span>, <span id="S2.p2.6.3" class="ltx_text ltx_markedasmath ltx_font_bold">v</span>, respectively. In other words,</p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.4" class="ltx_Math" alttext="\textbf{q}_{i}=\textbf{W}_{q}\textbf{x}_{i},\quad\textbf{k}_{i}=\textbf{W}_{k}\textbf{x}_{i},\quad\textbf{v}_{i}=\textbf{W}_{v}\textbf{x}_{i},\quad i=1,\cdots,T," display="block"><semantics id="S2.E1.m1.4a"><mrow id="S2.E1.m1.4.4.1"><mrow id="S2.E1.m1.4.4.1.1.2" xref="S2.E1.m1.4.4.1.1.3.cmml"><mrow id="S2.E1.m1.4.4.1.1.1.1" xref="S2.E1.m1.4.4.1.1.1.1.cmml"><msub id="S2.E1.m1.4.4.1.1.1.1.2" xref="S2.E1.m1.4.4.1.1.1.1.2.cmml"><mtext class="ltx_mathvariant_bold" id="S2.E1.m1.4.4.1.1.1.1.2.2" xref="S2.E1.m1.4.4.1.1.1.1.2.2a.cmml">q</mtext><mi id="S2.E1.m1.4.4.1.1.1.1.2.3" xref="S2.E1.m1.4.4.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S2.E1.m1.4.4.1.1.1.1.1" xref="S2.E1.m1.4.4.1.1.1.1.1.cmml">=</mo><mrow id="S2.E1.m1.4.4.1.1.1.1.3" xref="S2.E1.m1.4.4.1.1.1.1.3.cmml"><msub id="S2.E1.m1.4.4.1.1.1.1.3.2" xref="S2.E1.m1.4.4.1.1.1.1.3.2.cmml"><mtext class="ltx_mathvariant_bold" id="S2.E1.m1.4.4.1.1.1.1.3.2.2" xref="S2.E1.m1.4.4.1.1.1.1.3.2.2a.cmml">W</mtext><mi id="S2.E1.m1.4.4.1.1.1.1.3.2.3" xref="S2.E1.m1.4.4.1.1.1.1.3.2.3.cmml">q</mi></msub><mo lspace="0em" rspace="0em" id="S2.E1.m1.4.4.1.1.1.1.3.1" xref="S2.E1.m1.4.4.1.1.1.1.3.1.cmml">​</mo><msub id="S2.E1.m1.4.4.1.1.1.1.3.3" xref="S2.E1.m1.4.4.1.1.1.1.3.3.cmml"><mtext class="ltx_mathvariant_bold" id="S2.E1.m1.4.4.1.1.1.1.3.3.2" xref="S2.E1.m1.4.4.1.1.1.1.3.3.2a.cmml">x</mtext><mi id="S2.E1.m1.4.4.1.1.1.1.3.3.3" xref="S2.E1.m1.4.4.1.1.1.1.3.3.3.cmml">i</mi></msub></mrow></mrow><mo rspace="1.167em" id="S2.E1.m1.4.4.1.1.2.3" xref="S2.E1.m1.4.4.1.1.3a.cmml">,</mo><mrow id="S2.E1.m1.4.4.1.1.2.2.2" xref="S2.E1.m1.4.4.1.1.2.2.3.cmml"><mrow id="S2.E1.m1.4.4.1.1.2.2.1.1" xref="S2.E1.m1.4.4.1.1.2.2.1.1.cmml"><msub id="S2.E1.m1.4.4.1.1.2.2.1.1.2" xref="S2.E1.m1.4.4.1.1.2.2.1.1.2.cmml"><mtext class="ltx_mathvariant_bold" id="S2.E1.m1.4.4.1.1.2.2.1.1.2.2" xref="S2.E1.m1.4.4.1.1.2.2.1.1.2.2a.cmml">k</mtext><mi id="S2.E1.m1.4.4.1.1.2.2.1.1.2.3" xref="S2.E1.m1.4.4.1.1.2.2.1.1.2.3.cmml">i</mi></msub><mo id="S2.E1.m1.4.4.1.1.2.2.1.1.1" xref="S2.E1.m1.4.4.1.1.2.2.1.1.1.cmml">=</mo><mrow id="S2.E1.m1.4.4.1.1.2.2.1.1.3" xref="S2.E1.m1.4.4.1.1.2.2.1.1.3.cmml"><msub id="S2.E1.m1.4.4.1.1.2.2.1.1.3.2" xref="S2.E1.m1.4.4.1.1.2.2.1.1.3.2.cmml"><mtext class="ltx_mathvariant_bold" id="S2.E1.m1.4.4.1.1.2.2.1.1.3.2.2" xref="S2.E1.m1.4.4.1.1.2.2.1.1.3.2.2a.cmml">W</mtext><mi id="S2.E1.m1.4.4.1.1.2.2.1.1.3.2.3" xref="S2.E1.m1.4.4.1.1.2.2.1.1.3.2.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="S2.E1.m1.4.4.1.1.2.2.1.1.3.1" xref="S2.E1.m1.4.4.1.1.2.2.1.1.3.1.cmml">​</mo><msub id="S2.E1.m1.4.4.1.1.2.2.1.1.3.3" xref="S2.E1.m1.4.4.1.1.2.2.1.1.3.3.cmml"><mtext class="ltx_mathvariant_bold" id="S2.E1.m1.4.4.1.1.2.2.1.1.3.3.2" xref="S2.E1.m1.4.4.1.1.2.2.1.1.3.3.2a.cmml">x</mtext><mi id="S2.E1.m1.4.4.1.1.2.2.1.1.3.3.3" xref="S2.E1.m1.4.4.1.1.2.2.1.1.3.3.3.cmml">i</mi></msub></mrow></mrow><mo rspace="1.167em" id="S2.E1.m1.4.4.1.1.2.2.2.3" xref="S2.E1.m1.4.4.1.1.2.2.3a.cmml">,</mo><mrow id="S2.E1.m1.4.4.1.1.2.2.2.2.2" xref="S2.E1.m1.4.4.1.1.2.2.2.2.3.cmml"><mrow id="S2.E1.m1.4.4.1.1.2.2.2.2.1.1" xref="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.cmml"><msub id="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.2" xref="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.2.cmml"><mtext class="ltx_mathvariant_bold" id="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.2.2" xref="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.2.2a.cmml">v</mtext><mi id="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.2.3" xref="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.2.3.cmml">i</mi></msub><mo id="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.1" xref="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.1.cmml">=</mo><mrow id="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.3" xref="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.3.cmml"><msub id="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.3.2" xref="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.3.2.cmml"><mtext class="ltx_mathvariant_bold" id="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.3.2.2" xref="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.3.2.2a.cmml">W</mtext><mi id="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.3.2.3" xref="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.3.2.3.cmml">v</mi></msub><mo lspace="0em" rspace="0em" id="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.3.1" xref="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.3.1.cmml">​</mo><msub id="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.3.3" xref="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.3.3.cmml"><mtext class="ltx_mathvariant_bold" id="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.3.3.2" xref="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.3.3.2a.cmml">x</mtext><mi id="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.3.3.3" xref="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.3.3.3.cmml">i</mi></msub></mrow></mrow><mo rspace="1.167em" id="S2.E1.m1.4.4.1.1.2.2.2.2.2.3" xref="S2.E1.m1.4.4.1.1.2.2.2.2.3a.cmml">,</mo><mrow id="S2.E1.m1.4.4.1.1.2.2.2.2.2.2" xref="S2.E1.m1.4.4.1.1.2.2.2.2.2.2.cmml"><mi id="S2.E1.m1.4.4.1.1.2.2.2.2.2.2.2" xref="S2.E1.m1.4.4.1.1.2.2.2.2.2.2.2.cmml">i</mi><mo id="S2.E1.m1.4.4.1.1.2.2.2.2.2.2.1" xref="S2.E1.m1.4.4.1.1.2.2.2.2.2.2.1.cmml">=</mo><mrow id="S2.E1.m1.4.4.1.1.2.2.2.2.2.2.3.2" xref="S2.E1.m1.4.4.1.1.2.2.2.2.2.2.3.1.cmml"><mn id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">1</mn><mo id="S2.E1.m1.4.4.1.1.2.2.2.2.2.2.3.2.1" xref="S2.E1.m1.4.4.1.1.2.2.2.2.2.2.3.1.cmml">,</mo><mi mathvariant="normal" id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml">⋯</mi><mo id="S2.E1.m1.4.4.1.1.2.2.2.2.2.2.3.2.2" xref="S2.E1.m1.4.4.1.1.2.2.2.2.2.2.3.1.cmml">,</mo><mi id="S2.E1.m1.3.3" xref="S2.E1.m1.3.3.cmml">T</mi></mrow></mrow></mrow></mrow></mrow><mo id="S2.E1.m1.4.4.1.2">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.4b"><apply id="S2.E1.m1.4.4.1.1.3.cmml" xref="S2.E1.m1.4.4.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.1.1.3a.cmml" xref="S2.E1.m1.4.4.1.1.2.3">formulae-sequence</csymbol><apply id="S2.E1.m1.4.4.1.1.1.1.cmml" xref="S2.E1.m1.4.4.1.1.1.1"><eq id="S2.E1.m1.4.4.1.1.1.1.1.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1"></eq><apply id="S2.E1.m1.4.4.1.1.1.1.2.cmml" xref="S2.E1.m1.4.4.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.1.1.1.1.2.1.cmml" xref="S2.E1.m1.4.4.1.1.1.1.2">subscript</csymbol><ci id="S2.E1.m1.4.4.1.1.1.1.2.2a.cmml" xref="S2.E1.m1.4.4.1.1.1.1.2.2"><mtext class="ltx_mathvariant_bold" id="S2.E1.m1.4.4.1.1.1.1.2.2.cmml" xref="S2.E1.m1.4.4.1.1.1.1.2.2">q</mtext></ci><ci id="S2.E1.m1.4.4.1.1.1.1.2.3.cmml" xref="S2.E1.m1.4.4.1.1.1.1.2.3">𝑖</ci></apply><apply id="S2.E1.m1.4.4.1.1.1.1.3.cmml" xref="S2.E1.m1.4.4.1.1.1.1.3"><times id="S2.E1.m1.4.4.1.1.1.1.3.1.cmml" xref="S2.E1.m1.4.4.1.1.1.1.3.1"></times><apply id="S2.E1.m1.4.4.1.1.1.1.3.2.cmml" xref="S2.E1.m1.4.4.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.1.1.1.1.3.2.1.cmml" xref="S2.E1.m1.4.4.1.1.1.1.3.2">subscript</csymbol><ci id="S2.E1.m1.4.4.1.1.1.1.3.2.2a.cmml" xref="S2.E1.m1.4.4.1.1.1.1.3.2.2"><mtext class="ltx_mathvariant_bold" id="S2.E1.m1.4.4.1.1.1.1.3.2.2.cmml" xref="S2.E1.m1.4.4.1.1.1.1.3.2.2">W</mtext></ci><ci id="S2.E1.m1.4.4.1.1.1.1.3.2.3.cmml" xref="S2.E1.m1.4.4.1.1.1.1.3.2.3">𝑞</ci></apply><apply id="S2.E1.m1.4.4.1.1.1.1.3.3.cmml" xref="S2.E1.m1.4.4.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.1.1.1.1.3.3.1.cmml" xref="S2.E1.m1.4.4.1.1.1.1.3.3">subscript</csymbol><ci id="S2.E1.m1.4.4.1.1.1.1.3.3.2a.cmml" xref="S2.E1.m1.4.4.1.1.1.1.3.3.2"><mtext class="ltx_mathvariant_bold" id="S2.E1.m1.4.4.1.1.1.1.3.3.2.cmml" xref="S2.E1.m1.4.4.1.1.1.1.3.3.2">x</mtext></ci><ci id="S2.E1.m1.4.4.1.1.1.1.3.3.3.cmml" xref="S2.E1.m1.4.4.1.1.1.1.3.3.3">𝑖</ci></apply></apply></apply><apply id="S2.E1.m1.4.4.1.1.2.2.3.cmml" xref="S2.E1.m1.4.4.1.1.2.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.1.1.2.2.3a.cmml" xref="S2.E1.m1.4.4.1.1.2.2.2.3">formulae-sequence</csymbol><apply id="S2.E1.m1.4.4.1.1.2.2.1.1.cmml" xref="S2.E1.m1.4.4.1.1.2.2.1.1"><eq id="S2.E1.m1.4.4.1.1.2.2.1.1.1.cmml" xref="S2.E1.m1.4.4.1.1.2.2.1.1.1"></eq><apply id="S2.E1.m1.4.4.1.1.2.2.1.1.2.cmml" xref="S2.E1.m1.4.4.1.1.2.2.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.1.1.2.2.1.1.2.1.cmml" xref="S2.E1.m1.4.4.1.1.2.2.1.1.2">subscript</csymbol><ci id="S2.E1.m1.4.4.1.1.2.2.1.1.2.2a.cmml" xref="S2.E1.m1.4.4.1.1.2.2.1.1.2.2"><mtext class="ltx_mathvariant_bold" id="S2.E1.m1.4.4.1.1.2.2.1.1.2.2.cmml" xref="S2.E1.m1.4.4.1.1.2.2.1.1.2.2">k</mtext></ci><ci id="S2.E1.m1.4.4.1.1.2.2.1.1.2.3.cmml" xref="S2.E1.m1.4.4.1.1.2.2.1.1.2.3">𝑖</ci></apply><apply id="S2.E1.m1.4.4.1.1.2.2.1.1.3.cmml" xref="S2.E1.m1.4.4.1.1.2.2.1.1.3"><times id="S2.E1.m1.4.4.1.1.2.2.1.1.3.1.cmml" xref="S2.E1.m1.4.4.1.1.2.2.1.1.3.1"></times><apply id="S2.E1.m1.4.4.1.1.2.2.1.1.3.2.cmml" xref="S2.E1.m1.4.4.1.1.2.2.1.1.3.2"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.1.1.2.2.1.1.3.2.1.cmml" xref="S2.E1.m1.4.4.1.1.2.2.1.1.3.2">subscript</csymbol><ci id="S2.E1.m1.4.4.1.1.2.2.1.1.3.2.2a.cmml" xref="S2.E1.m1.4.4.1.1.2.2.1.1.3.2.2"><mtext class="ltx_mathvariant_bold" id="S2.E1.m1.4.4.1.1.2.2.1.1.3.2.2.cmml" xref="S2.E1.m1.4.4.1.1.2.2.1.1.3.2.2">W</mtext></ci><ci id="S2.E1.m1.4.4.1.1.2.2.1.1.3.2.3.cmml" xref="S2.E1.m1.4.4.1.1.2.2.1.1.3.2.3">𝑘</ci></apply><apply id="S2.E1.m1.4.4.1.1.2.2.1.1.3.3.cmml" xref="S2.E1.m1.4.4.1.1.2.2.1.1.3.3"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.1.1.2.2.1.1.3.3.1.cmml" xref="S2.E1.m1.4.4.1.1.2.2.1.1.3.3">subscript</csymbol><ci id="S2.E1.m1.4.4.1.1.2.2.1.1.3.3.2a.cmml" xref="S2.E1.m1.4.4.1.1.2.2.1.1.3.3.2"><mtext class="ltx_mathvariant_bold" id="S2.E1.m1.4.4.1.1.2.2.1.1.3.3.2.cmml" xref="S2.E1.m1.4.4.1.1.2.2.1.1.3.3.2">x</mtext></ci><ci id="S2.E1.m1.4.4.1.1.2.2.1.1.3.3.3.cmml" xref="S2.E1.m1.4.4.1.1.2.2.1.1.3.3.3">𝑖</ci></apply></apply></apply><apply id="S2.E1.m1.4.4.1.1.2.2.2.2.3.cmml" xref="S2.E1.m1.4.4.1.1.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.1.1.2.2.2.2.3a.cmml" xref="S2.E1.m1.4.4.1.1.2.2.2.2.2.3">formulae-sequence</csymbol><apply id="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.cmml" xref="S2.E1.m1.4.4.1.1.2.2.2.2.1.1"><eq id="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.1.cmml" xref="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.1"></eq><apply id="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.2.cmml" xref="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.2.1.cmml" xref="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.2">subscript</csymbol><ci id="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.2.2a.cmml" xref="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.2.2"><mtext class="ltx_mathvariant_bold" id="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.2.2.cmml" xref="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.2.2">v</mtext></ci><ci id="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.2.3.cmml" xref="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.2.3">𝑖</ci></apply><apply id="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.3.cmml" xref="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.3"><times id="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.3.1.cmml" xref="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.3.1"></times><apply id="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.3.2.cmml" xref="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.3.2"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.3.2.1.cmml" xref="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.3.2">subscript</csymbol><ci id="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.3.2.2a.cmml" xref="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.3.2.2"><mtext class="ltx_mathvariant_bold" id="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.3.2.2.cmml" xref="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.3.2.2">W</mtext></ci><ci id="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.3.2.3.cmml" xref="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.3.2.3">𝑣</ci></apply><apply id="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.3.3.cmml" xref="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.3.3"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.3.3.1.cmml" xref="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.3.3">subscript</csymbol><ci id="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.3.3.2a.cmml" xref="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.3.3.2"><mtext class="ltx_mathvariant_bold" id="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.3.3.2.cmml" xref="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.3.3.2">x</mtext></ci><ci id="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.3.3.3.cmml" xref="S2.E1.m1.4.4.1.1.2.2.2.2.1.1.3.3.3">𝑖</ci></apply></apply></apply><apply id="S2.E1.m1.4.4.1.1.2.2.2.2.2.2.cmml" xref="S2.E1.m1.4.4.1.1.2.2.2.2.2.2"><eq id="S2.E1.m1.4.4.1.1.2.2.2.2.2.2.1.cmml" xref="S2.E1.m1.4.4.1.1.2.2.2.2.2.2.1"></eq><ci id="S2.E1.m1.4.4.1.1.2.2.2.2.2.2.2.cmml" xref="S2.E1.m1.4.4.1.1.2.2.2.2.2.2.2">𝑖</ci><list id="S2.E1.m1.4.4.1.1.2.2.2.2.2.2.3.1.cmml" xref="S2.E1.m1.4.4.1.1.2.2.2.2.2.2.3.2"><cn type="integer" id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">1</cn><ci id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2">⋯</ci><ci id="S2.E1.m1.3.3.cmml" xref="S2.E1.m1.3.3">𝑇</ci></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.4c">\textbf{q}_{i}=\textbf{W}_{q}\textbf{x}_{i},\quad\textbf{k}_{i}=\textbf{W}_{k}\textbf{x}_{i},\quad\textbf{v}_{i}=\textbf{W}_{v}\textbf{x}_{i},\quad i=1,\cdots,T,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S2.p2.8" class="ltx_p">where <math id="S2.p2.7.m1.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S2.p2.7.m1.1a"><mi id="S2.p2.7.m1.1.1" xref="S2.p2.7.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S2.p2.7.m1.1b"><ci id="S2.p2.7.m1.1.1.cmml" xref="S2.p2.7.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.7.m1.1c">T</annotation></semantics></math> is the total number of tokens and each token is denoted by <span id="S2.p2.8.1" class="ltx_text ltx_markedasmath ltx_font_bold">x</span>. The output of the Multi-Head Attention block is given by</p>
<table id="S2.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E2.m1.5" class="ltx_Math" alttext="\text{MH Attention}(\textbf{Q},\textbf{K},\textbf{V})=\text{Concat}(\text{head}_{1},\cdots,\text{head}_{h})\textbf{W}^{O}." display="block"><semantics id="S2.E2.m1.5a"><mrow id="S2.E2.m1.5.5.1" xref="S2.E2.m1.5.5.1.1.cmml"><mrow id="S2.E2.m1.5.5.1.1" xref="S2.E2.m1.5.5.1.1.cmml"><mrow id="S2.E2.m1.5.5.1.1.4" xref="S2.E2.m1.5.5.1.1.4.cmml"><mtext id="S2.E2.m1.5.5.1.1.4.2" xref="S2.E2.m1.5.5.1.1.4.2a.cmml">MH Attention</mtext><mo lspace="0em" rspace="0em" id="S2.E2.m1.5.5.1.1.4.1" xref="S2.E2.m1.5.5.1.1.4.1.cmml">​</mo><mrow id="S2.E2.m1.5.5.1.1.4.3.2" xref="S2.E2.m1.5.5.1.1.4.3.1.cmml"><mo stretchy="false" id="S2.E2.m1.5.5.1.1.4.3.2.1" xref="S2.E2.m1.5.5.1.1.4.3.1.cmml">(</mo><mtext class="ltx_mathvariant_bold" id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1a.cmml">Q</mtext><mo id="S2.E2.m1.5.5.1.1.4.3.2.2" xref="S2.E2.m1.5.5.1.1.4.3.1.cmml">,</mo><mtext class="ltx_mathvariant_bold" id="S2.E2.m1.2.2" xref="S2.E2.m1.2.2a.cmml">K</mtext><mo id="S2.E2.m1.5.5.1.1.4.3.2.3" xref="S2.E2.m1.5.5.1.1.4.3.1.cmml">,</mo><mtext class="ltx_mathvariant_bold" id="S2.E2.m1.3.3" xref="S2.E2.m1.3.3a.cmml">V</mtext><mo stretchy="false" id="S2.E2.m1.5.5.1.1.4.3.2.4" xref="S2.E2.m1.5.5.1.1.4.3.1.cmml">)</mo></mrow></mrow><mo id="S2.E2.m1.5.5.1.1.3" xref="S2.E2.m1.5.5.1.1.3.cmml">=</mo><mrow id="S2.E2.m1.5.5.1.1.2" xref="S2.E2.m1.5.5.1.1.2.cmml"><mtext id="S2.E2.m1.5.5.1.1.2.4" xref="S2.E2.m1.5.5.1.1.2.4a.cmml">Concat</mtext><mo lspace="0em" rspace="0em" id="S2.E2.m1.5.5.1.1.2.3" xref="S2.E2.m1.5.5.1.1.2.3.cmml">​</mo><mrow id="S2.E2.m1.5.5.1.1.2.2.2" xref="S2.E2.m1.5.5.1.1.2.2.3.cmml"><mo stretchy="false" id="S2.E2.m1.5.5.1.1.2.2.2.3" xref="S2.E2.m1.5.5.1.1.2.2.3.cmml">(</mo><msub id="S2.E2.m1.5.5.1.1.1.1.1.1" xref="S2.E2.m1.5.5.1.1.1.1.1.1.cmml"><mtext id="S2.E2.m1.5.5.1.1.1.1.1.1.2" xref="S2.E2.m1.5.5.1.1.1.1.1.1.2a.cmml">head</mtext><mn id="S2.E2.m1.5.5.1.1.1.1.1.1.3" xref="S2.E2.m1.5.5.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S2.E2.m1.5.5.1.1.2.2.2.4" xref="S2.E2.m1.5.5.1.1.2.2.3.cmml">,</mo><mi mathvariant="normal" id="S2.E2.m1.4.4" xref="S2.E2.m1.4.4.cmml">⋯</mi><mo id="S2.E2.m1.5.5.1.1.2.2.2.5" xref="S2.E2.m1.5.5.1.1.2.2.3.cmml">,</mo><msub id="S2.E2.m1.5.5.1.1.2.2.2.2" xref="S2.E2.m1.5.5.1.1.2.2.2.2.cmml"><mtext id="S2.E2.m1.5.5.1.1.2.2.2.2.2" xref="S2.E2.m1.5.5.1.1.2.2.2.2.2a.cmml">head</mtext><mi id="S2.E2.m1.5.5.1.1.2.2.2.2.3" xref="S2.E2.m1.5.5.1.1.2.2.2.2.3.cmml">h</mi></msub><mo stretchy="false" id="S2.E2.m1.5.5.1.1.2.2.2.6" xref="S2.E2.m1.5.5.1.1.2.2.3.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S2.E2.m1.5.5.1.1.2.3a" xref="S2.E2.m1.5.5.1.1.2.3.cmml">​</mo><msup id="S2.E2.m1.5.5.1.1.2.5" xref="S2.E2.m1.5.5.1.1.2.5.cmml"><mtext class="ltx_mathvariant_bold" id="S2.E2.m1.5.5.1.1.2.5.2" xref="S2.E2.m1.5.5.1.1.2.5.2a.cmml">W</mtext><mi id="S2.E2.m1.5.5.1.1.2.5.3" xref="S2.E2.m1.5.5.1.1.2.5.3.cmml">O</mi></msup></mrow></mrow><mo lspace="0em" id="S2.E2.m1.5.5.1.2" xref="S2.E2.m1.5.5.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.5b"><apply id="S2.E2.m1.5.5.1.1.cmml" xref="S2.E2.m1.5.5.1"><eq id="S2.E2.m1.5.5.1.1.3.cmml" xref="S2.E2.m1.5.5.1.1.3"></eq><apply id="S2.E2.m1.5.5.1.1.4.cmml" xref="S2.E2.m1.5.5.1.1.4"><times id="S2.E2.m1.5.5.1.1.4.1.cmml" xref="S2.E2.m1.5.5.1.1.4.1"></times><ci id="S2.E2.m1.5.5.1.1.4.2a.cmml" xref="S2.E2.m1.5.5.1.1.4.2"><mtext id="S2.E2.m1.5.5.1.1.4.2.cmml" xref="S2.E2.m1.5.5.1.1.4.2">MH Attention</mtext></ci><vector id="S2.E2.m1.5.5.1.1.4.3.1.cmml" xref="S2.E2.m1.5.5.1.1.4.3.2"><ci id="S2.E2.m1.1.1a.cmml" xref="S2.E2.m1.1.1"><mtext class="ltx_mathvariant_bold" id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1">Q</mtext></ci><ci id="S2.E2.m1.2.2a.cmml" xref="S2.E2.m1.2.2"><mtext class="ltx_mathvariant_bold" id="S2.E2.m1.2.2.cmml" xref="S2.E2.m1.2.2">K</mtext></ci><ci id="S2.E2.m1.3.3a.cmml" xref="S2.E2.m1.3.3"><mtext class="ltx_mathvariant_bold" id="S2.E2.m1.3.3.cmml" xref="S2.E2.m1.3.3">V</mtext></ci></vector></apply><apply id="S2.E2.m1.5.5.1.1.2.cmml" xref="S2.E2.m1.5.5.1.1.2"><times id="S2.E2.m1.5.5.1.1.2.3.cmml" xref="S2.E2.m1.5.5.1.1.2.3"></times><ci id="S2.E2.m1.5.5.1.1.2.4a.cmml" xref="S2.E2.m1.5.5.1.1.2.4"><mtext id="S2.E2.m1.5.5.1.1.2.4.cmml" xref="S2.E2.m1.5.5.1.1.2.4">Concat</mtext></ci><vector id="S2.E2.m1.5.5.1.1.2.2.3.cmml" xref="S2.E2.m1.5.5.1.1.2.2.2"><apply id="S2.E2.m1.5.5.1.1.1.1.1.1.cmml" xref="S2.E2.m1.5.5.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E2.m1.5.5.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.5.5.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E2.m1.5.5.1.1.1.1.1.1.2a.cmml" xref="S2.E2.m1.5.5.1.1.1.1.1.1.2"><mtext id="S2.E2.m1.5.5.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.5.5.1.1.1.1.1.1.2">head</mtext></ci><cn type="integer" id="S2.E2.m1.5.5.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.5.5.1.1.1.1.1.1.3">1</cn></apply><ci id="S2.E2.m1.4.4.cmml" xref="S2.E2.m1.4.4">⋯</ci><apply id="S2.E2.m1.5.5.1.1.2.2.2.2.cmml" xref="S2.E2.m1.5.5.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S2.E2.m1.5.5.1.1.2.2.2.2.1.cmml" xref="S2.E2.m1.5.5.1.1.2.2.2.2">subscript</csymbol><ci id="S2.E2.m1.5.5.1.1.2.2.2.2.2a.cmml" xref="S2.E2.m1.5.5.1.1.2.2.2.2.2"><mtext id="S2.E2.m1.5.5.1.1.2.2.2.2.2.cmml" xref="S2.E2.m1.5.5.1.1.2.2.2.2.2">head</mtext></ci><ci id="S2.E2.m1.5.5.1.1.2.2.2.2.3.cmml" xref="S2.E2.m1.5.5.1.1.2.2.2.2.3">ℎ</ci></apply></vector><apply id="S2.E2.m1.5.5.1.1.2.5.cmml" xref="S2.E2.m1.5.5.1.1.2.5"><csymbol cd="ambiguous" id="S2.E2.m1.5.5.1.1.2.5.1.cmml" xref="S2.E2.m1.5.5.1.1.2.5">superscript</csymbol><ci id="S2.E2.m1.5.5.1.1.2.5.2a.cmml" xref="S2.E2.m1.5.5.1.1.2.5.2"><mtext class="ltx_mathvariant_bold" id="S2.E2.m1.5.5.1.1.2.5.2.cmml" xref="S2.E2.m1.5.5.1.1.2.5.2">W</mtext></ci><ci id="S2.E2.m1.5.5.1.1.2.5.3.cmml" xref="S2.E2.m1.5.5.1.1.2.5.3">𝑂</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.5c">\text{MH Attention}(\textbf{Q},\textbf{K},\textbf{V})=\text{Concat}(\text{head}_{1},\cdots,\text{head}_{h})\textbf{W}^{O}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S2.p2.10" class="ltx_p">where <math id="S2.p2.9.m1.1" class="ltx_Math" alttext="\textbf{W}^{O}\in\mathbf{R}^{hd_{v}\times d}" display="inline"><semantics id="S2.p2.9.m1.1a"><mrow id="S2.p2.9.m1.1.1" xref="S2.p2.9.m1.1.1.cmml"><msup id="S2.p2.9.m1.1.1.2" xref="S2.p2.9.m1.1.1.2.cmml"><mtext class="ltx_mathvariant_bold" id="S2.p2.9.m1.1.1.2.2" xref="S2.p2.9.m1.1.1.2.2a.cmml">W</mtext><mi id="S2.p2.9.m1.1.1.2.3" xref="S2.p2.9.m1.1.1.2.3.cmml">O</mi></msup><mo id="S2.p2.9.m1.1.1.1" xref="S2.p2.9.m1.1.1.1.cmml">∈</mo><msup id="S2.p2.9.m1.1.1.3" xref="S2.p2.9.m1.1.1.3.cmml"><mi id="S2.p2.9.m1.1.1.3.2" xref="S2.p2.9.m1.1.1.3.2.cmml">𝐑</mi><mrow id="S2.p2.9.m1.1.1.3.3" xref="S2.p2.9.m1.1.1.3.3.cmml"><mrow id="S2.p2.9.m1.1.1.3.3.2" xref="S2.p2.9.m1.1.1.3.3.2.cmml"><mi id="S2.p2.9.m1.1.1.3.3.2.2" xref="S2.p2.9.m1.1.1.3.3.2.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="S2.p2.9.m1.1.1.3.3.2.1" xref="S2.p2.9.m1.1.1.3.3.2.1.cmml">​</mo><msub id="S2.p2.9.m1.1.1.3.3.2.3" xref="S2.p2.9.m1.1.1.3.3.2.3.cmml"><mi id="S2.p2.9.m1.1.1.3.3.2.3.2" xref="S2.p2.9.m1.1.1.3.3.2.3.2.cmml">d</mi><mi id="S2.p2.9.m1.1.1.3.3.2.3.3" xref="S2.p2.9.m1.1.1.3.3.2.3.3.cmml">v</mi></msub></mrow><mo lspace="0.222em" rspace="0.222em" id="S2.p2.9.m1.1.1.3.3.1" xref="S2.p2.9.m1.1.1.3.3.1.cmml">×</mo><mi id="S2.p2.9.m1.1.1.3.3.3" xref="S2.p2.9.m1.1.1.3.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.9.m1.1b"><apply id="S2.p2.9.m1.1.1.cmml" xref="S2.p2.9.m1.1.1"><in id="S2.p2.9.m1.1.1.1.cmml" xref="S2.p2.9.m1.1.1.1"></in><apply id="S2.p2.9.m1.1.1.2.cmml" xref="S2.p2.9.m1.1.1.2"><csymbol cd="ambiguous" id="S2.p2.9.m1.1.1.2.1.cmml" xref="S2.p2.9.m1.1.1.2">superscript</csymbol><ci id="S2.p2.9.m1.1.1.2.2a.cmml" xref="S2.p2.9.m1.1.1.2.2"><mtext class="ltx_mathvariant_bold" id="S2.p2.9.m1.1.1.2.2.cmml" xref="S2.p2.9.m1.1.1.2.2">W</mtext></ci><ci id="S2.p2.9.m1.1.1.2.3.cmml" xref="S2.p2.9.m1.1.1.2.3">𝑂</ci></apply><apply id="S2.p2.9.m1.1.1.3.cmml" xref="S2.p2.9.m1.1.1.3"><csymbol cd="ambiguous" id="S2.p2.9.m1.1.1.3.1.cmml" xref="S2.p2.9.m1.1.1.3">superscript</csymbol><ci id="S2.p2.9.m1.1.1.3.2.cmml" xref="S2.p2.9.m1.1.1.3.2">𝐑</ci><apply id="S2.p2.9.m1.1.1.3.3.cmml" xref="S2.p2.9.m1.1.1.3.3"><times id="S2.p2.9.m1.1.1.3.3.1.cmml" xref="S2.p2.9.m1.1.1.3.3.1"></times><apply id="S2.p2.9.m1.1.1.3.3.2.cmml" xref="S2.p2.9.m1.1.1.3.3.2"><times id="S2.p2.9.m1.1.1.3.3.2.1.cmml" xref="S2.p2.9.m1.1.1.3.3.2.1"></times><ci id="S2.p2.9.m1.1.1.3.3.2.2.cmml" xref="S2.p2.9.m1.1.1.3.3.2.2">ℎ</ci><apply id="S2.p2.9.m1.1.1.3.3.2.3.cmml" xref="S2.p2.9.m1.1.1.3.3.2.3"><csymbol cd="ambiguous" id="S2.p2.9.m1.1.1.3.3.2.3.1.cmml" xref="S2.p2.9.m1.1.1.3.3.2.3">subscript</csymbol><ci id="S2.p2.9.m1.1.1.3.3.2.3.2.cmml" xref="S2.p2.9.m1.1.1.3.3.2.3.2">𝑑</ci><ci id="S2.p2.9.m1.1.1.3.3.2.3.3.cmml" xref="S2.p2.9.m1.1.1.3.3.2.3.3">𝑣</ci></apply></apply><ci id="S2.p2.9.m1.1.1.3.3.3.cmml" xref="S2.p2.9.m1.1.1.3.3.3">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.9.m1.1c">\textbf{W}^{O}\in\mathbf{R}^{hd_{v}\times d}</annotation></semantics></math>, <math id="S2.p2.10.m2.1" class="ltx_Math" alttext="d_{k}=d_{q}" display="inline"><semantics id="S2.p2.10.m2.1a"><mrow id="S2.p2.10.m2.1.1" xref="S2.p2.10.m2.1.1.cmml"><msub id="S2.p2.10.m2.1.1.2" xref="S2.p2.10.m2.1.1.2.cmml"><mi id="S2.p2.10.m2.1.1.2.2" xref="S2.p2.10.m2.1.1.2.2.cmml">d</mi><mi id="S2.p2.10.m2.1.1.2.3" xref="S2.p2.10.m2.1.1.2.3.cmml">k</mi></msub><mo id="S2.p2.10.m2.1.1.1" xref="S2.p2.10.m2.1.1.1.cmml">=</mo><msub id="S2.p2.10.m2.1.1.3" xref="S2.p2.10.m2.1.1.3.cmml"><mi id="S2.p2.10.m2.1.1.3.2" xref="S2.p2.10.m2.1.1.3.2.cmml">d</mi><mi id="S2.p2.10.m2.1.1.3.3" xref="S2.p2.10.m2.1.1.3.3.cmml">q</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.10.m2.1b"><apply id="S2.p2.10.m2.1.1.cmml" xref="S2.p2.10.m2.1.1"><eq id="S2.p2.10.m2.1.1.1.cmml" xref="S2.p2.10.m2.1.1.1"></eq><apply id="S2.p2.10.m2.1.1.2.cmml" xref="S2.p2.10.m2.1.1.2"><csymbol cd="ambiguous" id="S2.p2.10.m2.1.1.2.1.cmml" xref="S2.p2.10.m2.1.1.2">subscript</csymbol><ci id="S2.p2.10.m2.1.1.2.2.cmml" xref="S2.p2.10.m2.1.1.2.2">𝑑</ci><ci id="S2.p2.10.m2.1.1.2.3.cmml" xref="S2.p2.10.m2.1.1.2.3">𝑘</ci></apply><apply id="S2.p2.10.m2.1.1.3.cmml" xref="S2.p2.10.m2.1.1.3"><csymbol cd="ambiguous" id="S2.p2.10.m2.1.1.3.1.cmml" xref="S2.p2.10.m2.1.1.3">subscript</csymbol><ci id="S2.p2.10.m2.1.1.3.2.cmml" xref="S2.p2.10.m2.1.1.3.2">𝑑</ci><ci id="S2.p2.10.m2.1.1.3.3.cmml" xref="S2.p2.10.m2.1.1.3.3">𝑞</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.10.m2.1c">d_{k}=d_{q}</annotation></semantics></math>, and</p>
<table id="S2.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E3.m1.2" class="ltx_Math" alttext="\text{head}_{h}=\text{Attention}(\textbf{Q}_{h},\textbf{K}_{h},\textbf{V}_{h})=\text{Softmax}(\frac{\textbf{K}_{h}^{\top}\textbf{Q}_{h}}{\sqrt{d_{k}}})\textbf{V}_{h}^{\top}." display="block"><semantics id="S2.E3.m1.2a"><mrow id="S2.E3.m1.2.2.1" xref="S2.E3.m1.2.2.1.1.cmml"><mrow id="S2.E3.m1.2.2.1.1" xref="S2.E3.m1.2.2.1.1.cmml"><msub id="S2.E3.m1.2.2.1.1.5" xref="S2.E3.m1.2.2.1.1.5.cmml"><mtext id="S2.E3.m1.2.2.1.1.5.2" xref="S2.E3.m1.2.2.1.1.5.2a.cmml">head</mtext><mi id="S2.E3.m1.2.2.1.1.5.3" xref="S2.E3.m1.2.2.1.1.5.3.cmml">h</mi></msub><mo id="S2.E3.m1.2.2.1.1.6" xref="S2.E3.m1.2.2.1.1.6.cmml">=</mo><mrow id="S2.E3.m1.2.2.1.1.3" xref="S2.E3.m1.2.2.1.1.3.cmml"><mtext id="S2.E3.m1.2.2.1.1.3.5" xref="S2.E3.m1.2.2.1.1.3.5a.cmml">Attention</mtext><mo lspace="0em" rspace="0em" id="S2.E3.m1.2.2.1.1.3.4" xref="S2.E3.m1.2.2.1.1.3.4.cmml">​</mo><mrow id="S2.E3.m1.2.2.1.1.3.3.3" xref="S2.E3.m1.2.2.1.1.3.3.4.cmml"><mo stretchy="false" id="S2.E3.m1.2.2.1.1.3.3.3.4" xref="S2.E3.m1.2.2.1.1.3.3.4.cmml">(</mo><msub id="S2.E3.m1.2.2.1.1.1.1.1.1" xref="S2.E3.m1.2.2.1.1.1.1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S2.E3.m1.2.2.1.1.1.1.1.1.2" xref="S2.E3.m1.2.2.1.1.1.1.1.1.2a.cmml">Q</mtext><mi id="S2.E3.m1.2.2.1.1.1.1.1.1.3" xref="S2.E3.m1.2.2.1.1.1.1.1.1.3.cmml">h</mi></msub><mo id="S2.E3.m1.2.2.1.1.3.3.3.5" xref="S2.E3.m1.2.2.1.1.3.3.4.cmml">,</mo><msub id="S2.E3.m1.2.2.1.1.2.2.2.2" xref="S2.E3.m1.2.2.1.1.2.2.2.2.cmml"><mtext class="ltx_mathvariant_bold" id="S2.E3.m1.2.2.1.1.2.2.2.2.2" xref="S2.E3.m1.2.2.1.1.2.2.2.2.2a.cmml">K</mtext><mi id="S2.E3.m1.2.2.1.1.2.2.2.2.3" xref="S2.E3.m1.2.2.1.1.2.2.2.2.3.cmml">h</mi></msub><mo id="S2.E3.m1.2.2.1.1.3.3.3.6" xref="S2.E3.m1.2.2.1.1.3.3.4.cmml">,</mo><msub id="S2.E3.m1.2.2.1.1.3.3.3.3" xref="S2.E3.m1.2.2.1.1.3.3.3.3.cmml"><mtext class="ltx_mathvariant_bold" id="S2.E3.m1.2.2.1.1.3.3.3.3.2" xref="S2.E3.m1.2.2.1.1.3.3.3.3.2a.cmml">V</mtext><mi id="S2.E3.m1.2.2.1.1.3.3.3.3.3" xref="S2.E3.m1.2.2.1.1.3.3.3.3.3.cmml">h</mi></msub><mo stretchy="false" id="S2.E3.m1.2.2.1.1.3.3.3.7" xref="S2.E3.m1.2.2.1.1.3.3.4.cmml">)</mo></mrow></mrow><mo id="S2.E3.m1.2.2.1.1.7" xref="S2.E3.m1.2.2.1.1.7.cmml">=</mo><mrow id="S2.E3.m1.2.2.1.1.8" xref="S2.E3.m1.2.2.1.1.8.cmml"><mtext id="S2.E3.m1.2.2.1.1.8.2" xref="S2.E3.m1.2.2.1.1.8.2a.cmml">Softmax</mtext><mo lspace="0em" rspace="0em" id="S2.E3.m1.2.2.1.1.8.1" xref="S2.E3.m1.2.2.1.1.8.1.cmml">​</mo><mrow id="S2.E3.m1.2.2.1.1.8.3.2" xref="S2.E3.m1.1.1.cmml"><mo stretchy="false" id="S2.E3.m1.2.2.1.1.8.3.2.1" xref="S2.E3.m1.1.1.cmml">(</mo><mfrac id="S2.E3.m1.1.1" xref="S2.E3.m1.1.1.cmml"><mrow id="S2.E3.m1.1.1.2" xref="S2.E3.m1.1.1.2.cmml"><msubsup id="S2.E3.m1.1.1.2.2" xref="S2.E3.m1.1.1.2.2.cmml"><mtext class="ltx_mathvariant_bold" id="S2.E3.m1.1.1.2.2.2.2" xref="S2.E3.m1.1.1.2.2.2.2a.cmml">K</mtext><mi id="S2.E3.m1.1.1.2.2.2.3" xref="S2.E3.m1.1.1.2.2.2.3.cmml">h</mi><mo id="S2.E3.m1.1.1.2.2.3" xref="S2.E3.m1.1.1.2.2.3.cmml">⊤</mo></msubsup><mo lspace="0em" rspace="0em" id="S2.E3.m1.1.1.2.1" xref="S2.E3.m1.1.1.2.1.cmml">​</mo><msub id="S2.E3.m1.1.1.2.3" xref="S2.E3.m1.1.1.2.3.cmml"><mtext class="ltx_mathvariant_bold" id="S2.E3.m1.1.1.2.3.2" xref="S2.E3.m1.1.1.2.3.2a.cmml">Q</mtext><mi id="S2.E3.m1.1.1.2.3.3" xref="S2.E3.m1.1.1.2.3.3.cmml">h</mi></msub></mrow><msqrt id="S2.E3.m1.1.1.3" xref="S2.E3.m1.1.1.3.cmml"><msub id="S2.E3.m1.1.1.3.2" xref="S2.E3.m1.1.1.3.2.cmml"><mi id="S2.E3.m1.1.1.3.2.2" xref="S2.E3.m1.1.1.3.2.2.cmml">d</mi><mi id="S2.E3.m1.1.1.3.2.3" xref="S2.E3.m1.1.1.3.2.3.cmml">k</mi></msub></msqrt></mfrac><mo stretchy="false" id="S2.E3.m1.2.2.1.1.8.3.2.2" xref="S2.E3.m1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S2.E3.m1.2.2.1.1.8.1a" xref="S2.E3.m1.2.2.1.1.8.1.cmml">​</mo><msubsup id="S2.E3.m1.2.2.1.1.8.4" xref="S2.E3.m1.2.2.1.1.8.4.cmml"><mtext class="ltx_mathvariant_bold" id="S2.E3.m1.2.2.1.1.8.4.2.2" xref="S2.E3.m1.2.2.1.1.8.4.2.2a.cmml">V</mtext><mi id="S2.E3.m1.2.2.1.1.8.4.2.3" xref="S2.E3.m1.2.2.1.1.8.4.2.3.cmml">h</mi><mo id="S2.E3.m1.2.2.1.1.8.4.3" xref="S2.E3.m1.2.2.1.1.8.4.3.cmml">⊤</mo></msubsup></mrow></mrow><mo lspace="0em" id="S2.E3.m1.2.2.1.2" xref="S2.E3.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.2b"><apply id="S2.E3.m1.2.2.1.1.cmml" xref="S2.E3.m1.2.2.1"><and id="S2.E3.m1.2.2.1.1a.cmml" xref="S2.E3.m1.2.2.1"></and><apply id="S2.E3.m1.2.2.1.1b.cmml" xref="S2.E3.m1.2.2.1"><eq id="S2.E3.m1.2.2.1.1.6.cmml" xref="S2.E3.m1.2.2.1.1.6"></eq><apply id="S2.E3.m1.2.2.1.1.5.cmml" xref="S2.E3.m1.2.2.1.1.5"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.5.1.cmml" xref="S2.E3.m1.2.2.1.1.5">subscript</csymbol><ci id="S2.E3.m1.2.2.1.1.5.2a.cmml" xref="S2.E3.m1.2.2.1.1.5.2"><mtext id="S2.E3.m1.2.2.1.1.5.2.cmml" xref="S2.E3.m1.2.2.1.1.5.2">head</mtext></ci><ci id="S2.E3.m1.2.2.1.1.5.3.cmml" xref="S2.E3.m1.2.2.1.1.5.3">ℎ</ci></apply><apply id="S2.E3.m1.2.2.1.1.3.cmml" xref="S2.E3.m1.2.2.1.1.3"><times id="S2.E3.m1.2.2.1.1.3.4.cmml" xref="S2.E3.m1.2.2.1.1.3.4"></times><ci id="S2.E3.m1.2.2.1.1.3.5a.cmml" xref="S2.E3.m1.2.2.1.1.3.5"><mtext id="S2.E3.m1.2.2.1.1.3.5.cmml" xref="S2.E3.m1.2.2.1.1.3.5">Attention</mtext></ci><vector id="S2.E3.m1.2.2.1.1.3.3.4.cmml" xref="S2.E3.m1.2.2.1.1.3.3.3"><apply id="S2.E3.m1.2.2.1.1.1.1.1.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E3.m1.2.2.1.1.1.1.1.1.2a.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.1.2"><mtext class="ltx_mathvariant_bold" id="S2.E3.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.1.2">Q</mtext></ci><ci id="S2.E3.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.1.3">ℎ</ci></apply><apply id="S2.E3.m1.2.2.1.1.2.2.2.2.cmml" xref="S2.E3.m1.2.2.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.2.2.2.2.1.cmml" xref="S2.E3.m1.2.2.1.1.2.2.2.2">subscript</csymbol><ci id="S2.E3.m1.2.2.1.1.2.2.2.2.2a.cmml" xref="S2.E3.m1.2.2.1.1.2.2.2.2.2"><mtext class="ltx_mathvariant_bold" id="S2.E3.m1.2.2.1.1.2.2.2.2.2.cmml" xref="S2.E3.m1.2.2.1.1.2.2.2.2.2">K</mtext></ci><ci id="S2.E3.m1.2.2.1.1.2.2.2.2.3.cmml" xref="S2.E3.m1.2.2.1.1.2.2.2.2.3">ℎ</ci></apply><apply id="S2.E3.m1.2.2.1.1.3.3.3.3.cmml" xref="S2.E3.m1.2.2.1.1.3.3.3.3"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.3.3.3.3.1.cmml" xref="S2.E3.m1.2.2.1.1.3.3.3.3">subscript</csymbol><ci id="S2.E3.m1.2.2.1.1.3.3.3.3.2a.cmml" xref="S2.E3.m1.2.2.1.1.3.3.3.3.2"><mtext class="ltx_mathvariant_bold" id="S2.E3.m1.2.2.1.1.3.3.3.3.2.cmml" xref="S2.E3.m1.2.2.1.1.3.3.3.3.2">V</mtext></ci><ci id="S2.E3.m1.2.2.1.1.3.3.3.3.3.cmml" xref="S2.E3.m1.2.2.1.1.3.3.3.3.3">ℎ</ci></apply></vector></apply></apply><apply id="S2.E3.m1.2.2.1.1c.cmml" xref="S2.E3.m1.2.2.1"><eq id="S2.E3.m1.2.2.1.1.7.cmml" xref="S2.E3.m1.2.2.1.1.7"></eq><share href="#S2.E3.m1.2.2.1.1.3.cmml" id="S2.E3.m1.2.2.1.1d.cmml" xref="S2.E3.m1.2.2.1"></share><apply id="S2.E3.m1.2.2.1.1.8.cmml" xref="S2.E3.m1.2.2.1.1.8"><times id="S2.E3.m1.2.2.1.1.8.1.cmml" xref="S2.E3.m1.2.2.1.1.8.1"></times><ci id="S2.E3.m1.2.2.1.1.8.2a.cmml" xref="S2.E3.m1.2.2.1.1.8.2"><mtext id="S2.E3.m1.2.2.1.1.8.2.cmml" xref="S2.E3.m1.2.2.1.1.8.2">Softmax</mtext></ci><apply id="S2.E3.m1.1.1.cmml" xref="S2.E3.m1.2.2.1.1.8.3.2"><divide id="S2.E3.m1.1.1.1.cmml" xref="S2.E3.m1.2.2.1.1.8.3.2"></divide><apply id="S2.E3.m1.1.1.2.cmml" xref="S2.E3.m1.1.1.2"><times id="S2.E3.m1.1.1.2.1.cmml" xref="S2.E3.m1.1.1.2.1"></times><apply id="S2.E3.m1.1.1.2.2.cmml" xref="S2.E3.m1.1.1.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.2.2.1.cmml" xref="S2.E3.m1.1.1.2.2">superscript</csymbol><apply id="S2.E3.m1.1.1.2.2.2.cmml" xref="S2.E3.m1.1.1.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.2.2.2.1.cmml" xref="S2.E3.m1.1.1.2.2">subscript</csymbol><ci id="S2.E3.m1.1.1.2.2.2.2a.cmml" xref="S2.E3.m1.1.1.2.2.2.2"><mtext class="ltx_mathvariant_bold" id="S2.E3.m1.1.1.2.2.2.2.cmml" xref="S2.E3.m1.1.1.2.2.2.2">K</mtext></ci><ci id="S2.E3.m1.1.1.2.2.2.3.cmml" xref="S2.E3.m1.1.1.2.2.2.3">ℎ</ci></apply><csymbol cd="latexml" id="S2.E3.m1.1.1.2.2.3.cmml" xref="S2.E3.m1.1.1.2.2.3">top</csymbol></apply><apply id="S2.E3.m1.1.1.2.3.cmml" xref="S2.E3.m1.1.1.2.3"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.2.3.1.cmml" xref="S2.E3.m1.1.1.2.3">subscript</csymbol><ci id="S2.E3.m1.1.1.2.3.2a.cmml" xref="S2.E3.m1.1.1.2.3.2"><mtext class="ltx_mathvariant_bold" id="S2.E3.m1.1.1.2.3.2.cmml" xref="S2.E3.m1.1.1.2.3.2">Q</mtext></ci><ci id="S2.E3.m1.1.1.2.3.3.cmml" xref="S2.E3.m1.1.1.2.3.3">ℎ</ci></apply></apply><apply id="S2.E3.m1.1.1.3.cmml" xref="S2.E3.m1.1.1.3"><root id="S2.E3.m1.1.1.3a.cmml" xref="S2.E3.m1.1.1.3"></root><apply id="S2.E3.m1.1.1.3.2.cmml" xref="S2.E3.m1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.3.2.1.cmml" xref="S2.E3.m1.1.1.3.2">subscript</csymbol><ci id="S2.E3.m1.1.1.3.2.2.cmml" xref="S2.E3.m1.1.1.3.2.2">𝑑</ci><ci id="S2.E3.m1.1.1.3.2.3.cmml" xref="S2.E3.m1.1.1.3.2.3">𝑘</ci></apply></apply></apply><apply id="S2.E3.m1.2.2.1.1.8.4.cmml" xref="S2.E3.m1.2.2.1.1.8.4"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.8.4.1.cmml" xref="S2.E3.m1.2.2.1.1.8.4">superscript</csymbol><apply id="S2.E3.m1.2.2.1.1.8.4.2.cmml" xref="S2.E3.m1.2.2.1.1.8.4"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.8.4.2.1.cmml" xref="S2.E3.m1.2.2.1.1.8.4">subscript</csymbol><ci id="S2.E3.m1.2.2.1.1.8.4.2.2a.cmml" xref="S2.E3.m1.2.2.1.1.8.4.2.2"><mtext class="ltx_mathvariant_bold" id="S2.E3.m1.2.2.1.1.8.4.2.2.cmml" xref="S2.E3.m1.2.2.1.1.8.4.2.2">V</mtext></ci><ci id="S2.E3.m1.2.2.1.1.8.4.2.3.cmml" xref="S2.E3.m1.2.2.1.1.8.4.2.3">ℎ</ci></apply><csymbol cd="latexml" id="S2.E3.m1.2.2.1.1.8.4.3.cmml" xref="S2.E3.m1.2.2.1.1.8.4.3">top</csymbol></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.2c">\text{head}_{h}=\text{Attention}(\textbf{Q}_{h},\textbf{K}_{h},\textbf{V}_{h})=\text{Softmax}(\frac{\textbf{K}_{h}^{\top}\textbf{Q}_{h}}{\sqrt{d_{k}}})\textbf{V}_{h}^{\top}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S2.p2.12" class="ltx_p">Finally, the results obtained from the previous steps are combined with a skip connection and a normalization block. These vectors are then individually passed through a fully connected layer, applying an activation function to introduce non-linearity into the network. The parameters of this block are shared across all vectors. This process is repeated for a total of <math id="S2.p2.11.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.p2.11.m1.1a"><mi id="S2.p2.11.m1.1.1" xref="S2.p2.11.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.p2.11.m1.1b"><ci id="S2.p2.11.m1.1.1.cmml" xref="S2.p2.11.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.11.m1.1c">N</annotation></semantics></math> times, corresponding to the number of layers in the deep network.
In the decoder module, a similar process is applied using the vectors generated in the encoder, while also consuming the previously generated predictions/outputs as additional input. Ultimately, the output probabilities for the possible output classes are computed. Attention is achieved through the dot product operation between the key and query matrices, Eq. (<a href="#S2.E3" title="In 2 Background ‣ Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>), which computes weights for the linear combination of the matrix <span id="S2.p2.12.1" class="ltx_text ltx_markedasmath ltx_font_bold">V</span>.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.11" class="ltx_p">An alternative representation for the transformer is also provided in</p>
<table id="S2.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E4.m1.4" class="ltx_Math" alttext="\text{MH Attention}^{i}=\sum_{h}\textbf{W}^{O}_{h}\big{[}\sum_{k=1}^{T}A_{hik}\textbf{W}_{v}\textbf{x}_{k}\big{]},i=1,\cdots,T," display="block"><semantics id="S2.E4.m1.4a"><mrow id="S2.E4.m1.4.4.1"><mrow id="S2.E4.m1.4.4.1.1.2" xref="S2.E4.m1.4.4.1.1.3.cmml"><mrow id="S2.E4.m1.4.4.1.1.1.1" xref="S2.E4.m1.4.4.1.1.1.1.cmml"><msup id="S2.E4.m1.4.4.1.1.1.1.3" xref="S2.E4.m1.4.4.1.1.1.1.3.cmml"><mtext id="S2.E4.m1.4.4.1.1.1.1.3.2" xref="S2.E4.m1.4.4.1.1.1.1.3.2a.cmml">MH Attention</mtext><mi id="S2.E4.m1.4.4.1.1.1.1.3.3" xref="S2.E4.m1.4.4.1.1.1.1.3.3.cmml">i</mi></msup><mo rspace="0.111em" id="S2.E4.m1.4.4.1.1.1.1.2" xref="S2.E4.m1.4.4.1.1.1.1.2.cmml">=</mo><mrow id="S2.E4.m1.4.4.1.1.1.1.1" xref="S2.E4.m1.4.4.1.1.1.1.1.cmml"><munder id="S2.E4.m1.4.4.1.1.1.1.1.2" xref="S2.E4.m1.4.4.1.1.1.1.1.2.cmml"><mo movablelimits="false" id="S2.E4.m1.4.4.1.1.1.1.1.2.2" xref="S2.E4.m1.4.4.1.1.1.1.1.2.2.cmml">∑</mo><mi id="S2.E4.m1.4.4.1.1.1.1.1.2.3" xref="S2.E4.m1.4.4.1.1.1.1.1.2.3.cmml">h</mi></munder><mrow id="S2.E4.m1.4.4.1.1.1.1.1.1" xref="S2.E4.m1.4.4.1.1.1.1.1.1.cmml"><msubsup id="S2.E4.m1.4.4.1.1.1.1.1.1.3" xref="S2.E4.m1.4.4.1.1.1.1.1.1.3.cmml"><mtext class="ltx_mathvariant_bold" id="S2.E4.m1.4.4.1.1.1.1.1.1.3.2.2" xref="S2.E4.m1.4.4.1.1.1.1.1.1.3.2.2a.cmml">W</mtext><mi id="S2.E4.m1.4.4.1.1.1.1.1.1.3.3" xref="S2.E4.m1.4.4.1.1.1.1.1.1.3.3.cmml">h</mi><mi id="S2.E4.m1.4.4.1.1.1.1.1.1.3.2.3" xref="S2.E4.m1.4.4.1.1.1.1.1.1.3.2.3.cmml">O</mi></msubsup><mo lspace="0em" rspace="0em" id="S2.E4.m1.4.4.1.1.1.1.1.1.2" xref="S2.E4.m1.4.4.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.2.cmml"><mo maxsize="120%" minsize="120%" id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.2" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.2.1.cmml">[</mo><mrow id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.cmml"><munderover id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.1" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml"><mo lspace="0em" movablelimits="false" id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.2" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.2.cmml">∑</mo><mrow id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.3" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.3.2" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.3.2.cmml">k</mi><mo id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.3.1" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.3.1.cmml">=</mo><mn id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.3.3" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.3.3.cmml">1</mn></mrow><mi id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.1.3.cmml">T</mi></munderover><mrow id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.cmml"><msub id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.2" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.cmml"><mi id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2.cmml">A</mi><mrow id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.3" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.3.cmml"><mi id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.3.2" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.3.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.3.1" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.3.1.cmml">​</mo><mi id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.3.3" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.3.1a" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.3.1.cmml">​</mo><mi id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.3.4" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.3.4.cmml">k</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.1" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.1.cmml">​</mo><msub id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.3" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.3.cmml"><mtext class="ltx_mathvariant_bold" id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.3.2" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.3.2a.cmml">W</mtext><mi id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.3.3" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.3.3.cmml">v</mi></msub><mo lspace="0em" rspace="0em" id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.1a" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.1.cmml">​</mo><msub id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.4" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.4.cmml"><mtext class="ltx_mathvariant_bold" id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.4.2" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.4.2a.cmml">x</mtext><mi id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.4.3" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.4.3.cmml">k</mi></msub></mrow></mrow><mo maxsize="120%" minsize="120%" id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.3" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.2.1.cmml">]</mo></mrow></mrow></mrow></mrow><mo id="S2.E4.m1.4.4.1.1.2.3" xref="S2.E4.m1.4.4.1.1.3a.cmml">,</mo><mrow id="S2.E4.m1.4.4.1.1.2.2" xref="S2.E4.m1.4.4.1.1.2.2.cmml"><mi id="S2.E4.m1.4.4.1.1.2.2.2" xref="S2.E4.m1.4.4.1.1.2.2.2.cmml">i</mi><mo id="S2.E4.m1.4.4.1.1.2.2.1" xref="S2.E4.m1.4.4.1.1.2.2.1.cmml">=</mo><mrow id="S2.E4.m1.4.4.1.1.2.2.3.2" xref="S2.E4.m1.4.4.1.1.2.2.3.1.cmml"><mn id="S2.E4.m1.1.1" xref="S2.E4.m1.1.1.cmml">1</mn><mo id="S2.E4.m1.4.4.1.1.2.2.3.2.1" xref="S2.E4.m1.4.4.1.1.2.2.3.1.cmml">,</mo><mi mathvariant="normal" id="S2.E4.m1.2.2" xref="S2.E4.m1.2.2.cmml">⋯</mi><mo id="S2.E4.m1.4.4.1.1.2.2.3.2.2" xref="S2.E4.m1.4.4.1.1.2.2.3.1.cmml">,</mo><mi id="S2.E4.m1.3.3" xref="S2.E4.m1.3.3.cmml">T</mi></mrow></mrow></mrow><mo id="S2.E4.m1.4.4.1.2">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E4.m1.4b"><apply id="S2.E4.m1.4.4.1.1.3.cmml" xref="S2.E4.m1.4.4.1.1.2"><csymbol cd="ambiguous" id="S2.E4.m1.4.4.1.1.3a.cmml" xref="S2.E4.m1.4.4.1.1.2.3">formulae-sequence</csymbol><apply id="S2.E4.m1.4.4.1.1.1.1.cmml" xref="S2.E4.m1.4.4.1.1.1.1"><eq id="S2.E4.m1.4.4.1.1.1.1.2.cmml" xref="S2.E4.m1.4.4.1.1.1.1.2"></eq><apply id="S2.E4.m1.4.4.1.1.1.1.3.cmml" xref="S2.E4.m1.4.4.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E4.m1.4.4.1.1.1.1.3.1.cmml" xref="S2.E4.m1.4.4.1.1.1.1.3">superscript</csymbol><ci id="S2.E4.m1.4.4.1.1.1.1.3.2a.cmml" xref="S2.E4.m1.4.4.1.1.1.1.3.2"><mtext id="S2.E4.m1.4.4.1.1.1.1.3.2.cmml" xref="S2.E4.m1.4.4.1.1.1.1.3.2">MH Attention</mtext></ci><ci id="S2.E4.m1.4.4.1.1.1.1.3.3.cmml" xref="S2.E4.m1.4.4.1.1.1.1.3.3">𝑖</ci></apply><apply id="S2.E4.m1.4.4.1.1.1.1.1.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1"><apply id="S2.E4.m1.4.4.1.1.1.1.1.2.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E4.m1.4.4.1.1.1.1.1.2.1.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.2">subscript</csymbol><sum id="S2.E4.m1.4.4.1.1.1.1.1.2.2.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.2.2"></sum><ci id="S2.E4.m1.4.4.1.1.1.1.1.2.3.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.2.3">ℎ</ci></apply><apply id="S2.E4.m1.4.4.1.1.1.1.1.1.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1"><times id="S2.E4.m1.4.4.1.1.1.1.1.1.2.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.2"></times><apply id="S2.E4.m1.4.4.1.1.1.1.1.1.3.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E4.m1.4.4.1.1.1.1.1.1.3.1.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.3">subscript</csymbol><apply id="S2.E4.m1.4.4.1.1.1.1.1.1.3.2.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E4.m1.4.4.1.1.1.1.1.1.3.2.1.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.3">superscript</csymbol><ci id="S2.E4.m1.4.4.1.1.1.1.1.1.3.2.2a.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.3.2.2"><mtext class="ltx_mathvariant_bold" id="S2.E4.m1.4.4.1.1.1.1.1.1.3.2.2.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.3.2.2">W</mtext></ci><ci id="S2.E4.m1.4.4.1.1.1.1.1.1.3.2.3.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.3.2.3">𝑂</ci></apply><ci id="S2.E4.m1.4.4.1.1.1.1.1.1.3.3.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.3.3">ℎ</ci></apply><apply id="S2.E4.m1.4.4.1.1.1.1.1.1.1.2.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.E4.m1.4.4.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1"><apply id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><sum id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.2"></sum><apply id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.3"><eq id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.3.1"></eq><ci id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.3.2">𝑘</ci><cn type="integer" id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.3.3">1</cn></apply></apply><ci id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.1.3">𝑇</ci></apply><apply id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2"><times id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.1"></times><apply id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2">𝐴</ci><apply id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.3"><times id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.3.1.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.3.1"></times><ci id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.3.2.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.3.2">ℎ</ci><ci id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.3.3.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.3.3">𝑖</ci><ci id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.3.4.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.3.4">𝑘</ci></apply></apply><apply id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.3">subscript</csymbol><ci id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.3.2a.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.3.2"><mtext class="ltx_mathvariant_bold" id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.3.2">W</mtext></ci><ci id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.3.3">𝑣</ci></apply><apply id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.4.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.4"><csymbol cd="ambiguous" id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.4.1.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.4">subscript</csymbol><ci id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.4.2a.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.4.2"><mtext class="ltx_mathvariant_bold" id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.4.2.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.4.2">x</mtext></ci><ci id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.4.3.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.1.2.4.3">𝑘</ci></apply></apply></apply></apply></apply></apply></apply><apply id="S2.E4.m1.4.4.1.1.2.2.cmml" xref="S2.E4.m1.4.4.1.1.2.2"><eq id="S2.E4.m1.4.4.1.1.2.2.1.cmml" xref="S2.E4.m1.4.4.1.1.2.2.1"></eq><ci id="S2.E4.m1.4.4.1.1.2.2.2.cmml" xref="S2.E4.m1.4.4.1.1.2.2.2">𝑖</ci><list id="S2.E4.m1.4.4.1.1.2.2.3.1.cmml" xref="S2.E4.m1.4.4.1.1.2.2.3.2"><cn type="integer" id="S2.E4.m1.1.1.cmml" xref="S2.E4.m1.1.1">1</cn><ci id="S2.E4.m1.2.2.cmml" xref="S2.E4.m1.2.2">⋯</ci><ci id="S2.E4.m1.3.3.cmml" xref="S2.E4.m1.3.3">𝑇</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E4.m1.4c">\text{MH Attention}^{i}=\sum_{h}\textbf{W}^{O}_{h}\big{[}\sum_{k=1}^{T}A_{hik}\textbf{W}_{v}\textbf{x}_{k}\big{]},i=1,\cdots,T,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S2.p3.10" class="ltx_p">where <math id="S2.p3.1.m1.1" class="ltx_Math" alttext="\textbf{W}^{O}_{h}" display="inline"><semantics id="S2.p3.1.m1.1a"><msubsup id="S2.p3.1.m1.1.1" xref="S2.p3.1.m1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S2.p3.1.m1.1.1.2.2" xref="S2.p3.1.m1.1.1.2.2a.cmml">W</mtext><mi id="S2.p3.1.m1.1.1.3" xref="S2.p3.1.m1.1.1.3.cmml">h</mi><mi id="S2.p3.1.m1.1.1.2.3" xref="S2.p3.1.m1.1.1.2.3.cmml">O</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.p3.1.m1.1b"><apply id="S2.p3.1.m1.1.1.cmml" xref="S2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S2.p3.1.m1.1.1.1.cmml" xref="S2.p3.1.m1.1.1">subscript</csymbol><apply id="S2.p3.1.m1.1.1.2.cmml" xref="S2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S2.p3.1.m1.1.1.2.1.cmml" xref="S2.p3.1.m1.1.1">superscript</csymbol><ci id="S2.p3.1.m1.1.1.2.2a.cmml" xref="S2.p3.1.m1.1.1.2.2"><mtext class="ltx_mathvariant_bold" id="S2.p3.1.m1.1.1.2.2.cmml" xref="S2.p3.1.m1.1.1.2.2">W</mtext></ci><ci id="S2.p3.1.m1.1.1.2.3.cmml" xref="S2.p3.1.m1.1.1.2.3">𝑂</ci></apply><ci id="S2.p3.1.m1.1.1.3.cmml" xref="S2.p3.1.m1.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.1.m1.1c">\textbf{W}^{O}_{h}</annotation></semantics></math> is a submatrix of <math id="S2.p3.2.m2.1" class="ltx_Math" alttext="\textbf{W}^{O}" display="inline"><semantics id="S2.p3.2.m2.1a"><msup id="S2.p3.2.m2.1.1" xref="S2.p3.2.m2.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S2.p3.2.m2.1.1.2" xref="S2.p3.2.m2.1.1.2a.cmml">W</mtext><mi id="S2.p3.2.m2.1.1.3" xref="S2.p3.2.m2.1.1.3.cmml">O</mi></msup><annotation-xml encoding="MathML-Content" id="S2.p3.2.m2.1b"><apply id="S2.p3.2.m2.1.1.cmml" xref="S2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S2.p3.2.m2.1.1.1.cmml" xref="S2.p3.2.m2.1.1">superscript</csymbol><ci id="S2.p3.2.m2.1.1.2a.cmml" xref="S2.p3.2.m2.1.1.2"><mtext class="ltx_mathvariant_bold" id="S2.p3.2.m2.1.1.2.cmml" xref="S2.p3.2.m2.1.1.2">W</mtext></ci><ci id="S2.p3.2.m2.1.1.3.cmml" xref="S2.p3.2.m2.1.1.3">𝑂</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.2.m2.1c">\textbf{W}^{O}</annotation></semantics></math> that corresponds to <math id="S2.p3.3.m3.1" class="ltx_Math" alttext="h-" display="inline"><semantics id="S2.p3.3.m3.1a"><mrow id="S2.p3.3.m3.1.1" xref="S2.p3.3.m3.1.1.cmml"><mi id="S2.p3.3.m3.1.1.2" xref="S2.p3.3.m3.1.1.2.cmml">h</mi><mo id="S2.p3.3.m3.1.1.3" xref="S2.p3.3.m3.1.1.3.cmml">−</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.3.m3.1b"><apply id="S2.p3.3.m3.1.1.cmml" xref="S2.p3.3.m3.1.1"><csymbol cd="latexml" id="S2.p3.3.m3.1.1.1.cmml" xref="S2.p3.3.m3.1.1">limit-from</csymbol><ci id="S2.p3.3.m3.1.1.2.cmml" xref="S2.p3.3.m3.1.1.2">ℎ</ci><minus id="S2.p3.3.m3.1.1.3.cmml" xref="S2.p3.3.m3.1.1.3"></minus></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.3.m3.1c">h-</annotation></semantics></math>th head, and <math id="S2.p3.4.m4.1" class="ltx_Math" alttext="A_{hik}" display="inline"><semantics id="S2.p3.4.m4.1a"><msub id="S2.p3.4.m4.1.1" xref="S2.p3.4.m4.1.1.cmml"><mi id="S2.p3.4.m4.1.1.2" xref="S2.p3.4.m4.1.1.2.cmml">A</mi><mrow id="S2.p3.4.m4.1.1.3" xref="S2.p3.4.m4.1.1.3.cmml"><mi id="S2.p3.4.m4.1.1.3.2" xref="S2.p3.4.m4.1.1.3.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="S2.p3.4.m4.1.1.3.1" xref="S2.p3.4.m4.1.1.3.1.cmml">​</mo><mi id="S2.p3.4.m4.1.1.3.3" xref="S2.p3.4.m4.1.1.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.p3.4.m4.1.1.3.1a" xref="S2.p3.4.m4.1.1.3.1.cmml">​</mo><mi id="S2.p3.4.m4.1.1.3.4" xref="S2.p3.4.m4.1.1.3.4.cmml">k</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.p3.4.m4.1b"><apply id="S2.p3.4.m4.1.1.cmml" xref="S2.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S2.p3.4.m4.1.1.1.cmml" xref="S2.p3.4.m4.1.1">subscript</csymbol><ci id="S2.p3.4.m4.1.1.2.cmml" xref="S2.p3.4.m4.1.1.2">𝐴</ci><apply id="S2.p3.4.m4.1.1.3.cmml" xref="S2.p3.4.m4.1.1.3"><times id="S2.p3.4.m4.1.1.3.1.cmml" xref="S2.p3.4.m4.1.1.3.1"></times><ci id="S2.p3.4.m4.1.1.3.2.cmml" xref="S2.p3.4.m4.1.1.3.2">ℎ</ci><ci id="S2.p3.4.m4.1.1.3.3.cmml" xref="S2.p3.4.m4.1.1.3.3">𝑖</ci><ci id="S2.p3.4.m4.1.1.3.4.cmml" xref="S2.p3.4.m4.1.1.3.4">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.4.m4.1c">A_{hik}</annotation></semantics></math> is the attention weight in <math id="S2.p3.5.m5.1" class="ltx_Math" alttext="h-" display="inline"><semantics id="S2.p3.5.m5.1a"><mrow id="S2.p3.5.m5.1.1" xref="S2.p3.5.m5.1.1.cmml"><mi id="S2.p3.5.m5.1.1.2" xref="S2.p3.5.m5.1.1.2.cmml">h</mi><mo id="S2.p3.5.m5.1.1.3" xref="S2.p3.5.m5.1.1.3.cmml">−</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.5.m5.1b"><apply id="S2.p3.5.m5.1.1.cmml" xref="S2.p3.5.m5.1.1"><csymbol cd="latexml" id="S2.p3.5.m5.1.1.1.cmml" xref="S2.p3.5.m5.1.1">limit-from</csymbol><ci id="S2.p3.5.m5.1.1.2.cmml" xref="S2.p3.5.m5.1.1.2">ℎ</ci><minus id="S2.p3.5.m5.1.1.3.cmml" xref="S2.p3.5.m5.1.1.3"></minus></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.5.m5.1c">h-</annotation></semantics></math>th head which is the element in <math id="S2.p3.6.m6.1" class="ltx_Math" alttext="i-" display="inline"><semantics id="S2.p3.6.m6.1a"><mrow id="S2.p3.6.m6.1.1" xref="S2.p3.6.m6.1.1.cmml"><mi id="S2.p3.6.m6.1.1.2" xref="S2.p3.6.m6.1.1.2.cmml">i</mi><mo id="S2.p3.6.m6.1.1.3" xref="S2.p3.6.m6.1.1.3.cmml">−</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.6.m6.1b"><apply id="S2.p3.6.m6.1.1.cmml" xref="S2.p3.6.m6.1.1"><csymbol cd="latexml" id="S2.p3.6.m6.1.1.1.cmml" xref="S2.p3.6.m6.1.1">limit-from</csymbol><ci id="S2.p3.6.m6.1.1.2.cmml" xref="S2.p3.6.m6.1.1.2">𝑖</ci><minus id="S2.p3.6.m6.1.1.3.cmml" xref="S2.p3.6.m6.1.1.3"></minus></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.6.m6.1c">i-</annotation></semantics></math>th row (corresponds to <math id="S2.p3.7.m7.1" class="ltx_Math" alttext="i-" display="inline"><semantics id="S2.p3.7.m7.1a"><mrow id="S2.p3.7.m7.1.1" xref="S2.p3.7.m7.1.1.cmml"><mi id="S2.p3.7.m7.1.1.2" xref="S2.p3.7.m7.1.1.2.cmml">i</mi><mo id="S2.p3.7.m7.1.1.3" xref="S2.p3.7.m7.1.1.3.cmml">−</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.7.m7.1b"><apply id="S2.p3.7.m7.1.1.cmml" xref="S2.p3.7.m7.1.1"><csymbol cd="latexml" id="S2.p3.7.m7.1.1.1.cmml" xref="S2.p3.7.m7.1.1">limit-from</csymbol><ci id="S2.p3.7.m7.1.1.2.cmml" xref="S2.p3.7.m7.1.1.2">𝑖</ci><minus id="S2.p3.7.m7.1.1.3.cmml" xref="S2.p3.7.m7.1.1.3"></minus></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.7.m7.1c">i-</annotation></semantics></math>th query) and <math id="S2.p3.8.m8.1" class="ltx_Math" alttext="k-" display="inline"><semantics id="S2.p3.8.m8.1a"><mrow id="S2.p3.8.m8.1.1" xref="S2.p3.8.m8.1.1.cmml"><mi id="S2.p3.8.m8.1.1.2" xref="S2.p3.8.m8.1.1.2.cmml">k</mi><mo id="S2.p3.8.m8.1.1.3" xref="S2.p3.8.m8.1.1.3.cmml">−</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.8.m8.1b"><apply id="S2.p3.8.m8.1.1.cmml" xref="S2.p3.8.m8.1.1"><csymbol cd="latexml" id="S2.p3.8.m8.1.1.1.cmml" xref="S2.p3.8.m8.1.1">limit-from</csymbol><ci id="S2.p3.8.m8.1.1.2.cmml" xref="S2.p3.8.m8.1.1.2">𝑘</ci><minus id="S2.p3.8.m8.1.1.3.cmml" xref="S2.p3.8.m8.1.1.3"></minus></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.8.m8.1c">k-</annotation></semantics></math>th column (corresponds to <math id="S2.p3.9.m9.1" class="ltx_Math" alttext="k-" display="inline"><semantics id="S2.p3.9.m9.1a"><mrow id="S2.p3.9.m9.1.1" xref="S2.p3.9.m9.1.1.cmml"><mi id="S2.p3.9.m9.1.1.2" xref="S2.p3.9.m9.1.1.2.cmml">k</mi><mo id="S2.p3.9.m9.1.1.3" xref="S2.p3.9.m9.1.1.3.cmml">−</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.9.m9.1b"><apply id="S2.p3.9.m9.1.1.cmml" xref="S2.p3.9.m9.1.1"><csymbol cd="latexml" id="S2.p3.9.m9.1.1.1.cmml" xref="S2.p3.9.m9.1.1">limit-from</csymbol><ci id="S2.p3.9.m9.1.1.2.cmml" xref="S2.p3.9.m9.1.1.2">𝑘</ci><minus id="S2.p3.9.m9.1.1.3.cmml" xref="S2.p3.9.m9.1.1.3"></minus></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.9.m9.1c">k-</annotation></semantics></math>th key) of the matrix: <math id="S2.p3.10.m10.1" class="ltx_Math" alttext="\text{Softmax}(\frac{\textbf{K}_{h}^{\top}\textbf{Q}_{h}}{\sqrt{d_{k}}})" display="inline"><semantics id="S2.p3.10.m10.1a"><mrow id="S2.p3.10.m10.1.2" xref="S2.p3.10.m10.1.2.cmml"><mtext id="S2.p3.10.m10.1.2.2" xref="S2.p3.10.m10.1.2.2a.cmml">Softmax</mtext><mo lspace="0em" rspace="0em" id="S2.p3.10.m10.1.2.1" xref="S2.p3.10.m10.1.2.1.cmml">​</mo><mrow id="S2.p3.10.m10.1.2.3.2" xref="S2.p3.10.m10.1.1.cmml"><mo stretchy="false" id="S2.p3.10.m10.1.2.3.2.1" xref="S2.p3.10.m10.1.1.cmml">(</mo><mfrac id="S2.p3.10.m10.1.1" xref="S2.p3.10.m10.1.1.cmml"><mrow id="S2.p3.10.m10.1.1.2" xref="S2.p3.10.m10.1.1.2.cmml"><msubsup id="S2.p3.10.m10.1.1.2.2" xref="S2.p3.10.m10.1.1.2.2.cmml"><mtext class="ltx_mathvariant_bold" id="S2.p3.10.m10.1.1.2.2.2.2" xref="S2.p3.10.m10.1.1.2.2.2.2a.cmml">K</mtext><mi id="S2.p3.10.m10.1.1.2.2.2.3" xref="S2.p3.10.m10.1.1.2.2.2.3.cmml">h</mi><mo id="S2.p3.10.m10.1.1.2.2.3" xref="S2.p3.10.m10.1.1.2.2.3.cmml">⊤</mo></msubsup><mo lspace="0em" rspace="0em" id="S2.p3.10.m10.1.1.2.1" xref="S2.p3.10.m10.1.1.2.1.cmml">​</mo><msub id="S2.p3.10.m10.1.1.2.3" xref="S2.p3.10.m10.1.1.2.3.cmml"><mtext class="ltx_mathvariant_bold" id="S2.p3.10.m10.1.1.2.3.2" xref="S2.p3.10.m10.1.1.2.3.2a.cmml">Q</mtext><mi id="S2.p3.10.m10.1.1.2.3.3" xref="S2.p3.10.m10.1.1.2.3.3.cmml">h</mi></msub></mrow><msqrt id="S2.p3.10.m10.1.1.3" xref="S2.p3.10.m10.1.1.3.cmml"><msub id="S2.p3.10.m10.1.1.3.2" xref="S2.p3.10.m10.1.1.3.2.cmml"><mi id="S2.p3.10.m10.1.1.3.2.2" xref="S2.p3.10.m10.1.1.3.2.2.cmml">d</mi><mi id="S2.p3.10.m10.1.1.3.2.3" xref="S2.p3.10.m10.1.1.3.2.3.cmml">k</mi></msub></msqrt></mfrac><mo stretchy="false" id="S2.p3.10.m10.1.2.3.2.2" xref="S2.p3.10.m10.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.10.m10.1b"><apply id="S2.p3.10.m10.1.2.cmml" xref="S2.p3.10.m10.1.2"><times id="S2.p3.10.m10.1.2.1.cmml" xref="S2.p3.10.m10.1.2.1"></times><ci id="S2.p3.10.m10.1.2.2a.cmml" xref="S2.p3.10.m10.1.2.2"><mtext id="S2.p3.10.m10.1.2.2.cmml" xref="S2.p3.10.m10.1.2.2">Softmax</mtext></ci><apply id="S2.p3.10.m10.1.1.cmml" xref="S2.p3.10.m10.1.2.3.2"><divide id="S2.p3.10.m10.1.1.1.cmml" xref="S2.p3.10.m10.1.2.3.2"></divide><apply id="S2.p3.10.m10.1.1.2.cmml" xref="S2.p3.10.m10.1.1.2"><times id="S2.p3.10.m10.1.1.2.1.cmml" xref="S2.p3.10.m10.1.1.2.1"></times><apply id="S2.p3.10.m10.1.1.2.2.cmml" xref="S2.p3.10.m10.1.1.2.2"><csymbol cd="ambiguous" id="S2.p3.10.m10.1.1.2.2.1.cmml" xref="S2.p3.10.m10.1.1.2.2">superscript</csymbol><apply id="S2.p3.10.m10.1.1.2.2.2.cmml" xref="S2.p3.10.m10.1.1.2.2"><csymbol cd="ambiguous" id="S2.p3.10.m10.1.1.2.2.2.1.cmml" xref="S2.p3.10.m10.1.1.2.2">subscript</csymbol><ci id="S2.p3.10.m10.1.1.2.2.2.2a.cmml" xref="S2.p3.10.m10.1.1.2.2.2.2"><mtext class="ltx_mathvariant_bold" mathsize="70%" id="S2.p3.10.m10.1.1.2.2.2.2.cmml" xref="S2.p3.10.m10.1.1.2.2.2.2">K</mtext></ci><ci id="S2.p3.10.m10.1.1.2.2.2.3.cmml" xref="S2.p3.10.m10.1.1.2.2.2.3">ℎ</ci></apply><csymbol cd="latexml" id="S2.p3.10.m10.1.1.2.2.3.cmml" xref="S2.p3.10.m10.1.1.2.2.3">top</csymbol></apply><apply id="S2.p3.10.m10.1.1.2.3.cmml" xref="S2.p3.10.m10.1.1.2.3"><csymbol cd="ambiguous" id="S2.p3.10.m10.1.1.2.3.1.cmml" xref="S2.p3.10.m10.1.1.2.3">subscript</csymbol><ci id="S2.p3.10.m10.1.1.2.3.2a.cmml" xref="S2.p3.10.m10.1.1.2.3.2"><mtext class="ltx_mathvariant_bold" mathsize="70%" id="S2.p3.10.m10.1.1.2.3.2.cmml" xref="S2.p3.10.m10.1.1.2.3.2">Q</mtext></ci><ci id="S2.p3.10.m10.1.1.2.3.3.cmml" xref="S2.p3.10.m10.1.1.2.3.3">ℎ</ci></apply></apply><apply id="S2.p3.10.m10.1.1.3.cmml" xref="S2.p3.10.m10.1.1.3"><root id="S2.p3.10.m10.1.1.3a.cmml" xref="S2.p3.10.m10.1.1.3"></root><apply id="S2.p3.10.m10.1.1.3.2.cmml" xref="S2.p3.10.m10.1.1.3.2"><csymbol cd="ambiguous" id="S2.p3.10.m10.1.1.3.2.1.cmml" xref="S2.p3.10.m10.1.1.3.2">subscript</csymbol><ci id="S2.p3.10.m10.1.1.3.2.2.cmml" xref="S2.p3.10.m10.1.1.3.2.2">𝑑</ci><ci id="S2.p3.10.m10.1.1.3.2.3.cmml" xref="S2.p3.10.m10.1.1.3.2.3">𝑘</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.10.m10.1c">\text{Softmax}(\frac{\textbf{K}_{h}^{\top}\textbf{Q}_{h}}{\sqrt{d_{k}}})</annotation></semantics></math>.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Dosovitskiy <span id="S2.p4.1.1" class="ltx_text ltx_font_italic">et al.</span> were the first to utilize the architecture of transformers in computer vision tasks, including image recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. The remarkable performance exhibited by transformers in various vision tasks has paved the way for their application in the domain of object detection research. Two pioneering works in this area are the DEtection TRansformer (DETR) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> (Figure <a href="#S2.F3" title="Figure 3 ‣ 2 Background ‣ Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, Top) and ViT-FRCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> (Figure <a href="#S2.F3" title="Figure 3 ‣ 2 Background ‣ Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, Bottom).</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">DETR aimed to reduce the reliance on CNN-based techniques during post-processing by employing a set-based global loss. This particular loss function aids in the collapse of near-duplicate predictions through bipartite matching, ensuring each prediction is uniquely paired with its matching ground truth bounding boxes. As an end-to-end model, DETR benefits from global computation and perfect memory, making it suitable for handling long sequences generated from videos/images. The bipartite matching loss utilized in DETR is defined as follows:</p>
</div>
<figure id="S2.F4" class="ltx_figure"><img src="/html/2309.04902/assets/x1.jpg" id="S2.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="423" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F4.3.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S2.F4.4.2" class="ltx_text" style="font-size:90%;">Taxonomy of small object detection using transformers and popular object detection methods in each category.</span></figcaption>
</figure>
<div id="S2.p6" class="ltx_para">
<table id="S2.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E5.m1.2" class="ltx_Math" alttext="\hat{s}=\arg\min_{s\in\mathcal{S}}\sum_{i}^{N}\mathcal{L}_{match}(y_{i},\hat{y}_{s(i)})," display="block"><semantics id="S2.E5.m1.2a"><mrow id="S2.E5.m1.2.2.1" xref="S2.E5.m1.2.2.1.1.cmml"><mrow id="S2.E5.m1.2.2.1.1" xref="S2.E5.m1.2.2.1.1.cmml"><mover accent="true" id="S2.E5.m1.2.2.1.1.4" xref="S2.E5.m1.2.2.1.1.4.cmml"><mi id="S2.E5.m1.2.2.1.1.4.2" xref="S2.E5.m1.2.2.1.1.4.2.cmml">s</mi><mo id="S2.E5.m1.2.2.1.1.4.1" xref="S2.E5.m1.2.2.1.1.4.1.cmml">^</mo></mover><mo id="S2.E5.m1.2.2.1.1.3" xref="S2.E5.m1.2.2.1.1.3.cmml">=</mo><mrow id="S2.E5.m1.2.2.1.1.2" xref="S2.E5.m1.2.2.1.1.2.cmml"><mrow id="S2.E5.m1.2.2.1.1.2.4" xref="S2.E5.m1.2.2.1.1.2.4.cmml"><mi id="S2.E5.m1.2.2.1.1.2.4.1" xref="S2.E5.m1.2.2.1.1.2.4.1.cmml">arg</mi><mo lspace="0.167em" id="S2.E5.m1.2.2.1.1.2.4a" xref="S2.E5.m1.2.2.1.1.2.4.cmml">⁡</mo><munder id="S2.E5.m1.2.2.1.1.2.4.2" xref="S2.E5.m1.2.2.1.1.2.4.2.cmml"><mi id="S2.E5.m1.2.2.1.1.2.4.2.2" xref="S2.E5.m1.2.2.1.1.2.4.2.2.cmml">min</mi><mrow id="S2.E5.m1.2.2.1.1.2.4.2.3" xref="S2.E5.m1.2.2.1.1.2.4.2.3.cmml"><mi id="S2.E5.m1.2.2.1.1.2.4.2.3.2" xref="S2.E5.m1.2.2.1.1.2.4.2.3.2.cmml">s</mi><mo id="S2.E5.m1.2.2.1.1.2.4.2.3.1" xref="S2.E5.m1.2.2.1.1.2.4.2.3.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S2.E5.m1.2.2.1.1.2.4.2.3.3" xref="S2.E5.m1.2.2.1.1.2.4.2.3.3.cmml">𝒮</mi></mrow></munder></mrow><mo lspace="0em" rspace="0em" id="S2.E5.m1.2.2.1.1.2.3" xref="S2.E5.m1.2.2.1.1.2.3.cmml">​</mo><mrow id="S2.E5.m1.2.2.1.1.2.2" xref="S2.E5.m1.2.2.1.1.2.2.cmml"><munderover id="S2.E5.m1.2.2.1.1.2.2.3" xref="S2.E5.m1.2.2.1.1.2.2.3.cmml"><mo movablelimits="false" id="S2.E5.m1.2.2.1.1.2.2.3.2.2" xref="S2.E5.m1.2.2.1.1.2.2.3.2.2.cmml">∑</mo><mi id="S2.E5.m1.2.2.1.1.2.2.3.2.3" xref="S2.E5.m1.2.2.1.1.2.2.3.2.3.cmml">i</mi><mi id="S2.E5.m1.2.2.1.1.2.2.3.3" xref="S2.E5.m1.2.2.1.1.2.2.3.3.cmml">N</mi></munderover><mrow id="S2.E5.m1.2.2.1.1.2.2.2" xref="S2.E5.m1.2.2.1.1.2.2.2.cmml"><msub id="S2.E5.m1.2.2.1.1.2.2.2.4" xref="S2.E5.m1.2.2.1.1.2.2.2.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E5.m1.2.2.1.1.2.2.2.4.2" xref="S2.E5.m1.2.2.1.1.2.2.2.4.2.cmml">ℒ</mi><mrow id="S2.E5.m1.2.2.1.1.2.2.2.4.3" xref="S2.E5.m1.2.2.1.1.2.2.2.4.3.cmml"><mi id="S2.E5.m1.2.2.1.1.2.2.2.4.3.2" xref="S2.E5.m1.2.2.1.1.2.2.2.4.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.E5.m1.2.2.1.1.2.2.2.4.3.1" xref="S2.E5.m1.2.2.1.1.2.2.2.4.3.1.cmml">​</mo><mi id="S2.E5.m1.2.2.1.1.2.2.2.4.3.3" xref="S2.E5.m1.2.2.1.1.2.2.2.4.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.E5.m1.2.2.1.1.2.2.2.4.3.1a" xref="S2.E5.m1.2.2.1.1.2.2.2.4.3.1.cmml">​</mo><mi id="S2.E5.m1.2.2.1.1.2.2.2.4.3.4" xref="S2.E5.m1.2.2.1.1.2.2.2.4.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.E5.m1.2.2.1.1.2.2.2.4.3.1b" xref="S2.E5.m1.2.2.1.1.2.2.2.4.3.1.cmml">​</mo><mi id="S2.E5.m1.2.2.1.1.2.2.2.4.3.5" xref="S2.E5.m1.2.2.1.1.2.2.2.4.3.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S2.E5.m1.2.2.1.1.2.2.2.4.3.1c" xref="S2.E5.m1.2.2.1.1.2.2.2.4.3.1.cmml">​</mo><mi id="S2.E5.m1.2.2.1.1.2.2.2.4.3.6" xref="S2.E5.m1.2.2.1.1.2.2.2.4.3.6.cmml">h</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E5.m1.2.2.1.1.2.2.2.3" xref="S2.E5.m1.2.2.1.1.2.2.2.3.cmml">​</mo><mrow id="S2.E5.m1.2.2.1.1.2.2.2.2.2" xref="S2.E5.m1.2.2.1.1.2.2.2.2.3.cmml"><mo stretchy="false" id="S2.E5.m1.2.2.1.1.2.2.2.2.2.3" xref="S2.E5.m1.2.2.1.1.2.2.2.2.3.cmml">(</mo><msub id="S2.E5.m1.2.2.1.1.1.1.1.1.1.1" xref="S2.E5.m1.2.2.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E5.m1.2.2.1.1.1.1.1.1.1.1.2" xref="S2.E5.m1.2.2.1.1.1.1.1.1.1.1.2.cmml">y</mi><mi id="S2.E5.m1.2.2.1.1.1.1.1.1.1.1.3" xref="S2.E5.m1.2.2.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.E5.m1.2.2.1.1.2.2.2.2.2.4" xref="S2.E5.m1.2.2.1.1.2.2.2.2.3.cmml">,</mo><msub id="S2.E5.m1.2.2.1.1.2.2.2.2.2.2" xref="S2.E5.m1.2.2.1.1.2.2.2.2.2.2.cmml"><mover accent="true" id="S2.E5.m1.2.2.1.1.2.2.2.2.2.2.2" xref="S2.E5.m1.2.2.1.1.2.2.2.2.2.2.2.cmml"><mi id="S2.E5.m1.2.2.1.1.2.2.2.2.2.2.2.2" xref="S2.E5.m1.2.2.1.1.2.2.2.2.2.2.2.2.cmml">y</mi><mo id="S2.E5.m1.2.2.1.1.2.2.2.2.2.2.2.1" xref="S2.E5.m1.2.2.1.1.2.2.2.2.2.2.2.1.cmml">^</mo></mover><mrow id="S2.E5.m1.1.1.1" xref="S2.E5.m1.1.1.1.cmml"><mi id="S2.E5.m1.1.1.1.3" xref="S2.E5.m1.1.1.1.3.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.E5.m1.1.1.1.2" xref="S2.E5.m1.1.1.1.2.cmml">​</mo><mrow id="S2.E5.m1.1.1.1.4.2" xref="S2.E5.m1.1.1.1.cmml"><mo stretchy="false" id="S2.E5.m1.1.1.1.4.2.1" xref="S2.E5.m1.1.1.1.cmml">(</mo><mi id="S2.E5.m1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.cmml">i</mi><mo stretchy="false" id="S2.E5.m1.1.1.1.4.2.2" xref="S2.E5.m1.1.1.1.cmml">)</mo></mrow></mrow></msub><mo stretchy="false" id="S2.E5.m1.2.2.1.1.2.2.2.2.2.5" xref="S2.E5.m1.2.2.1.1.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S2.E5.m1.2.2.1.2" xref="S2.E5.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E5.m1.2b"><apply id="S2.E5.m1.2.2.1.1.cmml" xref="S2.E5.m1.2.2.1"><eq id="S2.E5.m1.2.2.1.1.3.cmml" xref="S2.E5.m1.2.2.1.1.3"></eq><apply id="S2.E5.m1.2.2.1.1.4.cmml" xref="S2.E5.m1.2.2.1.1.4"><ci id="S2.E5.m1.2.2.1.1.4.1.cmml" xref="S2.E5.m1.2.2.1.1.4.1">^</ci><ci id="S2.E5.m1.2.2.1.1.4.2.cmml" xref="S2.E5.m1.2.2.1.1.4.2">𝑠</ci></apply><apply id="S2.E5.m1.2.2.1.1.2.cmml" xref="S2.E5.m1.2.2.1.1.2"><times id="S2.E5.m1.2.2.1.1.2.3.cmml" xref="S2.E5.m1.2.2.1.1.2.3"></times><apply id="S2.E5.m1.2.2.1.1.2.4.cmml" xref="S2.E5.m1.2.2.1.1.2.4"><arg id="S2.E5.m1.2.2.1.1.2.4.1.cmml" xref="S2.E5.m1.2.2.1.1.2.4.1"></arg><apply id="S2.E5.m1.2.2.1.1.2.4.2.cmml" xref="S2.E5.m1.2.2.1.1.2.4.2"><csymbol cd="ambiguous" id="S2.E5.m1.2.2.1.1.2.4.2.1.cmml" xref="S2.E5.m1.2.2.1.1.2.4.2">subscript</csymbol><min id="S2.E5.m1.2.2.1.1.2.4.2.2.cmml" xref="S2.E5.m1.2.2.1.1.2.4.2.2"></min><apply id="S2.E5.m1.2.2.1.1.2.4.2.3.cmml" xref="S2.E5.m1.2.2.1.1.2.4.2.3"><in id="S2.E5.m1.2.2.1.1.2.4.2.3.1.cmml" xref="S2.E5.m1.2.2.1.1.2.4.2.3.1"></in><ci id="S2.E5.m1.2.2.1.1.2.4.2.3.2.cmml" xref="S2.E5.m1.2.2.1.1.2.4.2.3.2">𝑠</ci><ci id="S2.E5.m1.2.2.1.1.2.4.2.3.3.cmml" xref="S2.E5.m1.2.2.1.1.2.4.2.3.3">𝒮</ci></apply></apply></apply><apply id="S2.E5.m1.2.2.1.1.2.2.cmml" xref="S2.E5.m1.2.2.1.1.2.2"><apply id="S2.E5.m1.2.2.1.1.2.2.3.cmml" xref="S2.E5.m1.2.2.1.1.2.2.3"><csymbol cd="ambiguous" id="S2.E5.m1.2.2.1.1.2.2.3.1.cmml" xref="S2.E5.m1.2.2.1.1.2.2.3">superscript</csymbol><apply id="S2.E5.m1.2.2.1.1.2.2.3.2.cmml" xref="S2.E5.m1.2.2.1.1.2.2.3"><csymbol cd="ambiguous" id="S2.E5.m1.2.2.1.1.2.2.3.2.1.cmml" xref="S2.E5.m1.2.2.1.1.2.2.3">subscript</csymbol><sum id="S2.E5.m1.2.2.1.1.2.2.3.2.2.cmml" xref="S2.E5.m1.2.2.1.1.2.2.3.2.2"></sum><ci id="S2.E5.m1.2.2.1.1.2.2.3.2.3.cmml" xref="S2.E5.m1.2.2.1.1.2.2.3.2.3">𝑖</ci></apply><ci id="S2.E5.m1.2.2.1.1.2.2.3.3.cmml" xref="S2.E5.m1.2.2.1.1.2.2.3.3">𝑁</ci></apply><apply id="S2.E5.m1.2.2.1.1.2.2.2.cmml" xref="S2.E5.m1.2.2.1.1.2.2.2"><times id="S2.E5.m1.2.2.1.1.2.2.2.3.cmml" xref="S2.E5.m1.2.2.1.1.2.2.2.3"></times><apply id="S2.E5.m1.2.2.1.1.2.2.2.4.cmml" xref="S2.E5.m1.2.2.1.1.2.2.2.4"><csymbol cd="ambiguous" id="S2.E5.m1.2.2.1.1.2.2.2.4.1.cmml" xref="S2.E5.m1.2.2.1.1.2.2.2.4">subscript</csymbol><ci id="S2.E5.m1.2.2.1.1.2.2.2.4.2.cmml" xref="S2.E5.m1.2.2.1.1.2.2.2.4.2">ℒ</ci><apply id="S2.E5.m1.2.2.1.1.2.2.2.4.3.cmml" xref="S2.E5.m1.2.2.1.1.2.2.2.4.3"><times id="S2.E5.m1.2.2.1.1.2.2.2.4.3.1.cmml" xref="S2.E5.m1.2.2.1.1.2.2.2.4.3.1"></times><ci id="S2.E5.m1.2.2.1.1.2.2.2.4.3.2.cmml" xref="S2.E5.m1.2.2.1.1.2.2.2.4.3.2">𝑚</ci><ci id="S2.E5.m1.2.2.1.1.2.2.2.4.3.3.cmml" xref="S2.E5.m1.2.2.1.1.2.2.2.4.3.3">𝑎</ci><ci id="S2.E5.m1.2.2.1.1.2.2.2.4.3.4.cmml" xref="S2.E5.m1.2.2.1.1.2.2.2.4.3.4">𝑡</ci><ci id="S2.E5.m1.2.2.1.1.2.2.2.4.3.5.cmml" xref="S2.E5.m1.2.2.1.1.2.2.2.4.3.5">𝑐</ci><ci id="S2.E5.m1.2.2.1.1.2.2.2.4.3.6.cmml" xref="S2.E5.m1.2.2.1.1.2.2.2.4.3.6">ℎ</ci></apply></apply><interval closure="open" id="S2.E5.m1.2.2.1.1.2.2.2.2.3.cmml" xref="S2.E5.m1.2.2.1.1.2.2.2.2.2"><apply id="S2.E5.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S2.E5.m1.2.2.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E5.m1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E5.m1.2.2.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E5.m1.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E5.m1.2.2.1.1.1.1.1.1.1.1.2">𝑦</ci><ci id="S2.E5.m1.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E5.m1.2.2.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S2.E5.m1.2.2.1.1.2.2.2.2.2.2.cmml" xref="S2.E5.m1.2.2.1.1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.E5.m1.2.2.1.1.2.2.2.2.2.2.1.cmml" xref="S2.E5.m1.2.2.1.1.2.2.2.2.2.2">subscript</csymbol><apply id="S2.E5.m1.2.2.1.1.2.2.2.2.2.2.2.cmml" xref="S2.E5.m1.2.2.1.1.2.2.2.2.2.2.2"><ci id="S2.E5.m1.2.2.1.1.2.2.2.2.2.2.2.1.cmml" xref="S2.E5.m1.2.2.1.1.2.2.2.2.2.2.2.1">^</ci><ci id="S2.E5.m1.2.2.1.1.2.2.2.2.2.2.2.2.cmml" xref="S2.E5.m1.2.2.1.1.2.2.2.2.2.2.2.2">𝑦</ci></apply><apply id="S2.E5.m1.1.1.1.cmml" xref="S2.E5.m1.1.1.1"><times id="S2.E5.m1.1.1.1.2.cmml" xref="S2.E5.m1.1.1.1.2"></times><ci id="S2.E5.m1.1.1.1.3.cmml" xref="S2.E5.m1.1.1.1.3">𝑠</ci><ci id="S2.E5.m1.1.1.1.1.cmml" xref="S2.E5.m1.1.1.1.1">𝑖</ci></apply></apply></interval></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E5.m1.2c">\hat{s}=\arg\min_{s\in\mathcal{S}}\sum_{i}^{N}\mathcal{L}_{match}(y_{i},\hat{y}_{s(i)}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p id="S2.p6.10" class="ltx_p">where <math id="S2.p6.1.m1.3" class="ltx_Math" alttext="\mathcal{L}_{match}(y_{i},\hat{y}_{s(i)})" display="inline"><semantics id="S2.p6.1.m1.3a"><mrow id="S2.p6.1.m1.3.3" xref="S2.p6.1.m1.3.3.cmml"><msub id="S2.p6.1.m1.3.3.4" xref="S2.p6.1.m1.3.3.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.p6.1.m1.3.3.4.2" xref="S2.p6.1.m1.3.3.4.2.cmml">ℒ</mi><mrow id="S2.p6.1.m1.3.3.4.3" xref="S2.p6.1.m1.3.3.4.3.cmml"><mi id="S2.p6.1.m1.3.3.4.3.2" xref="S2.p6.1.m1.3.3.4.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.p6.1.m1.3.3.4.3.1" xref="S2.p6.1.m1.3.3.4.3.1.cmml">​</mo><mi id="S2.p6.1.m1.3.3.4.3.3" xref="S2.p6.1.m1.3.3.4.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.p6.1.m1.3.3.4.3.1a" xref="S2.p6.1.m1.3.3.4.3.1.cmml">​</mo><mi id="S2.p6.1.m1.3.3.4.3.4" xref="S2.p6.1.m1.3.3.4.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.p6.1.m1.3.3.4.3.1b" xref="S2.p6.1.m1.3.3.4.3.1.cmml">​</mo><mi id="S2.p6.1.m1.3.3.4.3.5" xref="S2.p6.1.m1.3.3.4.3.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S2.p6.1.m1.3.3.4.3.1c" xref="S2.p6.1.m1.3.3.4.3.1.cmml">​</mo><mi id="S2.p6.1.m1.3.3.4.3.6" xref="S2.p6.1.m1.3.3.4.3.6.cmml">h</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S2.p6.1.m1.3.3.3" xref="S2.p6.1.m1.3.3.3.cmml">​</mo><mrow id="S2.p6.1.m1.3.3.2.2" xref="S2.p6.1.m1.3.3.2.3.cmml"><mo stretchy="false" id="S2.p6.1.m1.3.3.2.2.3" xref="S2.p6.1.m1.3.3.2.3.cmml">(</mo><msub id="S2.p6.1.m1.2.2.1.1.1" xref="S2.p6.1.m1.2.2.1.1.1.cmml"><mi id="S2.p6.1.m1.2.2.1.1.1.2" xref="S2.p6.1.m1.2.2.1.1.1.2.cmml">y</mi><mi id="S2.p6.1.m1.2.2.1.1.1.3" xref="S2.p6.1.m1.2.2.1.1.1.3.cmml">i</mi></msub><mo id="S2.p6.1.m1.3.3.2.2.4" xref="S2.p6.1.m1.3.3.2.3.cmml">,</mo><msub id="S2.p6.1.m1.3.3.2.2.2" xref="S2.p6.1.m1.3.3.2.2.2.cmml"><mover accent="true" id="S2.p6.1.m1.3.3.2.2.2.2" xref="S2.p6.1.m1.3.3.2.2.2.2.cmml"><mi id="S2.p6.1.m1.3.3.2.2.2.2.2" xref="S2.p6.1.m1.3.3.2.2.2.2.2.cmml">y</mi><mo id="S2.p6.1.m1.3.3.2.2.2.2.1" xref="S2.p6.1.m1.3.3.2.2.2.2.1.cmml">^</mo></mover><mrow id="S2.p6.1.m1.1.1.1" xref="S2.p6.1.m1.1.1.1.cmml"><mi id="S2.p6.1.m1.1.1.1.3" xref="S2.p6.1.m1.1.1.1.3.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.p6.1.m1.1.1.1.2" xref="S2.p6.1.m1.1.1.1.2.cmml">​</mo><mrow id="S2.p6.1.m1.1.1.1.4.2" xref="S2.p6.1.m1.1.1.1.cmml"><mo stretchy="false" id="S2.p6.1.m1.1.1.1.4.2.1" xref="S2.p6.1.m1.1.1.1.cmml">(</mo><mi id="S2.p6.1.m1.1.1.1.1" xref="S2.p6.1.m1.1.1.1.1.cmml">i</mi><mo stretchy="false" id="S2.p6.1.m1.1.1.1.4.2.2" xref="S2.p6.1.m1.1.1.1.cmml">)</mo></mrow></mrow></msub><mo stretchy="false" id="S2.p6.1.m1.3.3.2.2.5" xref="S2.p6.1.m1.3.3.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p6.1.m1.3b"><apply id="S2.p6.1.m1.3.3.cmml" xref="S2.p6.1.m1.3.3"><times id="S2.p6.1.m1.3.3.3.cmml" xref="S2.p6.1.m1.3.3.3"></times><apply id="S2.p6.1.m1.3.3.4.cmml" xref="S2.p6.1.m1.3.3.4"><csymbol cd="ambiguous" id="S2.p6.1.m1.3.3.4.1.cmml" xref="S2.p6.1.m1.3.3.4">subscript</csymbol><ci id="S2.p6.1.m1.3.3.4.2.cmml" xref="S2.p6.1.m1.3.3.4.2">ℒ</ci><apply id="S2.p6.1.m1.3.3.4.3.cmml" xref="S2.p6.1.m1.3.3.4.3"><times id="S2.p6.1.m1.3.3.4.3.1.cmml" xref="S2.p6.1.m1.3.3.4.3.1"></times><ci id="S2.p6.1.m1.3.3.4.3.2.cmml" xref="S2.p6.1.m1.3.3.4.3.2">𝑚</ci><ci id="S2.p6.1.m1.3.3.4.3.3.cmml" xref="S2.p6.1.m1.3.3.4.3.3">𝑎</ci><ci id="S2.p6.1.m1.3.3.4.3.4.cmml" xref="S2.p6.1.m1.3.3.4.3.4">𝑡</ci><ci id="S2.p6.1.m1.3.3.4.3.5.cmml" xref="S2.p6.1.m1.3.3.4.3.5">𝑐</ci><ci id="S2.p6.1.m1.3.3.4.3.6.cmml" xref="S2.p6.1.m1.3.3.4.3.6">ℎ</ci></apply></apply><interval closure="open" id="S2.p6.1.m1.3.3.2.3.cmml" xref="S2.p6.1.m1.3.3.2.2"><apply id="S2.p6.1.m1.2.2.1.1.1.cmml" xref="S2.p6.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.p6.1.m1.2.2.1.1.1.1.cmml" xref="S2.p6.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S2.p6.1.m1.2.2.1.1.1.2.cmml" xref="S2.p6.1.m1.2.2.1.1.1.2">𝑦</ci><ci id="S2.p6.1.m1.2.2.1.1.1.3.cmml" xref="S2.p6.1.m1.2.2.1.1.1.3">𝑖</ci></apply><apply id="S2.p6.1.m1.3.3.2.2.2.cmml" xref="S2.p6.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.p6.1.m1.3.3.2.2.2.1.cmml" xref="S2.p6.1.m1.3.3.2.2.2">subscript</csymbol><apply id="S2.p6.1.m1.3.3.2.2.2.2.cmml" xref="S2.p6.1.m1.3.3.2.2.2.2"><ci id="S2.p6.1.m1.3.3.2.2.2.2.1.cmml" xref="S2.p6.1.m1.3.3.2.2.2.2.1">^</ci><ci id="S2.p6.1.m1.3.3.2.2.2.2.2.cmml" xref="S2.p6.1.m1.3.3.2.2.2.2.2">𝑦</ci></apply><apply id="S2.p6.1.m1.1.1.1.cmml" xref="S2.p6.1.m1.1.1.1"><times id="S2.p6.1.m1.1.1.1.2.cmml" xref="S2.p6.1.m1.1.1.1.2"></times><ci id="S2.p6.1.m1.1.1.1.3.cmml" xref="S2.p6.1.m1.1.1.1.3">𝑠</ci><ci id="S2.p6.1.m1.1.1.1.1.cmml" xref="S2.p6.1.m1.1.1.1.1">𝑖</ci></apply></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.1.m1.3c">\mathcal{L}_{match}(y_{i},\hat{y}_{s(i)})</annotation></semantics></math> measures the pair-wise matching cost between ground truth box <math id="S2.p6.2.m2.1" class="ltx_Math" alttext="y_{i}" display="inline"><semantics id="S2.p6.2.m2.1a"><msub id="S2.p6.2.m2.1.1" xref="S2.p6.2.m2.1.1.cmml"><mi id="S2.p6.2.m2.1.1.2" xref="S2.p6.2.m2.1.1.2.cmml">y</mi><mi id="S2.p6.2.m2.1.1.3" xref="S2.p6.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p6.2.m2.1b"><apply id="S2.p6.2.m2.1.1.cmml" xref="S2.p6.2.m2.1.1"><csymbol cd="ambiguous" id="S2.p6.2.m2.1.1.1.cmml" xref="S2.p6.2.m2.1.1">subscript</csymbol><ci id="S2.p6.2.m2.1.1.2.cmml" xref="S2.p6.2.m2.1.1.2">𝑦</ci><ci id="S2.p6.2.m2.1.1.3.cmml" xref="S2.p6.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.2.m2.1c">y_{i}</annotation></semantics></math> with size <math id="S2.p6.3.m3.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.p6.3.m3.1a"><mi id="S2.p6.3.m3.1.1" xref="S2.p6.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.p6.3.m3.1b"><ci id="S2.p6.3.m3.1.1.cmml" xref="S2.p6.3.m3.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.3.m3.1c">N</annotation></semantics></math> and the prediction with index <math id="S2.p6.4.m4.1" class="ltx_Math" alttext="s(i)" display="inline"><semantics id="S2.p6.4.m4.1a"><mrow id="S2.p6.4.m4.1.2" xref="S2.p6.4.m4.1.2.cmml"><mi id="S2.p6.4.m4.1.2.2" xref="S2.p6.4.m4.1.2.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.p6.4.m4.1.2.1" xref="S2.p6.4.m4.1.2.1.cmml">​</mo><mrow id="S2.p6.4.m4.1.2.3.2" xref="S2.p6.4.m4.1.2.cmml"><mo stretchy="false" id="S2.p6.4.m4.1.2.3.2.1" xref="S2.p6.4.m4.1.2.cmml">(</mo><mi id="S2.p6.4.m4.1.1" xref="S2.p6.4.m4.1.1.cmml">i</mi><mo stretchy="false" id="S2.p6.4.m4.1.2.3.2.2" xref="S2.p6.4.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p6.4.m4.1b"><apply id="S2.p6.4.m4.1.2.cmml" xref="S2.p6.4.m4.1.2"><times id="S2.p6.4.m4.1.2.1.cmml" xref="S2.p6.4.m4.1.2.1"></times><ci id="S2.p6.4.m4.1.2.2.cmml" xref="S2.p6.4.m4.1.2.2">𝑠</ci><ci id="S2.p6.4.m4.1.1.cmml" xref="S2.p6.4.m4.1.1">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.4.m4.1c">s(i)</annotation></semantics></math> where <math id="S2.p6.5.m5.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S2.p6.5.m5.1a"><mi id="S2.p6.5.m5.1.1" xref="S2.p6.5.m5.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S2.p6.5.m5.1b"><ci id="S2.p6.5.m5.1.1.cmml" xref="S2.p6.5.m5.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.5.m5.1c">s</annotation></semantics></math> is a specific order of predicted bounding boxes. In this formulation, <math id="S2.p6.6.m6.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.p6.6.m6.1a"><mi id="S2.p6.6.m6.1.1" xref="S2.p6.6.m6.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.p6.6.m6.1b"><ci id="S2.p6.6.m6.1.1.cmml" xref="S2.p6.6.m6.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.6.m6.1c">N</annotation></semantics></math> is the largest possible number of objects within an image. In the case of fewer objects in predictions and ground-truth, <math id="S2.p6.7.m7.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S2.p6.7.m7.1a"><mi id="S2.p6.7.m7.1.1" xref="S2.p6.7.m7.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S2.p6.7.m7.1b"><ci id="S2.p6.7.m7.1.1.cmml" xref="S2.p6.7.m7.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.7.m7.1c">y</annotation></semantics></math> and <math id="S2.p6.8.m8.1" class="ltx_Math" alttext="\hat{y}" display="inline"><semantics id="S2.p6.8.m8.1a"><mover accent="true" id="S2.p6.8.m8.1.1" xref="S2.p6.8.m8.1.1.cmml"><mi id="S2.p6.8.m8.1.1.2" xref="S2.p6.8.m8.1.1.2.cmml">y</mi><mo id="S2.p6.8.m8.1.1.1" xref="S2.p6.8.m8.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S2.p6.8.m8.1b"><apply id="S2.p6.8.m8.1.1.cmml" xref="S2.p6.8.m8.1.1"><ci id="S2.p6.8.m8.1.1.1.cmml" xref="S2.p6.8.m8.1.1.1">^</ci><ci id="S2.p6.8.m8.1.1.2.cmml" xref="S2.p6.8.m8.1.1.2">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.8.m8.1c">\hat{y}</annotation></semantics></math> will be padded with <math id="S2.p6.9.m9.1" class="ltx_Math" alttext="\emptyset" display="inline"><semantics id="S2.p6.9.m9.1a"><mi mathvariant="normal" id="S2.p6.9.m9.1.1" xref="S2.p6.9.m9.1.1.cmml">∅</mi><annotation-xml encoding="MathML-Content" id="S2.p6.9.m9.1b"><emptyset id="S2.p6.9.m9.1.1.cmml" xref="S2.p6.9.m9.1.1"></emptyset></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.9.m9.1c">\emptyset</annotation></semantics></math> (indicating no object). Consequently, this loss function considers all possible matching policies between predictions and ground truth, selecting the one that yields the minimum loss value. The optimal pairing can be efficiently computed using the Hungarian algorithm, as demonstrated in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. DETR used a CNN backbone to extract compact feature representations and an encoder-decoder transformer with a feed-forward network to produce the final predictions (see Figure <a href="#S2.F3" title="Figure 3 ‣ 2 Background ‣ Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, Top). In contrast, ViT-FRCNN uses the Vision Transformer (ViT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> for object detection and demonstrates that pre-trained ViT on large-scale datasets enhances the detection performance through rapid fine-tuning. While ViT-FRCNN, like DETR, incorporates CNN-based networks in its pipeline, specifically in the detection head, it diverges from DETR by using the Transformer (encoder only) to encode visual attributes. Additionally, a conventional Region Proposal Network (RPN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> is used for generating detections (illustrated in Figure <a href="#S2.F3" title="Figure 3 ‣ 2 Background ‣ Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, Bottom).
Both DETR and ViT-FRCNN have shown subpar results in the detection and classification of small objects. ViT-FRCNN even exhibited worse results when increasing the token size of the input image. The best outcomes were achieved when the token size was set to <math id="S2.p6.10.m10.1" class="ltx_Math" alttext="16\times 16" display="inline"><semantics id="S2.p6.10.m10.1a"><mrow id="S2.p6.10.m10.1.1" xref="S2.p6.10.m10.1.1.cmml"><mn id="S2.p6.10.m10.1.1.2" xref="S2.p6.10.m10.1.1.2.cmml">16</mn><mo lspace="0.222em" rspace="0.222em" id="S2.p6.10.m10.1.1.1" xref="S2.p6.10.m10.1.1.1.cmml">×</mo><mn id="S2.p6.10.m10.1.1.3" xref="S2.p6.10.m10.1.1.3.cmml">16</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.p6.10.m10.1b"><apply id="S2.p6.10.m10.1.1.cmml" xref="S2.p6.10.m10.1.1"><times id="S2.p6.10.m10.1.1.1.cmml" xref="S2.p6.10.m10.1.1.1"></times><cn type="integer" id="S2.p6.10.m10.1.1.2.cmml" xref="S2.p6.10.m10.1.1.2">16</cn><cn type="integer" id="S2.p6.10.m10.1.1.3.cmml" xref="S2.p6.10.m10.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.10.m10.1c">16\times 16</annotation></semantics></math>, and all intermediate transformer states were concatenated with the final transformed layer. Additionally, both detectors rely on CNNs at different stages, in DETR as the backbone for feature extraction and in ViT-FRCNN for the detection head. To improve the results of small object detection, it is crucial to retain the image patches as small as possible to preserve spatial resolution, which consequently increases the computational costs. To address these limitations and challenges, further research has been conducted, which will be discussed in detail in the following sections.</p>
</div>
<figure id="S2.F5" class="ltx_figure"><img src="/html/2309.04902/assets/Figures/BVRn.jpg" id="S2.F5.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="127" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F5.3.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S2.F5.4.2" class="ltx_text" style="font-size:90%;">BVR uses different representations. i.e., corner and center points to enhance features for anchor-based detection (left figure). Object representations are shown for another image (cat) where red dashes show the ground truth (figure from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>).</span></figcaption>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Transfomers For Small Object Detection</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we discuss transformer-based networks for SOD. A taxonomy of small object detectors is shown in Figure <a href="#S2.F4" title="Figure 4 ‣ 2 Background ‣ Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. We show that existing detectors based on novel transformers can be analyzed through one or a few of the following perspectives: object representation, fast attention for high-resolution or multi-scale feature maps, fully transformer-based detection, architecture and block modification, auxiliary techniques, improved feature representation, and spatio-temporal information. In the following subsections, each of these categories is discussed in detail separately.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span><span id="S3.SS1.1.1" class="ltx_text ltx_font_italic">Object Representation</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Various object representation techniques have been adopted in object detection techniques. The object of interest can be represented by rectangular boxes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, points such as center points <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> and point sets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, probabilistic objects <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, and keypoints <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>. Each object representation technique has its own strengths and weaknesses, with respect to the need for annotation formats and small object representation. The pursuit of finding the optimal representation technique, while keeping all the strengths of the existing representations, began with RelationNet++ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. This approach bridges various heterogeneous visual representations and combines their strengths via a module called Bridging Visual Representations (BVR). BVR operates efficiently without disrupting the overall inference process employed by the main representations, leveraging novel techniques of key sampling and shared location embedding. More importantly, BVR relies on an attention module that designates one representation form as the “master representation” (or query), while the other representations are designated as “auxiliary” representations (or keys). The BVR block is shown in Figure <a href="#S2.F5" title="Figure 5 ‣ 2 Background ‣ Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, where it enhances the feature representation of the anchor box by seamlessly integrating center and corner points (keys) into the anchor-based (query) object detection methodology. Different object representations are also shown in Figure <a href="#S2.F5" title="Figure 5 ‣ 2 Background ‣ Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. CenterNet++ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> was proposed as a novel bottom-up approach. Instead of estimating all the object’s parameters at once, CenterNet++ strategically identifies individual components of the object separately, i.e., top-left, bottom-left, and center keypoints. Then, post-processing methodologies are adopted to cluster points associated with the same objects. This technique has demonstrated a superior recall rate in SOD compared to top-down approaches that estimate entire objects as a whole.</p>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2309.04902/assets/Figures/Deformablen.jpg" id="S3.F6.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="277" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F6.10.4.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S3.F6.7.3" class="ltx_text" style="font-size:90%;">The block diagram for the Deformable attention module. <math id="S3.F6.5.1.m1.1" class="ltx_Math" alttext="\textbf{z}_{q}" display="inline"><semantics id="S3.F6.5.1.m1.1b"><msub id="S3.F6.5.1.m1.1.1" xref="S3.F6.5.1.m1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S3.F6.5.1.m1.1.1.2" xref="S3.F6.5.1.m1.1.1.2a.cmml">z</mtext><mi id="S3.F6.5.1.m1.1.1.3" xref="S3.F6.5.1.m1.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F6.5.1.m1.1c"><apply id="S3.F6.5.1.m1.1.1.cmml" xref="S3.F6.5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.F6.5.1.m1.1.1.1.cmml" xref="S3.F6.5.1.m1.1.1">subscript</csymbol><ci id="S3.F6.5.1.m1.1.1.2a.cmml" xref="S3.F6.5.1.m1.1.1.2"><mtext class="ltx_mathvariant_bold" id="S3.F6.5.1.m1.1.1.2.cmml" xref="S3.F6.5.1.m1.1.1.2">z</mtext></ci><ci id="S3.F6.5.1.m1.1.1.3.cmml" xref="S3.F6.5.1.m1.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F6.5.1.m1.1d">\textbf{z}_{q}</annotation></semantics></math> is the content feature of the query, <span id="S3.F6.7.3.1" class="ltx_text ltx_markedasmath ltx_font_bold">x</span> is the feature map and <math id="S3.F6.7.3.m3.1" class="ltx_Math" alttext="\textbf{p}_{q}" display="inline"><semantics id="S3.F6.7.3.m3.1b"><msub id="S3.F6.7.3.m3.1.1" xref="S3.F6.7.3.m3.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S3.F6.7.3.m3.1.1.2" xref="S3.F6.7.3.m3.1.1.2a.cmml">p</mtext><mi id="S3.F6.7.3.m3.1.1.3" xref="S3.F6.7.3.m3.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F6.7.3.m3.1c"><apply id="S3.F6.7.3.m3.1.1.cmml" xref="S3.F6.7.3.m3.1.1"><csymbol cd="ambiguous" id="S3.F6.7.3.m3.1.1.1.cmml" xref="S3.F6.7.3.m3.1.1">subscript</csymbol><ci id="S3.F6.7.3.m3.1.1.2a.cmml" xref="S3.F6.7.3.m3.1.1.2"><mtext class="ltx_mathvariant_bold" id="S3.F6.7.3.m3.1.1.2.cmml" xref="S3.F6.7.3.m3.1.1.2">p</mtext></ci><ci id="S3.F6.7.3.m3.1.1.3.cmml" xref="S3.F6.7.3.m3.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F6.7.3.m3.1d">\textbf{p}_{q}</annotation></semantics></math> is the refrence point in 2-D grid. In short, the deformable attention module only attends to a small set of key sampling points around the reference point (different in each head). This significantly reduces the complexity and further improves the convergence (figure from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>).</span></figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span><span id="S3.SS2.1.1" class="ltx_text ltx_font_italic">Fast Attention for High-Resolution or Multi-Scale Feature Maps</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Previous research has shown that maintaining a high resolution of feature maps is a necessary step for maintaining high performance in SOD. Transformers, inherently exhibit a notably higher complexity compared to CNNs due to their quadratic increase in complexity with respect to the number of tokens (e.g., pixel numbers). This complexity emerges from requirement of pairwise correlation computation across all tokens. Consequently, both training and inference times exceed expectations, rendering the detector inapplicable for small object detection in high-resolution images and videos.</p>
</div>
<figure id="S3.F7" class="ltx_figure"><img src="/html/2309.04902/assets/Figures/ViDT.jpg" id="S3.F7.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="127" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F7.3.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S3.F7.4.2" class="ltx_text" style="font-size:90%;">ViDT (c) mixes DETR (with ViT backbone or other fully transformer-based backbones) (a) with YOLOS architecture (b) in a multi-scale feature learning pipeline to achieve SOTA results (figure from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>).</span></figcaption>
</figure>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">In their work on Deformable DETR, Zhu <span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> addressed this issue that had been observed in DETR for the first time. They proposed attending to only a small set of key sampling points around a reference, significantly reducing the complexity. By adopting this strategy, they effectively preserved spatial resolution through the use of multi-scale deformable attention modules. Remarkably, this method eliminated the necessity for feature pyramid networks, thereby greatly enhancing the detection and recognition of small objects. The <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="i-" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">i</mi><mo id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">−</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><csymbol cd="latexml" id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">limit-from</csymbol><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">𝑖</ci><minus id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3"></minus></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">i-</annotation></semantics></math>th output of a multi-head attention module in Deformable attention is given by:</p>
<table id="S3.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E6.m1.1" class="ltx_Math" alttext="\text{MH Attention}^{i}=\sum_{h}\textbf{W}^{O}_{h}\big{[}\sum_{k=1}^{K}A_{hik}\textbf{W}_{v}\textbf{x}_{k}(\textbf{p}_{i}+\Delta\textbf{p}_{hik})\big{]}," display="block"><semantics id="S3.E6.m1.1a"><mrow id="S3.E6.m1.1.1.1" xref="S3.E6.m1.1.1.1.1.cmml"><mrow id="S3.E6.m1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.cmml"><msup id="S3.E6.m1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.3.cmml"><mtext id="S3.E6.m1.1.1.1.1.3.2" xref="S3.E6.m1.1.1.1.1.3.2a.cmml">MH Attention</mtext><mi id="S3.E6.m1.1.1.1.1.3.3" xref="S3.E6.m1.1.1.1.1.3.3.cmml">i</mi></msup><mo rspace="0.111em" id="S3.E6.m1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E6.m1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.cmml"><munder id="S3.E6.m1.1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.1.2.cmml"><mo movablelimits="false" id="S3.E6.m1.1.1.1.1.1.2.2" xref="S3.E6.m1.1.1.1.1.1.2.2.cmml">∑</mo><mi id="S3.E6.m1.1.1.1.1.1.2.3" xref="S3.E6.m1.1.1.1.1.1.2.3.cmml">h</mi></munder><mrow id="S3.E6.m1.1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.cmml"><msubsup id="S3.E6.m1.1.1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.1.1.3.cmml"><mtext class="ltx_mathvariant_bold" id="S3.E6.m1.1.1.1.1.1.1.3.2.2" xref="S3.E6.m1.1.1.1.1.1.1.3.2.2a.cmml">W</mtext><mi id="S3.E6.m1.1.1.1.1.1.1.3.3" xref="S3.E6.m1.1.1.1.1.1.1.3.3.cmml">h</mi><mi id="S3.E6.m1.1.1.1.1.1.1.3.2.3" xref="S3.E6.m1.1.1.1.1.1.1.3.2.3.cmml">O</mi></msubsup><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E6.m1.1.1.1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.1.2.cmml"><mo maxsize="120%" minsize="120%" id="S3.E6.m1.1.1.1.1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.1.1.1.2.1.cmml">[</mo><mrow id="S3.E6.m1.1.1.1.1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.cmml"><munderover id="S3.E6.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.2.cmml"><mo lspace="0em" movablelimits="false" id="S3.E6.m1.1.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E6.m1.1.1.1.1.1.1.1.1.1.2.2.3" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.2.2.3.cmml"><mi id="S3.E6.m1.1.1.1.1.1.1.1.1.1.2.2.3.2" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.2.2.3.2.cmml">k</mi><mo id="S3.E6.m1.1.1.1.1.1.1.1.1.1.2.2.3.1" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E6.m1.1.1.1.1.1.1.1.1.1.2.2.3.3" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E6.m1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.2.3.cmml">K</mi></munderover><mrow id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">A</mi><mrow id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.3.2" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.3.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.3.1" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.3.3" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.3.1a" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.3.4" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.3.4.cmml">k</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">​</mo><msub id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.4" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.4.cmml"><mtext class="ltx_mathvariant_bold" id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.4.2" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.4.2a.cmml">W</mtext><mi id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.4.3" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.4.3.cmml">v</mi></msub><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.2a" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">​</mo><msub id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.5" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.5.cmml"><mtext class="ltx_mathvariant_bold" id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.5.2" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.5.2a.cmml">x</mtext><mi id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.5.3" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.5.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.2b" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mtext class="ltx_mathvariant_bold" id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2a.cmml">p</mtext><mi id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">+</mo><mrow id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi mathvariant="normal" id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">​</mo><msub id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml"><mtext class="ltx_mathvariant_bold" id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.2" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.2a.cmml">p</mtext><mrow id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.3" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.3.cmml"><mi id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.3.2" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.3.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.3.1" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.3.1.cmml">​</mo><mi id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.3.3" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.3.1a" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.3.1.cmml">​</mo><mi id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.3.4" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.3.4.cmml">k</mi></mrow></msub></mrow></mrow><mo stretchy="false" id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo maxsize="120%" minsize="120%" id="S3.E6.m1.1.1.1.1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.1.1.1.2.1.cmml">]</mo></mrow></mrow></mrow></mrow><mo id="S3.E6.m1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m1.1b"><apply id="S3.E6.m1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1"><eq id="S3.E6.m1.1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1.2"></eq><apply id="S3.E6.m1.1.1.1.1.3.cmml" xref="S3.E6.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.3.1.cmml" xref="S3.E6.m1.1.1.1.1.3">superscript</csymbol><ci id="S3.E6.m1.1.1.1.1.3.2a.cmml" xref="S3.E6.m1.1.1.1.1.3.2"><mtext id="S3.E6.m1.1.1.1.1.3.2.cmml" xref="S3.E6.m1.1.1.1.1.3.2">MH Attention</mtext></ci><ci id="S3.E6.m1.1.1.1.1.3.3.cmml" xref="S3.E6.m1.1.1.1.1.3.3">𝑖</ci></apply><apply id="S3.E6.m1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1"><apply id="S3.E6.m1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.1.2.1.cmml" xref="S3.E6.m1.1.1.1.1.1.2">subscript</csymbol><sum id="S3.E6.m1.1.1.1.1.1.2.2.cmml" xref="S3.E6.m1.1.1.1.1.1.2.2"></sum><ci id="S3.E6.m1.1.1.1.1.1.2.3.cmml" xref="S3.E6.m1.1.1.1.1.1.2.3">ℎ</ci></apply><apply id="S3.E6.m1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1"><times id="S3.E6.m1.1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.2"></times><apply id="S3.E6.m1.1.1.1.1.1.1.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.3">subscript</csymbol><apply id="S3.E6.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.E6.m1.1.1.1.1.1.1.3.2.2a.cmml" xref="S3.E6.m1.1.1.1.1.1.1.3.2.2"><mtext class="ltx_mathvariant_bold" id="S3.E6.m1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.3.2.2">W</mtext></ci><ci id="S3.E6.m1.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.3.2.3">𝑂</ci></apply><ci id="S3.E6.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.3.3">ℎ</ci></apply><apply id="S3.E6.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E6.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.E6.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1"><apply id="S3.E6.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E6.m1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><sum id="S3.E6.m1.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.2.2.2"></sum><apply id="S3.E6.m1.1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.2.2.3"><eq id="S3.E6.m1.1.1.1.1.1.1.1.1.1.2.2.3.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.2.2.3.1"></eq><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.1.2.2.3.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.2.2.3.2">𝑘</ci><cn type="integer" id="S3.E6.m1.1.1.1.1.1.1.1.1.1.2.2.3.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.2.3">𝐾</ci></apply><apply id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1"><times id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.2"></times><apply id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.2">𝐴</ci><apply id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.3"><times id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.3.1"></times><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.3.2">ℎ</ci><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.3.3">𝑖</ci><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.3.4.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.3.3.4">𝑘</ci></apply></apply><apply id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.4.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.4.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.4">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.4.2a.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.4.2"><mtext class="ltx_mathvariant_bold" id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.4.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.4.2">W</mtext></ci><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.4.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.4.3">𝑣</ci></apply><apply id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.5.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.5"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.5.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.5">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.5.2a.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.5.2"><mtext class="ltx_mathvariant_bold" id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.5.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.5.2">x</mtext></ci><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.5.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.5.3">𝑘</ci></apply><apply id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1"><plus id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1"></plus><apply id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2a.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2"><mtext class="ltx_mathvariant_bold" id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2">p</mtext></ci><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3"><times id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1"></times><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2">Δ</ci><apply id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.2a.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.2"><mtext class="ltx_mathvariant_bold" id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.2">p</mtext></ci><apply id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.3"><times id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.3.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.3.1"></times><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.3.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.3.2">ℎ</ci><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.3.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.3.3">𝑖</ci><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.3.4.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.3.4">𝑘</ci></apply></apply></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.1c">\text{MH Attention}^{i}=\sum_{h}\textbf{W}^{O}_{h}\big{[}\sum_{k=1}^{K}A_{hik}\textbf{W}_{v}\textbf{x}_{k}(\textbf{p}_{i}+\Delta\textbf{p}_{hik})\big{]},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p2.14" class="ltx_p">where <math id="S3.SS2.p2.2.m1.3" class="ltx_Math" alttext="i=1,\cdots,T" display="inline"><semantics id="S3.SS2.p2.2.m1.3a"><mrow id="S3.SS2.p2.2.m1.3.4" xref="S3.SS2.p2.2.m1.3.4.cmml"><mi id="S3.SS2.p2.2.m1.3.4.2" xref="S3.SS2.p2.2.m1.3.4.2.cmml">i</mi><mo id="S3.SS2.p2.2.m1.3.4.1" xref="S3.SS2.p2.2.m1.3.4.1.cmml">=</mo><mrow id="S3.SS2.p2.2.m1.3.4.3.2" xref="S3.SS2.p2.2.m1.3.4.3.1.cmml"><mn id="S3.SS2.p2.2.m1.1.1" xref="S3.SS2.p2.2.m1.1.1.cmml">1</mn><mo id="S3.SS2.p2.2.m1.3.4.3.2.1" xref="S3.SS2.p2.2.m1.3.4.3.1.cmml">,</mo><mi mathvariant="normal" id="S3.SS2.p2.2.m1.2.2" xref="S3.SS2.p2.2.m1.2.2.cmml">⋯</mi><mo id="S3.SS2.p2.2.m1.3.4.3.2.2" xref="S3.SS2.p2.2.m1.3.4.3.1.cmml">,</mo><mi id="S3.SS2.p2.2.m1.3.3" xref="S3.SS2.p2.2.m1.3.3.cmml">T</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m1.3b"><apply id="S3.SS2.p2.2.m1.3.4.cmml" xref="S3.SS2.p2.2.m1.3.4"><eq id="S3.SS2.p2.2.m1.3.4.1.cmml" xref="S3.SS2.p2.2.m1.3.4.1"></eq><ci id="S3.SS2.p2.2.m1.3.4.2.cmml" xref="S3.SS2.p2.2.m1.3.4.2">𝑖</ci><list id="S3.SS2.p2.2.m1.3.4.3.1.cmml" xref="S3.SS2.p2.2.m1.3.4.3.2"><cn type="integer" id="S3.SS2.p2.2.m1.1.1.cmml" xref="S3.SS2.p2.2.m1.1.1">1</cn><ci id="S3.SS2.p2.2.m1.2.2.cmml" xref="S3.SS2.p2.2.m1.2.2">⋯</ci><ci id="S3.SS2.p2.2.m1.3.3.cmml" xref="S3.SS2.p2.2.m1.3.3">𝑇</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m1.3c">i=1,\cdots,T</annotation></semantics></math> and <math id="S3.SS2.p2.3.m2.1" class="ltx_Math" alttext="\textbf{p}_{i}" display="inline"><semantics id="S3.SS2.p2.3.m2.1a"><msub id="S3.SS2.p2.3.m2.1.1" xref="S3.SS2.p2.3.m2.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S3.SS2.p2.3.m2.1.1.2" xref="S3.SS2.p2.3.m2.1.1.2a.cmml">p</mtext><mi id="S3.SS2.p2.3.m2.1.1.3" xref="S3.SS2.p2.3.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m2.1b"><apply id="S3.SS2.p2.3.m2.1.1.cmml" xref="S3.SS2.p2.3.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m2.1.1.1.cmml" xref="S3.SS2.p2.3.m2.1.1">subscript</csymbol><ci id="S3.SS2.p2.3.m2.1.1.2a.cmml" xref="S3.SS2.p2.3.m2.1.1.2"><mtext class="ltx_mathvariant_bold" id="S3.SS2.p2.3.m2.1.1.2.cmml" xref="S3.SS2.p2.3.m2.1.1.2">p</mtext></ci><ci id="S3.SS2.p2.3.m2.1.1.3.cmml" xref="S3.SS2.p2.3.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m2.1c">\textbf{p}_{i}</annotation></semantics></math> is the reference point of the query and <math id="S3.SS2.p2.4.m3.1" class="ltx_Math" alttext="\Delta\textbf{p}_{hik}" display="inline"><semantics id="S3.SS2.p2.4.m3.1a"><mrow id="S3.SS2.p2.4.m3.1.1" xref="S3.SS2.p2.4.m3.1.1.cmml"><mi mathvariant="normal" id="S3.SS2.p2.4.m3.1.1.2" xref="S3.SS2.p2.4.m3.1.1.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.4.m3.1.1.1" xref="S3.SS2.p2.4.m3.1.1.1.cmml">​</mo><msub id="S3.SS2.p2.4.m3.1.1.3" xref="S3.SS2.p2.4.m3.1.1.3.cmml"><mtext class="ltx_mathvariant_bold" id="S3.SS2.p2.4.m3.1.1.3.2" xref="S3.SS2.p2.4.m3.1.1.3.2a.cmml">p</mtext><mrow id="S3.SS2.p2.4.m3.1.1.3.3" xref="S3.SS2.p2.4.m3.1.1.3.3.cmml"><mi id="S3.SS2.p2.4.m3.1.1.3.3.2" xref="S3.SS2.p2.4.m3.1.1.3.3.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.4.m3.1.1.3.3.1" xref="S3.SS2.p2.4.m3.1.1.3.3.1.cmml">​</mo><mi id="S3.SS2.p2.4.m3.1.1.3.3.3" xref="S3.SS2.p2.4.m3.1.1.3.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.4.m3.1.1.3.3.1a" xref="S3.SS2.p2.4.m3.1.1.3.3.1.cmml">​</mo><mi id="S3.SS2.p2.4.m3.1.1.3.3.4" xref="S3.SS2.p2.4.m3.1.1.3.3.4.cmml">k</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m3.1b"><apply id="S3.SS2.p2.4.m3.1.1.cmml" xref="S3.SS2.p2.4.m3.1.1"><times id="S3.SS2.p2.4.m3.1.1.1.cmml" xref="S3.SS2.p2.4.m3.1.1.1"></times><ci id="S3.SS2.p2.4.m3.1.1.2.cmml" xref="S3.SS2.p2.4.m3.1.1.2">Δ</ci><apply id="S3.SS2.p2.4.m3.1.1.3.cmml" xref="S3.SS2.p2.4.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m3.1.1.3.1.cmml" xref="S3.SS2.p2.4.m3.1.1.3">subscript</csymbol><ci id="S3.SS2.p2.4.m3.1.1.3.2a.cmml" xref="S3.SS2.p2.4.m3.1.1.3.2"><mtext class="ltx_mathvariant_bold" id="S3.SS2.p2.4.m3.1.1.3.2.cmml" xref="S3.SS2.p2.4.m3.1.1.3.2">p</mtext></ci><apply id="S3.SS2.p2.4.m3.1.1.3.3.cmml" xref="S3.SS2.p2.4.m3.1.1.3.3"><times id="S3.SS2.p2.4.m3.1.1.3.3.1.cmml" xref="S3.SS2.p2.4.m3.1.1.3.3.1"></times><ci id="S3.SS2.p2.4.m3.1.1.3.3.2.cmml" xref="S3.SS2.p2.4.m3.1.1.3.3.2">ℎ</ci><ci id="S3.SS2.p2.4.m3.1.1.3.3.3.cmml" xref="S3.SS2.p2.4.m3.1.1.3.3.3">𝑖</ci><ci id="S3.SS2.p2.4.m3.1.1.3.3.4.cmml" xref="S3.SS2.p2.4.m3.1.1.3.3.4">𝑘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m3.1c">\Delta\textbf{p}_{hik}</annotation></semantics></math> is the sampling offset (in 2D) in <math id="S3.SS2.p2.5.m4.1" class="ltx_Math" alttext="h-" display="inline"><semantics id="S3.SS2.p2.5.m4.1a"><mrow id="S3.SS2.p2.5.m4.1.1" xref="S3.SS2.p2.5.m4.1.1.cmml"><mi id="S3.SS2.p2.5.m4.1.1.2" xref="S3.SS2.p2.5.m4.1.1.2.cmml">h</mi><mo id="S3.SS2.p2.5.m4.1.1.3" xref="S3.SS2.p2.5.m4.1.1.3.cmml">−</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m4.1b"><apply id="S3.SS2.p2.5.m4.1.1.cmml" xref="S3.SS2.p2.5.m4.1.1"><csymbol cd="latexml" id="S3.SS2.p2.5.m4.1.1.1.cmml" xref="S3.SS2.p2.5.m4.1.1">limit-from</csymbol><ci id="S3.SS2.p2.5.m4.1.1.2.cmml" xref="S3.SS2.p2.5.m4.1.1.2">ℎ</ci><minus id="S3.SS2.p2.5.m4.1.1.3.cmml" xref="S3.SS2.p2.5.m4.1.1.3"></minus></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m4.1c">h-</annotation></semantics></math>th head with K samplings (K<math id="S3.SS2.p2.6.m5.1" class="ltx_Math" alttext="&lt;&lt;" display="inline"><semantics id="S3.SS2.p2.6.m5.1a"><mo id="S3.SS2.p2.6.m5.1.1" xref="S3.SS2.p2.6.m5.1.1.cmml">&lt;&lt;</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m5.1b"><csymbol cd="latexml" id="S3.SS2.p2.6.m5.1.1.cmml" xref="S3.SS2.p2.6.m5.1.1">much-less-than</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m5.1c">&lt;&lt;</annotation></semantics></math>T=HW). Figure <a href="#S3.F6" title="Figure 6 ‣ 3.1 Object Representation ‣ 3 Transfomers For Small Object Detection ‣ Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> illustrates the computation process within its multi-head attention module.
Deformable DETR benefits from both its encoder and decoder modules, with the complexity order within the encoder being <math id="S3.SS2.p2.7.m6.1" class="ltx_Math" alttext="\mathcal{O}(HWC^{2})" display="inline"><semantics id="S3.SS2.p2.7.m6.1a"><mrow id="S3.SS2.p2.7.m6.1.1" xref="S3.SS2.p2.7.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.7.m6.1.1.3" xref="S3.SS2.p2.7.m6.1.1.3.cmml">𝒪</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.7.m6.1.1.2" xref="S3.SS2.p2.7.m6.1.1.2.cmml">​</mo><mrow id="S3.SS2.p2.7.m6.1.1.1.1" xref="S3.SS2.p2.7.m6.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p2.7.m6.1.1.1.1.2" xref="S3.SS2.p2.7.m6.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p2.7.m6.1.1.1.1.1" xref="S3.SS2.p2.7.m6.1.1.1.1.1.cmml"><mi id="S3.SS2.p2.7.m6.1.1.1.1.1.2" xref="S3.SS2.p2.7.m6.1.1.1.1.1.2.cmml">H</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.7.m6.1.1.1.1.1.1" xref="S3.SS2.p2.7.m6.1.1.1.1.1.1.cmml">​</mo><mi id="S3.SS2.p2.7.m6.1.1.1.1.1.3" xref="S3.SS2.p2.7.m6.1.1.1.1.1.3.cmml">W</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.7.m6.1.1.1.1.1.1a" xref="S3.SS2.p2.7.m6.1.1.1.1.1.1.cmml">​</mo><msup id="S3.SS2.p2.7.m6.1.1.1.1.1.4" xref="S3.SS2.p2.7.m6.1.1.1.1.1.4.cmml"><mi id="S3.SS2.p2.7.m6.1.1.1.1.1.4.2" xref="S3.SS2.p2.7.m6.1.1.1.1.1.4.2.cmml">C</mi><mn id="S3.SS2.p2.7.m6.1.1.1.1.1.4.3" xref="S3.SS2.p2.7.m6.1.1.1.1.1.4.3.cmml">2</mn></msup></mrow><mo stretchy="false" id="S3.SS2.p2.7.m6.1.1.1.1.3" xref="S3.SS2.p2.7.m6.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.7.m6.1b"><apply id="S3.SS2.p2.7.m6.1.1.cmml" xref="S3.SS2.p2.7.m6.1.1"><times id="S3.SS2.p2.7.m6.1.1.2.cmml" xref="S3.SS2.p2.7.m6.1.1.2"></times><ci id="S3.SS2.p2.7.m6.1.1.3.cmml" xref="S3.SS2.p2.7.m6.1.1.3">𝒪</ci><apply id="S3.SS2.p2.7.m6.1.1.1.1.1.cmml" xref="S3.SS2.p2.7.m6.1.1.1.1"><times id="S3.SS2.p2.7.m6.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.7.m6.1.1.1.1.1.1"></times><ci id="S3.SS2.p2.7.m6.1.1.1.1.1.2.cmml" xref="S3.SS2.p2.7.m6.1.1.1.1.1.2">𝐻</ci><ci id="S3.SS2.p2.7.m6.1.1.1.1.1.3.cmml" xref="S3.SS2.p2.7.m6.1.1.1.1.1.3">𝑊</ci><apply id="S3.SS2.p2.7.m6.1.1.1.1.1.4.cmml" xref="S3.SS2.p2.7.m6.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.SS2.p2.7.m6.1.1.1.1.1.4.1.cmml" xref="S3.SS2.p2.7.m6.1.1.1.1.1.4">superscript</csymbol><ci id="S3.SS2.p2.7.m6.1.1.1.1.1.4.2.cmml" xref="S3.SS2.p2.7.m6.1.1.1.1.1.4.2">𝐶</ci><cn type="integer" id="S3.SS2.p2.7.m6.1.1.1.1.1.4.3.cmml" xref="S3.SS2.p2.7.m6.1.1.1.1.1.4.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.7.m6.1c">\mathcal{O}(HWC^{2})</annotation></semantics></math> where <math id="S3.SS2.p2.8.m7.1" class="ltx_Math" alttext="H" display="inline"><semantics id="S3.SS2.p2.8.m7.1a"><mi id="S3.SS2.p2.8.m7.1.1" xref="S3.SS2.p2.8.m7.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.8.m7.1b"><ci id="S3.SS2.p2.8.m7.1.1.cmml" xref="S3.SS2.p2.8.m7.1.1">𝐻</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.8.m7.1c">H</annotation></semantics></math> and <math id="S3.SS2.p2.9.m8.1" class="ltx_Math" alttext="W" display="inline"><semantics id="S3.SS2.p2.9.m8.1a"><mi id="S3.SS2.p2.9.m8.1.1" xref="S3.SS2.p2.9.m8.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.9.m8.1b"><ci id="S3.SS2.p2.9.m8.1.1.cmml" xref="S3.SS2.p2.9.m8.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.9.m8.1c">W</annotation></semantics></math> are the height and width of input feature map and <math id="S3.SS2.p2.10.m9.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.SS2.p2.10.m9.1a"><mi id="S3.SS2.p2.10.m9.1.1" xref="S3.SS2.p2.10.m9.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.10.m9.1b"><ci id="S3.SS2.p2.10.m9.1.1.cmml" xref="S3.SS2.p2.10.m9.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.10.m9.1c">C</annotation></semantics></math> is the number of channels. In contrast for the DETR encoder, the order of complexity is <math id="S3.SS2.p2.11.m10.1" class="ltx_Math" alttext="\mathcal{O}(H^{2}W^{2}C)" display="inline"><semantics id="S3.SS2.p2.11.m10.1a"><mrow id="S3.SS2.p2.11.m10.1.1" xref="S3.SS2.p2.11.m10.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.11.m10.1.1.3" xref="S3.SS2.p2.11.m10.1.1.3.cmml">𝒪</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.11.m10.1.1.2" xref="S3.SS2.p2.11.m10.1.1.2.cmml">​</mo><mrow id="S3.SS2.p2.11.m10.1.1.1.1" xref="S3.SS2.p2.11.m10.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p2.11.m10.1.1.1.1.2" xref="S3.SS2.p2.11.m10.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p2.11.m10.1.1.1.1.1" xref="S3.SS2.p2.11.m10.1.1.1.1.1.cmml"><msup id="S3.SS2.p2.11.m10.1.1.1.1.1.2" xref="S3.SS2.p2.11.m10.1.1.1.1.1.2.cmml"><mi id="S3.SS2.p2.11.m10.1.1.1.1.1.2.2" xref="S3.SS2.p2.11.m10.1.1.1.1.1.2.2.cmml">H</mi><mn id="S3.SS2.p2.11.m10.1.1.1.1.1.2.3" xref="S3.SS2.p2.11.m10.1.1.1.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S3.SS2.p2.11.m10.1.1.1.1.1.1" xref="S3.SS2.p2.11.m10.1.1.1.1.1.1.cmml">​</mo><msup id="S3.SS2.p2.11.m10.1.1.1.1.1.3" xref="S3.SS2.p2.11.m10.1.1.1.1.1.3.cmml"><mi id="S3.SS2.p2.11.m10.1.1.1.1.1.3.2" xref="S3.SS2.p2.11.m10.1.1.1.1.1.3.2.cmml">W</mi><mn id="S3.SS2.p2.11.m10.1.1.1.1.1.3.3" xref="S3.SS2.p2.11.m10.1.1.1.1.1.3.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S3.SS2.p2.11.m10.1.1.1.1.1.1a" xref="S3.SS2.p2.11.m10.1.1.1.1.1.1.cmml">​</mo><mi id="S3.SS2.p2.11.m10.1.1.1.1.1.4" xref="S3.SS2.p2.11.m10.1.1.1.1.1.4.cmml">C</mi></mrow><mo stretchy="false" id="S3.SS2.p2.11.m10.1.1.1.1.3" xref="S3.SS2.p2.11.m10.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.11.m10.1b"><apply id="S3.SS2.p2.11.m10.1.1.cmml" xref="S3.SS2.p2.11.m10.1.1"><times id="S3.SS2.p2.11.m10.1.1.2.cmml" xref="S3.SS2.p2.11.m10.1.1.2"></times><ci id="S3.SS2.p2.11.m10.1.1.3.cmml" xref="S3.SS2.p2.11.m10.1.1.3">𝒪</ci><apply id="S3.SS2.p2.11.m10.1.1.1.1.1.cmml" xref="S3.SS2.p2.11.m10.1.1.1.1"><times id="S3.SS2.p2.11.m10.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.11.m10.1.1.1.1.1.1"></times><apply id="S3.SS2.p2.11.m10.1.1.1.1.1.2.cmml" xref="S3.SS2.p2.11.m10.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.11.m10.1.1.1.1.1.2.1.cmml" xref="S3.SS2.p2.11.m10.1.1.1.1.1.2">superscript</csymbol><ci id="S3.SS2.p2.11.m10.1.1.1.1.1.2.2.cmml" xref="S3.SS2.p2.11.m10.1.1.1.1.1.2.2">𝐻</ci><cn type="integer" id="S3.SS2.p2.11.m10.1.1.1.1.1.2.3.cmml" xref="S3.SS2.p2.11.m10.1.1.1.1.1.2.3">2</cn></apply><apply id="S3.SS2.p2.11.m10.1.1.1.1.1.3.cmml" xref="S3.SS2.p2.11.m10.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.11.m10.1.1.1.1.1.3.1.cmml" xref="S3.SS2.p2.11.m10.1.1.1.1.1.3">superscript</csymbol><ci id="S3.SS2.p2.11.m10.1.1.1.1.1.3.2.cmml" xref="S3.SS2.p2.11.m10.1.1.1.1.1.3.2">𝑊</ci><cn type="integer" id="S3.SS2.p2.11.m10.1.1.1.1.1.3.3.cmml" xref="S3.SS2.p2.11.m10.1.1.1.1.1.3.3">2</cn></apply><ci id="S3.SS2.p2.11.m10.1.1.1.1.1.4.cmml" xref="S3.SS2.p2.11.m10.1.1.1.1.1.4">𝐶</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.11.m10.1c">\mathcal{O}(H^{2}W^{2}C)</annotation></semantics></math>, displaying a quadratic increase as <math id="S3.SS2.p2.12.m11.1" class="ltx_Math" alttext="H" display="inline"><semantics id="S3.SS2.p2.12.m11.1a"><mi id="S3.SS2.p2.12.m11.1.1" xref="S3.SS2.p2.12.m11.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.12.m11.1b"><ci id="S3.SS2.p2.12.m11.1.1.cmml" xref="S3.SS2.p2.12.m11.1.1">𝐻</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.12.m11.1c">H</annotation></semantics></math> and <math id="S3.SS2.p2.13.m12.1" class="ltx_Math" alttext="W" display="inline"><semantics id="S3.SS2.p2.13.m12.1a"><mi id="S3.SS2.p2.13.m12.1.1" xref="S3.SS2.p2.13.m12.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.13.m12.1b"><ci id="S3.SS2.p2.13.m12.1.1.cmml" xref="S3.SS2.p2.13.m12.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.13.m12.1c">W</annotation></semantics></math> increase in size. Deformable attention has played a prominent role in various other detectors, e.g., in T-TRD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>. Subsequently, Dynamic DETR was proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, featuring a dynamic encoder and a dynamic decoder that harness feature pyramids from low to high-resolution representations, resulting in efficient coarse-to-fine object detection and faster convergence. The dynamic encoder can be viewed as a sequentially decomposed approximation of full self-attention, dynamically adjusting attention mechanisms based on scale, spatial importance, and representation. Both Deformable DETR and Dynamic DETR make use of deformable convolution for feature extraction. In a distinct approach, O<sup id="S3.SS2.p2.14.1" class="ltx_sup">2</sup>DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> demonstrated that the global reasoning offered by a self-attention module is actually not essential for aerial images, where objects are usually densely packed in the same image area. Hence, replacing attention modules with local convolutions coupled with the integration of multi-scale feature maps, was proven to improve the detection performance in the context of oriented object detection. The authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> proposed the concept of Row-Column Decoupled
Attention (RCDA), decomposing the 2D attention of key features into two simpler forms: 1D row-wise and column-wise attentions. In the case of CF-DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>, an alternative approach to FPN was proposed whereby C5 features were replaced with encoder features at level 5 (E5), resulting in improved object presentation. This innovation was named Transformer Enhanced FPN (TEF) module. In another study, Xu <span id="S3.SS2.p2.14.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> developed a weighted Bidirectional Feature Pyramid Network (BiFPN) through the integration of skip connection operations with the Swin transformer. This approach effectively preserved information pertinent to small objects.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span><span id="S3.SS3.1.1" class="ltx_text ltx_font_italic">Fully Transformer-Based Detectors</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.5" class="ltx_p">The advent of transformers and their outstanding performance in many complex tasks in computer vision has gradually motivated researchers to shift from CNN-based or mixed systems to fully transformer-based vision systems. This line of work started with the application of a transformer-only architecture to the image recognition task, known as ViT, proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, ViDT extended the YOLOS model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> (the first fully transformer-based detector) to develop the first efficient detector suitable for SOD. In ViDT, the ResNet used in DETR for feature extraction is replaced with various ViT variants, such as Swin Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, ViTDet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>, and DeiT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>, along with the Reconfigured Attention Module (RAM). The RAM is capable of handling <math id="S3.SS3.p1.1.m1.2" class="ltx_Math" alttext="[\text{PATCH}]\times[\text{PATCH}]" display="inline"><semantics id="S3.SS3.p1.1.m1.2a"><mrow id="S3.SS3.p1.1.m1.2.3" xref="S3.SS3.p1.1.m1.2.3.cmml"><mrow id="S3.SS3.p1.1.m1.2.3.2.2" xref="S3.SS3.p1.1.m1.2.3.2.1.cmml"><mo stretchy="false" id="S3.SS3.p1.1.m1.2.3.2.2.1" xref="S3.SS3.p1.1.m1.2.3.2.1.1.cmml">[</mo><mtext id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1a.cmml">PATCH</mtext><mo rspace="0.055em" stretchy="false" id="S3.SS3.p1.1.m1.2.3.2.2.2" xref="S3.SS3.p1.1.m1.2.3.2.1.1.cmml">]</mo></mrow><mo rspace="0.222em" id="S3.SS3.p1.1.m1.2.3.1" xref="S3.SS3.p1.1.m1.2.3.1.cmml">×</mo><mrow id="S3.SS3.p1.1.m1.2.3.3.2" xref="S3.SS3.p1.1.m1.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS3.p1.1.m1.2.3.3.2.1" xref="S3.SS3.p1.1.m1.2.3.3.1.1.cmml">[</mo><mtext id="S3.SS3.p1.1.m1.2.2" xref="S3.SS3.p1.1.m1.2.2a.cmml">PATCH</mtext><mo stretchy="false" id="S3.SS3.p1.1.m1.2.3.3.2.2" xref="S3.SS3.p1.1.m1.2.3.3.1.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.2b"><apply id="S3.SS3.p1.1.m1.2.3.cmml" xref="S3.SS3.p1.1.m1.2.3"><times id="S3.SS3.p1.1.m1.2.3.1.cmml" xref="S3.SS3.p1.1.m1.2.3.1"></times><apply id="S3.SS3.p1.1.m1.2.3.2.1.cmml" xref="S3.SS3.p1.1.m1.2.3.2.2"><csymbol cd="latexml" id="S3.SS3.p1.1.m1.2.3.2.1.1.cmml" xref="S3.SS3.p1.1.m1.2.3.2.2.1">delimited-[]</csymbol><ci id="S3.SS3.p1.1.m1.1.1a.cmml" xref="S3.SS3.p1.1.m1.1.1"><mtext id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">PATCH</mtext></ci></apply><apply id="S3.SS3.p1.1.m1.2.3.3.1.cmml" xref="S3.SS3.p1.1.m1.2.3.3.2"><csymbol cd="latexml" id="S3.SS3.p1.1.m1.2.3.3.1.1.cmml" xref="S3.SS3.p1.1.m1.2.3.3.2.1">delimited-[]</csymbol><ci id="S3.SS3.p1.1.m1.2.2a.cmml" xref="S3.SS3.p1.1.m1.2.2"><mtext id="S3.SS3.p1.1.m1.2.2.cmml" xref="S3.SS3.p1.1.m1.2.2">PATCH</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.2c">[\text{PATCH}]\times[\text{PATCH}]</annotation></semantics></math>, <math id="S3.SS3.p1.2.m2.2" class="ltx_Math" alttext="[\text{DET}]\times[\text{PATCH}]" display="inline"><semantics id="S3.SS3.p1.2.m2.2a"><mrow id="S3.SS3.p1.2.m2.2.3" xref="S3.SS3.p1.2.m2.2.3.cmml"><mrow id="S3.SS3.p1.2.m2.2.3.2.2" xref="S3.SS3.p1.2.m2.2.3.2.1.cmml"><mo stretchy="false" id="S3.SS3.p1.2.m2.2.3.2.2.1" xref="S3.SS3.p1.2.m2.2.3.2.1.1.cmml">[</mo><mtext id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1a.cmml">DET</mtext><mo rspace="0.055em" stretchy="false" id="S3.SS3.p1.2.m2.2.3.2.2.2" xref="S3.SS3.p1.2.m2.2.3.2.1.1.cmml">]</mo></mrow><mo rspace="0.222em" id="S3.SS3.p1.2.m2.2.3.1" xref="S3.SS3.p1.2.m2.2.3.1.cmml">×</mo><mrow id="S3.SS3.p1.2.m2.2.3.3.2" xref="S3.SS3.p1.2.m2.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS3.p1.2.m2.2.3.3.2.1" xref="S3.SS3.p1.2.m2.2.3.3.1.1.cmml">[</mo><mtext id="S3.SS3.p1.2.m2.2.2" xref="S3.SS3.p1.2.m2.2.2a.cmml">PATCH</mtext><mo stretchy="false" id="S3.SS3.p1.2.m2.2.3.3.2.2" xref="S3.SS3.p1.2.m2.2.3.3.1.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.2b"><apply id="S3.SS3.p1.2.m2.2.3.cmml" xref="S3.SS3.p1.2.m2.2.3"><times id="S3.SS3.p1.2.m2.2.3.1.cmml" xref="S3.SS3.p1.2.m2.2.3.1"></times><apply id="S3.SS3.p1.2.m2.2.3.2.1.cmml" xref="S3.SS3.p1.2.m2.2.3.2.2"><csymbol cd="latexml" id="S3.SS3.p1.2.m2.2.3.2.1.1.cmml" xref="S3.SS3.p1.2.m2.2.3.2.2.1">delimited-[]</csymbol><ci id="S3.SS3.p1.2.m2.1.1a.cmml" xref="S3.SS3.p1.2.m2.1.1"><mtext id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">DET</mtext></ci></apply><apply id="S3.SS3.p1.2.m2.2.3.3.1.cmml" xref="S3.SS3.p1.2.m2.2.3.3.2"><csymbol cd="latexml" id="S3.SS3.p1.2.m2.2.3.3.1.1.cmml" xref="S3.SS3.p1.2.m2.2.3.3.2.1">delimited-[]</csymbol><ci id="S3.SS3.p1.2.m2.2.2a.cmml" xref="S3.SS3.p1.2.m2.2.2"><mtext id="S3.SS3.p1.2.m2.2.2.cmml" xref="S3.SS3.p1.2.m2.2.2">PATCH</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.2c">[\text{DET}]\times[\text{PATCH}]</annotation></semantics></math>, and <math id="S3.SS3.p1.3.m3.2" class="ltx_Math" alttext="[\text{DET}]\times[\text{DET}]" display="inline"><semantics id="S3.SS3.p1.3.m3.2a"><mrow id="S3.SS3.p1.3.m3.2.3" xref="S3.SS3.p1.3.m3.2.3.cmml"><mrow id="S3.SS3.p1.3.m3.2.3.2.2" xref="S3.SS3.p1.3.m3.2.3.2.1.cmml"><mo stretchy="false" id="S3.SS3.p1.3.m3.2.3.2.2.1" xref="S3.SS3.p1.3.m3.2.3.2.1.1.cmml">[</mo><mtext id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1a.cmml">DET</mtext><mo rspace="0.055em" stretchy="false" id="S3.SS3.p1.3.m3.2.3.2.2.2" xref="S3.SS3.p1.3.m3.2.3.2.1.1.cmml">]</mo></mrow><mo rspace="0.222em" id="S3.SS3.p1.3.m3.2.3.1" xref="S3.SS3.p1.3.m3.2.3.1.cmml">×</mo><mrow id="S3.SS3.p1.3.m3.2.3.3.2" xref="S3.SS3.p1.3.m3.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS3.p1.3.m3.2.3.3.2.1" xref="S3.SS3.p1.3.m3.2.3.3.1.1.cmml">[</mo><mtext id="S3.SS3.p1.3.m3.2.2" xref="S3.SS3.p1.3.m3.2.2a.cmml">DET</mtext><mo stretchy="false" id="S3.SS3.p1.3.m3.2.3.3.2.2" xref="S3.SS3.p1.3.m3.2.3.3.1.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.2b"><apply id="S3.SS3.p1.3.m3.2.3.cmml" xref="S3.SS3.p1.3.m3.2.3"><times id="S3.SS3.p1.3.m3.2.3.1.cmml" xref="S3.SS3.p1.3.m3.2.3.1"></times><apply id="S3.SS3.p1.3.m3.2.3.2.1.cmml" xref="S3.SS3.p1.3.m3.2.3.2.2"><csymbol cd="latexml" id="S3.SS3.p1.3.m3.2.3.2.1.1.cmml" xref="S3.SS3.p1.3.m3.2.3.2.2.1">delimited-[]</csymbol><ci id="S3.SS3.p1.3.m3.1.1a.cmml" xref="S3.SS3.p1.3.m3.1.1"><mtext id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">DET</mtext></ci></apply><apply id="S3.SS3.p1.3.m3.2.3.3.1.cmml" xref="S3.SS3.p1.3.m3.2.3.3.2"><csymbol cd="latexml" id="S3.SS3.p1.3.m3.2.3.3.1.1.cmml" xref="S3.SS3.p1.3.m3.2.3.3.2.1">delimited-[]</csymbol><ci id="S3.SS3.p1.3.m3.2.2a.cmml" xref="S3.SS3.p1.3.m3.2.2"><mtext id="S3.SS3.p1.3.m3.2.2.cmml" xref="S3.SS3.p1.3.m3.2.2">DET</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.2c">[\text{DET}]\times[\text{DET}]</annotation></semantics></math> attentions. These cross and self-attention modules are necessary because, similar to YOLOS, ViDT appends <math id="S3.SS3.p1.4.m4.1" class="ltx_Math" alttext="[\text{DET}]" display="inline"><semantics id="S3.SS3.p1.4.m4.1a"><mrow id="S3.SS3.p1.4.m4.1.2.2" xref="S3.SS3.p1.4.m4.1.2.1.cmml"><mo stretchy="false" id="S3.SS3.p1.4.m4.1.2.2.1" xref="S3.SS3.p1.4.m4.1.2.1.1.cmml">[</mo><mtext id="S3.SS3.p1.4.m4.1.1" xref="S3.SS3.p1.4.m4.1.1a.cmml">DET</mtext><mo stretchy="false" id="S3.SS3.p1.4.m4.1.2.2.2" xref="S3.SS3.p1.4.m4.1.2.1.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.1b"><apply id="S3.SS3.p1.4.m4.1.2.1.cmml" xref="S3.SS3.p1.4.m4.1.2.2"><csymbol cd="latexml" id="S3.SS3.p1.4.m4.1.2.1.1.cmml" xref="S3.SS3.p1.4.m4.1.2.2.1">delimited-[]</csymbol><ci id="S3.SS3.p1.4.m4.1.1a.cmml" xref="S3.SS3.p1.4.m4.1.1"><mtext id="S3.SS3.p1.4.m4.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1">DET</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">[\text{DET}]</annotation></semantics></math> and <math id="S3.SS3.p1.5.m5.1" class="ltx_Math" alttext="[\text{PATCH}]" display="inline"><semantics id="S3.SS3.p1.5.m5.1a"><mrow id="S3.SS3.p1.5.m5.1.2.2" xref="S3.SS3.p1.5.m5.1.2.1.cmml"><mo stretchy="false" id="S3.SS3.p1.5.m5.1.2.2.1" xref="S3.SS3.p1.5.m5.1.2.1.1.cmml">[</mo><mtext id="S3.SS3.p1.5.m5.1.1" xref="S3.SS3.p1.5.m5.1.1a.cmml">PATCH</mtext><mo stretchy="false" id="S3.SS3.p1.5.m5.1.2.2.2" xref="S3.SS3.p1.5.m5.1.2.1.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.5.m5.1b"><apply id="S3.SS3.p1.5.m5.1.2.1.cmml" xref="S3.SS3.p1.5.m5.1.2.2"><csymbol cd="latexml" id="S3.SS3.p1.5.m5.1.2.1.1.cmml" xref="S3.SS3.p1.5.m5.1.2.2.1">delimited-[]</csymbol><ci id="S3.SS3.p1.5.m5.1.1a.cmml" xref="S3.SS3.p1.5.m5.1.1"><mtext id="S3.SS3.p1.5.m5.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1">PATCH</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.5.m5.1c">[\text{PATCH}]</annotation></semantics></math> tokens in the input. ViDT only utilizes a transformer decoder as its neck to exploit multi-scale features generated at each stage of its body step. Figure <a href="#S3.F7" title="Figure 7 ‣ 3.2 Fast Attention for High-Resolution or Multi-Scale Feature Maps ‣ 3 Transfomers For Small Object Detection ‣ Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> illustrates the general structure of ViDT and highlights its differences from DETR and YOLOS.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Recognizing that the decoder module is the main source of inefficiency in transformer-based object detection, the Decoder-Free Fully Transformer (DFFT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> leverages two encoders: Scale-Aggregated Encoder (SAE) and Task-Aligned Encoder (TAE), to maintain high accuracy. SAE aggregates the multi-scale features (four scales) into a single feature map, while TAE aligns the single feature map for object type and position classification and regression. Multi-scale feature extraction with strong semantics is performed using a Detection-Oriented Transformer (DOT) backbone.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">In Sparse RoI-based deformable DETR (SRDD) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>, the authors proposed a lightweight transformer with a scoring system to ultimately remove redundant tokens in the encoder. This is achieved using RoI-based detection in an end-to-end learning scheme.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span><span id="S3.SS4.1.1" class="ltx_text ltx_font_italic">Architecture and Block Modifications</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">DETR, the first end-to-end object detection method, strugles with extended converge times during training and performs poorly on small objects. Several research works have addressed these issues to improve SOD performance. One notable contribution comes from Sun et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, who, drawing inspiration from FCOS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> (a fully convolutional single-stage detector) and Faster RCNN, proposed two encoder-only DETR variants with feature pyramids called TSP-FCOS and TSP-RCNN. This was accomplished by eliminating cross-attention modules from the decoder. Their findings demonstrated that cross-attention in the decoder and the instability of the Hungarian loss were the main reasons for the late convergence in DETR. This insight led them to discard the decoder and introduce a new bipartite matching technique in these new variants, i.e., TSP-FCOS and TSP-RCNN.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">In a combined approach using CNNs and transformers, Peng et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite> proposed a hybrid network structure called “Conformer”. This structure fuses the local feature representation provided by CNNs with the global feature representation provided by transformers at varying resolutions (see Figure <a href="#S3.F8" title="Figure 8 ‣ 3.4 Architecture and Block Modifications ‣ 3 Transfomers For Small Object Detection ‣ Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>). This was achieved through Feature Coupling Units (FCUs), with experimental results demonstrating its effectiveness compared to ResNet50, ResNet101, DeiT, and other models. A similar hybrid technique combining CNNs and transformers was proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>.
Recognizing the importance of local perception and long-range correlations, Xu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite> added a Local Perception Block (LPB) to the Swin Transformer block in the Swin Transformer. This new backbone, called the Local Perception Swin Transformer (LPSW), improved the detection of small-size objects in aerial images significantly. DIAG-TR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> introduced a Global-Local Feature Interweaving (GLFI) module in the encoder to adaptively and hierarchically embed local features into global representations. This technique counterbalances for the scale discrepancies of small objects. Furthermore, learnable anchor box coordinates were added to the content queries in the transformer decoder, providing an inductive bias.
In a recent study, Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite> proposed the Hybrid network Transformer (Hyneter), which extends the range of local information by embedding convolutions into the transformer blocks. This improvement led to enhanced detection results on the MS COCO dataset. Similar hybrid approaches have been adopted in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>. In another study <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>, the authors proposed a new backbone called NeXtFormer, which combines CNN and transformer to boost the local details and features of small objects, while also providing a global receptive field.</p>
</div>
<figure id="S3.F8" class="ltx_figure"><img src="/html/2309.04902/assets/Figures/conformern.jpg" id="S3.F8.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="419" height="239" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F8.3.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S3.F8.4.2" class="ltx_text" style="font-size:90%;">Conformer architecture which leverages both local features provided by CNNs and global features provided by transformers in Feature Coupling Unit (FCU) (figure from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>).</span></figcaption>
</figure>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p">Among various methods, O<sup id="S3.SS4.p3.1.1" class="ltx_sup">2</sup>DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> substituted the attention mechanism in transformers with depthwise separable convolution. This change not only decreased memory usage and computational costs associated with multi-scale features but also potentially enhanced the detection accuracy in aerial photographs.</p>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<p id="S3.SS4.p4.1" class="ltx_p">Questioning the object queries used in previous works, Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> proposed Anchor DETR, which used anchor points for object queries. These anchor points enhance the interpretability of the target query locations. The use of multiple patterns for each anchor point, improves the detection of multiple objects in one region. In contrast, Conditional DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite> emphasizes on the conditional spatial queries derived from the decoder content leading to spatial attention predictions. A subsequent version, Conditional DETR v2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>, enhanced the architecture by reformulating the object query into the form of a box query. This modification involves embedding a reference point and transforming boxes with respect to the reference point. In subsequent works, DAB-DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite> further improved on the idea of query design by using dynamically adjustable anchor boxes. These anchor boxes serve as both reference query points and anchor dimensions (see Figure <a href="#S3.F9" title="Figure 9 ‣ 3.4 Architecture and Block Modifications ‣ 3 Transfomers For Small Object Detection ‣ Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>).</p>
</div>
<figure id="S3.F9" class="ltx_figure"><img src="/html/2309.04902/assets/Figures/DAB-DETRn.jpg" id="S3.F9.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="282" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F9.3.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="S3.F9.4.2" class="ltx_text" style="font-size:90%;">DAB-DETR improves Conditional DETR and utilizes dynamic anchor boxes to sequentially provide better reference query points and anchor sizes (figure from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>).</span></figcaption>
</figure>
<div id="S3.SS4.p5" class="ltx_para">
<p id="S3.SS4.p5.1" class="ltx_p">In another work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>, the authors observed that while the mean average precision (mAP) of small objects in DETR is not competitive with state-of-the-art (SOTA) techniques, its performance for small intersection-over-union (IoU) thresholds is surprisingly better than its competitors. This indicates that while DETR provides strong perception abilities, it requires fine-tuning to achieve better localization accuracy. As a solution, the Coarse-to-Fine Detection Transformer (CF-DETR) has been proposed to perform this refinement through Adaptive Scale Fusion (ASF) and Local Cross-Attention (LCA) modules in the decoder layer. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite> the authors contend that the suboptimal performance of transformer-based detectors can be attributed to factors such as using a singular cross-attention module for both categorization and regression, inadequate initialization for content queries, and the absence of leveraging prior knowledge in the self-attention module. To address these concerns, they proposed Detection Split Transformer (DESTR). This model splits cross-attention into two branches, one for classification and one for regression. Moreover, DESTR uses a mini-detector to ensure proper content query initialization in the decoder and enhances the self-attention module. Another research <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>, introduced FEA-Swin, which leverages advanced foreground enhancement attention in the Swin Transformer framework to integrate context information into the original backbone. This was motivated by the fact that Swin Transformer does not adequately handle dense object detection due to missing connections between adjacent objects. Therefore, foreground enhancement highlights the objects for further correlation analysis. TOLO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite> is one of the recent works aiming to bring inductive bias (using CNN) to the transformer architecture through a simple neck module. This module combines features from different layers to incorporate high-resolution and high-semantic properties. Multiple light transformer heads were designed to detect objects at different scales. In a different approach, instead of modifying the modules in each architecture, CBNet, proposed by Liang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite>, groups multiple identical backbones that are connected through composite connections.</p>
</div>
<div id="S3.SS4.p6" class="ltx_para">
<p id="S3.SS4.p6.1" class="ltx_p">In the Multi-Source Aggregation Transformer (MATR) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>, the cross-attention module of the transformer is used to leverage other support images of the same object from different views. A similar approach is adopted in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite>, where the Multi-View Vision Transformer (MVViT) framework combines information from multiple views, including the target view, to improve the detection performance when objects are not visible in a single view.</p>
</div>
<div id="S3.SS4.p7" class="ltx_para">
<p id="S3.SS4.p7.1" class="ltx_p">Other works prefer to adhere to the YOLO family architecture. For instance, SPH-Yolov5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite> adds a new branch in the shallower layers of the Yolov5 network to fuse features for improved small object localization. It also incorporates for the first time the Swin Transformer prediction head in the Yolov5 pipeline.</p>
</div>
<div id="S3.SS4.p8" class="ltx_para">
<p id="S3.SS4.p8.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite>, the authors argue that the Hungarian loss’s direct one-to-one bounding box matching approach might not always be advantageous. They demonstrate that employing a one-to-many assignment strategy and utilizing the NMS (Non-Maximum Suppression) module leads to better detection results. Echoing this perspective, Group DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite> implements K groups of object queries with one-to-one label assignment, leading to K positive object queries for each ground-truth object to enhance performance.</p>
</div>
<div id="S3.SS4.p9" class="ltx_para">
<p id="S3.SS4.p9.4" class="ltx_p">A Dual-Key Transformer Network (DKTNet) is proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite>, where two keys are used—one key along with the <span id="S3.SS4.p9.4.1" class="ltx_text ltx_markedasmath ltx_font_bold">Q</span> stream and another key along with the <span id="S3.SS4.p9.4.2" class="ltx_text ltx_markedasmath ltx_font_bold">V</span> stream. This enhances the coherence between <span id="S3.SS4.p9.4.3" class="ltx_text ltx_markedasmath ltx_font_bold">Q</span> and <span id="S3.SS4.p9.4.4" class="ltx_text ltx_markedasmath ltx_font_bold">V</span>, leading to improved learning. Additionally, channel attention is computed instead of spatial attention, and 1D convolution is used to accelerate the process.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span><span id="S3.SS5.1.1" class="ltx_text ltx_font_italic">Auxiliary Techniques</span>
</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">Experimental results have demonstrated that auxiliary techniques or tasks, when combined with the main task, can enhance performance. In the context of transformers, several techniques have been adopted, including: <span id="S3.SS5.p1.1.1" class="ltx_text ltx_font_bold">(i)</span> Auxiliary Decoding/Encoding Loss: This refers to the approach where feed-forward networks designed for bounding box regression and object classification are connected to separate decoding layers. Hence individual losses at different scales are combined to train the models leading to better detection results. This technique or its variants have been used in ViDT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, MDef-DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite>, CBNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite>, SRDD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>. <span id="S3.SS5.p1.1.2" class="ltx_text ltx_font_bold">(ii)</span> Iterative Box Refinement: In this method, the bounding boxes within each decoding layer are refined based on the predictions from the previous layers. This feedback mechanism progressively improves detection accuracy. This technique has been used in ViDT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>. <span id="S3.SS5.p1.1.3" class="ltx_text ltx_font_bold">(iii)</span> Top-Down Supervision: This approach leverages human understandable semantics to aid in the intricate task of detecting small or class-agnostic objects, e.g., aligned image-text pairs in MDef-DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite>, or text-guided object detector in TGOD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>. <span id="S3.SS5.p1.1.4" class="ltx_text ltx_font_bold">(iv)</span> Pre-training: This involves training on large-scale datasets followed by specific fine-tuning for the detection task. This technique has been used in CBNet V2-TTA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite>, FP-DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite>, T-TRD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, SPH-Yolov5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite>, MATR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>, and extensively in Group DETR v2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>. <span id="S3.SS5.p1.1.5" class="ltx_text ltx_font_bold">(v)</span> Data Augmentation: This technique enriches the detection dataset by applying various augmentation techniques, such as rotation, flipping, zooming in and out, cropping, translation, adding noise, etc. Data augmentation is a commonly used approach to address various imbalance problems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>, e.g., imbalance in object size, within deep learning datasets. Data augmentation can be seen as an indirect approach to minimize the gap between train and test sets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite>. Several methods used augmentation in their detection task including T-TRD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, SPH-Yolov5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite>, MATR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>, NLFFTNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite>, DeoT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite>, HTDet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite>, and Sw-YoloX <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>. (<span id="S3.SS5.p1.1.6" class="ltx_text ltx_font_bold">vi</span>) One-to-Many Label Assignment: The one-to-one matching in DETR can result in poor discriminative features within the encoder. Hence, one-to-many assignments in other methods, e.g., Faster-RCNN, RetinaNet, and FCOS have been used as auxiliary heads in some studies such as CO-DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite>. (<span id="S3.SS5.p1.1.7" class="ltx_text ltx_font_bold">vii</span>) Denoising Training: This technique aims to boost the convergence speed of the decoder in DETR, which often faces an unstable convergence due to bipartite matching. In denoising training, the decoder is fed with noisy ground-truth
labels and boxes into the decoder. The model is then trained to reconstruct the original ground truth (guided by an auxiliary loss). Implementations like DINO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite> and DN-DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite> have demonstrated the effectiveness of this technique in enhancing the decoder’s stability.</p>
</div>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span><span id="S3.SS6.1.1" class="ltx_text ltx_font_italic">Improved Feature Representation</span>
</h3>

<div id="S3.SS6.p1" class="ltx_para">
<p id="S3.SS6.p1.1" class="ltx_p">Although current object detectors excel in a wide range of applications for regular-size or large objects, certain use-cases necessitate specialized feature representations for improved SOD. For instance, when it comes to detecting oriented objects in aerial imagery, any object rotation can drastically alter the feature representation due to increased background noise or clutter in the scene (region proposal). To address this, Dai <span id="S3.SS6.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite> ] proposed AO2-DETR, a method designed to be robust to arbitrary object rotations. This is achieved through three key components: <span id="S3.SS6.p1.1.2" class="ltx_text ltx_font_bold">(i)</span> the generation of oriented proposals, <span id="S3.SS6.p1.1.3" class="ltx_text ltx_font_bold">(ii)</span> a refinement module of the oriented proposal which extracts rotational-invariant features, and <span id="S3.SS6.p1.1.4" class="ltx_text ltx_font_bold">(iii)</span> a rotation-aware set matching loss. These modules help to negate the effects of any rotations of the objects. In a related approach, DETR++<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib91" title="" class="ltx_ref">91</a>]</cite>, uses multiple Bi-Directional Feature Pyramid layers (BiFPN) that are applied in a bottom-up fashion to feature maps from C3, C4, and C5. Then, only one scale which is representative of features at all scales is selected to be fed into DETR framework for detection. For some specific applications, such as plant safety monitoring, where objects of interest are usually related to human workers, leveraging this contextual information can greatly improve feature representation. PointDet++ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite> capitalizes on this by incorporating human pose estimation techniques, integrating local and global features to enhance SOD performance. Another crucial element that impacts feature quality is the backbone network and its ability to extract both semantic and high-resolution features. GhostNet introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite>, offers a streamlined and more efficient network that delivers high-quality, multi-scale features to the transformer. Their Ghost module in this network partially generates the output feature map, with the remainder being recovered using simple linear operations. This is a key step to alleviate the complexity of the backbone networks.
In the context of medical image analysis, MS Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite> used a self-supervised learning approach to perform a random mask on the input image, which aids in reconstructing richer features, that are less sensitive to the noise. In conjunction with a hierarchical transformer, this approach outperforms DETR frameworks with various backbones. The Small Object Favoring DETR (SOF-DETR) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite>, specifically favors the detection of small objects by merging convolutional features from layers 3 and 4 in a normalized inductive bias module prior to input into the DETR-Transformer. NLFFTNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite> addresses the limitation of only considering local interactions in current fusion techniques by introducing a nonlocal feature-fused transformer convolutional network, capturing long-distance semantic relationships between different feature layers. DeoT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite> merges an encoder-only transformer with a novel feature pyramid fusion module. This fusion is enhanced by the use of channel and spatial attention in the Channel Refinement Module (CRM) and Spatial Refinement Module (SRM), enabling the extraction of richer features. The authors in HTDet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite> proposed a fine-grained FPN to cumulatively fuse low-level and high-level features for better object detection. Meanwhile, in MDCT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref">96</a>]</cite> the author proposed a Multi-kernel Dilated Convolution (MDC) module to improve the performance of small
object-related feature extraction using both the ontology and adjacent spatial features of small objects. The proposed module leverages depth-wise separable convolution to reduce the computational cost. Lastly, in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite>, a feature fusion module paired with a lightweight backbone is engineered to enhance the visual features of small objects by broadening the receptive field. The hybrid attention module in RTD-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite> empowers the system to detect objects that are partially occluded, by incorporating contextual information surrounding small objects.</p>
</div>
<figure id="S3.F10" class="ltx_figure"><img src="/html/2309.04902/assets/x2.jpg" id="S3.F10.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="116" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F10.3.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span id="S3.F10.4.2" class="ltx_text" style="font-size:90%;">Chronology of SOD datasets with number of citations (based on Google Scholar).</span></figcaption>
</figure>
</section>
<section id="S3.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.7 </span><span id="S3.SS7.1.1" class="ltx_text ltx_font_italic">Spatio-Temporal Information</span>
</h3>

<div id="S3.SS7.p1" class="ltx_para">
<p id="S3.SS7.p1.1" class="ltx_p">In this section, our focus is exclusively on video-based object detectors that aim to identify small objects. While many of these studies have been tested on the ImageNet VID dataset <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://paperswithcode.com/sota/video-object-detection-on-imagenet-vid" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://paperswithcode.com/sota/video-object-detection-on-imagenet-vid</a> </span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite>, this dataset was not originally intended for small object detection. Nonetheless, a few of the works also reported their results for small objects of ImageNet VID dataset. The topic of tracking and detecting small objects in videos has also been explored using transformer architectures. Although techniques for image-based SOD can be applied to video, they generally do not utilize the valuable temporal information, which can be particularly beneficial for identifying small objects in cluttered or occluded frames. The application of transformers to generic object detection/tracking started with TrackFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite> and TransT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite>. These models used frame-to-frame (setting the previous frame as the reference) set prediction and template-to-frame (setting a template frame as the reference) detection. Liu <span id="S3.SS7.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib101" title="" class="ltx_ref">101</a>]</cite> were among the first to use transformers specifically for video-based small object detection and tracking. Their core concept is to update template frames to capture any small changes induced by the presence of small objects and to provide a global attention-driven relationship between the template frame and the search frame.
<br class="ltx_break">Transformer-based object detection gained formal recognition with the introduction of TransVOD, an end-to-end object detector, as presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib102" title="" class="ltx_ref">102</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib103" title="" class="ltx_ref">103</a>]</cite>. This model applies both spatial and temporal transformers to a series of video frames, thereby identifying and linking objects across these frames. TransVOD has spawned several variants, each with unique features, including capabilities for real-time detection. PTSEFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite> adopts a progressive strategy, focusing on both temporal information and the objects’ spatial transitions between frames. It employs multi-scale feature extraction to achieve this. Unlike other models, PTSEFormer directly regresses object queries from adjacent frames rather than the entire dataset, offering a more localized approach.
<br class="ltx_break">Sparse VOD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite> proposed an end-to-end trainable video object detector that incorporates temporal information to propose region proposals. In contrast, DAFA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib106" title="" class="ltx_ref">106</a>]</cite> highlights the significance of global features within a video as opposed to local temporal features. DEFA showed the inefficiency of the First In First Out (FIFO) memory structure and proposed a diversity-aware memory, which uses object-level memory instead of frame-level memory for the attention module. VSTAM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib107" title="" class="ltx_ref">107</a>]</cite> improves feature quality on an element-by-element basis and then performs sparse aggregation before these enhanced features are used for object candidate region detection. The model also incorporates external memory to take advantage of long-term contextual information.
<br class="ltx_break">In the FAQ work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib108" title="" class="ltx_ref">108</a>]</cite>, a novel video object detector is proposed that uses query feature aggregation in the decoder module. This is different than the methods that focus on either feature aggregation in the encoder or the methods that perform post-processing for various frames. The research indicates that this technique improves the detection performance outperforming SOTA methods.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Results and Benchmarks</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we quantitatively and qualitatively evaluate previous works of small object detection, identifying the most effective technique for a specific application. Prior to this comparison, we introduce a range of new datasets dedicated to small object detection, including both videos and images for diverse applications.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.8.1.1" class="ltx_text" style="font-size:90%;">TABLE II</span>: </span><span id="S4.T2.9.2" class="ltx_text" style="font-size:90%;">Commonly used datasets for SOD. NF: Not fixed.</span></figcaption>
<div id="S4.T2.6" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:672.5pt;height:196.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-144.1pt,42.1pt) scale(0.7,0.7) ;">
<table id="S4.T2.6.6" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.6.6.7" class="ltx_tr">
<td id="S4.T2.6.6.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_rr ltx_border_t"><span id="S4.T2.6.6.7.1.1" class="ltx_text ltx_font_bold">Dataset</span></td>
<td id="S4.T2.6.6.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.6.6.7.2.1" class="ltx_text ltx_font_bold">Application</span></td>
<td id="S4.T2.6.6.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.6.6.7.3.1" class="ltx_text ltx_font_bold">Video</span></td>
<td id="S4.T2.6.6.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.6.6.7.4.1" class="ltx_text ltx_font_bold">Image</span></td>
<td id="S4.T2.6.6.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.6.6.7.5.1" class="ltx_text ltx_font_bold">Shooting Angle (Type)</span></td>
<td id="S4.T2.6.6.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.6.6.7.6.1" class="ltx_text ltx_font_bold">Resolution (pixels)</span></td>
<td id="S4.T2.6.6.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">#<span id="S4.T2.6.6.7.7.1" class="ltx_text ltx_font_bold">Object Classes</span>
</td>
<td id="S4.T2.6.6.7.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">#<span id="S4.T2.6.6.7.8.1" class="ltx_text ltx_font_bold">Instances</span>
</td>
<td id="S4.T2.6.6.7.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">#<span id="S4.T2.6.6.7.9.1" class="ltx_text ltx_font_bold">Images/Videos</span>
</td>
<td id="S4.T2.6.6.7.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T2.6.6.7.10.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.6.6.7.10.1.1" class="ltx_p" style="width:56.9pt;"><span id="S4.T2.6.6.7.10.1.1.1" class="ltx_text ltx_font_bold">Public?</span></span>
</span>
</td>
</tr>
<tr id="S4.T2.1.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_rr ltx_border_tt">
<span id="S4.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">UAV123</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite>
</td>
<td id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">UAV Tracking</td>
<td id="S4.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">✓</td>
<td id="S4.T2.1.1.1.5" class="ltx_td ltx_border_r ltx_border_tt"></td>
<td id="S4.T2.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Aerial Perspective(RGB)</td>
<td id="S4.T2.1.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">–</td>
<td id="S4.T2.1.1.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">–</td>
<td id="S4.T2.1.1.1.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">–</td>
<td id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">123 (<math id="S4.T2.1.1.1.1.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S4.T2.1.1.1.1.m1.1a"><mo id="S4.T2.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.m1.1b"><gt id="S4.T2.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.m1.1c">&gt;</annotation></semantics></math>110K frames)</td>
<td id="S4.T2.1.1.1.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="S4.T2.1.1.1.10.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.1.1.10.1.1" class="ltx_p" style="width:56.9pt;">Yes: <a target="_blank" href="https://cemse.kaust.edu.sa/ivul/uav123" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Click Here</a></span>
</span>
</td>
</tr>
<tr id="S4.T2.6.6.8" class="ltx_tr">
<td id="S4.T2.6.6.8.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_rr ltx_border_t">
<span id="S4.T2.6.6.8.1.1" class="ltx_text ltx_font_bold">MRS-1800</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>
</td>
<td id="S4.T2.6.6.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Remote Sensing</td>
<td id="S4.T2.6.6.8.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T2.6.6.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.6.6.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Satellite based(RGB)</td>
<td id="S4.T2.6.6.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">NF</td>
<td id="S4.T2.6.6.8.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3</td>
<td id="S4.T2.6.6.8.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">16,318</td>
<td id="S4.T2.6.6.8.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1800</td>
<td id="S4.T2.6.6.8.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T2.6.6.8.10.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.6.6.8.10.1.1" class="ltx_p" style="width:56.9pt;">–</span>
</span>
</td>
</tr>
<tr id="S4.T2.6.6.9" class="ltx_tr">
<td id="S4.T2.6.6.9.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_rr ltx_border_t"><span id="S4.T2.6.6.9.1.1" class="ltx_text ltx_font_bold">SKU-110K<cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.6.6.9.1.1.1.1" class="ltx_text ltx_font_medium">[</span><a href="#bib.bib110" title="" class="ltx_ref">110</a><span id="S4.T2.6.6.9.1.1.2.2" class="ltx_text ltx_font_medium">]</span></cite></span></td>
<td id="S4.T2.6.6.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Commodity Detection</td>
<td id="S4.T2.6.6.9.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T2.6.6.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.6.6.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Normal</td>
<td id="S4.T2.6.6.9.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">NF</td>
<td id="S4.T2.6.6.9.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">110,712</td>
<td id="S4.T2.6.6.9.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">147.4 per image</td>
<td id="S4.T2.6.6.9.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11,762</td>
<td id="S4.T2.6.6.9.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T2.6.6.9.10.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.6.6.9.10.1.1" class="ltx_p" style="width:56.9pt;">Yes: <a target="_blank" href="https://github.com/eg4000/SKU110K_CVPR19" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Click Here</a></span>
</span>
</td>
</tr>
<tr id="S4.T2.6.6.10" class="ltx_tr">
<td id="S4.T2.6.6.10.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_rr ltx_border_t"><span id="S4.T2.6.6.10.1.1" class="ltx_text ltx_font_bold">BigDetection<cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.6.6.10.1.1.1.1" class="ltx_text ltx_font_medium">[</span><a href="#bib.bib79" title="" class="ltx_ref">79</a><span id="S4.T2.6.6.10.1.1.2.2" class="ltx_text ltx_font_medium">]</span></cite></span></td>
<td id="S4.T2.6.6.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Generic</td>
<td id="S4.T2.6.6.10.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T2.6.6.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.6.6.10.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Normal</td>
<td id="S4.T2.6.6.10.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">NF</td>
<td id="S4.T2.6.6.10.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">600</td>
<td id="S4.T2.6.6.10.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">36M</td>
<td id="S4.T2.6.6.10.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S4.T2.6.6.10.9.1" class="ltx_text"></span> <span id="S4.T2.6.6.10.9.2" class="ltx_text">
<span id="S4.T2.6.6.10.9.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.6.6.10.9.2.1.1" class="ltx_tr">
<span id="S4.T2.6.6.10.9.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">3.4M Training</span></span>
<span id="S4.T2.6.6.10.9.2.1.2" class="ltx_tr">
<span id="S4.T2.6.6.10.9.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">141K Test</span></span>
</span></span><span id="S4.T2.6.6.10.9.3" class="ltx_text"></span></td>
<td id="S4.T2.6.6.10.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T2.6.6.10.10.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.6.6.10.10.1.1" class="ltx_p" style="width:56.9pt;">Yes: <a target="_blank" href="https://github.com/amazon-science/bigdetection" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Click Here</a></span>
</span>
</td>
</tr>
<tr id="S4.T2.6.6.11" class="ltx_tr">
<td id="S4.T2.6.6.11.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_rr ltx_border_t">
<span id="S4.T2.6.6.11.1.1" class="ltx_text ltx_font_bold">Tang et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite>
</td>
<td id="S4.T2.6.6.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Cemical Plant Monitoring</td>
<td id="S4.T2.6.6.11.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T2.6.6.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.6.6.11.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Normal</td>
<td id="S4.T2.6.6.11.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">–</td>
<td id="S4.T2.6.6.11.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">19</td>
<td id="S4.T2.6.6.11.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">–</td>
<td id="S4.T2.6.6.11.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2400</td>
<td id="S4.T2.6.6.11.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T2.6.6.11.10.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.6.6.11.10.1.1" class="ltx_p" style="width:56.9pt;">–</span>
</span>
</td>
</tr>
<tr id="S4.T2.2.2.2" class="ltx_tr">
<td id="S4.T2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_rr ltx_border_t">
<span id="S4.T2.2.2.2.2.1" class="ltx_text ltx_font_bold">Xu et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>
</td>
<td id="S4.T2.2.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">UAV-based Detection</td>
<td id="S4.T2.2.2.2.4" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T2.2.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.2.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Aerial (RGB)</td>
<td id="S4.T2.2.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1920<math id="S4.T2.2.2.2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T2.2.2.2.1.m1.1a"><mo id="S4.T2.2.2.2.1.m1.1.1" xref="S4.T2.2.2.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.1.m1.1b"><times id="S4.T2.2.2.2.1.m1.1.1.cmml" xref="S4.T2.2.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.1.m1.1c">\times</annotation></semantics></math>1080</td>
<td id="S4.T2.2.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2</td>
<td id="S4.T2.2.2.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">12.5K</td>
<td id="S4.T2.2.2.2.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2K</td>
<td id="S4.T2.2.2.2.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T2.2.2.2.10.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.2.2.2.10.1.1" class="ltx_p" style="width:56.9pt;">Yes: <a target="_blank" href="https://pan.baidu.com/share/init?surl=4UcfTtZnvvVyCV2tAzHFKw" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Click Here</a></span>
</span>
</td>
</tr>
<tr id="S4.T2.6.6.12" class="ltx_tr">
<td id="S4.T2.6.6.12.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_rr ltx_border_t">
<span id="S4.T2.6.6.12.1.1" class="ltx_text ltx_font_bold">DeepLesion</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib111" title="" class="ltx_ref">111</a>]</cite>
</td>
<td id="S4.T2.6.6.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Lesion detection</td>
<td id="S4.T2.6.6.12.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T2.6.6.12.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.6.6.12.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">(CT)</td>
<td id="S4.T2.6.6.12.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">–</td>
<td id="S4.T2.6.6.12.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8</td>
<td id="S4.T2.6.6.12.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">32.7K</td>
<td id="S4.T2.6.6.12.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">32.1K</td>
<td id="S4.T2.6.6.12.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T2.6.6.12.10.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.6.6.12.10.1.1" class="ltx_p" style="width:56.9pt;">Yes: <a target="_blank" href="https://nihcc.app.box.com/v/DeepLesion" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Click Here</a></span>
</span>
</td>
</tr>
<tr id="S4.T2.3.3.3" class="ltx_tr">
<td id="S4.T2.3.3.3.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_rr ltx_border_t">
<span id="S4.T2.3.3.3.2.1" class="ltx_text ltx_font_bold">Udacity Self Driving Car</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib112" title="" class="ltx_ref">112</a>]</cite>
</td>
<td id="S4.T2.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Self-Driving</td>
<td id="S4.T2.3.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.3.3.3.5" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T2.3.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Normal</td>
<td id="S4.T2.3.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1920<math id="S4.T2.3.3.3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T2.3.3.3.1.m1.1a"><mo id="S4.T2.3.3.3.1.m1.1.1" xref="S4.T2.3.3.3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.1.m1.1b"><times id="S4.T2.3.3.3.1.m1.1.1.cmml" xref="S4.T2.3.3.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.1.m1.1c">\times</annotation></semantics></math>1200</td>
<td id="S4.T2.3.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3</td>
<td id="S4.T2.3.3.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">65K</td>
<td id="S4.T2.3.3.3.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">9,423</td>
<td id="S4.T2.3.3.3.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T2.3.3.3.10.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.3.3.3.10.1.1" class="ltx_p" style="width:56.9pt;">Yes: <a target="_blank" href="https://github.com/udacity/self-driving-car/tree/master/annotations" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Click Here</a></span>
</span>
</td>
</tr>
<tr id="S4.T2.6.6.6" class="ltx_tr">
<td id="S4.T2.6.6.6.4" class="ltx_td ltx_align_center ltx_border_l ltx_border_rr ltx_border_t">
<span id="S4.T2.6.6.6.4.1" class="ltx_text ltx_font_bold">AMMW Dataset</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite>
</td>
<td id="S4.T2.6.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Security Inspection</td>
<td id="S4.T2.6.6.6.6" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T2.6.6.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.6.6.6.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Normal (AMMW)</td>
<td id="S4.T2.4.4.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">160<math id="S4.T2.4.4.4.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T2.4.4.4.1.m1.1a"><mo id="S4.T2.4.4.4.1.m1.1.1" xref="S4.T2.4.4.4.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.4.1.m1.1b"><times id="S4.T2.4.4.4.1.m1.1.1.cmml" xref="S4.T2.4.4.4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.4.1.m1.1c">\times</annotation></semantics></math>400</td>
<td id="S4.T2.5.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<math id="S4.T2.5.5.5.2.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S4.T2.5.5.5.2.m1.1a"><mo id="S4.T2.5.5.5.2.m1.1.1" xref="S4.T2.5.5.5.2.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.5.2.m1.1b"><gt id="S4.T2.5.5.5.2.m1.1.1.cmml" xref="S4.T2.5.5.5.2.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.5.2.m1.1c">&gt;</annotation></semantics></math>30</td>
<td id="S4.T2.6.6.6.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">–</td>
<td id="S4.T2.6.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<math id="S4.T2.6.6.6.3.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S4.T2.6.6.6.3.m1.1a"><mo id="S4.T2.6.6.6.3.m1.1.1" xref="S4.T2.6.6.6.3.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.T2.6.6.6.3.m1.1b"><gt id="S4.T2.6.6.6.3.m1.1.1.cmml" xref="S4.T2.6.6.6.3.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.6.6.3.m1.1c">&gt;</annotation></semantics></math>58K</td>
<td id="S4.T2.6.6.6.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T2.6.6.6.10.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.6.6.6.10.1.1" class="ltx_p" style="width:56.9pt;">–</span>
</span>
</td>
</tr>
<tr id="S4.T2.6.6.13" class="ltx_tr">
<td id="S4.T2.6.6.13.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_rr ltx_border_t">
<span id="S4.T2.6.6.13.1.1" class="ltx_text ltx_font_bold">URPC 2018 Dataset</span> <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>http://en.cnurpc.org/</span></span></span>
</td>
<td id="S4.T2.6.6.13.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Underwater Detection</td>
<td id="S4.T2.6.6.13.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T2.6.6.13.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.6.6.13.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Normal</td>
<td id="S4.T2.6.6.13.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">–</td>
<td id="S4.T2.6.6.13.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4</td>
<td id="S4.T2.6.6.13.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">–</td>
<td id="S4.T2.6.6.13.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S4.T2.6.6.13.9.1" class="ltx_text"></span> <span id="S4.T2.6.6.13.9.2" class="ltx_text">
<span id="S4.T2.6.6.13.9.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.6.6.13.9.2.1.1" class="ltx_tr">
<span id="S4.T2.6.6.13.9.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">2,901 Training</span></span>
<span id="S4.T2.6.6.13.9.2.1.2" class="ltx_tr">
<span id="S4.T2.6.6.13.9.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">800 Test</span></span>
</span></span><span id="S4.T2.6.6.13.9.3" class="ltx_text"></span></td>
<td id="S4.T2.6.6.13.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T2.6.6.13.10.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.6.6.13.10.1.1" class="ltx_p" style="width:56.9pt;">–</span>
</span>
</td>
</tr>
<tr id="S4.T2.6.6.14" class="ltx_tr">
<td id="S4.T2.6.6.14.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_rr ltx_border_t">
<span id="S4.T2.6.6.14.1.1" class="ltx_text ltx_font_bold">UAV dataset</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite>
</td>
<td id="S4.T2.6.6.14.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">UAV-based detection</td>
<td id="S4.T2.6.6.14.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T2.6.6.14.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.6.6.14.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Aerial Perspective (RGB)</td>
<td id="S4.T2.6.6.14.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">–</td>
<td id="S4.T2.6.6.14.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">7</td>
<td id="S4.T2.6.6.14.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">320,624</td>
<td id="S4.T2.6.6.14.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">9,630</td>
<td id="S4.T2.6.6.14.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T2.6.6.14.10.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.6.6.14.10.1.1" class="ltx_p" style="width:56.9pt;">–</span>
</span>
</td>
</tr>
<tr id="S4.T2.6.6.15" class="ltx_tr">
<td id="S4.T2.6.6.15.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_rr ltx_border_t">
<span id="S4.T2.6.6.15.1.1" class="ltx_text ltx_font_bold">Drone-vs-bird</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib114" title="" class="ltx_ref">114</a>]</cite>
</td>
<td id="S4.T2.6.6.15.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Drone Detection</td>
<td id="S4.T2.6.6.15.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.6.6.15.4" class="ltx_td ltx_border_b ltx_border_r ltx_border_t"></td>
<td id="S4.T2.6.6.15.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Normal</td>
<td id="S4.T2.6.6.15.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">NF</td>
<td id="S4.T2.6.6.15.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">2</td>
<td id="S4.T2.6.6.15.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">–</td>
<td id="S4.T2.6.6.15.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">77 Training 1,384 Frames</td>
<td id="S4.T2.6.6.15.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T2.6.6.15.10.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.6.6.15.10.1.1" class="ltx_p" style="width:56.9pt;">Yes: <a target="_blank" href="https://github.com/wosdetc/challenge" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Click Here</a></span>
</span>
</td>
</tr>
</table>
</span></div>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span><span id="S4.SS1.1.1" class="ltx_text ltx_font_italic">Datasets</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">In this subsection, in addition to the widely used MS COCO dataset, we compile and present 12 new SOD datasets. These new datasets are primarily tailored for specific applications excluding the generic and maritime environments (which have been covered in our previous survey <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>). Figure <a href="#S3.F10" title="Figure 10 ‣ 3.6 Improved Feature Representation ‣ 3 Transfomers For Small Object Detection ‣ Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> displays the chronological order of these datasets along with their citation count as of June 15 2023, according to Google Scholar. 
<br class="ltx_break"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">UAV123</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite>: This dataset contains 123 videos acquired with UAVs and it is one of the largest object-tracking datasets with more than 110K frames. 
<br class="ltx_break"><span id="S4.SS1.p1.1.2" class="ltx_text ltx_font_bold">MRS-1800</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>: ]: This dataset consists of a combination of images from three other remote sensing datasets: DIOR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib115" title="" class="ltx_ref">115</a>]</cite>, NWPU VHR-10 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib116" title="" class="ltx_ref">116</a>]</cite>, and HRRSD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib117" title="" class="ltx_ref">117</a>]</cite>. MRD-1800 was created for the dual purpose of detection and instance segmentation, with 1800 manually annotated images which include 3 types of objects: airplanes, ships, and storage tanks. 
<br class="ltx_break"><span id="S4.SS1.p1.1.3" class="ltx_text ltx_font_bold">SKU-110K <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib110" title="" class="ltx_ref">110</a>]</cite></span>: This dataset serves as a rigorous testbed for commodity detection, featuring images captured from various supermarkets around the world. The dataset includes a range of scales, camera angles, lighting conditions, etc.
<br class="ltx_break"><span id="S4.SS1.p1.1.4" class="ltx_text ltx_font_bold">BigDetection<cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS1.p1.1.4.1.1" class="ltx_text ltx_font_medium">[</span><a href="#bib.bib79" title="" class="ltx_ref">79</a><span id="S4.SS1.p1.1.4.2.2" class="ltx_text ltx_font_medium">]</span></cite></span>: This is a large-scale dataset that is crafted by integrating existing datasets and meticulously eliminating duplicate boxes while labeling overlooked objects. It has a balanced number of objects across all sizes making it a pivotal resource for advancing the field object detection. Using this dataset for pretraining and subsequently fine-tuning on MS COCO significantly enhances performance outcomes. 
<br class="ltx_break"><span id="S4.SS1.p1.1.5" class="ltx_text ltx_font_bold">Tang et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite>: Originating from video footage of field activities within a chemical plant, this dataset covers various types of work such as hot work, aerial work, confined space operations, etc. It includes category labels like people, helmets, fire extinguishers, gloves, work clothes and other relevant objects.
<br class="ltx_break"></p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.25.2.1" class="ltx_text" style="font-size:90%;">TABLE III</span>: </span><span id="S4.T3.2.1" class="ltx_text" style="font-size:90%;">Detection performance (%) for small-scale objects on MS COCO image dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. The top section shows results for CNN-based techniques, the middle section shows results for mixed architectures, and the bottom section presents from transformer-only networks. DC5: Dilated C5 stage, MS: Multi-scale network, IBR: Iterative bounding box refinement, TS: Two-stage detection, DCN: Deformable convnets, TTA: Test time augmentation, BD: Pre-trained on BigDetection dataset, IN: Pre-trained on ImageNet, OB: Pre-trained on Object-365 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib118" title="" class="ltx_ref">118</a>]</cite>. While <sup id="S4.T3.2.1.1" class="ltx_sup">∗</sup> shows the results for COCO test-dev, the other values are reported for COCO val set.</span></figcaption>
<div id="S4.T3.22" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:759.9pt;height:925.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-11.8pt,14.3pt) scale(0.97,0.97) ;">
<table id="S4.T3.22.20" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.9.7.7" class="ltx_tr">
<td id="S4.T3.9.7.7.8" class="ltx_td ltx_align_left ltx_border_tt">Model</td>
<td id="S4.T3.9.7.7.9" class="ltx_td ltx_align_center ltx_border_tt">Backbone</td>
<td id="S4.T3.4.2.2.2" class="ltx_td ltx_align_center ltx_border_tt">GFLOPS<math id="S4.T3.3.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T3.3.1.1.1.m1.1a"><mo stretchy="false" id="S4.T3.3.1.1.1.m1.1.1" xref="S4.T3.3.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T3.3.1.1.1.m1.1b"><ci id="S4.T3.3.1.1.1.m1.1.1.cmml" xref="S4.T3.3.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.1.1.1.m1.1c">\downarrow</annotation></semantics></math>/FPS <math id="S4.T3.4.2.2.2.m2.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.4.2.2.2.m2.1a"><mo stretchy="false" id="S4.T3.4.2.2.2.m2.1.1" xref="S4.T3.4.2.2.2.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.4.2.2.2.m2.1b"><ci id="S4.T3.4.2.2.2.m2.1.1.cmml" xref="S4.T3.4.2.2.2.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.2.2.2.m2.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T3.6.4.4.4" class="ltx_td ltx_align_center ltx_border_tt">
<math id="S4.T3.5.3.3.3.m1.1" class="ltx_Math" alttext="\#" display="inline"><semantics id="S4.T3.5.3.3.3.m1.1a"><mi mathvariant="normal" id="S4.T3.5.3.3.3.m1.1.1" xref="S4.T3.5.3.3.3.m1.1.1.cmml">#</mi><annotation-xml encoding="MathML-Content" id="S4.T3.5.3.3.3.m1.1b"><ci id="S4.T3.5.3.3.3.m1.1.1.cmml" xref="S4.T3.5.3.3.3.m1.1.1">#</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.3.3.3.m1.1c">\#</annotation></semantics></math>params<math id="S4.T3.6.4.4.4.m2.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T3.6.4.4.4.m2.1a"><mo stretchy="false" id="S4.T3.6.4.4.4.m2.1.1" xref="S4.T3.6.4.4.4.m2.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T3.6.4.4.4.m2.1b"><ci id="S4.T3.6.4.4.4.m2.1.1.cmml" xref="S4.T3.6.4.4.4.m2.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.6.4.4.4.m2.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T3.8.6.6.6" class="ltx_td ltx_align_center ltx_border_tt">
<math id="S4.T3.7.5.5.5.m1.2" class="ltx_Math" alttext="\text{mAP}^{@[0.5,0.95]}" display="inline"><semantics id="S4.T3.7.5.5.5.m1.2a"><msup id="S4.T3.7.5.5.5.m1.2.3" xref="S4.T3.7.5.5.5.m1.2.3.cmml"><mtext id="S4.T3.7.5.5.5.m1.2.3.2" xref="S4.T3.7.5.5.5.m1.2.3.2a.cmml">mAP</mtext><mrow id="S4.T3.7.5.5.5.m1.2.2.2" xref="S4.T3.7.5.5.5.m1.2.2.2.cmml"><mi mathvariant="normal" id="S4.T3.7.5.5.5.m1.2.2.2.4" xref="S4.T3.7.5.5.5.m1.2.2.2.4.cmml">@</mi><mo lspace="0em" rspace="0em" id="S4.T3.7.5.5.5.m1.2.2.2.3" xref="S4.T3.7.5.5.5.m1.2.2.2.3.cmml">​</mo><mrow id="S4.T3.7.5.5.5.m1.2.2.2.5.2" xref="S4.T3.7.5.5.5.m1.2.2.2.5.1.cmml"><mo stretchy="false" id="S4.T3.7.5.5.5.m1.2.2.2.5.2.1" xref="S4.T3.7.5.5.5.m1.2.2.2.5.1.cmml">[</mo><mn id="S4.T3.7.5.5.5.m1.1.1.1.1" xref="S4.T3.7.5.5.5.m1.1.1.1.1.cmml">0.5</mn><mo id="S4.T3.7.5.5.5.m1.2.2.2.5.2.2" xref="S4.T3.7.5.5.5.m1.2.2.2.5.1.cmml">,</mo><mn id="S4.T3.7.5.5.5.m1.2.2.2.2" xref="S4.T3.7.5.5.5.m1.2.2.2.2.cmml">0.95</mn><mo stretchy="false" id="S4.T3.7.5.5.5.m1.2.2.2.5.2.3" xref="S4.T3.7.5.5.5.m1.2.2.2.5.1.cmml">]</mo></mrow></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.T3.7.5.5.5.m1.2b"><apply id="S4.T3.7.5.5.5.m1.2.3.cmml" xref="S4.T3.7.5.5.5.m1.2.3"><csymbol cd="ambiguous" id="S4.T3.7.5.5.5.m1.2.3.1.cmml" xref="S4.T3.7.5.5.5.m1.2.3">superscript</csymbol><ci id="S4.T3.7.5.5.5.m1.2.3.2a.cmml" xref="S4.T3.7.5.5.5.m1.2.3.2"><mtext id="S4.T3.7.5.5.5.m1.2.3.2.cmml" xref="S4.T3.7.5.5.5.m1.2.3.2">mAP</mtext></ci><apply id="S4.T3.7.5.5.5.m1.2.2.2.cmml" xref="S4.T3.7.5.5.5.m1.2.2.2"><times id="S4.T3.7.5.5.5.m1.2.2.2.3.cmml" xref="S4.T3.7.5.5.5.m1.2.2.2.3"></times><ci id="S4.T3.7.5.5.5.m1.2.2.2.4.cmml" xref="S4.T3.7.5.5.5.m1.2.2.2.4">@</ci><interval closure="closed" id="S4.T3.7.5.5.5.m1.2.2.2.5.1.cmml" xref="S4.T3.7.5.5.5.m1.2.2.2.5.2"><cn type="float" id="S4.T3.7.5.5.5.m1.1.1.1.1.cmml" xref="S4.T3.7.5.5.5.m1.1.1.1.1">0.5</cn><cn type="float" id="S4.T3.7.5.5.5.m1.2.2.2.2.cmml" xref="S4.T3.7.5.5.5.m1.2.2.2.2">0.95</cn></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.7.5.5.5.m1.2c">\text{mAP}^{@[0.5,0.95]}</annotation></semantics></math> <math id="S4.T3.8.6.6.6.m2.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.8.6.6.6.m2.1a"><mo stretchy="false" id="S4.T3.8.6.6.6.m2.1.1" xref="S4.T3.8.6.6.6.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.8.6.6.6.m2.1b"><ci id="S4.T3.8.6.6.6.m2.1.1.cmml" xref="S4.T3.8.6.6.6.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.8.6.6.6.m2.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T3.9.7.7.7" class="ltx_td ltx_align_center ltx_border_tt">Epochs<math id="S4.T3.9.7.7.7.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T3.9.7.7.7.m1.1a"><mo stretchy="false" id="S4.T3.9.7.7.7.m1.1.1" xref="S4.T3.9.7.7.7.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T3.9.7.7.7.m1.1b"><ci id="S4.T3.9.7.7.7.m1.1.1.cmml" xref="S4.T3.9.7.7.7.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.9.7.7.7.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T3.9.7.7.10" class="ltx_td ltx_align_center ltx_border_tt">URL</td>
</tr>
<tr id="S4.T3.22.20.21" class="ltx_tr">
<td id="S4.T3.22.20.21.1" class="ltx_td ltx_align_left ltx_border_t">Faster RCNN-DC5 (NeurIPS2015)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>
</td>
<td id="S4.T3.22.20.21.2" class="ltx_td ltx_align_center ltx_border_t">ResNet50</td>
<td id="S4.T3.22.20.21.3" class="ltx_td ltx_align_center ltx_border_t">320/16</td>
<td id="S4.T3.22.20.21.4" class="ltx_td ltx_align_center ltx_border_t">166M</td>
<td id="S4.T3.22.20.21.5" class="ltx_td ltx_align_center ltx_border_t">21.4</td>
<td id="S4.T3.22.20.21.6" class="ltx_td ltx_align_center ltx_border_t">37</td>
<td id="S4.T3.22.20.21.7" class="ltx_td ltx_align_center ltx_border_t"><a target="_blank" href="https://github.com/trzy/FasterRCNN" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T3.22.20.22" class="ltx_tr">
<td id="S4.T3.22.20.22.1" class="ltx_td ltx_align_left">Faster RCNN-FPN (NeurIPS2015)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>
</td>
<td id="S4.T3.22.20.22.2" class="ltx_td ltx_align_center">ResNet50</td>
<td id="S4.T3.22.20.22.3" class="ltx_td ltx_align_center">180/26</td>
<td id="S4.T3.22.20.22.4" class="ltx_td ltx_align_center">42M</td>
<td id="S4.T3.22.20.22.5" class="ltx_td ltx_align_center">24.2</td>
<td id="S4.T3.22.20.22.6" class="ltx_td ltx_align_center">37</td>
<td id="S4.T3.22.20.22.7" class="ltx_td ltx_align_center"><a target="_blank" href="https://github.com/trzy/FasterRCNN" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T3.22.20.23" class="ltx_tr">
<td id="S4.T3.22.20.23.1" class="ltx_td ltx_align_left">Faster RCNN-FPN (NeurIPS2015)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>
</td>
<td id="S4.T3.22.20.23.2" class="ltx_td ltx_align_center">ResNet101</td>
<td id="S4.T3.22.20.23.3" class="ltx_td ltx_align_center">246/20</td>
<td id="S4.T3.22.20.23.4" class="ltx_td ltx_align_center">60M</td>
<td id="S4.T3.22.20.23.5" class="ltx_td ltx_align_center">25.2</td>
<td id="S4.T3.22.20.23.6" class="ltx_td ltx_align_center">–</td>
<td id="S4.T3.22.20.23.7" class="ltx_td ltx_align_center"><a target="_blank" href="https://github.com/trzy/FasterRCNN" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T3.10.8.8" class="ltx_tr">
<td id="S4.T3.10.8.8.2" class="ltx_td ltx_align_left">RepPoints v2-DCN-MS (NeurIPS2020)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib119" title="" class="ltx_ref">119</a>]</cite>
</td>
<td id="S4.T3.10.8.8.3" class="ltx_td ltx_align_center">ResNeXt101</td>
<td id="S4.T3.10.8.8.4" class="ltx_td ltx_align_center">–/–</td>
<td id="S4.T3.10.8.8.5" class="ltx_td ltx_align_center">–</td>
<td id="S4.T3.10.8.8.1" class="ltx_td ltx_align_center">34.5<sup id="S4.T3.10.8.8.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S4.T3.10.8.8.6" class="ltx_td ltx_align_center">24</td>
<td id="S4.T3.10.8.8.7" class="ltx_td ltx_align_center"><a target="_blank" href="https://github.com/Scalsol/RepPointsV2" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T3.22.20.24" class="ltx_tr">
<td id="S4.T3.22.20.24.1" class="ltx_td ltx_align_left">FCOS (ICCV2019)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>
</td>
<td id="S4.T3.22.20.24.2" class="ltx_td ltx_align_center">ResNet50</td>
<td id="S4.T3.22.20.24.3" class="ltx_td ltx_align_center">177/17</td>
<td id="S4.T3.22.20.24.4" class="ltx_td ltx_align_center">–</td>
<td id="S4.T3.22.20.24.5" class="ltx_td ltx_align_center">26.2</td>
<td id="S4.T3.22.20.24.6" class="ltx_td ltx_align_center">36</td>
<td id="S4.T3.22.20.24.7" class="ltx_td ltx_align_center"><a target="_blank" href="https://github.com/tianzhi0549/FCOS" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T3.11.9.9" class="ltx_tr">
<td id="S4.T3.11.9.9.2" class="ltx_td ltx_align_left">CBNet V2-DCN(ATSS<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib120" title="" class="ltx_ref">120</a>]</cite>) (TIP2022)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite>
</td>
<td id="S4.T3.11.9.9.3" class="ltx_td ltx_align_center">Res2Net101</td>
<td id="S4.T3.11.9.9.4" class="ltx_td ltx_align_center">–/–</td>
<td id="S4.T3.11.9.9.5" class="ltx_td ltx_align_center">107M</td>
<td id="S4.T3.11.9.9.1" class="ltx_td ltx_align_center">35.7<sup id="S4.T3.11.9.9.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S4.T3.11.9.9.6" class="ltx_td ltx_align_center">20</td>
<td id="S4.T3.11.9.9.7" class="ltx_td ltx_align_center"><a target="_blank" href="https://github.com/VDIGPKU/CBNetV2" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T3.12.10.10" class="ltx_tr">
<td id="S4.T3.12.10.10.2" class="ltx_td ltx_align_left">CBNet V2-DCN(Cascade RCNN) (TIP2022)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite>
</td>
<td id="S4.T3.12.10.10.3" class="ltx_td ltx_align_center">Res2Net101</td>
<td id="S4.T3.12.10.10.4" class="ltx_td ltx_align_center">–/–</td>
<td id="S4.T3.12.10.10.5" class="ltx_td ltx_align_center">146M</td>
<td id="S4.T3.12.10.10.1" class="ltx_td ltx_align_center">37.4<sup id="S4.T3.12.10.10.1.1" class="ltx_sup"><span id="S4.T3.12.10.10.1.1.1" class="ltx_text ltx_font_italic">∗</span></sup>
</td>
<td id="S4.T3.12.10.10.6" class="ltx_td ltx_align_center">32</td>
<td id="S4.T3.12.10.10.7" class="ltx_td ltx_align_center"><a target="_blank" href="https://github.com/VDIGPKU/CBNetV2" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T3.22.20.25" class="ltx_tr">
<td id="S4.T3.22.20.25.1" class="ltx_td ltx_align_left ltx_border_t">DETR (ECCV2020)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>
</td>
<td id="S4.T3.22.20.25.2" class="ltx_td ltx_align_center ltx_border_t">ResNet50</td>
<td id="S4.T3.22.20.25.3" class="ltx_td ltx_align_center ltx_border_t">86/<span id="S4.T3.22.20.25.3.1" class="ltx_text ltx_font_bold">28</span>
</td>
<td id="S4.T3.22.20.25.4" class="ltx_td ltx_align_center ltx_border_t">41M</td>
<td id="S4.T3.22.20.25.5" class="ltx_td ltx_align_center ltx_border_t">20.5</td>
<td id="S4.T3.22.20.25.6" class="ltx_td ltx_align_center ltx_border_t">500</td>
<td id="S4.T3.22.20.25.7" class="ltx_td ltx_align_center ltx_border_t"><a target="_blank" href="https://github.com/facebookresearch/detr" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T3.22.20.26" class="ltx_tr">
<td id="S4.T3.22.20.26.1" class="ltx_td ltx_align_left">DETR-DC5 (ECCV2020)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>
</td>
<td id="S4.T3.22.20.26.2" class="ltx_td ltx_align_center">ResNet50</td>
<td id="S4.T3.22.20.26.3" class="ltx_td ltx_align_center">187/12</td>
<td id="S4.T3.22.20.26.4" class="ltx_td ltx_align_center">41M</td>
<td id="S4.T3.22.20.26.5" class="ltx_td ltx_align_center">22.5</td>
<td id="S4.T3.22.20.26.6" class="ltx_td ltx_align_center">500</td>
<td id="S4.T3.22.20.26.7" class="ltx_td ltx_align_center"><a target="_blank" href="https://github.com/facebookresearch/detr" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T3.22.20.27" class="ltx_tr">
<td id="S4.T3.22.20.27.1" class="ltx_td ltx_align_left">DETR (ECCV2020)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>
</td>
<td id="S4.T3.22.20.27.2" class="ltx_td ltx_align_center">ResNet101</td>
<td id="S4.T3.22.20.27.3" class="ltx_td ltx_align_center">
<span id="S4.T3.22.20.27.3.1" class="ltx_text ltx_font_bold">52</span>/20</td>
<td id="S4.T3.22.20.27.4" class="ltx_td ltx_align_center">60M</td>
<td id="S4.T3.22.20.27.5" class="ltx_td ltx_align_center">21.9</td>
<td id="S4.T3.22.20.27.6" class="ltx_td ltx_align_center">–</td>
<td id="S4.T3.22.20.27.7" class="ltx_td ltx_align_center"><a target="_blank" href="https://github.com/facebookresearch/detr" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T3.22.20.28" class="ltx_tr">
<td id="S4.T3.22.20.28.1" class="ltx_td ltx_align_left">DETR-DC5 (ECCV2020)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>
</td>
<td id="S4.T3.22.20.28.2" class="ltx_td ltx_align_center">ResNet101</td>
<td id="S4.T3.22.20.28.3" class="ltx_td ltx_align_center">253/10</td>
<td id="S4.T3.22.20.28.4" class="ltx_td ltx_align_center">60M</td>
<td id="S4.T3.22.20.28.5" class="ltx_td ltx_align_center">23.7</td>
<td id="S4.T3.22.20.28.6" class="ltx_td ltx_align_center">–</td>
<td id="S4.T3.22.20.28.7" class="ltx_td ltx_align_center"><a target="_blank" href="https://github.com/facebookresearch/detr" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T3.22.20.29" class="ltx_tr">
<td id="S4.T3.22.20.29.1" class="ltx_td ltx_align_left">ViT-FRCNN (arXiv2020)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>
</td>
<td id="S4.T3.22.20.29.2" class="ltx_td ltx_align_center">–</td>
<td id="S4.T3.22.20.29.3" class="ltx_td ltx_align_center">–/–</td>
<td id="S4.T3.22.20.29.4" class="ltx_td ltx_align_center">–</td>
<td id="S4.T3.22.20.29.5" class="ltx_td ltx_align_center">17.8</td>
<td id="S4.T3.22.20.29.6" class="ltx_td ltx_align_center">–</td>
<td id="S4.T3.22.20.29.7" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S4.T3.13.11.11" class="ltx_tr">
<td id="S4.T3.13.11.11.2" class="ltx_td ltx_align_left">RelationNet++ (NeurIPS2020)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>
</td>
<td id="S4.T3.13.11.11.3" class="ltx_td ltx_align_center">ResNeXt101</td>
<td id="S4.T3.13.11.11.4" class="ltx_td ltx_align_center">–/–</td>
<td id="S4.T3.13.11.11.5" class="ltx_td ltx_align_center">–</td>
<td id="S4.T3.13.11.11.1" class="ltx_td ltx_align_center">32.8<sup id="S4.T3.13.11.11.1.1" class="ltx_sup"><span id="S4.T3.13.11.11.1.1.1" class="ltx_text ltx_font_italic">∗</span></sup>
</td>
<td id="S4.T3.13.11.11.6" class="ltx_td ltx_align_center">–</td>
<td id="S4.T3.13.11.11.7" class="ltx_td ltx_align_center"><a target="_blank" href="https://github.com/microsoft/RelationNet2" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T3.14.12.12" class="ltx_tr">
<td id="S4.T3.14.12.12.2" class="ltx_td ltx_align_left">RelationNet++-MS (NeurIPS2020)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>
</td>
<td id="S4.T3.14.12.12.3" class="ltx_td ltx_align_center">ResNeXt101</td>
<td id="S4.T3.14.12.12.4" class="ltx_td ltx_align_center">–/–</td>
<td id="S4.T3.14.12.12.5" class="ltx_td ltx_align_center">–</td>
<td id="S4.T3.14.12.12.1" class="ltx_td ltx_align_center">35.8<sup id="S4.T3.14.12.12.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S4.T3.14.12.12.6" class="ltx_td ltx_align_center">–</td>
<td id="S4.T3.14.12.12.7" class="ltx_td ltx_align_center"><a target="_blank" href="https://github.com/microsoft/RelationNet2" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T3.22.20.30" class="ltx_tr">
<td id="S4.T3.22.20.30.1" class="ltx_td ltx_align_left">Deformable DETR (ICLR2021)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>
</td>
<td id="S4.T3.22.20.30.2" class="ltx_td ltx_align_center">ResNet50</td>
<td id="S4.T3.22.20.30.3" class="ltx_td ltx_align_center">173/19</td>
<td id="S4.T3.22.20.30.4" class="ltx_td ltx_align_center">40M</td>
<td id="S4.T3.22.20.30.5" class="ltx_td ltx_align_center">26.4</td>
<td id="S4.T3.22.20.30.6" class="ltx_td ltx_align_center">50</td>
<td id="S4.T3.22.20.30.7" class="ltx_td ltx_align_center"><a target="_blank" href="https://github.com/fundamentalvision/Deformable-DETR" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T3.22.20.31" class="ltx_tr">
<td id="S4.T3.22.20.31.1" class="ltx_td ltx_align_left">Deformable DETR-IBR (ICLR2021)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>
</td>
<td id="S4.T3.22.20.31.2" class="ltx_td ltx_align_center">ResNet50</td>
<td id="S4.T3.22.20.31.3" class="ltx_td ltx_align_center">173/19</td>
<td id="S4.T3.22.20.31.4" class="ltx_td ltx_align_center">40M</td>
<td id="S4.T3.22.20.31.5" class="ltx_td ltx_align_center">26.8</td>
<td id="S4.T3.22.20.31.6" class="ltx_td ltx_align_center">50</td>
<td id="S4.T3.22.20.31.7" class="ltx_td ltx_align_center"><a target="_blank" href="https://github.com/fundamentalvision/Deformable-DETR" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T3.22.20.32" class="ltx_tr">
<td id="S4.T3.22.20.32.1" class="ltx_td ltx_align_left">Deformable DETR-TS (ICLR2021)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>
</td>
<td id="S4.T3.22.20.32.2" class="ltx_td ltx_align_center">ResNet50</td>
<td id="S4.T3.22.20.32.3" class="ltx_td ltx_align_center">173/19</td>
<td id="S4.T3.22.20.32.4" class="ltx_td ltx_align_center">40M</td>
<td id="S4.T3.22.20.32.5" class="ltx_td ltx_align_center">28.8</td>
<td id="S4.T3.22.20.32.6" class="ltx_td ltx_align_center">50</td>
<td id="S4.T3.22.20.32.7" class="ltx_td ltx_align_center"><a target="_blank" href="https://github.com/fundamentalvision/Deformable-DETR" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T3.15.13.13" class="ltx_tr">
<td id="S4.T3.15.13.13.2" class="ltx_td ltx_align_left">Deformable DETR-TS-IBR-DCN (ICLR2021)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>
</td>
<td id="S4.T3.15.13.13.3" class="ltx_td ltx_align_center">ResNeXt101</td>
<td id="S4.T3.15.13.13.4" class="ltx_td ltx_align_center">–/–</td>
<td id="S4.T3.15.13.13.5" class="ltx_td ltx_align_center">–</td>
<td id="S4.T3.15.13.13.1" class="ltx_td ltx_align_center">34.4<sup id="S4.T3.15.13.13.1.1" class="ltx_sup"><span id="S4.T3.15.13.13.1.1.1" class="ltx_text ltx_font_italic">∗</span></sup>
</td>
<td id="S4.T3.15.13.13.6" class="ltx_td ltx_align_center">–</td>
<td id="S4.T3.15.13.13.7" class="ltx_td ltx_align_center"><a target="_blank" href="https://github.com/fundamentalvision/Deformable-DETR" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T3.16.14.14" class="ltx_tr">
<td id="S4.T3.16.14.14.2" class="ltx_td ltx_align_left">Dynamic DETR (ICCV2021)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>
</td>
<td id="S4.T3.16.14.14.3" class="ltx_td ltx_align_center">ResNet50</td>
<td id="S4.T3.16.14.14.4" class="ltx_td ltx_align_center">–/–</td>
<td id="S4.T3.16.14.14.5" class="ltx_td ltx_align_center">–</td>
<td id="S4.T3.16.14.14.1" class="ltx_td ltx_align_center">28.6<sup id="S4.T3.16.14.14.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S4.T3.16.14.14.6" class="ltx_td ltx_align_center">–</td>
<td id="S4.T3.16.14.14.7" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S4.T3.17.15.15" class="ltx_tr">
<td id="S4.T3.17.15.15.2" class="ltx_td ltx_align_left">Dynamic DETR-DCN (ICCV2021)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>
</td>
<td id="S4.T3.17.15.15.3" class="ltx_td ltx_align_center">ResNeXt101</td>
<td id="S4.T3.17.15.15.4" class="ltx_td ltx_align_center">–/–</td>
<td id="S4.T3.17.15.15.5" class="ltx_td ltx_align_center">–</td>
<td id="S4.T3.17.15.15.1" class="ltx_td ltx_align_center">30.3<sup id="S4.T3.17.15.15.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S4.T3.17.15.15.6" class="ltx_td ltx_align_center">–</td>
<td id="S4.T3.17.15.15.7" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S4.T3.22.20.33" class="ltx_tr">
<td id="S4.T3.22.20.33.1" class="ltx_td ltx_align_left">TSP-FCOS (ICCV2021)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>
</td>
<td id="S4.T3.22.20.33.2" class="ltx_td ltx_align_center">ResNet101</td>
<td id="S4.T3.22.20.33.3" class="ltx_td ltx_align_center">255/12</td>
<td id="S4.T3.22.20.33.4" class="ltx_td ltx_align_center">–</td>
<td id="S4.T3.22.20.33.5" class="ltx_td ltx_align_center">27.7</td>
<td id="S4.T3.22.20.33.6" class="ltx_td ltx_align_center">36</td>
<td id="S4.T3.22.20.33.7" class="ltx_td ltx_align_center"><a target="_blank" href="https://github.com/Edward-Sun/TSP-Detection" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T3.22.20.34" class="ltx_tr">
<td id="S4.T3.22.20.34.1" class="ltx_td ltx_align_left">TSP-RCNN (ICCV2021)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>
</td>
<td id="S4.T3.22.20.34.2" class="ltx_td ltx_align_center">ResNet101</td>
<td id="S4.T3.22.20.34.3" class="ltx_td ltx_align_center">254/9</td>
<td id="S4.T3.22.20.34.4" class="ltx_td ltx_align_center">–</td>
<td id="S4.T3.22.20.34.5" class="ltx_td ltx_align_center">29.9</td>
<td id="S4.T3.22.20.34.6" class="ltx_td ltx_align_center">96</td>
<td id="S4.T3.22.20.34.7" class="ltx_td ltx_align_center"><a target="_blank" href="https://github.com/Edward-Sun/TSP-Detection" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T3.22.20.35" class="ltx_tr">
<td id="S4.T3.22.20.35.1" class="ltx_td ltx_align_left">Mask R-CNN (ICCV2021)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>
</td>
<td id="S4.T3.22.20.35.2" class="ltx_td ltx_align_center">Conformer-S/16</td>
<td id="S4.T3.22.20.35.3" class="ltx_td ltx_align_center">457.7/–</td>
<td id="S4.T3.22.20.35.4" class="ltx_td ltx_align_center">56.9M</td>
<td id="S4.T3.22.20.35.5" class="ltx_td ltx_align_center">28.7</td>
<td id="S4.T3.22.20.35.6" class="ltx_td ltx_align_center"><span id="S4.T3.22.20.35.6.1" class="ltx_text ltx_font_bold">12</span></td>
<td id="S4.T3.22.20.35.7" class="ltx_td ltx_align_center"><a target="_blank" href="https://github.com/pengzhiliang/Conformer" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T3.22.20.36" class="ltx_tr">
<td id="S4.T3.22.20.36.1" class="ltx_td ltx_align_left">Conditional DETR-DC5 (ICCV2021)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite>
</td>
<td id="S4.T3.22.20.36.2" class="ltx_td ltx_align_center">ResNet101</td>
<td id="S4.T3.22.20.36.3" class="ltx_td ltx_align_center">262/–</td>
<td id="S4.T3.22.20.36.4" class="ltx_td ltx_align_center">63M</td>
<td id="S4.T3.22.20.36.5" class="ltx_td ltx_align_center">27.2</td>
<td id="S4.T3.22.20.36.6" class="ltx_td ltx_align_center">108</td>
<td id="S4.T3.22.20.36.7" class="ltx_td ltx_align_center"><a target="_blank" href="https://github.com/Atten4Vis/ConditionalDETR" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T3.22.20.37" class="ltx_tr">
<td id="S4.T3.22.20.37.1" class="ltx_td ltx_align_left">SOF-DETR (2022JVCIR) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite>
</td>
<td id="S4.T3.22.20.37.2" class="ltx_td ltx_align_center">ResNet50</td>
<td id="S4.T3.22.20.37.3" class="ltx_td ltx_align_center">–/–</td>
<td id="S4.T3.22.20.37.4" class="ltx_td ltx_align_center">–</td>
<td id="S4.T3.22.20.37.5" class="ltx_td ltx_align_center">21.7</td>
<td id="S4.T3.22.20.37.6" class="ltx_td ltx_align_center">–</td>
<td id="S4.T3.22.20.37.7" class="ltx_td ltx_align_center"><a target="_blank" href="https://github.com/shikha-gist/SOF-DETR/" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T3.22.20.38" class="ltx_tr">
<td id="S4.T3.22.20.38.1" class="ltx_td ltx_align_left">DETR++ (arXiv2022)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib91" title="" class="ltx_ref">91</a>]</cite>
</td>
<td id="S4.T3.22.20.38.2" class="ltx_td ltx_align_center">ResNet50</td>
<td id="S4.T3.22.20.38.3" class="ltx_td ltx_align_center">–/–</td>
<td id="S4.T3.22.20.38.4" class="ltx_td ltx_align_center">–</td>
<td id="S4.T3.22.20.38.5" class="ltx_td ltx_align_center">22.1</td>
<td id="S4.T3.22.20.38.6" class="ltx_td ltx_align_center">–</td>
<td id="S4.T3.22.20.38.7" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S4.T3.22.20.39" class="ltx_tr">
<td id="S4.T3.22.20.39.1" class="ltx_td ltx_align_left">TOLO-MS (NCA2022) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>
</td>
<td id="S4.T3.22.20.39.2" class="ltx_td ltx_align_center">–</td>
<td id="S4.T3.22.20.39.3" class="ltx_td ltx_align_center">–/57</td>
<td id="S4.T3.22.20.39.4" class="ltx_td ltx_align_center">–</td>
<td id="S4.T3.22.20.39.5" class="ltx_td ltx_align_center">24.1</td>
<td id="S4.T3.22.20.39.6" class="ltx_td ltx_align_center">–</td>
<td id="S4.T3.22.20.39.7" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S4.T3.22.20.40" class="ltx_tr">
<td id="S4.T3.22.20.40.1" class="ltx_td ltx_align_left">Anchor DETR-DC5 (AAAI2022) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>
</td>
<td id="S4.T3.22.20.40.2" class="ltx_td ltx_align_center">ResNet101</td>
<td id="S4.T3.22.20.40.3" class="ltx_td ltx_align_center">–/–</td>
<td id="S4.T3.22.20.40.4" class="ltx_td ltx_align_center">–</td>
<td id="S4.T3.22.20.40.5" class="ltx_td ltx_align_center">25.8</td>
<td id="S4.T3.22.20.40.6" class="ltx_td ltx_align_center">50</td>
<td id="S4.T3.22.20.40.7" class="ltx_td ltx_align_center"><a target="_blank" href="https://github.com/megvii-research/AnchorDETR" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T3.22.20.41" class="ltx_tr">
<td id="S4.T3.22.20.41.1" class="ltx_td ltx_align_left">DESTR-DC5 (CVPR2022)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>
</td>
<td id="S4.T3.22.20.41.2" class="ltx_td ltx_align_center">ResNet101</td>
<td id="S4.T3.22.20.41.3" class="ltx_td ltx_align_center">299/–</td>
<td id="S4.T3.22.20.41.4" class="ltx_td ltx_align_center">88M</td>
<td id="S4.T3.22.20.41.5" class="ltx_td ltx_align_center">28.2</td>
<td id="S4.T3.22.20.41.6" class="ltx_td ltx_align_center">50</td>
<td id="S4.T3.22.20.41.7" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S4.T3.22.20.42" class="ltx_tr">
<td id="S4.T3.22.20.42.1" class="ltx_td ltx_align_left">Conditional DETR v2-DC5 (arXiv2022)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>
</td>
<td id="S4.T3.22.20.42.2" class="ltx_td ltx_align_center">ResNet101</td>
<td id="S4.T3.22.20.42.3" class="ltx_td ltx_align_center">228/–</td>
<td id="S4.T3.22.20.42.4" class="ltx_td ltx_align_center">65M</td>
<td id="S4.T3.22.20.42.5" class="ltx_td ltx_align_center">26.3</td>
<td id="S4.T3.22.20.42.6" class="ltx_td ltx_align_center">50</td>
<td id="S4.T3.22.20.42.7" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S4.T3.22.20.43" class="ltx_tr">
<td id="S4.T3.22.20.43.1" class="ltx_td ltx_align_left">Conditional DETR v2 (arXiv2022)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>
</td>
<td id="S4.T3.22.20.43.2" class="ltx_td ltx_align_center">Hourglass48</td>
<td id="S4.T3.22.20.43.3" class="ltx_td ltx_align_center">521/–</td>
<td id="S4.T3.22.20.43.4" class="ltx_td ltx_align_center">90M</td>
<td id="S4.T3.22.20.43.5" class="ltx_td ltx_align_center">32.1</td>
<td id="S4.T3.22.20.43.6" class="ltx_td ltx_align_center">50</td>
<td id="S4.T3.22.20.43.7" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S4.T3.22.20.44" class="ltx_tr">
<td id="S4.T3.22.20.44.1" class="ltx_td ltx_align_left">FP-DETR-IN (ICLR2022) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite>
</td>
<td id="S4.T3.22.20.44.2" class="ltx_td ltx_align_center">–</td>
<td id="S4.T3.22.20.44.3" class="ltx_td ltx_align_center">–/–</td>
<td id="S4.T3.22.20.44.4" class="ltx_td ltx_align_center"><span id="S4.T3.22.20.44.4.1" class="ltx_text ltx_font_bold">36M</span></td>
<td id="S4.T3.22.20.44.5" class="ltx_td ltx_align_center">26.5</td>
<td id="S4.T3.22.20.44.6" class="ltx_td ltx_align_center">50</td>
<td id="S4.T3.22.20.44.7" class="ltx_td ltx_align_center"><a target="_blank" href="https://github.com/encounter1997/FP-DETR" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T3.22.20.45" class="ltx_tr">
<td id="S4.T3.22.20.45.1" class="ltx_td ltx_align_left">DAB-DETR-DC5 (arXiv2022)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>
</td>
<td id="S4.T3.22.20.45.2" class="ltx_td ltx_align_center">ResNet101</td>
<td id="S4.T3.22.20.45.3" class="ltx_td ltx_align_center">296/–</td>
<td id="S4.T3.22.20.45.4" class="ltx_td ltx_align_center">63M</td>
<td id="S4.T3.22.20.45.5" class="ltx_td ltx_align_center">28.1</td>
<td id="S4.T3.22.20.45.6" class="ltx_td ltx_align_center">50</td>
<td id="S4.T3.22.20.45.7" class="ltx_td ltx_align_center"><a target="_blank" href="https://github.com/IDEA-Research/DAB-DETR" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T3.22.20.46" class="ltx_tr">
<td id="S4.T3.22.20.46.1" class="ltx_td ltx_align_left">Ghostformer-MS (Sensors2022)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite>
</td>
<td id="S4.T3.22.20.46.2" class="ltx_td ltx_align_center">GhostNet</td>
<td id="S4.T3.22.20.46.3" class="ltx_td ltx_align_center">–/–</td>
<td id="S4.T3.22.20.46.4" class="ltx_td ltx_align_center">–</td>
<td id="S4.T3.22.20.46.5" class="ltx_td ltx_align_center">29.2</td>
<td id="S4.T3.22.20.46.6" class="ltx_td ltx_align_center">100</td>
<td id="S4.T3.22.20.46.7" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S4.T3.18.16.16" class="ltx_tr">
<td id="S4.T3.18.16.16.2" class="ltx_td ltx_align_left">CF-DETR-DCN-TTA (AAAI2022)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>
</td>
<td id="S4.T3.18.16.16.3" class="ltx_td ltx_align_center">ResNeXt101</td>
<td id="S4.T3.18.16.16.4" class="ltx_td ltx_align_center">–/–</td>
<td id="S4.T3.18.16.16.5" class="ltx_td ltx_align_center">–</td>
<td id="S4.T3.18.16.16.1" class="ltx_td ltx_align_center">35.1<sup id="S4.T3.18.16.16.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S4.T3.18.16.16.6" class="ltx_td ltx_align_center">–</td>
<td id="S4.T3.18.16.16.7" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S4.T3.22.20.47" class="ltx_tr">
<td id="S4.T3.22.20.47.1" class="ltx_td ltx_align_left">CBNet V2-TTA (CVPR2022)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite>
</td>
<td id="S4.T3.22.20.47.2" class="ltx_td ltx_align_center">Swin Transformer-base</td>
<td id="S4.T3.22.20.47.3" class="ltx_td ltx_align_center">–/–</td>
<td id="S4.T3.22.20.47.4" class="ltx_td ltx_align_center">–</td>
<td id="S4.T3.22.20.47.5" class="ltx_td ltx_align_center">41.7</td>
<td id="S4.T3.22.20.47.6" class="ltx_td ltx_align_center">–</td>
<td id="S4.T3.22.20.47.7" class="ltx_td ltx_align_center"><a target="_blank" href="https://github.com/amazon-science/bigdetection" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T3.22.20.48" class="ltx_tr">
<td id="S4.T3.22.20.48.1" class="ltx_td ltx_align_left">CBNet V2-TTA-BD (CVPR2022)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite>
</td>
<td id="S4.T3.22.20.48.2" class="ltx_td ltx_align_center">Swin Transformer-base</td>
<td id="S4.T3.22.20.48.3" class="ltx_td ltx_align_center">–/–</td>
<td id="S4.T3.22.20.48.4" class="ltx_td ltx_align_center">–</td>
<td id="S4.T3.22.20.48.5" class="ltx_td ltx_align_center">42.2</td>
<td id="S4.T3.22.20.48.6" class="ltx_td ltx_align_center">–</td>
<td id="S4.T3.22.20.48.7" class="ltx_td ltx_align_center"><a target="_blank" href="https://github.com/amazon-science/bigdetection" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T3.22.20.49" class="ltx_tr">
<td id="S4.T3.22.20.49.1" class="ltx_td ltx_align_left">DETA (arXiv2022)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite>
</td>
<td id="S4.T3.22.20.49.2" class="ltx_td ltx_align_center">ResNet50</td>
<td id="S4.T3.22.20.49.3" class="ltx_td ltx_align_center">–/13</td>
<td id="S4.T3.22.20.49.4" class="ltx_td ltx_align_center">48M</td>
<td id="S4.T3.22.20.49.5" class="ltx_td ltx_align_center">34.3</td>
<td id="S4.T3.22.20.49.6" class="ltx_td ltx_align_center">24</td>
<td id="S4.T3.22.20.49.7" class="ltx_td ltx_align_center"><a target="_blank" href="https://github.com/jozhang97/DETA" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T3.22.20.50" class="ltx_tr">
<td id="S4.T3.22.20.50.1" class="ltx_td ltx_align_left">DINO (arXiv2022)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite>
</td>
<td id="S4.T3.22.20.50.2" class="ltx_td ltx_align_center">ResNet50</td>
<td id="S4.T3.22.20.50.3" class="ltx_td ltx_align_center">860/10</td>
<td id="S4.T3.22.20.50.4" class="ltx_td ltx_align_center">47M</td>
<td id="S4.T3.22.20.50.5" class="ltx_td ltx_align_center">32.3</td>
<td id="S4.T3.22.20.50.6" class="ltx_td ltx_align_center"><span id="S4.T3.22.20.50.6.1" class="ltx_text ltx_font_bold">12</span></td>
<td id="S4.T3.22.20.50.7" class="ltx_td ltx_align_center"><a target="_blank" href="https://github.com/IDEA-Research/DINO" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T3.22.20.51" class="ltx_tr">
<td id="S4.T3.22.20.51.1" class="ltx_td ltx_align_left">CO-DINO Deformable DETR-MS-IN (arXiv2022)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite>
</td>
<td id="S4.T3.22.20.51.2" class="ltx_td ltx_align_center">Swin Transformer-large</td>
<td id="S4.T3.22.20.51.3" class="ltx_td ltx_align_center">–/–</td>
<td id="S4.T3.22.20.51.4" class="ltx_td ltx_align_center">–</td>
<td id="S4.T3.22.20.51.5" class="ltx_td ltx_align_center">43.7</td>
<td id="S4.T3.22.20.51.6" class="ltx_td ltx_align_center">36</td>
<td id="S4.T3.22.20.51.7" class="ltx_td ltx_align_center"><a target="_blank" href="https://github.com/Sense-X/Co-DETR" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T3.19.17.17" class="ltx_tr">
<td id="S4.T3.19.17.17.2" class="ltx_td ltx_align_left">HYNETER (ICASSP2023)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>
</td>
<td id="S4.T3.19.17.17.3" class="ltx_td ltx_align_center">Hyneter-Max</td>
<td id="S4.T3.19.17.17.4" class="ltx_td ltx_align_center">–/–</td>
<td id="S4.T3.19.17.17.5" class="ltx_td ltx_align_center">247M</td>
<td id="S4.T3.19.17.17.1" class="ltx_td ltx_align_center">29.8<sup id="S4.T3.19.17.17.1.1" class="ltx_sup"><span id="S4.T3.19.17.17.1.1.1" class="ltx_text ltx_font_italic">∗</span></sup>
</td>
<td id="S4.T3.19.17.17.6" class="ltx_td ltx_align_center">–</td>
<td id="S4.T3.19.17.17.7" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S4.T3.22.20.52" class="ltx_tr">
<td id="S4.T3.22.20.52.1" class="ltx_td ltx_align_left">DeoT (JRTIP2023) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite>
</td>
<td id="S4.T3.22.20.52.2" class="ltx_td ltx_align_center">ResNet101</td>
<td id="S4.T3.22.20.52.3" class="ltx_td ltx_align_center">217/14</td>
<td id="S4.T3.22.20.52.4" class="ltx_td ltx_align_center">58M</td>
<td id="S4.T3.22.20.52.5" class="ltx_td ltx_align_center">31.4</td>
<td id="S4.T3.22.20.52.6" class="ltx_td ltx_align_center">34</td>
<td id="S4.T3.22.20.52.7" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S4.T3.22.20.53" class="ltx_tr">
<td id="S4.T3.22.20.53.1" class="ltx_td ltx_align_left">ConformerDet-MS (TPAMI2023) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>
</td>
<td id="S4.T3.22.20.53.2" class="ltx_td ltx_align_center">Conformer-B</td>
<td id="S4.T3.22.20.53.3" class="ltx_td ltx_align_center">–/–</td>
<td id="S4.T3.22.20.53.4" class="ltx_td ltx_align_center">147M</td>
<td id="S4.T3.22.20.53.5" class="ltx_td ltx_align_center">35.3</td>
<td id="S4.T3.22.20.53.6" class="ltx_td ltx_align_center">36</td>
<td id="S4.T3.22.20.53.7" class="ltx_td ltx_align_center"><a target="_blank" href="https://github.com/pengzhiliang/Conformer" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T3.22.20.54" class="ltx_tr">
<td id="S4.T3.22.20.54.1" class="ltx_td ltx_align_left ltx_border_t">YOLOS (NeurIPS2021)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</td>
<td id="S4.T3.22.20.54.2" class="ltx_td ltx_align_center ltx_border_t">DeiT-base</td>
<td id="S4.T3.22.20.54.3" class="ltx_td ltx_align_center ltx_border_t">–/3.9</td>
<td id="S4.T3.22.20.54.4" class="ltx_td ltx_align_center ltx_border_t">100M</td>
<td id="S4.T3.22.20.54.5" class="ltx_td ltx_align_center ltx_border_t">19.5</td>
<td id="S4.T3.22.20.54.6" class="ltx_td ltx_align_center ltx_border_t">150</td>
<td id="S4.T3.22.20.54.7" class="ltx_td ltx_align_center ltx_border_t"><a target="_blank" href="https://github.com/hustvl/YOLOS" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T3.22.20.55" class="ltx_tr">
<td id="S4.T3.22.20.55.1" class="ltx_td ltx_align_left">DETR(ViT) (arXiv2021)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>
</td>
<td id="S4.T3.22.20.55.2" class="ltx_td ltx_align_center">Swin Transformer-base</td>
<td id="S4.T3.22.20.55.3" class="ltx_td ltx_align_center">–/9.7</td>
<td id="S4.T3.22.20.55.4" class="ltx_td ltx_align_center">100M</td>
<td id="S4.T3.22.20.55.5" class="ltx_td ltx_align_center">18.3</td>
<td id="S4.T3.22.20.55.6" class="ltx_td ltx_align_center">50</td>
<td id="S4.T3.22.20.55.7" class="ltx_td ltx_align_center"><a target="_blank" href="https://github.com/naver-ai/vidt" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T3.22.20.56" class="ltx_tr">
<td id="S4.T3.22.20.56.1" class="ltx_td ltx_align_left">Deformable DETR(ViT) (arXiv2021)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>
</td>
<td id="S4.T3.22.20.56.2" class="ltx_td ltx_align_center">Swin Transformer-base</td>
<td id="S4.T3.22.20.56.3" class="ltx_td ltx_align_center">–/4.8</td>
<td id="S4.T3.22.20.56.4" class="ltx_td ltx_align_center">100M</td>
<td id="S4.T3.22.20.56.5" class="ltx_td ltx_align_center">34.5</td>
<td id="S4.T3.22.20.56.6" class="ltx_td ltx_align_center">50</td>
<td id="S4.T3.22.20.56.7" class="ltx_td ltx_align_center"><a target="_blank" href="https://github.com/naver-ai/vidt" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T3.22.20.57" class="ltx_tr">
<td id="S4.T3.22.20.57.1" class="ltx_td ltx_align_left">ViDT (arXiv2022)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>
</td>
<td id="S4.T3.22.20.57.2" class="ltx_td ltx_align_center">Swin Transformer-base</td>
<td id="S4.T3.22.20.57.3" class="ltx_td ltx_align_center">–/9</td>
<td id="S4.T3.22.20.57.4" class="ltx_td ltx_align_center">100M</td>
<td id="S4.T3.22.20.57.5" class="ltx_td ltx_align_center">30.6</td>
<td id="S4.T3.22.20.57.6" class="ltx_td ltx_align_center">50</td>
<td id="S4.T3.22.20.57.7" class="ltx_td ltx_align_center"><a target="_blank" href="https://github.com/naver-ai/vidt/tree/main" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T3.22.20.58" class="ltx_tr">
<td id="S4.T3.22.20.58.1" class="ltx_td ltx_align_left">DFFT (ECCV2022) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>
</td>
<td id="S4.T3.22.20.58.2" class="ltx_td ltx_align_center">DOT-medium</td>
<td id="S4.T3.22.20.58.3" class="ltx_td ltx_align_center">67/–</td>
<td id="S4.T3.22.20.58.4" class="ltx_td ltx_align_center">–</td>
<td id="S4.T3.22.20.58.5" class="ltx_td ltx_align_center">25.5</td>
<td id="S4.T3.22.20.58.6" class="ltx_td ltx_align_center">36</td>
<td id="S4.T3.22.20.58.7" class="ltx_td ltx_align_center"><a target="_blank" href="https://github.com/PeixianChen/DFFT" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T3.20.18.18" class="ltx_tr">
<td id="S4.T3.20.18.18.2" class="ltx_td ltx_align_left">CenterNet++-MS (arXiv2022) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>
</td>
<td id="S4.T3.20.18.18.3" class="ltx_td ltx_align_center">Swin Transformer-large</td>
<td id="S4.T3.20.18.18.4" class="ltx_td ltx_align_center">–/–</td>
<td id="S4.T3.20.18.18.5" class="ltx_td ltx_align_center">–</td>
<td id="S4.T3.20.18.18.1" class="ltx_td ltx_align_center">38.7<sup id="S4.T3.20.18.18.1.1" class="ltx_sup"><span id="S4.T3.20.18.18.1.1.1" class="ltx_text ltx_font_italic">∗</span></sup>
</td>
<td id="S4.T3.20.18.18.6" class="ltx_td ltx_align_center">–</td>
<td id="S4.T3.20.18.18.7" class="ltx_td ltx_align_center"><a target="_blank" href="https://github.com/Duankaiwen/PyCenterNet" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T3.21.19.19" class="ltx_tr">
<td id="S4.T3.21.19.19.2" class="ltx_td ltx_align_left">DETA-OB (arXiv2022)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite>
</td>
<td id="S4.T3.21.19.19.3" class="ltx_td ltx_align_center">Swin Transformer-large</td>
<td id="S4.T3.21.19.19.4" class="ltx_td ltx_align_center">–/4.2</td>
<td id="S4.T3.21.19.19.5" class="ltx_td ltx_align_center">–</td>
<td id="S4.T3.21.19.19.1" class="ltx_td ltx_align_center">46.1<sup id="S4.T3.21.19.19.1.1" class="ltx_sup"><span id="S4.T3.21.19.19.1.1.1" class="ltx_text ltx_font_italic">∗</span></sup>
</td>
<td id="S4.T3.21.19.19.6" class="ltx_td ltx_align_center">24</td>
<td id="S4.T3.21.19.19.7" class="ltx_td ltx_align_center"><a target="_blank" href="https://github.com/jozhang97/DETA" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T3.22.20.20" class="ltx_tr">
<td id="S4.T3.22.20.20.2" class="ltx_td ltx_align_left">Group DETR v2-MS-IN-OB (arXiv2022) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>
</td>
<td id="S4.T3.22.20.20.3" class="ltx_td ltx_align_center">ViT-Huge</td>
<td id="S4.T3.22.20.20.4" class="ltx_td ltx_align_center">–/–</td>
<td id="S4.T3.22.20.20.5" class="ltx_td ltx_align_center">629M</td>
<td id="S4.T3.22.20.20.1" class="ltx_td ltx_align_center"><span id="S4.T3.22.20.20.1.1" class="ltx_text ltx_font_bold">48.4<sup id="S4.T3.22.20.20.1.1.1" class="ltx_sup"><span id="S4.T3.22.20.20.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">∗</span></sup></span></td>
<td id="S4.T3.22.20.20.6" class="ltx_td ltx_align_center">–</td>
<td id="S4.T3.22.20.20.7" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S4.T3.22.20.59" class="ltx_tr">
<td id="S4.T3.22.20.59.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">Best Results</td>
<td id="S4.T3.22.20.59.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">NA</td>
<td id="S4.T3.22.20.59.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">DETR</td>
<td id="S4.T3.22.20.59.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">FP-DETR</td>
<td id="S4.T3.22.20.59.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">Group DETR v2</td>
<td id="S4.T3.22.20.59.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">DINO</td>
<td id="S4.T3.22.20.59.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">NA</td>
</tr>
</table>
</span></div>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">Xu et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>: This publicly available dataset focuses on UAV (Unmanned Aerial Vehicle)-captured images and contains 2K images aimed at detecting both pedestrians and vehicles. The images were collected using a DJI drone and feature diverse conditions such as varying light levels and densely parked vehicles.
<br class="ltx_break"><span id="S4.SS1.p2.1.2" class="ltx_text ltx_font_bold">DeepLesion</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib111" title="" class="ltx_ref">111</a>]</cite>: Comprising CT scans from 4,427 patients, this dataset ranks among the largest of its kind. It includes a variety of lesion types, such as pulmonary nodules, bone abnormalities, kidney lesions, and enlarged lymph nodes. The objects of interest in these images are typically small and accompanied by noise, making their identification challenging.
<br class="ltx_break"><span id="S4.SS1.p2.1.3" class="ltx_text ltx_font_bold">Udacity Self Driving Car</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib112" title="" class="ltx_ref">112</a>]</cite>: Designed solely for educational use, this dataset features driving scenarios in Mountain View and nearby cities captured at a 2Hz image acquisition rate. The category labels within this dataset include cars, trucks, and pedestrians.
<br class="ltx_break"><span id="S4.SS1.p2.1.4" class="ltx_text ltx_font_bold">AMMW Dataset</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite>: Created for security applications, this active millimetre-wave image dataset includes more than 30 different types of objects. These include two kinds of lighters (made of plastic and metal), a simulated firearm, a knife, a blade, a bullet shell, a phone, a soup, a key, a magnet, a liquid bottle, an absorbent material, a match, and so on.
<br class="ltx_break"><span id="S4.SS1.p2.1.5" class="ltx_text ltx_font_bold">URPC 2018 Dataset</span>: This underwater image dataset includes four types of objects: holothurian, echinus, scallop and starfish <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib121" title="" class="ltx_ref">121</a>]</cite>. 
<br class="ltx_break"><span id="S4.SS1.p2.1.6" class="ltx_text ltx_font_bold">UAV dataset</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite>: This image dataset includes more than 9K images captured via UAVs in different weather and lighting conditions and various complex backgrounds. The objects in this dataset are sedans, people, motors, bicycles, trucks, buses, and tricycles. 
<br class="ltx_break"><span id="S4.SS1.p2.1.7" class="ltx_text ltx_font_bold">Drone-vs-bird</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib114" title="" class="ltx_ref">114</a>]</cite>: This video dataset aims to address the security concerns of drones flying over sensitive areas. It offers labeled video sequences to differentiate between birds and drones under various illumination, lighting, weather, and background conditions. 
<br class="ltx_break">A summary of these datasets, including their applications, type, resolutions, number of classes/instances/images/frame, and a link to their webpage, is provided in Table <a href="#S4.T2" title="TABLE II ‣ 4 Results and Benchmarks ‣ Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span><span id="S4.SS2.1.1" class="ltx_text ltx_font_italic">Benchmarks in Vision Applications</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">In this subsection, we introduce various vision-based applications where the detection performance of small objects is vital. For each application, we select one of the most popular datasets and report its performance metrics, along with details of the experimental setup.</p>
</div>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Generic Applications</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">For generic applications, we evaluate the performance of all small object detectors on the challenging MS COCO benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. The choice of this dataset is based on its wide acceptance in the object detection field and the accessibility of performance results. The MS COCO dataset consists of approximately 160K images across 80 categories. While the authors are advised to train their algorithms using the COCO 2017 training and validation sets, they are not restricted to these subsets.</p>
</div>
<figure id="S4.F11" class="ltx_figure">
<div id="S4.F11.20" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:521.9pt;height:1757.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-8.1pt,27.2pt) scale(0.97,0.97) ;">
<table id="S4.F11.20.20" class="ltx_tabular ltx_align_middle">
<tr id="S4.F11.4.4.4" class="ltx_tr">
<td id="S4.F11.4.4.4.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.F11.4.4.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F11.4.4.4.5.1.1" class="ltx_p" style="width:8.5pt;"><span id="S4.F11.4.4.4.5.1.1.1" class="ltx_text ltx_font_bold" style="font-size:50%;">Input</span></span>
</span>
</td>
<td id="S4.F11.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.F11.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F11.1.1.1.1.1.1" class="ltx_p" style="width:128.0pt;"><img src="/html/2309.04902/assets/Figures/000000037689.jpg" id="S4.F11.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="640" height="426" alt="Refer to caption"></span>
</span>
</td>
<td id="S4.F11.2.2.2.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.F11.2.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F11.2.2.2.2.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/2309.04902/assets/Figures/000000166287.jpg" id="S4.F11.2.2.2.2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="640" height="480" alt="Refer to caption"></span>
</span>
</td>
<td id="S4.F11.3.3.3.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.F11.3.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F11.3.3.3.3.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/2309.04902/assets/Figures/000000256775.jpg" id="S4.F11.3.3.3.3.1.1.g1" class="ltx_graphics ltx_img_landscape" width="640" height="480" alt="Refer to caption"></span>
</span>
</td>
<td id="S4.F11.4.4.4.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.F11.4.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F11.4.4.4.4.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/2309.04902/assets/Figures/000000350023.jpg" id="S4.F11.4.4.4.4.1.1.g1" class="ltx_graphics ltx_img_landscape" width="640" height="480" alt="Refer to caption"></span>
</span>
</td>
</tr>
<tr id="S4.F11.8.8.8" class="ltx_tr">
<td id="S4.F11.8.8.8.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.F11.8.8.8.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F11.8.8.8.5.1.1" class="ltx_p" style="width:8.5pt;"><span id="S4.F11.8.8.8.5.1.1.1" class="ltx_text ltx_font_bold" style="font-size:50%;">CBNet-V2</span></span>
</span>
</td>
<td id="S4.F11.5.5.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.F11.5.5.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F11.5.5.5.1.1.1" class="ltx_p" style="width:128.0pt;"><img src="/html/2309.04902/assets/Figures/CB37689.jpg" id="S4.F11.5.5.5.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="640" height="426" alt="Refer to caption"></span>
</span>
</td>
<td id="S4.F11.6.6.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.F11.6.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F11.6.6.6.2.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/2309.04902/assets/Figures/CB166287.jpg" id="S4.F11.6.6.6.2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="640" height="480" alt="Refer to caption"></span>
</span>
</td>
<td id="S4.F11.7.7.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.F11.7.7.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F11.7.7.7.3.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/2309.04902/assets/Figures/CB256775.jpg" id="S4.F11.7.7.7.3.1.1.g1" class="ltx_graphics ltx_img_landscape" width="640" height="480" alt="Refer to caption"></span>
</span>
</td>
<td id="S4.F11.8.8.8.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.F11.8.8.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F11.8.8.8.4.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/2309.04902/assets/Figures/CB350023.jpg" id="S4.F11.8.8.8.4.1.1.g1" class="ltx_graphics ltx_img_landscape" width="640" height="480" alt="Refer to caption"></span>
</span>
</td>
</tr>
<tr id="S4.F11.12.12.12" class="ltx_tr">
<td id="S4.F11.12.12.12.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.F11.12.12.12.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F11.12.12.12.5.1.1" class="ltx_p" style="width:8.5pt;"><span id="S4.F11.12.12.12.5.1.1.1" class="ltx_text ltx_font_bold" style="font-size:50%;">DETA-OB</span></span>
</span>
</td>
<td id="S4.F11.9.9.9.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.F11.9.9.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F11.9.9.9.1.1.1" class="ltx_p" style="width:128.0pt;"><img src="/html/2309.04902/assets/Figures/out_1.jpg" id="S4.F11.9.9.9.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="640" height="426" alt="Refer to caption"></span>
</span>
</td>
<td id="S4.F11.10.10.10.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.F11.10.10.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F11.10.10.10.2.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/2309.04902/assets/Figures/out_2.jpg" id="S4.F11.10.10.10.2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="640" height="470" alt="Refer to caption"></span>
</span>
</td>
<td id="S4.F11.11.11.11.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.F11.11.11.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F11.11.11.11.3.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/2309.04902/assets/Figures/out_3.jpg" id="S4.F11.11.11.11.3.1.1.g1" class="ltx_graphics ltx_img_landscape" width="640" height="479" alt="Refer to caption"></span>
</span>
</td>
<td id="S4.F11.12.12.12.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.F11.12.12.12.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F11.12.12.12.4.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/2309.04902/assets/Figures/out_4.jpg" id="S4.F11.12.12.12.4.1.1.g1" class="ltx_graphics ltx_img_landscape" width="640" height="478" alt="Refer to caption"></span>
</span>
</td>
</tr>
<tr id="S4.F11.16.16.16" class="ltx_tr">
<td id="S4.F11.16.16.16.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.F11.16.16.16.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F11.16.16.16.5.1.1" class="ltx_p" style="width:8.5pt;"><span id="S4.F11.16.16.16.5.1.1.1" class="ltx_text ltx_font_bold" style="font-size:50%;">DINO</span></span>
</span>
</td>
<td id="S4.F11.13.13.13.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.F11.13.13.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F11.13.13.13.1.1.1" class="ltx_p" style="width:128.0pt;"><img src="/html/2309.04902/assets/Figures/1_DINO.jpg" id="S4.F11.13.13.13.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="640" height="426" alt="Refer to caption"></span>
</span>
</td>
<td id="S4.F11.14.14.14.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.F11.14.14.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F11.14.14.14.2.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/2309.04902/assets/Figures/2_DINO.jpg" id="S4.F11.14.14.14.2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="640" height="479" alt="Refer to caption"></span>
</span>
</td>
<td id="S4.F11.15.15.15.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.F11.15.15.15.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F11.15.15.15.3.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/2309.04902/assets/Figures/3_DINO.jpg" id="S4.F11.15.15.15.3.1.1.g1" class="ltx_graphics ltx_img_landscape" width="640" height="480" alt="Refer to caption"></span>
</span>
</td>
<td id="S4.F11.16.16.16.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.F11.16.16.16.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F11.16.16.16.4.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/2309.04902/assets/Figures/4_DINO.jpg" id="S4.F11.16.16.16.4.1.1.g1" class="ltx_graphics ltx_img_landscape" width="640" height="481" alt="Refer to caption"></span>
</span>
</td>
</tr>
<tr id="S4.F11.20.20.20" class="ltx_tr">
<td id="S4.F11.20.20.20.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t">
<span id="S4.F11.20.20.20.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F11.20.20.20.5.1.1" class="ltx_p" style="width:8.5pt;"><span id="S4.F11.20.20.20.5.1.1.1" class="ltx_text ltx_font_bold" style="font-size:50%;">ViDT</span></span>
</span>
</td>
<td id="S4.F11.17.17.17.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t">
<span id="S4.F11.17.17.17.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F11.17.17.17.1.1.1" class="ltx_p" style="width:128.0pt;"><img src="/html/2309.04902/assets/Figures/vidt_1.jpg" id="S4.F11.17.17.17.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="640" height="427" alt="Refer to caption"></span>
</span>
</td>
<td id="S4.F11.18.18.18.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t">
<span id="S4.F11.18.18.18.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F11.18.18.18.2.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/2309.04902/assets/Figures/vidt_2.jpg" id="S4.F11.18.18.18.2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="640" height="477" alt="Refer to caption"></span>
</span>
</td>
<td id="S4.F11.19.19.19.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t">
<span id="S4.F11.19.19.19.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F11.19.19.19.3.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/2309.04902/assets/Figures/vidt_3.jpg" id="S4.F11.19.19.19.3.1.1.g1" class="ltx_graphics ltx_img_landscape" width="640" height="480" alt="Refer to caption"></span>
</span>
</td>
<td id="S4.F11.20.20.20.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t">
<span id="S4.F11.20.20.20.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F11.20.20.20.4.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/2309.04902/assets/Figures/vidt_4.jpg" id="S4.F11.20.20.20.4.1.1.g1" class="ltx_graphics ltx_img_landscape" width="640" height="497" alt="Refer to caption"></span>
</span>
</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F11.22.1.1" class="ltx_text" style="font-size:90%;">Figure 11</span>: </span><span id="S4.F11.23.2" class="ltx_text" style="font-size:90%;">Examples of detection results on COCO dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> for transformer-based SOTA small object detectors compared with Convolutional networks.</span></figcaption>
</figure>
<div id="S4.SS2.SSS1.p2" class="ltx_para">
<p id="S4.SS2.SSS1.p2.1" class="ltx_p">In Table <a href="#S4.T3" title="TABLE III ‣ 4.1 Datasets ‣ 4 Results and Benchmarks ‣ Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>, we examine and evaluate the performance of all the techniques under review that have reported their results on MS COCO (compiled from their papers). The table provides information on the backbone architecture, GFLOPS/FPS (indicating the computational overhead and execution speed), number of parameters (indicating the scale of the model), mAP (mean average precision: a measure of object detection performance), and epochs (indicating the inference time and convergence properties). Additionally, a link to each method’s webpage is provided for further information. The methods are categorized into three groups: CNN-based, mixed, and transformer only methods. The top-performing methods for each metric are shown in the table’s last row. It should be noted that this comparison was only feasible for methods that have reported values for each specific metric. In instances where there is a tie, the method with the highest mean average precision was deemed the best. The default mAP values are for the ”COCO 2017 val” set, while those for the ”COCO test-dev” set are marked with an asterisk. Please be aware that the reported mAP is only for objects with area<math id="S4.SS2.SSS1.p2.1.m1.1" class="ltx_Math" alttext="&lt;32^{2}" display="inline"><semantics id="S4.SS2.SSS1.p2.1.m1.1a"><mrow id="S4.SS2.SSS1.p2.1.m1.1.1" xref="S4.SS2.SSS1.p2.1.m1.1.1.cmml"><mi id="S4.SS2.SSS1.p2.1.m1.1.1.2" xref="S4.SS2.SSS1.p2.1.m1.1.1.2.cmml"></mi><mo id="S4.SS2.SSS1.p2.1.m1.1.1.1" xref="S4.SS2.SSS1.p2.1.m1.1.1.1.cmml">&lt;</mo><msup id="S4.SS2.SSS1.p2.1.m1.1.1.3" xref="S4.SS2.SSS1.p2.1.m1.1.1.3.cmml"><mn id="S4.SS2.SSS1.p2.1.m1.1.1.3.2" xref="S4.SS2.SSS1.p2.1.m1.1.1.3.2.cmml">32</mn><mn id="S4.SS2.SSS1.p2.1.m1.1.1.3.3" xref="S4.SS2.SSS1.p2.1.m1.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p2.1.m1.1b"><apply id="S4.SS2.SSS1.p2.1.m1.1.1.cmml" xref="S4.SS2.SSS1.p2.1.m1.1.1"><lt id="S4.SS2.SSS1.p2.1.m1.1.1.1.cmml" xref="S4.SS2.SSS1.p2.1.m1.1.1.1"></lt><csymbol cd="latexml" id="S4.SS2.SSS1.p2.1.m1.1.1.2.cmml" xref="S4.SS2.SSS1.p2.1.m1.1.1.2">absent</csymbol><apply id="S4.SS2.SSS1.p2.1.m1.1.1.3.cmml" xref="S4.SS2.SSS1.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p2.1.m1.1.1.3.1.cmml" xref="S4.SS2.SSS1.p2.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.SS2.SSS1.p2.1.m1.1.1.3.2.cmml" xref="S4.SS2.SSS1.p2.1.m1.1.1.3.2">32</cn><cn type="integer" id="S4.SS2.SSS1.p2.1.m1.1.1.3.3.cmml" xref="S4.SS2.SSS1.p2.1.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p2.1.m1.1c">&lt;32^{2}</annotation></semantics></math>.</p>
</div>
<figure id="S4.F12" class="ltx_figure">
<div id="S4.F12.12" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:521.9pt;height:950.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-8.1pt,14.7pt) scale(0.97,0.97) ;">
<table id="S4.F12.12.12" class="ltx_tabular ltx_align_middle">
<tr id="S4.F12.4.4.4" class="ltx_tr">
<td id="S4.F12.4.4.4.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.F12.4.4.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F12.4.4.4.5.1.1" class="ltx_p" style="width:8.5pt;"><span id="S4.F12.4.4.4.5.1.1.1" class="ltx_text ltx_font_bold" style="font-size:50%;">DETR</span></span>
</span>
</td>
<td id="S4.F12.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.F12.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F12.1.1.1.1.1.1" class="ltx_p" style="width:128.0pt;"><img src="/html/2309.04902/assets/x3.jpg" id="S4.F12.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="711" height="472" alt="Refer to caption"></span>
</span>
</td>
<td id="S4.F12.2.2.2.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.F12.2.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F12.2.2.2.2.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/2309.04902/assets/x4.jpg" id="S4.F12.2.2.2.2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="711" height="531" alt="Refer to caption"></span>
</span>
</td>
<td id="S4.F12.3.3.3.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.F12.3.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F12.3.3.3.3.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/2309.04902/assets/x5.jpg" id="S4.F12.3.3.3.3.1.1.g1" class="ltx_graphics ltx_img_landscape" width="711" height="534" alt="Refer to caption"></span>
</span>
</td>
<td id="S4.F12.4.4.4.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.F12.4.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F12.4.4.4.4.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/2309.04902/assets/x6.jpg" id="S4.F12.4.4.4.4.1.1.g1" class="ltx_graphics ltx_img_landscape" width="711" height="535" alt="Refer to caption"></span>
</span>
</td>
</tr>
<tr id="S4.F12.8.8.8" class="ltx_tr">
<td id="S4.F12.8.8.8.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.F12.8.8.8.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F12.8.8.8.5.1.1" class="ltx_p" style="width:8.5pt;"><span id="S4.F12.8.8.8.5.1.1.1" class="ltx_text ltx_font_bold" style="font-size:50%;">Faster RCNN</span></span>
<div class="ltx_pagination ltx_role_newpage"></div>
</span>
</td>
<td id="S4.F12.5.5.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.F12.5.5.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F12.5.5.5.1.1.1" class="ltx_p" style="width:128.0pt;"><img src="/html/2309.04902/assets/x7.jpg" id="S4.F12.5.5.5.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="711" height="437" alt="Refer to caption"></span>
</span>
</td>
<td id="S4.F12.6.6.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.F12.6.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F12.6.6.6.2.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/2309.04902/assets/x8.jpg" id="S4.F12.6.6.6.2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="711" height="448" alt="Refer to caption"></span>
</span>
</td>
<td id="S4.F12.7.7.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.F12.7.7.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F12.7.7.7.3.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/2309.04902/assets/x9.jpg" id="S4.F12.7.7.7.3.1.1.g1" class="ltx_graphics ltx_img_landscape" width="711" height="457" alt="Refer to caption"></span>
</span>
</td>
<td id="S4.F12.8.8.8.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.F12.8.8.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F12.8.8.8.4.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/2309.04902/assets/x10.jpg" id="S4.F12.8.8.8.4.1.1.g1" class="ltx_graphics ltx_img_landscape" width="711" height="451" alt="Refer to caption"></span>
</span>
</td>
</tr>
<tr id="S4.F12.12.12.12" class="ltx_tr">
<td id="S4.F12.12.12.12.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t">
<span id="S4.F12.12.12.12.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F12.12.12.12.5.1.1" class="ltx_p" style="width:8.5pt;"><span id="S4.F12.12.12.12.5.1.1.1" class="ltx_text ltx_font_bold" style="font-size:50%;">SSD</span></span>
</span>
</td>
<td id="S4.F12.9.9.9.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t">
<span id="S4.F12.9.9.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F12.9.9.9.1.1.1" class="ltx_p" style="width:128.0pt;"><img src="/html/2309.04902/assets/x11.jpg" id="S4.F12.9.9.9.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="711" height="451" alt="Refer to caption"></span>
</span>
</td>
<td id="S4.F12.10.10.10.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t">
<span id="S4.F12.10.10.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F12.10.10.10.2.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/2309.04902/assets/x12.jpg" id="S4.F12.10.10.10.2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="711" height="457" alt="Refer to caption"></span>
</span>
</td>
<td id="S4.F12.11.11.11.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t">
<span id="S4.F12.11.11.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F12.11.11.11.3.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/2309.04902/assets/x13.jpg" id="S4.F12.11.11.11.3.1.1.g1" class="ltx_graphics ltx_img_landscape" width="711" height="459" alt="Refer to caption"></span>
</span>
</td>
<td id="S4.F12.12.12.12.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t">
<span id="S4.F12.12.12.12.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F12.12.12.12.4.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/2309.04902/assets/x14.jpg" id="S4.F12.12.12.12.4.1.1.g1" class="ltx_graphics ltx_img_landscape" width="711" height="436" alt="Refer to caption"></span>
</span>
</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F12.14.1.1" class="ltx_text" style="font-size:90%;">Figure 12</span>: </span><span id="S4.F12.15.2" class="ltx_text" style="font-size:90%;">Examples of detection results on COCO dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> for transformer-based SOTA small object detectors compared with Convolutional networks.</span></figcaption>
</figure>
<figure id="S4.F13" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2309.04902/assets/Figures/zoom.jpg" id="S4.F13.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="150" height="164" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2309.04902/assets/Figures/SSD.jpg" id="S4.F13.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="147" height="164" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2309.04902/assets/x15.jpg" id="S4.F13.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="156" height="182" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2309.04902/assets/x16.jpg" id="S4.F13.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="162" height="182" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2309.04902/assets/Figures/vidtZoom.jpg" id="S4.F13.g5" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="144" height="162" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2309.04902/assets/Figures/DETA.jpg" id="S4.F13.g6" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="147" height="162" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2309.04902/assets/Figures/DINO.jpg" id="S4.F13.g7" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="146" height="158" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2309.04902/assets/Figures/CB.jpg" id="S4.F13.g8" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="147" height="164" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F13.2.1.1" class="ltx_text" style="font-size:90%;">Figure 13</span>: </span><span id="S4.F13.3.2" class="ltx_text" style="font-size:90%;">Detection results on a sample image when zoomed in. First row from the left: Input image, SSD, Faster RCNN, DETR. Second row from the left: ViDT, DETA-OB, DINO, CBNet v2.</span></figcaption>
</figure>
<div id="S4.SS2.SSS1.p3" class="ltx_para">
<p id="S4.SS2.SSS1.p3.2" class="ltx_p">Upon examining Table <a href="#S4.T3" title="TABLE III ‣ 4.1 Datasets ‣ 4 Results and Benchmarks ‣ Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>, it is obvious that most techniques benefit from using a mix of CNN and transformer architectures, essentially adopting hybrid strategies. Notably, Group DETR v2 which relies solely on a transformer-based architecture, attains a mAP of 48.4<math id="S4.SS2.SSS1.p3.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS2.SSS1.p3.1.m1.1a"><mo id="S4.SS2.SSS1.p3.1.m1.1.1" xref="S4.SS2.SSS1.p3.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p3.1.m1.1b"><csymbol cd="latexml" id="S4.SS2.SSS1.p3.1.m1.1.1.cmml" xref="S4.SS2.SSS1.p3.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p3.1.m1.1c">\%</annotation></semantics></math>. However, achieving such a performance requires the adoption of additional techniques such as pre-training on two large-scale datasets and multi-scale learning. In terms of convergence, DINO outperforms by reaching stable results after just 12 epochs, while also securing a commendable mAP of 32.3<math id="S4.SS2.SSS1.p3.2.m2.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS2.SSS1.p3.2.m2.1a"><mo id="S4.SS2.SSS1.p3.2.m2.1.1" xref="S4.SS2.SSS1.p3.2.m2.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p3.2.m2.1b"><csymbol cd="latexml" id="S4.SS2.SSS1.p3.2.m2.1.1.cmml" xref="S4.SS2.SSS1.p3.2.m2.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p3.2.m2.1c">\%</annotation></semantics></math>. Conversely, the original DETR model has the fastest inference time and the lowest GFLOPS. FP-DETR stands out for having the lightest network with only 36M parameters.</p>
</div>
<div id="S4.SS2.SSS1.p4" class="ltx_para">
<p id="S4.SS2.SSS1.p4.1" class="ltx_p">Drawing from these findings, we conclude that pre-training and multi-scale learning emerge as the most effective strategies for excelling in small object detection. This may be attributed to the imbalance in downstream tasks and the lack of informative features in small objects.</p>
</div>
<div id="S4.SS2.SSS1.p5" class="ltx_para">
<p id="S4.SS2.SSS1.p5.1" class="ltx_p">Figure <a href="#S4.F11" title="Figure 11 ‣ 4.2.1 Generic Applications ‣ 4.2 Benchmarks in Vision Applications ‣ 4 Results and Benchmarks ‣ Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> which spans two pages, along with its more detailed counterpart in Figure <a href="#S4.F13" title="Figure 13 ‣ 4.2.1 Generic Applications ‣ 4.2 Benchmarks in Vision Applications ‣ 4 Results and Benchmarks ‣ Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>, illustrates the detection results of various transformers and CNN-based methods. These are compared to each other using selected images from the COCO dataset and implemented by us using their public models available on their GitHub pages. The analysis reveals that Faster RCNN and SSD fall short in accurately detecting small objects. Specifically, SSD either misses most objects or generates numerous bounding boxes with false labels and poorly located bounding boxes. While Faster RCNN performs better, it still produces low-confidence bounding boxes and occasionally assigns incorrect labels.</p>
</div>
<div id="S4.SS2.SSS1.p6" class="ltx_para">
<p id="S4.SS2.SSS1.p6.1" class="ltx_p">In contrast, DETR has the tendency to over-estimate the number of objects, leading to multiple bounding boxes for individual objects. It is commonly noted t that DETR is prone to generating false positives. Finally, among the methods evaluated, CBNet V2 stands out for its superior performance. As observed, it produces high confidence scores for the objects it detects, even though it may occasionally misidentify some objects.</p>
</div>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Small Object Detection in Aerial Images </h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">Another interesting use of detecting small objects is in the area of remote sensing. This field is particularly appealing because many organizations and research bodies aim to routinely monitor the Earth’s surface through aerial images to collect both national and international data for statistics. While these images can be acquired using various modalities, this survey focuses only on non-SAR images. This is because SAR images have extensively been researched and deserve their own separate study. Nonetheless, the learning techniques discussed in this survey could also be applicable to SAR images.</p>
</div>
<div id="S4.SS2.SSS2.p2" class="ltx_para">
<p id="S4.SS2.SSS2.p2.1" class="ltx_p">In aerial images, objects often appear small due to their significant distance from the camera. The bird’s-eye view also adds complexity to the task of object detection, as objects can be situated anywhere within the image. To assess the performance of transformer-based detectors designed for such applications, we selected the DOTA image dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib122" title="" class="ltx_ref">122</a>]</cite>, which has become a widely used benchmark in the field of object detection. Figure <a href="#S4.F14" title="Figure 14 ‣ 4.2.2 Small Object Detection in Aerial Images ‣ 4.2 Benchmarks in Vision Applications ‣ 4 Results and Benchmarks ‣ Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a> displays some sample images from the DOTA dataset featuring small objects. The dataset includes a predefined Training set, Validation set, and Testing set. In comparison to generic applications, this particular application has received relatively less attention from transformer experts. However, as indicated in Table <a href="#S4.T4" title="TABLE IV ‣ 4.2.2 Small Object Detection in Aerial Images ‣ 4.2 Benchmarks in Vision Applications ‣ 4 Results and Benchmarks ‣ Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> (results are compiled from papers), ReDet distinguishes itself throuh its multi-scale learning strategy and pre-training on the ImageNet dataset, achieving the highest precision value (80.89<math id="S4.SS2.SSS2.p2.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS2.SSS2.p2.1.m1.1a"><mo id="S4.SS2.SSS2.p2.1.m1.1.1" xref="S4.SS2.SSS2.p2.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p2.1.m1.1b"><csymbol cd="latexml" id="S4.SS2.SSS2.p2.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p2.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p2.1.m1.1c">\%</annotation></semantics></math>) and requiring only 12 training epochs. This mirrors the insights gained from the COCO dataset analysis, suggesting that optimal performance can be attained by addressing imbalances in downstream tasks and including informative features from small objects.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.11.1.1" class="ltx_text" style="font-size:90%;">TABLE IV</span>: </span><span id="S4.T4.12.2" class="ltx_text" style="font-size:90%;">Detection performance (%) for small-scale objects on DOTA image dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib122" title="" class="ltx_ref">122</a>]</cite>. The top section shows results for CNN-based techniques, the middle section shows results for mixed architectures. MS: Multi-scale network, FT: Fine-tuned, FPN: Feature pyramid network, IN: Pre-trained on ImageNet.</span></figcaption>
<div id="S4.T4.9" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:739.1pt;height:270pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<table id="S4.T4.9.9" class="ltx_tabular ltx_align_middle">
<tr id="S4.T4.6.6.6" class="ltx_tr">
<td id="S4.T4.6.6.6.7" class="ltx_td ltx_align_left ltx_border_tt">Model</td>
<td id="S4.T4.6.6.6.8" class="ltx_td ltx_align_center ltx_border_tt">Backbone</td>
<td id="S4.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt">FPS <math id="S4.T4.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T4.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T4.1.1.1.1.m1.1.1" xref="S4.T4.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.1.m1.1b"><ci id="S4.T4.1.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T4.3.3.3.3" class="ltx_td ltx_align_center ltx_border_tt">
<math id="S4.T4.2.2.2.2.m1.1" class="ltx_Math" alttext="\#" display="inline"><semantics id="S4.T4.2.2.2.2.m1.1a"><mi mathvariant="normal" id="S4.T4.2.2.2.2.m1.1.1" xref="S4.T4.2.2.2.2.m1.1.1.cmml">#</mi><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.2.m1.1b"><ci id="S4.T4.2.2.2.2.m1.1.1.cmml" xref="S4.T4.2.2.2.2.m1.1.1">#</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.2.m1.1c">\#</annotation></semantics></math>params<math id="S4.T4.3.3.3.3.m2.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T4.3.3.3.3.m2.1a"><mo stretchy="false" id="S4.T4.3.3.3.3.m2.1.1" xref="S4.T4.3.3.3.3.m2.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T4.3.3.3.3.m2.1b"><ci id="S4.T4.3.3.3.3.m2.1.1.cmml" xref="S4.T4.3.3.3.3.m2.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.3.3.3.m2.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T4.5.5.5.5" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S4.T4.5.5.5.5.1" class="ltx_text ltx_markedasmath">mAP</span> <math id="S4.T4.5.5.5.5.m2.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T4.5.5.5.5.m2.1a"><mo stretchy="false" id="S4.T4.5.5.5.5.m2.1.1" xref="S4.T4.5.5.5.5.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.5.5.5.5.m2.1b"><ci id="S4.T4.5.5.5.5.m2.1.1.cmml" xref="S4.T4.5.5.5.5.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.5.5.5.5.m2.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T4.6.6.6.6" class="ltx_td ltx_align_center ltx_border_tt">Epochs<math id="S4.T4.6.6.6.6.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T4.6.6.6.6.m1.1a"><mo stretchy="false" id="S4.T4.6.6.6.6.m1.1.1" xref="S4.T4.6.6.6.6.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T4.6.6.6.6.m1.1b"><ci id="S4.T4.6.6.6.6.m1.1.1.cmml" xref="S4.T4.6.6.6.6.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.6.6.6.6.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T4.6.6.6.9" class="ltx_td ltx_align_center ltx_border_tt">URL</td>
</tr>
<tr id="S4.T4.9.9.10" class="ltx_tr">
<td id="S4.T4.9.9.10.1" class="ltx_td ltx_align_left ltx_border_t">Rotated Faster RCNN-MS (NeurIPS2015)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>
</td>
<td id="S4.T4.9.9.10.2" class="ltx_td ltx_align_center ltx_border_t">ResNet101</td>
<td id="S4.T4.9.9.10.3" class="ltx_td ltx_align_center ltx_border_t">–</td>
<td id="S4.T4.9.9.10.4" class="ltx_td ltx_align_center ltx_border_t">64M</td>
<td id="S4.T4.9.9.10.5" class="ltx_td ltx_align_center ltx_border_t">67.71</td>
<td id="S4.T4.9.9.10.6" class="ltx_td ltx_align_center ltx_border_t">50</td>
<td id="S4.T4.9.9.10.7" class="ltx_td ltx_align_center ltx_border_t"><a target="_blank" href="https://github.com/open-mmlab/mmrotate/tree/main/configs/rotated_faster_rcnn" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T4.9.9.11" class="ltx_tr">
<td id="S4.T4.9.9.11.1" class="ltx_td ltx_align_left">SSD (ECCV2016) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>
</td>
<td id="S4.T4.9.9.11.2" class="ltx_td ltx_align_center">–</td>
<td id="S4.T4.9.9.11.3" class="ltx_td ltx_align_center">–</td>
<td id="S4.T4.9.9.11.4" class="ltx_td ltx_align_center">–</td>
<td id="S4.T4.9.9.11.5" class="ltx_td ltx_align_center">56.1</td>
<td id="S4.T4.9.9.11.6" class="ltx_td ltx_align_center">–</td>
<td id="S4.T4.9.9.11.7" class="ltx_td ltx_align_center"><a target="_blank" href="https://github.com/pierluigiferrari/ssd_keras" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T4.9.9.12" class="ltx_tr">
<td id="S4.T4.9.9.12.1" class="ltx_td ltx_align_left">RetinaNet-MS (ICCV2017)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>
</td>
<td id="S4.T4.9.9.12.2" class="ltx_td ltx_align_center">ResNet101</td>
<td id="S4.T4.9.9.12.3" class="ltx_td ltx_align_center">–</td>
<td id="S4.T4.9.9.12.4" class="ltx_td ltx_align_center"><span id="S4.T4.9.9.12.4.1" class="ltx_text ltx_font_bold">59M</span></td>
<td id="S4.T4.9.9.12.5" class="ltx_td ltx_align_center">66.53</td>
<td id="S4.T4.9.9.12.6" class="ltx_td ltx_align_center">50</td>
<td id="S4.T4.9.9.12.7" class="ltx_td ltx_align_center"><a target="_blank" href="https://github.com/DetectionTeamUCAS/RetinaNet_Tensorflow" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T4.9.9.13" class="ltx_tr">
<td id="S4.T4.9.9.13.1" class="ltx_td ltx_align_left">ROI-Transformer-MS-IN (CVPR2019) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib123" title="" class="ltx_ref">123</a>, <a href="#bib.bib124" title="" class="ltx_ref">124</a>]</cite>
</td>
<td id="S4.T4.9.9.13.2" class="ltx_td ltx_align_center">ResNet50</td>
<td id="S4.T4.9.9.13.3" class="ltx_td ltx_align_center">–</td>
<td id="S4.T4.9.9.13.4" class="ltx_td ltx_align_center">–</td>
<td id="S4.T4.9.9.13.5" class="ltx_td ltx_align_center">80.06</td>
<td id="S4.T4.9.9.13.6" class="ltx_td ltx_align_center"><span id="S4.T4.9.9.13.6.1" class="ltx_text ltx_font_bold">12</span></td>
<td id="S4.T4.9.9.13.7" class="ltx_td ltx_align_center"><a target="_blank" href="https://github.com/open-mmlab/mmrotate/blob/main/configs/roi_trans/README.md" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T4.9.9.14" class="ltx_tr">
<td id="S4.T4.9.9.14.1" class="ltx_td ltx_align_left">Yolov5 (2020)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>
</td>
<td id="S4.T4.9.9.14.2" class="ltx_td ltx_align_center">–</td>
<td id="S4.T4.9.9.14.3" class="ltx_td ltx_align_center"><span id="S4.T4.9.9.14.3.1" class="ltx_text ltx_font_bold">95</span></td>
<td id="S4.T4.9.9.14.4" class="ltx_td ltx_align_center">–</td>
<td id="S4.T4.9.9.14.5" class="ltx_td ltx_align_center">64.5</td>
<td id="S4.T4.9.9.14.6" class="ltx_td ltx_align_center">–</td>
<td id="S4.T4.9.9.14.7" class="ltx_td ltx_align_center"><a target="_blank" href="https://github.com/ultralytics/yolov5" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T4.9.9.15" class="ltx_tr">
<td id="S4.T4.9.9.15.1" class="ltx_td ltx_align_left">ReDet-MS-FPN (CVPR2021)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib125" title="" class="ltx_ref">125</a>]</cite>
</td>
<td id="S4.T4.9.9.15.2" class="ltx_td ltx_align_center">ResNet50</td>
<td id="S4.T4.9.9.15.3" class="ltx_td ltx_align_center">–</td>
<td id="S4.T4.9.9.15.4" class="ltx_td ltx_align_center">–</td>
<td id="S4.T4.9.9.15.5" class="ltx_td ltx_align_center">80.1</td>
<td id="S4.T4.9.9.15.6" class="ltx_td ltx_align_center">–</td>
<td id="S4.T4.9.9.15.7" class="ltx_td ltx_align_center"><a target="_blank" href="https://github.com/csuhan/ReDet" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T4.7.7.7" class="ltx_tr">
<td id="S4.T4.7.7.7.1" class="ltx_td ltx_align_left ltx_border_t">O<sup id="S4.T4.7.7.7.1.1" class="ltx_sup">2</sup>DETR-MS (arXiv2021)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>
</td>
<td id="S4.T4.7.7.7.2" class="ltx_td ltx_align_center ltx_border_t">ResNet101</td>
<td id="S4.T4.7.7.7.3" class="ltx_td ltx_align_center ltx_border_t">–</td>
<td id="S4.T4.7.7.7.4" class="ltx_td ltx_align_center ltx_border_t">63M</td>
<td id="S4.T4.7.7.7.5" class="ltx_td ltx_align_center ltx_border_t">70.02</td>
<td id="S4.T4.7.7.7.6" class="ltx_td ltx_align_center ltx_border_t">50</td>
<td id="S4.T4.7.7.7.7" class="ltx_td ltx_align_center ltx_border_t">–</td>
</tr>
<tr id="S4.T4.8.8.8" class="ltx_tr">
<td id="S4.T4.8.8.8.1" class="ltx_td ltx_align_left">O<sup id="S4.T4.8.8.8.1.1" class="ltx_sup">2</sup>DETR-MS-FT (arXiv2021)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>
</td>
<td id="S4.T4.8.8.8.2" class="ltx_td ltx_align_center">ResNet101</td>
<td id="S4.T4.8.8.8.3" class="ltx_td ltx_align_center">–</td>
<td id="S4.T4.8.8.8.4" class="ltx_td ltx_align_center">–</td>
<td id="S4.T4.8.8.8.5" class="ltx_td ltx_align_center">76.23</td>
<td id="S4.T4.8.8.8.6" class="ltx_td ltx_align_center">62</td>
<td id="S4.T4.8.8.8.7" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S4.T4.9.9.9" class="ltx_tr">
<td id="S4.T4.9.9.9.1" class="ltx_td ltx_align_left">O<sup id="S4.T4.9.9.9.1.1" class="ltx_sup">2</sup>DETR-MS-FPN-FT (arXiv2021)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>
</td>
<td id="S4.T4.9.9.9.2" class="ltx_td ltx_align_center">ResNet50</td>
<td id="S4.T4.9.9.9.3" class="ltx_td ltx_align_center">–</td>
<td id="S4.T4.9.9.9.4" class="ltx_td ltx_align_center">–</td>
<td id="S4.T4.9.9.9.5" class="ltx_td ltx_align_center">79.66</td>
<td id="S4.T4.9.9.9.6" class="ltx_td ltx_align_center">–</td>
<td id="S4.T4.9.9.9.7" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S4.T4.9.9.16" class="ltx_tr">
<td id="S4.T4.9.9.16.1" class="ltx_td ltx_align_left">SPH-Yolov5 (RS2022) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite>
</td>
<td id="S4.T4.9.9.16.2" class="ltx_td ltx_align_center">Swin Transformer-base</td>
<td id="S4.T4.9.9.16.3" class="ltx_td ltx_align_center">51</td>
<td id="S4.T4.9.9.16.4" class="ltx_td ltx_align_center">–</td>
<td id="S4.T4.9.9.16.5" class="ltx_td ltx_align_center">71.6</td>
<td id="S4.T4.9.9.16.6" class="ltx_td ltx_align_center">150</td>
<td id="S4.T4.9.9.16.7" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S4.T4.9.9.17" class="ltx_tr">
<td id="S4.T4.9.9.17.1" class="ltx_td ltx_align_left">AO2-DETR-MS (TCSVT2022)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite>
</td>
<td id="S4.T4.9.9.17.2" class="ltx_td ltx_align_center">ResNet50</td>
<td id="S4.T4.9.9.17.3" class="ltx_td ltx_align_center">–</td>
<td id="S4.T4.9.9.17.4" class="ltx_td ltx_align_center">–</td>
<td id="S4.T4.9.9.17.5" class="ltx_td ltx_align_center">79.22</td>
<td id="S4.T4.9.9.17.6" class="ltx_td ltx_align_center">–</td>
<td id="S4.T4.9.9.17.7" class="ltx_td ltx_align_center"><a target="_blank" href="https://github.com/Ixiaohuihuihui/AO2-DETR" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T4.9.9.18" class="ltx_tr">
<td id="S4.T4.9.9.18.1" class="ltx_td ltx_align_left">MDCT (RS2023)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref">96</a>]</cite>
</td>
<td id="S4.T4.9.9.18.2" class="ltx_td ltx_align_center">–</td>
<td id="S4.T4.9.9.18.3" class="ltx_td ltx_align_center">–</td>
<td id="S4.T4.9.9.18.4" class="ltx_td ltx_align_center">–</td>
<td id="S4.T4.9.9.18.5" class="ltx_td ltx_align_center">75.7</td>
<td id="S4.T4.9.9.18.6" class="ltx_td ltx_align_center">–</td>
<td id="S4.T4.9.9.18.7" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S4.T4.9.9.19" class="ltx_tr">
<td id="S4.T4.9.9.19.1" class="ltx_td ltx_align_left">ReDet-MS-IN (arXiv2023)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib124" title="" class="ltx_ref">124</a>]</cite>
</td>
<td id="S4.T4.9.9.19.2" class="ltx_td ltx_align_center">ViTDet, ViT-B</td>
<td id="S4.T4.9.9.19.3" class="ltx_td ltx_align_center">–</td>
<td id="S4.T4.9.9.19.4" class="ltx_td ltx_align_center">–</td>
<td id="S4.T4.9.9.19.5" class="ltx_td ltx_align_center"><span id="S4.T4.9.9.19.5.1" class="ltx_text ltx_font_bold">80.89</span></td>
<td id="S4.T4.9.9.19.6" class="ltx_td ltx_align_center"><span id="S4.T4.9.9.19.6.1" class="ltx_text ltx_font_bold">12</span></td>
<td id="S4.T4.9.9.19.7" class="ltx_td ltx_align_center"><a target="_blank" href="https://github.com/csuhan/ReDet" title="" class="ltx_ref ltx_href" style="color:#0000FF;">Link</a></td>
</tr>
<tr id="S4.T4.9.9.20" class="ltx_tr">
<td id="S4.T4.9.9.20.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">Best Results</td>
<td id="S4.T4.9.9.20.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">NA</td>
<td id="S4.T4.9.9.20.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">Yolov5</td>
<td id="S4.T4.9.9.20.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">RetinaNet</td>
<td id="S4.T4.9.9.20.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">ReDet-MS-IN</td>
<td id="S4.T4.9.9.20.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">ReDet-MS-IN</td>
<td id="S4.T4.9.9.20.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">NA</td>
</tr>
</table>
</span></div>
</figure>
<figure id="S4.F14" class="ltx_figure"><img src="/html/2309.04902/assets/x17.jpg" id="S4.F14.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="345" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F14.2.1.1" class="ltx_text" style="font-size:90%;">Figure 14</span>: </span><span id="S4.F14.3.2" class="ltx_text" style="font-size:90%;">Example of small objects in DOTA image dataset.</span></figcaption>
</figure>
</section>
<section id="S4.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3 </span>Small Object Detection in Medical Images</h4>

<div id="S4.SS2.SSS3.p1" class="ltx_para">
<p id="S4.SS2.SSS3.p1.5" class="ltx_p">In the field of medical imaging, specialists are often tasked with early detection and identification of anomalies. Missing even barely visible or small abnormal cells can lead to serious repercussions for patients, including cancer and life-threatening conditions. These small-sized objects can be found as abnormalities in the retina of diabetic patients, early tumors, vascular plaques, etc. Despite the critical nature and potential life-threatening impact of this research area, only a handful of studies have tackled the challenges associated with detecting small objects in this crucial application. For those interested in this topic, the DeepLesion CT image dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib111" title="" class="ltx_ref">111</a>]</cite> has been selected as the benchmark due to the availability of the results for this particular dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib126" title="" class="ltx_ref">126</a>]</cite>. Sample images from this dataset are shown in Figure <a href="#S4.F15" title="Figure 15 ‣ 4.2.3 Small Object Detection in Medical Images ‣ 4.2 Benchmarks in Vision Applications ‣ 4 Results and Benchmarks ‣ Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a>. This dataset is divided into three sets: training (70<math id="S4.SS2.SSS3.p1.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS2.SSS3.p1.1.m1.1a"><mo id="S4.SS2.SSS3.p1.1.m1.1.1" xref="S4.SS2.SSS3.p1.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p1.1.m1.1b"><csymbol cd="latexml" id="S4.SS2.SSS3.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS3.p1.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p1.1.m1.1c">\%</annotation></semantics></math>), validation (15<math id="S4.SS2.SSS3.p1.2.m2.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS2.SSS3.p1.2.m2.1a"><mo id="S4.SS2.SSS3.p1.2.m2.1.1" xref="S4.SS2.SSS3.p1.2.m2.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p1.2.m2.1b"><csymbol cd="latexml" id="S4.SS2.SSS3.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS3.p1.2.m2.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p1.2.m2.1c">\%</annotation></semantics></math>), and test (15<math id="S4.SS2.SSS3.p1.3.m3.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS2.SSS3.p1.3.m3.1a"><mo id="S4.SS2.SSS3.p1.3.m3.1.1" xref="S4.SS2.SSS3.p1.3.m3.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p1.3.m3.1b"><csymbol cd="latexml" id="S4.SS2.SSS3.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS3.p1.3.m3.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p1.3.m3.1c">\%</annotation></semantics></math>) sets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite>. Table <a href="#S4.T5" title="TABLE V ‣ 4.2.3 Small Object Detection in Medical Images ‣ 4.2 Benchmarks in Vision Applications ‣ 4 Results and Benchmarks ‣ Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> compares the accuracy and mAP of three transformer-based studies against both two-stage and one-stage detectors (results are compiled from their papers). The MS Transformer emerges as the best technique with this dataset, albeit with limited competition. Its primary innovation lies in self-supervised learning and the incorporation of a masking mechanism within a hierarchical transformer model. Overall, with an accuracy of 90.3<math id="S4.SS2.SSS3.p1.4.m4.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS2.SSS3.p1.4.m4.1a"><mo id="S4.SS2.SSS3.p1.4.m4.1.1" xref="S4.SS2.SSS3.p1.4.m4.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p1.4.m4.1b"><csymbol cd="latexml" id="S4.SS2.SSS3.p1.4.m4.1.1.cmml" xref="S4.SS2.SSS3.p1.4.m4.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p1.4.m4.1c">\%</annotation></semantics></math> and an mAP of 89.6<math id="S4.SS2.SSS3.p1.5.m5.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS2.SSS3.p1.5.m5.1a"><mo id="S4.SS2.SSS3.p1.5.m5.1.1" xref="S4.SS2.SSS3.p1.5.m5.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p1.5.m5.1b"><csymbol cd="latexml" id="S4.SS2.SSS3.p1.5.m5.1.1.cmml" xref="S4.SS2.SSS3.p1.5.m5.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p1.5.m5.1c">\%</annotation></semantics></math>, this dataset appears to be less challenging compared to other medical imaging tasks, especially considering that some tumor detection tasks are virtually invisible to the human eyes.</p>
</div>
<figure id="S4.F15" class="ltx_figure"><img src="/html/2309.04902/assets/Figures/DeepLesion.jpg" id="S4.F15.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="586" height="144" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F15.2.1.1" class="ltx_text" style="font-size:90%;">Figure 15</span>: </span><span id="S4.F15.3.2" class="ltx_text" style="font-size:90%;">Example of small abnormalities in DeepLesion image dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib111" title="" class="ltx_ref">111</a>]</cite>.</span></figcaption>
</figure>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T5.5.1.1" class="ltx_text" style="font-size:90%;">TABLE V</span>: </span><span id="S4.T5.6.2" class="ltx_text" style="font-size:90%;">Detection performance (%) for DeepLesion CT image dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib111" title="" class="ltx_ref">111</a>]</cite>. The top section shows results for CNN-based techniques, the middle section shows results for mixed architectures. </span></figcaption>
<div id="S4.T5.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:388.1pt;height:126pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<table id="S4.T5.3.3" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.3.3.3" class="ltx_tr">
<td id="S4.T5.3.3.3.4" class="ltx_td ltx_align_center ltx_border_tt">Model</td>
<td id="S4.T5.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt">Accuracy <math id="S4.T5.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T5.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T5.1.1.1.1.m1.1.1" xref="S4.T5.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T5.1.1.1.1.m1.1b"><ci id="S4.T5.1.1.1.1.m1.1.1.cmml" xref="S4.T5.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T5.3.3.3.3" class="ltx_td ltx_align_center ltx_border_tt">
<math id="S4.T5.2.2.2.2.m1.1" class="ltx_Math" alttext="\text{mAP}^{0.5}" display="inline"><semantics id="S4.T5.2.2.2.2.m1.1a"><msup id="S4.T5.2.2.2.2.m1.1.1" xref="S4.T5.2.2.2.2.m1.1.1.cmml"><mtext id="S4.T5.2.2.2.2.m1.1.1.2" xref="S4.T5.2.2.2.2.m1.1.1.2a.cmml">mAP</mtext><mn id="S4.T5.2.2.2.2.m1.1.1.3" xref="S4.T5.2.2.2.2.m1.1.1.3.cmml">0.5</mn></msup><annotation-xml encoding="MathML-Content" id="S4.T5.2.2.2.2.m1.1b"><apply id="S4.T5.2.2.2.2.m1.1.1.cmml" xref="S4.T5.2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T5.2.2.2.2.m1.1.1.1.cmml" xref="S4.T5.2.2.2.2.m1.1.1">superscript</csymbol><ci id="S4.T5.2.2.2.2.m1.1.1.2a.cmml" xref="S4.T5.2.2.2.2.m1.1.1.2"><mtext id="S4.T5.2.2.2.2.m1.1.1.2.cmml" xref="S4.T5.2.2.2.2.m1.1.1.2">mAP</mtext></ci><cn type="float" id="S4.T5.2.2.2.2.m1.1.1.3.cmml" xref="S4.T5.2.2.2.2.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.2.2.2.2.m1.1c">\text{mAP}^{0.5}</annotation></semantics></math> <math id="S4.T5.3.3.3.3.m2.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T5.3.3.3.3.m2.1a"><mo stretchy="false" id="S4.T5.3.3.3.3.m2.1.1" xref="S4.T5.3.3.3.3.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T5.3.3.3.3.m2.1b"><ci id="S4.T5.3.3.3.3.m2.1.1.cmml" xref="S4.T5.3.3.3.3.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.3.3.3.3.m2.1c">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T5.3.3.4" class="ltx_tr">
<td id="S4.T5.3.3.4.1" class="ltx_td ltx_align_center ltx_border_t">Faster RCNN (NeurIPS2015)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>
</td>
<td id="S4.T5.3.3.4.2" class="ltx_td ltx_align_center ltx_border_t">83.3</td>
<td id="S4.T5.3.3.4.3" class="ltx_td ltx_align_center ltx_border_t">83.3</td>
</tr>
<tr id="S4.T5.3.3.5" class="ltx_tr">
<td id="S4.T5.3.3.5.1" class="ltx_td ltx_align_center">Yolov5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>
</td>
<td id="S4.T5.3.3.5.2" class="ltx_td ltx_align_center">85.2</td>
<td id="S4.T5.3.3.5.3" class="ltx_td ltx_align_center">88.2</td>
</tr>
<tr id="S4.T5.3.3.6" class="ltx_tr">
<td id="S4.T5.3.3.6.1" class="ltx_td ltx_align_center ltx_border_t">DETR (ECCV2020)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>
</td>
<td id="S4.T5.3.3.6.2" class="ltx_td ltx_align_center ltx_border_t">86.7</td>
<td id="S4.T5.3.3.6.3" class="ltx_td ltx_align_center ltx_border_t">87.8</td>
</tr>
<tr id="S4.T5.3.3.7" class="ltx_tr">
<td id="S4.T5.3.3.7.1" class="ltx_td ltx_align_center">Swin Transformer</td>
<td id="S4.T5.3.3.7.2" class="ltx_td ltx_align_center">82.9</td>
<td id="S4.T5.3.3.7.3" class="ltx_td ltx_align_center">81.2</td>
</tr>
<tr id="S4.T5.3.3.8" class="ltx_tr">
<td id="S4.T5.3.3.8.1" class="ltx_td ltx_align_center">MS Transformer (CIN2022)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite>
</td>
<td id="S4.T5.3.3.8.2" class="ltx_td ltx_align_center"><span id="S4.T5.3.3.8.2.1" class="ltx_text ltx_font_bold">90.3</span></td>
<td id="S4.T5.3.3.8.3" class="ltx_td ltx_align_center"><span id="S4.T5.3.3.8.3.1" class="ltx_text ltx_font_bold">89.6</span></td>
</tr>
<tr id="S4.T5.3.3.9" class="ltx_tr">
<td id="S4.T5.3.3.9.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">Best Results</td>
<td id="S4.T5.3.3.9.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">MS Transformer</td>
<td id="S4.T5.3.3.9.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">MS Transformer</td>
</tr>
</table>
</span></div>
</figure>
<figure id="S4.F16" class="ltx_figure"><img src="/html/2309.04902/assets/Figures/URPC.jpg" id="S4.F16.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="586" height="112" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F16.3.1.1" class="ltx_text" style="font-size:90%;">Figure 16</span>: </span><span id="S4.F16.4.2" class="ltx_text" style="font-size:90%;">Examples of low quality images in URPC2018 image dataset.</span></figcaption>
</figure>
<figure id="S4.F17" class="ltx_figure"><img src="/html/2309.04902/assets/x18.png" id="S4.F17.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="664" height="631" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F17.3.1.1" class="ltx_text" style="font-size:90%;">Figure 17</span>: </span><span id="S4.F17.4.2" class="ltx_text" style="font-size:90%;">Examples of detection results on AMMW image dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite> for SOTA small object detectors (figure from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>).</span></figcaption>
</figure>
</section>
<section id="S4.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.4 </span>Small Object Detection in Underwater Images</h4>

<div id="S4.SS2.SSS4.p1" class="ltx_para">
<p id="S4.SS2.SSS4.p1.1" class="ltx_p">With the growth of underwater activities, the demand to monitor hazy and low-light environments has increased for purposes like ecological surveillance, equipment maintenance, and monitoring of wreck fishing. Factors like scattering and light absorption of the water, make the SOD task even more challenging. Example images of such challenging environments are displayed in Figure <a href="#S4.F16" title="Figure 16 ‣ 4.2.3 Small Object Detection in Medical Images ‣ 4.2 Benchmarks in Vision Applications ‣ 4 Results and Benchmarks ‣ Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16</span></a>. Transformer-based detection methods should not only be adept at identifying small objects but also need to be robust against the poor image quality found in deep waters, as well as variations in color channels due to differing rates of light attenuation for each channel.</p>
</div>
<div id="S4.SS2.SSS4.p2" class="ltx_para">
<p id="S4.SS2.SSS4.p2.1" class="ltx_p">Table <a href="#S4.T6" title="TABLE VI ‣ 4.2.4 Small Object Detection in Underwater Images ‣ 4.2 Benchmarks in Vision Applications ‣ 4 Results and Benchmarks ‣ Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a> shows the performance metrics reported in existing studies for this dataset (results are compiled from their papers). HTDet is the sole transformer-based technique identified for this specific application. It significantly outperforms the SOTA CNN-based method by a huge margin (3.4<math id="S4.SS2.SSS4.p2.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS2.SSS4.p2.1.m1.1a"><mo id="S4.SS2.SSS4.p2.1.m1.1.1" xref="S4.SS2.SSS4.p2.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p2.1.m1.1b"><csymbol cd="latexml" id="S4.SS2.SSS4.p2.1.m1.1.1.cmml" xref="S4.SS2.SSS4.p2.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p2.1.m1.1c">\%</annotation></semantics></math> in mAP). However, the relatively low mAP scores confirm that object detection in underwater images remains a difficult task. It is worth noting that the training set of the URPC 2018 contains 2901 labeled images, and the testing set contains 800 unlabeled images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite>.</p>
</div>
<figure id="S4.T6" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T6.8.1.1" class="ltx_text" style="font-size:90%;">TABLE VI</span>: </span><span id="S4.T6.9.2" class="ltx_text" style="font-size:90%;">Detection performance (%) for URPC2018 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib121" title="" class="ltx_ref">121</a>]</cite>. The top section shows results for CNN-based techniques, the middle section shows results for mixed architectures. </span></figcaption>
<div id="S4.T6.6" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:363.8pt;height:122.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-32.1pt,10.8pt) scale(0.85,0.85) ;">
<table id="S4.T6.6.6" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.6.6.6" class="ltx_tr">
<td id="S4.T6.6.6.6.7" class="ltx_td ltx_align_left ltx_border_tt">Model</td>
<td id="S4.T6.2.2.2.2" class="ltx_td ltx_align_center ltx_border_tt">
<math id="S4.T6.1.1.1.1.m1.1" class="ltx_Math" alttext="\#" display="inline"><semantics id="S4.T6.1.1.1.1.m1.1a"><mi mathvariant="normal" id="S4.T6.1.1.1.1.m1.1.1" xref="S4.T6.1.1.1.1.m1.1.1.cmml">#</mi><annotation-xml encoding="MathML-Content" id="S4.T6.1.1.1.1.m1.1b"><ci id="S4.T6.1.1.1.1.m1.1.1.cmml" xref="S4.T6.1.1.1.1.m1.1.1">#</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.1.1.1.1.m1.1c">\#</annotation></semantics></math>Params<math id="S4.T6.2.2.2.2.m2.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T6.2.2.2.2.m2.1a"><mo stretchy="false" id="S4.T6.2.2.2.2.m2.1.1" xref="S4.T6.2.2.2.2.m2.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T6.2.2.2.2.m2.1b"><ci id="S4.T6.2.2.2.2.m2.1.1.cmml" xref="S4.T6.2.2.2.2.m2.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.2.2.2.2.m2.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T6.4.4.4.4" class="ltx_td ltx_align_center ltx_border_tt">
<math id="S4.T6.3.3.3.3.m1.2" class="ltx_Math" alttext="\text{mAP}^{@[0.5,0.95]}" display="inline"><semantics id="S4.T6.3.3.3.3.m1.2a"><msup id="S4.T6.3.3.3.3.m1.2.3" xref="S4.T6.3.3.3.3.m1.2.3.cmml"><mtext id="S4.T6.3.3.3.3.m1.2.3.2" xref="S4.T6.3.3.3.3.m1.2.3.2a.cmml">mAP</mtext><mrow id="S4.T6.3.3.3.3.m1.2.2.2" xref="S4.T6.3.3.3.3.m1.2.2.2.cmml"><mi mathvariant="normal" id="S4.T6.3.3.3.3.m1.2.2.2.4" xref="S4.T6.3.3.3.3.m1.2.2.2.4.cmml">@</mi><mo lspace="0em" rspace="0em" id="S4.T6.3.3.3.3.m1.2.2.2.3" xref="S4.T6.3.3.3.3.m1.2.2.2.3.cmml">​</mo><mrow id="S4.T6.3.3.3.3.m1.2.2.2.5.2" xref="S4.T6.3.3.3.3.m1.2.2.2.5.1.cmml"><mo stretchy="false" id="S4.T6.3.3.3.3.m1.2.2.2.5.2.1" xref="S4.T6.3.3.3.3.m1.2.2.2.5.1.cmml">[</mo><mn id="S4.T6.3.3.3.3.m1.1.1.1.1" xref="S4.T6.3.3.3.3.m1.1.1.1.1.cmml">0.5</mn><mo id="S4.T6.3.3.3.3.m1.2.2.2.5.2.2" xref="S4.T6.3.3.3.3.m1.2.2.2.5.1.cmml">,</mo><mn id="S4.T6.3.3.3.3.m1.2.2.2.2" xref="S4.T6.3.3.3.3.m1.2.2.2.2.cmml">0.95</mn><mo stretchy="false" id="S4.T6.3.3.3.3.m1.2.2.2.5.2.3" xref="S4.T6.3.3.3.3.m1.2.2.2.5.1.cmml">]</mo></mrow></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.T6.3.3.3.3.m1.2b"><apply id="S4.T6.3.3.3.3.m1.2.3.cmml" xref="S4.T6.3.3.3.3.m1.2.3"><csymbol cd="ambiguous" id="S4.T6.3.3.3.3.m1.2.3.1.cmml" xref="S4.T6.3.3.3.3.m1.2.3">superscript</csymbol><ci id="S4.T6.3.3.3.3.m1.2.3.2a.cmml" xref="S4.T6.3.3.3.3.m1.2.3.2"><mtext id="S4.T6.3.3.3.3.m1.2.3.2.cmml" xref="S4.T6.3.3.3.3.m1.2.3.2">mAP</mtext></ci><apply id="S4.T6.3.3.3.3.m1.2.2.2.cmml" xref="S4.T6.3.3.3.3.m1.2.2.2"><times id="S4.T6.3.3.3.3.m1.2.2.2.3.cmml" xref="S4.T6.3.3.3.3.m1.2.2.2.3"></times><ci id="S4.T6.3.3.3.3.m1.2.2.2.4.cmml" xref="S4.T6.3.3.3.3.m1.2.2.2.4">@</ci><interval closure="closed" id="S4.T6.3.3.3.3.m1.2.2.2.5.1.cmml" xref="S4.T6.3.3.3.3.m1.2.2.2.5.2"><cn type="float" id="S4.T6.3.3.3.3.m1.1.1.1.1.cmml" xref="S4.T6.3.3.3.3.m1.1.1.1.1">0.5</cn><cn type="float" id="S4.T6.3.3.3.3.m1.2.2.2.2.cmml" xref="S4.T6.3.3.3.3.m1.2.2.2.2">0.95</cn></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.3.3.3.3.m1.2c">\text{mAP}^{@[0.5,0.95]}</annotation></semantics></math> <math id="S4.T6.4.4.4.4.m2.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T6.4.4.4.4.m2.1a"><mo stretchy="false" id="S4.T6.4.4.4.4.m2.1.1" xref="S4.T6.4.4.4.4.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T6.4.4.4.4.m2.1b"><ci id="S4.T6.4.4.4.4.m2.1.1.cmml" xref="S4.T6.4.4.4.4.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.4.4.4.4.m2.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T6.6.6.6.6" class="ltx_td ltx_align_center ltx_border_tt">
<math id="S4.T6.5.5.5.5.m1.1" class="ltx_Math" alttext="\text{mAP}^{0.5}" display="inline"><semantics id="S4.T6.5.5.5.5.m1.1a"><msup id="S4.T6.5.5.5.5.m1.1.1" xref="S4.T6.5.5.5.5.m1.1.1.cmml"><mtext id="S4.T6.5.5.5.5.m1.1.1.2" xref="S4.T6.5.5.5.5.m1.1.1.2a.cmml">mAP</mtext><mn id="S4.T6.5.5.5.5.m1.1.1.3" xref="S4.T6.5.5.5.5.m1.1.1.3.cmml">0.5</mn></msup><annotation-xml encoding="MathML-Content" id="S4.T6.5.5.5.5.m1.1b"><apply id="S4.T6.5.5.5.5.m1.1.1.cmml" xref="S4.T6.5.5.5.5.m1.1.1"><csymbol cd="ambiguous" id="S4.T6.5.5.5.5.m1.1.1.1.cmml" xref="S4.T6.5.5.5.5.m1.1.1">superscript</csymbol><ci id="S4.T6.5.5.5.5.m1.1.1.2a.cmml" xref="S4.T6.5.5.5.5.m1.1.1.2"><mtext id="S4.T6.5.5.5.5.m1.1.1.2.cmml" xref="S4.T6.5.5.5.5.m1.1.1.2">mAP</mtext></ci><cn type="float" id="S4.T6.5.5.5.5.m1.1.1.3.cmml" xref="S4.T6.5.5.5.5.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.5.5.5.5.m1.1c">\text{mAP}^{0.5}</annotation></semantics></math> <math id="S4.T6.6.6.6.6.m2.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T6.6.6.6.6.m2.1a"><mo stretchy="false" id="S4.T6.6.6.6.6.m2.1.1" xref="S4.T6.6.6.6.6.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T6.6.6.6.6.m2.1b"><ci id="S4.T6.6.6.6.6.m2.1.1.cmml" xref="S4.T6.6.6.6.6.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.6.6.6.6.m2.1c">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T6.6.6.7" class="ltx_tr">
<td id="S4.T6.6.6.7.1" class="ltx_td ltx_align_left ltx_border_t">Faster RCNN (NeurIPS2015)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>
</td>
<td id="S4.T6.6.6.7.2" class="ltx_td ltx_align_center ltx_border_t">33.6M</td>
<td id="S4.T6.6.6.7.3" class="ltx_td ltx_align_center ltx_border_t">16.4</td>
<td id="S4.T6.6.6.7.4" class="ltx_td ltx_align_center ltx_border_t">–</td>
</tr>
<tr id="S4.T6.6.6.8" class="ltx_tr">
<td id="S4.T6.6.6.8.1" class="ltx_td ltx_align_left">Cascade RCNN (CVPR2018)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>
</td>
<td id="S4.T6.6.6.8.2" class="ltx_td ltx_align_center">68.9M</td>
<td id="S4.T6.6.6.8.3" class="ltx_td ltx_align_center">16</td>
<td id="S4.T6.6.6.8.4" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S4.T6.6.6.9" class="ltx_tr">
<td id="S4.T6.6.6.9.1" class="ltx_td ltx_align_left">Dynamic RCNN (ECCV2020) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib127" title="" class="ltx_ref">127</a>]</cite>
</td>
<td id="S4.T6.6.6.9.2" class="ltx_td ltx_align_center">41.5M</td>
<td id="S4.T6.6.6.9.3" class="ltx_td ltx_align_center">13.3</td>
<td id="S4.T6.6.6.9.4" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S4.T6.6.6.10" class="ltx_tr">
<td id="S4.T6.6.6.10.1" class="ltx_td ltx_align_left">Yolov3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>
</td>
<td id="S4.T6.6.6.10.2" class="ltx_td ltx_align_center">61.5M</td>
<td id="S4.T6.6.6.10.3" class="ltx_td ltx_align_center">19.4</td>
<td id="S4.T6.6.6.10.4" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S4.T6.6.6.11" class="ltx_tr">
<td id="S4.T6.6.6.11.1" class="ltx_td ltx_align_left">RoIMix (ICASSP2020) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib121" title="" class="ltx_ref">121</a>]</cite>
</td>
<td id="S4.T6.6.6.11.2" class="ltx_td ltx_align_center">–</td>
<td id="S4.T6.6.6.11.3" class="ltx_td ltx_align_center">–</td>
<td id="S4.T6.6.6.11.4" class="ltx_td ltx_align_center"><span id="S4.T6.6.6.11.4.1" class="ltx_text ltx_font_bold">74.92</span></td>
</tr>
<tr id="S4.T6.6.6.12" class="ltx_tr">
<td id="S4.T6.6.6.12.1" class="ltx_td ltx_align_left ltx_border_t">HTDet (RS2023) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite>
</td>
<td id="S4.T6.6.6.12.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.6.6.12.2.1" class="ltx_text ltx_font_bold">7.7M</span></td>
<td id="S4.T6.6.6.12.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.6.6.12.3.1" class="ltx_text ltx_font_bold">22.8</span></td>
<td id="S4.T6.6.6.12.4" class="ltx_td ltx_align_center ltx_border_t">–</td>
</tr>
<tr id="S4.T6.6.6.13" class="ltx_tr">
<td id="S4.T6.6.6.13.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">Best Results</td>
<td id="S4.T6.6.6.13.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">HTDet</td>
<td id="S4.T6.6.6.13.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">HTDet</td>
<td id="S4.T6.6.6.13.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">RoIMix</td>
</tr>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS2.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.5 </span>Small Object Detection in Active Milli-Meter Wave Images</h4>

<div id="S4.SS2.SSS5.p1" class="ltx_para">
<p id="S4.SS2.SSS5.p1.1" class="ltx_p">Small objects can easily be concealed or hidden from normal RGB cameras, for example, within a person’s clothing at an airport. Therefore, active imaging techniques are essential for security purposes. In these scenarios, multiple images are often captured from different angles to enhance the likelihood of detecting even minuscule objects. Interestingly, much like in the field of medical imaging, transformers are rarely used for this particular application.</p>
</div>
<div id="S4.SS2.SSS5.p2" class="ltx_para">
<p id="S4.SS2.SSS5.p2.4" class="ltx_p">In our study, we focused on the detection performance of existing techniques using the AMMW Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite> as shown in Table <a href="#S4.T7" title="TABLE VII ‣ 4.2.5 Small Object Detection in Active Milli-Meter Wave Images ‣ 4.2 Benchmarks in Vision Applications ‣ 4 Results and Benchmarks ‣ Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VII</span></a> (results are compiled from their papers). We have identified that MATR emerged as the sole technique that combines transformer and CNNs for this dataset. Despite being the only transformer-based technique, it could significantly improve the SOD performance (5.49<math id="S4.SS2.SSS5.p2.1.m1.1" class="ltx_math_unparsed" alttext="\%\uparrow" display="inline"><semantics id="S4.SS2.SSS5.p2.1.m1.1a"><mrow id="S4.SS2.SSS5.p2.1.m1.1b"><mo id="S4.SS2.SSS5.p2.1.m1.1.1">%</mo><mo stretchy="false" id="S4.SS2.SSS5.p2.1.m1.1.2">↑</mo></mrow><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p2.1.m1.1c">\%\uparrow</annotation></semantics></math> in <math id="S4.SS2.SSS5.p2.2.m2.1" class="ltx_Math" alttext="\text{mAP}^{0.5}" display="inline"><semantics id="S4.SS2.SSS5.p2.2.m2.1a"><msup id="S4.SS2.SSS5.p2.2.m2.1.1" xref="S4.SS2.SSS5.p2.2.m2.1.1.cmml"><mtext id="S4.SS2.SSS5.p2.2.m2.1.1.2" xref="S4.SS2.SSS5.p2.2.m2.1.1.2a.cmml">mAP</mtext><mn id="S4.SS2.SSS5.p2.2.m2.1.1.3" xref="S4.SS2.SSS5.p2.2.m2.1.1.3.cmml">0.5</mn></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p2.2.m2.1b"><apply id="S4.SS2.SSS5.p2.2.m2.1.1.cmml" xref="S4.SS2.SSS5.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS5.p2.2.m2.1.1.1.cmml" xref="S4.SS2.SSS5.p2.2.m2.1.1">superscript</csymbol><ci id="S4.SS2.SSS5.p2.2.m2.1.1.2a.cmml" xref="S4.SS2.SSS5.p2.2.m2.1.1.2"><mtext id="S4.SS2.SSS5.p2.2.m2.1.1.2.cmml" xref="S4.SS2.SSS5.p2.2.m2.1.1.2">mAP</mtext></ci><cn type="float" id="S4.SS2.SSS5.p2.2.m2.1.1.3.cmml" xref="S4.SS2.SSS5.p2.2.m2.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p2.2.m2.1c">\text{mAP}^{0.5}</annotation></semantics></math> with respect to Yolov5 and 4.22 <math id="S4.SS2.SSS5.p2.3.m3.1" class="ltx_math_unparsed" alttext="\%\uparrow" display="inline"><semantics id="S4.SS2.SSS5.p2.3.m3.1a"><mrow id="S4.SS2.SSS5.p2.3.m3.1b"><mo id="S4.SS2.SSS5.p2.3.m3.1.1">%</mo><mo stretchy="false" id="S4.SS2.SSS5.p2.3.m3.1.2">↑</mo></mrow><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p2.3.m3.1c">\%\uparrow</annotation></semantics></math> in <math id="S4.SS2.SSS5.p2.4.m4.2" class="ltx_Math" alttext="\text{mAP}^{@[0.5,0.95]}" display="inline"><semantics id="S4.SS2.SSS5.p2.4.m4.2a"><msup id="S4.SS2.SSS5.p2.4.m4.2.3" xref="S4.SS2.SSS5.p2.4.m4.2.3.cmml"><mtext id="S4.SS2.SSS5.p2.4.m4.2.3.2" xref="S4.SS2.SSS5.p2.4.m4.2.3.2a.cmml">mAP</mtext><mrow id="S4.SS2.SSS5.p2.4.m4.2.2.2" xref="S4.SS2.SSS5.p2.4.m4.2.2.2.cmml"><mi mathvariant="normal" id="S4.SS2.SSS5.p2.4.m4.2.2.2.4" xref="S4.SS2.SSS5.p2.4.m4.2.2.2.4.cmml">@</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS5.p2.4.m4.2.2.2.3" xref="S4.SS2.SSS5.p2.4.m4.2.2.2.3.cmml">​</mo><mrow id="S4.SS2.SSS5.p2.4.m4.2.2.2.5.2" xref="S4.SS2.SSS5.p2.4.m4.2.2.2.5.1.cmml"><mo stretchy="false" id="S4.SS2.SSS5.p2.4.m4.2.2.2.5.2.1" xref="S4.SS2.SSS5.p2.4.m4.2.2.2.5.1.cmml">[</mo><mn id="S4.SS2.SSS5.p2.4.m4.1.1.1.1" xref="S4.SS2.SSS5.p2.4.m4.1.1.1.1.cmml">0.5</mn><mo id="S4.SS2.SSS5.p2.4.m4.2.2.2.5.2.2" xref="S4.SS2.SSS5.p2.4.m4.2.2.2.5.1.cmml">,</mo><mn id="S4.SS2.SSS5.p2.4.m4.2.2.2.2" xref="S4.SS2.SSS5.p2.4.m4.2.2.2.2.cmml">0.95</mn><mo stretchy="false" id="S4.SS2.SSS5.p2.4.m4.2.2.2.5.2.3" xref="S4.SS2.SSS5.p2.4.m4.2.2.2.5.1.cmml">]</mo></mrow></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p2.4.m4.2b"><apply id="S4.SS2.SSS5.p2.4.m4.2.3.cmml" xref="S4.SS2.SSS5.p2.4.m4.2.3"><csymbol cd="ambiguous" id="S4.SS2.SSS5.p2.4.m4.2.3.1.cmml" xref="S4.SS2.SSS5.p2.4.m4.2.3">superscript</csymbol><ci id="S4.SS2.SSS5.p2.4.m4.2.3.2a.cmml" xref="S4.SS2.SSS5.p2.4.m4.2.3.2"><mtext id="S4.SS2.SSS5.p2.4.m4.2.3.2.cmml" xref="S4.SS2.SSS5.p2.4.m4.2.3.2">mAP</mtext></ci><apply id="S4.SS2.SSS5.p2.4.m4.2.2.2.cmml" xref="S4.SS2.SSS5.p2.4.m4.2.2.2"><times id="S4.SS2.SSS5.p2.4.m4.2.2.2.3.cmml" xref="S4.SS2.SSS5.p2.4.m4.2.2.2.3"></times><ci id="S4.SS2.SSS5.p2.4.m4.2.2.2.4.cmml" xref="S4.SS2.SSS5.p2.4.m4.2.2.2.4">@</ci><interval closure="closed" id="S4.SS2.SSS5.p2.4.m4.2.2.2.5.1.cmml" xref="S4.SS2.SSS5.p2.4.m4.2.2.2.5.2"><cn type="float" id="S4.SS2.SSS5.p2.4.m4.1.1.1.1.cmml" xref="S4.SS2.SSS5.p2.4.m4.1.1.1.1">0.5</cn><cn type="float" id="S4.SS2.SSS5.p2.4.m4.2.2.2.2.cmml" xref="S4.SS2.SSS5.p2.4.m4.2.2.2.2">0.95</cn></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p2.4.m4.2c">\text{mAP}^{@[0.5,0.95]}</annotation></semantics></math> with respect to TridentNet) with the same backbone (ResNet50). Figure <a href="#S4.F17" title="Figure 17 ‣ 4.2.3 Small Object Detection in Medical Images ‣ 4.2 Benchmarks in Vision Applications ‣ 4 Results and Benchmarks ‣ Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">17</span></a> visually compares MATR with other SOTA CNN-based techniques. Combining images from different angles largely helps to identify even small objects within this imaging approach. For training and testing, 35426 and 4019 images were used, respectively <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>.</p>
</div>
<figure id="S4.T7" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T7.6.1.1" class="ltx_text" style="font-size:90%;">TABLE VII</span>: </span><span id="S4.T7.7.2" class="ltx_text" style="font-size:90%;">Detection performance (%) for AMWW image dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite>. The top section shows results for CNN-based techniques, the middle section shows results for mixed architectures. </span></figcaption>
<div id="S4.T7.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:368.9pt;height:125.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-27.6pt,9.4pt) scale(0.87,0.87) ;">
<table id="S4.T7.4.4" class="ltx_tabular ltx_align_middle">
<tr id="S4.T7.4.4.4" class="ltx_tr">
<td id="S4.T7.4.4.4.5" class="ltx_td ltx_align_left ltx_border_tt">Model</td>
<td id="S4.T7.4.4.4.6" class="ltx_td ltx_align_center ltx_border_tt">Backbone</td>
<td id="S4.T7.2.2.2.2" class="ltx_td ltx_align_center ltx_border_tt">
<math id="S4.T7.1.1.1.1.m1.1" class="ltx_Math" alttext="\text{mAP}^{0.5}" display="inline"><semantics id="S4.T7.1.1.1.1.m1.1a"><msup id="S4.T7.1.1.1.1.m1.1.1" xref="S4.T7.1.1.1.1.m1.1.1.cmml"><mtext id="S4.T7.1.1.1.1.m1.1.1.2" xref="S4.T7.1.1.1.1.m1.1.1.2a.cmml">mAP</mtext><mn id="S4.T7.1.1.1.1.m1.1.1.3" xref="S4.T7.1.1.1.1.m1.1.1.3.cmml">0.5</mn></msup><annotation-xml encoding="MathML-Content" id="S4.T7.1.1.1.1.m1.1b"><apply id="S4.T7.1.1.1.1.m1.1.1.cmml" xref="S4.T7.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T7.1.1.1.1.m1.1.1.1.cmml" xref="S4.T7.1.1.1.1.m1.1.1">superscript</csymbol><ci id="S4.T7.1.1.1.1.m1.1.1.2a.cmml" xref="S4.T7.1.1.1.1.m1.1.1.2"><mtext id="S4.T7.1.1.1.1.m1.1.1.2.cmml" xref="S4.T7.1.1.1.1.m1.1.1.2">mAP</mtext></ci><cn type="float" id="S4.T7.1.1.1.1.m1.1.1.3.cmml" xref="S4.T7.1.1.1.1.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.1.1.1.1.m1.1c">\text{mAP}^{0.5}</annotation></semantics></math> <math id="S4.T7.2.2.2.2.m2.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T7.2.2.2.2.m2.1a"><mo stretchy="false" id="S4.T7.2.2.2.2.m2.1.1" xref="S4.T7.2.2.2.2.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T7.2.2.2.2.m2.1b"><ci id="S4.T7.2.2.2.2.m2.1.1.cmml" xref="S4.T7.2.2.2.2.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.2.2.2.2.m2.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T7.4.4.4.4" class="ltx_td ltx_align_center ltx_border_tt">
<math id="S4.T7.3.3.3.3.m1.2" class="ltx_Math" alttext="\text{mAP}^{@[0.5,0.95]}" display="inline"><semantics id="S4.T7.3.3.3.3.m1.2a"><msup id="S4.T7.3.3.3.3.m1.2.3" xref="S4.T7.3.3.3.3.m1.2.3.cmml"><mtext id="S4.T7.3.3.3.3.m1.2.3.2" xref="S4.T7.3.3.3.3.m1.2.3.2a.cmml">mAP</mtext><mrow id="S4.T7.3.3.3.3.m1.2.2.2" xref="S4.T7.3.3.3.3.m1.2.2.2.cmml"><mi mathvariant="normal" id="S4.T7.3.3.3.3.m1.2.2.2.4" xref="S4.T7.3.3.3.3.m1.2.2.2.4.cmml">@</mi><mo lspace="0em" rspace="0em" id="S4.T7.3.3.3.3.m1.2.2.2.3" xref="S4.T7.3.3.3.3.m1.2.2.2.3.cmml">​</mo><mrow id="S4.T7.3.3.3.3.m1.2.2.2.5.2" xref="S4.T7.3.3.3.3.m1.2.2.2.5.1.cmml"><mo stretchy="false" id="S4.T7.3.3.3.3.m1.2.2.2.5.2.1" xref="S4.T7.3.3.3.3.m1.2.2.2.5.1.cmml">[</mo><mn id="S4.T7.3.3.3.3.m1.1.1.1.1" xref="S4.T7.3.3.3.3.m1.1.1.1.1.cmml">0.5</mn><mo id="S4.T7.3.3.3.3.m1.2.2.2.5.2.2" xref="S4.T7.3.3.3.3.m1.2.2.2.5.1.cmml">,</mo><mn id="S4.T7.3.3.3.3.m1.2.2.2.2" xref="S4.T7.3.3.3.3.m1.2.2.2.2.cmml">0.95</mn><mo stretchy="false" id="S4.T7.3.3.3.3.m1.2.2.2.5.2.3" xref="S4.T7.3.3.3.3.m1.2.2.2.5.1.cmml">]</mo></mrow></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.T7.3.3.3.3.m1.2b"><apply id="S4.T7.3.3.3.3.m1.2.3.cmml" xref="S4.T7.3.3.3.3.m1.2.3"><csymbol cd="ambiguous" id="S4.T7.3.3.3.3.m1.2.3.1.cmml" xref="S4.T7.3.3.3.3.m1.2.3">superscript</csymbol><ci id="S4.T7.3.3.3.3.m1.2.3.2a.cmml" xref="S4.T7.3.3.3.3.m1.2.3.2"><mtext id="S4.T7.3.3.3.3.m1.2.3.2.cmml" xref="S4.T7.3.3.3.3.m1.2.3.2">mAP</mtext></ci><apply id="S4.T7.3.3.3.3.m1.2.2.2.cmml" xref="S4.T7.3.3.3.3.m1.2.2.2"><times id="S4.T7.3.3.3.3.m1.2.2.2.3.cmml" xref="S4.T7.3.3.3.3.m1.2.2.2.3"></times><ci id="S4.T7.3.3.3.3.m1.2.2.2.4.cmml" xref="S4.T7.3.3.3.3.m1.2.2.2.4">@</ci><interval closure="closed" id="S4.T7.3.3.3.3.m1.2.2.2.5.1.cmml" xref="S4.T7.3.3.3.3.m1.2.2.2.5.2"><cn type="float" id="S4.T7.3.3.3.3.m1.1.1.1.1.cmml" xref="S4.T7.3.3.3.3.m1.1.1.1.1">0.5</cn><cn type="float" id="S4.T7.3.3.3.3.m1.2.2.2.2.cmml" xref="S4.T7.3.3.3.3.m1.2.2.2.2">0.95</cn></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.3.3.3.3.m1.2c">\text{mAP}^{@[0.5,0.95]}</annotation></semantics></math> <math id="S4.T7.4.4.4.4.m2.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T7.4.4.4.4.m2.1a"><mo stretchy="false" id="S4.T7.4.4.4.4.m2.1.1" xref="S4.T7.4.4.4.4.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T7.4.4.4.4.m2.1b"><ci id="S4.T7.4.4.4.4.m2.1.1.cmml" xref="S4.T7.4.4.4.4.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.4.4.4.4.m2.1c">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T7.4.4.5" class="ltx_tr">
<td id="S4.T7.4.4.5.1" class="ltx_td ltx_align_left ltx_border_t">Faster RCNN (NeurIPS2015)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>
</td>
<td id="S4.T7.4.4.5.2" class="ltx_td ltx_align_center ltx_border_t">ResNet50</td>
<td id="S4.T7.4.4.5.3" class="ltx_td ltx_align_center ltx_border_t">70.7</td>
<td id="S4.T7.4.4.5.4" class="ltx_td ltx_align_center ltx_border_t">26.83</td>
</tr>
<tr id="S4.T7.4.4.6" class="ltx_tr">
<td id="S4.T7.4.4.6.1" class="ltx_td ltx_align_left">Cascade RCNN (CVPR2018)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>
</td>
<td id="S4.T7.4.4.6.2" class="ltx_td ltx_align_center">ResNet50</td>
<td id="S4.T7.4.4.6.3" class="ltx_td ltx_align_center">74.7</td>
<td id="S4.T7.4.4.6.4" class="ltx_td ltx_align_center">27.8</td>
</tr>
<tr id="S4.T7.4.4.7" class="ltx_tr">
<td id="S4.T7.4.4.7.1" class="ltx_td ltx_align_left">TridentNet (ICCV2019) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib128" title="" class="ltx_ref">128</a>]</cite>
</td>
<td id="S4.T7.4.4.7.2" class="ltx_td ltx_align_center">ResNet50</td>
<td id="S4.T7.4.4.7.3" class="ltx_td ltx_align_center">77.3</td>
<td id="S4.T7.4.4.7.4" class="ltx_td ltx_align_center">29.2</td>
</tr>
<tr id="S4.T7.4.4.8" class="ltx_tr">
<td id="S4.T7.4.4.8.1" class="ltx_td ltx_align_left">Dynamic RCNN (ECCV2020) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib127" title="" class="ltx_ref">127</a>]</cite>
</td>
<td id="S4.T7.4.4.8.2" class="ltx_td ltx_align_center">ResNet50</td>
<td id="S4.T7.4.4.8.3" class="ltx_td ltx_align_center">76.3</td>
<td id="S4.T7.4.4.8.4" class="ltx_td ltx_align_center">27.6</td>
</tr>
<tr id="S4.T7.4.4.9" class="ltx_tr">
<td id="S4.T7.4.4.9.1" class="ltx_td ltx_align_left">Yolov5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>
</td>
<td id="S4.T7.4.4.9.2" class="ltx_td ltx_align_center">ResNet50</td>
<td id="S4.T7.4.4.9.3" class="ltx_td ltx_align_center">76.67</td>
<td id="S4.T7.4.4.9.4" class="ltx_td ltx_align_center">28.48</td>
</tr>
<tr id="S4.T7.4.4.10" class="ltx_tr">
<td id="S4.T7.4.4.10.1" class="ltx_td ltx_align_left ltx_border_t">MATR (TCSVT2022) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>
</td>
<td id="S4.T7.4.4.10.2" class="ltx_td ltx_align_center ltx_border_t">ResNet50</td>
<td id="S4.T7.4.4.10.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.4.4.10.3.1" class="ltx_text ltx_font_bold">82.16</span></td>
<td id="S4.T7.4.4.10.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.4.4.10.4.1" class="ltx_text ltx_font_bold">33.42</span></td>
</tr>
<tr id="S4.T7.4.4.11" class="ltx_tr">
<td id="S4.T7.4.4.11.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">Best Results</td>
<td id="S4.T7.4.4.11.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">NA</td>
<td id="S4.T7.4.4.11.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">MATR</td>
<td id="S4.T7.4.4.11.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">MATR</td>
</tr>
</table>
</span></div>
</figure>
<figure id="S4.T8" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T8.4.1.1" class="ltx_text" style="font-size:90%;">TABLE VIII</span>: </span><span id="S4.T8.5.2" class="ltx_text" style="font-size:90%;">Detection performance (%) for ImageNet VID dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite> for small objects. The top section shows results for CNN-based techniques, the middle section shows results for mixed architectures. PT: Pre-trained on MS COCO. </span></figcaption>
<div id="S4.T8.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:455.2pt;height:104.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-46.6pt,10.7pt) scale(0.83,0.83) ;">
<table id="S4.T8.2.2" class="ltx_tabular ltx_align_middle">
<tr id="S4.T8.2.2.2" class="ltx_tr">
<td id="S4.T8.2.2.2.3" class="ltx_td ltx_align_left ltx_border_tt">Model</td>
<td id="S4.T8.2.2.2.4" class="ltx_td ltx_align_center ltx_border_tt">Backbone</td>
<td id="S4.T8.2.2.2.2" class="ltx_td ltx_align_center ltx_border_tt">
<math id="S4.T8.1.1.1.1.m1.2" class="ltx_Math" alttext="\text{mAP}^{@[0.5,0.95]}" display="inline"><semantics id="S4.T8.1.1.1.1.m1.2a"><msup id="S4.T8.1.1.1.1.m1.2.3" xref="S4.T8.1.1.1.1.m1.2.3.cmml"><mtext id="S4.T8.1.1.1.1.m1.2.3.2" xref="S4.T8.1.1.1.1.m1.2.3.2a.cmml">mAP</mtext><mrow id="S4.T8.1.1.1.1.m1.2.2.2" xref="S4.T8.1.1.1.1.m1.2.2.2.cmml"><mi mathvariant="normal" id="S4.T8.1.1.1.1.m1.2.2.2.4" xref="S4.T8.1.1.1.1.m1.2.2.2.4.cmml">@</mi><mo lspace="0em" rspace="0em" id="S4.T8.1.1.1.1.m1.2.2.2.3" xref="S4.T8.1.1.1.1.m1.2.2.2.3.cmml">​</mo><mrow id="S4.T8.1.1.1.1.m1.2.2.2.5.2" xref="S4.T8.1.1.1.1.m1.2.2.2.5.1.cmml"><mo stretchy="false" id="S4.T8.1.1.1.1.m1.2.2.2.5.2.1" xref="S4.T8.1.1.1.1.m1.2.2.2.5.1.cmml">[</mo><mn id="S4.T8.1.1.1.1.m1.1.1.1.1" xref="S4.T8.1.1.1.1.m1.1.1.1.1.cmml">0.5</mn><mo id="S4.T8.1.1.1.1.m1.2.2.2.5.2.2" xref="S4.T8.1.1.1.1.m1.2.2.2.5.1.cmml">,</mo><mn id="S4.T8.1.1.1.1.m1.2.2.2.2" xref="S4.T8.1.1.1.1.m1.2.2.2.2.cmml">0.95</mn><mo stretchy="false" id="S4.T8.1.1.1.1.m1.2.2.2.5.2.3" xref="S4.T8.1.1.1.1.m1.2.2.2.5.1.cmml">]</mo></mrow></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.T8.1.1.1.1.m1.2b"><apply id="S4.T8.1.1.1.1.m1.2.3.cmml" xref="S4.T8.1.1.1.1.m1.2.3"><csymbol cd="ambiguous" id="S4.T8.1.1.1.1.m1.2.3.1.cmml" xref="S4.T8.1.1.1.1.m1.2.3">superscript</csymbol><ci id="S4.T8.1.1.1.1.m1.2.3.2a.cmml" xref="S4.T8.1.1.1.1.m1.2.3.2"><mtext id="S4.T8.1.1.1.1.m1.2.3.2.cmml" xref="S4.T8.1.1.1.1.m1.2.3.2">mAP</mtext></ci><apply id="S4.T8.1.1.1.1.m1.2.2.2.cmml" xref="S4.T8.1.1.1.1.m1.2.2.2"><times id="S4.T8.1.1.1.1.m1.2.2.2.3.cmml" xref="S4.T8.1.1.1.1.m1.2.2.2.3"></times><ci id="S4.T8.1.1.1.1.m1.2.2.2.4.cmml" xref="S4.T8.1.1.1.1.m1.2.2.2.4">@</ci><interval closure="closed" id="S4.T8.1.1.1.1.m1.2.2.2.5.1.cmml" xref="S4.T8.1.1.1.1.m1.2.2.2.5.2"><cn type="float" id="S4.T8.1.1.1.1.m1.1.1.1.1.cmml" xref="S4.T8.1.1.1.1.m1.1.1.1.1">0.5</cn><cn type="float" id="S4.T8.1.1.1.1.m1.2.2.2.2.cmml" xref="S4.T8.1.1.1.1.m1.2.2.2.2">0.95</cn></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.1.1.1.1.m1.2c">\text{mAP}^{@[0.5,0.95]}</annotation></semantics></math> <math id="S4.T8.2.2.2.2.m2.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T8.2.2.2.2.m2.1a"><mo stretchy="false" id="S4.T8.2.2.2.2.m2.1.1" xref="S4.T8.2.2.2.2.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T8.2.2.2.2.m2.1b"><ci id="S4.T8.2.2.2.2.m2.1.1.cmml" xref="S4.T8.2.2.2.2.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.2.2.2.2.m2.1c">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T8.2.2.3" class="ltx_tr">
<td id="S4.T8.2.2.3.1" class="ltx_td ltx_align_left ltx_border_t">Faster RCNN (NeurIPS2015)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>+SELSA<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib129" title="" class="ltx_ref">129</a>]</cite>
</td>
<td id="S4.T8.2.2.3.2" class="ltx_td ltx_align_center ltx_border_t">ResNet50</td>
<td id="S4.T8.2.2.3.3" class="ltx_td ltx_align_center ltx_border_t">8.5</td>
</tr>
<tr id="S4.T8.2.2.4" class="ltx_tr">
<td id="S4.T8.2.2.4.1" class="ltx_td ltx_align_left ltx_border_t">Deformable-DETR-PT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>
</td>
<td id="S4.T8.2.2.4.2" class="ltx_td ltx_align_center ltx_border_t">ResNet50</td>
<td id="S4.T8.2.2.4.3" class="ltx_td ltx_align_center ltx_border_t">10.5</td>
</tr>
<tr id="S4.T8.2.2.5" class="ltx_tr">
<td id="S4.T8.2.2.5.1" class="ltx_td ltx_align_left">Deformable-DETR<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>+TransVOD-PT<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib103" title="" class="ltx_ref">103</a>]</cite>
</td>
<td id="S4.T8.2.2.5.2" class="ltx_td ltx_align_center">ResNet50</td>
<td id="S4.T8.2.2.5.3" class="ltx_td ltx_align_center">11</td>
</tr>
<tr id="S4.T8.2.2.6" class="ltx_tr">
<td id="S4.T8.2.2.6.1" class="ltx_td ltx_align_left">DAB-DETR<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>+FAQ-PT<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib108" title="" class="ltx_ref">108</a>]</cite>
</td>
<td id="S4.T8.2.2.6.2" class="ltx_td ltx_align_center">ResNet50</td>
<td id="S4.T8.2.2.6.3" class="ltx_td ltx_align_center">12</td>
</tr>
<tr id="S4.T8.2.2.7" class="ltx_tr">
<td id="S4.T8.2.2.7.1" class="ltx_td ltx_align_left">Deformable-DETR<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>+FAQ-PT<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib108" title="" class="ltx_ref">108</a>]</cite>
</td>
<td id="S4.T8.2.2.7.2" class="ltx_td ltx_align_center">ResNet50</td>
<td id="S4.T8.2.2.7.3" class="ltx_td ltx_align_center"><span id="S4.T8.2.2.7.3.1" class="ltx_text ltx_font_bold">13.2</span></td>
</tr>
<tr id="S4.T8.2.2.8" class="ltx_tr">
<td id="S4.T8.2.2.8.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">Best Results</td>
<td id="S4.T8.2.2.8.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">NA</td>
<td id="S4.T8.2.2.8.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">Deformable-DETR+FAQ</td>
</tr>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS2.SSS6" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.6 </span>Small Object Detection in Videos</h4>

<div id="S4.SS2.SSS6.p1" class="ltx_para">
<p id="S4.SS2.SSS6.p1.2" class="ltx_p">The field of object detection in videos gained considerable attention recently, as the temporal information in videos can improve the detection performance. To benchmark the SOTA techniques, the ImageNet VID dataset has been used with results specifically focused on the dataset’s small objects. This dataset includes 3862 training videos and 555 validation videos with 30 classes of objects. Table <a href="#S4.T8" title="TABLE VIII ‣ 4.2.5 Small Object Detection in Active Milli-Meter Wave Images ‣ 4.2 Benchmarks in Vision Applications ‣ 4 Results and Benchmarks ‣ Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VIII</span></a> reports the mAP of several recently developed transformer-based techniques (results are compiled from their papers). While transformers are increasingly being used in video object detection, their performance in SOD remains less explored. Among the methods that have reported SOD performance on the ImageNet VID dataset, Deformable DETR with FAQ stands out for achieving the highest performance- although it is notably low at 13.2 <math id="S4.SS2.SSS6.p1.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS2.SSS6.p1.1.m1.1a"><mo id="S4.SS2.SSS6.p1.1.m1.1.1" xref="S4.SS2.SSS6.p1.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS6.p1.1.m1.1b"><csymbol cd="latexml" id="S4.SS2.SSS6.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS6.p1.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS6.p1.1.m1.1c">\%</annotation></semantics></math> for <math id="S4.SS2.SSS6.p1.2.m2.2" class="ltx_Math" alttext="\text{mAP}^{@[0.5,0.95]}" display="inline"><semantics id="S4.SS2.SSS6.p1.2.m2.2a"><msup id="S4.SS2.SSS6.p1.2.m2.2.3" xref="S4.SS2.SSS6.p1.2.m2.2.3.cmml"><mtext id="S4.SS2.SSS6.p1.2.m2.2.3.2" xref="S4.SS2.SSS6.p1.2.m2.2.3.2a.cmml">mAP</mtext><mrow id="S4.SS2.SSS6.p1.2.m2.2.2.2" xref="S4.SS2.SSS6.p1.2.m2.2.2.2.cmml"><mi mathvariant="normal" id="S4.SS2.SSS6.p1.2.m2.2.2.2.4" xref="S4.SS2.SSS6.p1.2.m2.2.2.2.4.cmml">@</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS6.p1.2.m2.2.2.2.3" xref="S4.SS2.SSS6.p1.2.m2.2.2.2.3.cmml">​</mo><mrow id="S4.SS2.SSS6.p1.2.m2.2.2.2.5.2" xref="S4.SS2.SSS6.p1.2.m2.2.2.2.5.1.cmml"><mo stretchy="false" id="S4.SS2.SSS6.p1.2.m2.2.2.2.5.2.1" xref="S4.SS2.SSS6.p1.2.m2.2.2.2.5.1.cmml">[</mo><mn id="S4.SS2.SSS6.p1.2.m2.1.1.1.1" xref="S4.SS2.SSS6.p1.2.m2.1.1.1.1.cmml">0.5</mn><mo id="S4.SS2.SSS6.p1.2.m2.2.2.2.5.2.2" xref="S4.SS2.SSS6.p1.2.m2.2.2.2.5.1.cmml">,</mo><mn id="S4.SS2.SSS6.p1.2.m2.2.2.2.2" xref="S4.SS2.SSS6.p1.2.m2.2.2.2.2.cmml">0.95</mn><mo stretchy="false" id="S4.SS2.SSS6.p1.2.m2.2.2.2.5.2.3" xref="S4.SS2.SSS6.p1.2.m2.2.2.2.5.1.cmml">]</mo></mrow></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS6.p1.2.m2.2b"><apply id="S4.SS2.SSS6.p1.2.m2.2.3.cmml" xref="S4.SS2.SSS6.p1.2.m2.2.3"><csymbol cd="ambiguous" id="S4.SS2.SSS6.p1.2.m2.2.3.1.cmml" xref="S4.SS2.SSS6.p1.2.m2.2.3">superscript</csymbol><ci id="S4.SS2.SSS6.p1.2.m2.2.3.2a.cmml" xref="S4.SS2.SSS6.p1.2.m2.2.3.2"><mtext id="S4.SS2.SSS6.p1.2.m2.2.3.2.cmml" xref="S4.SS2.SSS6.p1.2.m2.2.3.2">mAP</mtext></ci><apply id="S4.SS2.SSS6.p1.2.m2.2.2.2.cmml" xref="S4.SS2.SSS6.p1.2.m2.2.2.2"><times id="S4.SS2.SSS6.p1.2.m2.2.2.2.3.cmml" xref="S4.SS2.SSS6.p1.2.m2.2.2.2.3"></times><ci id="S4.SS2.SSS6.p1.2.m2.2.2.2.4.cmml" xref="S4.SS2.SSS6.p1.2.m2.2.2.2.4">@</ci><interval closure="closed" id="S4.SS2.SSS6.p1.2.m2.2.2.2.5.1.cmml" xref="S4.SS2.SSS6.p1.2.m2.2.2.2.5.2"><cn type="float" id="S4.SS2.SSS6.p1.2.m2.1.1.1.1.cmml" xref="S4.SS2.SSS6.p1.2.m2.1.1.1.1">0.5</cn><cn type="float" id="S4.SS2.SSS6.p1.2.m2.2.2.2.2.cmml" xref="S4.SS2.SSS6.p1.2.m2.2.2.2.2">0.95</cn></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS6.p1.2.m2.2c">\text{mAP}^{@[0.5,0.95]}</annotation></semantics></math>). This highlights a significant research gap in the area of video-based SOD.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Discussion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this survey article, we explored how transformer-based approaches can address the challenges of SOD. Our taxonomy divides transformer-based small object detectors into seven main categories: object representation, fast attention (useful for high-resolution and multi-scale feature maps), architecture and block modification, spatio-temporal information, improved feature representation, auxiliary techniques, and fully transformer-based detectors.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">When juxtaposing this taxonomy with the one for CNN-based techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, we observe that some of these categories overlap, while others are unique to transformer-based techniques. Certain strategies are implicitly embedded into transformers, such as attention and context learning, which are performed via the self and cross-attention modules in the encoder and decoder. On the other hand, multi-scale learning, auxiliary tasks, architecture modification, and data augmentation are commonly used in both paradigms. However, it is important to note that while CNNs handle spatio-temporal analysis through 3D-CNN, RNN, or feature aggregation over time, transformers achieve this by using successive spatial and temporal transformers or by updating object queries for successive frames in the decoder.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">We have observed that pre-training and multi-scale learning stand out as the most commonly adopted strategies, contributing to state-of-the-art performance across various datasets performance on different datasets. Data fusion is another approach widely used for SOD. In the context of video-based detection systems, the focus is on effective methods for collecting temporal data and integrating it into the frame-specific detection module.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">While transformers have brought about substantial advancements in the localization and classification of small objects, it is important to acknowledge the trade-offs involved. These include a large number of parameters (in the order of billions), several days of training (a few hundred epochs), and pretraining on extremely large datasets (which is not feasible without powerful computational resources). All of these aspects pose limitations on the pool of users who can train and test these techniques for their downstream tasks. It is now more important than ever to recognize the need for lightweight networks with efficient learning paradigms and architectures. Despite the number of parameters is now on par with the human brain, the performance in small object detection still lags considerably behind human capabilities, underscoring a significant gap in current research.</p>
</div>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p">Furthermore, based on the findings presented in Figures <a href="#S4.F11" title="Figure 11 ‣ 4.2.1 Generic Applications ‣ 4.2 Benchmarks in Vision Applications ‣ 4 Results and Benchmarks ‣ Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> and <a href="#S4.F13" title="Figure 13 ‣ 4.2.1 Generic Applications ‣ 4.2 Benchmarks in Vision Applications ‣ 4 Results and Benchmarks ‣ Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>, we have identified two primary challenges in small object detection: missing objects or false negatives, and redundant detected boxes. The issue of missing objects is likely attributable to the limited information embedded in the tokens. This can be resolved by using high-resolution images or by enhancing feature pyramids although this comes with the drawback of increased latency—which could potentially be offset by using more efficient, lightweight networks. The problem of repeated detections has traditionally been managed through post-processing techniques such as Non-Maximum Suppression (NMS). However, in the context of transformers, this issue should be approached by minimizing object query similarity in the decoder, possibly through the use of auxiliary loss functions.</p>
</div>
<div id="S5.p6" class="ltx_para">
<p id="S5.p6.1" class="ltx_p">We also examined studies that employ transformer-based methods specifically dedicated to Small Object Detection (SOD) across a range of vision-based tasks. These include generic detection, detection in aerial images, abnormality detection in medical images, small hidden object detection in active millimeter-wave images for security purposes, underwater object detection, and small object detection in videos. Apart from generic and aerial image applications, transformers are underdeveloped in other applications, echoing observations made in Rekavandi <span id="S5.p6.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> regarding maritime detection. This is particularly surprising given the potentially significant impact transformers could have in life-critical fields like medical imaging.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This survey paper reviewed over 60 research papers that focus on the development of transformers for the task of small object detection, including both purely transformer-based and hybrid techniques that integrate CNNs. These techniques have been examined from seven different perspectives: object representation, fast attention mechanisms for high-resolution or multi-scale feature maps, architecture and block modifications, spatio-temporal information, improved feature representation, auxiliary techniques, and fully transformer-based detection. Each of these categories includes several state-of-the-art (SOTA) techniques, each with its own set of advantages. We also compared these transformer-based approaches to CNN-based frameworks, discussing the similarities and differences between the two. Furthermore, for a range of vision applications, we introduced well-established datasets that serve as benchmarks for future research. Additionally, 12 datasets that have been used in SOD applications are discussed in detail, providing convenience for future research efforts.
In future research, the unique challenges associated with the detection of small objects in each application could be explored and addressed. Fields like medical imaging and underwater image analysis stand to gain significantly from the use of transformer models. Additionally, rather than increasing the complexity of transformers using larger models, alternative strategies could be explored to boost performance.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Acknowledgment</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">We thank Likun Cai for providing the detection results for CBNet v2 in test images given in Figure <a href="#S4.F11" title="Figure 11 ‣ 4.2.1 Generic Applications ‣ 4.2 Benchmarks in Vision Applications ‣ 4 Results and Benchmarks ‣ Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> and <a href="#S4.F13" title="Figure 13 ‣ 4.2.1 Generic Applications ‣ 4.2 Benchmarks in Vision Applications ‣ 4 Results and Benchmarks ‣ Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>. This research was partially supported by the Australian Research Council (ARC DP210101682, DP210102674) and Defence Science and Technology Group (DSTG) for the project “Low Observer Detection of Small Objects in Maritime Scenes”.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Y. Liu, P. Sun, N. Wergeles, and Y. Shang, “A survey and performance
evaluation of deep learning methods for small object detection,”
<em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Expert Systems with Applications</em>, vol. 172, p. 114602, 2021.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Dollár, and C. L. Zitnick, “Microsoft coco: Common objects in
context,” in <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>.   Springer, 2014, pp. 740–755.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
J. Wu, C. Zhou, Q. Zhang, M. Yang, and J. Yuan, “Self-mimic learning for
small-scale pedestrian detection,” in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 28th ACM
International Conference on Multimedia</em>, 2020, pp. 2012–2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
S. Rashidi, K. Ehinger, A. Turpin, and L. Kulik, “Optimal visual search based
on a model of target detectability in natural images,” <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Advances in
Neural Information Processing Systems</em>, vol. 33, pp. 9288–9299, 2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
S. W. Cho, N. R. Baek, M. C. Kim, J. H. Koo, J. H. Kim, and K. R. Park, “Face
detection in nighttime images using visible-light camera sensors with
two-step faster region-based convolutional neural network,” <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Sensors</em>,
vol. 18, no. 9, p. 2995, 2018.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Z. Liu, J. Du, F. Tian, and J. Wen, “Mr-cnn: A multi-scale region-based
convolutional neural network for small traffic sign recognition,” <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">IEEE
Access</em>, vol. 7, pp. 57 120–57 128, 2019.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
D. Yudin and D. Slavioglo, “Usage of fully convolutional network with
clustering for traffic light detection,” in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">2018 7th Mediterranean
Conference on Embedded Computing (MECO)</em>.   IEEE, 2018, pp. 1–6.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
L. A. Varga and A. Zell, “Tackling the background bias in sparse object
detection via cropped windows,” in <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
International Conference on Computer Vision</em>, 2021, pp. 2768–2777.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
A. M. Rekavandi, A.-K. Seghouane, and R. J. Evans, “Robust subspace detectors
based on <math id="bib.bib9.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="bib.bib9.1.m1.1a"><mi id="bib.bib9.1.m1.1.1" xref="bib.bib9.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="bib.bib9.1.m1.1b"><ci id="bib.bib9.1.m1.1.1.cmml" xref="bib.bib9.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib9.1.m1.1c">\alpha</annotation></semantics></math>-divergence with application to detection in imaging,”
<em id="bib.bib9.2.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Image Processing</em>, vol. 30, pp. 5017–5031, 2021.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
A. Torralba, “Contextual priming for object detection,” <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">International
Journal of Computer Vision</em>, vol. 53, pp. 169–191, 2003.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
A. M. Rekavandi, L. Xu, F. Boussaid, A.-K. Seghouane, S. Hoefs, and
M. Bennamoun, “A guide to image and video based small object detection using
deep learning: Case study of maritime surveillance,” <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2207.12926</em>, 2022.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
G. Cheng, X. Yuan, X. Yao, K. Yan, Q. Zeng, X. Xie, and J. Han, “Towards
large-scale small object detection: Survey and benchmarks,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">IEEE
Transactions on Pattern Analysis and Machine Intelligence</em>, 2023.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once:
Unified, real-time object detection,” in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition</em>, 2016, pp. 779–788.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
J. Redmon and A. Farhadi, “Yolo9000: better, faster, stronger,” in
<em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2017, pp. 7263–7271.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
——, “Yolov3: An incremental improvement,” <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1804.02767</em>, 2018.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
A. Bochkovskiy <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Yolov4: Optimal speed and accuracy of object
detection,” <em id="bib.bib16.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2004.10934</em>, 2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
G. Jocher <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “yolov5,” <em id="bib.bib17.2.2" class="ltx_emph ltx_font_italic">Code repository https://github.
com/ultralytics/yolov5</em>, 2020.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
C. Li, L. Li, H. Jiang, K. Weng, Y. Geng, L. Li, Z. Ke, Q. Li, M. Cheng, W. Nie
<em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Yolov6: A single-stage object detection framework for
industrial applications,” <em id="bib.bib18.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2209.02976</em>, 2022.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
C.-Y. Wang, A. Bochkovskiy, and H.-Y. M. Liao, “Yolov7: Trainable
bag-of-freebies sets new state-of-the-art for real-time object detectors,”
<em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2207.02696</em>, 2022.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
W. Liu <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Ssd: Single shot multibox detector,” in
<em id="bib.bib20.2.2" class="ltx_emph ltx_font_italic">ECCV</em>.   Springer, 2016, pp.
21–37.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
T.-Y. Lin <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Focal loss for dense object detection,” in
<em id="bib.bib21.2.2" class="ltx_emph ltx_font_italic">ICCV</em>, 2017, pp. 2980–2988.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
K. He <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Spatial pyramid pooling in deep convolutional networks
for visual recognition,” <em id="bib.bib22.2.2" class="ltx_emph ltx_font_italic">TPAMI</em>, vol. 37, no. 9, pp. 1904–1916, 2015.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
R. Girshick, “Fast r-cnn,” in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2015, pp. 1440–1448.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
S. Ren <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Faster r-cnn: Towards real-time object detection with
region proposal networks,” <em id="bib.bib24.2.2" class="ltx_emph ltx_font_italic">NeurIPS</em>, vol. 28, 2015.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
J. Dai <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “R-FCN: Object detection via region-based fully
convolutional networks,” <em id="bib.bib25.2.2" class="ltx_emph ltx_font_italic">NeurIPS</em>, vol. 29, 2016.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
K. He <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Mask r-cnn,” in <em id="bib.bib26.2.2" class="ltx_emph ltx_font_italic">ICCV</em>, 2017, pp. 2961–2969.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
T.-Y. Lin <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Feature pyramid networks for object detection,” in
<em id="bib.bib27.2.2" class="ltx_emph ltx_font_italic">CVPR</em>, 2017, pp. 2117–2125.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Z. Cai and N. Vasconcelos, “Cascade r-cnn: high quality object detection and
instance segmentation,” <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">TPAMI</em>, vol. 43, no. 5, pp. 1483–1498, 2021.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
J. Pang <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Libra R-CNN: Towards balanced learning for object
detection,” in <em id="bib.bib29.2.2" class="ltx_emph ltx_font_italic">CVPR</em>, 2019, pp. 821–830.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”
<em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, vol. 30, 2017.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko,
“End-to-end object detection with transformers,” in <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Computer
Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28,
2020, Proceedings, Part I 16</em>.   Springer, 2020, pp. 213–229.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
J. Beal, E. Kim, E. Tzeng, D. H. Park, A. Zhai, and D. Kislyuk, “Toward
transformer-based object detection,” <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2012.09958</em>,
2020.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">et al.</em>,
“An image is worth 16x16 words: Transformers for image recognition at
scale,” <em id="bib.bib33.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.11929</em>, 2020.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
R. Stewart, M. Andriluka, and A. Y. Ng, “End-to-end people detection in
crowded scenes,” in <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer
vision and pattern recognition</em>, 2016, pp. 2325–2333.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
C. Chi, F. Wei, and H. Hu, “Relationnet++: Bridging visual representations for
object detection via transformer decoder,” <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Advances in Neural
Information Processing Systems</em>, vol. 33, pp. 13 564–13 574, 2020.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
X. Zhou, D. Wang, and P. Krähenbühl, “Objects as points,” <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:1904.07850</em>, 2019.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Z. Yang, S. Liu, H. Hu, L. Wang, and S. Lin, “Reppoints: Point set
representation for object detection,” in <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
international conference on computer vision</em>, 2019, pp. 9657–9666.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
J. Wang, C. Xu, W. Yang, and L. Yu, “A normalized gaussian wasserstein
distance for tiny object detection,” <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.13389</em>,
2021.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
H. Law and J. Deng, “Cornernet: Detecting objects as paired keypoints,” in
<em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Proceedings of the European conference on computer vision (ECCV)</em>,
2018, pp. 734–750.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
K. Duan, S. Bai, L. Xie, H. Qi, Q. Huang, and Q. Tian, “Centernet++ for object
detection,” <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2204.08394</em>, 2022.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, “Deformable detr: Deformable
transformers for end-to-end object detection,” <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">ICLR</em>, 2021.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
H. Song, D. Sun, S. Chun, V. Jampani, D. Han, B. Heo, W. Kim, and M.-H. Yang,
“Vidt: An efficient and effective fully transformer-based object detector,”
<em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.03921</em>, 2022.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Q. Li, Y. Chen, and Y. Zeng, “Transformer with transfer cnn for
remote-sensing-image object detection,” <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Remote Sensing</em>, vol. 14,
no. 4, p. 984, 2022.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
X. Dai, Y. Chen, J. Yang, P. Zhang, L. Yuan, and L. Zhang, “Dynamic detr:
End-to-end object detection with dynamic attention,” in <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Proceedings of
the IEEE/CVF International Conference on Computer Vision</em>, 2021, pp.
2988–2997.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
T. Ma, M. Mao, H. Zheng, P. Gao, X. Wang, S. Han, E. Ding, B. Zhang, and
D. Doermann, “Oriented object detection with transformer,” <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:2106.03146</em>, 2021.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Y. Wang, X. Zhang, T. Yang, and J. Sun, “Anchor detr: Query design for
transformer-based object detection,” <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">AAAI</em>, 2022.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
X. Cao, P. Yuan, B. Feng, and K. Niu, “Cf-detr: Coarse-to-fine transformers
for end-to-end object detection,” in <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI
Conference on Artificial Intelligence</em>, vol. 36, no. 1, 2022, pp. 185–193.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
W. Xu, C. Zhang, Q. Wang, and P. Dai, “Fea-swin: Foreground enhancement
attention swin transformer network for accurate uav-based dense object
detection,” <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Sensors</em>, vol. 22, no. 18, p. 6993, 2022.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Y. Fang, B. Liao, X. Wang, J. Fang, J. Qi, R. Wu, J. Niu, and W. Liu, “You
only look at one sequence: Rethinking transformer in vision through object
detection,” <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
vol. 34, pp. 26 183–26 197, 2021.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, “Swin
transformer: Hierarchical vision transformer using shifted windows,” in
<em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF international conference on computer
vision</em>, 2021, pp. 10 012–10 022.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Y. Li, H. Mao, R. Girshick, and K. He, “Exploring plain vision transformer
backbones for object detection,” in <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">Computer Vision–ECCV 2022: 17th
European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings,
Part IX</em>.   Springer, 2022, pp.
280–296.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Jégou,
“Training data-efficient image transformers &amp; distillation through
attention,” in <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>.   PMLR, 2021, pp. 10 347–10 357.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
P. Chen, M. Zhang, Y. Shen, K. Sheng, Y. Gao, X. Sun, K. Li, and C. Shen,
“Efficient decoder-free object detection with transformers,” in
<em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel,
October 23–27, 2022, Proceedings, Part X</em>.   Springer, 2022, pp. 70–86.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Y. Zhu, Q. Xia, and W. Jin, “Srdd: a lightweight end-to-end object detection
with transformer,” <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">Connection Science</em>, vol. 34, no. 1, pp.
2448–2465, 2022.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Z. Sun, S. Cao, Y. Yang, and K. M. Kitani, “Rethinking transformer-based set
prediction for object detection,” in <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
international conference on computer vision</em>, 2021, pp. 3611–3620.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Z. Tian, C. Shen, H. Chen, and T. He, “Fcos: Fully convolutional one-stage
object detection,” in <em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF international
conference on computer vision</em>, 2019, pp. 9627–9636.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Z. Peng, W. Huang, S. Gu, L. Xie, Y. Wang, J. Jiao, and Q. Ye, “Conformer:
Local features coupling global representations for visual recognition,” in
<em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF international conference on computer
vision</em>, 2021, pp. 367–376.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Z. Peng, Z. Guo, W. Huang, Y. Wang, L. Xie, J. Jiao, Q. Tian, and Q. Ye,
“Conformer: Local features coupling global representations for recognition
and detection,” <em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine
Intelligence</em>, 2023.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
W. Lu, C. Lan, C. Niu, W. Liu, L. Lyu, Q. Shi, and S. Wang, “A cnn-transformer
hybrid model based on cswin transformer for uav image object detection,”
<em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">IEEE Journal of Selected Topics in Applied Earth Observations and
Remote Sensing</em>, 2023.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
X. Xu, Z. Feng, C. Cao, M. Li, J. Wu, Z. Wu, Y. Shang, and S. Ye, “An improved
swin transformer-based model for remote sensing object detection and instance
segmentation,” <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">Remote Sensing</em>, vol. 13, no. 23, p. 4779, 2021.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
J. Xue, D. He, M. Liu, and Q. Shi, “Dual network structure with interweaved
global-local feature hierarchy for transformer-based object detection in
remote sensing image,” <em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">IEEE Journal of Selected Topics in Applied
Earth Observations and Remote Sensing</em>, vol. 15, pp. 6856–6866, 2022.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
D. Chen, D. Miao, and X. Zhao, “Hyneter: Hybrid network transformer for object
detection,” in <em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">ICASSP 2023-2023 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP)</em>.   IEEE, 2023, pp. 1–5.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
J. Ding, W. Li, L. Pei, M. Yang, C. Ye, and B. Yuan, “Sw-yolox: An anchor-free
detector based transformer for sea surface object detection,” <em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">Expert
Systems with Applications</em>, p. 119560, 2023.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
H. Yang, Z. Yang, A. Hu, C. Liu, T. J. Cui, and J. Miao, “Unifying convolution
and transformer for efficient concealed object detection in passive
millimeter-wave images,” <em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Circuits and Systems for
Video Technology</em>, 2023.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
D. Meng, X. Chen, Z. Fan, G. Zeng, H. Li, Y. Yuan, L. Sun, and J. Wang,
“Conditional detr for fast training convergence,” in <em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">Proceedings of
the IEEE/CVF International Conference on Computer Vision</em>, 2021, pp.
3651–3660.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
X. Chen, F. Wei, G. Zeng, and J. Wang, “Conditional detr v2: Efficient
detection transformer with box queries,” <em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2207.08914</em>, 2022.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
S. Liu, F. Li, H. Zhang, X. Yang, X. Qi, H. Su, J. Zhu, and L. Zhang,
“Dab-detr: Dynamic anchor boxes are better queries for detr,” <em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:2201.12329</em>, 2022.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
L. He and S. Todorovic, “Destr: Object detection with split transformer,” in
<em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition</em>, 2022, pp. 9377–9386.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
R. Xia, G. Li, Z. Huang, Y. Pang, and M. Qi, “Transformers only look once with
nonlinear combination for real-time object detection,” <em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">Neural
Computing and Applications</em>, vol. 34, no. 15, pp. 12 571–12 585, 2022.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
T. Liang, X. Chu, Y. Liu, Y. Wang, Z. Tang, W. Chu, J. Chen, and H. Ling,
“Cbnet: A composite backbone network architecture for object detection,”
<em id="bib.bib70.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Image Processing</em>, vol. 31, pp. 6893–6906, 2022.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
P. Sun, T. Liu, X. Chen, S. Zhang, Y. Zhao, and S. Wei, “Multi-source
aggregation transformer for concealed object detection in millimeter-wave
images,” <em id="bib.bib71.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Circuits and Systems for Video
Technology</em>, vol. 32, no. 9, pp. 6148–6159, 2022.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
B. K. Isaac-Medina, C. G. Willcocks, and T. P. Breckon, “Multi-view vision
transformers for object detection,” in <em id="bib.bib72.1.1" class="ltx_emph ltx_font_italic">2022 26th International
Conference on Pattern Recognition (ICPR)</em>.   IEEE, 2022, pp. 4678–4684.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
H. Gong, T. Mu, Q. Li, H. Dai, C. Li, Z. He, W. Wang, F. Han, A. Tuniyazi,
H. Li <em id="bib.bib73.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Swin-transformer-enabled yolov5 with attention
mechanism for small object detection on satellite images,” <em id="bib.bib73.2.2" class="ltx_emph ltx_font_italic">Remote
Sensing</em>, vol. 14, no. 12, p. 2861, 2022.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
J. Ouyang-Zhang, J. H. Cho, X. Zhou, and P. Krähenbühl, “Nms strikes
back,” <em id="bib.bib74.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2212.06137</em>, 2022.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
Q. Chen, X. Chen, J. Wang, H. Feng, J. Han, E. Ding, G. Zeng, and J. Wang,
“Group detr: Fast detr training with group-wise one-to-many assignment,”
<em id="bib.bib75.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2207.13085</em>, vol. 1, no. 2, 2022.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
S. Xu, J. Gu, Y. Hua, and Y. Liu, “Dktnet: Dual-key transformer network for
small object detection,” <em id="bib.bib76.1.1" class="ltx_emph ltx_font_italic">Neurocomputing</em>, 2023.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
M. Maaz, H. Rasheed, S. Khan, F. S. Khan, R. M. Anwer, and M.-H. Yang,
“Class-agnostic object detection with multi-modal transformer,” in
<em id="bib.bib77.1.1" class="ltx_emph ltx_font_italic">17th European Conference on Computer Vision (ECCV)</em>.   Springer, 2022.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
R. Shen, N. Inoue, and K. Shinoda, “Text-guided object detector for
multi-modal video question answering,” in <em id="bib.bib78.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
Winter Conference on Applications of Computer Vision</em>, 2023, pp. 1032–1042.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
L. Cai, Z. Zhang, Y. Zhu, L. Zhang, M. Li, and X. Xue, “Bigdetection: A
large-scale benchmark for improved object detector pre-training,” in
<em id="bib.bib79.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition</em>, 2022, pp. 4777–4787.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
W. Wang, Y. Cao, J. Zhang, and D. Tao, “Fp-detr: Detection transformer
advanced by fully pre-training,” in <em id="bib.bib80.1.1" class="ltx_emph ltx_font_italic">International Conference on
Learning Representations</em>, 2022.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
Q. Chen, J. Wang, C. Han, S. Zhang, Z. Li, X. Chen, J. Chen, X. Wang, S. Han,
G. Zhang <em id="bib.bib81.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Group detr v2: Strong object detector with
encoder-decoder pretraining,” <em id="bib.bib81.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2211.03594</em>, 2022.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
K. Oksuz, B. C. Cam, S. Kalkan, and E. Akbas, “Imbalance problems in object
detection: A review,” <em id="bib.bib82.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on pattern analysis and
machine intelligence</em>, vol. 43, no. 10, pp. 3388–3415, 2020.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
S. Rashidi, R. Tennakoon, A. M. Rekavandi, P. Jessadatavornwong, A. Freis,
G. Huff, M. Easton, A. Mouritz, R. Hoseinnezhad, and A. Bab-Hadiashar,
“It-ruda: Information theory assisted robust unsupervised domain
adaptation,” <em id="bib.bib83.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.12947</em>, 2022.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
K. Zeng, Q. Ma, J. Wu, S. Xiang, T. Shen, and L. Zhang, “Nlfftnet: A non-local
feature fusion transformer network for multi-scale object detection,”
<em id="bib.bib84.1.1" class="ltx_emph ltx_font_italic">Neurocomputing</em>, vol. 493, pp. 15–27, 2022.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
T. Ding, K. Feng, Y. Wei, Y. Han, and T. Li, “Deot: an end-to-end encoder-only
transformer object detector,” <em id="bib.bib85.1.1" class="ltx_emph ltx_font_italic">Journal of Real-Time Image Processing</em>,
vol. 20, no. 1, p. 1, 2023.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
G. Chen, Z. Mao, K. Wang, and J. Shen, “Htdet: A hybrid transformer-based
approach for underwater small object detection,” <em id="bib.bib86.1.1" class="ltx_emph ltx_font_italic">Remote Sensing</em>,
vol. 15, no. 4, p. 1076, 2023.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
Z. Zong, G. Song, and Y. Liu, “Detrs with collaborative hybrid assignments
training,” <em id="bib.bib87.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2211.12860</em>, 2022.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
H. Zhang, F. Li, S. Liu, L. Zhang, H. Su, J. Zhu, L. M. Ni, and H.-Y. Shum,
“Dino: Detr with improved denoising anchor boxes for end-to-end object
detection,” <em id="bib.bib88.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.03605</em>, 2022.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
F. Li, H. Zhang, S. Liu, J. Guo, L. M. Ni, and L. Zhang, “Dn-detr: Accelerate
detr training by introducing query denoising,” in <em id="bib.bib89.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2022, pp.
13 619–13 627.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
L. Dai, H. Liu, H. Tang, Z. Wu, and P. Song, “Ao2-detr: Arbitrary-oriented
object detection transformer,” <em id="bib.bib90.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Circuits and
Systems for Video Technology</em>, 2022.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
C. Zhang, L. Liu, X. Zang, F. Liu, H. Zhang, X. Song, and J. Chen, “Detr++:
Taming your multi-scale detection transformer,” <em id="bib.bib91.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2206.02977</em>, 2022.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
Y. Tang, B. Wang, W. He, and F. Qian, “Pointdet++: an object detection
framework based on human local features with transformer encoder,”
<em id="bib.bib92.1.1" class="ltx_emph ltx_font_italic">Neural Computing and Applications</em>, pp. 1–12, 2022.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
S. Li, F. Sultonov, J. Tursunboev, J.-H. Park, S. Yun, and J.-M. Kang,
“Ghostformer: A ghostnet-based two-stage transformer for small object
detection,” <em id="bib.bib93.1.1" class="ltx_emph ltx_font_italic">Sensors</em>, vol. 22, no. 18, p. 6939, 2022.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
Y. Shou, T. Meng, W. Ai, C. Xie, H. Liu, and Y. Wang, “Object detection in
medical images based on hierarchical transformer and mask mechanism,”
<em id="bib.bib94.1.1" class="ltx_emph ltx_font_italic">Computational Intelligence and Neuroscience</em>, vol. 2022, 2022.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
S. Dubey, F. Olimov, M. A. Rafique, and M. Jeon, “Improving small objects
detection using transformer,” <em id="bib.bib95.1.1" class="ltx_emph ltx_font_italic">Journal of Visual Communication and
Image Representation</em>, vol. 89, p. 103620, 2022.

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
J. Chen, H. Hong, B. Song, J. Guo, C. Chen, and J. Xu, “Mdct: Multi-kernel
dilated convolution and transformer for one-stage object detection of remote
sensing images,” <em id="bib.bib96.1.1" class="ltx_emph ltx_font_italic">Remote Sensing</em>, vol. 15, no. 2, p. 371, 2023.

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
T. Ye, W. Qin, Z. Zhao, X. Gao, X. Deng, and Y. Ouyang, “Real-time object
detection network in uav-vision based on cnn and transformer,” <em id="bib.bib97.1.1" class="ltx_emph ltx_font_italic">IEEE
Transactions on Instrumentation and Measurement</em>, vol. 72, pp. 1–13, 2023.

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
A. Karpathy, A. Khosla, M. Bernstein <em id="bib.bib98.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Imagenet large scale
visual recognition challenge,” <em id="bib.bib98.2.2" class="ltx_emph ltx_font_italic">International journal of computer
vision</em>, vol. 115, pp. 211–252, 2015.

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
T. Meinhardt, A. Kirillov, L. Leal-Taixe, and C. Feichtenhofer, “Trackformer:
Multi-object tracking with transformers,” in <em id="bib.bib99.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition</em>, 2022, pp.
8844–8854.

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
X. Chen, B. Yan, J. Zhu, D. Wang, X. Yang, and H. Lu, “Transformer tracking,”
in <em id="bib.bib100.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition</em>, 2021, pp. 8126–8135.

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
C. Liu, S. Xu, and B. Zhang, “Aerial small object tracking with
transformers,” in <em id="bib.bib101.1.1" class="ltx_emph ltx_font_italic">2021 IEEE International Conference on Unmanned
Systems (ICUS)</em>.   IEEE, 2021, pp.
954–959.

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
L. He, Q. Zhou, X. Li, L. Niu, G. Cheng, X. Li, W. Liu, Y. Tong, L. Ma, and
L. Zhang, “End-to-end video object detection with spatial-temporal
transformers,” in <em id="bib.bib102.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 29th ACM International Conference
on Multimedia</em>, 2021, pp. 1507–1516.

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
Q. Zhou, X. Li, L. He, Y. Yang, G. Cheng, Y. Tong, L. Ma, and D. Tao,
“Transvod: end-to-end video object detection with spatial-temporal
transformers,” <em id="bib.bib103.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine
Intelligence</em>, 2022.

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
H. Wang, J. Tang, X. Liu, S. Guan, R. Xie, and L. Song, “Ptseformer:
Progressive temporal-spatial enhanced transformer towards video object
detection,” in <em id="bib.bib104.1.1" class="ltx_emph ltx_font_italic">Computer Vision–ECCV 2022: 17th European Conference,
Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part VIII</em>.   Springer, 2022, pp. 732–747.

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">
K. A. Hashmi, D. Stricker, and M. Z. Afzal, “Spatio-temporal learnable
proposals for end-to-end video object detection,” <em id="bib.bib105.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2210.02368</em>, 2022.

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">
S.-D. Roh and K.-S. Chung, “Dafa: Diversity-aware feature aggregation for
attention-based video object detection,” <em id="bib.bib106.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, vol. 10, pp.
93 453–93 463, 2022.

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">
M. Fujitake and A. Sugimoto, “Video sparse transformer with attention-guided
memory for video object detection,” <em id="bib.bib107.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, vol. 10, pp.
65 886–65 900, 2022.

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">
Y. Cui, “Faq: Feature aggregated queries for transformer-based video object
detectors,” <em id="bib.bib108.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.08319</em>, 2023.

</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock">
M. Mueller, N. Smith, and B. Ghanem, “A benchmark and simulator for uav
tracking,” in <em id="bib.bib109.1.1" class="ltx_emph ltx_font_italic">Computer Vision–ECCV 2016: 14th European Conference,
Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part I
14</em>.   Springer, 2016, pp. 445–461.

</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock">
E. Goldman, R. Herzig, A. Eisenschtat, J. Goldberger, and T. Hassner, “Precise
detection in densely packed scenes,” in <em id="bib.bib110.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition</em>, 2019, pp. 5227–5236.

</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[111]</span>
<span class="ltx_bibblock">
K. Yan, X. Wang, L. Lu, and R. M. Summers, “Deeplesion: automated mining of
large-scale lesion annotations and universal lesion detection with deep
learning,” <em id="bib.bib111.1.1" class="ltx_emph ltx_font_italic">Journal of medical imaging</em>, vol. 5, no. 3, pp.
036 501–036 501, 2018.

</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[112]</span>
<span class="ltx_bibblock">
“Udacity self-driving car driving data, 2017 transformer,”
<em id="bib.bib112.1.1" class="ltx_emph ltx_font_italic">https://github.com/udacity/selfdriving- car/tree/master/annotations</em>.

</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[113]</span>
<span class="ltx_bibblock">
T. Liu, Y. Zhao, Y. Wei, Y. Zhao, and S. Wei, “Concealed object detection for
activate millimeter wave image,” <em id="bib.bib113.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Industrial
Electronics</em>, vol. 66, no. 12, pp. 9909–9917, 2019.

</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[114]</span>
<span class="ltx_bibblock">
A. Coluccia, A. Fascista, A. Schumann, L. Sommer, A. Dimou, D. Zarpalas, F. C.
Akyon, O. Eryuksel, K. A. Ozfuttu, S. O. Altinuc <em id="bib.bib114.1.1" class="ltx_emph ltx_font_italic">et al.</em>,
“Drone-vs-bird detection challenge at ieee avss2021,” in <em id="bib.bib114.2.2" class="ltx_emph ltx_font_italic">2021 17th
IEEE International Conference on Advanced Video and Signal Based Surveillance
(AVSS)</em>.   IEEE, 2021, pp. 1–8.

</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[115]</span>
<span class="ltx_bibblock">
K. Li, G. Wan, G. Cheng, L. Meng, and J. Han, “Object detection in optical
remote sensing images: A survey and a new benchmark,” <em id="bib.bib115.1.1" class="ltx_emph ltx_font_italic">ISPRS journal of
photogrammetry and remote sensing</em>, vol. 159, pp. 296–307, 2020.

</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[116]</span>
<span class="ltx_bibblock">
G. Cheng, P. Zhou, and J. Han, “Learning rotation-invariant convolutional
neural networks for object detection in vhr optical remote sensing images,”
<em id="bib.bib116.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Geoscience and Remote Sensing</em>, vol. 54, no. 12,
pp. 7405–7415, 2016.

</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[117]</span>
<span class="ltx_bibblock">
Y. Zhang, Y. Yuan, Y. Feng, and X. Lu, “Hierarchical and robust convolutional
neural network for very high-resolution remote sensing object detection,”
<em id="bib.bib117.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Geoscience and Remote Sensing</em>, vol. 57, no. 8,
pp. 5535–5548, 2019.

</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[118]</span>
<span class="ltx_bibblock">
Y. Gao, H. Shen, D. Zhong, J. Wang, Z. Liu, T. Bai, X. Long, and S. Wen, “A
solution for densely annotated large scale object detection task,” 2019.

</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[119]</span>
<span class="ltx_bibblock">
Y. Chen, Z. Zhang, Y. Cao, L. Wang, S. Lin, and H. Hu, “Reppoints v2:
Verification meets regression for object detection,” <em id="bib.bib119.1.1" class="ltx_emph ltx_font_italic">Advances in
Neural Information Processing Systems</em>, vol. 33, pp. 5621–5631, 2020.

</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[120]</span>
<span class="ltx_bibblock">
S. Zhang, C. Chi, Y. Yao, Z. Lei, and S. Z. Li, “Bridging the gap between
anchor-based and anchor-free detection via adaptive training sample
selection,” in <em id="bib.bib120.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition</em>, 2020, pp. 9759–9768.

</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[121]</span>
<span class="ltx_bibblock">
W.-H. Lin, J.-X. Zhong, S. Liu, T. Li, and G. Li, “Roimix: proposal-fusion
among multiple images for underwater object detection,” in <em id="bib.bib121.1.1" class="ltx_emph ltx_font_italic">ICASSP
2020-2020 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP)</em>.   IEEE, 2020, pp.
2588–2592.

</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[122]</span>
<span class="ltx_bibblock">
G.-S. Xia, X. Bai, J. Ding, Z. Zhu, S. Belongie, J. Luo, M. Datcu, M. Pelillo,
and L. Zhang, “Dota: A large-scale dataset for object detection in aerial
images,” in <em id="bib.bib122.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, 2018, pp. 3974–3983.

</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[123]</span>
<span class="ltx_bibblock">
J. Ding, N. Xue, Y. Long, G.-S. Xia, and Q. Lu, “Learning roi transformer for
oriented object detection in aerial images,” in <em id="bib.bib123.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2019, pp.
2849–2858.

</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[124]</span>
<span class="ltx_bibblock">
L. Wang and A. Tien, “Aerial image object detection with vision transformer
detector (vitdet),” <em id="bib.bib124.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2301.12058</em>, 2023.

</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[125]</span>
<span class="ltx_bibblock">
J. Han, J. Ding, N. Xue, and G.-S. Xia, “Redet: A rotation-equivariant
detector for aerial object detection,” in <em id="bib.bib125.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition</em>, 2021, pp. 2786–2795.

</span>
</li>
<li id="bib.bib126" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[126]</span>
<span class="ltx_bibblock">
J. Li, G. Zhu, C. Hua, M. Feng, B. Bennamoun, P. Li, X. Lu, J. Song, P. Shen,
X. Xu <em id="bib.bib126.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “A systematic collection of medical image datasets for
deep learning,” <em id="bib.bib126.2.2" class="ltx_emph ltx_font_italic">ACM Computing Surveys</em>, 2021.

</span>
</li>
<li id="bib.bib127" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[127]</span>
<span class="ltx_bibblock">
H. Zhang, H. Chang, B. Ma, N. Wang, and X. Chen, “Dynamic r-cnn: Towards high
quality object detection via dynamic training,” in <em id="bib.bib127.1.1" class="ltx_emph ltx_font_italic">Computer
Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28,
2020, Proceedings, Part XV 16</em>.   Springer, 2020, pp. 260–275.

</span>
</li>
<li id="bib.bib128" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[128]</span>
<span class="ltx_bibblock">
Y. Li, Y. Chen, N. Wang, and Z. Zhang, “Scale-aware trident networks for
object detection,” in <em id="bib.bib128.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF international
conference on computer vision</em>, 2019, pp. 6054–6063.

</span>
</li>
<li id="bib.bib129" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[129]</span>
<span class="ltx_bibblock">
H. Wu, Y. Chen, N. Wang, and Z. Zhang, “Sequence level semantics aggregation
for video object detection,” in <em id="bib.bib129.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
International Conference on Computer Vision</em>, 2019, pp. 9217–9225.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2309.04901" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2309.04902" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2309.04902">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2309.04902" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2309.04903" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 07:23:47 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
