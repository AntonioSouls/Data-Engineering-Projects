<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2307.12813] Described Object Detection: Liberating Object Detection with Flexible Expressions</title><meta property="og:description" content="Detecting objects based on language information is a popular task that includes Open-Vocabulary object Detection (OVD) and Referring Expression Comprehension (REC). In this paper, we advance them to a more practical se…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Described Object Detection: Liberating Object Detection with Flexible Expressions">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Described Object Detection: Liberating Object Detection with Flexible Expressions">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2307.12813">

<!--Generated on Wed Feb 28 16:31:43 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Described Object Detection: Liberating Object Detection with Flexible Expressions</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chi Xie<sup id="id12.10.id1" class="ltx_sup">1</sup><span id="footnotex1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Equal contribution.</span></span></span>  Zhao Zhang<sup id="id13.11.id2" class="ltx_sup"><span id="id13.11.id2.1" class="ltx_text ltx_font_italic">2</span></sup><span id="footnotex2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Equal contribution.</span></span></span>  Yixuan Wu<sup id="id14.12.id3" class="ltx_sup">3</sup>  Feng Zhu<sup id="id15.13.id4" class="ltx_sup">2</sup>  Rui Zhao<sup id="id16.14.id5" class="ltx_sup"><span id="id16.14.id5.1" class="ltx_text ltx_font_italic">2</span></sup>  Shuang Liang<sup id="id17.15.id6" class="ltx_sup"><span id="id17.15.id6.1" class="ltx_text ltx_font_italic">1</span></sup><span id="footnotex3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Corresponding author.</span></span></span>  
<br class="ltx_break"><sup id="id18.16.id7" class="ltx_sup"><span id="id18.16.id7.1" class="ltx_text ltx_font_italic">1</span></sup>Tongji University  <sup id="id19.17.id8" class="ltx_sup"><span id="id19.17.id8.1" class="ltx_text ltx_font_italic">2</span></sup>Sensetime Research  <sup id="id20.18.id9" class="ltx_sup"><span id="id20.18.id9.1" class="ltx_text ltx_font_italic">3</span></sup>Zhejiang University  
<br class="ltx_break"><span id="id21.19.id10" class="ltx_text ltx_font_typewriter" style="font-size:90%;">chixie@tongji.edu.cn  zzhang@mail.nankai.edu.cn  shuangliang@tongji.edu.cn</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id11.2" class="ltx_p">Detecting objects based on language information is a popular task that includes Open-Vocabulary object Detection (OVD) and Referring Expression Comprehension (REC). In this paper, we advance them to a more practical setting called <span id="id11.2.1" class="ltx_text ltx_font_italic">Described Object Detection</span> (DOD) by expanding category names to flexible language expressions for OVD and overcoming the limitation of REC only grounding the pre-existing object.
We establish the research foundation for DOD by constructing a <span id="id11.2.2" class="ltx_text ltx_font_italic">Description Detection Dataset</span> (D<sup id="id11.2.3" class="ltx_sup">3</sup>). This dataset features flexible language expressions, whether short category names or long descriptions, and annotating all described objects on all images without omission.
By evaluating previous SOTA methods on D<sup id="id11.2.4" class="ltx_sup">3</sup>, we find some troublemakers that fail current REC, OVD, and bi-functional methods.
REC methods struggle with confidence scores, rejecting negative instances, and multi-target scenarios, while OVD methods face constraints with long and complex descriptions. Recent bi-functional methods also do not work well on DOD due to their separated training procedures and inference strategies for REC and OVD tasks.
Building upon the aforementioned findings, we propose a baseline that largely improves REC methods by reconstructing the training data and introducing a binary classification sub-task, outperforming existing methods.
Data and code are available at <a target="_blank" href="https://github.com/shikras/d-cube" title="" class="ltx_ref ltx_href">this URL</a> and related works are tracked in <a target="_blank" href="https://github.com/Charles-Xie/awesome-described-object-detection" title="" class="ltx_ref ltx_href">this repo</a>.</p>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id22.id1" class="ltx_p"><span id="id22.id1.1" class="ltx_text ltx_font_bold">Content.</span> In this supplemental file, we provide more details of this work to supply the main paper.</p>
<ul id="A0.I1" class="ltx_itemize">
<li id="A0.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="A0.I1.ix1.1.1.m1.1" class="ltx_Math" alttext="\blacktriangleright" display="inline"><semantics id="A0.I1.ix1.1.1.m1.1b"><mo id="A0.I1.ix1.1.1.m1.1.1" xref="A0.I1.ix1.1.1.m1.1.1.cmml">▶</mo><annotation-xml encoding="MathML-Content" id="A0.I1.ix1.1.1.m1.1c"><ci id="A0.I1.ix1.1.1.m1.1.1.cmml" xref="A0.I1.ix1.1.1.m1.1.1">▶</ci></annotation-xml><annotation encoding="application/x-tex" id="A0.I1.ix1.1.1.m1.1d">\blacktriangleright</annotation></semantics></math></span> 
<div id="A0.I1.ix1.p1" class="ltx_para">
<p id="A0.I1.ix1.p1.1" class="ltx_p"><span id="A0.I1.ix1.p1.1.1" class="ltx_text ltx_font_bold">Dataset details and more examples</span> for the proposed D<sup id="A0.I1.ix1.p1.1.2" class="ltx_sup">3</sup> dataset are presented in <a href="#A1" title="Appendix A Dataset Details ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
</li>
<li id="A0.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="A0.I1.ix2.1.1.m1.1" class="ltx_Math" alttext="\blacktriangleright" display="inline"><semantics id="A0.I1.ix2.1.1.m1.1b"><mo id="A0.I1.ix2.1.1.m1.1.1" xref="A0.I1.ix2.1.1.m1.1.1.cmml">▶</mo><annotation-xml encoding="MathML-Content" id="A0.I1.ix2.1.1.m1.1c"><ci id="A0.I1.ix2.1.1.m1.1.1.cmml" xref="A0.I1.ix2.1.1.m1.1.1">▶</ci></annotation-xml><annotation encoding="application/x-tex" id="A0.I1.ix2.1.1.m1.1d">\blacktriangleright</annotation></semantics></math></span> 
<div id="A0.I1.ix2.p1" class="ltx_para">
<p id="A0.I1.ix2.p1.1" class="ltx_p"><span id="A0.I1.ix2.p1.1.1" class="ltx_text ltx_font_bold">Evaluation of previous methods</span> are presented in <a href="#A2" title="Appendix B Evaluating Existing Baselines ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">B</span></a>, which describes the existing works we evaluated and the specific details regarding how we adapt them to the DOD task.</p>
</div>
</li>
<li id="A0.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="A0.I1.ix3.1.1.m1.1" class="ltx_Math" alttext="\blacktriangleright" display="inline"><semantics id="A0.I1.ix3.1.1.m1.1b"><mo id="A0.I1.ix3.1.1.m1.1.1" xref="A0.I1.ix3.1.1.m1.1.1.cmml">▶</mo><annotation-xml encoding="MathML-Content" id="A0.I1.ix3.1.1.m1.1c"><ci id="A0.I1.ix3.1.1.m1.1.1.cmml" xref="A0.I1.ix3.1.1.m1.1.1">▶</ci></annotation-xml><annotation encoding="application/x-tex" id="A0.I1.ix3.1.1.m1.1d">\blacktriangleright</annotation></semantics></math></span> 
<div id="A0.I1.ix3.p1" class="ltx_para">
<p id="A0.I1.ix3.p1.1" class="ltx_p"><span id="A0.I1.ix3.p1.1.1" class="ltx_text ltx_font_bold">Details of the proposed baseline</span> are presented in <a href="#A3" title="Appendix C The Proposed Baseline ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">C</span></a>.</p>
</div>
</li>
<li id="A0.I1.ix4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="A0.I1.ix4.1.1.m1.1" class="ltx_Math" alttext="\blacktriangleright" display="inline"><semantics id="A0.I1.ix4.1.1.m1.1b"><mo id="A0.I1.ix4.1.1.m1.1.1" xref="A0.I1.ix4.1.1.m1.1.1.cmml">▶</mo><annotation-xml encoding="MathML-Content" id="A0.I1.ix4.1.1.m1.1c"><ci id="A0.I1.ix4.1.1.m1.1.1.cmml" xref="A0.I1.ix4.1.1.m1.1.1">▶</ci></annotation-xml><annotation encoding="application/x-tex" id="A0.I1.ix4.1.1.m1.1d">\blacktriangleright</annotation></semantics></math></span> 
<div id="A0.I1.ix4.p1" class="ltx_para">
<p id="A0.I1.ix4.p1.1" class="ltx_p"><span id="A0.I1.ix4.p1.1.1" class="ltx_text ltx_font_bold">More experimental results</span> are shown in <a href="#A4" title="Appendix D More experimental results ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">D</span></a>, including both quantitative and qualitative results.</p>
</div>
</li>
</ul>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Detecting objects of interest within a scene using language is a pivotal area of focus. This field encompasses two key tasks: Open-Vocabulary object Detection (OVD) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> and Referring Expression Comprehension (REC) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>.
We present an intuitive illustration of these two settings in <a href="#S1.F1" title="In 1 Introduction ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>.
The first task, OVD, expands the scope of object detection (OD) to any given short category name. However, these settings neglect the instances described by intricate descriptions.
The second task, REC, focuses on spatially locating one target described by an expression and assumes the target must exist in the image. However, in real-world scenarios, if the described objects do not exist in the image, REC algorithms output false-positive results.
Recent advancements have witnessed the joint training of bi-functional models, such as Grounding-DINO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> and UNINEXT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>, which involve both OVD and REC data.
Notwithstanding, these models still rely on separate training procedures and inference strategies for OVD and REC, and evaluate these two tasks independently.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2307.12813/assets/x1.png" id="S1.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="148" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.5.2.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.1" class="ltx_text" style="font-size:90%;">Examples showing the difference between REC, OVD and Described Object Detection (DOD).
OVD detects arbitrary number (including zero, denoted with <math id="S1.F1.3.1.m1.1" class="ltx_Math" alttext="\emptyset" display="inline"><semantics id="S1.F1.3.1.m1.1b"><mi mathvariant="normal" id="S1.F1.3.1.m1.1.1" xref="S1.F1.3.1.m1.1.1.cmml">∅</mi><annotation-xml encoding="MathML-Content" id="S1.F1.3.1.m1.1c"><emptyset id="S1.F1.3.1.m1.1.1.cmml" xref="S1.F1.3.1.m1.1.1"></emptyset></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.3.1.m1.1d">\emptyset</annotation></semantics></math>) of objects based on a category name; REC grounds one region based on a language description, whether the object truly exists or not; DOD detect all instances on each image in the dataset, based on a flexible reference.

</span></figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">As shown in <a href="#S1.F1" title="In 1 Introduction ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>, a more practical detection algorithm should be able to detect any described category, whether long or short, complex or simple, while discarding predictions in images where targets are absent.
In order to address this significant yet often overlooked scenario, we propose the concept of <span id="S1.p2.1.1" class="ltx_text ltx_font_bold">Described Object Detection (DOD)</span>. Note that this setting is a superset of OVD and REC.
When the language expression is limited to a short category name, it becomes OVD. When we limit the images to detect objects known to be present in the images beforehand, it downgrades to REC.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Can the existing SOTA algorithms of the community support DOD tasks?
To address this inquiry, this paper establishes the research foundation of DOD tasks by constructing a dataset, scrutinizing relevant methodologies, analyzing the relevant methods, and exploring improvement space.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p"><span id="S1.p4.1.1" class="ltx_text ltx_font_bold">Motivation &amp; real-world application of DOD.</span>
OVD is limited to categorical detection, focusing on <span id="S1.p4.1.2" class="ltx_text ltx_font_italic">classes</span> rather than specific attributes or relationships. It lacks detailed contextual understanding and cannot adapt to precise detection requirements from language.
REC comprehend longer descriptions for attributes or relationships, but assumes the existence of one target in the image. This leads to false positives when the target is absent, limiting its practical usability.
Consider detecting <span id="S1.p4.1.3" class="ltx_text ltx_font_typewriter">individuals without helmets</span> on a construction site using camera data: OVD can detect <span id="S1.p4.1.4" class="ltx_text ltx_font_typewriter">helmets</span> and <span id="S1.p4.1.5" class="ltx_text ltx_font_typewriter">people</span> but not determine their relationship. REC locate one region in any image and generate false positives frequently. Existing solutions involve using separate models for object detection then relationship classification, or REC after image classification, both resulting in inefficiency.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Hence, there is a demand for language-based object detection: a model with strong generalization capabilities that can verify the existence of described objects in images and localize them based on arbitrary expressions. The proposed DOD task addresses this need and finds practical applications in:
urban security, detecting
<span id="S1.p5.1.1" class="ltx_text ltx_font_typewriter">dogs without leashes</span> in communities, <span id="S1.p5.1.2" class="ltx_text ltx_font_typewriter">clothes hung outdoors</span> on streets, <span id="S1.p5.1.3" class="ltx_text ltx_font_typewriter">overloaded vehicles</span>, and <span id="S1.p5.1.4" class="ltx_text ltx_font_typewriter">fallen trees on roadsides</span>;
network security, like identifying sensitive images with violence or bloodshed within large datasets;
(fine-grained) photo album retrieval based on descriptions or keywords;
retrieval and filtering of web image data;
specific event detection in autonomous driving, such as <span id="S1.p5.1.5" class="ltx_text ltx_font_typewriter">pedestrians crossing the road</span>.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.6" class="ltx_p"><span id="S1.p6.6.1" class="ltx_text ltx_font_bold">Dataset &amp; benchmark.</span>
For DOD, we introduce the <span id="S1.p6.6.2" class="ltx_text ltx_font_bold">Description Detection Dataset</span> (D<sup id="S1.p6.6.3" class="ltx_sup">3</sup>, /dikju:b/), an evaluation-only benchmark containing 422 descriptions and 24,282 positive object-description pairs. Unlike previous OVD or REC datasets (see <a href="#S3.F2" title="In 3.1 Dataset highlight ‣ 3 Dataset ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>), D<sup id="S1.p6.6.4" class="ltx_sup">3</sup> stands out in three key aspects (see <a href="#S1.T1" title="In 1 Introduction ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a>):
1) <span id="S1.p6.6.5" class="ltx_text ltx_font_italic">Complete annotation</span>: All descriptions refer to objects annotated throughout the dataset, making D<sup id="S1.p6.6.6" class="ltx_sup">3</sup> a detection-style dataset akin to COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.
2) <span id="S1.p6.6.7" class="ltx_text ltx_font_italic">Unrestricted description</span>: Annotations in D<sup id="S1.p6.6.8" class="ltx_sup">3</sup> include diverse and flexible language expressions, varying in length and complexity.
3) <span id="S1.p6.6.9" class="ltx_text ltx_font_italic">Absence expression</span>: We include descriptions regarding absence of concepts, such as <span id="S1.p6.6.10" class="ltx_text ltx_font_typewriter">a person <span id="S1.p6.6.10.1" class="ltx_text ltx_font_italic">without</span> a safety helmet</span>, addressing an often-overlooked detection requirement.
The details of D<sup id="S1.p6.6.11" class="ltx_sup">3</sup> is elaborated in <a href="#S3" title="3 Dataset ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3</span></a>.
We evaluate state-of-the-art methods on D<sup id="S1.p6.6.12" class="ltx_sup">3</sup>: OWL-ViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>/CORA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> (OVD), OFA (REC) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, and UNINEXT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>/Grounding-DINO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> (bi-functional) to provide a reference for the community. This benchmark may serve as a starting point for the DOD task.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p"><span id="S1.p7.1.1" class="ltx_text ltx_font_bold">Findings &amp; improvements.</span>
The experimental analysis for different methods on D<sup id="S1.p7.1.2" class="ltx_sup">3</sup> yields some findings for future research (see <a href="#S5" title="5 Experimental Analyses ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">5</span></a>):
1) Existing REC methods perform poorly, lacking confidence scores and the ability to reject negatives, and struggling with multi-target situations. This is due to their task formulation of grounding, i.e., matching between text and image region and not distinguishing positive and negatives.
2) OVD methods excel REC ones on DOD, though lengthy descriptions, which is not available in their training data, limit their performance.
3) Bi-functional methods, while superior to REC and OVD ones, share similar challenges with REC methods. Sometimes they are surpassed by OVD models, indicating they have not fully benefited from REC and OVD.
Based on these findings, we propose <span id="S1.p7.1.3" class="ltx_text ltx_font_bold">a baseline OFA-DOD</span> that greatly improves a REC method, and outperforms current SOTAs. Its abilities to handle multiple targets and reject negative instances are improved by simple data reconstruction and an auxiliary sub-task. It is still far from a strong DOD method, but may provide some insights for research in the future.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S1.T1.2.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S1.T1.3.2" class="ltx_text" style="font-size:90%;">Comparison between the proposed dataset and previous REC datasets and OVD datasets.</span></figcaption>
<table id="S1.T1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S1.T1.4.1.1" class="ltx_tr">
<th id="S1.T1.4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S1.T1.4.1.1.1.1" class="ltx_text">Dataset</span></th>
<th id="S1.T1.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">annotation</th>
<th id="S1.T1.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">unrestricted</th>
<th id="S1.T1.4.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">absence</th>
<th id="S1.T1.4.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">instance-level</th>
</tr>
<tr id="S1.T1.4.2.2" class="ltx_tr">
<th id="S1.T1.4.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">completeness</th>
<th id="S1.T1.4.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">description</th>
<th id="S1.T1.4.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">expression</th>
<th id="S1.T1.4.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">annotation</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S1.T1.4.3.1" class="ltx_tr">
<th id="S1.T1.4.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">RefCOCO</th>
<td id="S1.T1.4.3.1.2" class="ltx_td ltx_align_center ltx_border_t">image-wise</td>
<td id="S1.T1.4.3.1.3" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S1.T1.4.3.1.4" class="ltx_td ltx_align_center ltx_border_t">✗</td>
<td id="S1.T1.4.3.1.5" class="ltx_td ltx_align_center ltx_border_t">✓</td>
</tr>
<tr id="S1.T1.4.4.2" class="ltx_tr">
<th id="S1.T1.4.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">COCO</th>
<td id="S1.T1.4.4.2.2" class="ltx_td ltx_align_center">dataset-wise</td>
<td id="S1.T1.4.4.2.3" class="ltx_td ltx_align_center">✗</td>
<td id="S1.T1.4.4.2.4" class="ltx_td ltx_align_center">✗</td>
<td id="S1.T1.4.4.2.5" class="ltx_td ltx_align_center">✓</td>
</tr>
<tr id="S1.T1.4.5.3" class="ltx_tr">
<th id="S1.T1.4.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">GRD</th>
<td id="S1.T1.4.5.3.2" class="ltx_td ltx_align_center">group-wise</td>
<td id="S1.T1.4.5.3.3" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.4.5.3.4" class="ltx_td ltx_align_center">✗</td>
<td id="S1.T1.4.5.3.5" class="ltx_td ltx_align_center">✗</td>
</tr>
<tr id="S1.T1.4.6.4" class="ltx_tr">
<th id="S1.T1.4.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r">Ours</th>
<td id="S1.T1.4.6.4.2" class="ltx_td ltx_align_center ltx_border_b">dataset-wise</td>
<td id="S1.T1.4.6.4.3" class="ltx_td ltx_align_center ltx_border_b">✓</td>
<td id="S1.T1.4.6.4.4" class="ltx_td ltx_align_center ltx_border_b">✓</td>
<td id="S1.T1.4.6.4.5" class="ltx_td ltx_align_center ltx_border_b">✓</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Relevant datasets and benchmarks</h3>

<div id="S2.SS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.p1.1" class="ltx_p"><span id="S2.SS1.p1.1.1" class="ltx_text ltx_font_bold">Object detection datasets.</span>
A variety of datasets have been proposed for object detection. Some have become standard benchmarks, like PASCAL VOC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>; while others are more frequently used for pretraining <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. A few works have focused on special settings, such as LVIS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> for long-tailed detection and ODinW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> for zero-shot evaluation in the wild.
Recently, V3Det <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>facilitates object detection with an extremely large vocabulary.
Some are re-splitted and frequently used in OVD as well, like COCO and LVIS.
As explained in <a href="#S1" title="1 Introduction ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">1</span></a>, these datasets are all annotated with simple category labels rather than flexible language expressions like D<sup id="S2.SS1.p1.1.2" class="ltx_sup">3</sup>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS1.p2.1" class="ltx_p"><span id="S2.SS1.p2.1.1" class="ltx_text ltx_font_bold">Referring expression comprehension datasets.</span>
Several datasets have been introduced to evaluate REC methods, including RefClef <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, RefCOCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, RefCOCO+ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, RefCOCOg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, Visual Genome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, and PhraseCut <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>.
Some <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> are collected interactively, and the expressions are more concise and less diverse.
RefCOCOg is collected non-interactively, resulting in more complex expressions.
Comparatively, Visual Genome focuses on visual relationships.
All these datasets only annotate a few positive images for each category and leave other images unknown, which makes them unsuitable for the detection task.
</p>
</div>
<div id="S2.SS1.p3" class="ltx_para ltx_noindent">
<p id="S2.SS1.p3.1" class="ltx_p"><span id="S2.SS1.p3.1.1" class="ltx_text ltx_font_bold">Other related tasks and datasets.</span>
Several related tasks and benchmarks exist, but they differ significantly from DOD.
Phrase Detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> lacks explicit negative labels as negative instances are unlabeled, and does not constitute a true detection task. Additionally, its references are simply phrases.
In contrast, DOD ensures exhaustive annotation of positive and negative labels, and its references can be words, phrases, or sentences.
Cops-Ref benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> focuses on evaluating the grounding capability of REC methods in difficult negative regions with related and distracting targets. It provides explicit negative certificates for only a limited set of images.
In D<sup id="S2.SS1.p3.1.2" class="ltx_sup">3</sup>, negative certificates are available across the entire dataset.
Zero-shot grounding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> centers on locating concepts not in the training set. It assumes the existence of the object referred by a reference in a image, and locates a single target per image, with a short phrase, while DOD makes no assumptions about the existence of the target, and locates zero to multiple targets, with varied expressions.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Current methods</h3>

<div id="S2.SS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.p1.1" class="ltx_p"><span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_bold">Open-vocabulary object detection methods.</span>
Open-vocabulary detection is currently receiving increased attention. It aims to detect arbitrary classes using language for generalization, even when trained on a limited set of classes.
The first approach, OVR-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>, utilizes image-caption pairs for pretraining the visual encoder to enhance its zero-shot generalization capabilities. With the introduction of CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, models such as Detic <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>, DetCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, RegionCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, and OV-DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> have further advanced image and language embeddings pretrained using CLIP.
ViLD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> further distills knowledge from CLIP to inherit language semantics for recognizing novel classes.
GLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> formulates object detection as a phrase grounding problem <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> and utilizes additional phrase grounding data to facilitate vision-language alignment.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.p2.1" class="ltx_p"><span id="S2.SS2.p2.1.1" class="ltx_text ltx_font_bold">Referring expression comprehension methods.</span>
Existing works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> can be divided into three categories.
(1) Specialist models tailored for REC.
Previously, two-staged works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> reformulate this as a ranking task.
More recently, one-stage approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> speed up the inference process.
(2) Multi-task models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.
They usually design a unified formulation for a few closely related tasks.
For example, SeqTR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite> unifies REC and RES as a point prediction problem.
(3) Multi-modal pre-training models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>.
Unified-IO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> and OFA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> propose unified sequence-to-sequence frameworks that can handle a variety of vision, language, and multi-modal tasks. Currently, OFA holds the SOTA among REC methods.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para ltx_noindent">
<p id="S2.SS2.p3.1" class="ltx_p"><span id="S2.SS2.p3.1.1" class="ltx_text ltx_font_bold">Bi-functional models for REC and OVD/OD.</span>
Some recent works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> aim to handle tasks such as OVD (or OD) and REC concurrently within a single model. They typically restructure the training approach for these tasks, enabling a single model to learn from datasets related to both tasks. However, the inference process for each task remains distinct and independent of the other.
FIBER <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> employs a two-stage pretraining strategy, separately utilizing image-text and image-text-box data to enhance data efficiency.
More recently, Grounding-DINO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> extends a closed-set detector by performing vision-language fusion at multiple stages and evaluating its performance on REC datasets.
UNINEXT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> reformulates various image and video tasks into a unified object discovery and retrieval paradigm.
Despite these models sharing knowledge between detection and REC through pretraining, they are still treated as distinct tasks in these bi-functional models.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">Methods with potential for DOD are continuously emerging and we will update them in <a target="_blank" href="https://github.com/Charles-Xie/awesome-described-object-detection" title="" class="ltx_ref ltx_href">this list</a>.

</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Dataset</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Dataset highlight</h3>

<figure id="S3.F2" class="ltx_figure">
<div id="S3.F2.4" class="ltx_block">
<figure id="S3.F2.sf1" class="ltx_figure ltx_align_center"><img src="/html/2307.12813/assets/x2.png" id="S3.F2.sf1.g1" class="ltx_graphics ltx_img_landscape" width="194" height="147" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S3.F2.sf1.3.2" class="ltx_text" style="font-size:90%;">Complete annotation.</span></figcaption>
</figure>
<figure id="S3.F2.sf2" class="ltx_figure ltx_align_center"><img src="/html/2307.12813/assets/x3.png" id="S3.F2.sf2.g1" class="ltx_graphics ltx_img_portrait" width="113" height="147" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S3.F2.sf2.3.2" class="ltx_text" style="font-size:90%;">Unrestricted reference.</span></figcaption>
</figure>
<figure id="S3.F2.sf3" class="ltx_figure ltx_align_center"><img src="/html/2307.12813/assets/x4.png" id="S3.F2.sf3.g1" class="ltx_graphics ltx_img_square" width="125" height="147" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S3.F2.sf3.3.2" class="ltx_text" style="font-size:90%;">Absence expression.</span></figcaption>
</figure>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.6.2.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.2.1" class="ltx_text" style="font-size:90%;">Some examples from previous datasets and the proposed D<sup id="S3.F2.2.1.1" class="ltx_sup">3</sup> dataset for DOD. (a) Our dataset for DOD is completely annotated for detection, while REC datasets like RefCOCO are not. (b) Our dataset has unrestricted reference, while OVD datasets like COCO are not. (c) Our dataset not only provides traditional presence descriptions, but also absence descriptions.</span></figcaption>
</figure>
<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The proposed dataset is re-annotated on GRD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>, a dataset for RES <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. As briefly introduced in <a href="#S1" title="1 Introduction ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">1</span></a>, it contains three major characteristics. In <a href="#S3.F2" title="In 3.1 Dataset highlight ‣ 3 Dataset ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>, we show some examples from previous datasets and D<sup id="S3.SS1.p1.1.1" class="ltx_sup">3</sup> to highlight them. Here we elaborate on them with a few other characteristics:</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">The first is <span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_italic">complete annotation</span>. For REC, the instances referred to by one description are only annotated in a few images. For other images without the annotation of this description, it is unknown whether the corresponding instance exists or not. That is to say, their annotations are not complete. Contrarily, as shown in <a href="#S3.F2.sf1" title="In Figure 2 ‣ 3.1 Dataset highlight ‣ 3 Dataset ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2(a)</span></a>, in D<sup id="S3.SS1.p2.1.2" class="ltx_sup">3</sup>, the objects referred to in all images by any description are annotated, as are the negative samples, like traditional object detection datasets.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">The second is <span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_italic">unrestricted language description</span>. As shown in <a href="#S3.F2.sf2" title="In Figure 2 ‣ 3.1 Dataset highlight ‣ 3 Dataset ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2(b)</span></a>, unlike (open vocabulary) object detection that retrieves objects with category names, we retrieve objects with language expressions, which is rather flexible. As is shown in <a href="#S3.F3.sf4" title="In Figure 3 ‣ 3.1 Dataset highlight ‣ 3 Dataset ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3(d)</span></a>, the lengths of descriptions in D<sup id="S3.SS1.p3.1.2" class="ltx_sup">3</sup> vary a lot. The shortest descriptions have one or two words, where the DOD task downgrades to OVD, while the longest may have 15 or more words, resulting in rather complex language expressions.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">The third is <span id="S3.SS1.p4.1.1" class="ltx_text ltx_font_italic">absence expression</span>. Current datasets with language description, like RefCOCO series for REC, usually describe objects with certain features. They usually focus on the ability to discover the existence of concepts but neglect their absence. Noticing the missing ability to verify such capability, we also annotate objects lacking a certain attribute. <a href="#S3.F2.sf3" title="In Figure 2 ‣ 3.1 Dataset highlight ‣ 3 Dataset ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2(c)</span></a> shows an example with presence description and another with absence description from D<sup id="S3.SS1.p4.1.2" class="ltx_sup">3</sup>. Such absence description makes up about one quarter of the references in this dataset. This is a first for existing benchmarks.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">The fourth is <span id="S3.SS1.p5.1.1" class="ltx_text ltx_font_italic">instance-level annotation</span>, a characteristic not held by GRD as it is intended for RES.</p>
</div>
<div id="S3.SS1.p6" class="ltx_para">
<p id="S3.SS1.p6.1" class="ltx_p">The fifth is <span id="S3.SS1.p6.1.1" class="ltx_text ltx_font_italic">one description can refer to multiple instances</span> in an image, as in <a href="#S3.F3.sf3" title="In Figure 3 ‣ 3.1 Dataset highlight ‣ 3 Dataset ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3(c)</span></a>. This is not true for REC datasets. If we regard category names as references, then OD datasets do have this feature.</p>
</div>
<div id="S3.SS1.p7" class="ltx_para">
<p id="S3.SS1.p7.1" class="ltx_p">In summary, the proposed dataset differs from the REC dataset primarily in terms of characteristics 1st, 3rd, and 5th. In contrast, when compared to OD datasets, the proposed dataset showcases disparities in the 2nd and 3rd characteristics, and when compared with GRD, in the 2nd, 3rd, 4th, and 5th characteristics. We refer the readers to the <span id="S3.SS1.p7.1.1" class="ltx_text ltx_font_italic" style="color:#B80000;">supplementary materials</span> for more information about the characteristics of D<sup id="S3.SS1.p7.1.2" class="ltx_sup">3</sup> and more examples.</p>
</div>
<figure id="S3.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S3.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2307.12813/assets/x5.png" id="S3.F3.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="346" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S3.F3.sf1.3.2" class="ltx_text" style="font-size:90%;">
</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S3.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2307.12813/assets/x6.png" id="S3.F3.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="346" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S3.F3.sf2.3.2" class="ltx_text" style="font-size:90%;">
</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S3.F3.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2307.12813/assets/x7.png" id="S3.F3.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="346" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S3.F3.sf3.3.2" class="ltx_text" style="font-size:90%;">
</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S3.F3.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2307.12813/assets/x8.png" id="S3.F3.sf4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="346" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S3.F3.sf4.3.2" class="ltx_text" style="font-size:90%;">
</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.3.2" class="ltx_text" style="font-size:90%;">Distribution of (a) number of positive images for a description in the dataset, (b) number of positive instances for a description, (c) number of instances in a positive image for a description, and (d) lengths of descriptions.</span></figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Annotation process</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We utilize the GRD dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> as the source for images, along with its original annotations. Originally, it is divided into multiple groups, each containing several references, with positive and negative samples annotated only within each group. We extend the annotations in three aspects:</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold">Adding instance-level annotation.</span>
GRD is designed for RES, where each reference corresponds to one semantic mask across one image. However, for the DOD task, which requires the recognition and localization of individual instances, we annotate each instance referred to by a description with an individual bounding box (along with an instance mask). This is the basic step to adapt the dataset for instance localization.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.p3.1" class="ltx_p"><span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_bold">Adding complete annotations.</span>
In addition to the intra-group annotation in GRD, we further annotate the positive and negative samples for each reference across the entire dataset. With complete dataset-wise annotations, the division into groups becomes unnecessary for evaluation, serving only as a means to organize references by scenarios. This enhancement makes the dataset suitable for detection tasks, significantly increasing the number of positive and negative samples.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">Note that we use the complete annotation similar to COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, i.e., explicit positive and negative certificates for all categories on all images, rather than federated annotation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. This allows using mAP (mean Average Precision) as the evaluation metric, which is elaborated in <a href="#S3.SS4" title="3.4 Evaluation metrics ‣ 3 Dataset ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.4</span></a>.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para ltx_noindent">
<p id="S3.SS2.p5.1" class="ltx_p"><span id="S3.SS2.p5.1.1" class="ltx_text ltx_font_bold">Adding annotations for absence expressions.</span>
We have designed many absence descriptions based on the scenarios within the dataset, in addition to the traditional presence expressions in GRD. We annotate the instances in the images across the entire dataset with these absence expressions.
This step increases the difficulty level of the proposed benchmark and enables the evaluation of existing models’ ability to comprehend the absence of concepts.</p>
</div>
<div id="S3.SS2.p6" class="ltx_para">
<p id="S3.SS2.p6.1" class="ltx_p">We present a concise overview of the overall annotation process here.
We organize groups of images and references (both for presence and absence). For each image, the references in its group are used. References from other groups may also appear, but with lower probability.
We employ CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> to select a large number of candidates from these references in other groups.
We manually check and adjust the hyper-parameters to make sure that such CLIP filtering usually do not miss positive refs.
Subsequently, annotators select the positive references from these candidates (rather than from all references in the dataset) and add bounding boxes to the images.
For more detailed information regarding the annotation process, please refer to <span id="S3.SS2.p6.1.1" class="ltx_text ltx_font_italic" style="color:#B80000;">supplementary materials</span>.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Dataset statistics</h3>

<div id="S3.SS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.p1.1" class="ltx_p"><span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_bold">GRD statistics.</span>
It has 10,578 images collected online, divided into 106 groups.
Each group has around 100 images and 3 expressions referring to segmentation masks in this group, resulting in 316 references, 9,323 positive image-text pairs and 22,201 negative pairs.
Note that it only annotates positive and negative samples inside each group, i.e., the annotation completeness is only <span id="S3.SS3.p1.1.2" class="ltx_text ltx_font_bold">group-level</span>, so a reference will not be verified outside its group.
The expressions have an average length of 5.9 words.
We refer the reader to the original paper for specific statistics of GRD.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS3.p2.2" class="ltx_p"><span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_bold">D<sup id="S3.SS3.p2.1.1.1" class="ltx_sup"><span id="S3.SS3.p2.1.1.1.1" class="ltx_text ltx_font_medium">3</span></sup> statistics.</span>
The proposed D<sup id="S3.SS3.p2.2.2" class="ltx_sup">3</sup> has 10,578 images, all from GRD. It has 422 well-designed expressions, including 316 expressions from GRD and 106 absence expressions we added (one for each scenario).
The instance-level annotation results in 18,514 boxes.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">Due to the effort in <span id="S3.SS3.p3.1.1" class="ltx_text ltx_font_italic">complete annotation</span>, for a reference, each image in the dataset is annotated for possible positive and negative samples, i.e., the annotation completeness is <span id="S3.SS3.p3.1.2" class="ltx_text ltx_font_bold">dataset-level</span>.
Thus, there are 24,282 positive object-text pairs and 7,788,626 negative pairs, orders of magnitude larger than GRD.
Among them, those with images and texts from the same scenario are probably more difficult, which includes 20,279 positive and 53,383 negative pairs.
The average length of expressions is 6.3 words, due to the relative longer absence expressions.
More statistics and examples of D<sup id="S3.SS3.p3.1.3" class="ltx_sup">3</sup> are available in <span id="S3.SS3.p3.1.4" class="ltx_text ltx_font_italic" style="color:#B80000;">supplementary materials</span>.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Evaluation metrics</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.3" class="ltx_p">The classification of instances in D<sup id="S3.SS4.p1.3.1" class="ltx_sup">3</sup> is <span id="S3.SS4.p1.3.2" class="ltx_text ltx_font_bold">multi-label</span>. Each description corresponds to a category. Naturally, there can be relationships between categories, such as parent-child hierarchies, synonyms, and partial overlap. When designing categories, we intentionally reduce parent-child or synonym relationships to ensure greater diversity and challenge. However, there exists partial overlap between categories. Therefore, in D<sup id="S3.SS4.p1.3.3" class="ltx_sup">3</sup>, one instance may correspond to multiple descriptions, and the classification in D<sup id="S3.SS4.p1.3.4" class="ltx_sup">3</sup> is multi-label <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> rather than single-label <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, making it suitable for categories with relationships. An effective detector should assign all relevant positive categories (e.g., <span id="S3.SS4.p1.3.5" class="ltx_text ltx_font_typewriter">dog not led by rope outside</span> and <span id="S3.SS4.p1.3.6" class="ltx_text ltx_font_typewriter">clothed dog</span> for a clothed dog not led by rope outside) for an instance.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">We use <span id="S3.SS4.p2.1.1" class="ltx_text ltx_font_bold">standard mAP</span> for evaluation. Given the multi-label setting and the exhaustive annotation (all positive and negative labels are known for an instance) of D<sup id="S3.SS4.p2.1.2" class="ltx_sup">3</sup>, category relationships will not affect the evaluation, so we can use consistent evaluation for each category across all images.
We describe the evaluation process here.
For inference, an instance predicted with category A and B is regarded as an instance for category A and an instance for B.
The AP for each category is computed as follows: Predictions for each category across all images are sorted by score in descending order, and those with a ground truth IoU exceeding a threshold are counted as TP (and the ground truth is marked as taken), while the rest are counted as false positives. With these TP and FP instances, we calculate the precision, recall, and AP. The mAP is calculated by averaging the AP across all categories.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p">We use <span id="S3.SS4.p3.1.1" class="ltx_text ltx_font_slanted">FULL</span>, <span id="S3.SS4.p3.1.2" class="ltx_text ltx_font_slanted">PRES</span>, and <span id="S3.SS4.p3.1.3" class="ltx_text ltx_font_slanted">ABS</span> to denote evaluation on all descriptions, presence descriptions only, and absence descriptions only. If not noted explicitly, the <span id="S3.SS4.p3.1.4" class="ltx_text ltx_font_slanted">FULL</span> setting is adopted.
The specific metrics for D<sup id="S3.SS4.p3.1.5" class="ltx_sup">3</sup> include:
<span id="S3.SS4.p3.1.6" class="ltx_text ltx_font_italic">Intra-scenario mAP:</span>
For this metric, we perform evaluation on each image with only the descriptions from the image’s scenario. The final metric is the mAP averaged on different IoU thresholds from 0.5 to 0.95, following COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. This is used as the default metric in our experimental settings.
<span id="S3.SS4.p3.1.7" class="ltx_text ltx_font_italic">Inter-scenario mAP:</span>
It is similar to the intra-scenario mAP described above, except that for each image, we detect the possible instances with all 422 references. This is aligned with the common mAP in object detection datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> and is much more challenging than the intra-scenario mAP.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Baselines</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Existing baselines from different tasks</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We choose multiple advanced methods to verify on D<sup id="S4.SS1.p1.1.1" class="ltx_sup">3</sup> from OVD, REC to bi-functional methods. More details of these methods and their inference process are in our <span id="S4.SS1.p1.1.2" class="ltx_text ltx_font_italic" style="color:#B80000;">supplementary materials</span>.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">REC methods.</span>
We employ the state-of-the-art REC method, OFA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, with two variants.
OFA is based on an encoder-decoder, sequence-to-sequence framework. It is a multi-modal multi-task generalist that deals with different tasks together and is trained on various tasks, including language tasks (masked language modeling), image-to-text tasks (image captioning and Visual Question Answering (VQA)), and localization tasks (REC). Notably, although it is trained with a detection dataset, it is not evaluated on object detection and achieves poor performance if we do. Currently, it holds the SOTA performance on standard REC benchmarks like the RefCOCO series.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">OVD methods.</span>
We evaluate OWL-ViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> with two variants and CORA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>. They are the SOTA methods on OVD tasks, with a vision transformer as well as a language transformer. They are pretrained with image-text contrastive learning and then fine-tuned on detection dataset.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para ltx_noindent">
<p id="S4.SS1.p4.1" class="ltx_p"><span id="S4.SS1.p4.1.1" class="ltx_text ltx_font_bold">Bi-functional methods utilizing both REC and OVD data.</span>
Methods falling into this category are not many but emerging fast recently. We test two methods: Grounding-DINO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> and UNINEXT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>, each with two variants. Both of them are based on DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. They are pretrained on multiple datasets, including detection and REC datasets, and then evaluated with different strategies for different tasks.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>A proposed baseline</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">D<sup id="S4.SS2.p1.1.1" class="ltx_sup">3</sup> is very challenging for existing works, as we will demonstrate in <a href="#S5.SS1" title="5.1 Comparison of baselines on our metrics ‣ 5 Experimental Analyses ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">5.1</span></a>. We have selected one of these works for adjustment to provide a better baseline.
The chosen work should (1) be capable of understanding text of various lengths; (2) excel in their original tasks; (3) have a framework with a rather simple technical design, allowing us to modify its components easily. We have chosen OFA because it (1) is a multi-modal multi-task framework with MLM (Masked Language Modeling) and image-to-text pretraining; (2) achieves SOTA on REC; (3) has a simple seq2seq framework.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">However, OFA faces several problems that make it unsatisfactory for this task, as discussed in <a href="#S5" title="5 Experimental Analyses ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">5</span></a>. First, forcing multiple tasks of different modalities into one seq2seq framework adversely affects the performance of specific tasks, especially tasks related to localization. Second, training on the grounding task results in poor ability to handle multiple instances. We evaluated the model on COCO detection, and it achieved less than 10 mAP. Thirdly, its REC paradigm also makes it predict only one instance, making it unable to reject negative images and irrelevant descriptions.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Therefore, we have made some modifications to OFA to make it more suitable for this task.
The first modification is <span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_bold">granularity decomposition</span> to make it more suitable for localization. We have divided the pretraining tasks of OFA into two different granularities: global tasks (related to language modeling, such as captioning, VQA, MLM, etc.) and local tasks (related to localization, such as detection and REC). We have added an additional decoder parallel to the original decoder in OFA that handles the local tasks, while the original decoder focuses on the global tasks. This alleviates conflicts between different tasks and enhances localization.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">The second modification is <span id="S4.SS2.p4.1.1" class="ltx_text ltx_font_bold">reconstructed data</span> for pretraining on REC, aiming to improve multi-target localization. We have reconstructed the data for REC to ensure that (1) multiple references are input for an image, and (2) a reference does not necessarily correspond to one object, but zero or multiple. This results in a unified data format for detection and REC, although the labels may be noisy since they were not initially prepared for DOD.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.1" class="ltx_p">The third modification is <span id="S4.SS2.p5.1.1" class="ltx_text ltx_font_bold">task decomposition</span> to empower the model with the ability to reject false positives. We have reformulated the training on reconstructed data into two tasks: REC (for locating a region based on a reference) and VQA (for determining if a region and a reference match each other, essentially a binary classification). The second step is responsible for rejecting false positives.</p>
</div>
<div id="S4.SS2.p6" class="ltx_para">
<p id="S4.SS2.p6.2" class="ltx_p">We refer to the model with all three modifications as <span id="S4.SS2.p6.2.1" class="ltx_text ltx_font_bold">OFA-DOD</span>. More details on the proposed improvements can be found in the <span id="S4.SS2.p6.2.2" class="ltx_text ltx_font_italic" style="color:#B80000;">supplementary materials</span>.
It is important to note that this model is far from perfect for the complex D<sup id="S4.SS2.p6.2.3" class="ltx_sup">3</sup> benchmark. As we will show in <a href="#S5.SS1" title="5.1 Comparison of baselines on our metrics ‣ 5 Experimental Analyses ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">5.1</span></a>, although it outperforms existing methods, it serves as a baseline for future tasks on D<sup id="S4.SS2.p6.2.4" class="ltx_sup">3</sup>.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experimental Analyses</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Comparison of baselines on our metrics</h3>

<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T2.12.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S5.T2.13.2" class="ltx_text" style="font-size:90%;">Comparison of different methods on the proposed dataset for different mAP metrics.</span></figcaption>
<table id="S5.T2.10" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T2.10.11.1" class="ltx_tr">
<th id="S5.T2.10.11.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="2"><span id="S5.T2.10.11.1.1.1" class="ltx_text">Task</span></th>
<th id="S5.T2.10.11.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S5.T2.10.11.1.2.1" class="ltx_text">Method</span></th>
<td id="S5.T2.10.11.1.3" class="ltx_td ltx_align_center ltx_border_t" colspan="3">Intra-scenario</td>
<td id="S5.T2.10.11.1.4" class="ltx_td ltx_align_center ltx_border_l ltx_border_t" colspan="3">Inter-scenario</td>
</tr>
<tr id="S5.T2.10.12.2" class="ltx_tr">
<td id="S5.T2.10.12.2.1" class="ltx_td ltx_align_center"><span id="S5.T2.10.12.2.1.1" class="ltx_text ltx_font_slanted">FULL</span></td>
<td id="S5.T2.10.12.2.2" class="ltx_td ltx_align_center"><span id="S5.T2.10.12.2.2.1" class="ltx_text ltx_font_slanted">PRES</span></td>
<td id="S5.T2.10.12.2.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.10.12.2.3.1" class="ltx_text ltx_font_slanted">ABS</span></td>
<td id="S5.T2.10.12.2.4" class="ltx_td ltx_align_center"><span id="S5.T2.10.12.2.4.1" class="ltx_text ltx_font_slanted">FULL</span></td>
<td id="S5.T2.10.12.2.5" class="ltx_td ltx_align_center"><span id="S5.T2.10.12.2.5.1" class="ltx_text ltx_font_slanted">PRES</span></td>
<td id="S5.T2.10.12.2.6" class="ltx_td ltx_align_center"><span id="S5.T2.10.12.2.6.1" class="ltx_text ltx_font_slanted">ABS</span></td>
</tr>
<tr id="S5.T2.1.1" class="ltx_tr">
<th id="S5.T2.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="2"><span id="S5.T2.1.1.2.1" class="ltx_text">REC</span></th>
<th id="S5.T2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">OFA<math id="S5.T2.1.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{base}}" display="inline"><semantics id="S5.T2.1.1.1.m1.1a"><msub id="S5.T2.1.1.1.m1.1.1" xref="S5.T2.1.1.1.m1.1.1.cmml"><mi id="S5.T2.1.1.1.m1.1.1a" xref="S5.T2.1.1.1.m1.1.1.cmml"></mi><mtext id="S5.T2.1.1.1.m1.1.1.1" xref="S5.T2.1.1.1.m1.1.1.1a.cmml">base</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.m1.1b"><apply id="S5.T2.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.m1.1.1"><ci id="S5.T2.1.1.1.m1.1.1.1a.cmml" xref="S5.T2.1.1.1.m1.1.1.1"><mtext mathsize="70%" id="S5.T2.1.1.1.m1.1.1.1.cmml" xref="S5.T2.1.1.1.m1.1.1.1">base</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.m1.1c">{}_{\text{base}}</annotation></semantics></math>
</th>
<td id="S5.T2.1.1.3" class="ltx_td ltx_align_center ltx_border_t">3.4</td>
<td id="S5.T2.1.1.4" class="ltx_td ltx_align_center ltx_border_t">3.0</td>
<td id="S5.T2.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.3</td>
<td id="S5.T2.1.1.6" class="ltx_td ltx_align_center ltx_border_t">0.1</td>
<td id="S5.T2.1.1.7" class="ltx_td ltx_align_center ltx_border_t">0.1</td>
<td id="S5.T2.1.1.8" class="ltx_td ltx_align_center ltx_border_t">0.1</td>
</tr>
<tr id="S5.T2.2.2" class="ltx_tr">
<th id="S5.T2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">OFA<math id="S5.T2.2.2.1.m1.1" class="ltx_Math" alttext="{}_{\text{large}}" display="inline"><semantics id="S5.T2.2.2.1.m1.1a"><msub id="S5.T2.2.2.1.m1.1.1" xref="S5.T2.2.2.1.m1.1.1.cmml"><mi id="S5.T2.2.2.1.m1.1.1a" xref="S5.T2.2.2.1.m1.1.1.cmml"></mi><mtext id="S5.T2.2.2.1.m1.1.1.1" xref="S5.T2.2.2.1.m1.1.1.1a.cmml">large</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T2.2.2.1.m1.1b"><apply id="S5.T2.2.2.1.m1.1.1.cmml" xref="S5.T2.2.2.1.m1.1.1"><ci id="S5.T2.2.2.1.m1.1.1.1a.cmml" xref="S5.T2.2.2.1.m1.1.1.1"><mtext mathsize="70%" id="S5.T2.2.2.1.m1.1.1.1.cmml" xref="S5.T2.2.2.1.m1.1.1.1">large</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.2.1.m1.1c">{}_{\text{large}}</annotation></semantics></math>
</th>
<td id="S5.T2.2.2.2" class="ltx_td ltx_align_center">4.2</td>
<td id="S5.T2.2.2.3" class="ltx_td ltx_align_center">4.1</td>
<td id="S5.T2.2.2.4" class="ltx_td ltx_align_center ltx_border_r">4.6</td>
<td id="S5.T2.2.2.5" class="ltx_td ltx_align_center">0.1</td>
<td id="S5.T2.2.2.6" class="ltx_td ltx_align_center">0.1</td>
<td id="S5.T2.2.2.7" class="ltx_td ltx_align_center">0.1</td>
</tr>
<tr id="S5.T2.3.3" class="ltx_tr">
<th id="S5.T2.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="3"><span id="S5.T2.3.3.2.1" class="ltx_text">OVD</span></th>
<th id="S5.T2.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">CORA<math id="S5.T2.3.3.1.m1.1" class="ltx_Math" alttext="{}_{\text{R50}}" display="inline"><semantics id="S5.T2.3.3.1.m1.1a"><msub id="S5.T2.3.3.1.m1.1.1" xref="S5.T2.3.3.1.m1.1.1.cmml"><mi id="S5.T2.3.3.1.m1.1.1a" xref="S5.T2.3.3.1.m1.1.1.cmml"></mi><mtext id="S5.T2.3.3.1.m1.1.1.1" xref="S5.T2.3.3.1.m1.1.1.1a.cmml">R50</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T2.3.3.1.m1.1b"><apply id="S5.T2.3.3.1.m1.1.1.cmml" xref="S5.T2.3.3.1.m1.1.1"><ci id="S5.T2.3.3.1.m1.1.1.1a.cmml" xref="S5.T2.3.3.1.m1.1.1.1"><mtext mathsize="70%" id="S5.T2.3.3.1.m1.1.1.1.cmml" xref="S5.T2.3.3.1.m1.1.1.1">R50</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.3.1.m1.1c">{}_{\text{R50}}</annotation></semantics></math>
</th>
<td id="S5.T2.3.3.3" class="ltx_td ltx_align_center ltx_border_t">6.2</td>
<td id="S5.T2.3.3.4" class="ltx_td ltx_align_center ltx_border_t">6.7</td>
<td id="S5.T2.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5.0</td>
<td id="S5.T2.3.3.6" class="ltx_td ltx_align_center ltx_border_t">2.0</td>
<td id="S5.T2.3.3.7" class="ltx_td ltx_align_center ltx_border_t">2.2</td>
<td id="S5.T2.3.3.8" class="ltx_td ltx_align_center ltx_border_t">1.3</td>
</tr>
<tr id="S5.T2.4.4" class="ltx_tr">
<th id="S5.T2.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">OWL-ViT<math id="S5.T2.4.4.1.m1.1" class="ltx_Math" alttext="{}_{\text{base}}" display="inline"><semantics id="S5.T2.4.4.1.m1.1a"><msub id="S5.T2.4.4.1.m1.1.1" xref="S5.T2.4.4.1.m1.1.1.cmml"><mi id="S5.T2.4.4.1.m1.1.1a" xref="S5.T2.4.4.1.m1.1.1.cmml"></mi><mtext id="S5.T2.4.4.1.m1.1.1.1" xref="S5.T2.4.4.1.m1.1.1.1a.cmml">base</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T2.4.4.1.m1.1b"><apply id="S5.T2.4.4.1.m1.1.1.cmml" xref="S5.T2.4.4.1.m1.1.1"><ci id="S5.T2.4.4.1.m1.1.1.1a.cmml" xref="S5.T2.4.4.1.m1.1.1.1"><mtext mathsize="70%" id="S5.T2.4.4.1.m1.1.1.1.cmml" xref="S5.T2.4.4.1.m1.1.1.1">base</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.4.4.1.m1.1c">{}_{\text{base}}</annotation></semantics></math>
</th>
<td id="S5.T2.4.4.2" class="ltx_td ltx_align_center">8.6</td>
<td id="S5.T2.4.4.3" class="ltx_td ltx_align_center">8.5</td>
<td id="S5.T2.4.4.4" class="ltx_td ltx_align_center ltx_border_r">8.8</td>
<td id="S5.T2.4.4.5" class="ltx_td ltx_align_center">3.2</td>
<td id="S5.T2.4.4.6" class="ltx_td ltx_align_center">3.7</td>
<td id="S5.T2.4.4.7" class="ltx_td ltx_align_center"><span id="S5.T2.4.4.7.1" class="ltx_text ltx_font_bold">4.7</span></td>
</tr>
<tr id="S5.T2.5.5" class="ltx_tr">
<th id="S5.T2.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">OWL-ViT<math id="S5.T2.5.5.1.m1.1" class="ltx_Math" alttext="{}_{\text{large}}" display="inline"><semantics id="S5.T2.5.5.1.m1.1a"><msub id="S5.T2.5.5.1.m1.1.1" xref="S5.T2.5.5.1.m1.1.1.cmml"><mi id="S5.T2.5.5.1.m1.1.1a" xref="S5.T2.5.5.1.m1.1.1.cmml"></mi><mtext id="S5.T2.5.5.1.m1.1.1.1" xref="S5.T2.5.5.1.m1.1.1.1a.cmml">large</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T2.5.5.1.m1.1b"><apply id="S5.T2.5.5.1.m1.1.1.cmml" xref="S5.T2.5.5.1.m1.1.1"><ci id="S5.T2.5.5.1.m1.1.1.1a.cmml" xref="S5.T2.5.5.1.m1.1.1.1"><mtext mathsize="70%" id="S5.T2.5.5.1.m1.1.1.1.cmml" xref="S5.T2.5.5.1.m1.1.1.1">large</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.5.5.1.m1.1c">{}_{\text{large}}</annotation></semantics></math>
</th>
<td id="S5.T2.5.5.2" class="ltx_td ltx_align_center">9.6</td>
<td id="S5.T2.5.5.3" class="ltx_td ltx_align_center">10.7</td>
<td id="S5.T2.5.5.4" class="ltx_td ltx_align_center ltx_border_r">6.4</td>
<td id="S5.T2.5.5.5" class="ltx_td ltx_align_center">2.5</td>
<td id="S5.T2.5.5.6" class="ltx_td ltx_align_center">2.9</td>
<td id="S5.T2.5.5.7" class="ltx_td ltx_align_center">2.1</td>
</tr>
<tr id="S5.T2.6.6" class="ltx_tr">
<th id="S5.T2.6.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="4"><span id="S5.T2.6.6.2.1" class="ltx_text">Bi-functional</span></th>
<th id="S5.T2.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">UNINEXT<math id="S5.T2.6.6.1.m1.1" class="ltx_Math" alttext="{}_{\text{large}}" display="inline"><semantics id="S5.T2.6.6.1.m1.1a"><msub id="S5.T2.6.6.1.m1.1.1" xref="S5.T2.6.6.1.m1.1.1.cmml"><mi id="S5.T2.6.6.1.m1.1.1a" xref="S5.T2.6.6.1.m1.1.1.cmml"></mi><mtext id="S5.T2.6.6.1.m1.1.1.1" xref="S5.T2.6.6.1.m1.1.1.1a.cmml">large</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T2.6.6.1.m1.1b"><apply id="S5.T2.6.6.1.m1.1.1.cmml" xref="S5.T2.6.6.1.m1.1.1"><ci id="S5.T2.6.6.1.m1.1.1.1a.cmml" xref="S5.T2.6.6.1.m1.1.1.1"><mtext mathsize="70%" id="S5.T2.6.6.1.m1.1.1.1.cmml" xref="S5.T2.6.6.1.m1.1.1.1">large</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.6.6.1.m1.1c">{}_{\text{large}}</annotation></semantics></math>
</th>
<td id="S5.T2.6.6.3" class="ltx_td ltx_align_center ltx_border_t">17.9</td>
<td id="S5.T2.6.6.4" class="ltx_td ltx_align_center ltx_border_t">18.6</td>
<td id="S5.T2.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">15.9</td>
<td id="S5.T2.6.6.6" class="ltx_td ltx_align_center ltx_border_t">2.9</td>
<td id="S5.T2.6.6.7" class="ltx_td ltx_align_center ltx_border_t">3.1</td>
<td id="S5.T2.6.6.8" class="ltx_td ltx_align_center ltx_border_t">2.5</td>
</tr>
<tr id="S5.T2.7.7" class="ltx_tr">
<th id="S5.T2.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">UNINEXT<math id="S5.T2.7.7.1.m1.1" class="ltx_Math" alttext="{}_{\text{huge}}" display="inline"><semantics id="S5.T2.7.7.1.m1.1a"><msub id="S5.T2.7.7.1.m1.1.1" xref="S5.T2.7.7.1.m1.1.1.cmml"><mi id="S5.T2.7.7.1.m1.1.1a" xref="S5.T2.7.7.1.m1.1.1.cmml"></mi><mtext id="S5.T2.7.7.1.m1.1.1.1" xref="S5.T2.7.7.1.m1.1.1.1a.cmml">huge</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T2.7.7.1.m1.1b"><apply id="S5.T2.7.7.1.m1.1.1.cmml" xref="S5.T2.7.7.1.m1.1.1"><ci id="S5.T2.7.7.1.m1.1.1.1a.cmml" xref="S5.T2.7.7.1.m1.1.1.1"><mtext mathsize="70%" id="S5.T2.7.7.1.m1.1.1.1.cmml" xref="S5.T2.7.7.1.m1.1.1.1">huge</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.7.7.1.m1.1c">{}_{\text{huge}}</annotation></semantics></math>
</th>
<td id="S5.T2.7.7.2" class="ltx_td ltx_align_center">20.0</td>
<td id="S5.T2.7.7.3" class="ltx_td ltx_align_center">20.6</td>
<td id="S5.T2.7.7.4" class="ltx_td ltx_align_center ltx_border_r">18.1</td>
<td id="S5.T2.7.7.5" class="ltx_td ltx_align_center">3.3</td>
<td id="S5.T2.7.7.6" class="ltx_td ltx_align_center">3.9</td>
<td id="S5.T2.7.7.7" class="ltx_td ltx_align_center">1.6</td>
</tr>
<tr id="S5.T2.8.8" class="ltx_tr">
<th id="S5.T2.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">G-DINO<math id="S5.T2.8.8.1.m1.1" class="ltx_Math" alttext="{}_{\text{tiny}}" display="inline"><semantics id="S5.T2.8.8.1.m1.1a"><msub id="S5.T2.8.8.1.m1.1.1" xref="S5.T2.8.8.1.m1.1.1.cmml"><mi id="S5.T2.8.8.1.m1.1.1a" xref="S5.T2.8.8.1.m1.1.1.cmml"></mi><mtext id="S5.T2.8.8.1.m1.1.1.1" xref="S5.T2.8.8.1.m1.1.1.1a.cmml">tiny</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T2.8.8.1.m1.1b"><apply id="S5.T2.8.8.1.m1.1.1.cmml" xref="S5.T2.8.8.1.m1.1.1"><ci id="S5.T2.8.8.1.m1.1.1.1a.cmml" xref="S5.T2.8.8.1.m1.1.1.1"><mtext mathsize="70%" id="S5.T2.8.8.1.m1.1.1.1.cmml" xref="S5.T2.8.8.1.m1.1.1.1">tiny</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.8.8.1.m1.1c">{}_{\text{tiny}}</annotation></semantics></math>
</th>
<td id="S5.T2.8.8.2" class="ltx_td ltx_align_center">19.2</td>
<td id="S5.T2.8.8.3" class="ltx_td ltx_align_center">18.5</td>
<td id="S5.T2.8.8.4" class="ltx_td ltx_align_center ltx_border_r">21.2</td>
<td id="S5.T2.8.8.5" class="ltx_td ltx_align_center">2.3</td>
<td id="S5.T2.8.8.6" class="ltx_td ltx_align_center">2.5</td>
<td id="S5.T2.8.8.7" class="ltx_td ltx_align_center">2.1</td>
</tr>
<tr id="S5.T2.9.9" class="ltx_tr">
<th id="S5.T2.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">G-DINO<math id="S5.T2.9.9.1.m1.1" class="ltx_Math" alttext="{}_{\text{base}}" display="inline"><semantics id="S5.T2.9.9.1.m1.1a"><msub id="S5.T2.9.9.1.m1.1.1" xref="S5.T2.9.9.1.m1.1.1.cmml"><mi id="S5.T2.9.9.1.m1.1.1a" xref="S5.T2.9.9.1.m1.1.1.cmml"></mi><mtext id="S5.T2.9.9.1.m1.1.1.1" xref="S5.T2.9.9.1.m1.1.1.1a.cmml">base</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T2.9.9.1.m1.1b"><apply id="S5.T2.9.9.1.m1.1.1.cmml" xref="S5.T2.9.9.1.m1.1.1"><ci id="S5.T2.9.9.1.m1.1.1.1a.cmml" xref="S5.T2.9.9.1.m1.1.1.1"><mtext mathsize="70%" id="S5.T2.9.9.1.m1.1.1.1.cmml" xref="S5.T2.9.9.1.m1.1.1.1">base</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.9.9.1.m1.1c">{}_{\text{base}}</annotation></semantics></math>
</th>
<td id="S5.T2.9.9.2" class="ltx_td ltx_align_center">20.7</td>
<td id="S5.T2.9.9.3" class="ltx_td ltx_align_center">20.1</td>
<td id="S5.T2.9.9.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.9.9.4.1" class="ltx_text ltx_font_bold">22.5</span></td>
<td id="S5.T2.9.9.5" class="ltx_td ltx_align_center">2.7</td>
<td id="S5.T2.9.9.6" class="ltx_td ltx_align_center">2.4</td>
<td id="S5.T2.9.9.7" class="ltx_td ltx_align_center">3.5</td>
</tr>
<tr id="S5.T2.10.10" class="ltx_tr">
<th id="S5.T2.10.10.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t">DOD</th>
<th id="S5.T2.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">OFA-DOD<math id="S5.T2.10.10.1.m1.1" class="ltx_Math" alttext="{}_{\text{base}}" display="inline"><semantics id="S5.T2.10.10.1.m1.1a"><msub id="S5.T2.10.10.1.m1.1.1" xref="S5.T2.10.10.1.m1.1.1.cmml"><mi id="S5.T2.10.10.1.m1.1.1a" xref="S5.T2.10.10.1.m1.1.1.cmml"></mi><mtext id="S5.T2.10.10.1.m1.1.1.1" xref="S5.T2.10.10.1.m1.1.1.1a.cmml">base</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T2.10.10.1.m1.1b"><apply id="S5.T2.10.10.1.m1.1.1.cmml" xref="S5.T2.10.10.1.m1.1.1"><ci id="S5.T2.10.10.1.m1.1.1.1a.cmml" xref="S5.T2.10.10.1.m1.1.1.1"><mtext mathsize="70%" id="S5.T2.10.10.1.m1.1.1.1.cmml" xref="S5.T2.10.10.1.m1.1.1.1">base</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.10.10.1.m1.1c">{}_{\text{base}}</annotation></semantics></math>
</th>
<td id="S5.T2.10.10.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S5.T2.10.10.3.1" class="ltx_text ltx_font_bold">21.6</span></td>
<td id="S5.T2.10.10.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S5.T2.10.10.4.1" class="ltx_text ltx_font_bold">23.7</span></td>
<td id="S5.T2.10.10.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">15.4</td>
<td id="S5.T2.10.10.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S5.T2.10.10.6.1" class="ltx_text ltx_font_bold">5.7</span></td>
<td id="S5.T2.10.10.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S5.T2.10.10.7.1" class="ltx_text ltx_font_bold">6.9</span></td>
<td id="S5.T2.10.10.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">2.3</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">We make comparisons on the baselines introduced in <a href="#S4" title="4 Baselines ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4</span></a>, mainly with the intra-scenario setting.
Unless explicitly noted, this is the default setting, instead of the more difficult inter-scenario.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para ltx_noindent">
<p id="S5.SS1.p2.1" class="ltx_p"><span id="S5.SS1.p2.1.1" class="ltx_text ltx_font_bold">Existing SOTAs are insufficient for DOD, and bi-functional models outperform others.</span>
As demonstrated in <a href="#S5.T2" title="In 5.1 Comparison of baselines on our metrics ‣ 5 Experimental Analyses ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">2</span></a>, existing methods, while achieving SOTA performance on their original benchmarks, fall short in delivering strong performance on D<sup id="S5.SS1.p2.1.2" class="ltx_sup">3</sup>.
Among them, recent bi-functional methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> are notably superior to others, and currently, OVD methods outperform REC.
The inferiority of REC methods is likely due to their impractical setting described in <a href="#S1" title="1 Introduction ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">1</span></a>, which involves predicting one and only one instance for each reference. We will delve into this further.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para ltx_noindent">
<p id="S5.SS1.p3.1" class="ltx_p"><span id="S5.SS1.p3.1.1" class="ltx_text ltx_font_bold">Rejecting irrelevant references are difficult, which REC are naturally incapable of.</span>
In contrast to intra-scenario evaluation, the inter-scenario setting assesses all references in the dataset for each image. Since references from other scenarios are likely not semantically relevant to the images, this necessitates the ability to reject irrelevant references for an image. This aligns with the evaluation in standard detection tasks.
From <a href="#S5.T2" title="In 5.1 Comparison of baselines on our metrics ‣ 5 Experimental Analyses ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">2</span></a>, it is evident that OFA, a REC method, almost completely fails in this setting. This is caused by its prediction of a region for every reference, resulting in a large number of false positives when there are numerous candidate references. This underscores the importance of empowering REC methods with the ability to reject false positives.
We find that none of the verified methods achieve good performance under the inter-scenario setting, indicating that existing methods are far from being capable of DOD. This highlights the challenge of D<sup id="S5.SS1.p3.1.2" class="ltx_sup">3</sup></p>
</div>
<div id="S5.SS1.p4" class="ltx_para ltx_noindent">
<p id="S5.SS1.p4.1" class="ltx_p"><span id="S5.SS1.p4.1.1" class="ltx_text ltx_font_bold">The proposed baseline outperforms existing methods.</span>
The proposed baseline is based on OFA, but our improvements significantly enhance its performance. It outperforms all existing methods in the intra-scenario setting and surpasses them by a wider margin in the inter-scenario setting. This may suggest that the proposed baseline has a stronger ability to reject irrelevant references.
Nonetheless, the proposed method is far from perfect and can only serve as a baseline for future research.</p>
</div>
<figure id="S5.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S5.F4.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2307.12813/assets/x9.png" id="S5.F4.sf1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="373" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F4.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S5.F4.sf1.3.2" class="ltx_text" style="font-size:90%;">OFA</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S5.F4.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2307.12813/assets/x10.png" id="S5.F4.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="355" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F4.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S5.F4.sf2.3.2" class="ltx_text" style="font-size:90%;">OWL-ViT</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S5.F4.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2307.12813/assets/x11.png" id="S5.F4.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="366" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F4.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S5.F4.sf3.3.2" class="ltx_text" style="font-size:90%;">Grounding-DINO</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S5.F4.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2307.12813/assets/x12.png" id="S5.F4.sf4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="373" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F4.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S5.F4.sf4.3.2" class="ltx_text" style="font-size:90%;">OFA-DOD</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S5.F4.3.2" class="ltx_text" style="font-size:90%;">Distribution of TP and FP scores from different baseline methods.</span></figcaption>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Further analysis</h3>

<div id="S5.SS2.p1" class="ltx_para ltx_noindent">
<p id="S5.SS2.p1.1" class="ltx_p"><span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_bold">Absence descriptions are more difficult for most methods.</span>
As shown in <a href="#S5.T2" title="In 5.1 Comparison of baselines on our metrics ‣ 5 Experimental Analyses ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">2</span></a>, the performance of baseline methods on <span id="S5.SS2.p1.1.2" class="ltx_text ltx_font_slanted">PRES</span> (presence descriptions) is consistently superior to that on <span id="S5.SS2.p1.1.3" class="ltx_text ltx_font_slanted">ABS</span> (absence descriptions). This suggests that existing methods may not effectively differentiate between the presence and absence of attributes in a language description.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para ltx_noindent">
<p id="S5.SS2.p2.1" class="ltx_p"><span id="S5.SS2.p2.1.1" class="ltx_text ltx_font_bold">REC methods fail to provide good confidence scores.</span>
We visualized the score distributions from baselines for TPs and FPs, to assess their capabilities in classification and confidence estimation.
As in <a href="#S5.F4" title="In 5.1 Comparison of baselines on our metrics ‣ 5 Experimental Analyses ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>, the confidence scores from OFA do not exhibit a clear distinction between TP and FP cases. This can be attributed in part to the seq2seq framework in OFA, which does not directly yield confidence scores, and in part to the grounding formulation of REC, which identifies the image region most similar to the text description without distinguishing between positive and negative.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">With a task decomposition step to enhance binary classification performance, our OFA-DOD demonstrates a significant disparity between TP and FP score distributions, yielding more reliable classification results. Note that this improvement does not necessitate modifications to the model framework or training datasets; rather, it is attributed to a more appropriate task formulation.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T3.5.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S5.T3.6.2" class="ltx_text" style="font-size:90%;">Evaluation regarding different number of instances in a image for each reference.</span></figcaption>
<table id="S5.T3.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T3.1.1" class="ltx_tr">
<td id="S5.T3.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Method</td>
<td id="S5.T3.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">No-instance</td>
<td id="S5.T3.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">One-instance</td>
<td id="S5.T3.1.1.1" class="ltx_td ltx_align_right ltx_border_t" colspan="4">Multi-instance mAP(%)    <math id="S5.T3.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T3.1.1.1.m1.1a"><mo stretchy="false" id="S5.T3.1.1.1.m1.1.1" xref="S5.T3.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.m1.1b"><ci id="S5.T3.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.m1.1c">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr id="S5.T3.3.3" class="ltx_tr">
<td id="S5.T3.3.3.3" class="ltx_td ltx_border_r"></td>
<td id="S5.T3.2.2.1" class="ltx_td ltx_align_center ltx_border_r">FPPC (%) <math id="S5.T3.2.2.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T3.2.2.1.m1.1a"><mo stretchy="false" id="S5.T3.2.2.1.m1.1.1" xref="S5.T3.2.2.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T3.2.2.1.m1.1b"><ci id="S5.T3.2.2.1.m1.1.1.cmml" xref="S5.T3.2.2.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.2.1.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S5.T3.3.3.2" class="ltx_td ltx_align_center ltx_border_r">mAP (%) <math id="S5.T3.3.3.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T3.3.3.2.m1.1a"><mo stretchy="false" id="S5.T3.3.3.2.m1.1.1" xref="S5.T3.3.3.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T3.3.3.2.m1.1b"><ci id="S5.T3.3.3.2.m1.1.1.cmml" xref="S5.T3.3.3.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.3.2.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S5.T3.3.3.4" class="ltx_td ltx_align_center">2</td>
<td id="S5.T3.3.3.5" class="ltx_td ltx_align_center">3</td>
<td id="S5.T3.3.3.6" class="ltx_td ltx_align_center">4</td>
<td id="S5.T3.3.3.7" class="ltx_td ltx_align_center">4+</td>
</tr>
<tr id="S5.T3.3.4.1" class="ltx_tr">
<td id="S5.T3.3.4.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">OFA</td>
<td id="S5.T3.3.4.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">100.0</td>
<td id="S5.T3.3.4.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">14.8</td>
<td id="S5.T3.3.4.1.4" class="ltx_td ltx_align_center ltx_border_t">9.5</td>
<td id="S5.T3.3.4.1.5" class="ltx_td ltx_align_center ltx_border_t">7.9</td>
<td id="S5.T3.3.4.1.6" class="ltx_td ltx_align_center ltx_border_t">5.4</td>
<td id="S5.T3.3.4.1.7" class="ltx_td ltx_align_center ltx_border_t">3.7</td>
</tr>
<tr id="S5.T3.3.5.2" class="ltx_tr">
<td id="S5.T3.3.5.2.1" class="ltx_td ltx_align_left ltx_border_r">CORA</td>
<td id="S5.T3.3.5.2.2" class="ltx_td ltx_align_center ltx_border_r">17.3</td>
<td id="S5.T3.3.5.2.3" class="ltx_td ltx_align_center ltx_border_r">9.7</td>
<td id="S5.T3.3.5.2.4" class="ltx_td ltx_align_center">8.4</td>
<td id="S5.T3.3.5.2.5" class="ltx_td ltx_align_center">9.5</td>
<td id="S5.T3.3.5.2.6" class="ltx_td ltx_align_center">9.0</td>
<td id="S5.T3.3.5.2.7" class="ltx_td ltx_align_center">8.5</td>
</tr>
<tr id="S5.T3.3.6.3" class="ltx_tr">
<td id="S5.T3.3.6.3.1" class="ltx_td ltx_align_left ltx_border_r">OWL-ViT</td>
<td id="S5.T3.3.6.3.2" class="ltx_td ltx_align_center ltx_border_r">41.9</td>
<td id="S5.T3.3.6.3.3" class="ltx_td ltx_align_center ltx_border_r">21.1</td>
<td id="S5.T3.3.6.3.4" class="ltx_td ltx_align_center">17.3</td>
<td id="S5.T3.3.6.3.5" class="ltx_td ltx_align_center">16.6</td>
<td id="S5.T3.3.6.3.6" class="ltx_td ltx_align_center">16.0</td>
<td id="S5.T3.3.6.3.7" class="ltx_td ltx_align_center">14.0</td>
</tr>
<tr id="S5.T3.3.7.4" class="ltx_tr">
<td id="S5.T3.3.7.4.1" class="ltx_td ltx_align_left ltx_border_r">UNINEXT</td>
<td id="S5.T3.3.7.4.2" class="ltx_td ltx_align_center ltx_border_r">100.0</td>
<td id="S5.T3.3.7.4.3" class="ltx_td ltx_align_center ltx_border_r">55.7</td>
<td id="S5.T3.3.7.4.4" class="ltx_td ltx_align_center">26.2</td>
<td id="S5.T3.3.7.4.5" class="ltx_td ltx_align_center">18.6</td>
<td id="S5.T3.3.7.4.6" class="ltx_td ltx_align_center">14.4</td>
<td id="S5.T3.3.7.4.7" class="ltx_td ltx_align_center">9.0</td>
</tr>
<tr id="S5.T3.3.8.5" class="ltx_tr">
<td id="S5.T3.3.8.5.1" class="ltx_td ltx_align_left ltx_border_r">G-DINO</td>
<td id="S5.T3.3.8.5.2" class="ltx_td ltx_align_center ltx_border_r">100.0</td>
<td id="S5.T3.3.8.5.3" class="ltx_td ltx_align_center ltx_border_r">63.7</td>
<td id="S5.T3.3.8.5.4" class="ltx_td ltx_align_center">28.3</td>
<td id="S5.T3.3.8.5.5" class="ltx_td ltx_align_center">19.7</td>
<td id="S5.T3.3.8.5.6" class="ltx_td ltx_align_center">15.9</td>
<td id="S5.T3.3.8.5.7" class="ltx_td ltx_align_center">10.1</td>
</tr>
<tr id="S5.T3.3.9.6" class="ltx_tr">
<td id="S5.T3.3.9.6.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">OFA-DOD</td>
<td id="S5.T3.3.9.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">35.6</td>
<td id="S5.T3.3.9.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">56.4</td>
<td id="S5.T3.3.9.6.4" class="ltx_td ltx_align_center ltx_border_b">19.6</td>
<td id="S5.T3.3.9.6.5" class="ltx_td ltx_align_center ltx_border_b">12.7</td>
<td id="S5.T3.3.9.6.6" class="ltx_td ltx_align_center ltx_border_b">10.3</td>
<td id="S5.T3.3.9.6.7" class="ltx_td ltx_align_center ltx_border_b">7.1</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS2.p4" class="ltx_para ltx_noindent">
<p id="S5.SS2.p4.1" class="ltx_p"><span id="S5.SS2.p4.1.1" class="ltx_text ltx_font_bold">Multi-instance detection is challenging for methods other than OVD.</span>
For each image, D<sup id="S5.SS2.p4.1.2" class="ltx_sup">3</sup> can have zero to multiple instances <span id="S5.SS2.p4.1.3" class="ltx_text ltx_font_bold">for a single description</span>. To assess how current methods handle varying numbers of instances, we conducted evaluations under three different settings: <span id="S5.SS2.p4.1.4" class="ltx_text ltx_font_bold">no-instance</span>, where for a reference, evaluations are limited to images without any referred instance; <span id="S5.SS2.p4.1.5" class="ltx_text ltx_font_bold">one-instance</span>, for images with a single instance; and <span id="S5.SS2.p4.1.6" class="ltx_text ltx_font_bold">multi-instance</span>, for images with multiple instances. As shown in <a href="#S5.T3" title="In 5.2 Further analysis ‣ 5 Experimental Analyses ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">3</span></a>, OVD methods outperform others when multiple instances are referred by the description, although they may not be as competitive on the entire dataset or images with few instances. Notably, OWL-ViT maintains consistent performance even as the number of instances increases, which sets it apart from other methods. In contrast, REC and current bi-functional methods struggle in multi-instance scenarios. This highlights the strength of OVD methods in multi-target detection, while REC and current bi-functional approaches are less robust in such situations.</p>
</div>
<div id="S5.SS2.p5" class="ltx_para ltx_noindent">
<p id="S5.SS2.p5.1" class="ltx_p"><span id="S5.SS2.p5.1.1" class="ltx_text ltx_font_bold">REC and bi-functional methods lack the ability to reject negative instances.</span> In the <span id="S5.SS2.p5.1.2" class="ltx_text ltx_font_bold">no-instance</span> column of <a href="#S5.T3" title="In 5.2 Further analysis ‣ 5 Experimental Analyses ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">3</span></a>, we do not report mAP since there are no positive instances in GT for the corresponding reference, making AP inapplicable. Predictions on such images are FPs, so we measure the ratio of images where FPs are produced to the total number of no-instance images for a given reference, namely False Positives Per Category (FPPC). We report the average FPPC over all references.
We observe that most baselines are incapable of determining whether an image contains the referred target or not, yet they still produce predictions. This behavior is expected for REC methods. Bi-functional methods, trained and inferred with the REC task formulation, also exhibit this issue. Only the OVD method and our proposed baseline can effectively reject such negative image-text pairs.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T4.2.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S5.T4.3.2" class="ltx_text" style="font-size:90%;">Evaluation one references with various lengths.</span></figcaption>
<table id="S5.T4.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T4.4.1.1" class="ltx_tr">
<th id="S5.T4.4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">Method</th>
<th id="S5.T4.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T4.4.1.1.2.1" class="ltx_text ltx_font_italic">short</span></th>
<th id="S5.T4.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T4.4.1.1.3.1" class="ltx_text ltx_font_italic">middle</span></th>
<th id="S5.T4.4.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T4.4.1.1.4.1" class="ltx_text ltx_font_italic">long</span></th>
<th id="S5.T4.4.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T4.4.1.1.5.1" class="ltx_text ltx_font_italic">very long</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T4.4.2.1" class="ltx_tr">
<th id="S5.T4.4.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">OFA</th>
<td id="S5.T4.4.2.1.2" class="ltx_td ltx_align_center ltx_border_t">4.9</td>
<td id="S5.T4.4.2.1.3" class="ltx_td ltx_align_center ltx_border_t">5.4</td>
<td id="S5.T4.4.2.1.4" class="ltx_td ltx_align_center ltx_border_t">3.0</td>
<td id="S5.T4.4.2.1.5" class="ltx_td ltx_align_center ltx_border_t">2.1</td>
</tr>
<tr id="S5.T4.4.3.2" class="ltx_tr">
<th id="S5.T4.4.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">OWL-ViT</th>
<td id="S5.T4.4.3.2.2" class="ltx_td ltx_align_center">20.7</td>
<td id="S5.T4.4.3.2.3" class="ltx_td ltx_align_center">9.4</td>
<td id="S5.T4.4.3.2.4" class="ltx_td ltx_align_center">6.0</td>
<td id="S5.T4.4.3.2.5" class="ltx_td ltx_align_center">5.3</td>
</tr>
<tr id="S5.T4.4.4.3" class="ltx_tr">
<th id="S5.T4.4.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">UNINEXT</th>
<td id="S5.T4.4.4.3.2" class="ltx_td ltx_align_center">18.5</td>
<td id="S5.T4.4.4.3.3" class="ltx_td ltx_align_center">23.3</td>
<td id="S5.T4.4.4.3.4" class="ltx_td ltx_align_center">17.4</td>
<td id="S5.T4.4.4.3.5" class="ltx_td ltx_align_center">16.1</td>
</tr>
<tr id="S5.T4.4.5.4" class="ltx_tr">
<th id="S5.T4.4.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">G-DINO</th>
<td id="S5.T4.4.5.4.2" class="ltx_td ltx_align_center">22.6</td>
<td id="S5.T4.4.5.4.3" class="ltx_td ltx_align_center">22.5</td>
<td id="S5.T4.4.5.4.4" class="ltx_td ltx_align_center">18.9</td>
<td id="S5.T4.4.5.4.5" class="ltx_td ltx_align_center">16.5</td>
</tr>
<tr id="S5.T4.4.6.5" class="ltx_tr">
<th id="S5.T4.4.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r">OFA-DOD</th>
<td id="S5.T4.4.6.5.2" class="ltx_td ltx_align_center ltx_border_b">23.6</td>
<td id="S5.T4.4.6.5.3" class="ltx_td ltx_align_center ltx_border_b">22.6</td>
<td id="S5.T4.4.6.5.4" class="ltx_td ltx_align_center ltx_border_b">20.5</td>
<td id="S5.T4.4.6.5.5" class="ltx_td ltx_align_center ltx_border_b">18.4</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS2.p6" class="ltx_para ltx_noindent">
<p id="S5.SS2.p6.1" class="ltx_p"><span id="S5.SS2.p6.1.1" class="ltx_text ltx_font_bold">OVD methods suffer from long descriptions greatly while others do not.</span>
We partition the references according to their lengths and then evaluate on these partitions. The results are shown in <a href="#S5.T4" title="In 5.2 Further analysis ‣ 5 Experimental Analyses ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">4</span></a>, where <span id="S5.SS2.p6.1.2" class="ltx_text ltx_font_italic">short</span>, <span id="S5.SS2.p6.1.3" class="ltx_text ltx_font_italic">middle</span>, <span id="S5.SS2.p6.1.4" class="ltx_text ltx_font_italic">long</span> and <span id="S5.SS2.p6.1.5" class="ltx_text ltx_font_italic">very long</span> corresponding to references with 1~3, 4~6, 7~9, and more than 9 words.
For <span id="S5.SS2.p6.1.6" class="ltx_text ltx_font_italic">short</span> descriptions, which is close to OVD setting, OVD and bi-functional methods obtain similar performance. However, as the length of references increases, the performance of OVD methods decrease fast, while REC and bi-functional methods suffer less from this. We can see that OVD methods are sensitive to long references, as expected, while other two types do not.</p>
</div>
<div id="S5.SS2.p7" class="ltx_para">
<p id="S5.SS2.p7.1" class="ltx_p">More experiments and additional <span id="S5.SS2.p7.1.1" class="ltx_text ltx_font_bold">qualitative results</span> are available in <span id="S5.SS2.p7.1.2" class="ltx_text ltx_font_italic" style="color:#B80000;">supplementary materials</span>.</p>
</div>
<figure id="S5.T5" class="ltx_table">

<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T5.2.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="S5.T5.3.2" class="ltx_text" style="font-size:90%;">Ablation on the proposed baseline for its improvement components and the training data.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.T5.st1" class="ltx_table ltx_figure_panel ltx_align_center">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T5.st1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S5.T5.st1.3.2" class="ltx_text" style="font-size:90%;">Method components.</span></figcaption>
<table id="S5.T5.st1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T5.st1.4.1.1" class="ltx_tr">
<th id="S5.T5.st1.4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">OFA</th>
<th id="S5.T5.st1.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">GD</th>
<th id="S5.T5.st1.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">RD</th>
<th id="S5.T5.st1.4.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">TD</th>
<th id="S5.T5.st1.4.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">mAP(%)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T5.st1.4.2.1" class="ltx_tr">
<td id="S5.T5.st1.4.2.1.1" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S5.T5.st1.4.2.1.2" class="ltx_td ltx_align_center ltx_border_t">✗</td>
<td id="S5.T5.st1.4.2.1.3" class="ltx_td ltx_align_center ltx_border_t">✗</td>
<td id="S5.T5.st1.4.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✗</td>
<td id="S5.T5.st1.4.2.1.5" class="ltx_td ltx_align_center ltx_border_t">3.4</td>
</tr>
<tr id="S5.T5.st1.4.3.2" class="ltx_tr">
<td id="S5.T5.st1.4.3.2.1" class="ltx_td ltx_align_center">✓</td>
<td id="S5.T5.st1.4.3.2.2" class="ltx_td ltx_align_center">✓</td>
<td id="S5.T5.st1.4.3.2.3" class="ltx_td ltx_align_center">✗</td>
<td id="S5.T5.st1.4.3.2.4" class="ltx_td ltx_align_center ltx_border_r">✗</td>
<td id="S5.T5.st1.4.3.2.5" class="ltx_td ltx_align_center">10.5</td>
</tr>
<tr id="S5.T5.st1.4.4.3" class="ltx_tr">
<td id="S5.T5.st1.4.4.3.1" class="ltx_td ltx_align_center">✓</td>
<td id="S5.T5.st1.4.4.3.2" class="ltx_td ltx_align_center">✓</td>
<td id="S5.T5.st1.4.4.3.3" class="ltx_td ltx_align_center">✓</td>
<td id="S5.T5.st1.4.4.3.4" class="ltx_td ltx_align_center ltx_border_r">✗</td>
<td id="S5.T5.st1.4.4.3.5" class="ltx_td ltx_align_center">17.2</td>
</tr>
<tr id="S5.T5.st1.4.5.4" class="ltx_tr">
<td id="S5.T5.st1.4.5.4.1" class="ltx_td ltx_align_center ltx_border_b">✓</td>
<td id="S5.T5.st1.4.5.4.2" class="ltx_td ltx_align_center ltx_border_b">✓</td>
<td id="S5.T5.st1.4.5.4.3" class="ltx_td ltx_align_center ltx_border_b">✓</td>
<td id="S5.T5.st1.4.5.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">✓</td>
<td id="S5.T5.st1.4.5.4.5" class="ltx_td ltx_align_center ltx_border_b">21.6</td>
</tr>
</tbody>
</table>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.T5.st2" class="ltx_table ltx_figure_panel ltx_align_center">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T5.st2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S5.T5.st2.3.2" class="ltx_text" style="font-size:90%;">Training data.</span></figcaption>
<table id="S5.T5.st2.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T5.st2.4.1.1" class="ltx_tr">
<th id="S5.T5.st2.4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">REC</th>
<th id="S5.T5.st2.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">OD</th>
<th id="S5.T5.st2.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">I2T</th>
<th id="S5.T5.st2.4.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">MLM</th>
<th id="S5.T5.st2.4.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">mAP(%)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T5.st2.4.2.1" class="ltx_tr">
<td id="S5.T5.st2.4.2.1.1" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S5.T5.st2.4.2.1.2" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S5.T5.st2.4.2.1.3" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S5.T5.st2.4.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S5.T5.st2.4.2.1.5" class="ltx_td ltx_align_center ltx_border_t">21.6</td>
</tr>
<tr id="S5.T5.st2.4.3.2" class="ltx_tr">
<td id="S5.T5.st2.4.3.2.1" class="ltx_td ltx_align_center">✓</td>
<td id="S5.T5.st2.4.3.2.2" class="ltx_td ltx_align_center">✗</td>
<td id="S5.T5.st2.4.3.2.3" class="ltx_td ltx_align_center">✓</td>
<td id="S5.T5.st2.4.3.2.4" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S5.T5.st2.4.3.2.5" class="ltx_td ltx_align_center">16.4</td>
</tr>
<tr id="S5.T5.st2.4.4.3" class="ltx_tr">
<td id="S5.T5.st2.4.4.3.1" class="ltx_td ltx_align_center">✓</td>
<td id="S5.T5.st2.4.4.3.2" class="ltx_td ltx_align_center">✓</td>
<td id="S5.T5.st2.4.4.3.3" class="ltx_td ltx_align_center">✗</td>
<td id="S5.T5.st2.4.4.3.4" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S5.T5.st2.4.4.3.5" class="ltx_td ltx_align_center">14.2</td>
</tr>
<tr id="S5.T5.st2.4.5.4" class="ltx_tr">
<td id="S5.T5.st2.4.5.4.1" class="ltx_td ltx_align_center ltx_border_b">✓</td>
<td id="S5.T5.st2.4.5.4.2" class="ltx_td ltx_align_center ltx_border_b">✓</td>
<td id="S5.T5.st2.4.5.4.3" class="ltx_td ltx_align_center ltx_border_b">✓</td>
<td id="S5.T5.st2.4.5.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">✗</td>
<td id="S5.T5.st2.4.5.4.5" class="ltx_td ltx_align_center ltx_border_b">20.3</td>
</tr>
</tbody>
</table>
</figure>
</div>
</div>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Ablation on the proposed baseline</h3>

<div id="S5.SS3.p1" class="ltx_para ltx_noindent">
<p id="S5.SS3.p1.1" class="ltx_p"><span id="S5.SS3.p1.1.1" class="ltx_text ltx_font_bold">Method components.</span>
In <a href="#S5.T5.st2" title="In Table 5 ‣ 5.2 Further analysis ‣ 5 Experimental Analyses ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">5(b)</span></a>, we perform ablation on the proposed improvements in our baseline, step-by-step from OFA to OFA-DOD, to see how they affect the performance.
Granularity decomposition (GD) makes the method more suitable for localization task.
It disentangle tasks of global or local granularity by handling them with 2 separated branch.
Reconstructed data (RD) uniforms REC and OD data into the same form, and prepares multi-instance samples with both short and long references.
Task decomposition (TD) is proposed to help rejecting FPs.
It breaks down the DOD task into a REC step followed by a VQA step.
All three of them improve the performance obviously.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para ltx_noindent">
<p id="S5.SS3.p2.2" class="ltx_p"><span id="S5.SS3.p2.2.1" class="ltx_text ltx_font_bold">Training tasks.</span>
We also perform a drop-one-out ablation on the multi-modal multi-task training data, in <a href="#S5.T5.st2" title="In Table 5 ‣ 5.2 Further analysis ‣ 5 Experimental Analyses ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">5(b)</span></a>.
<span id="S5.SS3.p2.2.2" class="ltx_text ltx_font_bold">Detection</span> data provides samples for localization, especially multi-instance situation.
It is instinctively important for learning to localize, and indeed matters for performance.
<span id="S5.SS3.p2.2.3" class="ltx_text ltx_font_bold">I2T</span> (image-to-text, like image captioning and visual question answering) often helps the generalization and zero-shot performance of multi-modal methods. We find that it does affect the zero-shot performance on D<sup id="S5.SS3.p2.2.4" class="ltx_sup">3</sup> greatly.
<span id="S5.SS3.p2.2.5" class="ltx_text ltx_font_bold">MLM</span> is theoretically important for language understanding and generalization. However, we find it actually is not.
Removing the MLM task has no significant effect on the performance.
We surmise that the generalization ability of OFA-DOD on D<sup id="S5.SS3.p2.2.6" class="ltx_sup">3</sup> mainly comes from I2T.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion and Limitation</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.2" class="ltx_p">In this paper, we bring the Described Object Detection (DOD) task to the foreground.
For this task, we introduce a dataset called D<sup id="S6.p1.2.1" class="ltx_sup">3</sup>, which annotates described objects without omission and features flexible language expressions, whether long or short, complex or simple.
Our evaluation of SOTA methods from REC or OVD on D<sup id="S6.p1.2.2" class="ltx_sup">3</sup> reveals challenges faced by REC, OVD, and bi-functional approaches.
Based on these observations, we propose a baseline that largely improves REC methods for DOD task.
We believe that the dataset and findings will contribute to advancing the understanding and development of DOD methods, facilitating future research in this area.</p>
</div>
<div id="S6.p2" class="ltx_para ltx_noindent">
<p id="S6.p2.1" class="ltx_p"><span id="S6.p2.1.1" class="ltx_text ltx_font_bold">Limitation and broader impact.</span>
This work does have some limitations. Due to the significant annotation cost brought by our complete annotation process, we are unable to propose a huge dataset with millions or billions of images. Besides, the evaluation and findings in this work may be dependent on the choice of descriptions and the image sources. This work only serves as a starting point for DOD and we hope there will be other DOD datasets with larger scales.
In the broader community, compared to traditional detection algorithms, DOD models have a lower customization threshold, enabling users to specify the detection target using language. This may lead to potential abuse.</p>
</div>
<div id="S6.p3" class="ltx_para ltx_noindent">
<p id="S6.p3.1" class="ltx_p"><span id="S6.p3.1.1" class="ltx_text ltx_font_bold">Future work.</span>
During peer-review process, some new works with potential for DOD emerges, including Shikra <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, Kosmos-2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> and Qwen-VL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. We will continue to investigate such methods for DOD and update them in <a target="_blank" href="https://github.com/Charles-Xie/awesome-described-object-detection" title="" class="ltx_ref ltx_href">this list</a>.</p>
</div>
<div id="S6.p4" class="ltx_para ltx_noindent">
<p id="S6.p4.1" class="ltx_p"><span id="S6.p4.1.1" class="ltx_text ltx_font_bold">Acknowledgments.</span>
This work was supported in part by the National Natural Science Foundation of China under Grant 62076183, 61936014 and 61976159, in part by the Natural Science Foundation of Shanghai under Grant 20ZR1473500, in part by the Shanghai Science and Technology Innovation Action Project under Grant 20511100700 and 22511105300, in part by the Shanghai Municipal Science and Technology Major Project under Grant 2021SHZDZX0100, and in part by the Fundamental Research Funds for the Central Universities. The authors would also like to thank the anonymous reviewers for their careful work and valuable suggestions.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
A. Arbelle, S. Doveh, A. Alfassy, J. Shtok, G. Lev, E. Schwartz, H. Kuehne,
H. B. Levi, P. Sattigeri, R. Panda, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Detector-free weakly supervised grounding by separation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, pages 1801–1812, 2021.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and
J. Zhou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Qwen-vl: A frontier large vision-language model with versatile
abilities.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2308.12966</span><span id="bib.bib2.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
L. Cai, Z. Zhang, Y. Zhu, L. Zhang, M. Li, and X. Xue.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Bigdetection: A large-scale benchmark for improved object detector
pre-training.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, pages 4777–4787, 2022.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">End-to-end object detection with transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2020: 16th European Conference,
Glasgow, UK, August 23–28, 2020, Proceedings, Part I 16</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, pages 213–229.
Springer, 2020.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
K. Chen, Z. Zhang, W. Zeng, R. Zhang, F. Zhu, and R. Zhao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Shikra: Unleashing multimodal llm’s referential dialogue magic.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2306.15195</span><span id="bib.bib5.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Y.-C. Chen, L. Li, L. Yu, A. El Kholy, F. Ahmed, Z. Gan, Y. Cheng, and J. Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Uniter: Universal image-text representation learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2020: 16th European Conference,
Glasgow, UK, August 23–28, 2020, Proceedings, Part XXX</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, pages 104–120.
Springer, 2020.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Z. Chen, P. Wang, L. Ma, K.-Y. K. Wong, and Q. Wu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Cops-ref: A new dataset and task on compositional referring
expression comprehension.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, pages 10086–10095, 2020.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
J. Deng, Z. Yang, T. Chen, W. Zhou, and H. Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Transvg: End-to-end visual grounding with transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, pages 1769–1779, 2021.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit,
and N. Houlsby.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">An image is worth 16x16 words: Transformers for image recognition at
scale.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Z.-Y. Dou, A. Kamath, Z. Gan, P. Zhang, J. Wang, L. Li, Z. Liu, C. Liu,
Y. LeCun, N. Peng, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Coarse-to-fine vision-language pre-training with fusion in the
backbone.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib10.4.2" class="ltx_text" style="font-size:90%;">,
35:32942–32956, 2022.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">The pascal visual object classes (voc) challenge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International journal of computer vision</span><span id="bib.bib11.4.2" class="ltx_text" style="font-size:90%;">, 2010.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
G. Ghiasi, X. Gu, Y. Cui, and T.-Y. Lin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Scaling open-vocabulary image segmentation with image-level labels.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2022: 17th European Conference, Tel
Aviv, Israel, October 23–27, 2022, Proceedings, Part XXXVI</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, pages 540–557.
Springer, 2022.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
X. Gu, T.-Y. Lin, W. Kuo, and Y. Cui.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Open-vocabulary object detection via vision and language knowledge
distillation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
A. Gupta, P. Dollar, and R. Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Lvis: A dataset for large vocabulary instance segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, pages 5356–5364, 2019.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
R. Hu, M. Rohrbach, J. Andreas, T. Darrell, and K. Saenko.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Modeling relationships in referential expressions with compositional
modular networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, pages 1115–1124, 2017.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
A. Kamath, M. Singh, Y. LeCun, G. Synnaeve, I. Misra, and N. Carion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Mdetr-modulated detection for end-to-end multi-modal understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, pages 1780–1790, 2021.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
S. Kazemzadeh, V. Ordonez, M. Matten, and T. Berg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Referitgame: Referring to objects in photographs of natural scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">EMNLP</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, pages 787–798, 2014.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
I. Krasin, T. Lin, T. Duerig, P. Krähenbühl, A. Gupta, C. Burgess, and
V. Ferrari.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">OpenImages: A public dataset for large-scale multi-label and
multi-class image classification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Dataset available from https://github.com/openimages</span><span id="bib.bib18.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen,
Y. Kalantidis, L.-J. Li, D. A. Shamma, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Visual Genome: Connecting language and vision using crowdsourced
dense image annotations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IJCV</span><span id="bib.bib19.4.2" class="ltx_text" style="font-size:90%;">, 123:32–73, 2017.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
W. Kuo, F. Bertsch, W. Li, A. Piergiovanni, M. Saffar, and A. Angelova.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Findit: Generalized localization with natural language queries.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">. Springer, 2022.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
C. Li, H. Liu, L. Li, P. Zhang, J. Aneja, J. Yang, P. Jin, H. Hu, Z. Liu, Y. J.
Lee, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Elevater: A benchmark and toolkit for evaluating language-augmented
visual models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib21.4.2" class="ltx_text" style="font-size:90%;">,
35:9287–9301, 2022.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
L. H. Li, P. Zhang, H. Zhang, J. Yang, C. Li, Y. Zhong, L. Wang, L. Yuan,
L. Zhang, J.-N. Hwang, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Grounded language-image pre-training.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, pages 10965–10975, 2022.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
M. Li and L. Sigal.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Referring transformer: A one-step approach to multi-task visual
grounding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib23.4.2" class="ltx_text" style="font-size:90%;">,
34:19652–19664, 2021.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Dollár, and C. L. Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Microsoft COCO: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, pages 740–755. Springer, 2014.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
J. Liu, H. Ding, Z. Cai, Y. Zhang, R. K. Satzoda, V. Mahadevan, and
R. Manmatha.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Polyformer: Referring image segmentation as sequential polygon
generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
S. Liu, Z. Zeng, T. Ren, F. Li, H. Zhang, J. Yang, C. Li, J. Yang, H. Su,
J. Zhu, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Grounding dino: Marrying dino with grounded pre-training for open-set
object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2303.05499</span><span id="bib.bib26.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Swin transformer: Hierarchical vision transformer using shifted
windows.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF international conference on
computer vision</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, pages 10012–10022, 2021.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
J. Lu, C. Clark, R. Zellers, R. Mottaghi, and A. Kembhavi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Unified-io: A unified model for vision, language, and multi-modal
tasks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2206.08916</span><span id="bib.bib28.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
G. Luo, Y. Zhou, X. Sun, L. Cao, C. Wu, C. Deng, and R. Ji.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Multi-task collaborative network for joint referring expression
comprehension and segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on computer vision and
pattern recognition</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, pages 10034–10043, 2020.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
J. Mao, J. Huang, A. Toshev, O. Camburu, A. L. Yuille, and K. Murphy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Generation and comprehension of unambiguous object descriptions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, pages 11–20, 2016.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
M. Minderer, A. Gritsenko, A. Stone, M. Neumann, D. Weissenborn,
A. Dosovitskiy, A. Mahendran, A. Arnab, M. Dehghani, Z. Shen, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Simple open-vocabulary object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, pages 728–755.
Springer, 2022.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
V. Ordonez, G. Kulkarni, and T. Berg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Im2text: Describing images using 1 million captioned photographs.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib32.4.2" class="ltx_text" style="font-size:90%;">, 24, 2011.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Z. Peng, W. Wang, L. Dong, Y. Hao, S. Huang, S. Ma, and F. Wei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Kosmos-2: Grounding multimodal large language models to the world.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2306.14824</span><span id="bib.bib33.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
B. A. Plummer, K. J. Shih, Y. Li, K. Xu, S. Lazebnik, S. Sclaroff, and
K. Saenko.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Revisiting image-language networks for open-ended phrase detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE transactions on pattern analysis and machine intelligence</span><span id="bib.bib34.4.2" class="ltx_text" style="font-size:90%;">,
44(4):2155–2167, 2020.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
B. A. Plummer, L. Wang, C. M. Cervantes, J. C. Caicedo, J. Hockenmaier, and
S. Lazebnik.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Flickr30k entities: Collecting region-to-phrase correspondences for
richer image-to-sentence models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE international conference on computer
vision</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:90%;">, pages 2641–2649, 2015.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry,
A. Askell, P. Mishkin, J. Clark, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Learning transferable visual models from natural language
supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International conference on machine learning</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:90%;">, pages
8748–8763. PMLR, 2021.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
A. Sadhu, K. Chen, and R. Nevatia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Zero-shot grounding of objects from natural language queries.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib37.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib37.5.3" class="ltx_text" style="font-size:90%;">, pages 4694–4703, 2019.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
S. Shao, Z. Li, T. Zhang, C. Peng, G. Yu, X. Zhang, J. Li, and J. Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Objects365: A large-scale, high-quality dataset for object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF international conference on
computer vision</span><span id="bib.bib38.5.3" class="ltx_text" style="font-size:90%;">, pages 8430–8439, 2019.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
P. Sharma, N. Ding, S. Goodman, and R. Soricut.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">Conceptual captions: A cleaned, hypernymed, image alt-text dataset
for automatic image captioning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib39.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers)</span><span id="bib.bib39.5.3" class="ltx_text" style="font-size:90%;">, pages 2556–2565, 2018.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
S. Song, X. Lin, J. Liu, Z. Guo, and S.-F. Chang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Co-grounding networks with semantic attention for referring
expression comprehension in videos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib40.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib40.5.3" class="ltx_text" style="font-size:90%;">, pages 1346–1355, 2021.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
S. Subramanian, W. Merrill, T. Darrell, M. Gardner, S. Singh, and A. Rohrbach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">Reclip: A strong zero-shot baseline for referring expression
comprehension.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib41.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 60th Annual Meeting of the Association for
Computational Linguistics</span><span id="bib.bib41.5.3" class="ltx_text" style="font-size:90%;">, Dublin, Ireland, May 2022. Association for
Computational Linguistics.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde, K. Ni, D. Poland, D. Borth,
and L.-J. Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Yfcc100m: The new data in multimedia research.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Communications of the ACM</span><span id="bib.bib42.4.2" class="ltx_text" style="font-size:90%;">, 59(2):64–73, 2016.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
J. Wang, P. Zhang, T. Chu, Y. Cao, Y. Zhou, T. Wu, B. Wang, C. He, and D. Lin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">V3det: Vast vocabulary visual detection dataset.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib43.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">The IEEE International Conference on Computer Vision (ICCV)</span><span id="bib.bib43.5.3" class="ltx_text" style="font-size:90%;">,
October 2023.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
P. Wang, A. Yang, R. Men, J. Lin, S. Bai, Z. Li, J. Ma, C. Zhou, J. Zhou, and
H. Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Ofa: Unifying architectures, tasks, and modalities through a simple
sequence-to-sequence learning framework.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib44.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</span><span id="bib.bib44.5.3" class="ltx_text" style="font-size:90%;">, pages
23318–23340. PMLR, 2022.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
C. Wu, Z. Lin, S. Cohen, T. Bui, and S. Maji.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">Phrasecut: Language-based image segmentation in the wild.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib45.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib45.5.3" class="ltx_text" style="font-size:90%;">, pages 10216–10225, 2020.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
X. Wu, F. Zhu, R. Zhao, and H. Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">Cora: Adapting clip for open-vocabulary detection with region
prompting and anchor pre-matching.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib46.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib46.5.3" class="ltx_text" style="font-size:90%;">, pages 7031–7040, 2023.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Y. Wu, Z. Zhang, C. Xie, F. Zhu, and R. Zhao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">Advancing referring expression segmentation beyond single image.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib47.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Computer Vision (ICCV)</span><span id="bib.bib47.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
B. Yan, Y. Jiang, J. Wu, D. Wang, P. Luo, Z. Yuan, and H. Lu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">Universal instance perception as object discovery and retrieval.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib48.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib48.5.3" class="ltx_text" style="font-size:90%;">, pages 15325–15336, 2023.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
L. Yao, J. Han, Y. Wen, X. Liang, D. Xu, W. Zhang, Z. Li, C. Xu, and H. Xu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">Detclip: Dictionary-enriched visual-concept paralleled pre-training
for open-world detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib49.4.2" class="ltx_text" style="font-size:90%;">,
35:9125–9138, 2022.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
L. Yu, P. Poirson, S. Yang, A. C. Berg, and T. L. Berg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">Modeling context in referring expressions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib50.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2016: 14th European Conference,
Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14</span><span id="bib.bib50.5.3" class="ltx_text" style="font-size:90%;">,
pages 69–85. Springer, 2016.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text" style="font-size:90%;">
Y. Zang, W. Li, K. Zhou, C. Huang, and C. C. Loy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.2.1" class="ltx_text" style="font-size:90%;">Open-vocabulary detr with conditional matching.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib51.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2022: 17th European Conference, Tel
Aviv, Israel, October 23–27, 2022, Proceedings, Part IX</span><span id="bib.bib51.5.3" class="ltx_text" style="font-size:90%;">, pages 106–122.
Springer, 2022.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text" style="font-size:90%;">
A. Zareian, K. D. Rosa, D. H. Hu, and S.-F. Chang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.2.1" class="ltx_text" style="font-size:90%;">Open-vocabulary object detection using captions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib52.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib52.5.3" class="ltx_text" style="font-size:90%;">, pages 14393–14402, 2021.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text" style="font-size:90%;">
H. Zhang, F. Li, S. Liu, L. Zhang, H. Su, J. Zhu, L. Ni, and H.-Y. Shum.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.2.1" class="ltx_text" style="font-size:90%;">DINO: DETR with improved denoising anchor boxes for end-to-end
object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib53.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">The Eleventh International Conference on Learning
Representations</span><span id="bib.bib53.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text" style="font-size:90%;">
H. Zhang, P. Zhang, X. Hu, Y.-C. Chen, L. Li, X. Dai, L. Wang, L. Yuan, J.-N.
Hwang, and J. Gao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.2.1" class="ltx_text" style="font-size:90%;">Glipv2: Unifying localization and vision-language understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib54.4.2" class="ltx_text" style="font-size:90%;">,
35:36067–36080, 2022.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text" style="font-size:90%;">
Y. Zhong, J. Yang, P. Zhang, C. Li, N. Codella, L. H. Li, L. Zhou, X. Dai,
L. Yuan, Y. Li, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.2.1" class="ltx_text" style="font-size:90%;">Regionclip: Region-based language-image pretraining.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib55.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib55.5.3" class="ltx_text" style="font-size:90%;">, pages 16793–16803, 2022.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text" style="font-size:90%;">
X. Zhou, R. Girdhar, A. Joulin, P. Krähenbühl, and I. Misra.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.2.1" class="ltx_text" style="font-size:90%;">Detecting twenty-thousand classes using image-level supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib56.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2022: 17th European Conference, Tel
Aviv, Israel, October 23–27, 2022, Proceedings, Part IX</span><span id="bib.bib56.5.3" class="ltx_text" style="font-size:90%;">, pages 350–368.
Springer, 2022.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text" style="font-size:90%;">
Y. Zhou, R. Ji, G. Luo, X. Sun, J. Su, X. Ding, C.-W. Lin, and Q. Tian.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.2.1" class="ltx_text" style="font-size:90%;">A real-time global inference network for one-stage referring
expression comprehension.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Neural Networks and Learning Systems</span><span id="bib.bib57.4.2" class="ltx_text" style="font-size:90%;">,
2021.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock"><span id="bib.bib58.1.1" class="ltx_text" style="font-size:90%;">
C. Zhu, Y. Zhou, Y. Shen, G. Luo, X. Pan, M. Lin, C. Chen, L. Cao, X. Sun, and
R. Ji.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.2.1" class="ltx_text" style="font-size:90%;">Seqtr: A simple yet universal network for visual grounding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib58.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2022: 17th European Conference, Tel
Aviv, Israel, October 23–27, 2022, Proceedings, Part XXXV</span><span id="bib.bib58.5.3" class="ltx_text" style="font-size:90%;">, pages 598–615.
Springer, 2022.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p ltx_align_center"><span id="p1.1.1" class="ltx_text ltx_font_bold" style="font-size:144%;">Described Object Detection: Liberating Object Detection with Flexible Expressions</span></p>
<p id="p1.2" class="ltx_p ltx_align_center"><span id="p1.2.1" class="ltx_text"><span class="ltx_rule" style="width:52.0pt;height:1.0pt;position:relative; bottom:3.0pt;background:black;display:inline-block;"> </span><span id="p1.2.1.1" class="ltx_text ltx_font_italic" style="font-size:120%;">  Supplemental File  <span class="ltx_rule" style="width:52.0pt;height:1.0pt;position:relative; bottom:3.0pt;background:black;display:inline-block;"> </span></span></span></p>
</div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Dataset Details</h2>

<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>More examples</h3>

<div id="A1.SS1.p1" class="ltx_para">
<p id="A1.SS1.p1.1" class="ltx_p">In the Section 3.1 of the main paper, we introduce the characteristics of the proposed D<sup id="A1.SS1.p1.1.1" class="ltx_sup">3</sup> dataset, and elaborate the 3 major ones in Fig. 2 with some examples. Here we provide more examples to supplement this part.</p>
</div>
<figure id="A1.F5" class="ltx_figure"><img src="/html/2307.12813/assets/x13.png" id="A1.F5.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="474" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F5.8.3.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="A1.F5.5.2" class="ltx_text" style="font-size:90%;">
Examples demonstrate that the proposed D<sup id="A1.F5.5.2.1" class="ltx_sup">3</sup> is fully annotated with positive and negative examples across the entire dataset. The visualizations include four descriptions along with selected positive and negative image samples from the dataset. Each description is accompanied by two rows of image samples: the first row contains positive images, and the second row contains negative images. For positive images, the specific description’s bounding boxes and instance masks are visualized. In contrast, for negative images, an empty set symbol <math id="A1.F5.5.2.m2.1" class="ltx_Math" alttext="\emptyset" display="inline"><semantics id="A1.F5.5.2.m2.1b"><mi mathcolor="#FF0000" mathvariant="normal" id="A1.F5.5.2.m2.1.1" xref="A1.F5.5.2.m2.1.1.cmml">∅</mi><annotation-xml encoding="MathML-Content" id="A1.F5.5.2.m2.1c"><emptyset id="A1.F5.5.2.m2.1.1.cmml" xref="A1.F5.5.2.m2.1.1"></emptyset></annotation-xml><annotation encoding="application/x-tex" id="A1.F5.5.2.m2.1d">\emptyset</annotation></semantics></math> is displayed in red at the right corner.
The visualizations are best observed in color and with zoomed-in view.
</span></figcaption>
</figure>
<div id="A1.SS1.p2" class="ltx_para ltx_noindent">
<p id="A1.SS1.p2.2" class="ltx_p"><span id="A1.SS1.p2.2.1" class="ltx_text ltx_font_bold">Complete annotation.</span>
The first characteristic of D<sup id="A1.SS1.p2.2.2" class="ltx_sup">3</sup> is the dataset-level complete and thorough annotations, setting it apart from REC datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. In D<sup id="A1.SS1.p2.2.3" class="ltx_sup">3</sup>, every image is annotated for possible positive and negative instances, as demonstrated in <a href="#A1.F5" title="In A.1 More examples ‣ Appendix A Dataset Details ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a>. This figure includes several images with positive instance labels (first row) and several images with negative instance labels (second row) for each of the four descriptions. Such comprehensive annotation makes the proposed dataset well-suited for detection tasks.</p>
</div>
<div id="A1.SS1.p3" class="ltx_para">
<p id="A1.SS1.p3.1" class="ltx_p">In comparison, REC datasets like RefCOCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> only annotate several positive instances in a few images for each description, leaving all the other images without annotations for that particular description; thus, their annotation completeness is limited to the image-level. On the other hand, GRD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> annotates a description for a group of images while dividing the entire set into multiple groups, resulting in an annotation completeness at the group-level.</p>
</div>
<figure id="A1.F6" class="ltx_figure"><img src="/html/2307.12813/assets/x14.png" id="A1.F6.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="402" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F6.6.2.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="A1.F6.3.1" class="ltx_text" style="font-size:90%;">Examples showing the descriptions in D<sup id="A1.F6.3.1.1" class="ltx_sup">3</sup> are free-form and unrestricted. The descriptions can be short and simple (like the top 3 descriptions, in yellow background) or long and complex (like the bottom 3, in green background).
Boxes and instance masks belonging to the specific description are visualized in each image.
The visualizations are best observed in color and with zoomed-in view.
</span></figcaption>
</figure>
<div id="A1.SS1.p4" class="ltx_para ltx_noindent">
<p id="A1.SS1.p4.2" class="ltx_p"><span id="A1.SS1.p4.2.1" class="ltx_text ltx_font_bold">Unrestricted description.</span>
The categories in D<sup id="A1.SS1.p4.2.2" class="ltx_sup">3</sup> encompass more than just simple object names, such as <span id="A1.SS1.p4.2.3" class="ltx_text ltx_font_typewriter">cat</span>, <span id="A1.SS1.p4.2.4" class="ltx_text ltx_font_typewriter">dog</span> and <span id="A1.SS1.p4.2.5" class="ltx_text ltx_font_typewriter">bird</span> found in typical object detection datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. As illustrated in <a href="#A1.F6" title="In A.1 More examples ‣ Appendix A Dataset Details ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">6</span></a>, the descriptions are expressed in unrestricted natural language. The longer and more complex descriptions resemble references found in REC datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. For instance, a description like <span id="A1.SS1.p4.2.6" class="ltx_text ltx_font_typewriter">a fisher who stands on the shore and whose lower body is not submerged by water</span> comprises 16 words and encompasses multiple attributes like <span id="A1.SS1.p4.2.7" class="ltx_text ltx_font_typewriter">fisher</span>, <span id="A1.SS1.p4.2.8" class="ltx_text ltx_font_typewriter">stands on the shore</span> and <span id="A1.SS1.p4.2.9" class="ltx_text ltx_font_typewriter">lower body is not submerged by water</span>. These attributes are semantically abstract and visually diverse. On the other hand, the shorter and simpler descriptions can be similar to the category names in OD datasets, such as <span id="A1.SS1.p4.2.10" class="ltx_text ltx_font_typewriter">backpack</span>, <span id="A1.SS1.p4.2.11" class="ltx_text ltx_font_typewriter">swing bench</span> and <span id="A1.SS1.p4.2.12" class="ltx_text ltx_font_typewriter">a sailboat</span>.
This illustrates that the descriptions of objects in D<sup id="A1.SS1.p4.2.13" class="ltx_sup">3</sup> are free-form and unrestricted, covering a wide range of description types present in both REC and OD datasets.</p>
</div>
<figure id="A1.F7" class="ltx_figure"><img src="/html/2307.12813/assets/x15.png" id="A1.F7.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="403" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F7.6.2.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="A1.F7.3.1" class="ltx_text" style="font-size:90%;">
Examples showing the presence and absence descriptions in D<sup id="A1.F7.3.1.1" class="ltx_sup">3</sup>.
Six descriptions, containing 3 pairs of contrary presence descriptions (in yellow background) and absence descriptions (in green background), are illustrated alongside their corresponding positive examples. The key words depicting absence expressions are in red. Boxes and instance masks belonging to the specific description are visualized in each image.
The visualizations are best observed in color and with zoomed-in view.
</span></figcaption>
</figure>
<div id="A1.SS1.p5" class="ltx_para ltx_noindent">
<p id="A1.SS1.p5.1" class="ltx_p"><span id="A1.SS1.p5.1.1" class="ltx_text ltx_font_bold">Absence description.</span>
To the best of our knowledge, the proposed dataset is the first annotated dataset specifically designed to address absence descriptions. Examples with annotations for both presence and absence descriptions from our dataset (D<sup id="A1.SS1.p5.1.2" class="ltx_sup">3</sup>) are illustrated in <a href="#A1.F7" title="In A.1 More examples ‣ Appendix A Dataset Details ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">7</span></a>. For visualization purposes, we have selected some absence descriptions that have contradictory presence descriptions. The absence descriptions and the corresponding presence descriptions differ primarily in the existence of key attributes. For instance, the first presence description emphasizes black/white boards <span id="A1.SS1.p5.1.3" class="ltx_text ltx_font_italic">with</span> words written, while the first absence description focuses on those <span id="A1.SS1.p5.1.4" class="ltx_text ltx_font_italic">without</span> words.</p>
</div>
<div id="A1.SS1.p6" class="ltx_para">
<p id="A1.SS1.p6.1" class="ltx_p">It is important to note that in certain cases, some images contain both absence and presence descriptions. For example, in the first example image of the second presence-absence pair, both dogs led by ropes and not led by ropes coexist. Such instances pose significant challenges, as they require the DOD model to comprehend the absence of concepts in a language description and to discern the subtle differences among instances within an image.</p>
</div>
<div id="A1.SS1.p7" class="ltx_para ltx_noindent">
<p id="A1.SS1.p7.1" class="ltx_p"><span id="A1.SS1.p7.1.1" class="ltx_text ltx_font_bold">Other characteristics for instance annotations.</span>
Examples in <a href="#A1.F5" title="In A.1 More examples ‣ Appendix A Dataset Details ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figs.</span> <span class="ltx_text ltx_ref_tag">5</span></a>, <a href="#A1.F6" title="Figure 6 ‣ A.1 More examples ‣ Appendix A Dataset Details ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> and <a href="#A1.F7" title="Figure 7 ‣ A.1 More examples ‣ Appendix A Dataset Details ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> all illustrate some additional characteristics of D<sup id="A1.SS1.p7.1.2" class="ltx_sup">3</sup>:</p>
</div>
<div id="A1.SS1.p8" class="ltx_para">
<p id="A1.SS1.p8.1" class="ltx_p">(1) Instance-level annotation, where each instance is individually labeled.
(2) One description can refer to multiple instances in an image.
(3) Each instance is annotated with both bounding boxes and masks. As a result, the proposed dataset is not limited to the Described Object Detection setting focused on in this work but can also support a similar task, producing instance segmentation masks rather than object detection bounding boxes.</p>
</div>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>More statistics</h3>

<div id="A1.SS2.p1" class="ltx_para">
<p id="A1.SS2.p1.1" class="ltx_p">The proposed dataset contains a total of 10,578 images, 18,514 boxes (including instance masks), and 422 well-designed descriptions. These descriptions comprise 316 presence descriptions and 106 absence descriptions.</p>
</div>
<div id="A1.SS2.p2" class="ltx_para">
<p id="A1.SS2.p2.1" class="ltx_p">Regarding the inter-scenario setting, considering all 422 descriptions, there are 24,282 positive object-text pairs and 7,788,626 negative pairs. When considering only positive descriptions, there are 16,480 positive pairs and 5,833,944 negative pairs.</p>
</div>
<div id="A1.SS2.p3" class="ltx_para">
<p id="A1.SS2.p3.1" class="ltx_p">For the intra-scenario setting (where candidate descriptions for an image only come from the same scenario), there are 20,279 positive pairs and 53,383 negative pairs. For the subset with only positive descriptions, there are 13,917 positive pairs and 41,231 negative pairs.</p>
</div>
<div id="A1.SS2.p4" class="ltx_para">
<p id="A1.SS2.p4.1" class="ltx_p">The average expression length in the dataset is 6.3 words.</p>
</div>
<figure id="A1.F8" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A1.F8.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2307.12813/assets/x16.png" id="A1.F8.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="346" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A1.F8.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="A1.F8.sf1.3.2" class="ltx_text" style="font-size:90%;">
Distribution of number of descriptions on one image.
</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A1.F8.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2307.12813/assets/x17.png" id="A1.F8.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="346" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A1.F8.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="A1.F8.sf2.3.2" class="ltx_text" style="font-size:90%;">
Distribution of number of instances on one image.
</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F8.2.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="A1.F8.3.2" class="ltx_text" style="font-size:90%;">Distribution of (a) number of positive descriptions on an image in the dataset, and (b) number of positive instances on an image in the dataset. (a) shows that the majority of images contains multiple positive descriptions in the proposed dataset, while (b) shows that many images contains multiple boxes.</span></figcaption>
</figure>
<div id="A1.SS2.p5" class="ltx_para">
<p id="A1.SS2.p5.1" class="ltx_p">In <a href="#A1.F8" title="In A.2 More statistics ‣ Appendix A Dataset Details ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">8</span></a>, two additional histograms demonstrate the distribution of the number of positive descriptions and the number of positive instances within a single image in the dataset. This visualization highlights the complexity of the proposed dataset, with frequent occurrences of multiple references and many instances within one image.</p>
</div>
<div id="A1.SS2.p6" class="ltx_para ltx_noindent">
<p id="A1.SS2.p6.2" class="ltx_p"><span id="A1.SS2.p6.2.1" class="ltx_text ltx_font_bold">Absence descriptions.</span>
To the best of our knowledge, the proposed D<sup id="A1.SS2.p6.2.2" class="ltx_sup">3</sup> benchmark is the first to investigate the capability of models to comprehend the absence of certain features and attributes and distinguish between absence and presence. This unique focus on absence-related comprehension sets it apart from previous benchmarks with description annotation (e.g., datasets like RefCOCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> for REC and RES tasks). Notably, RefCOCO contains an extremely small and neglectable number of instances with absence descriptions. In contrast, the D<sup id="A1.SS2.p6.2.3" class="ltx_sup">3</sup> dataset comprises 106 absence expressions out of a total of 422 descriptions, approximately 25%, and 7,802 positive annotated instances. This significant inclusion of absence-related expressions contributes to a vital and distinguishing characteristic of our proposed benchmark.</p>
</div>
<div id="A1.SS2.p7" class="ltx_para ltx_noindent">
<p id="A1.SS2.p7.7" class="ltx_p"><span id="A1.SS2.p7.7.1" class="ltx_text ltx_font_bold">Category overlapping with previous datasets.</span>
The proposed dataset can be regarded as an OVD benchmark (but with longer references rather than category names), if we take classes and references in previous OVD/REC datasets as <span id="A1.SS2.p7.7.2" class="ltx_text ltx_font_italic">base</span> classes, and the classes in D<sup id="A1.SS2.p7.7.3" class="ltx_sup">3</sup> as <span id="A1.SS2.p7.7.4" class="ltx_text ltx_font_italic">novel</span>.
Categories in D<sup id="A1.SS2.p7.7.5" class="ltx_sup">3</sup> has very little overlap with previous datasets. Here we try to quantify the minimal overlap between <span id="A1.SS2.p7.7.6" class="ltx_text ltx_font_italic">base</span> (OVD datasets like COCO/LVIS and REC datasets like RefCOCO/+/g) and <span id="A1.SS2.p7.7.7" class="ltx_text ltx_font_italic">novel</span> (<math id="A1.SS2.p7.3.m3.1" class="ltx_Math" alttext="D^{3}" display="inline"><semantics id="A1.SS2.p7.3.m3.1a"><msup id="A1.SS2.p7.3.m3.1.1" xref="A1.SS2.p7.3.m3.1.1.cmml"><mi id="A1.SS2.p7.3.m3.1.1.2" xref="A1.SS2.p7.3.m3.1.1.2.cmml">D</mi><mn id="A1.SS2.p7.3.m3.1.1.3" xref="A1.SS2.p7.3.m3.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="A1.SS2.p7.3.m3.1b"><apply id="A1.SS2.p7.3.m3.1.1.cmml" xref="A1.SS2.p7.3.m3.1.1"><csymbol cd="ambiguous" id="A1.SS2.p7.3.m3.1.1.1.cmml" xref="A1.SS2.p7.3.m3.1.1">superscript</csymbol><ci id="A1.SS2.p7.3.m3.1.1.2.cmml" xref="A1.SS2.p7.3.m3.1.1.2">𝐷</ci><cn type="integer" id="A1.SS2.p7.3.m3.1.1.3.cmml" xref="A1.SS2.p7.3.m3.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p7.3.m3.1c">D^{3}</annotation></semantics></math>).
For comparison with OVD datasets, we used ChatGPT to generate synonyms from category names in those datasets and then match them against references in <math id="A1.SS2.p7.4.m4.1" class="ltx_Math" alttext="D^{3}" display="inline"><semantics id="A1.SS2.p7.4.m4.1a"><msup id="A1.SS2.p7.4.m4.1.1" xref="A1.SS2.p7.4.m4.1.1.cmml"><mi id="A1.SS2.p7.4.m4.1.1.2" xref="A1.SS2.p7.4.m4.1.1.2.cmml">D</mi><mn id="A1.SS2.p7.4.m4.1.1.3" xref="A1.SS2.p7.4.m4.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="A1.SS2.p7.4.m4.1b"><apply id="A1.SS2.p7.4.m4.1.1.cmml" xref="A1.SS2.p7.4.m4.1.1"><csymbol cd="ambiguous" id="A1.SS2.p7.4.m4.1.1.1.cmml" xref="A1.SS2.p7.4.m4.1.1">superscript</csymbol><ci id="A1.SS2.p7.4.m4.1.1.2.cmml" xref="A1.SS2.p7.4.m4.1.1.2">𝐷</ci><cn type="integer" id="A1.SS2.p7.4.m4.1.1.3.cmml" xref="A1.SS2.p7.4.m4.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p7.4.m4.1c">D^{3}</annotation></semantics></math>. The overlapping percentage is 0.4% for COCO and 0.9% for LVIS.
For COCO, which have less categories, we also perform manual check and calculation, resulting in 0.7% overlap with <math id="A1.SS2.p7.5.m5.1" class="ltx_Math" alttext="D^{3}" display="inline"><semantics id="A1.SS2.p7.5.m5.1a"><msup id="A1.SS2.p7.5.m5.1.1" xref="A1.SS2.p7.5.m5.1.1.cmml"><mi id="A1.SS2.p7.5.m5.1.1.2" xref="A1.SS2.p7.5.m5.1.1.2.cmml">D</mi><mn id="A1.SS2.p7.5.m5.1.1.3" xref="A1.SS2.p7.5.m5.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="A1.SS2.p7.5.m5.1b"><apply id="A1.SS2.p7.5.m5.1.1.cmml" xref="A1.SS2.p7.5.m5.1.1"><csymbol cd="ambiguous" id="A1.SS2.p7.5.m5.1.1.1.cmml" xref="A1.SS2.p7.5.m5.1.1">superscript</csymbol><ci id="A1.SS2.p7.5.m5.1.1.2.cmml" xref="A1.SS2.p7.5.m5.1.1.2">𝐷</ci><cn type="integer" id="A1.SS2.p7.5.m5.1.1.3.cmml" xref="A1.SS2.p7.5.m5.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p7.5.m5.1c">D^{3}</annotation></semantics></math>.
For REC datasets, we apply a threshold on the sentence similarity calculated via HuggingFace’s <span id="A1.SS2.p7.7.8" class="ltx_text ltx_font_typewriter">bert-base-cased-finetuned-mrpc</span> model. The calculated overlaps of <math id="A1.SS2.p7.6.m6.1" class="ltx_Math" alttext="D^{3}" display="inline"><semantics id="A1.SS2.p7.6.m6.1a"><msup id="A1.SS2.p7.6.m6.1.1" xref="A1.SS2.p7.6.m6.1.1.cmml"><mi id="A1.SS2.p7.6.m6.1.1.2" xref="A1.SS2.p7.6.m6.1.1.2.cmml">D</mi><mn id="A1.SS2.p7.6.m6.1.1.3" xref="A1.SS2.p7.6.m6.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="A1.SS2.p7.6.m6.1b"><apply id="A1.SS2.p7.6.m6.1.1.cmml" xref="A1.SS2.p7.6.m6.1.1"><csymbol cd="ambiguous" id="A1.SS2.p7.6.m6.1.1.1.cmml" xref="A1.SS2.p7.6.m6.1.1">superscript</csymbol><ci id="A1.SS2.p7.6.m6.1.1.2.cmml" xref="A1.SS2.p7.6.m6.1.1.2">𝐷</ci><cn type="integer" id="A1.SS2.p7.6.m6.1.1.3.cmml" xref="A1.SS2.p7.6.m6.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p7.6.m6.1c">D^{3}</annotation></semantics></math> with RefCOCO/+/g is 0.0%, 0.2% and 0.7%, separately.
Thus, novel classes (<math id="A1.SS2.p7.7.m7.1" class="ltx_Math" alttext="D^{3}" display="inline"><semantics id="A1.SS2.p7.7.m7.1a"><msup id="A1.SS2.p7.7.m7.1.1" xref="A1.SS2.p7.7.m7.1.1.cmml"><mi id="A1.SS2.p7.7.m7.1.1.2" xref="A1.SS2.p7.7.m7.1.1.2.cmml">D</mi><mn id="A1.SS2.p7.7.m7.1.1.3" xref="A1.SS2.p7.7.m7.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="A1.SS2.p7.7.m7.1b"><apply id="A1.SS2.p7.7.m7.1.1.cmml" xref="A1.SS2.p7.7.m7.1.1"><csymbol cd="ambiguous" id="A1.SS2.p7.7.m7.1.1.1.cmml" xref="A1.SS2.p7.7.m7.1.1">superscript</csymbol><ci id="A1.SS2.p7.7.m7.1.1.2.cmml" xref="A1.SS2.p7.7.m7.1.1.2">𝐷</ci><cn type="integer" id="A1.SS2.p7.7.m7.1.1.3.cmml" xref="A1.SS2.p7.7.m7.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p7.7.m7.1c">D^{3}</annotation></semantics></math>) overlap &lt;1% with base classes (from OVD &amp; REC datasets).</p>
</div>
</section>
<section id="A1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Annotation process</h3>

<figure id="A1.F9" class="ltx_figure"><img src="/html/2307.12813/assets/x18.png" id="A1.F9.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="405" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F9.5.2.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="A1.F9.3.1" class="ltx_text" style="font-size:90%;">Annotation process of the proposed <math id="A1.F9.3.1.m1.1" class="ltx_Math" alttext="D^{3}" display="inline"><semantics id="A1.F9.3.1.m1.1b"><msup id="A1.F9.3.1.m1.1.1" xref="A1.F9.3.1.m1.1.1.cmml"><mi id="A1.F9.3.1.m1.1.1.2" xref="A1.F9.3.1.m1.1.1.2.cmml">D</mi><mn id="A1.F9.3.1.m1.1.1.3" xref="A1.F9.3.1.m1.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="A1.F9.3.1.m1.1c"><apply id="A1.F9.3.1.m1.1.1.cmml" xref="A1.F9.3.1.m1.1.1"><csymbol cd="ambiguous" id="A1.F9.3.1.m1.1.1.1.cmml" xref="A1.F9.3.1.m1.1.1">superscript</csymbol><ci id="A1.F9.3.1.m1.1.1.2.cmml" xref="A1.F9.3.1.m1.1.1.2">𝐷</ci><cn type="integer" id="A1.F9.3.1.m1.1.1.3.cmml" xref="A1.F9.3.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.F9.3.1.m1.1d">D^{3}</annotation></semantics></math> benchmark.
</span></figcaption>
</figure>
<div id="A1.SS3.p1" class="ltx_para">
<p id="A1.SS3.p1.1" class="ltx_p">The data source of D<sup id="A1.SS3.p1.1.1" class="ltx_sup">3</sup>] is 106 groups from GRD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>, with about 100 images crawled from <a target="_blank" href="https://www.flickr.com/" title="" class="ltx_ref ltx_href">Flickr</a> and 3~4 designed refs for each group. Each group belongs to a different scenario and the overlapping between refs from different groups are small (i.e, a ref for one group are not frequent (but possible) to appear in the images from another group). Now we have 10000+ images and 300+ refs.</p>
</div>
<div id="A1.SS3.p2" class="ltx_para">
<p id="A1.SS3.p2.1" class="ltx_p">A diagram illustrating the annotation process of D<sup id="A1.SS3.p2.1.1" class="ltx_sup">3</sup> is presented in <a href="#A1.F9" title="In A.3 Annotation process ‣ Appendix A Dataset Details ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">9</span></a>. Here we describe the details of the annotation steps as below:</p>
</div>
<div id="A1.SS3.p3" class="ltx_para">
<ol id="A1.I1" class="ltx_enumerate">
<li id="A1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="A1.I1.i1.p1" class="ltx_para">
<p id="A1.I1.i1.p1.1" class="ltx_p"><span id="A1.I1.i1.p1.1.1" class="ltx_text ltx_font_smallcaps">Manual</span> Adding absence refs: design 1~2 absence refs based on the images for each group and add them to the corresponding groups. Now we have 400+ refs.</p>
</div>
</li>
<li id="A1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="A1.I1.i2.p1" class="ltx_para">
<p id="A1.I1.i2.p1.4" class="ltx_p"><span id="A1.I1.i2.p1.4.1" class="ltx_text ltx_font_smallcaps">Automatic</span> Selecting possible positive refs: for each image, select <span id="A1.I1.i2.p1.4.2" class="ltx_text ltx_font_italic">all the refs</span> (4~6) from the group it belongs to, and also the other 105 groups (top-<math id="A1.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="A1.I1.i2.p1.1.m1.1a"><mi id="A1.I1.i2.p1.1.m1.1.1" xref="A1.I1.i2.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="A1.I1.i2.p1.1.m1.1b"><ci id="A1.I1.i2.p1.1.m1.1.1.cmml" xref="A1.I1.i2.p1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.I1.i2.p1.1.m1.1c">n</annotation></semantics></math> refs out of 400+ refs, by CLIP similarity between the image and each description). Now for each image, we have <math id="A1.I1.i2.p1.2.m2.1" class="ltx_Math" alttext="n+4" display="inline"><semantics id="A1.I1.i2.p1.2.m2.1a"><mrow id="A1.I1.i2.p1.2.m2.1.1" xref="A1.I1.i2.p1.2.m2.1.1.cmml"><mi id="A1.I1.i2.p1.2.m2.1.1.2" xref="A1.I1.i2.p1.2.m2.1.1.2.cmml">n</mi><mo id="A1.I1.i2.p1.2.m2.1.1.1" xref="A1.I1.i2.p1.2.m2.1.1.1.cmml">+</mo><mn id="A1.I1.i2.p1.2.m2.1.1.3" xref="A1.I1.i2.p1.2.m2.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.I1.i2.p1.2.m2.1b"><apply id="A1.I1.i2.p1.2.m2.1.1.cmml" xref="A1.I1.i2.p1.2.m2.1.1"><plus id="A1.I1.i2.p1.2.m2.1.1.1.cmml" xref="A1.I1.i2.p1.2.m2.1.1.1"></plus><ci id="A1.I1.i2.p1.2.m2.1.1.2.cmml" xref="A1.I1.i2.p1.2.m2.1.1.2">𝑛</ci><cn type="integer" id="A1.I1.i2.p1.2.m2.1.1.3.cmml" xref="A1.I1.i2.p1.2.m2.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.I1.i2.p1.2.m2.1c">n+4</annotation></semantics></math>~<math id="A1.I1.i2.p1.3.m3.1" class="ltx_Math" alttext="n+6" display="inline"><semantics id="A1.I1.i2.p1.3.m3.1a"><mrow id="A1.I1.i2.p1.3.m3.1.1" xref="A1.I1.i2.p1.3.m3.1.1.cmml"><mi id="A1.I1.i2.p1.3.m3.1.1.2" xref="A1.I1.i2.p1.3.m3.1.1.2.cmml">n</mi><mo id="A1.I1.i2.p1.3.m3.1.1.1" xref="A1.I1.i2.p1.3.m3.1.1.1.cmml">+</mo><mn id="A1.I1.i2.p1.3.m3.1.1.3" xref="A1.I1.i2.p1.3.m3.1.1.3.cmml">6</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.I1.i2.p1.3.m3.1b"><apply id="A1.I1.i2.p1.3.m3.1.1.cmml" xref="A1.I1.i2.p1.3.m3.1.1"><plus id="A1.I1.i2.p1.3.m3.1.1.1.cmml" xref="A1.I1.i2.p1.3.m3.1.1.1"></plus><ci id="A1.I1.i2.p1.3.m3.1.1.2.cmml" xref="A1.I1.i2.p1.3.m3.1.1.2">𝑛</ci><cn type="integer" id="A1.I1.i2.p1.3.m3.1.1.3.cmml" xref="A1.I1.i2.p1.3.m3.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.I1.i2.p1.3.m3.1c">n+6</annotation></semantics></math> candidate refs and all the other refs are filtered out. <math id="A1.I1.i2.p1.4.m4.1" class="ltx_Math" alttext="n" display="inline"><semantics id="A1.I1.i2.p1.4.m4.1a"><mi id="A1.I1.i2.p1.4.m4.1.1" xref="A1.I1.i2.p1.4.m4.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="A1.I1.i2.p1.4.m4.1b"><ci id="A1.I1.i2.p1.4.m4.1.1.cmml" xref="A1.I1.i2.p1.4.m4.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.I1.i2.p1.4.m4.1c">n</annotation></semantics></math> is set as 40 initially.</p>
</div>
</li>
<li id="A1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="A1.I1.i3.p1" class="ltx_para">
<p id="A1.I1.i3.p1.1" class="ltx_p"><span id="A1.I1.i3.p1.1.1" class="ltx_text ltx_font_smallcaps">Manual</span> Verification: randomly choose 5 groups of images, and check if there are any positive refs that should not be filtered out. If so, increase <math id="A1.I1.i3.p1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="A1.I1.i3.p1.1.m1.1a"><mi id="A1.I1.i3.p1.1.m1.1.1" xref="A1.I1.i3.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="A1.I1.i3.p1.1.m1.1b"><ci id="A1.I1.i3.p1.1.m1.1.1.cmml" xref="A1.I1.i3.p1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.I1.i3.p1.1.m1.1c">n</annotation></semantics></math> to cover that ref and go back to step 2.</p>
</div>
</li>
<li id="A1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="A1.I1.i4.p1" class="ltx_para">
<p id="A1.I1.i4.p1.1" class="ltx_p"><span id="A1.I1.i4.p1.1.1" class="ltx_text ltx_font_smallcaps">Manual</span> Human annotation: annotation by trained annotators on all images. The annotation of boxes (and instance masks) are instance-level, dataset-wise complete, and includes absence refs.</p>
</div>
</li>
<li id="A1.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="A1.I1.i5.p1" class="ltx_para">
<p id="A1.I1.i5.p1.1" class="ltx_p"><span id="A1.I1.i5.p1.1.1" class="ltx_text ltx_font_smallcaps">Manual</span> Quality check: this includes 3 small steps:</p>
<ol id="A1.I1.i5.I1" class="ltx_enumerate">
<li id="A1.I1.i5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span> 
<div id="A1.I1.i5.I1.i1.p1" class="ltx_para">
<p id="A1.I1.i5.I1.i1.p1.1" class="ltx_p">Discarding some images (unsuitable for annotation, e.g., ambiguity) or categories from the dataset. About 8% samples are discarded.</p>
</div>
</li>
<li id="A1.I1.i5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span> 
<div id="A1.I1.i5.I1.i2.p1" class="ltx_para">
<p id="A1.I1.i5.I1.i2.p1.1" class="ltx_p">Quality check on 100% samples. For each group, if image with error is more than 2%, it is returned for re-annotation. Otherwise the errors are fixed and this group passes this step.</p>
</div>
</li>
<li id="A1.I1.i5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(c)</span> 
<div id="A1.I1.i5.I1.i3.p1" class="ltx_para">
<p id="A1.I1.i5.I1.i3.p1.1" class="ltx_p">Final check on 5% samples. For each group, if there are image with error, it is returned, otherwise it is accepted.</p>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
</section>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Evaluating Existing Baselines</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.3" class="ltx_p">In Section 4.1 of the paper we evaluate several representative and SOTA methods for OVD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, REC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> and bi-functional methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> on the proposed D<sup id="A2.p1.3.1" class="ltx_sup">3</sup> for the DOD task.
Here we introduce these methods and describe how we adapt them to DOD and evaluate them on D<sup id="A2.p1.3.2" class="ltx_sup">3</sup>. Notably, the images in D<sup id="A2.p1.3.3" class="ltx_sup">3</sup> do not overlap with the training data of these existing baselines and our proposed baseline, so all the comparisons are actually conducted under zero-shot setting, and is relatively fair.</p>
</div>
<div id="A2.p2" class="ltx_para ltx_noindent">
<p id="A2.p2.1" class="ltx_p"><span id="A2.p2.1.1" class="ltx_text ltx_font_bold">OFA.</span>
OFA is the SOTA REC method. It is proposed as a general-purpose vision-language model, with ability to performing various tasks like image captioning (IC), VQA, referring expression comprehension (REC), etc.
It adopts data from various tasks for pretraining, including MLM, IC, VQA, REC, and OD.
Notably, through pretrained on object detection datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, it is not evaluated on these tasks at all. We find that a pretrained OFA model merely achieves 9.6 mAP on COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> benchmark, which is too far from modern object detectors. This is also the reason we do not include it as bi-functional models.</p>
</div>
<div id="A2.p3" class="ltx_para">
<p id="A2.p3.2" class="ltx_p">OFA can be evaluated on a downstream task either after pretraining or after fine-tuning on the specfic dataset.
On REC datasets, it is already strong with only pretraining and achieves SOTA performance after fine-tuning on REC only.
As the images in D<sup id="A2.p3.2.1" class="ltx_sup">3</sup> do not overlap with those in REC datasets, we use the pretrained model of OFA rather than the one fine-tuned on REC data, for better generalization ability.
The official checkpoints are used as the model to evaluate on D<sup id="A2.p3.2.2" class="ltx_sup">3</sup>. Model checkpoints of multiple sizes are available and we use the largest two, namely OFA-base and OFA-large.</p>
</div>
<div id="A2.p4" class="ltx_para">
<p id="A2.p4.1" class="ltx_p">For REC task, OFA takes in a pair of one image and one sentence, and predicts a sequence of 4 coordinates, which forms a bounding box.
For DOD, we apply a similar inference strategy. For a image and the candidate descriptions (for intra-scenario setting, only a few descriptions in that scenario; for inter-scenario setting, all the descriptions in the dataset), each description and the image form a input image-text pair and predicts a detected instance (bounding box) that will be saved as the result.
As OFA predicts token sequences of box coordinates and no classification scores, we use the average of the classification score on the 4 coordinate tokens as the confidence score for each detected instance. No further processing is applied.</p>
</div>
<div id="A2.p5" class="ltx_para ltx_noindent">
<p id="A2.p5.1" class="ltx_p"><span id="A2.p5.1.1" class="ltx_text ltx_font_bold">OWL-ViT.</span>
OWL-ViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> and CORA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> are the SOTA OVD methods.
OWL-ViT also adopts a pretraining and fine-tuning strategy for training. It is pretrained with image-text contrastive learning, similar to CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> and then transferred to OVD with simple modification and fine-tuning on standard detection datasets. For evaluation on D<sup id="A2.p5.1.2" class="ltx_sup">3</sup>, we use the model fine-tuned on detection datasets without other training.
Model checkpoints with ViT-base <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> and ViT-large backbones are available.</p>
</div>
<div id="A2.p6" class="ltx_para">
<p id="A2.p6.1" class="ltx_p">For OVD, OWL-ViT takes in some text sequences and one image, and predicts a lot of instances consisting of bounding boxes, class labels as well as classification scores. The text sequences are category names like <span id="A2.p6.1.1" class="ltx_text ltx_font_typewriter">giraffe</span>, <span id="A2.p6.1.2" class="ltx_text ltx_font_typewriter">car</span>, etc. The detected instances with a score less than threshold 0.1 are filtered.
For the proposed DOD, we apply a similar inference strategy. The input text is the candidate descriptions, and the output instances are filtered by the same threshold 0.1. No other modifications or post-process are applied.</p>
</div>
<div id="A2.p7" class="ltx_para ltx_noindent">
<p id="A2.p7.1" class="ltx_p"><span id="A2.p7.1.1" class="ltx_text ltx_font_bold">CORA.</span>
CORA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> is a DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> style method that adapts CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> to OVD. It takes CLIP as the pretrained model and fine-tune the modified framework on detection datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.</p>
</div>
<div id="A2.p8" class="ltx_para">
<p id="A2.p8.1" class="ltx_p">The inference of CORA on OVD is performed as a matching between image region features and category name embeddings encoded by CLIP text encoder. For inference on DOD, we adopt the same strategy. We only replace the input images with the images from D<sup id="A2.p8.1.1" class="ltx_sup">3</sup> and the category names with the candidate descriptions. Other details follow the settings in CORA for OVD.</p>
</div>
<div id="A2.p9" class="ltx_para ltx_noindent">
<p id="A2.p9.1" class="ltx_p"><span id="A2.p9.1.1" class="ltx_text ltx_font_bold">Grounding-DINO.</span>
The bi-functional Grounding-DINO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> extends a close-set object detector DINO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> to open-set object detection. It is pretrained on vast object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> and image captioning data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. However, this model is not competitive on REC, and a further fine-tuning on REC data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>is required to achieve a strong performance.
Official model checkpoints with Swin-tiny <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> and Swin-base backbones are available.</p>
</div>
<div id="A2.p10" class="ltx_para">
<p id="A2.p10.1" class="ltx_p">It produces a lot of detected instances for one image-text input, and filters some instances with a threshold hyper-parameter. For the inference on REC, given an image-reference pair, it merely keeps the one and only instance with the largest score.
We follow its inference process on REC task for the proposed DOD. We will dig more into the specific inference strategy and hyper-parameters in the additional experiments in <a href="#A4" title="Appendix D More experimental results ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">D</span></a>.</p>
</div>
<div id="A2.p11" class="ltx_para ltx_noindent">
<p id="A2.p11.1" class="ltx_p"><span id="A2.p11.1.1" class="ltx_text ltx_font_bold">UNINEXT.</span>
UNINEXT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> stands as another bi-functional method, reformulating a diverse array of tasks, such as object detection, REC, video-based tracking, image and video segmentation tasks, into a unified multi-task framework that excels in instance prediction and retrieval. This innovative approach involves three stages of pre-training without any single-task fine-tuning. In the first stage, training is performed with Object365 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, followed by the second stage with REC data and COCO, and finally, the third stage with extensive data from video tasks.</p>
</div>
<div id="A2.p12" class="ltx_para">
<p id="A2.p12.1" class="ltx_p">For evaluation on D<sup id="A2.p12.1.1" class="ltx_sup">3</sup>, we utilize the UNINEXT models trained in the second stage, which only utilizes image data and is relatively fair for comparison. Model checkpoints featuring ConvNeXt-large and ViT-huge backbones are available, and these are the ones we employ for evaluation.</p>
</div>
<div id="A2.p13" class="ltx_para">
<p id="A2.p13.1" class="ltx_p">For each task it is pretrained on, UNINEXT designs an individual inference strategy.
For the DOD task, we adopt an inference strategy similar to REC. To delve deeper into the specific inference strategy and hyper-parameters, we also conduct additional experiments in <a href="#A4" title="Appendix D More experimental results ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">D</span></a>.</p>
</div>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>The Proposed Baseline</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p">As stated in Section 4.2 of our paper, we choose OFA as the foundation for the proposed baseline. Here we provides two figures to show the differences between OFA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> in <a href="#A3.F10" title="In Appendix C The Proposed Baseline ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">10</span></a> and the proposed OFA-DOD in <a href="#A3.F11" title="In Appendix C The Proposed Baseline ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">11</span></a>.</p>
</div>
<div id="A3.p2" class="ltx_para">
<p id="A3.p2.1" class="ltx_p">As shown in the two figures, the first modification, granularity decomposition, corresponds to replacing a shared decoder with two parallel decoders, one for global tasks and one for local tasks; the second modification, reconstructed data, refers to the reconstructed OVD &amp; REC data for the local decoder, after which the input can be one or multiple references (or object category names) and they can corresponds to zero, one or multiple targets; the third modification, task decomposition, is depicted by adding a binary classification in the global decoder, which determines if a bounding box and a description is matched.</p>
</div>
<figure id="A3.F10" class="ltx_figure"><img src="/html/2307.12813/assets/x19.png" id="A3.F10.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="164" height="106" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A3.F10.3.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span id="A3.F10.4.2" class="ltx_text" style="font-size:90%;">Model structure of OFA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>.
</span></figcaption>
</figure>
<figure id="A3.F11" class="ltx_figure"><img src="/html/2307.12813/assets/x20.png" id="A3.F11.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="348" height="106" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A3.F11.3.1.1" class="ltx_text" style="font-size:90%;">Figure 11</span>: </span><span id="A3.F11.4.2" class="ltx_text" style="font-size:90%;">Model structure of the proposed OFA-DOD.
</span></figcaption>
</figure>
<div id="A3.p3" class="ltx_para">
<p id="A3.p3.1" class="ltx_p">More details regarding these 3 modifications are stated below:</p>
</div>
<section id="A3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1 </span>Granularity decomposition</h3>

<div id="A3.SS1.p1" class="ltx_para">
<p id="A3.SS1.p1.1" class="ltx_p">The aim of this adjustment is to enhance the suitability of the baseline for localization tasks such as OVD, REC, and DOD.
The original OFA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> consists of a multi-modal encoder and a decoder.
For each task, whether it involves image-only, text-only, or image-text inputs, an image (which can be omitted) and a text prompt are fed into the multi-modal encoder to predict the output as a text sequence.
All task processes are forced to co-exist in one encoder and one decoder.</p>
</div>
<div id="A3.SS1.p2" class="ltx_para">
<p id="A3.SS1.p2.1" class="ltx_p">To achieve this decomposition, we divide the pretraining tasks of OFA into two different granularities: global tasks for language modeling-related tasks like IC, VQA, MLM, etc., and local tasks for region localization-related tasks such as object detection and REC.
We add an extra decoder alongside the original one, which also takes input from the encoder. The two decoders handle global and local tasks independently, thereby avoiding mutual interference.</p>
</div>
<div id="A3.SS1.p3" class="ltx_para">
<p id="A3.SS1.p3.1" class="ltx_p">This improvement effectively resolves conflicts between different tasks and enhances the capability of the model for localization tasks.</p>
</div>
</section>
<section id="A3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2 </span>Reconstructed data</h3>

<div id="A3.SS2.p1" class="ltx_para">
<p id="A3.SS2.p1.1" class="ltx_p">This improvement is to benefit detection with multiple target instances.
For OFA, REC is performed with one image and one text prompt (question prefix concatenated with one description) as input, and a bounding box sequence with 4 coordinate tokens as output. The input sequence has the form:</p>
</div>
<div id="A3.SS2.p2" class="ltx_para">
<p id="A3.SS2.p2.1" class="ltx_p ltx_align_center"><span id="A3.SS2.p2.1.1" class="ltx_text" style="color:#0000FF;">Which region does the text [REF1] describe?</span> <span id="A3.SS2.p2.1.2" class="ltx_text" style="color:#00FF00;">[IMG1]</span>,</p>
</div>
<div id="A3.SS2.p3" class="ltx_para">
<p id="A3.SS2.p3.1" class="ltx_p">where <span id="A3.SS2.p3.1.1" class="ltx_text" style="color:#0000FF;">[REF1]</span> is a description annotated for the image, and <span id="A3.SS2.p3.1.2" class="ltx_text" style="color:#00FF00;">[IMG1]</span> is the image token sequence.</p>
</div>
<div id="A3.SS2.p4" class="ltx_para">
<p id="A3.SS2.p4.3" class="ltx_p">Originally, each input example in REC is a image-text-box pair, where one reference is annotated with one bounding box for one image.
We reconstruct the data of REC by 2 steps:
First, we grouping the descriptions belonging to one image, and each reconstructed input example is a combination of one image, <math id="A3.SS2.p4.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="A3.SS2.p4.1.m1.1a"><mi id="A3.SS2.p4.1.m1.1.1" xref="A3.SS2.p4.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="A3.SS2.p4.1.m1.1b"><ci id="A3.SS2.p4.1.m1.1.1.cmml" xref="A3.SS2.p4.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.SS2.p4.1.m1.1c">N</annotation></semantics></math> positive descriptions, and <math id="A3.SS2.p4.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="A3.SS2.p4.2.m2.1a"><mi id="A3.SS2.p4.2.m2.1.1" xref="A3.SS2.p4.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="A3.SS2.p4.2.m2.1b"><ci id="A3.SS2.p4.2.m2.1.1.cmml" xref="A3.SS2.p4.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.SS2.p4.2.m2.1c">N</annotation></semantics></math> boxes, where <math id="A3.SS2.p4.3.m3.1" class="ltx_Math" alttext="N" display="inline"><semantics id="A3.SS2.p4.3.m3.1a"><mi id="A3.SS2.p4.3.m3.1.1" xref="A3.SS2.p4.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="A3.SS2.p4.3.m3.1b"><ci id="A3.SS2.p4.3.m3.1.1.cmml" xref="A3.SS2.p4.3.m3.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.SS2.p4.3.m3.1c">N</annotation></semantics></math> is a integer equal to or larger than 1.
Second, for each image, we sample some descriptions from other images as the negative description.
With the prepared data, we change the input as:</p>
</div>
<div id="A3.SS2.p5" class="ltx_para">
<p id="A3.SS2.p5.1" class="ltx_p ltx_align_center"><span id="A3.SS2.p5.1.1" class="ltx_text" style="color:#0000FF;">Which of these options are in the image? Choose from options: [REF1] [REF2] [REF3] …</span>
<span id="A3.SS2.p5.1.2" class="ltx_text" style="color:#00FF00;">[IMG1]</span>,</p>
</div>
<div id="A3.SS2.p6" class="ltx_para">
<p id="A3.SS2.p6.1" class="ltx_p">where <span id="A3.SS2.p6.1.1" class="ltx_text" style="color:#0000FF;">[REF1] [REF2] [REF3]</span> are positive or negative randomly sampled.
The output is to predict a series of multiple boxes, each followed by its corresponding descriptions in the input.
This results in a unified data format for OD and REC. For OD, the negative descriptions are negative class names.
The reformulated data are noisy, as they are not initially prepared for DOD, and a sampled negative description is not necessarily negative due to the image-level annotation completeness of REC. Still, we find such reconstructed data helpful.</p>
</div>
</section>
<section id="A3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.3 </span>Task decomposition</h3>

<div id="A3.SS3.p1" class="ltx_para">
<p id="A3.SS3.p1.1" class="ltx_p">This step aims to enhance the baseline’s capability to discern false positives. In addition to training on REC (to locate a region based on a reference), we leverage the multi-task nature of OFA by introducing an additional VQA task. This task involves determining whether a predicted region and a description match with each other and can be viewed as a binary classification problem.
The input for this VQA task is:</p>
</div>
<div id="A3.SS3.p2" class="ltx_para">
<p id="A3.SS3.p2.1" class="ltx_p ltx_align_center"><span id="A3.SS3.p2.1.1" class="ltx_text" style="color:#0000FF;">Does the region [BOX1] describes [REF1]?</span>
<span id="A3.SS3.p2.1.2" class="ltx_text" style="color:#00FF00;">IMG1</span>,</p>
</div>
<div id="A3.SS3.p3" class="ltx_para">
<p id="A3.SS3.p3.1" class="ltx_p">where <span id="A3.SS3.p3.1.1" class="ltx_text" style="color:#0000FF;">[BOX1]</span> is the bounding box coordinate tokens corresponds to the description. For training, the box and the reference are either from a GT text-box pair, or the GT box is shifted (as negative sample), or the box and the reference are from different text-box pairs (as negative sample, too).
The output of this task is a text sequence <span id="A3.SS3.p3.1.2" class="ltx_text" style="color:#0000FF;">yes</span> for positive samples and <span id="A3.SS3.p3.1.3" class="ltx_text" style="color:#0000FF;">no</span> for negative samples.
This step is responsible for rejecting possible false positives.</p>
</div>
</section>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>More experimental results</h2>

<figure id="A4.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="A4.T6.12.1.1" class="ltx_text" style="font-size:90%;">Table 6</span>: </span><span id="A4.T6.13.2" class="ltx_text" style="font-size:90%;">Comparison of different methods on the proposed dataset for different mAP metrics: intra-secnario mAPs, inter-scenario mAPs, and average recalls. “Bi” denotes bi-functional methods.</span></figcaption>
<div id="A4.T6.10" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:215.1pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-1.9pt,0.9pt) scale(0.991245677558745,0.991245677558745) ;">
<table id="A4.T6.10.10" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A4.T6.10.10.11.1" class="ltx_tr">
<th id="A4.T6.10.10.11.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="2"><span id="A4.T6.10.10.11.1.1.1" class="ltx_text">Task</span></th>
<th id="A4.T6.10.10.11.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="A4.T6.10.10.11.1.2.1" class="ltx_text">Method</span></th>
<td id="A4.T6.10.10.11.1.3" class="ltx_td ltx_align_center ltx_border_t" colspan="3">Intra-scenario</td>
<td id="A4.T6.10.10.11.1.4" class="ltx_td ltx_align_center ltx_border_l ltx_border_t" colspan="3">Inter-scenario</td>
<td id="A4.T6.10.10.11.1.5" class="ltx_td ltx_align_center ltx_border_t" colspan="3">Average Recall</td>
</tr>
<tr id="A4.T6.10.10.12.2" class="ltx_tr">
<td id="A4.T6.10.10.12.2.1" class="ltx_td ltx_align_center"><span id="A4.T6.10.10.12.2.1.1" class="ltx_text ltx_font_slanted">FULL</span></td>
<td id="A4.T6.10.10.12.2.2" class="ltx_td ltx_align_center"><span id="A4.T6.10.10.12.2.2.1" class="ltx_text ltx_font_slanted">PRES</span></td>
<td id="A4.T6.10.10.12.2.3" class="ltx_td ltx_align_center ltx_border_r"><span id="A4.T6.10.10.12.2.3.1" class="ltx_text ltx_font_slanted">ABS</span></td>
<td id="A4.T6.10.10.12.2.4" class="ltx_td ltx_align_center"><span id="A4.T6.10.10.12.2.4.1" class="ltx_text ltx_font_slanted">FULL</span></td>
<td id="A4.T6.10.10.12.2.5" class="ltx_td ltx_align_center"><span id="A4.T6.10.10.12.2.5.1" class="ltx_text ltx_font_slanted">PRES</span></td>
<td id="A4.T6.10.10.12.2.6" class="ltx_td ltx_align_center ltx_border_r"><span id="A4.T6.10.10.12.2.6.1" class="ltx_text ltx_font_slanted">ABS</span></td>
<td id="A4.T6.10.10.12.2.7" class="ltx_td ltx_align_center"><span id="A4.T6.10.10.12.2.7.1" class="ltx_text ltx_font_slanted">FULL</span></td>
<td id="A4.T6.10.10.12.2.8" class="ltx_td ltx_align_center"><span id="A4.T6.10.10.12.2.8.1" class="ltx_text ltx_font_slanted">PRES</span></td>
<td id="A4.T6.10.10.12.2.9" class="ltx_td ltx_align_center"><span id="A4.T6.10.10.12.2.9.1" class="ltx_text ltx_font_slanted">ABS</span></td>
</tr>
<tr id="A4.T6.1.1.1" class="ltx_tr">
<th id="A4.T6.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="2"><span id="A4.T6.1.1.1.2.1" class="ltx_text">REC</span></th>
<th id="A4.T6.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">OFA<math id="A4.T6.1.1.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{base}}" display="inline"><semantics id="A4.T6.1.1.1.1.m1.1a"><msub id="A4.T6.1.1.1.1.m1.1.1" xref="A4.T6.1.1.1.1.m1.1.1.cmml"><mi id="A4.T6.1.1.1.1.m1.1.1a" xref="A4.T6.1.1.1.1.m1.1.1.cmml"></mi><mtext id="A4.T6.1.1.1.1.m1.1.1.1" xref="A4.T6.1.1.1.1.m1.1.1.1a.cmml">base</mtext></msub><annotation-xml encoding="MathML-Content" id="A4.T6.1.1.1.1.m1.1b"><apply id="A4.T6.1.1.1.1.m1.1.1.cmml" xref="A4.T6.1.1.1.1.m1.1.1"><ci id="A4.T6.1.1.1.1.m1.1.1.1a.cmml" xref="A4.T6.1.1.1.1.m1.1.1.1"><mtext mathsize="70%" id="A4.T6.1.1.1.1.m1.1.1.1.cmml" xref="A4.T6.1.1.1.1.m1.1.1.1">base</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.T6.1.1.1.1.m1.1c">{}_{\text{base}}</annotation></semantics></math>
</th>
<td id="A4.T6.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t">3.4</td>
<td id="A4.T6.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t">3.0</td>
<td id="A4.T6.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.3</td>
<td id="A4.T6.1.1.1.6" class="ltx_td ltx_align_center ltx_border_t">0.1</td>
<td id="A4.T6.1.1.1.7" class="ltx_td ltx_align_center ltx_border_t">0.1</td>
<td id="A4.T6.1.1.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.1</td>
<td id="A4.T6.1.1.1.9" class="ltx_td ltx_align_center ltx_border_t">13.7</td>
<td id="A4.T6.1.1.1.10" class="ltx_td ltx_align_center ltx_border_t">13.5</td>
<td id="A4.T6.1.1.1.11" class="ltx_td ltx_align_center ltx_border_t">14.3</td>
</tr>
<tr id="A4.T6.2.2.2" class="ltx_tr">
<th id="A4.T6.2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">OFA<math id="A4.T6.2.2.2.1.m1.1" class="ltx_Math" alttext="{}_{\text{large}}" display="inline"><semantics id="A4.T6.2.2.2.1.m1.1a"><msub id="A4.T6.2.2.2.1.m1.1.1" xref="A4.T6.2.2.2.1.m1.1.1.cmml"><mi id="A4.T6.2.2.2.1.m1.1.1a" xref="A4.T6.2.2.2.1.m1.1.1.cmml"></mi><mtext id="A4.T6.2.2.2.1.m1.1.1.1" xref="A4.T6.2.2.2.1.m1.1.1.1a.cmml">large</mtext></msub><annotation-xml encoding="MathML-Content" id="A4.T6.2.2.2.1.m1.1b"><apply id="A4.T6.2.2.2.1.m1.1.1.cmml" xref="A4.T6.2.2.2.1.m1.1.1"><ci id="A4.T6.2.2.2.1.m1.1.1.1a.cmml" xref="A4.T6.2.2.2.1.m1.1.1.1"><mtext mathsize="70%" id="A4.T6.2.2.2.1.m1.1.1.1.cmml" xref="A4.T6.2.2.2.1.m1.1.1.1">large</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.T6.2.2.2.1.m1.1c">{}_{\text{large}}</annotation></semantics></math>
</th>
<td id="A4.T6.2.2.2.2" class="ltx_td ltx_align_center">4.2</td>
<td id="A4.T6.2.2.2.3" class="ltx_td ltx_align_center">4.1</td>
<td id="A4.T6.2.2.2.4" class="ltx_td ltx_align_center ltx_border_r">4.6</td>
<td id="A4.T6.2.2.2.5" class="ltx_td ltx_align_center">0.1</td>
<td id="A4.T6.2.2.2.6" class="ltx_td ltx_align_center">0.1</td>
<td id="A4.T6.2.2.2.7" class="ltx_td ltx_align_center ltx_border_r">0.1</td>
<td id="A4.T6.2.2.2.8" class="ltx_td ltx_align_center">17.1</td>
<td id="A4.T6.2.2.2.9" class="ltx_td ltx_align_center">16.7</td>
<td id="A4.T6.2.2.2.10" class="ltx_td ltx_align_center">18.4</td>
</tr>
<tr id="A4.T6.3.3.3" class="ltx_tr">
<th id="A4.T6.3.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="3"><span id="A4.T6.3.3.3.2.1" class="ltx_text">OVD</span></th>
<th id="A4.T6.3.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">CORA<math id="A4.T6.3.3.3.1.m1.1" class="ltx_Math" alttext="{}_{\text{R50}}" display="inline"><semantics id="A4.T6.3.3.3.1.m1.1a"><msub id="A4.T6.3.3.3.1.m1.1.1" xref="A4.T6.3.3.3.1.m1.1.1.cmml"><mi id="A4.T6.3.3.3.1.m1.1.1a" xref="A4.T6.3.3.3.1.m1.1.1.cmml"></mi><mtext id="A4.T6.3.3.3.1.m1.1.1.1" xref="A4.T6.3.3.3.1.m1.1.1.1a.cmml">R50</mtext></msub><annotation-xml encoding="MathML-Content" id="A4.T6.3.3.3.1.m1.1b"><apply id="A4.T6.3.3.3.1.m1.1.1.cmml" xref="A4.T6.3.3.3.1.m1.1.1"><ci id="A4.T6.3.3.3.1.m1.1.1.1a.cmml" xref="A4.T6.3.3.3.1.m1.1.1.1"><mtext mathsize="70%" id="A4.T6.3.3.3.1.m1.1.1.1.cmml" xref="A4.T6.3.3.3.1.m1.1.1.1">R50</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.T6.3.3.3.1.m1.1c">{}_{\text{R50}}</annotation></semantics></math>
</th>
<td id="A4.T6.3.3.3.3" class="ltx_td ltx_align_center ltx_border_t">6.2</td>
<td id="A4.T6.3.3.3.4" class="ltx_td ltx_align_center ltx_border_t">6.7</td>
<td id="A4.T6.3.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5.0</td>
<td id="A4.T6.3.3.3.6" class="ltx_td ltx_align_center ltx_border_t">2.0</td>
<td id="A4.T6.3.3.3.7" class="ltx_td ltx_align_center ltx_border_t">2.2</td>
<td id="A4.T6.3.3.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.3</td>
<td id="A4.T6.3.3.3.9" class="ltx_td ltx_align_center ltx_border_t">10.0</td>
<td id="A4.T6.3.3.3.10" class="ltx_td ltx_align_center ltx_border_t">10.5</td>
<td id="A4.T6.3.3.3.11" class="ltx_td ltx_align_center ltx_border_t">8.7</td>
</tr>
<tr id="A4.T6.4.4.4" class="ltx_tr">
<th id="A4.T6.4.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">OWL-ViT<math id="A4.T6.4.4.4.1.m1.1" class="ltx_Math" alttext="{}_{\text{base}}" display="inline"><semantics id="A4.T6.4.4.4.1.m1.1a"><msub id="A4.T6.4.4.4.1.m1.1.1" xref="A4.T6.4.4.4.1.m1.1.1.cmml"><mi id="A4.T6.4.4.4.1.m1.1.1a" xref="A4.T6.4.4.4.1.m1.1.1.cmml"></mi><mtext id="A4.T6.4.4.4.1.m1.1.1.1" xref="A4.T6.4.4.4.1.m1.1.1.1a.cmml">base</mtext></msub><annotation-xml encoding="MathML-Content" id="A4.T6.4.4.4.1.m1.1b"><apply id="A4.T6.4.4.4.1.m1.1.1.cmml" xref="A4.T6.4.4.4.1.m1.1.1"><ci id="A4.T6.4.4.4.1.m1.1.1.1a.cmml" xref="A4.T6.4.4.4.1.m1.1.1.1"><mtext mathsize="70%" id="A4.T6.4.4.4.1.m1.1.1.1.cmml" xref="A4.T6.4.4.4.1.m1.1.1.1">base</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.T6.4.4.4.1.m1.1c">{}_{\text{base}}</annotation></semantics></math>
</th>
<td id="A4.T6.4.4.4.2" class="ltx_td ltx_align_center">8.6</td>
<td id="A4.T6.4.4.4.3" class="ltx_td ltx_align_center">8.5</td>
<td id="A4.T6.4.4.4.4" class="ltx_td ltx_align_center ltx_border_r">8.8</td>
<td id="A4.T6.4.4.4.5" class="ltx_td ltx_align_center">3.2</td>
<td id="A4.T6.4.4.4.6" class="ltx_td ltx_align_center">3.7</td>
<td id="A4.T6.4.4.4.7" class="ltx_td ltx_align_center ltx_border_r"><span id="A4.T6.4.4.4.7.1" class="ltx_text ltx_font_bold">4.7</span></td>
<td id="A4.T6.4.4.4.8" class="ltx_td ltx_align_center">13.5</td>
<td id="A4.T6.4.4.4.9" class="ltx_td ltx_align_center">13.7</td>
<td id="A4.T6.4.4.4.10" class="ltx_td ltx_align_center">13.1</td>
</tr>
<tr id="A4.T6.5.5.5" class="ltx_tr">
<th id="A4.T6.5.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">OWL-ViT<math id="A4.T6.5.5.5.1.m1.1" class="ltx_Math" alttext="{}_{\text{large}}" display="inline"><semantics id="A4.T6.5.5.5.1.m1.1a"><msub id="A4.T6.5.5.5.1.m1.1.1" xref="A4.T6.5.5.5.1.m1.1.1.cmml"><mi id="A4.T6.5.5.5.1.m1.1.1a" xref="A4.T6.5.5.5.1.m1.1.1.cmml"></mi><mtext id="A4.T6.5.5.5.1.m1.1.1.1" xref="A4.T6.5.5.5.1.m1.1.1.1a.cmml">large</mtext></msub><annotation-xml encoding="MathML-Content" id="A4.T6.5.5.5.1.m1.1b"><apply id="A4.T6.5.5.5.1.m1.1.1.cmml" xref="A4.T6.5.5.5.1.m1.1.1"><ci id="A4.T6.5.5.5.1.m1.1.1.1a.cmml" xref="A4.T6.5.5.5.1.m1.1.1.1"><mtext mathsize="70%" id="A4.T6.5.5.5.1.m1.1.1.1.cmml" xref="A4.T6.5.5.5.1.m1.1.1.1">large</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.T6.5.5.5.1.m1.1c">{}_{\text{large}}</annotation></semantics></math>
</th>
<td id="A4.T6.5.5.5.2" class="ltx_td ltx_align_center">9.6</td>
<td id="A4.T6.5.5.5.3" class="ltx_td ltx_align_center">10.7</td>
<td id="A4.T6.5.5.5.4" class="ltx_td ltx_align_center ltx_border_r">6.4</td>
<td id="A4.T6.5.5.5.5" class="ltx_td ltx_align_center">2.5</td>
<td id="A4.T6.5.5.5.6" class="ltx_td ltx_align_center">2.9</td>
<td id="A4.T6.5.5.5.7" class="ltx_td ltx_align_center ltx_border_r">2.1</td>
<td id="A4.T6.5.5.5.8" class="ltx_td ltx_align_center">17.5</td>
<td id="A4.T6.5.5.5.9" class="ltx_td ltx_align_center">19.4</td>
<td id="A4.T6.5.5.5.10" class="ltx_td ltx_align_center">11.8</td>
</tr>
<tr id="A4.T6.6.6.6" class="ltx_tr">
<th id="A4.T6.6.6.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="4"><span id="A4.T6.6.6.6.2.1" class="ltx_text">Bi</span></th>
<th id="A4.T6.6.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">UNINEXT<math id="A4.T6.6.6.6.1.m1.1" class="ltx_Math" alttext="{}_{\text{large}}" display="inline"><semantics id="A4.T6.6.6.6.1.m1.1a"><msub id="A4.T6.6.6.6.1.m1.1.1" xref="A4.T6.6.6.6.1.m1.1.1.cmml"><mi id="A4.T6.6.6.6.1.m1.1.1a" xref="A4.T6.6.6.6.1.m1.1.1.cmml"></mi><mtext id="A4.T6.6.6.6.1.m1.1.1.1" xref="A4.T6.6.6.6.1.m1.1.1.1a.cmml">large</mtext></msub><annotation-xml encoding="MathML-Content" id="A4.T6.6.6.6.1.m1.1b"><apply id="A4.T6.6.6.6.1.m1.1.1.cmml" xref="A4.T6.6.6.6.1.m1.1.1"><ci id="A4.T6.6.6.6.1.m1.1.1.1a.cmml" xref="A4.T6.6.6.6.1.m1.1.1.1"><mtext mathsize="70%" id="A4.T6.6.6.6.1.m1.1.1.1.cmml" xref="A4.T6.6.6.6.1.m1.1.1.1">large</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.T6.6.6.6.1.m1.1c">{}_{\text{large}}</annotation></semantics></math>
</th>
<td id="A4.T6.6.6.6.3" class="ltx_td ltx_align_center ltx_border_t">17.9</td>
<td id="A4.T6.6.6.6.4" class="ltx_td ltx_align_center ltx_border_t">18.6</td>
<td id="A4.T6.6.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">15.9</td>
<td id="A4.T6.6.6.6.6" class="ltx_td ltx_align_center ltx_border_t">2.9</td>
<td id="A4.T6.6.6.6.7" class="ltx_td ltx_align_center ltx_border_t">3.1</td>
<td id="A4.T6.6.6.6.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2.5</td>
<td id="A4.T6.6.6.6.9" class="ltx_td ltx_align_center ltx_border_t">40.7</td>
<td id="A4.T6.6.6.6.10" class="ltx_td ltx_align_center ltx_border_t">42.6</td>
<td id="A4.T6.6.6.6.11" class="ltx_td ltx_align_center ltx_border_t">34.7</td>
</tr>
<tr id="A4.T6.7.7.7" class="ltx_tr">
<th id="A4.T6.7.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">UNINEXT<math id="A4.T6.7.7.7.1.m1.1" class="ltx_Math" alttext="{}_{\text{huge}}" display="inline"><semantics id="A4.T6.7.7.7.1.m1.1a"><msub id="A4.T6.7.7.7.1.m1.1.1" xref="A4.T6.7.7.7.1.m1.1.1.cmml"><mi id="A4.T6.7.7.7.1.m1.1.1a" xref="A4.T6.7.7.7.1.m1.1.1.cmml"></mi><mtext id="A4.T6.7.7.7.1.m1.1.1.1" xref="A4.T6.7.7.7.1.m1.1.1.1a.cmml">huge</mtext></msub><annotation-xml encoding="MathML-Content" id="A4.T6.7.7.7.1.m1.1b"><apply id="A4.T6.7.7.7.1.m1.1.1.cmml" xref="A4.T6.7.7.7.1.m1.1.1"><ci id="A4.T6.7.7.7.1.m1.1.1.1a.cmml" xref="A4.T6.7.7.7.1.m1.1.1.1"><mtext mathsize="70%" id="A4.T6.7.7.7.1.m1.1.1.1.cmml" xref="A4.T6.7.7.7.1.m1.1.1.1">huge</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.T6.7.7.7.1.m1.1c">{}_{\text{huge}}</annotation></semantics></math>
</th>
<td id="A4.T6.7.7.7.2" class="ltx_td ltx_align_center">20.0</td>
<td id="A4.T6.7.7.7.3" class="ltx_td ltx_align_center">20.6</td>
<td id="A4.T6.7.7.7.4" class="ltx_td ltx_align_center ltx_border_r">18.1</td>
<td id="A4.T6.7.7.7.5" class="ltx_td ltx_align_center">3.3</td>
<td id="A4.T6.7.7.7.6" class="ltx_td ltx_align_center">3.9</td>
<td id="A4.T6.7.7.7.7" class="ltx_td ltx_align_center ltx_border_r">1.6</td>
<td id="A4.T6.7.7.7.8" class="ltx_td ltx_align_center">45.3</td>
<td id="A4.T6.7.7.7.9" class="ltx_td ltx_align_center">46.7</td>
<td id="A4.T6.7.7.7.10" class="ltx_td ltx_align_center">41.4</td>
</tr>
<tr id="A4.T6.8.8.8" class="ltx_tr">
<th id="A4.T6.8.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">G-DINO<math id="A4.T6.8.8.8.1.m1.1" class="ltx_Math" alttext="{}_{\text{tiny}}" display="inline"><semantics id="A4.T6.8.8.8.1.m1.1a"><msub id="A4.T6.8.8.8.1.m1.1.1" xref="A4.T6.8.8.8.1.m1.1.1.cmml"><mi id="A4.T6.8.8.8.1.m1.1.1a" xref="A4.T6.8.8.8.1.m1.1.1.cmml"></mi><mtext id="A4.T6.8.8.8.1.m1.1.1.1" xref="A4.T6.8.8.8.1.m1.1.1.1a.cmml">tiny</mtext></msub><annotation-xml encoding="MathML-Content" id="A4.T6.8.8.8.1.m1.1b"><apply id="A4.T6.8.8.8.1.m1.1.1.cmml" xref="A4.T6.8.8.8.1.m1.1.1"><ci id="A4.T6.8.8.8.1.m1.1.1.1a.cmml" xref="A4.T6.8.8.8.1.m1.1.1.1"><mtext mathsize="70%" id="A4.T6.8.8.8.1.m1.1.1.1.cmml" xref="A4.T6.8.8.8.1.m1.1.1.1">tiny</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.T6.8.8.8.1.m1.1c">{}_{\text{tiny}}</annotation></semantics></math>
</th>
<td id="A4.T6.8.8.8.2" class="ltx_td ltx_align_center">19.2</td>
<td id="A4.T6.8.8.8.3" class="ltx_td ltx_align_center">18.5</td>
<td id="A4.T6.8.8.8.4" class="ltx_td ltx_align_center ltx_border_r">21.2</td>
<td id="A4.T6.8.8.8.5" class="ltx_td ltx_align_center">2.3</td>
<td id="A4.T6.8.8.8.6" class="ltx_td ltx_align_center">2.5</td>
<td id="A4.T6.8.8.8.7" class="ltx_td ltx_align_center ltx_border_r">2.1</td>
<td id="A4.T6.8.8.8.8" class="ltx_td ltx_align_center">47.8</td>
<td id="A4.T6.8.8.8.9" class="ltx_td ltx_align_center">48.1</td>
<td id="A4.T6.8.8.8.10" class="ltx_td ltx_align_center">46.6</td>
</tr>
<tr id="A4.T6.9.9.9" class="ltx_tr">
<th id="A4.T6.9.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">G-DINO<math id="A4.T6.9.9.9.1.m1.1" class="ltx_Math" alttext="{}_{\text{base}}" display="inline"><semantics id="A4.T6.9.9.9.1.m1.1a"><msub id="A4.T6.9.9.9.1.m1.1.1" xref="A4.T6.9.9.9.1.m1.1.1.cmml"><mi id="A4.T6.9.9.9.1.m1.1.1a" xref="A4.T6.9.9.9.1.m1.1.1.cmml"></mi><mtext id="A4.T6.9.9.9.1.m1.1.1.1" xref="A4.T6.9.9.9.1.m1.1.1.1a.cmml">base</mtext></msub><annotation-xml encoding="MathML-Content" id="A4.T6.9.9.9.1.m1.1b"><apply id="A4.T6.9.9.9.1.m1.1.1.cmml" xref="A4.T6.9.9.9.1.m1.1.1"><ci id="A4.T6.9.9.9.1.m1.1.1.1a.cmml" xref="A4.T6.9.9.9.1.m1.1.1.1"><mtext mathsize="70%" id="A4.T6.9.9.9.1.m1.1.1.1.cmml" xref="A4.T6.9.9.9.1.m1.1.1.1">base</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.T6.9.9.9.1.m1.1c">{}_{\text{base}}</annotation></semantics></math>
</th>
<td id="A4.T6.9.9.9.2" class="ltx_td ltx_align_center">20.7</td>
<td id="A4.T6.9.9.9.3" class="ltx_td ltx_align_center">20.1</td>
<td id="A4.T6.9.9.9.4" class="ltx_td ltx_align_center ltx_border_r"><span id="A4.T6.9.9.9.4.1" class="ltx_text ltx_font_bold">22.5</span></td>
<td id="A4.T6.9.9.9.5" class="ltx_td ltx_align_center">2.7</td>
<td id="A4.T6.9.9.9.6" class="ltx_td ltx_align_center">2.4</td>
<td id="A4.T6.9.9.9.7" class="ltx_td ltx_align_center ltx_border_r">3.5</td>
<td id="A4.T6.9.9.9.8" class="ltx_td ltx_align_center">51.1</td>
<td id="A4.T6.9.9.9.9" class="ltx_td ltx_align_center">51.8</td>
<td id="A4.T6.9.9.9.10" class="ltx_td ltx_align_center">48.9</td>
</tr>
<tr id="A4.T6.10.10.10" class="ltx_tr">
<th id="A4.T6.10.10.10.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t">DOD</th>
<th id="A4.T6.10.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">OFA-DOD<math id="A4.T6.10.10.10.1.m1.1" class="ltx_Math" alttext="{}_{\text{base}}" display="inline"><semantics id="A4.T6.10.10.10.1.m1.1a"><msub id="A4.T6.10.10.10.1.m1.1.1" xref="A4.T6.10.10.10.1.m1.1.1.cmml"><mi id="A4.T6.10.10.10.1.m1.1.1a" xref="A4.T6.10.10.10.1.m1.1.1.cmml"></mi><mtext id="A4.T6.10.10.10.1.m1.1.1.1" xref="A4.T6.10.10.10.1.m1.1.1.1a.cmml">base</mtext></msub><annotation-xml encoding="MathML-Content" id="A4.T6.10.10.10.1.m1.1b"><apply id="A4.T6.10.10.10.1.m1.1.1.cmml" xref="A4.T6.10.10.10.1.m1.1.1"><ci id="A4.T6.10.10.10.1.m1.1.1.1a.cmml" xref="A4.T6.10.10.10.1.m1.1.1.1"><mtext mathsize="70%" id="A4.T6.10.10.10.1.m1.1.1.1.cmml" xref="A4.T6.10.10.10.1.m1.1.1.1">base</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.T6.10.10.10.1.m1.1c">{}_{\text{base}}</annotation></semantics></math>
</th>
<td id="A4.T6.10.10.10.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="A4.T6.10.10.10.3.1" class="ltx_text ltx_font_bold">21.6</span></td>
<td id="A4.T6.10.10.10.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="A4.T6.10.10.10.4.1" class="ltx_text ltx_font_bold">23.7</span></td>
<td id="A4.T6.10.10.10.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">15.4</td>
<td id="A4.T6.10.10.10.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="A4.T6.10.10.10.6.1" class="ltx_text ltx_font_bold">5.7</span></td>
<td id="A4.T6.10.10.10.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="A4.T6.10.10.10.7.1" class="ltx_text ltx_font_bold">6.9</span></td>
<td id="A4.T6.10.10.10.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">2.3</td>
<td id="A4.T6.10.10.10.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">47.4</td>
<td id="A4.T6.10.10.10.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">49.5</td>
<td id="A4.T6.10.10.10.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">41.2</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="A4.T7" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="A4.T7.5.1.1" class="ltx_text" style="font-size:90%;">Table 7</span>: </span><span id="A4.T7.6.2" class="ltx_text" style="font-size:90%;">Performance of bi-functional methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>, compared with the proposed baseline, under different score filtering thresholds. The mAP under <span id="A4.T7.6.2.1" class="ltx_text ltx_font_slanted">FULL</span> setting and the False Positive Per Category (FPPC) on images with no instance for one category are reported as metrics. For methods filtered with different score thresholds, we highlight the rows when they achieve a FPPC similar to our OFA-DOD.</span></figcaption>
<table id="A4.T7.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A4.T7.2.3.1" class="ltx_tr">
<th id="A4.T7.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="A4.T7.2.3.1.1.1" class="ltx_text">Method</span></th>
<th id="A4.T7.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="A4.T7.2.3.1.2.1" class="ltx_text">Threshold</span></th>
<td id="A4.T7.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t">No-instance</td>
<td id="A4.T7.2.3.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="A4.T7.2.3.1.4.1" class="ltx_text ltx_font_slanted">FULL</span></td>
</tr>
<tr id="A4.T7.2.2" class="ltx_tr">
<td id="A4.T7.1.1.1" class="ltx_td ltx_align_center">FPPC (%) <math id="A4.T7.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="A4.T7.1.1.1.m1.1a"><mo stretchy="false" id="A4.T7.1.1.1.m1.1.1" xref="A4.T7.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="A4.T7.1.1.1.m1.1b"><ci id="A4.T7.1.1.1.m1.1.1.cmml" xref="A4.T7.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="A4.T7.2.2.2" class="ltx_td ltx_align_center">mAP (%) <math id="A4.T7.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="A4.T7.2.2.2.m1.1a"><mo stretchy="false" id="A4.T7.2.2.2.m1.1.1" xref="A4.T7.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A4.T7.2.2.2.m1.1b"><ci id="A4.T7.2.2.2.m1.1.1.cmml" xref="A4.T7.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr id="A4.T7.2.4.2" class="ltx_tr">
<th id="A4.T7.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="7"><span id="A4.T7.2.4.2.1.1" class="ltx_text">UNINEXT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite></span></th>
<th id="A4.T7.2.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">-</th>
<td id="A4.T7.2.4.2.3" class="ltx_td ltx_align_center ltx_border_t">100.0</td>
<td id="A4.T7.2.4.2.4" class="ltx_td ltx_align_center ltx_border_t">20.0</td>
</tr>
<tr id="A4.T7.2.5.3" class="ltx_tr">
<th id="A4.T7.2.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">0.4</th>
<td id="A4.T7.2.5.3.2" class="ltx_td ltx_align_center">99.3</td>
<td id="A4.T7.2.5.3.3" class="ltx_td ltx_align_center">20.0</td>
</tr>
<tr id="A4.T7.2.6.4" class="ltx_tr">
<th id="A4.T7.2.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">0.5</th>
<td id="A4.T7.2.6.4.2" class="ltx_td ltx_align_center">96.5</td>
<td id="A4.T7.2.6.4.3" class="ltx_td ltx_align_center">19.9</td>
</tr>
<tr id="A4.T7.2.7.5" class="ltx_tr">
<th id="A4.T7.2.7.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">0.6</th>
<td id="A4.T7.2.7.5.2" class="ltx_td ltx_align_center">84.0</td>
<td id="A4.T7.2.7.5.3" class="ltx_td ltx_align_center">19.7</td>
</tr>
<tr id="A4.T7.2.8.6" class="ltx_tr">
<th id="A4.T7.2.8.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">0.7</th>
<td id="A4.T7.2.8.6.2" class="ltx_td ltx_align_center">57.8</td>
<td id="A4.T7.2.8.6.3" class="ltx_td ltx_align_center">18.1</td>
</tr>
<tr id="A4.T7.2.9.7" class="ltx_tr">
<th id="A4.T7.2.9.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="A4.T7.2.9.7.1.1" class="ltx_text ltx_font_bold">0.8</span></th>
<td id="A4.T7.2.9.7.2" class="ltx_td ltx_align_center"><span id="A4.T7.2.9.7.2.1" class="ltx_text ltx_font_bold">36.0</span></td>
<td id="A4.T7.2.9.7.3" class="ltx_td ltx_align_center"><span id="A4.T7.2.9.7.3.1" class="ltx_text ltx_font_bold">15.7</span></td>
</tr>
<tr id="A4.T7.2.10.8" class="ltx_tr">
<th id="A4.T7.2.10.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">0.9</th>
<td id="A4.T7.2.10.8.2" class="ltx_td ltx_align_center">11.5</td>
<td id="A4.T7.2.10.8.3" class="ltx_td ltx_align_center">8.7</td>
</tr>
<tr id="A4.T7.2.11.9" class="ltx_tr">
<th id="A4.T7.2.11.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="7"><span id="A4.T7.2.11.9.1.1" class="ltx_text">Grounding-DINO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite></span></th>
<th id="A4.T7.2.11.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">-</th>
<td id="A4.T7.2.11.9.3" class="ltx_td ltx_align_center ltx_border_t">100.0</td>
<td id="A4.T7.2.11.9.4" class="ltx_td ltx_align_center ltx_border_t">20.7</td>
</tr>
<tr id="A4.T7.2.12.10" class="ltx_tr">
<th id="A4.T7.2.12.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">0.4</th>
<td id="A4.T7.2.12.10.2" class="ltx_td ltx_align_center">80.8</td>
<td id="A4.T7.2.12.10.3" class="ltx_td ltx_align_center">20.2</td>
</tr>
<tr id="A4.T7.2.13.11" class="ltx_tr">
<th id="A4.T7.2.13.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">0.5</th>
<td id="A4.T7.2.13.11.2" class="ltx_td ltx_align_center">60.6</td>
<td id="A4.T7.2.13.11.3" class="ltx_td ltx_align_center">18.4</td>
</tr>
<tr id="A4.T7.2.14.12" class="ltx_tr">
<th id="A4.T7.2.14.12.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">0.6</th>
<td id="A4.T7.2.14.12.2" class="ltx_td ltx_align_center">45.2</td>
<td id="A4.T7.2.14.12.3" class="ltx_td ltx_align_center">16.2</td>
</tr>
<tr id="A4.T7.2.15.13" class="ltx_tr">
<th id="A4.T7.2.15.13.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="A4.T7.2.15.13.1.1" class="ltx_text ltx_font_bold">0.7</span></th>
<td id="A4.T7.2.15.13.2" class="ltx_td ltx_align_center"><span id="A4.T7.2.15.13.2.1" class="ltx_text ltx_font_bold">34.6</span></td>
<td id="A4.T7.2.15.13.3" class="ltx_td ltx_align_center"><span id="A4.T7.2.15.13.3.1" class="ltx_text ltx_font_bold">13.6</span></td>
</tr>
<tr id="A4.T7.2.16.14" class="ltx_tr">
<th id="A4.T7.2.16.14.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">0.8</th>
<td id="A4.T7.2.16.14.2" class="ltx_td ltx_align_center">23.3</td>
<td id="A4.T7.2.16.14.3" class="ltx_td ltx_align_center">9.5</td>
</tr>
<tr id="A4.T7.2.17.15" class="ltx_tr">
<th id="A4.T7.2.17.15.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">0.9</th>
<td id="A4.T7.2.17.15.2" class="ltx_td ltx_align_center">8.5</td>
<td id="A4.T7.2.17.15.3" class="ltx_td ltx_align_center">3.8</td>
</tr>
<tr id="A4.T7.2.18.16" class="ltx_tr">
<th id="A4.T7.2.18.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">OFA-DOD</th>
<th id="A4.T7.2.18.16.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">-</th>
<td id="A4.T7.2.18.16.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="A4.T7.2.18.16.3.1" class="ltx_text ltx_font_bold">35.6</span></td>
<td id="A4.T7.2.18.16.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="A4.T7.2.18.16.4.1" class="ltx_text ltx_font_bold">21.6</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="A4.F12" class="ltx_figure"><img src="/html/2307.12813/assets/x21.png" id="A4.F12.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="185" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A4.F12.3.1.1" class="ltx_text" style="font-size:90%;">Figure 12</span>: </span><span id="A4.F12.4.2" class="ltx_text" style="font-size:90%;">Visualization of detection results from different models on negative images for some descriptions. There is no GT instance on these images for the descriptions.
From left to right: GT, predictions from OVD, REC, bi-functional, and DOD methods.
Best viewed in color and zoomed in.
</span></figcaption>
</figure>
<section id="A4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.1 </span>Additional evaluation results for DOD</h3>

<div id="A4.SS1.p1" class="ltx_para ltx_noindent">
<p id="A4.SS1.p1.1" class="ltx_p"><span id="A4.SS1.p1.1.1" class="ltx_text ltx_font_bold">More comparison between baselines.</span>
In <a href="#A4.T6" title="In Appendix D More experimental results ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">6</span></a> we show a more complete comparison of the evaluated baselines on D<sup id="A4.SS1.p1.1.2" class="ltx_sup">3</sup> with different metrics. Results on average recalls are added.
In REC datasets like RefCOCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, the standard metric is accuracy (which equals to precision and also recall in REC setting). This is not suitable for DOD, which is essentially a detection task. Here we also report the average recall metric in COCO API, but it does not necessarily correspond to the effectiveness of a method for DOD, which requires rejecting negative instances while REC does not.</p>
</div>
<div id="A4.SS1.p2" class="ltx_para">
<p id="A4.SS1.p2.1" class="ltx_p">As shown in <a href="#A4.T6" title="In Appendix D More experimental results ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">6</span></a>, REC methods are bad at recall, possibly because it can only predict one instance for one description, no matter how many instances actually exists in GT.
OVD methods are also bad at this metric though they produce a dozen of output (see <a href="#A4.F13" title="In D.1 Additional evaluation results for DOD ‣ Appendix D More experimental results ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figs.</span> <span class="ltx_text ltx_ref_tag">13</span></a> and <a href="#A4.F12" title="Figure 12 ‣ Appendix D More experimental results ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>. This may partially explains its low mAP.
The bi-functional methods and the DOD one are all strong on this metric.
Grounding-DINO, though performs not as good as the proposed OFA-DOD in terms of mAPs, obtains the best recall. This indicates that it tends to produce more detection results.</p>
</div>
<div id="A4.SS1.p3" class="ltx_para ltx_noindent">
<p id="A4.SS1.p3.1" class="ltx_p"><span id="A4.SS1.p3.1.1" class="ltx_text ltx_font_bold">Inference of bi-functional methods.</span>
As discussed in Section 5.1 of the main paper, bi-functional methods obtain a 100% No-instance FPPC and fail to reject negative images on D<sup id="A4.SS1.p3.1.2" class="ltx_sup">3</sup>. This is due to the inference strategy based on REC. It is possible to apply other inference strategy for them.</p>
</div>
<div id="A4.SS1.p4" class="ltx_para">
<p id="A4.SS1.p4.1" class="ltx_p">We verify the effect of inference strategy on these two bi-functional methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, with No-instance FPPC and overall <span id="A4.SS1.p4.1.1" class="ltx_text ltx_font_slanted">FULL</span> mAP, and make comparison with the proposed baseline. As shown in <a href="#A4.T7" title="In Appendix D More experimental results ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">7</span></a>, we try to apply a threshold to filter out certain low-score predictions, similar to the post-processing steps in OVD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>.
With this inference strategy, we observe that the increase of score threshold does lower the No-instance FPPC significantly, but at the cost of overall mAP. Therefore, we apply the REC-based inference strategy for these bi-functional methods by default.</p>
</div>
<div id="A4.SS1.p5" class="ltx_para">
<p id="A4.SS1.p5.1" class="ltx_p">Furthermore, we find that when the score threshold is quite high (0.7 for Grounding-DINO and 0.8 for UNINEXT), they reach a FPPC similar to the proposed baseline but with much less overall mAP (15.7 mAP for UNINEXT and 13.6 mAP for Grounding-DINO, while ours 21.6 mAP). Therefore, it might be fair to say that the proposed baseline achieves a better balance between the ability to reject negative images and the overall detection capability.</p>
</div>
<figure id="A4.F13" class="ltx_figure"><img src="/html/2307.12813/assets/x22.png" id="A4.F13.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="377" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A4.F13.3.1.1" class="ltx_text" style="font-size:90%;">Figure 13</span>: </span><span id="A4.F13.4.2" class="ltx_text" style="font-size:90%;">Visualization of detection results from different models on absence descriptions and their contradictory presence descriptions.
The key words in absence descriptions are highlighted in red.
From left to right: GT, predictions from OVD, REC, bi-functional, and DOD methods.
Best viewed in color and zoomed in.
</span></figcaption>
</figure>
</section>
<section id="A4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.2 </span>Visual comparisons</h3>

<div id="A4.SS2.p1" class="ltx_para ltx_noindent">
<p id="A4.SS2.p1.1" class="ltx_p"><span id="A4.SS2.p1.1.1" class="ltx_text ltx_font_bold">Rejecting negative samples.</span>
As shown in <a href="#A4.F12" title="In Appendix D More experimental results ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">12</span></a>, we visualized two descriptions and two images with no corresponding GT instance. An ideal DOD method should refrain from predicting instances.
OWL-ViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, the OVD method, predicts multiple instances on these images, some of which overlap with each other. Such redundant predictions are not suitable for this setting.
OFA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, the REC method, always predicts an instance for one reference, making it highly prone to mistakes in such negative images.
Grounding-DINO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, the bi-functional method, correctly locates the <span id="A4.SS2.p1.1.2" class="ltx_text ltx_font_typewriter">hot air balloon</span> and <span id="A4.SS2.p1.1.3" class="ltx_text ltx_font_typewriter">dog</span> but fails to capture features related to <span id="A4.SS2.p1.1.4" class="ltx_text ltx_font_typewriter">with words</span> and <span id="A4.SS2.p1.1.5" class="ltx_text ltx_font_typewriter">clothed</span> in the language description.
In the last row, the proposed baseline for DOD successfully rejects one negative image but fails with the other one. This implies that it may perform better on such challenges compared to previous methods, but is still far from being strong.</p>
</div>
<div id="A4.SS2.p2" class="ltx_para ltx_noindent">
<p id="A4.SS2.p2.1" class="ltx_p"><span id="A4.SS2.p2.1.1" class="ltx_text ltx_font_bold">Absence or presence descriptions.</span>
In <a href="#A4.F13" title="In D.1 Additional evaluation results for DOD ‣ Appendix D More experimental results ‣ Described Object Detection: Liberating Object Detection with Flexible Expressions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">13</span></a>, we present the detection results for two pairs of descriptions, each with one absence description and its exact counterpart presence description. We visualize the GT (Ground Truth) and also predictions from 4 representative methods.</p>
</div>
<div id="A4.SS2.p3" class="ltx_para">
<p id="A4.SS2.p3.1" class="ltx_p">In the first pair, <span id="A4.SS2.p3.1.1" class="ltx_text ltx_font_typewriter">a butterfly that <span id="A4.SS2.p3.1.1.1" class="ltx_text" style="color:#FF0000;">doesn’t</span> stop on flowers</span>, the GT exists for the absence description, but not for the corresponding presence counterpart. We observe that previous methods are not sensitive to the distinction between presence and absence, leading to similar results for both descriptions. However, the proposed baseline stands as an exception by correctly predicting the bounding box for the absence description and successfully rejecting the presence one. This could be attributed to the language comprehension ability of OFA, as it is trained on multiple text-related tasks.</p>
</div>
<div id="A4.SS2.p4" class="ltx_para">
<p id="A4.SS2.p4.1" class="ltx_p">In the second pair, <span id="A4.SS2.p4.1.1" class="ltx_text ltx_font_typewriter">a person in santa claus clothes <span id="A4.SS2.p4.1.1.1" class="ltx_text" style="color:#FF0000;">without</span> bags</span>, most methods also yield similar results for both descriptions. Although OFA produces noticeably different bounding boxes for two descriptions, the one corresponding to the absence description is overly large, while the one for the presence description results in a negative prediction. Unfortunately, the proposed baseline incorrectly rejects the predictions for this case.</p>
</div>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2307.12812" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2307.12813" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2307.12813">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2307.12813" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2307.12814" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 16:31:43 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
