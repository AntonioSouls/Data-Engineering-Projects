<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2301.05892] Object Detection performance variation on compressed satellite image datasets with iquaflow</title><meta property="og:description" content="Increasing the performance of predictive models on images has been in the focus of many research projects lately.However, studies about the resilience of these models when they are trained on image datasets that suffer…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Object Detection performance variation on compressed satellite image datasets with iquaflow">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Object Detection performance variation on compressed satellite image datasets with iquaflow">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2301.05892">

<!--Generated on Fri Mar  1 06:53:01 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Object Detection performance variation on compressed satellite image datasets with iquaflow
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Pau Gallés, Katalin Takats and Javier Marin 
<br class="ltx_break">Satellogic Inc. SATL-NASDAQ 
<br class="ltx_break">Barcelona
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">{pau.galles, katalin.takats, jmarin}@satellogic.com</span> 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Increasing the performance of predictive models on images has been in the focus of many research projects lately.However, studies about the resilience of these models when they are trained on image datasets that suffer modifications altering their original quality are less common, even though their implications are often encountered in the industry <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>,<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>,<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. A good example of that is with earth observation satellites that are capturing many images. The energy and time of connection to the earth of an orbiting satellite are limited and must be carefully used. An approach to mitigate that is to compress the images on board before downloading. The compression can be regulated depending on the intended usage of the image and the requirements of this application. We present a new software tool with the name <span id="id2.id1.1" class="ltx_text ltx_font_smallcaps">iquaflow</span> that is designed to study image quality and model performance variation given an alteration of the image dataset. Furthermore, we do a showcase study about oriented object detection models adoption on a public image dataset DOTA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> given different compression levels. The optimal compression point is found and the usefulness of <span id="id2.id1.2" class="ltx_text ltx_font_smallcaps">iquaflow</span> becomes evident.</p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<p id="p1.8" class="ltx_p"><em id="p1.8.1" class="ltx_emph ltx_font_bold ltx_font_italic">K</em><span id="p1.8.2" class="ltx_text ltx_font_bold">eywords</span> vision  <math id="p1.1.m1.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.1.m1.1a"><mo id="p1.1.m1.1.1" xref="p1.1.m1.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.1.m1.1b"><ci id="p1.1.m1.1.1.cmml" xref="p1.1.m1.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.1.m1.1c">\cdot</annotation></semantics></math>
object detection  <math id="p1.2.m2.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.2.m2.1a"><mo id="p1.2.m2.1.1" xref="p1.2.m2.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.2.m2.1b"><ci id="p1.2.m2.1.1.cmml" xref="p1.2.m2.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.2.m2.1c">\cdot</annotation></semantics></math>
oriented bounding box  <math id="p1.3.m3.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.3.m3.1a"><mo id="p1.3.m3.1.1" xref="p1.3.m3.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.3.m3.1b"><ci id="p1.3.m3.1.1.cmml" xref="p1.3.m3.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.3.m3.1c">\cdot</annotation></semantics></math>
deep learning  <math id="p1.4.m4.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.4.m4.1a"><mo id="p1.4.m4.1.1" xref="p1.4.m4.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.4.m4.1b"><ci id="p1.4.m4.1.1.cmml" xref="p1.4.m4.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.4.m4.1c">\cdot</annotation></semantics></math>
compression  <math id="p1.5.m5.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.5.m5.1a"><mo id="p1.5.m5.1.1" xref="p1.5.m5.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.5.m5.1b"><ci id="p1.5.m5.1.1.cmml" xref="p1.5.m5.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.5.m5.1c">\cdot</annotation></semantics></math>
lossy compression  <math id="p1.6.m6.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.6.m6.1a"><mo id="p1.6.m6.1.1" xref="p1.6.m6.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.6.m6.1b"><ci id="p1.6.m6.1.1.cmml" xref="p1.6.m6.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.6.m6.1c">\cdot</annotation></semantics></math>
onboard compression  <math id="p1.7.m7.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.7.m7.1a"><mo id="p1.7.m7.1.1" xref="p1.7.m7.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.7.m7.1b"><ci id="p1.7.m7.1.1.cmml" xref="p1.7.m7.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.7.m7.1c">\cdot</annotation></semantics></math>
earth observation  <math id="p1.8.m8.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.8.m8.1a"><mo id="p1.8.m8.1.1" xref="p1.8.m8.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.8.m8.1b"><ci id="p1.8.m8.1.1.cmml" xref="p1.8.m8.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.8.m8.1c">\cdot</annotation></semantics></math>
image quality</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Predictive models that use images as inputs are constrained to any image alteration that can degrade the optimal performance of these models. Sometimes the degree of modification on the images can be regulated. A good example is when images are compressed before being sent to the algorithm for prediction. In case of earth observation satellites, the high cost of downloading the images can be significantly reduced by compressing the images first. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. One approach is to make images smaller to reduce the costs of downloading to earth <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. In this context, decision-makers need tools to study the optimal modification so that the performance of the predictive models is adequate despite the compression. <span id="S1.p1.1.1" class="ltx_text ltx_font_smallcaps">iquaflow<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span id="footnote1.1.1.1" class="ltx_text ltx_font_upright">1</span></span><a target="_blank" href="https://github.com/satellogic/iquaflow" title="" class="ltx_ref ltx_href ltx_font_upright">https://github.com/satellogic/iquaflow</a></span></span></span></span> is a software tool that has been designed precisely to study image quality as well as the performance of models trained on top of provided datasets that are modified with any user-defined alteration. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> studies object detection inference with compression algorithms based on decimation and scaling with interpolation in the context of earth observation from satellite applications. In the present work, the study is brought further with custom training for each level of compression, new kinds of compression, and new models of object detection that are suitable for oriented annotations as explained below.</p>
</div>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Compression</h3>

<div id="S1.SS1.p1" class="ltx_para ltx_noindent">
<p id="S1.SS1.p1.1" class="ltx_p">Compression algorithms can be lossless or lossy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. The first kind performs an operation on the image that allows the recovery of the original image before it was compressed. The second kind, on the other hand, does an irreversible operation. Using a lossy compression algorithm, we can achieve a greater reduction in file sizes than with a lossless one. A simple straightforward technique for lossy compression can be the interpolation of an image to fewer pixels. Then a smaller image will have lost information and it will also be smaller in file size. In this study the JPEG compression is used as explained in section <a href="#S2.SS2" title="2.2 Compression ‣ 2 Materials and Methods ‣ Object Detection performance variation on compressed satellite image datasets with iquaflow" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>.</p>
</div>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2 </span>Object detection</h3>

<div id="S1.SS2.p1" class="ltx_para ltx_noindent">
<p id="S1.SS2.p1.1" class="ltx_p">A good example of predictive models on images is object detection (such as vehicles from aerial images). Most detectors such as Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, SSD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and YOLOv2, v3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> rely on a set of pre-defined anchors that consist in a small set of bounding boxes summarizing the most relevant geometric shapes covering relevant scale, aspect ratios and elongation directions. The idea is that any object can be associated with a specific anchor box without having to have a perfect fit.</p>
</div>
<div id="S1.SS2.p2" class="ltx_para ltx_noindent">
<p id="S1.SS2.p2.1" class="ltx_p">However, the definition of this set of anchor boxes is a hyper-parameter that must be defined and has an effect on the detection performance. The models are, of course, sensitive to the sizes, aspect ratios, and a number of anchors defined in the set (see <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>).</p>
</div>
<div id="S1.SS2.p3" class="ltx_para ltx_noindent">
<p id="S1.SS2.p3.1" class="ltx_p">Another aspect to consider is the number of stages. Detectors can be composed of multiple stages and each of them has a trained model that solves a specific task in the workflow. A typical case in an object detection problem is the Region Proposal Network which is responsible for the task of generating bounding box proposals. Examples of that are <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. One advantage of the multistage approach is that each step in the workflow can be easily defined and understood by human logic. In single-stage detectors, the logic can be difficult to interpret inside an end-to-end network solution.</p>
</div>
<div id="S1.SS2.p4" class="ltx_para ltx_noindent">
<p id="S1.SS2.p4.1" class="ltx_p">Depending on the annotations one can use a model that predicts with horizontal bounding boxes (HBB) or oriented bounding boxes (OBB). One problem with HBB is distinguishing between overlapping instances of different objects. This is usually approached with the logic of Non-Maximum Suppression (NMS) that involves the measure of Intersection Over Union between different instances to asses the overlapping and whether or not candidate boxes belong to the same sample. This logic struggles when there are elongated objects that are diagonal and parallel to each other. In aerial images, these can be ships in a harbor or trucks in parking. One solution for this is to consider more complex geometries that have a better fit with the object. The simplest complexity, in this case, is to orient the bounding box.</p>
</div>
<div id="S1.SS2.p5" class="ltx_para ltx_noindent">
<p id="S1.SS2.p5.1" class="ltx_p">The models used in this study are explained in section <a href="#S2.SS3" title="2.3 Object detection ‣ 2 Materials and Methods ‣ Object Detection performance variation on compressed satellite image datasets with iquaflow" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>.</p>
</div>
</section>
<section id="S1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.3 </span>Iquaflow</h3>

<div id="S1.SS3.p1" class="ltx_para ltx_noindent">
<p id="S1.SS3.p1.1" class="ltx_p">Image quality can be often evaluated by the human eye. However, it is very challenging to define a numerical measurement for image quality. One of the reasons is that there are many aspects to consider such as the blur, the noise, the quality distribution along frequencies, etc. Moreover, image quality should be measured according to the particular application of the images being measured. Supervised super-resolution image prediction models are algorithms that translate an input image to a higher-resolution image that contains more pixels. These models are trained with a database containing pair samples of images with their respective higher resolution (also known as ground-truth or target images). In this context, the evaluation of quality will perform better by comparing the predicted image against the target image. These metrics are also known as similarity metrics and they include <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. Another context is when images are used as inputs for other predictive models with the aim to collect information from them. It is the case of an image classifier or object detection. For this case, a suitable image quality evaluation method can be the performance of this model on the images. This is assuming that changes in the input image quality are affecting the performance of the prediction model. Again, this is a way to measure image quality that is adapted to the actual application of the image.</p>
</div>
<div id="S1.SS3.p2" class="ltx_para ltx_noindent">
<p id="S1.SS3.p2.1" class="ltx_p"><span id="S1.SS3.p2.1.1" class="ltx_text ltx_font_smallcaps">iquaflow<cite class="ltx_cite ltx_citemacro_cite"><span id="S1.SS3.p2.1.1.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib18" title="" class="ltx_ref">18</a><span id="S1.SS3.p2.1.1.2.2" class="ltx_text ltx_font_upright">]</span></cite></span> is a python package tool that measures image quality by using different approaches. Deterministic metrics include blind metrics which are measured directly on the image without comparing against a reference image or similarity metrics when they are measuring affinity against an ideal case. There are two metrics that have been designed for <span id="S1.SS3.p2.1.2" class="ltx_text ltx_font_smallcaps">iquaflow</span> which are implicit measurements of blur and noise levels. The first relies on edges found within the images and it measures the slope in the Relative Edge Response (RER) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. Then the second is based on homogeneous areas where the noise can be estimated (Signal to Noise ratio - SNR) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. The Quality Metric Regression Network (QMRNet)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> has been designed, trained, and integrated into <span id="S1.SS3.p2.1.3" class="ltx_text ltx_font_smallcaps">iquaflow</span>. This is a classifier with small intervals that can predict quality parameters on images such as blur (measured as equivalent sigma from a gaussian bell), sharpness, pixel size and noise. Quality can also be measured by checking how predictive models trained on the image dataset are performing. A good example is the present study where object detection is trained on different quality datasets with different outcomes.</p>
</div>
<div id="S1.SS3.p3" class="ltx_para ltx_noindent">
<p id="S1.SS3.p3.1" class="ltx_p">Apart from measuring image quality, <span id="S1.SS3.p3.1.1" class="ltx_text ltx_font_smallcaps">iquaflow</span> has a whole ecosystem that facilitates the design of new studies and experiment sets made of several training runs with variations. <span id="S1.SS3.p3.1.2" class="ltx_text ltx_font_smallcaps">iquaflow</span> wraps another open source tool named <a target="_blank" href="https://mlflow.org/" title="" class="ltx_ref ltx_href">Mlflow</a> that is used for machine learning experiment tracking. It will record the executions in a standard format so that they are later easily visualized and compared from <a target="_blank" href="https://mlflow.org/" title="" class="ltx_ref ltx_href">Mlflow</a> user interface tool in the browser. In <span id="S1.SS3.p3.1.3" class="ltx_text ltx_font_smallcaps">iquaflow</span> the user can add custom metrics and dataset modifiers that are easily integrated into a use case study.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Materials and Methods</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">The aim of the study is to measure the variation of the object detection algorithm’s performance on a given image dataset that is modified with various compression ratios. Our goal is to evaluate what is the maximum compression level that still allows for acceptable model performance. In this section, the compression algorithm is described, and the object detection model(s) that we considered, as well as the tool used for managing our experiments.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Data</h3>

<div id="S2.SS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.p1.3" class="ltx_p">Two different datasets are used to carry out two experiments. The first analysis is based on the airplanes dataset<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Contact iquaflow@satellogic.com to request access to the dataset</span></span></span> which consists of 998 images of 1024 × 1024 pixels from airport areas with a total of almost <math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="17000" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><mn id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">17000</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><cn type="integer" id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">17000</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">17000</annotation></semantics></math> annotated planes. These captures were made using NewSat Satellogic constellation (<math id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="1~{}\mathrm{m}" display="inline"><semantics id="S2.SS1.p1.2.m2.1a"><mrow id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml"><mn id="S2.SS1.p1.2.m2.1.1.2" xref="S2.SS1.p1.2.m2.1.1.2.cmml">1</mn><mo lspace="0.330em" rspace="0em" id="S2.SS1.p1.2.m2.1.1.1" xref="S2.SS1.p1.2.m2.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S2.SS1.p1.2.m2.1.1.3" xref="S2.SS1.p1.2.m2.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><apply id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1"><times id="S2.SS1.p1.2.m2.1.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1.1"></times><cn type="integer" id="S2.SS1.p1.2.m2.1.1.2.cmml" xref="S2.SS1.p1.2.m2.1.1.2">1</cn><ci id="S2.SS1.p1.2.m2.1.1.3.cmml" xref="S2.SS1.p1.2.m2.1.1.3">m</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">1~{}\mathrm{m}</annotation></semantics></math> GSD) and the annotations were made using Happyrobot<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://happyrobot.ai" title="" class="ltx_ref ltx_href">https://happyrobot.ai</a></span></span></span> platform. The training partition contained <math id="S2.SS1.p1.3.m3.1" class="ltx_Math" alttext="13731" display="inline"><semantics id="S2.SS1.p1.3.m3.1a"><mn id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml">13731</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><cn type="integer" id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1">13731</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">13731</annotation></semantics></math> annotations and the remaining were used for evaluation.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS1.p2.13" class="ltx_p">The second experiment was based on the public dataset DOTA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. It is a dataset for object detection in aerial images. The images are collected from different sensors the image sizes are ranging from <math id="S2.SS1.p2.1.m1.1" class="ltx_Math" alttext="800~{}\times~{}800" display="inline"><semantics id="S2.SS1.p2.1.m1.1a"><mrow id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml"><mn id="S2.SS1.p2.1.m1.1.1.2" xref="S2.SS1.p2.1.m1.1.1.2.cmml">800</mn><mo lspace="0.552em" rspace="0.552em" id="S2.SS1.p2.1.m1.1.1.1" xref="S2.SS1.p2.1.m1.1.1.1.cmml">×</mo><mn id="S2.SS1.p2.1.m1.1.1.3" xref="S2.SS1.p2.1.m1.1.1.3.cmml">800</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><apply id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1"><times id="S2.SS1.p2.1.m1.1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1.1"></times><cn type="integer" id="S2.SS1.p2.1.m1.1.1.2.cmml" xref="S2.SS1.p2.1.m1.1.1.2">800</cn><cn type="integer" id="S2.SS1.p2.1.m1.1.1.3.cmml" xref="S2.SS1.p2.1.m1.1.1.3">800</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">800~{}\times~{}800</annotation></semantics></math> to <math id="S2.SS1.p2.2.m2.1" class="ltx_Math" alttext="20000~{}\times~{}20000" display="inline"><semantics id="S2.SS1.p2.2.m2.1a"><mrow id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml"><mn id="S2.SS1.p2.2.m2.1.1.2" xref="S2.SS1.p2.2.m2.1.1.2.cmml">20000</mn><mo lspace="0.552em" rspace="0.552em" id="S2.SS1.p2.2.m2.1.1.1" xref="S2.SS1.p2.2.m2.1.1.1.cmml">×</mo><mn id="S2.SS1.p2.2.m2.1.1.3" xref="S2.SS1.p2.2.m2.1.1.3.cmml">20000</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.1b"><apply id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1"><times id="S2.SS1.p2.2.m2.1.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1.1"></times><cn type="integer" id="S2.SS1.p2.2.m2.1.1.2.cmml" xref="S2.SS1.p2.2.m2.1.1.2">20000</cn><cn type="integer" id="S2.SS1.p2.2.m2.1.1.3.cmml" xref="S2.SS1.p2.2.m2.1.1.3">20000</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.1c">20000~{}\times~{}20000</annotation></semantics></math> pixels and the pixel size varies from <math id="S2.SS1.p2.3.m3.1" class="ltx_Math" alttext="0.3~{}\mathrm{m}" display="inline"><semantics id="S2.SS1.p2.3.m3.1a"><mrow id="S2.SS1.p2.3.m3.1.1" xref="S2.SS1.p2.3.m3.1.1.cmml"><mn id="S2.SS1.p2.3.m3.1.1.2" xref="S2.SS1.p2.3.m3.1.1.2.cmml">0.3</mn><mo lspace="0.330em" rspace="0em" id="S2.SS1.p2.3.m3.1.1.1" xref="S2.SS1.p2.3.m3.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S2.SS1.p2.3.m3.1.1.3" xref="S2.SS1.p2.3.m3.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.3.m3.1b"><apply id="S2.SS1.p2.3.m3.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1"><times id="S2.SS1.p2.3.m3.1.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1.1"></times><cn type="float" id="S2.SS1.p2.3.m3.1.1.2.cmml" xref="S2.SS1.p2.3.m3.1.1.2">0.3</cn><ci id="S2.SS1.p2.3.m3.1.1.3.cmml" xref="S2.SS1.p2.3.m3.1.1.3">m</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.3.m3.1c">0.3~{}\mathrm{m}</annotation></semantics></math> to <math id="S2.SS1.p2.4.m4.1" class="ltx_Math" alttext="2~{}\mathrm{m}" display="inline"><semantics id="S2.SS1.p2.4.m4.1a"><mrow id="S2.SS1.p2.4.m4.1.1" xref="S2.SS1.p2.4.m4.1.1.cmml"><mn id="S2.SS1.p2.4.m4.1.1.2" xref="S2.SS1.p2.4.m4.1.1.2.cmml">2</mn><mo lspace="0.330em" rspace="0em" id="S2.SS1.p2.4.m4.1.1.1" xref="S2.SS1.p2.4.m4.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S2.SS1.p2.4.m4.1.1.3" xref="S2.SS1.p2.4.m4.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.4.m4.1b"><apply id="S2.SS1.p2.4.m4.1.1.cmml" xref="S2.SS1.p2.4.m4.1.1"><times id="S2.SS1.p2.4.m4.1.1.1.cmml" xref="S2.SS1.p2.4.m4.1.1.1"></times><cn type="integer" id="S2.SS1.p2.4.m4.1.1.2.cmml" xref="S2.SS1.p2.4.m4.1.1.2">2</cn><ci id="S2.SS1.p2.4.m4.1.1.3.cmml" xref="S2.SS1.p2.4.m4.1.1.3">m</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.4.m4.1c">2~{}\mathrm{m}</annotation></semantics></math> resolution. DOTA has several versions and DOTA-v1.0 has been used in the present study which contains 15 common categories <span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>plane, ship, storage tank, baseball diamond, tennis court, basketball court, ground track field, harbor, bridge, large vehicle, small vehicle, helicopter, roundabout, soccer ball field and swimming pool.</span></span></span>, <math id="S2.SS1.p2.5.m5.1" class="ltx_Math" alttext="2806" display="inline"><semantics id="S2.SS1.p2.5.m5.1a"><mn id="S2.SS1.p2.5.m5.1.1" xref="S2.SS1.p2.5.m5.1.1.cmml">2806</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.5.m5.1b"><cn type="integer" id="S2.SS1.p2.5.m5.1.1.cmml" xref="S2.SS1.p2.5.m5.1.1">2806</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.5.m5.1c">2806</annotation></semantics></math> images and more than <math id="S2.SS1.p2.6.m6.1" class="ltx_Math" alttext="188k" display="inline"><semantics id="S2.SS1.p2.6.m6.1a"><mrow id="S2.SS1.p2.6.m6.1.1" xref="S2.SS1.p2.6.m6.1.1.cmml"><mn id="S2.SS1.p2.6.m6.1.1.2" xref="S2.SS1.p2.6.m6.1.1.2.cmml">188</mn><mo lspace="0em" rspace="0em" id="S2.SS1.p2.6.m6.1.1.1" xref="S2.SS1.p2.6.m6.1.1.1.cmml">​</mo><mi id="S2.SS1.p2.6.m6.1.1.3" xref="S2.SS1.p2.6.m6.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.6.m6.1b"><apply id="S2.SS1.p2.6.m6.1.1.cmml" xref="S2.SS1.p2.6.m6.1.1"><times id="S2.SS1.p2.6.m6.1.1.1.cmml" xref="S2.SS1.p2.6.m6.1.1.1"></times><cn type="integer" id="S2.SS1.p2.6.m6.1.1.2.cmml" xref="S2.SS1.p2.6.m6.1.1.2">188</cn><ci id="S2.SS1.p2.6.m6.1.1.3.cmml" xref="S2.SS1.p2.6.m6.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.6.m6.1c">188k</annotation></semantics></math> object instances. The annotations are oriented bounding boxes which allows us to train both oriented (OBB) and horizontal bounding boxes (HBB) models. The proportions of the training set, validation set, and testing set in DOTA-v1.0 are <math id="S2.SS1.p2.7.m7.1" class="ltx_Math" alttext="1/2" display="inline"><semantics id="S2.SS1.p2.7.m7.1a"><mrow id="S2.SS1.p2.7.m7.1.1" xref="S2.SS1.p2.7.m7.1.1.cmml"><mn id="S2.SS1.p2.7.m7.1.1.2" xref="S2.SS1.p2.7.m7.1.1.2.cmml">1</mn><mo id="S2.SS1.p2.7.m7.1.1.1" xref="S2.SS1.p2.7.m7.1.1.1.cmml">/</mo><mn id="S2.SS1.p2.7.m7.1.1.3" xref="S2.SS1.p2.7.m7.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.7.m7.1b"><apply id="S2.SS1.p2.7.m7.1.1.cmml" xref="S2.SS1.p2.7.m7.1.1"><divide id="S2.SS1.p2.7.m7.1.1.1.cmml" xref="S2.SS1.p2.7.m7.1.1.1"></divide><cn type="integer" id="S2.SS1.p2.7.m7.1.1.2.cmml" xref="S2.SS1.p2.7.m7.1.1.2">1</cn><cn type="integer" id="S2.SS1.p2.7.m7.1.1.3.cmml" xref="S2.SS1.p2.7.m7.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.7.m7.1c">1/2</annotation></semantics></math>, <math id="S2.SS1.p2.8.m8.1" class="ltx_Math" alttext="1/6" display="inline"><semantics id="S2.SS1.p2.8.m8.1a"><mrow id="S2.SS1.p2.8.m8.1.1" xref="S2.SS1.p2.8.m8.1.1.cmml"><mn id="S2.SS1.p2.8.m8.1.1.2" xref="S2.SS1.p2.8.m8.1.1.2.cmml">1</mn><mo id="S2.SS1.p2.8.m8.1.1.1" xref="S2.SS1.p2.8.m8.1.1.1.cmml">/</mo><mn id="S2.SS1.p2.8.m8.1.1.3" xref="S2.SS1.p2.8.m8.1.1.3.cmml">6</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.8.m8.1b"><apply id="S2.SS1.p2.8.m8.1.1.cmml" xref="S2.SS1.p2.8.m8.1.1"><divide id="S2.SS1.p2.8.m8.1.1.1.cmml" xref="S2.SS1.p2.8.m8.1.1.1"></divide><cn type="integer" id="S2.SS1.p2.8.m8.1.1.2.cmml" xref="S2.SS1.p2.8.m8.1.1.2">1</cn><cn type="integer" id="S2.SS1.p2.8.m8.1.1.3.cmml" xref="S2.SS1.p2.8.m8.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.8.m8.1c">1/6</annotation></semantics></math>, and <math id="S2.SS1.p2.9.m9.1" class="ltx_Math" alttext="1/3" display="inline"><semantics id="S2.SS1.p2.9.m9.1a"><mrow id="S2.SS1.p2.9.m9.1.1" xref="S2.SS1.p2.9.m9.1.1.cmml"><mn id="S2.SS1.p2.9.m9.1.1.2" xref="S2.SS1.p2.9.m9.1.1.2.cmml">1</mn><mo id="S2.SS1.p2.9.m9.1.1.1" xref="S2.SS1.p2.9.m9.1.1.1.cmml">/</mo><mn id="S2.SS1.p2.9.m9.1.1.3" xref="S2.SS1.p2.9.m9.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.9.m9.1b"><apply id="S2.SS1.p2.9.m9.1.1.cmml" xref="S2.SS1.p2.9.m9.1.1"><divide id="S2.SS1.p2.9.m9.1.1.1.cmml" xref="S2.SS1.p2.9.m9.1.1.1"></divide><cn type="integer" id="S2.SS1.p2.9.m9.1.1.2.cmml" xref="S2.SS1.p2.9.m9.1.1.2">1</cn><cn type="integer" id="S2.SS1.p2.9.m9.1.1.3.cmml" xref="S2.SS1.p2.9.m9.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.9.m9.1c">1/3</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. A disadvantage of this dataset is that the test set is not openly available, rather it is in a form of a remote service to query the predictions. This does not allow to alter the test on the same way the other partitions are modified in the present study. Because of that, 2 partitions are made from the validation set: half of is is used as actual validation and the other half for testing. Then the images are cropped to <math id="S2.SS1.p2.10.m10.1" class="ltx_Math" alttext="1024~{}\times~{}1024" display="inline"><semantics id="S2.SS1.p2.10.m10.1a"><mrow id="S2.SS1.p2.10.m10.1.1" xref="S2.SS1.p2.10.m10.1.1.cmml"><mn id="S2.SS1.p2.10.m10.1.1.2" xref="S2.SS1.p2.10.m10.1.1.2.cmml">1024</mn><mo lspace="0.552em" rspace="0.552em" id="S2.SS1.p2.10.m10.1.1.1" xref="S2.SS1.p2.10.m10.1.1.1.cmml">×</mo><mn id="S2.SS1.p2.10.m10.1.1.3" xref="S2.SS1.p2.10.m10.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.10.m10.1b"><apply id="S2.SS1.p2.10.m10.1.1.cmml" xref="S2.SS1.p2.10.m10.1.1"><times id="S2.SS1.p2.10.m10.1.1.1.cmml" xref="S2.SS1.p2.10.m10.1.1.1"></times><cn type="integer" id="S2.SS1.p2.10.m10.1.1.2.cmml" xref="S2.SS1.p2.10.m10.1.1.2">1024</cn><cn type="integer" id="S2.SS1.p2.10.m10.1.1.3.cmml" xref="S2.SS1.p2.10.m10.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.10.m10.1c">1024~{}\times~{}1024</annotation></semantics></math> with padding when necessary. After this operation the number of crops for the partitions train, validation and testing are respectively <math id="S2.SS1.p2.11.m11.1" class="ltx_Math" alttext="9734" display="inline"><semantics id="S2.SS1.p2.11.m11.1a"><mn id="S2.SS1.p2.11.m11.1.1" xref="S2.SS1.p2.11.m11.1.1.cmml">9734</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.11.m11.1b"><cn type="integer" id="S2.SS1.p2.11.m11.1.1.cmml" xref="S2.SS1.p2.11.m11.1.1">9734</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.11.m11.1c">9734</annotation></semantics></math>, <math id="S2.SS1.p2.12.m12.1" class="ltx_Math" alttext="2670" display="inline"><semantics id="S2.SS1.p2.12.m12.1a"><mn id="S2.SS1.p2.12.m12.1.1" xref="S2.SS1.p2.12.m12.1.1.cmml">2670</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.12.m12.1b"><cn type="integer" id="S2.SS1.p2.12.m12.1.1.cmml" xref="S2.SS1.p2.12.m12.1.1">2670</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.12.m12.1c">2670</annotation></semantics></math> and <math id="S2.SS1.p2.13.m13.1" class="ltx_Math" alttext="2627" display="inline"><semantics id="S2.SS1.p2.13.m13.1a"><mn id="S2.SS1.p2.13.m13.1.1" xref="S2.SS1.p2.13.m13.1.1.cmml">2627</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.13.m13.1b"><cn type="integer" id="S2.SS1.p2.13.m13.1.1.cmml" xref="S2.SS1.p2.13.m13.1.1">2627</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.13.m13.1c">2627</annotation></semantics></math>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Compression</h3>

<div id="S2.SS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.p1.2" class="ltx_p">In this study, JPEG compression <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> is used. It is a lossy form of compression based on the discrete cosine transform (DCT) that converts images into the frequency domain and discards high-frequency information by a quantization process. The degree of compression in JPEG can be adjusted: the greater the quality the bigger the file size. In the present study, the compression is set at different levels with the aim to find an optimal value with respect to the performance of predictive models trained on them. We used the JPEG compression from OpenCV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> that can be regulated with the parameter CV_IMWRITE_JPEG_QUALITY which can vary from <math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><mn id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><cn type="integer" id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">0</cn></annotation-xml></semantics></math> to <math id="S2.SS2.p1.2.m2.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S2.SS2.p1.2.m2.1a"><mn id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.1b"><cn type="integer" id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.1c">100</annotation></semantics></math> (the higher is the better) with a default value of 95.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.p2.1" class="ltx_p">Figure <a href="#S2.F1" title="Figure 1 ‣ 2.2 Compression ‣ 2 Materials and Methods ‣ Object Detection performance variation on compressed satellite image datasets with iquaflow" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows an example of the effect when compressing one of the images with JPEG method.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2301.05892/assets/Definitions/JPEG.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="412" height="129" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>JPEG compression effects (original, JPG10,and JPG5 from left to right). This image is from the airplane dataset from Satellogic.
</figcaption>
</figure>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Object detection</h3>

<div id="S2.SS3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS3.p1.1" class="ltx_p">The first experiment has HBB annotated objects and the model YOLOv5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> was used because of its fast training and implementation.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para ltx_noindent">
<p id="S2.SS3.p2.1" class="ltx_p">For the second experiment, two OBB models were used. The first was Oriented R-CNN which is a two-stage oriented detector that uses Region Proposal Network (oriented RPN) in the first stage in order to generate high-quality oriented proposals in a nearly cost-free manner <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para ltx_noindent">
<p id="S2.SS3.p3.1" class="ltx_p">Then the other model used was FCOS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> which is originally designed for horizontal bounding boxes but it can be adapted with an added convolution layer channel on the top of the regression features that define the direction of the bounding box. Intersection Over Union is often used as a loss function in object detection. However, the IoU calculation between oriented boxes is complex and often not differentiable. There are rotated IoU that implements differentiable IoU calculation for oriented bounding boxes. In this case, the PolyIoULoss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> between the OBB predictions and ground truths is used as a bounding box loss.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para ltx_noindent">
<p id="S2.SS3.p4.1" class="ltx_p">The performance of the detector is measured by calculating the average recall (AR) as well as the Mean Average Precision (mAP). AR is a ratio of correctly detected instances over the actual amount of objects. On the other hand, AP is defined with the same correctly detected instances over all the amount of detected cases (including wrong detection). The predicted bounding boxes do not have to have a perfect match with the ground truth. Because of that, the Intersection over Union (IoU) for each prediction and ground truth match candidate is measured to evaluate if they match. In which case it is considered a correct detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. In this study, mAP is calculated by taking the mean AP over all classes and over a range of IoU thresholds.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Experiment management</h3>

<div id="S2.SS4.p1" class="ltx_para ltx_noindent">
<p id="S2.SS4.p1.1" class="ltx_p">The present study involves a workflow with multiple versions of the original dataset with the corresponding partitions for each altered version (train, validation and test) as well as many training experiment executions and tracking of results that must be organized correctly. All this can be managed easily with a typical <span id="S2.SS4.p1.1.1" class="ltx_text ltx_font_smallcaps">iquaflow</span> workflow as follows:</p>
</div>
<div id="S2.SS4.p2" class="ltx_para ltx_noindent">
<ol id="S2.I1" class="ltx_enumerate">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">Optionally the user can start with a repository template of <span id="S2.I1.i1.p1.1.1" class="ltx_text ltx_font_smallcaps">iquaflow</span> use cases. This repository uses cookiecutter which is a python package tool for repository templates. By using this you can initialize a repository with the typical required files for a study in <span id="S2.I1.i1.p1.1.2" class="ltx_text ltx_font_smallcaps">iquaflow</span>.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p">The first step will be to set the modifications of the original dataset with different compression levels. This can be done with a list of Modifiers in <span id="S2.I1.i2.p1.1.1" class="ltx_text ltx_font_smallcaps">iquaflow</span>. There are some modifiers already available in <span id="S2.I1.i2.p1.1.2" class="ltx_text ltx_font_smallcaps">iquaflow</span> with performing specific alterations. However, one can set up a custom modifier by inheriting the DSModifier class of <span id="S2.I1.i2.p1.1.3" class="ltx_text ltx_font_smallcaps">iquaflow</span>. The list of modifiers will then be passed as an argument to the experiment setup.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p">Next step is to adapt the user training script to the <span id="S2.I1.i3.p1.1.1" class="ltx_text ltx_font_smallcaps">iquaflow</span> conventions. This is just to accept some input arguments such as the output path where the results are written. Optionally one can monitor in streaming the training by inputting additional arguments as explained in <span id="S2.I1.i3.p1.1.2" class="ltx_text ltx_font_smallcaps">iquaflow</span>’s guide.</p>
</div>
</li>
<li id="S2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S2.I1.i4.p1" class="ltx_para">
<p id="S2.I1.i4.p1.1" class="ltx_p">All previous definitions are introduced in the experimental setup that can be executed afterward. The whole experiment will contain all runs which are the result of combining dataset modifications (the diverse compression levels) and the two different detectors that are used which will be defined as hyperparameter variations in the experiment setup.</p>
</div>
</li>
<li id="S2.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S2.I1.i5.p1" class="ltx_para ltx_noindent">
<p id="S2.I1.i5.p1.1" class="ltx_p">The evaluation can be either done within the user’s custom training script or by using a Metric in <span id="S2.I1.i5.p1.1.1" class="ltx_text ltx_font_smallcaps">iquaflow</span>. Similar to Modifiers there are some specific Metrics already defined in <span id="S2.I1.i5.p1.1.2" class="ltx_text ltx_font_smallcaps">iquaflow</span>. Alternatively, the user can make a custom metric by inheriting the class Metric from <span id="S2.I1.i5.p1.1.3" class="ltx_text ltx_font_smallcaps">iquaflow</span>. The results can be collected from <span id="S2.I1.i5.p1.1.4" class="ltx_text ltx_font_smallcaps">iquaflow</span> or directly by raising an <a target="_blank" href="https://mlflow.org/" title="" class="ltx_ref ltx_href">mlflow</a> server which is a tool that is wrapped and used by <span id="S2.I1.i5.p1.1.5" class="ltx_text ltx_font_smallcaps">iquaflow</span>.</p>
</div>
</li>
</ol>
</div>
<div id="S2.SS4.p3" class="ltx_para ltx_noindent">
<p id="S2.SS4.p3.1" class="ltx_p">As you can see using <span id="S2.SS4.p3.1.1" class="ltx_text ltx_font_smallcaps">iquaflow</span> we can automate the compression algorithm on the data, run the user custom training script and evaluate a model. All the results are logged using mlflow and can be handily compared and visualized. <span id="S2.SS4.p3.1.2" class="ltx_text ltx_font_smallcaps">iquaflow</span> is the ideal tool for this purpose.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Results</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.4" class="ltx_p">The airplanes dataset from Satellogic<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://github.com/satellogic/iquaflow-airport-use-case" title="" class="ltx_ref ltx_href">https://github.com/satellogic/iquaflow-airport-use-case</a></span></span></span> has the unique category of planes. The image format is tiff and the original average image size is <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="3.204" display="inline"><semantics id="S3.p1.1.m1.1a"><mn id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">3.204</mn><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><cn type="float" id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">3.204</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">3.204</annotation></semantics></math>Megabytes. The average recall (AR) is measured and the Mean Average Precision (mAP) is calculated over different Intersection Over Union (IoU) thresholds varying from <math id="S3.p1.2.m2.1" class="ltx_Math" alttext="0.5" display="inline"><semantics id="S3.p1.2.m2.1a"><mn id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><cn type="float" id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">0.5</annotation></semantics></math> to <math id="S3.p1.3.m3.1" class="ltx_Math" alttext="0.95" display="inline"><semantics id="S3.p1.3.m3.1a"><mn id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml">0.95</mn><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><cn type="float" id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1">0.95</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">0.95</annotation></semantics></math> with a step of <math id="S3.p1.4.m4.1" class="ltx_Math" alttext="0.05" display="inline"><semantics id="S3.p1.4.m4.1a"><mn id="S3.p1.4.m4.1.1" xref="S3.p1.4.m4.1.1.cmml">0.05</mn><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.1b"><cn type="float" id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1">0.05</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.1c">0.05</annotation></semantics></math> and average again for the final score. Table <a href="#S3.T1" title="Table 1 ‣ 3 Results ‣ Object Detection performance variation on compressed satellite image datasets with iquaflow" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> contains the resultant metrics and Figure <a href="#S3.F2" title="Figure 2 ‣ 3 Results ‣ Object Detection performance variation on compressed satellite image datasets with iquaflow" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows performances (mAP) along different levels of compression.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Performance results at different compression levels using the airplanes dataset and two YOLOv5 model sizes with different architecture complexities. The scores for the different models are expressed as Mean Average Precision (mAP) and Average Recall (AR) as expressed in the methodology section. The last column shows the equivalent average image size from the dataset given the level of compression used.</figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S3.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">YOLOv5 NANO</span></th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S3.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">YOLOv5 SMALL</span></th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">size</th>
</tr>
<tr id="S3.T1.1.2.2" class="ltx_tr">
<th id="S3.T1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S3.T1.1.2.2.1.1" class="ltx_text ltx_font_bold">AR</span></th>
<th id="S3.T1.1.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S3.T1.1.2.2.2.1" class="ltx_text ltx_font_bold">mAP</span></th>
<th id="S3.T1.1.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S3.T1.1.2.2.3.1" class="ltx_text ltx_font_bold">AR</span></th>
<th id="S3.T1.1.2.2.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S3.T1.1.2.2.4.1" class="ltx_text ltx_font_bold">mAP</span></th>
<th id="S3.T1.1.2.2.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S3.T1.1.2.2.5.1" class="ltx_text ltx_font_bold">Mb</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.3.1" class="ltx_tr">
<td id="S3.T1.1.3.1.1" class="ltx_td ltx_align_left ltx_border_t">0.898</td>
<td id="S3.T1.1.3.1.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T1.1.3.1.2.1" class="ltx_text ltx_font_bold">0.669</span></td>
<td id="S3.T1.1.3.1.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T1.1.3.1.3.1" class="ltx_text ltx_font_bold">0.922</span></td>
<td id="S3.T1.1.3.1.4" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T1.1.3.1.4.1" class="ltx_text ltx_font_bold">0.714</span></td>
<td id="S3.T1.1.3.1.5" class="ltx_td ltx_align_left ltx_border_t">2.051</td>
</tr>
<tr id="S3.T1.1.4.2" class="ltx_tr">
<td id="S3.T1.1.4.2.1" class="ltx_td ltx_align_left"><span id="S3.T1.1.4.2.1.1" class="ltx_text ltx_font_bold">0.899</span></td>
<td id="S3.T1.1.4.2.2" class="ltx_td ltx_align_left">0.666</td>
<td id="S3.T1.1.4.2.3" class="ltx_td ltx_align_left">0.919</td>
<td id="S3.T1.1.4.2.4" class="ltx_td ltx_align_left">0.709</td>
<td id="S3.T1.1.4.2.5" class="ltx_td ltx_align_left">1.428</td>
</tr>
<tr id="S3.T1.1.5.3" class="ltx_tr">
<td id="S3.T1.1.5.3.1" class="ltx_td ltx_align_left">0.892</td>
<td id="S3.T1.1.5.3.2" class="ltx_td ltx_align_left">0.663</td>
<td id="S3.T1.1.5.3.3" class="ltx_td ltx_align_left">0.917</td>
<td id="S3.T1.1.5.3.4" class="ltx_td ltx_align_left">0.708</td>
<td id="S3.T1.1.5.3.5" class="ltx_td ltx_align_left">1.256</td>
</tr>
<tr id="S3.T1.1.6.4" class="ltx_tr">
<td id="S3.T1.1.6.4.1" class="ltx_td ltx_align_left">0.888</td>
<td id="S3.T1.1.6.4.2" class="ltx_td ltx_align_left">0.657</td>
<td id="S3.T1.1.6.4.3" class="ltx_td ltx_align_left">0.916</td>
<td id="S3.T1.1.6.4.4" class="ltx_td ltx_align_left">0.703</td>
<td id="S3.T1.1.6.4.5" class="ltx_td ltx_align_left">0.988</td>
</tr>
<tr id="S3.T1.1.7.5" class="ltx_tr">
<td id="S3.T1.1.7.5.1" class="ltx_td ltx_align_left ltx_border_bb">0.872</td>
<td id="S3.T1.1.7.5.2" class="ltx_td ltx_align_left ltx_border_bb">0.636</td>
<td id="S3.T1.1.7.5.3" class="ltx_td ltx_align_left ltx_border_bb">0.891</td>
<td id="S3.T1.1.7.5.4" class="ltx_td ltx_align_left ltx_border_bb">0.675</td>
<td id="S3.T1.1.7.5.5" class="ltx_td ltx_align_left ltx_border_bb">0.874</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2301.05892/assets/Definitions/ObjectDetection-YOLOv5-JPEGcompression.jpg" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="412" height="335" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Scatter plot that shows the performance of the models (mAP) evolution with different compression levels expressed as average image size of the files in the modified Satellogic’s airplanes dataset. Red with "x" and blue with "+" correspond to model size nano and small of YOLOv5 model respectively.
</figcaption>
</figure>
<div id="S3.p2" class="ltx_para ltx_noindent">
<p id="S3.p2.2" class="ltx_p">The DOTAv1.0 dataset has 15 categories and different metrics are measured for each class. The categories of ’plane’ and ’storage tank’ are performing the best whereas the categories ’bridge’ and ’soccer-ball-field’ are performing the worst. Table <a href="#S3.T2" title="Table 2 ‣ 3 Results ‣ Object Detection performance variation on compressed satellite image datasets with iquaflow" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> summarizes the averaged metrics for each run by aggregating with the mean of all the categories. Following the same logic, Figure <a href="#S3.F3" title="Figure 3 ‣ 3 Results ‣ Object Detection performance variation on compressed satellite image datasets with iquaflow" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> charts the evolution of performance (mAP) along different levels of compression. The original average image size of the <math id="S3.p2.1.m1.1" class="ltx_Math" alttext="1024~{}\times~{}1024" display="inline"><semantics id="S3.p2.1.m1.1a"><mrow id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml"><mn id="S3.p2.1.m1.1.1.2" xref="S3.p2.1.m1.1.1.2.cmml">1024</mn><mo lspace="0.552em" rspace="0.552em" id="S3.p2.1.m1.1.1.1" xref="S3.p2.1.m1.1.1.1.cmml">×</mo><mn id="S3.p2.1.m1.1.1.3" xref="S3.p2.1.m1.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><apply id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1"><times id="S3.p2.1.m1.1.1.1.cmml" xref="S3.p2.1.m1.1.1.1"></times><cn type="integer" id="S3.p2.1.m1.1.1.2.cmml" xref="S3.p2.1.m1.1.1.2">1024</cn><cn type="integer" id="S3.p2.1.m1.1.1.3.cmml" xref="S3.p2.1.m1.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">1024~{}\times~{}1024</annotation></semantics></math> crops without compression was <math id="S3.p2.2.m2.1" class="ltx_Math" alttext="1.13" display="inline"><semantics id="S3.p2.2.m2.1a"><mn id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml">1.13</mn><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><cn type="float" id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1">1.13</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">1.13</annotation></semantics></math>Megabytes<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a target="_blank" href="https://github.com/satellogic/iquaflow-dota-obb-use-case" title="" class="ltx_ref ltx_href">https://github.com/satellogic/iquaflow-dota-obb-use-case</a></span></span></span>.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Performance results at different compression levels using the DOTA1.0 dataset and two OBB models. The scores for the different models are expressed as Mean Average Precision (mAP) and Average Recall (AR) as expressed in the methodology section. The last column shows the equivalent average image size from the dataset given the level of compression used.</figcaption>
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<th id="S3.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S3.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">FCOS</span></th>
<th id="S3.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S3.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">RCNN</span></th>
<th id="S3.T2.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">size</th>
</tr>
<tr id="S3.T2.1.2.2" class="ltx_tr">
<th id="S3.T2.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S3.T2.1.2.2.1.1" class="ltx_text ltx_font_bold">AR</span></th>
<th id="S3.T2.1.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S3.T2.1.2.2.2.1" class="ltx_text ltx_font_bold">mAP</span></th>
<th id="S3.T2.1.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S3.T2.1.2.2.3.1" class="ltx_text ltx_font_bold">AR</span></th>
<th id="S3.T2.1.2.2.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S3.T2.1.2.2.4.1" class="ltx_text ltx_font_bold">mAP</span></th>
<th id="S3.T2.1.2.2.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S3.T2.1.2.2.5.1" class="ltx_text ltx_font_bold">Mb</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.1.3.1" class="ltx_tr">
<td id="S3.T2.1.3.1.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T2.1.3.1.1.1" class="ltx_text ltx_font_bold">0.869</span></td>
<td id="S3.T2.1.3.1.2" class="ltx_td ltx_align_left ltx_border_t">0.688</td>
<td id="S3.T2.1.3.1.3" class="ltx_td ltx_align_left ltx_border_t">0.806</td>
<td id="S3.T2.1.3.1.4" class="ltx_td ltx_align_left ltx_border_t">0.662</td>
<td id="S3.T2.1.3.1.5" class="ltx_td ltx_align_left ltx_border_t">0.332</td>
</tr>
<tr id="S3.T2.1.4.2" class="ltx_tr">
<td id="S3.T2.1.4.2.1" class="ltx_td ltx_align_left">0.856</td>
<td id="S3.T2.1.4.2.2" class="ltx_td ltx_align_left">0.677</td>
<td id="S3.T2.1.4.2.3" class="ltx_td ltx_align_left">0.812</td>
<td id="S3.T2.1.4.2.4" class="ltx_td ltx_align_left">0.658</td>
<td id="S3.T2.1.4.2.5" class="ltx_td ltx_align_left">0.321</td>
</tr>
<tr id="S3.T2.1.5.3" class="ltx_tr">
<td id="S3.T2.1.5.3.1" class="ltx_td ltx_align_left">0.865</td>
<td id="S3.T2.1.5.3.2" class="ltx_td ltx_align_left"><span id="S3.T2.1.5.3.2.1" class="ltx_text ltx_font_bold">0.692</span></td>
<td id="S3.T2.1.5.3.3" class="ltx_td ltx_align_left"><span id="S3.T2.1.5.3.3.1" class="ltx_text ltx_font_bold">0.813</span></td>
<td id="S3.T2.1.5.3.4" class="ltx_td ltx_align_left"><span id="S3.T2.1.5.3.4.1" class="ltx_text ltx_font_bold">0.668</span></td>
<td id="S3.T2.1.5.3.5" class="ltx_td ltx_align_left">0.311</td>
</tr>
<tr id="S3.T2.1.6.4" class="ltx_tr">
<td id="S3.T2.1.6.4.1" class="ltx_td ltx_align_left">0.861</td>
<td id="S3.T2.1.6.4.2" class="ltx_td ltx_align_left">0.679</td>
<td id="S3.T2.1.6.4.3" class="ltx_td ltx_align_left">0.812</td>
<td id="S3.T2.1.6.4.4" class="ltx_td ltx_align_left">0.666</td>
<td id="S3.T2.1.6.4.5" class="ltx_td ltx_align_left">0.313</td>
</tr>
<tr id="S3.T2.1.7.5" class="ltx_tr">
<td id="S3.T2.1.7.5.1" class="ltx_td ltx_align_left">0.861</td>
<td id="S3.T2.1.7.5.2" class="ltx_td ltx_align_left">0.679</td>
<td id="S3.T2.1.7.5.3" class="ltx_td ltx_align_left">0.810</td>
<td id="S3.T2.1.7.5.4" class="ltx_td ltx_align_left">0.663</td>
<td id="S3.T2.1.7.5.5" class="ltx_td ltx_align_left">0.305</td>
</tr>
<tr id="S3.T2.1.8.6" class="ltx_tr">
<td id="S3.T2.1.8.6.1" class="ltx_td ltx_align_left">0.861</td>
<td id="S3.T2.1.8.6.2" class="ltx_td ltx_align_left">0.685</td>
<td id="S3.T2.1.8.6.3" class="ltx_td ltx_align_left">0.806</td>
<td id="S3.T2.1.8.6.4" class="ltx_td ltx_align_left">0.668</td>
<td id="S3.T2.1.8.6.5" class="ltx_td ltx_align_left">0.273</td>
</tr>
<tr id="S3.T2.1.9.7" class="ltx_tr">
<td id="S3.T2.1.9.7.1" class="ltx_td ltx_align_left">0.849</td>
<td id="S3.T2.1.9.7.2" class="ltx_td ltx_align_left">0.677</td>
<td id="S3.T2.1.9.7.3" class="ltx_td ltx_align_left">0.811</td>
<td id="S3.T2.1.9.7.4" class="ltx_td ltx_align_left">0.669</td>
<td id="S3.T2.1.9.7.5" class="ltx_td ltx_align_left">0.245</td>
</tr>
<tr id="S3.T2.1.10.8" class="ltx_tr">
<td id="S3.T2.1.10.8.1" class="ltx_td ltx_align_left">0.856</td>
<td id="S3.T2.1.10.8.2" class="ltx_td ltx_align_left">0.675</td>
<td id="S3.T2.1.10.8.3" class="ltx_td ltx_align_left">0.804</td>
<td id="S3.T2.1.10.8.4" class="ltx_td ltx_align_left">0.659</td>
<td id="S3.T2.1.10.8.5" class="ltx_td ltx_align_left">0.226</td>
</tr>
<tr id="S3.T2.1.11.9" class="ltx_tr">
<td id="S3.T2.1.11.9.1" class="ltx_td ltx_align_left">0.847</td>
<td id="S3.T2.1.11.9.2" class="ltx_td ltx_align_left">0.673</td>
<td id="S3.T2.1.11.9.3" class="ltx_td ltx_align_left">0.800</td>
<td id="S3.T2.1.11.9.4" class="ltx_td ltx_align_left">0.660</td>
<td id="S3.T2.1.11.9.5" class="ltx_td ltx_align_left">0.209</td>
</tr>
<tr id="S3.T2.1.12.10" class="ltx_tr">
<td id="S3.T2.1.12.10.1" class="ltx_td ltx_align_left">0.846</td>
<td id="S3.T2.1.12.10.2" class="ltx_td ltx_align_left">0.666</td>
<td id="S3.T2.1.12.10.3" class="ltx_td ltx_align_left">0.798</td>
<td id="S3.T2.1.12.10.4" class="ltx_td ltx_align_left">0.658</td>
<td id="S3.T2.1.12.10.5" class="ltx_td ltx_align_left">0.191</td>
</tr>
<tr id="S3.T2.1.13.11" class="ltx_tr">
<td id="S3.T2.1.13.11.1" class="ltx_td ltx_align_left">0.835</td>
<td id="S3.T2.1.13.11.2" class="ltx_td ltx_align_left">0.651</td>
<td id="S3.T2.1.13.11.3" class="ltx_td ltx_align_left">0.785</td>
<td id="S3.T2.1.13.11.4" class="ltx_td ltx_align_left">0.649</td>
<td id="S3.T2.1.13.11.5" class="ltx_td ltx_align_left">0.171</td>
</tr>
<tr id="S3.T2.1.14.12" class="ltx_tr">
<td id="S3.T2.1.14.12.1" class="ltx_td ltx_align_left">0.831</td>
<td id="S3.T2.1.14.12.2" class="ltx_td ltx_align_left">0.643</td>
<td id="S3.T2.1.14.12.3" class="ltx_td ltx_align_left">0.785</td>
<td id="S3.T2.1.14.12.4" class="ltx_td ltx_align_left">0.636</td>
<td id="S3.T2.1.14.12.5" class="ltx_td ltx_align_left">0.138</td>
</tr>
<tr id="S3.T2.1.15.13" class="ltx_tr">
<td id="S3.T2.1.15.13.1" class="ltx_td ltx_align_left ltx_border_bb">0.799</td>
<td id="S3.T2.1.15.13.2" class="ltx_td ltx_align_left ltx_border_bb">0.598</td>
<td id="S3.T2.1.15.13.3" class="ltx_td ltx_align_left ltx_border_bb">0.741</td>
<td id="S3.T2.1.15.13.4" class="ltx_td ltx_align_left ltx_border_bb">0.588</td>
<td id="S3.T2.1.15.13.5" class="ltx_td ltx_align_left ltx_border_bb"><span id="S3.T2.1.15.13.5.1" class="ltx_text ltx_font_bold">0.097</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p id="S3.p3.1" class="ltx_p">The optimal compression ratio for the oriented-RCNN model seems to be around JPEG quality score of 70 which corresponds to an average image size of 0.245 Megabytes. This is because it corresponds to the minimum average file size that can be defined without lowering the performance. On the other hand, the adapted FCOS model seems to have an optimal around 80 for JPEG quality score which corresponds to an average image size of 0.273 Megabytes.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2301.05892/assets/Definitions/ObjectDetection-Oriented-JPEGcompression.jpg" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="412" height="335" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Scatter plot that shows the performance of the models (mAP) evolution with different compression levels expressed as average image size of the files using the DOTA1.0 dataset and two OBB models. Red dots correspond to the adapted FCOS model whereas blue dots are from the oriented RCNN model.
</figcaption>
</figure>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2301.05892/assets/Definitions/tp10ships.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="412" height="412" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>An example of prediction on an image with boats compressed with <math id="S3.F4.4.m1.1" class="ltx_Math" alttext="CV\_JPEG\_QUALITY" display="inline"><semantics id="S3.F4.4.m1.1b"><mrow id="S3.F4.4.m1.1.1" xref="S3.F4.4.m1.1.1.cmml"><mi id="S3.F4.4.m1.1.1.2" xref="S3.F4.4.m1.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.F4.4.m1.1.1.1" xref="S3.F4.4.m1.1.1.1.cmml">​</mo><mi id="S3.F4.4.m1.1.1.3" xref="S3.F4.4.m1.1.1.3.cmml">V</mi><mo lspace="0em" rspace="0em" id="S3.F4.4.m1.1.1.1b" xref="S3.F4.4.m1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S3.F4.4.m1.1.1.4" xref="S3.F4.4.m1.1.1.4.cmml">_</mi><mo lspace="0em" rspace="0em" id="S3.F4.4.m1.1.1.1c" xref="S3.F4.4.m1.1.1.1.cmml">​</mo><mi id="S3.F4.4.m1.1.1.5" xref="S3.F4.4.m1.1.1.5.cmml">J</mi><mo lspace="0em" rspace="0em" id="S3.F4.4.m1.1.1.1d" xref="S3.F4.4.m1.1.1.1.cmml">​</mo><mi id="S3.F4.4.m1.1.1.6" xref="S3.F4.4.m1.1.1.6.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.F4.4.m1.1.1.1e" xref="S3.F4.4.m1.1.1.1.cmml">​</mo><mi id="S3.F4.4.m1.1.1.7" xref="S3.F4.4.m1.1.1.7.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.F4.4.m1.1.1.1f" xref="S3.F4.4.m1.1.1.1.cmml">​</mo><mi id="S3.F4.4.m1.1.1.8" xref="S3.F4.4.m1.1.1.8.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.F4.4.m1.1.1.1g" xref="S3.F4.4.m1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S3.F4.4.m1.1.1.9" xref="S3.F4.4.m1.1.1.9.cmml">_</mi><mo lspace="0em" rspace="0em" id="S3.F4.4.m1.1.1.1h" xref="S3.F4.4.m1.1.1.1.cmml">​</mo><mi id="S3.F4.4.m1.1.1.10" xref="S3.F4.4.m1.1.1.10.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S3.F4.4.m1.1.1.1i" xref="S3.F4.4.m1.1.1.1.cmml">​</mo><mi id="S3.F4.4.m1.1.1.11" xref="S3.F4.4.m1.1.1.11.cmml">U</mi><mo lspace="0em" rspace="0em" id="S3.F4.4.m1.1.1.1j" xref="S3.F4.4.m1.1.1.1.cmml">​</mo><mi id="S3.F4.4.m1.1.1.12" xref="S3.F4.4.m1.1.1.12.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.F4.4.m1.1.1.1k" xref="S3.F4.4.m1.1.1.1.cmml">​</mo><mi id="S3.F4.4.m1.1.1.13" xref="S3.F4.4.m1.1.1.13.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.F4.4.m1.1.1.1l" xref="S3.F4.4.m1.1.1.1.cmml">​</mo><mi id="S3.F4.4.m1.1.1.14" xref="S3.F4.4.m1.1.1.14.cmml">I</mi><mo lspace="0em" rspace="0em" id="S3.F4.4.m1.1.1.1m" xref="S3.F4.4.m1.1.1.1.cmml">​</mo><mi id="S3.F4.4.m1.1.1.15" xref="S3.F4.4.m1.1.1.15.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.F4.4.m1.1.1.1n" xref="S3.F4.4.m1.1.1.1.cmml">​</mo><mi id="S3.F4.4.m1.1.1.16" xref="S3.F4.4.m1.1.1.16.cmml">Y</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.F4.4.m1.1c"><apply id="S3.F4.4.m1.1.1.cmml" xref="S3.F4.4.m1.1.1"><times id="S3.F4.4.m1.1.1.1.cmml" xref="S3.F4.4.m1.1.1.1"></times><ci id="S3.F4.4.m1.1.1.2.cmml" xref="S3.F4.4.m1.1.1.2">𝐶</ci><ci id="S3.F4.4.m1.1.1.3.cmml" xref="S3.F4.4.m1.1.1.3">𝑉</ci><ci id="S3.F4.4.m1.1.1.4.cmml" xref="S3.F4.4.m1.1.1.4">_</ci><ci id="S3.F4.4.m1.1.1.5.cmml" xref="S3.F4.4.m1.1.1.5">𝐽</ci><ci id="S3.F4.4.m1.1.1.6.cmml" xref="S3.F4.4.m1.1.1.6">𝑃</ci><ci id="S3.F4.4.m1.1.1.7.cmml" xref="S3.F4.4.m1.1.1.7">𝐸</ci><ci id="S3.F4.4.m1.1.1.8.cmml" xref="S3.F4.4.m1.1.1.8">𝐺</ci><ci id="S3.F4.4.m1.1.1.9.cmml" xref="S3.F4.4.m1.1.1.9">_</ci><ci id="S3.F4.4.m1.1.1.10.cmml" xref="S3.F4.4.m1.1.1.10">𝑄</ci><ci id="S3.F4.4.m1.1.1.11.cmml" xref="S3.F4.4.m1.1.1.11">𝑈</ci><ci id="S3.F4.4.m1.1.1.12.cmml" xref="S3.F4.4.m1.1.1.12">𝐴</ci><ci id="S3.F4.4.m1.1.1.13.cmml" xref="S3.F4.4.m1.1.1.13">𝐿</ci><ci id="S3.F4.4.m1.1.1.14.cmml" xref="S3.F4.4.m1.1.1.14">𝐼</ci><ci id="S3.F4.4.m1.1.1.15.cmml" xref="S3.F4.4.m1.1.1.15">𝑇</ci><ci id="S3.F4.4.m1.1.1.16.cmml" xref="S3.F4.4.m1.1.1.16">𝑌</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.4.m1.1d">CV\_JPEG\_QUALITY</annotation></semantics></math> of <math id="S3.F4.5.m2.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S3.F4.5.m2.1b"><mn id="S3.F4.5.m2.1.1" xref="S3.F4.5.m2.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S3.F4.5.m2.1c"><cn type="integer" id="S3.F4.5.m2.1.1.cmml" xref="S3.F4.5.m2.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.5.m2.1d">10</annotation></semantics></math> which is equivalent to an average dataset image size of <math id="S3.F4.6.m3.1" class="ltx_Math" alttext="0.097Mb" display="inline"><semantics id="S3.F4.6.m3.1b"><mrow id="S3.F4.6.m3.1.1" xref="S3.F4.6.m3.1.1.cmml"><mn id="S3.F4.6.m3.1.1.2" xref="S3.F4.6.m3.1.1.2.cmml">0.097</mn><mo lspace="0em" rspace="0em" id="S3.F4.6.m3.1.1.1" xref="S3.F4.6.m3.1.1.1.cmml">​</mo><mi id="S3.F4.6.m3.1.1.3" xref="S3.F4.6.m3.1.1.3.cmml">M</mi><mo lspace="0em" rspace="0em" id="S3.F4.6.m3.1.1.1b" xref="S3.F4.6.m3.1.1.1.cmml">​</mo><mi id="S3.F4.6.m3.1.1.4" xref="S3.F4.6.m3.1.1.4.cmml">b</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.F4.6.m3.1c"><apply id="S3.F4.6.m3.1.1.cmml" xref="S3.F4.6.m3.1.1"><times id="S3.F4.6.m3.1.1.1.cmml" xref="S3.F4.6.m3.1.1.1"></times><cn type="float" id="S3.F4.6.m3.1.1.2.cmml" xref="S3.F4.6.m3.1.1.2">0.097</cn><ci id="S3.F4.6.m3.1.1.3.cmml" xref="S3.F4.6.m3.1.1.3">𝑀</ci><ci id="S3.F4.6.m3.1.1.4.cmml" xref="S3.F4.6.m3.1.1.4">𝑏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.6.m3.1d">0.097Mb</annotation></semantics></math>. The model used is adapted FCOS. The image belongs to the testing partition.
</figcaption>
</figure>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2301.05892/assets/Definitions/planes.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="412" height="411" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>An example of prediction on an image with planes compressed with <math id="S3.F5.4.m1.1" class="ltx_Math" alttext="CV\_JPEG\_QUALITY" display="inline"><semantics id="S3.F5.4.m1.1b"><mrow id="S3.F5.4.m1.1.1" xref="S3.F5.4.m1.1.1.cmml"><mi id="S3.F5.4.m1.1.1.2" xref="S3.F5.4.m1.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.F5.4.m1.1.1.1" xref="S3.F5.4.m1.1.1.1.cmml">​</mo><mi id="S3.F5.4.m1.1.1.3" xref="S3.F5.4.m1.1.1.3.cmml">V</mi><mo lspace="0em" rspace="0em" id="S3.F5.4.m1.1.1.1b" xref="S3.F5.4.m1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S3.F5.4.m1.1.1.4" xref="S3.F5.4.m1.1.1.4.cmml">_</mi><mo lspace="0em" rspace="0em" id="S3.F5.4.m1.1.1.1c" xref="S3.F5.4.m1.1.1.1.cmml">​</mo><mi id="S3.F5.4.m1.1.1.5" xref="S3.F5.4.m1.1.1.5.cmml">J</mi><mo lspace="0em" rspace="0em" id="S3.F5.4.m1.1.1.1d" xref="S3.F5.4.m1.1.1.1.cmml">​</mo><mi id="S3.F5.4.m1.1.1.6" xref="S3.F5.4.m1.1.1.6.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.F5.4.m1.1.1.1e" xref="S3.F5.4.m1.1.1.1.cmml">​</mo><mi id="S3.F5.4.m1.1.1.7" xref="S3.F5.4.m1.1.1.7.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.F5.4.m1.1.1.1f" xref="S3.F5.4.m1.1.1.1.cmml">​</mo><mi id="S3.F5.4.m1.1.1.8" xref="S3.F5.4.m1.1.1.8.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.F5.4.m1.1.1.1g" xref="S3.F5.4.m1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S3.F5.4.m1.1.1.9" xref="S3.F5.4.m1.1.1.9.cmml">_</mi><mo lspace="0em" rspace="0em" id="S3.F5.4.m1.1.1.1h" xref="S3.F5.4.m1.1.1.1.cmml">​</mo><mi id="S3.F5.4.m1.1.1.10" xref="S3.F5.4.m1.1.1.10.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S3.F5.4.m1.1.1.1i" xref="S3.F5.4.m1.1.1.1.cmml">​</mo><mi id="S3.F5.4.m1.1.1.11" xref="S3.F5.4.m1.1.1.11.cmml">U</mi><mo lspace="0em" rspace="0em" id="S3.F5.4.m1.1.1.1j" xref="S3.F5.4.m1.1.1.1.cmml">​</mo><mi id="S3.F5.4.m1.1.1.12" xref="S3.F5.4.m1.1.1.12.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.F5.4.m1.1.1.1k" xref="S3.F5.4.m1.1.1.1.cmml">​</mo><mi id="S3.F5.4.m1.1.1.13" xref="S3.F5.4.m1.1.1.13.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.F5.4.m1.1.1.1l" xref="S3.F5.4.m1.1.1.1.cmml">​</mo><mi id="S3.F5.4.m1.1.1.14" xref="S3.F5.4.m1.1.1.14.cmml">I</mi><mo lspace="0em" rspace="0em" id="S3.F5.4.m1.1.1.1m" xref="S3.F5.4.m1.1.1.1.cmml">​</mo><mi id="S3.F5.4.m1.1.1.15" xref="S3.F5.4.m1.1.1.15.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.F5.4.m1.1.1.1n" xref="S3.F5.4.m1.1.1.1.cmml">​</mo><mi id="S3.F5.4.m1.1.1.16" xref="S3.F5.4.m1.1.1.16.cmml">Y</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.F5.4.m1.1c"><apply id="S3.F5.4.m1.1.1.cmml" xref="S3.F5.4.m1.1.1"><times id="S3.F5.4.m1.1.1.1.cmml" xref="S3.F5.4.m1.1.1.1"></times><ci id="S3.F5.4.m1.1.1.2.cmml" xref="S3.F5.4.m1.1.1.2">𝐶</ci><ci id="S3.F5.4.m1.1.1.3.cmml" xref="S3.F5.4.m1.1.1.3">𝑉</ci><ci id="S3.F5.4.m1.1.1.4.cmml" xref="S3.F5.4.m1.1.1.4">_</ci><ci id="S3.F5.4.m1.1.1.5.cmml" xref="S3.F5.4.m1.1.1.5">𝐽</ci><ci id="S3.F5.4.m1.1.1.6.cmml" xref="S3.F5.4.m1.1.1.6">𝑃</ci><ci id="S3.F5.4.m1.1.1.7.cmml" xref="S3.F5.4.m1.1.1.7">𝐸</ci><ci id="S3.F5.4.m1.1.1.8.cmml" xref="S3.F5.4.m1.1.1.8">𝐺</ci><ci id="S3.F5.4.m1.1.1.9.cmml" xref="S3.F5.4.m1.1.1.9">_</ci><ci id="S3.F5.4.m1.1.1.10.cmml" xref="S3.F5.4.m1.1.1.10">𝑄</ci><ci id="S3.F5.4.m1.1.1.11.cmml" xref="S3.F5.4.m1.1.1.11">𝑈</ci><ci id="S3.F5.4.m1.1.1.12.cmml" xref="S3.F5.4.m1.1.1.12">𝐴</ci><ci id="S3.F5.4.m1.1.1.13.cmml" xref="S3.F5.4.m1.1.1.13">𝐿</ci><ci id="S3.F5.4.m1.1.1.14.cmml" xref="S3.F5.4.m1.1.1.14">𝐼</ci><ci id="S3.F5.4.m1.1.1.15.cmml" xref="S3.F5.4.m1.1.1.15">𝑇</ci><ci id="S3.F5.4.m1.1.1.16.cmml" xref="S3.F5.4.m1.1.1.16">𝑌</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F5.4.m1.1d">CV\_JPEG\_QUALITY</annotation></semantics></math> of <math id="S3.F5.5.m2.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S3.F5.5.m2.1b"><mn id="S3.F5.5.m2.1.1" xref="S3.F5.5.m2.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S3.F5.5.m2.1c"><cn type="integer" id="S3.F5.5.m2.1.1.cmml" xref="S3.F5.5.m2.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F5.5.m2.1d">10</annotation></semantics></math> which is equivalent to an average dataset image size of <math id="S3.F5.6.m3.1" class="ltx_Math" alttext="0.097Mb" display="inline"><semantics id="S3.F5.6.m3.1b"><mrow id="S3.F5.6.m3.1.1" xref="S3.F5.6.m3.1.1.cmml"><mn id="S3.F5.6.m3.1.1.2" xref="S3.F5.6.m3.1.1.2.cmml">0.097</mn><mo lspace="0em" rspace="0em" id="S3.F5.6.m3.1.1.1" xref="S3.F5.6.m3.1.1.1.cmml">​</mo><mi id="S3.F5.6.m3.1.1.3" xref="S3.F5.6.m3.1.1.3.cmml">M</mi><mo lspace="0em" rspace="0em" id="S3.F5.6.m3.1.1.1b" xref="S3.F5.6.m3.1.1.1.cmml">​</mo><mi id="S3.F5.6.m3.1.1.4" xref="S3.F5.6.m3.1.1.4.cmml">b</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.F5.6.m3.1c"><apply id="S3.F5.6.m3.1.1.cmml" xref="S3.F5.6.m3.1.1"><times id="S3.F5.6.m3.1.1.1.cmml" xref="S3.F5.6.m3.1.1.1"></times><cn type="float" id="S3.F5.6.m3.1.1.2.cmml" xref="S3.F5.6.m3.1.1.2">0.097</cn><ci id="S3.F5.6.m3.1.1.3.cmml" xref="S3.F5.6.m3.1.1.3">𝑀</ci><ci id="S3.F5.6.m3.1.1.4.cmml" xref="S3.F5.6.m3.1.1.4">𝑏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F5.6.m3.1d">0.097Mb</annotation></semantics></math>. The model used is adapted FCOS. The image belongs to the testing partition.
</figcaption>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusions</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.3" class="ltx_p">In the experiment with Satellogic’s airplanes dataset, the decrease in performance with compression is consistent for both models. The variations of mAP is small between the ranges of <math id="S4.p1.1.m1.1" class="ltx_Math" alttext="0.15" display="inline"><semantics id="S4.p1.1.m1.1a"><mn id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">0.15</mn><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><cn type="float" id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1">0.15</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">0.15</annotation></semantics></math> and <math id="S4.p1.2.m2.1" class="ltx_Math" alttext="0.25" display="inline"><semantics id="S4.p1.2.m2.1a"><mn id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml">0.25</mn><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><cn type="float" id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1">0.25</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">0.25</annotation></semantics></math> average image size. The additional complexity of the Small model has a constant positive shift of <math id="S4.p1.3.m3.1" class="ltx_Math" alttext="0.5" display="inline"><semantics id="S4.p1.3.m3.1a"><mn id="S4.p1.3.m3.1.1" xref="S4.p1.3.m3.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S4.p1.3.m3.1b"><cn type="float" id="S4.p1.3.m3.1.1.cmml" xref="S4.p1.3.m3.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.3.m3.1c">0.5</annotation></semantics></math> in mAP with respect to the Nano model along all the analyzed compression rates.</p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.1" class="ltx_p">In the context of the second experiment the adapted FCOS model seems to perform better than oriented RCNN because the AR and mAP are greater for all levels of compression. On the other hand, oriented-RCNN seems more resilient because the optimal compression ratio is higher than the optimal case for the other model. However, the degraded performance of FCOS model given the same compression setting as the optimal value for oriented-RCNN still offers higher performance. FCOS is also easier to implement because it is a single-stage detector that does not require setting anchors as hyperparameters. So far, given the data and context of the study, FCOS seems the best option.</p>
</div>
<div id="S4.p3" class="ltx_para ltx_noindent">
<p id="S4.p3.7" class="ltx_p">Another interesting observation is the high resilience of the model for some specific applications. The figures <a href="#S3.F4" title="Figure 4 ‣ 3 Results ‣ Object Detection performance variation on compressed satellite image datasets with iquaflow" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and <a href="#S3.F5" title="Figure 5 ‣ 3 Results ‣ Object Detection performance variation on compressed satellite image datasets with iquaflow" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> show a prediction with the FCOS model on an image with boats and airplanes respectively. Both of the images were set with a compression rate of <math id="S4.p3.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S4.p3.1.m1.1a"><mn id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><cn type="integer" id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">10</annotation></semantics></math> for <math id="S4.p3.2.m2.1" class="ltx_Math" alttext="CV\_JPEG\_QUALITY" display="inline"><semantics id="S4.p3.2.m2.1a"><mrow id="S4.p3.2.m2.1.1" xref="S4.p3.2.m2.1.1.cmml"><mi id="S4.p3.2.m2.1.1.2" xref="S4.p3.2.m2.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S4.p3.2.m2.1.1.1" xref="S4.p3.2.m2.1.1.1.cmml">​</mo><mi id="S4.p3.2.m2.1.1.3" xref="S4.p3.2.m2.1.1.3.cmml">V</mi><mo lspace="0em" rspace="0em" id="S4.p3.2.m2.1.1.1a" xref="S4.p3.2.m2.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S4.p3.2.m2.1.1.4" xref="S4.p3.2.m2.1.1.4.cmml">_</mi><mo lspace="0em" rspace="0em" id="S4.p3.2.m2.1.1.1b" xref="S4.p3.2.m2.1.1.1.cmml">​</mo><mi id="S4.p3.2.m2.1.1.5" xref="S4.p3.2.m2.1.1.5.cmml">J</mi><mo lspace="0em" rspace="0em" id="S4.p3.2.m2.1.1.1c" xref="S4.p3.2.m2.1.1.1.cmml">​</mo><mi id="S4.p3.2.m2.1.1.6" xref="S4.p3.2.m2.1.1.6.cmml">P</mi><mo lspace="0em" rspace="0em" id="S4.p3.2.m2.1.1.1d" xref="S4.p3.2.m2.1.1.1.cmml">​</mo><mi id="S4.p3.2.m2.1.1.7" xref="S4.p3.2.m2.1.1.7.cmml">E</mi><mo lspace="0em" rspace="0em" id="S4.p3.2.m2.1.1.1e" xref="S4.p3.2.m2.1.1.1.cmml">​</mo><mi id="S4.p3.2.m2.1.1.8" xref="S4.p3.2.m2.1.1.8.cmml">G</mi><mo lspace="0em" rspace="0em" id="S4.p3.2.m2.1.1.1f" xref="S4.p3.2.m2.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S4.p3.2.m2.1.1.9" xref="S4.p3.2.m2.1.1.9.cmml">_</mi><mo lspace="0em" rspace="0em" id="S4.p3.2.m2.1.1.1g" xref="S4.p3.2.m2.1.1.1.cmml">​</mo><mi id="S4.p3.2.m2.1.1.10" xref="S4.p3.2.m2.1.1.10.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S4.p3.2.m2.1.1.1h" xref="S4.p3.2.m2.1.1.1.cmml">​</mo><mi id="S4.p3.2.m2.1.1.11" xref="S4.p3.2.m2.1.1.11.cmml">U</mi><mo lspace="0em" rspace="0em" id="S4.p3.2.m2.1.1.1i" xref="S4.p3.2.m2.1.1.1.cmml">​</mo><mi id="S4.p3.2.m2.1.1.12" xref="S4.p3.2.m2.1.1.12.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.p3.2.m2.1.1.1j" xref="S4.p3.2.m2.1.1.1.cmml">​</mo><mi id="S4.p3.2.m2.1.1.13" xref="S4.p3.2.m2.1.1.13.cmml">L</mi><mo lspace="0em" rspace="0em" id="S4.p3.2.m2.1.1.1k" xref="S4.p3.2.m2.1.1.1.cmml">​</mo><mi id="S4.p3.2.m2.1.1.14" xref="S4.p3.2.m2.1.1.14.cmml">I</mi><mo lspace="0em" rspace="0em" id="S4.p3.2.m2.1.1.1l" xref="S4.p3.2.m2.1.1.1.cmml">​</mo><mi id="S4.p3.2.m2.1.1.15" xref="S4.p3.2.m2.1.1.15.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.p3.2.m2.1.1.1m" xref="S4.p3.2.m2.1.1.1.cmml">​</mo><mi id="S4.p3.2.m2.1.1.16" xref="S4.p3.2.m2.1.1.16.cmml">Y</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.2.m2.1b"><apply id="S4.p3.2.m2.1.1.cmml" xref="S4.p3.2.m2.1.1"><times id="S4.p3.2.m2.1.1.1.cmml" xref="S4.p3.2.m2.1.1.1"></times><ci id="S4.p3.2.m2.1.1.2.cmml" xref="S4.p3.2.m2.1.1.2">𝐶</ci><ci id="S4.p3.2.m2.1.1.3.cmml" xref="S4.p3.2.m2.1.1.3">𝑉</ci><ci id="S4.p3.2.m2.1.1.4.cmml" xref="S4.p3.2.m2.1.1.4">_</ci><ci id="S4.p3.2.m2.1.1.5.cmml" xref="S4.p3.2.m2.1.1.5">𝐽</ci><ci id="S4.p3.2.m2.1.1.6.cmml" xref="S4.p3.2.m2.1.1.6">𝑃</ci><ci id="S4.p3.2.m2.1.1.7.cmml" xref="S4.p3.2.m2.1.1.7">𝐸</ci><ci id="S4.p3.2.m2.1.1.8.cmml" xref="S4.p3.2.m2.1.1.8">𝐺</ci><ci id="S4.p3.2.m2.1.1.9.cmml" xref="S4.p3.2.m2.1.1.9">_</ci><ci id="S4.p3.2.m2.1.1.10.cmml" xref="S4.p3.2.m2.1.1.10">𝑄</ci><ci id="S4.p3.2.m2.1.1.11.cmml" xref="S4.p3.2.m2.1.1.11">𝑈</ci><ci id="S4.p3.2.m2.1.1.12.cmml" xref="S4.p3.2.m2.1.1.12">𝐴</ci><ci id="S4.p3.2.m2.1.1.13.cmml" xref="S4.p3.2.m2.1.1.13">𝐿</ci><ci id="S4.p3.2.m2.1.1.14.cmml" xref="S4.p3.2.m2.1.1.14">𝐼</ci><ci id="S4.p3.2.m2.1.1.15.cmml" xref="S4.p3.2.m2.1.1.15">𝑇</ci><ci id="S4.p3.2.m2.1.1.16.cmml" xref="S4.p3.2.m2.1.1.16">𝑌</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.2.m2.1c">CV\_JPEG\_QUALITY</annotation></semantics></math> which is equivalent to an average dataset image size of <math id="S4.p3.3.m3.1" class="ltx_Math" alttext="0.097Mb" display="inline"><semantics id="S4.p3.3.m3.1a"><mrow id="S4.p3.3.m3.1.1" xref="S4.p3.3.m3.1.1.cmml"><mn id="S4.p3.3.m3.1.1.2" xref="S4.p3.3.m3.1.1.2.cmml">0.097</mn><mo lspace="0em" rspace="0em" id="S4.p3.3.m3.1.1.1" xref="S4.p3.3.m3.1.1.1.cmml">​</mo><mi id="S4.p3.3.m3.1.1.3" xref="S4.p3.3.m3.1.1.3.cmml">M</mi><mo lspace="0em" rspace="0em" id="S4.p3.3.m3.1.1.1a" xref="S4.p3.3.m3.1.1.1.cmml">​</mo><mi id="S4.p3.3.m3.1.1.4" xref="S4.p3.3.m3.1.1.4.cmml">b</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.3.m3.1b"><apply id="S4.p3.3.m3.1.1.cmml" xref="S4.p3.3.m3.1.1"><times id="S4.p3.3.m3.1.1.1.cmml" xref="S4.p3.3.m3.1.1.1"></times><cn type="float" id="S4.p3.3.m3.1.1.2.cmml" xref="S4.p3.3.m3.1.1.2">0.097</cn><ci id="S4.p3.3.m3.1.1.3.cmml" xref="S4.p3.3.m3.1.1.3">𝑀</ci><ci id="S4.p3.3.m3.1.1.4.cmml" xref="S4.p3.3.m3.1.1.4">𝑏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.3.m3.1c">0.097Mb</annotation></semantics></math>. In the first image, <math id="S4.p3.4.m4.1" class="ltx_Math" alttext="146" display="inline"><semantics id="S4.p3.4.m4.1a"><mn id="S4.p3.4.m4.1.1" xref="S4.p3.4.m4.1.1.cmml">146</mn><annotation-xml encoding="MathML-Content" id="S4.p3.4.m4.1b"><cn type="integer" id="S4.p3.4.m4.1.1.cmml" xref="S4.p3.4.m4.1.1">146</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.4.m4.1c">146</annotation></semantics></math> ships were correctly detected (True positives), <math id="S4.p3.5.m5.1" class="ltx_Math" alttext="9" display="inline"><semantics id="S4.p3.5.m5.1a"><mn id="S4.p3.5.m5.1.1" xref="S4.p3.5.m5.1.1.cmml">9</mn><annotation-xml encoding="MathML-Content" id="S4.p3.5.m5.1b"><cn type="integer" id="S4.p3.5.m5.1.1.cmml" xref="S4.p3.5.m5.1.1">9</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.5.m5.1c">9</annotation></semantics></math> were wrongly detected (False positive) and <math id="S4.p3.6.m6.1" class="ltx_Math" alttext="11" display="inline"><semantics id="S4.p3.6.m6.1a"><mn id="S4.p3.6.m6.1.1" xref="S4.p3.6.m6.1.1.cmml">11</mn><annotation-xml encoding="MathML-Content" id="S4.p3.6.m6.1b"><cn type="integer" id="S4.p3.6.m6.1.1.cmml" xref="S4.p3.6.m6.1.1">11</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.6.m6.1c">11</annotation></semantics></math> ships were missed (False negative). In the other example all the planes (total amount: <math id="S4.p3.7.m7.1" class="ltx_Math" alttext="39" display="inline"><semantics id="S4.p3.7.m7.1a"><mn id="S4.p3.7.m7.1.1" xref="S4.p3.7.m7.1.1.cmml">39</mn><annotation-xml encoding="MathML-Content" id="S4.p3.7.m7.1b"><cn type="integer" id="S4.p3.7.m7.1.1.cmml" xref="S4.p3.7.m7.1.1">39</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.7.m7.1c">39</annotation></semantics></math>) are correctly detected see <a href="#S3.F5" title="Figure 5 ‣ 3 Results ‣ Object Detection performance variation on compressed satellite image datasets with iquaflow" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> with no false positives or false negatives. This highlights the greater capacity of compressing images for usage such as the detection of airplanes over smaller or more difficult objects.</p>
</div>
<div id="S4.p4" class="ltx_para ltx_noindent">
<p id="S4.p4.1" class="ltx_p">This study highlights the potential of <span id="S4.p4.1.1" class="ltx_text ltx_font_smallcaps">iquaflow</span> for decision-makers as well as researchers that want to study performance variation in an agile and ordered way. The key effort has been the development of the tool so that it facilitates further studies with the aim to scale it. The tool also allows for mitigating the uncertainty of image quality by using several strategies to measure that. This is helping also in studies that are exploring suitable solutions for satellite image Super Resolution.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para ltx_noindent">
<p id="Sx1.p1.1" class="ltx_p">Conceptualization, P.G. and J.M.; methodology, P.G. and J.M.; software, P.G. and K.T.; validation, K.T. and J.M.; formal analysis, P.G.; investigation, P.G.; resources, J.M.; data curation, P.G.; writing—original draft preparation, P.G.; writing—review and editing, K.T. and J.M.; visualization, P.G.; supervision, J.M.; project administration, J.M.; funding acquisition, J.M. All authors have read and agreed to the published version of the manuscript.</p>
</div>
<div id="Sx1.p2" class="ltx_para ltx_noindent">
<p id="Sx1.p2.1" class="ltx_p">This research was funded by the Ministry of Science and Innovation and by the European Union within the framework of Retos-Collaboration of the State Program of Research, Development and Innovation Oriented to the Challenges of Society, within the State Research Plan Scientific and Technical and Innovation 2017-2020, with the main objective of promoting technological development, innovation, and quality research. grant number: RTC2019-007434-7.</p>
</div>
<div id="Sx1.p3" class="ltx_para ltx_noindent">
<p id="Sx1.p3.1" class="ltx_p">The authors declare no conflict of interest.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Martina Lofqvist and José Cano.

</span>
<span class="ltx_bibblock">Optimizing data processing in space for object detection in satellite
imagery, 2021.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Yong-Yeon Jo, Young Sang Choi, Hyun Woo Park, Jae Hyeok Lee, Hyojung Jung,
Hyo-Eun Kim, Kyounglan Ko, Chan Wha Lee, Hyo Soung Cha, and Yul Hwangbo.

</span>
<span class="ltx_bibblock">Impact of image compression on deep learning-based mammogram
classification.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Scientific Reports</span>, 11, 2021.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Kresimir Delac, Mislav Grgic, and Sonja Grgic.

</span>
<span class="ltx_bibblock">Effects of jpeg and jpeg2000 compression on face recognition.

</span>
<span class="ltx_bibblock">In Sameer Singh, Maneesha Singh, Chid Apte, and Petra Perner,
editors, <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Pattern Recognition and Image Analysis</span>, pages 136–145,
Berlin, Heidelberg, 2005. Springer Berlin Heidelberg.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Gui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge Belongie, Jiebo Luo, Mihai
Datcu, Marcello Pelillo, and Liangpei Zhang.

</span>
<span class="ltx_bibblock">Dota: A large-scale dataset for object detection in aerial images.

</span>
<span class="ltx_bibblock">In <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</span>, 5 2018.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Vinicius Alves de Oliveira, Marie Chabert, Thomas Oberlin, Charly Poulliat,
Mickael Bruno, Christophe Latry, Mikael Carlavan, Simon Henrot, Frederic
Falzon, and Roberto Camarero.

</span>
<span class="ltx_bibblock">Reduced-complexity end-to-end variational autoencoder for on board
satellite image compression.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Remote Sensing</span>, 13(3), 2021.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Abir Jaafar Hussain, Ali Al-Fayadh, and Naeem Radi.

</span>
<span class="ltx_bibblock">Image compression techniques: A survey in lossless and lossy
algorithms.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Neurocomputing</span>, 300:44–69, 2018.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.

</span>
<span class="ltx_bibblock">Faster R-CNN: Towards real-time object detection with region
proposal networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems (NIPS)</span>,
2015.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott E. Reed,
Cheng-Yang Fu, and Alexander C. Berg.

</span>
<span class="ltx_bibblock">Ssd: Single shot multibox detector.

</span>
<span class="ltx_bibblock">In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, editors,
<span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">ECCV (1)</span>, volume 9905 of <span id="bib.bib8.2.2" class="ltx_text ltx_font_italic">Lecture Notes in Computer Science</span>, pages
21–37. Springer, 2016.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Joseph Redmon and Ali Farhadi.

</span>
<span class="ltx_bibblock">Yolov3: An incremental improvement, 2018.

</span>
<span class="ltx_bibblock">cite arxiv:1804.02767Comment: Tech Report.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He, and Piotr Dollár.

</span>
<span class="ltx_bibblock">Focal loss for dense object detection.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">2017 IEEE International Conference on Computer Vision (ICCV)</span>,
pages 2999–3007, 2017.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Jian Ding, Nan Xue, Yang Long, Gui-Song Xia, and Qikai Lu.

</span>
<span class="ltx_bibblock">Learning roi transformer for detecting oriented objects in aerial
images.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1812.00155, 2018.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Yongchao Xu, Mingtao Fu, Qimeng Wang, Yukang Wang, Kai Chen, Gui-Song Xia, and
Xiang Bai.

</span>
<span class="ltx_bibblock">Gliding vertex on the horizontal bounding box for multi-oriented
object detection.

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine
Intelligence</span>, 43(4):1452–1459, 4 2021.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Xue Yang, Jirui Yang, Junchi Yan, Yue Zhang, Tengfei Zhang, Zhi Guo, Sun Xian,
and Kun Fu.

</span>
<span class="ltx_bibblock">Scrdet: Towards more robust detection for small, cluttered and
rotated objects, 2018.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Jim Nilsson and Tomas Akenine-Möller.

</span>
<span class="ltx_bibblock">Understanding ssim, 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang.

</span>
<span class="ltx_bibblock">The unreasonable effectiveness of deep features as a perceptual
metric, 2018.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Rafael Reisenhofer, Sebastian Bosse, Gitta Kutyniok, and Thomas Wiegand.

</span>
<span class="ltx_bibblock">A haar wavelet-based perceptual similarity index for image quality
assessment.

</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Signal Processing: Image Communication</span>, 61:33–43, feb 2018.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Keyan Ding, Kede Ma, Shiqi Wang, and Eero P. Simoncelli.

</span>
<span class="ltx_bibblock">Image quality assessment: Unifying structure and texture similarity.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine
Intelligence</span>, pages 1–1, 2020.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
P. Gallés, K. Takats, M. Hernández-Cabronero, D. Berga, L. Pega,
L. Riordan-Chen, C. Garcia, G. Becker, A. Garriga, A. Bukva,
J. Serra-Sagristà, D. Vilaseca, and J. Marín.

</span>
<span class="ltx_bibblock">Iquaflow: A new framework to measure image quality, 2022.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Jon Leachtenauer, William Malila, John Irvine, Linda Colburn, and Nanette
Salvaggio.

</span>
<span class="ltx_bibblock">General image-quality equation: Giqe.

</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Applied optics</span>, 36:8322–8, 12 1997.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Barry T. Bosworth, W. Robert Bernecky, James D. Nickila, Berhane Adal, and
G. Clifford Carter.

</span>
<span class="ltx_bibblock">Estimating signal-to-noise ratio (snr).

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">IEEE Journal of Oceanic Engineering</span>, 33(4):414–418, 2008.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
David Berga, Pau Gallés, Katalin Takáts, Eva Mohedano, Laura Riordan-Chen,
Clara Garcia-Moll, David Vilaseca, and Javier Marín.

</span>
<span class="ltx_bibblock">Qmrnet: Quality metric regression for eo image quality assessment and
super-resolution, 2022.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Gregory K. Wallace.

</span>
<span class="ltx_bibblock">The JPEG still picture compression standard.

</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">Communications of the ACM</span>, 34:31–44, 4 1991.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
G. Bradski.

</span>
<span class="ltx_bibblock">The OpenCV Library.

</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">Dr. Dobb’s Journal of Software Tools</span>, 2000.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Glenn Jocher, Alex Stoken, Jirka Borovec, NanoCode012, Ayush Chaurasia, TaoXie,
Liu Changyu, Abhiram V, Laughing, tkianai, yxNONG, Adam Hogan,
lorenzomammana, AlexWang1900, Jan Hajek, Laurentiu Diaconu, Marc, Yonghye
Kwon, oleg, wanghaoyang0106, Yann Defretin, Aditya Lohia, ml5ah, Ben Milanko,
Benjamin Fineran, Daniel Khromov, Ding Yiwei, Doug, Durgesh, and Francisco
Ingham.

</span>
<span class="ltx_bibblock">ultralytics/yolov5: v5.0 - YOLOv5-P6 1280 models, AWS, Supervise.ly
and YouTube integrations.

</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">Zenodo</span>, April 2021.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Xingxing Xie, Gong Cheng, Jiabao Wang, Xiwen Yao, and Junwei Han.

</span>
<span class="ltx_bibblock">Oriented r-cnn for object detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV)</span>, pages 3520–3529, 10 2021.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Zhi Tian, Chunhua Shen, Hao Chen, and Tong He.

</span>
<span class="ltx_bibblock">Fcos: A simple and strong anchor-free object detector.

</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>,
2021.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Jeffri M. Llerena, Luis Felipe Zeni, Lucas N. Kristen, and Claudio Jung.

</span>
<span class="ltx_bibblock">Gaussian bounding boxes and probabilistic intersection-over-union for
object detection, 2021.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
D. M. W. Powers.

</span>
<span class="ltx_bibblock">Evaluation: From precision, recall and f-measure to roc.,
informedness, markedness &amp; correlation.

</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">Journal of Machine Learning Technologies</span>, 2(1):37–63, 2011.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2301.05891" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2301.05892" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2301.05892">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2301.05892" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2301.05893" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 06:53:01 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
