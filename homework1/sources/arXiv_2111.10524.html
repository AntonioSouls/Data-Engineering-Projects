<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2111.10524] ACR-Pose: Adversarial Canonical Representation Reconstruction Network for Category Level 6D Object Pose Estimation</title><meta property="og:description" content="Recently, category-level 6D object pose estimation has achieved significant improvements with the development of reconstructing canonical 3D representations. However, the reconstruction quality of existing methods is s‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="ACR-Pose: Adversarial Canonical Representation Reconstruction Network for Category Level 6D Object Pose Estimation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="ACR-Pose: Adversarial Canonical Representation Reconstruction Network for Category Level 6D Object Pose Estimation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2111.10524">

<!--Generated on Tue Mar 19 16:58:28 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">ACR-Pose: Adversarial Canonical Representation Reconstruction Network for Category Level 6D Object Pose Estimation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id7.7.7" class="ltx_text ltx_font_bold">Zhaoxin Fan<sup id="id7.7.7.1" class="ltx_sup"><span id="id7.7.7.1.1" class="ltx_text ltx_font_medium">1</span></sup>, Zhengbo Song<sup id="id7.7.7.2" class="ltx_sup"><span id="id7.7.7.2.1" class="ltx_text ltx_font_medium">3</span></sup>, Jian Xu<sup id="id7.7.7.3" class="ltx_sup"><span id="id7.7.7.3.1" class="ltx_text ltx_font_medium">4</span></sup>, Zhicheng Wang<sup id="id7.7.7.4" class="ltx_sup"><span id="id7.7.7.4.1" class="ltx_text ltx_font_medium">4</span></sup>, Kejian Wu<sup id="id7.7.7.5" class="ltx_sup"><span id="id7.7.7.5.1" class="ltx_text ltx_font_medium">4</span></sup>, Hongyan Liu<sup id="id7.7.7.6" class="ltx_sup"><span id="id7.7.7.6.1" class="ltx_text ltx_font_medium">2</span></sup>, and Jun He<sup id="id7.7.7.7" class="ltx_sup"><span id="id7.7.7.7.1" class="ltx_text ltx_font_medium">1</span></sup>
<br class="ltx_break"></span>Renmin University of China, Tsinghua University
<br class="ltx_break">Nanjing University of Science and Technology, Nreal
<br class="ltx_break"><span id="id8.8.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{fanzhaoxin,hejun}@ruc.edu.cn, {xujian,zcwang,kejian}@nreal.ai
<br class="ltx_break">hyliu@tsinghua.edu.cn, songzb@njust.edu.cn
</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id9.id1" class="ltx_p">Recently, category-level 6D object pose estimation has achieved significant improvements with the development of reconstructing canonical 3D representations. However, the reconstruction quality of existing methods is still far from excellent. In this paper, we propose a novel <span id="id9.id1.1" class="ltx_text ltx_font_bold">A</span>dversarial <span id="id9.id1.2" class="ltx_text ltx_font_bold">C</span>anonical Representation <span id="id9.id1.3" class="ltx_text ltx_font_bold">R</span>econstruction Network named <span id="id9.id1.4" class="ltx_text ltx_font_bold">ACR-Pose</span>. ACR-Pose consists of a Reconstructor and a Discriminator. The Reconstructor is primarily composed of two novel sub-modules: Pose-Irrelevant Module¬†(PIM) and Relational Reconstruction Module¬†(RRM). PIM tends to learn canonical-related features to make the Reconstructor insensitive to rotation and translation, while RRM explores essential relational information between different input modalities to generate high-quality features. Subsequently, a Discriminator is employed to guide the Reconstructor to generate realistic canonical representations. The Reconstructor and the Discriminator learn to optimize through adversarial training. Experimental results on the prevalent NOCS-CAMERA and NOCS-REAL datasets demonstrate that our method achieves state-of-the-art performance.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">6D object pose estimation is at the core of many applications, such as robotic grasping <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>, augmented reality <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, and autonomous driving <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. Given an observation (RGB, RGBD, or LiDAR scan), 6D object pose estimation aims to estimate the rotation and translation of the target object in the camera coordinate system. Over the last decade, numerous 6D object pose estimation works<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> have emerged and been deployed in industrial products, demonstrating the usefulness of this line of research. However, most of the existing methods are instance-level approaches, i.e., a well-trained model works only for a particular object.
This prohibits large-scale applications in the wild, as it is both memory-inefficient and labor exhausting to train and deploy models per object.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2111.10524/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="213" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">Our Adversarial Canonical Representation Reconstruction Network for reconstructing canonical representations, which are then aligned with the object depth observation to estimate the 6D object pose.</span></figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">To address this issue, category-level 6D object pose estimation has become increasingly popular recently among academia and industry. Given a RGBD observation as input, a deep learning network is often used to predict a canonical representation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> of the target object. The canonical representation refers to a normalized 3D model that integrates a category of objects‚Äô most representative characteristics, independent of the object‚Äôs pose, such as the NOCS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> and the CASS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> representation. Once the canonical representation is obtained, it can be used to align with the back-projected object depth observation to recover the object‚Äôs pose. Therefore, the key to successful category-level object pose estimation is to accurately reconstruct the canonical representation.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Although existing methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> have achieved promising 6D object pose estimation performance through the canonical representation, their reconstruction quality still needs improvement. Specifically, previous methods have the following drawbacks: 1) They are ineffective in learning canonical-related features due to their sensitivity to pose-related characteristics. 2) They ignore the critical relational information between different input modalities. 3) Some of their reconstruction results appear to be unrealistic.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this paper, we propose an <span id="S1.p4.1.1" class="ltx_text ltx_font_bold">A</span>dversarial <span id="S1.p4.1.2" class="ltx_text ltx_font_bold">C</span>anonical Representation <span id="S1.p4.1.3" class="ltx_text ltx_font_bold">R</span>econstruction pipeline called <span id="S1.p4.1.4" class="ltx_text ltx_font_bold">ACR-Pose</span>, which aims at reconstructing high quality canonical representations for precise 6D object estimation as shown in Fig.¬†<a href="#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ ACR-Pose: Adversarial Canonical Representation Reconstruction Network for Category Level 6D Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. In ACR-Pose, we first detect and segment out objects from the RGBD images. Then, taking the image patch and depth region of an object as input, we train a Reconstructor and a Discriminator in an adversarial manner to generate realistic canonical representations.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Specifically, as the Reconstructor plays a significant role in generating high-quality canonical representations, two novel sub-modules are proposed to learn more discriminative and robust features within the Reconstructor. First, we note that observations of the objects contain both pose-related information¬†(rotation and translation) and shape information, and existing canonical representation reconstruction models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> are sensitive to pose-related characteristics. However, since the goal of reconstruction is <em id="S1.p5.1.1" class="ltx_emph ltx_font_italic">canonical</em>, the rotational and translational cues in the object observation must be filtered out, while only the shape information should be preserved. To achieve this, we introduce a Pose-Irrelevant Module (PIM) so as to learn canonical-related features only.
Second, inspired by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, we take a learned shape prior as an input of our network, in addition to the RGB and depth images. We hypothesize that the relational information between these three sources of information is important for high-quality reconstruction. Therefore, a Relational Reconstruction Module (RRM) is introduced to learn this relational information of the three inputs.
Finally, utilizing the features learned from the RRM, the canonical representation can be reconstructed using a Shape Prior Deformation step <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. Afterwards, 6D object pose can be easily solved by aligning the canonical representation with the back-projected object depth using the Umeyama algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Furthermore, we find that the reconstruction results could be unrealistic in some cases, which may cause inaccurate alignment results. To resolve this issue, we adopt the idea of Generative Adversarial Network (GAN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> in our pipeline and propose to use a Discriminator to adversarially guide the Reconstructor, making it more creative so that it can produce more realistic canonical representations. As a result, experiments on the NOCS-CAMERA and NOCS-REAL datasets show that our ACR-Pose can achieve state-of-the-art performance in both synthetic and real scenarios.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">To summarize, our contributions are:</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">1) We propose ACR-Pose, an Adversarial Canonical Representation Reconstruction Network, for learning high-quality canonical representations trained in an adversarial manner. To the best of our knowledge, we are the first to introduce adversarial training for category-level 6D object pose estimation.</p>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p">2) We propose a Pose-Irrelevant Module and a Relational Reconstruction Module to form the ACR-Pose‚Äôs Reconstructor, through which high-quality canonical-related features and relational features are effectively learned for credible reconstruction.</p>
</div>
<div id="S1.p10" class="ltx_para">
<p id="S1.p10.1" class="ltx_p">3) We conduct extensive experiments to evaluate our model and compare it with previous works, where our model achieves state-of-the-art performance on both synthetic and real world datasets.</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2111.10524/assets/x2.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="452" height="158" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S1.F2.3.2" class="ltx_text" style="font-size:90%;">The pipeline of ACR-Pose. We use the image patch, depth region, and shape prior as inputs at Initialization Phase. We predict the NOCS representation using our Reconstructor. We adopt adversarial training using the Discriminator.</span></figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we briefly review recent methods in 6D object pose estimation. Since our model is a deep learning model, we mainly introduce the deep learning-based counterparts. Then, we present works related to adversarial training.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>6D Object Pose Estimation</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Instance-level 6D object pose estimation only estimates the 6D pose of a particular object can be divided into five parts: direct-methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>, keypoint-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, dense coordinate-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>, refinement based methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> and self-supervised methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. There are also many methods propose to utilize RGBD data as input for instance-level object pose estimation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. For more details, we refer readers to Fan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> for a comprehensive overview. Though promising, generalization of instance-level methods is limited because we have to train different models for different objects even though they belong to the same category. This requires huge computational costs at both training and inference time. In this paper, we explore category-level 6D object pose estimation owing to its better generalization ability towards different objects.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">NOCS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> is the first deep learning-based category-level method. It proposes the Normalized Object Coordinate Space (NOCS) representation to handle huge shape variance between different objects. Then, Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> propose another canonical representation named Canonical Shape Space (CASS). Both NOCS and CASS attract the interests of the following works. For example, SPD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> proposes a shape prior deformation method for accurate NOCS representation reconstruction utilizing a category-prior as an additional model input. Lee et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> propose to simultaneously predict the NOCS representation and the metric-scale object shape from the RGB observation for category-level object pose detection, trying to remove the dependence on the depth information. However, its performance still lags behind RGBD based methods. Recently, DualPoseNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> proposes a Dual-Pose Network to learn pose consistency to improve the pose estimation accuracy, and FS-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> introduces a novel data augmentation method for improving models‚Äô performance. Nevertheless, no matter what efforts they make to improve performance, most of the existing methods adopt the idea of reconstructing the canonical representation in their models.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">Our method also reconstructs the NOCS representation for category-level 6D object pose estimation. We argue that the potential of the NOCS representation has not been fully explored. Therefore, we propose ACR-Pose to improve the reconstruction quality through adversarial training, which will be presented in detail later.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Generative Adversarial Networks (GANs)</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Generative Adversarial Network (GAN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> is widely used in many generation tasks like style translation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, image generation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, human reconstruction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, etc. Usually, GAN consists of a Generator and a Discriminator. The former is the main branch to predict the task goal, while the latter is used to distinguish real sample (ground-truth) and fake sample (output of the generator). The key insight of GAN is to use the Discriminator to guide the Generator through adversarial training. CGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> introduces the conditional version of GANs, which makes the generation process of GANs being controlled. DCGANs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> proposes to use GANs for unsupervised learning with CNNs. WGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> improves the stability of learning, getting rid of problems like mode collapse. CycleGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> proposes the concept of utilizing cycle consistency for training GANs in an unsupervised manner. Recently, there are aslo many works utilizing GANs for 3D vision tasks. For example, Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> introduces PUGAN for point cloud up-sampling. Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> introduces an unsupervised point cloud completion method mainly benefits from GAN inversion. There also exists methods like <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> adopts the idea of GANs to predict dense-coordinates for instance-level 6D object pose estimation.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">In this paper, we propose a novel and effective approach called ACR-Pose. Its Reconstructor taking use of two delicately designed submodules: PIM and RRM, to generate pose insensitive and essential relational information to get accurate reconstruction results. The Reconstructor and the Discriminator are optimized adversarially. As far as we know, we are the first to adopt adversarial training in canonical representation reconstruction.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>ACR-Pose</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we introduce ACR-Pose in detail. We first illustrate the overview of our pipeline (Sec. <a href="#S3.SS1" title="3.1 Pipeline ‚Ä£ 3 ACR-Pose ‚Ä£ ACR-Pose: Adversarial Canonical Representation Reconstruction Network for Category Level 6D Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>). Then we introduce at length the Pose-Irrelevant Module (Sec. <a href="#S3.SS2" title="3.2 Pose-Irrelevant Module in Reconstructor ‚Ä£ 3 ACR-Pose ‚Ä£ ACR-Pose: Adversarial Canonical Representation Reconstruction Network for Category Level 6D Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>) and Relational Reconstruction Module (Sec. <a href="#S3.SS3" title="3.3 Relational Reconstruction Module in Reconstructor ‚Ä£ 3 ACR-Pose ‚Ä£ ACR-Pose: Adversarial Canonical Representation Reconstruction Network for Category Level 6D Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>) respectively. Next, we shortly depict the Shape Prior Deformation step <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> (Sec. <a href="#S3.SS4" title="3.4 Shape Prior Deformation in Reconstructor ‚Ä£ 3 ACR-Pose ‚Ä£ ACR-Pose: Adversarial Canonical Representation Reconstruction Network for Category Level 6D Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>) we used for reconstruction. After that, we describe how the Discriminator and adversarial training (Sec. <a href="#S3.SS5" title="3.5 Discriminator and Adversarial Training ‚Ä£ 3 ACR-Pose ‚Ä£ ACR-Pose: Adversarial Canonical Representation Reconstruction Network for Category Level 6D Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.5</span></a>) improve the reconstruction reality. Finally, we present the loss function.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Pipeline</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In this paper, we aim to reconstruct the 3D canonical representation, for example, NOCS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> representation from a RGBD input. Fig. <a href="#S1.F2" title="Figure 2 ‚Ä£ 1 Introduction ‚Ä£ ACR-Pose: Adversarial Canonical Representation Reconstruction Network for Category Level 6D Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> illustrates the overview of our pipeline. It can be divided into three parts.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">The first part is the initialization phase. A detector is used to get the image patch and depth region of the target object from the RGBD input. The image patch and depth region are the main input of the following networks. Similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, we train an encoder-decoder network to learn a shape prior for each category and take the learned shape prior as one of the following network‚Äôs input. Totally, we have three modalities of input.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.6" class="ltx_p">The second part is the Reconstructor. In this part, the image patch is fed into an Image Encoder backbone to learn instance RGB image features <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="f_{I}\in R^{U\times V\times C}" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><mrow id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml"><msub id="S3.SS1.p3.1.m1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.2.cmml"><mi id="S3.SS1.p3.1.m1.1.1.2.2" xref="S3.SS1.p3.1.m1.1.1.2.2.cmml">f</mi><mi id="S3.SS1.p3.1.m1.1.1.2.3" xref="S3.SS1.p3.1.m1.1.1.2.3.cmml">I</mi></msub><mo id="S3.SS1.p3.1.m1.1.1.1" xref="S3.SS1.p3.1.m1.1.1.1.cmml">‚àà</mo><msup id="S3.SS1.p3.1.m1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.3.cmml"><mi id="S3.SS1.p3.1.m1.1.1.3.2" xref="S3.SS1.p3.1.m1.1.1.3.2.cmml">R</mi><mrow id="S3.SS1.p3.1.m1.1.1.3.3" xref="S3.SS1.p3.1.m1.1.1.3.3.cmml"><mi id="S3.SS1.p3.1.m1.1.1.3.3.2" xref="S3.SS1.p3.1.m1.1.1.3.3.2.cmml">U</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p3.1.m1.1.1.3.3.1" xref="S3.SS1.p3.1.m1.1.1.3.3.1.cmml">√ó</mo><mi id="S3.SS1.p3.1.m1.1.1.3.3.3" xref="S3.SS1.p3.1.m1.1.1.3.3.3.cmml">V</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p3.1.m1.1.1.3.3.1a" xref="S3.SS1.p3.1.m1.1.1.3.3.1.cmml">√ó</mo><mi id="S3.SS1.p3.1.m1.1.1.3.3.4" xref="S3.SS1.p3.1.m1.1.1.3.3.4.cmml">C</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1"><in id="S3.SS1.p3.1.m1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1.1"></in><apply id="S3.SS1.p3.1.m1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.1.2.1.cmml" xref="S3.SS1.p3.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS1.p3.1.m1.1.1.2.2.cmml" xref="S3.SS1.p3.1.m1.1.1.2.2">ùëì</ci><ci id="S3.SS1.p3.1.m1.1.1.2.3.cmml" xref="S3.SS1.p3.1.m1.1.1.2.3">ùêº</ci></apply><apply id="S3.SS1.p3.1.m1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.1.3.1.cmml" xref="S3.SS1.p3.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS1.p3.1.m1.1.1.3.2.cmml" xref="S3.SS1.p3.1.m1.1.1.3.2">ùëÖ</ci><apply id="S3.SS1.p3.1.m1.1.1.3.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3.3"><times id="S3.SS1.p3.1.m1.1.1.3.3.1.cmml" xref="S3.SS1.p3.1.m1.1.1.3.3.1"></times><ci id="S3.SS1.p3.1.m1.1.1.3.3.2.cmml" xref="S3.SS1.p3.1.m1.1.1.3.3.2">ùëà</ci><ci id="S3.SS1.p3.1.m1.1.1.3.3.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3.3.3">ùëâ</ci><ci id="S3.SS1.p3.1.m1.1.1.3.3.4.cmml" xref="S3.SS1.p3.1.m1.1.1.3.3.4">ùê∂</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">f_{I}\in R^{U\times V\times C}</annotation></semantics></math>, where <math id="S3.SS1.p3.2.m2.1" class="ltx_Math" alttext="U" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><mi id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml">U</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><ci id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">ùëà</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">U</annotation></semantics></math> and <math id="S3.SS1.p3.3.m3.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S3.SS1.p3.3.m3.1a"><mi id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><ci id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">ùëâ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">V</annotation></semantics></math> are the image size. The depth region is back-projected into a point cloud (observed points) and fed into the <em id="S3.SS1.p3.6.1" class="ltx_emph ltx_font_italic">PIM</em> to learn canonical-related instance point features <math id="S3.SS1.p3.4.m4.1" class="ltx_Math" alttext="f_{P}\in R^{N_{p}\times C}" display="inline"><semantics id="S3.SS1.p3.4.m4.1a"><mrow id="S3.SS1.p3.4.m4.1.1" xref="S3.SS1.p3.4.m4.1.1.cmml"><msub id="S3.SS1.p3.4.m4.1.1.2" xref="S3.SS1.p3.4.m4.1.1.2.cmml"><mi id="S3.SS1.p3.4.m4.1.1.2.2" xref="S3.SS1.p3.4.m4.1.1.2.2.cmml">f</mi><mi id="S3.SS1.p3.4.m4.1.1.2.3" xref="S3.SS1.p3.4.m4.1.1.2.3.cmml">P</mi></msub><mo id="S3.SS1.p3.4.m4.1.1.1" xref="S3.SS1.p3.4.m4.1.1.1.cmml">‚àà</mo><msup id="S3.SS1.p3.4.m4.1.1.3" xref="S3.SS1.p3.4.m4.1.1.3.cmml"><mi id="S3.SS1.p3.4.m4.1.1.3.2" xref="S3.SS1.p3.4.m4.1.1.3.2.cmml">R</mi><mrow id="S3.SS1.p3.4.m4.1.1.3.3" xref="S3.SS1.p3.4.m4.1.1.3.3.cmml"><msub id="S3.SS1.p3.4.m4.1.1.3.3.2" xref="S3.SS1.p3.4.m4.1.1.3.3.2.cmml"><mi id="S3.SS1.p3.4.m4.1.1.3.3.2.2" xref="S3.SS1.p3.4.m4.1.1.3.3.2.2.cmml">N</mi><mi id="S3.SS1.p3.4.m4.1.1.3.3.2.3" xref="S3.SS1.p3.4.m4.1.1.3.3.2.3.cmml">p</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p3.4.m4.1.1.3.3.1" xref="S3.SS1.p3.4.m4.1.1.3.3.1.cmml">√ó</mo><mi id="S3.SS1.p3.4.m4.1.1.3.3.3" xref="S3.SS1.p3.4.m4.1.1.3.3.3.cmml">C</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m4.1b"><apply id="S3.SS1.p3.4.m4.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1"><in id="S3.SS1.p3.4.m4.1.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1.1"></in><apply id="S3.SS1.p3.4.m4.1.1.2.cmml" xref="S3.SS1.p3.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p3.4.m4.1.1.2.1.cmml" xref="S3.SS1.p3.4.m4.1.1.2">subscript</csymbol><ci id="S3.SS1.p3.4.m4.1.1.2.2.cmml" xref="S3.SS1.p3.4.m4.1.1.2.2">ùëì</ci><ci id="S3.SS1.p3.4.m4.1.1.2.3.cmml" xref="S3.SS1.p3.4.m4.1.1.2.3">ùëÉ</ci></apply><apply id="S3.SS1.p3.4.m4.1.1.3.cmml" xref="S3.SS1.p3.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p3.4.m4.1.1.3.1.cmml" xref="S3.SS1.p3.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS1.p3.4.m4.1.1.3.2.cmml" xref="S3.SS1.p3.4.m4.1.1.3.2">ùëÖ</ci><apply id="S3.SS1.p3.4.m4.1.1.3.3.cmml" xref="S3.SS1.p3.4.m4.1.1.3.3"><times id="S3.SS1.p3.4.m4.1.1.3.3.1.cmml" xref="S3.SS1.p3.4.m4.1.1.3.3.1"></times><apply id="S3.SS1.p3.4.m4.1.1.3.3.2.cmml" xref="S3.SS1.p3.4.m4.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS1.p3.4.m4.1.1.3.3.2.1.cmml" xref="S3.SS1.p3.4.m4.1.1.3.3.2">subscript</csymbol><ci id="S3.SS1.p3.4.m4.1.1.3.3.2.2.cmml" xref="S3.SS1.p3.4.m4.1.1.3.3.2.2">ùëÅ</ci><ci id="S3.SS1.p3.4.m4.1.1.3.3.2.3.cmml" xref="S3.SS1.p3.4.m4.1.1.3.3.2.3">ùëù</ci></apply><ci id="S3.SS1.p3.4.m4.1.1.3.3.3.cmml" xref="S3.SS1.p3.4.m4.1.1.3.3.3">ùê∂</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m4.1c">f_{P}\in R^{N_{p}\times C}</annotation></semantics></math>, where <math id="S3.SS1.p3.5.m5.1" class="ltx_Math" alttext="N_{p}" display="inline"><semantics id="S3.SS1.p3.5.m5.1a"><msub id="S3.SS1.p3.5.m5.1.1" xref="S3.SS1.p3.5.m5.1.1.cmml"><mi id="S3.SS1.p3.5.m5.1.1.2" xref="S3.SS1.p3.5.m5.1.1.2.cmml">N</mi><mi id="S3.SS1.p3.5.m5.1.1.3" xref="S3.SS1.p3.5.m5.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.5.m5.1b"><apply id="S3.SS1.p3.5.m5.1.1.cmml" xref="S3.SS1.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.5.m5.1.1.1.cmml" xref="S3.SS1.p3.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p3.5.m5.1.1.2.cmml" xref="S3.SS1.p3.5.m5.1.1.2">ùëÅ</ci><ci id="S3.SS1.p3.5.m5.1.1.3.cmml" xref="S3.SS1.p3.5.m5.1.1.3">ùëù</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.5.m5.1c">N_{p}</annotation></semantics></math> is the number of observed points. The shape prior is fed into a simple Shape Prior Encoder to learn high-dimensional category prior features <math id="S3.SS1.p3.6.m6.1" class="ltx_Math" alttext="f_{c}\in R^{N_{c}\times C}" display="inline"><semantics id="S3.SS1.p3.6.m6.1a"><mrow id="S3.SS1.p3.6.m6.1.1" xref="S3.SS1.p3.6.m6.1.1.cmml"><msub id="S3.SS1.p3.6.m6.1.1.2" xref="S3.SS1.p3.6.m6.1.1.2.cmml"><mi id="S3.SS1.p3.6.m6.1.1.2.2" xref="S3.SS1.p3.6.m6.1.1.2.2.cmml">f</mi><mi id="S3.SS1.p3.6.m6.1.1.2.3" xref="S3.SS1.p3.6.m6.1.1.2.3.cmml">c</mi></msub><mo id="S3.SS1.p3.6.m6.1.1.1" xref="S3.SS1.p3.6.m6.1.1.1.cmml">‚àà</mo><msup id="S3.SS1.p3.6.m6.1.1.3" xref="S3.SS1.p3.6.m6.1.1.3.cmml"><mi id="S3.SS1.p3.6.m6.1.1.3.2" xref="S3.SS1.p3.6.m6.1.1.3.2.cmml">R</mi><mrow id="S3.SS1.p3.6.m6.1.1.3.3" xref="S3.SS1.p3.6.m6.1.1.3.3.cmml"><msub id="S3.SS1.p3.6.m6.1.1.3.3.2" xref="S3.SS1.p3.6.m6.1.1.3.3.2.cmml"><mi id="S3.SS1.p3.6.m6.1.1.3.3.2.2" xref="S3.SS1.p3.6.m6.1.1.3.3.2.2.cmml">N</mi><mi id="S3.SS1.p3.6.m6.1.1.3.3.2.3" xref="S3.SS1.p3.6.m6.1.1.3.3.2.3.cmml">c</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p3.6.m6.1.1.3.3.1" xref="S3.SS1.p3.6.m6.1.1.3.3.1.cmml">√ó</mo><mi id="S3.SS1.p3.6.m6.1.1.3.3.3" xref="S3.SS1.p3.6.m6.1.1.3.3.3.cmml">C</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.6.m6.1b"><apply id="S3.SS1.p3.6.m6.1.1.cmml" xref="S3.SS1.p3.6.m6.1.1"><in id="S3.SS1.p3.6.m6.1.1.1.cmml" xref="S3.SS1.p3.6.m6.1.1.1"></in><apply id="S3.SS1.p3.6.m6.1.1.2.cmml" xref="S3.SS1.p3.6.m6.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p3.6.m6.1.1.2.1.cmml" xref="S3.SS1.p3.6.m6.1.1.2">subscript</csymbol><ci id="S3.SS1.p3.6.m6.1.1.2.2.cmml" xref="S3.SS1.p3.6.m6.1.1.2.2">ùëì</ci><ci id="S3.SS1.p3.6.m6.1.1.2.3.cmml" xref="S3.SS1.p3.6.m6.1.1.2.3">ùëê</ci></apply><apply id="S3.SS1.p3.6.m6.1.1.3.cmml" xref="S3.SS1.p3.6.m6.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p3.6.m6.1.1.3.1.cmml" xref="S3.SS1.p3.6.m6.1.1.3">superscript</csymbol><ci id="S3.SS1.p3.6.m6.1.1.3.2.cmml" xref="S3.SS1.p3.6.m6.1.1.3.2">ùëÖ</ci><apply id="S3.SS1.p3.6.m6.1.1.3.3.cmml" xref="S3.SS1.p3.6.m6.1.1.3.3"><times id="S3.SS1.p3.6.m6.1.1.3.3.1.cmml" xref="S3.SS1.p3.6.m6.1.1.3.3.1"></times><apply id="S3.SS1.p3.6.m6.1.1.3.3.2.cmml" xref="S3.SS1.p3.6.m6.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS1.p3.6.m6.1.1.3.3.2.1.cmml" xref="S3.SS1.p3.6.m6.1.1.3.3.2">subscript</csymbol><ci id="S3.SS1.p3.6.m6.1.1.3.3.2.2.cmml" xref="S3.SS1.p3.6.m6.1.1.3.3.2.2">ùëÅ</ci><ci id="S3.SS1.p3.6.m6.1.1.3.3.2.3.cmml" xref="S3.SS1.p3.6.m6.1.1.3.3.2.3">ùëê</ci></apply><ci id="S3.SS1.p3.6.m6.1.1.3.3.3.cmml" xref="S3.SS1.p3.6.m6.1.1.3.3.3">ùê∂</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.6.m6.1c">f_{c}\in R^{N_{c}\times C}</annotation></semantics></math>. Then, learned features of the three modalities are integrated and enhanced by the <em id="S3.SS1.p3.6.2" class="ltx_emph ltx_font_italic">RRM</em>, which is another key component of our work. Finally, taking features learned by both modules as input, a Shape Prior Deformation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> step is employed to reconstruct the NOCS representation.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">In the Discriminator, the third part, we utilize an adversarial scheme during training. Specifically, the reconstructed NOCS representation and the ground-truth are fed into a Discriminator. The Discriminator would then judge the realistic level of the reconstructed NOCS presentations as well as the ground-truth, hence guiding and encouraging our Reconstructor to generate NOCS representations as realistic as possible. Thus, the Reconstructor and the Discriminator would compete with each other to become stronger. We call it <em id="S3.SS1.p4.1.1" class="ltx_emph ltx_font_italic">Adversarial Reconstruction</em>.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">At inference time, given the reconstructed NOCS representation and the back-projected observed object depth, we use the Umeyama algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> to recover the 6D object pose. Next, we introduce the PIM and the RRM in the Reconstructor first.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2111.10524/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="438" height="367" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.20.10.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.18.9" class="ltx_text" style="font-size:90%;">Rotation-Invariant Convolution. Each point <math id="S3.F3.10.1.m1.1" class="ltx_Math" alttext="x_{i}" display="inline"><semantics id="S3.F3.10.1.m1.1b"><msub id="S3.F3.10.1.m1.1.1" xref="S3.F3.10.1.m1.1.1.cmml"><mi id="S3.F3.10.1.m1.1.1.2" xref="S3.F3.10.1.m1.1.1.2.cmml">x</mi><mi id="S3.F3.10.1.m1.1.1.3" xref="S3.F3.10.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F3.10.1.m1.1c"><apply id="S3.F3.10.1.m1.1.1.cmml" xref="S3.F3.10.1.m1.1.1"><csymbol cd="ambiguous" id="S3.F3.10.1.m1.1.1.1.cmml" xref="S3.F3.10.1.m1.1.1">subscript</csymbol><ci id="S3.F3.10.1.m1.1.1.2.cmml" xref="S3.F3.10.1.m1.1.1.2">ùë•</ci><ci id="S3.F3.10.1.m1.1.1.3.cmml" xref="S3.F3.10.1.m1.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.10.1.m1.1d">x_{i}</annotation></semantics></math> and its <math id="S3.F3.11.2.m2.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.F3.11.2.m2.1b"><mi id="S3.F3.11.2.m2.1.1" xref="S3.F3.11.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.F3.11.2.m2.1c"><ci id="S3.F3.11.2.m2.1.1.cmml" xref="S3.F3.11.2.m2.1.1">ùêæ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.11.2.m2.1d">K</annotation></semantics></math> nearest points form a local group. First, we compute the mean geometric center (the yellow point) of the group, the nearest point of <math id="S3.F3.12.3.m3.1" class="ltx_Math" alttext="x_{i}" display="inline"><semantics id="S3.F3.12.3.m3.1b"><msub id="S3.F3.12.3.m3.1.1" xref="S3.F3.12.3.m3.1.1.cmml"><mi id="S3.F3.12.3.m3.1.1.2" xref="S3.F3.12.3.m3.1.1.2.cmml">x</mi><mi id="S3.F3.12.3.m3.1.1.3" xref="S3.F3.12.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F3.12.3.m3.1c"><apply id="S3.F3.12.3.m3.1.1.cmml" xref="S3.F3.12.3.m3.1.1"><csymbol cd="ambiguous" id="S3.F3.12.3.m3.1.1.1.cmml" xref="S3.F3.12.3.m3.1.1">subscript</csymbol><ci id="S3.F3.12.3.m3.1.1.2.cmml" xref="S3.F3.12.3.m3.1.1.2">ùë•</ci><ci id="S3.F3.12.3.m3.1.1.3.cmml" xref="S3.F3.12.3.m3.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.12.3.m3.1d">x_{i}</annotation></semantics></math> (the green point), and the farthest point of <math id="S3.F3.13.4.m4.1" class="ltx_Math" alttext="x_{i}" display="inline"><semantics id="S3.F3.13.4.m4.1b"><msub id="S3.F3.13.4.m4.1.1" xref="S3.F3.13.4.m4.1.1.cmml"><mi id="S3.F3.13.4.m4.1.1.2" xref="S3.F3.13.4.m4.1.1.2.cmml">x</mi><mi id="S3.F3.13.4.m4.1.1.3" xref="S3.F3.13.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F3.13.4.m4.1c"><apply id="S3.F3.13.4.m4.1.1.cmml" xref="S3.F3.13.4.m4.1.1"><csymbol cd="ambiguous" id="S3.F3.13.4.m4.1.1.1.cmml" xref="S3.F3.13.4.m4.1.1">subscript</csymbol><ci id="S3.F3.13.4.m4.1.1.2.cmml" xref="S3.F3.13.4.m4.1.1.2">ùë•</ci><ci id="S3.F3.13.4.m4.1.1.3.cmml" xref="S3.F3.13.4.m4.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.13.4.m4.1d">x_{i}</annotation></semantics></math> (the purple point). Then, for each point <math id="S3.F3.14.5.m5.1" class="ltx_Math" alttext="x_{ij}" display="inline"><semantics id="S3.F3.14.5.m5.1b"><msub id="S3.F3.14.5.m5.1.1" xref="S3.F3.14.5.m5.1.1.cmml"><mi id="S3.F3.14.5.m5.1.1.2" xref="S3.F3.14.5.m5.1.1.2.cmml">x</mi><mrow id="S3.F3.14.5.m5.1.1.3" xref="S3.F3.14.5.m5.1.1.3.cmml"><mi id="S3.F3.14.5.m5.1.1.3.2" xref="S3.F3.14.5.m5.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.F3.14.5.m5.1.1.3.1" xref="S3.F3.14.5.m5.1.1.3.1.cmml">‚Äã</mo><mi id="S3.F3.14.5.m5.1.1.3.3" xref="S3.F3.14.5.m5.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.F3.14.5.m5.1c"><apply id="S3.F3.14.5.m5.1.1.cmml" xref="S3.F3.14.5.m5.1.1"><csymbol cd="ambiguous" id="S3.F3.14.5.m5.1.1.1.cmml" xref="S3.F3.14.5.m5.1.1">subscript</csymbol><ci id="S3.F3.14.5.m5.1.1.2.cmml" xref="S3.F3.14.5.m5.1.1.2">ùë•</ci><apply id="S3.F3.14.5.m5.1.1.3.cmml" xref="S3.F3.14.5.m5.1.1.3"><times id="S3.F3.14.5.m5.1.1.3.1.cmml" xref="S3.F3.14.5.m5.1.1.3.1"></times><ci id="S3.F3.14.5.m5.1.1.3.2.cmml" xref="S3.F3.14.5.m5.1.1.3.2">ùëñ</ci><ci id="S3.F3.14.5.m5.1.1.3.3.cmml" xref="S3.F3.14.5.m5.1.1.3.3">ùëó</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.14.5.m5.1d">x_{ij}</annotation></semantics></math> in this group, nine low-level rotation-invariant geometry features (angles <math id="S3.F3.15.6.m6.1" class="ltx_Math" alttext="a1" display="inline"><semantics id="S3.F3.15.6.m6.1b"><mrow id="S3.F3.15.6.m6.1.1" xref="S3.F3.15.6.m6.1.1.cmml"><mi id="S3.F3.15.6.m6.1.1.2" xref="S3.F3.15.6.m6.1.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.F3.15.6.m6.1.1.1" xref="S3.F3.15.6.m6.1.1.1.cmml">‚Äã</mo><mn id="S3.F3.15.6.m6.1.1.3" xref="S3.F3.15.6.m6.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.F3.15.6.m6.1c"><apply id="S3.F3.15.6.m6.1.1.cmml" xref="S3.F3.15.6.m6.1.1"><times id="S3.F3.15.6.m6.1.1.1.cmml" xref="S3.F3.15.6.m6.1.1.1"></times><ci id="S3.F3.15.6.m6.1.1.2.cmml" xref="S3.F3.15.6.m6.1.1.2">ùëé</ci><cn type="integer" id="S3.F3.15.6.m6.1.1.3.cmml" xref="S3.F3.15.6.m6.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.15.6.m6.1d">a1</annotation></semantics></math> to <math id="S3.F3.16.7.m7.1" class="ltx_Math" alttext="a4" display="inline"><semantics id="S3.F3.16.7.m7.1b"><mrow id="S3.F3.16.7.m7.1.1" xref="S3.F3.16.7.m7.1.1.cmml"><mi id="S3.F3.16.7.m7.1.1.2" xref="S3.F3.16.7.m7.1.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.F3.16.7.m7.1.1.1" xref="S3.F3.16.7.m7.1.1.1.cmml">‚Äã</mo><mn id="S3.F3.16.7.m7.1.1.3" xref="S3.F3.16.7.m7.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.F3.16.7.m7.1c"><apply id="S3.F3.16.7.m7.1.1.cmml" xref="S3.F3.16.7.m7.1.1"><times id="S3.F3.16.7.m7.1.1.1.cmml" xref="S3.F3.16.7.m7.1.1.1"></times><ci id="S3.F3.16.7.m7.1.1.2.cmml" xref="S3.F3.16.7.m7.1.1.2">ùëé</ci><cn type="integer" id="S3.F3.16.7.m7.1.1.3.cmml" xref="S3.F3.16.7.m7.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.16.7.m7.1d">a4</annotation></semantics></math> and distances <math id="S3.F3.17.8.m8.1" class="ltx_Math" alttext="d1" display="inline"><semantics id="S3.F3.17.8.m8.1b"><mrow id="S3.F3.17.8.m8.1.1" xref="S3.F3.17.8.m8.1.1.cmml"><mi id="S3.F3.17.8.m8.1.1.2" xref="S3.F3.17.8.m8.1.1.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.F3.17.8.m8.1.1.1" xref="S3.F3.17.8.m8.1.1.1.cmml">‚Äã</mo><mn id="S3.F3.17.8.m8.1.1.3" xref="S3.F3.17.8.m8.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.F3.17.8.m8.1c"><apply id="S3.F3.17.8.m8.1.1.cmml" xref="S3.F3.17.8.m8.1.1"><times id="S3.F3.17.8.m8.1.1.1.cmml" xref="S3.F3.17.8.m8.1.1.1"></times><ci id="S3.F3.17.8.m8.1.1.2.cmml" xref="S3.F3.17.8.m8.1.1.2">ùëë</ci><cn type="integer" id="S3.F3.17.8.m8.1.1.3.cmml" xref="S3.F3.17.8.m8.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.17.8.m8.1d">d1</annotation></semantics></math> to <math id="S3.F3.18.9.m9.1" class="ltx_Math" alttext="d5" display="inline"><semantics id="S3.F3.18.9.m9.1b"><mrow id="S3.F3.18.9.m9.1.1" xref="S3.F3.18.9.m9.1.1.cmml"><mi id="S3.F3.18.9.m9.1.1.2" xref="S3.F3.18.9.m9.1.1.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.F3.18.9.m9.1.1.1" xref="S3.F3.18.9.m9.1.1.1.cmml">‚Äã</mo><mn id="S3.F3.18.9.m9.1.1.3" xref="S3.F3.18.9.m9.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.F3.18.9.m9.1c"><apply id="S3.F3.18.9.m9.1.1.cmml" xref="S3.F3.18.9.m9.1.1"><times id="S3.F3.18.9.m9.1.1.1.cmml" xref="S3.F3.18.9.m9.1.1.1"></times><ci id="S3.F3.18.9.m9.1.1.2.cmml" xref="S3.F3.18.9.m9.1.1.2">ùëë</ci><cn type="integer" id="S3.F3.18.9.m9.1.1.3.cmml" xref="S3.F3.18.9.m9.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.18.9.m9.1d">d5</annotation></semantics></math>) and two spherical signals are calculated and utilized to learn the rotation-invariant convolutional kernels. Finally, a PointConv operation is employed to apply the learned kernels to the input point-wise features for feature updating.</span></figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Pose-Irrelevant Module in Reconstructor</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The Reconstructor takes the back-projected object depth as one of its main inputs.
A normalization filter is occupied to eliminate the impact of translation and a Rotation-Invariant Network is utilized to filter the impact of rotation. These two modules together comprise the PIM.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.4" class="ltx_p">Specifically, denoting the back-projected point cloud of the object as <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="P" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">ùëÉ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">P</annotation></semantics></math>, we first filter out the impact of the translation by subtracting its geometric center, i.e. <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="\hat{P}=P-\overline{P}" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mrow id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mover accent="true" id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2.2" xref="S3.SS2.p2.2.m2.1.1.2.2.cmml">P</mi><mo id="S3.SS2.p2.2.m2.1.1.2.1" xref="S3.SS2.p2.2.m2.1.1.2.1.cmml">^</mo></mover><mo id="S3.SS2.p2.2.m2.1.1.1" xref="S3.SS2.p2.2.m2.1.1.1.cmml">=</mo><mrow id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml"><mi id="S3.SS2.p2.2.m2.1.1.3.2" xref="S3.SS2.p2.2.m2.1.1.3.2.cmml">P</mi><mo id="S3.SS2.p2.2.m2.1.1.3.1" xref="S3.SS2.p2.2.m2.1.1.3.1.cmml">‚àí</mo><mover accent="true" id="S3.SS2.p2.2.m2.1.1.3.3" xref="S3.SS2.p2.2.m2.1.1.3.3.cmml"><mi id="S3.SS2.p2.2.m2.1.1.3.3.2" xref="S3.SS2.p2.2.m2.1.1.3.3.2.cmml">P</mi><mo id="S3.SS2.p2.2.m2.1.1.3.3.1" xref="S3.SS2.p2.2.m2.1.1.3.3.1.cmml">¬Ø</mo></mover></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><eq id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1"></eq><apply id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2"><ci id="S3.SS2.p2.2.m2.1.1.2.1.cmml" xref="S3.SS2.p2.2.m2.1.1.2.1">^</ci><ci id="S3.SS2.p2.2.m2.1.1.2.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2.2">ùëÉ</ci></apply><apply id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3"><minus id="S3.SS2.p2.2.m2.1.1.3.1.cmml" xref="S3.SS2.p2.2.m2.1.1.3.1"></minus><ci id="S3.SS2.p2.2.m2.1.1.3.2.cmml" xref="S3.SS2.p2.2.m2.1.1.3.2">ùëÉ</ci><apply id="S3.SS2.p2.2.m2.1.1.3.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3.3"><ci id="S3.SS2.p2.2.m2.1.1.3.3.1.cmml" xref="S3.SS2.p2.2.m2.1.1.3.3.1">¬Ø</ci><ci id="S3.SS2.p2.2.m2.1.1.3.3.2.cmml" xref="S3.SS2.p2.2.m2.1.1.3.3.2">ùëÉ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">\hat{P}=P-\overline{P}</annotation></semantics></math>, where <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="\overline{P}" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><mover accent="true" id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml"><mi id="S3.SS2.p2.3.m3.1.1.2" xref="S3.SS2.p2.3.m3.1.1.2.cmml">P</mi><mo id="S3.SS2.p2.3.m3.1.1.1" xref="S3.SS2.p2.3.m3.1.1.1.cmml">¬Ø</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"><ci id="S3.SS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1.1">¬Ø</ci><ci id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2">ùëÉ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">\overline{P}</annotation></semantics></math> is the mean geometric center. Then, we input <math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="\hat{P}" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><mover accent="true" id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml"><mi id="S3.SS2.p2.4.m4.1.1.2" xref="S3.SS2.p2.4.m4.1.1.2.cmml">P</mi><mo id="S3.SS2.p2.4.m4.1.1.1" xref="S3.SS2.p2.4.m4.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><apply id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1"><ci id="S3.SS2.p2.4.m4.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1.1">^</ci><ci id="S3.SS2.p2.4.m4.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1.2">ùëÉ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">\hat{P}</annotation></semantics></math> into 4 layers of Rotation-Invariant Convolution for learning rotation-invariant features, called canonical-related features. The Rotation-Invariant Convolution is illustrated in Fig. <a href="#S3.F3" title="Figure 3 ‚Ä£ 3.1 Pipeline ‚Ä£ 3 ACR-Pose ‚Ä£ ACR-Pose: Adversarial Canonical Representation Reconstruction Network for Category Level 6D Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, given the input point cloud and its corresponding point-wise features as input, we choose nine low-level rotation-invariant geometry features (see Fig. <a href="#S3.F3" title="Figure 3 ‚Ä£ 3.1 Pipeline ‚Ä£ 3 ACR-Pose ‚Ä£ ACR-Pose: Adversarial Canonical Representation Reconstruction Network for Category Level 6D Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> middle) as well as two spherical signals <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> to learn rotation-invariant convolutional kernels using a MLP and a SE-block <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. Then we use a PointConv <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> to apply learned kernels to point cloud features for feature update.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.2" class="ltx_p">The output of the PIM is <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="f_{P}\in R^{N_{p}\times C}" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mrow id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml"><msub id="S3.SS2.p3.1.m1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.2.cmml"><mi id="S3.SS2.p3.1.m1.1.1.2.2" xref="S3.SS2.p3.1.m1.1.1.2.2.cmml">f</mi><mi id="S3.SS2.p3.1.m1.1.1.2.3" xref="S3.SS2.p3.1.m1.1.1.2.3.cmml">P</mi></msub><mo id="S3.SS2.p3.1.m1.1.1.1" xref="S3.SS2.p3.1.m1.1.1.1.cmml">‚àà</mo><msup id="S3.SS2.p3.1.m1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.3.cmml"><mi id="S3.SS2.p3.1.m1.1.1.3.2" xref="S3.SS2.p3.1.m1.1.1.3.2.cmml">R</mi><mrow id="S3.SS2.p3.1.m1.1.1.3.3" xref="S3.SS2.p3.1.m1.1.1.3.3.cmml"><msub id="S3.SS2.p3.1.m1.1.1.3.3.2" xref="S3.SS2.p3.1.m1.1.1.3.3.2.cmml"><mi id="S3.SS2.p3.1.m1.1.1.3.3.2.2" xref="S3.SS2.p3.1.m1.1.1.3.3.2.2.cmml">N</mi><mi id="S3.SS2.p3.1.m1.1.1.3.3.2.3" xref="S3.SS2.p3.1.m1.1.1.3.3.2.3.cmml">p</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p3.1.m1.1.1.3.3.1" xref="S3.SS2.p3.1.m1.1.1.3.3.1.cmml">√ó</mo><mi id="S3.SS2.p3.1.m1.1.1.3.3.3" xref="S3.SS2.p3.1.m1.1.1.3.3.3.cmml">C</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><in id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1.1"></in><apply id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.1.1.2.1.cmml" xref="S3.SS2.p3.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS2.p3.1.m1.1.1.2.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2.2">ùëì</ci><ci id="S3.SS2.p3.1.m1.1.1.2.3.cmml" xref="S3.SS2.p3.1.m1.1.1.2.3">ùëÉ</ci></apply><apply id="S3.SS2.p3.1.m1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.1.1.3.1.cmml" xref="S3.SS2.p3.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS2.p3.1.m1.1.1.3.2.cmml" xref="S3.SS2.p3.1.m1.1.1.3.2">ùëÖ</ci><apply id="S3.SS2.p3.1.m1.1.1.3.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3.3"><times id="S3.SS2.p3.1.m1.1.1.3.3.1.cmml" xref="S3.SS2.p3.1.m1.1.1.3.3.1"></times><apply id="S3.SS2.p3.1.m1.1.1.3.3.2.cmml" xref="S3.SS2.p3.1.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.1.1.3.3.2.1.cmml" xref="S3.SS2.p3.1.m1.1.1.3.3.2">subscript</csymbol><ci id="S3.SS2.p3.1.m1.1.1.3.3.2.2.cmml" xref="S3.SS2.p3.1.m1.1.1.3.3.2.2">ùëÅ</ci><ci id="S3.SS2.p3.1.m1.1.1.3.3.2.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3.3.2.3">ùëù</ci></apply><ci id="S3.SS2.p3.1.m1.1.1.3.3.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3.3.3">ùê∂</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">f_{P}\in R^{N_{p}\times C}</annotation></semantics></math>, where <math id="S3.SS2.p3.2.m2.1" class="ltx_Math" alttext="N_{p}" display="inline"><semantics id="S3.SS2.p3.2.m2.1a"><msub id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml"><mi id="S3.SS2.p3.2.m2.1.1.2" xref="S3.SS2.p3.2.m2.1.1.2.cmml">N</mi><mi id="S3.SS2.p3.2.m2.1.1.3" xref="S3.SS2.p3.2.m2.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><apply id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p3.2.m2.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1.2">ùëÅ</ci><ci id="S3.SS2.p3.2.m2.1.1.3.cmml" xref="S3.SS2.p3.2.m2.1.1.3">ùëù</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">N_{p}</annotation></semantics></math> is the number of observed points in the point cloud or the depth region. It will be incorporated with features of the other two modalities to achieve the NOCS representation reconstruction task.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Relational Reconstruction Module in Reconstructor</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">As mentioned before, the Reconstructor takes three modalities of data as input, they all contribute to reconstructing the NOCS representation. Therefore, to learn high-quality features for high-quality NOCS representation reconstruction, we propose the RRM, which consists of three different graph convolutional networks to learn and integrate three kinds of relational features in the feature space. Details of the three kinds of relational features are below:</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.10" class="ltx_p"><span id="S3.SS3.p2.10.1" class="ltx_text ltx_font_bold">Relational instance feature</span> is the point-wise feature of the observed object, including the texture feature and the geometry feature. To learn it, we first index each point‚Äôs point feature and RGB image feature from <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="f_{P}" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><msub id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mi id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">f</mi><mi id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml">P</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">ùëì</ci><ci id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3">ùëÉ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">f_{P}</annotation></semantics></math> and <math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="f_{I}" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><msub id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml"><mi id="S3.SS3.p2.2.m2.1.1.2" xref="S3.SS3.p2.2.m2.1.1.2.cmml">f</mi><mi id="S3.SS3.p2.2.m2.1.1.3" xref="S3.SS3.p2.2.m2.1.1.3.cmml">I</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><apply id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p2.2.m2.1.1.2.cmml" xref="S3.SS3.p2.2.m2.1.1.2">ùëì</ci><ci id="S3.SS3.p2.2.m2.1.1.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3">ùêº</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">f_{I}</annotation></semantics></math>, and concatenate them to form a <math id="S3.SS3.p2.3.m3.1" class="ltx_Math" alttext="N_{p}\times 2C" display="inline"><semantics id="S3.SS3.p2.3.m3.1a"><mrow id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml"><mrow id="S3.SS3.p2.3.m3.1.1.2" xref="S3.SS3.p2.3.m3.1.1.2.cmml"><msub id="S3.SS3.p2.3.m3.1.1.2.2" xref="S3.SS3.p2.3.m3.1.1.2.2.cmml"><mi id="S3.SS3.p2.3.m3.1.1.2.2.2" xref="S3.SS3.p2.3.m3.1.1.2.2.2.cmml">N</mi><mi id="S3.SS3.p2.3.m3.1.1.2.2.3" xref="S3.SS3.p2.3.m3.1.1.2.2.3.cmml">p</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p2.3.m3.1.1.2.1" xref="S3.SS3.p2.3.m3.1.1.2.1.cmml">√ó</mo><mn id="S3.SS3.p2.3.m3.1.1.2.3" xref="S3.SS3.p2.3.m3.1.1.2.3.cmml">2</mn></mrow><mo lspace="0em" rspace="0em" id="S3.SS3.p2.3.m3.1.1.1" xref="S3.SS3.p2.3.m3.1.1.1.cmml">‚Äã</mo><mi id="S3.SS3.p2.3.m3.1.1.3" xref="S3.SS3.p2.3.m3.1.1.3.cmml">C</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><apply id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1"><times id="S3.SS3.p2.3.m3.1.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1.1"></times><apply id="S3.SS3.p2.3.m3.1.1.2.cmml" xref="S3.SS3.p2.3.m3.1.1.2"><times id="S3.SS3.p2.3.m3.1.1.2.1.cmml" xref="S3.SS3.p2.3.m3.1.1.2.1"></times><apply id="S3.SS3.p2.3.m3.1.1.2.2.cmml" xref="S3.SS3.p2.3.m3.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.1.1.2.2.1.cmml" xref="S3.SS3.p2.3.m3.1.1.2.2">subscript</csymbol><ci id="S3.SS3.p2.3.m3.1.1.2.2.2.cmml" xref="S3.SS3.p2.3.m3.1.1.2.2.2">ùëÅ</ci><ci id="S3.SS3.p2.3.m3.1.1.2.2.3.cmml" xref="S3.SS3.p2.3.m3.1.1.2.2.3">ùëù</ci></apply><cn type="integer" id="S3.SS3.p2.3.m3.1.1.2.3.cmml" xref="S3.SS3.p2.3.m3.1.1.2.3">2</cn></apply><ci id="S3.SS3.p2.3.m3.1.1.3.cmml" xref="S3.SS3.p2.3.m3.1.1.3">ùê∂</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">N_{p}\times 2C</annotation></semantics></math> features matrix. A MLP is used to reduce the feature dimension to <math id="S3.SS3.p2.4.m4.1" class="ltx_Math" alttext="N_{p}\times C" display="inline"><semantics id="S3.SS3.p2.4.m4.1a"><mrow id="S3.SS3.p2.4.m4.1.1" xref="S3.SS3.p2.4.m4.1.1.cmml"><msub id="S3.SS3.p2.4.m4.1.1.2" xref="S3.SS3.p2.4.m4.1.1.2.cmml"><mi id="S3.SS3.p2.4.m4.1.1.2.2" xref="S3.SS3.p2.4.m4.1.1.2.2.cmml">N</mi><mi id="S3.SS3.p2.4.m4.1.1.2.3" xref="S3.SS3.p2.4.m4.1.1.2.3.cmml">p</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p2.4.m4.1.1.1" xref="S3.SS3.p2.4.m4.1.1.1.cmml">√ó</mo><mi id="S3.SS3.p2.4.m4.1.1.3" xref="S3.SS3.p2.4.m4.1.1.3.cmml">C</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.1b"><apply id="S3.SS3.p2.4.m4.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1"><times id="S3.SS3.p2.4.m4.1.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1.1"></times><apply id="S3.SS3.p2.4.m4.1.1.2.cmml" xref="S3.SS3.p2.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p2.4.m4.1.1.2.1.cmml" xref="S3.SS3.p2.4.m4.1.1.2">subscript</csymbol><ci id="S3.SS3.p2.4.m4.1.1.2.2.cmml" xref="S3.SS3.p2.4.m4.1.1.2.2">ùëÅ</ci><ci id="S3.SS3.p2.4.m4.1.1.2.3.cmml" xref="S3.SS3.p2.4.m4.1.1.2.3">ùëù</ci></apply><ci id="S3.SS3.p2.4.m4.1.1.3.cmml" xref="S3.SS3.p2.4.m4.1.1.3">ùê∂</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.1c">N_{p}\times C</annotation></semantics></math>. Then, we construct a graph <math id="S3.SS3.p2.5.m5.2" class="ltx_Math" alttext="G(V,E)" display="inline"><semantics id="S3.SS3.p2.5.m5.2a"><mrow id="S3.SS3.p2.5.m5.2.3" xref="S3.SS3.p2.5.m5.2.3.cmml"><mi id="S3.SS3.p2.5.m5.2.3.2" xref="S3.SS3.p2.5.m5.2.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.5.m5.2.3.1" xref="S3.SS3.p2.5.m5.2.3.1.cmml">‚Äã</mo><mrow id="S3.SS3.p2.5.m5.2.3.3.2" xref="S3.SS3.p2.5.m5.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS3.p2.5.m5.2.3.3.2.1" xref="S3.SS3.p2.5.m5.2.3.3.1.cmml">(</mo><mi id="S3.SS3.p2.5.m5.1.1" xref="S3.SS3.p2.5.m5.1.1.cmml">V</mi><mo id="S3.SS3.p2.5.m5.2.3.3.2.2" xref="S3.SS3.p2.5.m5.2.3.3.1.cmml">,</mo><mi id="S3.SS3.p2.5.m5.2.2" xref="S3.SS3.p2.5.m5.2.2.cmml">E</mi><mo stretchy="false" id="S3.SS3.p2.5.m5.2.3.3.2.3" xref="S3.SS3.p2.5.m5.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.5.m5.2b"><apply id="S3.SS3.p2.5.m5.2.3.cmml" xref="S3.SS3.p2.5.m5.2.3"><times id="S3.SS3.p2.5.m5.2.3.1.cmml" xref="S3.SS3.p2.5.m5.2.3.1"></times><ci id="S3.SS3.p2.5.m5.2.3.2.cmml" xref="S3.SS3.p2.5.m5.2.3.2">ùê∫</ci><interval closure="open" id="S3.SS3.p2.5.m5.2.3.3.1.cmml" xref="S3.SS3.p2.5.m5.2.3.3.2"><ci id="S3.SS3.p2.5.m5.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1">ùëâ</ci><ci id="S3.SS3.p2.5.m5.2.2.cmml" xref="S3.SS3.p2.5.m5.2.2">ùê∏</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m5.2c">G(V,E)</annotation></semantics></math> in the feature space using the feature matrix, where <math id="S3.SS3.p2.6.m6.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S3.SS3.p2.6.m6.1a"><mi id="S3.SS3.p2.6.m6.1.1" xref="S3.SS3.p2.6.m6.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.6.m6.1b"><ci id="S3.SS3.p2.6.m6.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1">ùëâ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.6.m6.1c">V</annotation></semantics></math> represents nodes in the graph, a node means an observed point, and <math id="S3.SS3.p2.7.m7.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S3.SS3.p2.7.m7.1a"><mi id="S3.SS3.p2.7.m7.1.1" xref="S3.SS3.p2.7.m7.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.7.m7.1b"><ci id="S3.SS3.p2.7.m7.1.1.cmml" xref="S3.SS3.p2.7.m7.1.1">ùê∏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.7.m7.1c">E</annotation></semantics></math> represents edges. The adjacent relation in the graph is built using the K-Nearest-Neighbors (KNNs) algorithm. The KNN algorithm would first compute the pair-wise distance in the feature space for each point pair. Then, it chooses each point‚Äôs <math id="S3.SS3.p2.8.m8.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS3.p2.8.m8.1a"><mi id="S3.SS3.p2.8.m8.1.1" xref="S3.SS3.p2.8.m8.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.8.m8.1b"><ci id="S3.SS3.p2.8.m8.1.1.cmml" xref="S3.SS3.p2.8.m8.1.1">ùêæ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.8.m8.1c">K</annotation></semantics></math> nearest neighbors to build adjacent edges. Next, an EdgeConv <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> operation is adopted to learn features taking the graph as input. Finally, we use another MLP to increase the per-point feature dimension back to <math id="S3.SS3.p2.9.m9.1" class="ltx_Math" alttext="2C" display="inline"><semantics id="S3.SS3.p2.9.m9.1a"><mrow id="S3.SS3.p2.9.m9.1.1" xref="S3.SS3.p2.9.m9.1.1.cmml"><mn id="S3.SS3.p2.9.m9.1.1.2" xref="S3.SS3.p2.9.m9.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.SS3.p2.9.m9.1.1.1" xref="S3.SS3.p2.9.m9.1.1.1.cmml">‚Äã</mo><mi id="S3.SS3.p2.9.m9.1.1.3" xref="S3.SS3.p2.9.m9.1.1.3.cmml">C</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.9.m9.1b"><apply id="S3.SS3.p2.9.m9.1.1.cmml" xref="S3.SS3.p2.9.m9.1.1"><times id="S3.SS3.p2.9.m9.1.1.1.cmml" xref="S3.SS3.p2.9.m9.1.1.1"></times><cn type="integer" id="S3.SS3.p2.9.m9.1.1.2.cmml" xref="S3.SS3.p2.9.m9.1.1.2">2</cn><ci id="S3.SS3.p2.9.m9.1.1.3.cmml" xref="S3.SS3.p2.9.m9.1.1.3">ùê∂</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.9.m9.1c">2C</annotation></semantics></math>. We denote the relational instance feature as <math id="S3.SS3.p2.10.m10.1" class="ltx_Math" alttext="f_{ins}\in R^{N_{p}\times 2C}" display="inline"><semantics id="S3.SS3.p2.10.m10.1a"><mrow id="S3.SS3.p2.10.m10.1.1" xref="S3.SS3.p2.10.m10.1.1.cmml"><msub id="S3.SS3.p2.10.m10.1.1.2" xref="S3.SS3.p2.10.m10.1.1.2.cmml"><mi id="S3.SS3.p2.10.m10.1.1.2.2" xref="S3.SS3.p2.10.m10.1.1.2.2.cmml">f</mi><mrow id="S3.SS3.p2.10.m10.1.1.2.3" xref="S3.SS3.p2.10.m10.1.1.2.3.cmml"><mi id="S3.SS3.p2.10.m10.1.1.2.3.2" xref="S3.SS3.p2.10.m10.1.1.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.10.m10.1.1.2.3.1" xref="S3.SS3.p2.10.m10.1.1.2.3.1.cmml">‚Äã</mo><mi id="S3.SS3.p2.10.m10.1.1.2.3.3" xref="S3.SS3.p2.10.m10.1.1.2.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.10.m10.1.1.2.3.1a" xref="S3.SS3.p2.10.m10.1.1.2.3.1.cmml">‚Äã</mo><mi id="S3.SS3.p2.10.m10.1.1.2.3.4" xref="S3.SS3.p2.10.m10.1.1.2.3.4.cmml">s</mi></mrow></msub><mo id="S3.SS3.p2.10.m10.1.1.1" xref="S3.SS3.p2.10.m10.1.1.1.cmml">‚àà</mo><msup id="S3.SS3.p2.10.m10.1.1.3" xref="S3.SS3.p2.10.m10.1.1.3.cmml"><mi id="S3.SS3.p2.10.m10.1.1.3.2" xref="S3.SS3.p2.10.m10.1.1.3.2.cmml">R</mi><mrow id="S3.SS3.p2.10.m10.1.1.3.3" xref="S3.SS3.p2.10.m10.1.1.3.3.cmml"><mrow id="S3.SS3.p2.10.m10.1.1.3.3.2" xref="S3.SS3.p2.10.m10.1.1.3.3.2.cmml"><msub id="S3.SS3.p2.10.m10.1.1.3.3.2.2" xref="S3.SS3.p2.10.m10.1.1.3.3.2.2.cmml"><mi id="S3.SS3.p2.10.m10.1.1.3.3.2.2.2" xref="S3.SS3.p2.10.m10.1.1.3.3.2.2.2.cmml">N</mi><mi id="S3.SS3.p2.10.m10.1.1.3.3.2.2.3" xref="S3.SS3.p2.10.m10.1.1.3.3.2.2.3.cmml">p</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p2.10.m10.1.1.3.3.2.1" xref="S3.SS3.p2.10.m10.1.1.3.3.2.1.cmml">√ó</mo><mn id="S3.SS3.p2.10.m10.1.1.3.3.2.3" xref="S3.SS3.p2.10.m10.1.1.3.3.2.3.cmml">2</mn></mrow><mo lspace="0em" rspace="0em" id="S3.SS3.p2.10.m10.1.1.3.3.1" xref="S3.SS3.p2.10.m10.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.SS3.p2.10.m10.1.1.3.3.3" xref="S3.SS3.p2.10.m10.1.1.3.3.3.cmml">C</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.10.m10.1b"><apply id="S3.SS3.p2.10.m10.1.1.cmml" xref="S3.SS3.p2.10.m10.1.1"><in id="S3.SS3.p2.10.m10.1.1.1.cmml" xref="S3.SS3.p2.10.m10.1.1.1"></in><apply id="S3.SS3.p2.10.m10.1.1.2.cmml" xref="S3.SS3.p2.10.m10.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p2.10.m10.1.1.2.1.cmml" xref="S3.SS3.p2.10.m10.1.1.2">subscript</csymbol><ci id="S3.SS3.p2.10.m10.1.1.2.2.cmml" xref="S3.SS3.p2.10.m10.1.1.2.2">ùëì</ci><apply id="S3.SS3.p2.10.m10.1.1.2.3.cmml" xref="S3.SS3.p2.10.m10.1.1.2.3"><times id="S3.SS3.p2.10.m10.1.1.2.3.1.cmml" xref="S3.SS3.p2.10.m10.1.1.2.3.1"></times><ci id="S3.SS3.p2.10.m10.1.1.2.3.2.cmml" xref="S3.SS3.p2.10.m10.1.1.2.3.2">ùëñ</ci><ci id="S3.SS3.p2.10.m10.1.1.2.3.3.cmml" xref="S3.SS3.p2.10.m10.1.1.2.3.3">ùëõ</ci><ci id="S3.SS3.p2.10.m10.1.1.2.3.4.cmml" xref="S3.SS3.p2.10.m10.1.1.2.3.4">ùë†</ci></apply></apply><apply id="S3.SS3.p2.10.m10.1.1.3.cmml" xref="S3.SS3.p2.10.m10.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p2.10.m10.1.1.3.1.cmml" xref="S3.SS3.p2.10.m10.1.1.3">superscript</csymbol><ci id="S3.SS3.p2.10.m10.1.1.3.2.cmml" xref="S3.SS3.p2.10.m10.1.1.3.2">ùëÖ</ci><apply id="S3.SS3.p2.10.m10.1.1.3.3.cmml" xref="S3.SS3.p2.10.m10.1.1.3.3"><times id="S3.SS3.p2.10.m10.1.1.3.3.1.cmml" xref="S3.SS3.p2.10.m10.1.1.3.3.1"></times><apply id="S3.SS3.p2.10.m10.1.1.3.3.2.cmml" xref="S3.SS3.p2.10.m10.1.1.3.3.2"><times id="S3.SS3.p2.10.m10.1.1.3.3.2.1.cmml" xref="S3.SS3.p2.10.m10.1.1.3.3.2.1"></times><apply id="S3.SS3.p2.10.m10.1.1.3.3.2.2.cmml" xref="S3.SS3.p2.10.m10.1.1.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS3.p2.10.m10.1.1.3.3.2.2.1.cmml" xref="S3.SS3.p2.10.m10.1.1.3.3.2.2">subscript</csymbol><ci id="S3.SS3.p2.10.m10.1.1.3.3.2.2.2.cmml" xref="S3.SS3.p2.10.m10.1.1.3.3.2.2.2">ùëÅ</ci><ci id="S3.SS3.p2.10.m10.1.1.3.3.2.2.3.cmml" xref="S3.SS3.p2.10.m10.1.1.3.3.2.2.3">ùëù</ci></apply><cn type="integer" id="S3.SS3.p2.10.m10.1.1.3.3.2.3.cmml" xref="S3.SS3.p2.10.m10.1.1.3.3.2.3">2</cn></apply><ci id="S3.SS3.p2.10.m10.1.1.3.3.3.cmml" xref="S3.SS3.p2.10.m10.1.1.3.3.3">ùê∂</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.10.m10.1c">f_{ins}\in R^{N_{p}\times 2C}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.8" class="ltx_p"><span id="S3.SS3.p3.8.1" class="ltx_text ltx_font_bold">Relational deformation feature</span> is the second kind of feature our network learns. It is mainly used to deform the shape prior into a canonical instance model of the object in the Shape Prior Deformation step (detailed in Sec <a href="#S3.SS4" title="3.4 Shape Prior Deformation in Reconstructor ‚Ä£ 3 ACR-Pose ‚Ä£ ACR-Pose: Adversarial Canonical Representation Reconstruction Network for Category Level 6D Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>). To achieve this goal, we should make the feature be able to implicitly reflect the relational context between the shape prior and the observed object. Therefore, as before, we use a graph network to learn this kind of relational information. Specifically, we use a MLP and an adaptive average pooling operation to embed the relational instance feature <math id="S3.SS3.p3.1.m1.1" class="ltx_Math" alttext="f_{ins}" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><msub id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml"><mi id="S3.SS3.p3.1.m1.1.1.2" xref="S3.SS3.p3.1.m1.1.1.2.cmml">f</mi><mrow id="S3.SS3.p3.1.m1.1.1.3" xref="S3.SS3.p3.1.m1.1.1.3.cmml"><mi id="S3.SS3.p3.1.m1.1.1.3.2" xref="S3.SS3.p3.1.m1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.1.m1.1.1.3.1" xref="S3.SS3.p3.1.m1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.SS3.p3.1.m1.1.1.3.3" xref="S3.SS3.p3.1.m1.1.1.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.1.m1.1.1.3.1a" xref="S3.SS3.p3.1.m1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.SS3.p3.1.m1.1.1.3.4" xref="S3.SS3.p3.1.m1.1.1.3.4.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><apply id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.1.m1.1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p3.1.m1.1.1.2.cmml" xref="S3.SS3.p3.1.m1.1.1.2">ùëì</ci><apply id="S3.SS3.p3.1.m1.1.1.3.cmml" xref="S3.SS3.p3.1.m1.1.1.3"><times id="S3.SS3.p3.1.m1.1.1.3.1.cmml" xref="S3.SS3.p3.1.m1.1.1.3.1"></times><ci id="S3.SS3.p3.1.m1.1.1.3.2.cmml" xref="S3.SS3.p3.1.m1.1.1.3.2">ùëñ</ci><ci id="S3.SS3.p3.1.m1.1.1.3.3.cmml" xref="S3.SS3.p3.1.m1.1.1.3.3">ùëõ</ci><ci id="S3.SS3.p3.1.m1.1.1.3.4.cmml" xref="S3.SS3.p3.1.m1.1.1.3.4">ùë†</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">f_{ins}</annotation></semantics></math> into a global feature vector <math id="S3.SS3.p3.2.m2.1" class="ltx_Math" alttext="v_{ins}" display="inline"><semantics id="S3.SS3.p3.2.m2.1a"><msub id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml"><mi id="S3.SS3.p3.2.m2.1.1.2" xref="S3.SS3.p3.2.m2.1.1.2.cmml">v</mi><mrow id="S3.SS3.p3.2.m2.1.1.3" xref="S3.SS3.p3.2.m2.1.1.3.cmml"><mi id="S3.SS3.p3.2.m2.1.1.3.2" xref="S3.SS3.p3.2.m2.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.2.m2.1.1.3.1" xref="S3.SS3.p3.2.m2.1.1.3.1.cmml">‚Äã</mo><mi id="S3.SS3.p3.2.m2.1.1.3.3" xref="S3.SS3.p3.2.m2.1.1.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.2.m2.1.1.3.1a" xref="S3.SS3.p3.2.m2.1.1.3.1.cmml">‚Äã</mo><mi id="S3.SS3.p3.2.m2.1.1.3.4" xref="S3.SS3.p3.2.m2.1.1.3.4.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><apply id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.2.m2.1.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p3.2.m2.1.1.2.cmml" xref="S3.SS3.p3.2.m2.1.1.2">ùë£</ci><apply id="S3.SS3.p3.2.m2.1.1.3.cmml" xref="S3.SS3.p3.2.m2.1.1.3"><times id="S3.SS3.p3.2.m2.1.1.3.1.cmml" xref="S3.SS3.p3.2.m2.1.1.3.1"></times><ci id="S3.SS3.p3.2.m2.1.1.3.2.cmml" xref="S3.SS3.p3.2.m2.1.1.3.2">ùëñ</ci><ci id="S3.SS3.p3.2.m2.1.1.3.3.cmml" xref="S3.SS3.p3.2.m2.1.1.3.3">ùëõ</ci><ci id="S3.SS3.p3.2.m2.1.1.3.4.cmml" xref="S3.SS3.p3.2.m2.1.1.3.4">ùë†</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">v_{ins}</annotation></semantics></math>. The global prior feature vector <math id="S3.SS3.p3.3.m3.1" class="ltx_Math" alttext="v_{c}" display="inline"><semantics id="S3.SS3.p3.3.m3.1a"><msub id="S3.SS3.p3.3.m3.1.1" xref="S3.SS3.p3.3.m3.1.1.cmml"><mi id="S3.SS3.p3.3.m3.1.1.2" xref="S3.SS3.p3.3.m3.1.1.2.cmml">v</mi><mi id="S3.SS3.p3.3.m3.1.1.3" xref="S3.SS3.p3.3.m3.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.3.m3.1b"><apply id="S3.SS3.p3.3.m3.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.3.m3.1.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.p3.3.m3.1.1.2.cmml" xref="S3.SS3.p3.3.m3.1.1.2">ùë£</ci><ci id="S3.SS3.p3.3.m3.1.1.3.cmml" xref="S3.SS3.p3.3.m3.1.1.3">ùëê</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.3.m3.1c">v_{c}</annotation></semantics></math> is obtained in the same way. Then, we repeat <math id="S3.SS3.p3.4.m4.1" class="ltx_Math" alttext="v_{ins}" display="inline"><semantics id="S3.SS3.p3.4.m4.1a"><msub id="S3.SS3.p3.4.m4.1.1" xref="S3.SS3.p3.4.m4.1.1.cmml"><mi id="S3.SS3.p3.4.m4.1.1.2" xref="S3.SS3.p3.4.m4.1.1.2.cmml">v</mi><mrow id="S3.SS3.p3.4.m4.1.1.3" xref="S3.SS3.p3.4.m4.1.1.3.cmml"><mi id="S3.SS3.p3.4.m4.1.1.3.2" xref="S3.SS3.p3.4.m4.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.4.m4.1.1.3.1" xref="S3.SS3.p3.4.m4.1.1.3.1.cmml">‚Äã</mo><mi id="S3.SS3.p3.4.m4.1.1.3.3" xref="S3.SS3.p3.4.m4.1.1.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.4.m4.1.1.3.1a" xref="S3.SS3.p3.4.m4.1.1.3.1.cmml">‚Äã</mo><mi id="S3.SS3.p3.4.m4.1.1.3.4" xref="S3.SS3.p3.4.m4.1.1.3.4.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.4.m4.1b"><apply id="S3.SS3.p3.4.m4.1.1.cmml" xref="S3.SS3.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.4.m4.1.1.1.cmml" xref="S3.SS3.p3.4.m4.1.1">subscript</csymbol><ci id="S3.SS3.p3.4.m4.1.1.2.cmml" xref="S3.SS3.p3.4.m4.1.1.2">ùë£</ci><apply id="S3.SS3.p3.4.m4.1.1.3.cmml" xref="S3.SS3.p3.4.m4.1.1.3"><times id="S3.SS3.p3.4.m4.1.1.3.1.cmml" xref="S3.SS3.p3.4.m4.1.1.3.1"></times><ci id="S3.SS3.p3.4.m4.1.1.3.2.cmml" xref="S3.SS3.p3.4.m4.1.1.3.2">ùëñ</ci><ci id="S3.SS3.p3.4.m4.1.1.3.3.cmml" xref="S3.SS3.p3.4.m4.1.1.3.3">ùëõ</ci><ci id="S3.SS3.p3.4.m4.1.1.3.4.cmml" xref="S3.SS3.p3.4.m4.1.1.3.4">ùë†</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.4.m4.1c">v_{ins}</annotation></semantics></math> and <math id="S3.SS3.p3.5.m5.1" class="ltx_Math" alttext="v_{c}" display="inline"><semantics id="S3.SS3.p3.5.m5.1a"><msub id="S3.SS3.p3.5.m5.1.1" xref="S3.SS3.p3.5.m5.1.1.cmml"><mi id="S3.SS3.p3.5.m5.1.1.2" xref="S3.SS3.p3.5.m5.1.1.2.cmml">v</mi><mi id="S3.SS3.p3.5.m5.1.1.3" xref="S3.SS3.p3.5.m5.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.5.m5.1b"><apply id="S3.SS3.p3.5.m5.1.1.cmml" xref="S3.SS3.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.5.m5.1.1.1.cmml" xref="S3.SS3.p3.5.m5.1.1">subscript</csymbol><ci id="S3.SS3.p3.5.m5.1.1.2.cmml" xref="S3.SS3.p3.5.m5.1.1.2">ùë£</ci><ci id="S3.SS3.p3.5.m5.1.1.3.cmml" xref="S3.SS3.p3.5.m5.1.1.3">ùëê</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.5.m5.1c">v_{c}</annotation></semantics></math> for <math id="S3.SS3.p3.6.m6.1" class="ltx_Math" alttext="N_{c}" display="inline"><semantics id="S3.SS3.p3.6.m6.1a"><msub id="S3.SS3.p3.6.m6.1.1" xref="S3.SS3.p3.6.m6.1.1.cmml"><mi id="S3.SS3.p3.6.m6.1.1.2" xref="S3.SS3.p3.6.m6.1.1.2.cmml">N</mi><mi id="S3.SS3.p3.6.m6.1.1.3" xref="S3.SS3.p3.6.m6.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.6.m6.1b"><apply id="S3.SS3.p3.6.m6.1.1.cmml" xref="S3.SS3.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.6.m6.1.1.1.cmml" xref="S3.SS3.p3.6.m6.1.1">subscript</csymbol><ci id="S3.SS3.p3.6.m6.1.1.2.cmml" xref="S3.SS3.p3.6.m6.1.1.2">ùëÅ</ci><ci id="S3.SS3.p3.6.m6.1.1.3.cmml" xref="S3.SS3.p3.6.m6.1.1.3">ùëê</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.6.m6.1c">N_{c}</annotation></semantics></math> times and concatenate them with the prior features <math id="S3.SS3.p3.7.m7.1" class="ltx_Math" alttext="f_{c}" display="inline"><semantics id="S3.SS3.p3.7.m7.1a"><msub id="S3.SS3.p3.7.m7.1.1" xref="S3.SS3.p3.7.m7.1.1.cmml"><mi id="S3.SS3.p3.7.m7.1.1.2" xref="S3.SS3.p3.7.m7.1.1.2.cmml">f</mi><mi id="S3.SS3.p3.7.m7.1.1.3" xref="S3.SS3.p3.7.m7.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.7.m7.1b"><apply id="S3.SS3.p3.7.m7.1.1.cmml" xref="S3.SS3.p3.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.7.m7.1.1.1.cmml" xref="S3.SS3.p3.7.m7.1.1">subscript</csymbol><ci id="S3.SS3.p3.7.m7.1.1.2.cmml" xref="S3.SS3.p3.7.m7.1.1.2">ùëì</ci><ci id="S3.SS3.p3.7.m7.1.1.3.cmml" xref="S3.SS3.p3.7.m7.1.1.3">ùëê</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.7.m7.1c">f_{c}</annotation></semantics></math>. The concatenated features are fed into another MLP for dimension reduction.
After that, we build a graph utilizing the output and pass messages between nodes using an EdgeConv. The final output is the relational deformation feature. We denote it as <math id="S3.SS3.p3.8.m8.1" class="ltx_Math" alttext="f_{d}\in R^{N_{c}\times C}" display="inline"><semantics id="S3.SS3.p3.8.m8.1a"><mrow id="S3.SS3.p3.8.m8.1.1" xref="S3.SS3.p3.8.m8.1.1.cmml"><msub id="S3.SS3.p3.8.m8.1.1.2" xref="S3.SS3.p3.8.m8.1.1.2.cmml"><mi id="S3.SS3.p3.8.m8.1.1.2.2" xref="S3.SS3.p3.8.m8.1.1.2.2.cmml">f</mi><mi id="S3.SS3.p3.8.m8.1.1.2.3" xref="S3.SS3.p3.8.m8.1.1.2.3.cmml">d</mi></msub><mo id="S3.SS3.p3.8.m8.1.1.1" xref="S3.SS3.p3.8.m8.1.1.1.cmml">‚àà</mo><msup id="S3.SS3.p3.8.m8.1.1.3" xref="S3.SS3.p3.8.m8.1.1.3.cmml"><mi id="S3.SS3.p3.8.m8.1.1.3.2" xref="S3.SS3.p3.8.m8.1.1.3.2.cmml">R</mi><mrow id="S3.SS3.p3.8.m8.1.1.3.3" xref="S3.SS3.p3.8.m8.1.1.3.3.cmml"><msub id="S3.SS3.p3.8.m8.1.1.3.3.2" xref="S3.SS3.p3.8.m8.1.1.3.3.2.cmml"><mi id="S3.SS3.p3.8.m8.1.1.3.3.2.2" xref="S3.SS3.p3.8.m8.1.1.3.3.2.2.cmml">N</mi><mi id="S3.SS3.p3.8.m8.1.1.3.3.2.3" xref="S3.SS3.p3.8.m8.1.1.3.3.2.3.cmml">c</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p3.8.m8.1.1.3.3.1" xref="S3.SS3.p3.8.m8.1.1.3.3.1.cmml">√ó</mo><mi id="S3.SS3.p3.8.m8.1.1.3.3.3" xref="S3.SS3.p3.8.m8.1.1.3.3.3.cmml">C</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.8.m8.1b"><apply id="S3.SS3.p3.8.m8.1.1.cmml" xref="S3.SS3.p3.8.m8.1.1"><in id="S3.SS3.p3.8.m8.1.1.1.cmml" xref="S3.SS3.p3.8.m8.1.1.1"></in><apply id="S3.SS3.p3.8.m8.1.1.2.cmml" xref="S3.SS3.p3.8.m8.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p3.8.m8.1.1.2.1.cmml" xref="S3.SS3.p3.8.m8.1.1.2">subscript</csymbol><ci id="S3.SS3.p3.8.m8.1.1.2.2.cmml" xref="S3.SS3.p3.8.m8.1.1.2.2">ùëì</ci><ci id="S3.SS3.p3.8.m8.1.1.2.3.cmml" xref="S3.SS3.p3.8.m8.1.1.2.3">ùëë</ci></apply><apply id="S3.SS3.p3.8.m8.1.1.3.cmml" xref="S3.SS3.p3.8.m8.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p3.8.m8.1.1.3.1.cmml" xref="S3.SS3.p3.8.m8.1.1.3">superscript</csymbol><ci id="S3.SS3.p3.8.m8.1.1.3.2.cmml" xref="S3.SS3.p3.8.m8.1.1.3.2">ùëÖ</ci><apply id="S3.SS3.p3.8.m8.1.1.3.3.cmml" xref="S3.SS3.p3.8.m8.1.1.3.3"><times id="S3.SS3.p3.8.m8.1.1.3.3.1.cmml" xref="S3.SS3.p3.8.m8.1.1.3.3.1"></times><apply id="S3.SS3.p3.8.m8.1.1.3.3.2.cmml" xref="S3.SS3.p3.8.m8.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS3.p3.8.m8.1.1.3.3.2.1.cmml" xref="S3.SS3.p3.8.m8.1.1.3.3.2">subscript</csymbol><ci id="S3.SS3.p3.8.m8.1.1.3.3.2.2.cmml" xref="S3.SS3.p3.8.m8.1.1.3.3.2.2">ùëÅ</ci><ci id="S3.SS3.p3.8.m8.1.1.3.3.2.3.cmml" xref="S3.SS3.p3.8.m8.1.1.3.3.2.3">ùëê</ci></apply><ci id="S3.SS3.p3.8.m8.1.1.3.3.3.cmml" xref="S3.SS3.p3.8.m8.1.1.3.3.3">ùê∂</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.8.m8.1c">f_{d}\in R^{N_{c}\times C}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.5" class="ltx_p"><span id="S3.SS3.p4.5.1" class="ltx_text ltx_font_bold">Relational assignment feature</span> is the third kind of feature we learn. In the Shape Prior Deformation step, we need to assign the above-mentioned deformed shape prior to the NOCS representation (detailed in Sec <a href="#S3.SS4" title="3.4 Shape Prior Deformation in Reconstructor ‚Ä£ 3 ACR-Pose ‚Ä£ ACR-Pose: Adversarial Canonical Representation Reconstruction Network for Category Level 6D Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>). Since the deformed shape prior and the NOCS representation are from different parameter fields, we need to establish a one-to-many correspondence (an assignment matrix) between them. To build the correspondence, we should mainly take advantages of two kinds of data, i.e., the observed instance points and the shape variation. The former works for providing instance characteristics, and the latter works for reducing shape variations. To better utilize their features and flowing information between them, we repeat <math id="S3.SS3.p4.1.m1.1" class="ltx_Math" alttext="v_{ins}" display="inline"><semantics id="S3.SS3.p4.1.m1.1a"><msub id="S3.SS3.p4.1.m1.1.1" xref="S3.SS3.p4.1.m1.1.1.cmml"><mi id="S3.SS3.p4.1.m1.1.1.2" xref="S3.SS3.p4.1.m1.1.1.2.cmml">v</mi><mrow id="S3.SS3.p4.1.m1.1.1.3" xref="S3.SS3.p4.1.m1.1.1.3.cmml"><mi id="S3.SS3.p4.1.m1.1.1.3.2" xref="S3.SS3.p4.1.m1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p4.1.m1.1.1.3.1" xref="S3.SS3.p4.1.m1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.SS3.p4.1.m1.1.1.3.3" xref="S3.SS3.p4.1.m1.1.1.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p4.1.m1.1.1.3.1a" xref="S3.SS3.p4.1.m1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.SS3.p4.1.m1.1.1.3.4" xref="S3.SS3.p4.1.m1.1.1.3.4.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.m1.1b"><apply id="S3.SS3.p4.1.m1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.1.m1.1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p4.1.m1.1.1.2.cmml" xref="S3.SS3.p4.1.m1.1.1.2">ùë£</ci><apply id="S3.SS3.p4.1.m1.1.1.3.cmml" xref="S3.SS3.p4.1.m1.1.1.3"><times id="S3.SS3.p4.1.m1.1.1.3.1.cmml" xref="S3.SS3.p4.1.m1.1.1.3.1"></times><ci id="S3.SS3.p4.1.m1.1.1.3.2.cmml" xref="S3.SS3.p4.1.m1.1.1.3.2">ùëñ</ci><ci id="S3.SS3.p4.1.m1.1.1.3.3.cmml" xref="S3.SS3.p4.1.m1.1.1.3.3">ùëõ</ci><ci id="S3.SS3.p4.1.m1.1.1.3.4.cmml" xref="S3.SS3.p4.1.m1.1.1.3.4">ùë†</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.1.m1.1c">v_{ins}</annotation></semantics></math> and <math id="S3.SS3.p4.2.m2.1" class="ltx_Math" alttext="v_{c}" display="inline"><semantics id="S3.SS3.p4.2.m2.1a"><msub id="S3.SS3.p4.2.m2.1.1" xref="S3.SS3.p4.2.m2.1.1.cmml"><mi id="S3.SS3.p4.2.m2.1.1.2" xref="S3.SS3.p4.2.m2.1.1.2.cmml">v</mi><mi id="S3.SS3.p4.2.m2.1.1.3" xref="S3.SS3.p4.2.m2.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.2.m2.1b"><apply id="S3.SS3.p4.2.m2.1.1.cmml" xref="S3.SS3.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.2.m2.1.1.1.cmml" xref="S3.SS3.p4.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p4.2.m2.1.1.2.cmml" xref="S3.SS3.p4.2.m2.1.1.2">ùë£</ci><ci id="S3.SS3.p4.2.m2.1.1.3.cmml" xref="S3.SS3.p4.2.m2.1.1.3">ùëê</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.2.m2.1c">v_{c}</annotation></semantics></math> for <math id="S3.SS3.p4.3.m3.1" class="ltx_Math" alttext="N_{p}" display="inline"><semantics id="S3.SS3.p4.3.m3.1a"><msub id="S3.SS3.p4.3.m3.1.1" xref="S3.SS3.p4.3.m3.1.1.cmml"><mi id="S3.SS3.p4.3.m3.1.1.2" xref="S3.SS3.p4.3.m3.1.1.2.cmml">N</mi><mi id="S3.SS3.p4.3.m3.1.1.3" xref="S3.SS3.p4.3.m3.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.3.m3.1b"><apply id="S3.SS3.p4.3.m3.1.1.cmml" xref="S3.SS3.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.3.m3.1.1.1.cmml" xref="S3.SS3.p4.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.p4.3.m3.1.1.2.cmml" xref="S3.SS3.p4.3.m3.1.1.2">ùëÅ</ci><ci id="S3.SS3.p4.3.m3.1.1.3.cmml" xref="S3.SS3.p4.3.m3.1.1.3">ùëù</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.3.m3.1c">N_{p}</annotation></semantics></math> times and concatenate them with the relational instance feature <math id="S3.SS3.p4.4.m4.1" class="ltx_Math" alttext="f_{ins}" display="inline"><semantics id="S3.SS3.p4.4.m4.1a"><msub id="S3.SS3.p4.4.m4.1.1" xref="S3.SS3.p4.4.m4.1.1.cmml"><mi id="S3.SS3.p4.4.m4.1.1.2" xref="S3.SS3.p4.4.m4.1.1.2.cmml">f</mi><mrow id="S3.SS3.p4.4.m4.1.1.3" xref="S3.SS3.p4.4.m4.1.1.3.cmml"><mi id="S3.SS3.p4.4.m4.1.1.3.2" xref="S3.SS3.p4.4.m4.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p4.4.m4.1.1.3.1" xref="S3.SS3.p4.4.m4.1.1.3.1.cmml">‚Äã</mo><mi id="S3.SS3.p4.4.m4.1.1.3.3" xref="S3.SS3.p4.4.m4.1.1.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p4.4.m4.1.1.3.1a" xref="S3.SS3.p4.4.m4.1.1.3.1.cmml">‚Äã</mo><mi id="S3.SS3.p4.4.m4.1.1.3.4" xref="S3.SS3.p4.4.m4.1.1.3.4.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.4.m4.1b"><apply id="S3.SS3.p4.4.m4.1.1.cmml" xref="S3.SS3.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.4.m4.1.1.1.cmml" xref="S3.SS3.p4.4.m4.1.1">subscript</csymbol><ci id="S3.SS3.p4.4.m4.1.1.2.cmml" xref="S3.SS3.p4.4.m4.1.1.2">ùëì</ci><apply id="S3.SS3.p4.4.m4.1.1.3.cmml" xref="S3.SS3.p4.4.m4.1.1.3"><times id="S3.SS3.p4.4.m4.1.1.3.1.cmml" xref="S3.SS3.p4.4.m4.1.1.3.1"></times><ci id="S3.SS3.p4.4.m4.1.1.3.2.cmml" xref="S3.SS3.p4.4.m4.1.1.3.2">ùëñ</ci><ci id="S3.SS3.p4.4.m4.1.1.3.3.cmml" xref="S3.SS3.p4.4.m4.1.1.3.3">ùëõ</ci><ci id="S3.SS3.p4.4.m4.1.1.3.4.cmml" xref="S3.SS3.p4.4.m4.1.1.3.4">ùë†</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.4.m4.1c">f_{ins}</annotation></semantics></math>. Then, we use a graph network as before to learn the assignment feature <math id="S3.SS3.p4.5.m5.1" class="ltx_Math" alttext="f_{a}\in R^{N_{p}\times C}" display="inline"><semantics id="S3.SS3.p4.5.m5.1a"><mrow id="S3.SS3.p4.5.m5.1.1" xref="S3.SS3.p4.5.m5.1.1.cmml"><msub id="S3.SS3.p4.5.m5.1.1.2" xref="S3.SS3.p4.5.m5.1.1.2.cmml"><mi id="S3.SS3.p4.5.m5.1.1.2.2" xref="S3.SS3.p4.5.m5.1.1.2.2.cmml">f</mi><mi id="S3.SS3.p4.5.m5.1.1.2.3" xref="S3.SS3.p4.5.m5.1.1.2.3.cmml">a</mi></msub><mo id="S3.SS3.p4.5.m5.1.1.1" xref="S3.SS3.p4.5.m5.1.1.1.cmml">‚àà</mo><msup id="S3.SS3.p4.5.m5.1.1.3" xref="S3.SS3.p4.5.m5.1.1.3.cmml"><mi id="S3.SS3.p4.5.m5.1.1.3.2" xref="S3.SS3.p4.5.m5.1.1.3.2.cmml">R</mi><mrow id="S3.SS3.p4.5.m5.1.1.3.3" xref="S3.SS3.p4.5.m5.1.1.3.3.cmml"><msub id="S3.SS3.p4.5.m5.1.1.3.3.2" xref="S3.SS3.p4.5.m5.1.1.3.3.2.cmml"><mi id="S3.SS3.p4.5.m5.1.1.3.3.2.2" xref="S3.SS3.p4.5.m5.1.1.3.3.2.2.cmml">N</mi><mi id="S3.SS3.p4.5.m5.1.1.3.3.2.3" xref="S3.SS3.p4.5.m5.1.1.3.3.2.3.cmml">p</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p4.5.m5.1.1.3.3.1" xref="S3.SS3.p4.5.m5.1.1.3.3.1.cmml">√ó</mo><mi id="S3.SS3.p4.5.m5.1.1.3.3.3" xref="S3.SS3.p4.5.m5.1.1.3.3.3.cmml">C</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.5.m5.1b"><apply id="S3.SS3.p4.5.m5.1.1.cmml" xref="S3.SS3.p4.5.m5.1.1"><in id="S3.SS3.p4.5.m5.1.1.1.cmml" xref="S3.SS3.p4.5.m5.1.1.1"></in><apply id="S3.SS3.p4.5.m5.1.1.2.cmml" xref="S3.SS3.p4.5.m5.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p4.5.m5.1.1.2.1.cmml" xref="S3.SS3.p4.5.m5.1.1.2">subscript</csymbol><ci id="S3.SS3.p4.5.m5.1.1.2.2.cmml" xref="S3.SS3.p4.5.m5.1.1.2.2">ùëì</ci><ci id="S3.SS3.p4.5.m5.1.1.2.3.cmml" xref="S3.SS3.p4.5.m5.1.1.2.3">ùëé</ci></apply><apply id="S3.SS3.p4.5.m5.1.1.3.cmml" xref="S3.SS3.p4.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p4.5.m5.1.1.3.1.cmml" xref="S3.SS3.p4.5.m5.1.1.3">superscript</csymbol><ci id="S3.SS3.p4.5.m5.1.1.3.2.cmml" xref="S3.SS3.p4.5.m5.1.1.3.2">ùëÖ</ci><apply id="S3.SS3.p4.5.m5.1.1.3.3.cmml" xref="S3.SS3.p4.5.m5.1.1.3.3"><times id="S3.SS3.p4.5.m5.1.1.3.3.1.cmml" xref="S3.SS3.p4.5.m5.1.1.3.3.1"></times><apply id="S3.SS3.p4.5.m5.1.1.3.3.2.cmml" xref="S3.SS3.p4.5.m5.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS3.p4.5.m5.1.1.3.3.2.1.cmml" xref="S3.SS3.p4.5.m5.1.1.3.3.2">subscript</csymbol><ci id="S3.SS3.p4.5.m5.1.1.3.3.2.2.cmml" xref="S3.SS3.p4.5.m5.1.1.3.3.2.2">ùëÅ</ci><ci id="S3.SS3.p4.5.m5.1.1.3.3.2.3.cmml" xref="S3.SS3.p4.5.m5.1.1.3.3.2.3">ùëù</ci></apply><ci id="S3.SS3.p4.5.m5.1.1.3.3.3.cmml" xref="S3.SS3.p4.5.m5.1.1.3.3.3">ùê∂</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.5.m5.1c">f_{a}\in R^{N_{p}\times C}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.1" class="ltx_p">The <em id="S3.SS3.p5.1.1" class="ltx_emph ltx_font_italic">Relational Reconstruction Module</em> uses graphs to learn high-quality features by integrating relational information between features of different modalities. Constructing graphs in the feature space is quite beneficial in learning more semantics about the object shape and category specialities. The learned features will be further used in the Shape Prior Deformation step for the final reconstruction (Sec. <a href="#S3.SS4" title="3.4 Shape Prior Deformation in Reconstructor ‚Ä£ 3 ACR-Pose ‚Ä£ ACR-Pose: Adversarial Canonical Representation Reconstruction Network for Category Level 6D Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>), which would all contribute greatly to high-quality NOCS representation reconstruction, verified by experimental analysis.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Shape Prior Deformation in Reconstructor</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">After all necessary features are learned. We adopt the Shape Prior Deformation approach to the reconstruct the NOCS representation in the Reconstructor. Shape Prior Deformation is first proposed by Tian et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. For the integrity of the paper, we briefly introduce it here.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.4" class="ltx_p">The Shape Prior Deformation step requires us to learn two matrices to transform the shape prior into the NOCS representation. They are the deformation field <math id="S3.SS4.p2.1.m1.1" class="ltx_Math" alttext="M_{d}\in R^{N_{c}\times 3}" display="inline"><semantics id="S3.SS4.p2.1.m1.1a"><mrow id="S3.SS4.p2.1.m1.1.1" xref="S3.SS4.p2.1.m1.1.1.cmml"><msub id="S3.SS4.p2.1.m1.1.1.2" xref="S3.SS4.p2.1.m1.1.1.2.cmml"><mi id="S3.SS4.p2.1.m1.1.1.2.2" xref="S3.SS4.p2.1.m1.1.1.2.2.cmml">M</mi><mi id="S3.SS4.p2.1.m1.1.1.2.3" xref="S3.SS4.p2.1.m1.1.1.2.3.cmml">d</mi></msub><mo id="S3.SS4.p2.1.m1.1.1.1" xref="S3.SS4.p2.1.m1.1.1.1.cmml">‚àà</mo><msup id="S3.SS4.p2.1.m1.1.1.3" xref="S3.SS4.p2.1.m1.1.1.3.cmml"><mi id="S3.SS4.p2.1.m1.1.1.3.2" xref="S3.SS4.p2.1.m1.1.1.3.2.cmml">R</mi><mrow id="S3.SS4.p2.1.m1.1.1.3.3" xref="S3.SS4.p2.1.m1.1.1.3.3.cmml"><msub id="S3.SS4.p2.1.m1.1.1.3.3.2" xref="S3.SS4.p2.1.m1.1.1.3.3.2.cmml"><mi id="S3.SS4.p2.1.m1.1.1.3.3.2.2" xref="S3.SS4.p2.1.m1.1.1.3.3.2.2.cmml">N</mi><mi id="S3.SS4.p2.1.m1.1.1.3.3.2.3" xref="S3.SS4.p2.1.m1.1.1.3.3.2.3.cmml">c</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p2.1.m1.1.1.3.3.1" xref="S3.SS4.p2.1.m1.1.1.3.3.1.cmml">√ó</mo><mn id="S3.SS4.p2.1.m1.1.1.3.3.3" xref="S3.SS4.p2.1.m1.1.1.3.3.3.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.1.m1.1b"><apply id="S3.SS4.p2.1.m1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1"><in id="S3.SS4.p2.1.m1.1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1.1"></in><apply id="S3.SS4.p2.1.m1.1.1.2.cmml" xref="S3.SS4.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p2.1.m1.1.1.2.1.cmml" xref="S3.SS4.p2.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS4.p2.1.m1.1.1.2.2.cmml" xref="S3.SS4.p2.1.m1.1.1.2.2">ùëÄ</ci><ci id="S3.SS4.p2.1.m1.1.1.2.3.cmml" xref="S3.SS4.p2.1.m1.1.1.2.3">ùëë</ci></apply><apply id="S3.SS4.p2.1.m1.1.1.3.cmml" xref="S3.SS4.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.p2.1.m1.1.1.3.1.cmml" xref="S3.SS4.p2.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS4.p2.1.m1.1.1.3.2.cmml" xref="S3.SS4.p2.1.m1.1.1.3.2">ùëÖ</ci><apply id="S3.SS4.p2.1.m1.1.1.3.3.cmml" xref="S3.SS4.p2.1.m1.1.1.3.3"><times id="S3.SS4.p2.1.m1.1.1.3.3.1.cmml" xref="S3.SS4.p2.1.m1.1.1.3.3.1"></times><apply id="S3.SS4.p2.1.m1.1.1.3.3.2.cmml" xref="S3.SS4.p2.1.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS4.p2.1.m1.1.1.3.3.2.1.cmml" xref="S3.SS4.p2.1.m1.1.1.3.3.2">subscript</csymbol><ci id="S3.SS4.p2.1.m1.1.1.3.3.2.2.cmml" xref="S3.SS4.p2.1.m1.1.1.3.3.2.2">ùëÅ</ci><ci id="S3.SS4.p2.1.m1.1.1.3.3.2.3.cmml" xref="S3.SS4.p2.1.m1.1.1.3.3.2.3">ùëê</ci></apply><cn type="integer" id="S3.SS4.p2.1.m1.1.1.3.3.3.cmml" xref="S3.SS4.p2.1.m1.1.1.3.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.1.m1.1c">M_{d}\in R^{N_{c}\times 3}</annotation></semantics></math> and the assignment matrix <math id="S3.SS4.p2.2.m2.1" class="ltx_Math" alttext="M_{a}\in R^{N_{p}\times N_{c}}" display="inline"><semantics id="S3.SS4.p2.2.m2.1a"><mrow id="S3.SS4.p2.2.m2.1.1" xref="S3.SS4.p2.2.m2.1.1.cmml"><msub id="S3.SS4.p2.2.m2.1.1.2" xref="S3.SS4.p2.2.m2.1.1.2.cmml"><mi id="S3.SS4.p2.2.m2.1.1.2.2" xref="S3.SS4.p2.2.m2.1.1.2.2.cmml">M</mi><mi id="S3.SS4.p2.2.m2.1.1.2.3" xref="S3.SS4.p2.2.m2.1.1.2.3.cmml">a</mi></msub><mo id="S3.SS4.p2.2.m2.1.1.1" xref="S3.SS4.p2.2.m2.1.1.1.cmml">‚àà</mo><msup id="S3.SS4.p2.2.m2.1.1.3" xref="S3.SS4.p2.2.m2.1.1.3.cmml"><mi id="S3.SS4.p2.2.m2.1.1.3.2" xref="S3.SS4.p2.2.m2.1.1.3.2.cmml">R</mi><mrow id="S3.SS4.p2.2.m2.1.1.3.3" xref="S3.SS4.p2.2.m2.1.1.3.3.cmml"><msub id="S3.SS4.p2.2.m2.1.1.3.3.2" xref="S3.SS4.p2.2.m2.1.1.3.3.2.cmml"><mi id="S3.SS4.p2.2.m2.1.1.3.3.2.2" xref="S3.SS4.p2.2.m2.1.1.3.3.2.2.cmml">N</mi><mi id="S3.SS4.p2.2.m2.1.1.3.3.2.3" xref="S3.SS4.p2.2.m2.1.1.3.3.2.3.cmml">p</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p2.2.m2.1.1.3.3.1" xref="S3.SS4.p2.2.m2.1.1.3.3.1.cmml">√ó</mo><msub id="S3.SS4.p2.2.m2.1.1.3.3.3" xref="S3.SS4.p2.2.m2.1.1.3.3.3.cmml"><mi id="S3.SS4.p2.2.m2.1.1.3.3.3.2" xref="S3.SS4.p2.2.m2.1.1.3.3.3.2.cmml">N</mi><mi id="S3.SS4.p2.2.m2.1.1.3.3.3.3" xref="S3.SS4.p2.2.m2.1.1.3.3.3.3.cmml">c</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.2.m2.1b"><apply id="S3.SS4.p2.2.m2.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1"><in id="S3.SS4.p2.2.m2.1.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1.1"></in><apply id="S3.SS4.p2.2.m2.1.1.2.cmml" xref="S3.SS4.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p2.2.m2.1.1.2.1.cmml" xref="S3.SS4.p2.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS4.p2.2.m2.1.1.2.2.cmml" xref="S3.SS4.p2.2.m2.1.1.2.2">ùëÄ</ci><ci id="S3.SS4.p2.2.m2.1.1.2.3.cmml" xref="S3.SS4.p2.2.m2.1.1.2.3">ùëé</ci></apply><apply id="S3.SS4.p2.2.m2.1.1.3.cmml" xref="S3.SS4.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.p2.2.m2.1.1.3.1.cmml" xref="S3.SS4.p2.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS4.p2.2.m2.1.1.3.2.cmml" xref="S3.SS4.p2.2.m2.1.1.3.2">ùëÖ</ci><apply id="S3.SS4.p2.2.m2.1.1.3.3.cmml" xref="S3.SS4.p2.2.m2.1.1.3.3"><times id="S3.SS4.p2.2.m2.1.1.3.3.1.cmml" xref="S3.SS4.p2.2.m2.1.1.3.3.1"></times><apply id="S3.SS4.p2.2.m2.1.1.3.3.2.cmml" xref="S3.SS4.p2.2.m2.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS4.p2.2.m2.1.1.3.3.2.1.cmml" xref="S3.SS4.p2.2.m2.1.1.3.3.2">subscript</csymbol><ci id="S3.SS4.p2.2.m2.1.1.3.3.2.2.cmml" xref="S3.SS4.p2.2.m2.1.1.3.3.2.2">ùëÅ</ci><ci id="S3.SS4.p2.2.m2.1.1.3.3.2.3.cmml" xref="S3.SS4.p2.2.m2.1.1.3.3.2.3">ùëù</ci></apply><apply id="S3.SS4.p2.2.m2.1.1.3.3.3.cmml" xref="S3.SS4.p2.2.m2.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS4.p2.2.m2.1.1.3.3.3.1.cmml" xref="S3.SS4.p2.2.m2.1.1.3.3.3">subscript</csymbol><ci id="S3.SS4.p2.2.m2.1.1.3.3.3.2.cmml" xref="S3.SS4.p2.2.m2.1.1.3.3.3.2">ùëÅ</ci><ci id="S3.SS4.p2.2.m2.1.1.3.3.3.3.cmml" xref="S3.SS4.p2.2.m2.1.1.3.3.3.3">ùëê</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.2.m2.1c">M_{a}\in R^{N_{p}\times N_{c}}</annotation></semantics></math>. We feed the relational deformation feature into a MLP for learning <math id="S3.SS4.p2.3.m3.1" class="ltx_Math" alttext="M_{d}" display="inline"><semantics id="S3.SS4.p2.3.m3.1a"><msub id="S3.SS4.p2.3.m3.1.1" xref="S3.SS4.p2.3.m3.1.1.cmml"><mi id="S3.SS4.p2.3.m3.1.1.2" xref="S3.SS4.p2.3.m3.1.1.2.cmml">M</mi><mi id="S3.SS4.p2.3.m3.1.1.3" xref="S3.SS4.p2.3.m3.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.3.m3.1b"><apply id="S3.SS4.p2.3.m3.1.1.cmml" xref="S3.SS4.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.3.m3.1.1.1.cmml" xref="S3.SS4.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS4.p2.3.m3.1.1.2.cmml" xref="S3.SS4.p2.3.m3.1.1.2">ùëÄ</ci><ci id="S3.SS4.p2.3.m3.1.1.3.cmml" xref="S3.SS4.p2.3.m3.1.1.3">ùëë</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.3.m3.1c">M_{d}</annotation></semantics></math> and use another MLP to take the relational assignment features as input to learn <math id="S3.SS4.p2.4.m4.1" class="ltx_Math" alttext="M_{a}" display="inline"><semantics id="S3.SS4.p2.4.m4.1a"><msub id="S3.SS4.p2.4.m4.1.1" xref="S3.SS4.p2.4.m4.1.1.cmml"><mi id="S3.SS4.p2.4.m4.1.1.2" xref="S3.SS4.p2.4.m4.1.1.2.cmml">M</mi><mi id="S3.SS4.p2.4.m4.1.1.3" xref="S3.SS4.p2.4.m4.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.4.m4.1b"><apply id="S3.SS4.p2.4.m4.1.1.cmml" xref="S3.SS4.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.4.m4.1.1.1.cmml" xref="S3.SS4.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS4.p2.4.m4.1.1.2.cmml" xref="S3.SS4.p2.4.m4.1.1.2">ùëÄ</ci><ci id="S3.SS4.p2.4.m4.1.1.3.cmml" xref="S3.SS4.p2.4.m4.1.1.3">ùëé</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.4.m4.1c">M_{a}</annotation></semantics></math>. The NOCS representation can be reconstructed by:</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="P_{nocs}={M_{a}}(P_{c}+M_{d})" display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><msub id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml">P</mi><mrow id="S3.E1.m1.1.1.3.3" xref="S3.E1.m1.1.1.3.3.cmml"><mi id="S3.E1.m1.1.1.3.3.2" xref="S3.E1.m1.1.1.3.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.3.1" xref="S3.E1.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.1.1.3.3.3" xref="S3.E1.m1.1.1.3.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.3.1a" xref="S3.E1.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.1.1.3.3.4" xref="S3.E1.m1.1.1.3.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.3.1b" xref="S3.E1.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.1.1.3.3.5" xref="S3.E1.m1.1.1.3.3.5.cmml">s</mi></mrow></msub><mo id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml"><msub id="S3.E1.m1.1.1.1.3" xref="S3.E1.m1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.3.2.cmml">M</mi><mi id="S3.E1.m1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.3.3.cmml">a</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.2.cmml">‚Äã</mo><mrow id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml"><msub id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.1.1.2.2.cmml">P</mi><mi id="S3.E1.m1.1.1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.1.1.2.3.cmml">c</mi></msub><mo id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">+</mo><msub id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.1.3.2.cmml">M</mi><mi id="S3.E1.m1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.1.3.3.cmml">d</mi></msub></mrow><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><eq id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2"></eq><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2">ùëÉ</ci><apply id="S3.E1.m1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.3.3"><times id="S3.E1.m1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.3.3.1"></times><ci id="S3.E1.m1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.3.3.2">ùëõ</ci><ci id="S3.E1.m1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.3.3.3">ùëú</ci><ci id="S3.E1.m1.1.1.3.3.4.cmml" xref="S3.E1.m1.1.1.3.3.4">ùëê</ci><ci id="S3.E1.m1.1.1.3.3.5.cmml" xref="S3.E1.m1.1.1.3.3.5">ùë†</ci></apply></apply><apply id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><times id="S3.E1.m1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.2"></times><apply id="S3.E1.m1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.3.2">ùëÄ</ci><ci id="S3.E1.m1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.3.3">ùëé</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"><plus id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1"></plus><apply id="S3.E1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.2">ùëÉ</ci><ci id="S3.E1.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.3">ùëê</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.2">ùëÄ</ci><ci id="S3.E1.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.3">ùëë</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">P_{nocs}={M_{a}}(P_{c}+M_{d})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<p id="S3.SS4.p4.2" class="ltx_p">where <math id="S3.SS4.p4.1.m1.1" class="ltx_Math" alttext="P_{c}" display="inline"><semantics id="S3.SS4.p4.1.m1.1a"><msub id="S3.SS4.p4.1.m1.1.1" xref="S3.SS4.p4.1.m1.1.1.cmml"><mi id="S3.SS4.p4.1.m1.1.1.2" xref="S3.SS4.p4.1.m1.1.1.2.cmml">P</mi><mi id="S3.SS4.p4.1.m1.1.1.3" xref="S3.SS4.p4.1.m1.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.1.m1.1b"><apply id="S3.SS4.p4.1.m1.1.1.cmml" xref="S3.SS4.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p4.1.m1.1.1.1.cmml" xref="S3.SS4.p4.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p4.1.m1.1.1.2.cmml" xref="S3.SS4.p4.1.m1.1.1.2">ùëÉ</ci><ci id="S3.SS4.p4.1.m1.1.1.3.cmml" xref="S3.SS4.p4.1.m1.1.1.3">ùëê</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.1.m1.1c">P_{c}</annotation></semantics></math> is the shape prior. Using the two-stage transformation operation, <math id="S3.SS4.p4.2.m2.1" class="ltx_Math" alttext="P_{c}" display="inline"><semantics id="S3.SS4.p4.2.m2.1a"><msub id="S3.SS4.p4.2.m2.1.1" xref="S3.SS4.p4.2.m2.1.1.cmml"><mi id="S3.SS4.p4.2.m2.1.1.2" xref="S3.SS4.p4.2.m2.1.1.2.cmml">P</mi><mi id="S3.SS4.p4.2.m2.1.1.3" xref="S3.SS4.p4.2.m2.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.2.m2.1b"><apply id="S3.SS4.p4.2.m2.1.1.cmml" xref="S3.SS4.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p4.2.m2.1.1.1.cmml" xref="S3.SS4.p4.2.m2.1.1">subscript</csymbol><ci id="S3.SS4.p4.2.m2.1.1.2.cmml" xref="S3.SS4.p4.2.m2.1.1.2">ùëÉ</ci><ci id="S3.SS4.p4.2.m2.1.1.3.cmml" xref="S3.SS4.p4.2.m2.1.1.3">ùëê</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.2.m2.1c">P_{c}</annotation></semantics></math> would first be transformed into the observed points‚Äô canonical instance model and then be transformed to the NOCS representation. The Shape Prior Deformation step ensures the robustness of the reconstruction. It serves as the final step of our Reconstructor.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.4.4" class="ltx_tr">
<th id="S3.T1.4.4.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Data</th>
<th id="S3.T1.4.4.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Methods</th>
<th id="S3.T1.4.4.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">IoU50</th>
<th id="S3.T1.4.4.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">IoU75</th>
<th id="S3.T1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">5<sup id="S3.T1.1.1.1.1" class="ltx_sup"><span id="S3.T1.1.1.1.1.1" class="ltx_text ltx_font_italic">‚àò</span></sup>2cm</th>
<th id="S3.T1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">5<sup id="S3.T1.2.2.2.1" class="ltx_sup"><span id="S3.T1.2.2.2.1.1" class="ltx_text ltx_font_italic">‚àò</span></sup>5cm</th>
<th id="S3.T1.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">10<sup id="S3.T1.3.3.3.1" class="ltx_sup"><span id="S3.T1.3.3.3.1.1" class="ltx_text ltx_font_italic">‚àò</span></sup>2cm</th>
<th id="S3.T1.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">10<sup id="S3.T1.4.4.4.1" class="ltx_sup"><span id="S3.T1.4.4.4.1.1" class="ltx_text ltx_font_italic">‚àò</span></sup>5cm</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.4.5.1" class="ltx_tr">
<td id="S3.T1.4.5.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="4"><span id="S3.T1.4.5.1.1.1" class="ltx_text">CAMERA</span></td>
<td id="S3.T1.4.5.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">NOCS<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>
</td>
<td id="S3.T1.4.5.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">83.9</td>
<td id="S3.T1.4.5.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">69.5</td>
<td id="S3.T1.4.5.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">32.3</td>
<td id="S3.T1.4.5.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">40.9</td>
<td id="S3.T1.4.5.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">48.2</td>
<td id="S3.T1.4.5.1.8" class="ltx_td ltx_align_center ltx_border_t">64.6</td>
</tr>
<tr id="S3.T1.4.6.2" class="ltx_tr">
<td id="S3.T1.4.6.2.1" class="ltx_td ltx_align_center ltx_border_r">SPD<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>
</td>
<td id="S3.T1.4.6.2.2" class="ltx_td ltx_align_center ltx_border_r">93.2</td>
<td id="S3.T1.4.6.2.3" class="ltx_td ltx_align_center ltx_border_r">83.1</td>
<td id="S3.T1.4.6.2.4" class="ltx_td ltx_align_center ltx_border_r">54.3</td>
<td id="S3.T1.4.6.2.5" class="ltx_td ltx_align_center ltx_border_r">59.0</td>
<td id="S3.T1.4.6.2.6" class="ltx_td ltx_align_center ltx_border_r">73.3</td>
<td id="S3.T1.4.6.2.7" class="ltx_td ltx_align_center">81.5</td>
</tr>
<tr id="S3.T1.4.7.3" class="ltx_tr">
<td id="S3.T1.4.7.3.1" class="ltx_td ltx_align_center ltx_border_r">DualPoseNet<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>
</td>
<td id="S3.T1.4.7.3.2" class="ltx_td ltx_align_center ltx_border_r">92.4</td>
<td id="S3.T1.4.7.3.3" class="ltx_td ltx_align_center ltx_border_r">86.4</td>
<td id="S3.T1.4.7.3.4" class="ltx_td ltx_align_center ltx_border_r">64.7</td>
<td id="S3.T1.4.7.3.5" class="ltx_td ltx_align_center ltx_border_r">70.7</td>
<td id="S3.T1.4.7.3.6" class="ltx_td ltx_align_center ltx_border_r">77.2</td>
<td id="S3.T1.4.7.3.7" class="ltx_td ltx_align_center">84.7</td>
</tr>
<tr id="S3.T1.4.8.4" class="ltx_tr">
<td id="S3.T1.4.8.4.1" class="ltx_td ltx_align_center ltx_border_r">FS-Net<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</td>
<td id="S3.T1.4.8.4.2" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S3.T1.4.8.4.3" class="ltx_td ltx_align_center ltx_border_r">85.2</td>
<td id="S3.T1.4.8.4.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S3.T1.4.8.4.5" class="ltx_td ltx_align_center ltx_border_r">62.0</td>
<td id="S3.T1.4.8.4.6" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S3.T1.4.8.4.7" class="ltx_td ltx_align_center">60.8</td>
</tr>
<tr id="S3.T1.4.9.5" class="ltx_tr">
<td id="S3.T1.4.9.5.1" class="ltx_td ltx_border_b ltx_border_r"></td>
<td id="S3.T1.4.9.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">ACR-Pose(Ours)</td>
<td id="S3.T1.4.9.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S3.T1.4.9.5.3.1" class="ltx_text ltx_font_bold">93.8</span></td>
<td id="S3.T1.4.9.5.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S3.T1.4.9.5.4.1" class="ltx_text ltx_font_bold">89.9</span></td>
<td id="S3.T1.4.9.5.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S3.T1.4.9.5.5.1" class="ltx_text ltx_font_bold">70.4</span></td>
<td id="S3.T1.4.9.5.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S3.T1.4.9.5.6.1" class="ltx_text ltx_font_bold">74.1</span></td>
<td id="S3.T1.4.9.5.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S3.T1.4.9.5.7.1" class="ltx_text ltx_font_bold">82.6</span></td>
<td id="S3.T1.4.9.5.8" class="ltx_td ltx_align_center ltx_border_b"><span id="S3.T1.4.9.5.8.1" class="ltx_text ltx_font_bold">87.8</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.6.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S3.T1.7.2" class="ltx_text" style="font-size:90%;">Performance on NOCS-CAMERA dataset.</span></figcaption>
</figure>
<figure id="S3.T2" class="ltx_table">
<table id="S3.T2.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.4.4" class="ltx_tr">
<th id="S3.T2.4.4.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Data</th>
<th id="S3.T2.4.4.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Methods</th>
<th id="S3.T2.4.4.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">IoU50</th>
<th id="S3.T2.4.4.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">IoU75</th>
<th id="S3.T2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">5<sup id="S3.T2.1.1.1.1" class="ltx_sup"><span id="S3.T2.1.1.1.1.1" class="ltx_text ltx_font_italic">‚àò</span></sup>2cm</th>
<th id="S3.T2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">5<sup id="S3.T2.2.2.2.1" class="ltx_sup"><span id="S3.T2.2.2.2.1.1" class="ltx_text ltx_font_italic">‚àò</span></sup>5cm</th>
<th id="S3.T2.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">10<sup id="S3.T2.3.3.3.1" class="ltx_sup"><span id="S3.T2.3.3.3.1.1" class="ltx_text ltx_font_italic">‚àò</span></sup>2cm</th>
<th id="S3.T2.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">10<sup id="S3.T2.4.4.4.1" class="ltx_sup"><span id="S3.T2.4.4.4.1.1" class="ltx_text ltx_font_italic">‚àò</span></sup>5cm</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.4.5.1" class="ltx_tr">
<td id="S3.T2.4.5.1.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" rowspan="6"><span id="S3.T2.4.5.1.1.1" class="ltx_text">REAL</span></td>
<td id="S3.T2.4.5.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">NOCS<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>
</td>
<td id="S3.T2.4.5.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">78.0</td>
<td id="S3.T2.4.5.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">30.1</td>
<td id="S3.T2.4.5.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">7.2</td>
<td id="S3.T2.4.5.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10.0</td>
<td id="S3.T2.4.5.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">13.8</td>
<td id="S3.T2.4.5.1.8" class="ltx_td ltx_align_center ltx_border_t">25.2</td>
</tr>
<tr id="S3.T2.4.6.2" class="ltx_tr">
<td id="S3.T2.4.6.2.1" class="ltx_td ltx_align_center ltx_border_r">CASS<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>
</td>
<td id="S3.T2.4.6.2.2" class="ltx_td ltx_align_center ltx_border_r">77.7</td>
<td id="S3.T2.4.6.2.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S3.T2.4.6.2.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S3.T2.4.6.2.5" class="ltx_td ltx_align_center ltx_border_r">23.5</td>
<td id="S3.T2.4.6.2.6" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S3.T2.4.6.2.7" class="ltx_td ltx_align_center">58.0</td>
</tr>
<tr id="S3.T2.4.7.3" class="ltx_tr">
<td id="S3.T2.4.7.3.1" class="ltx_td ltx_align_center ltx_border_r">SPD<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>
</td>
<td id="S3.T2.4.7.3.2" class="ltx_td ltx_align_center ltx_border_r">77.3</td>
<td id="S3.T2.4.7.3.3" class="ltx_td ltx_align_center ltx_border_r">53.2</td>
<td id="S3.T2.4.7.3.4" class="ltx_td ltx_align_center ltx_border_r">19.3</td>
<td id="S3.T2.4.7.3.5" class="ltx_td ltx_align_center ltx_border_r">21.4</td>
<td id="S3.T2.4.7.3.6" class="ltx_td ltx_align_center ltx_border_r">43.2</td>
<td id="S3.T2.4.7.3.7" class="ltx_td ltx_align_center">54.1</td>
</tr>
<tr id="S3.T2.4.8.4" class="ltx_tr">
<td id="S3.T2.4.8.4.1" class="ltx_td ltx_align_center ltx_border_r">DualPoseNet<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>
</td>
<td id="S3.T2.4.8.4.2" class="ltx_td ltx_align_center ltx_border_r">79.8</td>
<td id="S3.T2.4.8.4.3" class="ltx_td ltx_align_center ltx_border_r">62.2</td>
<td id="S3.T2.4.8.4.4" class="ltx_td ltx_align_center ltx_border_r">29.3</td>
<td id="S3.T2.4.8.4.5" class="ltx_td ltx_align_center ltx_border_r">35.9</td>
<td id="S3.T2.4.8.4.6" class="ltx_td ltx_align_center ltx_border_r">50.0</td>
<td id="S3.T2.4.8.4.7" class="ltx_td ltx_align_center"><span id="S3.T2.4.8.4.7.1" class="ltx_text ltx_font_bold">66.8</span></td>
</tr>
<tr id="S3.T2.4.9.5" class="ltx_tr">
<td id="S3.T2.4.9.5.1" class="ltx_td ltx_align_center ltx_border_r">FS-Net<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</td>
<td id="S3.T2.4.9.5.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T2.4.9.5.2.1" class="ltx_text ltx_font_bold">92.2</span></td>
<td id="S3.T2.4.9.5.3" class="ltx_td ltx_align_center ltx_border_r">63.5</td>
<td id="S3.T2.4.9.5.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S3.T2.4.9.5.5" class="ltx_td ltx_align_center ltx_border_r">28.2</td>
<td id="S3.T2.4.9.5.6" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S3.T2.4.9.5.7" class="ltx_td ltx_align_center">60.8</td>
</tr>
<tr id="S3.T2.4.10.6" class="ltx_tr">
<td id="S3.T2.4.10.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">ACR-Pose(Ours)</td>
<td id="S3.T2.4.10.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">82.8</td>
<td id="S3.T2.4.10.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S3.T2.4.10.6.3.1" class="ltx_text ltx_font_bold">66.0</span></td>
<td id="S3.T2.4.10.6.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S3.T2.4.10.6.4.1" class="ltx_text ltx_font_bold">31.6</span></td>
<td id="S3.T2.4.10.6.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S3.T2.4.10.6.5.1" class="ltx_text ltx_font_bold">36.9</span></td>
<td id="S3.T2.4.10.6.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S3.T2.4.10.6.6.1" class="ltx_text ltx_font_bold">54.8</span></td>
<td id="S3.T2.4.10.6.7" class="ltx_td ltx_align_center ltx_border_b">65.9</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T2.6.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S3.T2.7.2" class="ltx_text" style="font-size:90%;">Performance on NOCS-REAL dataset.</span></figcaption>
</figure>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Discriminator and Adversarial Training</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">Though the NOCS representation can be predicted by only using the Reconstructor, we observe that some reconstruction results are quite unrealistic, which may severely affect the 6D object pose estimation accuracy. In this paper, we propose to use an adversarial reconstruction strategy to solve the problem. Specifically, we propose to use a Discriminator to guide the Reconstructor. The Discriminator in our work consists of a MLP, a global max-pooling and two fully connected layers. It takes the reconstructed NOCS representation and the real one as input at training time. And the output is a probability value distributed from 0-1, where a higher value means a larger probability it regards the input point cloud as a real NOCS representation and vice versa. The optimization goal of the Discriminator is to minimize:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.4" class="ltx_Math" alttext="L_{d}=(\mathbb{D}(\hat{P_{nocs}})-1)^{2}+(\mathbb{D}(\overline{P_{nocs}}))^{2}" display="block"><semantics id="S3.E2.m1.4a"><mrow id="S3.E2.m1.4.4" xref="S3.E2.m1.4.4.cmml"><msub id="S3.E2.m1.4.4.4" xref="S3.E2.m1.4.4.4.cmml"><mi id="S3.E2.m1.4.4.4.2" xref="S3.E2.m1.4.4.4.2.cmml">L</mi><mi id="S3.E2.m1.4.4.4.3" xref="S3.E2.m1.4.4.4.3.cmml">d</mi></msub><mo id="S3.E2.m1.4.4.3" xref="S3.E2.m1.4.4.3.cmml">=</mo><mrow id="S3.E2.m1.4.4.2" xref="S3.E2.m1.4.4.2.cmml"><msup id="S3.E2.m1.3.3.1.1" xref="S3.E2.m1.3.3.1.1.cmml"><mrow id="S3.E2.m1.3.3.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.3.3.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.3.3.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.cmml"><mrow id="S3.E2.m1.3.3.1.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.2.2" xref="S3.E2.m1.3.3.1.1.1.1.1.2.2.cmml">ùîª</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.1.1.1.1.1.2.1" xref="S3.E2.m1.3.3.1.1.1.1.1.2.1.cmml">‚Äã</mo><mrow id="S3.E2.m1.3.3.1.1.1.1.1.2.3.2" xref="S3.E2.m1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.3.3.1.1.1.1.1.2.3.2.1" xref="S3.E2.m1.1.1.cmml">(</mo><mover accent="true" id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><msub id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml"><mi id="S3.E2.m1.1.1.2.2" xref="S3.E2.m1.1.1.2.2.cmml">P</mi><mrow id="S3.E2.m1.1.1.2.3" xref="S3.E2.m1.1.1.2.3.cmml"><mi id="S3.E2.m1.1.1.2.3.2" xref="S3.E2.m1.1.1.2.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.2.3.1" xref="S3.E2.m1.1.1.2.3.1.cmml">‚Äã</mo><mi id="S3.E2.m1.1.1.2.3.3" xref="S3.E2.m1.1.1.2.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.2.3.1a" xref="S3.E2.m1.1.1.2.3.1.cmml">‚Äã</mo><mi id="S3.E2.m1.1.1.2.3.4" xref="S3.E2.m1.1.1.2.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.2.3.1b" xref="S3.E2.m1.1.1.2.3.1.cmml">‚Äã</mo><mi id="S3.E2.m1.1.1.2.3.5" xref="S3.E2.m1.1.1.2.3.5.cmml">s</mi></mrow></msub><mo id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.cmml">^</mo></mover><mo stretchy="false" id="S3.E2.m1.3.3.1.1.1.1.1.2.3.2.2" xref="S3.E2.m1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.3.3.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.cmml">‚àí</mo><mn id="S3.E2.m1.3.3.1.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S3.E2.m1.3.3.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.1.1.cmml">)</mo></mrow><mn id="S3.E2.m1.3.3.1.1.3" xref="S3.E2.m1.3.3.1.1.3.cmml">2</mn></msup><mo id="S3.E2.m1.4.4.2.3" xref="S3.E2.m1.4.4.2.3.cmml">+</mo><msup id="S3.E2.m1.4.4.2.2" xref="S3.E2.m1.4.4.2.2.cmml"><mrow id="S3.E2.m1.4.4.2.2.1.1" xref="S3.E2.m1.4.4.2.2.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.4.4.2.2.1.1.2" xref="S3.E2.m1.4.4.2.2.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.4.4.2.2.1.1.1" xref="S3.E2.m1.4.4.2.2.1.1.1.cmml"><mi id="S3.E2.m1.4.4.2.2.1.1.1.2" xref="S3.E2.m1.4.4.2.2.1.1.1.2.cmml">ùîª</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.2.2.1.1.1.1" xref="S3.E2.m1.4.4.2.2.1.1.1.1.cmml">‚Äã</mo><mrow id="S3.E2.m1.4.4.2.2.1.1.1.3.2" xref="S3.E2.m1.2.2.cmml"><mo stretchy="false" id="S3.E2.m1.4.4.2.2.1.1.1.3.2.1" xref="S3.E2.m1.2.2.cmml">(</mo><mover accent="true" id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml"><msub id="S3.E2.m1.2.2.2" xref="S3.E2.m1.2.2.2.cmml"><mi id="S3.E2.m1.2.2.2.2" xref="S3.E2.m1.2.2.2.2.cmml">P</mi><mrow id="S3.E2.m1.2.2.2.3" xref="S3.E2.m1.2.2.2.3.cmml"><mi id="S3.E2.m1.2.2.2.3.2" xref="S3.E2.m1.2.2.2.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.2.3.1" xref="S3.E2.m1.2.2.2.3.1.cmml">‚Äã</mo><mi id="S3.E2.m1.2.2.2.3.3" xref="S3.E2.m1.2.2.2.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.2.3.1a" xref="S3.E2.m1.2.2.2.3.1.cmml">‚Äã</mo><mi id="S3.E2.m1.2.2.2.3.4" xref="S3.E2.m1.2.2.2.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.2.3.1b" xref="S3.E2.m1.2.2.2.3.1.cmml">‚Äã</mo><mi id="S3.E2.m1.2.2.2.3.5" xref="S3.E2.m1.2.2.2.3.5.cmml">s</mi></mrow></msub><mo id="S3.E2.m1.2.2.1" xref="S3.E2.m1.2.2.1.cmml">¬Ø</mo></mover><mo stretchy="false" id="S3.E2.m1.4.4.2.2.1.1.1.3.2.2" xref="S3.E2.m1.2.2.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E2.m1.4.4.2.2.1.1.3" xref="S3.E2.m1.4.4.2.2.1.1.1.cmml">)</mo></mrow><mn id="S3.E2.m1.4.4.2.2.3" xref="S3.E2.m1.4.4.2.2.3.cmml">2</mn></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.4b"><apply id="S3.E2.m1.4.4.cmml" xref="S3.E2.m1.4.4"><eq id="S3.E2.m1.4.4.3.cmml" xref="S3.E2.m1.4.4.3"></eq><apply id="S3.E2.m1.4.4.4.cmml" xref="S3.E2.m1.4.4.4"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.4.1.cmml" xref="S3.E2.m1.4.4.4">subscript</csymbol><ci id="S3.E2.m1.4.4.4.2.cmml" xref="S3.E2.m1.4.4.4.2">ùêø</ci><ci id="S3.E2.m1.4.4.4.3.cmml" xref="S3.E2.m1.4.4.4.3">ùëë</ci></apply><apply id="S3.E2.m1.4.4.2.cmml" xref="S3.E2.m1.4.4.2"><plus id="S3.E2.m1.4.4.2.3.cmml" xref="S3.E2.m1.4.4.2.3"></plus><apply id="S3.E2.m1.3.3.1.1.cmml" xref="S3.E2.m1.3.3.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1">superscript</csymbol><apply id="S3.E2.m1.3.3.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1"><minus id="S3.E2.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1"></minus><apply id="S3.E2.m1.3.3.1.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.2"><times id="S3.E2.m1.3.3.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.2.1"></times><ci id="S3.E2.m1.3.3.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.2.2">ùîª</ci><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.2.3.2"><ci id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1">^</ci><apply id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.2">subscript</csymbol><ci id="S3.E2.m1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.2.2">ùëÉ</ci><apply id="S3.E2.m1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.2.3"><times id="S3.E2.m1.1.1.2.3.1.cmml" xref="S3.E2.m1.1.1.2.3.1"></times><ci id="S3.E2.m1.1.1.2.3.2.cmml" xref="S3.E2.m1.1.1.2.3.2">ùëõ</ci><ci id="S3.E2.m1.1.1.2.3.3.cmml" xref="S3.E2.m1.1.1.2.3.3">ùëú</ci><ci id="S3.E2.m1.1.1.2.3.4.cmml" xref="S3.E2.m1.1.1.2.3.4">ùëê</ci><ci id="S3.E2.m1.1.1.2.3.5.cmml" xref="S3.E2.m1.1.1.2.3.5">ùë†</ci></apply></apply></apply></apply><cn type="integer" id="S3.E2.m1.3.3.1.1.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.3">1</cn></apply><cn type="integer" id="S3.E2.m1.3.3.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.3">2</cn></apply><apply id="S3.E2.m1.4.4.2.2.cmml" xref="S3.E2.m1.4.4.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.2.2.2.cmml" xref="S3.E2.m1.4.4.2.2">superscript</csymbol><apply id="S3.E2.m1.4.4.2.2.1.1.1.cmml" xref="S3.E2.m1.4.4.2.2.1.1"><times id="S3.E2.m1.4.4.2.2.1.1.1.1.cmml" xref="S3.E2.m1.4.4.2.2.1.1.1.1"></times><ci id="S3.E2.m1.4.4.2.2.1.1.1.2.cmml" xref="S3.E2.m1.4.4.2.2.1.1.1.2">ùîª</ci><apply id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.4.4.2.2.1.1.1.3.2"><ci id="S3.E2.m1.2.2.1.cmml" xref="S3.E2.m1.2.2.1">¬Ø</ci><apply id="S3.E2.m1.2.2.2.cmml" xref="S3.E2.m1.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.2.1.cmml" xref="S3.E2.m1.2.2.2">subscript</csymbol><ci id="S3.E2.m1.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2">ùëÉ</ci><apply id="S3.E2.m1.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.3"><times id="S3.E2.m1.2.2.2.3.1.cmml" xref="S3.E2.m1.2.2.2.3.1"></times><ci id="S3.E2.m1.2.2.2.3.2.cmml" xref="S3.E2.m1.2.2.2.3.2">ùëõ</ci><ci id="S3.E2.m1.2.2.2.3.3.cmml" xref="S3.E2.m1.2.2.2.3.3">ùëú</ci><ci id="S3.E2.m1.2.2.2.3.4.cmml" xref="S3.E2.m1.2.2.2.3.4">ùëê</ci><ci id="S3.E2.m1.2.2.2.3.5.cmml" xref="S3.E2.m1.2.2.2.3.5">ùë†</ci></apply></apply></apply></apply><cn type="integer" id="S3.E2.m1.4.4.2.2.3.cmml" xref="S3.E2.m1.4.4.2.2.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.4c">L_{d}=(\mathbb{D}(\hat{P_{nocs}})-1)^{2}+(\mathbb{D}(\overline{P_{nocs}}))^{2}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.2" class="ltx_p">where <math id="S3.SS5.p2.1.m1.1" class="ltx_Math" alttext="\overline{P_{nocs}}" display="inline"><semantics id="S3.SS5.p2.1.m1.1a"><mover accent="true" id="S3.SS5.p2.1.m1.1.1" xref="S3.SS5.p2.1.m1.1.1.cmml"><msub id="S3.SS5.p2.1.m1.1.1.2" xref="S3.SS5.p2.1.m1.1.1.2.cmml"><mi id="S3.SS5.p2.1.m1.1.1.2.2" xref="S3.SS5.p2.1.m1.1.1.2.2.cmml">P</mi><mrow id="S3.SS5.p2.1.m1.1.1.2.3" xref="S3.SS5.p2.1.m1.1.1.2.3.cmml"><mi id="S3.SS5.p2.1.m1.1.1.2.3.2" xref="S3.SS5.p2.1.m1.1.1.2.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.1.m1.1.1.2.3.1" xref="S3.SS5.p2.1.m1.1.1.2.3.1.cmml">‚Äã</mo><mi id="S3.SS5.p2.1.m1.1.1.2.3.3" xref="S3.SS5.p2.1.m1.1.1.2.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.1.m1.1.1.2.3.1a" xref="S3.SS5.p2.1.m1.1.1.2.3.1.cmml">‚Äã</mo><mi id="S3.SS5.p2.1.m1.1.1.2.3.4" xref="S3.SS5.p2.1.m1.1.1.2.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.1.m1.1.1.2.3.1b" xref="S3.SS5.p2.1.m1.1.1.2.3.1.cmml">‚Äã</mo><mi id="S3.SS5.p2.1.m1.1.1.2.3.5" xref="S3.SS5.p2.1.m1.1.1.2.3.5.cmml">s</mi></mrow></msub><mo id="S3.SS5.p2.1.m1.1.1.1" xref="S3.SS5.p2.1.m1.1.1.1.cmml">¬Ø</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.1.m1.1b"><apply id="S3.SS5.p2.1.m1.1.1.cmml" xref="S3.SS5.p2.1.m1.1.1"><ci id="S3.SS5.p2.1.m1.1.1.1.cmml" xref="S3.SS5.p2.1.m1.1.1.1">¬Ø</ci><apply id="S3.SS5.p2.1.m1.1.1.2.cmml" xref="S3.SS5.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS5.p2.1.m1.1.1.2.1.cmml" xref="S3.SS5.p2.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS5.p2.1.m1.1.1.2.2.cmml" xref="S3.SS5.p2.1.m1.1.1.2.2">ùëÉ</ci><apply id="S3.SS5.p2.1.m1.1.1.2.3.cmml" xref="S3.SS5.p2.1.m1.1.1.2.3"><times id="S3.SS5.p2.1.m1.1.1.2.3.1.cmml" xref="S3.SS5.p2.1.m1.1.1.2.3.1"></times><ci id="S3.SS5.p2.1.m1.1.1.2.3.2.cmml" xref="S3.SS5.p2.1.m1.1.1.2.3.2">ùëõ</ci><ci id="S3.SS5.p2.1.m1.1.1.2.3.3.cmml" xref="S3.SS5.p2.1.m1.1.1.2.3.3">ùëú</ci><ci id="S3.SS5.p2.1.m1.1.1.2.3.4.cmml" xref="S3.SS5.p2.1.m1.1.1.2.3.4">ùëê</ci><ci id="S3.SS5.p2.1.m1.1.1.2.3.5.cmml" xref="S3.SS5.p2.1.m1.1.1.2.3.5">ùë†</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.1.m1.1c">\overline{P_{nocs}}</annotation></semantics></math> is the reconstructed NOCS representation and <math id="S3.SS5.p2.2.m2.1" class="ltx_Math" alttext="\hat{P_{nocs}}" display="inline"><semantics id="S3.SS5.p2.2.m2.1a"><mover accent="true" id="S3.SS5.p2.2.m2.1.1" xref="S3.SS5.p2.2.m2.1.1.cmml"><msub id="S3.SS5.p2.2.m2.1.1.2" xref="S3.SS5.p2.2.m2.1.1.2.cmml"><mi id="S3.SS5.p2.2.m2.1.1.2.2" xref="S3.SS5.p2.2.m2.1.1.2.2.cmml">P</mi><mrow id="S3.SS5.p2.2.m2.1.1.2.3" xref="S3.SS5.p2.2.m2.1.1.2.3.cmml"><mi id="S3.SS5.p2.2.m2.1.1.2.3.2" xref="S3.SS5.p2.2.m2.1.1.2.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.2.m2.1.1.2.3.1" xref="S3.SS5.p2.2.m2.1.1.2.3.1.cmml">‚Äã</mo><mi id="S3.SS5.p2.2.m2.1.1.2.3.3" xref="S3.SS5.p2.2.m2.1.1.2.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.2.m2.1.1.2.3.1a" xref="S3.SS5.p2.2.m2.1.1.2.3.1.cmml">‚Äã</mo><mi id="S3.SS5.p2.2.m2.1.1.2.3.4" xref="S3.SS5.p2.2.m2.1.1.2.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.2.m2.1.1.2.3.1b" xref="S3.SS5.p2.2.m2.1.1.2.3.1.cmml">‚Äã</mo><mi id="S3.SS5.p2.2.m2.1.1.2.3.5" xref="S3.SS5.p2.2.m2.1.1.2.3.5.cmml">s</mi></mrow></msub><mo id="S3.SS5.p2.2.m2.1.1.1" xref="S3.SS5.p2.2.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.2.m2.1b"><apply id="S3.SS5.p2.2.m2.1.1.cmml" xref="S3.SS5.p2.2.m2.1.1"><ci id="S3.SS5.p2.2.m2.1.1.1.cmml" xref="S3.SS5.p2.2.m2.1.1.1">^</ci><apply id="S3.SS5.p2.2.m2.1.1.2.cmml" xref="S3.SS5.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS5.p2.2.m2.1.1.2.1.cmml" xref="S3.SS5.p2.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS5.p2.2.m2.1.1.2.2.cmml" xref="S3.SS5.p2.2.m2.1.1.2.2">ùëÉ</ci><apply id="S3.SS5.p2.2.m2.1.1.2.3.cmml" xref="S3.SS5.p2.2.m2.1.1.2.3"><times id="S3.SS5.p2.2.m2.1.1.2.3.1.cmml" xref="S3.SS5.p2.2.m2.1.1.2.3.1"></times><ci id="S3.SS5.p2.2.m2.1.1.2.3.2.cmml" xref="S3.SS5.p2.2.m2.1.1.2.3.2">ùëõ</ci><ci id="S3.SS5.p2.2.m2.1.1.2.3.3.cmml" xref="S3.SS5.p2.2.m2.1.1.2.3.3">ùëú</ci><ci id="S3.SS5.p2.2.m2.1.1.2.3.4.cmml" xref="S3.SS5.p2.2.m2.1.1.2.3.4">ùëê</ci><ci id="S3.SS5.p2.2.m2.1.1.2.3.5.cmml" xref="S3.SS5.p2.2.m2.1.1.2.3.5">ùë†</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.2.m2.1c">\hat{P_{nocs}}</annotation></semantics></math> is the ground-truth.</p>
</div>
<div id="S3.SS5.p3" class="ltx_para">
<p id="S3.SS5.p3.1" class="ltx_p">Correspondingly, the optimization goal of the Reconstructor is to minimize:</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.2" class="ltx_Math" alttext="L_{g}=(\mathbb{D}(\overline{P_{nocs}})-1)^{2}" display="block"><semantics id="S3.E3.m1.2a"><mrow id="S3.E3.m1.2.2" xref="S3.E3.m1.2.2.cmml"><msub id="S3.E3.m1.2.2.3" xref="S3.E3.m1.2.2.3.cmml"><mi id="S3.E3.m1.2.2.3.2" xref="S3.E3.m1.2.2.3.2.cmml">L</mi><mi id="S3.E3.m1.2.2.3.3" xref="S3.E3.m1.2.2.3.3.cmml">g</mi></msub><mo id="S3.E3.m1.2.2.2" xref="S3.E3.m1.2.2.2.cmml">=</mo><msup id="S3.E3.m1.2.2.1" xref="S3.E3.m1.2.2.1.cmml"><mrow id="S3.E3.m1.2.2.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.2.2.1.1.1.2" xref="S3.E3.m1.2.2.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.2.2.1.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.cmml"><mrow id="S3.E3.m1.2.2.1.1.1.1.2" xref="S3.E3.m1.2.2.1.1.1.1.2.cmml"><mi id="S3.E3.m1.2.2.1.1.1.1.2.2" xref="S3.E3.m1.2.2.1.1.1.1.2.2.cmml">ùîª</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.1.1.1.1.2.1" xref="S3.E3.m1.2.2.1.1.1.1.2.1.cmml">‚Äã</mo><mrow id="S3.E3.m1.2.2.1.1.1.1.2.3.2" xref="S3.E3.m1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.2.2.1.1.1.1.2.3.2.1" xref="S3.E3.m1.1.1.cmml">(</mo><mover accent="true" id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml"><msub id="S3.E3.m1.1.1.2" xref="S3.E3.m1.1.1.2.cmml"><mi id="S3.E3.m1.1.1.2.2" xref="S3.E3.m1.1.1.2.2.cmml">P</mi><mrow id="S3.E3.m1.1.1.2.3" xref="S3.E3.m1.1.1.2.3.cmml"><mi id="S3.E3.m1.1.1.2.3.2" xref="S3.E3.m1.1.1.2.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.2.3.1" xref="S3.E3.m1.1.1.2.3.1.cmml">‚Äã</mo><mi id="S3.E3.m1.1.1.2.3.3" xref="S3.E3.m1.1.1.2.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.2.3.1a" xref="S3.E3.m1.1.1.2.3.1.cmml">‚Äã</mo><mi id="S3.E3.m1.1.1.2.3.4" xref="S3.E3.m1.1.1.2.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.2.3.1b" xref="S3.E3.m1.1.1.2.3.1.cmml">‚Äã</mo><mi id="S3.E3.m1.1.1.2.3.5" xref="S3.E3.m1.1.1.2.3.5.cmml">s</mi></mrow></msub><mo id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.cmml">¬Ø</mo></mover><mo stretchy="false" id="S3.E3.m1.2.2.1.1.1.1.2.3.2.2" xref="S3.E3.m1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.2.2.1.1.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.1.cmml">‚àí</mo><mn id="S3.E3.m1.2.2.1.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S3.E3.m1.2.2.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.1.cmml">)</mo></mrow><mn id="S3.E3.m1.2.2.1.3" xref="S3.E3.m1.2.2.1.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.2b"><apply id="S3.E3.m1.2.2.cmml" xref="S3.E3.m1.2.2"><eq id="S3.E3.m1.2.2.2.cmml" xref="S3.E3.m1.2.2.2"></eq><apply id="S3.E3.m1.2.2.3.cmml" xref="S3.E3.m1.2.2.3"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.3.1.cmml" xref="S3.E3.m1.2.2.3">subscript</csymbol><ci id="S3.E3.m1.2.2.3.2.cmml" xref="S3.E3.m1.2.2.3.2">ùêø</ci><ci id="S3.E3.m1.2.2.3.3.cmml" xref="S3.E3.m1.2.2.3.3">ùëî</ci></apply><apply id="S3.E3.m1.2.2.1.cmml" xref="S3.E3.m1.2.2.1"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.2.cmml" xref="S3.E3.m1.2.2.1">superscript</csymbol><apply id="S3.E3.m1.2.2.1.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1"><minus id="S3.E3.m1.2.2.1.1.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1"></minus><apply id="S3.E3.m1.2.2.1.1.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.2"><times id="S3.E3.m1.2.2.1.1.1.1.2.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.2.1"></times><ci id="S3.E3.m1.2.2.1.1.1.1.2.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.2.2">ùîª</ci><apply id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.2.3.2"><ci id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1.1">¬Ø</ci><apply id="S3.E3.m1.1.1.2.cmml" xref="S3.E3.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.2">subscript</csymbol><ci id="S3.E3.m1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.2.2">ùëÉ</ci><apply id="S3.E3.m1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.2.3"><times id="S3.E3.m1.1.1.2.3.1.cmml" xref="S3.E3.m1.1.1.2.3.1"></times><ci id="S3.E3.m1.1.1.2.3.2.cmml" xref="S3.E3.m1.1.1.2.3.2">ùëõ</ci><ci id="S3.E3.m1.1.1.2.3.3.cmml" xref="S3.E3.m1.1.1.2.3.3">ùëú</ci><ci id="S3.E3.m1.1.1.2.3.4.cmml" xref="S3.E3.m1.1.1.2.3.4">ùëê</ci><ci id="S3.E3.m1.1.1.2.3.5.cmml" xref="S3.E3.m1.1.1.2.3.5">ùë†</ci></apply></apply></apply></apply><cn type="integer" id="S3.E3.m1.2.2.1.1.1.1.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.3">1</cn></apply><cn type="integer" id="S3.E3.m1.2.2.1.3.cmml" xref="S3.E3.m1.2.2.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.2c">L_{g}=(\mathbb{D}(\overline{P_{nocs}})-1)^{2}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS5.p4" class="ltx_para">
<p id="S3.SS5.p4.1" class="ltx_p">During training, the Discriminator would try its best to judge the difference between real and fake, while the Reconstructor would try its best to confuse the Discriminator. In the fierce competition, both of them will gradually become stronger and stronger, hence increasing the reality of the reconstructed NOCS representation.</p>
</div>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Loss Function</h3>

<div id="S3.SS6.p1" class="ltx_para">
<p id="S3.SS6.p1.8" class="ltx_p">Except for the adversarial loss <math id="S3.SS6.p1.1.m1.1" class="ltx_Math" alttext="L_{d}" display="inline"><semantics id="S3.SS6.p1.1.m1.1a"><msub id="S3.SS6.p1.1.m1.1.1" xref="S3.SS6.p1.1.m1.1.1.cmml"><mi id="S3.SS6.p1.1.m1.1.1.2" xref="S3.SS6.p1.1.m1.1.1.2.cmml">L</mi><mi id="S3.SS6.p1.1.m1.1.1.3" xref="S3.SS6.p1.1.m1.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.1.m1.1b"><apply id="S3.SS6.p1.1.m1.1.1.cmml" xref="S3.SS6.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS6.p1.1.m1.1.1.1.cmml" xref="S3.SS6.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS6.p1.1.m1.1.1.2.cmml" xref="S3.SS6.p1.1.m1.1.1.2">ùêø</ci><ci id="S3.SS6.p1.1.m1.1.1.3.cmml" xref="S3.SS6.p1.1.m1.1.1.3">ùëë</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.1.m1.1c">L_{d}</annotation></semantics></math> and <math id="S3.SS6.p1.2.m2.1" class="ltx_Math" alttext="L_{g}" display="inline"><semantics id="S3.SS6.p1.2.m2.1a"><msub id="S3.SS6.p1.2.m2.1.1" xref="S3.SS6.p1.2.m2.1.1.cmml"><mi id="S3.SS6.p1.2.m2.1.1.2" xref="S3.SS6.p1.2.m2.1.1.2.cmml">L</mi><mi id="S3.SS6.p1.2.m2.1.1.3" xref="S3.SS6.p1.2.m2.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.2.m2.1b"><apply id="S3.SS6.p1.2.m2.1.1.cmml" xref="S3.SS6.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS6.p1.2.m2.1.1.1.cmml" xref="S3.SS6.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS6.p1.2.m2.1.1.2.cmml" xref="S3.SS6.p1.2.m2.1.1.2">ùêø</ci><ci id="S3.SS6.p1.2.m2.1.1.3.cmml" xref="S3.SS6.p1.2.m2.1.1.3">ùëî</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.2.m2.1c">L_{g}</annotation></semantics></math> described before, we use the smooth L1 loss <math id="S3.SS6.p1.3.m3.1" class="ltx_Math" alttext="L_{corr}" display="inline"><semantics id="S3.SS6.p1.3.m3.1a"><msub id="S3.SS6.p1.3.m3.1.1" xref="S3.SS6.p1.3.m3.1.1.cmml"><mi id="S3.SS6.p1.3.m3.1.1.2" xref="S3.SS6.p1.3.m3.1.1.2.cmml">L</mi><mrow id="S3.SS6.p1.3.m3.1.1.3" xref="S3.SS6.p1.3.m3.1.1.3.cmml"><mi id="S3.SS6.p1.3.m3.1.1.3.2" xref="S3.SS6.p1.3.m3.1.1.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS6.p1.3.m3.1.1.3.1" xref="S3.SS6.p1.3.m3.1.1.3.1.cmml">‚Äã</mo><mi id="S3.SS6.p1.3.m3.1.1.3.3" xref="S3.SS6.p1.3.m3.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS6.p1.3.m3.1.1.3.1a" xref="S3.SS6.p1.3.m3.1.1.3.1.cmml">‚Äã</mo><mi id="S3.SS6.p1.3.m3.1.1.3.4" xref="S3.SS6.p1.3.m3.1.1.3.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS6.p1.3.m3.1.1.3.1b" xref="S3.SS6.p1.3.m3.1.1.3.1.cmml">‚Äã</mo><mi id="S3.SS6.p1.3.m3.1.1.3.5" xref="S3.SS6.p1.3.m3.1.1.3.5.cmml">r</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.3.m3.1b"><apply id="S3.SS6.p1.3.m3.1.1.cmml" xref="S3.SS6.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS6.p1.3.m3.1.1.1.cmml" xref="S3.SS6.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS6.p1.3.m3.1.1.2.cmml" xref="S3.SS6.p1.3.m3.1.1.2">ùêø</ci><apply id="S3.SS6.p1.3.m3.1.1.3.cmml" xref="S3.SS6.p1.3.m3.1.1.3"><times id="S3.SS6.p1.3.m3.1.1.3.1.cmml" xref="S3.SS6.p1.3.m3.1.1.3.1"></times><ci id="S3.SS6.p1.3.m3.1.1.3.2.cmml" xref="S3.SS6.p1.3.m3.1.1.3.2">ùëê</ci><ci id="S3.SS6.p1.3.m3.1.1.3.3.cmml" xref="S3.SS6.p1.3.m3.1.1.3.3">ùëú</ci><ci id="S3.SS6.p1.3.m3.1.1.3.4.cmml" xref="S3.SS6.p1.3.m3.1.1.3.4">ùëü</ci><ci id="S3.SS6.p1.3.m3.1.1.3.5.cmml" xref="S3.SS6.p1.3.m3.1.1.3.5">ùëü</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.3.m3.1c">L_{corr}</annotation></semantics></math> between the reconstructed NOCS representation and ground-truth to encourage better one-to-one correspondence. We use the chamfer distance loss <math id="S3.SS6.p1.4.m4.1" class="ltx_Math" alttext="L_{cd}" display="inline"><semantics id="S3.SS6.p1.4.m4.1a"><msub id="S3.SS6.p1.4.m4.1.1" xref="S3.SS6.p1.4.m4.1.1.cmml"><mi id="S3.SS6.p1.4.m4.1.1.2" xref="S3.SS6.p1.4.m4.1.1.2.cmml">L</mi><mrow id="S3.SS6.p1.4.m4.1.1.3" xref="S3.SS6.p1.4.m4.1.1.3.cmml"><mi id="S3.SS6.p1.4.m4.1.1.3.2" xref="S3.SS6.p1.4.m4.1.1.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS6.p1.4.m4.1.1.3.1" xref="S3.SS6.p1.4.m4.1.1.3.1.cmml">‚Äã</mo><mi id="S3.SS6.p1.4.m4.1.1.3.3" xref="S3.SS6.p1.4.m4.1.1.3.3.cmml">d</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.4.m4.1b"><apply id="S3.SS6.p1.4.m4.1.1.cmml" xref="S3.SS6.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS6.p1.4.m4.1.1.1.cmml" xref="S3.SS6.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS6.p1.4.m4.1.1.2.cmml" xref="S3.SS6.p1.4.m4.1.1.2">ùêø</ci><apply id="S3.SS6.p1.4.m4.1.1.3.cmml" xref="S3.SS6.p1.4.m4.1.1.3"><times id="S3.SS6.p1.4.m4.1.1.3.1.cmml" xref="S3.SS6.p1.4.m4.1.1.3.1"></times><ci id="S3.SS6.p1.4.m4.1.1.3.2.cmml" xref="S3.SS6.p1.4.m4.1.1.3.2">ùëê</ci><ci id="S3.SS6.p1.4.m4.1.1.3.3.cmml" xref="S3.SS6.p1.4.m4.1.1.3.3">ùëë</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.4.m4.1c">L_{cd}</annotation></semantics></math> between the deformed shape prior and the object‚Äôs canonical instance model to preserve appearance information. We also use a cross-entropy loss <math id="S3.SS6.p1.5.m5.1" class="ltx_Math" alttext="L_{entro}" display="inline"><semantics id="S3.SS6.p1.5.m5.1a"><msub id="S3.SS6.p1.5.m5.1.1" xref="S3.SS6.p1.5.m5.1.1.cmml"><mi id="S3.SS6.p1.5.m5.1.1.2" xref="S3.SS6.p1.5.m5.1.1.2.cmml">L</mi><mrow id="S3.SS6.p1.5.m5.1.1.3" xref="S3.SS6.p1.5.m5.1.1.3.cmml"><mi id="S3.SS6.p1.5.m5.1.1.3.2" xref="S3.SS6.p1.5.m5.1.1.3.2.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS6.p1.5.m5.1.1.3.1" xref="S3.SS6.p1.5.m5.1.1.3.1.cmml">‚Äã</mo><mi id="S3.SS6.p1.5.m5.1.1.3.3" xref="S3.SS6.p1.5.m5.1.1.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS6.p1.5.m5.1.1.3.1a" xref="S3.SS6.p1.5.m5.1.1.3.1.cmml">‚Äã</mo><mi id="S3.SS6.p1.5.m5.1.1.3.4" xref="S3.SS6.p1.5.m5.1.1.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS6.p1.5.m5.1.1.3.1b" xref="S3.SS6.p1.5.m5.1.1.3.1.cmml">‚Äã</mo><mi id="S3.SS6.p1.5.m5.1.1.3.5" xref="S3.SS6.p1.5.m5.1.1.3.5.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS6.p1.5.m5.1.1.3.1c" xref="S3.SS6.p1.5.m5.1.1.3.1.cmml">‚Äã</mo><mi id="S3.SS6.p1.5.m5.1.1.3.6" xref="S3.SS6.p1.5.m5.1.1.3.6.cmml">o</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.5.m5.1b"><apply id="S3.SS6.p1.5.m5.1.1.cmml" xref="S3.SS6.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS6.p1.5.m5.1.1.1.cmml" xref="S3.SS6.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS6.p1.5.m5.1.1.2.cmml" xref="S3.SS6.p1.5.m5.1.1.2">ùêø</ci><apply id="S3.SS6.p1.5.m5.1.1.3.cmml" xref="S3.SS6.p1.5.m5.1.1.3"><times id="S3.SS6.p1.5.m5.1.1.3.1.cmml" xref="S3.SS6.p1.5.m5.1.1.3.1"></times><ci id="S3.SS6.p1.5.m5.1.1.3.2.cmml" xref="S3.SS6.p1.5.m5.1.1.3.2">ùëí</ci><ci id="S3.SS6.p1.5.m5.1.1.3.3.cmml" xref="S3.SS6.p1.5.m5.1.1.3.3">ùëõ</ci><ci id="S3.SS6.p1.5.m5.1.1.3.4.cmml" xref="S3.SS6.p1.5.m5.1.1.3.4">ùë°</ci><ci id="S3.SS6.p1.5.m5.1.1.3.5.cmml" xref="S3.SS6.p1.5.m5.1.1.3.5">ùëü</ci><ci id="S3.SS6.p1.5.m5.1.1.3.6.cmml" xref="S3.SS6.p1.5.m5.1.1.3.6">ùëú</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.5.m5.1c">L_{entro}</annotation></semantics></math> to encourage peak distribution of the assignment matrix <math id="S3.SS6.p1.6.m6.1" class="ltx_Math" alttext="M_{a}" display="inline"><semantics id="S3.SS6.p1.6.m6.1a"><msub id="S3.SS6.p1.6.m6.1.1" xref="S3.SS6.p1.6.m6.1.1.cmml"><mi id="S3.SS6.p1.6.m6.1.1.2" xref="S3.SS6.p1.6.m6.1.1.2.cmml">M</mi><mi id="S3.SS6.p1.6.m6.1.1.3" xref="S3.SS6.p1.6.m6.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.6.m6.1b"><apply id="S3.SS6.p1.6.m6.1.1.cmml" xref="S3.SS6.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS6.p1.6.m6.1.1.1.cmml" xref="S3.SS6.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS6.p1.6.m6.1.1.2.cmml" xref="S3.SS6.p1.6.m6.1.1.2">ùëÄ</ci><ci id="S3.SS6.p1.6.m6.1.1.3.cmml" xref="S3.SS6.p1.6.m6.1.1.3">ùëé</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.6.m6.1c">M_{a}</annotation></semantics></math> and a L2 regularization loss <math id="S3.SS6.p1.7.m7.1" class="ltx_Math" alttext="L_{reg}" display="inline"><semantics id="S3.SS6.p1.7.m7.1a"><msub id="S3.SS6.p1.7.m7.1.1" xref="S3.SS6.p1.7.m7.1.1.cmml"><mi id="S3.SS6.p1.7.m7.1.1.2" xref="S3.SS6.p1.7.m7.1.1.2.cmml">L</mi><mrow id="S3.SS6.p1.7.m7.1.1.3" xref="S3.SS6.p1.7.m7.1.1.3.cmml"><mi id="S3.SS6.p1.7.m7.1.1.3.2" xref="S3.SS6.p1.7.m7.1.1.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS6.p1.7.m7.1.1.3.1" xref="S3.SS6.p1.7.m7.1.1.3.1.cmml">‚Äã</mo><mi id="S3.SS6.p1.7.m7.1.1.3.3" xref="S3.SS6.p1.7.m7.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS6.p1.7.m7.1.1.3.1a" xref="S3.SS6.p1.7.m7.1.1.3.1.cmml">‚Äã</mo><mi id="S3.SS6.p1.7.m7.1.1.3.4" xref="S3.SS6.p1.7.m7.1.1.3.4.cmml">g</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.7.m7.1b"><apply id="S3.SS6.p1.7.m7.1.1.cmml" xref="S3.SS6.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS6.p1.7.m7.1.1.1.cmml" xref="S3.SS6.p1.7.m7.1.1">subscript</csymbol><ci id="S3.SS6.p1.7.m7.1.1.2.cmml" xref="S3.SS6.p1.7.m7.1.1.2">ùêø</ci><apply id="S3.SS6.p1.7.m7.1.1.3.cmml" xref="S3.SS6.p1.7.m7.1.1.3"><times id="S3.SS6.p1.7.m7.1.1.3.1.cmml" xref="S3.SS6.p1.7.m7.1.1.3.1"></times><ci id="S3.SS6.p1.7.m7.1.1.3.2.cmml" xref="S3.SS6.p1.7.m7.1.1.3.2">ùëü</ci><ci id="S3.SS6.p1.7.m7.1.1.3.3.cmml" xref="S3.SS6.p1.7.m7.1.1.3.3">ùëí</ci><ci id="S3.SS6.p1.7.m7.1.1.3.4.cmml" xref="S3.SS6.p1.7.m7.1.1.3.4">ùëî</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.7.m7.1c">L_{reg}</annotation></semantics></math> on <math id="S3.SS6.p1.8.m8.1" class="ltx_Math" alttext="M_{d}" display="inline"><semantics id="S3.SS6.p1.8.m8.1a"><msub id="S3.SS6.p1.8.m8.1.1" xref="S3.SS6.p1.8.m8.1.1.cmml"><mi id="S3.SS6.p1.8.m8.1.1.2" xref="S3.SS6.p1.8.m8.1.1.2.cmml">M</mi><mi id="S3.SS6.p1.8.m8.1.1.3" xref="S3.SS6.p1.8.m8.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.8.m8.1b"><apply id="S3.SS6.p1.8.m8.1.1.cmml" xref="S3.SS6.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS6.p1.8.m8.1.1.1.cmml" xref="S3.SS6.p1.8.m8.1.1">subscript</csymbol><ci id="S3.SS6.p1.8.m8.1.1.2.cmml" xref="S3.SS6.p1.8.m8.1.1.2">ùëÄ</ci><ci id="S3.SS6.p1.8.m8.1.1.3.cmml" xref="S3.SS6.p1.8.m8.1.1.3">ùëë</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.8.m8.1c">M_{d}</annotation></semantics></math> to avoid collapsing deformation. The final loss function is:</p>
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.1" class="ltx_Math" alttext="L=\gamma_{1}L_{d}+\gamma_{2}L_{g}+\gamma_{3}L_{corr}+\gamma_{4}L_{cd}+\gamma_{5}L_{entro}+\gamma_{6}L_{reg}" display="block"><semantics id="S3.E4.m1.1a"><mrow id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml"><mi id="S3.E4.m1.1.1.2" xref="S3.E4.m1.1.1.2.cmml">L</mi><mo id="S3.E4.m1.1.1.1" xref="S3.E4.m1.1.1.1.cmml">=</mo><mrow id="S3.E4.m1.1.1.3" xref="S3.E4.m1.1.1.3.cmml"><mrow id="S3.E4.m1.1.1.3.2" xref="S3.E4.m1.1.1.3.2.cmml"><msub id="S3.E4.m1.1.1.3.2.2" xref="S3.E4.m1.1.1.3.2.2.cmml"><mi id="S3.E4.m1.1.1.3.2.2.2" xref="S3.E4.m1.1.1.3.2.2.2.cmml">Œ≥</mi><mn id="S3.E4.m1.1.1.3.2.2.3" xref="S3.E4.m1.1.1.3.2.2.3.cmml">1</mn></msub><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.2.1" xref="S3.E4.m1.1.1.3.2.1.cmml">‚Äã</mo><msub id="S3.E4.m1.1.1.3.2.3" xref="S3.E4.m1.1.1.3.2.3.cmml"><mi id="S3.E4.m1.1.1.3.2.3.2" xref="S3.E4.m1.1.1.3.2.3.2.cmml">L</mi><mi id="S3.E4.m1.1.1.3.2.3.3" xref="S3.E4.m1.1.1.3.2.3.3.cmml">d</mi></msub></mrow><mo id="S3.E4.m1.1.1.3.1" xref="S3.E4.m1.1.1.3.1.cmml">+</mo><mrow id="S3.E4.m1.1.1.3.3" xref="S3.E4.m1.1.1.3.3.cmml"><msub id="S3.E4.m1.1.1.3.3.2" xref="S3.E4.m1.1.1.3.3.2.cmml"><mi id="S3.E4.m1.1.1.3.3.2.2" xref="S3.E4.m1.1.1.3.3.2.2.cmml">Œ≥</mi><mn id="S3.E4.m1.1.1.3.3.2.3" xref="S3.E4.m1.1.1.3.3.2.3.cmml">2</mn></msub><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.3.1" xref="S3.E4.m1.1.1.3.3.1.cmml">‚Äã</mo><msub id="S3.E4.m1.1.1.3.3.3" xref="S3.E4.m1.1.1.3.3.3.cmml"><mi id="S3.E4.m1.1.1.3.3.3.2" xref="S3.E4.m1.1.1.3.3.3.2.cmml">L</mi><mi id="S3.E4.m1.1.1.3.3.3.3" xref="S3.E4.m1.1.1.3.3.3.3.cmml">g</mi></msub></mrow><mo id="S3.E4.m1.1.1.3.1a" xref="S3.E4.m1.1.1.3.1.cmml">+</mo><mrow id="S3.E4.m1.1.1.3.4" xref="S3.E4.m1.1.1.3.4.cmml"><msub id="S3.E4.m1.1.1.3.4.2" xref="S3.E4.m1.1.1.3.4.2.cmml"><mi id="S3.E4.m1.1.1.3.4.2.2" xref="S3.E4.m1.1.1.3.4.2.2.cmml">Œ≥</mi><mn id="S3.E4.m1.1.1.3.4.2.3" xref="S3.E4.m1.1.1.3.4.2.3.cmml">3</mn></msub><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.4.1" xref="S3.E4.m1.1.1.3.4.1.cmml">‚Äã</mo><msub id="S3.E4.m1.1.1.3.4.3" xref="S3.E4.m1.1.1.3.4.3.cmml"><mi id="S3.E4.m1.1.1.3.4.3.2" xref="S3.E4.m1.1.1.3.4.3.2.cmml">L</mi><mrow id="S3.E4.m1.1.1.3.4.3.3" xref="S3.E4.m1.1.1.3.4.3.3.cmml"><mi id="S3.E4.m1.1.1.3.4.3.3.2" xref="S3.E4.m1.1.1.3.4.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.4.3.3.1" xref="S3.E4.m1.1.1.3.4.3.3.1.cmml">‚Äã</mo><mi id="S3.E4.m1.1.1.3.4.3.3.3" xref="S3.E4.m1.1.1.3.4.3.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.4.3.3.1a" xref="S3.E4.m1.1.1.3.4.3.3.1.cmml">‚Äã</mo><mi id="S3.E4.m1.1.1.3.4.3.3.4" xref="S3.E4.m1.1.1.3.4.3.3.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.4.3.3.1b" xref="S3.E4.m1.1.1.3.4.3.3.1.cmml">‚Äã</mo><mi id="S3.E4.m1.1.1.3.4.3.3.5" xref="S3.E4.m1.1.1.3.4.3.3.5.cmml">r</mi></mrow></msub></mrow><mo id="S3.E4.m1.1.1.3.1b" xref="S3.E4.m1.1.1.3.1.cmml">+</mo><mrow id="S3.E4.m1.1.1.3.5" xref="S3.E4.m1.1.1.3.5.cmml"><msub id="S3.E4.m1.1.1.3.5.2" xref="S3.E4.m1.1.1.3.5.2.cmml"><mi id="S3.E4.m1.1.1.3.5.2.2" xref="S3.E4.m1.1.1.3.5.2.2.cmml">Œ≥</mi><mn id="S3.E4.m1.1.1.3.5.2.3" xref="S3.E4.m1.1.1.3.5.2.3.cmml">4</mn></msub><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.5.1" xref="S3.E4.m1.1.1.3.5.1.cmml">‚Äã</mo><msub id="S3.E4.m1.1.1.3.5.3" xref="S3.E4.m1.1.1.3.5.3.cmml"><mi id="S3.E4.m1.1.1.3.5.3.2" xref="S3.E4.m1.1.1.3.5.3.2.cmml">L</mi><mrow id="S3.E4.m1.1.1.3.5.3.3" xref="S3.E4.m1.1.1.3.5.3.3.cmml"><mi id="S3.E4.m1.1.1.3.5.3.3.2" xref="S3.E4.m1.1.1.3.5.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.5.3.3.1" xref="S3.E4.m1.1.1.3.5.3.3.1.cmml">‚Äã</mo><mi id="S3.E4.m1.1.1.3.5.3.3.3" xref="S3.E4.m1.1.1.3.5.3.3.3.cmml">d</mi></mrow></msub></mrow><mo id="S3.E4.m1.1.1.3.1c" xref="S3.E4.m1.1.1.3.1.cmml">+</mo><mrow id="S3.E4.m1.1.1.3.6" xref="S3.E4.m1.1.1.3.6.cmml"><msub id="S3.E4.m1.1.1.3.6.2" xref="S3.E4.m1.1.1.3.6.2.cmml"><mi id="S3.E4.m1.1.1.3.6.2.2" xref="S3.E4.m1.1.1.3.6.2.2.cmml">Œ≥</mi><mn id="S3.E4.m1.1.1.3.6.2.3" xref="S3.E4.m1.1.1.3.6.2.3.cmml">5</mn></msub><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.6.1" xref="S3.E4.m1.1.1.3.6.1.cmml">‚Äã</mo><msub id="S3.E4.m1.1.1.3.6.3" xref="S3.E4.m1.1.1.3.6.3.cmml"><mi id="S3.E4.m1.1.1.3.6.3.2" xref="S3.E4.m1.1.1.3.6.3.2.cmml">L</mi><mrow id="S3.E4.m1.1.1.3.6.3.3" xref="S3.E4.m1.1.1.3.6.3.3.cmml"><mi id="S3.E4.m1.1.1.3.6.3.3.2" xref="S3.E4.m1.1.1.3.6.3.3.2.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.6.3.3.1" xref="S3.E4.m1.1.1.3.6.3.3.1.cmml">‚Äã</mo><mi id="S3.E4.m1.1.1.3.6.3.3.3" xref="S3.E4.m1.1.1.3.6.3.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.6.3.3.1a" xref="S3.E4.m1.1.1.3.6.3.3.1.cmml">‚Äã</mo><mi id="S3.E4.m1.1.1.3.6.3.3.4" xref="S3.E4.m1.1.1.3.6.3.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.6.3.3.1b" xref="S3.E4.m1.1.1.3.6.3.3.1.cmml">‚Äã</mo><mi id="S3.E4.m1.1.1.3.6.3.3.5" xref="S3.E4.m1.1.1.3.6.3.3.5.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.6.3.3.1c" xref="S3.E4.m1.1.1.3.6.3.3.1.cmml">‚Äã</mo><mi id="S3.E4.m1.1.1.3.6.3.3.6" xref="S3.E4.m1.1.1.3.6.3.3.6.cmml">o</mi></mrow></msub></mrow><mo id="S3.E4.m1.1.1.3.1d" xref="S3.E4.m1.1.1.3.1.cmml">+</mo><mrow id="S3.E4.m1.1.1.3.7" xref="S3.E4.m1.1.1.3.7.cmml"><msub id="S3.E4.m1.1.1.3.7.2" xref="S3.E4.m1.1.1.3.7.2.cmml"><mi id="S3.E4.m1.1.1.3.7.2.2" xref="S3.E4.m1.1.1.3.7.2.2.cmml">Œ≥</mi><mn id="S3.E4.m1.1.1.3.7.2.3" xref="S3.E4.m1.1.1.3.7.2.3.cmml">6</mn></msub><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.7.1" xref="S3.E4.m1.1.1.3.7.1.cmml">‚Äã</mo><msub id="S3.E4.m1.1.1.3.7.3" xref="S3.E4.m1.1.1.3.7.3.cmml"><mi id="S3.E4.m1.1.1.3.7.3.2" xref="S3.E4.m1.1.1.3.7.3.2.cmml">L</mi><mrow id="S3.E4.m1.1.1.3.7.3.3" xref="S3.E4.m1.1.1.3.7.3.3.cmml"><mi id="S3.E4.m1.1.1.3.7.3.3.2" xref="S3.E4.m1.1.1.3.7.3.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.7.3.3.1" xref="S3.E4.m1.1.1.3.7.3.3.1.cmml">‚Äã</mo><mi id="S3.E4.m1.1.1.3.7.3.3.3" xref="S3.E4.m1.1.1.3.7.3.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.7.3.3.1a" xref="S3.E4.m1.1.1.3.7.3.3.1.cmml">‚Äã</mo><mi id="S3.E4.m1.1.1.3.7.3.3.4" xref="S3.E4.m1.1.1.3.7.3.3.4.cmml">g</mi></mrow></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1"><eq id="S3.E4.m1.1.1.1.cmml" xref="S3.E4.m1.1.1.1"></eq><ci id="S3.E4.m1.1.1.2.cmml" xref="S3.E4.m1.1.1.2">ùêø</ci><apply id="S3.E4.m1.1.1.3.cmml" xref="S3.E4.m1.1.1.3"><plus id="S3.E4.m1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.3.1"></plus><apply id="S3.E4.m1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.3.2"><times id="S3.E4.m1.1.1.3.2.1.cmml" xref="S3.E4.m1.1.1.3.2.1"></times><apply id="S3.E4.m1.1.1.3.2.2.cmml" xref="S3.E4.m1.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.3.2.2.1.cmml" xref="S3.E4.m1.1.1.3.2.2">subscript</csymbol><ci id="S3.E4.m1.1.1.3.2.2.2.cmml" xref="S3.E4.m1.1.1.3.2.2.2">ùõæ</ci><cn type="integer" id="S3.E4.m1.1.1.3.2.2.3.cmml" xref="S3.E4.m1.1.1.3.2.2.3">1</cn></apply><apply id="S3.E4.m1.1.1.3.2.3.cmml" xref="S3.E4.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.3.2.3.1.cmml" xref="S3.E4.m1.1.1.3.2.3">subscript</csymbol><ci id="S3.E4.m1.1.1.3.2.3.2.cmml" xref="S3.E4.m1.1.1.3.2.3.2">ùêø</ci><ci id="S3.E4.m1.1.1.3.2.3.3.cmml" xref="S3.E4.m1.1.1.3.2.3.3">ùëë</ci></apply></apply><apply id="S3.E4.m1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.3.3"><times id="S3.E4.m1.1.1.3.3.1.cmml" xref="S3.E4.m1.1.1.3.3.1"></times><apply id="S3.E4.m1.1.1.3.3.2.cmml" xref="S3.E4.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.3.3.2.1.cmml" xref="S3.E4.m1.1.1.3.3.2">subscript</csymbol><ci id="S3.E4.m1.1.1.3.3.2.2.cmml" xref="S3.E4.m1.1.1.3.3.2.2">ùõæ</ci><cn type="integer" id="S3.E4.m1.1.1.3.3.2.3.cmml" xref="S3.E4.m1.1.1.3.3.2.3">2</cn></apply><apply id="S3.E4.m1.1.1.3.3.3.cmml" xref="S3.E4.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.3.3.3.1.cmml" xref="S3.E4.m1.1.1.3.3.3">subscript</csymbol><ci id="S3.E4.m1.1.1.3.3.3.2.cmml" xref="S3.E4.m1.1.1.3.3.3.2">ùêø</ci><ci id="S3.E4.m1.1.1.3.3.3.3.cmml" xref="S3.E4.m1.1.1.3.3.3.3">ùëî</ci></apply></apply><apply id="S3.E4.m1.1.1.3.4.cmml" xref="S3.E4.m1.1.1.3.4"><times id="S3.E4.m1.1.1.3.4.1.cmml" xref="S3.E4.m1.1.1.3.4.1"></times><apply id="S3.E4.m1.1.1.3.4.2.cmml" xref="S3.E4.m1.1.1.3.4.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.3.4.2.1.cmml" xref="S3.E4.m1.1.1.3.4.2">subscript</csymbol><ci id="S3.E4.m1.1.1.3.4.2.2.cmml" xref="S3.E4.m1.1.1.3.4.2.2">ùõæ</ci><cn type="integer" id="S3.E4.m1.1.1.3.4.2.3.cmml" xref="S3.E4.m1.1.1.3.4.2.3">3</cn></apply><apply id="S3.E4.m1.1.1.3.4.3.cmml" xref="S3.E4.m1.1.1.3.4.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.3.4.3.1.cmml" xref="S3.E4.m1.1.1.3.4.3">subscript</csymbol><ci id="S3.E4.m1.1.1.3.4.3.2.cmml" xref="S3.E4.m1.1.1.3.4.3.2">ùêø</ci><apply id="S3.E4.m1.1.1.3.4.3.3.cmml" xref="S3.E4.m1.1.1.3.4.3.3"><times id="S3.E4.m1.1.1.3.4.3.3.1.cmml" xref="S3.E4.m1.1.1.3.4.3.3.1"></times><ci id="S3.E4.m1.1.1.3.4.3.3.2.cmml" xref="S3.E4.m1.1.1.3.4.3.3.2">ùëê</ci><ci id="S3.E4.m1.1.1.3.4.3.3.3.cmml" xref="S3.E4.m1.1.1.3.4.3.3.3">ùëú</ci><ci id="S3.E4.m1.1.1.3.4.3.3.4.cmml" xref="S3.E4.m1.1.1.3.4.3.3.4">ùëü</ci><ci id="S3.E4.m1.1.1.3.4.3.3.5.cmml" xref="S3.E4.m1.1.1.3.4.3.3.5">ùëü</ci></apply></apply></apply><apply id="S3.E4.m1.1.1.3.5.cmml" xref="S3.E4.m1.1.1.3.5"><times id="S3.E4.m1.1.1.3.5.1.cmml" xref="S3.E4.m1.1.1.3.5.1"></times><apply id="S3.E4.m1.1.1.3.5.2.cmml" xref="S3.E4.m1.1.1.3.5.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.3.5.2.1.cmml" xref="S3.E4.m1.1.1.3.5.2">subscript</csymbol><ci id="S3.E4.m1.1.1.3.5.2.2.cmml" xref="S3.E4.m1.1.1.3.5.2.2">ùõæ</ci><cn type="integer" id="S3.E4.m1.1.1.3.5.2.3.cmml" xref="S3.E4.m1.1.1.3.5.2.3">4</cn></apply><apply id="S3.E4.m1.1.1.3.5.3.cmml" xref="S3.E4.m1.1.1.3.5.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.3.5.3.1.cmml" xref="S3.E4.m1.1.1.3.5.3">subscript</csymbol><ci id="S3.E4.m1.1.1.3.5.3.2.cmml" xref="S3.E4.m1.1.1.3.5.3.2">ùêø</ci><apply id="S3.E4.m1.1.1.3.5.3.3.cmml" xref="S3.E4.m1.1.1.3.5.3.3"><times id="S3.E4.m1.1.1.3.5.3.3.1.cmml" xref="S3.E4.m1.1.1.3.5.3.3.1"></times><ci id="S3.E4.m1.1.1.3.5.3.3.2.cmml" xref="S3.E4.m1.1.1.3.5.3.3.2">ùëê</ci><ci id="S3.E4.m1.1.1.3.5.3.3.3.cmml" xref="S3.E4.m1.1.1.3.5.3.3.3">ùëë</ci></apply></apply></apply><apply id="S3.E4.m1.1.1.3.6.cmml" xref="S3.E4.m1.1.1.3.6"><times id="S3.E4.m1.1.1.3.6.1.cmml" xref="S3.E4.m1.1.1.3.6.1"></times><apply id="S3.E4.m1.1.1.3.6.2.cmml" xref="S3.E4.m1.1.1.3.6.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.3.6.2.1.cmml" xref="S3.E4.m1.1.1.3.6.2">subscript</csymbol><ci id="S3.E4.m1.1.1.3.6.2.2.cmml" xref="S3.E4.m1.1.1.3.6.2.2">ùõæ</ci><cn type="integer" id="S3.E4.m1.1.1.3.6.2.3.cmml" xref="S3.E4.m1.1.1.3.6.2.3">5</cn></apply><apply id="S3.E4.m1.1.1.3.6.3.cmml" xref="S3.E4.m1.1.1.3.6.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.3.6.3.1.cmml" xref="S3.E4.m1.1.1.3.6.3">subscript</csymbol><ci id="S3.E4.m1.1.1.3.6.3.2.cmml" xref="S3.E4.m1.1.1.3.6.3.2">ùêø</ci><apply id="S3.E4.m1.1.1.3.6.3.3.cmml" xref="S3.E4.m1.1.1.3.6.3.3"><times id="S3.E4.m1.1.1.3.6.3.3.1.cmml" xref="S3.E4.m1.1.1.3.6.3.3.1"></times><ci id="S3.E4.m1.1.1.3.6.3.3.2.cmml" xref="S3.E4.m1.1.1.3.6.3.3.2">ùëí</ci><ci id="S3.E4.m1.1.1.3.6.3.3.3.cmml" xref="S3.E4.m1.1.1.3.6.3.3.3">ùëõ</ci><ci id="S3.E4.m1.1.1.3.6.3.3.4.cmml" xref="S3.E4.m1.1.1.3.6.3.3.4">ùë°</ci><ci id="S3.E4.m1.1.1.3.6.3.3.5.cmml" xref="S3.E4.m1.1.1.3.6.3.3.5">ùëü</ci><ci id="S3.E4.m1.1.1.3.6.3.3.6.cmml" xref="S3.E4.m1.1.1.3.6.3.3.6">ùëú</ci></apply></apply></apply><apply id="S3.E4.m1.1.1.3.7.cmml" xref="S3.E4.m1.1.1.3.7"><times id="S3.E4.m1.1.1.3.7.1.cmml" xref="S3.E4.m1.1.1.3.7.1"></times><apply id="S3.E4.m1.1.1.3.7.2.cmml" xref="S3.E4.m1.1.1.3.7.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.3.7.2.1.cmml" xref="S3.E4.m1.1.1.3.7.2">subscript</csymbol><ci id="S3.E4.m1.1.1.3.7.2.2.cmml" xref="S3.E4.m1.1.1.3.7.2.2">ùõæ</ci><cn type="integer" id="S3.E4.m1.1.1.3.7.2.3.cmml" xref="S3.E4.m1.1.1.3.7.2.3">6</cn></apply><apply id="S3.E4.m1.1.1.3.7.3.cmml" xref="S3.E4.m1.1.1.3.7.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.3.7.3.1.cmml" xref="S3.E4.m1.1.1.3.7.3">subscript</csymbol><ci id="S3.E4.m1.1.1.3.7.3.2.cmml" xref="S3.E4.m1.1.1.3.7.3.2">ùêø</ci><apply id="S3.E4.m1.1.1.3.7.3.3.cmml" xref="S3.E4.m1.1.1.3.7.3.3"><times id="S3.E4.m1.1.1.3.7.3.3.1.cmml" xref="S3.E4.m1.1.1.3.7.3.3.1"></times><ci id="S3.E4.m1.1.1.3.7.3.3.2.cmml" xref="S3.E4.m1.1.1.3.7.3.3.2">ùëü</ci><ci id="S3.E4.m1.1.1.3.7.3.3.3.cmml" xref="S3.E4.m1.1.1.3.7.3.3.3">ùëí</ci><ci id="S3.E4.m1.1.1.3.7.3.3.4.cmml" xref="S3.E4.m1.1.1.3.7.3.3.4">ùëî</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">L=\gamma_{1}L_{d}+\gamma_{2}L_{g}+\gamma_{3}L_{corr}+\gamma_{4}L_{cd}+\gamma_{5}L_{entro}+\gamma_{6}L_{reg}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S3.SS6.p1.10" class="ltx_p">where <math id="S3.SS6.p1.9.m1.1" class="ltx_Math" alttext="\gamma_{1}" display="inline"><semantics id="S3.SS6.p1.9.m1.1a"><msub id="S3.SS6.p1.9.m1.1.1" xref="S3.SS6.p1.9.m1.1.1.cmml"><mi id="S3.SS6.p1.9.m1.1.1.2" xref="S3.SS6.p1.9.m1.1.1.2.cmml">Œ≥</mi><mn id="S3.SS6.p1.9.m1.1.1.3" xref="S3.SS6.p1.9.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.9.m1.1b"><apply id="S3.SS6.p1.9.m1.1.1.cmml" xref="S3.SS6.p1.9.m1.1.1"><csymbol cd="ambiguous" id="S3.SS6.p1.9.m1.1.1.1.cmml" xref="S3.SS6.p1.9.m1.1.1">subscript</csymbol><ci id="S3.SS6.p1.9.m1.1.1.2.cmml" xref="S3.SS6.p1.9.m1.1.1.2">ùõæ</ci><cn type="integer" id="S3.SS6.p1.9.m1.1.1.3.cmml" xref="S3.SS6.p1.9.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.9.m1.1c">\gamma_{1}</annotation></semantics></math> to <math id="S3.SS6.p1.10.m2.1" class="ltx_Math" alttext="\gamma_{6}" display="inline"><semantics id="S3.SS6.p1.10.m2.1a"><msub id="S3.SS6.p1.10.m2.1.1" xref="S3.SS6.p1.10.m2.1.1.cmml"><mi id="S3.SS6.p1.10.m2.1.1.2" xref="S3.SS6.p1.10.m2.1.1.2.cmml">Œ≥</mi><mn id="S3.SS6.p1.10.m2.1.1.3" xref="S3.SS6.p1.10.m2.1.1.3.cmml">6</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.10.m2.1b"><apply id="S3.SS6.p1.10.m2.1.1.cmml" xref="S3.SS6.p1.10.m2.1.1"><csymbol cd="ambiguous" id="S3.SS6.p1.10.m2.1.1.1.cmml" xref="S3.SS6.p1.10.m2.1.1">subscript</csymbol><ci id="S3.SS6.p1.10.m2.1.1.2.cmml" xref="S3.SS6.p1.10.m2.1.1.2">ùõæ</ci><cn type="integer" id="S3.SS6.p1.10.m2.1.1.3.cmml" xref="S3.SS6.p1.10.m2.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.10.m2.1c">\gamma_{6}</annotation></semantics></math> are balance terms.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We evaluate our method on the NOCS-CAMERA dataset and NOCS-REAL dataset proposed by Wang et al.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, which currently are the most widely used and authoritative datasets for benchmarking category-level 6D object pose estimation methods. The NOCS-CAMERA dataset is a synthetic dataset that contains 300K RGBD images (with 25K for evaluation) generated by rendering and compositing synthetic objects into real scenes. The NOCS-REAL dataset is a real-world dataset that contains
4.3K real-world RGBD images from 7 scenes for training, and 2.75K real-world RGBD images from 6 scenes for evaluation. Both datasets consist of six categories, i.e., bottle, bowl, camera, can, laptop and mug.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Implementation Details</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We choose Mask-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> as the detector. The Image Encoder is a PSP-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> with a ResNet-18 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> backbone. The Shape Prior Encoder is a MLP. We first train our model for 50 epochs on the NOCS-CAMERA train set and evaluate it on the NOCS-CAMERA val set. Then, we fine-tune it on the NOCS-CAMERA train set and NOCS-REAL train set for 10 epochs. And we evaluate our fine-tuned model on the NOCS-REAL test set. Note we also have to recover the object size, we simply use the average size of <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="P_{c}+M_{d}" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><msub id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml"><mi id="S4.SS2.p1.1.m1.1.1.2.2" xref="S4.SS2.p1.1.m1.1.1.2.2.cmml">P</mi><mi id="S4.SS2.p1.1.m1.1.1.2.3" xref="S4.SS2.p1.1.m1.1.1.2.3.cmml">c</mi></msub><mo id="S4.SS2.p1.1.m1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.cmml">+</mo><msub id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml"><mi id="S4.SS2.p1.1.m1.1.1.3.2" xref="S4.SS2.p1.1.m1.1.1.3.2.cmml">M</mi><mi id="S4.SS2.p1.1.m1.1.1.3.3" xref="S4.SS2.p1.1.m1.1.1.3.3.cmml">d</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><plus id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1"></plus><apply id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.1.1.2.1.cmml" xref="S4.SS2.p1.1.m1.1.1.2">subscript</csymbol><ci id="S4.SS2.p1.1.m1.1.1.2.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2.2">ùëÉ</ci><ci id="S4.SS2.p1.1.m1.1.1.2.3.cmml" xref="S4.SS2.p1.1.m1.1.1.2.3">ùëê</ci></apply><apply id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.1.1.3.1.cmml" xref="S4.SS2.p1.1.m1.1.1.3">subscript</csymbol><ci id="S4.SS2.p1.1.m1.1.1.3.2.cmml" xref="S4.SS2.p1.1.m1.1.1.3.2">ùëÄ</ci><ci id="S4.SS2.p1.1.m1.1.1.3.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3.3">ùëë</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">P_{c}+M_{d}</annotation></semantics></math> as the result following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. More details are in the SuppMat.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2111.10524/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="230" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.3.2" class="ltx_text" style="font-size:90%;">Visualization of the prediction results of our method. Green boxes are the ground-truth and red boxes are our predictions. The top two rows are results on the NOCS-CAMERA dataset and the bottom two rows are results on the NOCS-REAL dataset.</span></figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Comparison with State-of-the-art</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.4" class="ltx_p">We compare the performance of ACR-Pose with some recent state-of-the-art methods including NOCS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, CASS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, SPD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, DualPoseNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> and FS-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, to verify the effectiveness of our method. All of them are strong baselines. For comparison, we compute the IoU50, IoU75, 5<sup id="S4.SS3.p1.4.1" class="ltx_sup"><span id="S4.SS3.p1.4.1.1" class="ltx_text ltx_font_italic">‚àò</span></sup>2cm, 5<sup id="S4.SS3.p1.4.2" class="ltx_sup"><span id="S4.SS3.p1.4.2.1" class="ltx_text ltx_font_italic">‚àò</span></sup>5cm, 10<sup id="S4.SS3.p1.4.3" class="ltx_sup"><span id="S4.SS3.p1.4.3.1" class="ltx_text ltx_font_italic">‚àò</span></sup>2cm and 10<sup id="S4.SS3.p1.4.4" class="ltx_sup"><span id="S4.SS3.p1.4.4.1" class="ltx_text ltx_font_italic">‚àò</span></sup>5cm metrics and report the mean Average Precision (mAP) to measure the performance.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.4" class="ltx_p"><span id="S4.SS3.p2.4.1" class="ltx_text ltx_font_bold">Results on NOCS-CAMERA:</span> Table <a href="#S3.T1" title="Table 1 ‚Ä£ 3.4 Shape Prior Deformation in Reconstructor ‚Ä£ 3 ACR-Pose ‚Ä£ ACR-Pose: Adversarial Canonical Representation Reconstruction Network for Category Level 6D Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the result of our method and the results of all competitors on the val set of NOCS-CAMERA. It is obvious that our method outperforms all the strong baselines for a large margin. For example, SPD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> shares the same backbone and the same Shape Prior Deformation step as our work, however, it is outperformed by our ACR-Pose by 16.3%, 15.1%, 9.3% and 6.3% in terms of the 5<sup id="S4.SS3.p2.4.2" class="ltx_sup"><span id="S4.SS3.p2.4.2.1" class="ltx_text ltx_font_italic">‚àò</span></sup>2cm, 5<sup id="S4.SS3.p2.4.3" class="ltx_sup"><span id="S4.SS3.p2.4.3.1" class="ltx_text ltx_font_italic">‚àò</span></sup>5cm, 10<sup id="S4.SS3.p2.4.4" class="ltx_sup"><span id="S4.SS3.p2.4.4.1" class="ltx_text ltx_font_italic">‚àò</span></sup>2cm and 10<sup id="S4.SS3.p2.4.5" class="ltx_sup"><span id="S4.SS3.p2.4.5.1" class="ltx_text ltx_font_italic">‚àò</span></sup>5cm, respectively, which is a significant improvement. The comparison between SPD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> and our method demonstrates that the improvement really comes from our adversarial reconstruction scheme, which also benefits from the two proposed modules in the Reconstructor. They have increased our model‚Äôs 6D object pose estimation performance by increasing the reconstruction quality and reality of the NOCS representations through adversarial training. Our method also outperforms the second-best method DualPoseNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> by 5.7%, 3.4%, 4.6%, 3.1% at the above four metrics respectively, which is also a great improvement. This further proves the effectiveness our method.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.4.4" class="ltx_tr">
<th id="S4.T3.4.4.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Versions</th>
<th id="S4.T3.4.4.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">IoU50</th>
<th id="S4.T3.4.4.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">IoU75</th>
<th id="S4.T3.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">5<sup id="S4.T3.1.1.1.1" class="ltx_sup"><span id="S4.T3.1.1.1.1.1" class="ltx_text ltx_font_italic">‚àò</span></sup>2cm</th>
<th id="S4.T3.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">5<sup id="S4.T3.2.2.2.1" class="ltx_sup"><span id="S4.T3.2.2.2.1.1" class="ltx_text ltx_font_italic">‚àò</span></sup>5cm</th>
<th id="S4.T3.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">10<sup id="S4.T3.3.3.3.1" class="ltx_sup"><span id="S4.T3.3.3.3.1.1" class="ltx_text ltx_font_italic">‚àò</span></sup>2cm</th>
<th id="S4.T3.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">10<sup id="S4.T3.4.4.4.1" class="ltx_sup"><span id="S4.T3.4.4.4.1.1" class="ltx_text ltx_font_italic">‚àò</span></sup>5cm</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.4.5.1" class="ltx_tr">
<td id="S4.T3.4.5.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Baseline</td>
<td id="S4.T3.4.5.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">93.3</td>
<td id="S4.T3.4.5.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">86.7</td>
<td id="S4.T3.4.5.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">58.6</td>
<td id="S4.T3.4.5.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">63.3</td>
<td id="S4.T3.4.5.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">76.4</td>
<td id="S4.T3.4.5.1.7" class="ltx_td ltx_align_center ltx_border_t">84.2</td>
</tr>
<tr id="S4.T3.4.6.2" class="ltx_tr">
<td id="S4.T3.4.6.2.1" class="ltx_td ltx_align_center ltx_border_r">+ Relational Instance Features</td>
<td id="S4.T3.4.6.2.2" class="ltx_td ltx_align_center ltx_border_r">93.8</td>
<td id="S4.T3.4.6.2.3" class="ltx_td ltx_align_center ltx_border_r">84.6</td>
<td id="S4.T3.4.6.2.4" class="ltx_td ltx_align_center ltx_border_r">62.4</td>
<td id="S4.T3.4.6.2.5" class="ltx_td ltx_align_center ltx_border_r">67.0</td>
<td id="S4.T3.4.6.2.6" class="ltx_td ltx_align_center ltx_border_r">78.8</td>
<td id="S4.T3.4.6.2.7" class="ltx_td ltx_align_center">85.7</td>
</tr>
<tr id="S4.T3.4.7.3" class="ltx_tr">
<td id="S4.T3.4.7.3.1" class="ltx_td ltx_align_center ltx_border_r">+ Pose Irrelevant Module</td>
<td id="S4.T3.4.7.3.2" class="ltx_td ltx_align_center ltx_border_r">93.4</td>
<td id="S4.T3.4.7.3.3" class="ltx_td ltx_align_center ltx_border_r">88.9</td>
<td id="S4.T3.4.7.3.4" class="ltx_td ltx_align_center ltx_border_r">65.5</td>
<td id="S4.T3.4.7.3.5" class="ltx_td ltx_align_center ltx_border_r">69.2</td>
<td id="S4.T3.4.7.3.6" class="ltx_td ltx_align_center ltx_border_r">79.7</td>
<td id="S4.T3.4.7.3.7" class="ltx_td ltx_align_center">85.2</td>
</tr>
<tr id="S4.T3.4.8.4" class="ltx_tr">
<td id="S4.T3.4.8.4.1" class="ltx_td ltx_align_center ltx_border_r">+ Adversarial Reconstruction</td>
<td id="S4.T3.4.8.4.2" class="ltx_td ltx_align_center ltx_border_r">93.7</td>
<td id="S4.T3.4.8.4.3" class="ltx_td ltx_align_center ltx_border_r">84.0</td>
<td id="S4.T3.4.8.4.4" class="ltx_td ltx_align_center ltx_border_r">67.4</td>
<td id="S4.T3.4.8.4.5" class="ltx_td ltx_align_center ltx_border_r">71.0</td>
<td id="S4.T3.4.8.4.6" class="ltx_td ltx_align_center ltx_border_r">81.2</td>
<td id="S4.T3.4.8.4.7" class="ltx_td ltx_align_center">86.3</td>
</tr>
<tr id="S4.T3.4.9.5" class="ltx_tr">
<td id="S4.T3.4.9.5.1" class="ltx_td ltx_align_center ltx_border_r">+ Relational Deformation Features</td>
<td id="S4.T3.4.9.5.2" class="ltx_td ltx_align_center ltx_border_r">93.7</td>
<td id="S4.T3.4.9.5.3" class="ltx_td ltx_align_center ltx_border_r">89.3</td>
<td id="S4.T3.4.9.5.4" class="ltx_td ltx_align_center ltx_border_r">68.4</td>
<td id="S4.T3.4.9.5.5" class="ltx_td ltx_align_center ltx_border_r">72.1</td>
<td id="S4.T3.4.9.5.6" class="ltx_td ltx_align_center ltx_border_r">81.9</td>
<td id="S4.T3.4.9.5.7" class="ltx_td ltx_align_center">87.0</td>
</tr>
<tr id="S4.T3.4.10.6" class="ltx_tr">
<td id="S4.T3.4.10.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">+ Relational Assignment Features (Full model)</td>
<td id="S4.T3.4.10.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T3.4.10.6.2.1" class="ltx_text ltx_font_bold">93.8</span></td>
<td id="S4.T3.4.10.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T3.4.10.6.3.1" class="ltx_text ltx_font_bold">89.9</span></td>
<td id="S4.T3.4.10.6.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T3.4.10.6.4.1" class="ltx_text ltx_font_bold">70.4</span></td>
<td id="S4.T3.4.10.6.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T3.4.10.6.5.1" class="ltx_text ltx_font_bold">74.1</span></td>
<td id="S4.T3.4.10.6.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T3.4.10.6.6.1" class="ltx_text ltx_font_bold">82.6</span></td>
<td id="S4.T3.4.10.6.7" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T3.4.10.6.7.1" class="ltx_text ltx_font_bold">87.8</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.6.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S4.T3.7.2" class="ltx_text" style="font-size:90%;">Results of ablation study. We gradually add our designs one by one to the baseline to investigate their impacts.</span></figcaption>
</figure>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p"><span id="S4.SS3.p3.1.1" class="ltx_text ltx_font_bold">Results on NOCS-REAL</span>: We also compare ACR-Pose with other methods on the NOCS-REAL test set. Results are shown in Table <a href="#S3.T2" title="Table 2 ‚Ä£ 3.4 Shape Prior Deformation in Reconstructor ‚Ä£ 3 ACR-Pose ‚Ä£ ACR-Pose: Adversarial Canonical Representation Reconstruction Network for Category Level 6D Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Our method is defeated by FS-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> at the IoU50 metric and defeated by DualPoseNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> at the 10<sup id="S4.SS3.p3.1.2" class="ltx_sup"><span id="S4.SS3.p3.1.2.1" class="ltx_text ltx_font_italic">‚àò</span></sup>5cm metric. However, these two metrics are relatively loose metrics. Besides, the training setting of FS-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> is strongly different from all the other baselines. For example, it uses the YOLO v3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> object detector to detect and crop image patches and depth regions, while all the other baselines use the Mask-RCNN detector. When it comes to other strict metrics, our method outperforms other baselines for a large margin. The excellent performance of our method on the NOCS-REAL dataset at more strict metrics demonstrate that ACR-Pose is powered with strong generalization ability and owns the potential of being deployed into real-world industry products. We contribute the superiority of our model into PIM, RRM and the adversarial reconstruction scheme.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p">We also illustrate the average precision (AP) vs. different thresholds on 3D IoU, rotation error, and translation error of our method on Fig. <a href="#S4.F5" title="Figure 5 ‚Ä£ 4.3 Comparison with State-of-the-art ‚Ä£ 4 Experiments ‚Ä£ ACR-Pose: Adversarial Canonical Representation Reconstruction Network for Category Level 6D Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. We compare our method with the mean result of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>. Both versions of our method achieve excellent performance. And our method performs well for all categories.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2111.10524/assets/x5.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="452" height="442" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.3.2" class="ltx_text" style="font-size:90%;">The average precision (AP) vs. different thresholds on 3D IoU, rotation error, and translation error.</span></figcaption>
</figure>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2111.10524/assets/x6.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="337" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.3.2" class="ltx_text" style="font-size:90%;">Visualization of failure cases.</span></figcaption>
</figure>
<div id="S4.SS3.p5" class="ltx_para">
<p id="S4.SS3.p5.1" class="ltx_p"><span id="S4.SS3.p5.1.1" class="ltx_text ltx_font_bold">Qualitative results</span>: To better exhibit the performance of ACR-Pose, we visualize some prediction results on Fig. <a href="#S4.F4" title="Figure 4 ‚Ä£ 4.2 Implementation Details ‚Ä£ 4 Experiments ‚Ä£ ACR-Pose: Adversarial Canonical Representation Reconstruction Network for Category Level 6D Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. It can be seen that our model can predict accurate 6D object pose because the visualized bounding boxes have high overlap with the ground-truth and they can frame the target objects tightly. Our method achieves excellent performance in both synthetic and real scenarios. Fig. <a href="#S4.F6" title="Figure 6 ‚Ä£ 4.3 Comparison with State-of-the-art ‚Ä£ 4 Experiments ‚Ä£ ACR-Pose: Adversarial Canonical Representation Reconstruction Network for Category Level 6D Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> also illustrates some failure cases. When there exists truncation or occlusion, our model sometimes fails. And some objects may be missed by the detector. We will explore how to solve these limitations in our future work. More visualization results can be found in the SuppMat.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Ablation Study</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">In this section, we conduct experiments on the NOCS-CAMERA dataset to investigate the effectiveness and necessity of our several design choices. In Table <a href="#S4.T3" title="Table 3 ‚Ä£ 4.3 Comparison with State-of-the-art ‚Ä£ 4 Experiments ‚Ä£ ACR-Pose: Adversarial Canonical Representation Reconstruction Network for Category Level 6D Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we replace all of our novel network designs in ACR-Pose as MLPs and then gradually add them back one by one to study their impact on the final 6D object pose estimation results.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p"><span id="S4.SS4.p2.1.1" class="ltx_text ltx_font_bold">Impact of the Relational Reconstruction Module:</span> The RRM is proposed to learn relational information between three different modalities in our Reconstructor. It includes three graphs that learn relational instance features, relational deformation features and relational assignment features respectively. From row 4, row 6 and row 7 of Table <a href="#S4.T3" title="Table 3 ‚Ä£ 4.3 Comparison with State-of-the-art ‚Ä£ 4 Experiments ‚Ä£ ACR-Pose: Adversarial Canonical Representation Reconstruction Network for Category Level 6D Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we can find that all the three kinds of features contribute to the accuracy improvement because adding them one by one cumulatively increases the model‚Äôs performance. The improvement comes from the message passing through graphs in the feature space. Using them, powerful semantic features can be learned by the model to better reconstruct the canonical representation. These semantics can effectively describe both object-specified shape and common category-level characteristics (category specialities), therefore, high-quality NOCS representations can be reconstructed. The above results demonstrate that relational features are essential for high-quality NOCS representation reconstruction.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p"><span id="S4.SS4.p3.1.1" class="ltx_text ltx_font_bold">Impact of the Pose-Irrelevant Module:</span> The PIM contributes to learning canonical-related features to filter rotation-related and translation-related information that is irrelevant to high-quality NOCS representation reconstruction in the Reconstructor. When it is added to our baseline, all evaluation metrics are increased by 1% to 3%. That is because, without the interference of rotation and translation, the model can learn more information about the shape of the object‚Äôs observed part, which is critical for reconstructing the NOCS representation.</p>
</div>
<div id="S4.SS4.p4" class="ltx_para">
<p id="S4.SS4.p4.1" class="ltx_p"><span id="S4.SS4.p4.1.1" class="ltx_text ltx_font_bold">Impact of Adversarial Reconstruction:</span> The adversarial reconstruction scheme plays a role of increasing reconstruction reality. Besides, since the Discriminator is only required at the training time, it is an very efficient strategy for improving performance. The 5th row of Table <a href="#S4.T3" title="Table 3 ‚Ä£ 4.3 Comparison with State-of-the-art ‚Ä£ 4 Experiments ‚Ä£ ACR-Pose: Adversarial Canonical Representation Reconstruction Network for Category Level 6D Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> evidently verifies that the adversarial training scheme would help the model learn better, more realistic and more high-quality canonical representations because adding it brings significant performance gains. That is because the reconstructed canonical representations would be more realistic after adding the Discriminator. Further, the following Umeyama algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> would also perform much better and more robust if it receives more realistic canonical representations.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we present an Adversarial Canonical Representation Reconstruction Network, the ACR-Pose, for accurate category-level 6D object pose estimation. Our work focuses on reconstructing high-quality canonical representations for the observed objects, given the RGB patch, the depth region, and the learned shape prior. To achieve so, we propose an adversarial reconstruction scheme, which competitively trains a Reconstructor and a Discriminator to improve reconstruction performance. Besides, within our reconstructor, a Pose-Irrelevant Module is introduced to better extract the shape information of the objects, as well as a Relational Reconstruction Module that leverages the relational information among the three input sources. Experiments are conducted on synthetic and real datasets, where the effectiveness of each novel module is demonstrated, and our overall model achieves state-of-the-art performance.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">One of the limitations of our method is that it may fail in the case of truncation or occlusion, which we aim to address as part of our future works.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Martin Arjovsky, Soumith Chintala, and L√©on Bottou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Wasserstein generative adversarial networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International conference on machine learning</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, pages
214‚Äì223. PMLR, 2017.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Ronald¬†T Azuma.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">A survey of augmented reality.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Presence: teleoperators &amp; virtual environments</span><span id="bib.bib2.4.2" class="ltx_text" style="font-size:90%;">, 6(4):355‚Äì385,
1997.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Antonio Bicchi and Vijay Kumar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Robotic grasping and contact: A review.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings 2000 ICRA. Millennium Conference. IEEE
International Conference on Robotics and Automation. Symposia Proceedings
(Cat. No. 00CH37065)</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, volume¬†1, pages 348‚Äì353. IEEE, 2000.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Julie Carmigniani, Borko Furht, Marco Anisetti, Paolo Ceravolo, Ernesto
Damiani, and Misa Ivkovic.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Augmented reality technologies, systems and applications.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Multimedia tools and applications</span><span id="bib.bib4.4.2" class="ltx_text" style="font-size:90%;">, 51(1):341‚Äì377, 2011.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Dengsheng Chen, Jun Li, Zheng Wang, and Kai Xu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Learning canonical shape space for category-level 6d object pose and
size estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, pages 11973‚Äì11982, 2020.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Wei Chen, Xi Jia, Hyung¬†Jin Chang, Jinming Duan, Linlin Shen, and Ales
Leonardis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Fs-net: Fast shape-based network for category-level 6d object pose
estimation with decoupled rotation mechanism.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, pages 1581‚Äì1590, 2021.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Zhaoxin Fan, Zhenbo Song, Wenping Zhang, Hongyan Liu, Jun He, and Xiaoyong Du.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Attentive rotation invariant convolution for point cloud-based large
scale place recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2108.12790</span><span id="bib.bib7.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Zhaoxin Fan, Yazhi Zhu, Yulin He, Qi Sun, Hongyan Liu, and Jun He.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Deep learning on monocular object pose detection and tracking: A
comprehensive overview.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2105.14291</span><span id="bib.bib8.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Generative adversarial nets.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib9.4.2" class="ltx_text" style="font-size:90%;">, 27, 2014.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Georgia Gkioxari, Piotr Doll√°r, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Mask r-cnn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE international conference on computer
vision</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, pages 2961‚Äì2969, 2017.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, pages 770‚Äì778, 2016.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Yisheng He, Haibin Huang, Haoqiang Fan, Qifeng Chen, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Ffb6d: A full flow bidirectional fusion network for 6d pose
estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, pages 3003‚Äì3013, 2021.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Yisheng He, Wei Sun, Haibin Huang, Jianran Liu, Haoqiang Fan, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Pvn3d: A deep point-wise 3d keypoints voting network for 6dof pose
estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, pages 11632‚Äì11641, 2020.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Jie Hu, Li Shen, and Gang Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Squeeze-and-excitation networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, pages 7132‚Äì7141, 2018.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Tero Karras, Samuli Laine, and Timo Aila.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">A style-based generator architecture for generative adversarial
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, pages 4401‚Äì4410, 2019.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Lei Ke, Shichao Li, Yanan Sun, Yu-Wing Tai, and Chi-Keung Tang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Gsnet: Joint vehicle pose and shape reconstruction with geometrical
and scene-aware supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, pages 515‚Äì532.
Springer, 2020.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Wadim Kehl, Fabian Manhardt, Federico Tombari, Slobodan Ilic, and Nassir Navab.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Ssd-6d: Making rgb-based 3d detection and 6d pose estimation great
again.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE international conference on computer
vision</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, pages 1521‚Äì1529, 2017.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Muhammed Kocabas, Nikos Athanasiou, and Michael¬†J Black.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Vibe: Video inference for human body pose and shape estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, pages 5253‚Äì5263, 2020.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Taeyeop Lee, Byeong-Uk Lee, Myungchul Kim, and In¬†So Kweon.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Category-level metric scale object shape and pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Robotics and Automation Letters</span><span id="bib.bib19.4.2" class="ltx_text" style="font-size:90%;">, 6(4):8575‚Äì8582, 2021.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Ruihui Li, Xianzhi Li, Chi-Wing Fu, Daniel Cohen-Or, and Pheng-Ann Heng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Pu-gan: a point cloud upsampling adversarial network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, pages 7203‚Äì7212, 2019.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Yi Li, Gu Wang, Xiangyang Ji, Yu Xiang, and Dieter Fox.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Deepim: Deep iterative matching for 6d pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision
(ECCV)</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, pages 683‚Äì698, 2018.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Zhigang Li, Gu Wang, and Xiangyang Ji.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Cdpn: Coordinates-based disentangled pose network for real-time
rgb-based 6-dof object pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, pages 7678‚Äì7687, 2019.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Jiehong Lin, Zewei Wei, Zhihao Li, Songcen Xu, Kui Jia, and Yuanqing Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Dualposenet: Category-level 6d object pose and size estimation using
dual pose network with refined learning of pose consistency.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2103.06526</span><span id="bib.bib23.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Ming-Yu Liu and Oncel Tuzel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Coupled generative adversarial networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib24.4.2" class="ltx_text" style="font-size:90%;">, 29:469‚Äì477,
2016.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Fabian Manhardt, Wadim Kehl, Nassir Navab, and Federico Tombari.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Deep model-based 6d pose refinement in rgb.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision
(ECCV)</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, pages 800‚Äì815, 2018.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Mehdi Mirza and Simon Osindero.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Conditional generative adversarial nets.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1411.1784</span><span id="bib.bib26.4.2" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Kiru Park, Timothy Patten, and Markus Vincze.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Pix2pose: Pixel-wise coordinate regression of objects for 6d pose
estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, pages 7668‚Äì7677, 2019.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Kiru Park, Timothy Patten, and Markus Vincze.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Pix2pose:pixel-wise coordinate regression of objects for 6d pose
estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, pages 7668‚Äì7677, 2019.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Pvnet: Pixel-wise voting network for 6dof pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, pages 4561‚Äì4570, 2019.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Joseph Redmon and Ali Farhadi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Yolov3: An incremental improvement.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1804.02767</span><span id="bib.bib30.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Szymon Rusinkiewicz and Marc Levoy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Efficient variants of the icp algorithm.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings third international conference on 3-D digital
imaging and modeling</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, pages 145‚Äì152. IEEE, 2001.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Ashutosh Saxena, Justin Driemeyer, and Andrew¬†Y Ng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Robotic grasping of novel objects using vision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">The International Journal of Robotics Research</span><span id="bib.bib32.4.2" class="ltx_text" style="font-size:90%;">, 27(2):157‚Äì173,
2008.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Juil Sock, Guillermo Garcia-Hernando, Anil Armagan, and Tae-Kyun Kim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Introducing pose consistency and warp-alignment for self-supervised
6d object pose estimation in color images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2020 International Conference on 3D Vision (3DV)</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:90%;">, pages
291‚Äì300. IEEE, 2020.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Chen Song, Jiaru Song, and Qixing Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Hybridpose: 6d object pose estimation under hybrid representations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">, pages 431‚Äì440, 2020.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Xibin Song, Peng Wang, Dingfu Zhou, Rui Zhu, Chenye Guan, Yuchao Dai, Hao Su,
Hongdong Li, and Ruigang Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Apollocar3d: A large 3d car instance understanding benchmark for
autonomous driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:90%;">, pages 5452‚Äì5462, 2019.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai
Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et¬†al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Scalability in perception for autonomous driving: Waymo open dataset.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:90%;">, pages 2446‚Äì2454, 2020.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Bugra Tekin, Sudipta¬†N Sinha, and Pascal Fua.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Real-time seamless single shot 6d object pose prediction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib37.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib37.5.3" class="ltx_text" style="font-size:90%;">, pages 292‚Äì301, 2018.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Meng Tian, Marcelo¬†H Ang, and Gim¬†Hee Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Shape prior deformation for categorical 6d object pose and size
estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</span><span id="bib.bib38.5.3" class="ltx_text" style="font-size:90%;">, pages 530‚Äì546.
Springer, 2020.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Jonathan Tremblay, Thang To, Balakumar Sundaralingam, Yu Xiang, Dieter Fox, and
Stan Birchfield.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">Deep object pose estimation for semantic robotic grasping of
household objects.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1809.10790</span><span id="bib.bib39.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Shinji Umeyama.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Least-squares estimation of transformation parameters between two
point patterns.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</span><span id="bib.bib40.4.2" class="ltx_text" style="font-size:90%;">,
13(04):376‚Äì380, 1991.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Petr V√°vra, Jan Roman, Pavel Zonƒça, Peter Ihn√°t, Martin
Nƒõmec, Jayant Kumar, Nagy Habib, and Ahmed El-Gendi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">Recent development of augmented reality in surgery: a review.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Journal of healthcare engineering</span><span id="bib.bib41.4.2" class="ltx_text" style="font-size:90%;">, 2017, 2017.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Gu Wang, Fabian Manhardt, Jianzhun Shao, Xiangyang Ji, Nassir Navab, and
Federico Tombari.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Self6d: Self-supervised monocular 6d object pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib42.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</span><span id="bib.bib42.5.3" class="ltx_text" style="font-size:90%;">, pages 108‚Äì125.
Springer, 2020.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran Song, and
Leonidas¬†J Guibas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Normalized object coordinate space for category-level 6d object pose
and size estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib43.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib43.5.3" class="ltx_text" style="font-size:90%;">, pages 2642‚Äì2651, 2019.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay¬†E Sarma, Michael¬†M Bronstein, and
Justin¬†M Solomon.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Dynamic graph cnn for learning on point clouds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Acm Transactions On Graphics (tog)</span><span id="bib.bib44.4.2" class="ltx_text" style="font-size:90%;">, 38(5):1‚Äì12, 2019.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Wenxuan Wu, Zhongang Qi, and Li Fuxin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">Pointconv: Deep convolutional networks on 3d point clouds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib45.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib45.5.3" class="ltx_text" style="font-size:90%;">, pages 9621‚Äì9630, 2019.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Yangzheng Wu, Mohsen Zand, Ali Etemad, and Michael Greenspan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">Vote from the center: 6 dof pose estimation in rgb-d images by radial
keypoint voting.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2104.02527</span><span id="bib.bib46.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, and Dieter Fox.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">Posecnn: A convolutional neural network for 6d object pose estimation
in cluttered scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1711.00199</span><span id="bib.bib47.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
Lin Yen-Chen, Pete Florence, Jonathan¬†T Barron, Alberto Rodriguez, Phillip
Isola, and Tsung-Yi Lin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">inerf: Inverting neural radiance fields for pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2012.05877</span><span id="bib.bib48.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
Yang You, Yujing Lou, Ruoxi Shi, Qi Liu, Yu-Wing Tai, Lizhuang Ma, Weiming
Wang, and Cewu Lu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">Prin/sprin: On extracting point-wise rotation invariant features.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2102.12093</span><span id="bib.bib49.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
Yang Yu, Zhiqiang Gong, Ping Zhong, and Jiaxin Shan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">Unsupervised representation learning with deep convolutional neural
network for remote sensing images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib50.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Image and Graphics</span><span id="bib.bib50.5.3" class="ltx_text" style="font-size:90%;">, pages
97‚Äì108. Springer, 2017.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text" style="font-size:90%;">
Sergey Zakharov, Ivan Shugurov, and Slobodan Ilic.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.2.1" class="ltx_text" style="font-size:90%;">Dpod: 6d pose object detector and refiner.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib51.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib51.5.3" class="ltx_text" style="font-size:90%;">, pages 1941‚Äì1950, 2019.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text" style="font-size:90%;">
Junzhe Zhang, Xinyi Chen, Zhongang Cai, Liang Pan, Haiyu Zhao, Shuai Yi,
Chai¬†Kiat Yeo, Bo Dai, and Chen¬†Change Loy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.2.1" class="ltx_text" style="font-size:90%;">Unsupervised 3d shape completion through gan inversion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib52.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib52.5.3" class="ltx_text" style="font-size:90%;">, pages 1768‚Äì1777, 2021.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text" style="font-size:90%;">
Wenqiang Zhang, Jiemin Fang, Xinggang Wang, and Wenyu Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.2.1" class="ltx_text" style="font-size:90%;">Efficientpose: Efficient human pose estimation with neural
architecture search.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Computational Visual Media</span><span id="bib.bib53.4.2" class="ltx_text" style="font-size:90%;">, 7(3):335‚Äì347, 2021.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text" style="font-size:90%;">
Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.2.1" class="ltx_text" style="font-size:90%;">Pyramid scene parsing network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib54.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib54.5.3" class="ltx_text" style="font-size:90%;">, pages 2881‚Äì2890, 2017.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text" style="font-size:90%;">
Junbo Zhao, Michael Mathieu, and Yann LeCun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.2.1" class="ltx_text" style="font-size:90%;">Energy-based generative adversarial network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1609.03126</span><span id="bib.bib55.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text" style="font-size:90%;">
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei¬†A Efros.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.2.1" class="ltx_text" style="font-size:90%;">Unpaired image-to-image translation using cycle-consistent
adversarial networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib56.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE international conference on computer
vision</span><span id="bib.bib56.5.3" class="ltx_text" style="font-size:90%;">, pages 2223‚Äì2232, 2017.
</span>
</span>
</li>
</ul>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Appendix</h2>

<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Implementation details</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.5" class="ltx_p">ACR-Pose is implemented by PyTorch. The model is optimized by the Adam optimizer with a batch size of 96. We choose Mask-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> as the detector. The Image Encoder is a PSP-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> with a ResNet-18 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> backbone. The Shape Prior Encoder is a MLP. The category instances are sampled from the ShapeNet dataset. The feature dimension C is set to 64. During training, we alternately update the parameters of the Reconstructor and the Discriminator. The initial learning rates for training the Reconstructor and the Discriminator are 0.0001 and 0.00001 respectively. When training on the NOCS-CAMERA train set only, the learning rate is decayed at the 10th, 30th and 40th epoch. The decay rates are 0.5, 0.1, 0.01 w.r.t the initial learning rate repetitively. When fine-tuning, the initial learning rates are also 0.0001 and 0.00001, and they are decayed by half at the 5th epoch. The image patch is resized to <math id="S6.SS1.p1.1.m1.1" class="ltx_Math" alttext="192\times 192" display="inline"><semantics id="S6.SS1.p1.1.m1.1a"><mrow id="S6.SS1.p1.1.m1.1.1" xref="S6.SS1.p1.1.m1.1.1.cmml"><mn id="S6.SS1.p1.1.m1.1.1.2" xref="S6.SS1.p1.1.m1.1.1.2.cmml">192</mn><mo lspace="0.222em" rspace="0.222em" id="S6.SS1.p1.1.m1.1.1.1" xref="S6.SS1.p1.1.m1.1.1.1.cmml">√ó</mo><mn id="S6.SS1.p1.1.m1.1.1.3" xref="S6.SS1.p1.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.1.m1.1b"><apply id="S6.SS1.p1.1.m1.1.1.cmml" xref="S6.SS1.p1.1.m1.1.1"><times id="S6.SS1.p1.1.m1.1.1.1.cmml" xref="S6.SS1.p1.1.m1.1.1.1"></times><cn type="integer" id="S6.SS1.p1.1.m1.1.1.2.cmml" xref="S6.SS1.p1.1.m1.1.1.2">192</cn><cn type="integer" id="S6.SS1.p1.1.m1.1.1.3.cmml" xref="S6.SS1.p1.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.1.m1.1c">192\times 192</annotation></semantics></math>. The number of object points (back-projected object depth) <math id="S6.SS1.p1.2.m2.1" class="ltx_Math" alttext="P" display="inline"><semantics id="S6.SS1.p1.2.m2.1a"><mi id="S6.SS1.p1.2.m2.1.1" xref="S6.SS1.p1.2.m2.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.2.m2.1b"><ci id="S6.SS1.p1.2.m2.1.1.cmml" xref="S6.SS1.p1.2.m2.1.1">ùëÉ</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.2.m2.1c">P</annotation></semantics></math> is 1024. The number of shape prior points is also 1024. The number of adjacent neighbors <math id="S6.SS1.p1.3.m3.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S6.SS1.p1.3.m3.1a"><mi id="S6.SS1.p1.3.m3.1.1" xref="S6.SS1.p1.3.m3.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.3.m3.1b"><ci id="S6.SS1.p1.3.m3.1.1.cmml" xref="S6.SS1.p1.3.m3.1.1">ùêæ</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.3.m3.1c">K</annotation></semantics></math> in the feature graphs is 36. The balance terms <math id="S6.SS1.p1.4.m4.1" class="ltx_Math" alttext="\gamma_{1}" display="inline"><semantics id="S6.SS1.p1.4.m4.1a"><msub id="S6.SS1.p1.4.m4.1.1" xref="S6.SS1.p1.4.m4.1.1.cmml"><mi id="S6.SS1.p1.4.m4.1.1.2" xref="S6.SS1.p1.4.m4.1.1.2.cmml">Œ≥</mi><mn id="S6.SS1.p1.4.m4.1.1.3" xref="S6.SS1.p1.4.m4.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.4.m4.1b"><apply id="S6.SS1.p1.4.m4.1.1.cmml" xref="S6.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S6.SS1.p1.4.m4.1.1.1.cmml" xref="S6.SS1.p1.4.m4.1.1">subscript</csymbol><ci id="S6.SS1.p1.4.m4.1.1.2.cmml" xref="S6.SS1.p1.4.m4.1.1.2">ùõæ</ci><cn type="integer" id="S6.SS1.p1.4.m4.1.1.3.cmml" xref="S6.SS1.p1.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.4.m4.1c">\gamma_{1}</annotation></semantics></math> to <math id="S6.SS1.p1.5.m5.1" class="ltx_Math" alttext="\gamma_{6}" display="inline"><semantics id="S6.SS1.p1.5.m5.1a"><msub id="S6.SS1.p1.5.m5.1.1" xref="S6.SS1.p1.5.m5.1.1.cmml"><mi id="S6.SS1.p1.5.m5.1.1.2" xref="S6.SS1.p1.5.m5.1.1.2.cmml">Œ≥</mi><mn id="S6.SS1.p1.5.m5.1.1.3" xref="S6.SS1.p1.5.m5.1.1.3.cmml">6</mn></msub><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.5.m5.1b"><apply id="S6.SS1.p1.5.m5.1.1.cmml" xref="S6.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S6.SS1.p1.5.m5.1.1.1.cmml" xref="S6.SS1.p1.5.m5.1.1">subscript</csymbol><ci id="S6.SS1.p1.5.m5.1.1.2.cmml" xref="S6.SS1.p1.5.m5.1.1.2">ùõæ</ci><cn type="integer" id="S6.SS1.p1.5.m5.1.1.3.cmml" xref="S6.SS1.p1.5.m5.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.5.m5.1c">\gamma_{6}</annotation></semantics></math> in the loss function are 0.1, 0.1, 1.0, 5.0, 0.0001 and 0.01, respectively. When fine-tuning, we re-initial the discriminator and select data randomly from NOCS-CAMERA and NOCS-REAL at a ratio of 3:1 to train the network, following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. All experiments are conducted on a single A6000 GPU.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Overall accuracy</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.4" class="ltx_p">Except for the mAP metric we mainly report, we also calculate the overall accuracy at the threshold of IoU50, IoU75, 5<sup id="S6.SS2.p1.4.1" class="ltx_sup"><span id="S6.SS2.p1.4.1.1" class="ltx_text ltx_font_italic">‚àò</span></sup>2cm, 5<sup id="S6.SS2.p1.4.2" class="ltx_sup"><span id="S6.SS2.p1.4.2.1" class="ltx_text ltx_font_italic">‚àò</span></sup>5cm, 10<sup id="S6.SS2.p1.4.3" class="ltx_sup"><span id="S6.SS2.p1.4.3.1" class="ltx_text ltx_font_italic">‚àò</span></sup>2cm and 10<sup id="S6.SS2.p1.4.4" class="ltx_sup"><span id="S6.SS2.p1.4.4.1" class="ltx_text ltx_font_italic">‚àò</span></sup>5cm. We show the results on Table <a href="#S6.T4" title="Table 4 ‚Ä£ 6.6 More visualization results ‚Ä£ 6 Appendix ‚Ä£ ACR-Pose: Adversarial Canonical Representation Reconstruction Network for Category Level 6D Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and compare our ACR-Pose with SPD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. Our method outperforms SPD for a large margin in terms of overall accuracy. This is consistent with the result of using mAP as an evaluation metric, which further indicates the superiority of our method.</p>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Generalization ability evaluation</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">To evaluate the generalization ability, we evaluate ACR-Pose on the NOCA-REAL test set using the model only trained on the synthetic NOCS-CAMERA dataset. We also compare our method with SPD here. The evaluation metric is mAP. Results are shown in Table <a href="#S6.T5" title="Table 5 ‚Ä£ 6.6 More visualization results ‚Ä£ 6 Appendix ‚Ä£ ACR-Pose: Adversarial Canonical Representation Reconstruction Network for Category Level 6D Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. Our method demonstrates better generalization ability towards real-world scenarios compared to SPD. However, there is still much room for performance improvement compared to the model trained on both NOCS-CAMERA and NOCS-REAL.</p>
</div>
</section>
<section id="S6.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4 </span>Reconstruction quality evaluation</h3>

<div id="S6.SS4.p1" class="ltx_para">
<p id="S6.SS4.p1.1" class="ltx_p">To verify whether our model can reconstruct high-quality NOCS representations, we evaluate the reconstruction quality of the results of SPD and ACR-Pose using the chamfer distance. Results are shown in Table <a href="#S6.T6" title="Table 6 ‚Ä£ 6.6 More visualization results ‚Ä£ 6 Appendix ‚Ä£ ACR-Pose: Adversarial Canonical Representation Reconstruction Network for Category Level 6D Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. Our method significantly outperforms SPD. Since we use the same Shape Prior Deformation step and Umeyama algorithm for NOCS representation reconstruction and object pose estimation, we claim that the mAP improvement really comes from the higher quality NOCS representation. Note our work mainly benefits from the Pose-Irrelevant Module, the Relational Reconstruction Module and the adversarial reconstruction scheme, while SPD doesn‚Äôt use these modules.</p>
</div>
</section>
<section id="S6.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.5 </span>Impact of adjacent neighbors</h3>

<div id="S6.SS5.p1" class="ltx_para">
<p id="S6.SS5.p1.1" class="ltx_p">For the Relational Reconstruction Module, in each graph, we use KNN to search for the adjacent neighbors for each node. In Table <a href="#S6.T7" title="Table 7 ‚Ä£ 6.6 More visualization results ‚Ä£ 6 Appendix ‚Ä£ ACR-Pose: Adversarial Canonical Representation Reconstruction Network for Category Level 6D Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, we show the impact of the number of adjacent neighbors K. We conclude that setting K as 36 is the best choice. When K is too small, it may cause a small receptive field for learning relationships between different modalities. While when it is too large, it may cause over-fitting and additional unnecessary computational costs.</p>
</div>
</section>
<section id="S6.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.6 </span>More visualization results</h3>

<div id="S6.SS6.p1" class="ltx_para">
<p id="S6.SS6.p1.1" class="ltx_p">To better understand the performance of our method. We illustrate more visualization results on Fig. <a href="#S6.F7" title="Figure 7 ‚Ä£ 6.6 More visualization results ‚Ä£ 6 Appendix ‚Ä£ ACR-Pose: Adversarial Canonical Representation Reconstruction Network for Category Level 6D Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> and Fig. <a href="#S6.F8" title="Figure 8 ‚Ä£ 6.6 More visualization results ‚Ä£ 6 Appendix ‚Ä£ ACR-Pose: Adversarial Canonical Representation Reconstruction Network for Category Level 6D Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. It can be seen that no matter in real-world scenarios or in synthetic scenarios, our model can accurately detect all the objects and recover their poses. We also show more failure cases in Fig. <a href="#S6.F9" title="Figure 9 ‚Ä£ 6.6 More visualization results ‚Ä£ 6 Appendix ‚Ä£ ACR-Pose: Adversarial Canonical Representation Reconstruction Network for Category Level 6D Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>. Except for truncation and occlusion, our method may also sometimes suffers from ambiguities caused by symmetry. In our future work, we will investigate how to solve these disadvantages.</p>
</div>
<figure id="S6.T4" class="ltx_table">
<table id="S6.T4.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T4.4.4" class="ltx_tr">
<th id="S6.T4.4.4.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Data</th>
<th id="S6.T4.4.4.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Methods</th>
<th id="S6.T4.4.4.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">IoU50</th>
<th id="S6.T4.4.4.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">IoU75</th>
<th id="S6.T4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">5<sup id="S6.T4.1.1.1.1" class="ltx_sup"><span id="S6.T4.1.1.1.1.1" class="ltx_text ltx_font_italic">‚àò</span></sup>2cm</th>
<th id="S6.T4.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">5<sup id="S6.T4.2.2.2.1" class="ltx_sup"><span id="S6.T4.2.2.2.1.1" class="ltx_text ltx_font_italic">‚àò</span></sup>5cm</th>
<th id="S6.T4.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">10<sup id="S6.T4.3.3.3.1" class="ltx_sup"><span id="S6.T4.3.3.3.1.1" class="ltx_text ltx_font_italic">‚àò</span></sup>2cm</th>
<th id="S6.T4.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">10<sup id="S6.T4.4.4.4.1" class="ltx_sup"><span id="S6.T4.4.4.4.1.1" class="ltx_text ltx_font_italic">‚àò</span></sup>5cm</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T4.4.5.1" class="ltx_tr">
<td id="S6.T4.4.5.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S6.T4.4.5.1.1.1" class="ltx_text">CAMERA</span></td>
<td id="S6.T4.4.5.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">SPD<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>
</td>
<td id="S6.T4.4.5.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">84.5</td>
<td id="S6.T4.4.5.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">78.8</td>
<td id="S6.T4.4.5.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">67.5</td>
<td id="S6.T4.4.5.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">71.2</td>
<td id="S6.T4.4.5.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">82.3</td>
<td id="S6.T4.4.5.1.8" class="ltx_td ltx_align_center ltx_border_t">88.1</td>
</tr>
<tr id="S6.T4.4.6.2" class="ltx_tr">
<td id="S6.T4.4.6.2.1" class="ltx_td ltx_align_center ltx_border_r">ACR-Pose(Ours)</td>
<td id="S6.T4.4.6.2.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S6.T4.4.6.2.2.1" class="ltx_text ltx_font_bold">84.8</span></td>
<td id="S6.T4.4.6.2.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S6.T4.4.6.2.3.1" class="ltx_text ltx_font_bold">82.6</span></td>
<td id="S6.T4.4.6.2.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S6.T4.4.6.2.4.1" class="ltx_text ltx_font_bold">80.4</span></td>
<td id="S6.T4.4.6.2.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S6.T4.4.6.2.5.1" class="ltx_text ltx_font_bold">83.0</span></td>
<td id="S6.T4.4.6.2.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S6.T4.4.6.2.6.1" class="ltx_text ltx_font_bold">88.6</span></td>
<td id="S6.T4.4.6.2.7" class="ltx_td ltx_align_center"><span id="S6.T4.4.6.2.7.1" class="ltx_text ltx_font_bold">92.1</span></td>
</tr>
<tr id="S6.T4.4.7.3" class="ltx_tr">
<td id="S6.T4.4.7.3.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" rowspan="2"><span id="S6.T4.4.7.3.1.1" class="ltx_text">Real</span></td>
<td id="S6.T4.4.7.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">SPD<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>
</td>
<td id="S6.T4.4.7.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">87.8</td>
<td id="S6.T4.4.7.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">69.4</td>
<td id="S6.T4.4.7.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">31.4</td>
<td id="S6.T4.4.7.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">34.4</td>
<td id="S6.T4.4.7.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">54.9</td>
<td id="S6.T4.4.7.3.8" class="ltx_td ltx_align_center ltx_border_t">63.1</td>
</tr>
<tr id="S6.T4.4.8.4" class="ltx_tr">
<td id="S6.T4.4.8.4.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">ACR-Pose(Ours)</td>
<td id="S6.T4.4.8.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S6.T4.4.8.4.2.1" class="ltx_text ltx_font_bold">91.1</span></td>
<td id="S6.T4.4.8.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S6.T4.4.8.4.3.1" class="ltx_text ltx_font_bold">79.2</span></td>
<td id="S6.T4.4.8.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S6.T4.4.8.4.4.1" class="ltx_text ltx_font_bold">44.8</span></td>
<td id="S6.T4.4.8.4.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S6.T4.4.8.4.5.1" class="ltx_text ltx_font_bold">49.9</span></td>
<td id="S6.T4.4.8.4.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S6.T4.4.8.4.6.1" class="ltx_text ltx_font_bold">65.0</span></td>
<td id="S6.T4.4.8.4.7" class="ltx_td ltx_align_center ltx_border_b"><span id="S6.T4.4.8.4.7.1" class="ltx_text ltx_font_bold">73.4</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S6.T4.6.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S6.T4.7.2" class="ltx_text" style="font-size:90%;">Performance of SPD and our method evaluated by the overall accuracy metric.</span></figcaption>
</figure>
<figure id="S6.T5" class="ltx_table">
<table id="S6.T5.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T5.4.4" class="ltx_tr">
<th id="S6.T5.4.4.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">Methods</th>
<th id="S6.T5.4.4.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">IoU50</th>
<th id="S6.T5.4.4.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">IoU75</th>
<th id="S6.T5.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">5<sup id="S6.T5.1.1.1.1" class="ltx_sup"><span id="S6.T5.1.1.1.1.1" class="ltx_text ltx_font_italic">‚àò</span></sup>2cm</th>
<th id="S6.T5.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">5<sup id="S6.T5.2.2.2.1" class="ltx_sup"><span id="S6.T5.2.2.2.1.1" class="ltx_text ltx_font_italic">‚àò</span></sup>5cm</th>
<th id="S6.T5.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">10<sup id="S6.T5.3.3.3.1" class="ltx_sup"><span id="S6.T5.3.3.3.1.1" class="ltx_text ltx_font_italic">‚àò</span></sup>2cm</th>
<th id="S6.T5.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">10<sup id="S6.T5.4.4.4.1" class="ltx_sup"><span id="S6.T5.4.4.4.1.1" class="ltx_text ltx_font_italic">‚àò</span></sup>5cm</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T5.4.5.1" class="ltx_tr">
<th id="S6.T5.4.5.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">SPD<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> CAMERA only)</th>
<td id="S6.T5.4.5.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">70.0</td>
<td id="S6.T5.4.5.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T5.4.5.1.3.1" class="ltx_text ltx_font_bold">37.9</span></td>
<td id="S6.T5.4.5.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11.2</td>
<td id="S6.T5.4.5.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">12.9</td>
<td id="S6.T5.4.5.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">32.2</td>
<td id="S6.T5.4.5.1.7" class="ltx_td ltx_align_center ltx_border_t">41.3</td>
</tr>
<tr id="S6.T5.4.6.2" class="ltx_tr">
<th id="S6.T5.4.6.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">ACR-Pose(CAMERA only)</th>
<td id="S6.T5.4.6.2.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S6.T5.4.6.2.2.1" class="ltx_text ltx_font_bold">70.0</span></td>
<td id="S6.T5.4.6.2.3" class="ltx_td ltx_align_center ltx_border_r">35.8</td>
<td id="S6.T5.4.6.2.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S6.T5.4.6.2.4.1" class="ltx_text ltx_font_bold">12.2</span></td>
<td id="S6.T5.4.6.2.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S6.T5.4.6.2.5.1" class="ltx_text ltx_font_bold">14.8</span></td>
<td id="S6.T5.4.6.2.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S6.T5.4.6.2.6.1" class="ltx_text ltx_font_bold">32.2</span></td>
<td id="S6.T5.4.6.2.7" class="ltx_td ltx_align_center"><span id="S6.T5.4.6.2.7.1" class="ltx_text ltx_font_bold">42.9</span></td>
</tr>
<tr id="S6.T5.4.7.3" class="ltx_tr">
<th id="S6.T5.4.7.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">ACR-Pose(CAMERA+REAL)</th>
<td id="S6.T5.4.7.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S6.T5.4.7.3.2.1" class="ltx_text ltx_font_bold">82.8</span></td>
<td id="S6.T5.4.7.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S6.T5.4.7.3.3.1" class="ltx_text ltx_font_bold">66.0</span></td>
<td id="S6.T5.4.7.3.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S6.T5.4.7.3.4.1" class="ltx_text ltx_font_bold">31.6</span></td>
<td id="S6.T5.4.7.3.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S6.T5.4.7.3.5.1" class="ltx_text ltx_font_bold">36.9</span></td>
<td id="S6.T5.4.7.3.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S6.T5.4.7.3.6.1" class="ltx_text ltx_font_bold">54.8</span></td>
<td id="S6.T5.4.7.3.7" class="ltx_td ltx_align_center ltx_border_b"><span id="S6.T5.4.7.3.7.1" class="ltx_text ltx_font_bold">65.9</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S6.T5.6.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="S6.T5.7.2" class="ltx_text" style="font-size:90%;">Results of generalization ability evaluation.</span></figcaption>
</figure>
<figure id="S6.T6" class="ltx_table">
<table id="S6.T6.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T6.2.1.1" class="ltx_tr">
<th id="S6.T6.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Data</th>
<th id="S6.T6.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Methods</th>
<th id="S6.T6.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">bottle</th>
<th id="S6.T6.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">bowl</th>
<th id="S6.T6.2.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">camera</th>
<th id="S6.T6.2.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">can</th>
<th id="S6.T6.2.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">laptop</th>
<th id="S6.T6.2.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">mug</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T6.2.2.1" class="ltx_tr">
<td id="S6.T6.2.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S6.T6.2.2.1.1.1" class="ltx_text">CAMERA</span></td>
<td id="S6.T6.2.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">SPD<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>
</td>
<td id="S6.T6.2.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.0235</td>
<td id="S6.T6.2.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.0142</td>
<td id="S6.T6.2.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.0196</td>
<td id="S6.T6.2.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.0262</td>
<td id="S6.T6.2.2.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.0153</td>
<td id="S6.T6.2.2.1.8" class="ltx_td ltx_align_center ltx_border_t">0.0187</td>
</tr>
<tr id="S6.T6.2.3.2" class="ltx_tr">
<td id="S6.T6.2.3.2.1" class="ltx_td ltx_align_center ltx_border_r">ACR-Pose(Ours)</td>
<td id="S6.T6.2.3.2.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S6.T6.2.3.2.2.1" class="ltx_text ltx_font_bold">0.0216</span></td>
<td id="S6.T6.2.3.2.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S6.T6.2.3.2.3.1" class="ltx_text ltx_font_bold">0.0137</span></td>
<td id="S6.T6.2.3.2.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S6.T6.2.3.2.4.1" class="ltx_text ltx_font_bold">0.0185</span></td>
<td id="S6.T6.2.3.2.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S6.T6.2.3.2.5.1" class="ltx_text ltx_font_bold">0.0253</span></td>
<td id="S6.T6.2.3.2.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S6.T6.2.3.2.6.1" class="ltx_text ltx_font_bold">
0.0137</span></td>
<td id="S6.T6.2.3.2.7" class="ltx_td ltx_align_center"><span id="S6.T6.2.3.2.7.1" class="ltx_text ltx_font_bold">0.0172</span></td>
</tr>
<tr id="S6.T6.2.4.3" class="ltx_tr">
<td id="S6.T6.2.4.3.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" rowspan="2"><span id="S6.T6.2.4.3.1.1" class="ltx_text">Real</span></td>
<td id="S6.T6.2.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">SPD<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>
</td>
<td id="S6.T6.2.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.0251</td>
<td id="S6.T6.2.4.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.0192</td>
<td id="S6.T6.2.4.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.0209</td>
<td id="S6.T6.2.4.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.0213</td>
<td id="S6.T6.2.4.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.0207</td>
<td id="S6.T6.2.4.3.8" class="ltx_td ltx_align_center ltx_border_t">0.0210</td>
</tr>
<tr id="S6.T6.2.5.4" class="ltx_tr">
<td id="S6.T6.2.5.4.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">ACR-Pose(Ours)</td>
<td id="S6.T6.2.5.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S6.T6.2.5.4.2.1" class="ltx_text ltx_font_bold">0.0235</span></td>
<td id="S6.T6.2.5.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S6.T6.2.5.4.3.1" class="ltx_text ltx_font_bold">0.0157</span></td>
<td id="S6.T6.2.5.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S6.T6.2.5.4.4.1" class="ltx_text ltx_font_bold">0.0196</span></td>
<td id="S6.T6.2.5.4.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S6.T6.2.5.4.5.1" class="ltx_text ltx_font_bold">0.0182</span></td>
<td id="S6.T6.2.5.4.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S6.T6.2.5.4.6.1" class="ltx_text ltx_font_bold">0.0180</span></td>
<td id="S6.T6.2.5.4.7" class="ltx_td ltx_align_center ltx_border_b"><span id="S6.T6.2.5.4.7.1" class="ltx_text ltx_font_bold">0.0175</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S6.T6.3.1.1" class="ltx_text" style="font-size:90%;">Table 6</span>: </span><span id="S6.T6.4.2" class="ltx_text" style="font-size:90%;">Reconstruction quality evaluation. The evaluation metric is chamfer distance.</span></figcaption>
</figure>
<figure id="S6.T7" class="ltx_table">
<table id="S6.T7.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T7.4.4" class="ltx_tr">
<th id="S6.T7.4.4.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">Versions</th>
<th id="S6.T7.4.4.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">IoU50</th>
<th id="S6.T7.4.4.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">IoU75</th>
<th id="S6.T7.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">5<sup id="S6.T7.1.1.1.1" class="ltx_sup"><span id="S6.T7.1.1.1.1.1" class="ltx_text ltx_font_italic">‚àò</span></sup>2cm</th>
<th id="S6.T7.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">5<sup id="S6.T7.2.2.2.1" class="ltx_sup"><span id="S6.T7.2.2.2.1.1" class="ltx_text ltx_font_italic">‚àò</span></sup>5cm</th>
<th id="S6.T7.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">10<sup id="S6.T7.3.3.3.1" class="ltx_sup"><span id="S6.T7.3.3.3.1.1" class="ltx_text ltx_font_italic">‚àò</span></sup>2cm</th>
<th id="S6.T7.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">10<sup id="S6.T7.4.4.4.1" class="ltx_sup"><span id="S6.T7.4.4.4.1.1" class="ltx_text ltx_font_italic">‚àò</span></sup>5cm</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T7.4.5.1" class="ltx_tr">
<th id="S6.T7.4.5.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">K=24</th>
<th id="S6.T7.4.5.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">94.5</th>
<th id="S6.T7.4.5.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">93.7</th>
<td id="S6.T7.4.5.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">69.9</td>
<td id="S6.T7.4.5.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">73.7</td>
<td id="S6.T7.4.5.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">82.2</td>
<td id="S6.T7.4.5.1.7" class="ltx_td ltx_align_center ltx_border_t">87.5</td>
</tr>
<tr id="S6.T7.4.6.2" class="ltx_tr">
<th id="S6.T7.4.6.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">K=36</th>
<th id="S6.T7.4.6.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S6.T7.4.6.2.2.1" class="ltx_text ltx_font_bold">94.5</span></th>
<th id="S6.T7.4.6.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S6.T7.4.6.2.3.1" class="ltx_text ltx_font_bold">93.8</span></th>
<td id="S6.T7.4.6.2.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S6.T7.4.6.2.4.1" class="ltx_text ltx_font_bold">70.4</span></td>
<td id="S6.T7.4.6.2.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S6.T7.4.6.2.5.1" class="ltx_text ltx_font_bold">74.1</span></td>
<td id="S6.T7.4.6.2.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S6.T7.4.6.2.6.1" class="ltx_text ltx_font_bold">82.6</span></td>
<td id="S6.T7.4.6.2.7" class="ltx_td ltx_align_center"><span id="S6.T7.4.6.2.7.1" class="ltx_text ltx_font_bold">87.8</span></td>
</tr>
<tr id="S6.T7.4.7.3" class="ltx_tr">
<th id="S6.T7.4.7.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">K=48</th>
<th id="S6.T7.4.7.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">94.4</th>
<th id="S6.T7.4.7.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">93.7</th>
<td id="S6.T7.4.7.3.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">70.1</td>
<td id="S6.T7.4.7.3.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">73.7</td>
<td id="S6.T7.4.7.3.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">82.8</td>
<td id="S6.T7.4.7.3.7" class="ltx_td ltx_align_center ltx_border_b">87.8</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S6.T7.6.1.1" class="ltx_text" style="font-size:90%;">Table 7</span>: </span><span id="S6.T7.7.2" class="ltx_text" style="font-size:90%;">Impact of the number of adjacency neighbors.</span></figcaption>
</figure>
<figure id="S6.F7" class="ltx_figure"><img src="/html/2111.10524/assets/x7.png" id="S6.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="226" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S6.F7.3.2" class="ltx_text" style="font-size:90%;">More visualization results on NOCA-CAMERA dataset.</span></figcaption>
</figure>
<figure id="S6.F8" class="ltx_figure"><img src="/html/2111.10524/assets/x8.png" id="S6.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="225" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F8.2.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S6.F8.3.2" class="ltx_text" style="font-size:90%;">More visualization results on NOCA-REAL dataset.</span></figcaption>
</figure>
<figure id="S6.F9" class="ltx_figure"><img src="/html/2111.10524/assets/x9.png" id="S6.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="223" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F9.2.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="S6.F9.3.2" class="ltx_text" style="font-size:90%;">More visualization results of failure cases.</span></figcaption>
</figure>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2111.10523" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2111.10524" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2111.10524">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2111.10524" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2111.10525" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar 19 16:58:28 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
