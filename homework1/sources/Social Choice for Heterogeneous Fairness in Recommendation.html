<!DOCTYPE html><html lang="en" data-theme="light"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<title>Social Choice for Heterogeneous Fairness in Recommendation</title>
<!--Generated on Sun Oct  6 16:53:09 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">




<meta content="recommender systems,  fairness,  computational social choice" lang="en" name="keywords">
<base href="https://arxiv.org/html/2410.04551v1/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2410.04551v1">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" title="Toggle dark/light mode" aria-label="System preference">
        <label id="automatic-tog" class="toggle-icon" title="Switch to light mode" for="__palette_3">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"></path></svg>
        </label>
        
        
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="https://arxiv.org/html/2410.04551v1/#myForm">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2410.04551v1">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2410.04551v1" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" title="Toggle dark/light mode">
        <label id="automatic-tog" class="toggle-icon" title="Switch to light mode" for="__palette_3">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"></path></svg>
        </label>
        
        
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC active" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist"><li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#abstract" title="Abstract">
      <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref"></span>
        Abstract
      </span>
    </a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#S1" title="In Social Choice for Heterogeneous Fairness in Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#S2" title="In Social Choice for Heterogeneous Fairness in Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#S3" title="In Social Choice for Heterogeneous Fairness in Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>The SCRUF-D Architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#S4" title="In Social Choice for Heterogeneous Fairness in Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Fairness Agents</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#S5" title="In Social Choice for Heterogeneous Fairness in Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#S5.SS1" title="In 5. Methodology ‣ Social Choice for Heterogeneous Fairness in Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Agent Definitions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#S5.SS2" title="In 5. Methodology ‣ Social Choice for Heterogeneous Fairness in Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Mechanisms</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#S5.SS3" title="In 5. Methodology ‣ Social Choice for Heterogeneous Fairness in Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#S6" title="In Social Choice for Heterogeneous Fairness in Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#S7" title="In Social Choice for Heterogeneous Fairness in Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib" title="References">
      <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref"></span>
        References
      </span>
    </a></li></ol></nav>

<div class="ltx_page_content"><div id="target-section" class="section"><a id="license-tr" href="https://info.arxiv.org/help/license/index.html#licenses-available">License: CC BY 4.0</a><div id="watermark-tr">arXiv:2410.04551v1 [cs.IR] 06 Oct 2024</div></div>
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Social Choice for Heterogeneous Fairness in Recommendation</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Amanda Aird
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:amanda.aird@colorado.edu">amanda.aird@colorado.edu</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">Department of Information Science, University of Colorado, Boulder</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Boulder</span><span class="ltx_text ltx_affiliation_state" id="id3.3.id3">Colorado</span><span class="ltx_text ltx_affiliation_country" id="id4.4.id4">USA</span><span class="ltx_text ltx_affiliation_postcode" id="id5.5.id5">80309</span>
</span></span></span>
<span class="ltx_author_before">,&nbsp;</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Elena S̆tefancová
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:elena.stefancova@fmph.uniba.sk">elena.stefancova@fmph.uniba.sk</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id6.1.id1">Comenius University Bratislava</span><span class="ltx_text ltx_affiliation_streetaddress" id="id7.2.id2"></span><span class="ltx_text ltx_affiliation_city" id="id8.3.id3">Bratislava</span><span class="ltx_text ltx_affiliation_country" id="id9.4.id4">Slovakia</span>
</span></span></span>
<span class="ltx_author_before">,&nbsp;</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Cassidy All
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:cassidy.all@colorado.edu">cassidy.all@colorado.edu</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id10.1.id1">Department of Information Science, University of Colorado, Boulder</span><span class="ltx_text ltx_affiliation_city" id="id11.2.id2">Boulder</span><span class="ltx_text ltx_affiliation_state" id="id12.3.id3">Colorado</span><span class="ltx_text ltx_affiliation_country" id="id13.4.id4">USA</span><span class="ltx_text ltx_affiliation_postcode" id="id14.5.id5">80309</span>
</span></span></span>
<span class="ltx_author_before">,&nbsp;</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Amy Voida
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:amy.voida@colorado.edu">amy.voida@colorado.edu</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id15.1.id1">Department of Information Science, University of Colorado, Boulder</span><span class="ltx_text ltx_affiliation_city" id="id16.2.id2">Boulder</span><span class="ltx_text ltx_affiliation_state" id="id17.3.id3">Colorado</span><span class="ltx_text ltx_affiliation_country" id="id18.4.id4">USA</span><span class="ltx_text ltx_affiliation_postcode" id="id19.5.id5">80309</span>
</span></span></span>
<span class="ltx_author_before">,&nbsp;</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Martin Homola
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:homola@fmph.uniba.sk">homola@fmph.uniba.sk</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id20.1.id1">Comenius University Bratislava</span><span class="ltx_text ltx_affiliation_streetaddress" id="id21.2.id2"></span><span class="ltx_text ltx_affiliation_city" id="id22.3.id3">Bratislava</span><span class="ltx_text ltx_affiliation_country" id="id23.4.id4">Slovakia</span>
</span></span></span>
<span class="ltx_author_before">,&nbsp;</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Nicholas Mattei
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:nsmattei@tulane.edu">nsmattei@tulane.edu</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id24.1.id1">Department of Computer Science, Tulane University</span><span class="ltx_text ltx_affiliation_city" id="id25.2.id2">New Orleans</span><span class="ltx_text ltx_affiliation_state" id="id26.3.id3">Louisiana</span><span class="ltx_text ltx_affiliation_country" id="id27.4.id4">USA</span><span class="ltx_text ltx_affiliation_postcode" id="id28.5.id5">70118</span>
</span></span></span>
<span class="ltx_author_before">&nbsp;and&nbsp;</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Robin Burke
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:robin.burke@colorado.edu">robin.burke@colorado.edu</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0001-5766-6434" title="ORCID identifier">0000-0001-5766-6434</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id29.1.id1">Department of Information Science, University of Colorado, Boulder</span><span class="ltx_text ltx_affiliation_city" id="id30.2.id2">Boulder</span><span class="ltx_text ltx_affiliation_state" id="id31.3.id3">Colorado</span><span class="ltx_text ltx_affiliation_country" id="id32.4.id4">USA</span><span class="ltx_text ltx_affiliation_postcode" id="id33.5.id5">80309</span>
</span></span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_dates">(2024)</div>
<div class="ltx_abstract" id="abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id34.id1">Algorithmic fairness in recommender systems requires close attention to the needs of a diverse set of stakeholders that may have competing interests. Previous work in this area has often been limited by fixed, single-objective definitions of fairness, built into algorithms or optimization criteria that are applied to a single fairness dimension or, at most, applied identically across dimensions. These narrow conceptualizations limit the ability to adapt fairness-aware solutions to the wide range of stakeholder needs and fairness definitions that arise in practice. Our work approaches recommendation fairness from the standpoint of computational social choice, using a multi-agent framework. In this paper, we explore the properties of different social choice mechanisms and demonstrate the successful integration of multiple, heterogeneous fairness definitions across multiple data sets.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_keywords">recommender systems, fairness, computational social choice
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmlicensed</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>rightsretained</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>18th ACM Conference on Recommender Systems; October 14–18, 2024; Bari, Italy</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_booktitle" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">booktitle: </span>18th ACM Conference on Recommender Systems (RecSys ’24), October 14–18, 2024, Bari, Italy</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id6"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>10.1145/3640457.3691706</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_isbn" id="id7"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>979-8-4007-0505-2/24/10</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id8"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Information systems&nbsp;Recommender systems</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id9"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies&nbsp;Multi-agent systems</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id10"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Social and professional topics&nbsp;User characteristics</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Fairness in algorithmic systems is a complex and multi-faceted area that is of research and practical concern. Different definitions may apply in different contexts, to different stakeholders, and in different types of applications. This complexity is particularly evident in recommender systems <cite class="ltx_cite ltx_citemacro_citep">(Sonboli et&nbsp;al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib30" title="">2022b</a>)</cite> where there may be stakeholders of interest on the consumer or provider side of the recommendation interaction, or both <cite class="ltx_cite ltx_citemacro_citep">(Burke, <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib7" title="">2017</a>)</cite>. In the literature on fair recommender systems, a plethora of fairness definitions have emerged in the literature <cite class="ltx_cite ltx_citemacro_citep">(Ekstrand et&nbsp;al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib12" title="">2022</a>; Smith et&nbsp;al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib28" title="">2023</a>)</cite>, each with its own logic and associated algorithmic techniques. What nearly all of this literature has in common is two simplifying assumptions that are severely limiting to the potential applications of these ideas, and a barrier to deploying fair recommender systems in practice <cite class="ltx_cite ltx_citemacro_citep">(Cramer et&nbsp;al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib10" title="">2019</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The first limitation is an inability to capture the <span class="ltx_text ltx_font_italic" id="S1.p2.1.1">multiplicity</span> of fairness issues. With few exceptions <cite class="ltx_cite ltx_citemacro_citep">(Wu et&nbsp;al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib34" title="">2022</a>; Zehlike et&nbsp;al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib35" title="">2022</a>; Aird et&nbsp;al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib3" title="">2024</a>)</cite>, fairness-aware recommender systems apply a single fairness definition to a single protected group, potentially one on each side of the interaction. This limitation is not realistic, as in most application contexts there will be multiple dimensions of fairness that need to be implemented. A second limitation is the assumption that a <span class="ltx_text ltx_font_italic" id="S1.p2.1.2">single fairness definition</span> will be appropriate, regardless of the protected group or the area of application. An architecture that assumes fairness will always take a certain form and measure is limited in its applicability <cite class="ltx_cite ltx_citemacro_citep">(Selbst et&nbsp;al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib27" title="">2019</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">These limitations were addressed by the introduction of the Social Choice for Recommendation Fairness - Dynamic (SCRUF-D) architecture for fairness-aware re-ranking <cite class="ltx_cite ltx_citemacro_citep">(Burke et&nbsp;al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib8" title="">2022</a>; Aird et&nbsp;al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib2" title="">2023</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib3" title="">2024</a>)</cite>. In this paper, we extend prior results with two agents with the same underlying fairness definition to three agents with differing fairness definitions, showing that the system can support multiple heterogeneous fairness definitions simultaneously. We examine how different social choice mechanisms offer different trade-offs (and sometimes no trade-off) between fairness and accuracy.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Fairness in machine learning, especially in classification settings, is a popular topic, including formalizing definitions of fairness&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Chouldechova, <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib9" title="">2017</a>; Dwork et&nbsp;al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib11" title="">2012</a>; Hardt et&nbsp;al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib14" title="">2016</a>; Narayanan, <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib22" title="">2018</a>; Hutchinson and Mitchell, <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib18" title="">2019</a>)</cite> and offering algorithmic techniques to mitigate unfairness <cite class="ltx_cite ltx_citemacro_citep">(Kamiran et&nbsp;al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib19" title="">2010</a>; Pedreshi et&nbsp;al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib26" title="">2008</a>; Zemel et&nbsp;al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib36" title="">2013</a>; Zhang and Wu, <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib37" title="">2017</a>)</cite>. However, despite these recent research efforts, the state-of-the-art offers little concrete guidance to industry practitioners <cite class="ltx_cite ltx_citemacro_citep">(Cramer et&nbsp;al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib10" title="">2019</a>; Holstein et&nbsp;al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib17" title="">2019</a>)</cite>. The unique problems of fairness in recommender systems have also been studied; see <cite class="ltx_cite ltx_citemacro_citet">Ekstrand et&nbsp;al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib12" title="">2022</a>)</cite> for an overview. In recommendation, fairness concerns may arise on either the consumer side or on the provider side&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Burke, <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib7" title="">2017</a>)</cite>.
Other machine learning environments, such as classification, generally only need to consider the fairness properties of the system relative to the individuals being classified.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Both in the machine learning and the recommender systems formulation of fairness, there has been little recognition of the intersection of multiple fairness definitions and dimensions, although recent work has noted the benefits of combining multiple fairness definitions&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Beutel et&nbsp;al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib4" title="">2019</a>; Patro et&nbsp;al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib25" title="">2022</a>)</cite>. Most existing research considers only a single protected class. Even in cases where multiple groups are considered <cite class="ltx_cite ltx_citemacro_citep">(Buolamwini and Gebru, <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib6" title="">2018</a>; Hébert-Johnson et&nbsp;al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib16" title="">2018</a>; Kearns et&nbsp;al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib20" title="">2018</a>; Zhu et&nbsp;al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib38" title="">2018</a>)</cite>, fairness is defined the same way for all groups.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Many of the recent works on fair recommendation take a particular definition of fairness, and train a ranking model that includes fairness as a type of regularization on the subsequent loss function <cite class="ltx_cite ltx_citemacro_citep">(Patro et&nbsp;al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib24" title="">2020</a>; Sühr et&nbsp;al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib33" title="">2019</a>; Wu et&nbsp;al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib34" title="">2022</a>)</cite>. Such algorithms are brittle to changes in the definitions or measures of fairness. Post processing of the recommendations lists, often called re-ranking, is another popular method <cite class="ltx_cite ltx_citemacro_citep">(Ferraro et&nbsp;al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib13" title="">2021</a>)</cite> and one that we employ here for its ability to allow multiple and changeable definitions of fairness.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>The SCRUF-D Architecture</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">The Social Choice for Recommendation Under Fairness - Dynamic (SCRUF-D) platform was introduced in <cite class="ltx_cite ltx_citemacro_citet">Aird et&nbsp;al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib3" title="">2024</a>)</cite>, as an extension to a simpler version found in <cite class="ltx_cite ltx_citemacro_citet">Sonboli et&nbsp;al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib31" title="">2020a</a>)</cite>. At a high level, SCRUF-D embodies the fairness concerns of multiple stakeholders as a set of <em class="ltx_emph ltx_font_italic" id="S3.p1.1.1">fairness agents</em> that are <em class="ltx_emph ltx_font_italic" id="S3.p1.1.2">allocated</em> to a user when recommendations are being generated. Once the agents are allocated to a user, their ordering of items is aggregated with the user’s preferences (here coming from a traditional recommendation algorithm) by using a <em class="ltx_emph ltx_font_italic" id="S3.p1.1.3">choice mechanism</em>, and the result is delivered to the user as their recommendations.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">A fairness agent is defined by three functions: a fairness metric which is able to evaluate the history of the system and judge how fair the system has been to the agent over some time window, a compatibility metric which is the agent’s evaluation of how much they want to be matched to a particular user, and a ranking function which expresses the agent’s ordering/score of items.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">When a user arrives at the system, the set of agents express their preference for being matched with this user and their evaluation of how fair the system has been. These scores are passed to an <em class="ltx_emph ltx_font_italic" id="S3.p3.1.1">allocation mechanism</em> which outputs a (randomized) allocation over the set of agents. Formally this is a probability distribution over the agents, and we examine different allocation functions. Once we have an allocation, the system generates an initial recommendation scoring of the items for the user using any standard recommendation algorithm. Each of the allocated fairness agents also express their ranking/scores of the items to give us a set of lists of items. These lists are then passed through a <em class="ltx_emph ltx_font_italic" id="S3.p3.1.2">choice function</em>, which aggregates these lists into a final recommendation.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Fairness Agents</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">By necessity, our choice of fairness metrics is somewhat arbitrary. While our metrics are informed by research in particular application areas, we have not yet engaged in the full cycle of stakeholder consultation that would be required to formulate specific fairness metrics appropriate to each stakeholder group. Note also that we are examining only provider-side group fairness; consumer-side and individual fairness we leave to future work.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">The literature of provider-side fairness measures in recommender systems is extensive; see <cite class="ltx_cite ltx_citemacro_citet">Ekstrand et&nbsp;al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib12" title="">2022</a>)</cite>. For the present study, we make use of fairness metrics that can be tied to a particular protected group. This excludes <span class="ltx_text ltx_font_italic" id="S4.p2.1.1">global fairness</span> measures that provide a score for the overall performance of the system relative to a fairness target distribution <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>An agent seeking fairness with respect to such measures could be implemented within SCRUF-D but it would be using a single definition of fairness on all groups, a different case from what we are considering here.</span></span></span>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">In this paper, we explore the following types of fairness metrics. Note that we are not proposing these classes of metrics as appropriate or desirable for any particular application of fairness-aware recommendation. The goal in formulating these metrics is to implement common but very different types of fairness metrics and demonstrate the ability of SCRUF-D to accommodate this heterogeneity across agents:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p" id="S4.p4.1"><span class="ltx_text ltx_font_bold" id="S4.p4.1.1">Group Proportional Fairness (<math alttext="m_{GPF}" class="ltx_Math" display="inline" id="S4.p4.1.1.m1.1"><semantics id="S4.p4.1.1.m1.1a"><msub id="S4.p4.1.1.m1.1.1" xref="S4.p4.1.1.m1.1.1.cmml"><mi id="S4.p4.1.1.m1.1.1.2" xref="S4.p4.1.1.m1.1.1.2.cmml">m</mi><mrow id="S4.p4.1.1.m1.1.1.3" xref="S4.p4.1.1.m1.1.1.3.cmml"><mi id="S4.p4.1.1.m1.1.1.3.2" xref="S4.p4.1.1.m1.1.1.3.2.cmml">G</mi><mo id="S4.p4.1.1.m1.1.1.3.1" xref="S4.p4.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.p4.1.1.m1.1.1.3.3" xref="S4.p4.1.1.m1.1.1.3.3.cmml">P</mi><mo id="S4.p4.1.1.m1.1.1.3.1a" xref="S4.p4.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.p4.1.1.m1.1.1.3.4" xref="S4.p4.1.1.m1.1.1.3.4.cmml">F</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p4.1.1.m1.1b"><apply id="S4.p4.1.1.m1.1.1.cmml" xref="S4.p4.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p4.1.1.m1.1.1.1.cmml" xref="S4.p4.1.1.m1.1.1">subscript</csymbol><ci id="S4.p4.1.1.m1.1.1.2.cmml" xref="S4.p4.1.1.m1.1.1.2">𝑚</ci><apply id="S4.p4.1.1.m1.1.1.3.cmml" xref="S4.p4.1.1.m1.1.1.3"><times id="S4.p4.1.1.m1.1.1.3.1.cmml" xref="S4.p4.1.1.m1.1.1.3.1"></times><ci id="S4.p4.1.1.m1.1.1.3.2.cmml" xref="S4.p4.1.1.m1.1.1.3.2">𝐺</ci><ci id="S4.p4.1.1.m1.1.1.3.3.cmml" xref="S4.p4.1.1.m1.1.1.3.3">𝑃</ci><ci id="S4.p4.1.1.m1.1.1.3.4.cmml" xref="S4.p4.1.1.m1.1.1.3.4">𝐹</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.1.1.m1.1c">m_{GPF}</annotation><annotation encoding="application/x-llamapun" id="S4.p4.1.1.m1.1d">italic_m start_POSTSUBSCRIPT italic_G italic_P italic_F end_POSTSUBSCRIPT</annotation></semantics></math>):</span> Under group proportional fairness, we assume that there is a fixed proportion of recommendation results associated with the protected group that counts as a fair result from that group’s perspective. We count the number of items that protected items appear across some set of recommendation lists, divide by the total number of recommendations and normalize by the desired target proportion, truncating at 1 once the target proportion is reached to maintain the 0..1 range.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.p5">
<p class="ltx_p" id="S4.p5.5"><span class="ltx_text ltx_font_bold" id="S4.p5.1.1">Group Utility Fairness (<math alttext="m_{GUF}" class="ltx_Math" display="inline" id="S4.p5.1.1.m1.1"><semantics id="S4.p5.1.1.m1.1a"><msub id="S4.p5.1.1.m1.1.1" xref="S4.p5.1.1.m1.1.1.cmml"><mi id="S4.p5.1.1.m1.1.1.2" xref="S4.p5.1.1.m1.1.1.2.cmml">m</mi><mrow id="S4.p5.1.1.m1.1.1.3" xref="S4.p5.1.1.m1.1.1.3.cmml"><mi id="S4.p5.1.1.m1.1.1.3.2" xref="S4.p5.1.1.m1.1.1.3.2.cmml">G</mi><mo id="S4.p5.1.1.m1.1.1.3.1" xref="S4.p5.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.p5.1.1.m1.1.1.3.3" xref="S4.p5.1.1.m1.1.1.3.3.cmml">U</mi><mo id="S4.p5.1.1.m1.1.1.3.1a" xref="S4.p5.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.p5.1.1.m1.1.1.3.4" xref="S4.p5.1.1.m1.1.1.3.4.cmml">F</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p5.1.1.m1.1b"><apply id="S4.p5.1.1.m1.1.1.cmml" xref="S4.p5.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p5.1.1.m1.1.1.1.cmml" xref="S4.p5.1.1.m1.1.1">subscript</csymbol><ci id="S4.p5.1.1.m1.1.1.2.cmml" xref="S4.p5.1.1.m1.1.1.2">𝑚</ci><apply id="S4.p5.1.1.m1.1.1.3.cmml" xref="S4.p5.1.1.m1.1.1.3"><times id="S4.p5.1.1.m1.1.1.3.1.cmml" xref="S4.p5.1.1.m1.1.1.3.1"></times><ci id="S4.p5.1.1.m1.1.1.3.2.cmml" xref="S4.p5.1.1.m1.1.1.3.2">𝐺</ci><ci id="S4.p5.1.1.m1.1.1.3.3.cmml" xref="S4.p5.1.1.m1.1.1.3.3">𝑈</ci><ci id="S4.p5.1.1.m1.1.1.3.4.cmml" xref="S4.p5.1.1.m1.1.1.3.4">𝐹</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.1.1.m1.1c">m_{GUF}</annotation><annotation encoding="application/x-llamapun" id="S4.p5.1.1.m1.1d">italic_m start_POSTSUBSCRIPT italic_G italic_U italic_F end_POSTSUBSCRIPT</annotation></semantics></math>):</span> While <math alttext="m_{GPF}" class="ltx_Math" display="inline" id="S4.p5.2.m1.1"><semantics id="S4.p5.2.m1.1a"><msub id="S4.p5.2.m1.1.1" xref="S4.p5.2.m1.1.1.cmml"><mi id="S4.p5.2.m1.1.1.2" xref="S4.p5.2.m1.1.1.2.cmml">m</mi><mrow id="S4.p5.2.m1.1.1.3" xref="S4.p5.2.m1.1.1.3.cmml"><mi id="S4.p5.2.m1.1.1.3.2" xref="S4.p5.2.m1.1.1.3.2.cmml">G</mi><mo id="S4.p5.2.m1.1.1.3.1" xref="S4.p5.2.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.p5.2.m1.1.1.3.3" xref="S4.p5.2.m1.1.1.3.3.cmml">P</mi><mo id="S4.p5.2.m1.1.1.3.1a" xref="S4.p5.2.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.p5.2.m1.1.1.3.4" xref="S4.p5.2.m1.1.1.3.4.cmml">F</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p5.2.m1.1b"><apply id="S4.p5.2.m1.1.1.cmml" xref="S4.p5.2.m1.1.1"><csymbol cd="ambiguous" id="S4.p5.2.m1.1.1.1.cmml" xref="S4.p5.2.m1.1.1">subscript</csymbol><ci id="S4.p5.2.m1.1.1.2.cmml" xref="S4.p5.2.m1.1.1.2">𝑚</ci><apply id="S4.p5.2.m1.1.1.3.cmml" xref="S4.p5.2.m1.1.1.3"><times id="S4.p5.2.m1.1.1.3.1.cmml" xref="S4.p5.2.m1.1.1.3.1"></times><ci id="S4.p5.2.m1.1.1.3.2.cmml" xref="S4.p5.2.m1.1.1.3.2">𝐺</ci><ci id="S4.p5.2.m1.1.1.3.3.cmml" xref="S4.p5.2.m1.1.1.3.3">𝑃</ci><ci id="S4.p5.2.m1.1.1.3.4.cmml" xref="S4.p5.2.m1.1.1.3.4">𝐹</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.2.m1.1c">m_{GPF}</annotation><annotation encoding="application/x-llamapun" id="S4.p5.2.m1.1d">italic_m start_POSTSUBSCRIPT italic_G italic_P italic_F end_POSTSUBSCRIPT</annotation></semantics></math> above captures the presence or absence of protected items in a list, it is indifferent to the item’s position. However, highly-ranked positions may be of greater utility to providers. Also, <math alttext="m_{GPF}" class="ltx_Math" display="inline" id="S4.p5.3.m2.1"><semantics id="S4.p5.3.m2.1a"><msub id="S4.p5.3.m2.1.1" xref="S4.p5.3.m2.1.1.cmml"><mi id="S4.p5.3.m2.1.1.2" xref="S4.p5.3.m2.1.1.2.cmml">m</mi><mrow id="S4.p5.3.m2.1.1.3" xref="S4.p5.3.m2.1.1.3.cmml"><mi id="S4.p5.3.m2.1.1.3.2" xref="S4.p5.3.m2.1.1.3.2.cmml">G</mi><mo id="S4.p5.3.m2.1.1.3.1" xref="S4.p5.3.m2.1.1.3.1.cmml">⁢</mo><mi id="S4.p5.3.m2.1.1.3.3" xref="S4.p5.3.m2.1.1.3.3.cmml">P</mi><mo id="S4.p5.3.m2.1.1.3.1a" xref="S4.p5.3.m2.1.1.3.1.cmml">⁢</mo><mi id="S4.p5.3.m2.1.1.3.4" xref="S4.p5.3.m2.1.1.3.4.cmml">F</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p5.3.m2.1b"><apply id="S4.p5.3.m2.1.1.cmml" xref="S4.p5.3.m2.1.1"><csymbol cd="ambiguous" id="S4.p5.3.m2.1.1.1.cmml" xref="S4.p5.3.m2.1.1">subscript</csymbol><ci id="S4.p5.3.m2.1.1.2.cmml" xref="S4.p5.3.m2.1.1.2">𝑚</ci><apply id="S4.p5.3.m2.1.1.3.cmml" xref="S4.p5.3.m2.1.1.3"><times id="S4.p5.3.m2.1.1.3.1.cmml" xref="S4.p5.3.m2.1.1.3.1"></times><ci id="S4.p5.3.m2.1.1.3.2.cmml" xref="S4.p5.3.m2.1.1.3.2">𝐺</ci><ci id="S4.p5.3.m2.1.1.3.3.cmml" xref="S4.p5.3.m2.1.1.3.3">𝑃</ci><ci id="S4.p5.3.m2.1.1.3.4.cmml" xref="S4.p5.3.m2.1.1.3.4">𝐹</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.3.m2.1c">m_{GPF}</annotation><annotation encoding="application/x-llamapun" id="S4.p5.3.m2.1d">italic_m start_POSTSUBSCRIPT italic_G italic_P italic_F end_POSTSUBSCRIPT</annotation></semantics></math> does not take into account the <em class="ltx_emph ltx_font_italic" id="S4.p5.5.2">number</em> of items that might be in each category. These considerations can be captured by modifying <math alttext="m_{GPF}" class="ltx_Math" display="inline" id="S4.p5.4.m3.1"><semantics id="S4.p5.4.m3.1a"><msub id="S4.p5.4.m3.1.1" xref="S4.p5.4.m3.1.1.cmml"><mi id="S4.p5.4.m3.1.1.2" xref="S4.p5.4.m3.1.1.2.cmml">m</mi><mrow id="S4.p5.4.m3.1.1.3" xref="S4.p5.4.m3.1.1.3.cmml"><mi id="S4.p5.4.m3.1.1.3.2" xref="S4.p5.4.m3.1.1.3.2.cmml">G</mi><mo id="S4.p5.4.m3.1.1.3.1" xref="S4.p5.4.m3.1.1.3.1.cmml">⁢</mo><mi id="S4.p5.4.m3.1.1.3.3" xref="S4.p5.4.m3.1.1.3.3.cmml">P</mi><mo id="S4.p5.4.m3.1.1.3.1a" xref="S4.p5.4.m3.1.1.3.1.cmml">⁢</mo><mi id="S4.p5.4.m3.1.1.3.4" xref="S4.p5.4.m3.1.1.3.4.cmml">F</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p5.4.m3.1b"><apply id="S4.p5.4.m3.1.1.cmml" xref="S4.p5.4.m3.1.1"><csymbol cd="ambiguous" id="S4.p5.4.m3.1.1.1.cmml" xref="S4.p5.4.m3.1.1">subscript</csymbol><ci id="S4.p5.4.m3.1.1.2.cmml" xref="S4.p5.4.m3.1.1.2">𝑚</ci><apply id="S4.p5.4.m3.1.1.3.cmml" xref="S4.p5.4.m3.1.1.3"><times id="S4.p5.4.m3.1.1.3.1.cmml" xref="S4.p5.4.m3.1.1.3.1"></times><ci id="S4.p5.4.m3.1.1.3.2.cmml" xref="S4.p5.4.m3.1.1.3.2">𝐺</ci><ci id="S4.p5.4.m3.1.1.3.3.cmml" xref="S4.p5.4.m3.1.1.3.3">𝑃</ci><ci id="S4.p5.4.m3.1.1.3.4.cmml" xref="S4.p5.4.m3.1.1.3.4">𝐹</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.4.m3.1c">m_{GPF}</annotation><annotation encoding="application/x-llamapun" id="S4.p5.4.m3.1d">italic_m start_POSTSUBSCRIPT italic_G italic_P italic_F end_POSTSUBSCRIPT</annotation></semantics></math> to sum rank-discounted utilities (here we discount by <math alttext="log_{2}" class="ltx_Math" display="inline" id="S4.p5.5.m4.1"><semantics id="S4.p5.5.m4.1a"><mrow id="S4.p5.5.m4.1.1" xref="S4.p5.5.m4.1.1.cmml"><mi id="S4.p5.5.m4.1.1.2" xref="S4.p5.5.m4.1.1.2.cmml">l</mi><mo id="S4.p5.5.m4.1.1.1" xref="S4.p5.5.m4.1.1.1.cmml">⁢</mo><mi id="S4.p5.5.m4.1.1.3" xref="S4.p5.5.m4.1.1.3.cmml">o</mi><mo id="S4.p5.5.m4.1.1.1a" xref="S4.p5.5.m4.1.1.1.cmml">⁢</mo><msub id="S4.p5.5.m4.1.1.4" xref="S4.p5.5.m4.1.1.4.cmml"><mi id="S4.p5.5.m4.1.1.4.2" xref="S4.p5.5.m4.1.1.4.2.cmml">g</mi><mn id="S4.p5.5.m4.1.1.4.3" xref="S4.p5.5.m4.1.1.4.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.p5.5.m4.1b"><apply id="S4.p5.5.m4.1.1.cmml" xref="S4.p5.5.m4.1.1"><times id="S4.p5.5.m4.1.1.1.cmml" xref="S4.p5.5.m4.1.1.1"></times><ci id="S4.p5.5.m4.1.1.2.cmml" xref="S4.p5.5.m4.1.1.2">𝑙</ci><ci id="S4.p5.5.m4.1.1.3.cmml" xref="S4.p5.5.m4.1.1.3">𝑜</ci><apply id="S4.p5.5.m4.1.1.4.cmml" xref="S4.p5.5.m4.1.1.4"><csymbol cd="ambiguous" id="S4.p5.5.m4.1.1.4.1.cmml" xref="S4.p5.5.m4.1.1.4">subscript</csymbol><ci id="S4.p5.5.m4.1.1.4.2.cmml" xref="S4.p5.5.m4.1.1.4.2">𝑔</ci><cn id="S4.p5.5.m4.1.1.4.3.cmml" type="integer" xref="S4.p5.5.m4.1.1.4.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.5.m4.1c">log_{2}</annotation><annotation encoding="application/x-llamapun" id="S4.p5.5.m4.1d">italic_l italic_o italic_g start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> of the rank) rather than just count occurrences, and to normalize the utility for protected and unprotected groups by their respective sizes.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.p6">
<p class="ltx_p" id="S4.p6.1"><span class="ltx_text ltx_font_bold" id="S4.p6.1.1">Group MRR Fairness (<math alttext="m_{MRR}" class="ltx_Math" display="inline" id="S4.p6.1.1.m1.1"><semantics id="S4.p6.1.1.m1.1a"><msub id="S4.p6.1.1.m1.1.1" xref="S4.p6.1.1.m1.1.1.cmml"><mi id="S4.p6.1.1.m1.1.1.2" xref="S4.p6.1.1.m1.1.1.2.cmml">m</mi><mrow id="S4.p6.1.1.m1.1.1.3" xref="S4.p6.1.1.m1.1.1.3.cmml"><mi id="S4.p6.1.1.m1.1.1.3.2" xref="S4.p6.1.1.m1.1.1.3.2.cmml">M</mi><mo id="S4.p6.1.1.m1.1.1.3.1" xref="S4.p6.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.p6.1.1.m1.1.1.3.3" xref="S4.p6.1.1.m1.1.1.3.3.cmml">R</mi><mo id="S4.p6.1.1.m1.1.1.3.1a" xref="S4.p6.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.p6.1.1.m1.1.1.3.4" xref="S4.p6.1.1.m1.1.1.3.4.cmml">R</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p6.1.1.m1.1b"><apply id="S4.p6.1.1.m1.1.1.cmml" xref="S4.p6.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p6.1.1.m1.1.1.1.cmml" xref="S4.p6.1.1.m1.1.1">subscript</csymbol><ci id="S4.p6.1.1.m1.1.1.2.cmml" xref="S4.p6.1.1.m1.1.1.2">𝑚</ci><apply id="S4.p6.1.1.m1.1.1.3.cmml" xref="S4.p6.1.1.m1.1.1.3"><times id="S4.p6.1.1.m1.1.1.3.1.cmml" xref="S4.p6.1.1.m1.1.1.3.1"></times><ci id="S4.p6.1.1.m1.1.1.3.2.cmml" xref="S4.p6.1.1.m1.1.1.3.2">𝑀</ci><ci id="S4.p6.1.1.m1.1.1.3.3.cmml" xref="S4.p6.1.1.m1.1.1.3.3">𝑅</ci><ci id="S4.p6.1.1.m1.1.1.3.4.cmml" xref="S4.p6.1.1.m1.1.1.3.4">𝑅</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.1.1.m1.1c">m_{MRR}</annotation><annotation encoding="application/x-llamapun" id="S4.p6.1.1.m1.1d">italic_m start_POSTSUBSCRIPT italic_M italic_R italic_R end_POSTSUBSCRIPT</annotation></semantics></math>):</span> The measures above look at all recommended items in a set of lists summing a total value for protected items. In some cases it might be desirable to focus on a minimal degree of representation across recommendation lists, and we capture this type of metric using <span class="ltx_text ltx_font_italic" id="S4.p6.1.2">mean reciprocal rank</span>. We average the reciprocal rank of the highest ranking protected item across all lists and normalize by the target MRR value.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Methodology</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We make use of two data sets MovieLens and Microlending. For each data set, we define three sensitive attributes and protected values and define an agent for each using metrics based on the three different definitions above. The MovieLens 1M dataset <cite class="ltx_cite ltx_citemacro_citep">(Harper and Konstan, <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib15" title="">2015</a>)</cite> contains user ratings for movies with has 3,900 movies, 6,040 users, and approximately 1 million ratings. We consider the sensitive attributes in this dataset to be (1) movies with at least one female writer and/or director, (2) movies with non-English scripts, and (3) older movies (released before 1990). Our second dataset is the Microlending 2017 dataset <cite class="ltx_cite ltx_citemacro_citep">(Sonboli et&nbsp;al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib29" title="">2022a</a>)</cite>. This dataset includes 2,673 pseudo-items<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>The Microlending 2017 dataset represents user preferences relative to clusters of items (pseudo-items) in place of individual loans since loans in this dataset have small numbers of associated users and the unclustered data is too sparse for collaborative recommendation. See <cite class="ltx_cite ltx_citemacro_citep">(Sonboli et&nbsp;al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib29" title="">2022a</a>)</cite> for additional details.</span></span></span>, 4,005 lenders and 110,371 ratings. We consider the sensitive attributes in this dataset to be (1) loans from countries with low funding rates, (2) loans funding sectors that have low funding rates, and (3) loans larger than $5,000.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Agent Definitions</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">As noted in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#S3" title="3. The SCRUF-D Architecture ‣ Social Choice for Heterogeneous Fairness in Recommendation"><span class="ltx_text ltx_ref_tag">3</span></a>, fairness agents in SCRUF-D are defined by a fairness metric, a compatibility metric, and a ranking function. In this study, we concentrate on the fairness metric.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.4">We define the compatibility between a user and an item feature as the frequency of occurrence of that feature among the items that the user likes. For the Microlending dataset, all funded loans are considered liked; in the Movies dataset, we categorize any movie with a rating over 3 as liked. Compatibility is calculated as <math alttext="c_{u,f}=p_{u,f}/\bar{p}_{f}" class="ltx_Math" display="inline" id="S5.SS1.p2.1.m1.4"><semantics id="S5.SS1.p2.1.m1.4a"><mrow id="S5.SS1.p2.1.m1.4.5" xref="S5.SS1.p2.1.m1.4.5.cmml"><msub id="S5.SS1.p2.1.m1.4.5.2" xref="S5.SS1.p2.1.m1.4.5.2.cmml"><mi id="S5.SS1.p2.1.m1.4.5.2.2" xref="S5.SS1.p2.1.m1.4.5.2.2.cmml">c</mi><mrow id="S5.SS1.p2.1.m1.2.2.2.4" xref="S5.SS1.p2.1.m1.2.2.2.3.cmml"><mi id="S5.SS1.p2.1.m1.1.1.1.1" xref="S5.SS1.p2.1.m1.1.1.1.1.cmml">u</mi><mo id="S5.SS1.p2.1.m1.2.2.2.4.1" xref="S5.SS1.p2.1.m1.2.2.2.3.cmml">,</mo><mi id="S5.SS1.p2.1.m1.2.2.2.2" xref="S5.SS1.p2.1.m1.2.2.2.2.cmml">f</mi></mrow></msub><mo id="S5.SS1.p2.1.m1.4.5.1" xref="S5.SS1.p2.1.m1.4.5.1.cmml">=</mo><mrow id="S5.SS1.p2.1.m1.4.5.3" xref="S5.SS1.p2.1.m1.4.5.3.cmml"><msub id="S5.SS1.p2.1.m1.4.5.3.2" xref="S5.SS1.p2.1.m1.4.5.3.2.cmml"><mi id="S5.SS1.p2.1.m1.4.5.3.2.2" xref="S5.SS1.p2.1.m1.4.5.3.2.2.cmml">p</mi><mrow id="S5.SS1.p2.1.m1.4.4.2.4" xref="S5.SS1.p2.1.m1.4.4.2.3.cmml"><mi id="S5.SS1.p2.1.m1.3.3.1.1" xref="S5.SS1.p2.1.m1.3.3.1.1.cmml">u</mi><mo id="S5.SS1.p2.1.m1.4.4.2.4.1" xref="S5.SS1.p2.1.m1.4.4.2.3.cmml">,</mo><mi id="S5.SS1.p2.1.m1.4.4.2.2" xref="S5.SS1.p2.1.m1.4.4.2.2.cmml">f</mi></mrow></msub><mo id="S5.SS1.p2.1.m1.4.5.3.1" xref="S5.SS1.p2.1.m1.4.5.3.1.cmml">/</mo><msub id="S5.SS1.p2.1.m1.4.5.3.3" xref="S5.SS1.p2.1.m1.4.5.3.3.cmml"><mover accent="true" id="S5.SS1.p2.1.m1.4.5.3.3.2" xref="S5.SS1.p2.1.m1.4.5.3.3.2.cmml"><mi id="S5.SS1.p2.1.m1.4.5.3.3.2.2" xref="S5.SS1.p2.1.m1.4.5.3.3.2.2.cmml">p</mi><mo id="S5.SS1.p2.1.m1.4.5.3.3.2.1" xref="S5.SS1.p2.1.m1.4.5.3.3.2.1.cmml">¯</mo></mover><mi id="S5.SS1.p2.1.m1.4.5.3.3.3" xref="S5.SS1.p2.1.m1.4.5.3.3.3.cmml">f</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.4b"><apply id="S5.SS1.p2.1.m1.4.5.cmml" xref="S5.SS1.p2.1.m1.4.5"><eq id="S5.SS1.p2.1.m1.4.5.1.cmml" xref="S5.SS1.p2.1.m1.4.5.1"></eq><apply id="S5.SS1.p2.1.m1.4.5.2.cmml" xref="S5.SS1.p2.1.m1.4.5.2"><csymbol cd="ambiguous" id="S5.SS1.p2.1.m1.4.5.2.1.cmml" xref="S5.SS1.p2.1.m1.4.5.2">subscript</csymbol><ci id="S5.SS1.p2.1.m1.4.5.2.2.cmml" xref="S5.SS1.p2.1.m1.4.5.2.2">𝑐</ci><list id="S5.SS1.p2.1.m1.2.2.2.3.cmml" xref="S5.SS1.p2.1.m1.2.2.2.4"><ci id="S5.SS1.p2.1.m1.1.1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1.1.1">𝑢</ci><ci id="S5.SS1.p2.1.m1.2.2.2.2.cmml" xref="S5.SS1.p2.1.m1.2.2.2.2">𝑓</ci></list></apply><apply id="S5.SS1.p2.1.m1.4.5.3.cmml" xref="S5.SS1.p2.1.m1.4.5.3"><divide id="S5.SS1.p2.1.m1.4.5.3.1.cmml" xref="S5.SS1.p2.1.m1.4.5.3.1"></divide><apply id="S5.SS1.p2.1.m1.4.5.3.2.cmml" xref="S5.SS1.p2.1.m1.4.5.3.2"><csymbol cd="ambiguous" id="S5.SS1.p2.1.m1.4.5.3.2.1.cmml" xref="S5.SS1.p2.1.m1.4.5.3.2">subscript</csymbol><ci id="S5.SS1.p2.1.m1.4.5.3.2.2.cmml" xref="S5.SS1.p2.1.m1.4.5.3.2.2">𝑝</ci><list id="S5.SS1.p2.1.m1.4.4.2.3.cmml" xref="S5.SS1.p2.1.m1.4.4.2.4"><ci id="S5.SS1.p2.1.m1.3.3.1.1.cmml" xref="S5.SS1.p2.1.m1.3.3.1.1">𝑢</ci><ci id="S5.SS1.p2.1.m1.4.4.2.2.cmml" xref="S5.SS1.p2.1.m1.4.4.2.2">𝑓</ci></list></apply><apply id="S5.SS1.p2.1.m1.4.5.3.3.cmml" xref="S5.SS1.p2.1.m1.4.5.3.3"><csymbol cd="ambiguous" id="S5.SS1.p2.1.m1.4.5.3.3.1.cmml" xref="S5.SS1.p2.1.m1.4.5.3.3">subscript</csymbol><apply id="S5.SS1.p2.1.m1.4.5.3.3.2.cmml" xref="S5.SS1.p2.1.m1.4.5.3.3.2"><ci id="S5.SS1.p2.1.m1.4.5.3.3.2.1.cmml" xref="S5.SS1.p2.1.m1.4.5.3.3.2.1">¯</ci><ci id="S5.SS1.p2.1.m1.4.5.3.3.2.2.cmml" xref="S5.SS1.p2.1.m1.4.5.3.3.2.2">𝑝</ci></apply><ci id="S5.SS1.p2.1.m1.4.5.3.3.3.cmml" xref="S5.SS1.p2.1.m1.4.5.3.3.3">𝑓</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.4c">c_{u,f}=p_{u,f}/\bar{p}_{f}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.1.m1.4d">italic_c start_POSTSUBSCRIPT italic_u , italic_f end_POSTSUBSCRIPT = italic_p start_POSTSUBSCRIPT italic_u , italic_f end_POSTSUBSCRIPT / over¯ start_ARG italic_p end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT</annotation></semantics></math> where
<math alttext="p_{u,i}=\text{{count of liked items with feature f}}/\text{{total count of %
items rated}}" class="ltx_Math" display="inline" id="S5.SS1.p2.2.m2.2"><semantics id="S5.SS1.p2.2.m2.2a"><mrow id="S5.SS1.p2.2.m2.2.3" xref="S5.SS1.p2.2.m2.2.3.cmml"><msub id="S5.SS1.p2.2.m2.2.3.2" xref="S5.SS1.p2.2.m2.2.3.2.cmml"><mi id="S5.SS1.p2.2.m2.2.3.2.2" xref="S5.SS1.p2.2.m2.2.3.2.2.cmml">p</mi><mrow id="S5.SS1.p2.2.m2.2.2.2.4" xref="S5.SS1.p2.2.m2.2.2.2.3.cmml"><mi id="S5.SS1.p2.2.m2.1.1.1.1" xref="S5.SS1.p2.2.m2.1.1.1.1.cmml">u</mi><mo id="S5.SS1.p2.2.m2.2.2.2.4.1" xref="S5.SS1.p2.2.m2.2.2.2.3.cmml">,</mo><mi id="S5.SS1.p2.2.m2.2.2.2.2" xref="S5.SS1.p2.2.m2.2.2.2.2.cmml">i</mi></mrow></msub><mo id="S5.SS1.p2.2.m2.2.3.1" xref="S5.SS1.p2.2.m2.2.3.1.cmml">=</mo><mrow id="S5.SS1.p2.2.m2.2.3.3" xref="S5.SS1.p2.2.m2.2.3.3.cmml"><mtext id="S5.SS1.p2.2.m2.2.3.3.2" xref="S5.SS1.p2.2.m2.2.3.3.2a.cmml">count of liked items with feature f</mtext><mo id="S5.SS1.p2.2.m2.2.3.3.1" xref="S5.SS1.p2.2.m2.2.3.3.1.cmml">/</mo><mtext id="S5.SS1.p2.2.m2.2.3.3.3" xref="S5.SS1.p2.2.m2.2.3.3.3a.cmml">total count of items rated</mtext></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.2.m2.2b"><apply id="S5.SS1.p2.2.m2.2.3.cmml" xref="S5.SS1.p2.2.m2.2.3"><eq id="S5.SS1.p2.2.m2.2.3.1.cmml" xref="S5.SS1.p2.2.m2.2.3.1"></eq><apply id="S5.SS1.p2.2.m2.2.3.2.cmml" xref="S5.SS1.p2.2.m2.2.3.2"><csymbol cd="ambiguous" id="S5.SS1.p2.2.m2.2.3.2.1.cmml" xref="S5.SS1.p2.2.m2.2.3.2">subscript</csymbol><ci id="S5.SS1.p2.2.m2.2.3.2.2.cmml" xref="S5.SS1.p2.2.m2.2.3.2.2">𝑝</ci><list id="S5.SS1.p2.2.m2.2.2.2.3.cmml" xref="S5.SS1.p2.2.m2.2.2.2.4"><ci id="S5.SS1.p2.2.m2.1.1.1.1.cmml" xref="S5.SS1.p2.2.m2.1.1.1.1">𝑢</ci><ci id="S5.SS1.p2.2.m2.2.2.2.2.cmml" xref="S5.SS1.p2.2.m2.2.2.2.2">𝑖</ci></list></apply><apply id="S5.SS1.p2.2.m2.2.3.3.cmml" xref="S5.SS1.p2.2.m2.2.3.3"><divide id="S5.SS1.p2.2.m2.2.3.3.1.cmml" xref="S5.SS1.p2.2.m2.2.3.3.1"></divide><ci id="S5.SS1.p2.2.m2.2.3.3.2a.cmml" xref="S5.SS1.p2.2.m2.2.3.3.2"><mtext id="S5.SS1.p2.2.m2.2.3.3.2.cmml" xref="S5.SS1.p2.2.m2.2.3.3.2">count of liked items with feature f</mtext></ci><ci id="S5.SS1.p2.2.m2.2.3.3.3a.cmml" xref="S5.SS1.p2.2.m2.2.3.3.3"><mtext id="S5.SS1.p2.2.m2.2.3.3.3.cmml" xref="S5.SS1.p2.2.m2.2.3.3.3">total count of items rated</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.2.m2.2c">p_{u,i}=\text{{count of liked items with feature f}}/\text{{total count of %
items rated}}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.2.m2.2d">italic_p start_POSTSUBSCRIPT italic_u , italic_i end_POSTSUBSCRIPT = count of liked items with feature f / total count of items rated</annotation></semantics></math>
and <math alttext="\bar{p}_{f}" class="ltx_Math" display="inline" id="S5.SS1.p2.3.m3.1"><semantics id="S5.SS1.p2.3.m3.1a"><msub id="S5.SS1.p2.3.m3.1.1" xref="S5.SS1.p2.3.m3.1.1.cmml"><mover accent="true" id="S5.SS1.p2.3.m3.1.1.2" xref="S5.SS1.p2.3.m3.1.1.2.cmml"><mi id="S5.SS1.p2.3.m3.1.1.2.2" xref="S5.SS1.p2.3.m3.1.1.2.2.cmml">p</mi><mo id="S5.SS1.p2.3.m3.1.1.2.1" xref="S5.SS1.p2.3.m3.1.1.2.1.cmml">¯</mo></mover><mi id="S5.SS1.p2.3.m3.1.1.3" xref="S5.SS1.p2.3.m3.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.3.m3.1b"><apply id="S5.SS1.p2.3.m3.1.1.cmml" xref="S5.SS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS1.p2.3.m3.1.1.1.cmml" xref="S5.SS1.p2.3.m3.1.1">subscript</csymbol><apply id="S5.SS1.p2.3.m3.1.1.2.cmml" xref="S5.SS1.p2.3.m3.1.1.2"><ci id="S5.SS1.p2.3.m3.1.1.2.1.cmml" xref="S5.SS1.p2.3.m3.1.1.2.1">¯</ci><ci id="S5.SS1.p2.3.m3.1.1.2.2.cmml" xref="S5.SS1.p2.3.m3.1.1.2.2">𝑝</ci></apply><ci id="S5.SS1.p2.3.m3.1.1.3.cmml" xref="S5.SS1.p2.3.m3.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.3.m3.1c">\bar{p}_{f}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.3.m3.1d">over¯ start_ARG italic_p end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT</annotation></semantics></math> is the average number of items with feature <math alttext="f" class="ltx_Math" display="inline" id="S5.SS1.p2.4.m4.1"><semantics id="S5.SS1.p2.4.m4.1a"><mi id="S5.SS1.p2.4.m4.1.1" xref="S5.SS1.p2.4.m4.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.4.m4.1b"><ci id="S5.SS1.p2.4.m4.1.1.cmml" xref="S5.SS1.p2.4.m4.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.4.m4.1c">f</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.4.m4.1d">italic_f</annotation></semantics></math>. The compatibility scores are normalized across features for each user.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1">For ranking, we use two approaches depending on the type of choice mechanism. For the <em class="ltx_emph ltx_font_italic" id="S5.SS1.p3.1.1">Rescoring Mechanism</em>, we use a simple binary partial order <math alttext="\mathcal{V}_{s}>\mathcal{V}_{\neg s}" class="ltx_Math" display="inline" id="S5.SS1.p3.1.m1.1"><semantics id="S5.SS1.p3.1.m1.1a"><mrow id="S5.SS1.p3.1.m1.1.1" xref="S5.SS1.p3.1.m1.1.1.cmml"><msub id="S5.SS1.p3.1.m1.1.1.2" xref="S5.SS1.p3.1.m1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.SS1.p3.1.m1.1.1.2.2" xref="S5.SS1.p3.1.m1.1.1.2.2.cmml">𝒱</mi><mi id="S5.SS1.p3.1.m1.1.1.2.3" xref="S5.SS1.p3.1.m1.1.1.2.3.cmml">s</mi></msub><mo id="S5.SS1.p3.1.m1.1.1.1" xref="S5.SS1.p3.1.m1.1.1.1.cmml">&gt;</mo><msub id="S5.SS1.p3.1.m1.1.1.3" xref="S5.SS1.p3.1.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.SS1.p3.1.m1.1.1.3.2" xref="S5.SS1.p3.1.m1.1.1.3.2.cmml">𝒱</mi><mrow id="S5.SS1.p3.1.m1.1.1.3.3" xref="S5.SS1.p3.1.m1.1.1.3.3.cmml"><mo id="S5.SS1.p3.1.m1.1.1.3.3.1" rspace="0.167em" xref="S5.SS1.p3.1.m1.1.1.3.3.1.cmml">¬</mo><mi id="S5.SS1.p3.1.m1.1.1.3.3.2" xref="S5.SS1.p3.1.m1.1.1.3.3.2.cmml">s</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.1b"><apply id="S5.SS1.p3.1.m1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1"><gt id="S5.SS1.p3.1.m1.1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1.1"></gt><apply id="S5.SS1.p3.1.m1.1.1.2.cmml" xref="S5.SS1.p3.1.m1.1.1.2"><csymbol cd="ambiguous" id="S5.SS1.p3.1.m1.1.1.2.1.cmml" xref="S5.SS1.p3.1.m1.1.1.2">subscript</csymbol><ci id="S5.SS1.p3.1.m1.1.1.2.2.cmml" xref="S5.SS1.p3.1.m1.1.1.2.2">𝒱</ci><ci id="S5.SS1.p3.1.m1.1.1.2.3.cmml" xref="S5.SS1.p3.1.m1.1.1.2.3">𝑠</ci></apply><apply id="S5.SS1.p3.1.m1.1.1.3.cmml" xref="S5.SS1.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S5.SS1.p3.1.m1.1.1.3.1.cmml" xref="S5.SS1.p3.1.m1.1.1.3">subscript</csymbol><ci id="S5.SS1.p3.1.m1.1.1.3.2.cmml" xref="S5.SS1.p3.1.m1.1.1.3.2">𝒱</ci><apply id="S5.SS1.p3.1.m1.1.1.3.3.cmml" xref="S5.SS1.p3.1.m1.1.1.3.3"><not id="S5.SS1.p3.1.m1.1.1.3.3.1.cmml" xref="S5.SS1.p3.1.m1.1.1.3.3.1"></not><ci id="S5.SS1.p3.1.m1.1.1.3.3.2.cmml" xref="S5.SS1.p3.1.m1.1.1.3.3.2">𝑠</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.1c">\mathcal{V}_{s}&gt;\mathcal{V}_{\neg s}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.1.m1.1d">caligraphic_V start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT &gt; caligraphic_V start_POSTSUBSCRIPT ¬ italic_s end_POSTSUBSCRIPT</annotation></semantics></math> where the agent prefers all items that it considers as protected. For our other choice mechanisms, this two-level partial ranking works poorly as it induces many ties, forcing the system to rely on an arbitrary tie-breaking rule. To address this issue, we implement a <span class="ltx_text ltx_font_italic" id="S5.SS1.p3.1.2">cascaded</span> ranking function that integrates both the protected class preference and inherits, as a secondary ranking criterion, the ranking of the recommender system agent. This is effectively the same as moving all of the protected items to the front of the recommendation list, keeping the between-item preferences the same. This method removes ties, inducing a total order.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1">For each of our datasets we define three different fairness agents, with three different metrics, to explore the flexibility and trade-offs of SCRUF-D. For the Microlending experiments, the first agent is focused on loans in amounts greater than $5,000, which are considered to have proportionally stronger economic impact but also tend to be funded at a lower rate. This agent aims to have such loans represented across the recommendation lists with MRR target value of 0.5. The second agent is focused on loans from countries with low funding rates. The aim for this agent is to ensure higher utility for loans for countries with historically low funding rates. The third agent focuses on loans from sectors with low funding rates. This agent uses the proportional definition of fairness; the goal is that 20% of all loans recommended are from sectors with historically low funding rates.
For the Movie data experiments, the first agent is focused on movies with women writers and directors. This agent is to ensure that a movie with a woman writer and/or director is in the top 1–2 positions of users’ lists. The second agent is focused on non-English movies. This agent has a goal of ensuring non-English movies receive equal utility compared to movies in English. The third agent is focused on older movies, with the goal of making 25% of all recommended movies older movies.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S5.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F1.sf1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S5.F1.sf1.2">
<img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="355" id="S5.F1.sf1.g1" src="https://arxiv.org/html/2410.04551v1/extracted/5905347/img/kiva_scatter.png" width="479"><span class="ltx_ERROR ltx_centering undefined" id="S5.F1.sf1.2.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S5.F1.sf1.3">[A scatterplot with confidence intervals on X and Y]A scatterplot with points of each combination of allocation and choice mechanism. The dots (other than baseline) occupy the upper right half of the image.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F1.sf1.4.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S5.F1.sf1.5.2" style="font-size:90%;">Microlending data</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F1.sf2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S5.F1.sf2.2">
<img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="355" id="S5.F1.sf2.g1" src="https://arxiv.org/html/2410.04551v1/extracted/5905347/img/movie_scatter.png" width="479"><span class="ltx_ERROR ltx_centering undefined" id="S5.F1.sf2.2.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S5.F1.sf2.3">[A scatterplot with confidence intervals on X and Y]A scatterplot with points of each combination of allocation and choice mechanism. The dots (other than baseline) occupy the right half of the image and the confidence intervals are rel<span class="ltx_ERROR undefined" id="S5.F1.sf2.3.1">\Description</span>[A scatterplot with confidence intervals on X and Y]A scatterplot with points of each combination of allocation and choice mechanism. The dots (other than baseline) occupy the upper right half of the image.atively large.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F1.sf2.4.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S5.F1.sf2.5.2" style="font-size:90%;">Movies data</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F1.4.2.1" style="font-size:90%;">Figure 1</span>. </span><span class="ltx_text" id="S5.F1.2.1" style="font-size:90%;">nDCG vs <math alttext="l_{1/2}" class="ltx_Math" display="inline" id="S5.F1.2.1.m1.1"><semantics id="S5.F1.2.1.m1.1b"><msub id="S5.F1.2.1.m1.1.1" xref="S5.F1.2.1.m1.1.1.cmml"><mi id="S5.F1.2.1.m1.1.1.2" xref="S5.F1.2.1.m1.1.1.2.cmml">l</mi><mrow id="S5.F1.2.1.m1.1.1.3" xref="S5.F1.2.1.m1.1.1.3.cmml"><mn id="S5.F1.2.1.m1.1.1.3.2" xref="S5.F1.2.1.m1.1.1.3.2.cmml">1</mn><mo id="S5.F1.2.1.m1.1.1.3.1" xref="S5.F1.2.1.m1.1.1.3.1.cmml">/</mo><mn id="S5.F1.2.1.m1.1.1.3.3" xref="S5.F1.2.1.m1.1.1.3.3.cmml">2</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.F1.2.1.m1.1c"><apply id="S5.F1.2.1.m1.1.1.cmml" xref="S5.F1.2.1.m1.1.1"><csymbol cd="ambiguous" id="S5.F1.2.1.m1.1.1.1.cmml" xref="S5.F1.2.1.m1.1.1">subscript</csymbol><ci id="S5.F1.2.1.m1.1.1.2.cmml" xref="S5.F1.2.1.m1.1.1.2">𝑙</ci><apply id="S5.F1.2.1.m1.1.1.3.cmml" xref="S5.F1.2.1.m1.1.1.3"><divide id="S5.F1.2.1.m1.1.1.3.1.cmml" xref="S5.F1.2.1.m1.1.1.3.1"></divide><cn id="S5.F1.2.1.m1.1.1.3.2.cmml" type="integer" xref="S5.F1.2.1.m1.1.1.3.2">1</cn><cn id="S5.F1.2.1.m1.1.1.3.3.cmml" type="integer" xref="S5.F1.2.1.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F1.2.1.m1.1d">l_{1/2}</annotation><annotation encoding="application/x-llamapun" id="S5.F1.2.1.m1.1e">italic_l start_POSTSUBSCRIPT 1 / 2 end_POSTSUBSCRIPT</annotation></semantics></math> Fairness Norm for Microlending (Left) and Movies (Right).</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Mechanisms</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Instantiating the SCRUF-D system requires choosing mechanisms that will be applied in the allocation and choice phases. There are a wide variety of options for each of these tasks. For the purposes of this study, we concentrate on three allocation mechanisms:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p2.1.1">Least Fair</span>: This mechanism allocates a single agent for each recommendation opportunity by comparing the fairness metric <math alttext="m_{i}" class="ltx_Math" display="inline" id="S5.SS2.p2.1.m1.1"><semantics id="S5.SS2.p2.1.m1.1a"><msub id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml"><mi id="S5.SS2.p2.1.m1.1.1.2" xref="S5.SS2.p2.1.m1.1.1.2.cmml">m</mi><mi id="S5.SS2.p2.1.m1.1.1.3" xref="S5.SS2.p2.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><apply id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.1.m1.1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S5.SS2.p2.1.m1.1.1.2.cmml" xref="S5.SS2.p2.1.m1.1.1.2">𝑚</ci><ci id="S5.SS2.p2.1.m1.1.1.3.cmml" xref="S5.SS2.p2.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">m_{i}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.1.m1.1d">italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> for each agent and allocating the agent with the lowest value. If all agents have a fairness of 1.0 (the target), then no agents are allocated to the arriving user(s). This mechanism ignores the compatibility computation, which optimizes fairness but as we show, ignoring the user’s preferences results in a greater loss of accuracy.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p3.1.1">Lottery</span>: This mechanism allocates only a single agent using a non-deterministic allocation <cite class="ltx_cite ltx_citemacro_citep">(Brandt, <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib5" title="">2017</a>)</cite> over the set of agents using a probability distribution. The distribution is computed by calculating the product of unfairness (1 - fairness) and compatibility, where each is raised to a small integer power. An agent is to a recommendation opportunity with high probability if fairness need and the compatibility are high, and the exponentiation allows the product to be tuned. In our experiments we set the power of compatibility to <math alttext="2" class="ltx_Math" display="inline" id="S5.SS2.p3.1.m1.1"><semantics id="S5.SS2.p3.1.m1.1a"><mn id="S5.SS2.p3.1.m1.1.1" xref="S5.SS2.p3.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.1.m1.1b"><cn id="S5.SS2.p3.1.m1.1.1.cmml" type="integer" xref="S5.SS2.p3.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.1.m1.1c">2</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p3.1.m1.1d">2</annotation></semantics></math>, which discounts it somewhat compared to fairness. These scores are computed over all agents and normalized to sum to 1.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.SS2.p4">
<p class="ltx_p" id="S5.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p4.1.1">Weighted</span>: The weighted mechanism uses the same distribution as calculated for the Lottery mechanism, but instead of selecting only a single agent, all agents with fairnness <math alttext="<1.0" class="ltx_Math" display="inline" id="S5.SS2.p4.1.m1.1"><semantics id="S5.SS2.p4.1.m1.1a"><mrow id="S5.SS2.p4.1.m1.1.1" xref="S5.SS2.p4.1.m1.1.1.cmml"><mi id="S5.SS2.p4.1.m1.1.1.2" xref="S5.SS2.p4.1.m1.1.1.2.cmml"></mi><mo id="S5.SS2.p4.1.m1.1.1.1" xref="S5.SS2.p4.1.m1.1.1.1.cmml">&lt;</mo><mn id="S5.SS2.p4.1.m1.1.1.3" xref="S5.SS2.p4.1.m1.1.1.3.cmml">1.0</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p4.1.m1.1b"><apply id="S5.SS2.p4.1.m1.1.1.cmml" xref="S5.SS2.p4.1.m1.1.1"><lt id="S5.SS2.p4.1.m1.1.1.1.cmml" xref="S5.SS2.p4.1.m1.1.1.1"></lt><csymbol cd="latexml" id="S5.SS2.p4.1.m1.1.1.2.cmml" xref="S5.SS2.p4.1.m1.1.1.2">absent</csymbol><cn id="S5.SS2.p4.1.m1.1.1.3.cmml" type="float" xref="S5.SS2.p4.1.m1.1.1.3">1.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p4.1.m1.1c">&lt;1.0</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p4.1.m1.1d">&lt; 1.0</annotation></semantics></math> are allocated and weighted according to their value in the distribution.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.SS2.p5">
<p class="ltx_p" id="S5.SS2.p5.1">These allocation mechanisms allow us to explore two key aspects of the SCRUF-D platform. First, by comparing Least Fair against the other mechanisms, we can see the value of considering user characteristics in agent allocation. Second, by comparing the Lottery vs Weighted schemes, we can see the difference between sending all of the agents to the choice phase as opposed to allocating a single agent. Since the scoring and re-ranking processes can be computationally intensive, there is an advantage to having only two agents in the choice phase if there is no cost in performance.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.SS2.p6">
<p class="ltx_p" id="S5.SS2.p6.1">In the choice phase, we have again a wide variety of preference aggregation schemes to choose from. SCRUF-D integrates Whalrus<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://github.com/francois-durand/whalrus</span></span></span>, a well-known library implementing a variety of voting rules. From the available methods, this paper explores three:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.SS2.p7">
<p class="ltx_p" id="S5.SS2.p7.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p7.1.1">Borda</span>: The Borda method <cite class="ltx_cite ltx_citemacro_citep">(Zwicker, <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib39" title="">2016</a>)</cite> assigns a score to each rank and sums these scores for every item to achieve a final scoring / ranking. To allow our implementation to be tuned for the best trade-off between fairness and accuracy, we use a weighted version of the method with the recommender weight set to 0.6. This gives the recommender 1.5x as much weight in the final outcome as compared to the allocated agents.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.SS2.p8">
<p class="ltx_p" id="S5.SS2.p8.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p8.1.1">Copeland</span>: The Copeland mechanism is a pair-wise method. It uses the win-loss record for each pair of items on each the ballot, and awards each item a point per win. These scores are then used to order the items <cite class="ltx_cite ltx_citemacro_citep">(Pacuit, <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib23" title="">2019</a>)</cite>. We use a weighted version; as with Borda, this is realized by multiplying the number of ballots.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.SS2.p9">
<p class="ltx_p" id="S5.SS2.p9.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p9.1.1">Rescoring</span>: The Rescoring mechanism makes use of the scores from the recommendation function, rather than only rankings. A linear combination of the scores from each allocated agent (and the recommender) are computed and the items are reordered based on these values. In our scoring function, an agent contributes a fixed <math alttext="\delta=0.5" class="ltx_Math" display="inline" id="S5.SS2.p9.1.m1.1"><semantics id="S5.SS2.p9.1.m1.1a"><mrow id="S5.SS2.p9.1.m1.1.1" xref="S5.SS2.p9.1.m1.1.1.cmml"><mi id="S5.SS2.p9.1.m1.1.1.2" xref="S5.SS2.p9.1.m1.1.1.2.cmml">δ</mi><mo id="S5.SS2.p9.1.m1.1.1.1" xref="S5.SS2.p9.1.m1.1.1.1.cmml">=</mo><mn id="S5.SS2.p9.1.m1.1.1.3" xref="S5.SS2.p9.1.m1.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p9.1.m1.1b"><apply id="S5.SS2.p9.1.m1.1.1.cmml" xref="S5.SS2.p9.1.m1.1.1"><eq id="S5.SS2.p9.1.m1.1.1.1.cmml" xref="S5.SS2.p9.1.m1.1.1.1"></eq><ci id="S5.SS2.p9.1.m1.1.1.2.cmml" xref="S5.SS2.p9.1.m1.1.1.2">𝛿</ci><cn id="S5.SS2.p9.1.m1.1.1.3.cmml" type="float" xref="S5.SS2.p9.1.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p9.1.m1.1c">\delta=0.5</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p9.1.m1.1d">italic_δ = 0.5</annotation></semantics></math> to the score for protected items and 0 for unprotected items<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>This <math alttext="\delta" class="ltx_Math" display="inline" id="footnote4.m1.1"><semantics id="footnote4.m1.1b"><mi id="footnote4.m1.1.1" xref="footnote4.m1.1.1.cmml">δ</mi><annotation-xml encoding="MathML-Content" id="footnote4.m1.1c"><ci id="footnote4.m1.1.1.cmml" xref="footnote4.m1.1.1">𝛿</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote4.m1.1d">\delta</annotation><annotation encoding="application/x-llamapun" id="footnote4.m1.1e">italic_δ</annotation></semantics></math> value was found to provide a good fairness / accuracy trade-off in prior work.</span></span></span>. The values are weighted first by the agent’s allocation weight and then by the inverse of the weight associated with the recommender.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.SS2.p10">
<p class="ltx_p" id="S5.SS2.p10.1">These mechanisms allow us to explore several dimensions of preference aggregation. First, there is the pair-wise versus scoring-based distinction with Copeland, the pair-wise option. Second, there is the difference between ordinal social choice methods and the Rescoring method that makes use of the real-valued predicted ratings arising from the recommendation function.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.SS2.p11">
<p class="ltx_p" id="S5.SS2.p11.5">Layering the allocation and choice mechanisms onto an existing recommender system introduces a small computational overhead (complexity), however, we see the risks of integrating a single fairness definition into the recommender (which would need to be completely retrained for a new metric) as far outweighing the (computational) cost. In terms of the allocation stage, each of the proposed mechanisms can be computed from the lottery history in <math alttext="\mathcal{O}(n)" class="ltx_Math" display="inline" id="S5.SS2.p11.1.m1.1"><semantics id="S5.SS2.p11.1.m1.1a"><mrow id="S5.SS2.p11.1.m1.1.2" xref="S5.SS2.p11.1.m1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.SS2.p11.1.m1.1.2.2" xref="S5.SS2.p11.1.m1.1.2.2.cmml">𝒪</mi><mo id="S5.SS2.p11.1.m1.1.2.1" xref="S5.SS2.p11.1.m1.1.2.1.cmml">⁢</mo><mrow id="S5.SS2.p11.1.m1.1.2.3.2" xref="S5.SS2.p11.1.m1.1.2.cmml"><mo id="S5.SS2.p11.1.m1.1.2.3.2.1" stretchy="false" xref="S5.SS2.p11.1.m1.1.2.cmml">(</mo><mi id="S5.SS2.p11.1.m1.1.1" xref="S5.SS2.p11.1.m1.1.1.cmml">n</mi><mo id="S5.SS2.p11.1.m1.1.2.3.2.2" stretchy="false" xref="S5.SS2.p11.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p11.1.m1.1b"><apply id="S5.SS2.p11.1.m1.1.2.cmml" xref="S5.SS2.p11.1.m1.1.2"><times id="S5.SS2.p11.1.m1.1.2.1.cmml" xref="S5.SS2.p11.1.m1.1.2.1"></times><ci id="S5.SS2.p11.1.m1.1.2.2.cmml" xref="S5.SS2.p11.1.m1.1.2.2">𝒪</ci><ci id="S5.SS2.p11.1.m1.1.1.cmml" xref="S5.SS2.p11.1.m1.1.1">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p11.1.m1.1c">\mathcal{O}(n)</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p11.1.m1.1d">caligraphic_O ( italic_n )</annotation></semantics></math> time where <math alttext="n" class="ltx_Math" display="inline" id="S5.SS2.p11.2.m2.1"><semantics id="S5.SS2.p11.2.m2.1a"><mi id="S5.SS2.p11.2.m2.1.1" xref="S5.SS2.p11.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p11.2.m2.1b"><ci id="S5.SS2.p11.2.m2.1.1.cmml" xref="S5.SS2.p11.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p11.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p11.2.m2.1d">italic_n</annotation></semantics></math> is the length of the history to be considered. This computation is bounded by the window size and can scale depending on how reactive we want the system to be. For the choice mechanism, Borda and Rescoring can be computed in <math alttext="\mathcal{O}(n)" class="ltx_Math" display="inline" id="S5.SS2.p11.3.m3.1"><semantics id="S5.SS2.p11.3.m3.1a"><mrow id="S5.SS2.p11.3.m3.1.2" xref="S5.SS2.p11.3.m3.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.SS2.p11.3.m3.1.2.2" xref="S5.SS2.p11.3.m3.1.2.2.cmml">𝒪</mi><mo id="S5.SS2.p11.3.m3.1.2.1" xref="S5.SS2.p11.3.m3.1.2.1.cmml">⁢</mo><mrow id="S5.SS2.p11.3.m3.1.2.3.2" xref="S5.SS2.p11.3.m3.1.2.cmml"><mo id="S5.SS2.p11.3.m3.1.2.3.2.1" stretchy="false" xref="S5.SS2.p11.3.m3.1.2.cmml">(</mo><mi id="S5.SS2.p11.3.m3.1.1" xref="S5.SS2.p11.3.m3.1.1.cmml">n</mi><mo id="S5.SS2.p11.3.m3.1.2.3.2.2" stretchy="false" xref="S5.SS2.p11.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p11.3.m3.1b"><apply id="S5.SS2.p11.3.m3.1.2.cmml" xref="S5.SS2.p11.3.m3.1.2"><times id="S5.SS2.p11.3.m3.1.2.1.cmml" xref="S5.SS2.p11.3.m3.1.2.1"></times><ci id="S5.SS2.p11.3.m3.1.2.2.cmml" xref="S5.SS2.p11.3.m3.1.2.2">𝒪</ci><ci id="S5.SS2.p11.3.m3.1.1.cmml" xref="S5.SS2.p11.3.m3.1.1">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p11.3.m3.1c">\mathcal{O}(n)</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p11.3.m3.1d">caligraphic_O ( italic_n )</annotation></semantics></math> while Copeland is <math alttext="\mathcal{O}(n^{2})" class="ltx_Math" display="inline" id="S5.SS2.p11.4.m4.1"><semantics id="S5.SS2.p11.4.m4.1a"><mrow id="S5.SS2.p11.4.m4.1.1" xref="S5.SS2.p11.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.SS2.p11.4.m4.1.1.3" xref="S5.SS2.p11.4.m4.1.1.3.cmml">𝒪</mi><mo id="S5.SS2.p11.4.m4.1.1.2" xref="S5.SS2.p11.4.m4.1.1.2.cmml">⁢</mo><mrow id="S5.SS2.p11.4.m4.1.1.1.1" xref="S5.SS2.p11.4.m4.1.1.1.1.1.cmml"><mo id="S5.SS2.p11.4.m4.1.1.1.1.2" stretchy="false" xref="S5.SS2.p11.4.m4.1.1.1.1.1.cmml">(</mo><msup id="S5.SS2.p11.4.m4.1.1.1.1.1" xref="S5.SS2.p11.4.m4.1.1.1.1.1.cmml"><mi id="S5.SS2.p11.4.m4.1.1.1.1.1.2" xref="S5.SS2.p11.4.m4.1.1.1.1.1.2.cmml">n</mi><mn id="S5.SS2.p11.4.m4.1.1.1.1.1.3" xref="S5.SS2.p11.4.m4.1.1.1.1.1.3.cmml">2</mn></msup><mo id="S5.SS2.p11.4.m4.1.1.1.1.3" stretchy="false" xref="S5.SS2.p11.4.m4.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p11.4.m4.1b"><apply id="S5.SS2.p11.4.m4.1.1.cmml" xref="S5.SS2.p11.4.m4.1.1"><times id="S5.SS2.p11.4.m4.1.1.2.cmml" xref="S5.SS2.p11.4.m4.1.1.2"></times><ci id="S5.SS2.p11.4.m4.1.1.3.cmml" xref="S5.SS2.p11.4.m4.1.1.3">𝒪</ci><apply id="S5.SS2.p11.4.m4.1.1.1.1.1.cmml" xref="S5.SS2.p11.4.m4.1.1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p11.4.m4.1.1.1.1.1.1.cmml" xref="S5.SS2.p11.4.m4.1.1.1.1">superscript</csymbol><ci id="S5.SS2.p11.4.m4.1.1.1.1.1.2.cmml" xref="S5.SS2.p11.4.m4.1.1.1.1.1.2">𝑛</ci><cn id="S5.SS2.p11.4.m4.1.1.1.1.1.3.cmml" type="integer" xref="S5.SS2.p11.4.m4.1.1.1.1.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p11.4.m4.1c">\mathcal{O}(n^{2})</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p11.4.m4.1d">caligraphic_O ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )</annotation></semantics></math> where <math alttext="n" class="ltx_Math" display="inline" id="S5.SS2.p11.5.m5.1"><semantics id="S5.SS2.p11.5.m5.1a"><mi id="S5.SS2.p11.5.m5.1.1" xref="S5.SS2.p11.5.m5.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p11.5.m5.1b"><ci id="S5.SS2.p11.5.m5.1.1.cmml" xref="S5.SS2.p11.5.m5.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p11.5.m5.1c">n</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p11.5.m5.1d">italic_n</annotation></semantics></math> is the number of elements in the output list <cite class="ltx_cite ltx_citemacro_citep">(Zwicker, <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib39" title="">2016</a>)</cite>. We expect in most practical applications for both of these to be smaller numbers which do not incur significant overhead.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Evaluation</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">We evaluate the results of these experiments examining fairness and accuracy. All results were computed over five training / test folds of the data and averaged. Fairness is computed relative to the metric associated with each agent. However, this summative fairness score is over the entire experiment, not the time-bounded window that agents compute over during experiment execution.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.2">We summarize the fairness scores across all the agents using a function of the <math alttext="l_{1/2}" class="ltx_Math" display="inline" id="S5.SS3.p2.1.m1.1"><semantics id="S5.SS3.p2.1.m1.1a"><msub id="S5.SS3.p2.1.m1.1.1" xref="S5.SS3.p2.1.m1.1.1.cmml"><mi id="S5.SS3.p2.1.m1.1.1.2" xref="S5.SS3.p2.1.m1.1.1.2.cmml">l</mi><mrow id="S5.SS3.p2.1.m1.1.1.3" xref="S5.SS3.p2.1.m1.1.1.3.cmml"><mn id="S5.SS3.p2.1.m1.1.1.3.2" xref="S5.SS3.p2.1.m1.1.1.3.2.cmml">1</mn><mo id="S5.SS3.p2.1.m1.1.1.3.1" xref="S5.SS3.p2.1.m1.1.1.3.1.cmml">/</mo><mn id="S5.SS3.p2.1.m1.1.1.3.3" xref="S5.SS3.p2.1.m1.1.1.3.3.cmml">2</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.1.m1.1b"><apply id="S5.SS3.p2.1.m1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS3.p2.1.m1.1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1">subscript</csymbol><ci id="S5.SS3.p2.1.m1.1.1.2.cmml" xref="S5.SS3.p2.1.m1.1.1.2">𝑙</ci><apply id="S5.SS3.p2.1.m1.1.1.3.cmml" xref="S5.SS3.p2.1.m1.1.1.3"><divide id="S5.SS3.p2.1.m1.1.1.3.1.cmml" xref="S5.SS3.p2.1.m1.1.1.3.1"></divide><cn id="S5.SS3.p2.1.m1.1.1.3.2.cmml" type="integer" xref="S5.SS3.p2.1.m1.1.1.3.2">1</cn><cn id="S5.SS3.p2.1.m1.1.1.3.3.cmml" type="integer" xref="S5.SS3.p2.1.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.1.m1.1c">l_{1/2}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p2.1.m1.1d">italic_l start_POSTSUBSCRIPT 1 / 2 end_POSTSUBSCRIPT</annotation></semantics></math> norm of over the fairness scores for each individual agent <cite class="ltx_cite ltx_citemacro_citep">(Mehrotra et&nbsp;al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib21" title="">2018</a>)</cite>. This metric is maximized (and equal to the mean) when all of the scores are equal, and it is below the mean when the scores are unequal. The value is normalized to <math alttext="[0,1]" class="ltx_Math" display="inline" id="S5.SS3.p2.2.m2.2"><semantics id="S5.SS3.p2.2.m2.2a"><mrow id="S5.SS3.p2.2.m2.2.3.2" xref="S5.SS3.p2.2.m2.2.3.1.cmml"><mo id="S5.SS3.p2.2.m2.2.3.2.1" stretchy="false" xref="S5.SS3.p2.2.m2.2.3.1.cmml">[</mo><mn id="S5.SS3.p2.2.m2.1.1" xref="S5.SS3.p2.2.m2.1.1.cmml">0</mn><mo id="S5.SS3.p2.2.m2.2.3.2.2" xref="S5.SS3.p2.2.m2.2.3.1.cmml">,</mo><mn id="S5.SS3.p2.2.m2.2.2" xref="S5.SS3.p2.2.m2.2.2.cmml">1</mn><mo id="S5.SS3.p2.2.m2.2.3.2.3" stretchy="false" xref="S5.SS3.p2.2.m2.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.2.m2.2b"><interval closure="closed" id="S5.SS3.p2.2.m2.2.3.1.cmml" xref="S5.SS3.p2.2.m2.2.3.2"><cn id="S5.SS3.p2.2.m2.1.1.cmml" type="integer" xref="S5.SS3.p2.2.m2.1.1">0</cn><cn id="S5.SS3.p2.2.m2.2.2.cmml" type="integer" xref="S5.SS3.p2.2.m2.2.2">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.2.m2.2c">[0,1]</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p2.2.m2.2d">[ 0 , 1 ]</annotation></semantics></math>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1">Accuracy is computed using nDCG based on held-out test data in each data set. For the purposes of this study, we did not use any other algorithms as comparators; we are only comparing SCRUF-D’s output against the non-re-ranked results. As noted in Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#S2" title="2. Related Work ‣ Social Choice for Heterogeneous Fairness in Recommendation"><span class="ltx_text ltx_ref_tag">2</span></a>, there are very few algorithms that attempt to address multiple fairness concerns simultaneously and only one, OFair <cite class="ltx_cite ltx_citemacro_citep">(Sonboli et&nbsp;al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib32" title="">2020b</a>)</cite>, is capable of supporting heterogeneous fairness definitions. Prior work found that OFair is not competitive against SCRUF-D <cite class="ltx_cite ltx_citemacro_citep">(Aird et&nbsp;al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#bib.bib3" title="">2024</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Results</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#S5.F1.sf1" title="In Figure 1 ‣ 5.1. Agent Definitions ‣ 5. Methodology ‣ Social Choice for Heterogeneous Fairness in Recommendation"><span class="ltx_text ltx_ref_tag">1(a)</span></a> shows the results for the Microlending data comparing accuracy (x-axis) with the <math alttext="l_{1/2}" class="ltx_Math" display="inline" id="S6.p1.1.m1.1"><semantics id="S6.p1.1.m1.1a"><msub id="S6.p1.1.m1.1.1" xref="S6.p1.1.m1.1.1.cmml"><mi id="S6.p1.1.m1.1.1.2" xref="S6.p1.1.m1.1.1.2.cmml">l</mi><mrow id="S6.p1.1.m1.1.1.3" xref="S6.p1.1.m1.1.1.3.cmml"><mn id="S6.p1.1.m1.1.1.3.2" xref="S6.p1.1.m1.1.1.3.2.cmml">1</mn><mo id="S6.p1.1.m1.1.1.3.1" xref="S6.p1.1.m1.1.1.3.1.cmml">/</mo><mn id="S6.p1.1.m1.1.1.3.3" xref="S6.p1.1.m1.1.1.3.3.cmml">2</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S6.p1.1.m1.1b"><apply id="S6.p1.1.m1.1.1.cmml" xref="S6.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S6.p1.1.m1.1.1.1.cmml" xref="S6.p1.1.m1.1.1">subscript</csymbol><ci id="S6.p1.1.m1.1.1.2.cmml" xref="S6.p1.1.m1.1.1.2">𝑙</ci><apply id="S6.p1.1.m1.1.1.3.cmml" xref="S6.p1.1.m1.1.1.3"><divide id="S6.p1.1.m1.1.1.3.1.cmml" xref="S6.p1.1.m1.1.1.3.1"></divide><cn id="S6.p1.1.m1.1.1.3.2.cmml" type="integer" xref="S6.p1.1.m1.1.1.3.2">1</cn><cn id="S6.p1.1.m1.1.1.3.3.cmml" type="integer" xref="S6.p1.1.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.1.m1.1c">l_{1/2}</annotation><annotation encoding="application/x-llamapun" id="S6.p1.1.m1.1d">italic_l start_POSTSUBSCRIPT 1 / 2 end_POSTSUBSCRIPT</annotation></semantics></math> fairness norm on the y-axis. (Individual fairness results for each agent are not included for reasons of space.) Although Least Fair has very strong fairness outcomes across different options for the choice mechanism, it suffers from low accuracy. This matches our expectation that taking user compatibility into account allows the system to make a better trade-off.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">For the Borda and Rescore choice mechanisms, the Lottery and Weighted allocations look quite similar considering their 95% confidence intervals, suggesting that these allocation mechanisms can give similar results. This is encouraging because the Lottery has the potential for greater efficiency.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">Across the choice mechanisms, Borda tends to have the smallest fairness improvement and does a somewhat better job of preserving accuracy. Copeland offers improved fairness, but these choice mechanisms are dominated by Rescore, which achieves better than 0.9 on the <math alttext="l_{1/2}" class="ltx_Math" display="inline" id="S6.p3.1.m1.1"><semantics id="S6.p3.1.m1.1a"><msub id="S6.p3.1.m1.1.1" xref="S6.p3.1.m1.1.1.cmml"><mi id="S6.p3.1.m1.1.1.2" xref="S6.p3.1.m1.1.1.2.cmml">l</mi><mrow id="S6.p3.1.m1.1.1.3" xref="S6.p3.1.m1.1.1.3.cmml"><mn id="S6.p3.1.m1.1.1.3.2" xref="S6.p3.1.m1.1.1.3.2.cmml">1</mn><mo id="S6.p3.1.m1.1.1.3.1" xref="S6.p3.1.m1.1.1.3.1.cmml">/</mo><mn id="S6.p3.1.m1.1.1.3.3" xref="S6.p3.1.m1.1.1.3.3.cmml">2</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S6.p3.1.m1.1b"><apply id="S6.p3.1.m1.1.1.cmml" xref="S6.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S6.p3.1.m1.1.1.1.cmml" xref="S6.p3.1.m1.1.1">subscript</csymbol><ci id="S6.p3.1.m1.1.1.2.cmml" xref="S6.p3.1.m1.1.1.2">𝑙</ci><apply id="S6.p3.1.m1.1.1.3.cmml" xref="S6.p3.1.m1.1.1.3"><divide id="S6.p3.1.m1.1.1.3.1.cmml" xref="S6.p3.1.m1.1.1.3.1"></divide><cn id="S6.p3.1.m1.1.1.3.2.cmml" type="integer" xref="S6.p3.1.m1.1.1.3.2">1</cn><cn id="S6.p3.1.m1.1.1.3.3.cmml" type="integer" xref="S6.p3.1.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p3.1.m1.1c">l_{1/2}</annotation><annotation encoding="application/x-llamapun" id="S6.p3.1.m1.1d">italic_l start_POSTSUBSCRIPT 1 / 2 end_POSTSUBSCRIPT</annotation></semantics></math> metric without a noticeable loss in accuracy.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.p4">
<p class="ltx_p" id="S6.p4.1">Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.04551v1#S5.F1.sf2" title="In Figure 1 ‣ 5.1. Agent Definitions ‣ 5. Methodology ‣ Social Choice for Heterogeneous Fairness in Recommendation"><span class="ltx_text ltx_ref_tag">1(b)</span></a> shows the positioning of the different mechanism combinations in the accuracy / fairness space for the Movies dataset. We still see Rescoring as dominant and Borda in a lower fairness/higher accuracy position, with Copeland somewhere in the middle. We also see the Rescore and Copeland mechanisms clustered at the far right indicating that they are able to reach the fairness targets across all the agents almost fully.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.p5">
<p class="ltx_p" id="S6.p5.1">One interesting difference is the positioning of the Lottery and Weighted mechanisms. In the Microlending data, these mechanisms gave similar results with Lottery a bit lower in accuracy. In the Movies results, the difference is more pronounced and there is greater cost for the Lottery mechanism. On the other hand, the accuracy values are closer to baseline in Movies, showing that high fairness does not have to come at the cost of accuracy loss.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Conclusion</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">This paper addresses two key limitations in existing fairness-aware recommendation research that have not been addressed in prior work. First, we formulate multiple definitions of provider-side fairness relevant to real-world datasets. Prior work has been limited to, at most two concerns, usually on different sides of the recommendation interaction. Second, the work posits heterogeneous fairness definitions, allowing different fairness issues to be represented by different metrics. We believe that allowing a multiplicity of concerns and allowing for varied fairness definitions and targets is essential in practical settings.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">In the context of computational social choice, we show that SCRUF-D is compatible with a range of different allocation and choice mechanisms and, while some general patterns can be seen, that these mechanisms work differently across recommendation domains and datasets. We find that in some datasets a Lottery mechanism can be competitive with one that allocates multiple fairness agents at a time, suggesting potential efficiency in applying these techniques in practice.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S7.p3">
<p class="ltx_p" id="S7.p3.1">Numerous additional challenges remain. This work has concentrated on provider-side group fairness under multiple definitions. Additional definitions exist and are worth exploring. There is also the question of scale: how many simultaneous agents can be supported? In addition, we believe that SCRUF-D is capable of supporting individual fairness and consumer-side fairness, but additional work needs to be done to demonstrate this capacity.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
Burke, Voida and Aird were supported by the National Science Foundation under grant awards IIS-1911025 and IIS-2107577. Mattei was supported by NSF Grant IIS-2107505. Stefancova was supported by Slovak Research and Development Agency under Contract no. APVV-20-0353 and the Fulbright program.

</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aird et&nbsp;al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Amanda Aird, Cassidy All, Paresha Farastu, Elena Stefancova, Joshua Sun, Nicholas Mattei, and Robin Burke. 2023.

</span>
<span class="ltx_bibblock">Exploring Social Choice Mechanisms for Recommendation Fairness in SCRUF.

</span>
<span class="ltx_bibblock">Presented at the 2023 FAccTRec Workshop on Responsible Recommendation..

</span>
<span class="ltx_bibblock">arXiv:2309.08621&nbsp;[cs.IR]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2309.08621" title="">https://arxiv.org/abs/2309.08621</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aird et&nbsp;al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Amanda Aird, Paresha Farastu, Joshua Sun, Elena Štefancová, Cassidy All, Amy Voida, Nicholas Mattei, and Robin Burke. 2024.

</span>
<span class="ltx_bibblock">Dynamic fairness-aware recommendation through multi-agent social choice.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2303.00968&nbsp;[cs.AI]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2303.00968" title="">https://arxiv.org/abs/2303.00968</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Beutel et&nbsp;al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Alex Beutel, Jilin Chen, Tulsee Doshi, Hai Qian, Li Wei, Yi Wu, Lukasz Heldt, Zhe Zhao, Lichan Hong, Ed&nbsp;H. Chi, and Cristos Goodrow. 2019.

</span>
<span class="ltx_bibblock">Fairness in Recommendation Ranking through Pairwise Comparisons.

</span>
<span class="ltx_bibblock">, 16&nbsp;pages.

</span>
<span class="ltx_bibblock">arXiv:1903.00780&nbsp;[cs.CY]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1903.00780" title="">https://arxiv.org/abs/1903.00780</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brandt (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Felix Brandt. 2017.

</span>
<span class="ltx_bibblock">Rolling the dice: Recent results in probabilistic social choice.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Trends in Computational Social Choice</em>, Ulle Endriss (Ed.). AI Access, Chapter&nbsp;1, 3–26.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Buolamwini and Gebru (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Joy Buolamwini and Timnit Gebru. 2018.

</span>
<span class="ltx_bibblock">Gender shades: Intersectional accuracy disparities in commercial gender classification. In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Conference on Fairness, Accountability and Transparency</em>. ACM, 77–91.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Burke (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Robin Burke. 2017.

</span>
<span class="ltx_bibblock">Multisided Fairness for Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Workshop on Fairness, Accountability and Transparency in Machine Learning (FATML)</em>. Halifax, Nova Scotia, 5&nbsp;pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1707.00093" title="">https://arxiv.org/abs/1707.00093</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Burke et&nbsp;al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Robin Burke, Nicholas Mattei, Vladislav Grozin, Amy Voida, and Nasim Sonboli. 2022.

</span>
<span class="ltx_bibblock">Multi-agent Social Choice for Dynamic Fairness-aware Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">Adjunct Proceedings of the 30th ACM Conference on User Modeling, Adaptation and Personalization</em>. ACM, 234–244.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chouldechova (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Alexandra Chouldechova. 2017.

</span>
<span class="ltx_bibblock">Fair prediction with disparate impact: A study of bias in recidivism prediction instruments.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Big data</em> 5, 2 (2017), 153–163.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cramer et&nbsp;al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Henriette Cramer, Kenneth Holstein, Jennifer&nbsp;W. Vaughan, Hal Daumé&nbsp;III, Miroslav Dudík, Hanna Wallach, Sravana Reddy, and Jean Garcia-Gathright. 2019.

</span>
<span class="ltx_bibblock">Challenges of incorporating algorithmic fairness into industry practice.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dwork et&nbsp;al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2012)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. 2012.

</span>
<span class="ltx_bibblock">Fairness through awareness. In <em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">Proceedings of the 3rd Innovations in Theoretical Computer Science Conference</em>. ACM, 214–226.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ekstrand et&nbsp;al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Michael&nbsp;D. Ekstrand, Anubrata Das, Robin Burke, and Fernando Diaz. 2022.

</span>
<span class="ltx_bibblock">Fairness in Information Access Systems.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2105.05779&nbsp;[cs.IR]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ferraro et&nbsp;al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Andres Ferraro, Xavier Serra, and Christine Bauer. 2021.

</span>
<span class="ltx_bibblock">Break the loop: Gender imbalance in music recommenders. In <em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">Proceedings of the 2021 Conference on Human Information Interaction and Retrieval</em>. 249–254.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hardt et&nbsp;al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2016)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Moritz Hardt, Eric Price, and Nati Srebro. 2016.

</span>
<span class="ltx_bibblock">Equality of opportunity in supervised learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">Advances in neural information processing systems</em>. 3315–3323.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Harper and Konstan (2015)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
F&nbsp;Maxwell Harper and Joseph&nbsp;A Konstan. 2015.

</span>
<span class="ltx_bibblock">The MovieLens Datasets: History and Context.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">ACM Transactions on Interactive Intelligent Systems (TiiS)</em> 5, 4 (2015), 19.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hébert-Johnson et&nbsp;al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Úrsula Hébert-Johnson, Michael Kim, Omer Reingold, and Guy Rothblum. 2018.

</span>
<span class="ltx_bibblock">Multicalibration: Calibration for the (Computationally-identifiable) masses. In <em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">International Conference on Machine Learning</em>. 1944–1953.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Holstein et&nbsp;al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Kenneth Holstein, Jennifer&nbsp;W. Vaughan, Hal Daumé&nbsp;III, Miroslav Dudík, and Hanna Wallach. 2019.

</span>
<span class="ltx_bibblock">Improving fairness in machine learning systems: What do industry practitioners need?. In <em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">Proceedings of the Conference of Human Computer Interaction (CHI’19)</em>. ACM, 16&nbsp;pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="arxiv:1812.05239v2%5Bcs.HC%5D7Jan2019" title="">arXiv:1812.05239v2[cs.HC]7Jan2019</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hutchinson and Mitchell (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ben Hutchinson and Margaret Mitchell. 2019.

</span>
<span class="ltx_bibblock">50 years of test (un) fairness: Lessons for machine learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the conference on fairness, accountability, and transparency</em>. ACM, 49–58.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kamiran et&nbsp;al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2010)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Faisal Kamiran, Toon Calders, and Mykola Pechenizkiy. 2010.

</span>
<span class="ltx_bibblock">Discrimination aware decision tree learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">Data Mining (ICDM), 2010 IEEE 10th International Conference on</em>. IEEE, 869–874.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kearns et&nbsp;al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei&nbsp;Steven Wu. 2018.

</span>
<span class="ltx_bibblock">Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1711.05144&nbsp;[cs.LG]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mehrotra et&nbsp;al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rishabh Mehrotra, James McInerney, Hugues Bouchard, Mounia Lalmas, and Fernando Diaz. 2018.

</span>
<span class="ltx_bibblock">Towards a Fair Marketplace: Counterfactual Evaluation of the Trade-off between Relevance, Fairness &amp; Satisfaction in Recommendation Systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib21.3.1">Proceedings of the Conference on Information and Knowledge Management</em>. 2243–2251.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Narayanan (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Arvind Narayanan. 2018.

</span>
<span class="ltx_bibblock">Translation tutorial: 21 fairness definitions and their politics. In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proc. Conf. Fairness Accountability Transp., New York, USA</em>, Vol.&nbsp;1170. ACM, 3.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pacuit (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Eric Pacuit. 2019.

</span>
<span class="ltx_bibblock">Voting Methods.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">The Stanford Encyclopedia of Philosophy</em> (Fall 2019 ed.), Edward&nbsp;N. Zalta (Ed.). Metaphysics Research Lab, Stanford University.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patro et&nbsp;al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Gourab&nbsp;K Patro, Arpita Biswas, Niloy Ganguly, Krishna&nbsp;P Gummadi, and Abhijnan Chakraborty. 2020.

</span>
<span class="ltx_bibblock">FairRec: Two-Sided Fairness for Personalized Recommendations in Two-Sided Platforms. In <em class="ltx_emph ltx_font_italic" id="bib.bib24.3.1">Proceedings of The Web Conference 2020</em>. 1194–1204.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patro et&nbsp;al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Gourab&nbsp;K Patro, Lorenzo Porcaro, Laura Mitchell, Qiuyue Zhang, Meike Zehlike, and Nikhil Garg. 2022.

</span>
<span class="ltx_bibblock">Fair ranking: a critical review, challenges, and future directions. In <em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency</em>. 1929–1942.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pedreshi et&nbsp;al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2008)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Dino Pedreshi, Salvatore Ruggieri, and Franco Turini. 2008.

</span>
<span class="ltx_bibblock">Discrimination-aware data mining. In <em class="ltx_emph ltx_font_italic" id="bib.bib26.3.1">Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</em>. ACM, 560–568.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Selbst et&nbsp;al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Andrew&nbsp;D Selbst, Danah Boyd, Sorelle&nbsp;A Friedler, Suresh Venkatasubramanian, and Janet Vertesi. 2019.

</span>
<span class="ltx_bibblock">Fairness and abstraction in sociotechnical systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib27.3.1">Proceedings of the conference on fairness, accountability, and transparency</em>. ACM, New York, 59–68.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Smith et&nbsp;al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jessie&nbsp;J Smith, Lex Beattie, and Henriette Cramer. 2023.

</span>
<span class="ltx_bibblock">Scoping Fairness Objectives and Identifying Fairness Metrics for Recommender Systems: The Practitioners’ Perspective. In <em class="ltx_emph ltx_font_italic" id="bib.bib28.3.1">Proceedings of the ACM Web Conference 2023</em>. 3648–3659.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sonboli et&nbsp;al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2022a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Nasim Sonboli, Amanda Aird, and Robin Burke. 2022a.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.3.1">Microlending 2017 Data Set</em>.

</span>
<span class="ltx_bibblock">University of Colorado Boulder.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.25810/PGJK-RR19" title="">https://doi.org/10.25810/PGJK-RR19</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sonboli et&nbsp;al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2022b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Nasim Sonboli, Robin Burke, Michael Ekstrand, and Rishabh Mehrotra. 2022b.

</span>
<span class="ltx_bibblock">The multisided complexity of fairness in recommender systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.3.1">AI magazine</em> 43, 2 (2022), 164–176.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sonboli et&nbsp;al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2020a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Nasim Sonboli, Robin Burke, Nicholas Mattei, Farzad Eskandanian, and Tian Gao. 2020a.

</span>
<span class="ltx_bibblock">”And the Winner Is…”: Dynamic Lotteries for Multi-group Fairness-Aware Recommendation.

</span>
<span class="ltx_bibblock">Presented at the 2020 FAccTRec Workshop on Responsible Recommendation.

</span>
<span class="ltx_bibblock">arXiv:2009.02590&nbsp;[cs.IR]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sonboli et&nbsp;al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2020b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Nasim Sonboli, Farzad Eskandanian, Robin Burke, Weiwen Liu, and Bamshad Mobasher. 2020b.

</span>
<span class="ltx_bibblock">Opportunistic Multi-Aspect Fairness through Personalized Re-Ranking. In <em class="ltx_emph ltx_font_italic" id="bib.bib32.3.1">Proceedings of the 28th ACM Conference on User Modeling, Adaptation and Personalization</em> (Genoa, Italy) <em class="ltx_emph ltx_font_italic" id="bib.bib32.4.2">(UMAP ’20)</em>. Association for Computing Machinery, New York, NY, USA, 239–247.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3340631.3394846" title="">https://doi.org/10.1145/3340631.3394846</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sühr et&nbsp;al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tom Sühr, Asia&nbsp;J Biega, Meike Zehlike, Krishna&nbsp;P Gummadi, and Abhijnan Chakraborty. 2019.

</span>
<span class="ltx_bibblock">Two-sided fairness for repeated matchings in two-sided markets: A case study of a ride-hailing platform. In <em class="ltx_emph ltx_font_italic" id="bib.bib33.3.1">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em>. 3082–3092.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et&nbsp;al<span class="ltx_text" id="bib.bib34.2.2.1">.</span> (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Haolun Wu, Chen Ma, Bhaskar Mitra, Fernando Diaz, and Xue Liu. 2022.

</span>
<span class="ltx_bibblock">A multi-objective optimization framework for multi-stakeholder fairness-aware recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.3.1">ACM Transactions on Information Systems</em> 41, 2 (2022), 1–29.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zehlike et&nbsp;al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Meike Zehlike, Tom Sühr, Ricardo Baeza-Yates, Francesco Bonchi, Carlos Castillo, and Sara Hajian. 2022.

</span>
<span class="ltx_bibblock">Fair Top-k Ranking with multiple protected groups.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.3.1">Information Processing &amp; Management</em> 59, 1 (2022), 102707.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zemel et&nbsp;al<span class="ltx_text" id="bib.bib36.2.2.1">.</span> (2013)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. 2013.

</span>
<span class="ltx_bibblock">Learning fair representations. In <em class="ltx_emph ltx_font_italic" id="bib.bib36.3.1">Proceedings of the 30th International Conference on Machine Learning (ICML-13)</em>. 325–333.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang and Wu (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Lu Zhang and Xintao Wu. 2017.

</span>
<span class="ltx_bibblock">Anti-discrimination learning: a causal modeling-based framework.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">International Journal of Data Science and Analytics</em> 4 (2017), 1–16.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et&nbsp;al<span class="ltx_text" id="bib.bib38.2.2.1">.</span> (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ziwei Zhu, Xia Hu, and James Caverlee. 2018.

</span>
<span class="ltx_bibblock">Fairness-aware tensor-based recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib38.3.1">Proceedings of the 27th ACM International Conference on Information and Knowledge Management</em>. 1153–1162.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zwicker (2016)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
William&nbsp;S. Zwicker. 2016.

</span>
<span class="ltx_bibblock">Introduction to the Theory of Voting.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Handbook of Computational Social Choice</em>, Felix Brandt, Vincent Conitzer, Ulle Endriss, Jérôme Lang, and Ariel&nbsp;D. Procaccia (Eds.). Cambridge University Press, 23–56.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1017/CBO9781107446984.003" title="">https://doi.org/10.1017/CBO9781107446984.003</a>
</span>
</li>
</ul>
</section>
</article>
</div>

</div>


<button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed;">Report Issue for Selection</button><div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer></body></html>