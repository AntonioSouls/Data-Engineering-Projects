<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2011.08517] Bridging the Reality Gap for Pose Estimation Networks using Sensor-Based Domain Randomization</title><meta property="og:description" content="Since the introduction of modern deep learning methods for object pose estimation, test accuracy and efficiency has increased significantly. For training, however, large amounts of annotated training data are required …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Bridging the Reality Gap for Pose Estimation Networks using Sensor-Based Domain Randomization">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Bridging the Reality Gap for Pose Estimation Networks using Sensor-Based Domain Randomization">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2011.08517">

<!--Generated on Mon Feb 26 22:42:31 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Bridging the Reality Gap for Pose Estimation Networks using Sensor-Based Domain Randomization</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Frederik Hagelskjær
<br class="ltx_break">SDU Robotics
<br class="ltx_break">University of Southern Denmark
<br class="ltx_break">Odense, Denmark
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">frhag@mmmi.sdu.dk</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Anders Glent Buch
<br class="ltx_break">SDU Robotics
<br class="ltx_break">University of Southern Denmark
<br class="ltx_break">Odense, Denmark
<br class="ltx_break"><span id="id2.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">anbu@mmmi.sdu.dk</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id3.id1" class="ltx_p">Since the introduction of modern deep learning methods for object pose estimation, test accuracy and efficiency has increased significantly. For training, however, large amounts of annotated training data are required for good performance. While the use of synthetic training data prevents the need for manual annotation, there is currently a large performance gap between methods trained on real and synthetic data. This paper introduces a new method, which bridges this gap.</p>
<p id="id4.id2" class="ltx_p">Most methods trained on synthetic data use 2D images, as domain randomization in 2D is more developed. To obtain precise poses, many of these methods perform a final refinement using 3D data. Our method integrates the 3D data into the network to increase the accuracy of the pose estimation. To allow for domain randomization in 3D, a sensor-based data augmentation has been developed. Additionally, we introduce the SparseEdge feature, which uses a wider search space during point cloud propagation to avoid relying on specific features without increasing run-time.</p>
<p id="id5.id3" class="ltx_p">Experiments on three large pose estimation benchmarks show that the presented method outperforms previous methods trained on synthetic data and achieves comparable results to existing methods trained on real data.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In this paper, we present a pose estimation method trained entirely on synthetic data. By utilizing 3D data and sensor-based domain randomization, the trained network generalizes well to real test data. The method is tested on several datasets and attains state-of-the-art performance.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Pose estimation is generally a difficult challenge, and the set-up of new pose estimation systems is often time-consuming. A great deal of work is usually required to obtain satisfactory performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. The introduction of deep learning has allowed pose estimation to obtain much better performance compared with classic methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. However, the training of deep learning methods requires large amounts of data. For new use cases, this data needs to be collected and then manually labeled. This is an extensive task and limits the usability of deep learning methods for pose estimation. The amount of manual work can be drastically reduced by generating the data synthetically.
However, getting good performance on real data with methods trained on synthetic data is a difficult task.
Classical methods generally outperform deep learning methods when using synthetic training data.
An example of this is DPOD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, where accuracy on the LM dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> falls from 95.2% to 66.4% when switching from real to synthetic training data.
Another example is the method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> which is trained on synthetic depth data. This method only achieves a score of 46.8%, and is outperformed by the original Linemod <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> method at 63.0%.
As a result, most methods rely on real training data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper, we present a novel method for pose estimation trained entirely on synthetic data. As opposed to other deep learning methods, the pose estimation is performed in point clouds. This allows for the use of our sensor-based domain randomization, which generalizes to real data. To further increase the generalization, a modified edge feature compared to DGCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> is also presented. This edge feature allows for sparser and broader neighborhood searches, increasing the generalization while retaining speed.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The trained network performs both background segmentation and keypoint voting on the point cloud. This allows the network to learn the correct object segmentation when the correct keypoint is difficult to resolve. For example, determining the correct keypoint votes for a sphere is an impossible task, while learning the segmentation is much more simple. To handle symmetry, the method allows for multiple keypoint votes at a single point. This framework allows us to test the method on three different benchmarking datasets with 55 different objects without changing any settings.
Additionally, the method is able to predict whether the object is present inside the point cloud. This makes the method able to work with or without a candidate detector method. In this article, Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> is used to propose candidates, to speed up computation.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S1.F1.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="" id="S1.F1.sf1.g1" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S1.F1.sf1.3.2" class="ltx_text" style="font-size:90%;">Initial Image.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S1.F1.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="" id="S1.F1.sf2.g1" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S1.F1.sf2.3.2" class="ltx_text" style="font-size:90%;">Mask R-CNN results 
<br class="ltx_break">and cluster centers.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S1.F1.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="" id="S1.F1.sf3.g1" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S1.F1.sf3.3.2" class="ltx_text" style="font-size:90%;">Background Segmentation.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S1.F1.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="" id="S1.F1.sf4.g1" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S1.F1.sf4.3.2" class="ltx_text" style="font-size:90%;">Keypoint Voting.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S1.F1.sf5" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="" id="S1.F1.sf5.g1" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.sf5.2.1.1" class="ltx_text" style="font-size:90%;">(e)</span> </span><span id="S1.F1.sf5.3.2" class="ltx_text" style="font-size:90%;">Final pose projected 
<br class="ltx_break">into the image.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;"> The pipeline of our developed method, shown in a zoomed-in view of image 10 for object 6 in the Linemod (LM) dataset. From left to right: initial image, Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> and cluster detection with the four best clusters in green, background segmentation, keypoint voting, and finally the found pose in the scene shown in green. </span></figcaption>
</figure>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Our method achieves state-of-the-art performance on the Linemod (LM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> dataset for methods trained with synthetic data, and outperforms most methods trained on real data. On the Occlusion (LMO) dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> the method shows performance comparable with methods trained on real data. Additionally, on the four single instance datasets of the BOP dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, our method outperforms all other methods trained on the same synthetic data.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">The paper is structured as follows: We first review related papers in Sec. <a href="#S2" title="2 Related Work ‣ Bridging the Reality Gap for Pose Estimation Networks using Sensor-Based Domain Randomization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. In Sec. <a href="#S3" title="3 Method ‣ Bridging the Reality Gap for Pose Estimation Networks using Sensor-Based Domain Randomization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, our developed method is explained. In Sec. <a href="#S4" title="4 Evaluation ‣ Bridging the Reality Gap for Pose Estimation Networks using Sensor-Based Domain Randomization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, experiments to verify the method, and results are presented. Finally, in Sec. <a href="#S5" title="5 Conclusion ‣ Bridging the Reality Gap for Pose Estimation Networks using Sensor-Based Domain Randomization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, a conclusion is given to this paper, and further work is discussed.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Deep learning based methods have heavily dominated the performance in pose estimation for the last five years. Especially CNN-based models have shown very good performance. Several different approaches have been made to utilize CNN models for pose estimation.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">2D methods:</span>
In SSD-6D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, a network is trained to classify the appearance of an object in an image patch. By searching through the image at different scales and locations, the object can then be detected. A different approach is used in both BB-8 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> and another method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> where a YOLO-like <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> network is used to predict a set of sparse keypoints. In PVNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, the network instead locates keypoints by first segmenting the object and then letting all remaining pixels vote for keypoint locations. In PoseCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, the prediction is first made for the object center, after which a regression network determines the rotation. In CDPN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, the rotation and translation are also handled independently, where the translation is found by regression, and the rotation is found by determining keypoints and then applying PnP.
Similar to our method, the EPOS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> method uses an encoder-decoder network to predict both object segmentation and dense keypoint predictions. However, unlike our method, the network only runs in 2D images. The DPOD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> method also computes dense keypoint predictions in 2D and computes PnP, but also employs a final pose refinement.
Similar to other methods, CosyPose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> first uses an object detector to segment the image, after which a novel pose estimation based on EfficientNet-B3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> achieves state-of-the-art performance. In addition, CosyPose can then use candidate poses from several images to find a global pose refinement.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">3D methods: </span>
In DenseFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> initial segmentations are found in 2D, and the 2D features are then integrated with 3D features from PointNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> before a final PnP determines the pose. Our method also employs PointNet, but unlike DenseFusion our method can perform segmentation and keypoint voting independently of 2D data. More similar to our method is PointVoteNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, which uses a single PointNet network for pose estimation. However, unlike our method, PointVoteNet combines segmentation and keypoint voting into one output and does not utilize the Edge Feature from DGCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. Additionally, PointVoteNet is only trained on real data and does not employ a 2D segmentation. PVN3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> is a method that combines 2D CNN and point cloud DNN into a dense feature. Similar to our approach, keypoints are used for pose estimation. As opposed to our method, which votes for a single keypoint per point, each point votes for the position of nine keypoints. The method performs very well on the LM dataset, but does not generalize to the more challenging LMO dataset.</p>
</div>
<div id="S2.p4" class="ltx_para ltx_noindent">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text ltx_font_bold">Synthetic data:</span>
Of the above mentioned methods only SSD-6D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> and DPOD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> are trained purely on synthetic data. Data is created by combining random background images with renders. An isolated instance of the object is rendered, and this render is then overlaid on a random background image from the COCO dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. While this approach is simple and easy to integrate, it has certain shortcomings. As the rendered image is overlaid on a background image, light conditions and occlusions of the object will be arbitrary. Additionally, only 2D methods can be used to train on such data, as any resulting depth map would be nonsensical. For DPOD the performance gap is quite clear, as the method trained on real data achieves a performance of 95.2% recall, while the performance drops to 66.4% when trained on synthetic data, tested on the LM dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. For SSD-6D, the performance with synthetic data is higher at 79%, but still far from the mid-nineties of methods trained on real data. A method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> trained on synthetic depth data also exists. The objects are placed randomly in the scene, and camera positions are chosen according to the views in the dataset. The method applies domain randomization, but in contrast to our method, it is performed in 2D. The method does not perform well, and achieves a 46.8% recall on the LM dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.
For the BOP challenge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> synthetic data was created for each dataset using BlenderProc <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. In this approach, physical-based-rendering (PBR) is performed by dropping objects randomly in a scene, and randomizing camera pose, light conditions, and object properties. This allows for more realistic noise, as shadows, occlusion, and reflections are modeled, allowing for the training of 3D-based methods. Three methods, EPOS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, CDPN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> and CosyPose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> have been trained on this data and tested on the BOP challenge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. While our method is also trained on this data, we integrate both RGB and depth data by training on point clouds.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The goal of our method is to estimate the 6D pose of a set of known objects in a scene. The pose estimation process is often hindered by the fact that the objects are occluded, and the scenes contain high levels of clutter. This makes it challenging to construct meaningful features that can match the object in the scene to the model. When estimating a 6D pose, the object is moving in 3D space. It is, therefore, necessary to use 3D data to obtain precise pose estimates <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Methods using color based deep learning methods often employ depth data at the end-stage to refine the pose. However, by employing depth in the full pose estimation pipeline, the data can be integrated into the deep learning and, as we will show, produce more accurate pose estimates.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p"><span id="S3.p2.1.1" class="ltx_text ltx_font_bold">Pose Estimation:</span> On the basis of this, a method for pose estimation using deep learning in point clouds has been developed.
The point cloud consists of both color and depth information, along with surface normals.
The pose estimation is achieved by matching points in the point cloud to keypoints in the object CAD model. This is performed using a neural network based on a modified version of DGCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> explained in Sec. <a href="#S3.SS2" title="3.2 Network Structure ‣ 3 Method ‣ Bridging the Reality Gap for Pose Estimation Networks using Sensor-Based Domain Randomization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">The network structure is set to handle point clouds with 2048 points, as for part segmentation in DGCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, so the point cloud needs to be this size. This is achieved by subsampling a point sphere around a candidate point. The point sphere size is based on the object diagonal to only include point belonging to the object, but scaled to 120% as the candidate point is not necessarily in the object center.
If there are more than 2048 points in the point cloud, 2048 points are randomly selected. If less than 2048 points are present, the candidate point is rejected.
However, as the sphere radius is dependent on the object radius, smaller objects result in smaller point clouds. Therefore, to avoid rejecting these point clouds, objects with a diagonal under 120mm, allow point clouds with only 512 points, compared with the standard 2048 points.
The point cloud is given as input to the network, and the network predicts both the object’s presence, the background segmentation, and keypoint voting. An example of the network output is shown in Fig. <a href="#S1.F1.sf3" title="In Figure 1 ‣ 1 Introduction ‣ Bridging the Reality Gap for Pose Estimation Networks using Sensor-Based Domain Randomization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(c)</span></a> and Fig. <a href="#S1.F1.sf4" title="In Figure 1 ‣ 1 Introduction ‣ Bridging the Reality Gap for Pose Estimation Networks using Sensor-Based Domain Randomization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(d)</span></a>.
As the network is able to label whether the object is present in the point cloud, the object search can be performed entirely in 3D. However, this would be computationally infeasible as a large number of spheres would have to be sub-sampled and computed through the network. The first step in the method is, therefore, a candidate detector based on Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. To improve the robustness, 16 cluster centers spread over the mask are found as potential candidates. For each candidate point, point clouds are extracted, and the network computes the probability that the object is present. Expectedly, the 2D-based Mask R-CNN also returns a number of false positives, and the 3D network is able to filter out these, as shown in Fig. <a href="#S1.F1.sf2" title="In Figure 1 ‣ 1 Introduction ‣ Bridging the Reality Gap for Pose Estimation Networks using Sensor-Based Domain Randomization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(b)</span></a>. For the four point clouds with the highest probability, the pose estimation is performed using the background segmentation and keypoint matches. The four best is selected to increase the robustness.
RANSAC is then performed on these matches, and a coarse to fine ICP refines the position. Finally, using the CAD model, a depth image is created by rendering the object using the found pose. The generated depth image is then compared with the depth image of the test scene. The best pose for each object is thus selected based on this evaluation. The pose estimation inlier distance is set to 10mm, this value is used for both the RANSAC, the ICP, and the depth check. The coarse to fine ICP continues for three iterations down to 2.5mm distance, with 10 iterations for each level. The parameter values have been found empirically, and a further study is shown in Sec. <a href="#S4.SS4" title="4.4 Ablation Studies ‣ 4 Evaluation ‣ Bridging the Reality Gap for Pose Estimation Networks using Sensor-Based Domain Randomization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2011.08517/assets/x6.png" id="S3.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="110" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.7.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.8.2" class="ltx_text" style="font-size:90%;">The structure of the neural network. The network has three outputs (shown as circles). The output CLASS is described in Sec. 3.2. The output SEG and VOTE are described in Sec. 3.3. The SparseEdge feature is described in Sec. 3.4. The <span id="S3.F2.8.2.1" class="ltx_text ltx_font_bold">MaxPool</span> layer is used for the classification, while both the <span id="S3.F2.8.2.2" class="ltx_text ltx_font_bold">MaxPool</span> and <span id="S3.F2.8.2.3" class="ltx_text ltx_font_bold">Concat</span> layers are used for the segmentation and vote prediction. The input PointCloud is obtained from Mask R-CNN candidate detector, and the outputs are used by RANSAC to determine pose estimates. <span id="S3.F2.8.2.4" class="ltx_text ltx_font_bold">TN</span> is the transform net introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.</span></figcaption>
</figure>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p"><span id="S3.p4.1.1" class="ltx_text ltx_font_bold">Set-up procedure:</span> The first part of the set-up procedure is the creation of keypoints. The object CAD model is sub-sampled using a voxel-grid with a spacing of 25 mm, and the remaining points are selected as keypoints. If more than 100 keypoints are present, the voxel-grid spacing is continuously increased until no more than 100 points remain.
The training data used is synthetically rendered images from the BOP challenge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> generated using BlenderProc <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. The CAD model is projected into the scene, and points belonging to the object are found. The keypoints are also projected, and the nearest keypoint is found for each point.
Point clouds are extracted from this by choosing random points and determining the label based on whether the point belongs to the object. For each image, seven point clouds are extracted, with four positive labels and three negatives. To create hard negatives for the network, one of the negative labels is found by selecting a point with a distance between 20-40 mm to the object. For each object the full process continues until 40000 point clouds have been collected for training. The network training is described in Sec. <a href="#S3.SS6" title="3.6 Multi-Task Network Training ‣ 3 Method ‣ Bridging the Reality Gap for Pose Estimation Networks using Sensor-Based Domain Randomization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.6</span></a>, with the applied domain randomization described in Sec. <a href="#S3.SS5" title="3.5 Sensor-Based Domain Randomization ‣ 3 Method ‣ Bridging the Reality Gap for Pose Estimation Networks using Sensor-Based Domain Randomization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.5</span></a>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Candidate Detector</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">To speed up the detection process, Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> is used for an initial segmentation of the objects. The network is trained to predict an image mask of the visible surface of all objects in the scene, which we then use to get a number of candidate point clouds for the subsequent stages.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Instead of using a hard threshold for detected instances, we always return at least one top instance detection along with all other detections with a confidence above the standard threshold of 0.7. To train the network, the same synthetic data source is used, but now with image-specific randomizations. The images are randomly flipped horizontally and Gaussian blurring and noise are added with a standard deviation of, respectively 1.0 and 0.05. Additionally, hue and saturation shifts of 20 are added. Apart from this, the parameters are set according to the MASK R-CNN implementation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. It is initialized with weights trained on the COCO dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, and is trained for 25 epochs. However, as the TUDL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> dataset only contains three objects it is trained much faster, and 50 epochs are used instead.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Network Structure</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The network structure for our method is shown in Fig. <a href="#S3.F2" title="Figure 2 ‣ 3 Method ‣ Bridging the Reality Gap for Pose Estimation Networks using Sensor-Based Domain Randomization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. While the network shares similarities with DGCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, e.g. the size of each layer is the same, several differences exist. As opposed to DGCNN, which has a single output of either classification or segmentation, our network can output three different predictions simultaneously: point cloud classification, background segmentation and keypoint voting.
The networks ability to perform point cloud classification and background segmentation makes it less dependent on the candidate detector. Even if false positives are presented to the network, it can filter out wrong point clouds.
As the background segmentation and keypoint votes are split into two different tasks, the network is able to learn object structure independently of the keypoint voting. This makes it easier to train the network on symmetric objects where the actual keypoint voting is difficult.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Vote Threshold</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.5" class="ltx_p">Before the network output of background segmentation and keypoint voting can be used with the RANSAC algorithm, they need to be converted to actual matches between scene points and object keypoints.
The point cloud can be represented as a matrix <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="P" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">P</annotation></semantics></math>, consisting of <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mi id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><ci id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">n</annotation></semantics></math> points <math id="S3.SS3.p1.3.m3.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S3.SS3.p1.3.m3.1a"><mi id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><ci id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">p</annotation></semantics></math>. For each point <math id="S3.SS3.p1.4.m4.1" class="ltx_Math" alttext="p_{i}" display="inline"><semantics id="S3.SS3.p1.4.m4.1a"><msub id="S3.SS3.p1.4.m4.1.1" xref="S3.SS3.p1.4.m4.1.1.cmml"><mi id="S3.SS3.p1.4.m4.1.1.2" xref="S3.SS3.p1.4.m4.1.1.2.cmml">p</mi><mi id="S3.SS3.p1.4.m4.1.1.3" xref="S3.SS3.p1.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.1b"><apply id="S3.SS3.p1.4.m4.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.4.m4.1.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS3.p1.4.m4.1.1.2.cmml" xref="S3.SS3.p1.4.m4.1.1.2">𝑝</ci><ci id="S3.SS3.p1.4.m4.1.1.3.cmml" xref="S3.SS3.p1.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">p_{i}</annotation></semantics></math> in the point cloud, the network returns <math id="S3.SS3.p1.5.m5.1" class="ltx_Math" alttext="s(p_{i})" display="inline"><semantics id="S3.SS3.p1.5.m5.1a"><mrow id="S3.SS3.p1.5.m5.1.1" xref="S3.SS3.p1.5.m5.1.1.cmml"><mi id="S3.SS3.p1.5.m5.1.1.3" xref="S3.SS3.p1.5.m5.1.1.3.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.5.m5.1.1.2" xref="S3.SS3.p1.5.m5.1.1.2.cmml">​</mo><mrow id="S3.SS3.p1.5.m5.1.1.1.1" xref="S3.SS3.p1.5.m5.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS3.p1.5.m5.1.1.1.1.2" xref="S3.SS3.p1.5.m5.1.1.1.1.1.cmml">(</mo><msub id="S3.SS3.p1.5.m5.1.1.1.1.1" xref="S3.SS3.p1.5.m5.1.1.1.1.1.cmml"><mi id="S3.SS3.p1.5.m5.1.1.1.1.1.2" xref="S3.SS3.p1.5.m5.1.1.1.1.1.2.cmml">p</mi><mi id="S3.SS3.p1.5.m5.1.1.1.1.1.3" xref="S3.SS3.p1.5.m5.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS3.p1.5.m5.1.1.1.1.3" xref="S3.SS3.p1.5.m5.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.5.m5.1b"><apply id="S3.SS3.p1.5.m5.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1"><times id="S3.SS3.p1.5.m5.1.1.2.cmml" xref="S3.SS3.p1.5.m5.1.1.2"></times><ci id="S3.SS3.p1.5.m5.1.1.3.cmml" xref="S3.SS3.p1.5.m5.1.1.3">𝑠</ci><apply id="S3.SS3.p1.5.m5.1.1.1.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.5.m5.1.1.1.1.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p1.5.m5.1.1.1.1.1.2.cmml" xref="S3.SS3.p1.5.m5.1.1.1.1.1.2">𝑝</ci><ci id="S3.SS3.p1.5.m5.1.1.1.1.1.3.cmml" xref="S3.SS3.p1.5.m5.1.1.1.1.1.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.5.m5.1c">s(p_{i})</annotation></semantics></math>, representing the probability of belonging to the object vs. background. We use a lower threshold of 0.5 for classifying foreground objects.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.8" class="ltx_p">The network also returns the keypoint vote matrix <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mi id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><ci id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">V</annotation></semantics></math> of size <math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="n\times m," display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><mrow id="S3.SS3.p2.2.m2.1.1.1" xref="S3.SS3.p2.2.m2.1.1.1.1.cmml"><mrow id="S3.SS3.p2.2.m2.1.1.1.1" xref="S3.SS3.p2.2.m2.1.1.1.1.cmml"><mi id="S3.SS3.p2.2.m2.1.1.1.1.2" xref="S3.SS3.p2.2.m2.1.1.1.1.2.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p2.2.m2.1.1.1.1.1" xref="S3.SS3.p2.2.m2.1.1.1.1.1.cmml">×</mo><mi id="S3.SS3.p2.2.m2.1.1.1.1.3" xref="S3.SS3.p2.2.m2.1.1.1.1.3.cmml">m</mi></mrow><mo id="S3.SS3.p2.2.m2.1.1.1.2" xref="S3.SS3.p2.2.m2.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><apply id="S3.SS3.p2.2.m2.1.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1.1"><times id="S3.SS3.p2.2.m2.1.1.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1.1.1.1"></times><ci id="S3.SS3.p2.2.m2.1.1.1.1.2.cmml" xref="S3.SS3.p2.2.m2.1.1.1.1.2">𝑛</ci><ci id="S3.SS3.p2.2.m2.1.1.1.1.3.cmml" xref="S3.SS3.p2.2.m2.1.1.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">n\times m,</annotation></semantics></math> where <math id="S3.SS3.p2.3.m3.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S3.SS3.p2.3.m3.1a"><mi id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><ci id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">m</annotation></semantics></math> is the number of keypoints in the model. For each point we then have the vector of probabilities <math id="S3.SS3.p2.4.m4.1" class="ltx_Math" alttext="V(p_{i})" display="inline"><semantics id="S3.SS3.p2.4.m4.1a"><mrow id="S3.SS3.p2.4.m4.1.1" xref="S3.SS3.p2.4.m4.1.1.cmml"><mi id="S3.SS3.p2.4.m4.1.1.3" xref="S3.SS3.p2.4.m4.1.1.3.cmml">V</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.4.m4.1.1.2" xref="S3.SS3.p2.4.m4.1.1.2.cmml">​</mo><mrow id="S3.SS3.p2.4.m4.1.1.1.1" xref="S3.SS3.p2.4.m4.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS3.p2.4.m4.1.1.1.1.2" xref="S3.SS3.p2.4.m4.1.1.1.1.1.cmml">(</mo><msub id="S3.SS3.p2.4.m4.1.1.1.1.1" xref="S3.SS3.p2.4.m4.1.1.1.1.1.cmml"><mi id="S3.SS3.p2.4.m4.1.1.1.1.1.2" xref="S3.SS3.p2.4.m4.1.1.1.1.1.2.cmml">p</mi><mi id="S3.SS3.p2.4.m4.1.1.1.1.1.3" xref="S3.SS3.p2.4.m4.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS3.p2.4.m4.1.1.1.1.3" xref="S3.SS3.p2.4.m4.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.1b"><apply id="S3.SS3.p2.4.m4.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1"><times id="S3.SS3.p2.4.m4.1.1.2.cmml" xref="S3.SS3.p2.4.m4.1.1.2"></times><ci id="S3.SS3.p2.4.m4.1.1.3.cmml" xref="S3.SS3.p2.4.m4.1.1.3">𝑉</ci><apply id="S3.SS3.p2.4.m4.1.1.1.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.4.m4.1.1.1.1.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p2.4.m4.1.1.1.1.1.2.cmml" xref="S3.SS3.p2.4.m4.1.1.1.1.1.2">𝑝</ci><ci id="S3.SS3.p2.4.m4.1.1.1.1.1.3.cmml" xref="S3.SS3.p2.4.m4.1.1.1.1.1.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.1c">V(p_{i})</annotation></semantics></math>. The highest value in <math id="S3.SS3.p2.5.m5.1" class="ltx_Math" alttext="V(p_{i})" display="inline"><semantics id="S3.SS3.p2.5.m5.1a"><mrow id="S3.SS3.p2.5.m5.1.1" xref="S3.SS3.p2.5.m5.1.1.cmml"><mi id="S3.SS3.p2.5.m5.1.1.3" xref="S3.SS3.p2.5.m5.1.1.3.cmml">V</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.5.m5.1.1.2" xref="S3.SS3.p2.5.m5.1.1.2.cmml">​</mo><mrow id="S3.SS3.p2.5.m5.1.1.1.1" xref="S3.SS3.p2.5.m5.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS3.p2.5.m5.1.1.1.1.2" xref="S3.SS3.p2.5.m5.1.1.1.1.1.cmml">(</mo><msub id="S3.SS3.p2.5.m5.1.1.1.1.1" xref="S3.SS3.p2.5.m5.1.1.1.1.1.cmml"><mi id="S3.SS3.p2.5.m5.1.1.1.1.1.2" xref="S3.SS3.p2.5.m5.1.1.1.1.1.2.cmml">p</mi><mi id="S3.SS3.p2.5.m5.1.1.1.1.1.3" xref="S3.SS3.p2.5.m5.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS3.p2.5.m5.1.1.1.1.3" xref="S3.SS3.p2.5.m5.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.5.m5.1b"><apply id="S3.SS3.p2.5.m5.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1"><times id="S3.SS3.p2.5.m5.1.1.2.cmml" xref="S3.SS3.p2.5.m5.1.1.2"></times><ci id="S3.SS3.p2.5.m5.1.1.3.cmml" xref="S3.SS3.p2.5.m5.1.1.3">𝑉</ci><apply id="S3.SS3.p2.5.m5.1.1.1.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.5.m5.1.1.1.1.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p2.5.m5.1.1.1.1.1.2.cmml" xref="S3.SS3.p2.5.m5.1.1.1.1.1.2">𝑝</ci><ci id="S3.SS3.p2.5.m5.1.1.1.1.1.3.cmml" xref="S3.SS3.p2.5.m5.1.1.1.1.1.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m5.1c">V(p_{i})</annotation></semantics></math> is the keypoint vote which the point <math id="S3.SS3.p2.6.m6.1" class="ltx_Math" alttext="p_{i}" display="inline"><semantics id="S3.SS3.p2.6.m6.1a"><msub id="S3.SS3.p2.6.m6.1.1" xref="S3.SS3.p2.6.m6.1.1.cmml"><mi id="S3.SS3.p2.6.m6.1.1.2" xref="S3.SS3.p2.6.m6.1.1.2.cmml">p</mi><mi id="S3.SS3.p2.6.m6.1.1.3" xref="S3.SS3.p2.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.6.m6.1b"><apply id="S3.SS3.p2.6.m6.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.6.m6.1.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1">subscript</csymbol><ci id="S3.SS3.p2.6.m6.1.1.2.cmml" xref="S3.SS3.p2.6.m6.1.1.2">𝑝</ci><ci id="S3.SS3.p2.6.m6.1.1.3.cmml" xref="S3.SS3.p2.6.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.6.m6.1c">p_{i}</annotation></semantics></math> is most likely to belong to. However, the probability distribution cannot always be expected to be unimodal. In the case of objects which appear symmetric from certain views, a point is equally likely to belong to multiple keypoints <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. To account for this uncertainty in our model, a scene point is allowed to vote for multiple keypoints. The approach is shown in Eq. <a href="#S3.E1" title="In 3.3 Vote Threshold ‣ 3 Method ‣ Bridging the Reality Gap for Pose Estimation Networks using Sensor-Based Domain Randomization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. For each <math id="S3.SS3.p2.7.m7.2" class="ltx_Math" alttext="v_{j}(p_{i})\in V(p_{i})" display="inline"><semantics id="S3.SS3.p2.7.m7.2a"><mrow id="S3.SS3.p2.7.m7.2.2" xref="S3.SS3.p2.7.m7.2.2.cmml"><mrow id="S3.SS3.p2.7.m7.1.1.1" xref="S3.SS3.p2.7.m7.1.1.1.cmml"><msub id="S3.SS3.p2.7.m7.1.1.1.3" xref="S3.SS3.p2.7.m7.1.1.1.3.cmml"><mi id="S3.SS3.p2.7.m7.1.1.1.3.2" xref="S3.SS3.p2.7.m7.1.1.1.3.2.cmml">v</mi><mi id="S3.SS3.p2.7.m7.1.1.1.3.3" xref="S3.SS3.p2.7.m7.1.1.1.3.3.cmml">j</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS3.p2.7.m7.1.1.1.2" xref="S3.SS3.p2.7.m7.1.1.1.2.cmml">​</mo><mrow id="S3.SS3.p2.7.m7.1.1.1.1.1" xref="S3.SS3.p2.7.m7.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS3.p2.7.m7.1.1.1.1.1.2" xref="S3.SS3.p2.7.m7.1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS3.p2.7.m7.1.1.1.1.1.1" xref="S3.SS3.p2.7.m7.1.1.1.1.1.1.cmml"><mi id="S3.SS3.p2.7.m7.1.1.1.1.1.1.2" xref="S3.SS3.p2.7.m7.1.1.1.1.1.1.2.cmml">p</mi><mi id="S3.SS3.p2.7.m7.1.1.1.1.1.1.3" xref="S3.SS3.p2.7.m7.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS3.p2.7.m7.1.1.1.1.1.3" xref="S3.SS3.p2.7.m7.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS3.p2.7.m7.2.2.3" xref="S3.SS3.p2.7.m7.2.2.3.cmml">∈</mo><mrow id="S3.SS3.p2.7.m7.2.2.2" xref="S3.SS3.p2.7.m7.2.2.2.cmml"><mi id="S3.SS3.p2.7.m7.2.2.2.3" xref="S3.SS3.p2.7.m7.2.2.2.3.cmml">V</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.7.m7.2.2.2.2" xref="S3.SS3.p2.7.m7.2.2.2.2.cmml">​</mo><mrow id="S3.SS3.p2.7.m7.2.2.2.1.1" xref="S3.SS3.p2.7.m7.2.2.2.1.1.1.cmml"><mo stretchy="false" id="S3.SS3.p2.7.m7.2.2.2.1.1.2" xref="S3.SS3.p2.7.m7.2.2.2.1.1.1.cmml">(</mo><msub id="S3.SS3.p2.7.m7.2.2.2.1.1.1" xref="S3.SS3.p2.7.m7.2.2.2.1.1.1.cmml"><mi id="S3.SS3.p2.7.m7.2.2.2.1.1.1.2" xref="S3.SS3.p2.7.m7.2.2.2.1.1.1.2.cmml">p</mi><mi id="S3.SS3.p2.7.m7.2.2.2.1.1.1.3" xref="S3.SS3.p2.7.m7.2.2.2.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS3.p2.7.m7.2.2.2.1.1.3" xref="S3.SS3.p2.7.m7.2.2.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.7.m7.2b"><apply id="S3.SS3.p2.7.m7.2.2.cmml" xref="S3.SS3.p2.7.m7.2.2"><in id="S3.SS3.p2.7.m7.2.2.3.cmml" xref="S3.SS3.p2.7.m7.2.2.3"></in><apply id="S3.SS3.p2.7.m7.1.1.1.cmml" xref="S3.SS3.p2.7.m7.1.1.1"><times id="S3.SS3.p2.7.m7.1.1.1.2.cmml" xref="S3.SS3.p2.7.m7.1.1.1.2"></times><apply id="S3.SS3.p2.7.m7.1.1.1.3.cmml" xref="S3.SS3.p2.7.m7.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p2.7.m7.1.1.1.3.1.cmml" xref="S3.SS3.p2.7.m7.1.1.1.3">subscript</csymbol><ci id="S3.SS3.p2.7.m7.1.1.1.3.2.cmml" xref="S3.SS3.p2.7.m7.1.1.1.3.2">𝑣</ci><ci id="S3.SS3.p2.7.m7.1.1.1.3.3.cmml" xref="S3.SS3.p2.7.m7.1.1.1.3.3">𝑗</ci></apply><apply id="S3.SS3.p2.7.m7.1.1.1.1.1.1.cmml" xref="S3.SS3.p2.7.m7.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.7.m7.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p2.7.m7.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p2.7.m7.1.1.1.1.1.1.2.cmml" xref="S3.SS3.p2.7.m7.1.1.1.1.1.1.2">𝑝</ci><ci id="S3.SS3.p2.7.m7.1.1.1.1.1.1.3.cmml" xref="S3.SS3.p2.7.m7.1.1.1.1.1.1.3">𝑖</ci></apply></apply><apply id="S3.SS3.p2.7.m7.2.2.2.cmml" xref="S3.SS3.p2.7.m7.2.2.2"><times id="S3.SS3.p2.7.m7.2.2.2.2.cmml" xref="S3.SS3.p2.7.m7.2.2.2.2"></times><ci id="S3.SS3.p2.7.m7.2.2.2.3.cmml" xref="S3.SS3.p2.7.m7.2.2.2.3">𝑉</ci><apply id="S3.SS3.p2.7.m7.2.2.2.1.1.1.cmml" xref="S3.SS3.p2.7.m7.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.7.m7.2.2.2.1.1.1.1.cmml" xref="S3.SS3.p2.7.m7.2.2.2.1.1">subscript</csymbol><ci id="S3.SS3.p2.7.m7.2.2.2.1.1.1.2.cmml" xref="S3.SS3.p2.7.m7.2.2.2.1.1.1.2">𝑝</ci><ci id="S3.SS3.p2.7.m7.2.2.2.1.1.1.3.cmml" xref="S3.SS3.p2.7.m7.2.2.2.1.1.1.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.7.m7.2c">v_{j}(p_{i})\in V(p_{i})</annotation></semantics></math> a softmax is applied and if any vote is higher than the maximum with an applied weight <math id="S3.SS3.p2.8.m8.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S3.SS3.p2.8.m8.1a"><mi id="S3.SS3.p2.8.m8.1.1" xref="S3.SS3.p2.8.m8.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.8.m8.1b"><ci id="S3.SS3.p2.8.m8.1.1.cmml" xref="S3.SS3.p2.8.m8.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.8.m8.1c">\tau</annotation></semantics></math>, it is accepted:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.3" class="ltx_Math" alttext="v_{j}(p_{i})&gt;\tau\cdot\max_{k=1}^{m}(v_{k}(p_{i}))" display="block"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml"><msub id="S3.E1.m1.1.1.1.3" xref="S3.E1.m1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.3.2.cmml">v</mi><mi id="S3.E1.m1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.3.3.cmml">j</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.2.cmml">p</mi><mi id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.3.3.4" xref="S3.E1.m1.3.3.4.cmml">&gt;</mo><mrow id="S3.E1.m1.3.3.3" xref="S3.E1.m1.3.3.3.cmml"><mi id="S3.E1.m1.3.3.3.4" xref="S3.E1.m1.3.3.3.4.cmml">τ</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m1.3.3.3.3" xref="S3.E1.m1.3.3.3.3.cmml">⋅</mo><mrow id="S3.E1.m1.3.3.3.2.2" xref="S3.E1.m1.3.3.3.2.3.cmml"><munderover id="S3.E1.m1.2.2.2.1.1.1" xref="S3.E1.m1.2.2.2.1.1.1.cmml"><mi id="S3.E1.m1.2.2.2.1.1.1.2.2" xref="S3.E1.m1.2.2.2.1.1.1.2.2.cmml">max</mi><mrow id="S3.E1.m1.2.2.2.1.1.1.2.3" xref="S3.E1.m1.2.2.2.1.1.1.2.3.cmml"><mi id="S3.E1.m1.2.2.2.1.1.1.2.3.2" xref="S3.E1.m1.2.2.2.1.1.1.2.3.2.cmml">k</mi><mo id="S3.E1.m1.2.2.2.1.1.1.2.3.1" xref="S3.E1.m1.2.2.2.1.1.1.2.3.1.cmml">=</mo><mn id="S3.E1.m1.2.2.2.1.1.1.2.3.3" xref="S3.E1.m1.2.2.2.1.1.1.2.3.3.cmml">1</mn></mrow><mi id="S3.E1.m1.2.2.2.1.1.1.3" xref="S3.E1.m1.2.2.2.1.1.1.3.cmml">m</mi></munderover><mo id="S3.E1.m1.3.3.3.2.2a" xref="S3.E1.m1.3.3.3.2.3.cmml">⁡</mo><mrow id="S3.E1.m1.3.3.3.2.2.2" xref="S3.E1.m1.3.3.3.2.3.cmml"><mo stretchy="false" id="S3.E1.m1.3.3.3.2.2.2.2" xref="S3.E1.m1.3.3.3.2.3.cmml">(</mo><mrow id="S3.E1.m1.3.3.3.2.2.2.1" xref="S3.E1.m1.3.3.3.2.2.2.1.cmml"><msub id="S3.E1.m1.3.3.3.2.2.2.1.3" xref="S3.E1.m1.3.3.3.2.2.2.1.3.cmml"><mi id="S3.E1.m1.3.3.3.2.2.2.1.3.2" xref="S3.E1.m1.3.3.3.2.2.2.1.3.2.cmml">v</mi><mi id="S3.E1.m1.3.3.3.2.2.2.1.3.3" xref="S3.E1.m1.3.3.3.2.2.2.1.3.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.3.2.2.2.1.2" xref="S3.E1.m1.3.3.3.2.2.2.1.2.cmml">​</mo><mrow id="S3.E1.m1.3.3.3.2.2.2.1.1.1" xref="S3.E1.m1.3.3.3.2.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.3.3.3.2.2.2.1.1.1.2" xref="S3.E1.m1.3.3.3.2.2.2.1.1.1.1.cmml">(</mo><msub id="S3.E1.m1.3.3.3.2.2.2.1.1.1.1" xref="S3.E1.m1.3.3.3.2.2.2.1.1.1.1.cmml"><mi id="S3.E1.m1.3.3.3.2.2.2.1.1.1.1.2" xref="S3.E1.m1.3.3.3.2.2.2.1.1.1.1.2.cmml">p</mi><mi id="S3.E1.m1.3.3.3.2.2.2.1.1.1.1.3" xref="S3.E1.m1.3.3.3.2.2.2.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.E1.m1.3.3.3.2.2.2.1.1.1.3" xref="S3.E1.m1.3.3.3.2.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E1.m1.3.3.3.2.2.2.3" xref="S3.E1.m1.3.3.3.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.3b"><apply id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3"><gt id="S3.E1.m1.3.3.4.cmml" xref="S3.E1.m1.3.3.4"></gt><apply id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><times id="S3.E1.m1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.2"></times><apply id="S3.E1.m1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.3.2">𝑣</ci><ci id="S3.E1.m1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.3.3">𝑗</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2">𝑝</ci><ci id="S3.E1.m1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3">𝑖</ci></apply></apply><apply id="S3.E1.m1.3.3.3.cmml" xref="S3.E1.m1.3.3.3"><ci id="S3.E1.m1.3.3.3.3.cmml" xref="S3.E1.m1.3.3.3.3">⋅</ci><ci id="S3.E1.m1.3.3.3.4.cmml" xref="S3.E1.m1.3.3.3.4">𝜏</ci><apply id="S3.E1.m1.3.3.3.2.3.cmml" xref="S3.E1.m1.3.3.3.2.2"><apply id="S3.E1.m1.2.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.1.1.1.1.cmml" xref="S3.E1.m1.2.2.2.1.1.1">superscript</csymbol><apply id="S3.E1.m1.2.2.2.1.1.1.2.cmml" xref="S3.E1.m1.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.1.1.1.2.1.cmml" xref="S3.E1.m1.2.2.2.1.1.1">subscript</csymbol><max id="S3.E1.m1.2.2.2.1.1.1.2.2.cmml" xref="S3.E1.m1.2.2.2.1.1.1.2.2"></max><apply id="S3.E1.m1.2.2.2.1.1.1.2.3.cmml" xref="S3.E1.m1.2.2.2.1.1.1.2.3"><eq id="S3.E1.m1.2.2.2.1.1.1.2.3.1.cmml" xref="S3.E1.m1.2.2.2.1.1.1.2.3.1"></eq><ci id="S3.E1.m1.2.2.2.1.1.1.2.3.2.cmml" xref="S3.E1.m1.2.2.2.1.1.1.2.3.2">𝑘</ci><cn type="integer" id="S3.E1.m1.2.2.2.1.1.1.2.3.3.cmml" xref="S3.E1.m1.2.2.2.1.1.1.2.3.3">1</cn></apply></apply><ci id="S3.E1.m1.2.2.2.1.1.1.3.cmml" xref="S3.E1.m1.2.2.2.1.1.1.3">𝑚</ci></apply><apply id="S3.E1.m1.3.3.3.2.2.2.1.cmml" xref="S3.E1.m1.3.3.3.2.2.2.1"><times id="S3.E1.m1.3.3.3.2.2.2.1.2.cmml" xref="S3.E1.m1.3.3.3.2.2.2.1.2"></times><apply id="S3.E1.m1.3.3.3.2.2.2.1.3.cmml" xref="S3.E1.m1.3.3.3.2.2.2.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.3.2.2.2.1.3.1.cmml" xref="S3.E1.m1.3.3.3.2.2.2.1.3">subscript</csymbol><ci id="S3.E1.m1.3.3.3.2.2.2.1.3.2.cmml" xref="S3.E1.m1.3.3.3.2.2.2.1.3.2">𝑣</ci><ci id="S3.E1.m1.3.3.3.2.2.2.1.3.3.cmml" xref="S3.E1.m1.3.3.3.2.2.2.1.3.3">𝑘</ci></apply><apply id="S3.E1.m1.3.3.3.2.2.2.1.1.1.1.cmml" xref="S3.E1.m1.3.3.3.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.3.2.2.2.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.3.2.2.2.1.1.1">subscript</csymbol><ci id="S3.E1.m1.3.3.3.2.2.2.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.3.2.2.2.1.1.1.1.2">𝑝</ci><ci id="S3.E1.m1.3.3.3.2.2.2.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.3.2.2.2.1.1.1.1.3">𝑖</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.3c">v_{j}(p_{i})&gt;\tau\cdot\max_{k=1}^{m}(v_{k}(p_{i}))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p2.9" class="ltx_p">This allows for similar keypoints to still count in the voting process, relying on RANSAC to filter out erroneous votes. In all experiments, we use <math id="S3.SS3.p2.9.m1.1" class="ltx_Math" alttext="\tau=0.95" display="inline"><semantics id="S3.SS3.p2.9.m1.1a"><mrow id="S3.SS3.p2.9.m1.1.1" xref="S3.SS3.p2.9.m1.1.1.cmml"><mi id="S3.SS3.p2.9.m1.1.1.2" xref="S3.SS3.p2.9.m1.1.1.2.cmml">τ</mi><mo id="S3.SS3.p2.9.m1.1.1.1" xref="S3.SS3.p2.9.m1.1.1.1.cmml">=</mo><mn id="S3.SS3.p2.9.m1.1.1.3" xref="S3.SS3.p2.9.m1.1.1.3.cmml">0.95</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.9.m1.1b"><apply id="S3.SS3.p2.9.m1.1.1.cmml" xref="S3.SS3.p2.9.m1.1.1"><eq id="S3.SS3.p2.9.m1.1.1.1.cmml" xref="S3.SS3.p2.9.m1.1.1.1"></eq><ci id="S3.SS3.p2.9.m1.1.1.2.cmml" xref="S3.SS3.p2.9.m1.1.1.2">𝜏</ci><cn type="float" id="S3.SS3.p2.9.m1.1.1.3.cmml" xref="S3.SS3.p2.9.m1.1.1.3">0.95</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.9.m1.1c">\tau=0.95</annotation></semantics></math>.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>SparseEdge Feature</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.3" class="ltx_p">The edge feature introduced in DGCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> allows PointNet-like networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> to combine point-wise and local edge information through all layers. By using this edge feature, DGCNN significantly increased the performance compared to PointNet. The edge feature consists of two components, a k-NN search locating the nearest points or features, followed by a difference operator between the center point and its neighbors. The end result is a <math id="S3.SS4.p1.1.m1.1" class="ltx_Math" alttext="k\times i" display="inline"><semantics id="S3.SS4.p1.1.m1.1a"><mrow id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml"><mi id="S3.SS4.p1.1.m1.1.1.2" xref="S3.SS4.p1.1.m1.1.1.2.cmml">k</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p1.1.m1.1.1.1" xref="S3.SS4.p1.1.m1.1.1.1.cmml">×</mo><mi id="S3.SS4.p1.1.m1.1.1.3" xref="S3.SS4.p1.1.m1.1.1.3.cmml">i</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><apply id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1"><times id="S3.SS4.p1.1.m1.1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1.1"></times><ci id="S3.SS4.p1.1.m1.1.1.2.cmml" xref="S3.SS4.p1.1.m1.1.1.2">𝑘</ci><ci id="S3.SS4.p1.1.m1.1.1.3.cmml" xref="S3.SS4.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">k\times i</annotation></semantics></math> feature where <math id="S3.SS4.p1.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS4.p1.2.m2.1a"><mi id="S3.SS4.p1.2.m2.1.1" xref="S3.SS4.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.2.m2.1b"><ci id="S3.SS4.p1.2.m2.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.2.m2.1c">k</annotation></semantics></math> is the number of neighbors and <math id="S3.SS4.p1.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS4.p1.3.m3.1a"><mi id="S3.SS4.p1.3.m3.1.1" xref="S3.SS4.p1.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.3.m3.1b"><ci id="S3.SS4.p1.3.m3.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.3.m3.1c">i</annotation></semantics></math> is the dimension of the point representation in a layer. As the data structure from real scans is noisy, it is desirable to have a larger search space for neighbors. An increased search space will allow the method to learn a broader range of features, not only relying on the closest neighbour points. However, this increased representation capacity will also increase the computation time of the network.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.5" class="ltx_p">To overcome this, we introduce the SparseEdge feature. The SparseEdge feature is made to maintain the performance of the edge feature, but with less run-time. Instead of selecting the <math id="S3.SS4.p2.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS4.p2.1.m1.1a"><mi id="S3.SS4.p2.1.m1.1.1" xref="S3.SS4.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.1.m1.1b"><ci id="S3.SS4.p2.1.m1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.1.m1.1c">k</annotation></semantics></math> nearest neighbors, a search is performed with <math id="S3.SS4.p2.2.m2.1" class="ltx_Math" alttext="3k" display="inline"><semantics id="S3.SS4.p2.2.m2.1a"><mrow id="S3.SS4.p2.2.m2.1.1" xref="S3.SS4.p2.2.m2.1.1.cmml"><mn id="S3.SS4.p2.2.m2.1.1.2" xref="S3.SS4.p2.2.m2.1.1.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S3.SS4.p2.2.m2.1.1.1" xref="S3.SS4.p2.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS4.p2.2.m2.1.1.3" xref="S3.SS4.p2.2.m2.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.2.m2.1b"><apply id="S3.SS4.p2.2.m2.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1"><times id="S3.SS4.p2.2.m2.1.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1.1"></times><cn type="integer" id="S3.SS4.p2.2.m2.1.1.2.cmml" xref="S3.SS4.p2.2.m2.1.1.2">3</cn><ci id="S3.SS4.p2.2.m2.1.1.3.cmml" xref="S3.SS4.p2.2.m2.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.2.m2.1c">3k</annotation></semantics></math> neighbors, and from these, a subset of <math id="S3.SS4.p2.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS4.p2.3.m3.1a"><mi id="S3.SS4.p2.3.m3.1.1" xref="S3.SS4.p2.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.3.m3.1b"><ci id="S3.SS4.p2.3.m3.1.1.cmml" xref="S3.SS4.p2.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.3.m3.1c">k</annotation></semantics></math> is then selected. The method is shown in Fig. <a href="#S3.F3" title="Figure 3 ‣ 3.4 SparseEdge Feature ‣ 3 Method ‣ Bridging the Reality Gap for Pose Estimation Networks using Sensor-Based Domain Randomization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. At training time the <math id="S3.SS4.p2.4.m4.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS4.p2.4.m4.1a"><mi id="S3.SS4.p2.4.m4.1.1" xref="S3.SS4.p2.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.4.m4.1b"><ci id="S3.SS4.p2.4.m4.1.1.cmml" xref="S3.SS4.p2.4.m4.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.4.m4.1c">k</annotation></semantics></math> neighbors are selected randomly while at test time the feature is set to select every third in the list of neighbors, sorted by the distance to the center point. The random selection at training time ensures that the network does not pick up specific features. In our experiments, <math id="S3.SS4.p2.5.m5.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS4.p2.5.m5.1a"><mi id="S3.SS4.p2.5.m5.1.1" xref="S3.SS4.p2.5.m5.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.5.m5.1b"><ci id="S3.SS4.p2.5.m5.1.1.cmml" xref="S3.SS4.p2.5.m5.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.5.m5.1c">k</annotation></semantics></math> is set to 10. The effectiveness of the SparseEdge is validated in Sec. <a href="#S4.SS4" title="4.4 Ablation Studies ‣ 4 Evaluation ‣ Bridging the Reality Gap for Pose Estimation Networks using Sensor-Based Domain Randomization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2011.08517/assets/x7.png" id="S3.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="369" height="214" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.5.2.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.3.2.1" class="ltx_text" style="font-size:90%;">Example of the SparseEdge feature selection with <math id="S3.F3.3.2.1.m1.1" class="ltx_Math" alttext="k=10" display="inline"><semantics id="S3.F3.3.2.1.m1.1b"><mrow id="S3.F3.3.2.1.m1.1.1" xref="S3.F3.3.2.1.m1.1.1.cmml"><mi id="S3.F3.3.2.1.m1.1.1.2" xref="S3.F3.3.2.1.m1.1.1.2.cmml">k</mi><mo id="S3.F3.3.2.1.m1.1.1.1" xref="S3.F3.3.2.1.m1.1.1.1.cmml">=</mo><mn id="S3.F3.3.2.1.m1.1.1.3" xref="S3.F3.3.2.1.m1.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.F3.3.2.1.m1.1c"><apply id="S3.F3.3.2.1.m1.1.1.cmml" xref="S3.F3.3.2.1.m1.1.1"><eq id="S3.F3.3.2.1.m1.1.1.1.cmml" xref="S3.F3.3.2.1.m1.1.1.1"></eq><ci id="S3.F3.3.2.1.m1.1.1.2.cmml" xref="S3.F3.3.2.1.m1.1.1.2">𝑘</ci><cn type="integer" id="S3.F3.3.2.1.m1.1.1.3.cmml" xref="S3.F3.3.2.1.m1.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.3.2.1.m1.1d">k=10</annotation></semantics></math>. The plus sign shows the center point of the k-NN. While, the standard approach select the points within the two dashes, our method select the points with white centers.</span></figcaption>
</figure>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Sensor-Based Domain Randomization</h3>

<figure id="S3.F4" class="ltx_figure"><img src="/html/2011.08517/assets/gfx/dr_duck.png" id="S3.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="454" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.3.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.4.2" class="ltx_text" style="font-size:90%;">Example of the data sampling and domain randomization employed in this paper. From top left: CAD model of the object, object in the rendered scene, sampled point cloud, three visualizations of domain randomization applied to the point cloud.</span></figcaption>
</figure>
<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.2" class="ltx_p">The synthetic training data is generated using BlenderPROC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. As the training data is obtained by synthetic rendering, a domain gap will exist between the training and test data. During rendering, some of the standard approaches for modeling realistic variations are included. This includes placing the objects in random positions and using random camera positions. Additionally, different types of surface material and light positions are added to the simulation, but only to the RGB part. The only disturbances on the depth part are occlusions and clutter in the simulations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. From the given simulated RGB-D data, we reconstruct point clouds with XYZ positions, RGB values, and estimated surface normals. The XYZ values are defined in mm, while the remaining are normalized to <math id="S3.SS5.p1.1.m1.2" class="ltx_Math" alttext="[0,1]" display="inline"><semantics id="S3.SS5.p1.1.m1.2a"><mrow id="S3.SS5.p1.1.m1.2.3.2" xref="S3.SS5.p1.1.m1.2.3.1.cmml"><mo stretchy="false" id="S3.SS5.p1.1.m1.2.3.2.1" xref="S3.SS5.p1.1.m1.2.3.1.cmml">[</mo><mn id="S3.SS5.p1.1.m1.1.1" xref="S3.SS5.p1.1.m1.1.1.cmml">0</mn><mo id="S3.SS5.p1.1.m1.2.3.2.2" xref="S3.SS5.p1.1.m1.2.3.1.cmml">,</mo><mn id="S3.SS5.p1.1.m1.2.2" xref="S3.SS5.p1.1.m1.2.2.cmml">1</mn><mo stretchy="false" id="S3.SS5.p1.1.m1.2.3.2.3" xref="S3.SS5.p1.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.1.m1.2b"><interval closure="closed" id="S3.SS5.p1.1.m1.2.3.1.cmml" xref="S3.SS5.p1.1.m1.2.3.2"><cn type="integer" id="S3.SS5.p1.1.m1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1">0</cn><cn type="integer" id="S3.SS5.p1.1.m1.2.2.cmml" xref="S3.SS5.p1.1.m1.2.2">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.1.m1.2c">[0,1]</annotation></semantics></math>. The standard approach for data augmentation in point clouds is a Gaussian noise with <math id="S3.SS5.p1.2.m2.1" class="ltx_Math" alttext="\sigma=0.01" display="inline"><semantics id="S3.SS5.p1.2.m2.1a"><mrow id="S3.SS5.p1.2.m2.1.1" xref="S3.SS5.p1.2.m2.1.1.cmml"><mi id="S3.SS5.p1.2.m2.1.1.2" xref="S3.SS5.p1.2.m2.1.1.2.cmml">σ</mi><mo id="S3.SS5.p1.2.m2.1.1.1" xref="S3.SS5.p1.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS5.p1.2.m2.1.1.3" xref="S3.SS5.p1.2.m2.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.2.m2.1b"><apply id="S3.SS5.p1.2.m2.1.1.cmml" xref="S3.SS5.p1.2.m2.1.1"><eq id="S3.SS5.p1.2.m2.1.1.1.cmml" xref="S3.SS5.p1.2.m2.1.1.1"></eq><ci id="S3.SS5.p1.2.m2.1.1.2.cmml" xref="S3.SS5.p1.2.m2.1.1.2">𝜎</ci><cn type="float" id="S3.SS5.p1.2.m2.1.1.3.cmml" xref="S3.SS5.p1.2.m2.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.2.m2.1c">\sigma=0.01</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. As the general approach is to normalize the point cloud size, the standard for the XYZ deviation amounts to 1% of the point cloud size.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.1" class="ltx_p">For this paper the focus is on depth sensors like the Kinect with a resolution of 640x480 px. The sensor model is based on the Kinect sensor <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. Extensive analyses of the error model of the Kinect sensor have been performed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. Modelling realistic noise is very difficult as the surface properties are unknown, and non-Lambertian reflections can cause highly non-Gaussian noise. Additionally, we face the problem that the provided CAD models do not perfectly model the 3D structure and surface texture of the objects. The goal is, therefore, not to model the noise correctly, but to model noise that gives the same error for the pose estimation. A model trained with this noise will then generalize better to the real test data.</p>
</div>
<div id="S3.SS5.p3" class="ltx_para">
<p id="S3.SS5.p3.1" class="ltx_p">From the noise model one noteworthy aspect is that the error for each pixel is Gaussian and independent of its neighbors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. Another important aspect is that the error depends on the angle and distance to the camera <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. The angular error is mostly insignificant when lower than <math id="S3.SS5.p3.1.m1.1" class="ltx_Math" alttext="60^{\circ}" display="inline"><semantics id="S3.SS5.p3.1.m1.1a"><msup id="S3.SS5.p3.1.m1.1.1" xref="S3.SS5.p3.1.m1.1.1.cmml"><mn id="S3.SS5.p3.1.m1.1.1.2" xref="S3.SS5.p3.1.m1.1.1.2.cmml">60</mn><mo id="S3.SS5.p3.1.m1.1.1.3" xref="S3.SS5.p3.1.m1.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.1.m1.1b"><apply id="S3.SS5.p3.1.m1.1.1.cmml" xref="S3.SS5.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS5.p3.1.m1.1.1.1.cmml" xref="S3.SS5.p3.1.m1.1.1">superscript</csymbol><cn type="integer" id="S3.SS5.p3.1.m1.1.1.2.cmml" xref="S3.SS5.p3.1.m1.1.1.2">60</cn><compose id="S3.SS5.p3.1.m1.1.1.3.cmml" xref="S3.SS5.p3.1.m1.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.1.m1.1c">60^{\circ}</annotation></semantics></math> and then drastically increases. The angular error is, therefore, regarded as a point dropout, and is omitted in the noise model. The noise level can, therefore, be described as Eq. <a href="#S3.E2" title="In 3.5 Sensor-Based Domain Randomization ‣ 3 Method ‣ Bridging the Reality Gap for Pose Estimation Networks using Sensor-Based Domain Randomization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, where the constants are derived empirically.</p>
</div>
<div id="S3.SS5.p4" class="ltx_para">
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.2" class="ltx_Math" alttext="\sigma_{z}(z)=0.0012+0.0019(z-0.4)^{2}" display="block"><semantics id="S3.E2.m1.2a"><mrow id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml"><mrow id="S3.E2.m1.2.2.3" xref="S3.E2.m1.2.2.3.cmml"><msub id="S3.E2.m1.2.2.3.2" xref="S3.E2.m1.2.2.3.2.cmml"><mi id="S3.E2.m1.2.2.3.2.2" xref="S3.E2.m1.2.2.3.2.2.cmml">σ</mi><mi id="S3.E2.m1.2.2.3.2.3" xref="S3.E2.m1.2.2.3.2.3.cmml">z</mi></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.3.1" xref="S3.E2.m1.2.2.3.1.cmml">​</mo><mrow id="S3.E2.m1.2.2.3.3.2" xref="S3.E2.m1.2.2.3.cmml"><mo stretchy="false" id="S3.E2.m1.2.2.3.3.2.1" xref="S3.E2.m1.2.2.3.cmml">(</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">z</mi><mo stretchy="false" id="S3.E2.m1.2.2.3.3.2.2" xref="S3.E2.m1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.2.2.2" xref="S3.E2.m1.2.2.2.cmml">=</mo><mrow id="S3.E2.m1.2.2.1" xref="S3.E2.m1.2.2.1.cmml"><mn id="S3.E2.m1.2.2.1.3" xref="S3.E2.m1.2.2.1.3.cmml">0.0012</mn><mo id="S3.E2.m1.2.2.1.2" xref="S3.E2.m1.2.2.1.2.cmml">+</mo><mrow id="S3.E2.m1.2.2.1.1" xref="S3.E2.m1.2.2.1.1.cmml"><mn id="S3.E2.m1.2.2.1.1.3" xref="S3.E2.m1.2.2.1.1.3.cmml">0.0019</mn><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.2" xref="S3.E2.m1.2.2.1.1.2.cmml">​</mo><msup id="S3.E2.m1.2.2.1.1.1" xref="S3.E2.m1.2.2.1.1.1.cmml"><mrow id="S3.E2.m1.2.2.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.2.2.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.cmml">z</mi><mo id="S3.E2.m1.2.2.1.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml">−</mo><mn id="S3.E2.m1.2.2.1.1.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.3.cmml">0.4</mn></mrow><mo stretchy="false" id="S3.E2.m1.2.2.1.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.cmml">)</mo></mrow><mn id="S3.E2.m1.2.2.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.3.cmml">2</mn></msup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.2b"><apply id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2"><eq id="S3.E2.m1.2.2.2.cmml" xref="S3.E2.m1.2.2.2"></eq><apply id="S3.E2.m1.2.2.3.cmml" xref="S3.E2.m1.2.2.3"><times id="S3.E2.m1.2.2.3.1.cmml" xref="S3.E2.m1.2.2.3.1"></times><apply id="S3.E2.m1.2.2.3.2.cmml" xref="S3.E2.m1.2.2.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.3.2.1.cmml" xref="S3.E2.m1.2.2.3.2">subscript</csymbol><ci id="S3.E2.m1.2.2.3.2.2.cmml" xref="S3.E2.m1.2.2.3.2.2">𝜎</ci><ci id="S3.E2.m1.2.2.3.2.3.cmml" xref="S3.E2.m1.2.2.3.2.3">𝑧</ci></apply><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">𝑧</ci></apply><apply id="S3.E2.m1.2.2.1.cmml" xref="S3.E2.m1.2.2.1"><plus id="S3.E2.m1.2.2.1.2.cmml" xref="S3.E2.m1.2.2.1.2"></plus><cn type="float" id="S3.E2.m1.2.2.1.3.cmml" xref="S3.E2.m1.2.2.1.3">0.0012</cn><apply id="S3.E2.m1.2.2.1.1.cmml" xref="S3.E2.m1.2.2.1.1"><times id="S3.E2.m1.2.2.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.2"></times><cn type="float" id="S3.E2.m1.2.2.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.3">0.0019</cn><apply id="S3.E2.m1.2.2.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1">superscript</csymbol><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1"><minus id="S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1"></minus><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2">𝑧</ci><cn type="float" id="S3.E2.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.3">0.4</cn></apply><cn type="integer" id="S3.E2.m1.2.2.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.2c">\sigma_{z}(z)=0.0012+0.0019(z-0.4)^{2}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS5.p5" class="ltx_para">
<p id="S3.SS5.p5.2" class="ltx_p">The distance to the objects in the datasets is between 0.3 and 2.0 meters. From Eq. <a href="#S3.E2" title="In 3.5 Sensor-Based Domain Randomization ‣ 3 Method ‣ Bridging the Reality Gap for Pose Estimation Networks using Sensor-Based Domain Randomization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> this gives noise levels of 1.5 mm to 6 mm. The selected <math id="S3.SS5.p5.1.m1.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S3.SS5.p5.1.m1.1a"><mi id="S3.SS5.p5.1.m1.1.1" xref="S3.SS5.p5.1.m1.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p5.1.m1.1b"><ci id="S3.SS5.p5.1.m1.1.1.cmml" xref="S3.SS5.p5.1.m1.1.1">𝑧</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p5.1.m1.1c">z</annotation></semantics></math> distance is chosen to be 1.45 meters as this is the average maximum distance of the five tested datasets in this paper. Given <math id="S3.SS5.p5.2.m2.1" class="ltx_Math" alttext="z=1.45" display="inline"><semantics id="S3.SS5.p5.2.m2.1a"><mrow id="S3.SS5.p5.2.m2.1.1" xref="S3.SS5.p5.2.m2.1.1.cmml"><mi id="S3.SS5.p5.2.m2.1.1.2" xref="S3.SS5.p5.2.m2.1.1.2.cmml">z</mi><mo id="S3.SS5.p5.2.m2.1.1.1" xref="S3.SS5.p5.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS5.p5.2.m2.1.1.3" xref="S3.SS5.p5.2.m2.1.1.3.cmml">1.45</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p5.2.m2.1b"><apply id="S3.SS5.p5.2.m2.1.1.cmml" xref="S3.SS5.p5.2.m2.1.1"><eq id="S3.SS5.p5.2.m2.1.1.1.cmml" xref="S3.SS5.p5.2.m2.1.1.1"></eq><ci id="S3.SS5.p5.2.m2.1.1.2.cmml" xref="S3.SS5.p5.2.m2.1.1.2">𝑧</ci><cn type="float" id="S3.SS5.p5.2.m2.1.1.3.cmml" xref="S3.SS5.p5.2.m2.1.1.3">1.45</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p5.2.m2.1c">z=1.45</annotation></semantics></math> the returned noise level from the formula is approximately 3 mm, which is added as Gaussian noise to the XYZ part of the point cloud.</p>
</div>
<div id="S3.SS5.p6" class="ltx_para">
<p id="S3.SS5.p6.3" class="ltx_p">Additionally, a zero-centered Gaussian noise with a <math id="S3.SS5.p6.1.m1.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S3.SS5.p6.1.m1.1a"><mi id="S3.SS5.p6.1.m1.1.1" xref="S3.SS5.p6.1.m1.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p6.1.m1.1b"><ci id="S3.SS5.p6.1.m1.1.1.cmml" xref="S3.SS5.p6.1.m1.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p6.1.m1.1c">\sigma</annotation></semantics></math> of 0.06 is added randomly to the color values and the normal vectors. To handle overall color differences in the CAD model texture, all RGB values in the point cloud are also shifted together with a <math id="S3.SS5.p6.2.m2.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S3.SS5.p6.2.m2.1a"><mi id="S3.SS5.p6.2.m2.1.1" xref="S3.SS5.p6.2.m2.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p6.2.m2.1b"><ci id="S3.SS5.p6.2.m2.1.1.cmml" xref="S3.SS5.p6.2.m2.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p6.2.m2.1c">\sigma</annotation></semantics></math> of 0.03. To increase generalization, random rotations are applied to the point clouds. These rotations are limited to <math id="S3.SS5.p6.3.m3.1" class="ltx_Math" alttext="15^{\circ}" display="inline"><semantics id="S3.SS5.p6.3.m3.1a"><msup id="S3.SS5.p6.3.m3.1.1" xref="S3.SS5.p6.3.m3.1.1.cmml"><mn id="S3.SS5.p6.3.m3.1.1.2" xref="S3.SS5.p6.3.m3.1.1.2.cmml">15</mn><mo id="S3.SS5.p6.3.m3.1.1.3" xref="S3.SS5.p6.3.m3.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS5.p6.3.m3.1b"><apply id="S3.SS5.p6.3.m3.1.1.cmml" xref="S3.SS5.p6.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS5.p6.3.m3.1.1.1.cmml" xref="S3.SS5.p6.3.m3.1.1">superscript</csymbol><cn type="integer" id="S3.SS5.p6.3.m3.1.1.2.cmml" xref="S3.SS5.p6.3.m3.1.1.2">15</cn><compose id="S3.SS5.p6.3.m3.1.1.3.cmml" xref="S3.SS5.p6.3.m3.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p6.3.m3.1c">15^{\circ}</annotation></semantics></math> so the object rotations remain towards the camera as in the real test images. As the real test background is unknown, it is desirable also to learn the object structure independently of any background. To enable this, half of point clouds with the object present have all background points removed.</p>
</div>
<div id="S3.SS5.p7" class="ltx_para">
<p id="S3.SS5.p7.1" class="ltx_p">The process of sampling the training data and applying the domain randomization is shown in Fig. <a href="#S3.F4" title="Figure 4 ‣ 3.5 Sensor-Based Domain Randomization ‣ 3 Method ‣ Bridging the Reality Gap for Pose Estimation Networks using Sensor-Based Domain Randomization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. The effect of the domain randomization is validated in Sec. <a href="#S4.SS4" title="4.4 Ablation Studies ‣ 4 Evaluation ‣ Bridging the Reality Gap for Pose Estimation Networks using Sensor-Based Domain Randomization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>.</p>
</div>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Multi-Task Network Training</h3>

<div id="S3.SS6.p1" class="ltx_para">
<p id="S3.SS6.p1.5" class="ltx_p">As three different outputs are trained simultaneously, a weighing of the loss terms is required. The split is set according to the complexity of the different tasks, with the weights set at <math id="S3.SS6.p1.1.m1.1" class="ltx_Math" alttext="w_{l}=0.12" display="inline"><semantics id="S3.SS6.p1.1.m1.1a"><mrow id="S3.SS6.p1.1.m1.1.1" xref="S3.SS6.p1.1.m1.1.1.cmml"><msub id="S3.SS6.p1.1.m1.1.1.2" xref="S3.SS6.p1.1.m1.1.1.2.cmml"><mi id="S3.SS6.p1.1.m1.1.1.2.2" xref="S3.SS6.p1.1.m1.1.1.2.2.cmml">w</mi><mi id="S3.SS6.p1.1.m1.1.1.2.3" xref="S3.SS6.p1.1.m1.1.1.2.3.cmml">l</mi></msub><mo id="S3.SS6.p1.1.m1.1.1.1" xref="S3.SS6.p1.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS6.p1.1.m1.1.1.3" xref="S3.SS6.p1.1.m1.1.1.3.cmml">0.12</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.1.m1.1b"><apply id="S3.SS6.p1.1.m1.1.1.cmml" xref="S3.SS6.p1.1.m1.1.1"><eq id="S3.SS6.p1.1.m1.1.1.1.cmml" xref="S3.SS6.p1.1.m1.1.1.1"></eq><apply id="S3.SS6.p1.1.m1.1.1.2.cmml" xref="S3.SS6.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS6.p1.1.m1.1.1.2.1.cmml" xref="S3.SS6.p1.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS6.p1.1.m1.1.1.2.2.cmml" xref="S3.SS6.p1.1.m1.1.1.2.2">𝑤</ci><ci id="S3.SS6.p1.1.m1.1.1.2.3.cmml" xref="S3.SS6.p1.1.m1.1.1.2.3">𝑙</ci></apply><cn type="float" id="S3.SS6.p1.1.m1.1.1.3.cmml" xref="S3.SS6.p1.1.m1.1.1.3">0.12</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.1.m1.1c">w_{l}=0.12</annotation></semantics></math>, <math id="S3.SS6.p1.2.m2.1" class="ltx_Math" alttext="w_{s}=0.22" display="inline"><semantics id="S3.SS6.p1.2.m2.1a"><mrow id="S3.SS6.p1.2.m2.1.1" xref="S3.SS6.p1.2.m2.1.1.cmml"><msub id="S3.SS6.p1.2.m2.1.1.2" xref="S3.SS6.p1.2.m2.1.1.2.cmml"><mi id="S3.SS6.p1.2.m2.1.1.2.2" xref="S3.SS6.p1.2.m2.1.1.2.2.cmml">w</mi><mi id="S3.SS6.p1.2.m2.1.1.2.3" xref="S3.SS6.p1.2.m2.1.1.2.3.cmml">s</mi></msub><mo id="S3.SS6.p1.2.m2.1.1.1" xref="S3.SS6.p1.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS6.p1.2.m2.1.1.3" xref="S3.SS6.p1.2.m2.1.1.3.cmml">0.22</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.2.m2.1b"><apply id="S3.SS6.p1.2.m2.1.1.cmml" xref="S3.SS6.p1.2.m2.1.1"><eq id="S3.SS6.p1.2.m2.1.1.1.cmml" xref="S3.SS6.p1.2.m2.1.1.1"></eq><apply id="S3.SS6.p1.2.m2.1.1.2.cmml" xref="S3.SS6.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS6.p1.2.m2.1.1.2.1.cmml" xref="S3.SS6.p1.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS6.p1.2.m2.1.1.2.2.cmml" xref="S3.SS6.p1.2.m2.1.1.2.2">𝑤</ci><ci id="S3.SS6.p1.2.m2.1.1.2.3.cmml" xref="S3.SS6.p1.2.m2.1.1.2.3">𝑠</ci></apply><cn type="float" id="S3.SS6.p1.2.m2.1.1.3.cmml" xref="S3.SS6.p1.2.m2.1.1.3">0.22</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.2.m2.1c">w_{s}=0.22</annotation></semantics></math>, <math id="S3.SS6.p1.3.m3.1" class="ltx_Math" alttext="w_{v}=0.66" display="inline"><semantics id="S3.SS6.p1.3.m3.1a"><mrow id="S3.SS6.p1.3.m3.1.1" xref="S3.SS6.p1.3.m3.1.1.cmml"><msub id="S3.SS6.p1.3.m3.1.1.2" xref="S3.SS6.p1.3.m3.1.1.2.cmml"><mi id="S3.SS6.p1.3.m3.1.1.2.2" xref="S3.SS6.p1.3.m3.1.1.2.2.cmml">w</mi><mi id="S3.SS6.p1.3.m3.1.1.2.3" xref="S3.SS6.p1.3.m3.1.1.2.3.cmml">v</mi></msub><mo id="S3.SS6.p1.3.m3.1.1.1" xref="S3.SS6.p1.3.m3.1.1.1.cmml">=</mo><mn id="S3.SS6.p1.3.m3.1.1.3" xref="S3.SS6.p1.3.m3.1.1.3.cmml">0.66</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.3.m3.1b"><apply id="S3.SS6.p1.3.m3.1.1.cmml" xref="S3.SS6.p1.3.m3.1.1"><eq id="S3.SS6.p1.3.m3.1.1.1.cmml" xref="S3.SS6.p1.3.m3.1.1.1"></eq><apply id="S3.SS6.p1.3.m3.1.1.2.cmml" xref="S3.SS6.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS6.p1.3.m3.1.1.2.1.cmml" xref="S3.SS6.p1.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS6.p1.3.m3.1.1.2.2.cmml" xref="S3.SS6.p1.3.m3.1.1.2.2">𝑤</ci><ci id="S3.SS6.p1.3.m3.1.1.2.3.cmml" xref="S3.SS6.p1.3.m3.1.1.2.3">𝑣</ci></apply><cn type="float" id="S3.SS6.p1.3.m3.1.1.3.cmml" xref="S3.SS6.p1.3.m3.1.1.3">0.66</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.3.m3.1c">w_{v}=0.66</annotation></semantics></math> for point cloud label, background segmentation, and keypoint voting, respectively. An additional loss, <math id="S3.SS6.p1.4.m4.1" class="ltx_Math" alttext="L_{MD}" display="inline"><semantics id="S3.SS6.p1.4.m4.1a"><msub id="S3.SS6.p1.4.m4.1.1" xref="S3.SS6.p1.4.m4.1.1.cmml"><mi id="S3.SS6.p1.4.m4.1.1.2" xref="S3.SS6.p1.4.m4.1.1.2.cmml">L</mi><mrow id="S3.SS6.p1.4.m4.1.1.3" xref="S3.SS6.p1.4.m4.1.1.3.cmml"><mi id="S3.SS6.p1.4.m4.1.1.3.2" xref="S3.SS6.p1.4.m4.1.1.3.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S3.SS6.p1.4.m4.1.1.3.1" xref="S3.SS6.p1.4.m4.1.1.3.1.cmml">​</mo><mi id="S3.SS6.p1.4.m4.1.1.3.3" xref="S3.SS6.p1.4.m4.1.1.3.3.cmml">D</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.4.m4.1b"><apply id="S3.SS6.p1.4.m4.1.1.cmml" xref="S3.SS6.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS6.p1.4.m4.1.1.1.cmml" xref="S3.SS6.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS6.p1.4.m4.1.1.2.cmml" xref="S3.SS6.p1.4.m4.1.1.2">𝐿</ci><apply id="S3.SS6.p1.4.m4.1.1.3.cmml" xref="S3.SS6.p1.4.m4.1.1.3"><times id="S3.SS6.p1.4.m4.1.1.3.1.cmml" xref="S3.SS6.p1.4.m4.1.1.3.1"></times><ci id="S3.SS6.p1.4.m4.1.1.3.2.cmml" xref="S3.SS6.p1.4.m4.1.1.3.2">𝑀</ci><ci id="S3.SS6.p1.4.m4.1.1.3.3.cmml" xref="S3.SS6.p1.4.m4.1.1.3.3">𝐷</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.4.m4.1c">L_{MD}</annotation></semantics></math>, is added for the Transform Matrix as according to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, with weight <math id="S3.SS6.p1.5.m5.1" class="ltx_Math" alttext="w_{MD}=10^{-3}" display="inline"><semantics id="S3.SS6.p1.5.m5.1a"><mrow id="S3.SS6.p1.5.m5.1.1" xref="S3.SS6.p1.5.m5.1.1.cmml"><msub id="S3.SS6.p1.5.m5.1.1.2" xref="S3.SS6.p1.5.m5.1.1.2.cmml"><mi id="S3.SS6.p1.5.m5.1.1.2.2" xref="S3.SS6.p1.5.m5.1.1.2.2.cmml">w</mi><mrow id="S3.SS6.p1.5.m5.1.1.2.3" xref="S3.SS6.p1.5.m5.1.1.2.3.cmml"><mi id="S3.SS6.p1.5.m5.1.1.2.3.2" xref="S3.SS6.p1.5.m5.1.1.2.3.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S3.SS6.p1.5.m5.1.1.2.3.1" xref="S3.SS6.p1.5.m5.1.1.2.3.1.cmml">​</mo><mi id="S3.SS6.p1.5.m5.1.1.2.3.3" xref="S3.SS6.p1.5.m5.1.1.2.3.3.cmml">D</mi></mrow></msub><mo id="S3.SS6.p1.5.m5.1.1.1" xref="S3.SS6.p1.5.m5.1.1.1.cmml">=</mo><msup id="S3.SS6.p1.5.m5.1.1.3" xref="S3.SS6.p1.5.m5.1.1.3.cmml"><mn id="S3.SS6.p1.5.m5.1.1.3.2" xref="S3.SS6.p1.5.m5.1.1.3.2.cmml">10</mn><mrow id="S3.SS6.p1.5.m5.1.1.3.3" xref="S3.SS6.p1.5.m5.1.1.3.3.cmml"><mo id="S3.SS6.p1.5.m5.1.1.3.3a" xref="S3.SS6.p1.5.m5.1.1.3.3.cmml">−</mo><mn id="S3.SS6.p1.5.m5.1.1.3.3.2" xref="S3.SS6.p1.5.m5.1.1.3.3.2.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.5.m5.1b"><apply id="S3.SS6.p1.5.m5.1.1.cmml" xref="S3.SS6.p1.5.m5.1.1"><eq id="S3.SS6.p1.5.m5.1.1.1.cmml" xref="S3.SS6.p1.5.m5.1.1.1"></eq><apply id="S3.SS6.p1.5.m5.1.1.2.cmml" xref="S3.SS6.p1.5.m5.1.1.2"><csymbol cd="ambiguous" id="S3.SS6.p1.5.m5.1.1.2.1.cmml" xref="S3.SS6.p1.5.m5.1.1.2">subscript</csymbol><ci id="S3.SS6.p1.5.m5.1.1.2.2.cmml" xref="S3.SS6.p1.5.m5.1.1.2.2">𝑤</ci><apply id="S3.SS6.p1.5.m5.1.1.2.3.cmml" xref="S3.SS6.p1.5.m5.1.1.2.3"><times id="S3.SS6.p1.5.m5.1.1.2.3.1.cmml" xref="S3.SS6.p1.5.m5.1.1.2.3.1"></times><ci id="S3.SS6.p1.5.m5.1.1.2.3.2.cmml" xref="S3.SS6.p1.5.m5.1.1.2.3.2">𝑀</ci><ci id="S3.SS6.p1.5.m5.1.1.2.3.3.cmml" xref="S3.SS6.p1.5.m5.1.1.2.3.3">𝐷</ci></apply></apply><apply id="S3.SS6.p1.5.m5.1.1.3.cmml" xref="S3.SS6.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS6.p1.5.m5.1.1.3.1.cmml" xref="S3.SS6.p1.5.m5.1.1.3">superscript</csymbol><cn type="integer" id="S3.SS6.p1.5.m5.1.1.3.2.cmml" xref="S3.SS6.p1.5.m5.1.1.3.2">10</cn><apply id="S3.SS6.p1.5.m5.1.1.3.3.cmml" xref="S3.SS6.p1.5.m5.1.1.3.3"><minus id="S3.SS6.p1.5.m5.1.1.3.3.1.cmml" xref="S3.SS6.p1.5.m5.1.1.3.3"></minus><cn type="integer" id="S3.SS6.p1.5.m5.1.1.3.3.2.cmml" xref="S3.SS6.p1.5.m5.1.1.3.3.2">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.5.m5.1c">w_{MD}=10^{-3}</annotation></semantics></math>. The full loss is shown in Eq. <a href="#S3.E3" title="In 3.6 Multi-Task Network Training ‣ 3 Method ‣ Bridging the Reality Gap for Pose Estimation Networks using Sensor-Based Domain Randomization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div id="S3.SS6.p2" class="ltx_para">
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.1" class="ltx_Math" alttext="L_{total}=w_{l}L_{l}+w_{s}L_{s}+w_{v}L_{v}+w_{MD}L_{MD}" display="block"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml"><msub id="S3.E3.m1.1.1.2" xref="S3.E3.m1.1.1.2.cmml"><mi id="S3.E3.m1.1.1.2.2" xref="S3.E3.m1.1.1.2.2.cmml">L</mi><mrow id="S3.E3.m1.1.1.2.3" xref="S3.E3.m1.1.1.2.3.cmml"><mi id="S3.E3.m1.1.1.2.3.2" xref="S3.E3.m1.1.1.2.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.2.3.1" xref="S3.E3.m1.1.1.2.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.2.3.3" xref="S3.E3.m1.1.1.2.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.2.3.1a" xref="S3.E3.m1.1.1.2.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.2.3.4" xref="S3.E3.m1.1.1.2.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.2.3.1b" xref="S3.E3.m1.1.1.2.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.2.3.5" xref="S3.E3.m1.1.1.2.3.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.2.3.1c" xref="S3.E3.m1.1.1.2.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.2.3.6" xref="S3.E3.m1.1.1.2.3.6.cmml">l</mi></mrow></msub><mo id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.cmml">=</mo><mrow id="S3.E3.m1.1.1.3" xref="S3.E3.m1.1.1.3.cmml"><mrow id="S3.E3.m1.1.1.3.2" xref="S3.E3.m1.1.1.3.2.cmml"><msub id="S3.E3.m1.1.1.3.2.2" xref="S3.E3.m1.1.1.3.2.2.cmml"><mi id="S3.E3.m1.1.1.3.2.2.2" xref="S3.E3.m1.1.1.3.2.2.2.cmml">w</mi><mi id="S3.E3.m1.1.1.3.2.2.3" xref="S3.E3.m1.1.1.3.2.2.3.cmml">l</mi></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.3.2.1" xref="S3.E3.m1.1.1.3.2.1.cmml">​</mo><msub id="S3.E3.m1.1.1.3.2.3" xref="S3.E3.m1.1.1.3.2.3.cmml"><mi id="S3.E3.m1.1.1.3.2.3.2" xref="S3.E3.m1.1.1.3.2.3.2.cmml">L</mi><mi id="S3.E3.m1.1.1.3.2.3.3" xref="S3.E3.m1.1.1.3.2.3.3.cmml">l</mi></msub></mrow><mo id="S3.E3.m1.1.1.3.1" xref="S3.E3.m1.1.1.3.1.cmml">+</mo><mrow id="S3.E3.m1.1.1.3.3" xref="S3.E3.m1.1.1.3.3.cmml"><msub id="S3.E3.m1.1.1.3.3.2" xref="S3.E3.m1.1.1.3.3.2.cmml"><mi id="S3.E3.m1.1.1.3.3.2.2" xref="S3.E3.m1.1.1.3.3.2.2.cmml">w</mi><mi id="S3.E3.m1.1.1.3.3.2.3" xref="S3.E3.m1.1.1.3.3.2.3.cmml">s</mi></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.3.3.1" xref="S3.E3.m1.1.1.3.3.1.cmml">​</mo><msub id="S3.E3.m1.1.1.3.3.3" xref="S3.E3.m1.1.1.3.3.3.cmml"><mi id="S3.E3.m1.1.1.3.3.3.2" xref="S3.E3.m1.1.1.3.3.3.2.cmml">L</mi><mi id="S3.E3.m1.1.1.3.3.3.3" xref="S3.E3.m1.1.1.3.3.3.3.cmml">s</mi></msub></mrow><mo id="S3.E3.m1.1.1.3.1a" xref="S3.E3.m1.1.1.3.1.cmml">+</mo><mrow id="S3.E3.m1.1.1.3.4" xref="S3.E3.m1.1.1.3.4.cmml"><msub id="S3.E3.m1.1.1.3.4.2" xref="S3.E3.m1.1.1.3.4.2.cmml"><mi id="S3.E3.m1.1.1.3.4.2.2" xref="S3.E3.m1.1.1.3.4.2.2.cmml">w</mi><mi id="S3.E3.m1.1.1.3.4.2.3" xref="S3.E3.m1.1.1.3.4.2.3.cmml">v</mi></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.3.4.1" xref="S3.E3.m1.1.1.3.4.1.cmml">​</mo><msub id="S3.E3.m1.1.1.3.4.3" xref="S3.E3.m1.1.1.3.4.3.cmml"><mi id="S3.E3.m1.1.1.3.4.3.2" xref="S3.E3.m1.1.1.3.4.3.2.cmml">L</mi><mi id="S3.E3.m1.1.1.3.4.3.3" xref="S3.E3.m1.1.1.3.4.3.3.cmml">v</mi></msub></mrow><mo id="S3.E3.m1.1.1.3.1b" xref="S3.E3.m1.1.1.3.1.cmml">+</mo><mrow id="S3.E3.m1.1.1.3.5" xref="S3.E3.m1.1.1.3.5.cmml"><msub id="S3.E3.m1.1.1.3.5.2" xref="S3.E3.m1.1.1.3.5.2.cmml"><mi id="S3.E3.m1.1.1.3.5.2.2" xref="S3.E3.m1.1.1.3.5.2.2.cmml">w</mi><mrow id="S3.E3.m1.1.1.3.5.2.3" xref="S3.E3.m1.1.1.3.5.2.3.cmml"><mi id="S3.E3.m1.1.1.3.5.2.3.2" xref="S3.E3.m1.1.1.3.5.2.3.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.3.5.2.3.1" xref="S3.E3.m1.1.1.3.5.2.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.3.5.2.3.3" xref="S3.E3.m1.1.1.3.5.2.3.3.cmml">D</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.3.5.1" xref="S3.E3.m1.1.1.3.5.1.cmml">​</mo><msub id="S3.E3.m1.1.1.3.5.3" xref="S3.E3.m1.1.1.3.5.3.cmml"><mi id="S3.E3.m1.1.1.3.5.3.2" xref="S3.E3.m1.1.1.3.5.3.2.cmml">L</mi><mrow id="S3.E3.m1.1.1.3.5.3.3" xref="S3.E3.m1.1.1.3.5.3.3.cmml"><mi id="S3.E3.m1.1.1.3.5.3.3.2" xref="S3.E3.m1.1.1.3.5.3.3.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.3.5.3.3.1" xref="S3.E3.m1.1.1.3.5.3.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.3.5.3.3.3" xref="S3.E3.m1.1.1.3.5.3.3.3.cmml">D</mi></mrow></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1"><eq id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"></eq><apply id="S3.E3.m1.1.1.2.cmml" xref="S3.E3.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.2">subscript</csymbol><ci id="S3.E3.m1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.2.2">𝐿</ci><apply id="S3.E3.m1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.2.3"><times id="S3.E3.m1.1.1.2.3.1.cmml" xref="S3.E3.m1.1.1.2.3.1"></times><ci id="S3.E3.m1.1.1.2.3.2.cmml" xref="S3.E3.m1.1.1.2.3.2">𝑡</ci><ci id="S3.E3.m1.1.1.2.3.3.cmml" xref="S3.E3.m1.1.1.2.3.3">𝑜</ci><ci id="S3.E3.m1.1.1.2.3.4.cmml" xref="S3.E3.m1.1.1.2.3.4">𝑡</ci><ci id="S3.E3.m1.1.1.2.3.5.cmml" xref="S3.E3.m1.1.1.2.3.5">𝑎</ci><ci id="S3.E3.m1.1.1.2.3.6.cmml" xref="S3.E3.m1.1.1.2.3.6">𝑙</ci></apply></apply><apply id="S3.E3.m1.1.1.3.cmml" xref="S3.E3.m1.1.1.3"><plus id="S3.E3.m1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.3.1"></plus><apply id="S3.E3.m1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.3.2"><times id="S3.E3.m1.1.1.3.2.1.cmml" xref="S3.E3.m1.1.1.3.2.1"></times><apply id="S3.E3.m1.1.1.3.2.2.cmml" xref="S3.E3.m1.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.2.2.1.cmml" xref="S3.E3.m1.1.1.3.2.2">subscript</csymbol><ci id="S3.E3.m1.1.1.3.2.2.2.cmml" xref="S3.E3.m1.1.1.3.2.2.2">𝑤</ci><ci id="S3.E3.m1.1.1.3.2.2.3.cmml" xref="S3.E3.m1.1.1.3.2.2.3">𝑙</ci></apply><apply id="S3.E3.m1.1.1.3.2.3.cmml" xref="S3.E3.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.2.3.1.cmml" xref="S3.E3.m1.1.1.3.2.3">subscript</csymbol><ci id="S3.E3.m1.1.1.3.2.3.2.cmml" xref="S3.E3.m1.1.1.3.2.3.2">𝐿</ci><ci id="S3.E3.m1.1.1.3.2.3.3.cmml" xref="S3.E3.m1.1.1.3.2.3.3">𝑙</ci></apply></apply><apply id="S3.E3.m1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.3.3"><times id="S3.E3.m1.1.1.3.3.1.cmml" xref="S3.E3.m1.1.1.3.3.1"></times><apply id="S3.E3.m1.1.1.3.3.2.cmml" xref="S3.E3.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.3.2.1.cmml" xref="S3.E3.m1.1.1.3.3.2">subscript</csymbol><ci id="S3.E3.m1.1.1.3.3.2.2.cmml" xref="S3.E3.m1.1.1.3.3.2.2">𝑤</ci><ci id="S3.E3.m1.1.1.3.3.2.3.cmml" xref="S3.E3.m1.1.1.3.3.2.3">𝑠</ci></apply><apply id="S3.E3.m1.1.1.3.3.3.cmml" xref="S3.E3.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.3.3.1.cmml" xref="S3.E3.m1.1.1.3.3.3">subscript</csymbol><ci id="S3.E3.m1.1.1.3.3.3.2.cmml" xref="S3.E3.m1.1.1.3.3.3.2">𝐿</ci><ci id="S3.E3.m1.1.1.3.3.3.3.cmml" xref="S3.E3.m1.1.1.3.3.3.3">𝑠</ci></apply></apply><apply id="S3.E3.m1.1.1.3.4.cmml" xref="S3.E3.m1.1.1.3.4"><times id="S3.E3.m1.1.1.3.4.1.cmml" xref="S3.E3.m1.1.1.3.4.1"></times><apply id="S3.E3.m1.1.1.3.4.2.cmml" xref="S3.E3.m1.1.1.3.4.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.4.2.1.cmml" xref="S3.E3.m1.1.1.3.4.2">subscript</csymbol><ci id="S3.E3.m1.1.1.3.4.2.2.cmml" xref="S3.E3.m1.1.1.3.4.2.2">𝑤</ci><ci id="S3.E3.m1.1.1.3.4.2.3.cmml" xref="S3.E3.m1.1.1.3.4.2.3">𝑣</ci></apply><apply id="S3.E3.m1.1.1.3.4.3.cmml" xref="S3.E3.m1.1.1.3.4.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.4.3.1.cmml" xref="S3.E3.m1.1.1.3.4.3">subscript</csymbol><ci id="S3.E3.m1.1.1.3.4.3.2.cmml" xref="S3.E3.m1.1.1.3.4.3.2">𝐿</ci><ci id="S3.E3.m1.1.1.3.4.3.3.cmml" xref="S3.E3.m1.1.1.3.4.3.3">𝑣</ci></apply></apply><apply id="S3.E3.m1.1.1.3.5.cmml" xref="S3.E3.m1.1.1.3.5"><times id="S3.E3.m1.1.1.3.5.1.cmml" xref="S3.E3.m1.1.1.3.5.1"></times><apply id="S3.E3.m1.1.1.3.5.2.cmml" xref="S3.E3.m1.1.1.3.5.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.5.2.1.cmml" xref="S3.E3.m1.1.1.3.5.2">subscript</csymbol><ci id="S3.E3.m1.1.1.3.5.2.2.cmml" xref="S3.E3.m1.1.1.3.5.2.2">𝑤</ci><apply id="S3.E3.m1.1.1.3.5.2.3.cmml" xref="S3.E3.m1.1.1.3.5.2.3"><times id="S3.E3.m1.1.1.3.5.2.3.1.cmml" xref="S3.E3.m1.1.1.3.5.2.3.1"></times><ci id="S3.E3.m1.1.1.3.5.2.3.2.cmml" xref="S3.E3.m1.1.1.3.5.2.3.2">𝑀</ci><ci id="S3.E3.m1.1.1.3.5.2.3.3.cmml" xref="S3.E3.m1.1.1.3.5.2.3.3">𝐷</ci></apply></apply><apply id="S3.E3.m1.1.1.3.5.3.cmml" xref="S3.E3.m1.1.1.3.5.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.5.3.1.cmml" xref="S3.E3.m1.1.1.3.5.3">subscript</csymbol><ci id="S3.E3.m1.1.1.3.5.3.2.cmml" xref="S3.E3.m1.1.1.3.5.3.2">𝐿</ci><apply id="S3.E3.m1.1.1.3.5.3.3.cmml" xref="S3.E3.m1.1.1.3.5.3.3"><times id="S3.E3.m1.1.1.3.5.3.3.1.cmml" xref="S3.E3.m1.1.1.3.5.3.3.1"></times><ci id="S3.E3.m1.1.1.3.5.3.3.2.cmml" xref="S3.E3.m1.1.1.3.5.3.3.2">𝑀</ci><ci id="S3.E3.m1.1.1.3.5.3.3.3.cmml" xref="S3.E3.m1.1.1.3.5.3.3.3">𝐷</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">L_{total}=w_{l}L_{l}+w_{s}L_{s}+w_{v}L_{v}+w_{MD}L_{MD}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS6.p3" class="ltx_para">
<p id="S3.SS6.p3.6" class="ltx_p">Here <math id="S3.SS6.p3.1.m1.1" class="ltx_Math" alttext="L_{l}" display="inline"><semantics id="S3.SS6.p3.1.m1.1a"><msub id="S3.SS6.p3.1.m1.1.1" xref="S3.SS6.p3.1.m1.1.1.cmml"><mi id="S3.SS6.p3.1.m1.1.1.2" xref="S3.SS6.p3.1.m1.1.1.2.cmml">L</mi><mi id="S3.SS6.p3.1.m1.1.1.3" xref="S3.SS6.p3.1.m1.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS6.p3.1.m1.1b"><apply id="S3.SS6.p3.1.m1.1.1.cmml" xref="S3.SS6.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS6.p3.1.m1.1.1.1.cmml" xref="S3.SS6.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS6.p3.1.m1.1.1.2.cmml" xref="S3.SS6.p3.1.m1.1.1.2">𝐿</ci><ci id="S3.SS6.p3.1.m1.1.1.3.cmml" xref="S3.SS6.p3.1.m1.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p3.1.m1.1c">L_{l}</annotation></semantics></math> is the label loss found by the cross entropy between the correct label and the softmax output of the prediction. The loss for the background segmentation <math id="S3.SS6.p3.2.m2.1" class="ltx_Math" alttext="L_{s}" display="inline"><semantics id="S3.SS6.p3.2.m2.1a"><msub id="S3.SS6.p3.2.m2.1.1" xref="S3.SS6.p3.2.m2.1.1.cmml"><mi id="S3.SS6.p3.2.m2.1.1.2" xref="S3.SS6.p3.2.m2.1.1.2.cmml">L</mi><mi id="S3.SS6.p3.2.m2.1.1.3" xref="S3.SS6.p3.2.m2.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS6.p3.2.m2.1b"><apply id="S3.SS6.p3.2.m2.1.1.cmml" xref="S3.SS6.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS6.p3.2.m2.1.1.1.cmml" xref="S3.SS6.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS6.p3.2.m2.1.1.2.cmml" xref="S3.SS6.p3.2.m2.1.1.2">𝐿</ci><ci id="S3.SS6.p3.2.m2.1.1.3.cmml" xref="S3.SS6.p3.2.m2.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p3.2.m2.1c">L_{s}</annotation></semantics></math> is found in Eq. <a href="#S3.E4" title="In 3.6 Multi-Task Network Training ‣ 3 Method ‣ Bridging the Reality Gap for Pose Estimation Networks using Sensor-Based Domain Randomization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, where <math id="S3.SS6.p3.3.m3.1" class="ltx_Math" alttext="H" display="inline"><semantics id="S3.SS6.p3.3.m3.1a"><mi id="S3.SS6.p3.3.m3.1.1" xref="S3.SS6.p3.3.m3.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.p3.3.m3.1b"><ci id="S3.SS6.p3.3.m3.1.1.cmml" xref="S3.SS6.p3.3.m3.1.1">𝐻</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p3.3.m3.1c">H</annotation></semantics></math> is the cross entropy, <math id="S3.SS6.p3.4.m4.1" class="ltx_Math" alttext="s_{i}" display="inline"><semantics id="S3.SS6.p3.4.m4.1a"><msub id="S3.SS6.p3.4.m4.1.1" xref="S3.SS6.p3.4.m4.1.1.cmml"><mi id="S3.SS6.p3.4.m4.1.1.2" xref="S3.SS6.p3.4.m4.1.1.2.cmml">s</mi><mi id="S3.SS6.p3.4.m4.1.1.3" xref="S3.SS6.p3.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS6.p3.4.m4.1b"><apply id="S3.SS6.p3.4.m4.1.1.cmml" xref="S3.SS6.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS6.p3.4.m4.1.1.1.cmml" xref="S3.SS6.p3.4.m4.1.1">subscript</csymbol><ci id="S3.SS6.p3.4.m4.1.1.2.cmml" xref="S3.SS6.p3.4.m4.1.1.2">𝑠</ci><ci id="S3.SS6.p3.4.m4.1.1.3.cmml" xref="S3.SS6.p3.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p3.4.m4.1c">s_{i}</annotation></semantics></math> is the correct segmentation for a point, <math id="S3.SS6.p3.5.m5.2" class="ltx_Math" alttext="q_{i,seg}" display="inline"><semantics id="S3.SS6.p3.5.m5.2a"><msub id="S3.SS6.p3.5.m5.2.3" xref="S3.SS6.p3.5.m5.2.3.cmml"><mi id="S3.SS6.p3.5.m5.2.3.2" xref="S3.SS6.p3.5.m5.2.3.2.cmml">q</mi><mrow id="S3.SS6.p3.5.m5.2.2.2.2" xref="S3.SS6.p3.5.m5.2.2.2.3.cmml"><mi id="S3.SS6.p3.5.m5.1.1.1.1" xref="S3.SS6.p3.5.m5.1.1.1.1.cmml">i</mi><mo id="S3.SS6.p3.5.m5.2.2.2.2.2" xref="S3.SS6.p3.5.m5.2.2.2.3.cmml">,</mo><mrow id="S3.SS6.p3.5.m5.2.2.2.2.1" xref="S3.SS6.p3.5.m5.2.2.2.2.1.cmml"><mi id="S3.SS6.p3.5.m5.2.2.2.2.1.2" xref="S3.SS6.p3.5.m5.2.2.2.2.1.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS6.p3.5.m5.2.2.2.2.1.1" xref="S3.SS6.p3.5.m5.2.2.2.2.1.1.cmml">​</mo><mi id="S3.SS6.p3.5.m5.2.2.2.2.1.3" xref="S3.SS6.p3.5.m5.2.2.2.2.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS6.p3.5.m5.2.2.2.2.1.1a" xref="S3.SS6.p3.5.m5.2.2.2.2.1.1.cmml">​</mo><mi id="S3.SS6.p3.5.m5.2.2.2.2.1.4" xref="S3.SS6.p3.5.m5.2.2.2.2.1.4.cmml">g</mi></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS6.p3.5.m5.2b"><apply id="S3.SS6.p3.5.m5.2.3.cmml" xref="S3.SS6.p3.5.m5.2.3"><csymbol cd="ambiguous" id="S3.SS6.p3.5.m5.2.3.1.cmml" xref="S3.SS6.p3.5.m5.2.3">subscript</csymbol><ci id="S3.SS6.p3.5.m5.2.3.2.cmml" xref="S3.SS6.p3.5.m5.2.3.2">𝑞</ci><list id="S3.SS6.p3.5.m5.2.2.2.3.cmml" xref="S3.SS6.p3.5.m5.2.2.2.2"><ci id="S3.SS6.p3.5.m5.1.1.1.1.cmml" xref="S3.SS6.p3.5.m5.1.1.1.1">𝑖</ci><apply id="S3.SS6.p3.5.m5.2.2.2.2.1.cmml" xref="S3.SS6.p3.5.m5.2.2.2.2.1"><times id="S3.SS6.p3.5.m5.2.2.2.2.1.1.cmml" xref="S3.SS6.p3.5.m5.2.2.2.2.1.1"></times><ci id="S3.SS6.p3.5.m5.2.2.2.2.1.2.cmml" xref="S3.SS6.p3.5.m5.2.2.2.2.1.2">𝑠</ci><ci id="S3.SS6.p3.5.m5.2.2.2.2.1.3.cmml" xref="S3.SS6.p3.5.m5.2.2.2.2.1.3">𝑒</ci><ci id="S3.SS6.p3.5.m5.2.2.2.2.1.4.cmml" xref="S3.SS6.p3.5.m5.2.2.2.2.1.4">𝑔</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p3.5.m5.2c">q_{i,seg}</annotation></semantics></math> is the softmax of segmentation predictions for a point, and <math id="S3.SS6.p3.6.m6.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS6.p3.6.m6.1a"><mi id="S3.SS6.p3.6.m6.1.1" xref="S3.SS6.p3.6.m6.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.p3.6.m6.1b"><ci id="S3.SS6.p3.6.m6.1.1.cmml" xref="S3.SS6.p3.6.m6.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p3.6.m6.1c">n</annotation></semantics></math> is the number of points in the point cloud.</p>
</div>
<div id="S3.SS6.p4" class="ltx_para">
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.4" class="ltx_Math" alttext="L_{s}=\dfrac{\sum_{i}^{n}H(s_{i},q_{i,seg})}{n}" display="block"><semantics id="S3.E4.m1.4a"><mrow id="S3.E4.m1.4.5" xref="S3.E4.m1.4.5.cmml"><msub id="S3.E4.m1.4.5.2" xref="S3.E4.m1.4.5.2.cmml"><mi id="S3.E4.m1.4.5.2.2" xref="S3.E4.m1.4.5.2.2.cmml">L</mi><mi id="S3.E4.m1.4.5.2.3" xref="S3.E4.m1.4.5.2.3.cmml">s</mi></msub><mo id="S3.E4.m1.4.5.1" xref="S3.E4.m1.4.5.1.cmml">=</mo><mfrac id="S3.E4.m1.4.4" xref="S3.E4.m1.4.4.cmml"><mrow id="S3.E4.m1.4.4.4" xref="S3.E4.m1.4.4.4.cmml"><msubsup id="S3.E4.m1.4.4.4.5" xref="S3.E4.m1.4.4.4.5.cmml"><mo id="S3.E4.m1.4.4.4.5.2.2" xref="S3.E4.m1.4.4.4.5.2.2.cmml">∑</mo><mi id="S3.E4.m1.4.4.4.5.2.3" xref="S3.E4.m1.4.4.4.5.2.3.cmml">i</mi><mi id="S3.E4.m1.4.4.4.5.3" xref="S3.E4.m1.4.4.4.5.3.cmml">n</mi></msubsup><mrow id="S3.E4.m1.4.4.4.4" xref="S3.E4.m1.4.4.4.4.cmml"><mi id="S3.E4.m1.4.4.4.4.4" xref="S3.E4.m1.4.4.4.4.4.cmml">H</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.4.4.4.4.3" xref="S3.E4.m1.4.4.4.4.3.cmml">​</mo><mrow id="S3.E4.m1.4.4.4.4.2.2" xref="S3.E4.m1.4.4.4.4.2.3.cmml"><mo stretchy="false" id="S3.E4.m1.4.4.4.4.2.2.3" xref="S3.E4.m1.4.4.4.4.2.3.cmml">(</mo><msub id="S3.E4.m1.3.3.3.3.1.1.1" xref="S3.E4.m1.3.3.3.3.1.1.1.cmml"><mi id="S3.E4.m1.3.3.3.3.1.1.1.2" xref="S3.E4.m1.3.3.3.3.1.1.1.2.cmml">s</mi><mi id="S3.E4.m1.3.3.3.3.1.1.1.3" xref="S3.E4.m1.3.3.3.3.1.1.1.3.cmml">i</mi></msub><mo id="S3.E4.m1.4.4.4.4.2.2.4" xref="S3.E4.m1.4.4.4.4.2.3.cmml">,</mo><msub id="S3.E4.m1.4.4.4.4.2.2.2" xref="S3.E4.m1.4.4.4.4.2.2.2.cmml"><mi id="S3.E4.m1.4.4.4.4.2.2.2.2" xref="S3.E4.m1.4.4.4.4.2.2.2.2.cmml">q</mi><mrow id="S3.E4.m1.2.2.2.2.2.2" xref="S3.E4.m1.2.2.2.2.2.3.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.cmml">i</mi><mo id="S3.E4.m1.2.2.2.2.2.2.2" xref="S3.E4.m1.2.2.2.2.2.3.cmml">,</mo><mrow id="S3.E4.m1.2.2.2.2.2.2.1" xref="S3.E4.m1.2.2.2.2.2.2.1.cmml"><mi id="S3.E4.m1.2.2.2.2.2.2.1.2" xref="S3.E4.m1.2.2.2.2.2.2.1.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.2.2.2.2.1.1" xref="S3.E4.m1.2.2.2.2.2.2.1.1.cmml">​</mo><mi id="S3.E4.m1.2.2.2.2.2.2.1.3" xref="S3.E4.m1.2.2.2.2.2.2.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.2.2.2.2.1.1a" xref="S3.E4.m1.2.2.2.2.2.2.1.1.cmml">​</mo><mi id="S3.E4.m1.2.2.2.2.2.2.1.4" xref="S3.E4.m1.2.2.2.2.2.2.1.4.cmml">g</mi></mrow></mrow></msub><mo stretchy="false" id="S3.E4.m1.4.4.4.4.2.2.5" xref="S3.E4.m1.4.4.4.4.2.3.cmml">)</mo></mrow></mrow></mrow><mi id="S3.E4.m1.4.4.6" xref="S3.E4.m1.4.4.6.cmml">n</mi></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.4b"><apply id="S3.E4.m1.4.5.cmml" xref="S3.E4.m1.4.5"><eq id="S3.E4.m1.4.5.1.cmml" xref="S3.E4.m1.4.5.1"></eq><apply id="S3.E4.m1.4.5.2.cmml" xref="S3.E4.m1.4.5.2"><csymbol cd="ambiguous" id="S3.E4.m1.4.5.2.1.cmml" xref="S3.E4.m1.4.5.2">subscript</csymbol><ci id="S3.E4.m1.4.5.2.2.cmml" xref="S3.E4.m1.4.5.2.2">𝐿</ci><ci id="S3.E4.m1.4.5.2.3.cmml" xref="S3.E4.m1.4.5.2.3">𝑠</ci></apply><apply id="S3.E4.m1.4.4.cmml" xref="S3.E4.m1.4.4"><divide id="S3.E4.m1.4.4.5.cmml" xref="S3.E4.m1.4.4"></divide><apply id="S3.E4.m1.4.4.4.cmml" xref="S3.E4.m1.4.4.4"><apply id="S3.E4.m1.4.4.4.5.cmml" xref="S3.E4.m1.4.4.4.5"><csymbol cd="ambiguous" id="S3.E4.m1.4.4.4.5.1.cmml" xref="S3.E4.m1.4.4.4.5">superscript</csymbol><apply id="S3.E4.m1.4.4.4.5.2.cmml" xref="S3.E4.m1.4.4.4.5"><csymbol cd="ambiguous" id="S3.E4.m1.4.4.4.5.2.1.cmml" xref="S3.E4.m1.4.4.4.5">subscript</csymbol><sum id="S3.E4.m1.4.4.4.5.2.2.cmml" xref="S3.E4.m1.4.4.4.5.2.2"></sum><ci id="S3.E4.m1.4.4.4.5.2.3.cmml" xref="S3.E4.m1.4.4.4.5.2.3">𝑖</ci></apply><ci id="S3.E4.m1.4.4.4.5.3.cmml" xref="S3.E4.m1.4.4.4.5.3">𝑛</ci></apply><apply id="S3.E4.m1.4.4.4.4.cmml" xref="S3.E4.m1.4.4.4.4"><times id="S3.E4.m1.4.4.4.4.3.cmml" xref="S3.E4.m1.4.4.4.4.3"></times><ci id="S3.E4.m1.4.4.4.4.4.cmml" xref="S3.E4.m1.4.4.4.4.4">𝐻</ci><interval closure="open" id="S3.E4.m1.4.4.4.4.2.3.cmml" xref="S3.E4.m1.4.4.4.4.2.2"><apply id="S3.E4.m1.3.3.3.3.1.1.1.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.3.3.1.1.1.1.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1">subscript</csymbol><ci id="S3.E4.m1.3.3.3.3.1.1.1.2.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1.2">𝑠</ci><ci id="S3.E4.m1.3.3.3.3.1.1.1.3.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1.3">𝑖</ci></apply><apply id="S3.E4.m1.4.4.4.4.2.2.2.cmml" xref="S3.E4.m1.4.4.4.4.2.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.4.4.4.4.2.2.2.1.cmml" xref="S3.E4.m1.4.4.4.4.2.2.2">subscript</csymbol><ci id="S3.E4.m1.4.4.4.4.2.2.2.2.cmml" xref="S3.E4.m1.4.4.4.4.2.2.2.2">𝑞</ci><list id="S3.E4.m1.2.2.2.2.2.3.cmml" xref="S3.E4.m1.2.2.2.2.2.2"><ci id="S3.E4.m1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1">𝑖</ci><apply id="S3.E4.m1.2.2.2.2.2.2.1.cmml" xref="S3.E4.m1.2.2.2.2.2.2.1"><times id="S3.E4.m1.2.2.2.2.2.2.1.1.cmml" xref="S3.E4.m1.2.2.2.2.2.2.1.1"></times><ci id="S3.E4.m1.2.2.2.2.2.2.1.2.cmml" xref="S3.E4.m1.2.2.2.2.2.2.1.2">𝑠</ci><ci id="S3.E4.m1.2.2.2.2.2.2.1.3.cmml" xref="S3.E4.m1.2.2.2.2.2.2.1.3">𝑒</ci><ci id="S3.E4.m1.2.2.2.2.2.2.1.4.cmml" xref="S3.E4.m1.2.2.2.2.2.2.1.4">𝑔</ci></apply></list></apply></interval></apply></apply><ci id="S3.E4.m1.4.4.6.cmml" xref="S3.E4.m1.4.4.6">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.4c">L_{s}=\dfrac{\sum_{i}^{n}H(s_{i},q_{i,seg})}{n}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS6.p5" class="ltx_para">
<p id="S3.SS6.p5.4" class="ltx_p">When computing the keypoint voting loss, <math id="S3.SS6.p5.1.m1.1" class="ltx_Math" alttext="L_{v}" display="inline"><semantics id="S3.SS6.p5.1.m1.1a"><msub id="S3.SS6.p5.1.m1.1.1" xref="S3.SS6.p5.1.m1.1.1.cmml"><mi id="S3.SS6.p5.1.m1.1.1.2" xref="S3.SS6.p5.1.m1.1.1.2.cmml">L</mi><mi id="S3.SS6.p5.1.m1.1.1.3" xref="S3.SS6.p5.1.m1.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS6.p5.1.m1.1b"><apply id="S3.SS6.p5.1.m1.1.1.cmml" xref="S3.SS6.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS6.p5.1.m1.1.1.1.cmml" xref="S3.SS6.p5.1.m1.1.1">subscript</csymbol><ci id="S3.SS6.p5.1.m1.1.1.2.cmml" xref="S3.SS6.p5.1.m1.1.1.2">𝐿</ci><ci id="S3.SS6.p5.1.m1.1.1.3.cmml" xref="S3.SS6.p5.1.m1.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p5.1.m1.1c">L_{v}</annotation></semantics></math>, only the loss for points belonging to the object is desired. This is achieved by using <math id="S3.SS6.p5.2.m2.1" class="ltx_Math" alttext="s_{i}" display="inline"><semantics id="S3.SS6.p5.2.m2.1a"><msub id="S3.SS6.p5.2.m2.1.1" xref="S3.SS6.p5.2.m2.1.1.cmml"><mi id="S3.SS6.p5.2.m2.1.1.2" xref="S3.SS6.p5.2.m2.1.1.2.cmml">s</mi><mi id="S3.SS6.p5.2.m2.1.1.3" xref="S3.SS6.p5.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS6.p5.2.m2.1b"><apply id="S3.SS6.p5.2.m2.1.1.cmml" xref="S3.SS6.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS6.p5.2.m2.1.1.1.cmml" xref="S3.SS6.p5.2.m2.1.1">subscript</csymbol><ci id="S3.SS6.p5.2.m2.1.1.2.cmml" xref="S3.SS6.p5.2.m2.1.1.2">𝑠</ci><ci id="S3.SS6.p5.2.m2.1.1.3.cmml" xref="S3.SS6.p5.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p5.2.m2.1c">s_{i}</annotation></semantics></math> which returns zero or one, depending on whether the point belongs to background or object, respectively. The loss is thus computed as in Eq. <a href="#S3.E5" title="In 3.6 Multi-Task Network Training ‣ 3 Method ‣ Bridging the Reality Gap for Pose Estimation Networks using Sensor-Based Domain Randomization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, where <math id="S3.SS6.p5.3.m3.2" class="ltx_Math" alttext="q_{i,vote}" display="inline"><semantics id="S3.SS6.p5.3.m3.2a"><msub id="S3.SS6.p5.3.m3.2.3" xref="S3.SS6.p5.3.m3.2.3.cmml"><mi id="S3.SS6.p5.3.m3.2.3.2" xref="S3.SS6.p5.3.m3.2.3.2.cmml">q</mi><mrow id="S3.SS6.p5.3.m3.2.2.2.2" xref="S3.SS6.p5.3.m3.2.2.2.3.cmml"><mi id="S3.SS6.p5.3.m3.1.1.1.1" xref="S3.SS6.p5.3.m3.1.1.1.1.cmml">i</mi><mo id="S3.SS6.p5.3.m3.2.2.2.2.2" xref="S3.SS6.p5.3.m3.2.2.2.3.cmml">,</mo><mrow id="S3.SS6.p5.3.m3.2.2.2.2.1" xref="S3.SS6.p5.3.m3.2.2.2.2.1.cmml"><mi id="S3.SS6.p5.3.m3.2.2.2.2.1.2" xref="S3.SS6.p5.3.m3.2.2.2.2.1.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.SS6.p5.3.m3.2.2.2.2.1.1" xref="S3.SS6.p5.3.m3.2.2.2.2.1.1.cmml">​</mo><mi id="S3.SS6.p5.3.m3.2.2.2.2.1.3" xref="S3.SS6.p5.3.m3.2.2.2.2.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS6.p5.3.m3.2.2.2.2.1.1a" xref="S3.SS6.p5.3.m3.2.2.2.2.1.1.cmml">​</mo><mi id="S3.SS6.p5.3.m3.2.2.2.2.1.4" xref="S3.SS6.p5.3.m3.2.2.2.2.1.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS6.p5.3.m3.2.2.2.2.1.1b" xref="S3.SS6.p5.3.m3.2.2.2.2.1.1.cmml">​</mo><mi id="S3.SS6.p5.3.m3.2.2.2.2.1.5" xref="S3.SS6.p5.3.m3.2.2.2.2.1.5.cmml">e</mi></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS6.p5.3.m3.2b"><apply id="S3.SS6.p5.3.m3.2.3.cmml" xref="S3.SS6.p5.3.m3.2.3"><csymbol cd="ambiguous" id="S3.SS6.p5.3.m3.2.3.1.cmml" xref="S3.SS6.p5.3.m3.2.3">subscript</csymbol><ci id="S3.SS6.p5.3.m3.2.3.2.cmml" xref="S3.SS6.p5.3.m3.2.3.2">𝑞</ci><list id="S3.SS6.p5.3.m3.2.2.2.3.cmml" xref="S3.SS6.p5.3.m3.2.2.2.2"><ci id="S3.SS6.p5.3.m3.1.1.1.1.cmml" xref="S3.SS6.p5.3.m3.1.1.1.1">𝑖</ci><apply id="S3.SS6.p5.3.m3.2.2.2.2.1.cmml" xref="S3.SS6.p5.3.m3.2.2.2.2.1"><times id="S3.SS6.p5.3.m3.2.2.2.2.1.1.cmml" xref="S3.SS6.p5.3.m3.2.2.2.2.1.1"></times><ci id="S3.SS6.p5.3.m3.2.2.2.2.1.2.cmml" xref="S3.SS6.p5.3.m3.2.2.2.2.1.2">𝑣</ci><ci id="S3.SS6.p5.3.m3.2.2.2.2.1.3.cmml" xref="S3.SS6.p5.3.m3.2.2.2.2.1.3">𝑜</ci><ci id="S3.SS6.p5.3.m3.2.2.2.2.1.4.cmml" xref="S3.SS6.p5.3.m3.2.2.2.2.1.4">𝑡</ci><ci id="S3.SS6.p5.3.m3.2.2.2.2.1.5.cmml" xref="S3.SS6.p5.3.m3.2.2.2.2.1.5">𝑒</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p5.3.m3.2c">q_{i,vote}</annotation></semantics></math> is the softmax of the keypoint vote, and <math id="S3.SS6.p5.4.m4.1" class="ltx_Math" alttext="v_{i}" display="inline"><semantics id="S3.SS6.p5.4.m4.1a"><msub id="S3.SS6.p5.4.m4.1.1" xref="S3.SS6.p5.4.m4.1.1.cmml"><mi id="S3.SS6.p5.4.m4.1.1.2" xref="S3.SS6.p5.4.m4.1.1.2.cmml">v</mi><mi id="S3.SS6.p5.4.m4.1.1.3" xref="S3.SS6.p5.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS6.p5.4.m4.1b"><apply id="S3.SS6.p5.4.m4.1.1.cmml" xref="S3.SS6.p5.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS6.p5.4.m4.1.1.1.cmml" xref="S3.SS6.p5.4.m4.1.1">subscript</csymbol><ci id="S3.SS6.p5.4.m4.1.1.2.cmml" xref="S3.SS6.p5.4.m4.1.1.2">𝑣</ci><ci id="S3.SS6.p5.4.m4.1.1.3.cmml" xref="S3.SS6.p5.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p5.4.m4.1c">v_{i}</annotation></semantics></math> is the correct keypoint.</p>
</div>
<div id="S3.SS6.p6" class="ltx_para">
<table id="S3.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E5.m1.4" class="ltx_Math" alttext="L_{v}=\dfrac{\sum_{i}^{n}H(v_{i},q_{i,vote})s_{i}}{\sum_{i}^{n}s_{i}}" display="block"><semantics id="S3.E5.m1.4a"><mrow id="S3.E5.m1.4.5" xref="S3.E5.m1.4.5.cmml"><msub id="S3.E5.m1.4.5.2" xref="S3.E5.m1.4.5.2.cmml"><mi id="S3.E5.m1.4.5.2.2" xref="S3.E5.m1.4.5.2.2.cmml">L</mi><mi id="S3.E5.m1.4.5.2.3" xref="S3.E5.m1.4.5.2.3.cmml">v</mi></msub><mo id="S3.E5.m1.4.5.1" xref="S3.E5.m1.4.5.1.cmml">=</mo><mfrac id="S3.E5.m1.4.4" xref="S3.E5.m1.4.4.cmml"><mrow id="S3.E5.m1.4.4.4" xref="S3.E5.m1.4.4.4.cmml"><msubsup id="S3.E5.m1.4.4.4.5" xref="S3.E5.m1.4.4.4.5.cmml"><mo id="S3.E5.m1.4.4.4.5.2.2" xref="S3.E5.m1.4.4.4.5.2.2.cmml">∑</mo><mi id="S3.E5.m1.4.4.4.5.2.3" xref="S3.E5.m1.4.4.4.5.2.3.cmml">i</mi><mi id="S3.E5.m1.4.4.4.5.3" xref="S3.E5.m1.4.4.4.5.3.cmml">n</mi></msubsup><mrow id="S3.E5.m1.4.4.4.4" xref="S3.E5.m1.4.4.4.4.cmml"><mi id="S3.E5.m1.4.4.4.4.4" xref="S3.E5.m1.4.4.4.4.4.cmml">H</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.4.4.4.4.3" xref="S3.E5.m1.4.4.4.4.3.cmml">​</mo><mrow id="S3.E5.m1.4.4.4.4.2.2" xref="S3.E5.m1.4.4.4.4.2.3.cmml"><mo stretchy="false" id="S3.E5.m1.4.4.4.4.2.2.3" xref="S3.E5.m1.4.4.4.4.2.3.cmml">(</mo><msub id="S3.E5.m1.3.3.3.3.1.1.1" xref="S3.E5.m1.3.3.3.3.1.1.1.cmml"><mi id="S3.E5.m1.3.3.3.3.1.1.1.2" xref="S3.E5.m1.3.3.3.3.1.1.1.2.cmml">v</mi><mi id="S3.E5.m1.3.3.3.3.1.1.1.3" xref="S3.E5.m1.3.3.3.3.1.1.1.3.cmml">i</mi></msub><mo id="S3.E5.m1.4.4.4.4.2.2.4" xref="S3.E5.m1.4.4.4.4.2.3.cmml">,</mo><msub id="S3.E5.m1.4.4.4.4.2.2.2" xref="S3.E5.m1.4.4.4.4.2.2.2.cmml"><mi id="S3.E5.m1.4.4.4.4.2.2.2.2" xref="S3.E5.m1.4.4.4.4.2.2.2.2.cmml">q</mi><mrow id="S3.E5.m1.2.2.2.2.2.2" xref="S3.E5.m1.2.2.2.2.2.3.cmml"><mi id="S3.E5.m1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.cmml">i</mi><mo id="S3.E5.m1.2.2.2.2.2.2.2" xref="S3.E5.m1.2.2.2.2.2.3.cmml">,</mo><mrow id="S3.E5.m1.2.2.2.2.2.2.1" xref="S3.E5.m1.2.2.2.2.2.2.1.cmml"><mi id="S3.E5.m1.2.2.2.2.2.2.1.2" xref="S3.E5.m1.2.2.2.2.2.2.1.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.2.2.2.2.2.2.1.1" xref="S3.E5.m1.2.2.2.2.2.2.1.1.cmml">​</mo><mi id="S3.E5.m1.2.2.2.2.2.2.1.3" xref="S3.E5.m1.2.2.2.2.2.2.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.2.2.2.2.2.2.1.1a" xref="S3.E5.m1.2.2.2.2.2.2.1.1.cmml">​</mo><mi id="S3.E5.m1.2.2.2.2.2.2.1.4" xref="S3.E5.m1.2.2.2.2.2.2.1.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.2.2.2.2.2.2.1.1b" xref="S3.E5.m1.2.2.2.2.2.2.1.1.cmml">​</mo><mi id="S3.E5.m1.2.2.2.2.2.2.1.5" xref="S3.E5.m1.2.2.2.2.2.2.1.5.cmml">e</mi></mrow></mrow></msub><mo stretchy="false" id="S3.E5.m1.4.4.4.4.2.2.5" xref="S3.E5.m1.4.4.4.4.2.3.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E5.m1.4.4.4.4.3a" xref="S3.E5.m1.4.4.4.4.3.cmml">​</mo><msub id="S3.E5.m1.4.4.4.4.5" xref="S3.E5.m1.4.4.4.4.5.cmml"><mi id="S3.E5.m1.4.4.4.4.5.2" xref="S3.E5.m1.4.4.4.4.5.2.cmml">s</mi><mi id="S3.E5.m1.4.4.4.4.5.3" xref="S3.E5.m1.4.4.4.4.5.3.cmml">i</mi></msub></mrow></mrow><mrow id="S3.E5.m1.4.4.6" xref="S3.E5.m1.4.4.6.cmml"><msubsup id="S3.E5.m1.4.4.6.1" xref="S3.E5.m1.4.4.6.1.cmml"><mo id="S3.E5.m1.4.4.6.1.2.2" xref="S3.E5.m1.4.4.6.1.2.2.cmml">∑</mo><mi id="S3.E5.m1.4.4.6.1.2.3" xref="S3.E5.m1.4.4.6.1.2.3.cmml">i</mi><mi id="S3.E5.m1.4.4.6.1.3" xref="S3.E5.m1.4.4.6.1.3.cmml">n</mi></msubsup><msub id="S3.E5.m1.4.4.6.2" xref="S3.E5.m1.4.4.6.2.cmml"><mi id="S3.E5.m1.4.4.6.2.2" xref="S3.E5.m1.4.4.6.2.2.cmml">s</mi><mi id="S3.E5.m1.4.4.6.2.3" xref="S3.E5.m1.4.4.6.2.3.cmml">i</mi></msub></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.4b"><apply id="S3.E5.m1.4.5.cmml" xref="S3.E5.m1.4.5"><eq id="S3.E5.m1.4.5.1.cmml" xref="S3.E5.m1.4.5.1"></eq><apply id="S3.E5.m1.4.5.2.cmml" xref="S3.E5.m1.4.5.2"><csymbol cd="ambiguous" id="S3.E5.m1.4.5.2.1.cmml" xref="S3.E5.m1.4.5.2">subscript</csymbol><ci id="S3.E5.m1.4.5.2.2.cmml" xref="S3.E5.m1.4.5.2.2">𝐿</ci><ci id="S3.E5.m1.4.5.2.3.cmml" xref="S3.E5.m1.4.5.2.3">𝑣</ci></apply><apply id="S3.E5.m1.4.4.cmml" xref="S3.E5.m1.4.4"><divide id="S3.E5.m1.4.4.5.cmml" xref="S3.E5.m1.4.4"></divide><apply id="S3.E5.m1.4.4.4.cmml" xref="S3.E5.m1.4.4.4"><apply id="S3.E5.m1.4.4.4.5.cmml" xref="S3.E5.m1.4.4.4.5"><csymbol cd="ambiguous" id="S3.E5.m1.4.4.4.5.1.cmml" xref="S3.E5.m1.4.4.4.5">superscript</csymbol><apply id="S3.E5.m1.4.4.4.5.2.cmml" xref="S3.E5.m1.4.4.4.5"><csymbol cd="ambiguous" id="S3.E5.m1.4.4.4.5.2.1.cmml" xref="S3.E5.m1.4.4.4.5">subscript</csymbol><sum id="S3.E5.m1.4.4.4.5.2.2.cmml" xref="S3.E5.m1.4.4.4.5.2.2"></sum><ci id="S3.E5.m1.4.4.4.5.2.3.cmml" xref="S3.E5.m1.4.4.4.5.2.3">𝑖</ci></apply><ci id="S3.E5.m1.4.4.4.5.3.cmml" xref="S3.E5.m1.4.4.4.5.3">𝑛</ci></apply><apply id="S3.E5.m1.4.4.4.4.cmml" xref="S3.E5.m1.4.4.4.4"><times id="S3.E5.m1.4.4.4.4.3.cmml" xref="S3.E5.m1.4.4.4.4.3"></times><ci id="S3.E5.m1.4.4.4.4.4.cmml" xref="S3.E5.m1.4.4.4.4.4">𝐻</ci><interval closure="open" id="S3.E5.m1.4.4.4.4.2.3.cmml" xref="S3.E5.m1.4.4.4.4.2.2"><apply id="S3.E5.m1.3.3.3.3.1.1.1.cmml" xref="S3.E5.m1.3.3.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.3.3.3.3.1.1.1.1.cmml" xref="S3.E5.m1.3.3.3.3.1.1.1">subscript</csymbol><ci id="S3.E5.m1.3.3.3.3.1.1.1.2.cmml" xref="S3.E5.m1.3.3.3.3.1.1.1.2">𝑣</ci><ci id="S3.E5.m1.3.3.3.3.1.1.1.3.cmml" xref="S3.E5.m1.3.3.3.3.1.1.1.3">𝑖</ci></apply><apply id="S3.E5.m1.4.4.4.4.2.2.2.cmml" xref="S3.E5.m1.4.4.4.4.2.2.2"><csymbol cd="ambiguous" id="S3.E5.m1.4.4.4.4.2.2.2.1.cmml" xref="S3.E5.m1.4.4.4.4.2.2.2">subscript</csymbol><ci id="S3.E5.m1.4.4.4.4.2.2.2.2.cmml" xref="S3.E5.m1.4.4.4.4.2.2.2.2">𝑞</ci><list id="S3.E5.m1.2.2.2.2.2.3.cmml" xref="S3.E5.m1.2.2.2.2.2.2"><ci id="S3.E5.m1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1">𝑖</ci><apply id="S3.E5.m1.2.2.2.2.2.2.1.cmml" xref="S3.E5.m1.2.2.2.2.2.2.1"><times id="S3.E5.m1.2.2.2.2.2.2.1.1.cmml" xref="S3.E5.m1.2.2.2.2.2.2.1.1"></times><ci id="S3.E5.m1.2.2.2.2.2.2.1.2.cmml" xref="S3.E5.m1.2.2.2.2.2.2.1.2">𝑣</ci><ci id="S3.E5.m1.2.2.2.2.2.2.1.3.cmml" xref="S3.E5.m1.2.2.2.2.2.2.1.3">𝑜</ci><ci id="S3.E5.m1.2.2.2.2.2.2.1.4.cmml" xref="S3.E5.m1.2.2.2.2.2.2.1.4">𝑡</ci><ci id="S3.E5.m1.2.2.2.2.2.2.1.5.cmml" xref="S3.E5.m1.2.2.2.2.2.2.1.5">𝑒</ci></apply></list></apply></interval><apply id="S3.E5.m1.4.4.4.4.5.cmml" xref="S3.E5.m1.4.4.4.4.5"><csymbol cd="ambiguous" id="S3.E5.m1.4.4.4.4.5.1.cmml" xref="S3.E5.m1.4.4.4.4.5">subscript</csymbol><ci id="S3.E5.m1.4.4.4.4.5.2.cmml" xref="S3.E5.m1.4.4.4.4.5.2">𝑠</ci><ci id="S3.E5.m1.4.4.4.4.5.3.cmml" xref="S3.E5.m1.4.4.4.4.5.3">𝑖</ci></apply></apply></apply><apply id="S3.E5.m1.4.4.6.cmml" xref="S3.E5.m1.4.4.6"><apply id="S3.E5.m1.4.4.6.1.cmml" xref="S3.E5.m1.4.4.6.1"><csymbol cd="ambiguous" id="S3.E5.m1.4.4.6.1.1.cmml" xref="S3.E5.m1.4.4.6.1">superscript</csymbol><apply id="S3.E5.m1.4.4.6.1.2.cmml" xref="S3.E5.m1.4.4.6.1"><csymbol cd="ambiguous" id="S3.E5.m1.4.4.6.1.2.1.cmml" xref="S3.E5.m1.4.4.6.1">subscript</csymbol><sum id="S3.E5.m1.4.4.6.1.2.2.cmml" xref="S3.E5.m1.4.4.6.1.2.2"></sum><ci id="S3.E5.m1.4.4.6.1.2.3.cmml" xref="S3.E5.m1.4.4.6.1.2.3">𝑖</ci></apply><ci id="S3.E5.m1.4.4.6.1.3.cmml" xref="S3.E5.m1.4.4.6.1.3">𝑛</ci></apply><apply id="S3.E5.m1.4.4.6.2.cmml" xref="S3.E5.m1.4.4.6.2"><csymbol cd="ambiguous" id="S3.E5.m1.4.4.6.2.1.cmml" xref="S3.E5.m1.4.4.6.2">subscript</csymbol><ci id="S3.E5.m1.4.4.6.2.2.cmml" xref="S3.E5.m1.4.4.6.2.2">𝑠</ci><ci id="S3.E5.m1.4.4.6.2.3.cmml" xref="S3.E5.m1.4.4.6.2.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.4c">L_{v}=\dfrac{\sum_{i}^{n}H(v_{i},q_{i,vote})s_{i}}{\sum_{i}^{n}s_{i}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS6.p7" class="ltx_para">
<p id="S3.SS6.p7.1" class="ltx_p">The network is trained with a batch size of 48 over 40 epochs. For each object, the dataset consists of 40000 point clouds, making the complete number of training steps 1600000. The learning rate starts at 0.001 and is clipped at 0.00005, with a decay rate of 0.5 at every 337620 steps.
Batch normalization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> is added to all convolutional layers in the network, with parameters set according to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Evaluation</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">To verify the effectiveness of our developed method, and the ability to generalize to real data, we test on several benchmarking datasets. The methods compared against are all explained in Sec. <a href="#S2" title="2 Related Work ‣ Bridging the Reality Gap for Pose Estimation Networks using Sensor-Based Domain Randomization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
The method is tested on the popular LM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and LMO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> datasets. As the synthetic data is obtained using the method introduced for the BOP challenge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, the method is also compared with other methods using this synthetic data. The same trained weights were used to test both the LM and the LMO dataset, and the same weights were also used for the LM and LMO parts of the BOP challenge. An ablation study is also performed to verify the effect of our contributions, the sensor-based domain randomization, and the SparseEdge feature.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.2.1.1" class="ltx_tr">
<th id="S4.T1.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">
<table id="S4.T1.2.1.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.2.1.1.1.1.1" class="ltx_tr">
<td id="S4.T1.2.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T1.2.1.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Training</span></td>
</tr>
<tr id="S4.T1.2.1.1.1.1.2" class="ltx_tr">
<td id="S4.T1.2.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T1.2.1.1.1.1.2.1.1" class="ltx_text" style="font-size:70%;">Data</span></td>
</tr>
</table>
</th>
<th id="S4.T1.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t" colspan="5"><span id="S4.T1.2.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Real</span></th>
<th id="S4.T1.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3"><span id="S4.T1.2.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Synthetic</span></th>
</tr>
<tr id="S4.T1.2.2.2" class="ltx_tr">
<th id="S4.T1.2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T1.2.2.2.1.1" class="ltx_text" style="font-size:70%;">Modality</span></th>
<th id="S4.T1.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.2.2.2.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">RGB</span></th>
<th id="S4.T1.2.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t" colspan="4"><span id="S4.T1.2.2.2.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">RGB-D</span></th>
<th id="S4.T1.2.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3"><span id="S4.T1.2.2.2.4.1" class="ltx_text ltx_font_bold" style="font-size:70%;">RGB-D</span></th>
</tr>
<tr id="S4.T1.2.3.3" class="ltx_tr">
<th id="S4.T1.2.3.3.1" class="ltx_td ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="S4.T1.2.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.2.3.3.2.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib24" title="" class="ltx_ref">24</a><span id="S4.T1.2.3.3.2.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></th>
<th id="S4.T1.2.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.2.3.3.3.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib32" title="" class="ltx_ref">32</a><span id="S4.T1.2.3.3.3.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></th>
<th id="S4.T1.2.3.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.2.3.3.4.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib35" title="" class="ltx_ref">35</a><span id="S4.T1.2.3.3.4.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></th>
<th id="S4.T1.2.3.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.2.3.3.5.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib7" title="" class="ltx_ref">7</a><span id="S4.T1.2.3.3.5.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></th>
<th id="S4.T1.2.3.3.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.2.3.3.6.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a><span id="S4.T1.2.3.3.6.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></th>
<th id="S4.T1.2.3.3.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.2.3.3.7.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib35" title="" class="ltx_ref">35</a><span id="S4.T1.2.3.3.7.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></th>
<th id="S4.T1.2.3.3.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.2.3.3.8.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib19" title="" class="ltx_ref">19</a><span id="S4.T1.2.3.3.8.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></th>
<th id="S4.T1.2.3.3.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.2.3.3.9.1" class="ltx_text" style="font-size:70%;">Ours</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.2.4.1" class="ltx_tr">
<td id="S4.T1.2.4.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T1.2.4.1.1.1" class="ltx_text" style="font-size:70%;">Ape</span></td>
<td id="S4.T1.2.4.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.4.1.2.1" class="ltx_text" style="font-size:70%;">43.6</span></td>
<td id="S4.T1.2.4.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.4.1.3.1" class="ltx_text" style="font-size:70%;">92</span></td>
<td id="S4.T1.2.4.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.4.1.4.1" class="ltx_text" style="font-size:70%;">87.7</span></td>
<td id="S4.T1.2.4.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.4.1.5.1" class="ltx_text" style="font-size:70%;">80.7</span></td>
<td id="S4.T1.2.4.1.6" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="S4.T1.2.4.1.6.1" class="ltx_text" style="font-size:70%;">97.3</span></td>
<td id="S4.T1.2.4.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.4.1.7.1" class="ltx_text" style="font-size:70%;">55.2</span></td>
<td id="S4.T1.2.4.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.4.1.8.1" class="ltx_text" style="font-size:70%;">65</span></td>
<td id="S4.T1.2.4.1.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.4.1.9.1" class="ltx_text" style="font-size:70%;">97.7</span></td>
</tr>
<tr id="S4.T1.2.5.2" class="ltx_tr">
<td id="S4.T1.2.5.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><span id="S4.T1.2.5.2.1.1" class="ltx_text" style="font-size:70%;">Bench v.</span></td>
<td id="S4.T1.2.5.2.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.5.2.2.1" class="ltx_text" style="font-size:70%;">99.9</span></td>
<td id="S4.T1.2.5.2.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.5.2.3.1" class="ltx_text" style="font-size:70%;">93</span></td>
<td id="S4.T1.2.5.2.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.5.2.4.1" class="ltx_text" style="font-size:70%;">98.5</span></td>
<td id="S4.T1.2.5.2.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.5.2.5.1" class="ltx_text" style="font-size:70%;">100</span></td>
<td id="S4.T1.2.5.2.6" class="ltx_td ltx_align_center ltx_border_rr"><span id="S4.T1.2.5.2.6.1" class="ltx_text" style="font-size:70%;">99.7</span></td>
<td id="S4.T1.2.5.2.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.5.2.7.1" class="ltx_text" style="font-size:70%;">72.7</span></td>
<td id="S4.T1.2.5.2.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.5.2.8.1" class="ltx_text" style="font-size:70%;">80</span></td>
<td id="S4.T1.2.5.2.9" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.5.2.9.1" class="ltx_text" style="font-size:70%;">99.8</span></td>
</tr>
<tr id="S4.T1.2.6.3" class="ltx_tr">
<td id="S4.T1.2.6.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><span id="S4.T1.2.6.3.1.1" class="ltx_text" style="font-size:70%;">Camera</span></td>
<td id="S4.T1.2.6.3.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.6.3.2.1" class="ltx_text" style="font-size:70%;">86.9</span></td>
<td id="S4.T1.2.6.3.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.6.3.3.1" class="ltx_text" style="font-size:70%;">94</span></td>
<td id="S4.T1.2.6.3.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.6.3.4.1" class="ltx_text" style="font-size:70%;">96.1</span></td>
<td id="S4.T1.2.6.3.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.6.3.5.1" class="ltx_text" style="font-size:70%;">100</span></td>
<td id="S4.T1.2.6.3.6" class="ltx_td ltx_align_center ltx_border_rr"><span id="S4.T1.2.6.3.6.1" class="ltx_text" style="font-size:70%;">99.6</span></td>
<td id="S4.T1.2.6.3.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.6.3.7.1" class="ltx_text" style="font-size:70%;">34.8</span></td>
<td id="S4.T1.2.6.3.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.6.3.8.1" class="ltx_text" style="font-size:70%;">78</span></td>
<td id="S4.T1.2.6.3.9" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.6.3.9.1" class="ltx_text" style="font-size:70%;">98.3</span></td>
</tr>
<tr id="S4.T1.2.7.4" class="ltx_tr">
<td id="S4.T1.2.7.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><span id="S4.T1.2.7.4.1.1" class="ltx_text" style="font-size:70%;">Can</span></td>
<td id="S4.T1.2.7.4.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.7.4.2.1" class="ltx_text" style="font-size:70%;">95.5</span></td>
<td id="S4.T1.2.7.4.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.7.4.3.1" class="ltx_text" style="font-size:70%;">93</span></td>
<td id="S4.T1.2.7.4.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.7.4.4.1" class="ltx_text" style="font-size:70%;">99.7</span></td>
<td id="S4.T1.2.7.4.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.7.4.5.1" class="ltx_text" style="font-size:70%;">99.7</span></td>
<td id="S4.T1.2.7.4.6" class="ltx_td ltx_align_center ltx_border_rr"><span id="S4.T1.2.7.4.6.1" class="ltx_text" style="font-size:70%;">99.5</span></td>
<td id="S4.T1.2.7.4.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.7.4.7.1" class="ltx_text" style="font-size:70%;">83.6</span></td>
<td id="S4.T1.2.7.4.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.7.4.8.1" class="ltx_text" style="font-size:70%;">86</span></td>
<td id="S4.T1.2.7.4.9" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.7.4.9.1" class="ltx_text" style="font-size:70%;">98.8</span></td>
</tr>
<tr id="S4.T1.2.8.5" class="ltx_tr">
<td id="S4.T1.2.8.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><span id="S4.T1.2.8.5.1.1" class="ltx_text" style="font-size:70%;">Cat</span></td>
<td id="S4.T1.2.8.5.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.8.5.2.1" class="ltx_text" style="font-size:70%;">79.3</span></td>
<td id="S4.T1.2.8.5.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.8.5.3.1" class="ltx_text" style="font-size:70%;">97</span></td>
<td id="S4.T1.2.8.5.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.8.5.4.1" class="ltx_text" style="font-size:70%;">94.7</span></td>
<td id="S4.T1.2.8.5.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.8.5.5.1" class="ltx_text" style="font-size:70%;">99.8</span></td>
<td id="S4.T1.2.8.5.6" class="ltx_td ltx_align_center ltx_border_rr"><span id="S4.T1.2.8.5.6.1" class="ltx_text" style="font-size:70%;">99.8</span></td>
<td id="S4.T1.2.8.5.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.8.5.7.1" class="ltx_text" style="font-size:70%;">65.1</span></td>
<td id="S4.T1.2.8.5.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.8.5.8.1" class="ltx_text" style="font-size:70%;">70</span></td>
<td id="S4.T1.2.8.5.9" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.8.5.9.1" class="ltx_text" style="font-size:70%;">99.9</span></td>
</tr>
<tr id="S4.T1.2.9.6" class="ltx_tr">
<td id="S4.T1.2.9.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><span id="S4.T1.2.9.6.1.1" class="ltx_text" style="font-size:70%;">Driller</span></td>
<td id="S4.T1.2.9.6.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.9.6.2.1" class="ltx_text" style="font-size:70%;">96.4</span></td>
<td id="S4.T1.2.9.6.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.9.6.3.1" class="ltx_text" style="font-size:70%;">87</span></td>
<td id="S4.T1.2.9.6.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.9.6.4.1" class="ltx_text" style="font-size:70%;">98.8</span></td>
<td id="S4.T1.2.9.6.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.9.6.5.1" class="ltx_text" style="font-size:70%;">99.9</span></td>
<td id="S4.T1.2.9.6.6" class="ltx_td ltx_align_center ltx_border_rr"><span id="S4.T1.2.9.6.6.1" class="ltx_text" style="font-size:70%;">99.3</span></td>
<td id="S4.T1.2.9.6.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.9.6.7.1" class="ltx_text" style="font-size:70%;">73.3</span></td>
<td id="S4.T1.2.9.6.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.9.6.8.1" class="ltx_text" style="font-size:70%;">73</span></td>
<td id="S4.T1.2.9.6.9" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.9.6.9.1" class="ltx_text" style="font-size:70%;">99.2</span></td>
</tr>
<tr id="S4.T1.2.10.7" class="ltx_tr">
<td id="S4.T1.2.10.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><span id="S4.T1.2.10.7.1.1" class="ltx_text" style="font-size:70%;">Duck</span></td>
<td id="S4.T1.2.10.7.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.10.7.2.1" class="ltx_text" style="font-size:70%;">52.6</span></td>
<td id="S4.T1.2.10.7.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.10.7.3.1" class="ltx_text" style="font-size:70%;">92</span></td>
<td id="S4.T1.2.10.7.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.10.7.4.1" class="ltx_text" style="font-size:70%;">86.3</span></td>
<td id="S4.T1.2.10.7.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.10.7.5.1" class="ltx_text" style="font-size:70%;">97.9</span></td>
<td id="S4.T1.2.10.7.6" class="ltx_td ltx_align_center ltx_border_rr"><span id="S4.T1.2.10.7.6.1" class="ltx_text" style="font-size:70%;">98.2</span></td>
<td id="S4.T1.2.10.7.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.10.7.7.1" class="ltx_text" style="font-size:70%;">50.0</span></td>
<td id="S4.T1.2.10.7.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.10.7.8.1" class="ltx_text" style="font-size:70%;">66</span></td>
<td id="S4.T1.2.10.7.9" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.10.7.9.1" class="ltx_text" style="font-size:70%;">97.8</span></td>
</tr>
<tr id="S4.T1.2.11.8" class="ltx_tr">
<td id="S4.T1.2.11.8.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><span id="S4.T1.2.11.8.1.1" class="ltx_text" style="font-size:70%;">Eggbox*</span></td>
<td id="S4.T1.2.11.8.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.11.8.2.1" class="ltx_text" style="font-size:70%;">99.2</span></td>
<td id="S4.T1.2.11.8.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.11.8.3.1" class="ltx_text" style="font-size:70%;">100</span></td>
<td id="S4.T1.2.11.8.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.11.8.4.1" class="ltx_text" style="font-size:70%;">99.9</span></td>
<td id="S4.T1.2.11.8.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.11.8.5.1" class="ltx_text" style="font-size:70%;">99.9</span></td>
<td id="S4.T1.2.11.8.6" class="ltx_td ltx_align_center ltx_border_rr"><span id="S4.T1.2.11.8.6.1" class="ltx_text" style="font-size:70%;">99.8</span></td>
<td id="S4.T1.2.11.8.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.11.8.7.1" class="ltx_text" style="font-size:70%;">89.1</span></td>
<td id="S4.T1.2.11.8.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.11.8.8.1" class="ltx_text" style="font-size:70%;">100</span></td>
<td id="S4.T1.2.11.8.9" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.11.8.9.1" class="ltx_text" style="font-size:70%;">97.7</span></td>
</tr>
<tr id="S4.T1.2.12.9" class="ltx_tr">
<td id="S4.T1.2.12.9.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><span id="S4.T1.2.12.9.1.1" class="ltx_text" style="font-size:70%;">Glue*</span></td>
<td id="S4.T1.2.12.9.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.12.9.2.1" class="ltx_text" style="font-size:70%;">95.7</span></td>
<td id="S4.T1.2.12.9.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.12.9.3.1" class="ltx_text" style="font-size:70%;">100</span></td>
<td id="S4.T1.2.12.9.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.12.9.4.1" class="ltx_text" style="font-size:70%;">96.8</span></td>
<td id="S4.T1.2.12.9.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.12.9.5.1" class="ltx_text" style="font-size:70%;">84.4</span></td>
<td id="S4.T1.2.12.9.6" class="ltx_td ltx_align_center ltx_border_rr"><span id="S4.T1.2.12.9.6.1" class="ltx_text" style="font-size:70%;">100</span></td>
<td id="S4.T1.2.12.9.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.12.9.7.1" class="ltx_text" style="font-size:70%;">84.4</span></td>
<td id="S4.T1.2.12.9.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.12.9.8.1" class="ltx_text" style="font-size:70%;">100</span></td>
<td id="S4.T1.2.12.9.9" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.12.9.9.1" class="ltx_text" style="font-size:70%;">98.9</span></td>
</tr>
<tr id="S4.T1.2.13.10" class="ltx_tr">
<td id="S4.T1.2.13.10.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><span id="S4.T1.2.13.10.1.1" class="ltx_text" style="font-size:70%;">Hole p.</span></td>
<td id="S4.T1.2.13.10.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.13.10.2.1" class="ltx_text" style="font-size:70%;">81.9</span></td>
<td id="S4.T1.2.13.10.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.13.10.3.1" class="ltx_text" style="font-size:70%;">92</span></td>
<td id="S4.T1.2.13.10.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.13.10.4.1" class="ltx_text" style="font-size:70%;">86.9</span></td>
<td id="S4.T1.2.13.10.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.13.10.5.1" class="ltx_text" style="font-size:70%;">92.8</span></td>
<td id="S4.T1.2.13.10.6" class="ltx_td ltx_align_center ltx_border_rr"><span id="S4.T1.2.13.10.6.1" class="ltx_text" style="font-size:70%;">99.9</span></td>
<td id="S4.T1.2.13.10.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.13.10.7.1" class="ltx_text" style="font-size:70%;">35.4</span></td>
<td id="S4.T1.2.13.10.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.13.10.8.1" class="ltx_text" style="font-size:70%;">49</span></td>
<td id="S4.T1.2.13.10.9" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.13.10.9.1" class="ltx_text" style="font-size:70%;">94.1</span></td>
</tr>
<tr id="S4.T1.2.14.11" class="ltx_tr">
<td id="S4.T1.2.14.11.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><span id="S4.T1.2.14.11.1.1" class="ltx_text" style="font-size:70%;">Iron</span></td>
<td id="S4.T1.2.14.11.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.14.11.2.1" class="ltx_text" style="font-size:70%;">98.9</span></td>
<td id="S4.T1.2.14.11.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.14.11.3.1" class="ltx_text" style="font-size:70%;">97</span></td>
<td id="S4.T1.2.14.11.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.14.11.4.1" class="ltx_text" style="font-size:70%;">100</span></td>
<td id="S4.T1.2.14.11.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.14.11.5.1" class="ltx_text" style="font-size:70%;">100</span></td>
<td id="S4.T1.2.14.11.6" class="ltx_td ltx_align_center ltx_border_rr"><span id="S4.T1.2.14.11.6.1" class="ltx_text" style="font-size:70%;">99.7</span></td>
<td id="S4.T1.2.14.11.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.14.11.7.1" class="ltx_text" style="font-size:70%;">98.8</span></td>
<td id="S4.T1.2.14.11.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.14.11.8.1" class="ltx_text" style="font-size:70%;">78</span></td>
<td id="S4.T1.2.14.11.9" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.14.11.9.1" class="ltx_text" style="font-size:70%;">100</span></td>
</tr>
<tr id="S4.T1.2.15.12" class="ltx_tr">
<td id="S4.T1.2.15.12.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><span id="S4.T1.2.15.12.1.1" class="ltx_text" style="font-size:70%;">Lamp</span></td>
<td id="S4.T1.2.15.12.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.15.12.2.1" class="ltx_text" style="font-size:70%;">99.3</span></td>
<td id="S4.T1.2.15.12.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.15.12.3.1" class="ltx_text" style="font-size:70%;">95</span></td>
<td id="S4.T1.2.15.12.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.15.12.4.1" class="ltx_text" style="font-size:70%;">96.8</span></td>
<td id="S4.T1.2.15.12.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.15.12.5.1" class="ltx_text" style="font-size:70%;">100</span></td>
<td id="S4.T1.2.15.12.6" class="ltx_td ltx_align_center ltx_border_rr"><span id="S4.T1.2.15.12.6.1" class="ltx_text" style="font-size:70%;">99.8</span></td>
<td id="S4.T1.2.15.12.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.15.12.7.1" class="ltx_text" style="font-size:70%;">74.3</span></td>
<td id="S4.T1.2.15.12.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.15.12.8.1" class="ltx_text" style="font-size:70%;">73</span></td>
<td id="S4.T1.2.15.12.9" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.15.12.9.1" class="ltx_text" style="font-size:70%;">92.8</span></td>
</tr>
<tr id="S4.T1.2.16.13" class="ltx_tr">
<td id="S4.T1.2.16.13.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><span id="S4.T1.2.16.13.1.1" class="ltx_text" style="font-size:70%;">Phone</span></td>
<td id="S4.T1.2.16.13.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.16.13.2.1" class="ltx_text" style="font-size:70%;">92.4</span></td>
<td id="S4.T1.2.16.13.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.16.13.3.1" class="ltx_text" style="font-size:70%;">93</span></td>
<td id="S4.T1.2.16.13.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.16.13.4.1" class="ltx_text" style="font-size:70%;">94.7</span></td>
<td id="S4.T1.2.16.13.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.16.13.5.1" class="ltx_text" style="font-size:70%;">96.2</span></td>
<td id="S4.T1.2.16.13.6" class="ltx_td ltx_align_center ltx_border_rr"><span id="S4.T1.2.16.13.6.1" class="ltx_text" style="font-size:70%;">99.5</span></td>
<td id="S4.T1.2.16.13.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.16.13.7.1" class="ltx_text" style="font-size:70%;">47.0</span></td>
<td id="S4.T1.2.16.13.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.16.13.8.1" class="ltx_text" style="font-size:70%;">79</span></td>
<td id="S4.T1.2.16.13.9" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.2.16.13.9.1" class="ltx_text" style="font-size:70%;">99.1</span></td>
</tr>
<tr id="S4.T1.2.17.14" class="ltx_tr">
<td id="S4.T1.2.17.14.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T1.2.17.14.1.1" class="ltx_text" style="font-size:70%;">Average</span></td>
<td id="S4.T1.2.17.14.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.2.17.14.2.1" class="ltx_text" style="font-size:70%;">86.3</span></td>
<td id="S4.T1.2.17.14.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.2.17.14.3.1" class="ltx_text" style="font-size:70%;">94.3</span></td>
<td id="S4.T1.2.17.14.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.2.17.14.4.1" class="ltx_text" style="font-size:70%;">95.15</span></td>
<td id="S4.T1.2.17.14.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.2.17.14.5.1" class="ltx_text" style="font-size:70%;">96.3</span></td>
<td id="S4.T1.2.17.14.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr ltx_border_t"><span id="S4.T1.2.17.14.6.1" class="ltx_text" style="font-size:70%;">99.4</span></td>
<td id="S4.T1.2.17.14.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.2.17.14.7.1" class="ltx_text" style="font-size:70%;">66.4</span></td>
<td id="S4.T1.2.17.14.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.2.17.14.8.1" class="ltx_text" style="font-size:70%;">79</span></td>
<td id="S4.T1.2.17.14.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.2.17.14.9.1" class="ltx_text" style="font-size:70%;">98.0</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.29.1.1" class="ltx_text" style="font-size:129%;">Table 1</span>: </span><span id="S4.T1.30.2" class="ltx_text" style="font-size:129%;"> Results for the LM dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> in % accuracy with the ADD/I score. The competing methods are DPOD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, SSD-6D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> (obtained from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>), PVNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, DenseFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, PointVoteNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and PVN3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. Rotation invariant objects are marked with an *. </span></figcaption>
</figure>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.2.1.1" class="ltx_tr">
<th id="S4.T2.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">
<table id="S4.T2.2.1.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.2.1.1.1.1.1" class="ltx_tr">
<td id="S4.T2.2.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.2.1.1.1.1.1.1.1" class="ltx_text" style="font-size:90%;">Training</span></td>
</tr>
<tr id="S4.T2.2.1.1.1.1.2" class="ltx_tr">
<td id="S4.T2.2.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.2.1.1.1.1.2.1.1" class="ltx_text" style="font-size:90%;">Data</span></td>
</tr>
</table>
</th>
<th id="S4.T2.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t" colspan="5"><span id="S4.T2.2.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Real</span></th>
<th id="S4.T2.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T2.2.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Synthetic</span></th>
</tr>
<tr id="S4.T2.2.2.2" class="ltx_tr">
<th id="S4.T2.2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.2.2.2.1.1" class="ltx_text" style="font-size:90%;">Modality</span></th>
<th id="S4.T2.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2"><span id="S4.T2.2.2.2.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">RGB</span></th>
<th id="S4.T2.2.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t" colspan="3"><span id="S4.T2.2.2.2.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">RGB-D</span></th>
<th id="S4.T2.2.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T2.2.2.2.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">RGB-D</span></th>
</tr>
<tr id="S4.T2.2.3.3" class="ltx_tr">
<th id="S4.T2.2.3.3.1" class="ltx_td ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="S4.T2.2.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.2.3.3.2.1.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib34" title="" class="ltx_ref">34</a><span id="S4.T2.2.3.3.2.2.2" class="ltx_text" style="font-size:90%;">]</span></cite></th>
<th id="S4.T2.2.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.2.3.3.3.1.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib24" title="" class="ltx_ref">24</a><span id="S4.T2.2.3.3.3.2.2" class="ltx_text" style="font-size:90%;">]</span></cite></th>
<th id="S4.T2.2.3.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.2.3.3.4.1.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib34" title="" class="ltx_ref">34</a><span id="S4.T2.2.3.3.4.2.2" class="ltx_text" style="font-size:90%;">]</span></cite></th>
<th id="S4.T2.2.3.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.2.3.3.5.1.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib7" title="" class="ltx_ref">7</a><span id="S4.T2.2.3.3.5.2.2" class="ltx_text" style="font-size:90%;">]</span></cite></th>
<th id="S4.T2.2.3.3.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.2.3.3.6.1.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a><span id="S4.T2.2.3.3.6.2.2" class="ltx_text" style="font-size:90%;">]</span></cite></th>
<th id="S4.T2.2.3.3.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T2.2.3.3.7.1" class="ltx_text" style="font-size:90%;">Ours</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.2.4.1" class="ltx_tr">
<td id="S4.T2.2.4.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.2.4.1.1.1" class="ltx_text" style="font-size:90%;">Ape</span></td>
<td id="S4.T2.2.4.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.4.1.2.1" class="ltx_text" style="font-size:90%;">9.60</span></td>
<td id="S4.T2.2.4.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.4.1.3.1" class="ltx_text" style="font-size:90%;">15.0</span></td>
<td id="S4.T2.2.4.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.4.1.4.1" class="ltx_text" style="font-size:90%;">76.2</span></td>
<td id="S4.T2.2.4.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.4.1.5.1" class="ltx_text" style="font-size:90%;">70.0</span></td>
<td id="S4.T2.2.4.1.6" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="S4.T2.2.4.1.6.1" class="ltx_text" style="font-size:90%;">33.9</span></td>
<td id="S4.T2.2.4.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.4.1.7.1" class="ltx_text" style="font-size:90%;">66.1</span></td>
</tr>
<tr id="S4.T2.2.5.2" class="ltx_tr">
<td id="S4.T2.2.5.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><span id="S4.T2.2.5.2.1.1" class="ltx_text" style="font-size:90%;">Can</span></td>
<td id="S4.T2.2.5.2.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.5.2.2.1" class="ltx_text" style="font-size:90%;">45.2</span></td>
<td id="S4.T2.2.5.2.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.5.2.3.1" class="ltx_text" style="font-size:90%;">63.0</span></td>
<td id="S4.T2.2.5.2.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.5.2.4.1" class="ltx_text" style="font-size:90%;">87.4</span></td>
<td id="S4.T2.2.5.2.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.5.2.5.1" class="ltx_text" style="font-size:90%;">95.5</span></td>
<td id="S4.T2.2.5.2.6" class="ltx_td ltx_align_center ltx_border_rr"><span id="S4.T2.2.5.2.6.1" class="ltx_text" style="font-size:90%;">88.6</span></td>
<td id="S4.T2.2.5.2.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.5.2.7.1" class="ltx_text" style="font-size:90%;">91.5</span></td>
</tr>
<tr id="S4.T2.2.6.3" class="ltx_tr">
<td id="S4.T2.2.6.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><span id="S4.T2.2.6.3.1.1" class="ltx_text" style="font-size:90%;">Cat</span></td>
<td id="S4.T2.2.6.3.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.6.3.2.1" class="ltx_text" style="font-size:90%;">0.93</span></td>
<td id="S4.T2.2.6.3.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.6.3.3.1" class="ltx_text" style="font-size:90%;">16.0</span></td>
<td id="S4.T2.2.6.3.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.6.3.4.1" class="ltx_text" style="font-size:90%;">52.2</span></td>
<td id="S4.T2.2.6.3.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.6.3.5.1" class="ltx_text" style="font-size:90%;">60.8</span></td>
<td id="S4.T2.2.6.3.6" class="ltx_td ltx_align_center ltx_border_rr"><span id="S4.T2.2.6.3.6.1" class="ltx_text" style="font-size:90%;">39.1</span></td>
<td id="S4.T2.2.6.3.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.6.3.7.1" class="ltx_text" style="font-size:90%;">60.7</span></td>
</tr>
<tr id="S4.T2.2.7.4" class="ltx_tr">
<td id="S4.T2.2.7.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><span id="S4.T2.2.7.4.1.1" class="ltx_text" style="font-size:90%;">Driller</span></td>
<td id="S4.T2.2.7.4.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.7.4.2.1" class="ltx_text" style="font-size:90%;">41.4</span></td>
<td id="S4.T2.2.7.4.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.7.4.3.1" class="ltx_text" style="font-size:90%;">25.0</span></td>
<td id="S4.T2.2.7.4.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.7.4.4.1" class="ltx_text" style="font-size:90%;">90.3</span></td>
<td id="S4.T2.2.7.4.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.7.4.5.1" class="ltx_text" style="font-size:90%;">87.9</span></td>
<td id="S4.T2.2.7.4.6" class="ltx_td ltx_align_center ltx_border_rr"><span id="S4.T2.2.7.4.6.1" class="ltx_text" style="font-size:90%;">78.4</span></td>
<td id="S4.T2.2.7.4.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.7.4.7.1" class="ltx_text" style="font-size:90%;">92.8</span></td>
</tr>
<tr id="S4.T2.2.8.5" class="ltx_tr">
<td id="S4.T2.2.8.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><span id="S4.T2.2.8.5.1.1" class="ltx_text" style="font-size:90%;">Duck</span></td>
<td id="S4.T2.2.8.5.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.8.5.2.1" class="ltx_text" style="font-size:90%;">19.6</span></td>
<td id="S4.T2.2.8.5.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.8.5.3.1" class="ltx_text" style="font-size:90%;">65.0</span></td>
<td id="S4.T2.2.8.5.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.8.5.4.1" class="ltx_text" style="font-size:90%;">77.7</span></td>
<td id="S4.T2.2.8.5.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.8.5.5.1" class="ltx_text" style="font-size:90%;">70.7</span></td>
<td id="S4.T2.2.8.5.6" class="ltx_td ltx_align_center ltx_border_rr"><span id="S4.T2.2.8.5.6.1" class="ltx_text" style="font-size:90%;">41.9</span></td>
<td id="S4.T2.2.8.5.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.8.5.7.1" class="ltx_text" style="font-size:90%;">71.2</span></td>
</tr>
<tr id="S4.T2.2.9.6" class="ltx_tr">
<td id="S4.T2.2.9.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><span id="S4.T2.2.9.6.1.1" class="ltx_text" style="font-size:90%;">Eggbox*</span></td>
<td id="S4.T2.2.9.6.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.9.6.2.1" class="ltx_text" style="font-size:90%;">22.0</span></td>
<td id="S4.T2.2.9.6.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.9.6.3.1" class="ltx_text" style="font-size:90%;">50.0</span></td>
<td id="S4.T2.2.9.6.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.9.6.4.1" class="ltx_text" style="font-size:90%;">72.2</span></td>
<td id="S4.T2.2.9.6.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.9.6.5.1" class="ltx_text" style="font-size:90%;">58.7</span></td>
<td id="S4.T2.2.9.6.6" class="ltx_td ltx_align_center ltx_border_rr"><span id="S4.T2.2.9.6.6.1" class="ltx_text" style="font-size:90%;">80.9</span></td>
<td id="S4.T2.2.9.6.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.9.6.7.1" class="ltx_text" style="font-size:90%;">69.7</span></td>
</tr>
<tr id="S4.T2.2.10.7" class="ltx_tr">
<td id="S4.T2.2.10.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><span id="S4.T2.2.10.7.1.1" class="ltx_text" style="font-size:90%;">Glue*</span></td>
<td id="S4.T2.2.10.7.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.10.7.2.1" class="ltx_text" style="font-size:90%;">38.5</span></td>
<td id="S4.T2.2.10.7.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.10.7.3.1" class="ltx_text" style="font-size:90%;">49.0</span></td>
<td id="S4.T2.2.10.7.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.10.7.4.1" class="ltx_text" style="font-size:90%;">76.7</span></td>
<td id="S4.T2.2.10.7.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.10.7.5.1" class="ltx_text" style="font-size:90%;">66.9</span></td>
<td id="S4.T2.2.10.7.6" class="ltx_td ltx_align_center ltx_border_rr"><span id="S4.T2.2.10.7.6.1" class="ltx_text" style="font-size:90%;">68.1</span></td>
<td id="S4.T2.2.10.7.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.10.7.7.1" class="ltx_text" style="font-size:90%;">71.5</span></td>
</tr>
<tr id="S4.T2.2.11.8" class="ltx_tr">
<td id="S4.T2.2.11.8.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><span id="S4.T2.2.11.8.1.1" class="ltx_text" style="font-size:90%;">Hole p.</span></td>
<td id="S4.T2.2.11.8.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.11.8.2.1" class="ltx_text" style="font-size:90%;">22.1</span></td>
<td id="S4.T2.2.11.8.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.11.8.3.1" class="ltx_text" style="font-size:90%;">39.0</span></td>
<td id="S4.T2.2.11.8.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.11.8.4.1" class="ltx_text" style="font-size:90%;">91.4</span></td>
<td id="S4.T2.2.11.8.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.11.8.5.1" class="ltx_text" style="font-size:90%;">90.6</span></td>
<td id="S4.T2.2.11.8.6" class="ltx_td ltx_align_center ltx_border_rr"><span id="S4.T2.2.11.8.6.1" class="ltx_text" style="font-size:90%;">74.7</span></td>
<td id="S4.T2.2.11.8.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.11.8.7.1" class="ltx_text" style="font-size:90%;">91.5</span></td>
</tr>
<tr id="S4.T2.2.12.9" class="ltx_tr">
<td id="S4.T2.2.12.9.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.2.12.9.1.1" class="ltx_text" style="font-size:90%;">Average</span></td>
<td id="S4.T2.2.12.9.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T2.2.12.9.2.1" class="ltx_text" style="font-size:90%;">24.9</span></td>
<td id="S4.T2.2.12.9.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T2.2.12.9.3.1" class="ltx_text" style="font-size:90%;">40.8</span></td>
<td id="S4.T2.2.12.9.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T2.2.12.9.4.1" class="ltx_text" style="font-size:90%;">78.0</span></td>
<td id="S4.T2.2.12.9.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T2.2.12.9.5.1" class="ltx_text" style="font-size:90%;">75.1</span></td>
<td id="S4.T2.2.12.9.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr ltx_border_t"><span id="S4.T2.2.12.9.6.1" class="ltx_text" style="font-size:90%;">63.2</span></td>
<td id="S4.T2.2.12.9.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T2.2.12.9.7.1" class="ltx_text" style="font-size:90%;">77.2</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 2: </span> Results on the LMO dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> in % accuracy with the ADD/I score. The score for <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> is from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. Rotation invariant objects are marked with an *.</figcaption>
</figure>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.2.1.1" class="ltx_tr">
<th id="S4.T3.2.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="S4.T3.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T3.2.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">RGB</span></th>
<th id="S4.T3.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T3.2.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">RGB</span></th>
<th id="S4.T3.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T3.2.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">RGB-D</span></th>
<th id="S4.T3.2.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T3.2.1.1.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">RGB</span></th>
<th id="S4.T3.2.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T3.2.1.1.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">D</span></th>
<th id="S4.T3.2.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T3.2.1.1.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;">RGB-D</span></th>
</tr>
<tr id="S4.T3.2.2.2" class="ltx_tr">
<th id="S4.T3.2.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="S4.T3.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.2.2.2.2.1.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib13" title="" class="ltx_ref">13</a><span id="S4.T3.2.2.2.2.2.2" class="ltx_text" style="font-size:90%;">]</span></cite></th>
<th id="S4.T3.2.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.2.2.2.3.1.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib21" title="" class="ltx_ref">21</a><span id="S4.T3.2.2.2.3.2.2" class="ltx_text" style="font-size:90%;">]</span></cite></th>
<th id="S4.T3.2.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.2.2.2.4.1.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib21" title="" class="ltx_ref">21</a><span id="S4.T3.2.2.2.4.2.2" class="ltx_text" style="font-size:90%;">]</span></cite></th>
<th id="S4.T3.2.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.2.2.2.5.1.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib20" title="" class="ltx_ref">20</a><span id="S4.T3.2.2.2.5.2.2" class="ltx_text" style="font-size:90%;">]</span></cite></th>
<th id="S4.T3.2.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.2.2.2.6.1.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib31" title="" class="ltx_ref">31</a><span id="S4.T3.2.2.2.6.2.2" class="ltx_text" style="font-size:90%;">]</span></cite></th>
<th id="S4.T3.2.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T3.2.2.2.7.1" class="ltx_text" style="font-size:90%;">Ours</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.2.3.1" class="ltx_tr">
<td id="S4.T3.2.3.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T3.2.3.1.1.1" class="ltx_text" style="font-size:90%;">LMO</span></td>
<td id="S4.T3.2.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.2.3.1.2.1" class="ltx_text" style="font-size:90%;">54.7</span></td>
<td id="S4.T3.2.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.2.3.1.3.1" class="ltx_text" style="font-size:90%;">62.4</span></td>
<td id="S4.T3.2.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.2.3.1.4.1" class="ltx_text" style="font-size:90%;">63.0</span></td>
<td id="S4.T3.2.3.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.2.3.1.5.1" class="ltx_text" style="font-size:90%;">63.3</span></td>
<td id="S4.T3.2.3.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.2.3.1.6.1" class="ltx_text" style="font-size:90%;">58.2</span></td>
<td id="S4.T3.2.3.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.2.3.1.7.1" class="ltx_text" style="font-size:90%;">68.4</span></td>
</tr>
<tr id="S4.T3.2.4.2" class="ltx_tr">
<td id="S4.T3.2.4.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><span id="S4.T3.2.4.2.1.1" class="ltx_text" style="font-size:90%;">TUDL</span></td>
<td id="S4.T3.2.4.2.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.2.4.2.2.1" class="ltx_text" style="font-size:90%;">55.8</span></td>
<td id="S4.T3.2.4.2.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.2.4.2.3.1" class="ltx_text" style="font-size:90%;">58.8</span></td>
<td id="S4.T3.2.4.2.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.2.4.2.4.1" class="ltx_text" style="font-size:90%;">79.1</span></td>
<td id="S4.T3.2.4.2.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.2.4.2.5.1" class="ltx_text" style="font-size:90%;">68.5</span></td>
<td id="S4.T3.2.4.2.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.2.4.2.6.1" class="ltx_text" style="font-size:90%;">87.6</span></td>
<td id="S4.T3.2.4.2.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.2.4.2.7.1" class="ltx_text" style="font-size:90%;">78.2</span></td>
</tr>
<tr id="S4.T3.2.5.3" class="ltx_tr">
<td id="S4.T3.2.5.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><span id="S4.T3.2.5.3.1.1" class="ltx_text" style="font-size:90%;">HB</span></td>
<td id="S4.T3.2.5.3.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.2.5.3.2.1" class="ltx_text" style="font-size:90%;">58.0</span></td>
<td id="S4.T3.2.5.3.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.2.5.3.3.1" class="ltx_text" style="font-size:90%;">72.2</span></td>
<td id="S4.T3.2.5.3.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.2.5.3.4.1" class="ltx_text" style="font-size:90%;">71.2</span></td>
<td id="S4.T3.2.5.3.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.2.5.3.5.1" class="ltx_text" style="font-size:90%;">65.6</span></td>
<td id="S4.T3.2.5.3.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.2.5.3.6.1" class="ltx_text" style="font-size:90%;">70.6</span></td>
<td id="S4.T3.2.5.3.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.2.5.3.7.1" class="ltx_text" style="font-size:90%;">68.7</span></td>
</tr>
<tr id="S4.T3.2.6.4" class="ltx_tr">
<td id="S4.T3.2.6.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><span id="S4.T3.2.6.4.1.1" class="ltx_text" style="font-size:90%;">YCBV</span></td>
<td id="S4.T3.2.6.4.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.2.6.4.2.1" class="ltx_text" style="font-size:90%;">49.9</span></td>
<td id="S4.T3.2.6.4.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.2.6.4.3.1" class="ltx_text" style="font-size:90%;">39.0</span></td>
<td id="S4.T3.2.6.4.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.2.6.4.4.1" class="ltx_text" style="font-size:90%;">53.2</span></td>
<td id="S4.T3.2.6.4.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.2.6.4.5.1" class="ltx_text" style="font-size:90%;">57.4</span></td>
<td id="S4.T3.2.6.4.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.2.6.4.6.1" class="ltx_text" style="font-size:90%;">45.0</span></td>
<td id="S4.T3.2.6.4.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.2.6.4.7.1" class="ltx_text" style="font-size:90%;">58.5</span></td>
</tr>
<tr id="S4.T3.2.7.5" class="ltx_tr">
<td id="S4.T3.2.7.5.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T3.2.7.5.1.1" class="ltx_text" style="font-size:90%;">Avg.</span></td>
<td id="S4.T3.2.7.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T3.2.7.5.2.1" class="ltx_text" style="font-size:90%;">54.6</span></td>
<td id="S4.T3.2.7.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T3.2.7.5.3.1" class="ltx_text" style="font-size:90%;">58.1</span></td>
<td id="S4.T3.2.7.5.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T3.2.7.5.4.1" class="ltx_text" style="font-size:90%;">66.6</span></td>
<td id="S4.T3.2.7.5.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T3.2.7.5.5.1" class="ltx_text" style="font-size:90%;">63.7</span></td>
<td id="S4.T3.2.7.5.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T3.2.7.5.6.1" class="ltx_text" style="font-size:90%;">65.4</span></td>
<td id="S4.T3.2.7.5.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T3.2.7.5.7.1" class="ltx_text" style="font-size:90%;">68.2</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>Results in % using the BOP metric for methods trained on synthetic training data on the four single instance multiple object (SiMo) datasets of the BOP 2020 challenge: LMO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, TUDL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, HB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, and YCBV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite></figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Linemod (LM) and Occlusion (LMO)</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The LM dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> presents 13 objects, one object in each scene, with high levels of clutter, and some levels of occlusion. For each object, approximately 1200 images are available. The general procedure for training on the LM dataset is to use 15% of the dataset for training, around 200 images, and test on the remaining 85%. However, as we have trained only on synthetic data, our method is tested both using the 85% split and using all images in the dataset; the resulting score is the same. The test results are shown in Tab. <a href="#S4.T1" title="Table 1 ‣ 4 Evaluation ‣ Bridging the Reality Gap for Pose Estimation Networks using Sensor-Based Domain Randomization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, including other recent methods trained on both real and synthetic data. Our method clearly outperforms other methods using synthetic data and outperforms most methods using real training data. In the LMO dataset, eight objects from the LM dataset have been annotated, many of these with very high levels of occlusion. The general procedure for testing deep learning algorithms on the LMO dataset is to use the full LM dataset for training each object, giving approximately 1200 training images for each object. Our method is the only one tested on the LMO dataset using only synthetic training. The result on the LMO dataset is shown in Tab. <a href="#S4.T2" title="Table 2 ‣ 4 Evaluation ‣ Bridging the Reality Gap for Pose Estimation Networks using Sensor-Based Domain Randomization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Our method is comparable with state-of-the-art methods using real training data. Compared with PVN3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> which achieved the highest score on the LM dataset, but low scores on the LMO dataset, our method performs well for both datasets.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Our results show that a single method trained with synthetic data, without any changes in parameters can achieve very good results in two different scenarios.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>BOP Challenge on SiMo datasets</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The synthetic training data was generated for the BOP challenge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, and several other algorithms have also been trained on this data. To further validate our work, we compare it against these other methods.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">The BOP challenge consists of seven different datasets where the performance is measured for each dataset. As our method is created for single instance pose estimation, the four datasets with this configuration are retrieved, and an average is calculated. The BOP challenge score is based on an average of three metrics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, making the comparison with 2D methods more equal. We use the same metric to calculate our performance. We include the results for all methods trained on the synthetic data from the competition as well as last year’s winner <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>.
The results are shown in Tab. <a href="#S4.T3" title="Table 3 ‣ 4 Evaluation ‣ Bridging the Reality Gap for Pose Estimation Networks using Sensor-Based Domain Randomization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. It is seen that our method is able to outperform other methods trained on the synthetic data along with last year’s best-performing method. Visual examples of our pose estimation are shown for different images in the BOP benchmark in Fig. <a href="#S4.F5" title="Figure 5 ‣ 4.2 BOP Challenge on SiMo datasets ‣ 4 Evaluation ‣ Bridging the Reality Gap for Pose Estimation Networks using Sensor-Based Domain Randomization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
While the main challenge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> does not include the LM dataset, the associated web page contains a leaderboard<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://bop.felk.cvut.cz/leaderboards/bop19_lm" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://bop.felk.cvut.cz/leaderboards/bop19_lm</a></span></span></span> with results. Our method was tested on this dataset with the above-mentioned metric, and the resulting average BOP-specific score was 85.8%. This outperforms the current best method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, which has a score of 75.2%, and is trained with real data.</p>
</div>
<figure id="S4.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F5.sf1" class="ltx_figure ltx_figure_panel ltx_align_center">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2011.08517/assets/x8.png" id="S4.F5.sf1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" width="325" height="415" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2011.08517/assets/x9.png" id="S4.F5.sf1.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" width="325" height="415" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F5.sf1.3.2" class="ltx_text" style="font-size:90%;">LMO - Scene 2 - Image 13</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F5.sf2" class="ltx_figure ltx_figure_panel ltx_align_center">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2011.08517/assets/x10.png" id="S4.F5.sf2.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" width="325" height="415" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2011.08517/assets/x11.png" id="S4.F5.sf2.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" width="325" height="415" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F5.sf2.3.2" class="ltx_text" style="font-size:90%;">TUDL - Scene 1 - Image 65</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F5.sf3" class="ltx_figure ltx_figure_panel ltx_align_center">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2011.08517/assets/x12.png" id="S4.F5.sf3.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" width="325" height="415" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2011.08517/assets/x13.png" id="S4.F5.sf3.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" width="325" height="415" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S4.F5.sf3.3.2" class="ltx_text" style="font-size:90%;">YCB-V - Scene 54 - Image 38</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.3.2" class="ltx_text" style="font-size:90%;"> Examples of pose estimations in the BOP dataset with our method. For each image the original image is shown to the left with the pose estimation shown in right image. Successful pose estimates are shown in green and erroneous in red. </span></figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Running Time</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">For a scene with a single object, the full process including pre-processing, given a 640x480 RGB-D image, takes approximately 1 second on a PC environment (an Intel i9-9820X 3.30GHz CPU and an NVIDIA GeForce RTX 2080 GPU). For the LMO dataset with eight objects in the scene the run-time is around 3.6 seconds. The time distributions for the different parts of the method is shown in Tab. <a href="#S4.T4" title="Table 4 ‣ 4.3 Running Time ‣ 4 Evaluation ‣ Bridging the Reality Gap for Pose Estimation Networks using Sensor-Based Domain Randomization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.2.1.1" class="ltx_tr">
<th id="S4.T4.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T4.2.1.1.1.1" class="ltx_text" style="font-size:90%;">Part</span></th>
<th id="S4.T4.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T4.2.1.1.2.1" class="ltx_text" style="font-size:90%;">Preproc.</span></th>
<th id="S4.T4.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<table id="S4.T4.2.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T4.2.1.1.3.1.1" class="ltx_tr">
<td id="S4.T4.2.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T4.2.1.1.3.1.1.1.1" class="ltx_text" style="font-size:90%;">Mask</span></td>
</tr>
<tr id="S4.T4.2.1.1.3.1.2" class="ltx_tr">
<td id="S4.T4.2.1.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T4.2.1.1.3.1.2.1.1" class="ltx_text" style="font-size:90%;">R-CNN</span></td>
</tr>
</table>
</th>
<th id="S4.T4.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T4.2.1.1.4.1" class="ltx_text" style="font-size:90%;">DNN</span></th>
<th id="S4.T4.2.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<table id="S4.T4.2.1.1.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T4.2.1.1.5.1.1" class="ltx_tr">
<td id="S4.T4.2.1.1.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T4.2.1.1.5.1.1.1.1" class="ltx_text" style="font-size:90%;">RANSAC</span></td>
</tr>
<tr id="S4.T4.2.1.1.5.1.2" class="ltx_tr">
<td id="S4.T4.2.1.1.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T4.2.1.1.5.1.2.1.1" class="ltx_text" style="font-size:90%;">+ ICP</span></td>
</tr>
</table>
</th>
<th id="S4.T4.2.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<table id="S4.T4.2.1.1.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T4.2.1.1.6.1.1" class="ltx_tr">
<td id="S4.T4.2.1.1.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T4.2.1.1.6.1.1.1.1" class="ltx_text" style="font-size:90%;">Depth</span></td>
</tr>
<tr id="S4.T4.2.1.1.6.1.2" class="ltx_tr">
<td id="S4.T4.2.1.1.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T4.2.1.1.6.1.2.1.1" class="ltx_text" style="font-size:90%;">Check</span></td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.2.2.1" class="ltx_tr">
<td id="S4.T4.2.2.1.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T4.2.2.1.1.1" class="ltx_text" style="font-size:90%;">% Time</span></td>
<td id="S4.T4.2.2.1.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T4.2.2.1.2.1" class="ltx_text" style="font-size:90%;">15</span></td>
<td id="S4.T4.2.2.1.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T4.2.2.1.3.1" class="ltx_text" style="font-size:90%;">8</span></td>
<td id="S4.T4.2.2.1.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T4.2.2.1.4.1" class="ltx_text" style="font-size:90%;">24</span></td>
<td id="S4.T4.2.2.1.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T4.2.2.1.5.1" class="ltx_text" style="font-size:90%;">49</span></td>
<td id="S4.T4.2.2.1.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T4.2.2.1.6.1" class="ltx_text" style="font-size:90%;">4</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.3.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S4.T4.4.2" class="ltx_text" style="font-size:90%;"> Percentage of time used in of our pipeline.</span></figcaption>
</figure>
<div id="S4.SS3.p2" class="ltx_para ltx_noindent">
<p id="S4.SS3.p2.1" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_bold">Number of Cluster Centers (CC) and number of Clusters Tested (CT):</span> As increasing either CC and CT will increase the run-time, a selection of the best parameter values is necessary. These are tested on the LMO dataset. In Tab. <a href="#S4.T5" title="Table 5 ‣ 4.3 Running Time ‣ 4 Evaluation ‣ Bridging the Reality Gap for Pose Estimation Networks using Sensor-Based Domain Randomization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> CT is fixed at 4 and CC is varied. In Tab. <a href="#S4.T6" title="Table 6 ‣ 4.3 Running Time ‣ 4 Evaluation ‣ Bridging the Reality Gap for Pose Estimation Networks using Sensor-Based Domain Randomization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> CC is fixed at 16 and CC is varied. In our implementation, the number of CC and CT is set to 16 and 4, respectively, as the optimal trade-off between performance and speed.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<table id="S4.T5.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.2.1.1" class="ltx_tr">
<th id="S4.T5.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T5.2.1.1.1.1" class="ltx_text" style="font-size:90%;">Cluster Centers</span></th>
<th id="S4.T5.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T5.2.1.1.2.1" class="ltx_text" style="font-size:90%;">4</span></th>
<th id="S4.T5.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T5.2.1.1.3.1" class="ltx_text" style="font-size:90%;">8</span></th>
<th id="S4.T5.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T5.2.1.1.4.1" class="ltx_text" style="font-size:90%;">16</span></th>
<th id="S4.T5.2.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T5.2.1.1.5.1" class="ltx_text" style="font-size:90%;">32</span></th>
<th id="S4.T5.2.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T5.2.1.1.6.1" class="ltx_text" style="font-size:90%;">64</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.2.2.1" class="ltx_tr">
<th id="S4.T5.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T5.2.2.1.1.1" class="ltx_text" style="font-size:90%;">Run-time (s)</span></th>
<td id="S4.T5.2.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.2.2.1.2.1" class="ltx_text" style="font-size:90%;">2.6</span></td>
<td id="S4.T5.2.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.2.2.1.3.1" class="ltx_text" style="font-size:90%;">3.0</span></td>
<td id="S4.T5.2.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.2.2.1.4.1" class="ltx_text" style="font-size:90%;">3.6</span></td>
<td id="S4.T5.2.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.2.2.1.5.1" class="ltx_text" style="font-size:90%;">4.8</span></td>
<td id="S4.T5.2.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.2.2.1.6.1" class="ltx_text" style="font-size:90%;">7.2</span></td>
</tr>
<tr id="S4.T5.2.3.2" class="ltx_tr">
<th id="S4.T5.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T5.2.3.2.1.1" class="ltx_text" style="font-size:90%;">Recall</span></th>
<td id="S4.T5.2.3.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T5.2.3.2.2.1" class="ltx_text" style="font-size:90%;">73.8</span></td>
<td id="S4.T5.2.3.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T5.2.3.2.3.1" class="ltx_text" style="font-size:90%;">76.2</span></td>
<td id="S4.T5.2.3.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T5.2.3.2.4.1" class="ltx_text" style="font-size:90%;">77.2</span></td>
<td id="S4.T5.2.3.2.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T5.2.3.2.5.1" class="ltx_text" style="font-size:90%;">77.1</span></td>
<td id="S4.T5.2.3.2.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T5.2.3.2.6.1" class="ltx_text" style="font-size:90%;">77.3</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T5.3.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="S4.T5.4.2" class="ltx_text" style="font-size:90%;"> Recall and run-time as a result of cluster centers. </span></figcaption>
</figure>
<figure id="S4.T6" class="ltx_table">
<table id="S4.T6.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T6.2.1.1" class="ltx_tr">
<th id="S4.T6.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T6.2.1.1.1.1" class="ltx_text" style="font-size:90%;">Clusters Tested</span></th>
<th id="S4.T6.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T6.2.1.1.2.1" class="ltx_text" style="font-size:90%;">1</span></th>
<th id="S4.T6.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T6.2.1.1.3.1" class="ltx_text" style="font-size:90%;">2</span></th>
<th id="S4.T6.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T6.2.1.1.4.1" class="ltx_text" style="font-size:90%;">4</span></th>
<th id="S4.T6.2.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T6.2.1.1.5.1" class="ltx_text" style="font-size:90%;">6</span></th>
<th id="S4.T6.2.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T6.2.1.1.6.1" class="ltx_text" style="font-size:90%;">8</span></th>
<th id="S4.T6.2.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T6.2.1.1.7.1" class="ltx_text" style="font-size:90%;">16</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T6.2.2.1" class="ltx_tr">
<th id="S4.T6.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T6.2.2.1.1.1" class="ltx_text" style="font-size:90%;">Run-time (s)</span></th>
<td id="S4.T6.2.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T6.2.2.1.2.1" class="ltx_text" style="font-size:90%;">2.2</span></td>
<td id="S4.T6.2.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T6.2.2.1.3.1" class="ltx_text" style="font-size:90%;">2.7</span></td>
<td id="S4.T6.2.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T6.2.2.1.4.1" class="ltx_text" style="font-size:90%;">3.6</span></td>
<td id="S4.T6.2.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T6.2.2.1.5.1" class="ltx_text" style="font-size:90%;">4.4</span></td>
<td id="S4.T6.2.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T6.2.2.1.6.1" class="ltx_text" style="font-size:90%;">5.7</span></td>
<td id="S4.T6.2.2.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T6.2.2.1.7.1" class="ltx_text" style="font-size:90%;">8.0</span></td>
</tr>
<tr id="S4.T6.2.3.2" class="ltx_tr">
<th id="S4.T6.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T6.2.3.2.1.1" class="ltx_text" style="font-size:90%;">Recall</span></th>
<td id="S4.T6.2.3.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T6.2.3.2.2.1" class="ltx_text" style="font-size:90%;">73.9</span></td>
<td id="S4.T6.2.3.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T6.2.3.2.3.1" class="ltx_text" style="font-size:90%;">76.0</span></td>
<td id="S4.T6.2.3.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T6.2.3.2.4.1" class="ltx_text" style="font-size:90%;">77.2</span></td>
<td id="S4.T6.2.3.2.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T6.2.3.2.5.1" class="ltx_text" style="font-size:90%;">77.5</span></td>
<td id="S4.T6.2.3.2.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T6.2.3.2.6.1" class="ltx_text" style="font-size:90%;">77.4</span></td>
<td id="S4.T6.2.3.2.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T6.2.3.2.7.1" class="ltx_text" style="font-size:90%;">76.0</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T6.3.1.1" class="ltx_text" style="font-size:90%;">Table 6</span>: </span><span id="S4.T6.4.2" class="ltx_text" style="font-size:90%;"> Recall and run-time as a result of clusters tested. </span></figcaption>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Ablation Studies</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">To verify the effect of our contributions, ablation studies are performed. The test is performed by removing the contribution, retraining the network and testing against the baseline performance. The ablation studies are performed on the LMO dataset with eight objects and 1214 images, where the baseline is 77.2% accuracy (Tab. <a href="#S4.T2" title="Table 2 ‣ 4 Evaluation ‣ Bridging the Reality Gap for Pose Estimation Networks using Sensor-Based Domain Randomization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
<div id="S4.SS4.p2" class="ltx_para ltx_noindent">
<p id="S4.SS4.p2.1" class="ltx_p"><span id="S4.SS4.p2.1.1" class="ltx_text ltx_font_bold">Domain randomization:</span> To verify the effect of our domain randomization, the network is trained with standard randomization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> and without randomization. The Mask R-CNN network is the exact same for all tests. Without domain randomization the average score is 69.8% and with standard domain randomization it is 74.4%. The sensor-based domain randomization thus improves the performance by 11.1% compared with no domain randomization and 3.7% compared with standard domain randomization, both in relative numbers. If the noise level of the standard domain randomization is increased the score drops.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p">A more elaborated distribution of the individual parts of the ablation study is shown Tab. <a href="#S4.T7" title="Table 7 ‣ 4.4 Ablation Studies ‣ 4 Evaluation ‣ Bridging the Reality Gap for Pose Estimation Networks using Sensor-Based Domain Randomization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. While the typical jitter provides some generalization, the geometric noise types (XYZ and rotation) contribute most to the generalization and are needed to achieve optimal results.</p>
</div>
<figure id="S4.T7" class="ltx_table">
<table id="S4.T7.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T7.2.1.1" class="ltx_tr">
<th id="S4.T7.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T7.2.1.1.1.1" class="ltx_text" style="font-size:90%;">Removed</span></th>
<td id="S4.T7.2.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T7.2.1.1.2.1" class="ltx_text" style="font-size:90%;">None</span></td>
<td id="S4.T7.2.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T7.2.1.1.3.1" class="ltx_text" style="font-size:90%;">XYZ</span></td>
<td id="S4.T7.2.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T7.2.1.1.4.1" class="ltx_text" style="font-size:90%;">Rot.</span></td>
<td id="S4.T7.2.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T7.2.1.1.5.1" class="ltx_text" style="font-size:90%;">RGB</span></td>
<td id="S4.T7.2.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T7.2.1.1.6.1" class="ltx_text" style="font-size:90%;">Jit.</span></td>
<td id="S4.T7.2.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T7.2.1.1.7.1" class="ltx_text" style="font-size:90%;">All</span></td>
</tr>
<tr id="S4.T7.2.2.2" class="ltx_tr">
<th id="S4.T7.2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T7.2.2.2.1.1" class="ltx_text" style="font-size:90%;">Recall</span></th>
<td id="S4.T7.2.2.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T7.2.2.2.2.1" class="ltx_text" style="font-size:90%;">77.2</span></td>
<td id="S4.T7.2.2.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T7.2.2.2.3.1" class="ltx_text" style="font-size:90%;">73.1</span></td>
<td id="S4.T7.2.2.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T7.2.2.2.4.1" class="ltx_text" style="font-size:90%;">76.1</span></td>
<td id="S4.T7.2.2.2.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T7.2.2.2.5.1" class="ltx_text" style="font-size:90%;">77.0</span></td>
<td id="S4.T7.2.2.2.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T7.2.2.2.6.1" class="ltx_text" style="font-size:90%;">76.9</span></td>
<td id="S4.T7.2.2.2.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T7.2.2.2.7.1" class="ltx_text" style="font-size:90%;">69.8</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T7.3.1.1" class="ltx_text" style="font-size:90%;">Table 7</span>: </span><span id="S4.T7.4.2" class="ltx_text" style="font-size:90%;"> The performance on the LMO dataset for networks trained without specific Domain Randomization types.</span></figcaption>
</figure>
<div id="S4.SS4.p4" class="ltx_para ltx_noindent">
<p id="S4.SS4.p4.6" class="ltx_p"><span id="S4.SS4.p4.6.1" class="ltx_text ltx_font_bold">SparseEdge feature:</span> Our SparseEdge method is compared with the standard edge feature from DGCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, both with <math id="S4.SS4.p4.1.m1.1" class="ltx_Math" alttext="k=10" display="inline"><semantics id="S4.SS4.p4.1.m1.1a"><mrow id="S4.SS4.p4.1.m1.1.1" xref="S4.SS4.p4.1.m1.1.1.cmml"><mi id="S4.SS4.p4.1.m1.1.1.2" xref="S4.SS4.p4.1.m1.1.1.2.cmml">k</mi><mo id="S4.SS4.p4.1.m1.1.1.1" xref="S4.SS4.p4.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS4.p4.1.m1.1.1.3" xref="S4.SS4.p4.1.m1.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p4.1.m1.1b"><apply id="S4.SS4.p4.1.m1.1.1.cmml" xref="S4.SS4.p4.1.m1.1.1"><eq id="S4.SS4.p4.1.m1.1.1.1.cmml" xref="S4.SS4.p4.1.m1.1.1.1"></eq><ci id="S4.SS4.p4.1.m1.1.1.2.cmml" xref="S4.SS4.p4.1.m1.1.1.2">𝑘</ci><cn type="integer" id="S4.SS4.p4.1.m1.1.1.3.cmml" xref="S4.SS4.p4.1.m1.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p4.1.m1.1c">k=10</annotation></semantics></math> and <math id="S4.SS4.p4.2.m2.1" class="ltx_Math" alttext="k=30" display="inline"><semantics id="S4.SS4.p4.2.m2.1a"><mrow id="S4.SS4.p4.2.m2.1.1" xref="S4.SS4.p4.2.m2.1.1.cmml"><mi id="S4.SS4.p4.2.m2.1.1.2" xref="S4.SS4.p4.2.m2.1.1.2.cmml">k</mi><mo id="S4.SS4.p4.2.m2.1.1.1" xref="S4.SS4.p4.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS4.p4.2.m2.1.1.3" xref="S4.SS4.p4.2.m2.1.1.3.cmml">30</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p4.2.m2.1b"><apply id="S4.SS4.p4.2.m2.1.1.cmml" xref="S4.SS4.p4.2.m2.1.1"><eq id="S4.SS4.p4.2.m2.1.1.1.cmml" xref="S4.SS4.p4.2.m2.1.1.1"></eq><ci id="S4.SS4.p4.2.m2.1.1.2.cmml" xref="S4.SS4.p4.2.m2.1.1.2">𝑘</ci><cn type="integer" id="S4.SS4.p4.2.m2.1.1.3.cmml" xref="S4.SS4.p4.2.m2.1.1.3">30</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p4.2.m2.1c">k=30</annotation></semantics></math>. For <math id="S4.SS4.p4.3.m3.1" class="ltx_Math" alttext="k=10" display="inline"><semantics id="S4.SS4.p4.3.m3.1a"><mrow id="S4.SS4.p4.3.m3.1.1" xref="S4.SS4.p4.3.m3.1.1.cmml"><mi id="S4.SS4.p4.3.m3.1.1.2" xref="S4.SS4.p4.3.m3.1.1.2.cmml">k</mi><mo id="S4.SS4.p4.3.m3.1.1.1" xref="S4.SS4.p4.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS4.p4.3.m3.1.1.3" xref="S4.SS4.p4.3.m3.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p4.3.m3.1b"><apply id="S4.SS4.p4.3.m3.1.1.cmml" xref="S4.SS4.p4.3.m3.1.1"><eq id="S4.SS4.p4.3.m3.1.1.1.cmml" xref="S4.SS4.p4.3.m3.1.1.1"></eq><ci id="S4.SS4.p4.3.m3.1.1.2.cmml" xref="S4.SS4.p4.3.m3.1.1.2">𝑘</ci><cn type="integer" id="S4.SS4.p4.3.m3.1.1.3.cmml" xref="S4.SS4.p4.3.m3.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p4.3.m3.1c">k=10</annotation></semantics></math> the score is 75.4% and the run-time is 3.4s. For <math id="S4.SS4.p4.4.m4.1" class="ltx_Math" alttext="k=30" display="inline"><semantics id="S4.SS4.p4.4.m4.1a"><mrow id="S4.SS4.p4.4.m4.1.1" xref="S4.SS4.p4.4.m4.1.1.cmml"><mi id="S4.SS4.p4.4.m4.1.1.2" xref="S4.SS4.p4.4.m4.1.1.2.cmml">k</mi><mo id="S4.SS4.p4.4.m4.1.1.1" xref="S4.SS4.p4.4.m4.1.1.1.cmml">=</mo><mn id="S4.SS4.p4.4.m4.1.1.3" xref="S4.SS4.p4.4.m4.1.1.3.cmml">30</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p4.4.m4.1b"><apply id="S4.SS4.p4.4.m4.1.1.cmml" xref="S4.SS4.p4.4.m4.1.1"><eq id="S4.SS4.p4.4.m4.1.1.1.cmml" xref="S4.SS4.p4.4.m4.1.1.1"></eq><ci id="S4.SS4.p4.4.m4.1.1.2.cmml" xref="S4.SS4.p4.4.m4.1.1.2">𝑘</ci><cn type="integer" id="S4.SS4.p4.4.m4.1.1.3.cmml" xref="S4.SS4.p4.4.m4.1.1.3">30</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p4.4.m4.1c">k=30</annotation></semantics></math> run-time rises to 4.1s while the score goes up to 76.9%. For our method the run-time is 3.6s with a relative 2.4% better performance than <math id="S4.SS4.p4.5.m5.1" class="ltx_Math" alttext="k=10" display="inline"><semantics id="S4.SS4.p4.5.m5.1a"><mrow id="S4.SS4.p4.5.m5.1.1" xref="S4.SS4.p4.5.m5.1.1.cmml"><mi id="S4.SS4.p4.5.m5.1.1.2" xref="S4.SS4.p4.5.m5.1.1.2.cmml">k</mi><mo id="S4.SS4.p4.5.m5.1.1.1" xref="S4.SS4.p4.5.m5.1.1.1.cmml">=</mo><mn id="S4.SS4.p4.5.m5.1.1.3" xref="S4.SS4.p4.5.m5.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p4.5.m5.1b"><apply id="S4.SS4.p4.5.m5.1.1.cmml" xref="S4.SS4.p4.5.m5.1.1"><eq id="S4.SS4.p4.5.m5.1.1.1.cmml" xref="S4.SS4.p4.5.m5.1.1.1"></eq><ci id="S4.SS4.p4.5.m5.1.1.2.cmml" xref="S4.SS4.p4.5.m5.1.1.2">𝑘</ci><cn type="integer" id="S4.SS4.p4.5.m5.1.1.3.cmml" xref="S4.SS4.p4.5.m5.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p4.5.m5.1c">k=10</annotation></semantics></math> and the score is still higher than when using <math id="S4.SS4.p4.6.m6.1" class="ltx_Math" alttext="k=30" display="inline"><semantics id="S4.SS4.p4.6.m6.1a"><mrow id="S4.SS4.p4.6.m6.1.1" xref="S4.SS4.p4.6.m6.1.1.cmml"><mi id="S4.SS4.p4.6.m6.1.1.2" xref="S4.SS4.p4.6.m6.1.1.2.cmml">k</mi><mo id="S4.SS4.p4.6.m6.1.1.1" xref="S4.SS4.p4.6.m6.1.1.1.cmml">=</mo><mn id="S4.SS4.p4.6.m6.1.1.3" xref="S4.SS4.p4.6.m6.1.1.3.cmml">30</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p4.6.m6.1b"><apply id="S4.SS4.p4.6.m6.1.1.cmml" xref="S4.SS4.p4.6.m6.1.1"><eq id="S4.SS4.p4.6.m6.1.1.1.cmml" xref="S4.SS4.p4.6.m6.1.1.1"></eq><ci id="S4.SS4.p4.6.m6.1.1.2.cmml" xref="S4.SS4.p4.6.m6.1.1.2">𝑘</ci><cn type="integer" id="S4.SS4.p4.6.m6.1.1.3.cmml" xref="S4.SS4.p4.6.m6.1.1.3">30</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p4.6.m6.1c">k=30</annotation></semantics></math>. The increased performance of the SparseEdge could indicate that a higher generalization is obtained.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We presented a novel method for pose estimation trained on synthetic data. The method finds keypoint matches in 3D point clouds and uses our novel SparseEdge feature. Combined with our sensor-based domain randomization, the method outperforms previous methods using purely synthetic training data and achieves state-of-the-art performance on a range of benchmarks. An ablation study shows the significance of our contributions to the performance of the method.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">For future work, instance segmentation can be added to the point cloud network. This, along with training a single network to predict keypoint votes for multiple objects, will allow us to pass an entire scene point cloud through the network for a single pass pose estimation of multiple objects.</p>
</div>
<div id="S5.p3" class="ltx_para ltx_noindent">
<p id="S5.p3.1" class="ltx_p"><span id="S5.p3.1.1" class="ltx_text ltx_font_bold">Acknowledgements</span> The authors gratefully acknowledge the support from Innovation Fund Denmark through the project MADE FAST.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Waleed Abdulla.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Mask r-cnn for object detection and instance segmentation on keras
and tensorflow.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/matterport/Mask_RCNN" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/matterport/Mask_RCNN</a><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Eric Brachmann, Alexander Krull, Frank Michel, Stefan Gumhold, Jamie Shotton,
and Carsten Rother.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Learning 6d object pose estimation using 3d object coordinates.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European conference on computer vision</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, pages 536–551.
Springer, 2014.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Benjamin Choo, Michael Landau, Michael DeVore, and Peter A Beling.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Statistical analysis-based error models for the microsoft kinecttm
depth sensor.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Sensors</span><span id="bib.bib3.4.2" class="ltx_text" style="font-size:90%;">, 14(9):17430–17450, 2014.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Maximilian Denninger, Martin Sundermeyer, Dominik Winkelbauer, Youssef Zidan,
Dmitry Olefir, Mohamad Elbadrawy, Ahsan Lodhi, and Harinandan Katam.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Blenderproc.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1911.01911</span><span id="bib.bib4.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Bertram Drost, Markus Ulrich, Paul Bergmann, Philipp Härtinger, and Carsten
Steger.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Introducing mvtec itodd-a dataset for 3d object recognition in
industry.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV Workshops</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, pages 2200–2208, 2017.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Kartik Gupta, Lars Petersson, and Richard Hartley.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Cullnet: Calibrated and pose aware confidence scores for object pose
estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision Workshops</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, pages 0–0, 2019.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Frederik Hagelskjær and Anders Glent Buch.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Pointvotenet: Accurate object detection and 6dof pose estimation in
point clouds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2020 IEEE International Conference on Image Processing
(ICIP)</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Frederik Hagelskjær, Anders Glent Buch, and Norbert Krüger.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Does vision work well enough for industry?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 13th International Joint Conference on
Computer Vision, Imaging and Computer Graphics Theory and Applications</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">,
volume 4, pages 198–205. SCITEPRESS Digital Library, 2019.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Mask r-cnn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE international conference on computer
vision</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, pages 2961–2969, 2017.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Yisheng He, Wei Sun, Haibin Huang, Jianran Liu, Haoqiang Fan, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Pvn3d: A deep point-wise 3d keypoints voting network for 6dof pose
estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, pages 11632–11641, 2020.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Yisheng He, Wei Sun, Haibin Huang, Jianran Liu, Haoqiang Fan, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Supplementary material–pvn3d: A deep point-wise 3d keypoints voting
network for 6dof pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">2020.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Stefan Hinterstoisser, Vincent Lepetit, Slobodan Ilic, Stefan Holzer, Gary
Bradski, Kurt Konolige, and Nassir Navab.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Model based training, detection and pose estimation of texture-less
3d objects in heavily cluttered scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Asian conference on computer vision</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, pages 548–562.
Springer, 2012.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Tomas Hodan, Daniel Barath, and Jiri Matas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Epos: Estimating 6d pose of objects with symmetries.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, pages 11703–11712, 2020.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Tomas Hodan, Frank Michel, Eric Brachmann, Wadim Kehl, Anders GlentBuch, Dirk
Kraft, Bertram Drost, Joel Vidal, Stephan Ihrke, Xenophon Zabulis, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Bop: Benchmark for 6d object pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision
(ECCV)</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, pages 19–34, 2018.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Tomas Hodan, Martin Sundermeyer, Bertram Drost, Yann Labbe, Eric Brachmann,
Frank Michel, Carsten Rother, and Jiri Matas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Bop challenge 2020 on 6d object localization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2009.07378</span><span id="bib.bib15.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Yinlin Hu, Pascal Fua, Wei Wang, and Mathieu Salzmann.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Single-stage 6d object pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, pages 2930–2939, 2020.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Sergey Ioffe and Christian Szegedy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Batch normalization: Accelerating deep network training by reducing
internal covariate shift.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1502.03167</span><span id="bib.bib17.4.2" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Roman Kaskman, Sergey Zakharov, Ivan Shugurov, and Slobodan Ilic.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Homebreweddb: Rgb-d dataset for 6d pose estimation of 3d objects.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision Workshops</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, pages 0–0, 2019.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Wadim Kehl, Fabian Manhardt, Federico Tombari, Slobodan Ilic, and Nassir Navab.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Ssd-6d: Making rgb-based 3d detection and 6d pose estimation great
again.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE International Conference on Computer Vision</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, pages
1521–1529, 2017.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Yann Labbé, Justin Carpentier, Mathieu Aubry, and Josef Sivic.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Cosypose: Consistent multi-view multi-object 6d pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2008.08465</span><span id="bib.bib20.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Zhigang Li, Gu Wang, and Xiangyang Ji.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Cdpn: Coordinates-based disentangled pose network for real-time
rgb-based 6-dof object pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, pages 7678–7687, 2019.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Microsoft coco: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European conference on computer vision</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, pages 740–755.
Springer, 2014.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Chuong V Nguyen, Shahram Izadi, and David Lovell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Modeling kinect sensor noise for improved 3d reconstruction and
tracking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2012 second international conference on 3D imaging, modeling,
processing, visualization &amp; transmission</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, pages 524–530. IEEE, 2012.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Pvnet: Pixel-wise voting network for 6dof pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conference on Computer Vision and Pattern Recognition</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">,
pages 4561–4570, 2019.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Pointnet: Deep learning on point sets for 3d classification and
segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conference on Computer Vision and Pattern Recognition</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">,
pages 652–660, 2017.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Mahdi Rad and Vincent Lepetit.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Bb8: A scalable, accurate, robust to partial occlusion method for
predicting the 3d poses of challenging objects without using depth.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE International Conference on Computer Vision</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, pages
3828–3836, 2017.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">You only look once: Unified, real-time object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, pages 779–788, 2016.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Mingxing Tan and Quoc V Le.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Efficientnet: Rethinking model scaling for convolutional neural
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1905.11946</span><span id="bib.bib28.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Bugra Tekin, Sudipta N Sinha, and Pascal Fua.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Real-time seamless single shot 6d object pose prediction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, pages 292–301, 2018.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Stefan Thalhammer, Timothy Patten, and Markus Vincze.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Towards object detection and pose estimation in clutter using only
synthetic depth data for training.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of ARW and OAGM Workshop 2019</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Joel Vidal, Chyi-Yeu Lin, Xavier Lladó, and Robert Martí.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">A method for 6d pose estimation of free-form rigid objects using
point pair features on range data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Sensors</span><span id="bib.bib31.4.2" class="ltx_text" style="font-size:90%;">, 18(8):2678, 2018.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Chen Wang, Danfei Xu, Yuke Zhu, Roberto Martín-Martín, Cewu Lu, Li
Fei-Fei, and Silvio Savarese.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Densefusion: 6d object pose estimation by iterative dense fusion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conference on Computer Vision and Pattern Recognition</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">,
pages 3343–3352, 2019.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, and
Justin M. Solomon.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Dynamic graph cnn for learning on point clouds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ACM Transactions on Graphics</span><span id="bib.bib33.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, and Dieter Fox.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Posecnn: A convolutional neural network for 6d object pose estimation
in cluttered scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Robotics: Science and Systems</span><span id="bib.bib34.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Sergey Zakharov, Ivan Shugurov, and Slobodan Ilic.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Dpod: 6d pose object detector and refiner.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:90%;">, pages 1941–1950, 2019.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Zhengyou Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Microsoft kinect sensor and its effect.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE multimedia</span><span id="bib.bib36.4.2" class="ltx_text" style="font-size:90%;">, 19(2):4–10, 2012.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2011.08516" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2011.08517" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2011.08517">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2011.08517" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2011.08518" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Feb 26 22:42:31 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
