<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2310.05136] InstructDET: Diversifying Referring Object Detection with Generalized Instructions</title><meta property="og:description" content="We propose InstructDET, a data-centric method for referring object detection (ROD) that localizes target objects based on user instructions. While deriving from referring expressions (REC), the instructions we leverage…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="InstructDET: Diversifying Referring Object Detection with Generalized Instructions">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="InstructDET: Diversifying Referring Object Detection with Generalized Instructions">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2310.05136">

<!--Generated on Wed Feb 28 02:10:54 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">InstructDET: Diversifying Referring Object Detection with Generalized Instructions</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id1.1.1" class="ltx_text ltx_font_bold">Ronghao Dang<sup id="id1.1.1.1" class="ltx_sup"><span id="id1.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup></span>  <span id="id2.2.2" class="ltx_text ltx_font_bold">Jiangyan Feng<sup id="id2.2.2.1" class="ltx_sup"><span id="id2.2.2.1.1" class="ltx_text ltx_font_medium ltx_font_italic">2∗</span></sup></span>  <span id="id3.3.3" class="ltx_text ltx_font_bold">Haodong Zhang<sup id="id3.3.3.1" class="ltx_sup"><span id="id3.3.3.1.1" class="ltx_text ltx_font_medium">2</span></sup></span>  <span id="id4.4.4" class="ltx_text ltx_font_bold">Chongjian Ge<sup id="id4.4.4.1" class="ltx_sup"><span id="id4.4.4.1.1" class="ltx_text ltx_font_medium">3</span></sup></span>  <span id="id5.5.5" class="ltx_text ltx_font_bold">Lin Song<sup id="id5.5.5.1" class="ltx_sup"><span id="id5.5.5.1.1" class="ltx_text ltx_font_medium">4</span></sup></span> 
<br class="ltx_break"><span id="id6.6.6" class="ltx_text ltx_font_bold">Lijun Gong<sup id="id6.6.6.1" class="ltx_sup"><span id="id6.6.6.1.1" class="ltx_text ltx_font_medium">2</span></sup></span> <span id="id7.7.7" class="ltx_text ltx_font_bold">Chengju Liu<sup id="id7.7.7.1" class="ltx_sup"><span id="id7.7.7.1.1" class="ltx_text ltx_font_medium">1</span></sup></span>  <span id="id8.8.8" class="ltx_text ltx_font_bold">Qijun Chen<sup id="id8.8.8.1" class="ltx_sup"><span id="id8.8.8.1.1" class="ltx_text ltx_font_medium">1</span></sup></span>  <span id="id9.9.9" class="ltx_text ltx_font_bold">Feng Zhu<sup id="id9.9.9.1" class="ltx_sup"><span id="id9.9.9.1.1" class="ltx_text ltx_font_medium">2</span></sup></span>  <span id="id10.10.10" class="ltx_text ltx_font_bold">Rui Zhao<sup id="id10.10.10.1" class="ltx_sup"><span id="id10.10.10.1.1" class="ltx_text ltx_font_medium">2</span></sup></span> <span id="id11.11.11" class="ltx_text ltx_font_bold">Yibing Song<sup id="id11.11.11.1" class="ltx_sup"><span id="id11.11.11.1.1" class="ltx_text ltx_font_medium">5</span></sup></span> 
<br class="ltx_break"><sup id="id18.18.id1" class="ltx_sup">1</sup>Tongji University  <sup id="id19.19.id2" class="ltx_sup">2</sup>SenseTime  <sup id="id20.20.id3" class="ltx_sup">3</sup>The University of Hong Kong 
<br class="ltx_break"><sup id="id21.21.id4" class="ltx_sup">4</sup>Tencent AI Lab  <sup id="id22.22.id5" class="ltx_sup">5</sup>AI<sup id="id23.23.id6" class="ltx_sup">3</sup> Institute, Fudan University
</span><span class="ltx_author_notes">R. Dang and J. Feng contribute equally. This work is done when R. Dang is an intern in SenseTime. We provide <a target="_blank" href="https://github.com/jyFengGoGo/InstructDet" title="" class="ltx_ref ltx_href">homepage</a> for this project.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id24.id1" class="ltx_p">We propose InstructDET, a data-centric method for referring object detection (ROD) that localizes target objects based on user instructions. While deriving from referring expressions (REC), the instructions we leverage are greatly diversified to encompass common user intentions related to object detection. For one image, we produce tremendous instructions that refer to every single object and different combinations of multiple objects. Each instruction and its corresponding object bounding boxes (bbxs) constitute one training data pair. In order to encompass common detection expressions, we involve emerging vision-language model (VLM) and large language model (LLM) to generate instructions guided by text prompts and object bbxs, as the generalizations of foundation models are effective to produce human-like expressions (e.g., describing object property, category, and relationship). We name our constructed dataset as InDET. It contains images, bbxs and generalized instructions that are from foundation models. Our InDET is developed from existing REC datasets and object detection datasets, with the expanding potential that any image with object bbxs can be incorporated through using our InstructDET method. By using our InDET dataset, we show that a conventional ROD model surpasses existing methods on standard REC datasets and our InDET test set. Our data-centric method InstructDET, with automatic data expansion by leveraging foundation models, directs a promising field that ROD can be greatly diversified to execute common object detection instructions.</p>
</div>
<figure id="S0.F1" class="ltx_figure"><img src="/html/2310.05136/assets/x1.png" id="S0.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="380" height="201" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Our ROD aims to execute diversified user detection instructions compared to visual grounding. For images with object bbxs, we use foundation models to produce human-like object detection instructions. By training a conventional ROD model with incorporating tremendous instructions, we largely push ROD towards practical usage from a data-centric perspective.</figcaption>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Referring object detection (ROD) aims to detect target objects according to language reference that represents user intentions. ROD is closely related to visual grounding where there are phrase grounding <cite class="ltx_cite ltx_citemacro_citep">(Akbari et al., <a href="#bib.bib1" title="" class="ltx_ref">2019</a>; Li et al., <a href="#bib.bib28" title="" class="ltx_ref">2022a</a>; Gao et al., <a href="#bib.bib12" title="" class="ltx_ref">2023</a>)</cite> and referring expression comprehension <cite class="ltx_cite ltx_citemacro_citep">(Su et al., <a href="#bib.bib47" title="" class="ltx_ref">2020</a>; Zhu et al., <a href="#bib.bib59" title="" class="ltx_ref">2022</a>)</cite>. As shown in Fig. <a href="#S0.F1" title="Figure 1 ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, phrase grounding detects all objects mentioned in one sentence, while referring expression comprehension (REC) only detects one single object that the text refers to. As such, the language reference in REC shall be discriminative and specifically relates to one object without ambiguity.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">Currently, visual grounding develops at an initial stage and leaves a gap for practical usage. The phrase grounding does not differentiate which object ought to be detected via language description, while REC only targets for one object with single text reference. In the current REC datasets, each image contains few expressions (e.g., 1 or 2 phrases). These expressions are insufficient to represent user intentions. In an image where there are several objects, users may want to detect each single object by using different descriptions (e.g., object color, shape, or location), or detect multiple objects in different combinations (e.g., similar properties or relationships). These diverse expressions are not conveyed within current REC datasets, leaving the gap for existing methods to practically fulfill user intentions for visual grounding. Moreover, the manual collection of these expressions are cumbersome, and subject bias prevents an effective coverage of common user intentions when perceiving each image. Therefore, the practical user expressions are not well fulfilled when they expect to detect various objects in one image.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">In this work<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>We do not differentiate “instruction” and “expression” in this paper, as both of them represent user intentions. For presentation clarity, in our InstructDET pipeline we refer expressions that are generated by foundation models, and we further refine expressions to instructions for InDET inclusion. As we only focus on ROD, we can formalize our instruction by simply adding the word ‘detect’ beforehand.</span></span></span>, we aim to push visual grounding toward practical usage from a data-centric perspective. Instead of developing REC models to generalize based on current data, we set up referring object detection (ROD) scenario to automatically diversify user expressions. Inspired by the generalization of foundation models that execute common user instructions based on the image and text inputs, our InstructDET borrows their capabilities to produce human-like instructions that encompass user intentions related to object detection. The generalized instructions produced by the foundation models can be regarded as an expansion of existing user expressions in REC. We produce instructions that describe single object from two pathways. In the first pathway (i.e., single-modality), we convert an image into an elaborate text description via LLaVA <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib32" title="" class="ltx_ref">2023a</a>)</cite>. The text description, together with object bbxs coordinates, are sent to the LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a href="#bib.bib48" title="" class="ltx_ref">2023</a>)</cite> for instruction generation in single-modality. During generation, we manually write 3 in-context examples and leverage the in-context learning <cite class="ltx_cite ltx_citemacro_citep">(Dong et al., <a href="#bib.bib8" title="" class="ltx_ref">2023</a>)</cite> ability of LLaMA to describe the content related to each object following the format of our examples.</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">In the second pathway (i.e., multi-modality), we send the image and text prompts into LLaVA. The objects in the image are marked with bbxs and the text prompts require LLaVA to describe the object content. We initialize LLaVA with miniGPT4 weights and find it tends to produce lengthy and global descriptions. So we perform a partial finetuning on LLaVA by using REC data to let it focus on local objects. Through these two pathways, we observe that instructions generated from single-modality pathway focus on the object relationship, while instructions generated from multi-modality pathway focus on rich visual details and advanced logic reasoning. Naturally, we combine instructions from these two pathways to formulate expressions for single referred object. During instruction generation, the uncontrolled model hallucination <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib30" title="" class="ltx_ref">2022b</a>)</cite> brings incorrect or irrelevant instructions. We propose to use visual-textual verification via CLIP <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a href="#bib.bib42" title="" class="ltx_ref">2021</a>)</cite> for an effective dropout.</p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p id="S1.p5.1" class="ltx_p">The generalization and reasoning of foundation models <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a href="#bib.bib49" title="" class="ltx_ref">2022</a>; Zhou et al., <a href="#bib.bib58" title="" class="ltx_ref">2022</a>)</cite> provide sufficient instructions encompassing user intentions for single object description. When describing multiple objects, we divide descriptions into two parts. The first part is to independently describe each single object followed by concatenation, and the second part is to summarize commonalities of multiple objects. The commonality summarization requires unifying similar or related objectives by a higher-level language abstraction that describes their similarities and relationships. We collect the combinations of different objects via semantic clustering, and utilize LLM to generate commonality summarizations for each combination.</p>
</div>
<div id="S1.p6" class="ltx_para ltx_noindent">
<p id="S1.p6.1" class="ltx_p">We automatically collect instructions for images and construct our InDET dataset. Sec. <a href="#S4" title="4 Dataset Analysis ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows an in-depth analysis of our dataset where we establish a guideline to organize these instructions from 6 aspects. Compared to existing REC datasets where the instructions only reside in sub-parts of our groups, our InDET is more comprehensive to incorporate user intentions of object detection.
Fig. <a href="#S0.F1" title="Figure 1 ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows an intuitive example of the generalized expressions produced by foundation models. By using our InDET dataset, we train a conventional ROD model and find it surpasses existing VG models on standard benchmarks and our InDET test set. Moreover, we also validate that our model is learned to effectively understand the instruction meaning rather than only key words, which is because of the tremendously expressive instructions incorporated for our model training. Our InstructDET method can automatically expand training data by using in-the-wild images with object bbxs, which improves our model generalizations towards practical usage. In addition, our model can already serve as the detection module of the neural-symbolic visual compositional task solution given arbitrary language instructions beyond object detection (e.g., Visual ChatGPT <cite class="ltx_cite ltx_citemacro_citep">(Wu et al., <a href="#bib.bib51" title="" class="ltx_ref">2023</a>)</cite>, VISPROG <cite class="ltx_cite ltx_citemacro_citep">(Gupta &amp; Kembhavi, <a href="#bib.bib14" title="" class="ltx_ref">2023</a>)</cite>, ViperGPT <cite class="ltx_cite ltx_citemacro_citep">(Dídac et al., <a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite>).</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Visual Grounding</span>.
Studies on visual grounding <cite class="ltx_cite ltx_citemacro_citep">(Kamath et al., <a href="#bib.bib18" title="" class="ltx_ref">2021</a>; Chen et al., <a href="#bib.bib4" title="" class="ltx_ref">2021</a>; Deng et al., <a href="#bib.bib5" title="" class="ltx_ref">2021</a>; Yang et al., <a href="#bib.bib54" title="" class="ltx_ref">2023b</a>; Su et al., <a href="#bib.bib46" title="" class="ltx_ref">2023</a>)</cite> can be mainly categorized as phrase grounding <cite class="ltx_cite ltx_citemacro_citep">(Plummer et al., <a href="#bib.bib40" title="" class="ltx_ref">2022</a>; Kojima et al., <a href="#bib.bib22" title="" class="ltx_ref">2023</a>; Shaharabany &amp; Wolf, <a href="#bib.bib43" title="" class="ltx_ref">2023</a>)</cite> and REC <cite class="ltx_cite ltx_citemacro_citep">(Hudson &amp; Manning, <a href="#bib.bib17" title="" class="ltx_ref">2018</a>; Li &amp; Sigal, <a href="#bib.bib29" title="" class="ltx_ref">2021</a>; Dou et al., <a href="#bib.bib10" title="" class="ltx_ref">2022</a>; He et al., <a href="#bib.bib16" title="" class="ltx_ref">2023</a>)</cite>. Phrase grounding detects all objects mentioned in the text while REC localizes one object that the text referred to. In <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a href="#bib.bib57" title="" class="ltx_ref">2022</a>; Liu et al., <a href="#bib.bib34" title="" class="ltx_ref">2023b</a>)</cite>, the objects mentioned in the text are verified to each visual object proposal one-by-one. These methods require a clear and specific object referring in the text. On the other hand, methods <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al., <a href="#bib.bib59" title="" class="ltx_ref">2022</a>; Yan et al., <a href="#bib.bib52" title="" class="ltx_ref">2023</a>)</cite> based on DETR <cite class="ltx_cite ltx_citemacro_citep">(Carion et al., <a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite> can accept abstract and summarized descriptions such as “red objects” and “all objects”. Our ROD model follows DETR-based design to enrich interpretation of various instructions. Note that our model is learned via InDET dataset where instructions are produced based on preset object bbxs. To this end, our model is not targeted for open-set object grounding but focuses on executing diversified and closed-set instructions. Nevertheless, our InstructDET can adapt to open-set data construction when accompanied with open-set detectors <cite class="ltx_cite ltx_citemacro_citep">(Gu et al., <a href="#bib.bib13" title="" class="ltx_ref">2022</a>; Zang et al., <a href="#bib.bib56" title="" class="ltx_ref">2022</a>)</cite> for data expansion.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Referring Expression Datasets</span>.
The REC datasets are usually constructed via manual annotation on the images. A two-player game is utilized in <cite class="ltx_cite ltx_citemacro_citep">(Kazemzadeh et al., <a href="#bib.bib19" title="" class="ltx_ref">2014</a>)</cite> where the text descriptions are concise due to limited relevant visual contents. The RefCOCO, RefCOCO+ <cite class="ltx_cite ltx_citemacro_citep">(Yu et al., <a href="#bib.bib55" title="" class="ltx_ref">2016</a>)</cite>, and RefCOCOg <cite class="ltx_cite ltx_citemacro_citep">(Mao et al., <a href="#bib.bib36" title="" class="ltx_ref">2016</a>)</cite> employ MSCOCO <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a href="#bib.bib31" title="" class="ltx_ref">2014</a>)</cite> images for manual expression production. The expression flexibility and diversity of these datasets are limited to encompass common detection intentions. Recent datasets <cite class="ltx_cite ltx_citemacro_citep">(Krishna et al., <a href="#bib.bib23" title="" class="ltx_ref">2017</a>; Kuznetsova et al., <a href="#bib.bib24" title="" class="ltx_ref">2020</a>; Kebe et al., <a href="#bib.bib20" title="" class="ltx_ref">2021</a>)</cite> focuses on data scalability rather than auto generation. CLEVR-Ref+ <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib33" title="" class="ltx_ref">2019</a>)</cite> combines synthetic images and predefined templates to autonomously produce target text pairs. Cops-Ref <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite> leverages scene graph as reasoning groundwork, thus forming a tree structure to generate expressions with varying compositionality. Different from these methods based on template guided expression generation, our InstructDET relies on foundation models to produce well generalized and human-like instructions.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Data Generation via Foundation Models</span>.
The InstructGPT <cite class="ltx_cite ltx_citemacro_citep">(Ouyang et al., <a href="#bib.bib39" title="" class="ltx_ref">2022</a>)</cite> and GPT4 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a href="#bib.bib38" title="" class="ltx_ref">2023</a>)</cite> have shown generalization and reasoning abilities for data generation. LLaVA <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib32" title="" class="ltx_ref">2023a</a>)</cite> first uses GPT4 for multi-modal data generation following instructions. The LLaVA-Med <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib26" title="" class="ltx_ref">2023b</a>)</cite> is narrowed and deepened into medical visual question answering. Otter <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib25" title="" class="ltx_ref">2023a</a>)</cite> performs multi-modality in-context instruction tuning by levering multiple images, questions, and answers. Currently, these models focus on global image and language understanding, with less focus on local object analysis. Moreover, these multi-modality models, although processing multi-modality data, still outputs single-modality text description. There is a gap for these foundation models to function in the computer vision scenarios, especially visual recognition. In comparison, our InstructDET uses foundation models to benefit ROD model training, which contributes directly to improve object detection performance.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2310.05136/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="422" height="91" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>An overview of our InstructDET. We use two pathways to produce detection expressions via foundation models. In the single modality pathway, we use LLaVA to describe an image via text, and combine this text with other text prompts for LLaMA input. In the multi-modality pathway, we use the same image with object bbxs and text prompts as multi modality input for LLaVA. The produced expressions are further refined to instructions and incorporated into our InDET dataset.</figcaption>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>InstructDET</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">Fig. <a href="#S2.F2" title="Figure 2 ‣ 2 Related Works ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows an overview of our InstructDET method for data construction. Given an input image with object bbxs, we use two pathways to produce detection expressions from foundation models. The expressions are further refined to instructions and incorporated into our InDET dataset.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Single Modality Pathway</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.1" class="ltx_p">The large language model (LLM) has shown surprising generalizations to well execute common user instructions. We use LLM to simulate user intentions when perceiving objects in an image. Our single modality pathway produces a text prompt for the LLaMA model. This prompt consists of several information including global image description, object bbx coordinates, in-context samples, and task description. Without tuning LLaMA, we obtain instructions describing objects in this image. A detailed example is shown in Sec. <a href="#A3" title="Appendix C Expression Generation via SINGLE-MODALITY PATHWAY ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a> for an intuitive illustration of how we leverage foundation models to produce expressions. We elucidate the key steps during this process as follows:</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p">Given an image with object bbxs, we first obtain global image description in text form. If this image already contains dense captions (e.g., from Flicker30K), we directly load these captions. Otherwise, we use LLaVA to describe this image. The text prompt we use for LLaVA contains our language guidance that
the specific object category shall be mentioned. As such, LLaVA will describe each labeled object in its output. Then, we involve object bbx coordinates and the related object content. If this image is from the REC dataset, we use referring expression as the object content. Otherwise, we simply use the category name.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.1" class="ltx_p">When designing the task description prompt, we expect LLaMA to produce diverse expressions that contain different properties of single object as much as possible. We manually list the attributes from the user perspective, including the object type, color, function, motions, and etc. Besides, we include the object attributes based on its relationship with other objects in this image, such as object interactions, object relative positions, and etc. When using these aforementioned prompts for LLaMA input, we find that the output text varies significantly and becomes irrelevant to our target objects. Inspired by the in-context learning ability of foundation models, we manually design in-context samples to regularize the output content and format. The output results will thus resemble our in-context examples but with our expected object descriptions in input images.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Multi-Modality Pathway</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p">The single modality pathway produces expressions according to text prompts. Naturally, we can feed both image and text prompt to the multi-modality foundation model for object description. Given an image, we mark the object with bbx rectangle, and send this image to LLaVA, together with the text prompt that requires LLaVA to describe the object according to the bbx. Here, the bbx serves as a visual highlight for LLaVA to comprehend the target object that we expect to describe.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.1" class="ltx_p">Our LLaVA model is initialized with miniGPT4 weights. When we send these multi-modality inputs to LLaVA, we observe that LLaVA produces detailed and dense descriptions for the image, rather than specific expressions of the target object. We analyze that the vision-language alignment module in LLaVA is the Q-Former <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib27" title="" class="ltx_ref">2023c</a>)</cite>, which transforms one image into only 32 visual tokens without concentrating on the local objects. Meanwhile, LLaVA itself tends to produce lengthy and dense descriptions. In order to generate instructions suitable for ROD, we finetune a part of LLaVA by using existing REC datasets. Specifically, we only update a linear layer that transforms visual tokens to the text embedding space during training. The linear layer is learned to attend local objects with concise expressions. After finetuning, we observe the LLaVA output becomes informative and closely related to the target object. Detailed examples of generating expressions in multi-modality pathway are shown in Sec. <a href="#A2" title="Appendix B Expression Generation via MULTI-MODALITY PATHWAY ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>, and the detailed analysis on how finetuning improves LLaVA output is provided in Sec. <a href="#A8" title="Appendix H Ablation Studies ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">H</span></a>.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Expression Dropout</h3>

<div id="S3.SS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.p1.1" class="ltx_p">During expression generation from foundation models, we have regularized the model output from several aspects including text prompt specification, in-context learning, and model finetuning. In practice, we still observe the model hallucination phenomena that sometimes generated expressions describe object content not appeared in the image. Moreover, the expressions from the multi-modality pathway sometimes describe the whole image rather than the local objects. This is due to the miniGPT4 initialization of LLaVA that utilizes dense captions for instruction tuning. The global image description is mitigated via our model finetuning to focus on local object, but not completely disappeared. To further improve the expression quality, we introduce visual and language matching via CLIP <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a href="#bib.bib42" title="" class="ltx_ref">2021</a>)</cite> to dropout inappropriate expressions. Fig. <a href="#S3.F3" title="Figure 3 ‣ 3.3 Expression Dropout ‣ 3 InstructDET ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows an overview. It contains image visual prompting and visual-textual matching.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2310.05136/assets/x3.png" id="S3.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="211" height="83" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Expression dropout by image visual prompting and visual-textual matching via CLIP.</figcaption>
</figure>
<section id="S3.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Visual Prompting</h4>

<div id="S3.SS3.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.SSS0.Px1.p1.1" class="ltx_p">We study visual language pretraining (VLP) <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a href="#bib.bib53" title="" class="ltx_ref">2023a</a>; Shtedritski et al., <a href="#bib.bib45" title="" class="ltx_ref">2023</a>)</cite> where visual prompting is developed for images. We observe that in zero-shot REC, coupling VLP with visual prompts enables robust pairing of local image region and corresponding text description. In the pairing process, the design of visual prompting heavily influences the visual-textual matching results. Generally, there are two types of visual prompting. The first one is to emphasize image local regions via shape division such as box, circle and contour. The second one is to highlight regions via crop-based, line-based, mask-based, grayscale and blur reversion-based image editing operations. We have tried these designs and found out that the superposition of a line-based circle prompt above a blur reversed contour prompt achieves favorable results. Specifically, for one image with given object bbxs, we make two image copies. For the first copy, we plot a red ellipse on this image covering the target object. For the second copy, we use SAM <cite class="ltx_cite ltx_citemacro_citep">(Kirillov et al., <a href="#bib.bib21" title="" class="ltx_ref">2023</a>)</cite> to segment the target object and perform Gaussian blur reversion on the image such that the remaining image regions are blurry. Then, we fuse these two copies to produce the visual prompting result as shown in Fig. <a href="#S3.F3" title="Figure 3 ‣ 3.3 Expression Dropout ‣ 3 InstructDET ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. A detailed pipeline illustrating visual prompting is in Sec. <a href="#A4" title="Appendix D Visual Prompting and Visual-Textual Matching ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a>.</p>
</div>
<div id="S3.SS3.SSS0.Px1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS3.SSS0.Px1.p2.3" class="ltx_p"><span id="S3.SS3.SSS0.Px1.p2.3.1" class="ltx_text ltx_font_bold">Visual-Textual Matching</span>.
We use images with visual prompting that emphasizes target objects to verify the corresponding text descriptions via a frozen CLIP model. While local object contents are well aligned with target referring expressions, we observe that expressions describing the whole image are not eliminated by CLIP. We analyze that CLIP is originally trained to focus on the correlations between global image features and global textual semantics. The global visual-textual matching makes CLIP model to prefer global image description accordingly. To remove this effect, we establish a referring measurement from both local and global perspectives. For the image in Fig. <a href="#S3.F3" title="Figure 3 ‣ 3.3 Expression Dropout ‣ 3 InstructDET ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we compute a global score <math id="S3.SS3.SSS0.Px1.p2.1.m1.1" class="ltx_Math" alttext="S_{g}" display="inline"><semantics id="S3.SS3.SSS0.Px1.p2.1.m1.1a"><msub id="S3.SS3.SSS0.Px1.p2.1.m1.1.1" xref="S3.SS3.SSS0.Px1.p2.1.m1.1.1.cmml"><mi id="S3.SS3.SSS0.Px1.p2.1.m1.1.1.2" xref="S3.SS3.SSS0.Px1.p2.1.m1.1.1.2.cmml">S</mi><mi id="S3.SS3.SSS0.Px1.p2.1.m1.1.1.3" xref="S3.SS3.SSS0.Px1.p2.1.m1.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p2.1.m1.1b"><apply id="S3.SS3.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p2.1.m1.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p2.1.m1.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p2.1.m1.1.1.2">𝑆</ci><ci id="S3.SS3.SSS0.Px1.p2.1.m1.1.1.3.cmml" xref="S3.SS3.SSS0.Px1.p2.1.m1.1.1.3">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p2.1.m1.1c">S_{g}</annotation></semantics></math> and a local prompt score <math id="S3.SS3.SSS0.Px1.p2.2.m2.1" class="ltx_Math" alttext="S_{l}" display="inline"><semantics id="S3.SS3.SSS0.Px1.p2.2.m2.1a"><msub id="S3.SS3.SSS0.Px1.p2.2.m2.1.1" xref="S3.SS3.SSS0.Px1.p2.2.m2.1.1.cmml"><mi id="S3.SS3.SSS0.Px1.p2.2.m2.1.1.2" xref="S3.SS3.SSS0.Px1.p2.2.m2.1.1.2.cmml">S</mi><mi id="S3.SS3.SSS0.Px1.p2.2.m2.1.1.3" xref="S3.SS3.SSS0.Px1.p2.2.m2.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p2.2.m2.1b"><apply id="S3.SS3.SSS0.Px1.p2.2.m2.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p2.2.m2.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p2.2.m2.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p2.2.m2.1.1.2">𝑆</ci><ci id="S3.SS3.SSS0.Px1.p2.2.m2.1.1.3.cmml" xref="S3.SS3.SSS0.Px1.p2.2.m2.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p2.2.m2.1c">S_{l}</annotation></semantics></math>. The magnitude of referring expression can be measured via our local enhancement score <math id="S3.SS3.SSS0.Px1.p2.3.m3.1" class="ltx_Math" alttext="S_{e}=S_{l}-S_{g}" display="inline"><semantics id="S3.SS3.SSS0.Px1.p2.3.m3.1a"><mrow id="S3.SS3.SSS0.Px1.p2.3.m3.1.1" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.cmml"><msub id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.2" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.2.cmml"><mi id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.2.2" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.2.2.cmml">S</mi><mi id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.2.3" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.2.3.cmml">e</mi></msub><mo id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.1" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.1.cmml">=</mo><mrow id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3.cmml"><msub id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3.2" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3.2.cmml"><mi id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3.2.2" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3.2.2.cmml">S</mi><mi id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3.2.3" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3.2.3.cmml">l</mi></msub><mo id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3.1" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3.1.cmml">−</mo><msub id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3.3" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3.3.cmml"><mi id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3.3.2" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3.3.2.cmml">S</mi><mi id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3.3.3" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3.3.3.cmml">g</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p2.3.m3.1b"><apply id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1"><eq id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.1"></eq><apply id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.2.1.cmml" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.2.2.cmml" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.2.2">𝑆</ci><ci id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.2.3.cmml" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.2.3">𝑒</ci></apply><apply id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3.cmml" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3"><minus id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3.1.cmml" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3.1"></minus><apply id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3.2.cmml" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3.2.1.cmml" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3.2">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3.2.2.cmml" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3.2.2">𝑆</ci><ci id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3.2.3.cmml" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3.2.3">𝑙</ci></apply><apply id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3.3.cmml" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3.3.1.cmml" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3.3">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3.3.2.cmml" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3.3.2">𝑆</ci><ci id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3.3.3.cmml" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3.3.3">𝑔</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p2.3.m3.1c">S_{e}=S_{l}-S_{g}</annotation></semantics></math>. Our final expression evaluation score can be computed as:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="S_{f}=\alpha_{1}S_{e}+\alpha_{2}S_{l}=(\alpha_{1}+\alpha_{2})S_{l}-\alpha_{1}S_{g}=S_{l}-\alpha_{1}S_{g}" display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><msub id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml">S</mi><mi id="S3.E1.m1.1.1.3.3" xref="S3.E1.m1.1.1.3.3.cmml">f</mi></msub><mo id="S3.E1.m1.1.1.4" xref="S3.E1.m1.1.1.4.cmml">=</mo><mrow id="S3.E1.m1.1.1.5" xref="S3.E1.m1.1.1.5.cmml"><mrow id="S3.E1.m1.1.1.5.2" xref="S3.E1.m1.1.1.5.2.cmml"><msub id="S3.E1.m1.1.1.5.2.2" xref="S3.E1.m1.1.1.5.2.2.cmml"><mi id="S3.E1.m1.1.1.5.2.2.2" xref="S3.E1.m1.1.1.5.2.2.2.cmml">α</mi><mn id="S3.E1.m1.1.1.5.2.2.3" xref="S3.E1.m1.1.1.5.2.2.3.cmml">1</mn></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.5.2.1" xref="S3.E1.m1.1.1.5.2.1.cmml">​</mo><msub id="S3.E1.m1.1.1.5.2.3" xref="S3.E1.m1.1.1.5.2.3.cmml"><mi id="S3.E1.m1.1.1.5.2.3.2" xref="S3.E1.m1.1.1.5.2.3.2.cmml">S</mi><mi id="S3.E1.m1.1.1.5.2.3.3" xref="S3.E1.m1.1.1.5.2.3.3.cmml">e</mi></msub></mrow><mo id="S3.E1.m1.1.1.5.1" xref="S3.E1.m1.1.1.5.1.cmml">+</mo><mrow id="S3.E1.m1.1.1.5.3" xref="S3.E1.m1.1.1.5.3.cmml"><msub id="S3.E1.m1.1.1.5.3.2" xref="S3.E1.m1.1.1.5.3.2.cmml"><mi id="S3.E1.m1.1.1.5.3.2.2" xref="S3.E1.m1.1.1.5.3.2.2.cmml">α</mi><mn id="S3.E1.m1.1.1.5.3.2.3" xref="S3.E1.m1.1.1.5.3.2.3.cmml">2</mn></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.5.3.1" xref="S3.E1.m1.1.1.5.3.1.cmml">​</mo><msub id="S3.E1.m1.1.1.5.3.3" xref="S3.E1.m1.1.1.5.3.3.cmml"><mi id="S3.E1.m1.1.1.5.3.3.2" xref="S3.E1.m1.1.1.5.3.3.2.cmml">S</mi><mi id="S3.E1.m1.1.1.5.3.3.3" xref="S3.E1.m1.1.1.5.3.3.3.cmml">l</mi></msub></mrow></mrow><mo id="S3.E1.m1.1.1.6" xref="S3.E1.m1.1.1.6.cmml">=</mo><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml"><msub id="S3.E1.m1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.cmml">α</mi><mn id="S3.E1.m1.1.1.1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.1.1.1.2.3.cmml">1</mn></msub><mo id="S3.E1.m1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">+</mo><msub id="S3.E1.m1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.3.2.cmml">α</mi><mn id="S3.E1.m1.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.3.3.cmml">2</mn></msub></mrow><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml">​</mo><msub id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.3.2.cmml">S</mi><mi id="S3.E1.m1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.3.3.cmml">l</mi></msub></mrow><mo id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.2.cmml">−</mo><mrow id="S3.E1.m1.1.1.1.3" xref="S3.E1.m1.1.1.1.3.cmml"><msub id="S3.E1.m1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.3.2.cmml"><mi id="S3.E1.m1.1.1.1.3.2.2" xref="S3.E1.m1.1.1.1.3.2.2.cmml">α</mi><mn id="S3.E1.m1.1.1.1.3.2.3" xref="S3.E1.m1.1.1.1.3.2.3.cmml">1</mn></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.3.1" xref="S3.E1.m1.1.1.1.3.1.cmml">​</mo><msub id="S3.E1.m1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.3.3.cmml"><mi id="S3.E1.m1.1.1.1.3.3.2" xref="S3.E1.m1.1.1.1.3.3.2.cmml">S</mi><mi id="S3.E1.m1.1.1.1.3.3.3" xref="S3.E1.m1.1.1.1.3.3.3.cmml">g</mi></msub></mrow></mrow><mo id="S3.E1.m1.1.1.7" xref="S3.E1.m1.1.1.7.cmml">=</mo><mrow id="S3.E1.m1.1.1.8" xref="S3.E1.m1.1.1.8.cmml"><msub id="S3.E1.m1.1.1.8.2" xref="S3.E1.m1.1.1.8.2.cmml"><mi id="S3.E1.m1.1.1.8.2.2" xref="S3.E1.m1.1.1.8.2.2.cmml">S</mi><mi id="S3.E1.m1.1.1.8.2.3" xref="S3.E1.m1.1.1.8.2.3.cmml">l</mi></msub><mo id="S3.E1.m1.1.1.8.1" xref="S3.E1.m1.1.1.8.1.cmml">−</mo><mrow id="S3.E1.m1.1.1.8.3" xref="S3.E1.m1.1.1.8.3.cmml"><msub id="S3.E1.m1.1.1.8.3.2" xref="S3.E1.m1.1.1.8.3.2.cmml"><mi id="S3.E1.m1.1.1.8.3.2.2" xref="S3.E1.m1.1.1.8.3.2.2.cmml">α</mi><mn id="S3.E1.m1.1.1.8.3.2.3" xref="S3.E1.m1.1.1.8.3.2.3.cmml">1</mn></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.8.3.1" xref="S3.E1.m1.1.1.8.3.1.cmml">​</mo><msub id="S3.E1.m1.1.1.8.3.3" xref="S3.E1.m1.1.1.8.3.3.cmml"><mi id="S3.E1.m1.1.1.8.3.3.2" xref="S3.E1.m1.1.1.8.3.3.2.cmml">S</mi><mi id="S3.E1.m1.1.1.8.3.3.3" xref="S3.E1.m1.1.1.8.3.3.3.cmml">g</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><and id="S3.E1.m1.1.1a.cmml" xref="S3.E1.m1.1.1"></and><apply id="S3.E1.m1.1.1b.cmml" xref="S3.E1.m1.1.1"><eq id="S3.E1.m1.1.1.4.cmml" xref="S3.E1.m1.1.1.4"></eq><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2">𝑆</ci><ci id="S3.E1.m1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.3.3">𝑓</ci></apply><apply id="S3.E1.m1.1.1.5.cmml" xref="S3.E1.m1.1.1.5"><plus id="S3.E1.m1.1.1.5.1.cmml" xref="S3.E1.m1.1.1.5.1"></plus><apply id="S3.E1.m1.1.1.5.2.cmml" xref="S3.E1.m1.1.1.5.2"><times id="S3.E1.m1.1.1.5.2.1.cmml" xref="S3.E1.m1.1.1.5.2.1"></times><apply id="S3.E1.m1.1.1.5.2.2.cmml" xref="S3.E1.m1.1.1.5.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.5.2.2.1.cmml" xref="S3.E1.m1.1.1.5.2.2">subscript</csymbol><ci id="S3.E1.m1.1.1.5.2.2.2.cmml" xref="S3.E1.m1.1.1.5.2.2.2">𝛼</ci><cn type="integer" id="S3.E1.m1.1.1.5.2.2.3.cmml" xref="S3.E1.m1.1.1.5.2.2.3">1</cn></apply><apply id="S3.E1.m1.1.1.5.2.3.cmml" xref="S3.E1.m1.1.1.5.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.5.2.3.1.cmml" xref="S3.E1.m1.1.1.5.2.3">subscript</csymbol><ci id="S3.E1.m1.1.1.5.2.3.2.cmml" xref="S3.E1.m1.1.1.5.2.3.2">𝑆</ci><ci id="S3.E1.m1.1.1.5.2.3.3.cmml" xref="S3.E1.m1.1.1.5.2.3.3">𝑒</ci></apply></apply><apply id="S3.E1.m1.1.1.5.3.cmml" xref="S3.E1.m1.1.1.5.3"><times id="S3.E1.m1.1.1.5.3.1.cmml" xref="S3.E1.m1.1.1.5.3.1"></times><apply id="S3.E1.m1.1.1.5.3.2.cmml" xref="S3.E1.m1.1.1.5.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.5.3.2.1.cmml" xref="S3.E1.m1.1.1.5.3.2">subscript</csymbol><ci id="S3.E1.m1.1.1.5.3.2.2.cmml" xref="S3.E1.m1.1.1.5.3.2.2">𝛼</ci><cn type="integer" id="S3.E1.m1.1.1.5.3.2.3.cmml" xref="S3.E1.m1.1.1.5.3.2.3">2</cn></apply><apply id="S3.E1.m1.1.1.5.3.3.cmml" xref="S3.E1.m1.1.1.5.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.5.3.3.1.cmml" xref="S3.E1.m1.1.1.5.3.3">subscript</csymbol><ci id="S3.E1.m1.1.1.5.3.3.2.cmml" xref="S3.E1.m1.1.1.5.3.3.2">𝑆</ci><ci id="S3.E1.m1.1.1.5.3.3.3.cmml" xref="S3.E1.m1.1.1.5.3.3.3">𝑙</ci></apply></apply></apply></apply><apply id="S3.E1.m1.1.1c.cmml" xref="S3.E1.m1.1.1"><eq id="S3.E1.m1.1.1.6.cmml" xref="S3.E1.m1.1.1.6"></eq><share href="#S3.E1.m1.1.1.5.cmml" id="S3.E1.m1.1.1d.cmml" xref="S3.E1.m1.1.1"></share><apply id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><minus id="S3.E1.m1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.2"></minus><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1"><times id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"></times><apply id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1"><plus id="S3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1"></plus><apply id="S3.E1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2">𝛼</ci><cn type="integer" id="S3.E1.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.3">1</cn></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3.2">𝛼</ci><cn type="integer" id="S3.E1.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3.3">2</cn></apply></apply><apply id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2">𝑆</ci><ci id="S3.E1.m1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3">𝑙</ci></apply></apply><apply id="S3.E1.m1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.3"><times id="S3.E1.m1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.3.1"></times><apply id="S3.E1.m1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.1.3.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.1.3.2.2">𝛼</ci><cn type="integer" id="S3.E1.m1.1.1.1.3.2.3.cmml" xref="S3.E1.m1.1.1.1.3.2.3">1</cn></apply><apply id="S3.E1.m1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.1.3.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.1.3.3.2">𝑆</ci><ci id="S3.E1.m1.1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.1.3.3.3">𝑔</ci></apply></apply></apply></apply><apply id="S3.E1.m1.1.1e.cmml" xref="S3.E1.m1.1.1"><eq id="S3.E1.m1.1.1.7.cmml" xref="S3.E1.m1.1.1.7"></eq><share href="#S3.E1.m1.1.1.1.cmml" id="S3.E1.m1.1.1f.cmml" xref="S3.E1.m1.1.1"></share><apply id="S3.E1.m1.1.1.8.cmml" xref="S3.E1.m1.1.1.8"><minus id="S3.E1.m1.1.1.8.1.cmml" xref="S3.E1.m1.1.1.8.1"></minus><apply id="S3.E1.m1.1.1.8.2.cmml" xref="S3.E1.m1.1.1.8.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.8.2.1.cmml" xref="S3.E1.m1.1.1.8.2">subscript</csymbol><ci id="S3.E1.m1.1.1.8.2.2.cmml" xref="S3.E1.m1.1.1.8.2.2">𝑆</ci><ci id="S3.E1.m1.1.1.8.2.3.cmml" xref="S3.E1.m1.1.1.8.2.3">𝑙</ci></apply><apply id="S3.E1.m1.1.1.8.3.cmml" xref="S3.E1.m1.1.1.8.3"><times id="S3.E1.m1.1.1.8.3.1.cmml" xref="S3.E1.m1.1.1.8.3.1"></times><apply id="S3.E1.m1.1.1.8.3.2.cmml" xref="S3.E1.m1.1.1.8.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.8.3.2.1.cmml" xref="S3.E1.m1.1.1.8.3.2">subscript</csymbol><ci id="S3.E1.m1.1.1.8.3.2.2.cmml" xref="S3.E1.m1.1.1.8.3.2.2">𝛼</ci><cn type="integer" id="S3.E1.m1.1.1.8.3.2.3.cmml" xref="S3.E1.m1.1.1.8.3.2.3">1</cn></apply><apply id="S3.E1.m1.1.1.8.3.3.cmml" xref="S3.E1.m1.1.1.8.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.8.3.3.1.cmml" xref="S3.E1.m1.1.1.8.3.3">subscript</csymbol><ci id="S3.E1.m1.1.1.8.3.3.2.cmml" xref="S3.E1.m1.1.1.8.3.3.2">𝑆</ci><ci id="S3.E1.m1.1.1.8.3.3.3.cmml" xref="S3.E1.m1.1.1.8.3.3.3">𝑔</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">S_{f}=\alpha_{1}S_{e}+\alpha_{2}S_{l}=(\alpha_{1}+\alpha_{2})S_{l}-\alpha_{1}S_{g}=S_{l}-\alpha_{1}S_{g}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.SSS0.Px1.p2.14" class="ltx_p">where <math id="S3.SS3.SSS0.Px1.p2.4.m1.1" class="ltx_Math" alttext="\alpha_{1}" display="inline"><semantics id="S3.SS3.SSS0.Px1.p2.4.m1.1a"><msub id="S3.SS3.SSS0.Px1.p2.4.m1.1.1" xref="S3.SS3.SSS0.Px1.p2.4.m1.1.1.cmml"><mi id="S3.SS3.SSS0.Px1.p2.4.m1.1.1.2" xref="S3.SS3.SSS0.Px1.p2.4.m1.1.1.2.cmml">α</mi><mn id="S3.SS3.SSS0.Px1.p2.4.m1.1.1.3" xref="S3.SS3.SSS0.Px1.p2.4.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p2.4.m1.1b"><apply id="S3.SS3.SSS0.Px1.p2.4.m1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.4.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p2.4.m1.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.4.m1.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p2.4.m1.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p2.4.m1.1.1.2">𝛼</ci><cn type="integer" id="S3.SS3.SSS0.Px1.p2.4.m1.1.1.3.cmml" xref="S3.SS3.SSS0.Px1.p2.4.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p2.4.m1.1c">\alpha_{1}</annotation></semantics></math> and <math id="S3.SS3.SSS0.Px1.p2.5.m2.1" class="ltx_Math" alttext="\alpha_{2}" display="inline"><semantics id="S3.SS3.SSS0.Px1.p2.5.m2.1a"><msub id="S3.SS3.SSS0.Px1.p2.5.m2.1.1" xref="S3.SS3.SSS0.Px1.p2.5.m2.1.1.cmml"><mi id="S3.SS3.SSS0.Px1.p2.5.m2.1.1.2" xref="S3.SS3.SSS0.Px1.p2.5.m2.1.1.2.cmml">α</mi><mn id="S3.SS3.SSS0.Px1.p2.5.m2.1.1.3" xref="S3.SS3.SSS0.Px1.p2.5.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p2.5.m2.1b"><apply id="S3.SS3.SSS0.Px1.p2.5.m2.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.5.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p2.5.m2.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.5.m2.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p2.5.m2.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p2.5.m2.1.1.2">𝛼</ci><cn type="integer" id="S3.SS3.SSS0.Px1.p2.5.m2.1.1.3.cmml" xref="S3.SS3.SSS0.Px1.p2.5.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p2.5.m2.1c">\alpha_{2}</annotation></semantics></math> are scalars balancing the contributions of <math id="S3.SS3.SSS0.Px1.p2.6.m3.1" class="ltx_Math" alttext="S_{g}" display="inline"><semantics id="S3.SS3.SSS0.Px1.p2.6.m3.1a"><msub id="S3.SS3.SSS0.Px1.p2.6.m3.1.1" xref="S3.SS3.SSS0.Px1.p2.6.m3.1.1.cmml"><mi id="S3.SS3.SSS0.Px1.p2.6.m3.1.1.2" xref="S3.SS3.SSS0.Px1.p2.6.m3.1.1.2.cmml">S</mi><mi id="S3.SS3.SSS0.Px1.p2.6.m3.1.1.3" xref="S3.SS3.SSS0.Px1.p2.6.m3.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p2.6.m3.1b"><apply id="S3.SS3.SSS0.Px1.p2.6.m3.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.6.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p2.6.m3.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.6.m3.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p2.6.m3.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p2.6.m3.1.1.2">𝑆</ci><ci id="S3.SS3.SSS0.Px1.p2.6.m3.1.1.3.cmml" xref="S3.SS3.SSS0.Px1.p2.6.m3.1.1.3">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p2.6.m3.1c">S_{g}</annotation></semantics></math> and <math id="S3.SS3.SSS0.Px1.p2.7.m4.1" class="ltx_Math" alttext="S_{l}" display="inline"><semantics id="S3.SS3.SSS0.Px1.p2.7.m4.1a"><msub id="S3.SS3.SSS0.Px1.p2.7.m4.1.1" xref="S3.SS3.SSS0.Px1.p2.7.m4.1.1.cmml"><mi id="S3.SS3.SSS0.Px1.p2.7.m4.1.1.2" xref="S3.SS3.SSS0.Px1.p2.7.m4.1.1.2.cmml">S</mi><mi id="S3.SS3.SSS0.Px1.p2.7.m4.1.1.3" xref="S3.SS3.SSS0.Px1.p2.7.m4.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p2.7.m4.1b"><apply id="S3.SS3.SSS0.Px1.p2.7.m4.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.7.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p2.7.m4.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.7.m4.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p2.7.m4.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p2.7.m4.1.1.2">𝑆</ci><ci id="S3.SS3.SSS0.Px1.p2.7.m4.1.1.3.cmml" xref="S3.SS3.SSS0.Px1.p2.7.m4.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p2.7.m4.1c">S_{l}</annotation></semantics></math> with <math id="S3.SS3.SSS0.Px1.p2.8.m5.1" class="ltx_Math" alttext="\alpha_{1}+\alpha_{2}=1" display="inline"><semantics id="S3.SS3.SSS0.Px1.p2.8.m5.1a"><mrow id="S3.SS3.SSS0.Px1.p2.8.m5.1.1" xref="S3.SS3.SSS0.Px1.p2.8.m5.1.1.cmml"><mrow id="S3.SS3.SSS0.Px1.p2.8.m5.1.1.2" xref="S3.SS3.SSS0.Px1.p2.8.m5.1.1.2.cmml"><msub id="S3.SS3.SSS0.Px1.p2.8.m5.1.1.2.2" xref="S3.SS3.SSS0.Px1.p2.8.m5.1.1.2.2.cmml"><mi id="S3.SS3.SSS0.Px1.p2.8.m5.1.1.2.2.2" xref="S3.SS3.SSS0.Px1.p2.8.m5.1.1.2.2.2.cmml">α</mi><mn id="S3.SS3.SSS0.Px1.p2.8.m5.1.1.2.2.3" xref="S3.SS3.SSS0.Px1.p2.8.m5.1.1.2.2.3.cmml">1</mn></msub><mo id="S3.SS3.SSS0.Px1.p2.8.m5.1.1.2.1" xref="S3.SS3.SSS0.Px1.p2.8.m5.1.1.2.1.cmml">+</mo><msub id="S3.SS3.SSS0.Px1.p2.8.m5.1.1.2.3" xref="S3.SS3.SSS0.Px1.p2.8.m5.1.1.2.3.cmml"><mi id="S3.SS3.SSS0.Px1.p2.8.m5.1.1.2.3.2" xref="S3.SS3.SSS0.Px1.p2.8.m5.1.1.2.3.2.cmml">α</mi><mn id="S3.SS3.SSS0.Px1.p2.8.m5.1.1.2.3.3" xref="S3.SS3.SSS0.Px1.p2.8.m5.1.1.2.3.3.cmml">2</mn></msub></mrow><mo id="S3.SS3.SSS0.Px1.p2.8.m5.1.1.1" xref="S3.SS3.SSS0.Px1.p2.8.m5.1.1.1.cmml">=</mo><mn id="S3.SS3.SSS0.Px1.p2.8.m5.1.1.3" xref="S3.SS3.SSS0.Px1.p2.8.m5.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p2.8.m5.1b"><apply id="S3.SS3.SSS0.Px1.p2.8.m5.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.8.m5.1.1"><eq id="S3.SS3.SSS0.Px1.p2.8.m5.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.8.m5.1.1.1"></eq><apply id="S3.SS3.SSS0.Px1.p2.8.m5.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p2.8.m5.1.1.2"><plus id="S3.SS3.SSS0.Px1.p2.8.m5.1.1.2.1.cmml" xref="S3.SS3.SSS0.Px1.p2.8.m5.1.1.2.1"></plus><apply id="S3.SS3.SSS0.Px1.p2.8.m5.1.1.2.2.cmml" xref="S3.SS3.SSS0.Px1.p2.8.m5.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p2.8.m5.1.1.2.2.1.cmml" xref="S3.SS3.SSS0.Px1.p2.8.m5.1.1.2.2">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p2.8.m5.1.1.2.2.2.cmml" xref="S3.SS3.SSS0.Px1.p2.8.m5.1.1.2.2.2">𝛼</ci><cn type="integer" id="S3.SS3.SSS0.Px1.p2.8.m5.1.1.2.2.3.cmml" xref="S3.SS3.SSS0.Px1.p2.8.m5.1.1.2.2.3">1</cn></apply><apply id="S3.SS3.SSS0.Px1.p2.8.m5.1.1.2.3.cmml" xref="S3.SS3.SSS0.Px1.p2.8.m5.1.1.2.3"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p2.8.m5.1.1.2.3.1.cmml" xref="S3.SS3.SSS0.Px1.p2.8.m5.1.1.2.3">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p2.8.m5.1.1.2.3.2.cmml" xref="S3.SS3.SSS0.Px1.p2.8.m5.1.1.2.3.2">𝛼</ci><cn type="integer" id="S3.SS3.SSS0.Px1.p2.8.m5.1.1.2.3.3.cmml" xref="S3.SS3.SSS0.Px1.p2.8.m5.1.1.2.3.3">2</cn></apply></apply><cn type="integer" id="S3.SS3.SSS0.Px1.p2.8.m5.1.1.3.cmml" xref="S3.SS3.SSS0.Px1.p2.8.m5.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p2.8.m5.1c">\alpha_{1}+\alpha_{2}=1</annotation></semantics></math>. So <math id="S3.SS3.SSS0.Px1.p2.9.m6.2" class="ltx_Math" alttext="\alpha_{1}\in[0,1]" display="inline"><semantics id="S3.SS3.SSS0.Px1.p2.9.m6.2a"><mrow id="S3.SS3.SSS0.Px1.p2.9.m6.2.3" xref="S3.SS3.SSS0.Px1.p2.9.m6.2.3.cmml"><msub id="S3.SS3.SSS0.Px1.p2.9.m6.2.3.2" xref="S3.SS3.SSS0.Px1.p2.9.m6.2.3.2.cmml"><mi id="S3.SS3.SSS0.Px1.p2.9.m6.2.3.2.2" xref="S3.SS3.SSS0.Px1.p2.9.m6.2.3.2.2.cmml">α</mi><mn id="S3.SS3.SSS0.Px1.p2.9.m6.2.3.2.3" xref="S3.SS3.SSS0.Px1.p2.9.m6.2.3.2.3.cmml">1</mn></msub><mo id="S3.SS3.SSS0.Px1.p2.9.m6.2.3.1" xref="S3.SS3.SSS0.Px1.p2.9.m6.2.3.1.cmml">∈</mo><mrow id="S3.SS3.SSS0.Px1.p2.9.m6.2.3.3.2" xref="S3.SS3.SSS0.Px1.p2.9.m6.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS3.SSS0.Px1.p2.9.m6.2.3.3.2.1" xref="S3.SS3.SSS0.Px1.p2.9.m6.2.3.3.1.cmml">[</mo><mn id="S3.SS3.SSS0.Px1.p2.9.m6.1.1" xref="S3.SS3.SSS0.Px1.p2.9.m6.1.1.cmml">0</mn><mo id="S3.SS3.SSS0.Px1.p2.9.m6.2.3.3.2.2" xref="S3.SS3.SSS0.Px1.p2.9.m6.2.3.3.1.cmml">,</mo><mn id="S3.SS3.SSS0.Px1.p2.9.m6.2.2" xref="S3.SS3.SSS0.Px1.p2.9.m6.2.2.cmml">1</mn><mo stretchy="false" id="S3.SS3.SSS0.Px1.p2.9.m6.2.3.3.2.3" xref="S3.SS3.SSS0.Px1.p2.9.m6.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p2.9.m6.2b"><apply id="S3.SS3.SSS0.Px1.p2.9.m6.2.3.cmml" xref="S3.SS3.SSS0.Px1.p2.9.m6.2.3"><in id="S3.SS3.SSS0.Px1.p2.9.m6.2.3.1.cmml" xref="S3.SS3.SSS0.Px1.p2.9.m6.2.3.1"></in><apply id="S3.SS3.SSS0.Px1.p2.9.m6.2.3.2.cmml" xref="S3.SS3.SSS0.Px1.p2.9.m6.2.3.2"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p2.9.m6.2.3.2.1.cmml" xref="S3.SS3.SSS0.Px1.p2.9.m6.2.3.2">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p2.9.m6.2.3.2.2.cmml" xref="S3.SS3.SSS0.Px1.p2.9.m6.2.3.2.2">𝛼</ci><cn type="integer" id="S3.SS3.SSS0.Px1.p2.9.m6.2.3.2.3.cmml" xref="S3.SS3.SSS0.Px1.p2.9.m6.2.3.2.3">1</cn></apply><interval closure="closed" id="S3.SS3.SSS0.Px1.p2.9.m6.2.3.3.1.cmml" xref="S3.SS3.SSS0.Px1.p2.9.m6.2.3.3.2"><cn type="integer" id="S3.SS3.SSS0.Px1.p2.9.m6.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.9.m6.1.1">0</cn><cn type="integer" id="S3.SS3.SSS0.Px1.p2.9.m6.2.2.cmml" xref="S3.SS3.SSS0.Px1.p2.9.m6.2.2">1</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p2.9.m6.2c">\alpha_{1}\in[0,1]</annotation></semantics></math> adjusts the final score towards local content referring or global semantics. Note that we introduce <math id="S3.SS3.SSS0.Px1.p2.10.m7.1" class="ltx_Math" alttext="S_{e}" display="inline"><semantics id="S3.SS3.SSS0.Px1.p2.10.m7.1a"><msub id="S3.SS3.SSS0.Px1.p2.10.m7.1.1" xref="S3.SS3.SSS0.Px1.p2.10.m7.1.1.cmml"><mi id="S3.SS3.SSS0.Px1.p2.10.m7.1.1.2" xref="S3.SS3.SSS0.Px1.p2.10.m7.1.1.2.cmml">S</mi><mi id="S3.SS3.SSS0.Px1.p2.10.m7.1.1.3" xref="S3.SS3.SSS0.Px1.p2.10.m7.1.1.3.cmml">e</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p2.10.m7.1b"><apply id="S3.SS3.SSS0.Px1.p2.10.m7.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.10.m7.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p2.10.m7.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.10.m7.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p2.10.m7.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p2.10.m7.1.1.2">𝑆</ci><ci id="S3.SS3.SSS0.Px1.p2.10.m7.1.1.3.cmml" xref="S3.SS3.SSS0.Px1.p2.10.m7.1.1.3">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p2.10.m7.1c">S_{e}</annotation></semantics></math> to measure difference between local and global scores. If the expression is more related to the target object, <math id="S3.SS3.SSS0.Px1.p2.11.m8.1" class="ltx_Math" alttext="S_{e}" display="inline"><semantics id="S3.SS3.SSS0.Px1.p2.11.m8.1a"><msub id="S3.SS3.SSS0.Px1.p2.11.m8.1.1" xref="S3.SS3.SSS0.Px1.p2.11.m8.1.1.cmml"><mi id="S3.SS3.SSS0.Px1.p2.11.m8.1.1.2" xref="S3.SS3.SSS0.Px1.p2.11.m8.1.1.2.cmml">S</mi><mi id="S3.SS3.SSS0.Px1.p2.11.m8.1.1.3" xref="S3.SS3.SSS0.Px1.p2.11.m8.1.1.3.cmml">e</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p2.11.m8.1b"><apply id="S3.SS3.SSS0.Px1.p2.11.m8.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.11.m8.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p2.11.m8.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.11.m8.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p2.11.m8.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p2.11.m8.1.1.2">𝑆</ci><ci id="S3.SS3.SSS0.Px1.p2.11.m8.1.1.3.cmml" xref="S3.SS3.SSS0.Px1.p2.11.m8.1.1.3">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p2.11.m8.1c">S_{e}</annotation></semantics></math> becomes higher after visual prompting for object highlights. After computing <math id="S3.SS3.SSS0.Px1.p2.12.m9.1" class="ltx_Math" alttext="S_{f}" display="inline"><semantics id="S3.SS3.SSS0.Px1.p2.12.m9.1a"><msub id="S3.SS3.SSS0.Px1.p2.12.m9.1.1" xref="S3.SS3.SSS0.Px1.p2.12.m9.1.1.cmml"><mi id="S3.SS3.SSS0.Px1.p2.12.m9.1.1.2" xref="S3.SS3.SSS0.Px1.p2.12.m9.1.1.2.cmml">S</mi><mi id="S3.SS3.SSS0.Px1.p2.12.m9.1.1.3" xref="S3.SS3.SSS0.Px1.p2.12.m9.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p2.12.m9.1b"><apply id="S3.SS3.SSS0.Px1.p2.12.m9.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.12.m9.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p2.12.m9.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.12.m9.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p2.12.m9.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p2.12.m9.1.1.2">𝑆</ci><ci id="S3.SS3.SSS0.Px1.p2.12.m9.1.1.3.cmml" xref="S3.SS3.SSS0.Px1.p2.12.m9.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p2.12.m9.1c">S_{f}</annotation></semantics></math>, we set a dynamic threshold to dropout expressions. This is because <math id="S3.SS3.SSS0.Px1.p2.13.m10.1" class="ltx_Math" alttext="S_{f}" display="inline"><semantics id="S3.SS3.SSS0.Px1.p2.13.m10.1a"><msub id="S3.SS3.SSS0.Px1.p2.13.m10.1.1" xref="S3.SS3.SSS0.Px1.p2.13.m10.1.1.cmml"><mi id="S3.SS3.SSS0.Px1.p2.13.m10.1.1.2" xref="S3.SS3.SSS0.Px1.p2.13.m10.1.1.2.cmml">S</mi><mi id="S3.SS3.SSS0.Px1.p2.13.m10.1.1.3" xref="S3.SS3.SSS0.Px1.p2.13.m10.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p2.13.m10.1b"><apply id="S3.SS3.SSS0.Px1.p2.13.m10.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.13.m10.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p2.13.m10.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.13.m10.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p2.13.m10.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p2.13.m10.1.1.2">𝑆</ci><ci id="S3.SS3.SSS0.Px1.p2.13.m10.1.1.3.cmml" xref="S3.SS3.SSS0.Px1.p2.13.m10.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p2.13.m10.1c">S_{f}</annotation></semantics></math> is based on CLIP’s preference that a small target object with well matched expression achieves a lower score than a large object with mismatched expression. Therefore, we use provided expression (for images from REC) or category name (for images out of REC) to compute a final score, and dropout generated instructions whose <math id="S3.SS3.SSS0.Px1.p2.14.m11.1" class="ltx_Math" alttext="S_{f}" display="inline"><semantics id="S3.SS3.SSS0.Px1.p2.14.m11.1a"><msub id="S3.SS3.SSS0.Px1.p2.14.m11.1.1" xref="S3.SS3.SSS0.Px1.p2.14.m11.1.1.cmml"><mi id="S3.SS3.SSS0.Px1.p2.14.m11.1.1.2" xref="S3.SS3.SSS0.Px1.p2.14.m11.1.1.2.cmml">S</mi><mi id="S3.SS3.SSS0.Px1.p2.14.m11.1.1.3" xref="S3.SS3.SSS0.Px1.p2.14.m11.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p2.14.m11.1b"><apply id="S3.SS3.SSS0.Px1.p2.14.m11.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.14.m11.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p2.14.m11.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.14.m11.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p2.14.m11.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p2.14.m11.1.1.2">𝑆</ci><ci id="S3.SS3.SSS0.Px1.p2.14.m11.1.1.3.cmml" xref="S3.SS3.SSS0.Px1.p2.14.m11.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p2.14.m11.1c">S_{f}</annotation></semantics></math> is lower than this score.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2310.05136/assets/x4.png" id="S3.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="211" height="86" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Mining commonalities among multi-objects via expression concatenation and text semantic clustering, followed by LLaMA descriptions on each cluster center.</figcaption>
</figure>
</section>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Multi-Objects Expression Generation</h3>

<div id="S3.SS4.p1" class="ltx_para ltx_noindent">
<p id="S3.SS4.p1.1" class="ltx_p">Our expression generation pipeline illustrated above targets for each object independently. In practice, users may refer to multiple objects in one image. We study common user expressions for multi-objects, and conclude them into two aspects. The first one contains splicing expressions that combine different single object expressions with ‘and’ or comma. In this case, the objects mentioned in the expression are not related to each other. The second aspect contains generalization expressions that summarize the common properties of multiple objects (e.g., color, category, or location) to produce an abstract and conceptual description. It resembles mining similarities between multiple objects and thus is not straightforward to conclude. Therefore, we need to discover object combinations where similar properties may exist, and then summarize the commonalities among them to constitute the summarized expressions.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para ltx_noindent">
<p id="S3.SS4.p2.1" class="ltx_p">Our process to produce summarized expression is shown in Fig. <a href="#S3.F4" title="Figure 4 ‣ Visual Prompting ‣ 3.3 Expression Dropout ‣ 3 InstructDET ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. For each object, we first concatenate all its related expressions with commas. Through this concatenation, we can obtain this object expression from different perspectives (i.e., different properties). Then, we use the text encoder in BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a href="#bib.bib6" title="" class="ltx_ref">2018</a>)</cite> to map this concatenated expression to a semantic embedding space. As such, we obtain embeddings of concatenated expressions from all objects. Then, we cluster these embeddings into indeterminate number of clusters by using DBSCAN <cite class="ltx_cite ltx_citemacro_citep">(Ester et al., <a href="#bib.bib11" title="" class="ltx_ref">1996</a>)</cite> method. We use LLaMA to generate text from each cluster center. The details of using LLaMA to mine object commonalities are in Sec. <a href="#A5" title="Appendix E Instruction Generation for Mining Clusters of Multi-objects ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E</span></a>. The generated text indicates the summarized expression we aim to produce for multiple objects. We also note that there will be many clusters containing one point, which means that object represented by this point has no commonality with other objects.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para ltx_noindent">
<p id="S3.SS4.p3.1" class="ltx_p"><span id="S3.SS4.p3.1.1" class="ltx_text ltx_font_bold">Post Processing</span>.
After generating expressions for single and multiple objects, we verify and remove the expressions that repeatedly pertain to the same object. Then, we utilize LLaMA to diversify these expressions while preserving their original intents. We observe that for different objects in one image, the expression for each object may be similar to others. These expressions are ambiguous since we can not refer to a unique object based on their referring. Nevertheless, we transfer these expressions to refer to multi-objects since they express a set of objects in one image. This transfer further augments multi-object referring expressions. Finally, we collect these remaining expressions after post processing as instructions. Together with corresponding object bbxs and images, we construct our InDET dataset by incorporating diversified object detection instructions encompassing user intentions.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Dataset Analysis</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">Our InDET dataset contains images from MSCOCO <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a href="#bib.bib31" title="" class="ltx_ref">2014</a>)</cite>, Flicker <cite class="ltx_cite ltx_citemacro_citep">(Plummer et al., <a href="#bib.bib41" title="" class="ltx_ref">2015</a>)</cite>, and Objects365 <cite class="ltx_cite ltx_citemacro_citep">(Shao et al., <a href="#bib.bib44" title="" class="ltx_ref">2019</a>)</cite>. There are 120.6K images with 908.4K referring object sets in total. Together with original expressions, there are 3.6M instructions in total, making InDET the largest real-world REC dataset at present. The average instruction length is 6.2 words and the vocabulary size is 63k words, which surpasses existing automatically annotated datasets in terms of instruction quantity, richness, and vocabulary breadth. We split the images into training, validation, and testing sets, with the corresponding instruction amount of 3139K, 240K, and 247K, respectively. In the following, we first propose a guideline that represent common user intentions and divides existing instructions into 6 groups. Then, we analyze all the instructions in InDET according to this guideline to show how our InDET advances REC scenario compared to existing datasets.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Instruction guideline and samples. Our guideline contains 6 aspects and covers common user intentions. These aspects are built upon object category, attribute, and relations with different emphasis levels. We use <math id="S4.T1.3.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T1.3.m1.1b"><mo id="S4.T1.3.m1.1.1" xref="S4.T1.3.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.m1.1c"><ci id="S4.T1.3.m1.1.1.cmml" xref="S4.T1.3.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.m1.1d">\star</annotation></semantics></math> and <math id="S4.T1.4.m2.2" class="ltx_Math" alttext="\star\star" display="inline"><semantics id="S4.T1.4.m2.2b"><mrow id="S4.T1.4.m2.2.3.2" xref="S4.T1.4.m2.2.3.1.cmml"><mo id="S4.T1.4.m2.1.1" xref="S4.T1.4.m2.1.1.cmml">⋆</mo><mo lspace="0em" id="S4.T1.4.m2.2.3.2.1" xref="S4.T1.4.m2.2.3.1.cmml">⁣</mo><mo id="S4.T1.4.m2.2.2" xref="S4.T1.4.m2.2.2.cmml">⋆</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.4.m2.2c"><list id="S4.T1.4.m2.2.3.1.cmml" xref="S4.T1.4.m2.2.3.2"><ci id="S4.T1.4.m2.1.1.cmml" xref="S4.T1.4.m2.1.1">⋆</ci><ci id="S4.T1.4.m2.2.2.cmml" xref="S4.T1.4.m2.2.2">⋆</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.m2.2d">\star\star</annotation></semantics></math> to indicate the description level of using a single phrase, and using multiple phrases, respectively.</figcaption><img src="/html/2310.05136/assets/x5.png" id="S4.T1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="338" height="141" alt="[Uncaptioned image]">
</figure>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.1" class="ltx_p"><span id="S4.p2.1.1" class="ltx_text ltx_font_bold">Instruction Guideline</span>.
The instructions in InDET dataset describe objects from various perspectives. We observe that these descriptions all focus on object category, attribute, and relations, but with different emphasis extent. Based on expression complexity, we establish a guideline that divides all instructions into 6 groups. Each group reflects one level of emphasis on category, attribute, and relations. Table <a href="#S4.T1" title="Table 1 ‣ 4 Dataset Analysis ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows our guideline and examples. The first four groups are for single object and the last two groups are for multiple objects. In the first group (G1), there is only one single phrase to describe object category, which is similar to the traditional object detection task. From G2 to G4, more phrases are involved to describe the target object. For G5, we construct a spliced form to combine instructions from different single objects. In G6, the instruction is a general description of commonality between multiple objects. To this end, the instructions from G1 to G6 gradually introduces semantic understanding, visual language grounding, and logic reasoning for ROD. After guideline construction, we use LLaMA to assign each instruction into our groups by using in-context learning that let LLaMA to understand assigning principles and in-context assigning examples. The detailed usage of LLaMA for instruction assign is shown in Sec. <a href="#A6" title="Appendix F Instruction Grouping ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">F</span></a>.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2310.05136/assets/x6.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="380" height="95" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Dataset analysis of expression length, diversity and group distributions.</figcaption>
</figure>
<div id="S4.p3" class="ltx_para ltx_noindent">
<p id="S4.p3.1" class="ltx_p"><span id="S4.p3.1.1" class="ltx_text ltx_font_bold">Instruction Length, Diversity, and Aspect Ratio Distributions</span>.
We analyze our InDET from the instruction length, diversity, and ratio distribution in our guideline groups. The RefCOCO and Flicker datasets are introduced for comparison.
Fig. <a href="#S4.F5" title="Figure 5 ‣ 4 Dataset Analysis ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>(a) shows the number of word distribution where the instruction of InDET contains more words than the other two datasets. Moreover, there are 100K instructions in our InDET consist of more than 10 words, while other datasets do not contain such informative expressions. In Fig. <a href="#S4.F5" title="Figure 5 ‣ 4 Dataset Analysis ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>(b), we show diversity comparison where we use CLIP to map all instructions into a semantic space. Then, for the same target objects we compute average pairwise cosine similarity. The results show that our InDET contains lower value than other datasets, which indicates that our instructions are more diverse when describing the same target object. In Fig. <a href="#S4.F5" title="Figure 5 ‣ 4 Dataset Analysis ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>(c), we show aspect ratio distribution of expressions assigned by our guideline. For existing datasets, user expressions commonly exist in G1 and G2. In contrast to the user expressions that seldom exist from G3 to G5 for Flicker, and seldom exist in G5 and G6 for RefCOCO, the instructions in our InDET exist normally in all groups. This distribution shows that our InDET is more effective to encompass common user intentions, especially for multiple objects. By leveraging our InDET, the ROD model becomes more practically applicable.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Referring Object Detection</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p">In this section, we illustrate our model design for ROD task. We notice that ROD shares little difference with visual grounding (VG). First, ROD produces uncertain number of object bbxs (i.e., 0, 1, or multiple) based on one input instruction, as shown in Fig. <a href="#S0.F1" title="Figure 1 ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Second, ROD supports abstract and summarized object descriptions (e.g., “all objects on the table”) that do not clearly refer to specific objects such as “bottle”, “orange”, and “knife”. As recent VG models <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a href="#bib.bib57" title="" class="ltx_ref">2022</a>; Liu et al., <a href="#bib.bib34" title="" class="ltx_ref">2023b</a>)</cite> require a one-by-one verification between visual objects and expression words, they are not able to execute such instructions. Motivated by the difference, we set up a conventional framework from DETR-based VG models <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al., <a href="#bib.bib59" title="" class="ltx_ref">2022</a>; Yan et al., <a href="#bib.bib52" title="" class="ltx_ref">2023</a>)</cite>. Fig. <a href="#S5.F6" title="Figure 6 ‣ 5 Referring Object Detection ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows an overview of our DROD model. We illustrate key steps as follows:</p>
</div>
<figure id="S5.F6" class="ltx_figure"><img src="/html/2310.05136/assets/x7.png" id="S5.F6.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="211" height="99" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>An overview of our diversified referring object detection (DROD) model.</figcaption>
</figure>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.2" class="ltx_p">Given an image with text instruction, we use visual and text encoders <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy et al., <a href="#bib.bib9" title="" class="ltx_ref">2021</a>; Devlin et al., <a href="#bib.bib6" title="" class="ltx_ref">2018</a>)</cite> to obtain their embeddings. Then, we use a bi-directional cross attention module to perform multi-modality embedding fusion. For the fused visual embedding, we sent it to the transformer encoder and decoder structure <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al., <a href="#bib.bib60" title="" class="ltx_ref">2020</a>)</cite> with <math id="S5.p2.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S5.p2.1.m1.1a"><mi id="S5.p2.1.m1.1.1" xref="S5.p2.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.p2.1.m1.1b"><ci id="S5.p2.1.m1.1.1.cmml" xref="S5.p2.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.1.m1.1c">N</annotation></semantics></math> learnable queries as position priors  <cite class="ltx_cite ltx_citemacro_citep">(Meinhardt et al., <a href="#bib.bib37" title="" class="ltx_ref">2022</a>; Wang et al., <a href="#bib.bib50" title="" class="ltx_ref">2021</a>)</cite>. Then, the decoder produces <math id="S5.p2.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S5.p2.2.m2.1a"><mi id="S5.p2.2.m2.1.1" xref="S5.p2.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.p2.2.m2.1b"><ci id="S5.p2.2.m2.1.1.cmml" xref="S5.p2.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.2.m2.1c">N</annotation></semantics></math> instance proposals for further selection. For the fused text embedding, we pass it through a global average pooling and MLP for text2visual embedding space mapping. Finally, we use cosine similarity to match proposals and mapped text embedding. During the training stage, we use confidence loss and localization loss via supervised learning. During the inference stage, we select proposals whose matching scores are above a predefined threshold, which allows our model to produce arbitrary number of bbxs for diversified instruction execution. A detailed model design and experimental configurations are in Sec. <a href="#A7" title="Appendix G Model and Implementation Details ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">G</span></a>.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Experiments</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.1" class="ltx_p">We evaluate the ROD performance on standard VG benchmarks (i.e., RefCOCO, RefCOCO+, and RefCOCOg) and our InDET dataset. As illustrated in Sec. <a href="#S4" title="4 Dataset Analysis ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, the images with marked objects of our InDET dataset are collected from existing datasets while the instructions are significantly enriched. We split the training and test set of InDET following RefCOCO/g/+ where the test set contains 6.5k images with an increased number of instructions to 315K. Moreover, these instructions are assigned to 6 groups according to our guideline. The performance on each group reflects how VG methods perform when processing different aspects of user instructions. The comparing methods in our experiments are from recent VG methods including MDETR <cite class="ltx_cite ltx_citemacro_citep">(Kamath et al., <a href="#bib.bib18" title="" class="ltx_ref">2021</a>)</cite>, Grounding-DINO <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib34" title="" class="ltx_ref">2023b</a>)</cite> and UNINEXT <cite class="ltx_cite ltx_citemacro_citep">(Yan et al., <a href="#bib.bib52" title="" class="ltx_ref">2023</a>)</cite>. Due to page limit, we show the evaluation results on our InDET, our InDET with shuffled expression, and standard benchmarks. Model training, and ablation studies on partial LLaVA finetuning and visual prompt selection are provided in Sec. <a href="#A7" title="Appendix G Model and Implementation Details ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">G</span></a> and Sec. <a href="#A8" title="Appendix H Ablation Studies ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">H</span></a>. We also provide visual comparison results of these methods in Sec. <a href="#A1" title="Appendix A Visual Comparison Results ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
<figure id="S6.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Evaluation results on our InDET and shuffled InDET test sets. We show the object bbx average precision (AP) values (<math id="S6.T2.2.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S6.T2.2.m1.1b"><mo id="S6.T2.2.m1.1.1" xref="S6.T2.2.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S6.T2.2.m1.1c"><csymbol cd="latexml" id="S6.T2.2.m1.1.1.cmml" xref="S6.T2.2.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.2.m1.1d">\%</annotation></semantics></math>) of these two test sets with a slash (‘/’) separation.</figcaption>
<table id="S6.T2.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T2.3.1.1" class="ltx_tr">
<td id="S6.T2.3.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" style="padding:1pt 0.0pt;" rowspan="2"><span id="S6.T2.3.1.1.1.1" class="ltx_text">Method</span></td>
<td id="S6.T2.3.1.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding:1pt 0.0pt;" rowspan="2"><span id="S6.T2.3.1.1.2.1" class="ltx_text">Backbone</span></td>
<td id="S6.T2.3.1.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding:1pt 0.0pt;" rowspan="2"><span id="S6.T2.3.1.1.3.1" class="ltx_text">AP</span></td>
<td id="S6.T2.3.1.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;" colspan="6">AP by Group</td>
</tr>
<tr id="S6.T2.3.2.2" class="ltx_tr">
<td id="S6.T2.3.2.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">G1</td>
<td id="S6.T2.3.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">G2</td>
<td id="S6.T2.3.2.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">G3</td>
<td id="S6.T2.3.2.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">G4</td>
<td id="S6.T2.3.2.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">G5</td>
<td id="S6.T2.3.2.2.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">G6</td>
</tr>
<tr id="S6.T2.3.3.3" class="ltx_tr">
<td id="S6.T2.3.3.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" style="padding:1pt 0.0pt;">MDETR</td>
<td id="S6.T2.3.3.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding:1pt 0.0pt;">ResNet101</td>
<td id="S6.T2.3.3.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding:1pt 0.0pt;">34.86 / 31.21</td>
<td id="S6.T2.3.3.3.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;">47.44 / 46.61</td>
<td id="S6.T2.3.3.3.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;">46.79 / 42.55</td>
<td id="S6.T2.3.3.3.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;">34.14 / 28.13</td>
<td id="S6.T2.3.3.3.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;">23.22 / 16.86</td>
<td id="S6.T2.3.3.3.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;">25.91 / 23.52</td>
<td id="S6.T2.3.3.3.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;">28.17 / 23.66</td>
</tr>
<tr id="S6.T2.3.4.4" class="ltx_tr">
<td id="S6.T2.3.4.4.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" style="padding:1pt 0.0pt;">G-DINO</td>
<td id="S6.T2.3.4.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:1pt 0.0pt;">SwinB</td>
<td id="S6.T2.3.4.4.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:1pt 0.0pt;">35.96 / 30.43</td>
<td id="S6.T2.3.4.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">47.10 / 45.91</td>
<td id="S6.T2.3.4.4.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">47.17 / 42.56</td>
<td id="S6.T2.3.4.4.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">35.29 / 27.28</td>
<td id="S6.T2.3.4.4.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">26.84 / 18.46</td>
<td id="S6.T2.3.4.4.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">27.95 / 23.74</td>
<td id="S6.T2.3.4.4.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">27.61 / 23.57</td>
</tr>
<tr id="S6.T2.3.5.5" class="ltx_tr">
<td id="S6.T2.3.5.5.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" style="padding:1pt 0.0pt;">UNINEXT</td>
<td id="S6.T2.3.5.5.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:1pt 0.0pt;">ResNet50</td>
<td id="S6.T2.3.5.5.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:1pt 0.0pt;">43.37 / 37.61</td>
<td id="S6.T2.3.5.5.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">54.49 / 53.09</td>
<td id="S6.T2.3.5.5.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">54.52 / 49.91</td>
<td id="S6.T2.3.5.5.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">44.49 / 35.59</td>
<td id="S6.T2.3.5.5.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">37.17 / 28.30</td>
<td id="S6.T2.3.5.5.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">31.41 / 28.28</td>
<td id="S6.T2.3.5.5.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">32.01 / 27.52</td>
</tr>
<tr id="S6.T2.3.6.6" class="ltx_tr">
<td id="S6.T2.3.6.6.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" style="padding:1pt 0.0pt;">DROD (Ours)</td>
<td id="S6.T2.3.6.6.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding:1pt 0.0pt;">ResNet50</td>
<td id="S6.T2.3.6.6.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding:1pt 0.0pt;">62.24 / 53.78</td>
<td id="S6.T2.3.6.6.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;">67.14 / 65.08</td>
<td id="S6.T2.3.6.6.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;">67.34 / 61.56</td>
<td id="S6.T2.3.6.6.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;">60.89 / 48.82</td>
<td id="S6.T2.3.6.6.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;">55.10 / 41.50</td>
<td id="S6.T2.3.6.6.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;">70.15 / 64.64</td>
<td id="S6.T2.3.6.6.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;">74.22 / 67.11</td>
</tr>
<tr id="S6.T2.3.7.7" class="ltx_tr">
<td id="S6.T2.3.7.7.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_b ltx_border_r" style="padding:1pt 0.0pt;">DROD (Ours)</td>
<td id="S6.T2.3.7.7.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r" style="padding:1pt 0.0pt;">ViT-H</td>
<td id="S6.T2.3.7.7.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r" style="padding:1pt 0.0pt;">66.90 / 57.32</td>
<td id="S6.T2.3.7.7.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b" style="padding:1pt 0.0pt;">72.53 / 69.79</td>
<td id="S6.T2.3.7.7.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b" style="padding:1pt 0.0pt;">72.47 / 65.44</td>
<td id="S6.T2.3.7.7.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b" style="padding:1pt 0.0pt;">66.42 / 52.50</td>
<td id="S6.T2.3.7.7.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b" style="padding:1pt 0.0pt;">59.86 / 46.01</td>
<td id="S6.T2.3.7.7.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b" style="padding:1pt 0.0pt;">73.34 / 67.82</td>
<td id="S6.T2.3.7.7.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b" style="padding:1pt 0.0pt;">75.92 / 68.73</td>
</tr>
</tbody>
</table>
</figure>
<div id="S6.p2" class="ltx_para ltx_noindent">
<p id="S6.p2.1" class="ltx_p">In our InDET test set, we compare our DROD model to other methods under the evaluation metric of object bbx average precision with a threshold of 0.5. On the other hand, we investigate whether these methods have truly comprehended the meaning of instruction, or they perform ROD only based on the key words (e.g., noun) without comprehending the whole expression. So we shuffle the InDET test set by randomly ordering the words in each instruction. We produce results of existing VG methods on our InDET test set without assuming object numbers in advance. For one method, if its performance drops more on the shuffled data, this method is shown to better comprehend the meaning of instruction.</p>
</div>
<div id="S6.p3" class="ltx_para ltx_noindent">
<p id="S6.p3.1" class="ltx_p">Table <a href="#S6.T2" title="Table 2 ‣ 6 Experiments ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the evaluation results. Overall, UNINEXT achieves a higher AP than MDETR (i.e., 43.37 v.s. 34.86) in our InDET test set, while decreasing more than MDETR (i.e., 37.61 v.s. 31.21) in shuffled data. This indicates that UNINEXT is more effective than MDETR for ROD and better comprehends instruction meaning. Meanwhile, UNINEXT achieves a higher AP value than Grounding-DINO. In comparison, our DROD largely surpasses UNINEXT (62.24 v.s. 43.37) on the overall AP comparison, and using a VIT encoder further increases our performance. This indicates that our DROD is more effective to comprehend generalized instructions for ROD. Meanwhile, we observe that our performance drop is larger than UNINEXT (8.46 v.s. 5.76), which shows that our model better comprehends different expressions. Specifically for the results in each group, we notice that our performance drop is little in G1, and becomes larger from G2 to G4. This is because more and more words are introduced from G1 to G4 for object description. A random order gradually affects our model comprehension. For G5 and G6, we note that our method largely outperform other methods. The multi-object instructions incorporated in the dataset improves our performance.</p>
</div>
<figure id="S6.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Evaluation results on the RefCOCO/g/+ datasets. We follow evaluation protocols to report AP values (<math id="S6.T3.2.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S6.T3.2.m1.1b"><mo id="S6.T3.2.m1.1.1" xref="S6.T3.2.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S6.T3.2.m1.1c"><csymbol cd="latexml" id="S6.T3.2.m1.1.1.cmml" xref="S6.T3.2.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.2.m1.1d">\%</annotation></semantics></math>) of comparing methods. We use the notations ”CC”, ”VG”, ”OI”, ”O365”, ”RIGame”, for COCO, Visual Genome, OpenImage, Objects365, ReferItGame, respectively.</figcaption>
<table id="S6.T3.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T3.3.1.1" class="ltx_tr">
<th id="S6.T3.3.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding:1pt 0.0pt;" rowspan="2"><span id="S6.T3.3.1.1.1.1" class="ltx_text">Method</span></th>
<th id="S6.T3.3.1.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding:1pt 0.0pt;" rowspan="2"><span id="S6.T3.3.1.1.2.1" class="ltx_text">Backbone</span></th>
<th id="S6.T3.3.1.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding:1pt 0.0pt;" rowspan="2"><span id="S6.T3.3.1.1.3.1" class="ltx_text">Data</span></th>
<th id="S6.T3.3.1.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding:1pt 0.0pt;" colspan="3">RefCOCO</th>
<th id="S6.T3.3.1.1.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding:1pt 0.0pt;" colspan="3">RefCOCO+</th>
<th id="S6.T3.3.1.1.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding:1pt 0.0pt;" colspan="2">RefCOCOg</th>
</tr>
<tr id="S6.T3.3.2.2" class="ltx_tr">
<th id="S6.T3.3.2.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column" style="padding:1pt 0.0pt;">val</th>
<th id="S6.T3.3.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column" style="padding:1pt 0.0pt;">testA</th>
<th id="S6.T3.3.2.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding:1pt 0.0pt;">testB</th>
<th id="S6.T3.3.2.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column" style="padding:1pt 0.0pt;">val</th>
<th id="S6.T3.3.2.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column" style="padding:1pt 0.0pt;">testA</th>
<th id="S6.T3.3.2.2.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding:1pt 0.0pt;">testB</th>
<th id="S6.T3.3.2.2.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column" style="padding:1pt 0.0pt;">val-u</th>
<th id="S6.T3.3.2.2.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column" style="padding:1pt 0.0pt;">test-u</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T3.3.3.1" class="ltx_tr">
<td id="S6.T3.3.3.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" style="padding:1pt 0.0pt;">RefTR</td>
<td id="S6.T3.3.3.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding:1pt 0.0pt;">ResNet101</td>
<td id="S6.T3.3.3.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding:1pt 0.0pt;">VG</td>
<td id="S6.T3.3.3.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;">85.65</td>
<td id="S6.T3.3.3.1.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;">88.73</td>
<td id="S6.T3.3.3.1.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding:1pt 0.0pt;">81.16</td>
<td id="S6.T3.3.3.1.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;">77.55</td>
<td id="S6.T3.3.3.1.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;">82.26</td>
<td id="S6.T3.3.3.1.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding:1pt 0.0pt;">68.99</td>
<td id="S6.T3.3.3.1.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;">79.25</td>
<td id="S6.T3.3.3.1.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;">80.01</td>
</tr>
<tr id="S6.T3.3.4.2" class="ltx_tr">
<td id="S6.T3.3.4.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" style="padding:1pt 0.0pt;">SeqTR</td>
<td id="S6.T3.3.4.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:1pt 0.0pt;">DarkNet53</td>
<td id="S6.T3.3.4.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:1pt 0.0pt;">VG,RIGame,Flickr,RefC</td>
<td id="S6.T3.3.4.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">87.00</td>
<td id="S6.T3.3.4.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">90.15</td>
<td id="S6.T3.3.4.2.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:1pt 0.0pt;">83.59</td>
<td id="S6.T3.3.4.2.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">78.69</td>
<td id="S6.T3.3.4.2.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">84.51</td>
<td id="S6.T3.3.4.2.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:1pt 0.0pt;">71.87</td>
<td id="S6.T3.3.4.2.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">82.69</td>
<td id="S6.T3.3.4.2.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">83.37</td>
</tr>
<tr id="S6.T3.3.5.3" class="ltx_tr">
<td id="S6.T3.3.5.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" style="padding:1pt 0.0pt;">MDETR</td>
<td id="S6.T3.3.5.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:1pt 0.0pt;">ResNet101</td>
<td id="S6.T3.3.5.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:1pt 0.0pt;">GoldG,RefC</td>
<td id="S6.T3.3.5.3.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">86.75</td>
<td id="S6.T3.3.5.3.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">89.58</td>
<td id="S6.T3.3.5.3.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:1pt 0.0pt;">81.41</td>
<td id="S6.T3.3.5.3.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">79.52</td>
<td id="S6.T3.3.5.3.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">84.09</td>
<td id="S6.T3.3.5.3.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:1pt 0.0pt;">70.62</td>
<td id="S6.T3.3.5.3.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">81.64</td>
<td id="S6.T3.3.5.3.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">80.89</td>
</tr>
<tr id="S6.T3.3.6.4" class="ltx_tr">
<td id="S6.T3.3.6.4.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" style="padding:1pt 0.0pt;">G-DINO</td>
<td id="S6.T3.3.6.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:1pt 0.0pt;">SwinB</td>
<td id="S6.T3.3.6.4.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:1pt 0.0pt;">O365,CC,RefC,GoldG,etc</td>
<td id="S6.T3.3.6.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">83.95</td>
<td id="S6.T3.3.6.4.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">87.79</td>
<td id="S6.T3.3.6.4.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:1pt 0.0pt;">79.16</td>
<td id="S6.T3.3.6.4.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">72.91</td>
<td id="S6.T3.3.6.4.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">80.91</td>
<td id="S6.T3.3.6.4.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:1pt 0.0pt;">62.96</td>
<td id="S6.T3.3.6.4.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">76.98</td>
<td id="S6.T3.3.6.4.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">76.76</td>
</tr>
<tr id="S6.T3.3.7.5" class="ltx_tr">
<td id="S6.T3.3.7.5.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" style="padding:1pt 0.0pt;">UNINEXT</td>
<td id="S6.T3.3.7.5.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:1pt 0.0pt;">ResNet50</td>
<td id="S6.T3.3.7.5.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:1pt 0.0pt;">O365,CC,RefC</td>
<td id="S6.T3.3.7.5.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">87.64</td>
<td id="S6.T3.3.7.5.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">90.35</td>
<td id="S6.T3.3.7.5.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:1pt 0.0pt;">83.49</td>
<td id="S6.T3.3.7.5.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">78.14</td>
<td id="S6.T3.3.7.5.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">83.22</td>
<td id="S6.T3.3.7.5.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:1pt 0.0pt;">68.71</td>
<td id="S6.T3.3.7.5.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">80.96</td>
<td id="S6.T3.3.7.5.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">81.86</td>
</tr>
<tr id="S6.T3.3.8.6" class="ltx_tr">
<td id="S6.T3.3.8.6.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_b ltx_border_r" style="padding:1pt 0.0pt;">DROD (Ours)</td>
<td id="S6.T3.3.8.6.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r" style="padding:1pt 0.0pt;">ResNet50</td>
<td id="S6.T3.3.8.6.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r" style="padding:1pt 0.0pt;">O365,CC,InDET</td>
<td id="S6.T3.3.8.6.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b" style="padding:1pt 0.0pt;">88.92</td>
<td id="S6.T3.3.8.6.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b" style="padding:1pt 0.0pt;">90.86</td>
<td id="S6.T3.3.8.6.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r" style="padding:1pt 0.0pt;">85.57</td>
<td id="S6.T3.3.8.6.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b" style="padding:1pt 0.0pt;">78.27</td>
<td id="S6.T3.3.8.6.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b" style="padding:1pt 0.0pt;">83.39</td>
<td id="S6.T3.3.8.6.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r" style="padding:1pt 0.0pt;">71.04</td>
<td id="S6.T3.3.8.6.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b" style="padding:1pt 0.0pt;">83.01</td>
<td id="S6.T3.3.8.6.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b" style="padding:1pt 0.0pt;">82.91</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S6.F7" class="ltx_figure"><img src="/html/2310.05136/assets/x8.png" id="S6.F7.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="169" height="123" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Visual comparison results on a RefCOCO image. The instruction is “person who is <span id="S6.F7.3.1" class="ltx_text ltx_font_bold">NOT</span> holding a bat”.</figcaption>
</figure>
<div id="S6.p4" class="ltx_para ltx_noindent">
<p id="S6.p4.1" class="ltx_p">Besides evaluating our InDET test set, we compare our DROD model with existing VG methods <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al., <a href="#bib.bib59" title="" class="ltx_ref">2022</a>; Li &amp; Sigal, <a href="#bib.bib29" title="" class="ltx_ref">2021</a>)</cite> on the standard VG benchmarks RefCOCO/g/+. Table <a href="#S6.T3" title="Table 3 ‣ 6 Experiments ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the evaluation results. Overall, our DROD model achieves favorable performance on these datasets. This is because our DROD model utilizes InDET dataset where diversified instructions improve model generalizations. By using a conventional ROD model, we improve the VG performance from the data diversity perspective. Furthermore, we show visual comparisons in Fig. <a href="#S6.F7" title="Figure 7 ‣ 6 Experiments ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> where the image is from RefCOCO. Given the instruction that contains opposite description (‘not’), both Grounding-DINO and UNINEXT can not comprehend this meaning by detecting the person not referred to. In comparison, our DROD model comprehends the instruction meaning and executes ROD correctly. This success is because of sufficient data coverage in our InDET dataset where diversified instructions improve model generalizations.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Concluding Remarks</h2>

<div id="S7.p1" class="ltx_para ltx_noindent">
<p id="S7.p1.1" class="ltx_p">We aim to push ROD into practical usage from a data-centric perspective. On one hand, we notice that current REC expressions are insufficient to encompass user detection intentions. On the other hand, foundation models have shown promising generalizations to simulate manual understanding and description abilities. To this end, we develop InstructDET that leverages foundation models to produce human-like expressions in REC, which tends to incorporate common user intentions into ROD training. As a result, our DROD model achieves favorable performance compared to existing VG methods. Since our data-centric method is only able to generalize based on recognized objects in bbxs, we only produce expressions in a closed set. In the future, we can combine our method with open-set object detectors to fully explore in-the-wild images (e.g., Internet images) for comprehensive user expression generation. We expect our DROD model to generalize as much as existing foundation models, and thus take a huge step towards completely solving ROD task.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Akbari et al. (2019)</span>
<span class="ltx_bibblock">
Hassan Akbari, Svebor Karaman, Surabhi Bhargava, Brian Chen, Carl Vondrick, and Shih-Fu Chang.

</span>
<span class="ltx_bibblock">Multi-level multimodal common semantic space for image-phrase grounding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2019.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carion et al. (2020)</span>
<span class="ltx_bibblock">
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko.

</span>
<span class="ltx_bibblock">End-to-end object detection with transformers.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2020)</span>
<span class="ltx_bibblock">
Zhenfang Chen, Peng Wang, Lin Ma, Kwan-Yee K Wong, and Qi Wu.

</span>
<span class="ltx_bibblock">Cops-ref: A new dataset and task on compositional referring expression comprehension.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2021)</span>
<span class="ltx_bibblock">
Zhenfang Chen, Jiayuan Mao, Jiajun Wu, Kwan-Yee Kenneth Wong, Joshua B. Tenenbaum, and Chuang Gan.

</span>
<span class="ltx_bibblock">Grounding physical concepts of objects and events through dynamic visual reasoning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2021.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al. (2021)</span>
<span class="ltx_bibblock">
Jiajun Deng, Zhengyou Yang, Tianlang Chen, Wengang Zhou, and Houqiang Li.

</span>
<span class="ltx_bibblock">Transvg: End-to-end visual grounding with transformers.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">IEEE/CVF International Conference on Computer Vision</em>, 2021.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2018)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint arXiv:1810.04805</em>, 2018.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dídac et al. (2023)</span>
<span class="ltx_bibblock">
Surís Dídac, Sachit Menon, and Carl Vondrick.

</span>
<span class="ltx_bibblock">Vipergpt: Visual inference via python execution for reasoning.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.08128</em>, 2023.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et al. (2023)</span>
<span class="ltx_bibblock">
Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui.

</span>
<span class="ltx_bibblock">A survey on in-context learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint arXiv:2301.00234</em>, 2023.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy et al. (2021)</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.

</span>
<span class="ltx_bibblock">An image is worth 16x16 words: Transformers for image recognition at scale.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2021.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dou et al. (2022)</span>
<span class="ltx_bibblock">
Zi-Yi Dou, Aishwarya Kamath, Zhe Gan, Pengchuan Zhang, JianFeng Wang, Linjie Li, Zicheng Liu, Ce Liu, Yann Lecun, Nanyun Peng, and Jianfeng Gao.

</span>
<span class="ltx_bibblock">Coarse-to-fine vision-language pre-training with fusion in the backbone.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 2022.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ester et al. (1996)</span>
<span class="ltx_bibblock">
Martin Ester, Hans-Peter Kriegel, Jörg Sander, Xiaowei Xu, et al.

</span>
<span class="ltx_bibblock">A density-based algorithm for discovering clusters in large spatial databases with noise.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Special Interest Group on Knowledge Discovery and Data Mining</em>, 1996.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2023)</span>
<span class="ltx_bibblock">
Shangqian Gao, Burak Uzkent, Yilin Shen, Heng Huang, and Hongxia Jin.

</span>
<span class="ltx_bibblock">Learning to jointly share and prune weights for grounding based vision and language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2023.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et al. (2022)</span>
<span class="ltx_bibblock">
Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui.

</span>
<span class="ltx_bibblock">Open-vocabulary object detection via vision and language knowledge distillation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2022.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta &amp; Kembhavi (2023)</span>
<span class="ltx_bibblock">
Tanmay Gupta and Aniruddha Kembhavi.

</span>
<span class="ltx_bibblock">Visual programming: Compositional visual reasoning without training.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2023.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2016)</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2016.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2023)</span>
<span class="ltx_bibblock">
Shuting He, Henghui Ding, Chang Liu, and Xudong Jiang.

</span>
<span class="ltx_bibblock">Grec: Generalized referring expression comprehension.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint arXiv:2308.16182</em>, 2023.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hudson &amp; Manning (2018)</span>
<span class="ltx_bibblock">
Drew Hudson and Christopher Manning.

</span>
<span class="ltx_bibblock">Compositional attention networks for machine reasoning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2018.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kamath et al. (2021)</span>
<span class="ltx_bibblock">
Aishwarya Kamath, Mannat Singh, Yann LeCun, Ishan Misra, Gabriel Synnaeve, and Nicolas Carion.

</span>
<span class="ltx_bibblock">Mdetr–modulated detection for end-to-end multi-modal understanding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">IEEE/CVF International Conference on Computer Vision</em>, 2021.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kazemzadeh et al. (2014)</span>
<span class="ltx_bibblock">
Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg.

</span>
<span class="ltx_bibblock">Referitgame: Referring to objects in photographs of natural scenes.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Conference on Empirical Methods in Natural Language Processing</em>, 2014.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kebe et al. (2021)</span>
<span class="ltx_bibblock">
Gaoussou Youssouf Kebe, Padraig Higgins, Patrick Jenkins, Kasra Darvish, Rishabh Sachdeva, Ryan Barron, John Winder, Donald Engel, Edward Raff, Francis Ferraro, and Cynthia Matuszek.

</span>
<span class="ltx_bibblock">A spoken language dataset of descriptions for speech-based grounded language learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Advances on Neural Information Processing Systems Datasets and Benchmarks Track</em>, 2021.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kirillov et al. (2023)</span>
<span class="ltx_bibblock">
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al.

</span>
<span class="ltx_bibblock">Segment anything.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint arXiv:2304.02643</em>, 2023.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kojima et al. (2023)</span>
<span class="ltx_bibblock">
Noriyuki Kojima, Hadar Averbuch-Elor, and Yoav Artzi.

</span>
<span class="ltx_bibblock">A joint study of phrase grounding and task performance in vision and language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint arXiv:2309.02691</em>, 2023.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishna et al. (2017)</span>
<span class="ltx_bibblock">
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al.

</span>
<span class="ltx_bibblock">Visual genome: Connecting language and vision using crowdsourced dense image annotations.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">International Journal of Computer Vision</em>, 2017.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kuznetsova et al. (2020)</span>
<span class="ltx_bibblock">
Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, Tom Duerig, and Vittorio Ferrari.

</span>
<span class="ltx_bibblock">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">International Journal of Computer Vision</em>, 2020.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023a)</span>
<span class="ltx_bibblock">
Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu.

</span>
<span class="ltx_bibblock">Otter: A multi-modal model with in-context instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint arXiv:2305.03726</em>, 2023a.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023b)</span>
<span class="ltx_bibblock">
Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao.

</span>
<span class="ltx_bibblock">Llava-med: Training a large language-and-vision assistant for biomedicine in one day.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint arXiv:2306.00890</em>, 2023b.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023c)</span>
<span class="ltx_bibblock">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.

</span>
<span class="ltx_bibblock">Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint arXiv:2301.12597</em>, 2023c.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2022a)</span>
<span class="ltx_bibblock">
Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, and Jianfeng Gao.

</span>
<span class="ltx_bibblock">Grounded language-image pre-training.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2022a.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li &amp; Sigal (2021)</span>
<span class="ltx_bibblock">
Muchen Li and Leonid Sigal.

</span>
<span class="ltx_bibblock">Referring transformer: A one-step approach to multi-task visual grounding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 2021.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2022b)</span>
<span class="ltx_bibblock">
Xiang Li, John Thickstun, Ishaan Gulrajani, Percy S Liang, and Tatsunori B Hashimoto.

</span>
<span class="ltx_bibblock">Diffusion-lm improves controllable text generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 2022b.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2014)</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, 2014.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023a)</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.

</span>
<span class="ltx_bibblock">Visual instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint arXiv:2304.08485</em>, 2023a.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2019)</span>
<span class="ltx_bibblock">
Runtao Liu, Chenxi Liu, Yutong Bai, and Alan L Yuille.

</span>
<span class="ltx_bibblock">Clevr-ref+: Diagnosing visual reasoning with referring expressions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2019.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023b)</span>
<span class="ltx_bibblock">
Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, and Lei Zhang.

</span>
<span class="ltx_bibblock">Grounding dino: Marrying dino with grounded pre-training for open-set object detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint arXiv:2303.05499</em>, 2023b.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov &amp; Hutter (2019)</span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter.

</span>
<span class="ltx_bibblock">Decoupled weight decay regularization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019</em>. OpenReview.net, 2019.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mao et al. (2016)</span>
<span class="ltx_bibblock">
Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy.

</span>
<span class="ltx_bibblock">Generation and comprehension of unambiguous object descriptions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2016.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meinhardt et al. (2022)</span>
<span class="ltx_bibblock">
Tim Meinhardt, Alexander Kirillov, Laura Leal-Taixe, and Christoph Feichtenhofer.

</span>
<span class="ltx_bibblock">Trackformer: Multi-object tracking with transformers.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2022.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et al. (2022)</span>
<span class="ltx_bibblock">
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock">In <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 2022.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Plummer et al. (2022)</span>
<span class="ltx_bibblock">
Bryan Plummer, Nikoli Dryden, Julius Frost, Torsten Hoefler, and Kate Saenko.

</span>
<span class="ltx_bibblock">Neural parameter allocation search.

</span>
<span class="ltx_bibblock">In <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2022.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Plummer et al. (2015)</span>
<span class="ltx_bibblock">
Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik.

</span>
<span class="ltx_bibblock">Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">IEEE/CVF International Conference on Computer Vision</em>, 2015.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2021)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision.

</span>
<span class="ltx_bibblock">In <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, 2021.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shaharabany &amp; Wolf (2023)</span>
<span class="ltx_bibblock">
Tal Shaharabany and Lior Wolf.

</span>
<span class="ltx_bibblock">Similarity maps for self-training weakly-supervised phrase grounding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2023.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao et al. (2019)</span>
<span class="ltx_bibblock">
Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun.

</span>
<span class="ltx_bibblock">Objects365: A large-scale, high-quality dataset for object detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">IEEE/CVF International Conference on Computer Vision</em>, 2019.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shtedritski et al. (2023)</span>
<span class="ltx_bibblock">
Aleksandar Shtedritski, Christian Rupprecht, and Andrea Vedaldi.

</span>
<span class="ltx_bibblock">What does clip know about a red circle? visual prompt engineering for vlms.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint arXiv:2304.06712</em>, 2023.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al. (2023)</span>
<span class="ltx_bibblock">
Wei Su, Peihan Miao, Huanzhang Dou, Gaoang Wang, Liang Qiao, Zheyang Li, and Xi Li.

</span>
<span class="ltx_bibblock">Language adaptive weight generation for multi-task visual grounding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2023.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al. (2020)</span>
<span class="ltx_bibblock">
Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai.

</span>
<span class="ltx_bibblock">Vl-bert: Pre-training of generic visual-linguistic representations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2020.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint arXiv:2302.13971</em>, 2023.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2022)</span>
<span class="ltx_bibblock">
Thomas Wang, Adam Roberts, Daniel Hesslow, Teven Le Scao, Hyung Won Chung, Iz Beltagy, Julien Launay, and Colin Raffel.

</span>
<span class="ltx_bibblock">What language model architecture and pretraining objective works best for zero-shot generalization?

</span>
<span class="ltx_bibblock">In <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, 2022.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2021)</span>
<span class="ltx_bibblock">
Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao Shen, and Huaxia Xia.

</span>
<span class="ltx_bibblock">End-to-end video instance segmentation with transformers.

</span>
<span class="ltx_bibblock">In <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2021.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2023)</span>
<span class="ltx_bibblock">
Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan.

</span>
<span class="ltx_bibblock">Visual chatgpt: Talking, drawing and editing with visual foundation models.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint arXiv:2303.04671</em>, 2023.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et al. (2023)</span>
<span class="ltx_bibblock">
Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Ping Luo, Zehuan Yuan, and Huchuan Lu.

</span>
<span class="ltx_bibblock">Universal instance perception as object discovery and retrieval.

</span>
<span class="ltx_bibblock">In <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2023.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2023a)</span>
<span class="ltx_bibblock">
Lingfeng Yang, Yueze Wang, Xiang Li, Xinlong Wang, and Jian Yang.

</span>
<span class="ltx_bibblock">Fine-grained visual prompting.

</span>
<span class="ltx_bibblock"><em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint arXiv:2306.04356</em>, 2023a.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2023b)</span>
<span class="ltx_bibblock">
Ziyan Yang, Kushal Kafle, Franck Dernoncourt, and Vicente Ordonez.

</span>
<span class="ltx_bibblock">Improving visual grounding by encouraging consistent gradient-based explanations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2023b.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2016)</span>
<span class="ltx_bibblock">
Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg.

</span>
<span class="ltx_bibblock">Modeling context in referring expressions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, 2016.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zang et al. (2022)</span>
<span class="ltx_bibblock">
Yuhang Zang, Wei Li, Kaiyang Zhou, Chen Huang, and Chen Change Loy.

</span>
<span class="ltx_bibblock">Open-vocabulary detr with conditional matching.

</span>
<span class="ltx_bibblock">In <em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, 2022.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2022)</span>
<span class="ltx_bibblock">
Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun Chen, Liunian Li, Xiyang Dai, Lijuan Wang, Lu Yuan, Jenq-Neng Hwang, and Jianfeng Gao.

</span>
<span class="ltx_bibblock">Glipv2: Unifying localization and vision-language understanding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 2022.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2022)</span>
<span class="ltx_bibblock">
Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al.

</span>
<span class="ltx_bibblock">Least-to-most prompting enables complex reasoning in large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint arXiv:2205.10625</em>, 2022.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2022)</span>
<span class="ltx_bibblock">
Chaoyang Zhu, Yiyi Zhou, Yunhang Shen, Gen Luo, Xingjia Pan, Mingbao Lin, Chao Chen, Liujuan Cao, Xiaoshuai Sun, and Rongrong Ji.

</span>
<span class="ltx_bibblock">Seqtr: A simple yet universal network for visual grounding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, 2022.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2020)</span>
<span class="ltx_bibblock">
Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai.

</span>
<span class="ltx_bibblock">Deformable detr: Deformable transformers for end-to-end object detection.

</span>
<span class="ltx_bibblock"><em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint arXiv:2010.04159</em>, 2020.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="Ax1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">Appendix Overview</h2>

<div id="Ax1.p1" class="ltx_para ltx_noindent">
<p id="Ax1.p1.1" class="ltx_p">We provide an overview to present a clear understanding of this section.</p>
</div>
<div id="Ax1.p2" class="ltx_para ltx_noindent">
<ul id="Ax1.I1" class="ltx_itemize">
<li id="Ax1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ax1.I1.i1.p1" class="ltx_para">
<p id="Ax1.I1.i1.p1.1" class="ltx_p">In Sec. <a href="#A1" title="Appendix A Visual Comparison Results ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>, we present visual comparisons of recent VG methods under various instructions.</p>
</div>
</li>
<li id="Ax1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ax1.I1.i2.p1" class="ltx_para">
<p id="Ax1.I1.i2.p1.1" class="ltx_p">In Sec. <a href="#A2" title="Appendix B Expression Generation via MULTI-MODALITY PATHWAY ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a> and Sec. <a href="#A3" title="Appendix C Expression Generation via SINGLE-MODALITY PATHWAY ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a>, we describe the process of generating expressions from foundation models in both multi-modality and single-modality pathways, respectively.</p>
</div>
</li>
<li id="Ax1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ax1.I1.i3.p1" class="ltx_para">
<p id="Ax1.I1.i3.p1.1" class="ltx_p">In Sec. <a href="#A4" title="Appendix D Visual Prompting and Visual-Textual Matching ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a>, we detail the aspects of visual prompting and visual-textual matching.</p>
</div>
</li>
<li id="Ax1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ax1.I1.i4.p1" class="ltx_para">
<p id="Ax1.I1.i4.p1.1" class="ltx_p">In Sec. <a href="#A5" title="Appendix E Instruction Generation for Mining Clusters of Multi-objects ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E</span></a>, we explain our approach to designing prompts that enable LLaVA to extract commonalities among multiple objects for summarized expression generation.</p>
</div>
</li>
<li id="Ax1.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ax1.I1.i5.p1" class="ltx_para">
<p id="Ax1.I1.i5.p1.1" class="ltx_p">In Sec. <a href="#A6" title="Appendix F Instruction Grouping ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">F</span></a>, we discuss our strategy for designing prompts that allow LLaMA to assign generated instructions to our predefined groups.</p>
</div>
</li>
<li id="Ax1.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ax1.I1.i6.p1" class="ltx_para">
<p id="Ax1.I1.i6.p1.1" class="ltx_p">In Sec. <a href="#A7" title="Appendix G Model and Implementation Details ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">G</span></a>, we provide an overview of our model architecture and implementation specifics.</p>
</div>
</li>
<li id="Ax1.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ax1.I1.i7.p1" class="ltx_para ltx_noindent">
<p id="Ax1.I1.i7.p1.1" class="ltx_p">In Sec. <a href="#A8" title="Appendix H Ablation Studies ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">H</span></a>, we present ablation studies investigating LLaVA fine-tuning in multi-modality pathway and visual prompting selection.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Visual Comparison Results</h2>

<figure id="A1.F8" class="ltx_figure"><img src="/html/2310.05136/assets/x9.png" id="A1.F8.g1" class="ltx_graphics ltx_centering ltx_img_square" width="338" height="326" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Visual comparison results. Our DROD well executes detection instruction compared to Grounding-DINO and UNINEXT.</figcaption>
</figure>
<figure id="A1.F9" class="ltx_figure"><img src="/html/2310.05136/assets/x10.png" id="A1.F9.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="338" height="500" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Visual comparison results. Our DROD well executes detection instruction compared to Grounding-DINO and UNINEXT.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Expression Generation via MULTI-MODALITY PATHWAY</h2>

<div id="A2.p1" class="ltx_para ltx_noindent">
<p id="A2.p1.1" class="ltx_p">We show the text prompts we use below in Table <a href="#A2.T4" title="Table 4 ‣ Appendix B Expression Generation via MULTI-MODALITY PATHWAY ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. For the LLaVA input, We randomly select one text prompt together with one image marked with object bbx. The inputs and samples generated from LLaVA are shown in Fig. <a href="#A2.F10" title="Figure 10 ‣ Appendix B Expression Generation via MULTI-MODALITY PATHWAY ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>.</p>
</div>
<figure id="A2.T4" class="ltx_table"><svg id="A2.T4.pic1" class="ltx_picture" height="138.18" overflow="visible" version="1.1" width="551.18"><g transform="translate(0,138.18) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000" fill-opacity="1.0"><path d="M 0 13.78 L 0 124.4 C 0 132.01 6.17 138.18 13.78 138.18 L 537.4 138.18 C 545.01 138.18 551.18 132.01 551.18 124.4 L 551.18 13.78 C 551.18 6.17 545.01 0 537.4 0 L 13.78 0 C 6.17 0 0 6.17 0 13.78 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 13.78 L 1.97 124.4 C 1.97 130.92 7.26 136.21 13.78 136.21 L 537.4 136.21 C 543.92 136.21 549.21 130.92 549.21 124.4 L 549.21 13.78 C 549.21 7.26 543.92 1.97 537.4 1.97 L 13.78 1.97 C 7.26 1.97 1.97 7.26 1.97 13.78 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="507.87" height="110.62" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span id="A2.T4.pic1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:367.0pt;">
<span id="A2.I1" class="ltx_itemize">
<span id="A2.I1.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="A2.I1.i1.p1" class="ltx_para ltx_noindent">
<span id="A2.I1.i1.p1.1" class="ltx_p">“Describe the objects in the red box.”</span>
</span></span>
<span id="A2.I1.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="A2.I1.i2.p1" class="ltx_para">
<span id="A2.I1.i2.p1.1" class="ltx_p">“Take a look at this image and describe What’s in the red box.”</span>
</span></span>
<span id="A2.I1.i3" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="A2.I1.i3.p1" class="ltx_para ltx_noindent">
<span id="A2.I1.i3.p1.1" class="ltx_p">“Please provide a description of the object in the red box.”</span>
</span></span>
<span id="A2.I1.i4" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="A2.I1.i4.p1" class="ltx_para">
<span id="A2.I1.i4.p1.1" class="ltx_p">“Could you describe the contents in the red box of the image for me?”</span>
</span></span>
<span id="A2.I1.i5" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="A2.I1.i5.p1" class="ltx_para ltx_noindent">
<span id="A2.I1.i5.p1.1" class="ltx_p">“Use one sentence to index the objects in the red box.”</span>
</span></span>
<span id="A2.I1.i6" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="A2.I1.i6.p1" class="ltx_para ltx_noindent">
<span id="A2.I1.i6.p1.1" class="ltx_p">“Output a sentence describing the objects in the red box, so that people can locate the objects without ambiguity through this sentence.”</span>
</span></span>
<span id="A2.I1.i7" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="A2.I1.i7.p1" class="ltx_para ltx_noindent">
<span id="A2.I1.i7.p1.1" class="ltx_p">“Look carefully at the objects in the red box and describe them in one sentence to distinguish them from other objects.”</span>
</span></span>
</span>
</span></foreignObject></g></g></svg>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Text prompts we use for LLaVA inputs.</figcaption>
</figure>
<figure id="A2.F10" class="ltx_figure"><img src="/html/2310.05136/assets/x11.png" id="A2.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="422" height="262" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Examples of text prompts, images with marked object bbxs, and LLaVA outputs.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Expression Generation via SINGLE-MODALITY PATHWAY</h2>

<div id="A3.p1" class="ltx_para ltx_noindent">
<p id="A3.p1.1" class="ltx_p">We choose an image from Objects365 as an example to illustrate the instruction generation pipeline via our single-modality generation pathway. This pathway consists of inputs, two steps, in-context samples, and LLaMA outputs, which are illustrated one-by-one in the following:
<br class="ltx_break"></p>
</div>
<section id="A3.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Inputs</h4>

<div id="A3.SS0.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="A3.SS0.SSS0.Px1.p1.1" class="ltx_p">Our input is an example image with object bbx coordinates, which is shown in Fig. <a href="#A3.F11" title="Figure 11 ‣ Inputs ‣ Appendix C Expression Generation via SINGLE-MODALITY PATHWAY ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>.</p>
</div>
<figure id="A3.F11" class="ltx_figure"><img src="/html/2310.05136/assets/x12.png" id="A3.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="422" height="214" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>An input example. The left part is an image from the Objects365 dataset. The right part is the object category names and corresponding bounding boxes coordinates.</figcaption>
</figure>
</section>
<section id="A3.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Output</h4>

<div id="A3.SS0.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="A3.SS0.SSS0.Px2.p1.1" class="ltx_p">We send the final text prompt shown in Table <a href="#A3.T8" title="Table 8 ‣ Step 2 ‣ Appendix C Expression Generation via SINGLE-MODALITY PATHWAY ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> to LLaMA. Table <a href="#A3.T5" title="Table 5 ‣ Output ‣ Appendix C Expression Generation via SINGLE-MODALITY PATHWAY ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows the output expression of the inputs shown in Fig. <a href="#A3.F11" title="Figure 11 ‣ Inputs ‣ Appendix C Expression Generation via SINGLE-MODALITY PATHWAY ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>. The detailed steps to prepare text prompts are illustrated as follows:</p>
</div>
<figure id="A3.T5" class="ltx_table"><svg id="A3.T5.pic1" class="ltx_picture" height="156.09" overflow="visible" version="1.1" width="551.18"><g transform="translate(0,156.09) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000" fill-opacity="1.0"><path d="M 0 13.78 L 0 142.31 C 0 149.92 6.17 156.09 13.78 156.09 L 537.4 156.09 C 545.01 156.09 551.18 149.92 551.18 142.31 L 551.18 13.78 C 551.18 6.17 545.01 0 537.4 0 L 13.78 0 C 6.17 0 0 6.17 0 13.78 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 13.78 L 1.97 142.31 C 1.97 148.83 7.26 154.12 13.78 154.12 L 537.4 154.12 C 543.92 154.12 549.21 148.83 549.21 142.31 L 549.21 13.78 C 549.21 7.26 543.92 1.97 537.4 1.97 L 13.78 1.97 C 7.26 1.97 1.97 7.26 1.97 13.78 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="507.87" height="128.53" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span id="A3.T5.pic1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:367.0pt;">
<span id="A3.T5.pic1.1.1.1.1.1.1" class="ltx_p"><span id="A3.T5.pic1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">[Fire Truck]</span></span>
<span id="A3.T5.pic1.1.1.1.1.1.2" class="ltx_p">(1) vehicle, emergency vehicle, fire engine, parked outside the fire station, an essential part of the fire station’s resources, essential part of the fire station’s resources</span>
<span id="A3.T5.pic1.1.1.1.1.1.3" class="ltx_p">(2) lined up in a neat row, ready for use, object parked in the row with other fire trucks, object with ladders and equipment</span>
<span id="A3.T5.pic1.1.1.1.1.1.4" class="ltx_p"><span id="A3.T5.pic1.1.1.1.1.1.4.1" class="ltx_text ltx_font_bold">[Street Lights]</span></span>
<span id="A3.T5.pic1.1.1.1.1.1.5" class="ltx_p">(1) light fixtures, outdoor lighting, two lights visible, providing illumination, source of illumination</span>
<span id="A3.T5.pic1.1.1.1.1.1.6" class="ltx_p">(2) objects providing illumination, objects in the parking lot, objects providing a clear representation of the overall setting, objects providing light for the parking lot</span>
</span></foreignObject></g></g></svg>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>LLaMA output for the inputs shown in Fig. <a href="#A3.F11" title="Figure 11 ‣ Inputs ‣ Appendix C Expression Generation via SINGLE-MODALITY PATHWAY ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A3.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Step 1</h4>

<div id="A3.SS0.SSS0.Px3.p1" class="ltx_para ltx_noindent">
<p id="A3.SS0.SSS0.Px3.p1.1" class="ltx_p">We use LLaVA to generate image captions. Table <a href="#A3.T6" title="Table 6 ‣ Step 1 ‣ Appendix C Expression Generation via SINGLE-MODALITY PATHWAY ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the text prompt list we use. One random text prompt from this list is combined with object content description (i.e., referring expression or category name) for LLaVA input. For each image, we send these prompts into LLaVA twice and obtain two diverse text descriptions. These two text descriptions are both utilized in the following steps. Examples of generated text descriptions are shown in Table <a href="#A3.T7" title="Table 7 ‣ Step 1 ‣ Appendix C Expression Generation via SINGLE-MODALITY PATHWAY ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<figure id="A3.T6" class="ltx_table"><svg id="A3.T6.pic1" class="ltx_picture" height="190.68" overflow="visible" version="1.1" width="551.18"><g transform="translate(0,190.68) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000" fill-opacity="1.0"><path d="M 0 13.78 L 0 176.9 C 0 184.51 6.17 190.68 13.78 190.68 L 537.4 190.68 C 545.01 190.68 551.18 184.51 551.18 176.9 L 551.18 13.78 C 551.18 6.17 545.01 0 537.4 0 L 13.78 0 C 6.17 0 0 6.17 0 13.78 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 13.78 L 1.97 176.9 C 1.97 183.43 7.26 188.71 13.78 188.71 L 537.4 188.71 C 543.92 188.71 549.21 183.43 549.21 176.9 L 549.21 13.78 C 549.21 7.26 543.92 1.97 537.4 1.97 L 13.78 1.97 C 7.26 1.97 1.97 7.26 1.97 13.78 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="507.87" height="163.12" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span id="A3.T6.pic1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:367.0pt;">
<span id="A3.I1" class="ltx_itemize">
<span id="A3.I1.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="A3.I1.i1.p1" class="ltx_para">
<span id="A3.I1.i1.p1.1" class="ltx_p">“Describe the following image in detail”</span>
</span></span>
<span id="A3.I1.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="A3.I1.i2.p1" class="ltx_para">
<span id="A3.I1.i2.p1.1" class="ltx_p">“Provide a detailed description of the given image”</span>
</span></span>
<span id="A3.I1.i3" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="A3.I1.i3.p1" class="ltx_para">
<span id="A3.I1.i3.p1.1" class="ltx_p">“Give an elaborate explanation of the image you see”</span>
</span></span>
<span id="A3.I1.i4" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="A3.I1.i4.p1" class="ltx_para">
<span id="A3.I1.i4.p1.1" class="ltx_p">“Share a comprehensive rundown of the presented image”</span>
</span></span>
<span id="A3.I1.i5" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="A3.I1.i5.p1" class="ltx_para">
<span id="A3.I1.i5.p1.1" class="ltx_p">“Offer a thorough analysis of the image”</span>
</span></span>
<span id="A3.I1.i6" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="A3.I1.i6.p1" class="ltx_para">
<span id="A3.I1.i6.p1.1" class="ltx_p">“Explain the various aspects of the image before you”,</span>
</span></span>
<span id="A3.I1.i7" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="A3.I1.i7.p1" class="ltx_para">
<span id="A3.I1.i7.p1.1" class="ltx_p">“Clarify the contents of the displayed image with great detail”</span>
</span></span>
<span id="A3.I1.i8" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="A3.I1.i8.p1" class="ltx_para">
<span id="A3.I1.i8.p1.1" class="ltx_p">“Characterize the image using a well-detailed description”</span>
</span></span>
<span id="A3.I1.i9" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="A3.I1.i9.p1" class="ltx_para">
<span id="A3.I1.i9.p1.1" class="ltx_p">“Break down the elements of the image in a detailed manner”</span>
</span></span>
<span id="A3.I1.i10" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="A3.I1.i10.p1" class="ltx_para">
<span id="A3.I1.i10.p1.1" class="ltx_p">“Walk through the important details of the image”</span>
</span></span>
<span id="A3.I1.i11" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="A3.I1.i11.p1" class="ltx_para">
<span id="A3.I1.i11.p1.1" class="ltx_p">“Portray the image with a rich, descriptive narrative”</span>
</span></span>
<span id="A3.I1.i12" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="A3.I1.i12.p1" class="ltx_para">
<span id="A3.I1.i12.p1.1" class="ltx_p">“Narrate the contents of the image with precision”</span>
</span></span>
<span id="A3.I1.i13" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="A3.I1.i13.p1" class="ltx_para">
<span id="A3.I1.i13.p1.1" class="ltx_p">“Analyze the image in a comprehensive and detailed manner”</span>
</span></span>
<span id="A3.I1.i14" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="A3.I1.i14.p1" class="ltx_para">
<span id="A3.I1.i14.p1.1" class="ltx_p">“Illustrate the image through a descriptive explanation”</span>
</span></span>
<span id="A3.I1.i15" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="A3.I1.i15.p1" class="ltx_para">
<span id="A3.I1.i15.p1.1" class="ltx_p">“Examine the image closely and share its details”</span>
</span></span>
<span id="A3.I1.i16" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="A3.I1.i16.p1" class="ltx_para ltx_noindent">
<span id="A3.I1.i16.p1.1" class="ltx_p">“Write an exhaustive depiction of the given image”</span>
</span></span>
</span>
</span></foreignObject></g></g></svg>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span>Text prompts we use for LLaVA. These prompts follow the author usage in the original LLaVA work.</figcaption>
</figure>
<figure id="A3.T7" class="ltx_table"><svg id="A3.T7.pic1" class="ltx_picture" height="265.98" overflow="visible" version="1.1" width="551.18"><g transform="translate(0,265.98) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000" fill-opacity="1.0"><path d="M 0 13.78 L 0 252.2 C 0 259.81 6.17 265.98 13.78 265.98 L 537.4 265.98 C 545.01 265.98 551.18 259.81 551.18 252.2 L 551.18 13.78 C 551.18 6.17 545.01 0 537.4 0 L 13.78 0 C 6.17 0 0 6.17 0 13.78 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 13.78 L 1.97 252.2 C 1.97 258.72 7.26 264.01 13.78 264.01 L 537.4 264.01 C 543.92 264.01 549.21 258.72 549.21 252.2 L 549.21 13.78 C 549.21 7.26 543.92 1.97 537.4 1.97 L 13.78 1.97 C 7.26 1.97 1.97 7.26 1.97 13.78 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="507.87" height="238.42" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span id="A3.T7.pic1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:367.0pt;">
<span id="A3.T7.pic1.1.1.1.1.1.1" class="ltx_p"><span id="A3.T7.pic1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Prompt for LLaVA</span></span>
<span id="A3.T7.pic1.1.1.1.1.1.2" class="ltx_p">Provide a detailed description of the given image, including objects: <span id="A3.T7.pic1.1.1.1.1.1.2.1" class="ltx_text ltx_font_italic">Street lights</span>, <span id="A3.T7.pic1.1.1.1.1.1.2.2" class="ltx_text ltx_font_italic">Fire Truck</span></span>
<span id="A3.T7.pic1.1.1.1.1.1.3" class="ltx_p"><span id="A3.T7.pic1.1.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Output from LLaVA</span></span>
<span id="A3.T7.pic1.1.1.1.1.1.4" class="ltx_p">Output 1st The image displays a parking lot outside a fire station where several fire trucks are parked. In total, there are five fire trucks of varying sizes, all lined up neatly in the lot. Additionally, there are two street lights visible in the scene, providing illumination for the area. The fire trucks appear ready for use and are an bessential part of the fire station’s resources.</span>
<span id="A3.T7.pic1.1.1.1.1.1.5" class="ltx_p">Output 2nd The image depicts a parking lot outside a fire station, where several fire trucks are parked in a neat row. There are a total of five fire trucks of varying sizes, all aligned and ready for use. In addition to the fire trucks, there are two street lights visible in the scene, providing illumination for the parking lot. The overall setting gives a clear representation of the organized and prepared nature of the fire station.</span>
</span></foreignObject></g></g></svg>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7: </span>Generated examples of LLaVA based on one random text prompt from Table <a href="#A3.T6" title="Table 6 ‣ Step 1 ‣ Appendix C Expression Generation via SINGLE-MODALITY PATHWAY ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</figcaption>
</figure>
</section>
<section id="A3.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Step 2</h4>

<div id="A3.SS0.SSS0.Px4.p1" class="ltx_para ltx_noindent">
<p id="A3.SS0.SSS0.Px4.p1.1" class="ltx_p">We write text prompt for LLaMA. In this step, the text prompt contains three parts including task descriptions, in-context examples, and text description of image (i.e., Table <a href="#A3.T7" title="Table 7 ‣ Step 1 ‣ Appendix C Expression Generation via SINGLE-MODALITY PATHWAY ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>). An example text prompt for LLaMA is shown in Table <a href="#A3.T8" title="Table 8 ‣ Step 2 ‣ Appendix C Expression Generation via SINGLE-MODALITY PATHWAY ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> where an in-context sample is shown in Table <a href="#A3.T9" title="Table 9 ‣ Step 2 ‣ Appendix C Expression Generation via SINGLE-MODALITY PATHWAY ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="A3.T8" class="ltx_table"><svg id="A3.T8.pic1" class="ltx_picture" height="2693.33" overflow="visible" version="1.1" width="551.18"><g transform="translate(0,2693.33) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000" fill-opacity="1.0"><path d="M 0 13.78 L 0 2679.55 C 0 2687.16 6.17 2693.33 13.78 2693.33 L 537.4 2693.33 C 545.01 2693.33 551.18 2687.16 551.18 2679.55 L 551.18 13.78 C 551.18 6.17 545.01 0 537.4 0 L 13.78 0 C 6.17 0 0 6.17 0 13.78 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 13.78 L 1.97 2679.55 C 1.97 2686.08 7.26 2691.37 13.78 2691.37 L 537.4 2691.37 C 543.92 2691.37 549.21 2686.08 549.21 2679.55 L 549.21 13.78 C 549.21 7.26 543.92 1.97 537.4 1.97 L 13.78 1.97 C 7.26 1.97 1.97 7.26 1.97 13.78 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="507.87" height="2665.77" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span id="A3.T8.pic1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:367.0pt;">
<span id="A3.T8.pic1.1.1.1.1.1.1" class="ltx_listing ltx_lst_language_Python ltx_lstlisting ltx_listing"><span class="ltx_listing_data"><a href="data:text/plain;base64,dGFza19kZXNjcmlwdGlvbiA9IGYiIiI=" download="">⬇</a></span>
<span id="lstnumberx1" class="ltx_listingline"><span id="lstnumberx1.1" class="ltx_text ltx_lst_identifier ltx_font_typewriter ltx_font_bold">task_description</span><span id="lstnumberx1.2" class="ltx_text ltx_lst_space ltx_font_typewriter ltx_font_bold"> </span><span id="lstnumberx1.3" class="ltx_text ltx_font_typewriter ltx_font_bold">=</span><span id="lstnumberx1.4" class="ltx_text ltx_lst_space ltx_font_typewriter ltx_font_bold"> </span><span id="lstnumberx1.5" class="ltx_text ltx_lst_identifier ltx_font_typewriter ltx_font_bold">f</span><span id="lstnumberx1.6" class="ltx_text ltx_lst_string ltx_font_typewriter ltx_font_bold">""</span><span id="lstnumberx1.7" class="ltx_text ltx_lst_string ltx_font_typewriter ltx_font_bold">""</span>
</span>
</span>
<span id="A3.T8.pic1.1.1.1.1.1.2" class="ltx_p"><span id="A3.T8.pic1.1.1.1.1.1.2.1" class="ltx_text ltx_font_italic"># Task description prompt
<br class="ltx_break"></span></span>
<span id="A3.T8.pic1.1.1.1.1.1.3" class="ltx_p">You are an AI visual assistant that can analyze a single image.</span>
<span id="A3.T8.pic1.1.1.1.1.1.4" class="ltx_p">User will give you several sentences, describing the same image you are observing. In addition, specific interested object locations within the image are given, along with detailed coordinates. These coordinates are in the form of bounding boxes, represented as (x1, y1, x2, y2) with floating numbers ranging from 0 to 1. These values correspond to the left top x, left top y, right bottom x, and right bottom y.</span>
<span id="A3.T8.pic1.1.1.1.1.1.5" class="ltx_p">Using the provided caption and bounding box information, give descriptions about the visual content of each interested objects as if you are seeing the image, as an assistant:</span>
<span id="A3.T8.pic1.1.1.1.1.1.6" class="ltx_p">(1) give descriptions about the object itself, including <span id="A3.T8.pic1.1.1.1.1.1.6.1" class="ltx_text ltx_font_bold">object types, object functions, object counts, object locations, object actions</span>, etc.</span>
<span id="A3.T8.pic1.1.1.1.1.1.7" class="ltx_p">(2) give descriptions about the object and other objects, including <span id="A3.T8.pic1.1.1.1.1.1.7.1" class="ltx_text ltx_font_bold">the relative positions between objects, the interaction between objects in the image</span>, etc.</span>
<span id="A3.T8.pic1.1.1.1.1.1.8" class="ltx_p">Descriptions should be a series of phrases, not whole sentence. Give descriptions for specific interested objects only, do not centered on other objects.</span>
<span id="A3.T8.pic1.1.1.1.1.1.9" class="ltx_p">Again, give descriptions centered on specific objects only.</span>
<span id="A3.T8.pic1.1.1.1.1.1.10" class="ltx_listing ltx_lst_language_Python ltx_lstlisting ltx_listing"><span class="ltx_listing_data"><a href="data:text/plain;base64,IiIiCgppbWFnZTJ0ZXh0ID0gZiIiIg==" download="">⬇</a></span>
<span id="lstnumberx2" class="ltx_listingline"><span id="lstnumberx2.1" class="ltx_text ltx_lst_string ltx_font_typewriter ltx_font_bold">""</span><span id="lstnumberx2.2" class="ltx_text ltx_lst_string ltx_font_typewriter ltx_font_bold">"</span>
</span>
<span id="lstnumberx3" class="ltx_listingline">
</span>
<span id="lstnumberx4" class="ltx_listingline"><span id="lstnumberx4.1" class="ltx_text ltx_font_typewriter ltx_font_bold">image2text</span><span id="lstnumberx4.2" class="ltx_text ltx_lst_space ltx_font_typewriter ltx_font_bold"> </span><span id="lstnumberx4.3" class="ltx_text ltx_font_typewriter ltx_font_bold">=</span><span id="lstnumberx4.4" class="ltx_text ltx_lst_space ltx_font_typewriter ltx_font_bold"> </span><span id="lstnumberx4.5" class="ltx_text ltx_font_typewriter ltx_font_bold">f"</span><span id="lstnumberx4.6" class="ltx_text ltx_lst_string ltx_font_typewriter ltx_font_bold">""</span>
</span>
</span>
<span id="A3.T8.pic1.1.1.1.1.1.11" class="ltx_p"><span id="A3.T8.pic1.1.1.1.1.1.11.1" class="ltx_text ltx_font_italic"># Image description of LLaVA, the following two paragraphs are from Table <a href="#A3.T7" title="Table 7 ‣ Step 1 ‣ Appendix C Expression Generation via SINGLE-MODALITY PATHWAY ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>
<br class="ltx_break"></span></span>
<span id="A3.T8.pic1.1.1.1.1.1.12" class="ltx_p">The image displays a parking lot outside a fire station where several fire trucks are parked. In total, there are five fire trucks of varying sizes, all lined up neatly in the lot. Additionally, there are two street lights visible in the scene, providing illumination for the area. The fire trucks appear ready for use and are an bessential part of the fire station’s resources.</span>
<span id="A3.T8.pic1.1.1.1.1.1.13" class="ltx_p">The image depicts a parking lot outside a fire station, where several fire trucks are parked in a neat row. There are a total of five fire trucks of varying sizes, all aligned and ready for use. In addition to the fire trucks, there are two street lights visible in the scene, providing illumination for the parking lot. The overall setting gives a clear representation of the organized and prepared nature of the fire station.</span>
<span id="A3.T8.pic1.1.1.1.1.1.14" class="ltx_p">Street lights: [0.0, 0.23, 0.03, 0.26]</span>
<span id="A3.T8.pic1.1.1.1.1.1.15" class="ltx_p">Fire truck: [0.05, 0.6, 0.21, 0.76], [0.19, 0.58, 0.37, 0.77], [0.33, 0.55, 0.61, 0.77], [0.56, 0.57, 0.74, 0.77], [0.72, 0.57, 1.0, 0.76]</span>
<span id="A3.T8.pic1.1.1.1.1.1.16" class="ltx_listing ltx_lst_language_Python ltx_lstlisting ltx_listing"><span class="ltx_listing_data"><a href="data:text/plain;base64,IiIi" download="">⬇</a></span>
<span id="lstnumberx5" class="ltx_listingline"><span id="lstnumberx5.1" class="ltx_text ltx_lst_string ltx_font_typewriter ltx_font_bold">""</span><span id="lstnumberx5.2" class="ltx_text ltx_lst_string ltx_font_typewriter ltx_font_bold">""</span>
</span>
</span>
<span id="A3.T8.pic1.1.1.1.1.1.17" class="ltx_p"><span id="A3.T8.pic1.1.1.1.1.1.17.1" class="ltx_text ltx_font_italic"># Python code together with above text prompts are directly sent to LLaMA</span></span>
<span id="A3.T8.pic1.1.1.1.1.1.18" class="ltx_listing ltx_lst_language_Python ltx_lstlisting ltx_listing"><span class="ltx_listing_data"><a href="data:text/plain;base64,bWVzc2FnZXMgPSBbeyJyb2xlIjogc3lzdGVtLCAiY29udGVudCI6IHRhc2tfZGVzY3JpcHRpb259XQpmb3IgaW5fY29udGV4dF9leGFtcGxlIGluIGluX2NvbnRleHRfZXhhbXBsZXM6CiAgICBtZXNzYWdlcy5hcHBlbmQoeyJyb2xlIjogdXNlciwgImNvbnRlbnQiOiBpbl9jb250ZXh0X2V4YW1wbGVbImNvbnRlbnQiXX0pCiAgICBtZXNzYWdlcy5hcHBlbmQoeyJyb2xlIjogYXNzaXN0YW50LCAiY29udGVudCI6IGluX2NvbnRleHRfZXhhbXBsZVsicmVzcG9uc2UiXX0KbWVzc2FnZXMuYXBwZW5kKHsicm9sZSI6IHVzZXIsICJjb250ZW50IjogaW1hZ2UydGV4dH0p" download="">⬇</a></span>
<span id="lstnumberx6" class="ltx_listingline"><span id="lstnumberx6.1" class="ltx_text ltx_lst_identifier ltx_font_typewriter ltx_font_bold">messages</span><span id="lstnumberx6.2" class="ltx_text ltx_lst_space ltx_font_typewriter ltx_font_bold"> </span><span id="lstnumberx6.3" class="ltx_text ltx_font_typewriter ltx_font_bold">=</span><span id="lstnumberx6.4" class="ltx_text ltx_lst_space ltx_font_typewriter ltx_font_bold"> </span><span id="lstnumberx6.5" class="ltx_text ltx_font_typewriter ltx_font_bold">[{</span><span id="lstnumberx6.6" class="ltx_text ltx_lst_string ltx_font_typewriter ltx_font_bold">"role"</span><span id="lstnumberx6.7" class="ltx_text ltx_font_typewriter ltx_font_bold">:</span><span id="lstnumberx6.8" class="ltx_text ltx_lst_space ltx_font_typewriter ltx_font_bold"> </span><span id="lstnumberx6.9" class="ltx_text ltx_lst_identifier ltx_font_typewriter ltx_font_bold">system</span><span id="lstnumberx6.10" class="ltx_text ltx_font_typewriter ltx_font_bold">,</span><span id="lstnumberx6.11" class="ltx_text ltx_lst_space ltx_font_typewriter ltx_font_bold"> </span><span id="lstnumberx6.12" class="ltx_text ltx_lst_string ltx_font_typewriter ltx_font_bold">"content"</span><span id="lstnumberx6.13" class="ltx_text ltx_font_typewriter ltx_font_bold">:</span><span id="lstnumberx6.14" class="ltx_text ltx_lst_space ltx_font_typewriter ltx_font_bold"> </span><span id="lstnumberx6.15" class="ltx_text ltx_lst_identifier ltx_font_typewriter ltx_font_bold">task_description</span><span id="lstnumberx6.16" class="ltx_text ltx_font_typewriter ltx_font_bold">}]</span>
</span>
<span id="lstnumberx7" class="ltx_listingline"><span id="lstnumberx7.1" class="ltx_text ltx_lst_keyword ltx_font_typewriter ltx_font_bold">for</span><span id="lstnumberx7.2" class="ltx_text ltx_lst_space ltx_font_typewriter ltx_font_bold"> </span><span id="lstnumberx7.3" class="ltx_text ltx_lst_identifier ltx_font_typewriter ltx_font_bold">in_context_example</span><span id="lstnumberx7.4" class="ltx_text ltx_lst_space ltx_font_typewriter ltx_font_bold"> </span><span id="lstnumberx7.5" class="ltx_text ltx_lst_keyword ltx_font_typewriter ltx_font_bold">in</span><span id="lstnumberx7.6" class="ltx_text ltx_lst_space ltx_font_typewriter ltx_font_bold"> </span><span id="lstnumberx7.7" class="ltx_text ltx_lst_identifier ltx_font_typewriter ltx_font_bold">in_context_examples</span><span id="lstnumberx7.8" class="ltx_text ltx_font_typewriter ltx_font_bold">:</span>
</span>
<span id="lstnumberx8" class="ltx_listingline"><span id="lstnumberx8.1" class="ltx_text ltx_lst_space ltx_font_typewriter ltx_font_bold">    </span><span id="lstnumberx8.2" class="ltx_text ltx_lst_identifier ltx_font_typewriter ltx_font_bold">messages</span><span id="lstnumberx8.3" class="ltx_text ltx_font_typewriter ltx_font_bold">.</span><span id="lstnumberx8.4" class="ltx_text ltx_lst_identifier ltx_font_typewriter ltx_font_bold">append</span><span id="lstnumberx8.5" class="ltx_text ltx_font_typewriter ltx_font_bold">({</span><span id="lstnumberx8.6" class="ltx_text ltx_lst_string ltx_font_typewriter ltx_font_bold">"role"</span><span id="lstnumberx8.7" class="ltx_text ltx_font_typewriter ltx_font_bold">:</span><span id="lstnumberx8.8" class="ltx_text ltx_lst_space ltx_font_typewriter ltx_font_bold"> </span><span id="lstnumberx8.9" class="ltx_text ltx_lst_identifier ltx_font_typewriter ltx_font_bold">user</span><span id="lstnumberx8.10" class="ltx_text ltx_font_typewriter ltx_font_bold">,</span><span id="lstnumberx8.11" class="ltx_text ltx_lst_space ltx_font_typewriter ltx_font_bold"> </span><span id="lstnumberx8.12" class="ltx_text ltx_lst_string ltx_font_typewriter ltx_font_bold">"content"</span><span id="lstnumberx8.13" class="ltx_text ltx_font_typewriter ltx_font_bold">:</span><span id="lstnumberx8.14" class="ltx_text ltx_lst_space ltx_font_typewriter ltx_font_bold"> </span><span id="lstnumberx8.15" class="ltx_text ltx_lst_identifier ltx_font_typewriter ltx_font_bold">in_context_example</span><span id="lstnumberx8.16" class="ltx_text ltx_font_typewriter ltx_font_bold">[</span><span id="lstnumberx8.17" class="ltx_text ltx_lst_string ltx_font_typewriter ltx_font_bold">"content"</span><span id="lstnumberx8.18" class="ltx_text ltx_font_typewriter ltx_font_bold">]})</span>
</span>
<span id="lstnumberx9" class="ltx_listingline"><span id="lstnumberx9.1" class="ltx_text ltx_lst_space ltx_font_typewriter ltx_font_bold">    </span><span id="lstnumberx9.2" class="ltx_text ltx_lst_identifier ltx_font_typewriter ltx_font_bold">messages</span><span id="lstnumberx9.3" class="ltx_text ltx_font_typewriter ltx_font_bold">.</span><span id="lstnumberx9.4" class="ltx_text ltx_lst_identifier ltx_font_typewriter ltx_font_bold">append</span><span id="lstnumberx9.5" class="ltx_text ltx_font_typewriter ltx_font_bold">({</span><span id="lstnumberx9.6" class="ltx_text ltx_lst_string ltx_font_typewriter ltx_font_bold">"role"</span><span id="lstnumberx9.7" class="ltx_text ltx_font_typewriter ltx_font_bold">:</span><span id="lstnumberx9.8" class="ltx_text ltx_lst_space ltx_font_typewriter ltx_font_bold"> </span><span id="lstnumberx9.9" class="ltx_text ltx_lst_identifier ltx_font_typewriter ltx_font_bold">assistant</span><span id="lstnumberx9.10" class="ltx_text ltx_font_typewriter ltx_font_bold">,</span><span id="lstnumberx9.11" class="ltx_text ltx_lst_space ltx_font_typewriter ltx_font_bold"> </span><span id="lstnumberx9.12" class="ltx_text ltx_lst_string ltx_font_typewriter ltx_font_bold">"content"</span><span id="lstnumberx9.13" class="ltx_text ltx_font_typewriter ltx_font_bold">:</span><span id="lstnumberx9.14" class="ltx_text ltx_lst_space ltx_font_typewriter ltx_font_bold"> </span><span id="lstnumberx9.15" class="ltx_text ltx_lst_identifier ltx_font_typewriter ltx_font_bold">in_context_example</span><span id="lstnumberx9.16" class="ltx_text ltx_font_typewriter ltx_font_bold">[</span><span id="lstnumberx9.17" class="ltx_text ltx_lst_string ltx_font_typewriter ltx_font_bold">"response"</span><span id="lstnumberx9.18" class="ltx_text ltx_font_typewriter ltx_font_bold">]}</span>
</span>
<span id="lstnumberx10" class="ltx_listingline"><span id="lstnumberx10.1" class="ltx_text ltx_lst_identifier ltx_font_typewriter ltx_font_bold">messages</span><span id="lstnumberx10.2" class="ltx_text ltx_font_typewriter ltx_font_bold">.</span><span id="lstnumberx10.3" class="ltx_text ltx_lst_identifier ltx_font_typewriter ltx_font_bold">append</span><span id="lstnumberx10.4" class="ltx_text ltx_font_typewriter ltx_font_bold">({</span><span id="lstnumberx10.5" class="ltx_text ltx_lst_string ltx_font_typewriter ltx_font_bold">"role"</span><span id="lstnumberx10.6" class="ltx_text ltx_font_typewriter ltx_font_bold">:</span><span id="lstnumberx10.7" class="ltx_text ltx_lst_space ltx_font_typewriter ltx_font_bold"> </span><span id="lstnumberx10.8" class="ltx_text ltx_lst_identifier ltx_font_typewriter ltx_font_bold">user</span><span id="lstnumberx10.9" class="ltx_text ltx_font_typewriter ltx_font_bold">,</span><span id="lstnumberx10.10" class="ltx_text ltx_lst_space ltx_font_typewriter ltx_font_bold"> </span><span id="lstnumberx10.11" class="ltx_text ltx_lst_string ltx_font_typewriter ltx_font_bold">"content"</span><span id="lstnumberx10.12" class="ltx_text ltx_font_typewriter ltx_font_bold">:</span><span id="lstnumberx10.13" class="ltx_text ltx_lst_space ltx_font_typewriter ltx_font_bold"> </span><span id="lstnumberx10.14" class="ltx_text ltx_lst_identifier ltx_font_typewriter ltx_font_bold">image2text</span><span id="lstnumberx10.15" class="ltx_text ltx_font_typewriter ltx_font_bold">})</span>
</span>
</span>
</span></foreignObject></g></g></svg>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 8: </span>An example of prompt generation for LLaMA input. For each image, we first obtain text description in Table <a href="#A3.T7" title="Table 7 ‣ Step 1 ‣ Appendix C Expression Generation via SINGLE-MODALITY PATHWAY ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. Then, we organize the above prompts and python code, together with in-context examples for LLaMA input. An in-context example is in Table <a href="#A3.T9" title="Table 9 ‣ Step 2 ‣ Appendix C Expression Generation via SINGLE-MODALITY PATHWAY ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="A3.T9" class="ltx_table"><svg id="A3.T9.pic1" class="ltx_picture" height="704.21" overflow="visible" version="1.1" width="551.18"><g transform="translate(0,704.21) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000" fill-opacity="1.0"><path d="M 0 13.78 L 0 690.43 C 0 698.04 6.17 704.21 13.78 704.21 L 537.4 704.21 C 545.01 704.21 551.18 698.04 551.18 690.43 L 551.18 13.78 C 551.18 6.17 545.01 0 537.4 0 L 13.78 0 C 6.17 0 0 6.17 0 13.78 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 13.78 L 1.97 690.43 C 1.97 696.95 7.26 702.24 13.78 702.24 L 537.4 702.24 C 543.92 702.24 549.21 696.95 549.21 690.43 L 549.21 13.78 C 549.21 7.26 543.92 1.97 537.4 1.97 L 13.78 1.97 C 7.26 1.97 1.97 7.26 1.97 13.78 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="507.87" height="676.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000"><span id="A3.T9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1" class="ltx_inline-logical-block ltx_minipage ltx_align_bottom" style="width:367.0pt;">
<span id="A3.T9.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_figure ltx_align_floatright"><img src="/html/2310.05136/assets/image/flickr30k_seed_example_1006452823.jpg" id="A3.T9.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="192" height="144" alt="[Uncaptioned image]">
</span>
<span id="A3.T9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.p1" class="ltx_para">
<span id="A3.T9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.p1.1" class="ltx_p"><span id="A3.T9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.p1.1.1" class="ltx_text ltx_font_bold">Example image2text</span></span>
<span id="A3.T9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.p1.2" class="ltx_p">Two children, a girl and a boy are practicing their writing.</span>
<span id="A3.T9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.p1.3" class="ltx_p">Two children sit on a small seesaw in the sand.</span>
<span id="A3.T9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.p1.4" class="ltx_p">Two children sitting on a teeter totter.</span>
<span id="A3.T9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.p1.5" class="ltx_p">2 kids playing on a seesaw.</span>
<span id="A3.T9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.p1.6" class="ltx_p">Two kids sit on a seesaw.</span>
<span id="A3.T9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.p1.7" class="ltx_p">2 kids/two kids/two children: [0.09, 0.59, 0.49, 0.94]</span>
<span id="A3.T9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.p1.8" class="ltx_p">girl: [0.09, 0.59, 0.23, 0.92]</span>
<span id="A3.T9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.p1.9" class="ltx_p">boy: [0.34, 0.62, 0.49, 0.94]</span>
<span id="A3.T9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.p1.10" class="ltx_p">seesaw/small seesaw/teeter totter: [0.06, 0.83, 0.57, 0.93]</span>
<span id="A3.T9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.p1.11" class="ltx_p">sand: [0.01, 0.61, 1.0, 1.0]</span>
<span id="A3.T9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.p1.12" class="ltx_p"><span class="ltx_rule" style="width:369.9pt;height:0.4pt;background:black;display:inline-block;"> </span>

<span id="A3.T9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.p1.12.1" class="ltx_text ltx_font_bold">Example Response</span></span>
<span id="A3.T9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.p1.13" class="ltx_p"><span id="A3.T9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.p1.13.1" class="ltx_text ltx_font_bold">[2 kids/two kids/two children]</span></span>
<span id="A3.T9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.p1.14" class="ltx_p">(1) persons, children, two kids, children learning, kids playing, kids sitting, children practicing their writing</span>
<span id="A3.T9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.p1.15" class="ltx_p">(2) 2 kids playing on a seesaw, Two kids sitting on a seesaw, a girl and a boy, two person on the teeter totter, kids by the sea</span>
<span id="A3.T9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.p1.16" class="ltx_p"><span id="A3.T9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.p1.16.1" class="ltx_text ltx_font_bold">[girl]</span></span>
<span id="A3.T9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.p1.17" class="ltx_p">(1) girl, kid on the left of the image, girl sitting, girl playing</span>
<span id="A3.T9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.p1.18" class="ltx_p">(2) kid playing with the boy, girl sitting on the small seesaw, girl playing in the sand, girl reading a book, a friend of the boy on the right, the taller kid</span>
<span id="A3.T9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.p1.19" class="ltx_p"><span id="A3.T9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.p1.19.1" class="ltx_text ltx_font_bold">[boy]</span></span>
<span id="A3.T9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.p1.20" class="ltx_p">(1) boy, kid on the right, boy sitting, boy playing, boy practicing his writing</span>
<span id="A3.T9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.p1.21" class="ltx_p">(2) kid playing with the girl, boy sitting on a teeter totter, a friend of the girl on the left, boy playing in the sand</span>
<span id="A3.T9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.p1.22" class="ltx_p"><span id="A3.T9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.p1.22.1" class="ltx_text ltx_font_bold">[seesaw/small seesaw/teeter totter]</span></span>
<span id="A3.T9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.p1.23" class="ltx_p">(1) small seesaw, teeter totter, item to be played on, common facilities in parks and playground, game of two people</span>
<span id="A3.T9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.p1.24" class="ltx_p">(2) seesaw in the sand, item the kids are sitting on, item the girl is sitting on, item the boy is playing on</span>
<span id="A3.T9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.p1.25" class="ltx_p"><span id="A3.T9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.p1.25.1" class="ltx_text ltx_font_bold">[sand]</span></span>
<span id="A3.T9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.p1.26" class="ltx_p">(1) common by the sea, the background of the scene</span>
<span id="A3.T9.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.p1.27" class="ltx_p">(2) item on which the seesaw is placed, item on which the kids are standing</span>
</span></span></foreignObject></g></g></svg>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 9: </span>An in-context example. We manually prepare three in-context examples and show one here. On the top block there are image captions and object bbx coordinates. Note that the displayed image is only for image caption reference and is not used in practice. The bottom block shows our expected expressions for each object. The expressions listed in (1) focus on the object attribute itself, and listed in (2) focus on relationship between current object and other objects in this image.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Visual Prompting and Visual-Textual Matching</h2>

<figure id="A4.F12" class="ltx_figure"><img src="/html/2310.05136/assets/x13.png" id="A4.F12.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="380" height="203" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>A detailed pipeline of visual prompting and visual-textual matching. We use visual prompting to emphasize target object in the image. Then, we use the CLIP model to compute the scores for expression dropout.</figcaption>
</figure>
<div id="A4.p1" class="ltx_para ltx_noindent">
<p id="A4.p1.1" class="ltx_p">Motivated by recent applications <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a href="#bib.bib53" title="" class="ltx_ref">2023a</a>; Shtedritski et al., <a href="#bib.bib45" title="" class="ltx_ref">2023</a>)</cite> of visual language pretraining (VLP) models in solving zero-shot REC tasks, we discover that coupling VLP with visual prompting enables robust pairing of image regions and text on the basis of potent generalization. The efficacy of pairing is largely influenced by the configuration of visual prompting, which can be divided into two types as follows:</p>
</div>
<div id="A4.p2" class="ltx_para ltx_noindent">
<p id="A4.p2.1" class="ltx_p">(i) Shape division of emphasized areas: box, circle and contour.</p>
</div>
<div id="A4.p3" class="ltx_para ltx_noindent">
<p id="A4.p3.1" class="ltx_p">(ii) Highlighting methods for emphasized areas: crop-based, line-based, mask-based, grayscale reversion and blur reverion based image editing operations.</p>
</div>
<div id="A4.p4" class="ltx_para ltx_noindent">
<p id="A4.p4.1" class="ltx_p">As shown in Fig. <a href="#A4.F12" title="Figure 12 ‣ Appendix D Visual Prompting and Visual-Textual Matching ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>, after various combinations of experiments, we find the superposition of line-based circle prompt and blur reverse contour prompt yields the optimal visual prompting strategy. The image <math id="A4.p4.1.m1.1" class="ltx_Math" alttext="\widetilde{X}" display="inline"><semantics id="A4.p4.1.m1.1a"><mover accent="true" id="A4.p4.1.m1.1.1" xref="A4.p4.1.m1.1.1.cmml"><mi id="A4.p4.1.m1.1.1.2" xref="A4.p4.1.m1.1.1.2.cmml">X</mi><mo id="A4.p4.1.m1.1.1.1" xref="A4.p4.1.m1.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="A4.p4.1.m1.1b"><apply id="A4.p4.1.m1.1.1.cmml" xref="A4.p4.1.m1.1.1"><ci id="A4.p4.1.m1.1.1.1.cmml" xref="A4.p4.1.m1.1.1.1">~</ci><ci id="A4.p4.1.m1.1.1.2.cmml" xref="A4.p4.1.m1.1.1.2">𝑋</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.1.m1.1c">\widetilde{X}</annotation></semantics></math> after visual prompting can be expressed as:</p>
<table id="A4.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A4.E2.m1.11" class="ltx_Math" alttext="\widetilde{X}=f_{\theta}(f_{\gamma}(X))\quad\text{with}\quad f_{\gamma}(\cdot)=BR(\cdot,SAM(\cdot,B))\quad f_{\theta}(\cdot)=RC(\cdot,B)" display="block"><semantics id="A4.E2.m1.11a"><mrow id="A4.E2.m1.11.11.2" xref="A4.E2.m1.11.11.3.cmml"><mrow id="A4.E2.m1.10.10.1.1" xref="A4.E2.m1.10.10.1.1.cmml"><mover accent="true" id="A4.E2.m1.10.10.1.1.3" xref="A4.E2.m1.10.10.1.1.3.cmml"><mi id="A4.E2.m1.10.10.1.1.3.2" xref="A4.E2.m1.10.10.1.1.3.2.cmml">X</mi><mo id="A4.E2.m1.10.10.1.1.3.1" xref="A4.E2.m1.10.10.1.1.3.1.cmml">~</mo></mover><mo id="A4.E2.m1.10.10.1.1.2" xref="A4.E2.m1.10.10.1.1.2.cmml">=</mo><mrow id="A4.E2.m1.10.10.1.1.1.1" xref="A4.E2.m1.10.10.1.1.1.2.cmml"><mrow id="A4.E2.m1.10.10.1.1.1.1.1" xref="A4.E2.m1.10.10.1.1.1.1.1.cmml"><msub id="A4.E2.m1.10.10.1.1.1.1.1.3" xref="A4.E2.m1.10.10.1.1.1.1.1.3.cmml"><mi id="A4.E2.m1.10.10.1.1.1.1.1.3.2" xref="A4.E2.m1.10.10.1.1.1.1.1.3.2.cmml">f</mi><mi id="A4.E2.m1.10.10.1.1.1.1.1.3.3" xref="A4.E2.m1.10.10.1.1.1.1.1.3.3.cmml">θ</mi></msub><mo lspace="0em" rspace="0em" id="A4.E2.m1.10.10.1.1.1.1.1.2" xref="A4.E2.m1.10.10.1.1.1.1.1.2.cmml">​</mo><mrow id="A4.E2.m1.10.10.1.1.1.1.1.1.1" xref="A4.E2.m1.10.10.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="A4.E2.m1.10.10.1.1.1.1.1.1.1.2" xref="A4.E2.m1.10.10.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="A4.E2.m1.10.10.1.1.1.1.1.1.1.1" xref="A4.E2.m1.10.10.1.1.1.1.1.1.1.1.cmml"><msub id="A4.E2.m1.10.10.1.1.1.1.1.1.1.1.2" xref="A4.E2.m1.10.10.1.1.1.1.1.1.1.1.2.cmml"><mi id="A4.E2.m1.10.10.1.1.1.1.1.1.1.1.2.2" xref="A4.E2.m1.10.10.1.1.1.1.1.1.1.1.2.2.cmml">f</mi><mi id="A4.E2.m1.10.10.1.1.1.1.1.1.1.1.2.3" xref="A4.E2.m1.10.10.1.1.1.1.1.1.1.1.2.3.cmml">γ</mi></msub><mo lspace="0em" rspace="0em" id="A4.E2.m1.10.10.1.1.1.1.1.1.1.1.1" xref="A4.E2.m1.10.10.1.1.1.1.1.1.1.1.1.cmml">​</mo><mrow id="A4.E2.m1.10.10.1.1.1.1.1.1.1.1.3.2" xref="A4.E2.m1.10.10.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="A4.E2.m1.10.10.1.1.1.1.1.1.1.1.3.2.1" xref="A4.E2.m1.10.10.1.1.1.1.1.1.1.1.cmml">(</mo><mi id="A4.E2.m1.1.1" xref="A4.E2.m1.1.1.cmml">X</mi><mo stretchy="false" id="A4.E2.m1.10.10.1.1.1.1.1.1.1.1.3.2.2" xref="A4.E2.m1.10.10.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="A4.E2.m1.10.10.1.1.1.1.1.1.1.3" xref="A4.E2.m1.10.10.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mspace width="1em" id="A4.E2.m1.10.10.1.1.1.1.2" xref="A4.E2.m1.10.10.1.1.1.2.cmml"></mspace><mtext id="A4.E2.m1.9.9" xref="A4.E2.m1.9.9a.cmml">with</mtext></mrow></mrow><mspace width="1em" id="A4.E2.m1.11.11.2.3" xref="A4.E2.m1.11.11.3a.cmml"></mspace><mrow id="A4.E2.m1.11.11.2.2.2" xref="A4.E2.m1.11.11.2.2.3.cmml"><mrow id="A4.E2.m1.11.11.2.2.1.1" xref="A4.E2.m1.11.11.2.2.1.1.cmml"><mrow id="A4.E2.m1.11.11.2.2.1.1.3" xref="A4.E2.m1.11.11.2.2.1.1.3.cmml"><msub id="A4.E2.m1.11.11.2.2.1.1.3.2" xref="A4.E2.m1.11.11.2.2.1.1.3.2.cmml"><mi id="A4.E2.m1.11.11.2.2.1.1.3.2.2" xref="A4.E2.m1.11.11.2.2.1.1.3.2.2.cmml">f</mi><mi id="A4.E2.m1.11.11.2.2.1.1.3.2.3" xref="A4.E2.m1.11.11.2.2.1.1.3.2.3.cmml">γ</mi></msub><mo lspace="0em" rspace="0em" id="A4.E2.m1.11.11.2.2.1.1.3.1" xref="A4.E2.m1.11.11.2.2.1.1.3.1.cmml">​</mo><mrow id="A4.E2.m1.11.11.2.2.1.1.3.3.2" xref="A4.E2.m1.11.11.2.2.1.1.3.cmml"><mo stretchy="false" id="A4.E2.m1.11.11.2.2.1.1.3.3.2.1" xref="A4.E2.m1.11.11.2.2.1.1.3.cmml">(</mo><mo lspace="0em" rspace="0em" id="A4.E2.m1.2.2" xref="A4.E2.m1.2.2.cmml">⋅</mo><mo stretchy="false" id="A4.E2.m1.11.11.2.2.1.1.3.3.2.2" xref="A4.E2.m1.11.11.2.2.1.1.3.cmml">)</mo></mrow></mrow><mo id="A4.E2.m1.11.11.2.2.1.1.2" xref="A4.E2.m1.11.11.2.2.1.1.2.cmml">=</mo><mrow id="A4.E2.m1.11.11.2.2.1.1.1" xref="A4.E2.m1.11.11.2.2.1.1.1.cmml"><mi id="A4.E2.m1.11.11.2.2.1.1.1.3" xref="A4.E2.m1.11.11.2.2.1.1.1.3.cmml">B</mi><mo lspace="0em" rspace="0em" id="A4.E2.m1.11.11.2.2.1.1.1.2" xref="A4.E2.m1.11.11.2.2.1.1.1.2.cmml">​</mo><mi id="A4.E2.m1.11.11.2.2.1.1.1.4" xref="A4.E2.m1.11.11.2.2.1.1.1.4.cmml">R</mi><mo lspace="0em" rspace="0em" id="A4.E2.m1.11.11.2.2.1.1.1.2a" xref="A4.E2.m1.11.11.2.2.1.1.1.2.cmml">​</mo><mrow id="A4.E2.m1.11.11.2.2.1.1.1.1.1" xref="A4.E2.m1.11.11.2.2.1.1.1.1.2.cmml"><mo stretchy="false" id="A4.E2.m1.11.11.2.2.1.1.1.1.1.2" xref="A4.E2.m1.11.11.2.2.1.1.1.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="A4.E2.m1.5.5" xref="A4.E2.m1.5.5.cmml">⋅</mo><mo id="A4.E2.m1.11.11.2.2.1.1.1.1.1.3" xref="A4.E2.m1.11.11.2.2.1.1.1.1.2.cmml">,</mo><mrow id="A4.E2.m1.11.11.2.2.1.1.1.1.1.1" xref="A4.E2.m1.11.11.2.2.1.1.1.1.1.1.cmml"><mi id="A4.E2.m1.11.11.2.2.1.1.1.1.1.1.2" xref="A4.E2.m1.11.11.2.2.1.1.1.1.1.1.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="A4.E2.m1.11.11.2.2.1.1.1.1.1.1.1" xref="A4.E2.m1.11.11.2.2.1.1.1.1.1.1.1.cmml">​</mo><mi id="A4.E2.m1.11.11.2.2.1.1.1.1.1.1.3" xref="A4.E2.m1.11.11.2.2.1.1.1.1.1.1.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="A4.E2.m1.11.11.2.2.1.1.1.1.1.1.1a" xref="A4.E2.m1.11.11.2.2.1.1.1.1.1.1.1.cmml">​</mo><mi id="A4.E2.m1.11.11.2.2.1.1.1.1.1.1.4" xref="A4.E2.m1.11.11.2.2.1.1.1.1.1.1.4.cmml">M</mi><mo lspace="0em" rspace="0em" id="A4.E2.m1.11.11.2.2.1.1.1.1.1.1.1b" xref="A4.E2.m1.11.11.2.2.1.1.1.1.1.1.1.cmml">​</mo><mrow id="A4.E2.m1.11.11.2.2.1.1.1.1.1.1.5.2" xref="A4.E2.m1.11.11.2.2.1.1.1.1.1.1.5.1.cmml"><mo stretchy="false" id="A4.E2.m1.11.11.2.2.1.1.1.1.1.1.5.2.1" xref="A4.E2.m1.11.11.2.2.1.1.1.1.1.1.5.1.cmml">(</mo><mo lspace="0em" rspace="0em" id="A4.E2.m1.3.3" xref="A4.E2.m1.3.3.cmml">⋅</mo><mo id="A4.E2.m1.11.11.2.2.1.1.1.1.1.1.5.2.2" xref="A4.E2.m1.11.11.2.2.1.1.1.1.1.1.5.1.cmml">,</mo><mi id="A4.E2.m1.4.4" xref="A4.E2.m1.4.4.cmml">B</mi><mo stretchy="false" id="A4.E2.m1.11.11.2.2.1.1.1.1.1.1.5.2.3" xref="A4.E2.m1.11.11.2.2.1.1.1.1.1.1.5.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="A4.E2.m1.11.11.2.2.1.1.1.1.1.4" xref="A4.E2.m1.11.11.2.2.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mspace width="1em" id="A4.E2.m1.11.11.2.2.2.3" xref="A4.E2.m1.11.11.2.2.3a.cmml"></mspace><mrow id="A4.E2.m1.11.11.2.2.2.2" xref="A4.E2.m1.11.11.2.2.2.2.cmml"><mrow id="A4.E2.m1.11.11.2.2.2.2.2" xref="A4.E2.m1.11.11.2.2.2.2.2.cmml"><msub id="A4.E2.m1.11.11.2.2.2.2.2.2" xref="A4.E2.m1.11.11.2.2.2.2.2.2.cmml"><mi id="A4.E2.m1.11.11.2.2.2.2.2.2.2" xref="A4.E2.m1.11.11.2.2.2.2.2.2.2.cmml">f</mi><mi id="A4.E2.m1.11.11.2.2.2.2.2.2.3" xref="A4.E2.m1.11.11.2.2.2.2.2.2.3.cmml">θ</mi></msub><mo lspace="0em" rspace="0em" id="A4.E2.m1.11.11.2.2.2.2.2.1" xref="A4.E2.m1.11.11.2.2.2.2.2.1.cmml">​</mo><mrow id="A4.E2.m1.11.11.2.2.2.2.2.3.2" xref="A4.E2.m1.11.11.2.2.2.2.2.cmml"><mo stretchy="false" id="A4.E2.m1.11.11.2.2.2.2.2.3.2.1" xref="A4.E2.m1.11.11.2.2.2.2.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="A4.E2.m1.6.6" xref="A4.E2.m1.6.6.cmml">⋅</mo><mo stretchy="false" id="A4.E2.m1.11.11.2.2.2.2.2.3.2.2" xref="A4.E2.m1.11.11.2.2.2.2.2.cmml">)</mo></mrow></mrow><mo id="A4.E2.m1.11.11.2.2.2.2.1" xref="A4.E2.m1.11.11.2.2.2.2.1.cmml">=</mo><mrow id="A4.E2.m1.11.11.2.2.2.2.3" xref="A4.E2.m1.11.11.2.2.2.2.3.cmml"><mi id="A4.E2.m1.11.11.2.2.2.2.3.2" xref="A4.E2.m1.11.11.2.2.2.2.3.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="A4.E2.m1.11.11.2.2.2.2.3.1" xref="A4.E2.m1.11.11.2.2.2.2.3.1.cmml">​</mo><mi id="A4.E2.m1.11.11.2.2.2.2.3.3" xref="A4.E2.m1.11.11.2.2.2.2.3.3.cmml">C</mi><mo lspace="0em" rspace="0em" id="A4.E2.m1.11.11.2.2.2.2.3.1a" xref="A4.E2.m1.11.11.2.2.2.2.3.1.cmml">​</mo><mrow id="A4.E2.m1.11.11.2.2.2.2.3.4.2" xref="A4.E2.m1.11.11.2.2.2.2.3.4.1.cmml"><mo stretchy="false" id="A4.E2.m1.11.11.2.2.2.2.3.4.2.1" xref="A4.E2.m1.11.11.2.2.2.2.3.4.1.cmml">(</mo><mo lspace="0em" rspace="0em" id="A4.E2.m1.7.7" xref="A4.E2.m1.7.7.cmml">⋅</mo><mo id="A4.E2.m1.11.11.2.2.2.2.3.4.2.2" xref="A4.E2.m1.11.11.2.2.2.2.3.4.1.cmml">,</mo><mi id="A4.E2.m1.8.8" xref="A4.E2.m1.8.8.cmml">B</mi><mo stretchy="false" id="A4.E2.m1.11.11.2.2.2.2.3.4.2.3" xref="A4.E2.m1.11.11.2.2.2.2.3.4.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="A4.E2.m1.11b"><apply id="A4.E2.m1.11.11.3.cmml" xref="A4.E2.m1.11.11.2"><csymbol cd="ambiguous" id="A4.E2.m1.11.11.3a.cmml" xref="A4.E2.m1.11.11.2.3">formulae-sequence</csymbol><apply id="A4.E2.m1.10.10.1.1.cmml" xref="A4.E2.m1.10.10.1.1"><eq id="A4.E2.m1.10.10.1.1.2.cmml" xref="A4.E2.m1.10.10.1.1.2"></eq><apply id="A4.E2.m1.10.10.1.1.3.cmml" xref="A4.E2.m1.10.10.1.1.3"><ci id="A4.E2.m1.10.10.1.1.3.1.cmml" xref="A4.E2.m1.10.10.1.1.3.1">~</ci><ci id="A4.E2.m1.10.10.1.1.3.2.cmml" xref="A4.E2.m1.10.10.1.1.3.2">𝑋</ci></apply><list id="A4.E2.m1.10.10.1.1.1.2.cmml" xref="A4.E2.m1.10.10.1.1.1.1"><apply id="A4.E2.m1.10.10.1.1.1.1.1.cmml" xref="A4.E2.m1.10.10.1.1.1.1.1"><times id="A4.E2.m1.10.10.1.1.1.1.1.2.cmml" xref="A4.E2.m1.10.10.1.1.1.1.1.2"></times><apply id="A4.E2.m1.10.10.1.1.1.1.1.3.cmml" xref="A4.E2.m1.10.10.1.1.1.1.1.3"><csymbol cd="ambiguous" id="A4.E2.m1.10.10.1.1.1.1.1.3.1.cmml" xref="A4.E2.m1.10.10.1.1.1.1.1.3">subscript</csymbol><ci id="A4.E2.m1.10.10.1.1.1.1.1.3.2.cmml" xref="A4.E2.m1.10.10.1.1.1.1.1.3.2">𝑓</ci><ci id="A4.E2.m1.10.10.1.1.1.1.1.3.3.cmml" xref="A4.E2.m1.10.10.1.1.1.1.1.3.3">𝜃</ci></apply><apply id="A4.E2.m1.10.10.1.1.1.1.1.1.1.1.cmml" xref="A4.E2.m1.10.10.1.1.1.1.1.1.1"><times id="A4.E2.m1.10.10.1.1.1.1.1.1.1.1.1.cmml" xref="A4.E2.m1.10.10.1.1.1.1.1.1.1.1.1"></times><apply id="A4.E2.m1.10.10.1.1.1.1.1.1.1.1.2.cmml" xref="A4.E2.m1.10.10.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="A4.E2.m1.10.10.1.1.1.1.1.1.1.1.2.1.cmml" xref="A4.E2.m1.10.10.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="A4.E2.m1.10.10.1.1.1.1.1.1.1.1.2.2.cmml" xref="A4.E2.m1.10.10.1.1.1.1.1.1.1.1.2.2">𝑓</ci><ci id="A4.E2.m1.10.10.1.1.1.1.1.1.1.1.2.3.cmml" xref="A4.E2.m1.10.10.1.1.1.1.1.1.1.1.2.3">𝛾</ci></apply><ci id="A4.E2.m1.1.1.cmml" xref="A4.E2.m1.1.1">𝑋</ci></apply></apply><ci id="A4.E2.m1.9.9a.cmml" xref="A4.E2.m1.9.9"><mtext id="A4.E2.m1.9.9.cmml" xref="A4.E2.m1.9.9">with</mtext></ci></list></apply><apply id="A4.E2.m1.11.11.2.2.3.cmml" xref="A4.E2.m1.11.11.2.2.2"><csymbol cd="ambiguous" id="A4.E2.m1.11.11.2.2.3a.cmml" xref="A4.E2.m1.11.11.2.2.2.3">formulae-sequence</csymbol><apply id="A4.E2.m1.11.11.2.2.1.1.cmml" xref="A4.E2.m1.11.11.2.2.1.1"><eq id="A4.E2.m1.11.11.2.2.1.1.2.cmml" xref="A4.E2.m1.11.11.2.2.1.1.2"></eq><apply id="A4.E2.m1.11.11.2.2.1.1.3.cmml" xref="A4.E2.m1.11.11.2.2.1.1.3"><times id="A4.E2.m1.11.11.2.2.1.1.3.1.cmml" xref="A4.E2.m1.11.11.2.2.1.1.3.1"></times><apply id="A4.E2.m1.11.11.2.2.1.1.3.2.cmml" xref="A4.E2.m1.11.11.2.2.1.1.3.2"><csymbol cd="ambiguous" id="A4.E2.m1.11.11.2.2.1.1.3.2.1.cmml" xref="A4.E2.m1.11.11.2.2.1.1.3.2">subscript</csymbol><ci id="A4.E2.m1.11.11.2.2.1.1.3.2.2.cmml" xref="A4.E2.m1.11.11.2.2.1.1.3.2.2">𝑓</ci><ci id="A4.E2.m1.11.11.2.2.1.1.3.2.3.cmml" xref="A4.E2.m1.11.11.2.2.1.1.3.2.3">𝛾</ci></apply><ci id="A4.E2.m1.2.2.cmml" xref="A4.E2.m1.2.2">⋅</ci></apply><apply id="A4.E2.m1.11.11.2.2.1.1.1.cmml" xref="A4.E2.m1.11.11.2.2.1.1.1"><times id="A4.E2.m1.11.11.2.2.1.1.1.2.cmml" xref="A4.E2.m1.11.11.2.2.1.1.1.2"></times><ci id="A4.E2.m1.11.11.2.2.1.1.1.3.cmml" xref="A4.E2.m1.11.11.2.2.1.1.1.3">𝐵</ci><ci id="A4.E2.m1.11.11.2.2.1.1.1.4.cmml" xref="A4.E2.m1.11.11.2.2.1.1.1.4">𝑅</ci><interval closure="open" id="A4.E2.m1.11.11.2.2.1.1.1.1.2.cmml" xref="A4.E2.m1.11.11.2.2.1.1.1.1.1"><ci id="A4.E2.m1.5.5.cmml" xref="A4.E2.m1.5.5">⋅</ci><apply id="A4.E2.m1.11.11.2.2.1.1.1.1.1.1.cmml" xref="A4.E2.m1.11.11.2.2.1.1.1.1.1.1"><times id="A4.E2.m1.11.11.2.2.1.1.1.1.1.1.1.cmml" xref="A4.E2.m1.11.11.2.2.1.1.1.1.1.1.1"></times><ci id="A4.E2.m1.11.11.2.2.1.1.1.1.1.1.2.cmml" xref="A4.E2.m1.11.11.2.2.1.1.1.1.1.1.2">𝑆</ci><ci id="A4.E2.m1.11.11.2.2.1.1.1.1.1.1.3.cmml" xref="A4.E2.m1.11.11.2.2.1.1.1.1.1.1.3">𝐴</ci><ci id="A4.E2.m1.11.11.2.2.1.1.1.1.1.1.4.cmml" xref="A4.E2.m1.11.11.2.2.1.1.1.1.1.1.4">𝑀</ci><interval closure="open" id="A4.E2.m1.11.11.2.2.1.1.1.1.1.1.5.1.cmml" xref="A4.E2.m1.11.11.2.2.1.1.1.1.1.1.5.2"><ci id="A4.E2.m1.3.3.cmml" xref="A4.E2.m1.3.3">⋅</ci><ci id="A4.E2.m1.4.4.cmml" xref="A4.E2.m1.4.4">𝐵</ci></interval></apply></interval></apply></apply><apply id="A4.E2.m1.11.11.2.2.2.2.cmml" xref="A4.E2.m1.11.11.2.2.2.2"><eq id="A4.E2.m1.11.11.2.2.2.2.1.cmml" xref="A4.E2.m1.11.11.2.2.2.2.1"></eq><apply id="A4.E2.m1.11.11.2.2.2.2.2.cmml" xref="A4.E2.m1.11.11.2.2.2.2.2"><times id="A4.E2.m1.11.11.2.2.2.2.2.1.cmml" xref="A4.E2.m1.11.11.2.2.2.2.2.1"></times><apply id="A4.E2.m1.11.11.2.2.2.2.2.2.cmml" xref="A4.E2.m1.11.11.2.2.2.2.2.2"><csymbol cd="ambiguous" id="A4.E2.m1.11.11.2.2.2.2.2.2.1.cmml" xref="A4.E2.m1.11.11.2.2.2.2.2.2">subscript</csymbol><ci id="A4.E2.m1.11.11.2.2.2.2.2.2.2.cmml" xref="A4.E2.m1.11.11.2.2.2.2.2.2.2">𝑓</ci><ci id="A4.E2.m1.11.11.2.2.2.2.2.2.3.cmml" xref="A4.E2.m1.11.11.2.2.2.2.2.2.3">𝜃</ci></apply><ci id="A4.E2.m1.6.6.cmml" xref="A4.E2.m1.6.6">⋅</ci></apply><apply id="A4.E2.m1.11.11.2.2.2.2.3.cmml" xref="A4.E2.m1.11.11.2.2.2.2.3"><times id="A4.E2.m1.11.11.2.2.2.2.3.1.cmml" xref="A4.E2.m1.11.11.2.2.2.2.3.1"></times><ci id="A4.E2.m1.11.11.2.2.2.2.3.2.cmml" xref="A4.E2.m1.11.11.2.2.2.2.3.2">𝑅</ci><ci id="A4.E2.m1.11.11.2.2.2.2.3.3.cmml" xref="A4.E2.m1.11.11.2.2.2.2.3.3">𝐶</ci><interval closure="open" id="A4.E2.m1.11.11.2.2.2.2.3.4.1.cmml" xref="A4.E2.m1.11.11.2.2.2.2.3.4.2"><ci id="A4.E2.m1.7.7.cmml" xref="A4.E2.m1.7.7">⋅</ci><ci id="A4.E2.m1.8.8.cmml" xref="A4.E2.m1.8.8">𝐵</ci></interval></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.E2.m1.11c">\widetilde{X}=f_{\theta}(f_{\gamma}(X))\quad\text{with}\quad f_{\gamma}(\cdot)=BR(\cdot,SAM(\cdot,B))\quad f_{\theta}(\cdot)=RC(\cdot,B)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="A4.p4.5" class="ltx_p">where <math id="A4.p4.2.m1.1" class="ltx_Math" alttext="B" display="inline"><semantics id="A4.p4.2.m1.1a"><mi id="A4.p4.2.m1.1.1" xref="A4.p4.2.m1.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="A4.p4.2.m1.1b"><ci id="A4.p4.2.m1.1.1.cmml" xref="A4.p4.2.m1.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.2.m1.1c">B</annotation></semantics></math> represents the box coordinates of the target, <math id="A4.p4.3.m2.1" class="ltx_Math" alttext="BR" display="inline"><semantics id="A4.p4.3.m2.1a"><mrow id="A4.p4.3.m2.1.1" xref="A4.p4.3.m2.1.1.cmml"><mi id="A4.p4.3.m2.1.1.2" xref="A4.p4.3.m2.1.1.2.cmml">B</mi><mo lspace="0em" rspace="0em" id="A4.p4.3.m2.1.1.1" xref="A4.p4.3.m2.1.1.1.cmml">​</mo><mi id="A4.p4.3.m2.1.1.3" xref="A4.p4.3.m2.1.1.3.cmml">R</mi></mrow><annotation-xml encoding="MathML-Content" id="A4.p4.3.m2.1b"><apply id="A4.p4.3.m2.1.1.cmml" xref="A4.p4.3.m2.1.1"><times id="A4.p4.3.m2.1.1.1.cmml" xref="A4.p4.3.m2.1.1.1"></times><ci id="A4.p4.3.m2.1.1.2.cmml" xref="A4.p4.3.m2.1.1.2">𝐵</ci><ci id="A4.p4.3.m2.1.1.3.cmml" xref="A4.p4.3.m2.1.1.3">𝑅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.3.m2.1c">BR</annotation></semantics></math> and <math id="A4.p4.4.m3.1" class="ltx_Math" alttext="RC" display="inline"><semantics id="A4.p4.4.m3.1a"><mrow id="A4.p4.4.m3.1.1" xref="A4.p4.4.m3.1.1.cmml"><mi id="A4.p4.4.m3.1.1.2" xref="A4.p4.4.m3.1.1.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="A4.p4.4.m3.1.1.1" xref="A4.p4.4.m3.1.1.1.cmml">​</mo><mi id="A4.p4.4.m3.1.1.3" xref="A4.p4.4.m3.1.1.3.cmml">C</mi></mrow><annotation-xml encoding="MathML-Content" id="A4.p4.4.m3.1b"><apply id="A4.p4.4.m3.1.1.cmml" xref="A4.p4.4.m3.1.1"><times id="A4.p4.4.m3.1.1.1.cmml" xref="A4.p4.4.m3.1.1.1"></times><ci id="A4.p4.4.m3.1.1.2.cmml" xref="A4.p4.4.m3.1.1.2">𝑅</ci><ci id="A4.p4.4.m3.1.1.3.cmml" xref="A4.p4.4.m3.1.1.3">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.4.m3.1c">RC</annotation></semantics></math> represent the reverse Gaussian blur for the mask area and the inscribed ellipse for the target bounding box, respectively. <math id="A4.p4.5.m4.1" class="ltx_Math" alttext="SAM" display="inline"><semantics id="A4.p4.5.m4.1a"><mrow id="A4.p4.5.m4.1.1" xref="A4.p4.5.m4.1.1.cmml"><mi id="A4.p4.5.m4.1.1.2" xref="A4.p4.5.m4.1.1.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="A4.p4.5.m4.1.1.1" xref="A4.p4.5.m4.1.1.1.cmml">​</mo><mi id="A4.p4.5.m4.1.1.3" xref="A4.p4.5.m4.1.1.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="A4.p4.5.m4.1.1.1a" xref="A4.p4.5.m4.1.1.1.cmml">​</mo><mi id="A4.p4.5.m4.1.1.4" xref="A4.p4.5.m4.1.1.4.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="A4.p4.5.m4.1b"><apply id="A4.p4.5.m4.1.1.cmml" xref="A4.p4.5.m4.1.1"><times id="A4.p4.5.m4.1.1.1.cmml" xref="A4.p4.5.m4.1.1.1"></times><ci id="A4.p4.5.m4.1.1.2.cmml" xref="A4.p4.5.m4.1.1.2">𝑆</ci><ci id="A4.p4.5.m4.1.1.3.cmml" xref="A4.p4.5.m4.1.1.3">𝐴</ci><ci id="A4.p4.5.m4.1.1.4.cmml" xref="A4.p4.5.m4.1.1.4">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.5.m4.1c">SAM</annotation></semantics></math> denotes Segment Anything Model <cite class="ltx_cite ltx_citemacro_citep">(Kirillov et al., <a href="#bib.bib21" title="" class="ltx_ref">2023</a>)</cite>, which makes visual prompting more refined.</p>
</div>
<div id="A4.p5" class="ltx_para ltx_noindent">
<p id="A4.p5.6" class="ltx_p">Once we obtain image <math id="A4.p5.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="A4.p5.1.m1.1a"><mi id="A4.p5.1.m1.1.1" xref="A4.p5.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="A4.p5.1.m1.1b"><ci id="A4.p5.1.m1.1.1.cmml" xref="A4.p5.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p5.1.m1.1c">x</annotation></semantics></math> and visually prompted <math id="A4.p5.2.m2.1" class="ltx_Math" alttext="\widetilde{X}" display="inline"><semantics id="A4.p5.2.m2.1a"><mover accent="true" id="A4.p5.2.m2.1.1" xref="A4.p5.2.m2.1.1.cmml"><mi id="A4.p5.2.m2.1.1.2" xref="A4.p5.2.m2.1.1.2.cmml">X</mi><mo id="A4.p5.2.m2.1.1.1" xref="A4.p5.2.m2.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="A4.p5.2.m2.1b"><apply id="A4.p5.2.m2.1.1.cmml" xref="A4.p5.2.m2.1.1"><ci id="A4.p5.2.m2.1.1.1.cmml" xref="A4.p5.2.m2.1.1.1">~</ci><ci id="A4.p5.2.m2.1.1.2.cmml" xref="A4.p5.2.m2.1.1.2">𝑋</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p5.2.m2.1c">\widetilde{X}</annotation></semantics></math>, we send both of them to the CLIP model together with the generated expressions <math id="A4.p5.3.m3.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="A4.p5.3.m3.1a"><mi id="A4.p5.3.m3.1.1" xref="A4.p5.3.m3.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="A4.p5.3.m3.1b"><ci id="A4.p5.3.m3.1.1.cmml" xref="A4.p5.3.m3.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p5.3.m3.1c">Y</annotation></semantics></math> from single modality and multi-modality pathways. Then, we can compute the global score <math id="A4.p5.4.m4.1" class="ltx_Math" alttext="S_{g}" display="inline"><semantics id="A4.p5.4.m4.1a"><msub id="A4.p5.4.m4.1.1" xref="A4.p5.4.m4.1.1.cmml"><mi id="A4.p5.4.m4.1.1.2" xref="A4.p5.4.m4.1.1.2.cmml">S</mi><mi id="A4.p5.4.m4.1.1.3" xref="A4.p5.4.m4.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="A4.p5.4.m4.1b"><apply id="A4.p5.4.m4.1.1.cmml" xref="A4.p5.4.m4.1.1"><csymbol cd="ambiguous" id="A4.p5.4.m4.1.1.1.cmml" xref="A4.p5.4.m4.1.1">subscript</csymbol><ci id="A4.p5.4.m4.1.1.2.cmml" xref="A4.p5.4.m4.1.1.2">𝑆</ci><ci id="A4.p5.4.m4.1.1.3.cmml" xref="A4.p5.4.m4.1.1.3">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p5.4.m4.1c">S_{g}</annotation></semantics></math> and local score <math id="A4.p5.5.m5.1" class="ltx_Math" alttext="S_{l}" display="inline"><semantics id="A4.p5.5.m5.1a"><msub id="A4.p5.5.m5.1.1" xref="A4.p5.5.m5.1.1.cmml"><mi id="A4.p5.5.m5.1.1.2" xref="A4.p5.5.m5.1.1.2.cmml">S</mi><mi id="A4.p5.5.m5.1.1.3" xref="A4.p5.5.m5.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="A4.p5.5.m5.1b"><apply id="A4.p5.5.m5.1.1.cmml" xref="A4.p5.5.m5.1.1"><csymbol cd="ambiguous" id="A4.p5.5.m5.1.1.1.cmml" xref="A4.p5.5.m5.1.1">subscript</csymbol><ci id="A4.p5.5.m5.1.1.2.cmml" xref="A4.p5.5.m5.1.1.2">𝑆</ci><ci id="A4.p5.5.m5.1.1.3.cmml" xref="A4.p5.5.m5.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p5.5.m5.1c">S_{l}</annotation></semantics></math>, and thus obtain the final score <math id="A4.p5.6.m6.1" class="ltx_Math" alttext="S_{f}" display="inline"><semantics id="A4.p5.6.m6.1a"><msub id="A4.p5.6.m6.1.1" xref="A4.p5.6.m6.1.1.cmml"><mi id="A4.p5.6.m6.1.1.2" xref="A4.p5.6.m6.1.1.2.cmml">S</mi><mi id="A4.p5.6.m6.1.1.3" xref="A4.p5.6.m6.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="A4.p5.6.m6.1b"><apply id="A4.p5.6.m6.1.1.cmml" xref="A4.p5.6.m6.1.1"><csymbol cd="ambiguous" id="A4.p5.6.m6.1.1.1.cmml" xref="A4.p5.6.m6.1.1">subscript</csymbol><ci id="A4.p5.6.m6.1.1.2.cmml" xref="A4.p5.6.m6.1.1.2">𝑆</ci><ci id="A4.p5.6.m6.1.1.3.cmml" xref="A4.p5.6.m6.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p5.6.m6.1c">S_{f}</annotation></semantics></math> as illustrated in equation <a href="#S3.E1" title="In Visual Prompting ‣ 3.3 Expression Dropout ‣ 3 InstructDET ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> for expression dropout.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Instruction Generation for Mining Clusters of Multi-objects</h2>

<div id="A5.p1" class="ltx_para ltx_noindent">
<p id="A5.p1.1" class="ltx_p">In order to find and summarize the common attributes among multiple objects in the clusters from DBSCAN <cite class="ltx_cite ltx_citemacro_citep">(Ester et al., <a href="#bib.bib11" title="" class="ltx_ref">1996</a>)</cite>, we use LLaMA for a further analysis. Table <a href="#A5.T10" title="Table 10 ‣ Appendix E Instruction Generation for Mining Clusters of Multi-objects ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> shows the text prompt of task description and in-context examples we use for the LLaMA inputs. Then LLaMA produces expressions for multi-objects as shown in Fig. <a href="#S3.F4" title="Figure 4 ‣ Visual Prompting ‣ 3.3 Expression Dropout ‣ 3 InstructDET ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure id="A5.T10" class="ltx_table"><svg id="A5.T10.pic1" class="ltx_picture" height="335.09" overflow="visible" version="1.1" width="551.18"><g transform="translate(0,335.09) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000" fill-opacity="1.0"><path d="M 0 13.78 L 0 321.31 C 0 328.92 6.17 335.09 13.78 335.09 L 537.4 335.09 C 545.01 335.09 551.18 328.92 551.18 321.31 L 551.18 13.78 C 551.18 6.17 545.01 0 537.4 0 L 13.78 0 C 6.17 0 0 6.17 0 13.78 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 13.78 L 1.97 321.31 C 1.97 327.83 7.26 333.12 13.78 333.12 L 537.4 333.12 C 543.92 333.12 549.21 327.83 549.21 321.31 L 549.21 13.78 C 549.21 7.26 543.92 1.97 537.4 1.97 L 13.78 1.97 C 7.26 1.97 1.97 7.26 1.97 13.78 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="507.87" height="307.53" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span id="A5.T10.pic1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:367.0pt;">
<span id="A5.T10.pic1.1.1.1.1.1.1" class="ltx_p"><span id="A5.T10.pic1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Task Description</span></span>
<span id="A5.T10.pic1.1.1.1.1.1.2" class="ltx_p">You are an AI language assistant that can analyze phrases and sentenses.
User will give you descriptions of several objects in an image. Descriptions of each object are several phrases or short sentences.</span>
<span id="A5.T10.pic1.1.1.1.1.1.3" class="ltx_p">The given objects are expected to have similar properties. Based on the descriptions, find the common properties between given objects and summerize precisely as an assistant: common properties between objects can include same types, same functions, same color components, same poses, same relationships with other objects, engaging in the same activity, etc.</span>
<span id="A5.T10.pic1.1.1.1.1.1.4" class="ltx_p">If there are no common properties between given objects, just tell that there are no common properties. Your summery should also be phrases. Do not repeat.</span>
<span id="A5.T10.pic1.1.1.1.1.1.5" class="ltx_p">Give similarity between all given objects, contrary properties like different positions or different colors of clothes should not be included together in your descriptions.</span>
<span id="A5.T10.pic1.1.1.1.1.1.6" class="ltx_p"><span id="A5.T10.pic1.1.1.1.1.1.6.1" class="ltx_text ltx_font_bold">One In-context Example</span></span>
<span id="A5.T10.pic1.1.1.1.1.1.7" class="ltx_p">Prompt:
Objects and their descriptions:</span>
<span id="A5.T10.pic1.1.1.1.1.1.8" class="ltx_p">## object 2: girl sitting on bed, girl with toy, girl sitting on bed</span>
<span id="A5.T10.pic1.1.1.1.1.1.9" class="ltx_p">## object 3: man looking down, boy sitting on the bed, man sitting on bed</span>
<span id="A5.T10.pic1.1.1.1.1.1.10" class="ltx_p">Please find an summarize the similar properties of given objects.</span>
<span id="A5.T10.pic1.1.1.1.1.1.11" class="ltx_p">Response:
Summary of common properties of given objects:</span>
<span id="A5.T10.pic1.1.1.1.1.1.12" class="ltx_p">## people on bed; person sitting on bed; people playing on bed; who sitting on bed;</span>
</span></foreignObject></g></g></svg>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 10: </span>Task description and an in-context example for multi-objects instruction generation.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A6" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Instruction Grouping</h2>

<div id="A6.p1" class="ltx_para ltx_noindent">
<p id="A6.p1.1" class="ltx_p">We use LLaMA to analyze different extents of description based on object category, attribute, and relations. For each instruction of single object in the InDET dataset, we use LLaMA to assign it into one of the preset 4 groups. Table <a href="#A6.T11" title="Table 11 ‣ Appendix F Instruction Grouping ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> shows the text prompts of task description and in-context examples we use for instruction grouping. For instructions of multiple objects, we assign them to G5 if there is the combination (e.g., “and”) of single instructions, or we assign them to G6 if the instructions are generated without concatenation.</p>
</div>
<figure id="A6.T11" class="ltx_table"><svg id="A6.T11.pic1" class="ltx_picture" height="454.97" overflow="visible" version="1.1" width="551.18"><g transform="translate(0,454.97) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000" fill-opacity="1.0"><path d="M 0 13.78 L 0 441.19 C 0 448.8 6.17 454.97 13.78 454.97 L 537.4 454.97 C 545.01 454.97 551.18 448.8 551.18 441.19 L 551.18 13.78 C 551.18 6.17 545.01 0 537.4 0 L 13.78 0 C 6.17 0 0 6.17 0 13.78 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 13.78 L 1.97 441.19 C 1.97 447.71 7.26 453 13.78 453 L 537.4 453 C 543.92 453 549.21 447.71 549.21 441.19 L 549.21 13.78 C 549.21 7.26 543.92 1.97 537.4 1.97 L 13.78 1.97 C 7.26 1.97 1.97 7.26 1.97 13.78 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="507.87" height="427.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span id="A6.T11.pic1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:367.0pt;">
<span id="A6.T11.pic1.1.1.1.1.1.1" class="ltx_p"><span id="A6.T11.pic1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Task Description</span></span>
<span id="A6.T11.pic1.1.1.1.1.1.2" class="ltx_p">You are an AI language assistant that can analyze the language complexity of sentences or phrases.</span>
<span id="A6.T11.pic1.1.1.1.1.1.3" class="ltx_p">User will give you a phrase or sentence describing a specific object in a image, which could be composed of nouns, adjectives, verbs, etc.</span>
<span id="A6.T11.pic1.1.1.1.1.1.4" class="ltx_p">Grade the description according to its language complexity as an AI language assistant.</span>
<span id="A6.T11.pic1.1.1.1.1.1.5" class="ltx_p">The language complexity of a phrase or sentence describing a specific object in an image can be graded into four levels:</span>
<span id="A6.T11.pic1.1.1.1.1.1.6" class="ltx_p"><span id="A6.T11.pic1.1.1.1.1.1.6.1" class="ltx_text ltx_font_bold">Level 0.</span> A single noun is used to give the object’s name or type.</span>
<span id="A6.T11.pic1.1.1.1.1.1.7" class="ltx_p"><span id="A6.T11.pic1.1.1.1.1.1.7.1" class="ltx_text ltx_font_bold">Level 1.</span> A phrase with one or more nouns, verbs and abjectives is used to describe simple attributes of the object itself, such as its color, its function or purpose, location in the image, or actions.</span>
<span id="A6.T11.pic1.1.1.1.1.1.8" class="ltx_p"><span id="A6.T11.pic1.1.1.1.1.1.8.1" class="ltx_text ltx_font_bold">Level 2</span> A phrase with one or more nouns, verbs and abjectives is used to describe the object by referring to other objects in the image and describing their relationship, such as their relative positions or interactions.</span>
<span id="A6.T11.pic1.1.1.1.1.1.9" class="ltx_p"><span id="A6.T11.pic1.1.1.1.1.1.9.1" class="ltx_text ltx_font_bold">Level 3.</span> A long phrase or sentence is used to describe attributes of the object and also refer to a few other objects in detail, or describe a complicated or comprehensive/implicit relationship between multiple objects.</span>
<span id="A6.T11.pic1.1.1.1.1.1.10" class="ltx_p">The level of descriptions increase as the language complexity and length increase, and also increase as the phrases or sentences become more descriptive and use more abjectives and nouns to describe the object.</span>
<span id="A6.T11.pic1.1.1.1.1.1.11" class="ltx_p"><span id="A6.T11.pic1.1.1.1.1.1.11.1" class="ltx_text ltx_font_bold">One In-context Example</span></span>
<span id="A6.T11.pic1.1.1.1.1.1.12" class="ltx_p">Prompt:
Grade description: people who are sitting under an umbrella.</span>
<span id="A6.T11.pic1.1.1.1.1.1.13" class="ltx_p">Response:
My grading for description people who are sitting under an umbrella: This phrase is referring to the object of people, and gives simple object action of sitting and object relationship with the umbrella. The level of this description is: level 2.</span>
</span></foreignObject></g></g></svg>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 11: </span>Task description and in-context examples for single-object instruction grouping.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A7" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>Model and Implementation Details</h2>

<div id="A7.p1" class="ltx_para ltx_noindent">
<p id="A7.p1.1" class="ltx_p">In this section, we illustrate our model architecture and training configurations.</p>
</div>
<section id="A7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">G.1 </span>Model Architecture</h3>

<figure id="A7.F13" class="ltx_figure"><img src="/html/2310.05136/assets/x14.png" id="A7.F13.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="380" height="172" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>A detailed overview of our diversified referring object detection (DROD) model.</figcaption>
</figure>
<div id="A7.SS1.p1" class="ltx_para ltx_noindent">
<p id="A7.SS1.p1.1" class="ltx_p">In the main text, We briefly introduced the DROD model architecture we use. Here, we introduce the overall model structure in Fig. <a href="#A7.F13" title="Figure 13 ‣ G.1 Model Architecture ‣ Appendix G Model and Implementation Details ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>.</p>
</div>
<section id="A7.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Visual Textual Feature Fusion</h4>

<div id="A7.SS1.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="A7.SS1.SSS0.Px1.p1.2" class="ltx_p">We use a general text encoder <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a href="#bib.bib6" title="" class="ltx_ref">2018</a>)</cite> and vision encoder <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy et al., <a href="#bib.bib9" title="" class="ltx_ref">2021</a>)</cite> to obtain text features <math id="A7.SS1.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="F_{y}" display="inline"><semantics id="A7.SS1.SSS0.Px1.p1.1.m1.1a"><msub id="A7.SS1.SSS0.Px1.p1.1.m1.1.1" xref="A7.SS1.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="A7.SS1.SSS0.Px1.p1.1.m1.1.1.2" xref="A7.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml">F</mi><mi id="A7.SS1.SSS0.Px1.p1.1.m1.1.1.3" xref="A7.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml">y</mi></msub><annotation-xml encoding="MathML-Content" id="A7.SS1.SSS0.Px1.p1.1.m1.1b"><apply id="A7.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="A7.SS1.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A7.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="A7.SS1.SSS0.Px1.p1.1.m1.1.1">subscript</csymbol><ci id="A7.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="A7.SS1.SSS0.Px1.p1.1.m1.1.1.2">𝐹</ci><ci id="A7.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="A7.SS1.SSS0.Px1.p1.1.m1.1.1.3">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.SS1.SSS0.Px1.p1.1.m1.1c">F_{y}</annotation></semantics></math> and image features <math id="A7.SS1.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="F_{x}" display="inline"><semantics id="A7.SS1.SSS0.Px1.p1.2.m2.1a"><msub id="A7.SS1.SSS0.Px1.p1.2.m2.1.1" xref="A7.SS1.SSS0.Px1.p1.2.m2.1.1.cmml"><mi id="A7.SS1.SSS0.Px1.p1.2.m2.1.1.2" xref="A7.SS1.SSS0.Px1.p1.2.m2.1.1.2.cmml">F</mi><mi id="A7.SS1.SSS0.Px1.p1.2.m2.1.1.3" xref="A7.SS1.SSS0.Px1.p1.2.m2.1.1.3.cmml">x</mi></msub><annotation-xml encoding="MathML-Content" id="A7.SS1.SSS0.Px1.p1.2.m2.1b"><apply id="A7.SS1.SSS0.Px1.p1.2.m2.1.1.cmml" xref="A7.SS1.SSS0.Px1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="A7.SS1.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="A7.SS1.SSS0.Px1.p1.2.m2.1.1">subscript</csymbol><ci id="A7.SS1.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="A7.SS1.SSS0.Px1.p1.2.m2.1.1.2">𝐹</ci><ci id="A7.SS1.SSS0.Px1.p1.2.m2.1.1.3.cmml" xref="A7.SS1.SSS0.Px1.p1.2.m2.1.1.3">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.SS1.SSS0.Px1.p1.2.m2.1c">F_{x}</annotation></semantics></math>. In order to improve the attention of visual contents on the described regions by the text expressions, we conduct a multi-modality feature fusion. Specifically, we use a bidirectional cross attention (Bi-XAtt) module to enhance image and text features through information transfer between modalities. The enhanced image and text features are then added to the original features. This process can be formulated as follows:</p>
<table id="A7.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A7.E3.m1.3" class="ltx_Math" alttext="F_{x}^{\prime}=F_{x}+F_{y2x}\;;\;F_{y}^{\prime}=F_{y}+F_{x2y}\;\;\;\;\text{with}\;\;\;\;F_{y2x},F_{x2y}=\text{Bi-XAtt}(F_{x},F_{y})" display="block"><semantics id="A7.E3.m1.3a"><mrow id="A7.E3.m1.3.3.2" xref="A7.E3.m1.3.3.3.cmml"><mrow id="A7.E3.m1.2.2.1.1" xref="A7.E3.m1.2.2.1.1.cmml"><msubsup id="A7.E3.m1.2.2.1.1.2" xref="A7.E3.m1.2.2.1.1.2.cmml"><mi id="A7.E3.m1.2.2.1.1.2.2.2" xref="A7.E3.m1.2.2.1.1.2.2.2.cmml">F</mi><mi id="A7.E3.m1.2.2.1.1.2.2.3" xref="A7.E3.m1.2.2.1.1.2.2.3.cmml">x</mi><mo id="A7.E3.m1.2.2.1.1.2.3" xref="A7.E3.m1.2.2.1.1.2.3.cmml">′</mo></msubsup><mo id="A7.E3.m1.2.2.1.1.1" xref="A7.E3.m1.2.2.1.1.1.cmml">=</mo><mrow id="A7.E3.m1.2.2.1.1.3" xref="A7.E3.m1.2.2.1.1.3.cmml"><msub id="A7.E3.m1.2.2.1.1.3.2" xref="A7.E3.m1.2.2.1.1.3.2.cmml"><mi id="A7.E3.m1.2.2.1.1.3.2.2" xref="A7.E3.m1.2.2.1.1.3.2.2.cmml">F</mi><mi id="A7.E3.m1.2.2.1.1.3.2.3" xref="A7.E3.m1.2.2.1.1.3.2.3.cmml">x</mi></msub><mo id="A7.E3.m1.2.2.1.1.3.1" xref="A7.E3.m1.2.2.1.1.3.1.cmml">+</mo><msub id="A7.E3.m1.2.2.1.1.3.3" xref="A7.E3.m1.2.2.1.1.3.3.cmml"><mi id="A7.E3.m1.2.2.1.1.3.3.2" xref="A7.E3.m1.2.2.1.1.3.3.2.cmml">F</mi><mrow id="A7.E3.m1.2.2.1.1.3.3.3" xref="A7.E3.m1.2.2.1.1.3.3.3.cmml"><mi id="A7.E3.m1.2.2.1.1.3.3.3.2" xref="A7.E3.m1.2.2.1.1.3.3.3.2.cmml">y</mi><mo lspace="0em" rspace="0em" id="A7.E3.m1.2.2.1.1.3.3.3.1" xref="A7.E3.m1.2.2.1.1.3.3.3.1.cmml">​</mo><mn id="A7.E3.m1.2.2.1.1.3.3.3.3" xref="A7.E3.m1.2.2.1.1.3.3.3.3.cmml">2</mn><mo lspace="0em" rspace="0em" id="A7.E3.m1.2.2.1.1.3.3.3.1a" xref="A7.E3.m1.2.2.1.1.3.3.3.1.cmml">​</mo><mi id="A7.E3.m1.2.2.1.1.3.3.3.4" xref="A7.E3.m1.2.2.1.1.3.3.3.4.cmml">x</mi></mrow></msub></mrow></mrow><mo rspace="0.447em" id="A7.E3.m1.3.3.2.3" xref="A7.E3.m1.3.3.3a.cmml">;</mo><mrow id="A7.E3.m1.3.3.2.2.2" xref="A7.E3.m1.3.3.2.2.3.cmml"><mrow id="A7.E3.m1.3.3.2.2.1.1" xref="A7.E3.m1.3.3.2.2.1.1.cmml"><msubsup id="A7.E3.m1.3.3.2.2.1.1.4" xref="A7.E3.m1.3.3.2.2.1.1.4.cmml"><mi id="A7.E3.m1.3.3.2.2.1.1.4.2.2" xref="A7.E3.m1.3.3.2.2.1.1.4.2.2.cmml">F</mi><mi id="A7.E3.m1.3.3.2.2.1.1.4.2.3" xref="A7.E3.m1.3.3.2.2.1.1.4.2.3.cmml">y</mi><mo id="A7.E3.m1.3.3.2.2.1.1.4.3" xref="A7.E3.m1.3.3.2.2.1.1.4.3.cmml">′</mo></msubsup><mo id="A7.E3.m1.3.3.2.2.1.1.3" xref="A7.E3.m1.3.3.2.2.1.1.3.cmml">=</mo><mrow id="A7.E3.m1.3.3.2.2.1.1.2.2" xref="A7.E3.m1.3.3.2.2.1.1.2.3.cmml"><mrow id="A7.E3.m1.3.3.2.2.1.1.1.1.1" xref="A7.E3.m1.3.3.2.2.1.1.1.1.1.cmml"><msub id="A7.E3.m1.3.3.2.2.1.1.1.1.1.2" xref="A7.E3.m1.3.3.2.2.1.1.1.1.1.2.cmml"><mi id="A7.E3.m1.3.3.2.2.1.1.1.1.1.2.2" xref="A7.E3.m1.3.3.2.2.1.1.1.1.1.2.2.cmml">F</mi><mi id="A7.E3.m1.3.3.2.2.1.1.1.1.1.2.3" xref="A7.E3.m1.3.3.2.2.1.1.1.1.1.2.3.cmml">y</mi></msub><mo id="A7.E3.m1.3.3.2.2.1.1.1.1.1.1" xref="A7.E3.m1.3.3.2.2.1.1.1.1.1.1.cmml">+</mo><msub id="A7.E3.m1.3.3.2.2.1.1.1.1.1.3" xref="A7.E3.m1.3.3.2.2.1.1.1.1.1.3.cmml"><mi id="A7.E3.m1.3.3.2.2.1.1.1.1.1.3.2" xref="A7.E3.m1.3.3.2.2.1.1.1.1.1.3.2.cmml">F</mi><mrow id="A7.E3.m1.3.3.2.2.1.1.1.1.1.3.3" xref="A7.E3.m1.3.3.2.2.1.1.1.1.1.3.3.cmml"><mi id="A7.E3.m1.3.3.2.2.1.1.1.1.1.3.3.2" xref="A7.E3.m1.3.3.2.2.1.1.1.1.1.3.3.2.cmml">x</mi><mo lspace="0em" rspace="0em" id="A7.E3.m1.3.3.2.2.1.1.1.1.1.3.3.1" xref="A7.E3.m1.3.3.2.2.1.1.1.1.1.3.3.1.cmml">​</mo><mn id="A7.E3.m1.3.3.2.2.1.1.1.1.1.3.3.3" xref="A7.E3.m1.3.3.2.2.1.1.1.1.1.3.3.3.cmml">2</mn><mo lspace="0em" rspace="0em" id="A7.E3.m1.3.3.2.2.1.1.1.1.1.3.3.1a" xref="A7.E3.m1.3.3.2.2.1.1.1.1.1.3.3.1.cmml">​</mo><mi id="A7.E3.m1.3.3.2.2.1.1.1.1.1.3.3.4" xref="A7.E3.m1.3.3.2.2.1.1.1.1.1.3.3.4.cmml">y</mi></mrow></msub></mrow><mspace width="1.11111111111111em" id="A7.E3.m1.3.3.2.2.1.1.2.2.3" xref="A7.E3.m1.3.3.2.2.1.1.2.3.cmml"></mspace><mtext id="A7.E3.m1.1.1" xref="A7.E3.m1.1.1a.cmml">with</mtext><mspace width="1.11111111111111em" id="A7.E3.m1.3.3.2.2.1.1.2.2.4" xref="A7.E3.m1.3.3.2.2.1.1.2.3.cmml"></mspace><msub id="A7.E3.m1.3.3.2.2.1.1.2.2.2" xref="A7.E3.m1.3.3.2.2.1.1.2.2.2.cmml"><mi id="A7.E3.m1.3.3.2.2.1.1.2.2.2.2" xref="A7.E3.m1.3.3.2.2.1.1.2.2.2.2.cmml">F</mi><mrow id="A7.E3.m1.3.3.2.2.1.1.2.2.2.3" xref="A7.E3.m1.3.3.2.2.1.1.2.2.2.3.cmml"><mi id="A7.E3.m1.3.3.2.2.1.1.2.2.2.3.2" xref="A7.E3.m1.3.3.2.2.1.1.2.2.2.3.2.cmml">y</mi><mo lspace="0em" rspace="0em" id="A7.E3.m1.3.3.2.2.1.1.2.2.2.3.1" xref="A7.E3.m1.3.3.2.2.1.1.2.2.2.3.1.cmml">​</mo><mn id="A7.E3.m1.3.3.2.2.1.1.2.2.2.3.3" xref="A7.E3.m1.3.3.2.2.1.1.2.2.2.3.3.cmml">2</mn><mo lspace="0em" rspace="0em" id="A7.E3.m1.3.3.2.2.1.1.2.2.2.3.1a" xref="A7.E3.m1.3.3.2.2.1.1.2.2.2.3.1.cmml">​</mo><mi id="A7.E3.m1.3.3.2.2.1.1.2.2.2.3.4" xref="A7.E3.m1.3.3.2.2.1.1.2.2.2.3.4.cmml">x</mi></mrow></msub></mrow></mrow><mo id="A7.E3.m1.3.3.2.2.2.3" xref="A7.E3.m1.3.3.2.2.3a.cmml">,</mo><mrow id="A7.E3.m1.3.3.2.2.2.2" xref="A7.E3.m1.3.3.2.2.2.2.cmml"><msub id="A7.E3.m1.3.3.2.2.2.2.4" xref="A7.E3.m1.3.3.2.2.2.2.4.cmml"><mi id="A7.E3.m1.3.3.2.2.2.2.4.2" xref="A7.E3.m1.3.3.2.2.2.2.4.2.cmml">F</mi><mrow id="A7.E3.m1.3.3.2.2.2.2.4.3" xref="A7.E3.m1.3.3.2.2.2.2.4.3.cmml"><mi id="A7.E3.m1.3.3.2.2.2.2.4.3.2" xref="A7.E3.m1.3.3.2.2.2.2.4.3.2.cmml">x</mi><mo lspace="0em" rspace="0em" id="A7.E3.m1.3.3.2.2.2.2.4.3.1" xref="A7.E3.m1.3.3.2.2.2.2.4.3.1.cmml">​</mo><mn id="A7.E3.m1.3.3.2.2.2.2.4.3.3" xref="A7.E3.m1.3.3.2.2.2.2.4.3.3.cmml">2</mn><mo lspace="0em" rspace="0em" id="A7.E3.m1.3.3.2.2.2.2.4.3.1a" xref="A7.E3.m1.3.3.2.2.2.2.4.3.1.cmml">​</mo><mi id="A7.E3.m1.3.3.2.2.2.2.4.3.4" xref="A7.E3.m1.3.3.2.2.2.2.4.3.4.cmml">y</mi></mrow></msub><mo id="A7.E3.m1.3.3.2.2.2.2.3" xref="A7.E3.m1.3.3.2.2.2.2.3.cmml">=</mo><mrow id="A7.E3.m1.3.3.2.2.2.2.2" xref="A7.E3.m1.3.3.2.2.2.2.2.cmml"><mtext id="A7.E3.m1.3.3.2.2.2.2.2.4" xref="A7.E3.m1.3.3.2.2.2.2.2.4a.cmml">Bi-XAtt</mtext><mo lspace="0em" rspace="0em" id="A7.E3.m1.3.3.2.2.2.2.2.3" xref="A7.E3.m1.3.3.2.2.2.2.2.3.cmml">​</mo><mrow id="A7.E3.m1.3.3.2.2.2.2.2.2.2" xref="A7.E3.m1.3.3.2.2.2.2.2.2.3.cmml"><mo stretchy="false" id="A7.E3.m1.3.3.2.2.2.2.2.2.2.3" xref="A7.E3.m1.3.3.2.2.2.2.2.2.3.cmml">(</mo><msub id="A7.E3.m1.3.3.2.2.2.2.1.1.1.1" xref="A7.E3.m1.3.3.2.2.2.2.1.1.1.1.cmml"><mi id="A7.E3.m1.3.3.2.2.2.2.1.1.1.1.2" xref="A7.E3.m1.3.3.2.2.2.2.1.1.1.1.2.cmml">F</mi><mi id="A7.E3.m1.3.3.2.2.2.2.1.1.1.1.3" xref="A7.E3.m1.3.3.2.2.2.2.1.1.1.1.3.cmml">x</mi></msub><mo id="A7.E3.m1.3.3.2.2.2.2.2.2.2.4" xref="A7.E3.m1.3.3.2.2.2.2.2.2.3.cmml">,</mo><msub id="A7.E3.m1.3.3.2.2.2.2.2.2.2.2" xref="A7.E3.m1.3.3.2.2.2.2.2.2.2.2.cmml"><mi id="A7.E3.m1.3.3.2.2.2.2.2.2.2.2.2" xref="A7.E3.m1.3.3.2.2.2.2.2.2.2.2.2.cmml">F</mi><mi id="A7.E3.m1.3.3.2.2.2.2.2.2.2.2.3" xref="A7.E3.m1.3.3.2.2.2.2.2.2.2.2.3.cmml">y</mi></msub><mo stretchy="false" id="A7.E3.m1.3.3.2.2.2.2.2.2.2.5" xref="A7.E3.m1.3.3.2.2.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="A7.E3.m1.3b"><apply id="A7.E3.m1.3.3.3.cmml" xref="A7.E3.m1.3.3.2"><csymbol cd="ambiguous" id="A7.E3.m1.3.3.3a.cmml" xref="A7.E3.m1.3.3.2.3">formulae-sequence</csymbol><apply id="A7.E3.m1.2.2.1.1.cmml" xref="A7.E3.m1.2.2.1.1"><eq id="A7.E3.m1.2.2.1.1.1.cmml" xref="A7.E3.m1.2.2.1.1.1"></eq><apply id="A7.E3.m1.2.2.1.1.2.cmml" xref="A7.E3.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="A7.E3.m1.2.2.1.1.2.1.cmml" xref="A7.E3.m1.2.2.1.1.2">superscript</csymbol><apply id="A7.E3.m1.2.2.1.1.2.2.cmml" xref="A7.E3.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="A7.E3.m1.2.2.1.1.2.2.1.cmml" xref="A7.E3.m1.2.2.1.1.2">subscript</csymbol><ci id="A7.E3.m1.2.2.1.1.2.2.2.cmml" xref="A7.E3.m1.2.2.1.1.2.2.2">𝐹</ci><ci id="A7.E3.m1.2.2.1.1.2.2.3.cmml" xref="A7.E3.m1.2.2.1.1.2.2.3">𝑥</ci></apply><ci id="A7.E3.m1.2.2.1.1.2.3.cmml" xref="A7.E3.m1.2.2.1.1.2.3">′</ci></apply><apply id="A7.E3.m1.2.2.1.1.3.cmml" xref="A7.E3.m1.2.2.1.1.3"><plus id="A7.E3.m1.2.2.1.1.3.1.cmml" xref="A7.E3.m1.2.2.1.1.3.1"></plus><apply id="A7.E3.m1.2.2.1.1.3.2.cmml" xref="A7.E3.m1.2.2.1.1.3.2"><csymbol cd="ambiguous" id="A7.E3.m1.2.2.1.1.3.2.1.cmml" xref="A7.E3.m1.2.2.1.1.3.2">subscript</csymbol><ci id="A7.E3.m1.2.2.1.1.3.2.2.cmml" xref="A7.E3.m1.2.2.1.1.3.2.2">𝐹</ci><ci id="A7.E3.m1.2.2.1.1.3.2.3.cmml" xref="A7.E3.m1.2.2.1.1.3.2.3">𝑥</ci></apply><apply id="A7.E3.m1.2.2.1.1.3.3.cmml" xref="A7.E3.m1.2.2.1.1.3.3"><csymbol cd="ambiguous" id="A7.E3.m1.2.2.1.1.3.3.1.cmml" xref="A7.E3.m1.2.2.1.1.3.3">subscript</csymbol><ci id="A7.E3.m1.2.2.1.1.3.3.2.cmml" xref="A7.E3.m1.2.2.1.1.3.3.2">𝐹</ci><apply id="A7.E3.m1.2.2.1.1.3.3.3.cmml" xref="A7.E3.m1.2.2.1.1.3.3.3"><times id="A7.E3.m1.2.2.1.1.3.3.3.1.cmml" xref="A7.E3.m1.2.2.1.1.3.3.3.1"></times><ci id="A7.E3.m1.2.2.1.1.3.3.3.2.cmml" xref="A7.E3.m1.2.2.1.1.3.3.3.2">𝑦</ci><cn type="integer" id="A7.E3.m1.2.2.1.1.3.3.3.3.cmml" xref="A7.E3.m1.2.2.1.1.3.3.3.3">2</cn><ci id="A7.E3.m1.2.2.1.1.3.3.3.4.cmml" xref="A7.E3.m1.2.2.1.1.3.3.3.4">𝑥</ci></apply></apply></apply></apply><apply id="A7.E3.m1.3.3.2.2.3.cmml" xref="A7.E3.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="A7.E3.m1.3.3.2.2.3a.cmml" xref="A7.E3.m1.3.3.2.2.2.3">formulae-sequence</csymbol><apply id="A7.E3.m1.3.3.2.2.1.1.cmml" xref="A7.E3.m1.3.3.2.2.1.1"><eq id="A7.E3.m1.3.3.2.2.1.1.3.cmml" xref="A7.E3.m1.3.3.2.2.1.1.3"></eq><apply id="A7.E3.m1.3.3.2.2.1.1.4.cmml" xref="A7.E3.m1.3.3.2.2.1.1.4"><csymbol cd="ambiguous" id="A7.E3.m1.3.3.2.2.1.1.4.1.cmml" xref="A7.E3.m1.3.3.2.2.1.1.4">superscript</csymbol><apply id="A7.E3.m1.3.3.2.2.1.1.4.2.cmml" xref="A7.E3.m1.3.3.2.2.1.1.4"><csymbol cd="ambiguous" id="A7.E3.m1.3.3.2.2.1.1.4.2.1.cmml" xref="A7.E3.m1.3.3.2.2.1.1.4">subscript</csymbol><ci id="A7.E3.m1.3.3.2.2.1.1.4.2.2.cmml" xref="A7.E3.m1.3.3.2.2.1.1.4.2.2">𝐹</ci><ci id="A7.E3.m1.3.3.2.2.1.1.4.2.3.cmml" xref="A7.E3.m1.3.3.2.2.1.1.4.2.3">𝑦</ci></apply><ci id="A7.E3.m1.3.3.2.2.1.1.4.3.cmml" xref="A7.E3.m1.3.3.2.2.1.1.4.3">′</ci></apply><list id="A7.E3.m1.3.3.2.2.1.1.2.3.cmml" xref="A7.E3.m1.3.3.2.2.1.1.2.2"><apply id="A7.E3.m1.3.3.2.2.1.1.1.1.1.cmml" xref="A7.E3.m1.3.3.2.2.1.1.1.1.1"><plus id="A7.E3.m1.3.3.2.2.1.1.1.1.1.1.cmml" xref="A7.E3.m1.3.3.2.2.1.1.1.1.1.1"></plus><apply id="A7.E3.m1.3.3.2.2.1.1.1.1.1.2.cmml" xref="A7.E3.m1.3.3.2.2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="A7.E3.m1.3.3.2.2.1.1.1.1.1.2.1.cmml" xref="A7.E3.m1.3.3.2.2.1.1.1.1.1.2">subscript</csymbol><ci id="A7.E3.m1.3.3.2.2.1.1.1.1.1.2.2.cmml" xref="A7.E3.m1.3.3.2.2.1.1.1.1.1.2.2">𝐹</ci><ci id="A7.E3.m1.3.3.2.2.1.1.1.1.1.2.3.cmml" xref="A7.E3.m1.3.3.2.2.1.1.1.1.1.2.3">𝑦</ci></apply><apply id="A7.E3.m1.3.3.2.2.1.1.1.1.1.3.cmml" xref="A7.E3.m1.3.3.2.2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="A7.E3.m1.3.3.2.2.1.1.1.1.1.3.1.cmml" xref="A7.E3.m1.3.3.2.2.1.1.1.1.1.3">subscript</csymbol><ci id="A7.E3.m1.3.3.2.2.1.1.1.1.1.3.2.cmml" xref="A7.E3.m1.3.3.2.2.1.1.1.1.1.3.2">𝐹</ci><apply id="A7.E3.m1.3.3.2.2.1.1.1.1.1.3.3.cmml" xref="A7.E3.m1.3.3.2.2.1.1.1.1.1.3.3"><times id="A7.E3.m1.3.3.2.2.1.1.1.1.1.3.3.1.cmml" xref="A7.E3.m1.3.3.2.2.1.1.1.1.1.3.3.1"></times><ci id="A7.E3.m1.3.3.2.2.1.1.1.1.1.3.3.2.cmml" xref="A7.E3.m1.3.3.2.2.1.1.1.1.1.3.3.2">𝑥</ci><cn type="integer" id="A7.E3.m1.3.3.2.2.1.1.1.1.1.3.3.3.cmml" xref="A7.E3.m1.3.3.2.2.1.1.1.1.1.3.3.3">2</cn><ci id="A7.E3.m1.3.3.2.2.1.1.1.1.1.3.3.4.cmml" xref="A7.E3.m1.3.3.2.2.1.1.1.1.1.3.3.4">𝑦</ci></apply></apply></apply><ci id="A7.E3.m1.1.1a.cmml" xref="A7.E3.m1.1.1"><mtext id="A7.E3.m1.1.1.cmml" xref="A7.E3.m1.1.1">with</mtext></ci><apply id="A7.E3.m1.3.3.2.2.1.1.2.2.2.cmml" xref="A7.E3.m1.3.3.2.2.1.1.2.2.2"><csymbol cd="ambiguous" id="A7.E3.m1.3.3.2.2.1.1.2.2.2.1.cmml" xref="A7.E3.m1.3.3.2.2.1.1.2.2.2">subscript</csymbol><ci id="A7.E3.m1.3.3.2.2.1.1.2.2.2.2.cmml" xref="A7.E3.m1.3.3.2.2.1.1.2.2.2.2">𝐹</ci><apply id="A7.E3.m1.3.3.2.2.1.1.2.2.2.3.cmml" xref="A7.E3.m1.3.3.2.2.1.1.2.2.2.3"><times id="A7.E3.m1.3.3.2.2.1.1.2.2.2.3.1.cmml" xref="A7.E3.m1.3.3.2.2.1.1.2.2.2.3.1"></times><ci id="A7.E3.m1.3.3.2.2.1.1.2.2.2.3.2.cmml" xref="A7.E3.m1.3.3.2.2.1.1.2.2.2.3.2">𝑦</ci><cn type="integer" id="A7.E3.m1.3.3.2.2.1.1.2.2.2.3.3.cmml" xref="A7.E3.m1.3.3.2.2.1.1.2.2.2.3.3">2</cn><ci id="A7.E3.m1.3.3.2.2.1.1.2.2.2.3.4.cmml" xref="A7.E3.m1.3.3.2.2.1.1.2.2.2.3.4">𝑥</ci></apply></apply></list></apply><apply id="A7.E3.m1.3.3.2.2.2.2.cmml" xref="A7.E3.m1.3.3.2.2.2.2"><eq id="A7.E3.m1.3.3.2.2.2.2.3.cmml" xref="A7.E3.m1.3.3.2.2.2.2.3"></eq><apply id="A7.E3.m1.3.3.2.2.2.2.4.cmml" xref="A7.E3.m1.3.3.2.2.2.2.4"><csymbol cd="ambiguous" id="A7.E3.m1.3.3.2.2.2.2.4.1.cmml" xref="A7.E3.m1.3.3.2.2.2.2.4">subscript</csymbol><ci id="A7.E3.m1.3.3.2.2.2.2.4.2.cmml" xref="A7.E3.m1.3.3.2.2.2.2.4.2">𝐹</ci><apply id="A7.E3.m1.3.3.2.2.2.2.4.3.cmml" xref="A7.E3.m1.3.3.2.2.2.2.4.3"><times id="A7.E3.m1.3.3.2.2.2.2.4.3.1.cmml" xref="A7.E3.m1.3.3.2.2.2.2.4.3.1"></times><ci id="A7.E3.m1.3.3.2.2.2.2.4.3.2.cmml" xref="A7.E3.m1.3.3.2.2.2.2.4.3.2">𝑥</ci><cn type="integer" id="A7.E3.m1.3.3.2.2.2.2.4.3.3.cmml" xref="A7.E3.m1.3.3.2.2.2.2.4.3.3">2</cn><ci id="A7.E3.m1.3.3.2.2.2.2.4.3.4.cmml" xref="A7.E3.m1.3.3.2.2.2.2.4.3.4">𝑦</ci></apply></apply><apply id="A7.E3.m1.3.3.2.2.2.2.2.cmml" xref="A7.E3.m1.3.3.2.2.2.2.2"><times id="A7.E3.m1.3.3.2.2.2.2.2.3.cmml" xref="A7.E3.m1.3.3.2.2.2.2.2.3"></times><ci id="A7.E3.m1.3.3.2.2.2.2.2.4a.cmml" xref="A7.E3.m1.3.3.2.2.2.2.2.4"><mtext id="A7.E3.m1.3.3.2.2.2.2.2.4.cmml" xref="A7.E3.m1.3.3.2.2.2.2.2.4">Bi-XAtt</mtext></ci><interval closure="open" id="A7.E3.m1.3.3.2.2.2.2.2.2.3.cmml" xref="A7.E3.m1.3.3.2.2.2.2.2.2.2"><apply id="A7.E3.m1.3.3.2.2.2.2.1.1.1.1.cmml" xref="A7.E3.m1.3.3.2.2.2.2.1.1.1.1"><csymbol cd="ambiguous" id="A7.E3.m1.3.3.2.2.2.2.1.1.1.1.1.cmml" xref="A7.E3.m1.3.3.2.2.2.2.1.1.1.1">subscript</csymbol><ci id="A7.E3.m1.3.3.2.2.2.2.1.1.1.1.2.cmml" xref="A7.E3.m1.3.3.2.2.2.2.1.1.1.1.2">𝐹</ci><ci id="A7.E3.m1.3.3.2.2.2.2.1.1.1.1.3.cmml" xref="A7.E3.m1.3.3.2.2.2.2.1.1.1.1.3">𝑥</ci></apply><apply id="A7.E3.m1.3.3.2.2.2.2.2.2.2.2.cmml" xref="A7.E3.m1.3.3.2.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="A7.E3.m1.3.3.2.2.2.2.2.2.2.2.1.cmml" xref="A7.E3.m1.3.3.2.2.2.2.2.2.2.2">subscript</csymbol><ci id="A7.E3.m1.3.3.2.2.2.2.2.2.2.2.2.cmml" xref="A7.E3.m1.3.3.2.2.2.2.2.2.2.2.2">𝐹</ci><ci id="A7.E3.m1.3.3.2.2.2.2.2.2.2.2.3.cmml" xref="A7.E3.m1.3.3.2.2.2.2.2.2.2.2.3">𝑦</ci></apply></interval></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.E3.m1.3c">F_{x}^{\prime}=F_{x}+F_{y2x}\;;\;F_{y}^{\prime}=F_{y}+F_{x2y}\;\;\;\;\text{with}\;\;\;\;F_{y2x},F_{x2y}=\text{Bi-XAtt}(F_{x},F_{y})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="A7.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Target Discovery and Retrieval</h4>

<div id="A7.SS1.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="A7.SS1.SSS0.Px2.p1.1" class="ltx_p">With the enhanced visual and language representations, we need to extract the targets referred to by the expression from the image features. Our DROD model applies the encoder-decoder architecture of Deformable DETR <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al., <a href="#bib.bib60" title="" class="ltx_ref">2020</a>)</cite>, which allows for more flexible query retrieving. Here we provide a detailed introduction to our internal module.</p>
</div>
<div id="A7.SS1.SSS0.Px2.p2" class="ltx_para ltx_noindent">
<p id="A7.SS1.SSS0.Px2.p2.1" class="ltx_p">The transformer encoder receives multi-scale visual features after text enhancement. Multi-scale Deformable encoder utilizes flexible attention at different scales and spatial shapes to obtain hierarchical characteristics for instances. In addition, following the design of two-stage Deformable DETR, we add an auxiliary prediction head for reference points. The top <math id="A7.SS1.SSS0.Px2.p2.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="A7.SS1.SSS0.Px2.p2.1.m1.1a"><mi id="A7.SS1.SSS0.Px2.p2.1.m1.1.1" xref="A7.SS1.SSS0.Px2.p2.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="A7.SS1.SSS0.Px2.p2.1.m1.1b"><ci id="A7.SS1.SSS0.Px2.p2.1.m1.1.1.cmml" xref="A7.SS1.SSS0.Px2.p2.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="A7.SS1.SSS0.Px2.p2.1.m1.1c">N</annotation></semantics></math> points are input into the decoder as the position prior.</p>
</div>
<div id="A7.SS1.SSS0.Px2.p3" class="ltx_para ltx_noindent">
<p id="A7.SS1.SSS0.Px2.p3.4" class="ltx_p">The Transformer decoder uses learnable queries to retrieve instance-related information from encoded multi-scale visual features. The design of the query is critical for ensuring stability and efficiency in training, based on previous works <cite class="ltx_cite ltx_citemacro_citep">(Meinhardt et al., <a href="#bib.bib37" title="" class="ltx_ref">2022</a>; Wang et al., <a href="#bib.bib50" title="" class="ltx_ref">2021</a>)</cite>. The <math id="A7.SS1.SSS0.Px2.p3.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="A7.SS1.SSS0.Px2.p3.1.m1.1a"><mi id="A7.SS1.SSS0.Px2.p3.1.m1.1.1" xref="A7.SS1.SSS0.Px2.p3.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="A7.SS1.SSS0.Px2.p3.1.m1.1b"><ci id="A7.SS1.SSS0.Px2.p3.1.m1.1.1.cmml" xref="A7.SS1.SSS0.Px2.p3.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="A7.SS1.SSS0.Px2.p3.1.m1.1c">N</annotation></semantics></math> reference points served by encoder act as the position priors of the <math id="A7.SS1.SSS0.Px2.p3.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="A7.SS1.SSS0.Px2.p3.2.m2.1a"><mi id="A7.SS1.SSS0.Px2.p3.2.m2.1.1" xref="A7.SS1.SSS0.Px2.p3.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="A7.SS1.SSS0.Px2.p3.2.m2.1b"><ci id="A7.SS1.SSS0.Px2.p3.2.m2.1.1.cmml" xref="A7.SS1.SSS0.Px2.p3.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="A7.SS1.SSS0.Px2.p3.2.m2.1c">N</annotation></semantics></math> queries. The content part of each query is a static learnable vector. Moreover, following DINO, we add denoising queries to make the decoder’s convergence more stable and faster. Through Deformable attention, <math id="A7.SS1.SSS0.Px2.p3.3.m3.1" class="ltx_Math" alttext="N" display="inline"><semantics id="A7.SS1.SSS0.Px2.p3.3.m3.1a"><mi id="A7.SS1.SSS0.Px2.p3.3.m3.1.1" xref="A7.SS1.SSS0.Px2.p3.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="A7.SS1.SSS0.Px2.p3.3.m3.1b"><ci id="A7.SS1.SSS0.Px2.p3.3.m3.1.1.cmml" xref="A7.SS1.SSS0.Px2.p3.3.m3.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="A7.SS1.SSS0.Px2.p3.3.m3.1c">N</annotation></semantics></math> queries efficiently retrieve instance embedding <math id="A7.SS1.SSS0.Px2.p3.4.m4.1" class="ltx_Math" alttext="F_{i}\in\mathbb{R}^{N\times d}" display="inline"><semantics id="A7.SS1.SSS0.Px2.p3.4.m4.1a"><mrow id="A7.SS1.SSS0.Px2.p3.4.m4.1.1" xref="A7.SS1.SSS0.Px2.p3.4.m4.1.1.cmml"><msub id="A7.SS1.SSS0.Px2.p3.4.m4.1.1.2" xref="A7.SS1.SSS0.Px2.p3.4.m4.1.1.2.cmml"><mi id="A7.SS1.SSS0.Px2.p3.4.m4.1.1.2.2" xref="A7.SS1.SSS0.Px2.p3.4.m4.1.1.2.2.cmml">F</mi><mi id="A7.SS1.SSS0.Px2.p3.4.m4.1.1.2.3" xref="A7.SS1.SSS0.Px2.p3.4.m4.1.1.2.3.cmml">i</mi></msub><mo id="A7.SS1.SSS0.Px2.p3.4.m4.1.1.1" xref="A7.SS1.SSS0.Px2.p3.4.m4.1.1.1.cmml">∈</mo><msup id="A7.SS1.SSS0.Px2.p3.4.m4.1.1.3" xref="A7.SS1.SSS0.Px2.p3.4.m4.1.1.3.cmml"><mi id="A7.SS1.SSS0.Px2.p3.4.m4.1.1.3.2" xref="A7.SS1.SSS0.Px2.p3.4.m4.1.1.3.2.cmml">ℝ</mi><mrow id="A7.SS1.SSS0.Px2.p3.4.m4.1.1.3.3" xref="A7.SS1.SSS0.Px2.p3.4.m4.1.1.3.3.cmml"><mi id="A7.SS1.SSS0.Px2.p3.4.m4.1.1.3.3.2" xref="A7.SS1.SSS0.Px2.p3.4.m4.1.1.3.3.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="A7.SS1.SSS0.Px2.p3.4.m4.1.1.3.3.1" xref="A7.SS1.SSS0.Px2.p3.4.m4.1.1.3.3.1.cmml">×</mo><mi id="A7.SS1.SSS0.Px2.p3.4.m4.1.1.3.3.3" xref="A7.SS1.SSS0.Px2.p3.4.m4.1.1.3.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A7.SS1.SSS0.Px2.p3.4.m4.1b"><apply id="A7.SS1.SSS0.Px2.p3.4.m4.1.1.cmml" xref="A7.SS1.SSS0.Px2.p3.4.m4.1.1"><in id="A7.SS1.SSS0.Px2.p3.4.m4.1.1.1.cmml" xref="A7.SS1.SSS0.Px2.p3.4.m4.1.1.1"></in><apply id="A7.SS1.SSS0.Px2.p3.4.m4.1.1.2.cmml" xref="A7.SS1.SSS0.Px2.p3.4.m4.1.1.2"><csymbol cd="ambiguous" id="A7.SS1.SSS0.Px2.p3.4.m4.1.1.2.1.cmml" xref="A7.SS1.SSS0.Px2.p3.4.m4.1.1.2">subscript</csymbol><ci id="A7.SS1.SSS0.Px2.p3.4.m4.1.1.2.2.cmml" xref="A7.SS1.SSS0.Px2.p3.4.m4.1.1.2.2">𝐹</ci><ci id="A7.SS1.SSS0.Px2.p3.4.m4.1.1.2.3.cmml" xref="A7.SS1.SSS0.Px2.p3.4.m4.1.1.2.3">𝑖</ci></apply><apply id="A7.SS1.SSS0.Px2.p3.4.m4.1.1.3.cmml" xref="A7.SS1.SSS0.Px2.p3.4.m4.1.1.3"><csymbol cd="ambiguous" id="A7.SS1.SSS0.Px2.p3.4.m4.1.1.3.1.cmml" xref="A7.SS1.SSS0.Px2.p3.4.m4.1.1.3">superscript</csymbol><ci id="A7.SS1.SSS0.Px2.p3.4.m4.1.1.3.2.cmml" xref="A7.SS1.SSS0.Px2.p3.4.m4.1.1.3.2">ℝ</ci><apply id="A7.SS1.SSS0.Px2.p3.4.m4.1.1.3.3.cmml" xref="A7.SS1.SSS0.Px2.p3.4.m4.1.1.3.3"><times id="A7.SS1.SSS0.Px2.p3.4.m4.1.1.3.3.1.cmml" xref="A7.SS1.SSS0.Px2.p3.4.m4.1.1.3.3.1"></times><ci id="A7.SS1.SSS0.Px2.p3.4.m4.1.1.3.3.2.cmml" xref="A7.SS1.SSS0.Px2.p3.4.m4.1.1.3.3.2">𝑁</ci><ci id="A7.SS1.SSS0.Px2.p3.4.m4.1.1.3.3.3.cmml" xref="A7.SS1.SSS0.Px2.p3.4.m4.1.1.3.3.3">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.SS1.SSS0.Px2.p3.4.m4.1c">F_{i}\in\mathbb{R}^{N\times d}</annotation></semantics></math> from expression-aware visual features.</p>
</div>
<div id="A7.SS1.SSS0.Px2.p4" class="ltx_para ltx_noindent">
<p id="A7.SS1.SSS0.Px2.p4.5" class="ltx_p">Finally, we need to select the instances referred to by expression from the <math id="A7.SS1.SSS0.Px2.p4.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="A7.SS1.SSS0.Px2.p4.1.m1.1a"><mi id="A7.SS1.SSS0.Px2.p4.1.m1.1.1" xref="A7.SS1.SSS0.Px2.p4.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="A7.SS1.SSS0.Px2.p4.1.m1.1b"><ci id="A7.SS1.SSS0.Px2.p4.1.m1.1.1.cmml" xref="A7.SS1.SSS0.Px2.p4.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="A7.SS1.SSS0.Px2.p4.1.m1.1c">N</annotation></semantics></math> instance proposals. As shown in Fig. <a href="#A7.F13" title="Figure 13 ‣ G.1 Model Architecture ‣ Appendix G Model and Implementation Details ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>, we use global average pooling and MLP to map the visual-aware expression feature <math id="A7.SS1.SSS0.Px2.p4.2.m2.1" class="ltx_Math" alttext="F_{y}^{\prime}" display="inline"><semantics id="A7.SS1.SSS0.Px2.p4.2.m2.1a"><msubsup id="A7.SS1.SSS0.Px2.p4.2.m2.1.1" xref="A7.SS1.SSS0.Px2.p4.2.m2.1.1.cmml"><mi id="A7.SS1.SSS0.Px2.p4.2.m2.1.1.2.2" xref="A7.SS1.SSS0.Px2.p4.2.m2.1.1.2.2.cmml">F</mi><mi id="A7.SS1.SSS0.Px2.p4.2.m2.1.1.2.3" xref="A7.SS1.SSS0.Px2.p4.2.m2.1.1.2.3.cmml">y</mi><mo id="A7.SS1.SSS0.Px2.p4.2.m2.1.1.3" xref="A7.SS1.SSS0.Px2.p4.2.m2.1.1.3.cmml">′</mo></msubsup><annotation-xml encoding="MathML-Content" id="A7.SS1.SSS0.Px2.p4.2.m2.1b"><apply id="A7.SS1.SSS0.Px2.p4.2.m2.1.1.cmml" xref="A7.SS1.SSS0.Px2.p4.2.m2.1.1"><csymbol cd="ambiguous" id="A7.SS1.SSS0.Px2.p4.2.m2.1.1.1.cmml" xref="A7.SS1.SSS0.Px2.p4.2.m2.1.1">superscript</csymbol><apply id="A7.SS1.SSS0.Px2.p4.2.m2.1.1.2.cmml" xref="A7.SS1.SSS0.Px2.p4.2.m2.1.1"><csymbol cd="ambiguous" id="A7.SS1.SSS0.Px2.p4.2.m2.1.1.2.1.cmml" xref="A7.SS1.SSS0.Px2.p4.2.m2.1.1">subscript</csymbol><ci id="A7.SS1.SSS0.Px2.p4.2.m2.1.1.2.2.cmml" xref="A7.SS1.SSS0.Px2.p4.2.m2.1.1.2.2">𝐹</ci><ci id="A7.SS1.SSS0.Px2.p4.2.m2.1.1.2.3.cmml" xref="A7.SS1.SSS0.Px2.p4.2.m2.1.1.2.3">𝑦</ci></apply><ci id="A7.SS1.SSS0.Px2.p4.2.m2.1.1.3.cmml" xref="A7.SS1.SSS0.Px2.p4.2.m2.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.SS1.SSS0.Px2.p4.2.m2.1c">F_{y}^{\prime}</annotation></semantics></math> to the semantic space of the query embedding to obtain <math id="A7.SS1.SSS0.Px2.p4.3.m3.1" class="ltx_Math" alttext="\widetilde{F_{y}}\in\mathbb{R}^{1\times d}" display="inline"><semantics id="A7.SS1.SSS0.Px2.p4.3.m3.1a"><mrow id="A7.SS1.SSS0.Px2.p4.3.m3.1.1" xref="A7.SS1.SSS0.Px2.p4.3.m3.1.1.cmml"><mover accent="true" id="A7.SS1.SSS0.Px2.p4.3.m3.1.1.2" xref="A7.SS1.SSS0.Px2.p4.3.m3.1.1.2.cmml"><msub id="A7.SS1.SSS0.Px2.p4.3.m3.1.1.2.2" xref="A7.SS1.SSS0.Px2.p4.3.m3.1.1.2.2.cmml"><mi id="A7.SS1.SSS0.Px2.p4.3.m3.1.1.2.2.2" xref="A7.SS1.SSS0.Px2.p4.3.m3.1.1.2.2.2.cmml">F</mi><mi id="A7.SS1.SSS0.Px2.p4.3.m3.1.1.2.2.3" xref="A7.SS1.SSS0.Px2.p4.3.m3.1.1.2.2.3.cmml">y</mi></msub><mo id="A7.SS1.SSS0.Px2.p4.3.m3.1.1.2.1" xref="A7.SS1.SSS0.Px2.p4.3.m3.1.1.2.1.cmml">~</mo></mover><mo id="A7.SS1.SSS0.Px2.p4.3.m3.1.1.1" xref="A7.SS1.SSS0.Px2.p4.3.m3.1.1.1.cmml">∈</mo><msup id="A7.SS1.SSS0.Px2.p4.3.m3.1.1.3" xref="A7.SS1.SSS0.Px2.p4.3.m3.1.1.3.cmml"><mi id="A7.SS1.SSS0.Px2.p4.3.m3.1.1.3.2" xref="A7.SS1.SSS0.Px2.p4.3.m3.1.1.3.2.cmml">ℝ</mi><mrow id="A7.SS1.SSS0.Px2.p4.3.m3.1.1.3.3" xref="A7.SS1.SSS0.Px2.p4.3.m3.1.1.3.3.cmml"><mn id="A7.SS1.SSS0.Px2.p4.3.m3.1.1.3.3.2" xref="A7.SS1.SSS0.Px2.p4.3.m3.1.1.3.3.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="A7.SS1.SSS0.Px2.p4.3.m3.1.1.3.3.1" xref="A7.SS1.SSS0.Px2.p4.3.m3.1.1.3.3.1.cmml">×</mo><mi id="A7.SS1.SSS0.Px2.p4.3.m3.1.1.3.3.3" xref="A7.SS1.SSS0.Px2.p4.3.m3.1.1.3.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A7.SS1.SSS0.Px2.p4.3.m3.1b"><apply id="A7.SS1.SSS0.Px2.p4.3.m3.1.1.cmml" xref="A7.SS1.SSS0.Px2.p4.3.m3.1.1"><in id="A7.SS1.SSS0.Px2.p4.3.m3.1.1.1.cmml" xref="A7.SS1.SSS0.Px2.p4.3.m3.1.1.1"></in><apply id="A7.SS1.SSS0.Px2.p4.3.m3.1.1.2.cmml" xref="A7.SS1.SSS0.Px2.p4.3.m3.1.1.2"><ci id="A7.SS1.SSS0.Px2.p4.3.m3.1.1.2.1.cmml" xref="A7.SS1.SSS0.Px2.p4.3.m3.1.1.2.1">~</ci><apply id="A7.SS1.SSS0.Px2.p4.3.m3.1.1.2.2.cmml" xref="A7.SS1.SSS0.Px2.p4.3.m3.1.1.2.2"><csymbol cd="ambiguous" id="A7.SS1.SSS0.Px2.p4.3.m3.1.1.2.2.1.cmml" xref="A7.SS1.SSS0.Px2.p4.3.m3.1.1.2.2">subscript</csymbol><ci id="A7.SS1.SSS0.Px2.p4.3.m3.1.1.2.2.2.cmml" xref="A7.SS1.SSS0.Px2.p4.3.m3.1.1.2.2.2">𝐹</ci><ci id="A7.SS1.SSS0.Px2.p4.3.m3.1.1.2.2.3.cmml" xref="A7.SS1.SSS0.Px2.p4.3.m3.1.1.2.2.3">𝑦</ci></apply></apply><apply id="A7.SS1.SSS0.Px2.p4.3.m3.1.1.3.cmml" xref="A7.SS1.SSS0.Px2.p4.3.m3.1.1.3"><csymbol cd="ambiguous" id="A7.SS1.SSS0.Px2.p4.3.m3.1.1.3.1.cmml" xref="A7.SS1.SSS0.Px2.p4.3.m3.1.1.3">superscript</csymbol><ci id="A7.SS1.SSS0.Px2.p4.3.m3.1.1.3.2.cmml" xref="A7.SS1.SSS0.Px2.p4.3.m3.1.1.3.2">ℝ</ci><apply id="A7.SS1.SSS0.Px2.p4.3.m3.1.1.3.3.cmml" xref="A7.SS1.SSS0.Px2.p4.3.m3.1.1.3.3"><times id="A7.SS1.SSS0.Px2.p4.3.m3.1.1.3.3.1.cmml" xref="A7.SS1.SSS0.Px2.p4.3.m3.1.1.3.3.1"></times><cn type="integer" id="A7.SS1.SSS0.Px2.p4.3.m3.1.1.3.3.2.cmml" xref="A7.SS1.SSS0.Px2.p4.3.m3.1.1.3.3.2">1</cn><ci id="A7.SS1.SSS0.Px2.p4.3.m3.1.1.3.3.3.cmml" xref="A7.SS1.SSS0.Px2.p4.3.m3.1.1.3.3.3">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.SS1.SSS0.Px2.p4.3.m3.1c">\widetilde{F_{y}}\in\mathbb{R}^{1\times d}</annotation></semantics></math>. Thus, the matching score between each query proposal and the expression can be expressed by the cosine similarity between vectors. <math id="A7.SS1.SSS0.Px2.p4.4.m4.1" class="ltx_Math" alttext="S=F_{i}\times\widetilde{F_{y}}^{\top}" display="inline"><semantics id="A7.SS1.SSS0.Px2.p4.4.m4.1a"><mrow id="A7.SS1.SSS0.Px2.p4.4.m4.1.1" xref="A7.SS1.SSS0.Px2.p4.4.m4.1.1.cmml"><mi id="A7.SS1.SSS0.Px2.p4.4.m4.1.1.2" xref="A7.SS1.SSS0.Px2.p4.4.m4.1.1.2.cmml">S</mi><mo id="A7.SS1.SSS0.Px2.p4.4.m4.1.1.1" xref="A7.SS1.SSS0.Px2.p4.4.m4.1.1.1.cmml">=</mo><mrow id="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3" xref="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.cmml"><msub id="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.2" xref="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.2.cmml"><mi id="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.2.2" xref="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.2.2.cmml">F</mi><mi id="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.2.3" xref="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.2.3.cmml">i</mi></msub><mo lspace="0.222em" rspace="0.222em" id="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.1" xref="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.1.cmml">×</mo><msup id="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.3" xref="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.3.cmml"><mover accent="true" id="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.3.2" xref="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.3.2.cmml"><msub id="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.3.2.2" xref="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.3.2.2.cmml"><mi id="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.3.2.2.2" xref="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.3.2.2.2.cmml">F</mi><mi id="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.3.2.2.3" xref="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.3.2.2.3.cmml">y</mi></msub><mo id="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.3.2.1" xref="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.3.2.1.cmml">~</mo></mover><mo id="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.3.3" xref="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.3.3.cmml">⊤</mo></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="A7.SS1.SSS0.Px2.p4.4.m4.1b"><apply id="A7.SS1.SSS0.Px2.p4.4.m4.1.1.cmml" xref="A7.SS1.SSS0.Px2.p4.4.m4.1.1"><eq id="A7.SS1.SSS0.Px2.p4.4.m4.1.1.1.cmml" xref="A7.SS1.SSS0.Px2.p4.4.m4.1.1.1"></eq><ci id="A7.SS1.SSS0.Px2.p4.4.m4.1.1.2.cmml" xref="A7.SS1.SSS0.Px2.p4.4.m4.1.1.2">𝑆</ci><apply id="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.cmml" xref="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3"><times id="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.1.cmml" xref="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.1"></times><apply id="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.2.cmml" xref="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.2"><csymbol cd="ambiguous" id="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.2.1.cmml" xref="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.2">subscript</csymbol><ci id="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.2.2.cmml" xref="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.2.2">𝐹</ci><ci id="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.2.3.cmml" xref="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.2.3">𝑖</ci></apply><apply id="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.3.cmml" xref="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.3"><csymbol cd="ambiguous" id="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.3.1.cmml" xref="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.3">superscript</csymbol><apply id="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.3.2.cmml" xref="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.3.2"><ci id="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.3.2.1.cmml" xref="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.3.2.1">~</ci><apply id="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.3.2.2.cmml" xref="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.3.2.2"><csymbol cd="ambiguous" id="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.3.2.2.1.cmml" xref="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.3.2.2">subscript</csymbol><ci id="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.3.2.2.2.cmml" xref="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.3.2.2.2">𝐹</ci><ci id="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.3.2.2.3.cmml" xref="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.3.2.2.3">𝑦</ci></apply></apply><csymbol cd="latexml" id="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.3.3.cmml" xref="A7.SS1.SSS0.Px2.p4.4.m4.1.1.3.3.3">top</csymbol></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.SS1.SSS0.Px2.p4.4.m4.1c">S=F_{i}\times\widetilde{F_{y}}^{\top}</annotation></semantics></math>. In the inference stage, proposals with scores above threshold <math id="A7.SS1.SSS0.Px2.p4.5.m5.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="A7.SS1.SSS0.Px2.p4.5.m5.1a"><mi id="A7.SS1.SSS0.Px2.p4.5.m5.1.1" xref="A7.SS1.SSS0.Px2.p4.5.m5.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="A7.SS1.SSS0.Px2.p4.5.m5.1b"><ci id="A7.SS1.SSS0.Px2.p4.5.m5.1.1.cmml" xref="A7.SS1.SSS0.Px2.p4.5.m5.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="A7.SS1.SSS0.Px2.p4.5.m5.1c">\theta</annotation></semantics></math> are selected. This flexible selecting strategy allows our architecture to output any number of results that satisfy user requirements. In the training stage, the overall model is supervised by a combination of confidence loss and localization loss.</p>
</div>
</section>
</section>
<section id="A7.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">G.2 </span>Training Configurations</h3>

<section id="A7.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Generation Engine</h4>

<div id="A7.SS2.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="A7.SS2.SSS0.Px1.p1.3" class="ltx_p"><math id="A7.SS2.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="\alpha_{1}" display="inline"><semantics id="A7.SS2.SSS0.Px1.p1.1.m1.1a"><msub id="A7.SS2.SSS0.Px1.p1.1.m1.1.1" xref="A7.SS2.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="A7.SS2.SSS0.Px1.p1.1.m1.1.1.2" xref="A7.SS2.SSS0.Px1.p1.1.m1.1.1.2.cmml">α</mi><mn id="A7.SS2.SSS0.Px1.p1.1.m1.1.1.3" xref="A7.SS2.SSS0.Px1.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A7.SS2.SSS0.Px1.p1.1.m1.1b"><apply id="A7.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="A7.SS2.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A7.SS2.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="A7.SS2.SSS0.Px1.p1.1.m1.1.1">subscript</csymbol><ci id="A7.SS2.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="A7.SS2.SSS0.Px1.p1.1.m1.1.1.2">𝛼</ci><cn type="integer" id="A7.SS2.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="A7.SS2.SSS0.Px1.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.SS2.SSS0.Px1.p1.1.m1.1c">\alpha_{1}</annotation></semantics></math> in the expression filter responsible for balancing referentiality and semantic accuracy is set to 0.5. While generating multi-target expressions, DBSCAN clustering method has two hyperparameters: neighbourhood radius <math id="A7.SS2.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="eps" display="inline"><semantics id="A7.SS2.SSS0.Px1.p1.2.m2.1a"><mrow id="A7.SS2.SSS0.Px1.p1.2.m2.1.1" xref="A7.SS2.SSS0.Px1.p1.2.m2.1.1.cmml"><mi id="A7.SS2.SSS0.Px1.p1.2.m2.1.1.2" xref="A7.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml">e</mi><mo lspace="0em" rspace="0em" id="A7.SS2.SSS0.Px1.p1.2.m2.1.1.1" xref="A7.SS2.SSS0.Px1.p1.2.m2.1.1.1.cmml">​</mo><mi id="A7.SS2.SSS0.Px1.p1.2.m2.1.1.3" xref="A7.SS2.SSS0.Px1.p1.2.m2.1.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="A7.SS2.SSS0.Px1.p1.2.m2.1.1.1a" xref="A7.SS2.SSS0.Px1.p1.2.m2.1.1.1.cmml">​</mo><mi id="A7.SS2.SSS0.Px1.p1.2.m2.1.1.4" xref="A7.SS2.SSS0.Px1.p1.2.m2.1.1.4.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="A7.SS2.SSS0.Px1.p1.2.m2.1b"><apply id="A7.SS2.SSS0.Px1.p1.2.m2.1.1.cmml" xref="A7.SS2.SSS0.Px1.p1.2.m2.1.1"><times id="A7.SS2.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="A7.SS2.SSS0.Px1.p1.2.m2.1.1.1"></times><ci id="A7.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="A7.SS2.SSS0.Px1.p1.2.m2.1.1.2">𝑒</ci><ci id="A7.SS2.SSS0.Px1.p1.2.m2.1.1.3.cmml" xref="A7.SS2.SSS0.Px1.p1.2.m2.1.1.3">𝑝</ci><ci id="A7.SS2.SSS0.Px1.p1.2.m2.1.1.4.cmml" xref="A7.SS2.SSS0.Px1.p1.2.m2.1.1.4">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.SS2.SSS0.Px1.p1.2.m2.1c">eps</annotation></semantics></math> is 1.5 and minimum neighbors of the core point <math id="A7.SS2.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="minPts" display="inline"><semantics id="A7.SS2.SSS0.Px1.p1.3.m3.1a"><mrow id="A7.SS2.SSS0.Px1.p1.3.m3.1.1" xref="A7.SS2.SSS0.Px1.p1.3.m3.1.1.cmml"><mi id="A7.SS2.SSS0.Px1.p1.3.m3.1.1.2" xref="A7.SS2.SSS0.Px1.p1.3.m3.1.1.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="A7.SS2.SSS0.Px1.p1.3.m3.1.1.1" xref="A7.SS2.SSS0.Px1.p1.3.m3.1.1.1.cmml">​</mo><mi id="A7.SS2.SSS0.Px1.p1.3.m3.1.1.3" xref="A7.SS2.SSS0.Px1.p1.3.m3.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="A7.SS2.SSS0.Px1.p1.3.m3.1.1.1a" xref="A7.SS2.SSS0.Px1.p1.3.m3.1.1.1.cmml">​</mo><mi id="A7.SS2.SSS0.Px1.p1.3.m3.1.1.4" xref="A7.SS2.SSS0.Px1.p1.3.m3.1.1.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="A7.SS2.SSS0.Px1.p1.3.m3.1.1.1b" xref="A7.SS2.SSS0.Px1.p1.3.m3.1.1.1.cmml">​</mo><mi id="A7.SS2.SSS0.Px1.p1.3.m3.1.1.5" xref="A7.SS2.SSS0.Px1.p1.3.m3.1.1.5.cmml">P</mi><mo lspace="0em" rspace="0em" id="A7.SS2.SSS0.Px1.p1.3.m3.1.1.1c" xref="A7.SS2.SSS0.Px1.p1.3.m3.1.1.1.cmml">​</mo><mi id="A7.SS2.SSS0.Px1.p1.3.m3.1.1.6" xref="A7.SS2.SSS0.Px1.p1.3.m3.1.1.6.cmml">t</mi><mo lspace="0em" rspace="0em" id="A7.SS2.SSS0.Px1.p1.3.m3.1.1.1d" xref="A7.SS2.SSS0.Px1.p1.3.m3.1.1.1.cmml">​</mo><mi id="A7.SS2.SSS0.Px1.p1.3.m3.1.1.7" xref="A7.SS2.SSS0.Px1.p1.3.m3.1.1.7.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="A7.SS2.SSS0.Px1.p1.3.m3.1b"><apply id="A7.SS2.SSS0.Px1.p1.3.m3.1.1.cmml" xref="A7.SS2.SSS0.Px1.p1.3.m3.1.1"><times id="A7.SS2.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="A7.SS2.SSS0.Px1.p1.3.m3.1.1.1"></times><ci id="A7.SS2.SSS0.Px1.p1.3.m3.1.1.2.cmml" xref="A7.SS2.SSS0.Px1.p1.3.m3.1.1.2">𝑚</ci><ci id="A7.SS2.SSS0.Px1.p1.3.m3.1.1.3.cmml" xref="A7.SS2.SSS0.Px1.p1.3.m3.1.1.3">𝑖</ci><ci id="A7.SS2.SSS0.Px1.p1.3.m3.1.1.4.cmml" xref="A7.SS2.SSS0.Px1.p1.3.m3.1.1.4">𝑛</ci><ci id="A7.SS2.SSS0.Px1.p1.3.m3.1.1.5.cmml" xref="A7.SS2.SSS0.Px1.p1.3.m3.1.1.5">𝑃</ci><ci id="A7.SS2.SSS0.Px1.p1.3.m3.1.1.6.cmml" xref="A7.SS2.SSS0.Px1.p1.3.m3.1.1.6">𝑡</ci><ci id="A7.SS2.SSS0.Px1.p1.3.m3.1.1.7.cmml" xref="A7.SS2.SSS0.Px1.p1.3.m3.1.1.7">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.SS2.SSS0.Px1.p1.3.m3.1c">minPts</annotation></semantics></math> is 2. The temperatures of LLaMA are set to 0.7.</p>
</div>
</section>
<section id="A7.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">DROD Model</h4>

<div id="A7.SS2.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="A7.SS2.SSS0.Px2.p1.1" class="ltx_p">We use ResNet-50 <cite class="ltx_cite ltx_citemacro_citep">(He et al., <a href="#bib.bib15" title="" class="ltx_ref">2016</a>)</cite> and ViT-Huge <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy et al., <a href="#bib.bib9" title="" class="ltx_ref">2021</a>)</cite> as visual encoders and Bert <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a href="#bib.bib6" title="" class="ltx_ref">2018</a>)</cite> as the text encoder. The transformer encoder-decoder architecture consists of a six-layer encoder and a six-layer decoder. The number of object queries <math id="A7.SS2.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="A7.SS2.SSS0.Px2.p1.1.m1.1a"><mi id="A7.SS2.SSS0.Px2.p1.1.m1.1.1" xref="A7.SS2.SSS0.Px2.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="A7.SS2.SSS0.Px2.p1.1.m1.1b"><ci id="A7.SS2.SSS0.Px2.p1.1.m1.1.1.cmml" xref="A7.SS2.SSS0.Px2.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="A7.SS2.SSS0.Px2.p1.1.m1.1c">N</annotation></semantics></math> is set to 900. Our DROD model is initialized by weights pretrained on Objects365 released by UNINEXT <cite class="ltx_cite ltx_citemacro_citep">(Yan et al., <a href="#bib.bib52" title="" class="ltx_ref">2023</a>)</cite>. The optimizer we use is AdamW <cite class="ltx_cite ltx_citemacro_cite">Loshchilov &amp; Hutter (<a href="#bib.bib35" title="" class="ltx_ref">2019</a>)</cite> with a learning rate of 2e-4, a weight decay of 0.05 and the warm-up steps are 400 with an initial learning rate of 4e-5. The model is trained on 32 and 16 V100 GPUs for pretraining and finetuning, respectively.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</section>
<section id="A8" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix H </span>Ablation Studies</h2>

<div id="A8.p1" class="ltx_para ltx_noindent">
<p id="A8.p1.1" class="ltx_p">In this section, we study the necessity of finetuning LLaVA during expression generation from multi-modality pathway. Then, we investigate how different visual prompting strategies affect the CLIP model to maintain our expected expressions.</p>
</div>
<section id="A8.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">LLaVA Finetuning</h4>

<div id="A8.SS0.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="A8.SS0.SSS0.Px1.p1.1" class="ltx_p">In our InstructDET, we do not finetune LLaMA in single-modality pathway, but finetune LLaVA in multi-modality pathway. There are two main reasons illustrated as follows:</p>
</div>
<figure id="A8.F14" class="ltx_figure"><img src="/html/2310.05136/assets/x15.png" id="A8.F14.g1" class="ltx_graphics ltx_centering ltx_img_square" width="338" height="298" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>Comparison of expressions generated by LLaVA before and after our finetuning. Blue words indicate incorrect descriptions.</figcaption>
</figure>
<div id="A8.SS0.SSS0.Px1.p2" class="ltx_para ltx_noindent">
<p id="A8.SS0.SSS0.Px1.p2.1" class="ltx_p">(i) Asking questions directly to LLaVA will get the answers with dense description, rather than the exact expressions we expect for the specific target objects.</p>
</div>
<div id="A8.SS0.SSS0.Px1.p3" class="ltx_para ltx_noindent">
<p id="A8.SS0.SSS0.Px1.p3.1" class="ltx_p">(ii) Due to the limited number of visual tokens output by Q-Former, it is difficult to fully display all the visual features of the entire image. With limited visual information, LLaVA produces lengthy descriptions and thus lead to massive hallucinations (blue words in Fig. <a href="#A8.F14" title="Figure 14 ‣ LLaVA Finetuning ‣ Appendix H Ablation Studies ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>). The generated expressions in this way becomes unstable.</p>
</div>
<div id="A8.SS0.SSS0.Px1.p4" class="ltx_para ltx_noindent">
<p id="A8.SS0.SSS0.Px1.p4.1" class="ltx_p">Based on above analysis, we partially finetune LLaVA on the linear layer that that transforms visual features into the semantic text embedding space. To this end, the model is updated to understand which area we aim to emphasize (i.e., marked in the red box in Fig. <a href="#A8.F14" title="Figure 14 ‣ LLaVA Finetuning ‣ Appendix H Ablation Studies ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>) and which visual features are important for target object description. Fig. <a href="#A8.F14" title="Figure 14 ‣ LLaVA Finetuning ‣ Appendix H Ablation Studies ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a> shows that after finetuning the linear layer using REC datasets, the probabilities of generating our expected high-quality expressions increases from 11.4% to 80.2%.</p>
</div>
<figure id="A8.T12" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 12: </span>Visual Prompting (VP) Evaluation. Different VP methods make differences in the ability of CLIP to retrieve high-quality expressions.</figcaption>
<table id="A8.T12.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A8.T12.2.3.1" class="ltx_tr">
<th id="A8.T12.2.3.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;" rowspan="2"><span id="A8.T12.2.3.1.1.1" class="ltx_text">Strategy</span></th>
<td id="A8.T12.2.3.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;" colspan="2">Visual Prompting</td>
<td id="A8.T12.2.3.1.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;" colspan="2">Expression Retrieval (%)</td>
</tr>
<tr id="A8.T12.2.2" class="ltx_tr">
<td id="A8.T12.2.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">Shape</td>
<td id="A8.T12.2.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.0pt;padding-right:0.0pt;">Tool</td>
<td id="A8.T12.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">Easy <math id="A8.T12.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="A8.T12.1.1.1.m1.1a"><mo stretchy="false" id="A8.T12.1.1.1.m1.1.1" xref="A8.T12.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A8.T12.1.1.1.m1.1b"><ci id="A8.T12.1.1.1.m1.1.1.cmml" xref="A8.T12.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A8.T12.1.1.1.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="A8.T12.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">Hard <math id="A8.T12.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="A8.T12.2.2.2.m1.1a"><mo stretchy="false" id="A8.T12.2.2.2.m1.1.1" xref="A8.T12.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A8.T12.2.2.2.m1.1b"><ci id="A8.T12.2.2.2.m1.1.1.cmml" xref="A8.T12.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A8.T12.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr id="A8.T12.2.4.2" class="ltx_tr">
<th id="A8.T12.2.4.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">1</th>
<td id="A8.T12.2.4.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;" rowspan="5"><span id="A8.T12.2.4.2.2.1" class="ltx_text">Box</span></td>
<td id="A8.T12.2.4.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">Crop</td>
<td id="A8.T12.2.4.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">31.52</td>
<td id="A8.T12.2.4.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">24.09</td>
</tr>
<tr id="A8.T12.2.5.3" class="ltx_tr">
<th id="A8.T12.2.5.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:0.0pt;padding-right:0.0pt;">2</th>
<td id="A8.T12.2.5.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.0pt;padding-right:0.0pt;">Gray</td>
<td id="A8.T12.2.5.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">39.66</td>
<td id="A8.T12.2.5.3.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">31.85</td>
</tr>
<tr id="A8.T12.2.6.4" class="ltx_tr">
<th id="A8.T12.2.6.4.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:0.0pt;padding-right:0.0pt;">3</th>
<td id="A8.T12.2.6.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.0pt;padding-right:0.0pt;">Line</td>
<td id="A8.T12.2.6.4.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">50.12</td>
<td id="A8.T12.2.6.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">42.30</td>
</tr>
<tr id="A8.T12.2.7.5" class="ltx_tr">
<th id="A8.T12.2.7.5.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:0.0pt;padding-right:0.0pt;">4</th>
<td id="A8.T12.2.7.5.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.0pt;padding-right:0.0pt;">Mask</td>
<td id="A8.T12.2.7.5.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">48.29</td>
<td id="A8.T12.2.7.5.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">39.93</td>
</tr>
<tr id="A8.T12.2.8.6" class="ltx_tr">
<th id="A8.T12.2.8.6.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:0.0pt;padding-right:0.0pt;">5</th>
<td id="A8.T12.2.8.6.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.0pt;padding-right:0.0pt;">Blur</td>
<td id="A8.T12.2.8.6.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">52.75</td>
<td id="A8.T12.2.8.6.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">44.30</td>
</tr>
<tr id="A8.T12.2.9.7" class="ltx_tr">
<th id="A8.T12.2.9.7.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">6</th>
<td id="A8.T12.2.9.7.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;" rowspan="3"><span id="A8.T12.2.9.7.2.1" class="ltx_text">Circle</span></td>
<td id="A8.T12.2.9.7.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">Line</td>
<td id="A8.T12.2.9.7.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">52.78</td>
<td id="A8.T12.2.9.7.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">44.33</td>
</tr>
<tr id="A8.T12.2.10.8" class="ltx_tr">
<th id="A8.T12.2.10.8.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:0.0pt;padding-right:0.0pt;">7</th>
<td id="A8.T12.2.10.8.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.0pt;padding-right:0.0pt;">Mask</td>
<td id="A8.T12.2.10.8.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">51.01</td>
<td id="A8.T12.2.10.8.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">42.87</td>
</tr>
<tr id="A8.T12.2.11.9" class="ltx_tr">
<th id="A8.T12.2.11.9.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:0.0pt;padding-right:0.0pt;">8</th>
<td id="A8.T12.2.11.9.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.0pt;padding-right:0.0pt;">Blur</td>
<td id="A8.T12.2.11.9.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">54.13</td>
<td id="A8.T12.2.11.9.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">46.22</td>
</tr>
<tr id="A8.T12.2.12.10" class="ltx_tr">
<th id="A8.T12.2.12.10.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">9</th>
<td id="A8.T12.2.12.10.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;" rowspan="3"><span id="A8.T12.2.12.10.2.1" class="ltx_text">Contour</span></td>
<td id="A8.T12.2.12.10.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">Line</td>
<td id="A8.T12.2.12.10.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">52.34</td>
<td id="A8.T12.2.12.10.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">44.05</td>
</tr>
<tr id="A8.T12.2.13.11" class="ltx_tr">
<th id="A8.T12.2.13.11.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:0.0pt;padding-right:0.0pt;">10</th>
<td id="A8.T12.2.13.11.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.0pt;padding-right:0.0pt;">Mask</td>
<td id="A8.T12.2.13.11.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">51.90</td>
<td id="A8.T12.2.13.11.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">43.44</td>
</tr>
<tr id="A8.T12.2.14.12" class="ltx_tr">
<th id="A8.T12.2.14.12.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:0.0pt;padding-right:0.0pt;">11</th>
<td id="A8.T12.2.14.12.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.0pt;padding-right:0.0pt;">Blur</td>
<td id="A8.T12.2.14.12.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">55.79</td>
<td id="A8.T12.2.14.12.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">47.78</td>
</tr>
<tr id="A8.T12.2.15.13" class="ltx_tr">
<th id="A8.T12.2.15.13.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">12</th>
<td id="A8.T12.2.15.13.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;" colspan="2">VP3 + VP11</td>
<td id="A8.T12.2.15.13.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">56.81</td>
<td id="A8.T12.2.15.13.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">48.97</td>
</tr>
<tr id="A8.T12.2.16.14" class="ltx_tr">
<th id="A8.T12.2.16.14.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:0.0pt;padding-right:0.0pt;">13</th>
<td id="A8.T12.2.16.14.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r" style="padding-left:0.0pt;padding-right:0.0pt;" colspan="2"><span id="A8.T12.2.16.14.2.1" class="ltx_text ltx_font_bold">VP6 + VP11</span></td>
<td id="A8.T12.2.16.14.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="A8.T12.2.16.14.3.1" class="ltx_text ltx_font_bold">58.29</span></td>
<td id="A8.T12.2.16.14.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="A8.T12.2.16.14.4.1" class="ltx_text ltx_font_bold">50.99</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="A8.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Visual Prompting Selection</h4>

<div id="A8.SS0.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="A8.SS0.SSS0.Px2.p1.2" class="ltx_p">The selection of visual prompting (VP) is crucial for expression dropout. We evaluate the affect of various visual prompting methods based on the retrieval performance of CLIP model in Table <a href="#A8.T12" title="Table 12 ‣ LLaVA Finetuning ‣ Appendix H Ablation Studies ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>. First, we define a new metric called expression retrieval ratio, which indicates the proportion of correct expressions that can be retrieved based on the visual-textual matching of CLIP model. During testing, each minibatch contains <math id="A8.SS0.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="A8.SS0.SSS0.Px2.p1.1.m1.1a"><mi id="A8.SS0.SSS0.Px2.p1.1.m1.1.1" xref="A8.SS0.SSS0.Px2.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A8.SS0.SSS0.Px2.p1.1.m1.1b"><ci id="A8.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="A8.SS0.SSS0.Px2.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A8.SS0.SSS0.Px2.p1.1.m1.1c">k</annotation></semantics></math> correct target expressions, and the remaining expressions are negative samples. We take the expressions with the top <math id="A8.SS0.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="A8.SS0.SSS0.Px2.p1.2.m2.1a"><mi id="A8.SS0.SSS0.Px2.p1.2.m2.1.1" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A8.SS0.SSS0.Px2.p1.2.m2.1b"><ci id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.cmml" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A8.SS0.SSS0.Px2.p1.2.m2.1c">k</annotation></semantics></math> Clip score as the retrieved expressions. Finally, the proportion of correct expressions in the retrieved expressions is the expression retrieval ratio. Under the Easy setting, the negative samples in the minibatch come from other images. Under the Hard setting, the negative samples in the minibatch may come from different targets in the same image. Table <a href="#A8.T12" title="Table 12 ‣ LLaVA Finetuning ‣ Appendix H Ablation Studies ‣ InstructDET: Diversifying Referring Object Detection with Generalized Instructions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> shows that cropping (1) and grayscale reversion (2) methods achieve poor results. Because cropping loses all surrounding environment information, and grayscale reversion loses all color features. The best single prompt is the contour blur reversion. The best combination prompt is the contour blur reversion and circle line. Circle line can indicate the rough areas that needs attention, and contour blur reversion can highlight the target object in a fine-grained manner that eliminates background interference. Inevitably, the prior knowledge in the CLIP model is also important to the difference in visual prompt effects. In CLIP’s pre-trained web-scale dataset, images with red circles often indicate that the targets in the red circles are more important and need to be noticed. A large amount of photography also exists in the web-scale dataset, which employs “Bokeh” to blur the background and highlight the subject.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2310.05135" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2310.05136" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2310.05136">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2310.05136" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2310.05137" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 02:10:54 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
