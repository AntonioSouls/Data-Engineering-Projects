<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2110.11636] Occlusion-Robust Object Pose Estimation with Holistic Representation</title><meta property="og:description" content="Practical object pose estimation demands robustness against occlusions to the target object. State-of-the-art (SOTA) object pose estimators take a two-stage approach, where the first stage predicts 2D landmarks using aâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Occlusion-Robust Object Pose Estimation with Holistic Representation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Occlusion-Robust Object Pose Estimation with Holistic Representation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2110.11636">

<!--Generated on Tue Mar 19 15:34:14 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Occlusion-Robust Object Pose Estimation with Holistic Representation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Bo Chen
<br class="ltx_break">The University of Adelaide
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">bo.chen@adelaide.edu.au</span>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tat-Jun Chin
<br class="ltx_break">The University of Adelaide
<br class="ltx_break"><span id="id2.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">tat-jun.chin@adelaide.edu.au</span>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Marius Klimavicius
<br class="ltx_break">Blackswan Technologies
<br class="ltx_break"><span id="id3.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">marius@blackswan.ltd</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id4.id1" class="ltx_p">Practical object pose estimation demands robustness against occlusions to the target object. State-of-the-art (SOTA) object pose estimators take a two-stage approach, where the first stage predicts 2D landmarks using a deep network and the second stage solves for 6DOF pose from 2D-3D correspondences. Albeit widely adopted, such two-stage approaches could suffer from novel occlusions when generalising and weak landmark coherence due to disrupted features. To address these issues, we develop a novel occlude-and-blackout batch augmentation technique to learn occlusion-robust deep features, and a multi-precision supervision architecture to encourage holistic pose representation learning for accurate and coherent landmark predictions. We perform careful ablation tests to verify the impact of our innovations and compare our method to SOTA pose estimators. Without the need of any post-processing or refinement, our method exhibits superior performance on the LINEMOD dataset. On the YCB-Video dataset our method outperforms all non-refinement methods in terms of the ADD(-S) metric. We also demonstrate the high data-efficiency of our method. Our code is available at <a target="_blank" href="http://github.com/BoChenYS/ROPE" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://github.com/BoChenYS/ROPE</a></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure"><img src="/html/2110.11636/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="535" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Qualitative results of our robust object pose estimation on the Occluded-LINEMOD (top and middle) and the YCB-Video (bottom) datasets. <span id="S1.F1.3.1" class="ltx_text ltx_font_bold">Left</span>: prediction of bounding boxes and landmarks of the target object in a test image (zoomed view). <span id="S1.F1.4.2" class="ltx_text ltx_font_bold">Right</span>: prediction of 6DOF poses without post-processing or refinement.</figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Object pose estimation is the task of inferring the relative orientation and position between the target object and the observer. Such inference is crucial in many vision applications such as robotic manipulationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, augmented realityÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, autonomous drivingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib61" title="" class="ltx_ref">61</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite> and spacecraft navigationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>. The problem can be simplified if depth information is availableÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib58" title="" class="ltx_ref">58</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. However, depth sensors are not always practical. Pose estimation from images is thus an important research problem.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In this paper we consider the problem of object pose estimation from a single RGB image. Our focus lies in the base estimator, <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.p2.1.2" class="ltx_text"></span>, from input image to the output pose, before any refinement step. For the base estimator, a number of worksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> adopt direct regression approaches which map the input image that contains the target object to its 6 DOF pose. However, such approaches tend to be sensitive to occlusions and are observed to be similar to performing image retrievalÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Rather than directly regressing the pose, two-stage approachesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> first predict landmarks on the object to establish 2D-3D correspondences, then use a Perspective-n-Point (PnP) like algorithm to solve for the pose. Previous results suggest that two-stage methods are generally more accurateÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. Their strengths derive from training the model with richer supervision signals (<em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.p3.1.2" class="ltx_text"></span>, groundtruth landmarks) rather than just the pose, and injecting tolerance towards inaccurate landmark predictions by robust PnP.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">However, two-stage approaches are not intrinsically immune to occlusion. Current works to improve robustness often take the pixel-wise or patch-wise approachÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, <em id="S1.p4.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.p4.1.2" class="ltx_text"></span>, generating an ensemble of predictions from each image pixel or patch, and aggregate them to obtain a more robust final prediction. Although ensembling can mitigate some occlusion-induced inaccuracies, landmark coherence are easily disrupted by large and novel occlusions, because the network predicts landmarks independently and consistency is only imposed by the PnP algorithm, which is not part of the networkÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In this paper, we aim to address the shortcomings of current two-stage approaches. Firstly, we enforce occlusion-robust feature learning to enable models to deal with novel and severe occlusions. Secondly, a good pose representation should produce landmarks that are consistent to the object shape, rather than predicting individual landmarks independently. To this end, during model training we encourage a holistic pose representation learning in order to strengthen the connections between landmark predictions and enhance their coherence.</p>
</div>
<section id="S1.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Our contributions</h5>

<div id="S1.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S1.SS0.SSS0.Px1.p1.1" class="ltx_p">We propose the Robust Object Pose Estimation (ROPE) framework which achieves excellent robustness against occlusions without the need of pose refinement.
As shown in FigureÂ <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Occlusion-Robust Object Pose Estimation with Holistic Representation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, our model predicts landmarks and pose robustly without any post-processing.</p>
</div>
<div id="S1.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S1.SS0.SSS0.Px1.p2.1" class="ltx_p">To enforce occlusion-robust feature learning, we combine hide-and-seekÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, random erasingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite> and batch augmentationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> and propose a occlude-and-blackout batch augmentation technique for model training. To encourage the model to learn holistic pose representations, we propose a multi-precision supervision architecture, which boosts the modelâ€™s ability to extrapolate occluded object parts, leading to spatially more accurate and structurally more coherent landmark predictions.
To alleviate the need for pose refinement we further utilise the multi-precision supervision architecture to filter landmark predictions with a simple verification step.</p>
</div>
<div id="S1.SS0.SSS0.Px1.p3" class="ltx_para">
<p id="S1.SS0.SSS0.Px1.p3.1" class="ltx_p">We conduct extensive experiments to verify the efficacy of the proposed techniques, and compare our method to SOTA object pose estimators. In terms of the ADD(-S) metirc, our method outperforms all contestants on LINEMODÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> and all non-refinement methods on YCB-VideoÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>. Without any refinement, it is also competitive to SOTA methods that includes a refinement step. Compared to methods that relies on large amount of synthetic training images, we show that ROPE is highly data-efficient.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related works</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Traditional object pose estimation methodsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> rely on hand-crafted features or template matching techniques, which are susceptible to occlusions or other appearance change. Recent advancements of deep learning has nurtured a lot of learning-based methods. We briefly survey a few prominent works from one-stage, two-stage and other methods.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">PoseNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> was a pioneer work on using a deep model to directly regress the 6DOF from an image. Although it was proposed for camera localisation rather than object pose estimation, its principle applies to both tasks. SSD-6DÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> combines an SSD detectorÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> and a pose regressor in a single network. RenderForCNNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> uses an image renderer to synthesize training images as well as groundtruth pose for training a pose regressor.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Compared to one-stage approaches, two-stage methods typically predicts intermediate features in the first stage, and then solve for the pose in the second stage. This mechanism receives more attention because its intermediate feature learning facilitates more potential improvements. For example, Tekin <em id="S2.p3.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p3.1.2" class="ltx_text"></span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> apply the YOLO object detectorÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> in the first stage to predict object landmarks.
Hu <em id="S2.p3.1.3" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p3.1.4" class="ltx_text"></span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> predict landmark locations for each small patch of the input image. They then aggregate all patch predictions to establish 2D-3D correspondences for solving the pose. Oberweger <em id="S2.p3.1.5" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p3.1.6" class="ltx_text"></span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> on the other hand, only use patches of images to train the landmark predictor. The idea is that at least some patches are not corrupted by the occluder and they could produce accurate landmark heatmaps. The ensemble of heatmaps predicted from many patches are combined to obtain final landmarks. PVNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> predicts the object mask and, for each pixel within the mask, unit vectors that points to the landmarks. It then utilises a generalised Hough voting schemeÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> to determine the distribution of the landmarks.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">There are other notable works tackling object pose estimation from different perspectives. Sundermeyer <em id="S2.p4.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p4.1.2" class="ltx_text"></span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> use autoencoders to learn implicit pose representations by reconstructing the input objects. Cai and ReidÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> propose a 3D model-free pose estimator via 2D-3D mapping. To make two-stage methods into a single stage pipeline, Hu <em id="S2.p4.1.3" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p4.1.4" class="ltx_text"></span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> and Wang <em id="S2.p4.1.5" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p4.1.6" class="ltx_text"></span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite> propose deep architectures to replace the PnP algorithm in the second stage, while Chen <em id="S2.p4.1.7" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p4.1.8" class="ltx_text"></span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> propose a differentiable PnP method to achieve end-to-end learning.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2110.11636/assets/figs/hm_arch.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="226" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Illustration of an occlude-and-blackout augmented example and the architecture of our heatmap prediction network. For clarity, the backbone and the RPN are represented in the RoI Align module, other modules in the Mask R-CNN framework such as the box head, as well as relevant losses, are not shown. Our model replaces the original mask head with three keypoint heads.</figcaption>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>The ROPE framework</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.5" class="ltx_p">We focus on the problem of 6DOF object pose estimation from a single RGB image. Given an image <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.p1.1.m1.1a"><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">ğ¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">I</annotation></semantics></math> and a known 3D point cloud <math id="S3.p1.2.m2.1" class="ltx_Math" alttext="\{\bm{z}_{i}\}^{n}_{i=1}" display="inline"><semantics id="S3.p1.2.m2.1a"><msubsup id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml"><mrow id="S3.p1.2.m2.1.1.1.1.1" xref="S3.p1.2.m2.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.p1.2.m2.1.1.1.1.1.2" xref="S3.p1.2.m2.1.1.1.1.2.cmml">{</mo><msub id="S3.p1.2.m2.1.1.1.1.1.1" xref="S3.p1.2.m2.1.1.1.1.1.1.cmml"><mi id="S3.p1.2.m2.1.1.1.1.1.1.2" xref="S3.p1.2.m2.1.1.1.1.1.1.2.cmml">ğ’›</mi><mi id="S3.p1.2.m2.1.1.1.1.1.1.3" xref="S3.p1.2.m2.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.p1.2.m2.1.1.1.1.1.3" xref="S3.p1.2.m2.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.p1.2.m2.1.1.3" xref="S3.p1.2.m2.1.1.3.cmml"><mi id="S3.p1.2.m2.1.1.3.2" xref="S3.p1.2.m2.1.1.3.2.cmml">i</mi><mo id="S3.p1.2.m2.1.1.3.1" xref="S3.p1.2.m2.1.1.3.1.cmml">=</mo><mn id="S3.p1.2.m2.1.1.3.3" xref="S3.p1.2.m2.1.1.3.3.cmml">1</mn></mrow><mi id="S3.p1.2.m2.1.1.1.3" xref="S3.p1.2.m2.1.1.1.3.cmml">n</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><apply id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p1.2.m2.1.1.2.cmml" xref="S3.p1.2.m2.1.1">subscript</csymbol><apply id="S3.p1.2.m2.1.1.1.cmml" xref="S3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p1.2.m2.1.1.1.2.cmml" xref="S3.p1.2.m2.1.1">superscript</csymbol><set id="S3.p1.2.m2.1.1.1.1.2.cmml" xref="S3.p1.2.m2.1.1.1.1.1"><apply id="S3.p1.2.m2.1.1.1.1.1.1.cmml" xref="S3.p1.2.m2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.p1.2.m2.1.1.1.1.1.1.1.cmml" xref="S3.p1.2.m2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.p1.2.m2.1.1.1.1.1.1.2.cmml" xref="S3.p1.2.m2.1.1.1.1.1.1.2">ğ’›</ci><ci id="S3.p1.2.m2.1.1.1.1.1.1.3.cmml" xref="S3.p1.2.m2.1.1.1.1.1.1.3">ğ‘–</ci></apply></set><ci id="S3.p1.2.m2.1.1.1.3.cmml" xref="S3.p1.2.m2.1.1.1.3">ğ‘›</ci></apply><apply id="S3.p1.2.m2.1.1.3.cmml" xref="S3.p1.2.m2.1.1.3"><eq id="S3.p1.2.m2.1.1.3.1.cmml" xref="S3.p1.2.m2.1.1.3.1"></eq><ci id="S3.p1.2.m2.1.1.3.2.cmml" xref="S3.p1.2.m2.1.1.3.2">ğ‘–</ci><cn type="integer" id="S3.p1.2.m2.1.1.3.3.cmml" xref="S3.p1.2.m2.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">\{\bm{z}_{i}\}^{n}_{i=1}</annotation></semantics></math> of the target object, we first predict a set of 2D landmarks <math id="S3.p1.3.m3.1" class="ltx_Math" alttext="\{\bm{x}_{i}\}^{n}_{i=1}" display="inline"><semantics id="S3.p1.3.m3.1a"><msubsup id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml"><mrow id="S3.p1.3.m3.1.1.1.1.1" xref="S3.p1.3.m3.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.p1.3.m3.1.1.1.1.1.2" xref="S3.p1.3.m3.1.1.1.1.2.cmml">{</mo><msub id="S3.p1.3.m3.1.1.1.1.1.1" xref="S3.p1.3.m3.1.1.1.1.1.1.cmml"><mi id="S3.p1.3.m3.1.1.1.1.1.1.2" xref="S3.p1.3.m3.1.1.1.1.1.1.2.cmml">ğ’™</mi><mi id="S3.p1.3.m3.1.1.1.1.1.1.3" xref="S3.p1.3.m3.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.p1.3.m3.1.1.1.1.1.3" xref="S3.p1.3.m3.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.p1.3.m3.1.1.3" xref="S3.p1.3.m3.1.1.3.cmml"><mi id="S3.p1.3.m3.1.1.3.2" xref="S3.p1.3.m3.1.1.3.2.cmml">i</mi><mo id="S3.p1.3.m3.1.1.3.1" xref="S3.p1.3.m3.1.1.3.1.cmml">=</mo><mn id="S3.p1.3.m3.1.1.3.3" xref="S3.p1.3.m3.1.1.3.3.cmml">1</mn></mrow><mi id="S3.p1.3.m3.1.1.1.3" xref="S3.p1.3.m3.1.1.1.3.cmml">n</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><apply id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p1.3.m3.1.1.2.cmml" xref="S3.p1.3.m3.1.1">subscript</csymbol><apply id="S3.p1.3.m3.1.1.1.cmml" xref="S3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p1.3.m3.1.1.1.2.cmml" xref="S3.p1.3.m3.1.1">superscript</csymbol><set id="S3.p1.3.m3.1.1.1.1.2.cmml" xref="S3.p1.3.m3.1.1.1.1.1"><apply id="S3.p1.3.m3.1.1.1.1.1.1.cmml" xref="S3.p1.3.m3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.p1.3.m3.1.1.1.1.1.1.1.cmml" xref="S3.p1.3.m3.1.1.1.1.1.1">subscript</csymbol><ci id="S3.p1.3.m3.1.1.1.1.1.1.2.cmml" xref="S3.p1.3.m3.1.1.1.1.1.1.2">ğ’™</ci><ci id="S3.p1.3.m3.1.1.1.1.1.1.3.cmml" xref="S3.p1.3.m3.1.1.1.1.1.1.3">ğ‘–</ci></apply></set><ci id="S3.p1.3.m3.1.1.1.3.cmml" xref="S3.p1.3.m3.1.1.1.3">ğ‘›</ci></apply><apply id="S3.p1.3.m3.1.1.3.cmml" xref="S3.p1.3.m3.1.1.3"><eq id="S3.p1.3.m3.1.1.3.1.cmml" xref="S3.p1.3.m3.1.1.3.1"></eq><ci id="S3.p1.3.m3.1.1.3.2.cmml" xref="S3.p1.3.m3.1.1.3.2">ğ‘–</ci><cn type="integer" id="S3.p1.3.m3.1.1.3.3.cmml" xref="S3.p1.3.m3.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">\{\bm{x}_{i}\}^{n}_{i=1}</annotation></semantics></math> in <math id="S3.p1.4.m4.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.p1.4.m4.1a"><mi id="S3.p1.4.m4.1.1" xref="S3.p1.4.m4.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.1b"><ci id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1">ğ¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.1c">I</annotation></semantics></math> that correspond to the point cloud, then solve the pose <math id="S3.p1.5.m5.1" class="ltx_Math" alttext="\bm{y}" display="inline"><semantics id="S3.p1.5.m5.1a"><mi id="S3.p1.5.m5.1.1" xref="S3.p1.5.m5.1.1.cmml">ğ’š</mi><annotation-xml encoding="MathML-Content" id="S3.p1.5.m5.1b"><ci id="S3.p1.5.m5.1.1.cmml" xref="S3.p1.5.m5.1.1">ğ’š</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.5.m5.1c">\bm{y}</annotation></semantics></math> via a RANSAC-based PnP solver from filtered 2D-3D correspondences.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Robust landmark prediction</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Our 2D landmark prediction is based on the Mask R-CNNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> framework. The specific architecture and training scheme are shown in FigureÂ <a href="#S2.F2" title="Figure 2 â€£ 2 Related works â€£ Occlusion-Robust Object Pose Estimation with Holistic Representation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. A basic improvement is substituting the original backbone network with HRNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite> to exploit its high-resolution feature maps which preserve rich semantic information and increase spatial accuracy. Next, we describe two key innovations to boost occlusion robustness and landmark coherence.</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Occlude-and-blackout batch augmentation</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">Fundamentally, pose estimation for the typical 3D object will suffer from the problem of self-occlusion. Landmarks that are at the opposite side of the object would be hard to predict since their visual features are hidden. In fact, a practical pose estimator must also contend with additional occlusions due to, <em id="S3.SS1.SSS1.p1.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.SS1.SSS1.p1.1.2" class="ltx_text"></span>, other objects or scene elements that further conceal part of the target object from view. It is thus important that the landmark predictor infers the robust pose information from potentially different kinds occlusions imposed on the object.</p>
</div>
<div id="S3.SS1.SSS1.p2" class="ltx_para">
<p id="S3.SS1.SSS1.p2.1" class="ltx_p">Inspired by the ideas of random erasingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite>, hide-and-seekÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, and batch augmentationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> (all not originally developed for pose estimation), we develop a novel Occlude-and-blackout Batch Augmentation (OBA) to promote robust landmark prediction under occlusion. For each training batch, after performing regular data augmentations including rotation, translation, scaling and color jitter, we extend the batch by including a copy of itself with extra augmentations, namely, occlude and blackout. Similar to hide-and-seek, we divide the image region enveloped by the object bounding box into a grid of patches and replace each patch, under certain probability, with either noise or a random patch elsewhere from the same image. We then blackout everything outside of the object bounding box. An example is shown in FigureÂ <a href="#S2.F2" title="Figure 2 â€£ 2 Related works â€£ Occlusion-Robust Object Pose Estimation with Holistic Representation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S3.SS1.SSS1.p3" class="ltx_para">
<p id="S3.SS1.SSS1.p3.1" class="ltx_p">With random occlusions the network is forced to infer the pose information from a partial view of the object. Erasing the background helps reducing overfitting and enhance generalisability. Moreover, the OBA augmented images are fed to the network with the original ones in the same batch, and supervised by the same groundtruth labels. This encourages the network to learn occlusion-robust and background-invariant representations.</p>
</div>
<div id="S3.SS1.SSS1.p4" class="ltx_para">
<p id="S3.SS1.SSS1.p4.1" class="ltx_p">If the potential occluders are known beforehand, injecting occluder specific information in the training phase can significantly improve performanceÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>. However this knowledge is often not available in practice. Compared to methods that augment training images with known objectsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, our method is occluder-agnostic yet it generalises well in the testing sets.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Multi-precision supervision</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">Current heatmap-based landmark prediction networks use a single groundtruth Gaussian heatmap per landmark for training. The variance of these heatmaps is a hyper parameter which requires careful tuning: a smaller variance may increase prediction accuracy for each individual landmark however risk structural inconsistency in the case of occlusion, due to the lack of holistic understanding of the object pose. To address this issue we propose a Multi-Precision Supervision (MPS) architecture: using three keypoint heads to predict groundtruth Gaussian heatmaps with different variance.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2110.11636/assets/figs/sps_vs_mps.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="460" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Conceptual illustration of holistic representation learning via MPS. Note the difference on the information learned by the feature section <math id="S3.F3.2.m1.1" class="ltx_Math" alttext="S1" display="inline"><semantics id="S3.F3.2.m1.1b"><mrow id="S3.F3.2.m1.1.1" xref="S3.F3.2.m1.1.1.cmml"><mi id="S3.F3.2.m1.1.1.2" xref="S3.F3.2.m1.1.1.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.F3.2.m1.1.1.1" xref="S3.F3.2.m1.1.1.1.cmml">â€‹</mo><mn id="S3.F3.2.m1.1.1.3" xref="S3.F3.2.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.F3.2.m1.1c"><apply id="S3.F3.2.m1.1.1.cmml" xref="S3.F3.2.m1.1.1"><times id="S3.F3.2.m1.1.1.1.cmml" xref="S3.F3.2.m1.1.1.1"></times><ci id="S3.F3.2.m1.1.1.2.cmml" xref="S3.F3.2.m1.1.1.2">ğ‘†</ci><cn type="integer" id="S3.F3.2.m1.1.1.3.cmml" xref="S3.F3.2.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.2.m1.1d">S1</annotation></semantics></math>.</figcaption>
</figure>
<div id="S3.SS1.SSS2.p2" class="ltx_para">
<p id="S3.SS1.SSS2.p2.1" class="ltx_p">In Mask R-CNN, the output feature map of the backbone is aligned with RoI proposals and the RoI features are then passed to the mask head. We replace the mask head with three keypoint heads to regress the landmark heatmaps, as shown in FigureÂ <a href="#S2.F2" title="Figure 2 â€£ 2 Related works â€£ Occlusion-Robust Object Pose Estimation with Holistic Representation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Each keypoint head consists of 8 convolutional layers and 2 upsampling layers.</p>
</div>
<div id="S3.SS1.SSS2.p3" class="ltx_para">
<p id="S3.SS1.SSS2.p3.5" class="ltx_p">In the training phase, the groundtruth heatmaps <math id="S3.SS1.SSS2.p3.1.m1.1" class="ltx_Math" alttext="\Phi^{*}" display="inline"><semantics id="S3.SS1.SSS2.p3.1.m1.1a"><msup id="S3.SS1.SSS2.p3.1.m1.1.1" xref="S3.SS1.SSS2.p3.1.m1.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.SSS2.p3.1.m1.1.1.2" xref="S3.SS1.SSS2.p3.1.m1.1.1.2.cmml">Î¦</mi><mo id="S3.SS1.SSS2.p3.1.m1.1.1.3" xref="S3.SS1.SSS2.p3.1.m1.1.1.3.cmml">âˆ—</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p3.1.m1.1b"><apply id="S3.SS1.SSS2.p3.1.m1.1.1.cmml" xref="S3.SS1.SSS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p3.1.m1.1.1.1.cmml" xref="S3.SS1.SSS2.p3.1.m1.1.1">superscript</csymbol><ci id="S3.SS1.SSS2.p3.1.m1.1.1.2.cmml" xref="S3.SS1.SSS2.p3.1.m1.1.1.2">Î¦</ci><times id="S3.SS1.SSS2.p3.1.m1.1.1.3.cmml" xref="S3.SS1.SSS2.p3.1.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p3.1.m1.1c">\Phi^{*}</annotation></semantics></math> are constructed as 2D Gaussian feature maps centred on groundtruth 2D landmarks <math id="S3.SS1.SSS2.p3.2.m2.1" class="ltx_Math" alttext="\bm{x}^{*}" display="inline"><semantics id="S3.SS1.SSS2.p3.2.m2.1a"><msup id="S3.SS1.SSS2.p3.2.m2.1.1" xref="S3.SS1.SSS2.p3.2.m2.1.1.cmml"><mi id="S3.SS1.SSS2.p3.2.m2.1.1.2" xref="S3.SS1.SSS2.p3.2.m2.1.1.2.cmml">ğ’™</mi><mo id="S3.SS1.SSS2.p3.2.m2.1.1.3" xref="S3.SS1.SSS2.p3.2.m2.1.1.3.cmml">âˆ—</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p3.2.m2.1b"><apply id="S3.SS1.SSS2.p3.2.m2.1.1.cmml" xref="S3.SS1.SSS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p3.2.m2.1.1.1.cmml" xref="S3.SS1.SSS2.p3.2.m2.1.1">superscript</csymbol><ci id="S3.SS1.SSS2.p3.2.m2.1.1.2.cmml" xref="S3.SS1.SSS2.p3.2.m2.1.1.2">ğ’™</ci><times id="S3.SS1.SSS2.p3.2.m2.1.1.3.cmml" xref="S3.SS1.SSS2.p3.2.m2.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p3.2.m2.1c">\bm{x}^{*}</annotation></semantics></math> and spreading with variance <math id="S3.SS1.SSS2.p3.3.m3.1" class="ltx_Math" alttext="\sigma^{2}" display="inline"><semantics id="S3.SS1.SSS2.p3.3.m3.1a"><msup id="S3.SS1.SSS2.p3.3.m3.1.1" xref="S3.SS1.SSS2.p3.3.m3.1.1.cmml"><mi id="S3.SS1.SSS2.p3.3.m3.1.1.2" xref="S3.SS1.SSS2.p3.3.m3.1.1.2.cmml">Ïƒ</mi><mn id="S3.SS1.SSS2.p3.3.m3.1.1.3" xref="S3.SS1.SSS2.p3.3.m3.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p3.3.m3.1b"><apply id="S3.SS1.SSS2.p3.3.m3.1.1.cmml" xref="S3.SS1.SSS2.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p3.3.m3.1.1.1.cmml" xref="S3.SS1.SSS2.p3.3.m3.1.1">superscript</csymbol><ci id="S3.SS1.SSS2.p3.3.m3.1.1.2.cmml" xref="S3.SS1.SSS2.p3.3.m3.1.1.2">ğœ</ci><cn type="integer" id="S3.SS1.SSS2.p3.3.m3.1.1.3.cmml" xref="S3.SS1.SSS2.p3.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p3.3.m3.1c">\sigma^{2}</annotation></semantics></math>. We use <math id="S3.SS1.SSS2.p3.4.m4.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S3.SS1.SSS2.p3.4.m4.1a"><mi id="S3.SS1.SSS2.p3.4.m4.1.1" xref="S3.SS1.SSS2.p3.4.m4.1.1.cmml">Ïƒ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p3.4.m4.1b"><ci id="S3.SS1.SSS2.p3.4.m4.1.1.cmml" xref="S3.SS1.SSS2.p3.4.m4.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p3.4.m4.1c">\sigma</annotation></semantics></math> equal to 8, 3 and 1.5 pixels respectively for the three keypoint heads, thus creating low, medium and high precision target heatmaps <math id="S3.SS1.SSS2.p3.5.m5.1" class="ltx_Math" alttext="\Phi^{*}" display="inline"><semantics id="S3.SS1.SSS2.p3.5.m5.1a"><msup id="S3.SS1.SSS2.p3.5.m5.1.1" xref="S3.SS1.SSS2.p3.5.m5.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.SSS2.p3.5.m5.1.1.2" xref="S3.SS1.SSS2.p3.5.m5.1.1.2.cmml">Î¦</mi><mo id="S3.SS1.SSS2.p3.5.m5.1.1.3" xref="S3.SS1.SSS2.p3.5.m5.1.1.3.cmml">âˆ—</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p3.5.m5.1b"><apply id="S3.SS1.SSS2.p3.5.m5.1.1.cmml" xref="S3.SS1.SSS2.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p3.5.m5.1.1.1.cmml" xref="S3.SS1.SSS2.p3.5.m5.1.1">superscript</csymbol><ci id="S3.SS1.SSS2.p3.5.m5.1.1.2.cmml" xref="S3.SS1.SSS2.p3.5.m5.1.1.2">Î¦</ci><times id="S3.SS1.SSS2.p3.5.m5.1.1.3.cmml" xref="S3.SS1.SSS2.p3.5.m5.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p3.5.m5.1c">\Phi^{*}</annotation></semantics></math>. The loss function is</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.2" class="ltx_Math" alttext="L_{JS}=\text{JSD}(\phi(\Phi),\Phi^{*})," display="block"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.2.1" xref="S3.E1.m1.2.2.1.1.cmml"><mrow id="S3.E1.m1.2.2.1.1" xref="S3.E1.m1.2.2.1.1.cmml"><msub id="S3.E1.m1.2.2.1.1.4" xref="S3.E1.m1.2.2.1.1.4.cmml"><mi id="S3.E1.m1.2.2.1.1.4.2" xref="S3.E1.m1.2.2.1.1.4.2.cmml">L</mi><mrow id="S3.E1.m1.2.2.1.1.4.3" xref="S3.E1.m1.2.2.1.1.4.3.cmml"><mi id="S3.E1.m1.2.2.1.1.4.3.2" xref="S3.E1.m1.2.2.1.1.4.3.2.cmml">J</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.1.1.4.3.1" xref="S3.E1.m1.2.2.1.1.4.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.2.2.1.1.4.3.3" xref="S3.E1.m1.2.2.1.1.4.3.3.cmml">S</mi></mrow></msub><mo id="S3.E1.m1.2.2.1.1.3" xref="S3.E1.m1.2.2.1.1.3.cmml">=</mo><mrow id="S3.E1.m1.2.2.1.1.2" xref="S3.E1.m1.2.2.1.1.2.cmml"><mtext id="S3.E1.m1.2.2.1.1.2.4" xref="S3.E1.m1.2.2.1.1.2.4a.cmml">JSD</mtext><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.1.1.2.3" xref="S3.E1.m1.2.2.1.1.2.3.cmml">â€‹</mo><mrow id="S3.E1.m1.2.2.1.1.2.2.2" xref="S3.E1.m1.2.2.1.1.2.2.3.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.1.1.2.2.2.3" xref="S3.E1.m1.2.2.1.1.2.2.3.cmml">(</mo><mrow id="S3.E1.m1.2.2.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.2.cmml">Ï•</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.1.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml">â€‹</mo><mrow id="S3.E1.m1.2.2.1.1.1.1.1.1.3.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.1.1.1.3.2.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.cmml">(</mo><mi mathvariant="normal" id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">Î¦</mi><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.1.1.1.3.2.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.2.2.1.1.2.2.2.4" xref="S3.E1.m1.2.2.1.1.2.2.3.cmml">,</mo><msup id="S3.E1.m1.2.2.1.1.2.2.2.2" xref="S3.E1.m1.2.2.1.1.2.2.2.2.cmml"><mi mathvariant="normal" id="S3.E1.m1.2.2.1.1.2.2.2.2.2" xref="S3.E1.m1.2.2.1.1.2.2.2.2.2.cmml">Î¦</mi><mo id="S3.E1.m1.2.2.1.1.2.2.2.2.3" xref="S3.E1.m1.2.2.1.1.2.2.2.2.3.cmml">âˆ—</mo></msup><mo stretchy="false" id="S3.E1.m1.2.2.1.1.2.2.2.5" xref="S3.E1.m1.2.2.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.2.2.1.2" xref="S3.E1.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1"><eq id="S3.E1.m1.2.2.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.3"></eq><apply id="S3.E1.m1.2.2.1.1.4.cmml" xref="S3.E1.m1.2.2.1.1.4"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.4.1.cmml" xref="S3.E1.m1.2.2.1.1.4">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.4.2.cmml" xref="S3.E1.m1.2.2.1.1.4.2">ğ¿</ci><apply id="S3.E1.m1.2.2.1.1.4.3.cmml" xref="S3.E1.m1.2.2.1.1.4.3"><times id="S3.E1.m1.2.2.1.1.4.3.1.cmml" xref="S3.E1.m1.2.2.1.1.4.3.1"></times><ci id="S3.E1.m1.2.2.1.1.4.3.2.cmml" xref="S3.E1.m1.2.2.1.1.4.3.2">ğ½</ci><ci id="S3.E1.m1.2.2.1.1.4.3.3.cmml" xref="S3.E1.m1.2.2.1.1.4.3.3">ğ‘†</ci></apply></apply><apply id="S3.E1.m1.2.2.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.2"><times id="S3.E1.m1.2.2.1.1.2.3.cmml" xref="S3.E1.m1.2.2.1.1.2.3"></times><ci id="S3.E1.m1.2.2.1.1.2.4a.cmml" xref="S3.E1.m1.2.2.1.1.2.4"><mtext id="S3.E1.m1.2.2.1.1.2.4.cmml" xref="S3.E1.m1.2.2.1.1.2.4">JSD</mtext></ci><interval closure="open" id="S3.E1.m1.2.2.1.1.2.2.3.cmml" xref="S3.E1.m1.2.2.1.1.2.2.2"><apply id="S3.E1.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1"><times id="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1"></times><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.2">italic-Ï•</ci><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">Î¦</ci></apply><apply id="S3.E1.m1.2.2.1.1.2.2.2.2.cmml" xref="S3.E1.m1.2.2.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.2.2.2.2.1.cmml" xref="S3.E1.m1.2.2.1.1.2.2.2.2">superscript</csymbol><ci id="S3.E1.m1.2.2.1.1.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.1.1.2.2.2.2.2">Î¦</ci><times id="S3.E1.m1.2.2.1.1.2.2.2.2.3.cmml" xref="S3.E1.m1.2.2.1.1.2.2.2.2.3"></times></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">L_{JS}=\text{JSD}(\phi(\Phi),\Phi^{*}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.SSS2.p3.7" class="ltx_p">where JSD<math id="S3.SS1.SSS2.p3.6.m1.1" class="ltx_Math" alttext="(\cdot)" display="inline"><semantics id="S3.SS1.SSS2.p3.6.m1.1a"><mrow id="S3.SS1.SSS2.p3.6.m1.1.2.2"><mo stretchy="false" id="S3.SS1.SSS2.p3.6.m1.1.2.2.1">(</mo><mo lspace="0em" rspace="0em" id="S3.SS1.SSS2.p3.6.m1.1.1" xref="S3.SS1.SSS2.p3.6.m1.1.1.cmml">â‹…</mo><mo stretchy="false" id="S3.SS1.SSS2.p3.6.m1.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p3.6.m1.1b"><ci id="S3.SS1.SSS2.p3.6.m1.1.1.cmml" xref="S3.SS1.SSS2.p3.6.m1.1.1">â‹…</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p3.6.m1.1c">(\cdot)</annotation></semantics></math> is the Jensenâ€“Shannon divergenceÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and <math id="S3.SS1.SSS2.p3.7.m2.1" class="ltx_Math" alttext="\phi(\cdot)" display="inline"><semantics id="S3.SS1.SSS2.p3.7.m2.1a"><mrow id="S3.SS1.SSS2.p3.7.m2.1.2" xref="S3.SS1.SSS2.p3.7.m2.1.2.cmml"><mi id="S3.SS1.SSS2.p3.7.m2.1.2.2" xref="S3.SS1.SSS2.p3.7.m2.1.2.2.cmml">Ï•</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS2.p3.7.m2.1.2.1" xref="S3.SS1.SSS2.p3.7.m2.1.2.1.cmml">â€‹</mo><mrow id="S3.SS1.SSS2.p3.7.m2.1.2.3.2" xref="S3.SS1.SSS2.p3.7.m2.1.2.cmml"><mo stretchy="false" id="S3.SS1.SSS2.p3.7.m2.1.2.3.2.1" xref="S3.SS1.SSS2.p3.7.m2.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS1.SSS2.p3.7.m2.1.1" xref="S3.SS1.SSS2.p3.7.m2.1.1.cmml">â‹…</mo><mo stretchy="false" id="S3.SS1.SSS2.p3.7.m2.1.2.3.2.2" xref="S3.SS1.SSS2.p3.7.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p3.7.m2.1b"><apply id="S3.SS1.SSS2.p3.7.m2.1.2.cmml" xref="S3.SS1.SSS2.p3.7.m2.1.2"><times id="S3.SS1.SSS2.p3.7.m2.1.2.1.cmml" xref="S3.SS1.SSS2.p3.7.m2.1.2.1"></times><ci id="S3.SS1.SSS2.p3.7.m2.1.2.2.cmml" xref="S3.SS1.SSS2.p3.7.m2.1.2.2">italic-Ï•</ci><ci id="S3.SS1.SSS2.p3.7.m2.1.1.cmml" xref="S3.SS1.SSS2.p3.7.m2.1.1">â‹…</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p3.7.m2.1c">\phi(\cdot)</annotation></semantics></math> is the channel-wise softmax function, <em id="S3.SS1.SSS2.p3.7.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS1.SSS2.p3.7.2" class="ltx_text"></span>, each channel is normalised to be a probability distribution over the pixels.</p>
</div>
<div id="S3.SS1.SSS2.p4" class="ltx_para">
<p id="S3.SS1.SSS2.p4.6" class="ltx_p">In the testing phase, we only use the predicted heatmaps <math id="S3.SS1.SSS2.p4.1.m1.1" class="ltx_Math" alttext="\Phi" display="inline"><semantics id="S3.SS1.SSS2.p4.1.m1.1a"><mi mathvariant="normal" id="S3.SS1.SSS2.p4.1.m1.1.1" xref="S3.SS1.SSS2.p4.1.m1.1.1.cmml">Î¦</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p4.1.m1.1b"><ci id="S3.SS1.SSS2.p4.1.m1.1.1.cmml" xref="S3.SS1.SSS2.p4.1.m1.1.1">Î¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p4.1.m1.1c">\Phi</annotation></semantics></math> from the high-precision keypoint head to obtain the landmark coordinates <math id="S3.SS1.SSS2.p4.2.m2.1" class="ltx_Math" alttext="\bm{x}" display="inline"><semantics id="S3.SS1.SSS2.p4.2.m2.1a"><mi id="S3.SS1.SSS2.p4.2.m2.1.1" xref="S3.SS1.SSS2.p4.2.m2.1.1.cmml">ğ’™</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p4.2.m2.1b"><ci id="S3.SS1.SSS2.p4.2.m2.1.1.cmml" xref="S3.SS1.SSS2.p4.2.m2.1.1">ğ’™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p4.2.m2.1c">\bm{x}</annotation></semantics></math>. Instead of simply taking the â€œargmaxâ€ of <math id="S3.SS1.SSS2.p4.3.m3.1" class="ltx_Math" alttext="\Phi" display="inline"><semantics id="S3.SS1.SSS2.p4.3.m3.1a"><mi mathvariant="normal" id="S3.SS1.SSS2.p4.3.m3.1.1" xref="S3.SS1.SSS2.p4.3.m3.1.1.cmml">Î¦</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p4.3.m3.1b"><ci id="S3.SS1.SSS2.p4.3.m3.1.1.cmml" xref="S3.SS1.SSS2.p4.3.m3.1.1">Î¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p4.3.m3.1c">\Phi</annotation></semantics></math> as <math id="S3.SS1.SSS2.p4.4.m4.1" class="ltx_Math" alttext="\bm{x}" display="inline"><semantics id="S3.SS1.SSS2.p4.4.m4.1a"><mi id="S3.SS1.SSS2.p4.4.m4.1.1" xref="S3.SS1.SSS2.p4.4.m4.1.1.cmml">ğ’™</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p4.4.m4.1b"><ci id="S3.SS1.SSS2.p4.4.m4.1.1.cmml" xref="S3.SS1.SSS2.p4.4.m4.1.1">ğ’™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p4.4.m4.1c">\bm{x}</annotation></semantics></math>, we treat the normalised heatmaps <math id="S3.SS1.SSS2.p4.5.m5.1" class="ltx_Math" alttext="\phi(\Phi)" display="inline"><semantics id="S3.SS1.SSS2.p4.5.m5.1a"><mrow id="S3.SS1.SSS2.p4.5.m5.1.2" xref="S3.SS1.SSS2.p4.5.m5.1.2.cmml"><mi id="S3.SS1.SSS2.p4.5.m5.1.2.2" xref="S3.SS1.SSS2.p4.5.m5.1.2.2.cmml">Ï•</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS2.p4.5.m5.1.2.1" xref="S3.SS1.SSS2.p4.5.m5.1.2.1.cmml">â€‹</mo><mrow id="S3.SS1.SSS2.p4.5.m5.1.2.3.2" xref="S3.SS1.SSS2.p4.5.m5.1.2.cmml"><mo stretchy="false" id="S3.SS1.SSS2.p4.5.m5.1.2.3.2.1" xref="S3.SS1.SSS2.p4.5.m5.1.2.cmml">(</mo><mi mathvariant="normal" id="S3.SS1.SSS2.p4.5.m5.1.1" xref="S3.SS1.SSS2.p4.5.m5.1.1.cmml">Î¦</mi><mo stretchy="false" id="S3.SS1.SSS2.p4.5.m5.1.2.3.2.2" xref="S3.SS1.SSS2.p4.5.m5.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p4.5.m5.1b"><apply id="S3.SS1.SSS2.p4.5.m5.1.2.cmml" xref="S3.SS1.SSS2.p4.5.m5.1.2"><times id="S3.SS1.SSS2.p4.5.m5.1.2.1.cmml" xref="S3.SS1.SSS2.p4.5.m5.1.2.1"></times><ci id="S3.SS1.SSS2.p4.5.m5.1.2.2.cmml" xref="S3.SS1.SSS2.p4.5.m5.1.2.2">italic-Ï•</ci><ci id="S3.SS1.SSS2.p4.5.m5.1.1.cmml" xref="S3.SS1.SSS2.p4.5.m5.1.1">Î¦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p4.5.m5.1c">\phi(\Phi)</annotation></semantics></math> as probability maps and take their spatial expectations as <math id="S3.SS1.SSS2.p4.6.m6.1" class="ltx_Math" alttext="\bm{x}" display="inline"><semantics id="S3.SS1.SSS2.p4.6.m6.1a"><mi id="S3.SS1.SSS2.p4.6.m6.1.1" xref="S3.SS1.SSS2.p4.6.m6.1.1.cmml">ğ’™</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p4.6.m6.1b"><ci id="S3.SS1.SSS2.p4.6.m6.1.1.cmml" xref="S3.SS1.SSS2.p4.6.m6.1.1">ğ’™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p4.6.m6.1c">\bm{x}</annotation></semantics></math>. This has two advantages over the â€œargmaxâ€ approach: it has higher accuracy because it is continuous rather than discrete; it is more robust to outlying pixel values.</p>
</div>
<div id="S3.SS1.SSS2.p5" class="ltx_para">
<p id="S3.SS1.SSS2.p5.2" class="ltx_p">Although only the high-precision heatmaps are used to compute the landmark coordinates, the medium and low-precision keypoint heads play an important role in the pipeline. Firstly, having target heatmaps with different variances <math id="S3.SS1.SSS2.p5.1.m1.1" class="ltx_Math" alttext="\sigma^{2}" display="inline"><semantics id="S3.SS1.SSS2.p5.1.m1.1a"><msup id="S3.SS1.SSS2.p5.1.m1.1.1" xref="S3.SS1.SSS2.p5.1.m1.1.1.cmml"><mi id="S3.SS1.SSS2.p5.1.m1.1.1.2" xref="S3.SS1.SSS2.p5.1.m1.1.1.2.cmml">Ïƒ</mi><mn id="S3.SS1.SSS2.p5.1.m1.1.1.3" xref="S3.SS1.SSS2.p5.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p5.1.m1.1b"><apply id="S3.SS1.SSS2.p5.1.m1.1.1.cmml" xref="S3.SS1.SSS2.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p5.1.m1.1.1.1.cmml" xref="S3.SS1.SSS2.p5.1.m1.1.1">superscript</csymbol><ci id="S3.SS1.SSS2.p5.1.m1.1.1.2.cmml" xref="S3.SS1.SSS2.p5.1.m1.1.1.2">ğœ</ci><cn type="integer" id="S3.SS1.SSS2.p5.1.m1.1.1.3.cmml" xref="S3.SS1.SSS2.p5.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p5.1.m1.1c">\sigma^{2}</annotation></semantics></math> helps the model adapt to objects of different sizes. This also relieves the need for tuning <math id="S3.SS1.SSS2.p5.2.m2.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S3.SS1.SSS2.p5.2.m2.1a"><mi id="S3.SS1.SSS2.p5.2.m2.1.1" xref="S3.SS1.SSS2.p5.2.m2.1.1.cmml">Ïƒ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p5.2.m2.1b"><ci id="S3.SS1.SSS2.p5.2.m2.1.1.cmml" xref="S3.SS1.SSS2.p5.2.m2.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p5.2.m2.1c">\sigma</annotation></semantics></math> as a hyper parameter for each object. Secondly, heatmaps from the medium-precision keypoint head are used as an auxiliary for filtering predicted landmarks, as will be explained in the next subsection. Lastly and most importantly, MPS boosts holistic representation learning in the feature maps and increases landmark coherence. An conceptual illustration is shown in FigureÂ <a href="#S3.F3" title="Figure 3 â€£ 3.1.2 Multi-precision supervision â€£ 3.1 Robust landmark prediction â€£ 3 The ROPE framework â€£ Occlusion-Robust Object Pose Estimation with Holistic Representation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div id="S3.SS1.SSS2.p6" class="ltx_para">
<p id="S3.SS1.SSS2.p6.8" class="ltx_p">In FigureÂ <a href="#S3.F3" title="Figure 3 â€£ 3.1.2 Multi-precision supervision â€£ 3.1 Robust landmark prediction â€£ 3 The ROPE framework â€£ Occlusion-Robust Object Pose Estimation with Holistic Representation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we take one section of the feature tensor <math id="S3.SS1.SSS2.p6.1.m1.1" class="ltx_Math" alttext="S1" display="inline"><semantics id="S3.SS1.SSS2.p6.1.m1.1a"><mrow id="S3.SS1.SSS2.p6.1.m1.1.1" xref="S3.SS1.SSS2.p6.1.m1.1.1.cmml"><mi id="S3.SS1.SSS2.p6.1.m1.1.1.2" xref="S3.SS1.SSS2.p6.1.m1.1.1.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS2.p6.1.m1.1.1.1" xref="S3.SS1.SSS2.p6.1.m1.1.1.1.cmml">â€‹</mo><mn id="S3.SS1.SSS2.p6.1.m1.1.1.3" xref="S3.SS1.SSS2.p6.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p6.1.m1.1b"><apply id="S3.SS1.SSS2.p6.1.m1.1.1.cmml" xref="S3.SS1.SSS2.p6.1.m1.1.1"><times id="S3.SS1.SSS2.p6.1.m1.1.1.1.cmml" xref="S3.SS1.SSS2.p6.1.m1.1.1.1"></times><ci id="S3.SS1.SSS2.p6.1.m1.1.1.2.cmml" xref="S3.SS1.SSS2.p6.1.m1.1.1.2">ğ‘†</ci><cn type="integer" id="S3.SS1.SSS2.p6.1.m1.1.1.3.cmml" xref="S3.SS1.SSS2.p6.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p6.1.m1.1c">S1</annotation></semantics></math> for examination. With single precision supervision, <math id="S3.SS1.SSS2.p6.2.m2.1" class="ltx_Math" alttext="S1" display="inline"><semantics id="S3.SS1.SSS2.p6.2.m2.1a"><mrow id="S3.SS1.SSS2.p6.2.m2.1.1" xref="S3.SS1.SSS2.p6.2.m2.1.1.cmml"><mi id="S3.SS1.SSS2.p6.2.m2.1.1.2" xref="S3.SS1.SSS2.p6.2.m2.1.1.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS2.p6.2.m2.1.1.1" xref="S3.SS1.SSS2.p6.2.m2.1.1.1.cmml">â€‹</mo><mn id="S3.SS1.SSS2.p6.2.m2.1.1.3" xref="S3.SS1.SSS2.p6.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p6.2.m2.1b"><apply id="S3.SS1.SSS2.p6.2.m2.1.1.cmml" xref="S3.SS1.SSS2.p6.2.m2.1.1"><times id="S3.SS1.SSS2.p6.2.m2.1.1.1.cmml" xref="S3.SS1.SSS2.p6.2.m2.1.1.1"></times><ci id="S3.SS1.SSS2.p6.2.m2.1.1.2.cmml" xref="S3.SS1.SSS2.p6.2.m2.1.1.2">ğ‘†</ci><cn type="integer" id="S3.SS1.SSS2.p6.2.m2.1.1.3.cmml" xref="S3.SS1.SSS2.p6.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p6.2.m2.1c">S1</annotation></semantics></math> is only responsible for activating the region <math id="S3.SS1.SSS2.p6.3.m3.1" class="ltx_Math" alttext="A1" display="inline"><semantics id="S3.SS1.SSS2.p6.3.m3.1a"><mrow id="S3.SS1.SSS2.p6.3.m3.1.1" xref="S3.SS1.SSS2.p6.3.m3.1.1.cmml"><mi id="S3.SS1.SSS2.p6.3.m3.1.1.2" xref="S3.SS1.SSS2.p6.3.m3.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS2.p6.3.m3.1.1.1" xref="S3.SS1.SSS2.p6.3.m3.1.1.1.cmml">â€‹</mo><mn id="S3.SS1.SSS2.p6.3.m3.1.1.3" xref="S3.SS1.SSS2.p6.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p6.3.m3.1b"><apply id="S3.SS1.SSS2.p6.3.m3.1.1.cmml" xref="S3.SS1.SSS2.p6.3.m3.1.1"><times id="S3.SS1.SSS2.p6.3.m3.1.1.1.cmml" xref="S3.SS1.SSS2.p6.3.m3.1.1.1"></times><ci id="S3.SS1.SSS2.p6.3.m3.1.1.2.cmml" xref="S3.SS1.SSS2.p6.3.m3.1.1.2">ğ´</ci><cn type="integer" id="S3.SS1.SSS2.p6.3.m3.1.1.3.cmml" xref="S3.SS1.SSS2.p6.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p6.3.m3.1c">A1</annotation></semantics></math> in the predicted heatmap of Landmark 1. It does not learn useful information about Landmark 2. In the MPS scenario, besides learning about Landmark 1 via <math id="S3.SS1.SSS2.p6.4.m4.1" class="ltx_Math" alttext="A1" display="inline"><semantics id="S3.SS1.SSS2.p6.4.m4.1a"><mrow id="S3.SS1.SSS2.p6.4.m4.1.1" xref="S3.SS1.SSS2.p6.4.m4.1.1.cmml"><mi id="S3.SS1.SSS2.p6.4.m4.1.1.2" xref="S3.SS1.SSS2.p6.4.m4.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS2.p6.4.m4.1.1.1" xref="S3.SS1.SSS2.p6.4.m4.1.1.1.cmml">â€‹</mo><mn id="S3.SS1.SSS2.p6.4.m4.1.1.3" xref="S3.SS1.SSS2.p6.4.m4.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p6.4.m4.1b"><apply id="S3.SS1.SSS2.p6.4.m4.1.1.cmml" xref="S3.SS1.SSS2.p6.4.m4.1.1"><times id="S3.SS1.SSS2.p6.4.m4.1.1.1.cmml" xref="S3.SS1.SSS2.p6.4.m4.1.1.1"></times><ci id="S3.SS1.SSS2.p6.4.m4.1.1.2.cmml" xref="S3.SS1.SSS2.p6.4.m4.1.1.2">ğ´</ci><cn type="integer" id="S3.SS1.SSS2.p6.4.m4.1.1.3.cmml" xref="S3.SS1.SSS2.p6.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p6.4.m4.1c">A1</annotation></semantics></math> and <math id="S3.SS1.SSS2.p6.5.m5.1" class="ltx_Math" alttext="A3" display="inline"><semantics id="S3.SS1.SSS2.p6.5.m5.1a"><mrow id="S3.SS1.SSS2.p6.5.m5.1.1" xref="S3.SS1.SSS2.p6.5.m5.1.1.cmml"><mi id="S3.SS1.SSS2.p6.5.m5.1.1.2" xref="S3.SS1.SSS2.p6.5.m5.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS2.p6.5.m5.1.1.1" xref="S3.SS1.SSS2.p6.5.m5.1.1.1.cmml">â€‹</mo><mn id="S3.SS1.SSS2.p6.5.m5.1.1.3" xref="S3.SS1.SSS2.p6.5.m5.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p6.5.m5.1b"><apply id="S3.SS1.SSS2.p6.5.m5.1.1.cmml" xref="S3.SS1.SSS2.p6.5.m5.1.1"><times id="S3.SS1.SSS2.p6.5.m5.1.1.1.cmml" xref="S3.SS1.SSS2.p6.5.m5.1.1.1"></times><ci id="S3.SS1.SSS2.p6.5.m5.1.1.2.cmml" xref="S3.SS1.SSS2.p6.5.m5.1.1.2">ğ´</ci><cn type="integer" id="S3.SS1.SSS2.p6.5.m5.1.1.3.cmml" xref="S3.SS1.SSS2.p6.5.m5.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p6.5.m5.1c">A3</annotation></semantics></math>, <math id="S3.SS1.SSS2.p6.6.m6.1" class="ltx_Math" alttext="S1" display="inline"><semantics id="S3.SS1.SSS2.p6.6.m6.1a"><mrow id="S3.SS1.SSS2.p6.6.m6.1.1" xref="S3.SS1.SSS2.p6.6.m6.1.1.cmml"><mi id="S3.SS1.SSS2.p6.6.m6.1.1.2" xref="S3.SS1.SSS2.p6.6.m6.1.1.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS2.p6.6.m6.1.1.1" xref="S3.SS1.SSS2.p6.6.m6.1.1.1.cmml">â€‹</mo><mn id="S3.SS1.SSS2.p6.6.m6.1.1.3" xref="S3.SS1.SSS2.p6.6.m6.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p6.6.m6.1b"><apply id="S3.SS1.SSS2.p6.6.m6.1.1.cmml" xref="S3.SS1.SSS2.p6.6.m6.1.1"><times id="S3.SS1.SSS2.p6.6.m6.1.1.1.cmml" xref="S3.SS1.SSS2.p6.6.m6.1.1.1"></times><ci id="S3.SS1.SSS2.p6.6.m6.1.1.2.cmml" xref="S3.SS1.SSS2.p6.6.m6.1.1.2">ğ‘†</ci><cn type="integer" id="S3.SS1.SSS2.p6.6.m6.1.1.3.cmml" xref="S3.SS1.SSS2.p6.6.m6.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p6.6.m6.1c">S1</annotation></semantics></math> is also exposed to the receptive field of <math id="S3.SS1.SSS2.p6.7.m7.1" class="ltx_Math" alttext="A4" display="inline"><semantics id="S3.SS1.SSS2.p6.7.m7.1a"><mrow id="S3.SS1.SSS2.p6.7.m7.1.1" xref="S3.SS1.SSS2.p6.7.m7.1.1.cmml"><mi id="S3.SS1.SSS2.p6.7.m7.1.1.2" xref="S3.SS1.SSS2.p6.7.m7.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS2.p6.7.m7.1.1.1" xref="S3.SS1.SSS2.p6.7.m7.1.1.1.cmml">â€‹</mo><mn id="S3.SS1.SSS2.p6.7.m7.1.1.3" xref="S3.SS1.SSS2.p6.7.m7.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p6.7.m7.1b"><apply id="S3.SS1.SSS2.p6.7.m7.1.1.cmml" xref="S3.SS1.SSS2.p6.7.m7.1.1"><times id="S3.SS1.SSS2.p6.7.m7.1.1.1.cmml" xref="S3.SS1.SSS2.p6.7.m7.1.1.1"></times><ci id="S3.SS1.SSS2.p6.7.m7.1.1.2.cmml" xref="S3.SS1.SSS2.p6.7.m7.1.1.2">ğ´</ci><cn type="integer" id="S3.SS1.SSS2.p6.7.m7.1.1.3.cmml" xref="S3.SS1.SSS2.p6.7.m7.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p6.7.m7.1c">A4</annotation></semantics></math> from Landmark 2. This enforces <math id="S3.SS1.SSS2.p6.8.m8.1" class="ltx_Math" alttext="S1" display="inline"><semantics id="S3.SS1.SSS2.p6.8.m8.1a"><mrow id="S3.SS1.SSS2.p6.8.m8.1.1" xref="S3.SS1.SSS2.p6.8.m8.1.1.cmml"><mi id="S3.SS1.SSS2.p6.8.m8.1.1.2" xref="S3.SS1.SSS2.p6.8.m8.1.1.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS2.p6.8.m8.1.1.1" xref="S3.SS1.SSS2.p6.8.m8.1.1.1.cmml">â€‹</mo><mn id="S3.SS1.SSS2.p6.8.m8.1.1.3" xref="S3.SS1.SSS2.p6.8.m8.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p6.8.m8.1b"><apply id="S3.SS1.SSS2.p6.8.m8.1.1.cmml" xref="S3.SS1.SSS2.p6.8.m8.1.1"><times id="S3.SS1.SSS2.p6.8.m8.1.1.1.cmml" xref="S3.SS1.SSS2.p6.8.m8.1.1.1"></times><ci id="S3.SS1.SSS2.p6.8.m8.1.1.2.cmml" xref="S3.SS1.SSS2.p6.8.m8.1.1.2">ğ‘†</ci><cn type="integer" id="S3.SS1.SSS2.p6.8.m8.1.1.3.cmml" xref="S3.SS1.SSS2.p6.8.m8.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p6.8.m8.1c">S1</annotation></semantics></math> to incorporate relevant information and become more â€œawareâ€ of the location of Landmark 2. The overall effect is that, each part of the feature tensor not only learns the necessary information to predict a local landmark, but also integrates knowledge of other landmarks to understand a wider context, thus learns a more holistic representation of the target object pose.</p>
</div>
<div id="S3.SS1.SSS2.p7" class="ltx_para">
<p id="S3.SS1.SSS2.p7.1" class="ltx_p">A holistic representation enables heatmap predictions to be more robust against occlusions. As shown in FigureÂ <a href="#S3.F4" title="Figure 4 â€£ 3.1.2 Multi-precision supervision â€£ 3.1 Robust landmark prediction â€£ 3 The ROPE framework â€£ Occlusion-Robust Object Pose Estimation with Holistic Representation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, when trained without MPS, novel occlusions result in confused heatmap activations. On the other hand, a holistic representation learned via MPS is able to produces stable heatmaps for the occluded landmarks. This also boosts the structural consistency of landmark predictions as shown in FigureÂ <a href="#S4.F5" title="Figure 5 â€£ 4 Experiments â€£ Occlusion-Robust Object Pose Estimation with Holistic Representation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> and <a href="#S4.F7" title="Figure 7 â€£ 4.3.1 Model variations â€£ 4.3 Ablation studies â€£ 4 Experiments â€£ Occlusion-Robust Object Pose Estimation with Holistic Representation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, which is further discussed in SectionÂ <a href="#S4.SS3" title="4.3 Ablation studies â€£ 4 Experiments â€£ Occlusion-Robust Object Pose Estimation with Holistic Representation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2110.11636/assets/x2.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="461" height="583" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The effect of holistic representation learning in heatmap prediction. Predictions of heatmap 1 are from a model (MV1) trained without MPS while those of heatmap 2 are from the full model (original) with MPS. Details of models (MV1 and original) are provided in SectionÂ <a href="#S4.SS3.SSS1" title="4.3.1 Model variations â€£ 4.3 Ablation studies â€£ 4 Experiments â€£ Occlusion-Robust Object Pose Estimation with Holistic Representation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3.1</span></a>. </figcaption>
</figure>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Landmark filtering</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Many pose estimation pipelines include a refinement stage which is either optimisation-based <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> or learning-based <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>. While such post-processing is effective in boosting prediction accuracy, it adds additional computation burdens which is a disadvantage especially for real-time applications. In order to boost prediction accuracy while at the same time avoiding heavy post-processing computation, we make use of the multi-heads design of MPS for selecting high-quality landmark predictions before passing them to the PnP solver, thus alleviating the need for significant pose refinement.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.3" class="ltx_p">Specifically, for an image <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">ğ¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">I</annotation></semantics></math>, let <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="\{\bm{x}_{i}\}" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mrow id="S3.SS2.p2.2.m2.1.1.1" xref="S3.SS2.p2.2.m2.1.1.2.cmml"><mo stretchy="false" id="S3.SS2.p2.2.m2.1.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml">{</mo><msub id="S3.SS2.p2.2.m2.1.1.1.1" xref="S3.SS2.p2.2.m2.1.1.1.1.cmml"><mi id="S3.SS2.p2.2.m2.1.1.1.1.2" xref="S3.SS2.p2.2.m2.1.1.1.1.2.cmml">ğ’™</mi><mi id="S3.SS2.p2.2.m2.1.1.1.1.3" xref="S3.SS2.p2.2.m2.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS2.p2.2.m2.1.1.1.3" xref="S3.SS2.p2.2.m2.1.1.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><set id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.1"><apply id="S3.SS2.p2.2.m2.1.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p2.2.m2.1.1.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.2">ğ’™</ci><ci id="S3.SS2.p2.2.m2.1.1.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.3">ğ‘–</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">\{\bm{x}_{i}\}</annotation></semantics></math> denote the set of predicted landmark coordinates from the high-precision keypoint head, and <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="\{\bm{x}_{i}^{m}\}" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><mrow id="S3.SS2.p2.3.m3.1.1.1" xref="S3.SS2.p2.3.m3.1.1.2.cmml"><mo stretchy="false" id="S3.SS2.p2.3.m3.1.1.1.2" xref="S3.SS2.p2.3.m3.1.1.2.cmml">{</mo><msubsup id="S3.SS2.p2.3.m3.1.1.1.1" xref="S3.SS2.p2.3.m3.1.1.1.1.cmml"><mi id="S3.SS2.p2.3.m3.1.1.1.1.2.2" xref="S3.SS2.p2.3.m3.1.1.1.1.2.2.cmml">ğ’™</mi><mi id="S3.SS2.p2.3.m3.1.1.1.1.2.3" xref="S3.SS2.p2.3.m3.1.1.1.1.2.3.cmml">i</mi><mi id="S3.SS2.p2.3.m3.1.1.1.1.3" xref="S3.SS2.p2.3.m3.1.1.1.1.3.cmml">m</mi></msubsup><mo stretchy="false" id="S3.SS2.p2.3.m3.1.1.1.3" xref="S3.SS2.p2.3.m3.1.1.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><set id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.1"><apply id="S3.SS2.p2.3.m3.1.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.1.1.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1.1.1">superscript</csymbol><apply id="S3.SS2.p2.3.m3.1.1.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.1.1.1.1.2.1.cmml" xref="S3.SS2.p2.3.m3.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p2.3.m3.1.1.1.1.2.2.cmml" xref="S3.SS2.p2.3.m3.1.1.1.1.2.2">ğ’™</ci><ci id="S3.SS2.p2.3.m3.1.1.1.1.2.3.cmml" xref="S3.SS2.p2.3.m3.1.1.1.1.2.3">ğ‘–</ci></apply><ci id="S3.SS2.p2.3.m3.1.1.1.1.3.cmml" xref="S3.SS2.p2.3.m3.1.1.1.1.3">ğ‘š</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">\{\bm{x}_{i}^{m}\}</annotation></semantics></math> denote the set of landmark coordinates predicted from the medium-precision keypoint head. We then select a subset</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.1" class="ltx_math_unparsed" alttext="\{\bm{x}_{i}|\rVert\bm{x}_{i}-\bm{x}_{i}^{m}\rVert_{2}\leq\epsilon\}" display="block"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1b"><msub id="S3.E2.m1.1.1"><mrow id="S3.E2.m1.1.1.2"><mrow id="S3.E2.m1.1.1.2.1"><mo stretchy="false" id="S3.E2.m1.1.1.2.1.1">{</mo><msub id="S3.E2.m1.1.1.2.1.2"><mi id="S3.E2.m1.1.1.2.1.2.2">ğ’™</mi><mi id="S3.E2.m1.1.1.2.1.2.3">i</mi></msub><mo fence="false" stretchy="false" id="S3.E2.m1.1.1.2.1.3">|</mo><mo fence="true" lspace="0.167em" rspace="0em" id="S3.E2.m1.1.1.2.1.4">âˆ¥</mo></mrow><msub id="S3.E2.m1.1.1.2.2"><mi id="S3.E2.m1.1.1.2.2.2">ğ’™</mi><mi id="S3.E2.m1.1.1.2.2.3">i</mi></msub><mo id="S3.E2.m1.1.1.2.3">âˆ’</mo><msubsup id="S3.E2.m1.1.1.2.4"><mi id="S3.E2.m1.1.1.2.4.2.2">ğ’™</mi><mi id="S3.E2.m1.1.1.2.4.2.3">i</mi><mi id="S3.E2.m1.1.1.2.4.3">m</mi></msubsup><mo fence="true" lspace="0em" id="S3.E2.m1.1.1.2.5">âˆ¥</mo></mrow><mn id="S3.E2.m1.1.1.3">2</mn></msub><mo id="S3.E2.m1.1.2">â‰¤</mo><mi id="S3.E2.m1.1.3">Ïµ</mi><mo stretchy="false" id="S3.E2.m1.1.4">}</mo></mrow><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\{\bm{x}_{i}|\rVert\bm{x}_{i}-\bm{x}_{i}^{m}\rVert_{2}\leq\epsilon\}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p2.5" class="ltx_p">for the PnP solver to compute the pose. In other words, a landmark prediction from the high-precision head will only be selected for the pose solver if it is verified by the corresponding medium-precision prediction, where <math id="S3.SS2.p2.4.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.SS2.p2.4.m1.1a"><mi id="S3.SS2.p2.4.m1.1.1" xref="S3.SS2.p2.4.m1.1.1.cmml">Ïµ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m1.1b"><ci id="S3.SS2.p2.4.m1.1.1.cmml" xref="S3.SS2.p2.4.m1.1.1">italic-Ïµ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m1.1c">\epsilon</annotation></semantics></math> is the verification threshold. In the case that the selected subset has fewer than 4 points, which is the minimum number required by a PnP solver, we then use the 4 points with the smallest <math id="S3.SS2.p2.5.m2.1" class="ltx_math_unparsed" alttext="\rVert\bm{x}_{i}-\bm{x}_{i}^{m}\rVert_{2}" display="inline"><semantics id="S3.SS2.p2.5.m2.1a"><mrow id="S3.SS2.p2.5.m2.1b"><mo fence="true" rspace="0em" id="S3.SS2.p2.5.m2.1.1">âˆ¥</mo><msub id="S3.SS2.p2.5.m2.1.2"><mi id="S3.SS2.p2.5.m2.1.2.2">ğ’™</mi><mi id="S3.SS2.p2.5.m2.1.2.3">i</mi></msub><mo id="S3.SS2.p2.5.m2.1.3">âˆ’</mo><msubsup id="S3.SS2.p2.5.m2.1.4"><mi id="S3.SS2.p2.5.m2.1.4.2.2">ğ’™</mi><mi id="S3.SS2.p2.5.m2.1.4.2.3">i</mi><mi id="S3.SS2.p2.5.m2.1.4.3">m</mi></msubsup><mo fence="true" lspace="0em" rspace="0em" id="S3.SS2.p2.5.m2.1.5">âˆ¥</mo><msub id="S3.SS2.p2.5.m2.1.6"><mi id="S3.SS2.p2.5.m2.1.6a"></mi><mn id="S3.SS2.p2.5.m2.1.6.1">2</mn></msub></mrow><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m2.1c">\rVert\bm{x}_{i}-\bm{x}_{i}^{m}\rVert_{2}</annotation></semantics></math> values as the subset.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">While in this work we focus on the base pose estimator and report its performances without any refinement, our pipeline can be easily extended to stack one or multiple refiners such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section we conduct experiments to validate the effectiveness of ROPE as well as to compare it to SOTA methods of RGB image-based pose estimation.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2110.11636/assets/x3.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="164" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Comparing performances of model variants on the Occluded-LINEMOD dataset with qualitative examples. </figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets and metrics</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We choose the widely used LINEMODÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, its extension Occluded-LINEMODÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> and the YCB-VideoÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite> datasets for our experiments.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">For LINEMOD, we follow the convention of previous worksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> by using 15% of the images of each object as training set and the remaining 85% as testing set. The training images are selected in such a way that the relative rotation between them are larger than a threshold. For each object, we additionally use 1312 rendered images of the isolated object for training, which are obtained fromÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. For Occluded-LINEMOD the whole dataset is used for testing while images of the corresponding objects in LINEMOD, as well as the rendered images, are used for training. We also follow the protocol of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> for the YCB-Video dataset: we use 80 out of the 92 video sequences as well as the 80000 synthetic images for training, and test on 2949 key frames from the reserved 12 sequences.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">We report the ADD(-S) metric which combines the ADD metricÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> for asymmetric objects and the ADD-S metricÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite> for symmetric ones. The ADD metric computes the percentage of correctly estimated poses. A pose is considered correct if the object model points, when transformed by the predicted and groundtruth poses respectively, have an average distance of less 10% of the model diameter. For ADD-S, this distance is instead computed based on the closest point distance. The ADD(-S) metric is preferred over the 2D projection metricÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> because it directly measures the alignment discrepancy in 3D.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">For the YCB-Video dataset we also report the AUC metric proposed inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite> and adopted in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>. The AUC metric is the area under the ADD(-S) curve when varying the distance threshold for a pose to be deemed correct. We vary this threshold from 0 to 10 cm, in accordance withÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Implementation details</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">For each object model we apply the farthest point sampling (FPS) algorithmÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> on the 3D point cloud and select 11 landmarks. The groundtruth 2D landmarks are then obtained by projecting the 3D landmarks with groundtruth camera pose and intrinsics.
We use ImgAugÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> for regular data augmentations including rotation, translation, scaling and color jitter before the OBA. We use the Adam optimizerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> and train the model for 250 epochs on LINEMOD and 200 epochs on Occluded-LINEMOD and YCB-Video. We set the landmark verification threshold <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">Ïµ</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">italic-Ïµ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">\epsilon</annotation></semantics></math> to 1 pixel for all datasets.</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2110.11636/assets/x4.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="147" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>A toy example for the intuition of incoherence measure <math id="S4.F6.4.m1.1" class="ltx_Math" alttext="c_{i}" display="inline"><semantics id="S4.F6.4.m1.1b"><msub id="S4.F6.4.m1.1.1" xref="S4.F6.4.m1.1.1.cmml"><mi id="S4.F6.4.m1.1.1.2" xref="S4.F6.4.m1.1.1.2.cmml">c</mi><mi id="S4.F6.4.m1.1.1.3" xref="S4.F6.4.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.F6.4.m1.1c"><apply id="S4.F6.4.m1.1.1.cmml" xref="S4.F6.4.m1.1.1"><csymbol cd="ambiguous" id="S4.F6.4.m1.1.1.1.cmml" xref="S4.F6.4.m1.1.1">subscript</csymbol><ci id="S4.F6.4.m1.1.1.2.cmml" xref="S4.F6.4.m1.1.1.2">ğ‘</ci><ci id="S4.F6.4.m1.1.1.3.cmml" xref="S4.F6.4.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.4.m1.1d">c_{i}</annotation></semantics></math>. The mean residual <math id="S4.F6.5.m2.1" class="ltx_Math" alttext="r_{i}" display="inline"><semantics id="S4.F6.5.m2.1b"><msub id="S4.F6.5.m2.1.1" xref="S4.F6.5.m2.1.1.cmml"><mi id="S4.F6.5.m2.1.1.2" xref="S4.F6.5.m2.1.1.2.cmml">r</mi><mi id="S4.F6.5.m2.1.1.3" xref="S4.F6.5.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.F6.5.m2.1c"><apply id="S4.F6.5.m2.1.1.cmml" xref="S4.F6.5.m2.1.1"><csymbol cd="ambiguous" id="S4.F6.5.m2.1.1.1.cmml" xref="S4.F6.5.m2.1.1">subscript</csymbol><ci id="S4.F6.5.m2.1.1.2.cmml" xref="S4.F6.5.m2.1.1.2">ğ‘Ÿ</ci><ci id="S4.F6.5.m2.1.1.3.cmml" xref="S4.F6.5.m2.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.5.m2.1d">r_{i}</annotation></semantics></math> for prediction 1 (blue) and prediction 2 (green) are both 0.608. However, their mean incoherence measure <math id="S4.F6.6.m3.1" class="ltx_Math" alttext="c_{i}" display="inline"><semantics id="S4.F6.6.m3.1b"><msub id="S4.F6.6.m3.1.1" xref="S4.F6.6.m3.1.1.cmml"><mi id="S4.F6.6.m3.1.1.2" xref="S4.F6.6.m3.1.1.2.cmml">c</mi><mi id="S4.F6.6.m3.1.1.3" xref="S4.F6.6.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.F6.6.m3.1c"><apply id="S4.F6.6.m3.1.1.cmml" xref="S4.F6.6.m3.1.1"><csymbol cd="ambiguous" id="S4.F6.6.m3.1.1.1.cmml" xref="S4.F6.6.m3.1.1">subscript</csymbol><ci id="S4.F6.6.m3.1.1.2.cmml" xref="S4.F6.6.m3.1.1.2">ğ‘</ci><ci id="S4.F6.6.m3.1.1.3.cmml" xref="S4.F6.6.m3.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.6.m3.1d">c_{i}</annotation></semantics></math> are 0.604 and 0.074, respectively. Although both predictions are identical in terms of accuracy, prediction 2 has much better coherence as the green triangle is much more similar in shape to the groundtruth than the blue one.</figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Ablation studies</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We conduct various ablation tests to investigate the effect of the proposed OBA and MPS.</p>
</div>
<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Model variations</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.1" class="ltx_p">To verify the efficacy of OBA and MPS, we create two Model Variants (MV) of ROPE:</p>
<ol id="S4.I1" class="ltx_enumerate">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">(MV1: w/ OBA, w/o MPS) While keeping everything else of the original ROPE unchanged, we remove the low and medium-precision keypoint heads, and train the one-head-model with high-precision groundtruth heatmaps.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">(MV2: w/o OBA, w/o MPS) On top MV1, we further remove OBA in training. Note that <em id="S4.I1.i2.p1.1.1" class="ltx_emph ltx_font_italic">common data augmentations including rotation, translation, scaling and color jitter, are still kept</em>.</p>
</div>
</li>
</ol>
</div>
<div id="S4.SS3.SSS1.p2" class="ltx_para">
<p id="S4.SS3.SSS1.p2.1" class="ltx_p">FigureÂ <a href="#S4.F5" title="Figure 5 â€£ 4 Experiments â€£ Occlusion-Robust Object Pose Estimation with Holistic Representation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows the overall ADD(-S) on the Occluded-LINEMOD dataset, as well as qualitative results of all model variants. Without both OBA and MPS, object detection can easily fail and landmark prediction is precarious. We can clearly see that occlusion-robust feature learning enforced by OBA significantly increases the reliability of object detection and landmark prediction. In addition, by comparing MV1 and the original model, it is obvious that MPS boosts the structural consistency of the predicted landmarks, especially in occluded regions. This shows that a holistic representation induced by MPS enhances landmark coherence, strengthening the modelâ€™s ability to extrapolate to the occluded part of the object.</p>
</div>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2110.11636/assets/figs/bubble.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="402" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Comparing the results of training with and without MPS on LINEMOD, while keeping all else equal. The vertical location of each bubble represents the mean prediction residual <math id="S4.F7.3.m1.1" class="ltx_Math" alttext="r_{i}" display="inline"><semantics id="S4.F7.3.m1.1b"><msub id="S4.F7.3.m1.1.1" xref="S4.F7.3.m1.1.1.cmml"><mi id="S4.F7.3.m1.1.1.2" xref="S4.F7.3.m1.1.1.2.cmml">r</mi><mi id="S4.F7.3.m1.1.1.3" xref="S4.F7.3.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.F7.3.m1.1c"><apply id="S4.F7.3.m1.1.1.cmml" xref="S4.F7.3.m1.1.1"><csymbol cd="ambiguous" id="S4.F7.3.m1.1.1.1.cmml" xref="S4.F7.3.m1.1.1">subscript</csymbol><ci id="S4.F7.3.m1.1.1.2.cmml" xref="S4.F7.3.m1.1.1.2">ğ‘Ÿ</ci><ci id="S4.F7.3.m1.1.1.3.cmml" xref="S4.F7.3.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F7.3.m1.1d">r_{i}</annotation></semantics></math> of all landmarks in the testing sets. The size of each bubble indicates the mean incoherence <math id="S4.F7.4.m2.1" class="ltx_Math" alttext="c_{i}" display="inline"><semantics id="S4.F7.4.m2.1b"><msub id="S4.F7.4.m2.1.1" xref="S4.F7.4.m2.1.1.cmml"><mi id="S4.F7.4.m2.1.1.2" xref="S4.F7.4.m2.1.1.2.cmml">c</mi><mi id="S4.F7.4.m2.1.1.3" xref="S4.F7.4.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.F7.4.m2.1c"><apply id="S4.F7.4.m2.1.1.cmml" xref="S4.F7.4.m2.1.1"><csymbol cd="ambiguous" id="S4.F7.4.m2.1.1.1.cmml" xref="S4.F7.4.m2.1.1">subscript</csymbol><ci id="S4.F7.4.m2.1.1.2.cmml" xref="S4.F7.4.m2.1.1.2">ğ‘</ci><ci id="S4.F7.4.m2.1.1.3.cmml" xref="S4.F7.4.m2.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F7.4.m2.1d">c_{i}</annotation></semantics></math>. </figcaption>
</figure>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" rowspan="3"><span id="S4.T1.1.1.1.1.1" class="ltx_text">ADD(-S)</span></th>
<th id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="6">Without refinement</th>
<th id="S4.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="4">With refinement</th>
</tr>
<tr id="S4.T1.1.2.2" class="ltx_tr">
<th id="S4.T1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">PVNet</th>
<th id="S4.T1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Pix2Pose</th>
<th id="S4.T1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">DPOD</th>
<th id="S4.T1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">CDPN</th>
<th id="S4.T1.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">GDR</th>
<th id="S4.T1.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Ours</th>
<th id="S4.T1.1.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">SSD-6D</th>
<th id="S4.T1.1.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">DPOD+</th>
<th id="S4.T1.1.2.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">HybridPose</th>
<th id="S4.T1.1.2.2.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">DeepIM</th>
</tr>
<tr id="S4.T1.1.3.3" class="ltx_tr">
<th id="S4.T1.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_column"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite></th>
<th id="S4.T1.1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_column"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite></th>
<th id="S4.T1.1.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite></th>
<th id="S4.T1.1.3.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_column"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite></th>
<th id="S4.T1.1.3.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_column"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite></th>
<th id="S4.T1.1.3.3.6" class="ltx_td ltx_th ltx_th_column ltx_border_r"></th>
<th id="S4.T1.1.3.3.7" class="ltx_td ltx_align_center ltx_th ltx_th_column"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite></th>
<th id="S4.T1.1.3.3.8" class="ltx_td ltx_align_center ltx_th ltx_th_column"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite></th>
<th id="S4.T1.1.3.3.9" class="ltx_td ltx_align_center ltx_th ltx_th_column"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite></th>
<th id="S4.T1.1.3.3.10" class="ltx_td ltx_align_center ltx_th ltx_th_column"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.4.1" class="ltx_tr">
<th id="S4.T1.1.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">ape</th>
<td id="S4.T1.1.4.1.2" class="ltx_td ltx_align_center ltx_border_t">43.62</td>
<td id="S4.T1.1.4.1.3" class="ltx_td ltx_align_center ltx_border_t">58.10</td>
<td id="S4.T1.1.4.1.4" class="ltx_td ltx_align_center ltx_border_t">53.28</td>
<td id="S4.T1.1.4.1.5" class="ltx_td ltx_align_center ltx_border_t">64.38</td>
<td id="S4.T1.1.4.1.6" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T1.1.4.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.1.4.1.7.1" class="ltx_text ltx_font_bold">81.52</span></td>
<td id="S4.T1.1.4.1.8" class="ltx_td ltx_align_center ltx_border_t">65.00</td>
<td id="S4.T1.1.4.1.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.4.1.9.1" class="ltx_text ltx_font_bold">87.70</span></td>
<td id="S4.T1.1.4.1.10" class="ltx_td ltx_align_center ltx_border_t">63.10</td>
<td id="S4.T1.1.4.1.11" class="ltx_td ltx_align_center ltx_border_t">77.00</td>
</tr>
<tr id="S4.T1.1.5.2" class="ltx_tr">
<th id="S4.T1.1.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">benchevise</th>
<td id="S4.T1.1.5.2.2" class="ltx_td ltx_align_center">99.90</td>
<td id="S4.T1.1.5.2.3" class="ltx_td ltx_align_center">91.00</td>
<td id="S4.T1.1.5.2.4" class="ltx_td ltx_align_center">95.34</td>
<td id="S4.T1.1.5.2.5" class="ltx_td ltx_align_center">97.77</td>
<td id="S4.T1.1.5.2.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T1.1.5.2.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.1.5.2.7.1" class="ltx_text ltx_font_bold">100.00</span></td>
<td id="S4.T1.1.5.2.8" class="ltx_td ltx_align_center">80.00</td>
<td id="S4.T1.1.5.2.9" class="ltx_td ltx_align_center">98.50</td>
<td id="S4.T1.1.5.2.10" class="ltx_td ltx_align_center"><span id="S4.T1.1.5.2.10.1" class="ltx_text ltx_font_bold">99.90</span></td>
<td id="S4.T1.1.5.2.11" class="ltx_td ltx_align_center">97.50</td>
</tr>
<tr id="S4.T1.1.6.3" class="ltx_tr">
<th id="S4.T1.1.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">cam</th>
<td id="S4.T1.1.6.3.2" class="ltx_td ltx_align_center">86.86</td>
<td id="S4.T1.1.6.3.3" class="ltx_td ltx_align_center">60.90</td>
<td id="S4.T1.1.6.3.4" class="ltx_td ltx_align_center">90.36</td>
<td id="S4.T1.1.6.3.5" class="ltx_td ltx_align_center">91.67</td>
<td id="S4.T1.1.6.3.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T1.1.6.3.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.1.6.3.7.1" class="ltx_text ltx_font_bold">96.86</span></td>
<td id="S4.T1.1.6.3.8" class="ltx_td ltx_align_center">78.00</td>
<td id="S4.T1.1.6.3.9" class="ltx_td ltx_align_center"><span id="S4.T1.1.6.3.9.1" class="ltx_text ltx_font_bold">96.10</span></td>
<td id="S4.T1.1.6.3.10" class="ltx_td ltx_align_center">90.40</td>
<td id="S4.T1.1.6.3.11" class="ltx_td ltx_align_center">93.50</td>
</tr>
<tr id="S4.T1.1.7.4" class="ltx_tr">
<th id="S4.T1.1.7.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">can</th>
<td id="S4.T1.1.7.4.2" class="ltx_td ltx_align_center">95.47</td>
<td id="S4.T1.1.7.4.3" class="ltx_td ltx_align_center">84.40</td>
<td id="S4.T1.1.7.4.4" class="ltx_td ltx_align_center">94.10</td>
<td id="S4.T1.1.7.4.5" class="ltx_td ltx_align_center">95.87</td>
<td id="S4.T1.1.7.4.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T1.1.7.4.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.1.7.4.7.1" class="ltx_text ltx_font_bold">98.72</span></td>
<td id="S4.T1.1.7.4.8" class="ltx_td ltx_align_center">86.00</td>
<td id="S4.T1.1.7.4.9" class="ltx_td ltx_align_center"><span id="S4.T1.1.7.4.9.1" class="ltx_text ltx_font_bold">99.70</span></td>
<td id="S4.T1.1.7.4.10" class="ltx_td ltx_align_center">98.50</td>
<td id="S4.T1.1.7.4.11" class="ltx_td ltx_align_center">96.50</td>
</tr>
<tr id="S4.T1.1.8.5" class="ltx_tr">
<th id="S4.T1.1.8.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">cat</th>
<td id="S4.T1.1.8.5.2" class="ltx_td ltx_align_center">79.34</td>
<td id="S4.T1.1.8.5.3" class="ltx_td ltx_align_center">65.00</td>
<td id="S4.T1.1.8.5.4" class="ltx_td ltx_align_center">60.38</td>
<td id="S4.T1.1.8.5.5" class="ltx_td ltx_align_center">83.83</td>
<td id="S4.T1.1.8.5.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T1.1.8.5.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.1.8.5.7.1" class="ltx_text ltx_font_bold">94.71</span></td>
<td id="S4.T1.1.8.5.8" class="ltx_td ltx_align_center">70.00</td>
<td id="S4.T1.1.8.5.9" class="ltx_td ltx_align_center"><span id="S4.T1.1.8.5.9.1" class="ltx_text ltx_font_bold">94.70</span></td>
<td id="S4.T1.1.8.5.10" class="ltx_td ltx_align_center">89.40</td>
<td id="S4.T1.1.8.5.11" class="ltx_td ltx_align_center">82.10</td>
</tr>
<tr id="S4.T1.1.9.6" class="ltx_tr">
<th id="S4.T1.1.9.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">driller</th>
<td id="S4.T1.1.9.6.2" class="ltx_td ltx_align_center">96.43</td>
<td id="S4.T1.1.9.6.3" class="ltx_td ltx_align_center">76.30</td>
<td id="S4.T1.1.9.6.4" class="ltx_td ltx_align_center">97.72</td>
<td id="S4.T1.1.9.6.5" class="ltx_td ltx_align_center">96.23</td>
<td id="S4.T1.1.9.6.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T1.1.9.6.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.1.9.6.7.1" class="ltx_text ltx_font_bold">99.01</span></td>
<td id="S4.T1.1.9.6.8" class="ltx_td ltx_align_center">73.00</td>
<td id="S4.T1.1.9.6.9" class="ltx_td ltx_align_center"><span id="S4.T1.1.9.6.9.1" class="ltx_text ltx_font_bold">98.80</span></td>
<td id="S4.T1.1.9.6.10" class="ltx_td ltx_align_center">98.50</td>
<td id="S4.T1.1.9.6.11" class="ltx_td ltx_align_center">95.00</td>
</tr>
<tr id="S4.T1.1.10.7" class="ltx_tr">
<th id="S4.T1.1.10.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">duck</th>
<td id="S4.T1.1.10.7.2" class="ltx_td ltx_align_center">52.58</td>
<td id="S4.T1.1.10.7.3" class="ltx_td ltx_align_center">43.80</td>
<td id="S4.T1.1.10.7.4" class="ltx_td ltx_align_center">66.01</td>
<td id="S4.T1.1.10.7.5" class="ltx_td ltx_align_center">66.76</td>
<td id="S4.T1.1.10.7.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T1.1.10.7.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.1.10.7.7.1" class="ltx_text ltx_font_bold">85.35</span></td>
<td id="S4.T1.1.10.7.8" class="ltx_td ltx_align_center">66.00</td>
<td id="S4.T1.1.10.7.9" class="ltx_td ltx_align_center"><span id="S4.T1.1.10.7.9.1" class="ltx_text ltx_font_bold">86.30</span></td>
<td id="S4.T1.1.10.7.10" class="ltx_td ltx_align_center">65.00</td>
<td id="S4.T1.1.10.7.11" class="ltx_td ltx_align_center">77.70</td>
</tr>
<tr id="S4.T1.1.11.8" class="ltx_tr">
<th id="S4.T1.1.11.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">eggbox*</th>
<td id="S4.T1.1.11.8.2" class="ltx_td ltx_align_center">99.15</td>
<td id="S4.T1.1.11.8.3" class="ltx_td ltx_align_center">96.80</td>
<td id="S4.T1.1.11.8.4" class="ltx_td ltx_align_center">99.72</td>
<td id="S4.T1.1.11.8.5" class="ltx_td ltx_align_center">99.72</td>
<td id="S4.T1.1.11.8.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T1.1.11.8.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.1.11.8.7.1" class="ltx_text ltx_font_bold">100.00</span></td>
<td id="S4.T1.1.11.8.8" class="ltx_td ltx_align_center"><span id="S4.T1.1.11.8.8.1" class="ltx_text ltx_font_bold">100.00</span></td>
<td id="S4.T1.1.11.8.9" class="ltx_td ltx_align_center">99.90</td>
<td id="S4.T1.1.11.8.10" class="ltx_td ltx_align_center"><span id="S4.T1.1.11.8.10.1" class="ltx_text ltx_font_bold">100.00</span></td>
<td id="S4.T1.1.11.8.11" class="ltx_td ltx_align_center">97.10</td>
</tr>
<tr id="S4.T1.1.12.9" class="ltx_tr">
<th id="S4.T1.1.12.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">glue*</th>
<td id="S4.T1.1.12.9.2" class="ltx_td ltx_align_center">95.66</td>
<td id="S4.T1.1.12.9.3" class="ltx_td ltx_align_center">79.40</td>
<td id="S4.T1.1.12.9.4" class="ltx_td ltx_align_center">93.83</td>
<td id="S4.T1.1.12.9.5" class="ltx_td ltx_align_center"><span id="S4.T1.1.12.9.5.1" class="ltx_text ltx_font_bold">99.61</span></td>
<td id="S4.T1.1.12.9.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T1.1.12.9.7" class="ltx_td ltx_align_center ltx_border_r">99.42</td>
<td id="S4.T1.1.12.9.8" class="ltx_td ltx_align_center"><span id="S4.T1.1.12.9.8.1" class="ltx_text ltx_font_bold">100.00</span></td>
<td id="S4.T1.1.12.9.9" class="ltx_td ltx_align_center">96.80</td>
<td id="S4.T1.1.12.9.10" class="ltx_td ltx_align_center">98.80</td>
<td id="S4.T1.1.12.9.11" class="ltx_td ltx_align_center">99.40</td>
</tr>
<tr id="S4.T1.1.13.10" class="ltx_tr">
<th id="S4.T1.1.13.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">holepuncher</th>
<td id="S4.T1.1.13.10.2" class="ltx_td ltx_align_center">81.92</td>
<td id="S4.T1.1.13.10.3" class="ltx_td ltx_align_center">74.80</td>
<td id="S4.T1.1.13.10.4" class="ltx_td ltx_align_center">65.83</td>
<td id="S4.T1.1.13.10.5" class="ltx_td ltx_align_center">85.82</td>
<td id="S4.T1.1.13.10.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T1.1.13.10.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.1.13.10.7.1" class="ltx_text ltx_font_bold">90.39</span></td>
<td id="S4.T1.1.13.10.8" class="ltx_td ltx_align_center">49.00</td>
<td id="S4.T1.1.13.10.9" class="ltx_td ltx_align_center">86.90</td>
<td id="S4.T1.1.13.10.10" class="ltx_td ltx_align_center"><span id="S4.T1.1.13.10.10.1" class="ltx_text ltx_font_bold">89.70</span></td>
<td id="S4.T1.1.13.10.11" class="ltx_td ltx_align_center">52.80</td>
</tr>
<tr id="S4.T1.1.14.11" class="ltx_tr">
<th id="S4.T1.1.14.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">iron</th>
<td id="S4.T1.1.14.11.2" class="ltx_td ltx_align_center">98.88</td>
<td id="S4.T1.1.14.11.3" class="ltx_td ltx_align_center">83.40</td>
<td id="S4.T1.1.14.11.4" class="ltx_td ltx_align_center">99.80</td>
<td id="S4.T1.1.14.11.5" class="ltx_td ltx_align_center">97.85</td>
<td id="S4.T1.1.14.11.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T1.1.14.11.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.1.14.11.7.1" class="ltx_text ltx_font_bold">100.00</span></td>
<td id="S4.T1.1.14.11.8" class="ltx_td ltx_align_center">78.00</td>
<td id="S4.T1.1.14.11.9" class="ltx_td ltx_align_center"><span id="S4.T1.1.14.11.9.1" class="ltx_text ltx_font_bold">100.00</span></td>
<td id="S4.T1.1.14.11.10" class="ltx_td ltx_align_center"><span id="S4.T1.1.14.11.10.1" class="ltx_text ltx_font_bold">100.00</span></td>
<td id="S4.T1.1.14.11.11" class="ltx_td ltx_align_center">98.30</td>
</tr>
<tr id="S4.T1.1.15.12" class="ltx_tr">
<th id="S4.T1.1.15.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">lamp</th>
<td id="S4.T1.1.15.12.2" class="ltx_td ltx_align_center">99.33</td>
<td id="S4.T1.1.15.12.3" class="ltx_td ltx_align_center">82.00</td>
<td id="S4.T1.1.15.12.4" class="ltx_td ltx_align_center">88.11</td>
<td id="S4.T1.1.15.12.5" class="ltx_td ltx_align_center">97.89</td>
<td id="S4.T1.1.15.12.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T1.1.15.12.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.1.15.12.7.1" class="ltx_text ltx_font_bold">99.42</span></td>
<td id="S4.T1.1.15.12.8" class="ltx_td ltx_align_center">73.00</td>
<td id="S4.T1.1.15.12.9" class="ltx_td ltx_align_center">96.80</td>
<td id="S4.T1.1.15.12.10" class="ltx_td ltx_align_center"><span id="S4.T1.1.15.12.10.1" class="ltx_text ltx_font_bold">99.50</span></td>
<td id="S4.T1.1.15.12.11" class="ltx_td ltx_align_center">97.50</td>
</tr>
<tr id="S4.T1.1.16.13" class="ltx_tr">
<th id="S4.T1.1.16.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">phone</th>
<td id="S4.T1.1.16.13.2" class="ltx_td ltx_align_center">92.41</td>
<td id="S4.T1.1.16.13.3" class="ltx_td ltx_align_center">45.00</td>
<td id="S4.T1.1.16.13.4" class="ltx_td ltx_align_center">74.24</td>
<td id="S4.T1.1.16.13.5" class="ltx_td ltx_align_center">90.75</td>
<td id="S4.T1.1.16.13.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T1.1.16.13.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.1.16.13.7.1" class="ltx_text ltx_font_bold">97.64</span></td>
<td id="S4.T1.1.16.13.8" class="ltx_td ltx_align_center">79.00</td>
<td id="S4.T1.1.16.13.9" class="ltx_td ltx_align_center">94.70</td>
<td id="S4.T1.1.16.13.10" class="ltx_td ltx_align_center"><span id="S4.T1.1.16.13.10.1" class="ltx_text ltx_font_bold">94.90</span></td>
<td id="S4.T1.1.16.13.11" class="ltx_td ltx_align_center">87.70</td>
</tr>
<tr id="S4.T1.1.17.14" class="ltx_tr">
<th id="S4.T1.1.17.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">average</th>
<td id="S4.T1.1.17.14.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">86.27</td>
<td id="S4.T1.1.17.14.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">72.38</td>
<td id="S4.T1.1.17.14.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">82.98</td>
<td id="S4.T1.1.17.14.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">89.86</td>
<td id="S4.T1.1.17.14.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">93.70</td>
<td id="S4.T1.1.17.14.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.1.17.14.7.1" class="ltx_text ltx_font_bold">95.61</span></td>
<td id="S4.T1.1.17.14.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">76.69</td>
<td id="S4.T1.1.17.14.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.1.17.14.9.1" class="ltx_text ltx_font_bold">95.15</span></td>
<td id="S4.T1.1.17.14.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">91.36</td>
<td id="S4.T1.1.17.14.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">88.60</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>
Test accuracy on the LINEMOD dataset in terms of the ADD(-S) metric. Objects with a â€œ*â€ sign are considered as symmetric objects and the ADD-S metric is used. The result of SSD-6D is obtained from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>. The result of HybridPose is from its fourth version update inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>.</figcaption>
</figure>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" rowspan="3"><span id="S4.T2.1.1.1.1.1" class="ltx_text">ADD(-S)</span></th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="8">Without refinement</th>
<th id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2">With refinement</th>
</tr>
<tr id="S4.T2.1.2.2" class="ltx_tr">
<th id="S4.T2.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">HM</th>
<th id="S4.T2.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">PVNet</th>
<th id="S4.T2.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Hu</th>
<th id="S4.T2.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Pix2Pose</th>
<th id="S4.T2.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">DPOD</th>
<th id="S4.T2.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Hu2</th>
<th id="S4.T2.1.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">GDR</th>
<th id="S4.T2.1.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Ours</th>
<th id="S4.T2.1.2.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">DPOD+</th>
<th id="S4.T2.1.2.2.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">HybridPose</th>
</tr>
<tr id="S4.T2.1.3.3" class="ltx_tr">
<th id="S4.T2.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_column"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite></th>
<th id="S4.T2.1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_column"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite></th>
<th id="S4.T2.1.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite></th>
<th id="S4.T2.1.3.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_column"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite></th>
<th id="S4.T2.1.3.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_column"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite></th>
<th id="S4.T2.1.3.3.6" class="ltx_td ltx_align_center ltx_th ltx_th_column"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite></th>
<th id="S4.T2.1.3.3.7" class="ltx_td ltx_align_center ltx_th ltx_th_column"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite></th>
<th id="S4.T2.1.3.3.8" class="ltx_td ltx_th ltx_th_column ltx_border_r"></th>
<th id="S4.T2.1.3.3.9" class="ltx_td ltx_align_center ltx_th ltx_th_column"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite></th>
<th id="S4.T2.1.3.3.10" class="ltx_td ltx_align_center ltx_th ltx_th_column"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.4.1" class="ltx_tr">
<td id="S4.T2.1.4.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">ape</td>
<td id="S4.T2.1.4.1.2" class="ltx_td ltx_align_center ltx_border_t">15.30</td>
<td id="S4.T2.1.4.1.3" class="ltx_td ltx_align_center ltx_border_t">15.81</td>
<td id="S4.T2.1.4.1.4" class="ltx_td ltx_align_center ltx_border_t">12.10</td>
<td id="S4.T2.1.4.1.5" class="ltx_td ltx_align_center ltx_border_t">22.00</td>
<td id="S4.T2.1.4.1.6" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T2.1.4.1.7" class="ltx_td ltx_align_center ltx_border_t">19.20</td>
<td id="S4.T2.1.4.1.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.4.1.8.1" class="ltx_text ltx_font_bold">39.30</span></td>
<td id="S4.T2.1.4.1.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">28.03</td>
<td id="S4.T2.1.4.1.10" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T2.1.4.1.11" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.4.1.11.1" class="ltx_text ltx_font_bold">20.90</span></td>
</tr>
<tr id="S4.T2.1.5.2" class="ltx_tr">
<td id="S4.T2.1.5.2.1" class="ltx_td ltx_align_left ltx_border_r">can</td>
<td id="S4.T2.1.5.2.2" class="ltx_td ltx_align_center">44.70</td>
<td id="S4.T2.1.5.2.3" class="ltx_td ltx_align_center">63.30</td>
<td id="S4.T2.1.5.2.4" class="ltx_td ltx_align_center">39.90</td>
<td id="S4.T2.1.5.2.5" class="ltx_td ltx_align_center">44.70</td>
<td id="S4.T2.1.5.2.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.1.5.2.7" class="ltx_td ltx_align_center">65.10</td>
<td id="S4.T2.1.5.2.8" class="ltx_td ltx_align_center"><span id="S4.T2.1.5.2.8.1" class="ltx_text ltx_font_bold">79.20</span></td>
<td id="S4.T2.1.5.2.9" class="ltx_td ltx_align_center ltx_border_r">75.06</td>
<td id="S4.T2.1.5.2.10" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.1.5.2.11" class="ltx_td ltx_align_center"><span id="S4.T2.1.5.2.11.1" class="ltx_text ltx_font_bold">75.30</span></td>
</tr>
<tr id="S4.T2.1.6.3" class="ltx_tr">
<td id="S4.T2.1.6.3.1" class="ltx_td ltx_align_left ltx_border_r">cat</td>
<td id="S4.T2.1.6.3.2" class="ltx_td ltx_align_center">9.33</td>
<td id="S4.T2.1.6.3.3" class="ltx_td ltx_align_center">16.68</td>
<td id="S4.T2.1.6.3.4" class="ltx_td ltx_align_center">8.20</td>
<td id="S4.T2.1.6.3.5" class="ltx_td ltx_align_center">22.70</td>
<td id="S4.T2.1.6.3.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.1.6.3.7" class="ltx_td ltx_align_center">18.90</td>
<td id="S4.T2.1.6.3.8" class="ltx_td ltx_align_center">23.50</td>
<td id="S4.T2.1.6.3.9" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.1.6.3.9.1" class="ltx_text ltx_font_bold">25.53</span></td>
<td id="S4.T2.1.6.3.10" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.1.6.3.11" class="ltx_td ltx_align_center"><span id="S4.T2.1.6.3.11.1" class="ltx_text ltx_font_bold">24.90</span></td>
</tr>
<tr id="S4.T2.1.7.4" class="ltx_tr">
<td id="S4.T2.1.7.4.1" class="ltx_td ltx_align_left ltx_border_r">driller</td>
<td id="S4.T2.1.7.4.2" class="ltx_td ltx_align_center">55.40</td>
<td id="S4.T2.1.7.4.3" class="ltx_td ltx_align_center">65.65</td>
<td id="S4.T2.1.7.4.4" class="ltx_td ltx_align_center">45.20</td>
<td id="S4.T2.1.7.4.5" class="ltx_td ltx_align_center">44.70</td>
<td id="S4.T2.1.7.4.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.1.7.4.7" class="ltx_td ltx_align_center">69.00</td>
<td id="S4.T2.1.7.4.8" class="ltx_td ltx_align_center"><span id="S4.T2.1.7.4.8.1" class="ltx_text ltx_font_bold">71.30</span></td>
<td id="S4.T2.1.7.4.9" class="ltx_td ltx_align_center ltx_border_r">61.86</td>
<td id="S4.T2.1.7.4.10" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.1.7.4.11" class="ltx_td ltx_align_center"><span id="S4.T2.1.7.4.11.1" class="ltx_text ltx_font_bold">70.20</span></td>
</tr>
<tr id="S4.T2.1.8.5" class="ltx_tr">
<td id="S4.T2.1.8.5.1" class="ltx_td ltx_align_left ltx_border_r">duck</td>
<td id="S4.T2.1.8.5.2" class="ltx_td ltx_align_center">19.60</td>
<td id="S4.T2.1.8.5.3" class="ltx_td ltx_align_center">25.24</td>
<td id="S4.T2.1.8.5.4" class="ltx_td ltx_align_center">17.20</td>
<td id="S4.T2.1.8.5.5" class="ltx_td ltx_align_center">15.00</td>
<td id="S4.T2.1.8.5.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.1.8.5.7" class="ltx_td ltx_align_center">25.30</td>
<td id="S4.T2.1.8.5.8" class="ltx_td ltx_align_center"><span id="S4.T2.1.8.5.8.1" class="ltx_text ltx_font_bold">44.40</span></td>
<td id="S4.T2.1.8.5.9" class="ltx_td ltx_align_center ltx_border_r">19.07</td>
<td id="S4.T2.1.8.5.10" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.1.8.5.11" class="ltx_td ltx_align_center"><span id="S4.T2.1.8.5.11.1" class="ltx_text ltx_font_bold">27.90</span></td>
</tr>
<tr id="S4.T2.1.9.6" class="ltx_tr">
<td id="S4.T2.1.9.6.1" class="ltx_td ltx_align_left ltx_border_r">eggbox*</td>
<td id="S4.T2.1.9.6.2" class="ltx_td ltx_align_center">23.00</td>
<td id="S4.T2.1.9.6.3" class="ltx_td ltx_align_center">50.17</td>
<td id="S4.T2.1.9.6.4" class="ltx_td ltx_align_center">22.10</td>
<td id="S4.T2.1.9.6.5" class="ltx_td ltx_align_center">25.20</td>
<td id="S4.T2.1.9.6.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.1.9.6.7" class="ltx_td ltx_align_center">52.00</td>
<td id="S4.T2.1.9.6.8" class="ltx_td ltx_align_center"><span id="S4.T2.1.9.6.8.1" class="ltx_text ltx_font_bold">58.20</span></td>
<td id="S4.T2.1.9.6.9" class="ltx_td ltx_align_center ltx_border_r">45.62</td>
<td id="S4.T2.1.9.6.10" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.1.9.6.11" class="ltx_td ltx_align_center"><span id="S4.T2.1.9.6.11.1" class="ltx_text ltx_font_bold">52.40</span></td>
</tr>
<tr id="S4.T2.1.10.7" class="ltx_tr">
<td id="S4.T2.1.10.7.1" class="ltx_td ltx_align_left ltx_border_r">glue*</td>
<td id="S4.T2.1.10.7.2" class="ltx_td ltx_align_center">41.40</td>
<td id="S4.T2.1.10.7.3" class="ltx_td ltx_align_center">49.62</td>
<td id="S4.T2.1.10.7.4" class="ltx_td ltx_align_center">35.80</td>
<td id="S4.T2.1.10.7.5" class="ltx_td ltx_align_center">32.40</td>
<td id="S4.T2.1.10.7.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.1.10.7.7" class="ltx_td ltx_align_center">51.40</td>
<td id="S4.T2.1.10.7.8" class="ltx_td ltx_align_center">49.30</td>
<td id="S4.T2.1.10.7.9" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.1.10.7.9.1" class="ltx_text ltx_font_bold">56.92</span></td>
<td id="S4.T2.1.10.7.10" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.1.10.7.11" class="ltx_td ltx_align_center"><span id="S4.T2.1.10.7.11.1" class="ltx_text ltx_font_bold">53.80</span></td>
</tr>
<tr id="S4.T2.1.11.8" class="ltx_tr">
<td id="S4.T2.1.11.8.1" class="ltx_td ltx_align_left ltx_border_r">holepuncher</td>
<td id="S4.T2.1.11.8.2" class="ltx_td ltx_align_center">20.40</td>
<td id="S4.T2.1.11.8.3" class="ltx_td ltx_align_center">39.67</td>
<td id="S4.T2.1.11.8.4" class="ltx_td ltx_align_center">36.00</td>
<td id="S4.T2.1.11.8.5" class="ltx_td ltx_align_center">49.50</td>
<td id="S4.T2.1.11.8.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.1.11.8.7" class="ltx_td ltx_align_center">45.60</td>
<td id="S4.T2.1.11.8.8" class="ltx_td ltx_align_center"><span id="S4.T2.1.11.8.8.1" class="ltx_text ltx_font_bold">58.70</span></td>
<td id="S4.T2.1.11.8.9" class="ltx_td ltx_align_center ltx_border_r">55.54</td>
<td id="S4.T2.1.11.8.10" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.1.11.8.11" class="ltx_td ltx_align_center"><span id="S4.T2.1.11.8.11.1" class="ltx_text ltx_font_bold">54.20</span></td>
</tr>
<tr id="S4.T2.1.12.9" class="ltx_tr">
<td id="S4.T2.1.12.9.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">average</td>
<td id="S4.T2.1.12.9.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">28.64</td>
<td id="S4.T2.1.12.9.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">40.77</td>
<td id="S4.T2.1.12.9.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">27.06</td>
<td id="S4.T2.1.12.9.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">32.03</td>
<td id="S4.T2.1.12.9.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">32.80</td>
<td id="S4.T2.1.12.9.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">43.30</td>
<td id="S4.T2.1.12.9.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T2.1.12.9.8.1" class="ltx_text ltx_font_bold">53.00</span></td>
<td id="S4.T2.1.12.9.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">45.95</td>
<td id="S4.T2.1.12.9.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">47.30</td>
<td id="S4.T2.1.12.9.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T2.1.12.9.11.1" class="ltx_text ltx_font_bold">47.45</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>
Test accuracy on the Occluded-LINEMOD dataset in terms of the ADD(-S) metric. Objects with a â€œ*â€ sign are considered as symmetric objects and the ADD-S metric is used. The result of HybridPose is from its fourth version update inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>.</figcaption>
</figure>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Accuracy and coherence of landmarks</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.6" class="ltx_p">To formally analyse the effect of holistic representation learning, we quantify accuracy and structural consistency of landmark predictions and compare them when trained with and without MPS. For accuracy, we define</p>
<table id="S4.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E3.m1.1" class="ltx_Math" alttext="\bm{r}_{i}=\lVert\bm{x}_{i}-\bm{x}_{i}^{*}\rVert_{2}" display="block"><semantics id="S4.E3.m1.1a"><mrow id="S4.E3.m1.1.1" xref="S4.E3.m1.1.1.cmml"><msub id="S4.E3.m1.1.1.3" xref="S4.E3.m1.1.1.3.cmml"><mi id="S4.E3.m1.1.1.3.2" xref="S4.E3.m1.1.1.3.2.cmml">ğ’“</mi><mi id="S4.E3.m1.1.1.3.3" xref="S4.E3.m1.1.1.3.3.cmml">i</mi></msub><mo rspace="0.1389em" id="S4.E3.m1.1.1.2" xref="S4.E3.m1.1.1.2.cmml">=</mo><msub id="S4.E3.m1.1.1.1" xref="S4.E3.m1.1.1.1.cmml"><mrow id="S4.E3.m1.1.1.1.1.1" xref="S4.E3.m1.1.1.1.1.2.cmml"><mo fence="true" lspace="0.1389em" rspace="0em" id="S4.E3.m1.1.1.1.1.1.2" xref="S4.E3.m1.1.1.1.1.2.1.cmml">âˆ¥</mo><mrow id="S4.E3.m1.1.1.1.1.1.1" xref="S4.E3.m1.1.1.1.1.1.1.cmml"><msub id="S4.E3.m1.1.1.1.1.1.1.2" xref="S4.E3.m1.1.1.1.1.1.1.2.cmml"><mi id="S4.E3.m1.1.1.1.1.1.1.2.2" xref="S4.E3.m1.1.1.1.1.1.1.2.2.cmml">ğ’™</mi><mi id="S4.E3.m1.1.1.1.1.1.1.2.3" xref="S4.E3.m1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S4.E3.m1.1.1.1.1.1.1.1" xref="S4.E3.m1.1.1.1.1.1.1.1.cmml">âˆ’</mo><msubsup id="S4.E3.m1.1.1.1.1.1.1.3" xref="S4.E3.m1.1.1.1.1.1.1.3.cmml"><mi id="S4.E3.m1.1.1.1.1.1.1.3.2.2" xref="S4.E3.m1.1.1.1.1.1.1.3.2.2.cmml">ğ’™</mi><mi id="S4.E3.m1.1.1.1.1.1.1.3.2.3" xref="S4.E3.m1.1.1.1.1.1.1.3.2.3.cmml">i</mi><mo id="S4.E3.m1.1.1.1.1.1.1.3.3" xref="S4.E3.m1.1.1.1.1.1.1.3.3.cmml">âˆ—</mo></msubsup></mrow><mo fence="true" lspace="0em" id="S4.E3.m1.1.1.1.1.1.3" xref="S4.E3.m1.1.1.1.1.2.1.cmml">âˆ¥</mo></mrow><mn id="S4.E3.m1.1.1.1.3" xref="S4.E3.m1.1.1.1.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m1.1b"><apply id="S4.E3.m1.1.1.cmml" xref="S4.E3.m1.1.1"><eq id="S4.E3.m1.1.1.2.cmml" xref="S4.E3.m1.1.1.2"></eq><apply id="S4.E3.m1.1.1.3.cmml" xref="S4.E3.m1.1.1.3"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.3.1.cmml" xref="S4.E3.m1.1.1.3">subscript</csymbol><ci id="S4.E3.m1.1.1.3.2.cmml" xref="S4.E3.m1.1.1.3.2">ğ’“</ci><ci id="S4.E3.m1.1.1.3.3.cmml" xref="S4.E3.m1.1.1.3.3">ğ‘–</ci></apply><apply id="S4.E3.m1.1.1.1.cmml" xref="S4.E3.m1.1.1.1"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.1.2.cmml" xref="S4.E3.m1.1.1.1">subscript</csymbol><apply id="S4.E3.m1.1.1.1.1.2.cmml" xref="S4.E3.m1.1.1.1.1.1"><csymbol cd="latexml" id="S4.E3.m1.1.1.1.1.2.1.cmml" xref="S4.E3.m1.1.1.1.1.1.2">delimited-âˆ¥âˆ¥</csymbol><apply id="S4.E3.m1.1.1.1.1.1.1.cmml" xref="S4.E3.m1.1.1.1.1.1.1"><minus id="S4.E3.m1.1.1.1.1.1.1.1.cmml" xref="S4.E3.m1.1.1.1.1.1.1.1"></minus><apply id="S4.E3.m1.1.1.1.1.1.1.2.cmml" xref="S4.E3.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.E3.m1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.E3.m1.1.1.1.1.1.1.2.2.cmml" xref="S4.E3.m1.1.1.1.1.1.1.2.2">ğ’™</ci><ci id="S4.E3.m1.1.1.1.1.1.1.2.3.cmml" xref="S4.E3.m1.1.1.1.1.1.1.2.3">ğ‘–</ci></apply><apply id="S4.E3.m1.1.1.1.1.1.1.3.cmml" xref="S4.E3.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.1.1.1.1.3.1.cmml" xref="S4.E3.m1.1.1.1.1.1.1.3">superscript</csymbol><apply id="S4.E3.m1.1.1.1.1.1.1.3.2.cmml" xref="S4.E3.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.1.1.1.1.3.2.1.cmml" xref="S4.E3.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E3.m1.1.1.1.1.1.1.3.2.2.cmml" xref="S4.E3.m1.1.1.1.1.1.1.3.2.2">ğ’™</ci><ci id="S4.E3.m1.1.1.1.1.1.1.3.2.3.cmml" xref="S4.E3.m1.1.1.1.1.1.1.3.2.3">ğ‘–</ci></apply><times id="S4.E3.m1.1.1.1.1.1.1.3.3.cmml" xref="S4.E3.m1.1.1.1.1.1.1.3.3"></times></apply></apply></apply><cn type="integer" id="S4.E3.m1.1.1.1.3.cmml" xref="S4.E3.m1.1.1.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m1.1c">\bm{r}_{i}=\lVert\bm{x}_{i}-\bm{x}_{i}^{*}\rVert_{2}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S4.SS3.SSS2.p1.1" class="ltx_p">as the prediction residual of a 2D landmark <math id="S4.SS3.SSS2.p1.1.m1.1" class="ltx_Math" alttext="\bm{x}_{i}" display="inline"><semantics id="S4.SS3.SSS2.p1.1.m1.1a"><msub id="S4.SS3.SSS2.p1.1.m1.1.1" xref="S4.SS3.SSS2.p1.1.m1.1.1.cmml"><mi id="S4.SS3.SSS2.p1.1.m1.1.1.2" xref="S4.SS3.SSS2.p1.1.m1.1.1.2.cmml">ğ’™</mi><mi id="S4.SS3.SSS2.p1.1.m1.1.1.3" xref="S4.SS3.SSS2.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p1.1.m1.1b"><apply id="S4.SS3.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS3.SSS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS2.p1.1.m1.1.1.1.cmml" xref="S4.SS3.SSS2.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.SSS2.p1.1.m1.1.1.2.cmml" xref="S4.SS3.SSS2.p1.1.m1.1.1.2">ğ’™</ci><ci id="S4.SS3.SSS2.p1.1.m1.1.1.3.cmml" xref="S4.SS3.SSS2.p1.1.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p1.1.m1.1c">\bm{x}_{i}</annotation></semantics></math>. We also define a measure of incoherence</p>
<table id="S4.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E4.m1.1" class="ltx_Math" alttext="c_{i}=\lVert(\bm{x}_{i}-\bm{x}_{i}^{*})-\bm{m}\rVert_{2}" display="block"><semantics id="S4.E4.m1.1a"><mrow id="S4.E4.m1.1.1" xref="S4.E4.m1.1.1.cmml"><msub id="S4.E4.m1.1.1.3" xref="S4.E4.m1.1.1.3.cmml"><mi id="S4.E4.m1.1.1.3.2" xref="S4.E4.m1.1.1.3.2.cmml">c</mi><mi id="S4.E4.m1.1.1.3.3" xref="S4.E4.m1.1.1.3.3.cmml">i</mi></msub><mo rspace="0.1389em" id="S4.E4.m1.1.1.2" xref="S4.E4.m1.1.1.2.cmml">=</mo><msub id="S4.E4.m1.1.1.1" xref="S4.E4.m1.1.1.1.cmml"><mrow id="S4.E4.m1.1.1.1.1.1" xref="S4.E4.m1.1.1.1.1.2.cmml"><mo fence="true" lspace="0.1389em" rspace="0em" id="S4.E4.m1.1.1.1.1.1.2" xref="S4.E4.m1.1.1.1.1.2.1.cmml">âˆ¥</mo><mrow id="S4.E4.m1.1.1.1.1.1.1" xref="S4.E4.m1.1.1.1.1.1.1.cmml"><mrow id="S4.E4.m1.1.1.1.1.1.1.1.1" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E4.m1.1.1.1.1.1.1.1.1.2" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E4.m1.1.1.1.1.1.1.1.1.1" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S4.E4.m1.1.1.1.1.1.1.1.1.1.2" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S4.E4.m1.1.1.1.1.1.1.1.1.1.2.2" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.2.2.cmml">ğ’™</mi><mi id="S4.E4.m1.1.1.1.1.1.1.1.1.1.2.3" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><msubsup id="S4.E4.m1.1.1.1.1.1.1.1.1.1.3" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E4.m1.1.1.1.1.1.1.1.1.1.3.2.2" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.3.2.2.cmml">ğ’™</mi><mi id="S4.E4.m1.1.1.1.1.1.1.1.1.1.3.2.3" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.3.2.3.cmml">i</mi><mo id="S4.E4.m1.1.1.1.1.1.1.1.1.1.3.3" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.3.3.cmml">âˆ—</mo></msubsup></mrow><mo stretchy="false" id="S4.E4.m1.1.1.1.1.1.1.1.1.3" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S4.E4.m1.1.1.1.1.1.1.2" xref="S4.E4.m1.1.1.1.1.1.1.2.cmml">âˆ’</mo><mi id="S4.E4.m1.1.1.1.1.1.1.3" xref="S4.E4.m1.1.1.1.1.1.1.3.cmml">ğ’</mi></mrow><mo fence="true" lspace="0em" id="S4.E4.m1.1.1.1.1.1.3" xref="S4.E4.m1.1.1.1.1.2.1.cmml">âˆ¥</mo></mrow><mn id="S4.E4.m1.1.1.1.3" xref="S4.E4.m1.1.1.1.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.E4.m1.1b"><apply id="S4.E4.m1.1.1.cmml" xref="S4.E4.m1.1.1"><eq id="S4.E4.m1.1.1.2.cmml" xref="S4.E4.m1.1.1.2"></eq><apply id="S4.E4.m1.1.1.3.cmml" xref="S4.E4.m1.1.1.3"><csymbol cd="ambiguous" id="S4.E4.m1.1.1.3.1.cmml" xref="S4.E4.m1.1.1.3">subscript</csymbol><ci id="S4.E4.m1.1.1.3.2.cmml" xref="S4.E4.m1.1.1.3.2">ğ‘</ci><ci id="S4.E4.m1.1.1.3.3.cmml" xref="S4.E4.m1.1.1.3.3">ğ‘–</ci></apply><apply id="S4.E4.m1.1.1.1.cmml" xref="S4.E4.m1.1.1.1"><csymbol cd="ambiguous" id="S4.E4.m1.1.1.1.2.cmml" xref="S4.E4.m1.1.1.1">subscript</csymbol><apply id="S4.E4.m1.1.1.1.1.2.cmml" xref="S4.E4.m1.1.1.1.1.1"><csymbol cd="latexml" id="S4.E4.m1.1.1.1.1.2.1.cmml" xref="S4.E4.m1.1.1.1.1.1.2">delimited-âˆ¥âˆ¥</csymbol><apply id="S4.E4.m1.1.1.1.1.1.1.cmml" xref="S4.E4.m1.1.1.1.1.1.1"><minus id="S4.E4.m1.1.1.1.1.1.1.2.cmml" xref="S4.E4.m1.1.1.1.1.1.1.2"></minus><apply id="S4.E4.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1"><minus id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1"></minus><apply id="S4.E4.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E4.m1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.E4.m1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.2.2">ğ’™</ci><ci id="S4.E4.m1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.2.3">ğ‘–</ci></apply><apply id="S4.E4.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E4.m1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.3">superscript</csymbol><apply id="S4.E4.m1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E4.m1.1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E4.m1.1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.3.2.2">ğ’™</ci><ci id="S4.E4.m1.1.1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.3.2.3">ğ‘–</ci></apply><times id="S4.E4.m1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.3.3"></times></apply></apply><ci id="S4.E4.m1.1.1.1.1.1.1.3.cmml" xref="S4.E4.m1.1.1.1.1.1.1.3">ğ’</ci></apply></apply><cn type="integer" id="S4.E4.m1.1.1.1.3.cmml" xref="S4.E4.m1.1.1.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E4.m1.1c">c_{i}=\lVert(\bm{x}_{i}-\bm{x}_{i}^{*})-\bm{m}\rVert_{2}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S4.SS3.SSS2.p1.5" class="ltx_p">for a landmark prediction <math id="S4.SS3.SSS2.p1.2.m1.1" class="ltx_Math" alttext="\bm{x}_{i}" display="inline"><semantics id="S4.SS3.SSS2.p1.2.m1.1a"><msub id="S4.SS3.SSS2.p1.2.m1.1.1" xref="S4.SS3.SSS2.p1.2.m1.1.1.cmml"><mi id="S4.SS3.SSS2.p1.2.m1.1.1.2" xref="S4.SS3.SSS2.p1.2.m1.1.1.2.cmml">ğ’™</mi><mi id="S4.SS3.SSS2.p1.2.m1.1.1.3" xref="S4.SS3.SSS2.p1.2.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p1.2.m1.1b"><apply id="S4.SS3.SSS2.p1.2.m1.1.1.cmml" xref="S4.SS3.SSS2.p1.2.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS2.p1.2.m1.1.1.1.cmml" xref="S4.SS3.SSS2.p1.2.m1.1.1">subscript</csymbol><ci id="S4.SS3.SSS2.p1.2.m1.1.1.2.cmml" xref="S4.SS3.SSS2.p1.2.m1.1.1.2">ğ’™</ci><ci id="S4.SS3.SSS2.p1.2.m1.1.1.3.cmml" xref="S4.SS3.SSS2.p1.2.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p1.2.m1.1c">\bm{x}_{i}</annotation></semantics></math> where <math id="S4.SS3.SSS2.p1.3.m2.1" class="ltx_Math" alttext="\bm{m}=\frac{1}{n}\sum_{i=1}^{n}(\bm{x}_{i}-\bm{x}_{i}^{*})" display="inline"><semantics id="S4.SS3.SSS2.p1.3.m2.1a"><mrow id="S4.SS3.SSS2.p1.3.m2.1.1" xref="S4.SS3.SSS2.p1.3.m2.1.1.cmml"><mi id="S4.SS3.SSS2.p1.3.m2.1.1.3" xref="S4.SS3.SSS2.p1.3.m2.1.1.3.cmml">ğ’</mi><mo id="S4.SS3.SSS2.p1.3.m2.1.1.2" xref="S4.SS3.SSS2.p1.3.m2.1.1.2.cmml">=</mo><mrow id="S4.SS3.SSS2.p1.3.m2.1.1.1" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.cmml"><mfrac id="S4.SS3.SSS2.p1.3.m2.1.1.1.3" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.3.cmml"><mn id="S4.SS3.SSS2.p1.3.m2.1.1.1.3.2" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.3.2.cmml">1</mn><mi id="S4.SS3.SSS2.p1.3.m2.1.1.1.3.3" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.3.3.cmml">n</mi></mfrac><mo lspace="0em" rspace="0em" id="S4.SS3.SSS2.p1.3.m2.1.1.1.2" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.2.cmml">â€‹</mo><mrow id="S4.SS3.SSS2.p1.3.m2.1.1.1.1" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.1.cmml"><msubsup id="S4.SS3.SSS2.p1.3.m2.1.1.1.1.2" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.1.2.cmml"><mo rspace="0em" id="S4.SS3.SSS2.p1.3.m2.1.1.1.1.2.2.2" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.1.2.2.2.cmml">âˆ‘</mo><mrow id="S4.SS3.SSS2.p1.3.m2.1.1.1.1.2.2.3" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.1.2.2.3.cmml"><mi id="S4.SS3.SSS2.p1.3.m2.1.1.1.1.2.2.3.2" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S4.SS3.SSS2.p1.3.m2.1.1.1.1.2.2.3.1" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S4.SS3.SSS2.p1.3.m2.1.1.1.1.2.2.3.3" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S4.SS3.SSS2.p1.3.m2.1.1.1.1.2.3" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.1.2.3.cmml">n</mi></msubsup><mrow id="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.2" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.cmml"><msub id="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.2" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.2.cmml"><mi id="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.2.2" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.2.2.cmml">ğ’™</mi><mi id="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.2.3" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.1" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><msubsup id="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.3" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.3.cmml"><mi id="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.3.2.2" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.3.2.2.cmml">ğ’™</mi><mi id="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.3.2.3" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.3.2.3.cmml">i</mi><mo id="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.3.3" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.3.3.cmml">âˆ—</mo></msubsup></mrow><mo stretchy="false" id="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.3" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p1.3.m2.1b"><apply id="S4.SS3.SSS2.p1.3.m2.1.1.cmml" xref="S4.SS3.SSS2.p1.3.m2.1.1"><eq id="S4.SS3.SSS2.p1.3.m2.1.1.2.cmml" xref="S4.SS3.SSS2.p1.3.m2.1.1.2"></eq><ci id="S4.SS3.SSS2.p1.3.m2.1.1.3.cmml" xref="S4.SS3.SSS2.p1.3.m2.1.1.3">ğ’</ci><apply id="S4.SS3.SSS2.p1.3.m2.1.1.1.cmml" xref="S4.SS3.SSS2.p1.3.m2.1.1.1"><times id="S4.SS3.SSS2.p1.3.m2.1.1.1.2.cmml" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.2"></times><apply id="S4.SS3.SSS2.p1.3.m2.1.1.1.3.cmml" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.3"><divide id="S4.SS3.SSS2.p1.3.m2.1.1.1.3.1.cmml" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.3"></divide><cn type="integer" id="S4.SS3.SSS2.p1.3.m2.1.1.1.3.2.cmml" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.3.2">1</cn><ci id="S4.SS3.SSS2.p1.3.m2.1.1.1.3.3.cmml" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.3.3">ğ‘›</ci></apply><apply id="S4.SS3.SSS2.p1.3.m2.1.1.1.1.cmml" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.1"><apply id="S4.SS3.SSS2.p1.3.m2.1.1.1.1.2.cmml" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.SS3.SSS2.p1.3.m2.1.1.1.1.2.1.cmml" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.1.2">superscript</csymbol><apply id="S4.SS3.SSS2.p1.3.m2.1.1.1.1.2.2.cmml" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.SS3.SSS2.p1.3.m2.1.1.1.1.2.2.1.cmml" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.1.2">subscript</csymbol><sum id="S4.SS3.SSS2.p1.3.m2.1.1.1.1.2.2.2.cmml" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.1.2.2.2"></sum><apply id="S4.SS3.SSS2.p1.3.m2.1.1.1.1.2.2.3.cmml" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.1.2.2.3"><eq id="S4.SS3.SSS2.p1.3.m2.1.1.1.1.2.2.3.1.cmml" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.1.2.2.3.1"></eq><ci id="S4.SS3.SSS2.p1.3.m2.1.1.1.1.2.2.3.2.cmml" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.1.2.2.3.2">ğ‘–</ci><cn type="integer" id="S4.SS3.SSS2.p1.3.m2.1.1.1.1.2.2.3.3.cmml" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S4.SS3.SSS2.p1.3.m2.1.1.1.1.2.3.cmml" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.1.2.3">ğ‘›</ci></apply><apply id="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.cmml" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1"><minus id="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.1.cmml" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.1"></minus><apply id="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.2.cmml" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.2.1.cmml" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.2.2.cmml" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.2.2">ğ’™</ci><ci id="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.2.3.cmml" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.2.3">ğ‘–</ci></apply><apply id="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.3.cmml" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.3.1.cmml" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.3">superscript</csymbol><apply id="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.3.2.cmml" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.3.2.1.cmml" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.3.2.2.cmml" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.3.2.2">ğ’™</ci><ci id="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.3.2.3.cmml" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.3.2.3">ğ‘–</ci></apply><times id="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.3.3.cmml" xref="S4.SS3.SSS2.p1.3.m2.1.1.1.1.1.1.1.3.3"></times></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p1.3.m2.1c">\bm{m}=\frac{1}{n}\sum_{i=1}^{n}(\bm{x}_{i}-\bm{x}_{i}^{*})</annotation></semantics></math> is the mean error vector for an image. The smaller <math id="S4.SS3.SSS2.p1.4.m3.1" class="ltx_Math" alttext="c_{i}" display="inline"><semantics id="S4.SS3.SSS2.p1.4.m3.1a"><msub id="S4.SS3.SSS2.p1.4.m3.1.1" xref="S4.SS3.SSS2.p1.4.m3.1.1.cmml"><mi id="S4.SS3.SSS2.p1.4.m3.1.1.2" xref="S4.SS3.SSS2.p1.4.m3.1.1.2.cmml">c</mi><mi id="S4.SS3.SSS2.p1.4.m3.1.1.3" xref="S4.SS3.SSS2.p1.4.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p1.4.m3.1b"><apply id="S4.SS3.SSS2.p1.4.m3.1.1.cmml" xref="S4.SS3.SSS2.p1.4.m3.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS2.p1.4.m3.1.1.1.cmml" xref="S4.SS3.SSS2.p1.4.m3.1.1">subscript</csymbol><ci id="S4.SS3.SSS2.p1.4.m3.1.1.2.cmml" xref="S4.SS3.SSS2.p1.4.m3.1.1.2">ğ‘</ci><ci id="S4.SS3.SSS2.p1.4.m3.1.1.3.cmml" xref="S4.SS3.SSS2.p1.4.m3.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p1.4.m3.1c">c_{i}</annotation></semantics></math> is, the more coherent a prediction <math id="S4.SS3.SSS2.p1.5.m4.1" class="ltx_Math" alttext="\bm{x}_{i}" display="inline"><semantics id="S4.SS3.SSS2.p1.5.m4.1a"><msub id="S4.SS3.SSS2.p1.5.m4.1.1" xref="S4.SS3.SSS2.p1.5.m4.1.1.cmml"><mi id="S4.SS3.SSS2.p1.5.m4.1.1.2" xref="S4.SS3.SSS2.p1.5.m4.1.1.2.cmml">ğ’™</mi><mi id="S4.SS3.SSS2.p1.5.m4.1.1.3" xref="S4.SS3.SSS2.p1.5.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p1.5.m4.1b"><apply id="S4.SS3.SSS2.p1.5.m4.1.1.cmml" xref="S4.SS3.SSS2.p1.5.m4.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS2.p1.5.m4.1.1.1.cmml" xref="S4.SS3.SSS2.p1.5.m4.1.1">subscript</csymbol><ci id="S4.SS3.SSS2.p1.5.m4.1.1.2.cmml" xref="S4.SS3.SSS2.p1.5.m4.1.1.2">ğ’™</ci><ci id="S4.SS3.SSS2.p1.5.m4.1.1.3.cmml" xref="S4.SS3.SSS2.p1.5.m4.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p1.5.m4.1c">\bm{x}_{i}</annotation></semantics></math> is, resulting a more consistent structure of prediction to the groundtruth. An intuitive example is shown in FigureÂ <a href="#S4.F6" title="Figure 6 â€£ 4.2 Implementation details â€£ 4 Experiments â€£ Occlusion-Robust Object Pose Estimation with Holistic Representation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<div id="S4.SS3.SSS2.p2" class="ltx_para">
<p id="S4.SS3.SSS2.p2.1" class="ltx_p">As shown in FigureÂ <a href="#S4.F7" title="Figure 7 â€£ 4.3.1 Model variations â€£ 4.3 Ablation studies â€£ 4 Experiments â€£ Occlusion-Robust Object Pose Estimation with Holistic Representation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, training with MPS effectively lowers the mean residuals. Furthermore, the mean incoherence are also smaller for all objects. This confirms that a more holistic understanding of the object pose can produce more accurate and structurally consistent landmark predictions.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;" rowspan="2"></th>
<th id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;" colspan="5">ADD(-S)</th>
<th id="S4.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;" colspan="6">AUC of ADD(-S)</th>
</tr>
<tr id="S4.T3.1.2.2" class="ltx_tr">
<th id="S4.T3.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;" colspan="5">Without refinement</th>
<th id="S4.T3.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;" colspan="4">Without refinement</th>
<th id="S4.T3.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;" colspan="2">With refinement</th>
</tr>
<tr id="S4.T3.1.3.3" class="ltx_tr">
<th id="S4.T3.1.3.3.1" class="ltx_td ltx_th ltx_th_column ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;"></th>
<th id="S4.T3.1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">HM</th>
<th id="S4.T3.1.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">Hu</th>
<th id="S4.T3.1.3.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">Hu2</th>
<th id="S4.T3.1.3.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">GDR</th>
<th id="S4.T3.1.3.3.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">Ours</th>
<th id="S4.T3.1.3.3.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">HM</th>
<th id="S4.T3.1.3.3.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">PVNet</th>
<th id="S4.T3.1.3.3.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">GDR</th>
<th id="S4.T3.1.3.3.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">Ours</th>
<th id="S4.T3.1.3.3.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">DeepIM</th>
<th id="S4.T3.1.3.3.12" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">CosyPose</th>
</tr>
<tr id="S4.T3.1.4.4" class="ltx_tr">
<th id="S4.T3.1.4.4.1" class="ltx_td ltx_th ltx_th_column ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;"></th>
<th id="S4.T3.1.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:5.5pt;padding-right:5.5pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite></th>
<th id="S4.T3.1.4.4.3" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:5.5pt;padding-right:5.5pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite></th>
<th id="S4.T3.1.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:5.5pt;padding-right:5.5pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite></th>
<th id="S4.T3.1.4.4.5" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:5.5pt;padding-right:5.5pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite></th>
<th id="S4.T3.1.4.4.6" class="ltx_td ltx_th ltx_th_column ltx_border_rr" style="padding-left:5.5pt;padding-right:5.5pt;"></th>
<th id="S4.T3.1.4.4.7" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:5.5pt;padding-right:5.5pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite></th>
<th id="S4.T3.1.4.4.8" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:5.5pt;padding-right:5.5pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite></th>
<th id="S4.T3.1.4.4.9" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:5.5pt;padding-right:5.5pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite></th>
<th id="S4.T3.1.4.4.10" class="ltx_td ltx_th ltx_th_column ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;"></th>
<th id="S4.T3.1.4.4.11" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:5.5pt;padding-right:5.5pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite></th>
<th id="S4.T3.1.4.4.12" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:5.5pt;padding-right:5.5pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.5.1" class="ltx_tr">
<td id="S4.T3.1.5.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">master chef can</td>
<td id="S4.T3.1.5.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">31.20</td>
<td id="S4.T3.1.5.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">33.00</td>
<td id="S4.T3.1.5.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.5.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.5.1.6" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.5.1.6.1" class="ltx_text ltx_font_bold">46.52</span></td>
<td id="S4.T3.1.5.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">69.00</td>
<td id="S4.T3.1.5.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.5.1.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.5.1.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.5.1.10.1" class="ltx_text ltx_font_bold">71.17</span></td>
<td id="S4.T3.1.5.1.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.5.1.11.1" class="ltx_text ltx_font_bold">71.20</span></td>
<td id="S4.T3.1.5.1.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
</tr>
<tr id="S4.T3.1.6.2" class="ltx_tr">
<td id="S4.T3.1.6.2.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">cracker box</td>
<td id="S4.T3.1.6.2.2" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">75.00</td>
<td id="S4.T3.1.6.2.3" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">44.60</td>
<td id="S4.T3.1.6.2.4" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.6.2.5" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.6.2.6" class="ltx_td ltx_align_center ltx_border_rr" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.6.2.6.1" class="ltx_text ltx_font_bold">92.63</span></td>
<td id="S4.T3.1.6.2.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">80.20</td>
<td id="S4.T3.1.6.2.8" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.6.2.9" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.6.2.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.6.2.10.1" class="ltx_text ltx_font_bold">89.86</span></td>
<td id="S4.T3.1.6.2.11" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.6.2.11.1" class="ltx_text ltx_font_bold">83.60</span></td>
<td id="S4.T3.1.6.2.12" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
</tr>
<tr id="S4.T3.1.7.3" class="ltx_tr">
<td id="S4.T3.1.7.3.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">sugar box</td>
<td id="S4.T3.1.7.3.2" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">47.20</td>
<td id="S4.T3.1.7.3.3" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">75.60</td>
<td id="S4.T3.1.7.3.4" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.7.3.5" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.7.3.6" class="ltx_td ltx_align_center ltx_border_rr" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.7.3.6.1" class="ltx_text ltx_font_bold">99.15</span></td>
<td id="S4.T3.1.7.3.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">76.20</td>
<td id="S4.T3.1.7.3.8" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.7.3.9" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.7.3.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.7.3.10.1" class="ltx_text ltx_font_bold">93.21</span></td>
<td id="S4.T3.1.7.3.11" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.7.3.11.1" class="ltx_text ltx_font_bold">94.10</span></td>
<td id="S4.T3.1.7.3.12" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
</tr>
<tr id="S4.T3.1.8.4" class="ltx_tr">
<td id="S4.T3.1.8.4.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">tomato soup can</td>
<td id="S4.T3.1.8.4.2" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">30.20</td>
<td id="S4.T3.1.8.4.3" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">40.80</td>
<td id="S4.T3.1.8.4.4" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.8.4.5" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.8.4.6" class="ltx_td ltx_align_center ltx_border_rr" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.8.4.6.1" class="ltx_text ltx_font_bold">60.90</span></td>
<td id="S4.T3.1.8.4.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">70.00</td>
<td id="S4.T3.1.8.4.8" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.8.4.9" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.8.4.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.8.4.10.1" class="ltx_text ltx_font_bold">82.53</span></td>
<td id="S4.T3.1.8.4.11" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.8.4.11.1" class="ltx_text ltx_font_bold">86.10</span></td>
<td id="S4.T3.1.8.4.12" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
</tr>
<tr id="S4.T3.1.9.5" class="ltx_tr">
<td id="S4.T3.1.9.5.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">mustard bottle</td>
<td id="S4.T3.1.9.5.2" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">72.50</td>
<td id="S4.T3.1.9.5.3" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">70.60</td>
<td id="S4.T3.1.9.5.4" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.9.5.5" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.9.5.6" class="ltx_td ltx_align_center ltx_border_rr" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.9.5.6.1" class="ltx_text ltx_font_bold">100.00</span></td>
<td id="S4.T3.1.9.5.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">84.80</td>
<td id="S4.T3.1.9.5.8" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.9.5.9" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.9.5.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.9.5.10.1" class="ltx_text ltx_font_bold">95.34</span></td>
<td id="S4.T3.1.9.5.11" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.9.5.11.1" class="ltx_text ltx_font_bold">91.50</span></td>
<td id="S4.T3.1.9.5.12" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
</tr>
<tr id="S4.T3.1.10.6" class="ltx_tr">
<td id="S4.T3.1.10.6.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">tuna fish can</td>
<td id="S4.T3.1.10.6.2" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">4.31</td>
<td id="S4.T3.1.10.6.3" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">18.10</td>
<td id="S4.T3.1.10.6.4" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.10.6.5" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.10.6.6" class="ltx_td ltx_align_center ltx_border_rr" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.10.6.6.1" class="ltx_text ltx_font_bold">52.96</span></td>
<td id="S4.T3.1.10.6.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">49.40</td>
<td id="S4.T3.1.10.6.8" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.10.6.9" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.10.6.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.10.6.10.1" class="ltx_text ltx_font_bold">88.01</span></td>
<td id="S4.T3.1.10.6.11" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.10.6.11.1" class="ltx_text ltx_font_bold">87.70</span></td>
<td id="S4.T3.1.10.6.12" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
</tr>
<tr id="S4.T3.1.11.7" class="ltx_tr">
<td id="S4.T3.1.11.7.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">pudding box</td>
<td id="S4.T3.1.11.7.2" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">48.30</td>
<td id="S4.T3.1.11.7.3" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">12.20</td>
<td id="S4.T3.1.11.7.4" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.11.7.5" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.11.7.6" class="ltx_td ltx_align_center ltx_border_rr" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.11.7.6.1" class="ltx_text ltx_font_bold">79.91</span></td>
<td id="S4.T3.1.11.7.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">82.20</td>
<td id="S4.T3.1.11.7.8" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.11.7.9" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.11.7.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.11.7.10.1" class="ltx_text ltx_font_bold">90.5</span></td>
<td id="S4.T3.1.11.7.11" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.11.7.11.1" class="ltx_text ltx_font_bold">82.70</span></td>
<td id="S4.T3.1.11.7.12" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
</tr>
<tr id="S4.T3.1.12.8" class="ltx_tr">
<td id="S4.T3.1.12.8.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">gelatin box</td>
<td id="S4.T3.1.12.8.2" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">37.20</td>
<td id="S4.T3.1.12.8.3" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.12.8.3.1" class="ltx_text ltx_font_bold">59.40</span></td>
<td id="S4.T3.1.12.8.4" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.12.8.5" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.12.8.6" class="ltx_td ltx_align_center ltx_border_rr" style="padding-left:5.5pt;padding-right:5.5pt;">58.88</td>
<td id="S4.T3.1.12.8.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">81.80</td>
<td id="S4.T3.1.12.8.8" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.12.8.9" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.12.8.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.12.8.10.1" class="ltx_text ltx_font_bold">89.36</span></td>
<td id="S4.T3.1.12.8.11" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.12.8.11.1" class="ltx_text ltx_font_bold">91.90</span></td>
<td id="S4.T3.1.12.8.12" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
</tr>
<tr id="S4.T3.1.13.9" class="ltx_tr">
<td id="S4.T3.1.13.9.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">potted meat can</td>
<td id="S4.T3.1.13.9.2" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">40.30</td>
<td id="S4.T3.1.13.9.3" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">33.30</td>
<td id="S4.T3.1.13.9.4" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.13.9.5" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.13.9.6" class="ltx_td ltx_align_center ltx_border_rr" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.13.9.6.1" class="ltx_text ltx_font_bold">58.62</span></td>
<td id="S4.T3.1.13.9.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">66.20</td>
<td id="S4.T3.1.13.9.8" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.13.9.9" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.13.9.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.13.9.10.1" class="ltx_text ltx_font_bold">74.54</span></td>
<td id="S4.T3.1.13.9.11" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.13.9.11.1" class="ltx_text ltx_font_bold">76.20</span></td>
<td id="S4.T3.1.13.9.12" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
</tr>
<tr id="S4.T3.1.14.10" class="ltx_tr">
<td id="S4.T3.1.14.10.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">banana</td>
<td id="S4.T3.1.14.10.2" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">6.20</td>
<td id="S4.T3.1.14.10.3" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">16.60</td>
<td id="S4.T3.1.14.10.4" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.14.10.5" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.14.10.6" class="ltx_td ltx_align_center ltx_border_rr" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.14.10.6.1" class="ltx_text ltx_font_bold">36.94</span></td>
<td id="S4.T3.1.14.10.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">52.90</td>
<td id="S4.T3.1.14.10.8" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.14.10.9" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.14.10.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.14.10.10.1" class="ltx_text ltx_font_bold">58.77</span></td>
<td id="S4.T3.1.14.10.11" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.14.10.11.1" class="ltx_text ltx_font_bold">81.20</span></td>
<td id="S4.T3.1.14.10.12" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
</tr>
<tr id="S4.T3.1.15.11" class="ltx_tr">
<td id="S4.T3.1.15.11.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">pitcher base</td>
<td id="S4.T3.1.15.11.2" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">53.80</td>
<td id="S4.T3.1.15.11.3" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">90.00</td>
<td id="S4.T3.1.15.11.4" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.15.11.5" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.15.11.6" class="ltx_td ltx_align_center ltx_border_rr" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.15.11.6.1" class="ltx_text ltx_font_bold">99.65</span></td>
<td id="S4.T3.1.15.11.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">69.90</td>
<td id="S4.T3.1.15.11.8" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.15.11.9" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.15.11.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.15.11.10.1" class="ltx_text ltx_font_bold">92.86</span></td>
<td id="S4.T3.1.15.11.11" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.15.11.11.1" class="ltx_text ltx_font_bold">90.10</span></td>
<td id="S4.T3.1.15.11.12" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
</tr>
<tr id="S4.T3.1.16.12" class="ltx_tr">
<td id="S4.T3.1.16.12.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">bleach cleanser</td>
<td id="S4.T3.1.16.12.2" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">57.20</td>
<td id="S4.T3.1.16.12.3" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">70.90</td>
<td id="S4.T3.1.16.12.4" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.16.12.5" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.16.12.6" class="ltx_td ltx_align_center ltx_border_rr" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.16.12.6.1" class="ltx_text ltx_font_bold">75.22</span></td>
<td id="S4.T3.1.16.12.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">73.30</td>
<td id="S4.T3.1.16.12.8" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.16.12.9" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.16.12.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.16.12.10.1" class="ltx_text ltx_font_bold">77.35</span></td>
<td id="S4.T3.1.16.12.11" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.16.12.11.1" class="ltx_text ltx_font_bold">81.20</span></td>
<td id="S4.T3.1.16.12.12" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
</tr>
<tr id="S4.T3.1.17.13" class="ltx_tr">
<td id="S4.T3.1.17.13.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">bowl*</td>
<td id="S4.T3.1.17.13.2" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.17.13.2.1" class="ltx_text ltx_font_bold">49.50</span></td>
<td id="S4.T3.1.17.13.3" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">30.50</td>
<td id="S4.T3.1.17.13.4" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.17.13.5" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.17.13.6" class="ltx_td ltx_align_center ltx_border_rr" style="padding-left:5.5pt;padding-right:5.5pt;">45.07</td>
<td id="S4.T3.1.17.13.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.17.13.7.1" class="ltx_text ltx_font_bold">80.30</span></td>
<td id="S4.T3.1.17.13.8" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.17.13.9" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.17.13.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">70.81</td>
<td id="S4.T3.1.17.13.11" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.17.13.11.1" class="ltx_text ltx_font_bold">81.40</span></td>
<td id="S4.T3.1.17.13.12" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
</tr>
<tr id="S4.T3.1.18.14" class="ltx_tr">
<td id="S4.T3.1.18.14.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">mug</td>
<td id="S4.T3.1.18.14.2" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">10.50</td>
<td id="S4.T3.1.18.14.3" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">40.70</td>
<td id="S4.T3.1.18.14.4" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.18.14.5" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.18.14.6" class="ltx_td ltx_align_center ltx_border_rr" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.18.14.6.1" class="ltx_text ltx_font_bold">66.04</span></td>
<td id="S4.T3.1.18.14.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">50.50</td>
<td id="S4.T3.1.18.14.8" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.18.14.9" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.18.14.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.18.14.10.1" class="ltx_text ltx_font_bold">89.1</span></td>
<td id="S4.T3.1.18.14.11" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.18.14.11.1" class="ltx_text ltx_font_bold">81.40</span></td>
<td id="S4.T3.1.18.14.12" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
</tr>
<tr id="S4.T3.1.19.15" class="ltx_tr">
<td id="S4.T3.1.19.15.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">power drill</td>
<td id="S4.T3.1.19.15.2" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">63.00</td>
<td id="S4.T3.1.19.15.3" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">63.50</td>
<td id="S4.T3.1.19.15.4" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.19.15.5" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.19.15.6" class="ltx_td ltx_align_center ltx_border_rr" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.19.15.6.1" class="ltx_text ltx_font_bold">94.99</span></td>
<td id="S4.T3.1.19.15.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">78.30</td>
<td id="S4.T3.1.19.15.8" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.19.15.9" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.19.15.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.19.15.10.1" class="ltx_text ltx_font_bold">89.4</span></td>
<td id="S4.T3.1.19.15.11" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.19.15.11.1" class="ltx_text ltx_font_bold">85.50</span></td>
<td id="S4.T3.1.19.15.12" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
</tr>
<tr id="S4.T3.1.20.16" class="ltx_tr">
<td id="S4.T3.1.20.16.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">wood block*</td>
<td id="S4.T3.1.20.16.2" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">48.20</td>
<td id="S4.T3.1.20.16.3" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">27.70</td>
<td id="S4.T3.1.20.16.4" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.20.16.5" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.20.16.6" class="ltx_td ltx_align_center ltx_border_rr" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.20.16.6.1" class="ltx_text ltx_font_bold">55.37</span></td>
<td id="S4.T3.1.20.16.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">65.20</td>
<td id="S4.T3.1.20.16.8" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.20.16.9" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.20.16.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.20.16.10.1" class="ltx_text ltx_font_bold">70.62</span></td>
<td id="S4.T3.1.20.16.11" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.20.16.11.1" class="ltx_text ltx_font_bold">81.90</span></td>
<td id="S4.T3.1.20.16.12" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
</tr>
<tr id="S4.T3.1.21.17" class="ltx_tr">
<td id="S4.T3.1.21.17.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">scissors</td>
<td id="S4.T3.1.21.17.2" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">0.55</td>
<td id="S4.T3.1.21.17.3" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">17.10</td>
<td id="S4.T3.1.21.17.4" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.21.17.5" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.21.17.6" class="ltx_td ltx_align_center ltx_border_rr" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.21.17.6.1" class="ltx_text ltx_font_bold">71.27</span></td>
<td id="S4.T3.1.21.17.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">28.20</td>
<td id="S4.T3.1.21.17.8" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.21.17.9" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.21.17.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.21.17.10.1" class="ltx_text ltx_font_bold">84.82</span></td>
<td id="S4.T3.1.21.17.11" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.21.17.11.1" class="ltx_text ltx_font_bold">60.90</span></td>
<td id="S4.T3.1.21.17.12" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
</tr>
<tr id="S4.T3.1.22.18" class="ltx_tr">
<td id="S4.T3.1.22.18.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">large marker</td>
<td id="S4.T3.1.22.18.2" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">11.70</td>
<td id="S4.T3.1.22.18.3" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">4.80</td>
<td id="S4.T3.1.22.18.4" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.22.18.5" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.22.18.6" class="ltx_td ltx_align_center ltx_border_rr" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.22.18.6.1" class="ltx_text ltx_font_bold">11.73</span></td>
<td id="S4.T3.1.22.18.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">48.20</td>
<td id="S4.T3.1.22.18.8" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.22.18.9" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.22.18.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.22.18.10.1" class="ltx_text ltx_font_bold">53.25</span></td>
<td id="S4.T3.1.22.18.11" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.22.18.11.1" class="ltx_text ltx_font_bold">75.60</span></td>
<td id="S4.T3.1.22.18.12" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
</tr>
<tr id="S4.T3.1.23.19" class="ltx_tr">
<td id="S4.T3.1.23.19.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">large clamp*</td>
<td id="S4.T3.1.23.19.2" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">12.20</td>
<td id="S4.T3.1.23.19.3" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">25.60</td>
<td id="S4.T3.1.23.19.4" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.23.19.5" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.23.19.6" class="ltx_td ltx_align_center ltx_border_rr" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.23.19.6.1" class="ltx_text ltx_font_bold">68.12</span></td>
<td id="S4.T3.1.23.19.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">47.20</td>
<td id="S4.T3.1.23.19.8" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.23.19.9" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.23.19.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.23.19.10.1" class="ltx_text ltx_font_bold">77.1</span></td>
<td id="S4.T3.1.23.19.11" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.23.19.11.1" class="ltx_text ltx_font_bold">74.30</span></td>
<td id="S4.T3.1.23.19.12" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
</tr>
<tr id="S4.T3.1.24.20" class="ltx_tr">
<td id="S4.T3.1.24.20.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">extra large clamp*</td>
<td id="S4.T3.1.24.20.2" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">17.30</td>
<td id="S4.T3.1.24.20.3" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">8.80</td>
<td id="S4.T3.1.24.20.4" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.24.20.5" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.24.20.6" class="ltx_td ltx_align_center ltx_border_rr" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.24.20.6.1" class="ltx_text ltx_font_bold">56.16</span></td>
<td id="S4.T3.1.24.20.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">47.50</td>
<td id="S4.T3.1.24.20.8" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.24.20.9" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.24.20.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.24.20.10.1" class="ltx_text ltx_font_bold">55.19</span></td>
<td id="S4.T3.1.24.20.11" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.24.20.11.1" class="ltx_text ltx_font_bold">73.30</span></td>
<td id="S4.T3.1.24.20.12" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
</tr>
<tr id="S4.T3.1.25.21" class="ltx_tr">
<td id="S4.T3.1.25.21.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">foam brick*</td>
<td id="S4.T3.1.25.21.2" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">63.80</td>
<td id="S4.T3.1.25.21.3" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">34.70</td>
<td id="S4.T3.1.25.21.4" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.25.21.5" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.25.21.6" class="ltx_td ltx_align_center ltx_border_rr" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.25.21.6.1" class="ltx_text ltx_font_bold">68.40</span></td>
<td id="S4.T3.1.25.21.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.25.21.7.1" class="ltx_text ltx_font_bold">85.60</span></td>
<td id="S4.T3.1.25.21.8" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.25.21.9" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td id="S4.T3.1.25.21.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">83.78</td>
<td id="S4.T3.1.25.21.11" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.25.21.11.1" class="ltx_text ltx_font_bold">81.90</span></td>
<td id="S4.T3.1.25.21.12" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
</tr>
<tr id="S4.T3.1.26.22" class="ltx_tr">
<td id="S4.T3.1.26.22.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">average</td>
<td id="S4.T3.1.26.22.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">37.15</td>
<td id="S4.T3.1.26.22.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">38.98</td>
<td id="S4.T3.1.26.22.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">53.90</td>
<td id="S4.T3.1.26.22.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">60.10</td>
<td id="S4.T3.1.26.22.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.26.22.6.1" class="ltx_text ltx_font_bold">66.59</span></td>
<td id="S4.T3.1.26.22.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">66.04</td>
<td id="S4.T3.1.26.22.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">73.40</td>
<td id="S4.T3.1.26.22.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.26.22.9.1" class="ltx_text ltx_font_bold">84.40</span></td>
<td id="S4.T3.1.26.22.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">79.88</td>
<td id="S4.T3.1.26.22.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">81.90</td>
<td id="S4.T3.1.26.22.12" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.1.26.22.12.1" class="ltx_text ltx_font_bold">84.50</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>
Test accuracy on the YCB-Video dataset. Objects with a â€œ*â€ sign are considered as symmetric objects.</figcaption>
</figure>
</section>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Comparing to SOTA methods</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">We report results on the LINEMOD dataset in TableÂ <a href="#S4.T1" title="Table 1 â€£ 4.3.1 Model variations â€£ 4.3 Ablation studies â€£ 4 Experiments â€£ Occlusion-Robust Object Pose Estimation with Holistic Representation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. We group methods into two types depending on whether they include a separate refinement step or not. Our method achieves the best average ADD(-S), as well as the best ADD(-S) on most individual objects. Moreover, our method even outperforms all SOTA methods with refinement, further attesting the power of ROPE.
The results on the Occluded-LINEMOD dataset are summarised in TableÂ <a href="#S4.T2" title="Table 2 â€£ 4.3.1 Model variations â€£ 4.3 Ablation studies â€£ 4 Experiments â€£ Occlusion-Robust Object Pose Estimation with Holistic Representation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. In the non-refinement group, our method ranked second amongst current SOTA methods overall and best on two individual objects. A sample of qualitative results are provided in FiguresÂ <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Occlusion-Robust Object Pose Estimation with Holistic Representation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and <a href="#S4.F5" title="Figure 5 â€£ 4 Experiments â€£ Occlusion-Robust Object Pose Estimation with Holistic Representation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
The results on the YCB-Video dataset are reported in TableÂ <a href="#S4.T3" title="Table 3 â€£ 4.3.2 Accuracy and coherence of landmarks â€£ 4.3 Ablation studies â€£ 4 Experiments â€£ Occlusion-Robust Object Pose Estimation with Holistic Representation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Without refinement, ROPE has the best performance when evaluated with ADD(-S).</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Data efficiency</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">The LINEMOD dataset has about 1200 images for each object, which results in approximately 180 images (15%) for the training set. To supplement such a small training set many methods generate a large amount of synthetic images. For example,
PVNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> renders 20000 images for each object and the same strategy is adopted inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>. Although we only use a moderate amount of 1312 synthetic images on top of the 180 in training, we test our modelâ€™s performance in a extremely data-efficient case: only using the <math id="S4.SS5.p1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S4.SS5.p1.1.m1.1a"><mo id="S4.SS5.p1.1.m1.1.1" xref="S4.SS5.p1.1.m1.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.1.m1.1b"><csymbol cd="latexml" id="S4.SS5.p1.1.m1.1.1.cmml" xref="S4.SS5.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.1.m1.1c">\sim</annotation></semantics></math>180 images for training.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.2.2.3.1" class="ltx_tr">
<th id="S4.T4.2.2.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T4.2.2.3.1.1.1" class="ltx_text">ADD(-S)</span></th>
<th id="S4.T4.2.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2">Training images</th>
</tr>
<tr id="S4.T4.2.2.2" class="ltx_tr">
<th id="S4.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<math id="S4.T4.1.1.1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S4.T4.1.1.1.1.m1.1a"><mo id="S4.T4.1.1.1.1.m1.1.1" xref="S4.T4.1.1.1.1.m1.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S4.T4.1.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.1.m1.1c">\sim</annotation></semantics></math>180</th>
<th id="S4.T4.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<math id="S4.T4.2.2.2.2.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S4.T4.2.2.2.2.m1.1a"><mo id="S4.T4.2.2.2.2.m1.1.1" xref="S4.T4.2.2.2.2.m1.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.2.m1.1b"><csymbol cd="latexml" id="S4.T4.2.2.2.2.m1.1.1.cmml" xref="S4.T4.2.2.2.2.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.2.m1.1c">\sim</annotation></semantics></math>1500</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.2.2.4.1" class="ltx_tr">
<th id="S4.T4.2.2.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">ape</th>
<td id="S4.T4.2.2.4.1.2" class="ltx_td ltx_align_center ltx_border_t">78.57</td>
<td id="S4.T4.2.2.4.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.2.4.1.3.1" class="ltx_text ltx_font_bold">81.52</span></td>
</tr>
<tr id="S4.T4.2.2.5.2" class="ltx_tr">
<th id="S4.T4.2.2.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">benchvise</th>
<td id="S4.T4.2.2.5.2.2" class="ltx_td ltx_align_center">98.93</td>
<td id="S4.T4.2.2.5.2.3" class="ltx_td ltx_align_center"><span id="S4.T4.2.2.5.2.3.1" class="ltx_text ltx_font_bold">100.00</span></td>
</tr>
<tr id="S4.T4.2.2.6.3" class="ltx_tr">
<th id="S4.T4.2.2.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">cam</th>
<td id="S4.T4.2.2.6.3.2" class="ltx_td ltx_align_center">90.88</td>
<td id="S4.T4.2.2.6.3.3" class="ltx_td ltx_align_center"><span id="S4.T4.2.2.6.3.3.1" class="ltx_text ltx_font_bold">96.86</span></td>
</tr>
<tr id="S4.T4.2.2.7.4" class="ltx_tr">
<th id="S4.T4.2.2.7.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">can</th>
<td id="S4.T4.2.2.7.4.2" class="ltx_td ltx_align_center">98.03</td>
<td id="S4.T4.2.2.7.4.3" class="ltx_td ltx_align_center"><span id="S4.T4.2.2.7.4.3.1" class="ltx_text ltx_font_bold">98.72</span></td>
</tr>
<tr id="S4.T4.2.2.8.5" class="ltx_tr">
<th id="S4.T4.2.2.8.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">cat</th>
<td id="S4.T4.2.2.8.5.2" class="ltx_td ltx_align_center">92.22</td>
<td id="S4.T4.2.2.8.5.3" class="ltx_td ltx_align_center"><span id="S4.T4.2.2.8.5.3.1" class="ltx_text ltx_font_bold">94.71</span></td>
</tr>
<tr id="S4.T4.2.2.9.6" class="ltx_tr">
<th id="S4.T4.2.2.9.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">driller</th>
<td id="S4.T4.2.2.9.6.2" class="ltx_td ltx_align_center">98.02</td>
<td id="S4.T4.2.2.9.6.3" class="ltx_td ltx_align_center"><span id="S4.T4.2.2.9.6.3.1" class="ltx_text ltx_font_bold">99.01</span></td>
</tr>
<tr id="S4.T4.2.2.10.7" class="ltx_tr">
<th id="S4.T4.2.2.10.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">duck</th>
<td id="S4.T4.2.2.10.7.2" class="ltx_td ltx_align_center">79.06</td>
<td id="S4.T4.2.2.10.7.3" class="ltx_td ltx_align_center"><span id="S4.T4.2.2.10.7.3.1" class="ltx_text ltx_font_bold">85.35</span></td>
</tr>
<tr id="S4.T4.2.2.11.8" class="ltx_tr">
<th id="S4.T4.2.2.11.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">eggbox*</th>
<td id="S4.T4.2.2.11.8.2" class="ltx_td ltx_align_center">99.72</td>
<td id="S4.T4.2.2.11.8.3" class="ltx_td ltx_align_center"><span id="S4.T4.2.2.11.8.3.1" class="ltx_text ltx_font_bold">100.00</span></td>
</tr>
<tr id="S4.T4.2.2.12.9" class="ltx_tr">
<th id="S4.T4.2.2.12.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">glue*</th>
<td id="S4.T4.2.2.12.9.2" class="ltx_td ltx_align_center">97.68</td>
<td id="S4.T4.2.2.12.9.3" class="ltx_td ltx_align_center"><span id="S4.T4.2.2.12.9.3.1" class="ltx_text ltx_font_bold">99.42</span></td>
</tr>
<tr id="S4.T4.2.2.13.10" class="ltx_tr">
<th id="S4.T4.2.2.13.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">holepuncher</th>
<td id="S4.T4.2.2.13.10.2" class="ltx_td ltx_align_center">88.30</td>
<td id="S4.T4.2.2.13.10.3" class="ltx_td ltx_align_center"><span id="S4.T4.2.2.13.10.3.1" class="ltx_text ltx_font_bold">90.39</span></td>
</tr>
<tr id="S4.T4.2.2.14.11" class="ltx_tr">
<th id="S4.T4.2.2.14.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">iron</th>
<td id="S4.T4.2.2.14.11.2" class="ltx_td ltx_align_center">96.83</td>
<td id="S4.T4.2.2.14.11.3" class="ltx_td ltx_align_center"><span id="S4.T4.2.2.14.11.3.1" class="ltx_text ltx_font_bold">100.00</span></td>
</tr>
<tr id="S4.T4.2.2.15.12" class="ltx_tr">
<th id="S4.T4.2.2.15.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">lamp</th>
<td id="S4.T4.2.2.15.12.2" class="ltx_td ltx_align_center">98.85</td>
<td id="S4.T4.2.2.15.12.3" class="ltx_td ltx_align_center"><span id="S4.T4.2.2.15.12.3.1" class="ltx_text ltx_font_bold">99.42</span></td>
</tr>
<tr id="S4.T4.2.2.16.13" class="ltx_tr">
<th id="S4.T4.2.2.16.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">phone</th>
<td id="S4.T4.2.2.16.13.2" class="ltx_td ltx_align_center">94.81</td>
<td id="S4.T4.2.2.16.13.3" class="ltx_td ltx_align_center"><span id="S4.T4.2.2.16.13.3.1" class="ltx_text ltx_font_bold">97.64</span></td>
</tr>
<tr id="S4.T4.2.2.17.14" class="ltx_tr">
<th id="S4.T4.2.2.17.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">average</th>
<td id="S4.T4.2.2.17.14.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">93.22</td>
<td id="S4.T4.2.2.17.14.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T4.2.2.17.14.3.1" class="ltx_text ltx_font_bold">95.61</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Comparing performances of ROPE in the extremely data-efficient setting (<math id="S4.T4.5.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S4.T4.5.m1.1b"><mo id="S4.T4.5.m1.1.1" xref="S4.T4.5.m1.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S4.T4.5.m1.1c"><csymbol cd="latexml" id="S4.T4.5.m1.1.1.cmml" xref="S4.T4.5.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.5.m1.1d">\sim</annotation></semantics></math>180) and in the original setting (<math id="S4.T4.6.m2.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S4.T4.6.m2.1b"><mo id="S4.T4.6.m2.1.1" xref="S4.T4.6.m2.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S4.T4.6.m2.1c"><csymbol cd="latexml" id="S4.T4.6.m2.1.1.cmml" xref="S4.T4.6.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.6.m2.1d">\sim</annotation></semantics></math>1500) on the LINEMOD dataset. Both models are without refinement.</figcaption>
</figure>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.1" class="ltx_p">As shown in TableÂ <a href="#S4.T4" title="Table 4 â€£ 4.5 Data efficiency â€£ 4 Experiments â€£ Occlusion-Robust Object Pose Estimation with Holistic Representation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, despite having slightly lower ADD(-S) then the baseline, our model achieves an overall accuracy of 93.22% which is close to the current SOTA method GDRÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>. This is accomplished with as few as around 180 training images, demonstrating superior data efficiency for our method.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We propose ROPE, a framework for robust object pose estimation against occlusions. We show that enforcing occlusion-robust feature learning and encouraging holistic representation learning are the key to achieve occlusion-robustness. Evaluations on three popularly used benchmark datasets, LINEMOD, Occluded-LINEMOD and YCB-Video, show that ROPE either outperforms or is competitive to SOTA methods, without the need of refinement. Our method is also highly data-efficient.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
HassanÂ Abu Alhaija, SivaÂ Karthik Mustikovela, Lars Mescheder, Andreas Geiger,
and Carsten Rother.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Augmented reality meets deep learning for car instance segmentation
in urban scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">BMVC</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
DanaÂ H Ballard.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Generalizing the hough transform to detect arbitrary shapes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Pattern recognition</span><span id="bib.bib2.4.2" class="ltx_text" style="font-size:90%;">, 13(2):111â€“122, 1981.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Eric Brachmann, Alexander Krull, Frank Michel, Stefan Gumhold, Jamie Shotton,
and Carsten Rother.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Learning 6d object pose estimation using 3d object coordinates.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Eric Brachmann, Frank Michel, Alexander Krull, MichaelÂ Ying Yang, Stefan
Gumhold, and Carsten Rother.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Uncertainty-driven 6d pose estimation of objects and scenes from a
single rgb image.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Ming Cai and Ian Reid.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Reconstruct locally, localize globally: A model free method for
object pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
LorenzoÂ Pasqualetto Cassinis, Robert Fonod, and Eberhard Gill.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Review of the robustness and applicability of monocular pose
estimation systems for relative navigation with an uncooperative spacecraft.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Progress in Aerospace Sciences</span><span id="bib.bib6.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Bo Chen, Jiewei Cao, Alvaro Parra, and Tat-Jun Chin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Satellite pose estimation with deep landmark regression and nonlinear
pose refinement.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCVW</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Bo Chen, Alvaro Parra, Jiewei Cao, Nan Li, and Tat-Jun Chin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">End-to-end learnable geometric vision by backpropagating pnp
optimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Wei Chen, Xi Jia, HyungÂ Jin Chang, Jinming Duan, Linlin Shen, and Ales
Leonardis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Fs-net: Fast shape-based network for category-level 6d object pose
estimation with decoupled rotation mechanism.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, pages 1581â€“1590, 2021.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, and Tian Xia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Multi-view 3d object detection network for autonomous driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Alvaro Collet, Manuel Martinez, and SiddharthaÂ S Srinivasa.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">The moped framework: Object recognition and pose estimation for
manipulation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">The international journal of robotics research</span><span id="bib.bib11.4.2" class="ltx_text" style="font-size:90%;">,
30(10):1284â€“1306, 2011.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
A. Crivellaro, M. Rad, Y. Verdie, K.Â M. Yi, P. Fua, and V. Lepetit.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Robust 3d object tracking from monocular images using stable parts.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">TPAMI</span><span id="bib.bib12.4.2" class="ltx_text" style="font-size:90%;">, 40(6):1465â€“1479, 2018.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Thanh-Toan Do, Ming Cai, Trung Pham, and Ian Reid.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Deep-6dpose: Recovering 6d object pose from a single rgb image.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1802.10367</span><span id="bib.bib13.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Bent Fuglede and Flemming Topsoe.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Jensen-shannon divergence and hilbert space embedding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of The International Symposium on Information
Theory</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, 2004.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Chunhui Gu and Xiaofeng Ren.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Discriminative mixture-of-templates for viewpoint classification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, 2010.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Mask r-cnn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">TPAMI</span><span id="bib.bib16.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Yisheng He, Wei Sun, Haibin Huang, Jianran Liu, Haoqiang Fan, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Pvn3d: A deep point-wise 3d keypoints voting network for 6dof pose
estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Stefan Hinterstoisser, Cedric Cagniart, Slobodan Ilic, Peter Sturm, Nassir
Navab, Pascal Fua, and Vincent Lepetit.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Gradient response maps for real-time detection of textureless
objects.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">TPAMI</span><span id="bib.bib18.4.2" class="ltx_text" style="font-size:90%;">, 34(5):876â€“888, 2011.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Stefan Hinterstoisser, Vincent Lepetit, Slobodan Ilic, Stefan Holzer, Gary
Bradski, Kurt Konolige, and Nassir Navab.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Model based training, detection and pose estimation of texture-less
3d objects in heavily cluttered scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ACCV</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, 2012.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Tomas Hodan, Frank Michel, Eric Brachmann, Wadim Kehl, Anders GlentBuch, Dirk
Kraft, Bertram Drost, Joel Vidal, Stephan Ihrke, Xenophon Zabulis, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Bop: Benchmark for 6d object pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoefler, and Daniel
Soudry.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Augment your batch: Improving generalization through instance
repetition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Yinlin Hu, Pascal Fua, Wei Wang, and Mathieu Salzmann.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Single-stage 6d object pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Yinlin Hu, Joachim Hugonot, Pascal Fua, and Mathieu Salzmann.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Segmentation-driven 6d object pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
DanielÂ P Huttenlocher, GregoryÂ A. Klanderman, and WilliamÂ J Rucklidge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Comparing images using the hausdorff distance.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">TPAMI</span><span id="bib.bib24.4.2" class="ltx_text" style="font-size:90%;">, 15(9):850â€“863, 1993.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
OmidÂ Hosseini Jafari, SivaÂ Karthik Mustikovela, Karl Pertsch, Eric Brachmann,
and Carsten Rother.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">ipose: instance-aware 6d pose estimation of partly occluded objects.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ACCV</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
AlexanderÂ B. Jung, Kentaro Wada, Jon Crall, Satoshi Tanaka, Jake Graving,
Christoph Reinders, Sarthak Yadav, Joy Banerjee, GÃ¡bor Vecsei, Adam Kraft,
Zheng Rui, Jirka Borovec, Christian Vallentin, Semen Zhydenko, Kilian
Pfeiffer, Ben Cook, Ismael FernÃ¡ndez, FranÃ§ois-Michel DeÂ Rainville,
Chi-Hung Weng, Abner Ayala-Acevedo, Raphael Meudec, Matias Laporte, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">imgaug.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/aleju/imgaug" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/aleju/imgaug</a><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.4.1" class="ltx_text" style="font-size:90%;">Online; accessed 01-Feb-2020.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Wadim Kehl, Fabian Manhardt, Federico Tombari, Slobodan Ilic, and Nassir Navab.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Ssd-6d: Making rgb-based 3d detection and 6d pose estimation great
again.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Alex Kendall, Matthew Grimes, and Roberto Cipolla.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Posenet: A convolutional network for real-time 6-dof camera
relocalization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
DiederikÂ P. Kingma and Jimmy Ba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Adam: A method for stochastic optimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Yann LabbÃ©, Justin Carpentier, Mathieu Aubry, and Josef Sivic.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Cosypose: Consistent multi-view multi-object 6d pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Vincent Lepetit, Pascal Fua, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Monocular model-based 3d tracking of rigid objects: A survey.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Foundations and TrendsÂ® in Computer Graphics and
Vision</span><span id="bib.bib31.4.2" class="ltx_text" style="font-size:90%;">, 1(1):1â€“89, 2005.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Chi Li, M ZeeshanÂ Zia, Quoc-Huy Tran, Xiang Yu, GregoryÂ D Hager, and Manmohan
Chandraker.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Deep supervision with shape concepts for occlusion-aware 3d object
parsing.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Yi Li, Gu Wang, Xiangyang Ji, Yu Xiang, and Dieter Fox.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Deepim: Deep iterative matching for 6d pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Yi Li, Gu Wang, Xiangyang Ji, Yu Xiang, and Dieter Fox.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Deepim: Deep iterative matching for 6d pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Computer Vision</span><span id="bib.bib34.4.2" class="ltx_text" style="font-size:90%;">, 128(3):657â€“678,
2020.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Zhigang Li, Gu Wang, and Xiangyang Ji.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Cdpn: Coordinates-based disentangled pose network for real-time
rgb-based 6-dof object pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed,
Cheng-Yang Fu, and AlexanderÂ C Berg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Ssd: Single shot multibox detector.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
DavidÂ G Lowe.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Object recognition from local scale-invariant features.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib37.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib37.5.3" class="ltx_text" style="font-size:90%;">, 1999.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Fabian Manhardt, Wadim Kehl, Nassir Navab, and Federico Tombari.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Deep model-based 6d pose refinement in rgb.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib38.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Eric Marchand, Hideaki Uchiyama, and Fabien Spindler.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">Pose estimation for augmented reality: a hands-on survey.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE transactions on visualization and computer graphics</span><span id="bib.bib39.4.2" class="ltx_text" style="font-size:90%;">,
22(12):2633â€“2651, 2015.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Frank Michel, Alexander Kirillov, Eric Brachmann, Alexander Krull, Stefan
Gumhold, Bogdan Savchynskyy, and Carsten Rother.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Global hypothesis generation for 6d object pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib40.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib40.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Markus Oberweger, Mahdi Rad, and Vincent Lepetit.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">Making deep heatmaps robust to partial occlusions for 3d object pose
estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib41.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib41.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Kiru Park, Timothy Patten, and Markus Vincze.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Pix2pose: Pixel-wise coordinate regression of objects for 6d pose
estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib42.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib42.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Georgios Pavlakos, Xiaowei Zhou, Aaron Chan, KonstantinosÂ G Derpanis, and
Kostas Daniilidis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">6-dof object pose from semantic keypoints.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib43.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICRA</span><span id="bib.bib43.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Pvnet: Pixel-wise voting network for 6dof pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib44.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib44.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Patrick Poirson, Phil Ammirato, Cheng-Yang Fu, Wei Liu, Jana Kosecka, and
AlexanderÂ C Berg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">Fast single shot detection and pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib45.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">The Fourth International Conference on 3D Vision</span><span id="bib.bib45.5.3" class="ltx_text" style="font-size:90%;">. IEEE,
2016.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Mahdi Rad and Vincent Lepetit.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">Bb8: A scalable, accurate, robust to partial occlusion method for
predicting the 3d poses of challenging objects without using depth.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib46.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib46.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Joseph Redmon and Ali Farhadi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">Yolo9000: better, faster, stronger.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib47.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib47.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
Torsten Sattler, Qunjie Zhou, Marc Pollefeys, and Laura Leal-Taixe.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">Understanding the limitations of cnn-based absolute camera pose
regression.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib48.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib48.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
S. Sharma, C. Beierle, and S. Dâ€™Amico.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">Pose estimation for non-cooperative spacecraft rendezvous using
convolutional neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib49.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Aerospace Conference</span><span id="bib.bib49.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
KrishnaÂ Kumar Singh and YongÂ Jae Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">Hide-and-seek: Forcing a network to be meticulous for
weakly-supervised object and action localization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib50.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib50.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text" style="font-size:90%;">
Chen Song, Jiaru Song, and Qixing Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.2.1" class="ltx_text" style="font-size:90%;">Hybridpose: 6d object pose estimation under hybrid representations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib51.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib51.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text" style="font-size:90%;">
Chen Song, Jiaru Song, and Qixing Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.2.1" class="ltx_text" style="font-size:90%;">Hybridpose: 6d object pose estimation under hybrid representations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2001.01869</span><span id="bib.bib52.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text" style="font-size:90%;">
Hao Su, CharlesÂ R Qi, Yangyan Li, and LeonidasÂ J Guibas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.2.1" class="ltx_text" style="font-size:90%;">Render for cnn: Viewpoint estimation in images using cnns trained
with rendered 3d model views.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib53.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib53.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text" style="font-size:90%;">
Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.2.1" class="ltx_text" style="font-size:90%;">Deep high-resolution representation learning for human pose
estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib54.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib54.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text" style="font-size:90%;">
Martin Sundermeyer, Maximilian Durner, EnÂ Yen Puang, Zoltan-Csaba Marton,
Narunas Vaskevicius, KaiÂ O Arras, and Rudolph Triebel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.2.1" class="ltx_text" style="font-size:90%;">Multi-path learning for object pose estimation across domains.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib55.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib55.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text" style="font-size:90%;">
Martin Sundermeyer, Zoltan-Csaba Marton, Maximilian Durner, and Rudolph
Triebel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.2.1" class="ltx_text" style="font-size:90%;">Augmented autoencoders: Implicit 3d orientation learning for 6d
object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IJCV</span><span id="bib.bib56.4.2" class="ltx_text" style="font-size:90%;">, 128(3):714â€“729, 2020.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text" style="font-size:90%;">
Bugra Tekin, SudiptaÂ N Sinha, and Pascal Fua.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.2.1" class="ltx_text" style="font-size:90%;">Real-time seamless single shot 6d object pose prediction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib57.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib57.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock"><span id="bib.bib58.1.1" class="ltx_text" style="font-size:90%;">
Chen Wang, Danfei Xu, Yuke Zhu, Roberto MartÃ­n-MartÃ­n, Cewu Lu, Li
Fei-Fei, and Silvio Savarese.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.2.1" class="ltx_text" style="font-size:90%;">Densefusion: 6d object pose estimation by iterative dense fusion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib58.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib58.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock"><span id="bib.bib59.1.1" class="ltx_text" style="font-size:90%;">
Gu Wang, Fabian Manhardt, Federico Tombari, and Xiangyang Ji.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.2.1" class="ltx_text" style="font-size:90%;">Gdr-net: Geometry-guided direct regression network for monocular 6d
object pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib59.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib59.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock"><span id="bib.bib60.1.1" class="ltx_text" style="font-size:90%;">
J. Wang, K. Sun, T. Cheng, B. Jiang, C. Deng, Y. Zhao, D. Liu, Y.
Mu, M. Tan, X. Wang, W. Liu, and B. Xiao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.2.1" class="ltx_text" style="font-size:90%;">Deep high-resolution representation learning for visual recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">TPAMI</span><span id="bib.bib60.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock"><span id="bib.bib61.1.1" class="ltx_text" style="font-size:90%;">
Di Wu, Zhaoyong Zhuang, Canqun Xiang, Wenbin Zou, and Xia Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.2.1" class="ltx_text" style="font-size:90%;">6d-vnet: End-to-end 6-dof vehicle pose estimation from monocular rgb
images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib61.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPRW</span><span id="bib.bib61.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock"><span id="bib.bib62.1.1" class="ltx_text" style="font-size:90%;">
Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, and Dieter Fox.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.2.1" class="ltx_text" style="font-size:90%;">Posecnn: A convolutional neural network for 6d object pose estimation
in cluttered scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib62.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Robotics: Science and Systems (RSS)</span><span id="bib.bib62.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock"><span id="bib.bib63.1.1" class="ltx_text" style="font-size:90%;">
Danfei Xu, Dragomir Anguelov, and Ashesh Jain.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.2.1" class="ltx_text" style="font-size:90%;">Pointfusion: Deep sensor fusion for 3d bounding box estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib63.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib63.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock"><span id="bib.bib64.1.1" class="ltx_text" style="font-size:90%;">
Sergey Zakharov, Ivan Shugurov, and Slobodan Ilic.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.2.1" class="ltx_text" style="font-size:90%;">Dpod: 6d pose object detector and refiner.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib64.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib64.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock"><span id="bib.bib65.1.1" class="ltx_text" style="font-size:90%;">
Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.2.1" class="ltx_text" style="font-size:90%;">Random erasing data augmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib65.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">AAAI</span><span id="bib.bib65.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock"><span id="bib.bib66.1.1" class="ltx_text" style="font-size:90%;">
Menglong Zhu, KonstantinosÂ G Derpanis, Yinfei Yang, Samarth Brahmbhatt, Mabel
Zhang, Cody Phillips, Matthieu Lecce, and Kostas Daniilidis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.2.1" class="ltx_text" style="font-size:90%;">Single image 3d object detection and pose estimation for grasping.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib66.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICRA</span><span id="bib.bib66.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock"><span id="bib.bib67.1.1" class="ltx_text" style="font-size:90%;">
Yiming Zuo, Weichao Qiu, Lingxi Xie, Fangwei Zhong, Yizhou Wang, and AlanÂ L.
Yuille.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.2.1" class="ltx_text" style="font-size:90%;">Craves: Controlling robotic arm with a vision-based economic system.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib67.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib67.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2110.11635" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2110.11636" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2110.11636">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2110.11636" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2110.11637" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar 19 15:34:14 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
