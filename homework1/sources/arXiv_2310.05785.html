<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2310.05785] Joint object detection and re-identification for 3D obstacle multi-camera systems</title><meta property="og:description" content="In recent years, the field of autonomous driving has witnessed remarkable advancements, driven by the integration of a multitude of sensors, including cameras and LiDAR systems, in different prototypes. However, with t…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Joint object detection and re-identification for 3D obstacle multi-camera systems">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Joint object detection and re-identification for 3D obstacle multi-camera systems">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2310.05785">

<!--Generated on Wed Feb 28 02:09:48 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Joint object detection and re-identification for 3D obstacle multi-camera systems
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Irene Cortés 
<br class="ltx_break">Department of Systems Engineering and Automation 
<br class="ltx_break">Universidad Carlos III de Madrid 
<br class="ltx_break">Leganés
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">irecorte@ing.uc3m.es</span> 
<br class="ltx_break"><span id="id2.2.id2" class="ltx_ERROR undefined">\And</span>Jorge Beltrán 
<br class="ltx_break">Department of Signal Theory, Telematics, and Computer Science 
<br class="ltx_break">Universidad Rey Juan Carlos 
<br class="ltx_break">Fuenlabrada
<br class="ltx_break"><span id="id3.3.id3" class="ltx_ERROR undefined">\And</span>Arturo de la Escalera 
<br class="ltx_break">Department of Systems Engineering and Automation 
<br class="ltx_break">Universidad Carlos III de Madrid 
<br class="ltx_break">Leganés
<br class="ltx_break"><span id="id4.4.id4" class="ltx_ERROR undefined">\And</span>Fernando García 
<br class="ltx_break">Department of Systems Engineering and Automation 
<br class="ltx_break">Universidad Carlos III de Madrid 
<br class="ltx_break">Leganés
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p">In recent years, the field of autonomous driving has witnessed remarkable advancements, driven by the integration of a multitude of sensors, including cameras and LiDAR systems, in different prototypes. However, with the proliferation of sensor data comes the pressing need for more sophisticated information processing techniques. This research paper introduces a novel modification to an object detection network that uses camera and lidar information, incorporating an additional branch designed for the task of re-identifying objects across adjacent cameras within the same vehicle while elevating the quality of the baseline 3D object detection outcomes.
The proposed methodology employs a two-step detection pipeline: initially, an object detection network is employed, followed by a 3D box estimator that operates on the filtered point cloud generated from the network’s detections. Extensive experimental evaluations encompassing both 2D and 3D domains validate the effectiveness of the proposed approach and the results underscore the superiority of this method over traditional Non-Maximum Suppression (NMS) techniques, with an improvement of more than 5% in the car category in the overlapping areas.</p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<p id="p1.3" class="ltx_p"><em id="p1.3.1" class="ltx_emph ltx_font_bold ltx_font_italic">K</em><span id="p1.3.2" class="ltx_text ltx_font_bold">eywords</span> 3D Object Detection  <math id="p1.1.m1.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.1.m1.1a"><mo id="p1.1.m1.1.1" xref="p1.1.m1.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.1.m1.1b"><ci id="p1.1.m1.1.1.cmml" xref="p1.1.m1.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.1.m1.1c">\cdot</annotation></semantics></math>
Multi-Camera Setup  <math id="p1.2.m2.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.2.m2.1a"><mo id="p1.2.m2.1.1" xref="p1.2.m2.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.2.m2.1b"><ci id="p1.2.m2.1.1.cmml" xref="p1.2.m2.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.2.m2.1c">\cdot</annotation></semantics></math>
Siamese Network  <math id="p1.3.m3.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.3.m3.1a"><mo id="p1.3.m3.1.1" xref="p1.3.m3.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.3.m3.1b"><ci id="p1.3.m3.1.1.cmml" xref="p1.3.m3.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.3.m3.1c">\cdot</annotation></semantics></math>
Non-Maxima Suppression</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>INTRODUCTION</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">In the previous decade, research on perception systems for autonomous driving was mainly focused on understanding the traffic situation in front of the vehicle, mainly powered by popular datasets such as KITTI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> or Cityscapes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Although this approach limits the complexity of the problem and the computational requirements, its outcome restricts the set of use cases to simple scenarios where an automated operation of the car can be safely performed (e.g. highways).</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">More recently, the increase in GPU capabilities and the great advances in deep learning models have led to the emergence of new research platforms and prototypes targeted to drive in more challenging traffic environments, with a higher degree of interaction with other road users and involving difficult maneuvers, like in cities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. In order to achieve this milestone, the perception pipeline of an Autonomous Vehicle must be able to identify the different participants and potential hazards in the whole scene, not only in the forward direction.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">To keep pace with these new needs, more complex sensor configurations aimed at covering the whole horizontal field of view around the vehicle have become the prevailing trend. Nowadays, using setups composed of multiple cameras and one or more LiDAR devices is a common choice to capture meaningful information in 360<math id="S1.p3.1.m1.1" class="ltx_Math" alttext="\mathrm{\SIUnitSymbolDegree}" display="inline"><semantics id="S1.p3.1.m1.1a"><mi class="ltx_unit" mathvariant="normal" id="S1.p3.1.m1.1.1" xref="S1.p3.1.m1.1.1.cmml">°</mi><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b"><csymbol cd="latexml" id="S1.p3.1.m1.1.1.cmml" xref="S1.p3.1.m1.1.1">degree</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.1.m1.1c">\mathrm{\SIUnitSymbolDegree}</annotation></semantics></math> with sensor redundancy, so that safe navigation can be achieved <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">Apart from redundancy, the popularization of perception systems made of multiple units of different technologies brings many opportunities to build robust pipelines capable of providing a precise understanding of the driving environment. For example, combining the data from cameras and LiDAR devices allows for a better estimation of object positions, sizes, and velocities. Furthermore, fusing heterogeneous information from different modalities can help overcome each sensor’s limitations, such as the incapability of cameras to perceive objects in low light situations or the sensitivity of LiDARs to adverse weather conditions like rain or fog.</p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p id="S1.p5.1" class="ltx_p">However, all these advances also bring with them new challenges that must be addressed, such as extrinsic calibrations between sensors of different types <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, the synchronization of all sensors to obtain information at known time instants <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, and the merging of data and detections between sensors of the same and different types <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. Nevertheless, the potential benefits of these perception systems for autonomous driving, including increased safety and efficiency promising.</p>
</div>
<div id="S1.p6" class="ltx_para ltx_noindent">
<p id="S1.p6.1" class="ltx_p">In this paper, we propose a solution to address the last mentioned challenge, which manages the entire space around the vehicle by merging detections in areas covered by more than one camera of the same type and would otherwise be detected in duplicate or with partial information.
The work is based on a two-step 3D detection pipeline <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, which uses a 2D detection model in the camera modality to generate candidates so that the LiDAR cloud can be filtered into smaller regions of interest to estimate final 3D boxes efficiently. While the performance of this approach was generally robust in a variety of traffic situations, the architecture suffered when objects appeared on the overlapping areas of consecutive cameras due to truncated or duplicate detections.</p>
</div>
<div id="S1.p7" class="ltx_para ltx_noindent">
<p id="S1.p7.1" class="ltx_p">To tackle this pitfall, the image detection network is modified to include the re-identification module presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> as a third branch.
In the second step, the point cloud subsets associated with those detections identified as belonging to the same obstacle are merged. That way, the filtered 3D information for each obstacle does not suffer truncations due to limits in the horizontal field of view (HFOV) of the cameras.
As a result, the proposed pipeline ensures that the estimation of the parameters of every obstacle is based on its complete representation in the 3D space, avoiding the inference of a single instance from multiple partial views.</p>
</div>
<div id="S1.p8" class="ltx_para ltx_noindent">
<p id="S1.p8.1" class="ltx_p">The remainder of this paper is organized as follows. In Section <a href="#S2" title="2 RELATED WORK ‣ Joint object detection and re-identification for 3D obstacle multi-camera systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, a brief review of related work is provided. Section <a href="#S3" title="3 PROPOSED APPROACH ‣ Joint object detection and re-identification for 3D obstacle multi-camera systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> presents a general overview of the proposed algorithm. Section <a href="#S4" title="4 EXPERIMENTAL RESULTS ‣ Joint object detection and re-identification for 3D obstacle multi-camera systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> provides experimental results that assess the performance of the method. Finally, conclusions and open issues are discussed in Section <a href="#S5" title="5 CONCLUSIONS ‣ Joint object detection and re-identification for 3D obstacle multi-camera systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>RELATED WORK</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">Several significant advancements have been noted in recent years in the field of 3D obstacle detection with multi-camera systems. The first topic examines the evolution of in-vehicle object detection datasets. Compared to older datasets like KITTI, newer ones provide detailed annotations under diverse conditions. The next area tackles the challenge of managing multiple detections of a single object. Several techniques, from improved Non-Maximal Suppression methods to the innovative Deterministic Point Process, have been developed to address this. The final section explores Siamese networks’ role in re-identifying obstacles, a vital task when dealing with views from multiple cameras. These networks have shown their value by distinguishing small differences among objects in crowded settings.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>In-Vehicle Object Detection Datasets</h3>

<div id="S2.SS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.p1.1" class="ltx_p">The pursuit for greater automation levels in the automotive industry has led to a demand for richer annotated datasets that allow perception systems to cope with more complex traffic scenarios.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS1.p2.1" class="ltx_p">Compared to the classic KITTI benchmark, with daytime-only frames and focused on the forward direction, recent datasets have multiple cameras and LiDARs, recorded in both day and night situations, in rain or fog:
Datasets like Waymo <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> consist of data from 5 LiDARs and 5 cameras, annotated both in 2D and 3D. Argoverse <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> features two LiDARs and seven cameras with 3D box annotations. PandaSet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> combines data from six cameras and two LiDARs, providing annotations per LiDAR point and 3D boxes. Similarly, the nuScenes dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> integrates one LiDAR, six cameras, and multiple radars, offering annotated 3D boxes for each object.
These datasets reflect a growing trend toward more complex and information-rich perception systems, leveraging multiple sensors to capture a more complex, complete, and detailed view of the surrounding environment. However, with the inclusion of multiple cameras comes the inherent challenge of efficiently handling and processing redundant and sometimes conflicting information between views. Duplicate detections, occlusions, and truncation of objects at the edges of the field of view are common problems that arise in multi-camera configurations. In addition, variability in lighting and environmental conditions, such as night, rain, or fog, adds another layer of complexity to data processing and analysis.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Elimination of Multiple Detections for the Same Element</h3>

<div id="S2.SS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.p1.1" class="ltx_p">Several papers have explored techniques related to reducing the number of detections for the same element. The suppression of redundant detections is essential to ensure accurate and consistent perception.
In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, a variant of the traditional Non-Maximal Suppression (NMS) method called Soft-NMS is proposed. Unlike traditional NMS which discards detections based on a predefined threshold, Soft-NMS modifies detection scores continuously, resulting in improved detection accuracy.
In crowded environments, pedestrian detection can be challenging due to multiple overlapping detections. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, this problem is addressed by dynamically adapting the NMS threshold according to the density of detections in the local region, enabling better discrimination between nearby pedestrians.
In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, the use of Deterministic Point Process (DPP) is introduced as an alternative to traditional NMS. DPP selects a subset of detections that are representative and diverse, which can be beneficial in crowded scenarios where detections overlap.
Finally, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> focuses on learning pairwise relationships between detections to improve accuracy in crowded scenes. The method can reduce false detections and improve discrimination between nearby objects by explicitly modeling these relationships.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Siamese Networks for Obstacle Re-Identification</h3>

<div id="S2.SS3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS3.p1.1" class="ltx_p">On multi-camera systems, the management of multiple detections of a single object needs to be handled in a different fashion, as NMS techniques cannot be applied to bounding boxes belonging to images captured from different perspectives (e.g. distributed along the roof of the car). Even so, there are usually parts of the environment that are seen from two cameras, due to an overlap between their horizontal fields of view. For specific situations where the cameras are in close proximity to each other, methods such as image-stitching can be used to form a large panoramic image composed of images from several cameras <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. In a more general approach, this problem is often solved through the use of agent re-identification networks, which allow to get rid of redundant detections based on feature similarities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para ltx_noindent">
<p id="S2.SS3.p2.1" class="ltx_p">Siamese networks are a powerful tool in object re-identification, especially in scenarios with multiple obstacles and crowded environments.
In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, Siamese networks are used to learn to recognize objects from a single example by leveraging the network structure to compare images. This capability is essential in scenarios where data collection is costly or impractical.
Another example is <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, where this type of network is applied to differentiate between similar vehicles. By learning relative distances, the network is able to identify subtle differences between vehicles that at first glance appear identical. This technique is especially useful in applications such as urban surveillance, where accurate vehicle re-identification is essential.
Examples of the use of Siamese networks can be seen in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> or in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, where re-identification is approached from a "parts" perspective: instead of treating the person as a whole, the network is trained to recognize and compare individual parts (such as head, torso, legs), allowing for greater robustness against varied occlusions and postures.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2310.05785/assets/figures/global-graph.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="126" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Complete detection pipeline</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>PROPOSED APPROACH</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">In this paper, we attempt to address the issues identified in the two-step multicamera-lidar detection system presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> by endowing the image detector with a new re-identification branch to better handle duplicate or truncated detections by the HFOV of the cameras.</p>
</div>
<div id="S3.p2" class="ltx_para ltx_noindent">
<p id="S3.p2.1" class="ltx_p">In the selected pipeline a 3D obstacle detection is performed in two steps; the first one consists of a generation of image proposals, obtained with the Faster R-CNN network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> in each of the vehicle’s cameras.</p>
</div>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p id="S3.p3.1" class="ltx_p">Subsequently, by means of the extrinsic calibration parameters, a set of frustums will be obtained by filtering the LiDAR point cloud with each of the detected bounding boxes, which will be used as input to the second step, the Frustum PointNets network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, obtaining a 3D box for each of the proposed obstacles. A complete outlook of the whole pipeline can be seen in Figure <a href="#S2.F1" title="Figure 1 ‣ 2.3 Siamese Networks for Obstacle Re-Identification ‣ 2 RELATED WORK ‣ Joint object detection and re-identification for 3D obstacle multi-camera systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S3.p4" class="ltx_para ltx_noindent">
<p id="S3.p4.1" class="ltx_p">Ideally, all the proposals would be unique for each obstacle and would cover it completely, surrounding the whole area of the object visible from the ego-car. Unfortunately, this is not feasible, partly because of the possible failures of the detector in the image, but also due to the limited horizontal field of view (HFOV) of the cameras, which will generate truncations in the detections and possible duplicates in obstacle detections that appear in several contiguous cameras at the same time.</p>
</div>
<div id="S3.p5" class="ltx_para ltx_noindent">
<p id="S3.p5.1" class="ltx_p">To tackle this, this work integrates the siaNMS module presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, allowing the end-to-end training of the model. In addition, incorporating the module as a branch of the original network ensures that the learned feature map encodes meaningful characteristics for the three tasks: class prediction, bounding box regression, and re-identification. Furthermore, utilizing this information when filtering the point cloud facilitates obtaining a single frustum per obstacle by combining the matched regions of interest from both images.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Re-Identification Branch Description</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.1" class="ltx_p">The objective of the re-identification branch introduced in the detection network is to obtain an embedding that encodes the information obtained from each obstacle. The network is trained in such a way that the embeddings generated for the different detections of the same object are very similar
while maximizing the differences between detections of distinct objects.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p">For this purpose, we start from a Faster R-CNN detection network structure, composed of a convolutional backbone, from which regions of interest are obtained by means of a Region Proposal Network (RPN) stage. The encoded tensor is cut using these candidates and an ROI Align layer is used to obtain a feature map of constant size for each proposal. These maps are fed to the Fully Connected (FC) layers stage, from which we obtain the outputs of the network: in the case of the original network that was the class of the object and its bounding box.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.1" class="ltx_p">In this paper, a third output branch is added tailored for re-identification. This branch is also fed with the fixed-size feature maps obtained for each network proposal and consists of a series of convolutional and FC layers that compress the information of each obstacle into an embedding, with fixed output dimensions, <span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_italic">d</span>. A schematic of the modified network structure can be seen in Figure <a href="#S3.F2" title="Figure 2 ‣ 3.2 Training Data Organization ‣ 3 PROPOSED APPROACH ‣ Joint object detection and re-identification for 3D obstacle multi-camera systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Training Data Organization</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p">In the original 2D stage, a set of annotated images is passed as input in a randomized order through the network (grouped in batches) until the entire data set (epoch) is completed. This is repeated as many times as necessary until the network is sufficiently trained.
Although this is the standard procedure for conventional image detectors, this way of ordering the data is unsuitable for the purpose of this paper since we need the same object to appear several times in the same forward pass of the network so that the re-identification branch can be trained.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2310.05785/assets/figures/sianms-graph.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="150" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Image detection network</figcaption>
</figure>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.1" class="ltx_p">Intuitively, in order to train the class and bounding box branches we need the annotations of the obstacles present in each image. However, to train the re-identification branch the type of annotation needed is different, namely which pairs of annotations correspond to the same obstacle and which do not. Therefore, in each batch of images, we need examples of both negative pairs of detections, which do not correspond to the same object, and positive ones, which are detections of the same object.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.p3.1" class="ltx_p">In this study, we have used the nuScenes dataset, composed of 6 cameras distributed in such a way that they cover 360º around the car.
Taking advantage of this, we have trained the network using batches of 6 images, one corresponding to each of the surrounding cameras. This way, the input of the model during the training process matches the one that will be used in the inference, while objects appearing in regions shared by contiguous cameras can serve as positive pairs for the training.
That said, the order of time instants for the 6-image batches is randomized, mimicking the behavior of traditional CNN training.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Network Training and Loss Function Definition</h3>

<div id="S3.SS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.p1.1" class="ltx_p">During training, the process explained below is performed for each batch of images: First, a forward pass is made, and with the results obtained at the output of the class and box branches, a loss is calculated for each image individually:</p>
</div>
<div id="S3.SS3.p2" class="ltx_para ltx_noindent">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.5" class="ltx_Math" alttext="\mathcal{L}_{box\_head}=\sum_{i=0}^{F}\mathcal{L}_{box\_reg,i}+\sum_{i=0}^{F+B}\mathcal{L}_{cls,i}," display="block"><semantics id="S3.E1.m1.5a"><mrow id="S3.E1.m1.5.5.1" xref="S3.E1.m1.5.5.1.1.cmml"><mrow id="S3.E1.m1.5.5.1.1" xref="S3.E1.m1.5.5.1.1.cmml"><msub id="S3.E1.m1.5.5.1.1.2" xref="S3.E1.m1.5.5.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.5.5.1.1.2.2" xref="S3.E1.m1.5.5.1.1.2.2.cmml">ℒ</mi><mrow id="S3.E1.m1.5.5.1.1.2.3" xref="S3.E1.m1.5.5.1.1.2.3.cmml"><mi id="S3.E1.m1.5.5.1.1.2.3.2" xref="S3.E1.m1.5.5.1.1.2.3.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.5.5.1.1.2.3.1" xref="S3.E1.m1.5.5.1.1.2.3.1.cmml">​</mo><mi id="S3.E1.m1.5.5.1.1.2.3.3" xref="S3.E1.m1.5.5.1.1.2.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.5.5.1.1.2.3.1a" xref="S3.E1.m1.5.5.1.1.2.3.1.cmml">​</mo><mi id="S3.E1.m1.5.5.1.1.2.3.4" xref="S3.E1.m1.5.5.1.1.2.3.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.5.5.1.1.2.3.1b" xref="S3.E1.m1.5.5.1.1.2.3.1.cmml">​</mo><mi mathvariant="normal" id="S3.E1.m1.5.5.1.1.2.3.5" xref="S3.E1.m1.5.5.1.1.2.3.5.cmml">_</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.5.5.1.1.2.3.1c" xref="S3.E1.m1.5.5.1.1.2.3.1.cmml">​</mo><mi id="S3.E1.m1.5.5.1.1.2.3.6" xref="S3.E1.m1.5.5.1.1.2.3.6.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.5.5.1.1.2.3.1d" xref="S3.E1.m1.5.5.1.1.2.3.1.cmml">​</mo><mi id="S3.E1.m1.5.5.1.1.2.3.7" xref="S3.E1.m1.5.5.1.1.2.3.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.5.5.1.1.2.3.1e" xref="S3.E1.m1.5.5.1.1.2.3.1.cmml">​</mo><mi id="S3.E1.m1.5.5.1.1.2.3.8" xref="S3.E1.m1.5.5.1.1.2.3.8.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.5.5.1.1.2.3.1f" xref="S3.E1.m1.5.5.1.1.2.3.1.cmml">​</mo><mi id="S3.E1.m1.5.5.1.1.2.3.9" xref="S3.E1.m1.5.5.1.1.2.3.9.cmml">d</mi></mrow></msub><mo rspace="0.111em" id="S3.E1.m1.5.5.1.1.1" xref="S3.E1.m1.5.5.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.5.5.1.1.3" xref="S3.E1.m1.5.5.1.1.3.cmml"><mrow id="S3.E1.m1.5.5.1.1.3.2" xref="S3.E1.m1.5.5.1.1.3.2.cmml"><munderover id="S3.E1.m1.5.5.1.1.3.2.1" xref="S3.E1.m1.5.5.1.1.3.2.1.cmml"><mo movablelimits="false" id="S3.E1.m1.5.5.1.1.3.2.1.2.2" xref="S3.E1.m1.5.5.1.1.3.2.1.2.2.cmml">∑</mo><mrow id="S3.E1.m1.5.5.1.1.3.2.1.2.3" xref="S3.E1.m1.5.5.1.1.3.2.1.2.3.cmml"><mi id="S3.E1.m1.5.5.1.1.3.2.1.2.3.2" xref="S3.E1.m1.5.5.1.1.3.2.1.2.3.2.cmml">i</mi><mo id="S3.E1.m1.5.5.1.1.3.2.1.2.3.1" xref="S3.E1.m1.5.5.1.1.3.2.1.2.3.1.cmml">=</mo><mn id="S3.E1.m1.5.5.1.1.3.2.1.2.3.3" xref="S3.E1.m1.5.5.1.1.3.2.1.2.3.3.cmml">0</mn></mrow><mi id="S3.E1.m1.5.5.1.1.3.2.1.3" xref="S3.E1.m1.5.5.1.1.3.2.1.3.cmml">F</mi></munderover><msub id="S3.E1.m1.5.5.1.1.3.2.2" xref="S3.E1.m1.5.5.1.1.3.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.5.5.1.1.3.2.2.2" xref="S3.E1.m1.5.5.1.1.3.2.2.2.cmml">ℒ</mi><mrow id="S3.E1.m1.2.2.2.2" xref="S3.E1.m1.2.2.2.3.cmml"><mrow id="S3.E1.m1.2.2.2.2.1" xref="S3.E1.m1.2.2.2.2.1.cmml"><mi id="S3.E1.m1.2.2.2.2.1.2" xref="S3.E1.m1.2.2.2.2.1.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.1.1" xref="S3.E1.m1.2.2.2.2.1.1.cmml">​</mo><mi id="S3.E1.m1.2.2.2.2.1.3" xref="S3.E1.m1.2.2.2.2.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.1.1a" xref="S3.E1.m1.2.2.2.2.1.1.cmml">​</mo><mi id="S3.E1.m1.2.2.2.2.1.4" xref="S3.E1.m1.2.2.2.2.1.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.1.1b" xref="S3.E1.m1.2.2.2.2.1.1.cmml">​</mo><mi mathvariant="normal" id="S3.E1.m1.2.2.2.2.1.5" xref="S3.E1.m1.2.2.2.2.1.5.cmml">_</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.1.1c" xref="S3.E1.m1.2.2.2.2.1.1.cmml">​</mo><mi id="S3.E1.m1.2.2.2.2.1.6" xref="S3.E1.m1.2.2.2.2.1.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.1.1d" xref="S3.E1.m1.2.2.2.2.1.1.cmml">​</mo><mi id="S3.E1.m1.2.2.2.2.1.7" xref="S3.E1.m1.2.2.2.2.1.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.1.1e" xref="S3.E1.m1.2.2.2.2.1.1.cmml">​</mo><mi id="S3.E1.m1.2.2.2.2.1.8" xref="S3.E1.m1.2.2.2.2.1.8.cmml">g</mi></mrow><mo id="S3.E1.m1.2.2.2.2.2" xref="S3.E1.m1.2.2.2.3.cmml">,</mo><mi id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml">i</mi></mrow></msub></mrow><mo rspace="0.055em" id="S3.E1.m1.5.5.1.1.3.1" xref="S3.E1.m1.5.5.1.1.3.1.cmml">+</mo><mrow id="S3.E1.m1.5.5.1.1.3.3" xref="S3.E1.m1.5.5.1.1.3.3.cmml"><munderover id="S3.E1.m1.5.5.1.1.3.3.1" xref="S3.E1.m1.5.5.1.1.3.3.1.cmml"><mo movablelimits="false" id="S3.E1.m1.5.5.1.1.3.3.1.2.2" xref="S3.E1.m1.5.5.1.1.3.3.1.2.2.cmml">∑</mo><mrow id="S3.E1.m1.5.5.1.1.3.3.1.2.3" xref="S3.E1.m1.5.5.1.1.3.3.1.2.3.cmml"><mi id="S3.E1.m1.5.5.1.1.3.3.1.2.3.2" xref="S3.E1.m1.5.5.1.1.3.3.1.2.3.2.cmml">i</mi><mo id="S3.E1.m1.5.5.1.1.3.3.1.2.3.1" xref="S3.E1.m1.5.5.1.1.3.3.1.2.3.1.cmml">=</mo><mn id="S3.E1.m1.5.5.1.1.3.3.1.2.3.3" xref="S3.E1.m1.5.5.1.1.3.3.1.2.3.3.cmml">0</mn></mrow><mrow id="S3.E1.m1.5.5.1.1.3.3.1.3" xref="S3.E1.m1.5.5.1.1.3.3.1.3.cmml"><mi id="S3.E1.m1.5.5.1.1.3.3.1.3.2" xref="S3.E1.m1.5.5.1.1.3.3.1.3.2.cmml">F</mi><mo id="S3.E1.m1.5.5.1.1.3.3.1.3.1" xref="S3.E1.m1.5.5.1.1.3.3.1.3.1.cmml">+</mo><mi id="S3.E1.m1.5.5.1.1.3.3.1.3.3" xref="S3.E1.m1.5.5.1.1.3.3.1.3.3.cmml">B</mi></mrow></munderover><msub id="S3.E1.m1.5.5.1.1.3.3.2" xref="S3.E1.m1.5.5.1.1.3.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.5.5.1.1.3.3.2.2" xref="S3.E1.m1.5.5.1.1.3.3.2.2.cmml">ℒ</mi><mrow id="S3.E1.m1.4.4.2.2" xref="S3.E1.m1.4.4.2.3.cmml"><mrow id="S3.E1.m1.4.4.2.2.1" xref="S3.E1.m1.4.4.2.2.1.cmml"><mi id="S3.E1.m1.4.4.2.2.1.2" xref="S3.E1.m1.4.4.2.2.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.2.2.1.1" xref="S3.E1.m1.4.4.2.2.1.1.cmml">​</mo><mi id="S3.E1.m1.4.4.2.2.1.3" xref="S3.E1.m1.4.4.2.2.1.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.2.2.1.1a" xref="S3.E1.m1.4.4.2.2.1.1.cmml">​</mo><mi id="S3.E1.m1.4.4.2.2.1.4" xref="S3.E1.m1.4.4.2.2.1.4.cmml">s</mi></mrow><mo id="S3.E1.m1.4.4.2.2.2" xref="S3.E1.m1.4.4.2.3.cmml">,</mo><mi id="S3.E1.m1.3.3.1.1" xref="S3.E1.m1.3.3.1.1.cmml">i</mi></mrow></msub></mrow></mrow></mrow><mo id="S3.E1.m1.5.5.1.2" xref="S3.E1.m1.5.5.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.5b"><apply id="S3.E1.m1.5.5.1.1.cmml" xref="S3.E1.m1.5.5.1"><eq id="S3.E1.m1.5.5.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1"></eq><apply id="S3.E1.m1.5.5.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.2.1.cmml" xref="S3.E1.m1.5.5.1.1.2">subscript</csymbol><ci id="S3.E1.m1.5.5.1.1.2.2.cmml" xref="S3.E1.m1.5.5.1.1.2.2">ℒ</ci><apply id="S3.E1.m1.5.5.1.1.2.3.cmml" xref="S3.E1.m1.5.5.1.1.2.3"><times id="S3.E1.m1.5.5.1.1.2.3.1.cmml" xref="S3.E1.m1.5.5.1.1.2.3.1"></times><ci id="S3.E1.m1.5.5.1.1.2.3.2.cmml" xref="S3.E1.m1.5.5.1.1.2.3.2">𝑏</ci><ci id="S3.E1.m1.5.5.1.1.2.3.3.cmml" xref="S3.E1.m1.5.5.1.1.2.3.3">𝑜</ci><ci id="S3.E1.m1.5.5.1.1.2.3.4.cmml" xref="S3.E1.m1.5.5.1.1.2.3.4">𝑥</ci><ci id="S3.E1.m1.5.5.1.1.2.3.5.cmml" xref="S3.E1.m1.5.5.1.1.2.3.5">_</ci><ci id="S3.E1.m1.5.5.1.1.2.3.6.cmml" xref="S3.E1.m1.5.5.1.1.2.3.6">ℎ</ci><ci id="S3.E1.m1.5.5.1.1.2.3.7.cmml" xref="S3.E1.m1.5.5.1.1.2.3.7">𝑒</ci><ci id="S3.E1.m1.5.5.1.1.2.3.8.cmml" xref="S3.E1.m1.5.5.1.1.2.3.8">𝑎</ci><ci id="S3.E1.m1.5.5.1.1.2.3.9.cmml" xref="S3.E1.m1.5.5.1.1.2.3.9">𝑑</ci></apply></apply><apply id="S3.E1.m1.5.5.1.1.3.cmml" xref="S3.E1.m1.5.5.1.1.3"><plus id="S3.E1.m1.5.5.1.1.3.1.cmml" xref="S3.E1.m1.5.5.1.1.3.1"></plus><apply id="S3.E1.m1.5.5.1.1.3.2.cmml" xref="S3.E1.m1.5.5.1.1.3.2"><apply id="S3.E1.m1.5.5.1.1.3.2.1.cmml" xref="S3.E1.m1.5.5.1.1.3.2.1"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.3.2.1.1.cmml" xref="S3.E1.m1.5.5.1.1.3.2.1">superscript</csymbol><apply id="S3.E1.m1.5.5.1.1.3.2.1.2.cmml" xref="S3.E1.m1.5.5.1.1.3.2.1"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.3.2.1.2.1.cmml" xref="S3.E1.m1.5.5.1.1.3.2.1">subscript</csymbol><sum id="S3.E1.m1.5.5.1.1.3.2.1.2.2.cmml" xref="S3.E1.m1.5.5.1.1.3.2.1.2.2"></sum><apply id="S3.E1.m1.5.5.1.1.3.2.1.2.3.cmml" xref="S3.E1.m1.5.5.1.1.3.2.1.2.3"><eq id="S3.E1.m1.5.5.1.1.3.2.1.2.3.1.cmml" xref="S3.E1.m1.5.5.1.1.3.2.1.2.3.1"></eq><ci id="S3.E1.m1.5.5.1.1.3.2.1.2.3.2.cmml" xref="S3.E1.m1.5.5.1.1.3.2.1.2.3.2">𝑖</ci><cn type="integer" id="S3.E1.m1.5.5.1.1.3.2.1.2.3.3.cmml" xref="S3.E1.m1.5.5.1.1.3.2.1.2.3.3">0</cn></apply></apply><ci id="S3.E1.m1.5.5.1.1.3.2.1.3.cmml" xref="S3.E1.m1.5.5.1.1.3.2.1.3">𝐹</ci></apply><apply id="S3.E1.m1.5.5.1.1.3.2.2.cmml" xref="S3.E1.m1.5.5.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.3.2.2.1.cmml" xref="S3.E1.m1.5.5.1.1.3.2.2">subscript</csymbol><ci id="S3.E1.m1.5.5.1.1.3.2.2.2.cmml" xref="S3.E1.m1.5.5.1.1.3.2.2.2">ℒ</ci><list id="S3.E1.m1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.2"><apply id="S3.E1.m1.2.2.2.2.1.cmml" xref="S3.E1.m1.2.2.2.2.1"><times id="S3.E1.m1.2.2.2.2.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1"></times><ci id="S3.E1.m1.2.2.2.2.1.2.cmml" xref="S3.E1.m1.2.2.2.2.1.2">𝑏</ci><ci id="S3.E1.m1.2.2.2.2.1.3.cmml" xref="S3.E1.m1.2.2.2.2.1.3">𝑜</ci><ci id="S3.E1.m1.2.2.2.2.1.4.cmml" xref="S3.E1.m1.2.2.2.2.1.4">𝑥</ci><ci id="S3.E1.m1.2.2.2.2.1.5.cmml" xref="S3.E1.m1.2.2.2.2.1.5">_</ci><ci id="S3.E1.m1.2.2.2.2.1.6.cmml" xref="S3.E1.m1.2.2.2.2.1.6">𝑟</ci><ci id="S3.E1.m1.2.2.2.2.1.7.cmml" xref="S3.E1.m1.2.2.2.2.1.7">𝑒</ci><ci id="S3.E1.m1.2.2.2.2.1.8.cmml" xref="S3.E1.m1.2.2.2.2.1.8">𝑔</ci></apply><ci id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1">𝑖</ci></list></apply></apply><apply id="S3.E1.m1.5.5.1.1.3.3.cmml" xref="S3.E1.m1.5.5.1.1.3.3"><apply id="S3.E1.m1.5.5.1.1.3.3.1.cmml" xref="S3.E1.m1.5.5.1.1.3.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.3.3.1.1.cmml" xref="S3.E1.m1.5.5.1.1.3.3.1">superscript</csymbol><apply id="S3.E1.m1.5.5.1.1.3.3.1.2.cmml" xref="S3.E1.m1.5.5.1.1.3.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.3.3.1.2.1.cmml" xref="S3.E1.m1.5.5.1.1.3.3.1">subscript</csymbol><sum id="S3.E1.m1.5.5.1.1.3.3.1.2.2.cmml" xref="S3.E1.m1.5.5.1.1.3.3.1.2.2"></sum><apply id="S3.E1.m1.5.5.1.1.3.3.1.2.3.cmml" xref="S3.E1.m1.5.5.1.1.3.3.1.2.3"><eq id="S3.E1.m1.5.5.1.1.3.3.1.2.3.1.cmml" xref="S3.E1.m1.5.5.1.1.3.3.1.2.3.1"></eq><ci id="S3.E1.m1.5.5.1.1.3.3.1.2.3.2.cmml" xref="S3.E1.m1.5.5.1.1.3.3.1.2.3.2">𝑖</ci><cn type="integer" id="S3.E1.m1.5.5.1.1.3.3.1.2.3.3.cmml" xref="S3.E1.m1.5.5.1.1.3.3.1.2.3.3">0</cn></apply></apply><apply id="S3.E1.m1.5.5.1.1.3.3.1.3.cmml" xref="S3.E1.m1.5.5.1.1.3.3.1.3"><plus id="S3.E1.m1.5.5.1.1.3.3.1.3.1.cmml" xref="S3.E1.m1.5.5.1.1.3.3.1.3.1"></plus><ci id="S3.E1.m1.5.5.1.1.3.3.1.3.2.cmml" xref="S3.E1.m1.5.5.1.1.3.3.1.3.2">𝐹</ci><ci id="S3.E1.m1.5.5.1.1.3.3.1.3.3.cmml" xref="S3.E1.m1.5.5.1.1.3.3.1.3.3">𝐵</ci></apply></apply><apply id="S3.E1.m1.5.5.1.1.3.3.2.cmml" xref="S3.E1.m1.5.5.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.3.3.2.1.cmml" xref="S3.E1.m1.5.5.1.1.3.3.2">subscript</csymbol><ci id="S3.E1.m1.5.5.1.1.3.3.2.2.cmml" xref="S3.E1.m1.5.5.1.1.3.3.2.2">ℒ</ci><list id="S3.E1.m1.4.4.2.3.cmml" xref="S3.E1.m1.4.4.2.2"><apply id="S3.E1.m1.4.4.2.2.1.cmml" xref="S3.E1.m1.4.4.2.2.1"><times id="S3.E1.m1.4.4.2.2.1.1.cmml" xref="S3.E1.m1.4.4.2.2.1.1"></times><ci id="S3.E1.m1.4.4.2.2.1.2.cmml" xref="S3.E1.m1.4.4.2.2.1.2">𝑐</ci><ci id="S3.E1.m1.4.4.2.2.1.3.cmml" xref="S3.E1.m1.4.4.2.2.1.3">𝑙</ci><ci id="S3.E1.m1.4.4.2.2.1.4.cmml" xref="S3.E1.m1.4.4.2.2.1.4">𝑠</ci></apply><ci id="S3.E1.m1.3.3.1.1.cmml" xref="S3.E1.m1.3.3.1.1">𝑖</ci></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.5c">\mathcal{L}_{box\_head}=\sum_{i=0}^{F}\mathcal{L}_{box\_reg,i}+\sum_{i=0}^{F+B}\mathcal{L}_{cls,i},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p2.4" class="ltx_p">where <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="F" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mi id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><ci id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">F</annotation></semantics></math> and <math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><mi id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><ci id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">B</annotation></semantics></math> are the numbers of foreground and background detections in that image, <math id="S3.SS3.p2.3.m3.2" class="ltx_Math" alttext="\mathcal{L}_{box\_reg,i}" display="inline"><semantics id="S3.SS3.p2.3.m3.2a"><msub id="S3.SS3.p2.3.m3.2.3" xref="S3.SS3.p2.3.m3.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p2.3.m3.2.3.2" xref="S3.SS3.p2.3.m3.2.3.2.cmml">ℒ</mi><mrow id="S3.SS3.p2.3.m3.2.2.2.2" xref="S3.SS3.p2.3.m3.2.2.2.3.cmml"><mrow id="S3.SS3.p2.3.m3.2.2.2.2.1" xref="S3.SS3.p2.3.m3.2.2.2.2.1.cmml"><mi id="S3.SS3.p2.3.m3.2.2.2.2.1.2" xref="S3.SS3.p2.3.m3.2.2.2.2.1.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.3.m3.2.2.2.2.1.1" xref="S3.SS3.p2.3.m3.2.2.2.2.1.1.cmml">​</mo><mi id="S3.SS3.p2.3.m3.2.2.2.2.1.3" xref="S3.SS3.p2.3.m3.2.2.2.2.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.3.m3.2.2.2.2.1.1a" xref="S3.SS3.p2.3.m3.2.2.2.2.1.1.cmml">​</mo><mi id="S3.SS3.p2.3.m3.2.2.2.2.1.4" xref="S3.SS3.p2.3.m3.2.2.2.2.1.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.3.m3.2.2.2.2.1.1b" xref="S3.SS3.p2.3.m3.2.2.2.2.1.1.cmml">​</mo><mi mathvariant="normal" id="S3.SS3.p2.3.m3.2.2.2.2.1.5" xref="S3.SS3.p2.3.m3.2.2.2.2.1.5.cmml">_</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.3.m3.2.2.2.2.1.1c" xref="S3.SS3.p2.3.m3.2.2.2.2.1.1.cmml">​</mo><mi id="S3.SS3.p2.3.m3.2.2.2.2.1.6" xref="S3.SS3.p2.3.m3.2.2.2.2.1.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.3.m3.2.2.2.2.1.1d" xref="S3.SS3.p2.3.m3.2.2.2.2.1.1.cmml">​</mo><mi id="S3.SS3.p2.3.m3.2.2.2.2.1.7" xref="S3.SS3.p2.3.m3.2.2.2.2.1.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.3.m3.2.2.2.2.1.1e" xref="S3.SS3.p2.3.m3.2.2.2.2.1.1.cmml">​</mo><mi id="S3.SS3.p2.3.m3.2.2.2.2.1.8" xref="S3.SS3.p2.3.m3.2.2.2.2.1.8.cmml">g</mi></mrow><mo id="S3.SS3.p2.3.m3.2.2.2.2.2" xref="S3.SS3.p2.3.m3.2.2.2.3.cmml">,</mo><mi id="S3.SS3.p2.3.m3.1.1.1.1" xref="S3.SS3.p2.3.m3.1.1.1.1.cmml">i</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.2b"><apply id="S3.SS3.p2.3.m3.2.3.cmml" xref="S3.SS3.p2.3.m3.2.3"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.2.3.1.cmml" xref="S3.SS3.p2.3.m3.2.3">subscript</csymbol><ci id="S3.SS3.p2.3.m3.2.3.2.cmml" xref="S3.SS3.p2.3.m3.2.3.2">ℒ</ci><list id="S3.SS3.p2.3.m3.2.2.2.3.cmml" xref="S3.SS3.p2.3.m3.2.2.2.2"><apply id="S3.SS3.p2.3.m3.2.2.2.2.1.cmml" xref="S3.SS3.p2.3.m3.2.2.2.2.1"><times id="S3.SS3.p2.3.m3.2.2.2.2.1.1.cmml" xref="S3.SS3.p2.3.m3.2.2.2.2.1.1"></times><ci id="S3.SS3.p2.3.m3.2.2.2.2.1.2.cmml" xref="S3.SS3.p2.3.m3.2.2.2.2.1.2">𝑏</ci><ci id="S3.SS3.p2.3.m3.2.2.2.2.1.3.cmml" xref="S3.SS3.p2.3.m3.2.2.2.2.1.3">𝑜</ci><ci id="S3.SS3.p2.3.m3.2.2.2.2.1.4.cmml" xref="S3.SS3.p2.3.m3.2.2.2.2.1.4">𝑥</ci><ci id="S3.SS3.p2.3.m3.2.2.2.2.1.5.cmml" xref="S3.SS3.p2.3.m3.2.2.2.2.1.5">_</ci><ci id="S3.SS3.p2.3.m3.2.2.2.2.1.6.cmml" xref="S3.SS3.p2.3.m3.2.2.2.2.1.6">𝑟</ci><ci id="S3.SS3.p2.3.m3.2.2.2.2.1.7.cmml" xref="S3.SS3.p2.3.m3.2.2.2.2.1.7">𝑒</ci><ci id="S3.SS3.p2.3.m3.2.2.2.2.1.8.cmml" xref="S3.SS3.p2.3.m3.2.2.2.2.1.8">𝑔</ci></apply><ci id="S3.SS3.p2.3.m3.1.1.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1.1.1">𝑖</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.2c">\mathcal{L}_{box\_reg,i}</annotation></semantics></math> is the Smooth-L1 Loss, and <math id="S3.SS3.p2.4.m4.2" class="ltx_Math" alttext="\mathcal{L}_{cls,i}" display="inline"><semantics id="S3.SS3.p2.4.m4.2a"><msub id="S3.SS3.p2.4.m4.2.3" xref="S3.SS3.p2.4.m4.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p2.4.m4.2.3.2" xref="S3.SS3.p2.4.m4.2.3.2.cmml">ℒ</mi><mrow id="S3.SS3.p2.4.m4.2.2.2.2" xref="S3.SS3.p2.4.m4.2.2.2.3.cmml"><mrow id="S3.SS3.p2.4.m4.2.2.2.2.1" xref="S3.SS3.p2.4.m4.2.2.2.2.1.cmml"><mi id="S3.SS3.p2.4.m4.2.2.2.2.1.2" xref="S3.SS3.p2.4.m4.2.2.2.2.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.4.m4.2.2.2.2.1.1" xref="S3.SS3.p2.4.m4.2.2.2.2.1.1.cmml">​</mo><mi id="S3.SS3.p2.4.m4.2.2.2.2.1.3" xref="S3.SS3.p2.4.m4.2.2.2.2.1.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.4.m4.2.2.2.2.1.1a" xref="S3.SS3.p2.4.m4.2.2.2.2.1.1.cmml">​</mo><mi id="S3.SS3.p2.4.m4.2.2.2.2.1.4" xref="S3.SS3.p2.4.m4.2.2.2.2.1.4.cmml">s</mi></mrow><mo id="S3.SS3.p2.4.m4.2.2.2.2.2" xref="S3.SS3.p2.4.m4.2.2.2.3.cmml">,</mo><mi id="S3.SS3.p2.4.m4.1.1.1.1" xref="S3.SS3.p2.4.m4.1.1.1.1.cmml">i</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.2b"><apply id="S3.SS3.p2.4.m4.2.3.cmml" xref="S3.SS3.p2.4.m4.2.3"><csymbol cd="ambiguous" id="S3.SS3.p2.4.m4.2.3.1.cmml" xref="S3.SS3.p2.4.m4.2.3">subscript</csymbol><ci id="S3.SS3.p2.4.m4.2.3.2.cmml" xref="S3.SS3.p2.4.m4.2.3.2">ℒ</ci><list id="S3.SS3.p2.4.m4.2.2.2.3.cmml" xref="S3.SS3.p2.4.m4.2.2.2.2"><apply id="S3.SS3.p2.4.m4.2.2.2.2.1.cmml" xref="S3.SS3.p2.4.m4.2.2.2.2.1"><times id="S3.SS3.p2.4.m4.2.2.2.2.1.1.cmml" xref="S3.SS3.p2.4.m4.2.2.2.2.1.1"></times><ci id="S3.SS3.p2.4.m4.2.2.2.2.1.2.cmml" xref="S3.SS3.p2.4.m4.2.2.2.2.1.2">𝑐</ci><ci id="S3.SS3.p2.4.m4.2.2.2.2.1.3.cmml" xref="S3.SS3.p2.4.m4.2.2.2.2.1.3">𝑙</ci><ci id="S3.SS3.p2.4.m4.2.2.2.2.1.4.cmml" xref="S3.SS3.p2.4.m4.2.2.2.2.1.4">𝑠</ci></apply><ci id="S3.SS3.p2.4.m4.1.1.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1.1.1">𝑖</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.2c">\mathcal{L}_{cls,i}</annotation></semantics></math> is the Cross-Entropy Loss.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para ltx_noindent">
<p id="S3.SS3.p3.1" class="ltx_p">Once the loss of the detections of the batch images has been acquired, the loss of re-identification between the detections of the 6 images is calculated. The total loss of the batch is the sum of all the losses of the individual images plus the re-identification loss:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.3" class="ltx_Math" alttext="\mathcal{L}_{batch}=\mathcal{L}_{reID}+\sum_{n=0}^{N}\mathcal{L}_{box\_head,n}," display="block"><semantics id="S3.E2.m1.3a"><mrow id="S3.E2.m1.3.3.1" xref="S3.E2.m1.3.3.1.1.cmml"><mrow id="S3.E2.m1.3.3.1.1" xref="S3.E2.m1.3.3.1.1.cmml"><msub id="S3.E2.m1.3.3.1.1.2" xref="S3.E2.m1.3.3.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.3.3.1.1.2.2" xref="S3.E2.m1.3.3.1.1.2.2.cmml">ℒ</mi><mrow id="S3.E2.m1.3.3.1.1.2.3" xref="S3.E2.m1.3.3.1.1.2.3.cmml"><mi id="S3.E2.m1.3.3.1.1.2.3.2" xref="S3.E2.m1.3.3.1.1.2.3.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.1.1.2.3.1" xref="S3.E2.m1.3.3.1.1.2.3.1.cmml">​</mo><mi id="S3.E2.m1.3.3.1.1.2.3.3" xref="S3.E2.m1.3.3.1.1.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.1.1.2.3.1a" xref="S3.E2.m1.3.3.1.1.2.3.1.cmml">​</mo><mi id="S3.E2.m1.3.3.1.1.2.3.4" xref="S3.E2.m1.3.3.1.1.2.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.1.1.2.3.1b" xref="S3.E2.m1.3.3.1.1.2.3.1.cmml">​</mo><mi id="S3.E2.m1.3.3.1.1.2.3.5" xref="S3.E2.m1.3.3.1.1.2.3.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.1.1.2.3.1c" xref="S3.E2.m1.3.3.1.1.2.3.1.cmml">​</mo><mi id="S3.E2.m1.3.3.1.1.2.3.6" xref="S3.E2.m1.3.3.1.1.2.3.6.cmml">h</mi></mrow></msub><mo id="S3.E2.m1.3.3.1.1.1" xref="S3.E2.m1.3.3.1.1.1.cmml">=</mo><mrow id="S3.E2.m1.3.3.1.1.3" xref="S3.E2.m1.3.3.1.1.3.cmml"><msub id="S3.E2.m1.3.3.1.1.3.2" xref="S3.E2.m1.3.3.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.3.3.1.1.3.2.2" xref="S3.E2.m1.3.3.1.1.3.2.2.cmml">ℒ</mi><mrow id="S3.E2.m1.3.3.1.1.3.2.3" xref="S3.E2.m1.3.3.1.1.3.2.3.cmml"><mi id="S3.E2.m1.3.3.1.1.3.2.3.2" xref="S3.E2.m1.3.3.1.1.3.2.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.1.1.3.2.3.1" xref="S3.E2.m1.3.3.1.1.3.2.3.1.cmml">​</mo><mi id="S3.E2.m1.3.3.1.1.3.2.3.3" xref="S3.E2.m1.3.3.1.1.3.2.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.1.1.3.2.3.1a" xref="S3.E2.m1.3.3.1.1.3.2.3.1.cmml">​</mo><mi id="S3.E2.m1.3.3.1.1.3.2.3.4" xref="S3.E2.m1.3.3.1.1.3.2.3.4.cmml">I</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.1.1.3.2.3.1b" xref="S3.E2.m1.3.3.1.1.3.2.3.1.cmml">​</mo><mi id="S3.E2.m1.3.3.1.1.3.2.3.5" xref="S3.E2.m1.3.3.1.1.3.2.3.5.cmml">D</mi></mrow></msub><mo rspace="0.055em" id="S3.E2.m1.3.3.1.1.3.1" xref="S3.E2.m1.3.3.1.1.3.1.cmml">+</mo><mrow id="S3.E2.m1.3.3.1.1.3.3" xref="S3.E2.m1.3.3.1.1.3.3.cmml"><munderover id="S3.E2.m1.3.3.1.1.3.3.1" xref="S3.E2.m1.3.3.1.1.3.3.1.cmml"><mo movablelimits="false" id="S3.E2.m1.3.3.1.1.3.3.1.2.2" xref="S3.E2.m1.3.3.1.1.3.3.1.2.2.cmml">∑</mo><mrow id="S3.E2.m1.3.3.1.1.3.3.1.2.3" xref="S3.E2.m1.3.3.1.1.3.3.1.2.3.cmml"><mi id="S3.E2.m1.3.3.1.1.3.3.1.2.3.2" xref="S3.E2.m1.3.3.1.1.3.3.1.2.3.2.cmml">n</mi><mo id="S3.E2.m1.3.3.1.1.3.3.1.2.3.1" xref="S3.E2.m1.3.3.1.1.3.3.1.2.3.1.cmml">=</mo><mn id="S3.E2.m1.3.3.1.1.3.3.1.2.3.3" xref="S3.E2.m1.3.3.1.1.3.3.1.2.3.3.cmml">0</mn></mrow><mi id="S3.E2.m1.3.3.1.1.3.3.1.3" xref="S3.E2.m1.3.3.1.1.3.3.1.3.cmml">N</mi></munderover><msub id="S3.E2.m1.3.3.1.1.3.3.2" xref="S3.E2.m1.3.3.1.1.3.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.3.3.1.1.3.3.2.2" xref="S3.E2.m1.3.3.1.1.3.3.2.2.cmml">ℒ</mi><mrow id="S3.E2.m1.2.2.2.2" xref="S3.E2.m1.2.2.2.3.cmml"><mrow id="S3.E2.m1.2.2.2.2.1" xref="S3.E2.m1.2.2.2.2.1.cmml"><mi id="S3.E2.m1.2.2.2.2.1.2" xref="S3.E2.m1.2.2.2.2.1.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.2.2.1.1" xref="S3.E2.m1.2.2.2.2.1.1.cmml">​</mo><mi id="S3.E2.m1.2.2.2.2.1.3" xref="S3.E2.m1.2.2.2.2.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.2.2.1.1a" xref="S3.E2.m1.2.2.2.2.1.1.cmml">​</mo><mi id="S3.E2.m1.2.2.2.2.1.4" xref="S3.E2.m1.2.2.2.2.1.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.2.2.1.1b" xref="S3.E2.m1.2.2.2.2.1.1.cmml">​</mo><mi mathvariant="normal" id="S3.E2.m1.2.2.2.2.1.5" xref="S3.E2.m1.2.2.2.2.1.5.cmml">_</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.2.2.1.1c" xref="S3.E2.m1.2.2.2.2.1.1.cmml">​</mo><mi id="S3.E2.m1.2.2.2.2.1.6" xref="S3.E2.m1.2.2.2.2.1.6.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.2.2.1.1d" xref="S3.E2.m1.2.2.2.2.1.1.cmml">​</mo><mi id="S3.E2.m1.2.2.2.2.1.7" xref="S3.E2.m1.2.2.2.2.1.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.2.2.1.1e" xref="S3.E2.m1.2.2.2.2.1.1.cmml">​</mo><mi id="S3.E2.m1.2.2.2.2.1.8" xref="S3.E2.m1.2.2.2.2.1.8.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.2.2.1.1f" xref="S3.E2.m1.2.2.2.2.1.1.cmml">​</mo><mi id="S3.E2.m1.2.2.2.2.1.9" xref="S3.E2.m1.2.2.2.2.1.9.cmml">d</mi></mrow><mo id="S3.E2.m1.2.2.2.2.2" xref="S3.E2.m1.2.2.2.3.cmml">,</mo><mi id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml">n</mi></mrow></msub></mrow></mrow></mrow><mo id="S3.E2.m1.3.3.1.2" xref="S3.E2.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.3b"><apply id="S3.E2.m1.3.3.1.1.cmml" xref="S3.E2.m1.3.3.1"><eq id="S3.E2.m1.3.3.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1"></eq><apply id="S3.E2.m1.3.3.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.2.1.cmml" xref="S3.E2.m1.3.3.1.1.2">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.2.2.cmml" xref="S3.E2.m1.3.3.1.1.2.2">ℒ</ci><apply id="S3.E2.m1.3.3.1.1.2.3.cmml" xref="S3.E2.m1.3.3.1.1.2.3"><times id="S3.E2.m1.3.3.1.1.2.3.1.cmml" xref="S3.E2.m1.3.3.1.1.2.3.1"></times><ci id="S3.E2.m1.3.3.1.1.2.3.2.cmml" xref="S3.E2.m1.3.3.1.1.2.3.2">𝑏</ci><ci id="S3.E2.m1.3.3.1.1.2.3.3.cmml" xref="S3.E2.m1.3.3.1.1.2.3.3">𝑎</ci><ci id="S3.E2.m1.3.3.1.1.2.3.4.cmml" xref="S3.E2.m1.3.3.1.1.2.3.4">𝑡</ci><ci id="S3.E2.m1.3.3.1.1.2.3.5.cmml" xref="S3.E2.m1.3.3.1.1.2.3.5">𝑐</ci><ci id="S3.E2.m1.3.3.1.1.2.3.6.cmml" xref="S3.E2.m1.3.3.1.1.2.3.6">ℎ</ci></apply></apply><apply id="S3.E2.m1.3.3.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.3"><plus id="S3.E2.m1.3.3.1.1.3.1.cmml" xref="S3.E2.m1.3.3.1.1.3.1"></plus><apply id="S3.E2.m1.3.3.1.1.3.2.cmml" xref="S3.E2.m1.3.3.1.1.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.3.2.1.cmml" xref="S3.E2.m1.3.3.1.1.3.2">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.3.2.2.cmml" xref="S3.E2.m1.3.3.1.1.3.2.2">ℒ</ci><apply id="S3.E2.m1.3.3.1.1.3.2.3.cmml" xref="S3.E2.m1.3.3.1.1.3.2.3"><times id="S3.E2.m1.3.3.1.1.3.2.3.1.cmml" xref="S3.E2.m1.3.3.1.1.3.2.3.1"></times><ci id="S3.E2.m1.3.3.1.1.3.2.3.2.cmml" xref="S3.E2.m1.3.3.1.1.3.2.3.2">𝑟</ci><ci id="S3.E2.m1.3.3.1.1.3.2.3.3.cmml" xref="S3.E2.m1.3.3.1.1.3.2.3.3">𝑒</ci><ci id="S3.E2.m1.3.3.1.1.3.2.3.4.cmml" xref="S3.E2.m1.3.3.1.1.3.2.3.4">𝐼</ci><ci id="S3.E2.m1.3.3.1.1.3.2.3.5.cmml" xref="S3.E2.m1.3.3.1.1.3.2.3.5">𝐷</ci></apply></apply><apply id="S3.E2.m1.3.3.1.1.3.3.cmml" xref="S3.E2.m1.3.3.1.1.3.3"><apply id="S3.E2.m1.3.3.1.1.3.3.1.cmml" xref="S3.E2.m1.3.3.1.1.3.3.1"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.3.3.1.1.cmml" xref="S3.E2.m1.3.3.1.1.3.3.1">superscript</csymbol><apply id="S3.E2.m1.3.3.1.1.3.3.1.2.cmml" xref="S3.E2.m1.3.3.1.1.3.3.1"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.3.3.1.2.1.cmml" xref="S3.E2.m1.3.3.1.1.3.3.1">subscript</csymbol><sum id="S3.E2.m1.3.3.1.1.3.3.1.2.2.cmml" xref="S3.E2.m1.3.3.1.1.3.3.1.2.2"></sum><apply id="S3.E2.m1.3.3.1.1.3.3.1.2.3.cmml" xref="S3.E2.m1.3.3.1.1.3.3.1.2.3"><eq id="S3.E2.m1.3.3.1.1.3.3.1.2.3.1.cmml" xref="S3.E2.m1.3.3.1.1.3.3.1.2.3.1"></eq><ci id="S3.E2.m1.3.3.1.1.3.3.1.2.3.2.cmml" xref="S3.E2.m1.3.3.1.1.3.3.1.2.3.2">𝑛</ci><cn type="integer" id="S3.E2.m1.3.3.1.1.3.3.1.2.3.3.cmml" xref="S3.E2.m1.3.3.1.1.3.3.1.2.3.3">0</cn></apply></apply><ci id="S3.E2.m1.3.3.1.1.3.3.1.3.cmml" xref="S3.E2.m1.3.3.1.1.3.3.1.3">𝑁</ci></apply><apply id="S3.E2.m1.3.3.1.1.3.3.2.cmml" xref="S3.E2.m1.3.3.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.3.3.2.1.cmml" xref="S3.E2.m1.3.3.1.1.3.3.2">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.3.3.2.2.cmml" xref="S3.E2.m1.3.3.1.1.3.3.2.2">ℒ</ci><list id="S3.E2.m1.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.2"><apply id="S3.E2.m1.2.2.2.2.1.cmml" xref="S3.E2.m1.2.2.2.2.1"><times id="S3.E2.m1.2.2.2.2.1.1.cmml" xref="S3.E2.m1.2.2.2.2.1.1"></times><ci id="S3.E2.m1.2.2.2.2.1.2.cmml" xref="S3.E2.m1.2.2.2.2.1.2">𝑏</ci><ci id="S3.E2.m1.2.2.2.2.1.3.cmml" xref="S3.E2.m1.2.2.2.2.1.3">𝑜</ci><ci id="S3.E2.m1.2.2.2.2.1.4.cmml" xref="S3.E2.m1.2.2.2.2.1.4">𝑥</ci><ci id="S3.E2.m1.2.2.2.2.1.5.cmml" xref="S3.E2.m1.2.2.2.2.1.5">_</ci><ci id="S3.E2.m1.2.2.2.2.1.6.cmml" xref="S3.E2.m1.2.2.2.2.1.6">ℎ</ci><ci id="S3.E2.m1.2.2.2.2.1.7.cmml" xref="S3.E2.m1.2.2.2.2.1.7">𝑒</ci><ci id="S3.E2.m1.2.2.2.2.1.8.cmml" xref="S3.E2.m1.2.2.2.2.1.8">𝑎</ci><ci id="S3.E2.m1.2.2.2.2.1.9.cmml" xref="S3.E2.m1.2.2.2.2.1.9">𝑑</ci></apply><ci id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1">𝑛</ci></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.3c">\mathcal{L}_{batch}=\mathcal{L}_{reID}+\sum_{n=0}^{N}\mathcal{L}_{box\_head,n},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p3.2" class="ltx_p">where N is the number of images in the batch, 6 for this case. This way, the gradients calculated will take into account the information on the three kinds of output generated by the network.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para ltx_noindent">
<p id="S3.SS3.p4.1" class="ltx_p">To calculate the re-identification loss, all the possible pairs between the foreground detections (those with an IoU &gt; 0.7 with the Ground Truth) are obtained, that is, all the possible combinations between the detections of the batch images. The loss will be the sum of the individual losses of all positive pairs and the same number of negative pairs. To select which negative pairs are used, their losses are sorted in decreasing order and the ones with the highest loss value are chosen, following an Online Hard Example Mining technique (OHEM), as presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>,</p>
</div>
<div id="S3.SS3.p5" class="ltx_para ltx_noindent">
<p id="S3.SS3.p5.1" class="ltx_p">Then, the Double Margin Contrastive Loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> is calculated for each pair, as explained in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>:</p>
</div>
<div id="S3.SS3.p6" class="ltx_para ltx_noindent">
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.2" class="ltx_math_unparsed" alttext="\mathcal{L}_{reID}=\frac{1}{2}\sum_{i}^{N}\Big{[}\max\left(\left\|f\left(x_{i}^{r}\right)-f\left(x_{i}^{p}\right)\right\|_{2}-\alpha,0\right)^{2}+\max\left(\beta-\left\|f\left(x_{i}^{r}\right)-f\left(x_{i}^{n}\right)\right\|_{2},0\right)^{2}\Big{]}," display="block"><semantics id="S3.E3.m1.2a"><mrow id="S3.E3.m1.2b"><msub id="S3.E3.m1.2.3"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.2.3.2">ℒ</mi><mrow id="S3.E3.m1.2.3.3"><mi id="S3.E3.m1.2.3.3.2">r</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.3.3.1">​</mo><mi id="S3.E3.m1.2.3.3.3">e</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.3.3.1a">​</mo><mi id="S3.E3.m1.2.3.3.4">I</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.3.3.1b">​</mo><mi id="S3.E3.m1.2.3.3.5">D</mi></mrow></msub><mo id="S3.E3.m1.2.4">=</mo><mfrac id="S3.E3.m1.2.5"><mn id="S3.E3.m1.2.5.2">1</mn><mn id="S3.E3.m1.2.5.3">2</mn></mfrac><munderover id="S3.E3.m1.2.6"><mo movablelimits="false" rspace="0em" id="S3.E3.m1.2.6.2.2">∑</mo><mi id="S3.E3.m1.2.6.2.3">i</mi><mi id="S3.E3.m1.2.6.3">N</mi></munderover><mrow id="S3.E3.m1.2.7"><mo maxsize="160%" minsize="160%" id="S3.E3.m1.2.7.1">[</mo><mi id="S3.E3.m1.1.1">max</mi><msup id="S3.E3.m1.2.7.2"><mrow id="S3.E3.m1.2.7.2.2"><mo id="S3.E3.m1.2.7.2.2.1">(</mo><mo lspace="0em" rspace="0.167em" stretchy="true" id="S3.E3.m1.2.7.2.2.2">∥</mo><mi id="S3.E3.m1.2.7.2.2.3">f</mi><mrow id="S3.E3.m1.2.7.2.2.4"><mo id="S3.E3.m1.2.7.2.2.4.1">(</mo><msubsup id="S3.E3.m1.2.7.2.2.4.2"><mi id="S3.E3.m1.2.7.2.2.4.2.2.2">x</mi><mi id="S3.E3.m1.2.7.2.2.4.2.2.3">i</mi><mi id="S3.E3.m1.2.7.2.2.4.2.3">r</mi></msubsup><mo id="S3.E3.m1.2.7.2.2.4.3">)</mo></mrow><mo id="S3.E3.m1.2.7.2.2.5">−</mo><mi id="S3.E3.m1.2.7.2.2.6">f</mi><mrow id="S3.E3.m1.2.7.2.2.7"><mo id="S3.E3.m1.2.7.2.2.7.1">(</mo><msubsup id="S3.E3.m1.2.7.2.2.7.2"><mi id="S3.E3.m1.2.7.2.2.7.2.2.2">x</mi><mi id="S3.E3.m1.2.7.2.2.7.2.2.3">i</mi><mi id="S3.E3.m1.2.7.2.2.7.2.3">p</mi></msubsup><mo id="S3.E3.m1.2.7.2.2.7.3">)</mo></mrow><msub id="S3.E3.m1.2.7.2.2.8"><mo lspace="0em" rspace="0em" stretchy="true" id="S3.E3.m1.2.7.2.2.8.2">∥</mo><mn id="S3.E3.m1.2.7.2.2.8.3">2</mn></msub><mo lspace="0em" id="S3.E3.m1.2.7.2.2.9">−</mo><mi id="S3.E3.m1.2.7.2.2.10">α</mi><mo id="S3.E3.m1.2.7.2.2.11">,</mo><mn id="S3.E3.m1.2.2">0</mn><mo id="S3.E3.m1.2.7.2.2.12">)</mo></mrow><mn id="S3.E3.m1.2.7.2.3">2</mn></msup><mo id="S3.E3.m1.2.7.3">+</mo><mi id="S3.E3.m1.2.7.4">max</mi><msup id="S3.E3.m1.2.7.5"><mrow id="S3.E3.m1.2.7.5.2"><mo id="S3.E3.m1.2.7.5.2.1">(</mo><mi id="S3.E3.m1.2.7.5.2.2">β</mi><mo rspace="0em" id="S3.E3.m1.2.7.5.2.3">−</mo><mo lspace="0em" rspace="0.167em" stretchy="true" id="S3.E3.m1.2.7.5.2.4">∥</mo><mi id="S3.E3.m1.2.7.5.2.5">f</mi><mrow id="S3.E3.m1.2.7.5.2.6"><mo id="S3.E3.m1.2.7.5.2.6.1">(</mo><msubsup id="S3.E3.m1.2.7.5.2.6.2"><mi id="S3.E3.m1.2.7.5.2.6.2.2.2">x</mi><mi id="S3.E3.m1.2.7.5.2.6.2.2.3">i</mi><mi id="S3.E3.m1.2.7.5.2.6.2.3">r</mi></msubsup><mo id="S3.E3.m1.2.7.5.2.6.3">)</mo></mrow><mo id="S3.E3.m1.2.7.5.2.7">−</mo><mi id="S3.E3.m1.2.7.5.2.8">f</mi><mrow id="S3.E3.m1.2.7.5.2.9"><mo id="S3.E3.m1.2.7.5.2.9.1">(</mo><msubsup id="S3.E3.m1.2.7.5.2.9.2"><mi id="S3.E3.m1.2.7.5.2.9.2.2.2">x</mi><mi id="S3.E3.m1.2.7.5.2.9.2.2.3">i</mi><mi id="S3.E3.m1.2.7.5.2.9.2.3">n</mi></msubsup><mo id="S3.E3.m1.2.7.5.2.9.3">)</mo></mrow><msub id="S3.E3.m1.2.7.5.2.10"><mo lspace="0em" rspace="0.167em" stretchy="true" id="S3.E3.m1.2.7.5.2.10.2">∥</mo><mn id="S3.E3.m1.2.7.5.2.10.3">2</mn></msub><mo id="S3.E3.m1.2.7.5.2.11">,</mo><mn id="S3.E3.m1.2.7.5.2.12">0</mn><mo id="S3.E3.m1.2.7.5.2.13">)</mo></mrow><mn id="S3.E3.m1.2.7.5.3">2</mn></msup><mo maxsize="160%" minsize="160%" id="S3.E3.m1.2.7.6">]</mo></mrow><mo id="S3.E3.m1.2.8">,</mo></mrow><annotation encoding="application/x-tex" id="S3.E3.m1.2c">\mathcal{L}_{reID}=\frac{1}{2}\sum_{i}^{N}\Big{[}\max\left(\left\|f\left(x_{i}^{r}\right)-f\left(x_{i}^{p}\right)\right\|_{2}-\alpha,0\right)^{2}+\max\left(\beta-\left\|f\left(x_{i}^{r}\right)-f\left(x_{i}^{n}\right)\right\|_{2},0\right)^{2}\Big{]},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p6.5" class="ltx_p">where <math id="S3.SS3.p6.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS3.p6.1.m1.1a"><mi id="S3.SS3.p6.1.m1.1.1" xref="S3.SS3.p6.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.1.m1.1b"><ci id="S3.SS3.p6.1.m1.1.1.cmml" xref="S3.SS3.p6.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.1.m1.1c">\alpha</annotation></semantics></math> and <math id="S3.SS3.p6.2.m2.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S3.SS3.p6.2.m2.1a"><mi id="S3.SS3.p6.2.m2.1.1" xref="S3.SS3.p6.2.m2.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.2.m2.1b"><ci id="S3.SS3.p6.2.m2.1.1.cmml" xref="S3.SS3.p6.2.m2.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.2.m2.1c">\beta</annotation></semantics></math> are the two constant margins and <math id="S3.SS3.p6.3.m3.1" class="ltx_Math" alttext="f\left(x_{i}^{r}\right)" display="inline"><semantics id="S3.SS3.p6.3.m3.1a"><mrow id="S3.SS3.p6.3.m3.1.1" xref="S3.SS3.p6.3.m3.1.1.cmml"><mi id="S3.SS3.p6.3.m3.1.1.3" xref="S3.SS3.p6.3.m3.1.1.3.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p6.3.m3.1.1.2" xref="S3.SS3.p6.3.m3.1.1.2.cmml">​</mo><mrow id="S3.SS3.p6.3.m3.1.1.1.1" xref="S3.SS3.p6.3.m3.1.1.1.1.1.cmml"><mo id="S3.SS3.p6.3.m3.1.1.1.1.2" xref="S3.SS3.p6.3.m3.1.1.1.1.1.cmml">(</mo><msubsup id="S3.SS3.p6.3.m3.1.1.1.1.1" xref="S3.SS3.p6.3.m3.1.1.1.1.1.cmml"><mi id="S3.SS3.p6.3.m3.1.1.1.1.1.2.2" xref="S3.SS3.p6.3.m3.1.1.1.1.1.2.2.cmml">x</mi><mi id="S3.SS3.p6.3.m3.1.1.1.1.1.2.3" xref="S3.SS3.p6.3.m3.1.1.1.1.1.2.3.cmml">i</mi><mi id="S3.SS3.p6.3.m3.1.1.1.1.1.3" xref="S3.SS3.p6.3.m3.1.1.1.1.1.3.cmml">r</mi></msubsup><mo id="S3.SS3.p6.3.m3.1.1.1.1.3" xref="S3.SS3.p6.3.m3.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.3.m3.1b"><apply id="S3.SS3.p6.3.m3.1.1.cmml" xref="S3.SS3.p6.3.m3.1.1"><times id="S3.SS3.p6.3.m3.1.1.2.cmml" xref="S3.SS3.p6.3.m3.1.1.2"></times><ci id="S3.SS3.p6.3.m3.1.1.3.cmml" xref="S3.SS3.p6.3.m3.1.1.3">𝑓</ci><apply id="S3.SS3.p6.3.m3.1.1.1.1.1.cmml" xref="S3.SS3.p6.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p6.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS3.p6.3.m3.1.1.1.1">superscript</csymbol><apply id="S3.SS3.p6.3.m3.1.1.1.1.1.2.cmml" xref="S3.SS3.p6.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p6.3.m3.1.1.1.1.1.2.1.cmml" xref="S3.SS3.p6.3.m3.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p6.3.m3.1.1.1.1.1.2.2.cmml" xref="S3.SS3.p6.3.m3.1.1.1.1.1.2.2">𝑥</ci><ci id="S3.SS3.p6.3.m3.1.1.1.1.1.2.3.cmml" xref="S3.SS3.p6.3.m3.1.1.1.1.1.2.3">𝑖</ci></apply><ci id="S3.SS3.p6.3.m3.1.1.1.1.1.3.cmml" xref="S3.SS3.p6.3.m3.1.1.1.1.1.3">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.3.m3.1c">f\left(x_{i}^{r}\right)</annotation></semantics></math>, <math id="S3.SS3.p6.4.m4.1" class="ltx_Math" alttext="f\left(x_{i}^{p}\right)" display="inline"><semantics id="S3.SS3.p6.4.m4.1a"><mrow id="S3.SS3.p6.4.m4.1.1" xref="S3.SS3.p6.4.m4.1.1.cmml"><mi id="S3.SS3.p6.4.m4.1.1.3" xref="S3.SS3.p6.4.m4.1.1.3.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p6.4.m4.1.1.2" xref="S3.SS3.p6.4.m4.1.1.2.cmml">​</mo><mrow id="S3.SS3.p6.4.m4.1.1.1.1" xref="S3.SS3.p6.4.m4.1.1.1.1.1.cmml"><mo id="S3.SS3.p6.4.m4.1.1.1.1.2" xref="S3.SS3.p6.4.m4.1.1.1.1.1.cmml">(</mo><msubsup id="S3.SS3.p6.4.m4.1.1.1.1.1" xref="S3.SS3.p6.4.m4.1.1.1.1.1.cmml"><mi id="S3.SS3.p6.4.m4.1.1.1.1.1.2.2" xref="S3.SS3.p6.4.m4.1.1.1.1.1.2.2.cmml">x</mi><mi id="S3.SS3.p6.4.m4.1.1.1.1.1.2.3" xref="S3.SS3.p6.4.m4.1.1.1.1.1.2.3.cmml">i</mi><mi id="S3.SS3.p6.4.m4.1.1.1.1.1.3" xref="S3.SS3.p6.4.m4.1.1.1.1.1.3.cmml">p</mi></msubsup><mo id="S3.SS3.p6.4.m4.1.1.1.1.3" xref="S3.SS3.p6.4.m4.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.4.m4.1b"><apply id="S3.SS3.p6.4.m4.1.1.cmml" xref="S3.SS3.p6.4.m4.1.1"><times id="S3.SS3.p6.4.m4.1.1.2.cmml" xref="S3.SS3.p6.4.m4.1.1.2"></times><ci id="S3.SS3.p6.4.m4.1.1.3.cmml" xref="S3.SS3.p6.4.m4.1.1.3">𝑓</ci><apply id="S3.SS3.p6.4.m4.1.1.1.1.1.cmml" xref="S3.SS3.p6.4.m4.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p6.4.m4.1.1.1.1.1.1.cmml" xref="S3.SS3.p6.4.m4.1.1.1.1">superscript</csymbol><apply id="S3.SS3.p6.4.m4.1.1.1.1.1.2.cmml" xref="S3.SS3.p6.4.m4.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p6.4.m4.1.1.1.1.1.2.1.cmml" xref="S3.SS3.p6.4.m4.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p6.4.m4.1.1.1.1.1.2.2.cmml" xref="S3.SS3.p6.4.m4.1.1.1.1.1.2.2">𝑥</ci><ci id="S3.SS3.p6.4.m4.1.1.1.1.1.2.3.cmml" xref="S3.SS3.p6.4.m4.1.1.1.1.1.2.3">𝑖</ci></apply><ci id="S3.SS3.p6.4.m4.1.1.1.1.1.3.cmml" xref="S3.SS3.p6.4.m4.1.1.1.1.1.3">𝑝</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.4.m4.1c">f\left(x_{i}^{p}\right)</annotation></semantics></math> and <math id="S3.SS3.p6.5.m5.1" class="ltx_Math" alttext="f\left(x_{i}^{n}\right)" display="inline"><semantics id="S3.SS3.p6.5.m5.1a"><mrow id="S3.SS3.p6.5.m5.1.1" xref="S3.SS3.p6.5.m5.1.1.cmml"><mi id="S3.SS3.p6.5.m5.1.1.3" xref="S3.SS3.p6.5.m5.1.1.3.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p6.5.m5.1.1.2" xref="S3.SS3.p6.5.m5.1.1.2.cmml">​</mo><mrow id="S3.SS3.p6.5.m5.1.1.1.1" xref="S3.SS3.p6.5.m5.1.1.1.1.1.cmml"><mo id="S3.SS3.p6.5.m5.1.1.1.1.2" xref="S3.SS3.p6.5.m5.1.1.1.1.1.cmml">(</mo><msubsup id="S3.SS3.p6.5.m5.1.1.1.1.1" xref="S3.SS3.p6.5.m5.1.1.1.1.1.cmml"><mi id="S3.SS3.p6.5.m5.1.1.1.1.1.2.2" xref="S3.SS3.p6.5.m5.1.1.1.1.1.2.2.cmml">x</mi><mi id="S3.SS3.p6.5.m5.1.1.1.1.1.2.3" xref="S3.SS3.p6.5.m5.1.1.1.1.1.2.3.cmml">i</mi><mi id="S3.SS3.p6.5.m5.1.1.1.1.1.3" xref="S3.SS3.p6.5.m5.1.1.1.1.1.3.cmml">n</mi></msubsup><mo id="S3.SS3.p6.5.m5.1.1.1.1.3" xref="S3.SS3.p6.5.m5.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.5.m5.1b"><apply id="S3.SS3.p6.5.m5.1.1.cmml" xref="S3.SS3.p6.5.m5.1.1"><times id="S3.SS3.p6.5.m5.1.1.2.cmml" xref="S3.SS3.p6.5.m5.1.1.2"></times><ci id="S3.SS3.p6.5.m5.1.1.3.cmml" xref="S3.SS3.p6.5.m5.1.1.3">𝑓</ci><apply id="S3.SS3.p6.5.m5.1.1.1.1.1.cmml" xref="S3.SS3.p6.5.m5.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p6.5.m5.1.1.1.1.1.1.cmml" xref="S3.SS3.p6.5.m5.1.1.1.1">superscript</csymbol><apply id="S3.SS3.p6.5.m5.1.1.1.1.1.2.cmml" xref="S3.SS3.p6.5.m5.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p6.5.m5.1.1.1.1.1.2.1.cmml" xref="S3.SS3.p6.5.m5.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p6.5.m5.1.1.1.1.1.2.2.cmml" xref="S3.SS3.p6.5.m5.1.1.1.1.1.2.2">𝑥</ci><ci id="S3.SS3.p6.5.m5.1.1.1.1.1.2.3.cmml" xref="S3.SS3.p6.5.m5.1.1.1.1.1.2.3">𝑖</ci></apply><ci id="S3.SS3.p6.5.m5.1.1.1.1.1.3.cmml" xref="S3.SS3.p6.5.m5.1.1.1.1.1.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.5.m5.1c">f\left(x_{i}^{n}\right)</annotation></semantics></math> are the embeddings of the reference object, their positive pair and the hardest negative pair, respectively.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Re-Identification evaluation</h3>

<div id="S3.SS4.p1" class="ltx_para ltx_noindent">
<p id="S3.SS4.p1.1" class="ltx_p">To evaluate the quality of the re-identifications at the image stage, an algorithm has been developed. It performs the following steps:</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">For each group of images at each instant the detections in contiguous images are compared. This distinction is made to speed up the evaluation procedure since there is no possibility of re-identifiable detections between two images that are not adjacent.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">For each pair of images, all possible combinations between detections of the same class are compared. In this case, no additional geometric constraints are imposed, since we want to test the performance of the embedding generation module and thus the challenge is more significant.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">A matrix of distances between the comparable detected objects is calculated (having eliminated the pairs of different classes in the previous step) and the pairs whose distance is below a threshold are chosen following the Hungarian method.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S3.I1.i4.p1" class="ltx_para ltx_noindent">
<p id="S3.I1.i4.p1.1" class="ltx_p">A comparison of whether the obtained pairs actually correspond to the same object according to the ground truth is made and the statistics of TP, TN, FP, and FN are elaborated.</p>
</div>
</li>
</ol>
<p id="S3.SS4.p2.1" class="ltx_p">A visual example of this process can be seen in Figure <a href="#S3.F3" title="Figure 3 ‣ 3.4 Re-Identification evaluation ‣ 3 PROPOSED APPROACH ‣ Joint object detection and re-identification for 3D obstacle multi-camera systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S3.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.05785/assets/figures/eval_1.png" id="S3.F3.sf1.g1" class="ltx_graphics ltx_img_landscape" width="583" height="55" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>The six images of the same instant, with the detections obtained in each image.</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.05785/assets/figures/eval_2b.png" id="S3.F3.sf2.g1" class="ltx_graphics ltx_img_landscape" width="583" height="138" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Obtained distance matrix between the detections of the front and right cameras</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F3.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.05785/assets/figures/eval_3b.png" id="S3.F3.sf3.g1" class="ltx_graphics ltx_img_landscape" width="583" height="68" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>Re-identified pairs made in the 6 images, that have been marked as True Positives (TP)</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Re-Identification evaluation process step-by-step</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>EXPERIMENTAL RESULTS</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">For evaluation, the nuScenes dataset has been used, as it provides suitable sensor configuration and annotations for assessing the performance of the presented approach: in the 2D space, to validate the re-identification capabilities of the networks and its effect on the baseline model; and in the 3D space, so that it can be compared to traditional alternatives. To accommodate the original label for the 2D experiments, the minimum bounding boxes have been obtained from the projection of the 3D boxes in each of the cameras. Moreover, the unique ID information of each obstacle has been kept to allow subsequent re-identification validation.</p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.1" class="ltx_p">For the experiments of the first part of the configuration, those concerning the 2D detection in the image, the metrics of the KITTI Object <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> dataset will be used. The minimum overlap percentage between detections and annotations required to consider an object as detected is 50% for all classes and the difficulty level selected is Moderate (Min. bounding box height: 25 Px, Max. occlusion level: Partly occluded, Max. truncation: 30%). Although only the 3 most commonly used classes (Pedestrian, Car, and Cyclist) are shown in the tables, the evaluation is performed for all the classes of the nuScenes dataset, and the <span id="S4.p2.1.1" class="ltx_text ltx_font_italic">all</span> column is calculated with the weighted average of all of them. The reID columns have been obtained following the procedure explained in the previous Section <a href="#S3.SS4" title="3.4 Re-Identification evaluation ‣ 3 PROPOSED APPROACH ‣ Joint object detection and re-identification for 3D obstacle multi-camera systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>.</p>
</div>
<div id="S4.p3" class="ltx_para ltx_noindent">
<p id="S4.p3.1" class="ltx_p">In order to evaluate the performance of the whole pipeline after the integration of the re-identification module, the official nuScenes Detection Benchmark metrics are used.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Ablation Studies. Number of Embedding Dimensions</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p">First, an analysis of the adjustment of the hyperparameters of the network has been made, choosing the original configuration of the siaNMS network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> but varying the number of dimensions of the output. Results are obtained for the network detection output and the re-identification branch for each of the output dimensions. In addition, we compare the results with those obtained in an equivalent training but without the re-identification branch included.
This experiment has been performed for two main network configurations: (1) the first one with 2000 pre-NMS proposals and 1000 post-NMS proposals in training, 1000 pre- and post-NMS proposals in test, shown in Table <a href="#S4.T2" title="Table 2 ‣ 4.1 Ablation Studies. Number of Embedding Dimensions ‣ 4 EXPERIMENTAL RESULTS ‣ Joint object detection and re-identification for 3D obstacle multi-camera systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>; and (2) the second with 1000 pre-NMS proposals and 500 post-NMS proposals in training, 500 pre- and post-NMS proposals in the test stage, shown in Table <a href="#S4.T2" title="Table 2 ‣ 4.1 Ablation Studies. Number of Embedding Dimensions ‣ 4 EXPERIMENTAL RESULTS ‣ Joint object detection and re-identification for 3D obstacle multi-camera systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Both configurations have been tested following the KITTI object image detection metrics for all nuScenes classes (Pedestrian, Car, Cycle, Cyclist, Bus, Truck, Construction Vehicle, Trailer, Barrier, and Cone) for the three frontal cameras of the validation split of the nuScenes dataset.
The <em id="S4.SS1.p1.1.1" class="ltx_emph ltx_font_italic">Cycle</em> and <em id="S4.SS1.p1.1.2" class="ltx_emph ltx_font_italic">Cyclist</em> classes have been obtained by selecting the nuScenes <em id="S4.SS1.p1.1.3" class="ltx_emph ltx_font_italic">Bicycle</em> and <em id="S4.SS1.p1.1.4" class="ltx_emph ltx_font_italic">Motorcycle</em> classes and combining them with the <span id="S4.SS1.p1.1.5" class="ltx_text ltx_font_italic">with_rider</span> attribute. This way the <em id="S4.SS1.p1.1.6" class="ltx_emph ltx_font_italic">Bicycle</em> and <em id="S4.SS1.p1.1.7" class="ltx_emph ltx_font_italic">Motorcycle</em> objects with rider are <em id="S4.SS1.p1.1.8" class="ltx_emph ltx_font_italic">Cyclists</em>, and the ones without rider are <em id="S4.SS1.p1.1.9" class="ltx_emph ltx_font_italic">Cycles</em>.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S4.T2.1" class="ltx_tabular ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" rowspan="2"><span id="S4.T2.1.1.1.1.1" class="ltx_text">num dims</span></th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3">Classes (AP [%])</th>
<th id="S4.T2.1.1.1.3" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></th>
<th id="S4.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3">reID</th>
</tr>
<tr id="S4.T2.1.2.2" class="ltx_tr">
<th id="S4.T2.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Ped</th>
<th id="S4.T2.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Car</th>
<th id="S4.T2.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Cyclist</th>
<th id="S4.T2.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">all</th>
<th id="S4.T2.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">precision</th>
<th id="S4.T2.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">recall</th>
<th id="S4.T2.1.2.2.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t">f-score</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.3.1" class="ltx_tr">
<th id="S4.T2.1.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">off</th>
<td id="S4.T2.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t">42,56</td>
<td id="S4.T2.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">51,03</td>
<td id="S4.T2.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">38,01</td>
<td id="S4.T2.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">40,857</td>
<td id="S4.T2.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T2.1.3.1.7" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T2.1.3.1.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S4.T2.1.4.2" class="ltx_tr">
<th id="S4.T2.1.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">10</th>
<td id="S4.T2.1.4.2.2" class="ltx_td ltx_align_center">47,41</td>
<td id="S4.T2.1.4.2.3" class="ltx_td ltx_align_center">56,87</td>
<td id="S4.T2.1.4.2.4" class="ltx_td ltx_align_center">39,28</td>
<td id="S4.T2.1.4.2.5" class="ltx_td ltx_align_center">45,076</td>
<td id="S4.T2.1.4.2.6" class="ltx_td ltx_align_center">0,835</td>
<td id="S4.T2.1.4.2.7" class="ltx_td ltx_align_center">0,847</td>
<td id="S4.T2.1.4.2.8" class="ltx_td ltx_nopad_r ltx_align_center">0,841</td>
</tr>
<tr id="S4.T2.1.5.3" class="ltx_tr">
<th id="S4.T2.1.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">15</th>
<td id="S4.T2.1.5.3.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.5.3.2.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">52,23</span></td>
<td id="S4.T2.1.5.3.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.5.3.3.1" class="ltx_text ltx_font_bold">60,80</span></td>
<td id="S4.T2.1.5.3.4" class="ltx_td ltx_align_center"><span id="S4.T2.1.5.3.4.1" class="ltx_text ltx_font_bold">41,50</span></td>
<td id="S4.T2.1.5.3.5" class="ltx_td ltx_align_center"><span id="S4.T2.1.5.3.5.1" class="ltx_text ltx_font_bold">47,827</span></td>
<td id="S4.T2.1.5.3.6" class="ltx_td ltx_align_center">0,861</td>
<td id="S4.T2.1.5.3.7" class="ltx_td ltx_align_center">0,838</td>
<td id="S4.T2.1.5.3.8" class="ltx_td ltx_nopad_r ltx_align_center">0,850</td>
</tr>
<tr id="S4.T2.1.6.4" class="ltx_tr">
<th id="S4.T2.1.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">20</th>
<td id="S4.T2.1.6.4.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.6.4.2.1" class="ltx_text ltx_font_bold">51,17</span></td>
<td id="S4.T2.1.6.4.3" class="ltx_td ltx_align_center">60,53</td>
<td id="S4.T2.1.6.4.4" class="ltx_td ltx_align_center">41,15</td>
<td id="S4.T2.1.6.4.5" class="ltx_td ltx_align_center">47,381</td>
<td id="S4.T2.1.6.4.6" class="ltx_td ltx_align_center">0,864</td>
<td id="S4.T2.1.6.4.7" class="ltx_td ltx_align_center">0,849</td>
<td id="S4.T2.1.6.4.8" class="ltx_td ltx_nopad_r ltx_align_center">0,857</td>
</tr>
<tr id="S4.T2.1.7.5" class="ltx_tr">
<th id="S4.T2.1.7.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">25</th>
<td id="S4.T2.1.7.5.2" class="ltx_td ltx_align_center">45,20</td>
<td id="S4.T2.1.7.5.3" class="ltx_td ltx_align_center">51,39</td>
<td id="S4.T2.1.7.5.4" class="ltx_td ltx_align_center">34,87</td>
<td id="S4.T2.1.7.5.5" class="ltx_td ltx_align_center">41,282</td>
<td id="S4.T2.1.7.5.6" class="ltx_td ltx_align_center">0,859</td>
<td id="S4.T2.1.7.5.7" class="ltx_td ltx_align_center">0,853</td>
<td id="S4.T2.1.7.5.8" class="ltx_td ltx_nopad_r ltx_align_center">0,856</td>
</tr>
<tr id="S4.T2.1.8.6" class="ltx_tr">
<th id="S4.T2.1.8.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">30</th>
<td id="S4.T2.1.8.6.2" class="ltx_td ltx_align_center">41,79</td>
<td id="S4.T2.1.8.6.3" class="ltx_td ltx_align_center">53,97</td>
<td id="S4.T2.1.8.6.4" class="ltx_td ltx_align_center">35,16</td>
<td id="S4.T2.1.8.6.5" class="ltx_td ltx_align_center">41,872</td>
<td id="S4.T2.1.8.6.6" class="ltx_td ltx_align_center">0,863</td>
<td id="S4.T2.1.8.6.7" class="ltx_td ltx_align_center"><span id="S4.T2.1.8.6.7.1" class="ltx_text ltx_font_bold">0,854</span></td>
<td id="S4.T2.1.8.6.8" class="ltx_td ltx_nopad_r ltx_align_center">0,858</td>
</tr>
<tr id="S4.T2.1.9.7" class="ltx_tr">
<th id="S4.T2.1.9.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">40</th>
<td id="S4.T2.1.9.7.2" class="ltx_td ltx_align_center">50,96</td>
<td id="S4.T2.1.9.7.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.9.7.3.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">63,12</span></td>
<td id="S4.T2.1.9.7.4" class="ltx_td ltx_align_center"><span id="S4.T2.1.9.7.4.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">42,09</span></td>
<td id="S4.T2.1.9.7.5" class="ltx_td ltx_align_center"><span id="S4.T2.1.9.7.5.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">48,501</span></td>
<td id="S4.T2.1.9.7.6" class="ltx_td ltx_align_center">0,870</td>
<td id="S4.T2.1.9.7.7" class="ltx_td ltx_align_center">0,852</td>
<td id="S4.T2.1.9.7.8" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.1.9.7.8.1" class="ltx_text ltx_font_bold">0,861</span></td>
</tr>
<tr id="S4.T2.1.10.8" class="ltx_tr">
<th id="S4.T2.1.10.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">50</th>
<td id="S4.T2.1.10.8.2" class="ltx_td ltx_align_center ltx_border_bb">42,90</td>
<td id="S4.T2.1.10.8.3" class="ltx_td ltx_align_center ltx_border_bb">51,58</td>
<td id="S4.T2.1.10.8.4" class="ltx_td ltx_align_center ltx_border_bb">36,33</td>
<td id="S4.T2.1.10.8.5" class="ltx_td ltx_align_center ltx_border_bb">41,161</td>
<td id="S4.T2.1.10.8.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.1.10.8.6.1" class="ltx_text ltx_font_bold">0,872</span></td>
<td id="S4.T2.1.10.8.7" class="ltx_td ltx_align_center ltx_border_bb">0,845</td>
<td id="S4.T2.1.10.8.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">0,858</td>
</tr>
</tbody>
</table>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Evaluation results for the proposed approach while varying the output layer dimension of the siaNMS branch. 1000 proposals are selected during inference. The best result in each column is marked in bold and underlined and the second best result is in bold only.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S4.T2.2" class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.2.1.1" class="ltx_tr">
<th id="S4.T2.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" rowspan="2"><span id="S4.T2.2.1.1.1.1" class="ltx_text">num dims</span></th>
<th id="S4.T2.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3">Classes (AP [%])</th>
<th id="S4.T2.2.1.1.3" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></th>
<th id="S4.T2.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3">reID</th>
</tr>
<tr id="S4.T2.2.2.2" class="ltx_tr">
<th id="S4.T2.2.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Ped</th>
<th id="S4.T2.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Car</th>
<th id="S4.T2.2.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Cyclist</th>
<th id="S4.T2.2.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">med</th>
<th id="S4.T2.2.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">precision</th>
<th id="S4.T2.2.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">recall</th>
<th id="S4.T2.2.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">f-score</th>
</tr>
<tr id="S4.T2.2.3.3" class="ltx_tr">
<th id="S4.T2.2.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">off</th>
<th id="S4.T2.2.3.3.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">41,56</th>
<th id="S4.T2.2.3.3.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">49,05</th>
<th id="S4.T2.2.3.3.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">31,77</th>
<th id="S4.T2.2.3.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">38,896</th>
<th id="S4.T2.2.3.3.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">-</th>
<th id="S4.T2.2.3.3.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">-</th>
<th id="S4.T2.2.3.3.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">-</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.2.4.1" class="ltx_tr">
<td id="S4.T2.2.4.1.1" class="ltx_td ltx_align_center">10</td>
<td id="S4.T2.2.4.1.2" class="ltx_td ltx_align_right">41,04</td>
<td id="S4.T2.2.4.1.3" class="ltx_td ltx_align_right">48,90</td>
<td id="S4.T2.2.4.1.4" class="ltx_td ltx_align_right">29,51</td>
<td id="S4.T2.2.4.1.5" class="ltx_td ltx_align_center">38,478</td>
<td id="S4.T2.2.4.1.6" class="ltx_td ltx_align_right">0,856</td>
<td id="S4.T2.2.4.1.7" class="ltx_td ltx_align_right">0,852</td>
<td id="S4.T2.2.4.1.8" class="ltx_td ltx_nopad_r ltx_align_right">0,854</td>
</tr>
<tr id="S4.T2.2.5.2" class="ltx_tr">
<td id="S4.T2.2.5.2.1" class="ltx_td ltx_align_center">15</td>
<td id="S4.T2.2.5.2.2" class="ltx_td ltx_align_right">44,22</td>
<td id="S4.T2.2.5.2.3" class="ltx_td ltx_align_right">51,33</td>
<td id="S4.T2.2.5.2.4" class="ltx_td ltx_align_right"><span id="S4.T2.2.5.2.4.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">39,96</span></td>
<td id="S4.T2.2.5.2.5" class="ltx_td ltx_align_center">40,408</td>
<td id="S4.T2.2.5.2.6" class="ltx_td ltx_align_right">0,859</td>
<td id="S4.T2.2.5.2.7" class="ltx_td ltx_align_right">0,849</td>
<td id="S4.T2.2.5.2.8" class="ltx_td ltx_nopad_r ltx_align_right">0,854</td>
</tr>
<tr id="S4.T2.2.6.3" class="ltx_tr">
<td id="S4.T2.2.6.3.1" class="ltx_td ltx_align_center">20</td>
<td id="S4.T2.2.6.3.2" class="ltx_td ltx_align_right">43,74</td>
<td id="S4.T2.2.6.3.3" class="ltx_td ltx_align_right">49,20</td>
<td id="S4.T2.2.6.3.4" class="ltx_td ltx_align_right">29,93</td>
<td id="S4.T2.2.6.3.5" class="ltx_td ltx_align_center">39,666</td>
<td id="S4.T2.2.6.3.6" class="ltx_td ltx_align_right">0,868</td>
<td id="S4.T2.2.6.3.7" class="ltx_td ltx_align_right">0,850</td>
<td id="S4.T2.2.6.3.8" class="ltx_td ltx_nopad_r ltx_align_right">0,859</td>
</tr>
<tr id="S4.T2.2.7.4" class="ltx_tr">
<td id="S4.T2.2.7.4.1" class="ltx_td ltx_align_center">25</td>
<td id="S4.T2.2.7.4.2" class="ltx_td ltx_align_right">43,77</td>
<td id="S4.T2.2.7.4.3" class="ltx_td ltx_align_right">50,96</td>
<td id="S4.T2.2.7.4.4" class="ltx_td ltx_align_right">31,71</td>
<td id="S4.T2.2.7.4.5" class="ltx_td ltx_align_center">40,366</td>
<td id="S4.T2.2.7.4.6" class="ltx_td ltx_align_right"><span id="S4.T2.2.7.4.6.1" class="ltx_text ltx_font_bold">0,873</span></td>
<td id="S4.T2.2.7.4.7" class="ltx_td ltx_align_right"><span id="S4.T2.2.7.4.7.1" class="ltx_text ltx_font_bold">0,874</span></td>
<td id="S4.T2.2.7.4.8" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S4.T2.2.7.4.8.1" class="ltx_text ltx_font_bold">0,874</span></td>
</tr>
<tr id="S4.T2.2.8.5" class="ltx_tr">
<td id="S4.T2.2.8.5.1" class="ltx_td ltx_align_center">30</td>
<td id="S4.T2.2.8.5.2" class="ltx_td ltx_align_right"><span id="S4.T2.2.8.5.2.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">48,69</span></td>
<td id="S4.T2.2.8.5.3" class="ltx_td ltx_align_right"><span id="S4.T2.2.8.5.3.1" class="ltx_text ltx_font_bold">53,23</span></td>
<td id="S4.T2.2.8.5.4" class="ltx_td ltx_align_right">38,75</td>
<td id="S4.T2.2.8.5.5" class="ltx_td ltx_align_center"><span id="S4.T2.2.8.5.5.1" class="ltx_text ltx_font_bold">42,612</span></td>
<td id="S4.T2.2.8.5.6" class="ltx_td ltx_align_right">0,869</td>
<td id="S4.T2.2.8.5.7" class="ltx_td ltx_align_right">0,872</td>
<td id="S4.T2.2.8.5.8" class="ltx_td ltx_nopad_r ltx_align_right">0,870</td>
</tr>
<tr id="S4.T2.2.9.6" class="ltx_tr">
<td id="S4.T2.2.9.6.1" class="ltx_td ltx_align_center">40</td>
<td id="S4.T2.2.9.6.2" class="ltx_td ltx_align_right">43,86</td>
<td id="S4.T2.2.9.6.3" class="ltx_td ltx_align_right">50,91</td>
<td id="S4.T2.2.9.6.4" class="ltx_td ltx_align_right">37,62</td>
<td id="S4.T2.2.9.6.5" class="ltx_td ltx_align_center">40,211</td>
<td id="S4.T2.2.9.6.6" class="ltx_td ltx_align_right">0,872</td>
<td id="S4.T2.2.9.6.7" class="ltx_td ltx_align_right"><span id="S4.T2.2.9.6.7.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">0,876</span></td>
<td id="S4.T2.2.9.6.8" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S4.T2.2.9.6.8.1" class="ltx_text ltx_font_bold">0,874</span></td>
</tr>
<tr id="S4.T2.2.10.7" class="ltx_tr">
<td id="S4.T2.2.10.7.1" class="ltx_td ltx_align_center ltx_border_bb">50</td>
<td id="S4.T2.2.10.7.2" class="ltx_td ltx_align_right ltx_border_bb"><span id="S4.T2.2.10.7.2.1" class="ltx_text ltx_font_bold">47,09</span></td>
<td id="S4.T2.2.10.7.3" class="ltx_td ltx_align_right ltx_border_bb"><span id="S4.T2.2.10.7.3.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">54,48</span></td>
<td id="S4.T2.2.10.7.4" class="ltx_td ltx_align_right ltx_border_bb"><span id="S4.T2.2.10.7.4.1" class="ltx_text ltx_font_bold">39,28</span></td>
<td id="S4.T2.2.10.7.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.2.10.7.5.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">42,699</span></td>
<td id="S4.T2.2.10.7.6" class="ltx_td ltx_align_right ltx_border_bb"><span id="S4.T2.2.10.7.6.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">0,877</span></td>
<td id="S4.T2.2.10.7.7" class="ltx_td ltx_align_right ltx_border_bb">0,855</td>
<td id="S4.T2.2.10.7.8" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb">0,866</td>
</tr>
</tbody>
</table>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Evaluation results for the proposed approach while varying the output layer dimension of the siaNMS branch. 500 proposals are selected during inference. The best result in each column is marked in bold and underlined and the second best result is in bold only.</figcaption>
</figure>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p">As can be seen in Tables <a href="#S4.T2" title="Table 2 ‣ 4.1 Ablation Studies. Number of Embedding Dimensions ‣ 4 EXPERIMENTAL RESULTS ‣ Joint object detection and re-identification for 3D obstacle multi-camera systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and <a href="#S4.T2" title="Table 2 ‣ 4.1 Ablation Studies. Number of Embedding Dimensions ‣ 4 EXPERIMENTAL RESULTS ‣ Joint object detection and re-identification for 3D obstacle multi-camera systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the results of both the re-identification branch and the detection and classification branches vary depending on the output size of the re-ID branch. This is because the training of the network is done in an end-to-end manner, and the ability of the network to generalize is altered by the introduction of the third branch. That said, it can be generally observed that the inclusion of the obstacle re-identification branch improves the network’s ability to detect and classify obstacles, which implies that the quality of the feature maps obtained in the intermediate layers of the network is improved by its optimization for a complementary task, as previously observed in the literature <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. It is worth highlighting the results marked in bold and underlined, where an improvement of more than 10% is achieved for some classes, improving the performance of the network for all classes by almost 8%. Likewise, we see that the re-identification results for all cases are similar and relatively good.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Ablation Studies. Re-Identification Branch Configuration</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p">In the next experiment, we study the effect of changes in the structure of the re-identification branch. For this purpose, several alternatives have been proposed, with different numbers of neurons and intermediate layers, or eliminating altogether the Convolutional layers and keeping only the Fully Connected ones, in a similar way as the other two output branches are constructed. Table <a href="#S4.T3" title="Table 3 ‣ 4.2 Ablation Studies. Re-Identification Branch Configuration ‣ 4 EXPERIMENTAL RESULTS ‣ Joint object detection and re-identification for 3D obstacle multi-camera systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the configuration of all the studied options. Finally, Table <a href="#S4.T4" title="Table 4 ‣ 4.2 Ablation Studies. Re-Identification Branch Configuration ‣ 4 EXPERIMENTAL RESULTS ‣ Joint object detection and re-identification for 3D obstacle multi-camera systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the detection and re-identification results for each design. For all these configurations a constant number of output dimensions, 25, has been chosen so that the results are comparable. This number of output dimensions has been decided based on the configuration that gave the best results in the re-ID evaluation (see Table <a href="#S4.T2" title="Table 2 ‣ 4.1 Ablation Studies. Number of Embedding Dimensions ‣ 4 EXPERIMENTAL RESULTS ‣ Joint object detection and re-identification for 3D obstacle multi-camera systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). For the following experiment, 500 proposals during the test phase have been chosen. Once more, the evaluation has been performed following the KITTI object image detection metrics for all nuScenes classes, but this time the six cameras of the validation split of the nuScenes dataset have been used.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<div id="S4.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:133.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-46.8pt,14.4pt) scale(0.822546648967856,0.822546648967856) ;">
<table id="S4.T3.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.1.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">Conf</th>
<td id="S4.T3.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">#0</td>
<td id="S4.T3.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">#1</td>
<td id="S4.T3.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">#2</td>
<td id="S4.T3.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt">#3</td>
<td id="S4.T3.1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt">#4</td>
<td id="S4.T3.1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt">#5</td>
<td id="S4.T3.1.1.1.1.8" class="ltx_td ltx_align_center ltx_border_tt">#6</td>
<td id="S4.T3.1.1.1.1.9" class="ltx_td ltx_align_center ltx_border_tt">#7</td>
<td id="S4.T3.1.1.1.1.10" class="ltx_td ltx_align_center ltx_border_tt">#8</td>
<td id="S4.T3.1.1.1.1.11" class="ltx_td ltx_align_center ltx_border_tt">#9</td>
<td id="S4.T3.1.1.1.1.12" class="ltx_td ltx_align_center ltx_border_tt">#10</td>
<td id="S4.T3.1.1.1.1.13" class="ltx_td ltx_align_center ltx_border_tt">#11</td>
<td id="S4.T3.1.1.1.1.14" class="ltx_td ltx_align_center ltx_border_tt">#12</td>
<td id="S4.T3.1.1.1.1.15" class="ltx_td ltx_align_center ltx_border_tt">#13</td>
<td id="S4.T3.1.1.1.1.16" class="ltx_td ltx_align_center ltx_border_tt">#14</td>
<td id="S4.T3.1.1.1.1.17" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt">#15</td>
</tr>
<tr id="S4.T3.1.1.2.2" class="ltx_tr">
<th id="S4.T3.1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">k</th>
<td id="S4.T3.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">3</td>
<td id="S4.T3.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">3</td>
<td id="S4.T3.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">5</td>
<td id="S4.T3.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t">5</td>
<td id="S4.T3.1.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t">5</td>
<td id="S4.T3.1.1.2.2.7" class="ltx_td ltx_align_center ltx_border_t">5</td>
<td id="S4.T3.1.1.2.2.8" class="ltx_td ltx_align_center ltx_border_t">3</td>
<td id="S4.T3.1.1.2.2.9" class="ltx_td ltx_align_center ltx_border_t">3</td>
<td id="S4.T3.1.1.2.2.10" class="ltx_td ltx_align_center ltx_border_t">3</td>
<td id="S4.T3.1.1.2.2.11" class="ltx_td ltx_align_center ltx_border_t">3</td>
<td id="S4.T3.1.1.2.2.12" class="ltx_td ltx_align_center ltx_border_t">3</td>
<td id="S4.T3.1.1.2.2.13" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T3.1.1.2.2.14" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T3.1.1.2.2.15" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T3.1.1.2.2.16" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T3.1.1.2.2.17" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S4.T3.1.1.3.3" class="ltx_tr">
<th id="S4.T3.1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">C1</th>
<td id="S4.T3.1.1.3.3.2" class="ltx_td ltx_align_center">32</td>
<td id="S4.T3.1.1.3.3.3" class="ltx_td ltx_align_center">16</td>
<td id="S4.T3.1.1.3.3.4" class="ltx_td ltx_align_center">32</td>
<td id="S4.T3.1.1.3.3.5" class="ltx_td ltx_align_center">16</td>
<td id="S4.T3.1.1.3.3.6" class="ltx_td ltx_align_center">64</td>
<td id="S4.T3.1.1.3.3.7" class="ltx_td ltx_align_center">32</td>
<td id="S4.T3.1.1.3.3.8" class="ltx_td ltx_align_center">32</td>
<td id="S4.T3.1.1.3.3.9" class="ltx_td ltx_align_center">32</td>
<td id="S4.T3.1.1.3.3.10" class="ltx_td ltx_align_center">32</td>
<td id="S4.T3.1.1.3.3.11" class="ltx_td ltx_align_center">32</td>
<td id="S4.T3.1.1.3.3.12" class="ltx_td ltx_align_center">16</td>
<td id="S4.T3.1.1.3.3.13" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.3.3.14" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.3.3.15" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.3.3.16" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.3.3.17" class="ltx_td ltx_nopad_r ltx_align_center">-</td>
</tr>
<tr id="S4.T3.1.1.4.4" class="ltx_tr">
<th id="S4.T3.1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">C2</th>
<td id="S4.T3.1.1.4.4.2" class="ltx_td ltx_align_center">64</td>
<td id="S4.T3.1.1.4.4.3" class="ltx_td ltx_align_center">32</td>
<td id="S4.T3.1.1.4.4.4" class="ltx_td ltx_align_center">64</td>
<td id="S4.T3.1.1.4.4.5" class="ltx_td ltx_align_center">32</td>
<td id="S4.T3.1.1.4.4.6" class="ltx_td ltx_align_center">128</td>
<td id="S4.T3.1.1.4.4.7" class="ltx_td ltx_align_center">64</td>
<td id="S4.T3.1.1.4.4.8" class="ltx_td ltx_align_center">64</td>
<td id="S4.T3.1.1.4.4.9" class="ltx_td ltx_align_center">64</td>
<td id="S4.T3.1.1.4.4.10" class="ltx_td ltx_align_center">64</td>
<td id="S4.T3.1.1.4.4.11" class="ltx_td ltx_align_center">64</td>
<td id="S4.T3.1.1.4.4.12" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.4.4.13" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.4.4.14" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.4.4.15" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.4.4.16" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.4.4.17" class="ltx_td ltx_nopad_r ltx_align_center">-</td>
</tr>
<tr id="S4.T3.1.1.5.5" class="ltx_tr">
<th id="S4.T3.1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">C3</th>
<td id="S4.T3.1.1.5.5.2" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.5.5.3" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.5.5.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.5.5.5" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.5.5.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.5.5.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.5.5.8" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.5.5.9" class="ltx_td ltx_align_center">64</td>
<td id="S4.T3.1.1.5.5.10" class="ltx_td ltx_align_center">128</td>
<td id="S4.T3.1.1.5.5.11" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.5.5.12" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.5.5.13" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.5.5.14" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.5.5.15" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.5.5.16" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.5.5.17" class="ltx_td ltx_nopad_r ltx_align_center">-</td>
</tr>
<tr id="S4.T3.1.1.6.6" class="ltx_tr">
<th id="S4.T3.1.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">C4</th>
<td id="S4.T3.1.1.6.6.2" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.6.6.3" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.6.6.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.6.6.5" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.6.6.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.6.6.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.6.6.8" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.6.6.9" class="ltx_td ltx_align_center">128</td>
<td id="S4.T3.1.1.6.6.10" class="ltx_td ltx_align_center">256</td>
<td id="S4.T3.1.1.6.6.11" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.6.6.12" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.6.6.13" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.6.6.14" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.6.6.15" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.6.6.16" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.6.6.17" class="ltx_td ltx_nopad_r ltx_align_center">-</td>
</tr>
<tr id="S4.T3.1.1.7.7" class="ltx_tr">
<th id="S4.T3.1.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">FC1</th>
<td id="S4.T3.1.1.7.7.2" class="ltx_td ltx_align_center">1024</td>
<td id="S4.T3.1.1.7.7.3" class="ltx_td ltx_align_center">4096</td>
<td id="S4.T3.1.1.7.7.4" class="ltx_td ltx_align_center">1024</td>
<td id="S4.T3.1.1.7.7.5" class="ltx_td ltx_align_center">4096</td>
<td id="S4.T3.1.1.7.7.6" class="ltx_td ltx_align_center">512</td>
<td id="S4.T3.1.1.7.7.7" class="ltx_td ltx_align_center">4096</td>
<td id="S4.T3.1.1.7.7.8" class="ltx_td ltx_align_center">4096</td>
<td id="S4.T3.1.1.7.7.9" class="ltx_td ltx_align_center">1024</td>
<td id="S4.T3.1.1.7.7.10" class="ltx_td ltx_align_center">1024</td>
<td id="S4.T3.1.1.7.7.11" class="ltx_td ltx_align_center">512</td>
<td id="S4.T3.1.1.7.7.12" class="ltx_td ltx_align_center">512</td>
<td id="S4.T3.1.1.7.7.13" class="ltx_td ltx_align_center">1024</td>
<td id="S4.T3.1.1.7.7.14" class="ltx_td ltx_align_center">1024</td>
<td id="S4.T3.1.1.7.7.15" class="ltx_td ltx_align_center">1024</td>
<td id="S4.T3.1.1.7.7.16" class="ltx_td ltx_align_center">1024</td>
<td id="S4.T3.1.1.7.7.17" class="ltx_td ltx_nopad_r ltx_align_center">1024</td>
</tr>
<tr id="S4.T3.1.1.8.8" class="ltx_tr">
<th id="S4.T3.1.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">FC2</th>
<td id="S4.T3.1.1.8.8.2" class="ltx_td ltx_align_center">256</td>
<td id="S4.T3.1.1.8.8.3" class="ltx_td ltx_align_center">1024</td>
<td id="S4.T3.1.1.8.8.4" class="ltx_td ltx_align_center">256</td>
<td id="S4.T3.1.1.8.8.5" class="ltx_td ltx_align_center">1024</td>
<td id="S4.T3.1.1.8.8.6" class="ltx_td ltx_align_center">256</td>
<td id="S4.T3.1.1.8.8.7" class="ltx_td ltx_align_center">1024</td>
<td id="S4.T3.1.1.8.8.8" class="ltx_td ltx_align_center">1024</td>
<td id="S4.T3.1.1.8.8.9" class="ltx_td ltx_align_center">256</td>
<td id="S4.T3.1.1.8.8.10" class="ltx_td ltx_align_center">256</td>
<td id="S4.T3.1.1.8.8.11" class="ltx_td ltx_align_center">256</td>
<td id="S4.T3.1.1.8.8.12" class="ltx_td ltx_align_center">256</td>
<td id="S4.T3.1.1.8.8.13" class="ltx_td ltx_align_center">1024</td>
<td id="S4.T3.1.1.8.8.14" class="ltx_td ltx_align_center">1024</td>
<td id="S4.T3.1.1.8.8.15" class="ltx_td ltx_align_center">512</td>
<td id="S4.T3.1.1.8.8.16" class="ltx_td ltx_align_center">512</td>
<td id="S4.T3.1.1.8.8.17" class="ltx_td ltx_nopad_r ltx_align_center">256</td>
</tr>
<tr id="S4.T3.1.1.9.9" class="ltx_tr">
<th id="S4.T3.1.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">FC3</th>
<td id="S4.T3.1.1.9.9.2" class="ltx_td ltx_align_center ltx_border_bb">256</td>
<td id="S4.T3.1.1.9.9.3" class="ltx_td ltx_align_center ltx_border_bb">256</td>
<td id="S4.T3.1.1.9.9.4" class="ltx_td ltx_align_center ltx_border_bb">256</td>
<td id="S4.T3.1.1.9.9.5" class="ltx_td ltx_align_center ltx_border_bb">256</td>
<td id="S4.T3.1.1.9.9.6" class="ltx_td ltx_align_center ltx_border_bb">256</td>
<td id="S4.T3.1.1.9.9.7" class="ltx_td ltx_align_center ltx_border_bb">256</td>
<td id="S4.T3.1.1.9.9.8" class="ltx_td ltx_align_center ltx_border_bb">256</td>
<td id="S4.T3.1.1.9.9.9" class="ltx_td ltx_align_center ltx_border_bb">256</td>
<td id="S4.T3.1.1.9.9.10" class="ltx_td ltx_align_center ltx_border_bb">256</td>
<td id="S4.T3.1.1.9.9.11" class="ltx_td ltx_align_center ltx_border_bb">128</td>
<td id="S4.T3.1.1.9.9.12" class="ltx_td ltx_align_center ltx_border_bb">128</td>
<td id="S4.T3.1.1.9.9.13" class="ltx_td ltx_align_center ltx_border_bb">256</td>
<td id="S4.T3.1.1.9.9.14" class="ltx_td ltx_align_center ltx_border_bb">-</td>
<td id="S4.T3.1.1.9.9.15" class="ltx_td ltx_align_center ltx_border_bb">128</td>
<td id="S4.T3.1.1.9.9.16" class="ltx_td ltx_align_center ltx_border_bb">-</td>
<td id="S4.T3.1.1.9.9.17" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">-</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Specifications of the tested configurations for the re-identification branch. k is the kernel size used, C1-C4 is the number of output neurons of the Convolutional Layer, FC1-FC3 is the number of output neurons of the Fully Connected Layer.</figcaption>
</figure>
<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.1.1.1" class="ltx_tr">
<th id="S4.T4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" rowspan="2"><span id="S4.T4.1.1.1.1.1" class="ltx_text">Conf.</span></th>
<th id="S4.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4">Classes (AP [%])</th>
<th id="S4.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3">reID</th>
</tr>
<tr id="S4.T4.1.2.2" class="ltx_tr">
<th id="S4.T4.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Ped</th>
<th id="S4.T4.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Car</th>
<th id="S4.T4.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Cyclist</th>
<th id="S4.T4.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">all</th>
<th id="S4.T4.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">precision</th>
<th id="S4.T4.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">recall</th>
<th id="S4.T4.1.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">f-score</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.1.3.1" class="ltx_tr">
<th id="S4.T4.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">#0</th>
<td id="S4.T4.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t">44,42</td>
<td id="S4.T4.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">53,39</td>
<td id="S4.T4.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">30,84</td>
<td id="S4.T4.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">41,253</td>
<td id="S4.T4.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t">0.817</td>
<td id="S4.T4.1.3.1.7" class="ltx_td ltx_align_center ltx_border_t">0.846</td>
<td id="S4.T4.1.3.1.8" class="ltx_td ltx_align_center ltx_border_t">0,832</td>
</tr>
<tr id="S4.T4.1.4.2" class="ltx_tr">
<th id="S4.T4.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">#1</th>
<td id="S4.T4.1.4.2.2" class="ltx_td ltx_align_center">45,33</td>
<td id="S4.T4.1.4.2.3" class="ltx_td ltx_align_center">54,38</td>
<td id="S4.T4.1.4.2.4" class="ltx_td ltx_align_center">31,13</td>
<td id="S4.T4.1.4.2.5" class="ltx_td ltx_align_center">41,622</td>
<td id="S4.T4.1.4.2.6" class="ltx_td ltx_align_center">0,815</td>
<td id="S4.T4.1.4.2.7" class="ltx_td ltx_align_center">0,838</td>
<td id="S4.T4.1.4.2.8" class="ltx_td ltx_align_center">0,827</td>
</tr>
<tr id="S4.T4.1.5.3" class="ltx_tr">
<th id="S4.T4.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">#2</th>
<td id="S4.T4.1.5.3.2" class="ltx_td ltx_align_center">42,36</td>
<td id="S4.T4.1.5.3.3" class="ltx_td ltx_align_center">53,53</td>
<td id="S4.T4.1.5.3.4" class="ltx_td ltx_align_center"><span id="S4.T4.1.5.3.4.1" class="ltx_text ltx_font_bold">31,64</span></td>
<td id="S4.T4.1.5.3.5" class="ltx_td ltx_align_center">41,048</td>
<td id="S4.T4.1.5.3.6" class="ltx_td ltx_align_center">0,816</td>
<td id="S4.T4.1.5.3.7" class="ltx_td ltx_align_center">0,844</td>
<td id="S4.T4.1.5.3.8" class="ltx_td ltx_align_center">0,830</td>
</tr>
<tr id="S4.T4.1.6.4" class="ltx_tr">
<th id="S4.T4.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">#3</th>
<td id="S4.T4.1.6.4.2" class="ltx_td ltx_align_center">41,97</td>
<td id="S4.T4.1.6.4.3" class="ltx_td ltx_align_center">51,42</td>
<td id="S4.T4.1.6.4.4" class="ltx_td ltx_align_center">28,91</td>
<td id="S4.T4.1.6.4.5" class="ltx_td ltx_align_center">40,020</td>
<td id="S4.T4.1.6.4.6" class="ltx_td ltx_align_center">0,806</td>
<td id="S4.T4.1.6.4.7" class="ltx_td ltx_align_center"><span id="S4.T4.1.6.4.7.1" class="ltx_text ltx_font_bold">0,856</span></td>
<td id="S4.T4.1.6.4.8" class="ltx_td ltx_align_center">0,831</td>
</tr>
<tr id="S4.T4.1.7.5" class="ltx_tr">
<th id="S4.T4.1.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">#4</th>
<td id="S4.T4.1.7.5.2" class="ltx_td ltx_align_center">44,23</td>
<td id="S4.T4.1.7.5.3" class="ltx_td ltx_align_center">53,18</td>
<td id="S4.T4.1.7.5.4" class="ltx_td ltx_align_center">29,81</td>
<td id="S4.T4.1.7.5.5" class="ltx_td ltx_align_center">41,209</td>
<td id="S4.T4.1.7.5.6" class="ltx_td ltx_align_center">0,836</td>
<td id="S4.T4.1.7.5.7" class="ltx_td ltx_align_center">0,841</td>
<td id="S4.T4.1.7.5.8" class="ltx_td ltx_align_center">0,839</td>
</tr>
<tr id="S4.T4.1.8.6" class="ltx_tr">
<th id="S4.T4.1.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">#5</th>
<td id="S4.T4.1.8.6.2" class="ltx_td ltx_align_center">42,47</td>
<td id="S4.T4.1.8.6.3" class="ltx_td ltx_align_center">53,25</td>
<td id="S4.T4.1.8.6.4" class="ltx_td ltx_align_center">28,78</td>
<td id="S4.T4.1.8.6.5" class="ltx_td ltx_align_center">40,814</td>
<td id="S4.T4.1.8.6.6" class="ltx_td ltx_align_center">0,818</td>
<td id="S4.T4.1.8.6.7" class="ltx_td ltx_align_center">0,851</td>
<td id="S4.T4.1.8.6.8" class="ltx_td ltx_align_center">0,835</td>
</tr>
<tr id="S4.T4.1.9.7" class="ltx_tr">
<th id="S4.T4.1.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">#6</th>
<td id="S4.T4.1.9.7.2" class="ltx_td ltx_align_center">44,53</td>
<td id="S4.T4.1.9.7.3" class="ltx_td ltx_align_center">53,49</td>
<td id="S4.T4.1.9.7.4" class="ltx_td ltx_align_center">31,25</td>
<td id="S4.T4.1.9.7.5" class="ltx_td ltx_align_center">41,525</td>
<td id="S4.T4.1.9.7.6" class="ltx_td ltx_align_center">0,826</td>
<td id="S4.T4.1.9.7.7" class="ltx_td ltx_align_center">0,846</td>
<td id="S4.T4.1.9.7.8" class="ltx_td ltx_align_center">0,836</td>
</tr>
<tr id="S4.T4.1.10.8" class="ltx_tr">
<th id="S4.T4.1.10.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">#7</th>
<td id="S4.T4.1.10.8.2" class="ltx_td ltx_align_center">45,83</td>
<td id="S4.T4.1.10.8.3" class="ltx_td ltx_align_center">54,5</td>
<td id="S4.T4.1.10.8.4" class="ltx_td ltx_align_center">29,43</td>
<td id="S4.T4.1.10.8.5" class="ltx_td ltx_align_center">42,261</td>
<td id="S4.T4.1.10.8.6" class="ltx_td ltx_align_center">0,728</td>
<td id="S4.T4.1.10.8.7" class="ltx_td ltx_align_center">0,829</td>
<td id="S4.T4.1.10.8.8" class="ltx_td ltx_align_center">0,779</td>
</tr>
<tr id="S4.T4.1.11.9" class="ltx_tr">
<th id="S4.T4.1.11.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">#8</th>
<td id="S4.T4.1.11.9.2" class="ltx_td ltx_align_center">45,15</td>
<td id="S4.T4.1.11.9.3" class="ltx_td ltx_align_center">53,28</td>
<td id="S4.T4.1.11.9.4" class="ltx_td ltx_align_center">30,48</td>
<td id="S4.T4.1.11.9.5" class="ltx_td ltx_align_center">40,998</td>
<td id="S4.T4.1.11.9.6" class="ltx_td ltx_align_center">0,728</td>
<td id="S4.T4.1.11.9.7" class="ltx_td ltx_align_center">0,839</td>
<td id="S4.T4.1.11.9.8" class="ltx_td ltx_align_center">0,784</td>
</tr>
<tr id="S4.T4.1.12.10" class="ltx_tr">
<th id="S4.T4.1.12.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">#9</th>
<td id="S4.T4.1.12.10.2" class="ltx_td ltx_align_center">45,71</td>
<td id="S4.T4.1.12.10.3" class="ltx_td ltx_align_center">51,61</td>
<td id="S4.T4.1.12.10.4" class="ltx_td ltx_align_center">29,88</td>
<td id="S4.T4.1.12.10.5" class="ltx_td ltx_align_center">40,653</td>
<td id="S4.T4.1.12.10.6" class="ltx_td ltx_align_center">0,818</td>
<td id="S4.T4.1.12.10.7" class="ltx_td ltx_align_center">0,846</td>
<td id="S4.T4.1.12.10.8" class="ltx_td ltx_align_center">0,832</td>
</tr>
<tr id="S4.T4.1.13.11" class="ltx_tr">
<th id="S4.T4.1.13.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">#10</th>
<td id="S4.T4.1.13.11.2" class="ltx_td ltx_align_center">44,43</td>
<td id="S4.T4.1.13.11.3" class="ltx_td ltx_align_center">53,63</td>
<td id="S4.T4.1.13.11.4" class="ltx_td ltx_align_center">30,66</td>
<td id="S4.T4.1.13.11.5" class="ltx_td ltx_align_center">41,358</td>
<td id="S4.T4.1.13.11.6" class="ltx_td ltx_align_center">0,817</td>
<td id="S4.T4.1.13.11.7" class="ltx_td ltx_align_center">0,855</td>
<td id="S4.T4.1.13.11.8" class="ltx_td ltx_align_center">0,836</td>
</tr>
<tr id="S4.T4.1.14.12" class="ltx_tr">
<th id="S4.T4.1.14.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">#11</th>
<td id="S4.T4.1.14.12.2" class="ltx_td ltx_align_center"><span id="S4.T4.1.14.12.2.1" class="ltx_text ltx_font_bold">45,90</span></td>
<td id="S4.T4.1.14.12.3" class="ltx_td ltx_align_center"><span id="S4.T4.1.14.12.3.1" class="ltx_text ltx_font_bold">55,16</span></td>
<td id="S4.T4.1.14.12.4" class="ltx_td ltx_align_center">29,54</td>
<td id="S4.T4.1.14.12.5" class="ltx_td ltx_align_center"><span id="S4.T4.1.14.12.5.1" class="ltx_text ltx_font_bold">42,353</span></td>
<td id="S4.T4.1.14.12.6" class="ltx_td ltx_align_center">0,857</td>
<td id="S4.T4.1.14.12.7" class="ltx_td ltx_align_center">0,845</td>
<td id="S4.T4.1.14.12.8" class="ltx_td ltx_align_center">0,851</td>
</tr>
<tr id="S4.T4.1.15.13" class="ltx_tr">
<th id="S4.T4.1.15.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">#12</th>
<td id="S4.T4.1.15.13.2" class="ltx_td ltx_align_center">42,52</td>
<td id="S4.T4.1.15.13.3" class="ltx_td ltx_align_center">53,1</td>
<td id="S4.T4.1.15.13.4" class="ltx_td ltx_align_center">29,01</td>
<td id="S4.T4.1.15.13.5" class="ltx_td ltx_align_center">41,016</td>
<td id="S4.T4.1.15.13.6" class="ltx_td ltx_align_center"><span id="S4.T4.1.15.13.6.1" class="ltx_text ltx_font_bold">0,863</span></td>
<td id="S4.T4.1.15.13.7" class="ltx_td ltx_align_center">0,844</td>
<td id="S4.T4.1.15.13.8" class="ltx_td ltx_align_center">0,854</td>
</tr>
<tr id="S4.T4.1.16.14" class="ltx_tr">
<th id="S4.T4.1.16.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">#13</th>
<td id="S4.T4.1.16.14.2" class="ltx_td ltx_align_center">44,45</td>
<td id="S4.T4.1.16.14.3" class="ltx_td ltx_align_center">54,62</td>
<td id="S4.T4.1.16.14.4" class="ltx_td ltx_align_center">28,96</td>
<td id="S4.T4.1.16.14.5" class="ltx_td ltx_align_center">41,659</td>
<td id="S4.T4.1.16.14.6" class="ltx_td ltx_align_center">0,860</td>
<td id="S4.T4.1.16.14.7" class="ltx_td ltx_align_center">0,849</td>
<td id="S4.T4.1.16.14.8" class="ltx_td ltx_align_center"><span id="S4.T4.1.16.14.8.1" class="ltx_text ltx_font_bold">0,855</span></td>
</tr>
<tr id="S4.T4.1.17.15" class="ltx_tr">
<th id="S4.T4.1.17.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">#14</th>
<td id="S4.T4.1.17.15.2" class="ltx_td ltx_align_center">42,13</td>
<td id="S4.T4.1.17.15.3" class="ltx_td ltx_align_center">51,68</td>
<td id="S4.T4.1.17.15.4" class="ltx_td ltx_align_center">29,07</td>
<td id="S4.T4.1.17.15.5" class="ltx_td ltx_align_center">40,278</td>
<td id="S4.T4.1.17.15.6" class="ltx_td ltx_align_center">0,851</td>
<td id="S4.T4.1.17.15.7" class="ltx_td ltx_align_center">0,848</td>
<td id="S4.T4.1.17.15.8" class="ltx_td ltx_align_center">0,850</td>
</tr>
<tr id="S4.T4.1.18.16" class="ltx_tr">
<th id="S4.T4.1.18.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">#15</th>
<td id="S4.T4.1.18.16.2" class="ltx_td ltx_align_center ltx_border_bb">41,98</td>
<td id="S4.T4.1.18.16.3" class="ltx_td ltx_align_center ltx_border_bb">51,48</td>
<td id="S4.T4.1.18.16.4" class="ltx_td ltx_align_center ltx_border_bb">31,19</td>
<td id="S4.T4.1.18.16.5" class="ltx_td ltx_align_center ltx_border_bb">40,294</td>
<td id="S4.T4.1.18.16.6" class="ltx_td ltx_align_center ltx_border_bb">0,859</td>
<td id="S4.T4.1.18.16.7" class="ltx_td ltx_align_center ltx_border_bb">0,836</td>
<td id="S4.T4.1.18.16.8" class="ltx_td ltx_align_center ltx_border_bb">0,848</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Results of the evaluation of the different configurations for the re-identification branch within the image detection network. Details of each conf. are given in Table <a href="#S4.T3" title="Table 3 ‣ 4.2 Ablation Studies. Re-Identification Branch Configuration ‣ 4 EXPERIMENTAL RESULTS ‣ Joint object detection and re-identification for 3D obstacle multi-camera systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. The best result for each column is highlighted in bold.</figcaption>
</figure>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.1" class="ltx_p">Based on the reference configuration #0, as shown in Table <a href="#S4.T4" title="Table 4 ‣ 4.2 Ablation Studies. Re-Identification Branch Configuration ‣ 4 EXPERIMENTAL RESULTS ‣ Joint object detection and re-identification for 3D obstacle multi-camera systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, variations made in the other configurations have a slight effect on the obtained results.
Although the alterations in the network composition yield minor performance changes, configuration #11 presents the best overall results, achieving the highest precision for pedestrians (45.9%), cars (55.16%), and all classes combined (42.353%), and a re-ID f-score (0.851) in pair with the best case scenario.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Image Detection Qualitative Results</h3>

<div id="S4.SS3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS3.p1.1" class="ltx_p">To visually assess the operation of the pipeline, some qualitative results in the nuScenes benchmark are presented. The model used follows the configuration #11 from <a href="#S4.T3" title="Table 3 ‣ 4.2 Ablation Studies. Re-Identification Branch Configuration ‣ 4 EXPERIMENTAL RESULTS ‣ Joint object detection and re-identification for 3D obstacle multi-camera systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> with an output dimension of 30, using 500 proposals in the RPN stage. For the remaining experiments, these hyperparameters are kept.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para ltx_noindent">
<p id="S4.SS3.p2.1" class="ltx_p">Some examples of detection and re-identification results are shown in Figure <a href="#S4.F4" title="Figure 4 ‣ 4.3 Image Detection Qualitative Results ‣ 4 EXPERIMENTAL RESULTS ‣ Joint object detection and re-identification for 3D obstacle multi-camera systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. The images show frames captured by the different cameras at the same temporal instant. It can be seen how obstacle detection even at long distances is appropriately done and how the obstacles appearing in adjacent cameras are re-identified (the bounding box of the re-identified obstacles is drawn with the same color, chosen randomly).</p>
</div>
<figure id="S4.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F4.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.05785/assets/figures/2d_4.png" id="S4.F4.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="583" height="97" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F4.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.05785/assets/figures/2d_2.png" id="S4.F4.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="583" height="97" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F4.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.05785/assets/figures/2d_3.png" id="S4.F4.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="583" height="97" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Qualitative examples of the image detections. The same color bounding box indicates that the two obstacles from different cameras have been re-identified as the same. For the rest of the bounding boxes: orange for cars, blue for pedestrians, and purple for cyclists.</figcaption>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>3D Obstacle Detection Quantitative Results</h3>

<div id="S4.SS4.p1" class="ltx_para ltx_noindent">
<p id="S4.SS4.p1.3" class="ltx_p">In this section, we evaluate the second stage of the proposed detection pipeline, which involves taking as input a filtered point cloud in the form of a frustum for each of the objects detected in the previous image-based detection stage and processing them through a series of pointnet-based networks to estimate the parameters defining a three-dimensional bounding box -position (<math id="S4.SS4.p1.1.m1.3" class="ltx_Math" alttext="x,y,z" display="inline"><semantics id="S4.SS4.p1.1.m1.3a"><mrow id="S4.SS4.p1.1.m1.3.4.2" xref="S4.SS4.p1.1.m1.3.4.1.cmml"><mi id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml">x</mi><mo id="S4.SS4.p1.1.m1.3.4.2.1" xref="S4.SS4.p1.1.m1.3.4.1.cmml">,</mo><mi id="S4.SS4.p1.1.m1.2.2" xref="S4.SS4.p1.1.m1.2.2.cmml">y</mi><mo id="S4.SS4.p1.1.m1.3.4.2.2" xref="S4.SS4.p1.1.m1.3.4.1.cmml">,</mo><mi id="S4.SS4.p1.1.m1.3.3" xref="S4.SS4.p1.1.m1.3.3.cmml">z</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.3b"><list id="S4.SS4.p1.1.m1.3.4.1.cmml" xref="S4.SS4.p1.1.m1.3.4.2"><ci id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1">𝑥</ci><ci id="S4.SS4.p1.1.m1.2.2.cmml" xref="S4.SS4.p1.1.m1.2.2">𝑦</ci><ci id="S4.SS4.p1.1.m1.3.3.cmml" xref="S4.SS4.p1.1.m1.3.3">𝑧</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.3c">x,y,z</annotation></semantics></math>), dimensions (<math id="S4.SS4.p1.2.m2.3" class="ltx_Math" alttext="l,w,h" display="inline"><semantics id="S4.SS4.p1.2.m2.3a"><mrow id="S4.SS4.p1.2.m2.3.4.2" xref="S4.SS4.p1.2.m2.3.4.1.cmml"><mi id="S4.SS4.p1.2.m2.1.1" xref="S4.SS4.p1.2.m2.1.1.cmml">l</mi><mo id="S4.SS4.p1.2.m2.3.4.2.1" xref="S4.SS4.p1.2.m2.3.4.1.cmml">,</mo><mi id="S4.SS4.p1.2.m2.2.2" xref="S4.SS4.p1.2.m2.2.2.cmml">w</mi><mo id="S4.SS4.p1.2.m2.3.4.2.2" xref="S4.SS4.p1.2.m2.3.4.1.cmml">,</mo><mi id="S4.SS4.p1.2.m2.3.3" xref="S4.SS4.p1.2.m2.3.3.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.2.m2.3b"><list id="S4.SS4.p1.2.m2.3.4.1.cmml" xref="S4.SS4.p1.2.m2.3.4.2"><ci id="S4.SS4.p1.2.m2.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1">𝑙</ci><ci id="S4.SS4.p1.2.m2.2.2.cmml" xref="S4.SS4.p1.2.m2.2.2">𝑤</ci><ci id="S4.SS4.p1.2.m2.3.3.cmml" xref="S4.SS4.p1.2.m2.3.3">ℎ</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.2.m2.3c">l,w,h</annotation></semantics></math>) and orientation (<math id="S4.SS4.p1.3.m3.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S4.SS4.p1.3.m3.1a"><mi id="S4.SS4.p1.3.m3.1.1" xref="S4.SS4.p1.3.m3.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.3.m3.1b"><ci id="S4.SS4.p1.3.m3.1.1.cmml" xref="S4.SS4.p1.3.m3.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.3.m3.1c">\theta</annotation></semantics></math>)- that encapsulates the detected obstacle in the image space.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para ltx_noindent">
<p id="S4.SS4.p2.1" class="ltx_p">As introduced In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, to leverage the outcome of the re-identification step in the 3D box estimation, the <span id="S4.SS4.p2.1.1" class="ltx_text ltx_font_italic">region2frustum</span> operation shown in <a href="#S2.F1" title="Figure 1 ‣ 2.3 Siamese Networks for Obstacle Re-Identification ‣ 2 RELATED WORK ‣ Joint object detection and re-identification for 3D obstacle multi-camera systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> needs to be adapted so that it can produce more refined filtered point cloud candidates from matched detections across adjacent cameras:</p>
<ol id="S4.I1" class="ltx_enumerate">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">For detections appearing in a single camera, we follow the process outlined in the original F-PointNets paper. A projection of the point cloud onto the bounding box of the obstacle is performed, and the points within the box are selected.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I1.i2.p1" class="ltx_para ltx_noindent">
<p id="S4.I1.i2.p1.1" class="ltx_p">For detections appearing in two cameras simultaneously, we employ a union of the filtered point clouds from each camera. The process involves the following steps:
We first obtain the filtered frustum for each individual detection, following the same procedure as for unique detections.
Next, we verify if the point clouds obtained for each instance have common points. If there are no shared points, it is considered a false positive re-identification, and the merging of objects is discarded.
If shared points exist the match is considered spatially coherent, indicating a correct re-identification. The two filtered point clouds are merged, retaining only the unique points.
Finally, the central axis of the detection is computed by calculating the average angle between the two outermost angles from the detections in both cameras. A deeper explanation of this process can be read in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
</li>
</ol>
</div>
<div id="S4.SS4.p3" class="ltx_para ltx_noindent">
<p id="S4.SS4.p3.1" class="ltx_p">To evaluate the results, the proposed method (<span id="S4.SS4.p3.1.1" class="ltx_text ltx_font_italic">siaNMS</span>) is studied against three other approaches. The <span id="S4.SS4.p3.1.2" class="ltx_text ltx_font_italic">Original</span> configuration, namely the results obtained by the baseline F-PointNets model with the 2D detection network unaltered. A second one, which uses the detection network presented in this article, but without carrying out the union of re-identified objects (<span id="S4.SS4.p3.1.3" class="ltx_text ltx_font_italic">2D+embedding</span>). A third pipeline, where a non-maximum suppression (NMS) algorithm is applied to the results obtained from the Original configuration (<span id="S4.SS4.p3.1.4" class="ltx_text ltx_font_italic">Original+NMS</span>), such as the one presented in the original paper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
<div id="S4.SS4.p4" class="ltx_para ltx_noindent">
<p id="S4.SS4.p4.1" class="ltx_p">The assessment encompasses the different metrics provided by the nuScenes dataset, considering various aspects such as positional error, size error, and orientation error, among others, for all annotated types of obstacles. More details can be found in the nuScenes object detection task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
<div id="S4.SS4.p5" class="ltx_para ltx_noindent">
<p id="S4.SS4.p5.4" class="ltx_p">Table <a href="#S4.T5" title="Table 5 ‣ 4.4 3D Obstacle Detection Quantitative Results ‣ 4 EXPERIMENTAL RESULTS ‣ Joint object detection and re-identification for 3D obstacle multi-camera systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows the results for Average Precision (AP <math id="S4.SS4.p5.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.SS4.p5.1.m1.1a"><mo stretchy="false" id="S4.SS4.p5.1.m1.1.1" xref="S4.SS4.p5.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.p5.1.m1.1b"><ci id="S4.SS4.p5.1.m1.1.1.cmml" xref="S4.SS4.p5.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p5.1.m1.1c">\uparrow</annotation></semantics></math>) [%], Average Translation Error (ATE <math id="S4.SS4.p5.2.m2.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.SS4.p5.2.m2.1a"><mo stretchy="false" id="S4.SS4.p5.2.m2.1.1" xref="S4.SS4.p5.2.m2.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.p5.2.m2.1b"><ci id="S4.SS4.p5.2.m2.1.1.cmml" xref="S4.SS4.p5.2.m2.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p5.2.m2.1c">\downarrow</annotation></semantics></math>) [m], Average Size Error (ASE <math id="S4.SS4.p5.3.m3.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.SS4.p5.3.m3.1a"><mo stretchy="false" id="S4.SS4.p5.3.m3.1.1" xref="S4.SS4.p5.3.m3.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.p5.3.m3.1b"><ci id="S4.SS4.p5.3.m3.1.1.cmml" xref="S4.SS4.p5.3.m3.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p5.3.m3.1c">\downarrow</annotation></semantics></math>) [%] and Average Orientation Error (AOE <math id="S4.SS4.p5.4.m4.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.SS4.p5.4.m4.1a"><mo stretchy="false" id="S4.SS4.p5.4.m4.1.1" xref="S4.SS4.p5.4.m4.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.p5.4.m4.1b"><ci id="S4.SS4.p5.4.m4.1.1.cmml" xref="S4.SS4.p5.4.m4.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p5.4.m4.1c">\downarrow</annotation></semantics></math>) [rad] for the car, pedestrian, and cyclist classes. Similarly, Table <a href="#S4.T6" title="Table 6 ‣ 4.4 3D Obstacle Detection Quantitative Results ‣ 4 EXPERIMENTAL RESULTS ‣ Joint object detection and re-identification for 3D obstacle multi-camera systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the averaged results across all classes. The experiments have been done in the following manner: The Frustum Pointnets detection network has been trained with the frames of the training split of the nuScenes dataset and the validation split of the nuScenes dataset has been used for evaluation.
To better understand the impact of the proposed approach, the same evaluation has been performed taking into account only the object instances that appear in more than one camera.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<div id="S4.T5.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:150.5pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-43.8pt,15.1pt) scale(0.831883348536475,0.831883348536475) ;">
<table id="S4.T5.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.1.1.1.1" class="ltx_tr">
<th id="S4.T5.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" rowspan="2"><span id="S4.T5.1.1.1.1.1.1" class="ltx_text">Area</span></th>
<th id="S4.T5.1.1.1.1.2" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_t" rowspan="2"></th>
<th id="S4.T5.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" colspan="4">car</th>
<th id="S4.T5.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="4">pedestrian</th>
<th id="S4.T5.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="4">cyclist</th>
</tr>
<tr id="S4.T5.1.1.2.2" class="ltx_tr">
<th id="S4.T5.1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">AP</th>
<th id="S4.T5.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">ATE</th>
<th id="S4.T5.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">ASE</th>
<th id="S4.T5.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AOE</th>
<th id="S4.T5.1.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AP</th>
<th id="S4.T5.1.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">ATE</th>
<th id="S4.T5.1.1.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">ASE</th>
<th id="S4.T5.1.1.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AOE</th>
<th id="S4.T5.1.1.2.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AP</th>
<th id="S4.T5.1.1.2.2.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">ATE</th>
<th id="S4.T5.1.1.2.2.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">ASE</th>
<th id="S4.T5.1.1.2.2.12" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AOE</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.1.1.3.1" class="ltx_tr">
<th id="S4.T5.1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="4"><span id="S4.T5.1.1.3.1.1.1" class="ltx_text">all</span></th>
<th id="S4.T5.1.1.3.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Original</th>
<th id="S4.T5.1.1.3.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">48.2</th>
<td id="S4.T5.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">0.434</td>
<td id="S4.T5.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">17.1</td>
<td id="S4.T5.1.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t">0.493</td>
<td id="S4.T5.1.1.3.1.7" class="ltx_td ltx_align_center ltx_border_t">58.4</td>
<td id="S4.T5.1.1.3.1.8" class="ltx_td ltx_align_center ltx_border_t">0.263</td>
<td id="S4.T5.1.1.3.1.9" class="ltx_td ltx_align_center ltx_border_t">28.5</td>
<td id="S4.T5.1.1.3.1.10" class="ltx_td ltx_align_center ltx_border_t">1.194</td>
<td id="S4.T5.1.1.3.1.11" class="ltx_td ltx_align_center ltx_border_t">44.2</td>
<td id="S4.T5.1.1.3.1.12" class="ltx_td ltx_align_center ltx_border_t">0.252</td>
<td id="S4.T5.1.1.3.1.13" class="ltx_td ltx_align_center ltx_border_t">26.1</td>
<td id="S4.T5.1.1.3.1.14" class="ltx_td ltx_align_center ltx_border_t">0.898</td>
</tr>
<tr id="S4.T5.1.1.4.2" class="ltx_tr">
<th id="S4.T5.1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">2D+embedding</th>
<th id="S4.T5.1.1.4.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">47.8</th>
<td id="S4.T5.1.1.4.2.3" class="ltx_td ltx_align_center">0.427</td>
<td id="S4.T5.1.1.4.2.4" class="ltx_td ltx_align_center">17.1</td>
<td id="S4.T5.1.1.4.2.5" class="ltx_td ltx_align_center">0.486</td>
<td id="S4.T5.1.1.4.2.6" class="ltx_td ltx_align_center">57.1</td>
<td id="S4.T5.1.1.4.2.7" class="ltx_td ltx_align_center">0.259</td>
<td id="S4.T5.1.1.4.2.8" class="ltx_td ltx_align_center"><span id="S4.T5.1.1.4.2.8.1" class="ltx_text ltx_font_bold">28.4</span></td>
<td id="S4.T5.1.1.4.2.9" class="ltx_td ltx_align_center">1.200</td>
<td id="S4.T5.1.1.4.2.10" class="ltx_td ltx_align_center">43.9</td>
<td id="S4.T5.1.1.4.2.11" class="ltx_td ltx_align_center">0.247</td>
<td id="S4.T5.1.1.4.2.12" class="ltx_td ltx_align_center">26.4</td>
<td id="S4.T5.1.1.4.2.13" class="ltx_td ltx_align_center">0.890</td>
</tr>
<tr id="S4.T5.1.1.5.3" class="ltx_tr">
<th id="S4.T5.1.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Original + NMS</th>
<th id="S4.T5.1.1.5.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">48.7</th>
<td id="S4.T5.1.1.5.3.3" class="ltx_td ltx_align_center">0.436</td>
<td id="S4.T5.1.1.5.3.4" class="ltx_td ltx_align_center">17.1</td>
<td id="S4.T5.1.1.5.3.5" class="ltx_td ltx_align_center">0.487</td>
<td id="S4.T5.1.1.5.3.6" class="ltx_td ltx_align_center">56.0</td>
<td id="S4.T5.1.1.5.3.7" class="ltx_td ltx_align_center">0.259</td>
<td id="S4.T5.1.1.5.3.8" class="ltx_td ltx_align_center">28.5</td>
<td id="S4.T5.1.1.5.3.9" class="ltx_td ltx_align_center"><span id="S4.T5.1.1.5.3.9.1" class="ltx_text ltx_font_bold">1.188</span></td>
<td id="S4.T5.1.1.5.3.10" class="ltx_td ltx_align_center"><span id="S4.T5.1.1.5.3.10.1" class="ltx_text ltx_font_bold">46.6</span></td>
<td id="S4.T5.1.1.5.3.11" class="ltx_td ltx_align_center">0.257</td>
<td id="S4.T5.1.1.5.3.12" class="ltx_td ltx_align_center"><span id="S4.T5.1.1.5.3.12.1" class="ltx_text ltx_font_bold">26.1</span></td>
<td id="S4.T5.1.1.5.3.13" class="ltx_td ltx_align_center">0.903</td>
</tr>
<tr id="S4.T5.1.1.6.4" class="ltx_tr">
<th id="S4.T5.1.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">siaNMS</th>
<th id="S4.T5.1.1.6.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T5.1.1.6.4.2.1" class="ltx_text ltx_font_bold">51.5</span></th>
<td id="S4.T5.1.1.6.4.3" class="ltx_td ltx_align_center"><span id="S4.T5.1.1.6.4.3.1" class="ltx_text ltx_font_bold">0.422</span></td>
<td id="S4.T5.1.1.6.4.4" class="ltx_td ltx_align_center">17.1</td>
<td id="S4.T5.1.1.6.4.5" class="ltx_td ltx_align_center"><span id="S4.T5.1.1.6.4.5.1" class="ltx_text ltx_font_bold">0.476</span></td>
<td id="S4.T5.1.1.6.4.6" class="ltx_td ltx_align_center"><span id="S4.T5.1.1.6.4.6.1" class="ltx_text ltx_font_bold">59.7</span></td>
<td id="S4.T5.1.1.6.4.7" class="ltx_td ltx_align_center"><span id="S4.T5.1.1.6.4.7.1" class="ltx_text ltx_font_bold">0.246</span></td>
<td id="S4.T5.1.1.6.4.8" class="ltx_td ltx_align_center">28.5</td>
<td id="S4.T5.1.1.6.4.9" class="ltx_td ltx_align_center">1.203</td>
<td id="S4.T5.1.1.6.4.10" class="ltx_td ltx_align_center">46.3</td>
<td id="S4.T5.1.1.6.4.11" class="ltx_td ltx_align_center"><span id="S4.T5.1.1.6.4.11.1" class="ltx_text ltx_font_bold">0.243</span></td>
<td id="S4.T5.1.1.6.4.12" class="ltx_td ltx_align_center">26.2</td>
<td id="S4.T5.1.1.6.4.13" class="ltx_td ltx_align_center"><span id="S4.T5.1.1.6.4.13.1" class="ltx_text ltx_font_bold">0.874</span></td>
</tr>
<tr id="S4.T5.1.1.7.5" class="ltx_tr">
<th id="S4.T5.1.1.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" rowspan="4"><span id="S4.T5.1.1.7.5.1.1" class="ltx_text">overlap</span></th>
<th id="S4.T5.1.1.7.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Original</th>
<th id="S4.T5.1.1.7.5.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">41.7</th>
<td id="S4.T5.1.1.7.5.4" class="ltx_td ltx_align_center ltx_border_t">0.439</td>
<td id="S4.T5.1.1.7.5.5" class="ltx_td ltx_align_center ltx_border_t">16.8</td>
<td id="S4.T5.1.1.7.5.6" class="ltx_td ltx_align_center ltx_border_t">0.477</td>
<td id="S4.T5.1.1.7.5.7" class="ltx_td ltx_align_center ltx_border_t">44.2</td>
<td id="S4.T5.1.1.7.5.8" class="ltx_td ltx_align_center ltx_border_t">0.355</td>
<td id="S4.T5.1.1.7.5.9" class="ltx_td ltx_align_center ltx_border_t">28.4</td>
<td id="S4.T5.1.1.7.5.10" class="ltx_td ltx_align_center ltx_border_t">1.136</td>
<td id="S4.T5.1.1.7.5.11" class="ltx_td ltx_align_center ltx_border_t">34.0</td>
<td id="S4.T5.1.1.7.5.12" class="ltx_td ltx_align_center ltx_border_t">0.301</td>
<td id="S4.T5.1.1.7.5.13" class="ltx_td ltx_align_center ltx_border_t">26.7</td>
<td id="S4.T5.1.1.7.5.14" class="ltx_td ltx_align_center ltx_border_t">0.843</td>
</tr>
<tr id="S4.T5.1.1.8.6" class="ltx_tr">
<th id="S4.T5.1.1.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">2D+embedding</th>
<th id="S4.T5.1.1.8.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">41.6</th>
<td id="S4.T5.1.1.8.6.3" class="ltx_td ltx_align_center">0.433</td>
<td id="S4.T5.1.1.8.6.4" class="ltx_td ltx_align_center"><span id="S4.T5.1.1.8.6.4.1" class="ltx_text ltx_font_bold">16.7</span></td>
<td id="S4.T5.1.1.8.6.5" class="ltx_td ltx_align_center">0.463</td>
<td id="S4.T5.1.1.8.6.6" class="ltx_td ltx_align_center">43.5</td>
<td id="S4.T5.1.1.8.6.7" class="ltx_td ltx_align_center">0.359</td>
<td id="S4.T5.1.1.8.6.8" class="ltx_td ltx_align_center"><span id="S4.T5.1.1.8.6.8.1" class="ltx_text ltx_font_bold">28.3</span></td>
<td id="S4.T5.1.1.8.6.9" class="ltx_td ltx_align_center"><span id="S4.T5.1.1.8.6.9.1" class="ltx_text ltx_font_bold">1.133</span></td>
<td id="S4.T5.1.1.8.6.10" class="ltx_td ltx_align_center">32.5</td>
<td id="S4.T5.1.1.8.6.11" class="ltx_td ltx_align_center">0.288</td>
<td id="S4.T5.1.1.8.6.12" class="ltx_td ltx_align_center">26.4</td>
<td id="S4.T5.1.1.8.6.13" class="ltx_td ltx_align_center">0.799</td>
</tr>
<tr id="S4.T5.1.1.9.7" class="ltx_tr">
<th id="S4.T5.1.1.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Original + NMS</th>
<th id="S4.T5.1.1.9.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">44.8</th>
<td id="S4.T5.1.1.9.7.3" class="ltx_td ltx_align_center">0.442</td>
<td id="S4.T5.1.1.9.7.4" class="ltx_td ltx_align_center">16.8</td>
<td id="S4.T5.1.1.9.7.5" class="ltx_td ltx_align_center">0.472</td>
<td id="S4.T5.1.1.9.7.6" class="ltx_td ltx_align_center">47.9</td>
<td id="S4.T5.1.1.9.7.7" class="ltx_td ltx_align_center">0.345</td>
<td id="S4.T5.1.1.9.7.8" class="ltx_td ltx_align_center">28.4</td>
<td id="S4.T5.1.1.9.7.9" class="ltx_td ltx_align_center">1.138</td>
<td id="S4.T5.1.1.9.7.10" class="ltx_td ltx_align_center">35.7</td>
<td id="S4.T5.1.1.9.7.11" class="ltx_td ltx_align_center">0.311</td>
<td id="S4.T5.1.1.9.7.12" class="ltx_td ltx_align_center">26.3</td>
<td id="S4.T5.1.1.9.7.13" class="ltx_td ltx_align_center">0.876</td>
</tr>
<tr id="S4.T5.1.1.10.8" class="ltx_tr">
<th id="S4.T5.1.1.10.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">siaNMS</th>
<th id="S4.T5.1.1.10.8.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b"><span id="S4.T5.1.1.10.8.2.1" class="ltx_text ltx_font_bold">50.1</span></th>
<td id="S4.T5.1.1.10.8.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T5.1.1.10.8.3.1" class="ltx_text ltx_font_bold">0.416</span></td>
<td id="S4.T5.1.1.10.8.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T5.1.1.10.8.4.1" class="ltx_text ltx_font_bold">16.7</span></td>
<td id="S4.T5.1.1.10.8.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T5.1.1.10.8.5.1" class="ltx_text ltx_font_bold">0.435</span></td>
<td id="S4.T5.1.1.10.8.6" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T5.1.1.10.8.6.1" class="ltx_text ltx_font_bold">50.2</span></td>
<td id="S4.T5.1.1.10.8.7" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T5.1.1.10.8.7.1" class="ltx_text ltx_font_bold">0.330</span></td>
<td id="S4.T5.1.1.10.8.8" class="ltx_td ltx_align_center ltx_border_b">28.6</td>
<td id="S4.T5.1.1.10.8.9" class="ltx_td ltx_align_center ltx_border_b">1.148</td>
<td id="S4.T5.1.1.10.8.10" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T5.1.1.10.8.10.1" class="ltx_text ltx_font_bold">41.5</span></td>
<td id="S4.T5.1.1.10.8.11" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T5.1.1.10.8.11.1" class="ltx_text ltx_font_bold">0.261</span></td>
<td id="S4.T5.1.1.10.8.12" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T5.1.1.10.8.12.1" class="ltx_text ltx_font_bold">26.0</span></td>
<td id="S4.T5.1.1.10.8.13" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T5.1.1.10.8.13.1" class="ltx_text ltx_font_bold">0.597</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>Comparison of the 3D Object Detection Performance on the nuScenes Validation Set in Different Regions of Interest. Results for Car, Pedestrian, and Cyclist Classes. The best result for each column is highlighted in bold.</figcaption>
</figure>
<figure id="S4.T6" class="ltx_table">
<table id="S4.T6.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T6.1.1.1" class="ltx_tr">
<th id="S4.T6.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">Area</th>
<th id="S4.T6.1.1.1.2" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_t"></th>
<th id="S4.T6.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AP</th>
<th id="S4.T6.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">ATE</th>
<th id="S4.T6.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">ASE</th>
<th id="S4.T6.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AOE</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T6.1.2.1" class="ltx_tr">
<th id="S4.T6.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="4"><span id="S4.T6.1.2.1.1.1" class="ltx_text">all</span></th>
<th id="S4.T6.1.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Original</th>
<td id="S4.T6.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">32.87</td>
<td id="S4.T6.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">0.526</td>
<td id="S4.T6.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">29.81</td>
<td id="S4.T6.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">0.928</td>
</tr>
<tr id="S4.T6.1.3.2" class="ltx_tr">
<th id="S4.T6.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">2D+embedding</th>
<td id="S4.T6.1.3.2.2" class="ltx_td ltx_align_center">31.87</td>
<td id="S4.T6.1.3.2.3" class="ltx_td ltx_align_center">0.525</td>
<td id="S4.T6.1.3.2.4" class="ltx_td ltx_align_center">29.72</td>
<td id="S4.T6.1.3.2.5" class="ltx_td ltx_align_center">0.930</td>
</tr>
<tr id="S4.T6.1.4.3" class="ltx_tr">
<th id="S4.T6.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Original + NMS</th>
<td id="S4.T6.1.4.3.2" class="ltx_td ltx_align_center">32.46</td>
<td id="S4.T6.1.4.3.3" class="ltx_td ltx_align_center">0.529</td>
<td id="S4.T6.1.4.3.4" class="ltx_td ltx_align_center">29.75</td>
<td id="S4.T6.1.4.3.5" class="ltx_td ltx_align_center">0.925</td>
</tr>
<tr id="S4.T6.1.5.4" class="ltx_tr">
<th id="S4.T6.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">siaNMS</th>
<td id="S4.T6.1.5.4.2" class="ltx_td ltx_align_center"><span id="S4.T6.1.5.4.2.1" class="ltx_text ltx_font_bold">33.18</span></td>
<td id="S4.T6.1.5.4.3" class="ltx_td ltx_align_center"><span id="S4.T6.1.5.4.3.1" class="ltx_text ltx_font_bold">0.521</span></td>
<td id="S4.T6.1.5.4.4" class="ltx_td ltx_align_center"><span id="S4.T6.1.5.4.4.1" class="ltx_text ltx_font_bold">29.58</span></td>
<td id="S4.T6.1.5.4.5" class="ltx_td ltx_align_center"><span id="S4.T6.1.5.4.5.1" class="ltx_text ltx_font_bold">0.915</span></td>
</tr>
<tr id="S4.T6.1.6.5" class="ltx_tr">
<th id="S4.T6.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" rowspan="4"><span id="S4.T6.1.6.5.1.1" class="ltx_text">overlap</span></th>
<th id="S4.T6.1.6.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Original</th>
<td id="S4.T6.1.6.5.3" class="ltx_td ltx_align_center ltx_border_t">25.59</td>
<td id="S4.T6.1.6.5.4" class="ltx_td ltx_align_center ltx_border_t">0.585</td>
<td id="S4.T6.1.6.5.5" class="ltx_td ltx_align_center ltx_border_t">30.15</td>
<td id="S4.T6.1.6.5.6" class="ltx_td ltx_align_center ltx_border_t">0.947</td>
</tr>
<tr id="S4.T6.1.7.6" class="ltx_tr">
<th id="S4.T6.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">2D+embedding</th>
<td id="S4.T6.1.7.6.2" class="ltx_td ltx_align_center">24.93</td>
<td id="S4.T6.1.7.6.3" class="ltx_td ltx_align_center">0.588</td>
<td id="S4.T6.1.7.6.4" class="ltx_td ltx_align_center">29.93</td>
<td id="S4.T6.1.7.6.5" class="ltx_td ltx_align_center">0.929</td>
</tr>
<tr id="S4.T6.1.8.7" class="ltx_tr">
<th id="S4.T6.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Original + NMS</th>
<td id="S4.T6.1.8.7.2" class="ltx_td ltx_align_center">26.90</td>
<td id="S4.T6.1.8.7.3" class="ltx_td ltx_align_center">0.583</td>
<td id="S4.T6.1.8.7.4" class="ltx_td ltx_align_center">29.99</td>
<td id="S4.T6.1.8.7.5" class="ltx_td ltx_align_center">0.954</td>
</tr>
<tr id="S4.T6.1.9.8" class="ltx_tr">
<th id="S4.T6.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">siaNMS</th>
<td id="S4.T6.1.9.8.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T6.1.9.8.2.1" class="ltx_text ltx_font_bold">28.40</span></td>
<td id="S4.T6.1.9.8.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T6.1.9.8.3.1" class="ltx_text ltx_font_bold">0.573</span></td>
<td id="S4.T6.1.9.8.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T6.1.9.8.4.1" class="ltx_text ltx_font_bold">29.80</span></td>
<td id="S4.T6.1.9.8.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T6.1.9.8.5.1" class="ltx_text ltx_font_bold">0.899</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Comparison of the 3D Object Detection Performance on the nuScenes Validation Set in Different Regions of Interest. Results for All Classes. The best result for each column is highlighted in bold.</figcaption>
</figure>
<div id="S4.SS4.p6" class="ltx_para ltx_noindent">
<p id="S4.SS4.p6.1" class="ltx_p">As can be seen in Tables <a href="#S4.T5" title="Table 5 ‣ 4.4 3D Obstacle Detection Quantitative Results ‣ 4 EXPERIMENTAL RESULTS ‣ Joint object detection and re-identification for 3D obstacle multi-camera systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> and <a href="#S4.T6" title="Table 6 ‣ 4.4 3D Obstacle Detection Quantitative Results ‣ 4 EXPERIMENTAL RESULTS ‣ Joint object detection and re-identification for 3D obstacle multi-camera systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, the AP results of all classes improve when using the method proposed in the article in contrast to conventional methods.
For the car class, the average accuracy increases by more than 3% with respect to the original method, more than 1% for pedestrians, and more than 2% for cyclists. Although these numbers may not suggest a major effect of the proposed additional re-identification branch, its real impact is being diluted by the fact that only a reduced portion of objects leads to redundant detections and is affected by truncation.</p>
</div>
<div id="S4.SS4.p7" class="ltx_para ltx_noindent">
<p id="S4.SS4.p7.1" class="ltx_p">As a consequence, when the results are analyzed considering only those objects falling in the regions of overlap between contiguous cameras, it can be observed that these improvements are much more accentuated, especially in bulky categories such as cars -more prone to be seen from multiple views- where the gain in AP goes up to 8.4%.
In addition to improving the number of detections, the quality of the detections is also affected, reducing the average errors of position, size, and orientation of all classes with respect to the original method and practically all with respect to a conventional NMS method.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>3D Object Detection Qualitative Results</h3>

<figure id="S4.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F5.sf1" class="ltx_figure ltx_figure_panel">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2310.05785/assets/fotos/off_nms_72ab6eb24ebe4b6188b86363d6e99feb.png" id="S4.F5.sf1.g1" class="ltx_graphics ltx_figure_panel ltx_img_portrait" width="210" height="306" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2310.05785/assets/fotos/off_nms_e4cd9dabbbd9468796c2ef8d0bc0a568.png" id="S4.F5.sf1.g2" class="ltx_graphics ltx_figure_panel ltx_img_portrait" width="224" height="306" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2310.05785/assets/fotos/off_nms_f4550267cd0240e1a1ceb844e33e97d4.png" id="S4.F5.sf1.g3" class="ltx_graphics ltx_figure_panel ltx_img_portrait" width="195" height="306" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F5.sf2" class="ltx_figure ltx_figure_panel">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2310.05785/assets/fotos/joined_nms_72ab6eb24ebe4b6188b86363d6e99feb.png" id="S4.F5.sf2.g1" class="ltx_graphics ltx_figure_panel ltx_img_portrait" width="210" height="306" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2310.05785/assets/fotos/joined_nms_e4cd9dabbbd9468796c2ef8d0bc0a568.png" id="S4.F5.sf2.g2" class="ltx_graphics ltx_figure_panel ltx_img_portrait" width="224" height="306" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2310.05785/assets/fotos/joined_nms_f4550267cd0240e1a1ceb844e33e97d4.png" id="S4.F5.sf2.g3" class="ltx_graphics ltx_figure_panel ltx_img_portrait" width="195" height="306" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Results on nuScenes validation set. In (a) the original approach with NMS is used, while in (b), the proposed architecture is used.</figcaption>
</figure>
<div id="S4.SS5.p1" class="ltx_para ltx_noindent">
<p id="S4.SS5.p1.1" class="ltx_p">Similar to what was done to visualize the impact of the proposal in the image space, this section includes qualitative 3D results of the final system compared to the ones of the reference framework.
Here the detections are presented in the LiDAR bird’s-eye view, which displays the LiDAR readings as seen from an orthographic top view, where each cell represents a square pillar lying in a theoretical ground plane. The images in Figure <a href="#S4.F5" title="Figure 5 ‣ 4.5 3D Object Detection Qualitative Results ‣ 4 EXPERIMENTAL RESULTS ‣ Joint object detection and re-identification for 3D obstacle multi-camera systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> show some frames from the validation split of the nuScenes dataset comparing the two best approaches (i.e. <span id="S4.SS5.p1.1.1" class="ltx_text ltx_font_italic">Original+NMS</span> and <span id="S4.SS5.p1.1.2" class="ltx_text ltx_font_italic">siaNMS</span>). In green are the ground truth boxes, while the boxes obtained by the full pipeline proposed in this article are painted in blue. As can be seen in the three cases, the number of false positives is reduced, and the quality of the detections is improved in terms of positioning and orientation of the obtained boxes.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>CONCLUSIONS</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p">In this work, the capabilities of a well-established camera-LiDAR sequential fusion 3D object detection pipeline have been enriched to make it more suitable for its use in multi-camera setups, where its performance deteriorates when duplicate detections of the same object appear.</p>
</div>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.1" class="ltx_p">To this end, the 2D object detector module of its first stage has been extended by means of an embedding branch that enables the re-identification of instances across contiguous cameras. Unlike the predecessor study, the siaNMS block is now integrated as part of the decoding layers of the network, in a multi-task fashion, allowing for its training in an end-to-end manner.</p>
</div>
<div id="S5.p3" class="ltx_para ltx_noindent">
<p id="S5.p3.1" class="ltx_p">The presented approach has led to an improvement in the quality of the features extracted by the network, boosting not only the re-ID outcomes but also the primary function of the network, the detection, and classification of obstacles in the image, as demonstrated in the conducted analysis on the nuScenes dataset.</p>
</div>
<div id="S5.p4" class="ltx_para ltx_noindent">
<p id="S5.p4.1" class="ltx_p">Further experimentation shows that the early elimination of redundant detections also benefits the results provided by the 3D box characterization model, reducing the negative effects of truncated boxes on the image plane and thus contributing to a significant gain in the overall performance of the system.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Andreas Geiger, Philip Lenz, and Raquel Urtasun.

</span>
<span class="ltx_bibblock">Are we ready for autonomous driving? the kitti vision benchmark suite.

</span>
<span class="ltx_bibblock">In <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 3354–3361, 2012.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele.

</span>
<span class="ltx_bibblock">The cityscapes dataset for semantic urban scene understanding.

</span>
<span class="ltx_bibblock">In <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 3213–3223, 2016.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom.

</span>
<span class="ltx_bibblock">nuScenes: A multimodal dataset for autonomous driving.

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1903.11027</span>, 2019.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Ming-Fang Chang, John Lambert, Patsorn Sangkloy, Jagjeet Singh, Slawomir Bak, Andrew Hartnett, De Wang, Peter Carr, Simon Lucey, Deva Ramanan, and James Hays.

</span>
<span class="ltx_bibblock">Argoverse: 3d tracking and forecasting with rich maps.

</span>
<span class="ltx_bibblock">In <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 8748–8757, June 2019.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Joel Janai, Fatma Güney, Aseem Behl, Andreas Geiger, et al.

</span>
<span class="ltx_bibblock">Computer vision for autonomous vehicles: Problems, datasets and state of the art.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Foundations and Trends® in Computer Graphics and Vision</span>, 12(1–3):1–308, 2020.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Jorge Beltrán, Carlos Guindel, Fernando García, et al.

</span>
<span class="ltx_bibblock">Automatic extrinsic calibration method for lidar and camera sensor setups.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Intelligent Transportation Systems</span>, 2022.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Vicente Milanés, David González, Francisco Navas, Imane Mahtout, Alexandre Armand, Clement Zinoune, Arunkumar Ramaswamy, Farid Bekka, Nievsabel Molina, Emmanuel Battesti, et al.

</span>
<span class="ltx_bibblock">The tornado project: An automated driving demonstration in peri-urban and rural areas.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">IEEE Intelligent Transportation Systems Magazine</span>, 14(4):20–36, 2021.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Christian Kinzig, Irene Cortés, Carlos Fernández, and Martin Lauer.

</span>
<span class="ltx_bibblock">Real-time seamless image stitching in autonomous driving.

</span>
<span class="ltx_bibblock">In <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">2022 25th International Conference on Information Fusion (FUSION)</span>, pages 1–8. IEEE, 2022.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Irene Cortés, Jorge Beltrán, Arturo de la Escalera, and Fernando García.

</span>
<span class="ltx_bibblock">sianms: Non-maximum suppression with siamese networks for multi-camera 3d object detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">2020 IEEE Intelligent Vehicles Symposium (IV)</span>, pages 933–938. IEEE, 2020.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Jorge Beltrán, Carlos Guindel, Irene Cortés, Alejandro Barrera, Armando Astudillo, Jesús Urdiales, Mario Álvarez, Farid Bekka, Vicente Milanés, and Fernando García.

</span>
<span class="ltx_bibblock">Towards autonomous driving: a multi-modal 360 perception proposal.

</span>
<span class="ltx_bibblock">In <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC)</span>, pages 1–6. IEEE, 2020.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al.

</span>
<span class="ltx_bibblock">Scalability in perception for autonomous driving: An open dataset benchmark.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1912.04838</span>, 2019.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Pengchuan Xiao, Zhenlei Shao, Steven Hao, Zishuo Zhang, Xiaolin Chai, Judy Jiao, Zesong Li, Jian Wu, Kai Sun, Kun Jiang, et al.

</span>
<span class="ltx_bibblock">Pandaset: Advanced sensor suite dataset for autonomous driving.

</span>
<span class="ltx_bibblock">In <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">2021 IEEE International Intelligent Transportation Systems Conference (ITSC)</span>, pages 3095–3101. IEEE, 2021.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Navaneeth Bodla, Bharat Singh, Rama Chellappa, and Larry S Davis.

</span>
<span class="ltx_bibblock">Soft-nms–improving object detection with one line of code.

</span>
<span class="ltx_bibblock">In <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE international conference on computer vision</span>, pages 5561–5569, 2017.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Songtao Liu, Di Huang, and Yunhong Wang.

</span>
<span class="ltx_bibblock">Adaptive nms: Refining pedestrian detection in a crowd.

</span>
<span class="ltx_bibblock">In <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>, pages 6459–6468, 2019.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Samik Some, Mithun Das Gupta, and Vinay P Namboodiri.

</span>
<span class="ltx_bibblock">Determinantal point process as an alternative to nms.

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2008.11451</span>, 2020.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Yu Liu, Lingqiao Liu, Hamid Rezatofighi, Thanh-Toan Do, Qinfeng Shi, and Ian Reid.

</span>
<span class="ltx_bibblock">Learning pairwise relationship for multi-object detection in crowded scenes.

</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1901.03796</span>, 2019.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Tianzhu Xiang, Gui-Song Xia, and Liangpei Zhang.

</span>
<span class="ltx_bibblock">Image stitching with perspective-preserving warping.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1605.05019</span>, 2016.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Mingxiu Lin, Gang Xu, Xingning Ren, and Ke Xu.

</span>
<span class="ltx_bibblock">Cylindrical panoramic image stitching method based on multi-cameras.

</span>
<span class="ltx_bibblock">In <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">2015 IEEE International Conference on Cyber Technology in Automation, Control, and Intelligent Systems (CYBER)</span>, pages 1091–1096. IEEE, 2015.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Shuting He, Hao Luo, Pichao Wang, Fan Wang, Hao Li, and Wei Jiang.

</span>
<span class="ltx_bibblock">Transreid: Transformer-based object re-identification.

</span>
<span class="ltx_bibblock">In <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF international conference on computer vision</span>, pages 15013–15022, 2021.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Gregory Koch, Richard Zemel, Ruslan Salakhutdinov, et al.

</span>
<span class="ltx_bibblock">Siamese neural networks for one-shot image recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">ICML deep learning workshop</span>, volume 2. Lille, 2015.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Hongye Liu, Yonghong Tian, Yaowei Yang, Lu Pang, and Tiejun Huang.

</span>
<span class="ltx_bibblock">Deep relative distance learning: Tell the difference between similar vehicles.

</span>
<span class="ltx_bibblock">In <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</span>, pages 2167–2175, 2016.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Liang Zheng, Hengheng Zhang, Shaoyan Sun, Manmohan Chandraker, Yi Yang, and Qi Tian.

</span>
<span class="ltx_bibblock">Person re-identification in the wild.

</span>
<span class="ltx_bibblock">In <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</span>, pages 1367–1376, 2017.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Dong Yi, Zhen Lei, Shengcai Liao, and Stan Z Li.

</span>
<span class="ltx_bibblock">Deep metric learning for person re-identification.

</span>
<span class="ltx_bibblock">In <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">2014 22nd International Conference on Pattern Recognition</span>, pages 34–39. IEEE, 2014.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
María José Gómez-Silva, José María Armingol, and Arturo de la Escalera.

</span>
<span class="ltx_bibblock">Deep parts similarity learning for person re-identification.

</span>
<span class="ltx_bibblock">In <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">VISIGRAPP (5: VISAPP)</span>, pages 419–428, 2018.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.

</span>
<span class="ltx_bibblock">Faster r-cnn: Towards real-time object detection with region proposal networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, pages 91–99, 2015.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick.

</span>
<span class="ltx_bibblock">Detectron2.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/facebookresearch/detectron2" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/facebookresearch/detectron2</a>, 2019.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Charles R Qi, Wei Liu, Chenxia Wu, Hao Su, and Leonidas J Guibas.

</span>
<span class="ltx_bibblock">Frustum pointnets for 3d object detection from rgb-d data.

</span>
<span class="ltx_bibblock">In <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 918–927, 2018.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Abhinav Shrivastava, Abhinav Gupta, and Ross Girshick.

</span>
<span class="ltx_bibblock">Training region-based object detectors with online hard example mining.

</span>
<span class="ltx_bibblock">In <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</span>, pages 761–769, 2016.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.

</span>
<span class="ltx_bibblock">Mask r-cnn.

</span>
<span class="ltx_bibblock">In <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE international conference on computer vision</span>, pages 2961–2969, 2017.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Ming Liang, Bin Yang, Yun Chen, Rui Hu, and Raquel Urtasun.

</span>
<span class="ltx_bibblock">Multi-task multi-sensor fusion for 3d object detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</span>, pages 7345–7353, 2019.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2310.05784" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2310.05785" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2310.05785">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2310.05785" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2310.05786" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 02:09:48 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
