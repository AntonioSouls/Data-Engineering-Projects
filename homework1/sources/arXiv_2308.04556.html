<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2308.04556] FocalFormer3D : Focusing on Hard Instance for 3D Object Detection</title><meta property="og:description" content="False negatives (FN) in 3D object detection, e.g., missing predictions of pedestrians, vehicles, or other obstacles, can lead to potentially dangerous situations in autonomous driving. While being fatal, this issue is â€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="FocalFormer3D : Focusing on Hard Instance for 3D Object Detection">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="FocalFormer3D : Focusing on Hard Instance for 3D Object Detection">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2308.04556">

<!--Generated on Wed Feb 28 13:36:06 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">FocalFormer3D : Focusing on Hard Instance for 3D Object Detection</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yilun Chen<sup id="id11.11.id1" class="ltx_sup">1</sup> â€ƒZhiding Yu<sup id="id12.12.id2" class="ltx_sup">3</sup> â€ƒYukang Chen<sup id="id13.13.id3" class="ltx_sup">1</sup> â€ƒ
<br class="ltx_break">Shiyi Lan<sup id="id14.14.id4" class="ltx_sup">3</sup> â€ƒAnima Anandkumar<sup id="id15.15.id5" class="ltx_sup"><span id="id15.15.id5.1" class="ltx_text ltx_font_italic">2,3</span></sup> â€ƒJiaya Jia<sup id="id16.16.id6" class="ltx_sup">1</sup> â€ƒJose M. Alvarez<sup id="id17.17.id7" class="ltx_sup">3</sup> 
<br class="ltx_break"><sup id="id18.18.id8" class="ltx_sup">1</sup>The Chinese University of Hong Kongâ€ƒ<sup id="id19.19.id9" class="ltx_sup">2</sup>Caltech â€ƒ<sup id="id20.20.id10" class="ltx_sup">3</sup>NVIDIA
</span><span class="ltx_author_notes">Work done during an internship at NVIDIA.The corresponding author is Zhiding Yu.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id21.id1" class="ltx_p">False negatives (FN) in 3D object detection, <span id="id21.id1.1" class="ltx_text ltx_font_italic">e.g.</span>, missing predictions of pedestrians, vehicles, or other obstacles, can lead to potentially dangerous situations in autonomous driving. While being fatal, this issue is understudied in many current 3D detection methods. In this work, we propose Hard Instance Probing (HIP), a general pipeline that identifies <span id="id21.id1.2" class="ltx_text ltx_font_italic">FN</span> in a multi-stage manner and guides the models to focus on excavating difficult instances. For 3D object detection, we instantiate this method as FocalFormer3D, a simple yet effective detector that excels at excavating difficult objects and improving prediction recall. FocalFormer3D features a multi-stage query generation to discover hard objects and a box-level transformer decoder to efficiently distinguish objects from massive object candidates. Experimental results on the nuScenes and Waymo datasets validate the superior performance of FocalFormer3D. The advantage leads to strong performance on both detection and tracking, in both LiDAR and multi-modal settings. Notably, FocalFormer3D achieves a 70.5 mAP and 73.9 NDS on nuScenes detection benchmark, while the nuScenes tracking benchmark shows 72.1 AMOTA, both ranking 1st place on the nuScenes LiDAR leaderboard.
Our code is available at <a target="_blank" href="https://github.com/NVlabs/FocalFormer3D" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/NVlabs/FocalFormer3D</a>.</p>
</div>
<figure id="S0.F1" class="ltx_figure"><img src="/html/2308.04556/assets/x1.png" id="S0.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="548" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span id="S0.F1.13.1" class="ltx_text ltx_font_bold">Visual example for Hard Instance Probing (HIP).</span> By utilizing this multi-stage prediction approach, our model can progressively focus on hard instances and facilitate its ability to gradually detect them. At each stage, the model generates some <span id="S0.F1.14.2" class="ltx_text ltx_font_italic">Positive</span> object candidates (represented by green circles). Object candidates assigned to the ground-truth objects can be classified as either <span id="S0.F1.15.3" class="ltx_text ltx_font_italic">True Positive</span>s (<span id="S0.F1.16.4" class="ltx_text ltx_font_italic">TP</span>, represented by <span id="S0.F1.17.5" class="ltx_text" style="color:#698C21;">green boxes</span>) and <span id="S0.F1.18.6" class="ltx_text ltx_font_italic">False Negative</span>s (<span id="S0.F1.19.7" class="ltx_text ltx_font_italic">FN</span>, represented by <span id="S0.F1.20.8" class="ltx_text" style="color:#FF0000;"> red boxes</span>) during training. We explicitly model the unmatched ground-truth objects as the hard instances, which become the main targets for the subsequent stage. Conversely, <span id="S0.F1.21.9" class="ltx_text ltx_font_italic">Positive</span>s are considered easy samples (represented by <span id="S0.F1.22.10" class="ltx_text" style="color:#808080;"> gray boxes</span>) and will be ignored in subsequent stages at both training and inference time. At last, all heatmap predictions across stages are collected as the initial object candidates. We ignored the <span id="S0.F1.23.11" class="ltx_text ltx_font_italic">False Positive</span>s for better visualizations.</figcaption>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">3D object detection is an important yet challenging perception task. Recent state-of-the-art 3D object detectors mainly rely on birdâ€™s eye view (BEV) representationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, where features from multiple sensors are aggregated to construct a unified representation in the ego-vehicle coordinate space. There is a rich yet growing literature on BEV-based 3D detection, including multi-modal fusionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, second-stage refinements (surface point poolingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, RoIPoolÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, and cross attention modulesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>).</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Despite the tremendous efforts, there has been limited exploration to explicitly address <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">false negatives or missed objects</span> often caused by occlusions and clutter background. False negatives are particularly concerning in autonomous driving as they cause missing information in the prediction and planning stacks.
When an object or a part of an object is not detected, this can result in the autonomous vehicle being unaware of potential obstacles such as pedestrians, cyclists, or other vehicles. This is especially hazardous when the vehicle is moving at high speeds and can lead to potentially dangerous situations. Therefore, reducing false negatives is crucial to ensure the safety of autonomous driving.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To address the challenge of <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">False Negative</span>s in 3D detection, we propose and formulate a pipeline called <span id="S1.p3.1.2" class="ltx_text ltx_font_italic">Hard Instance Probing</span> (HIP). Motivated by cascade-style decoder head for object detectionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, we propose a pipeline to probe false negative samples progressively, which significantly improves the recall rate Fig.Â <a href="#S0.F1" title="Figure 1 â€£ FocalFormer3D : Focusing on Hard Instance for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates the pipeline in a cascade manner. In each stage, HIP suppresses the true positive candidates and focuses on the false negative candidates from the previous stages. By iterating the HIP stage, our approach can save those hard false negatives.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Based on HIP, we introduce a 3D object detector, FocalFormer3D, as shown in Fig.Â <a href="#S2.F2" title="Figure 2 â€£ 2 Related Work â€£ FocalFormer3D : Focusing on Hard Instance for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Especially, multi-stage heatmap predictionsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> are employed to excavate difficult instances. We maintain a class-aware <span id="S1.p4.1.1" class="ltx_text ltx_font_italic">Accumulated Positive Mask</span>, indicating positive regions from prior stages. Through this masking design, the model omits the training of easy positive candidates and thereby focuses on the hard instances (<span id="S1.p4.1.2" class="ltx_text ltx_font_italic">False Negatives</span>). Finally, our decoder collects the positive predictions from all stages to produce the object candidates. FocalFormer3D consistently demonstrates considerable gains over baselines in terms of average recall.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In addition, we also introduce a box-level refinement step to eliminate redundant object candidates. The approach employs a deformable transformer decoderÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> and represents the candidates as box-level queries using RoIAlign. This allows for box-level query interaction and iterative box refinements, binding the object queries with sufficient box context through RoIAlignÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> on the birdâ€™s eye view to perform relative bounding box refinements. Finally, a rescoring strategy is adopted to select positive objects from object candidates. Our ablation study in TableÂ <a href="#S4.T6" title="Table 6 â€£ 4.4 Ablation Study â€£ 4 Experiments â€£ FocalFormer3D : Focusing on Hard Instance for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> demonstrates the effectiveness of the local refinement approach in processing adequate object candidates.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Our contributions can be summarized as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We propose Hard Instance Probing (HIP), a learnable scheme to automatically identify <span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">False Negatives</span> in a multi-stage manner.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We present FocalFormer3D for 3D object detection that effectively harvests hard instances on the BEV and demonstrates effectiveness in terms of average recall.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Without bells and whistles, our model achieves state-of-the-art detection performance on <span id="S1.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">both</span> LiDAR-based and multi-modal settings. Notably, our model ranks <span id="S1.I1.i3.p1.1.2" class="ltx_text ltx_font_bold">1st</span> places on <span id="S1.I1.i3.p1.1.3" class="ltx_text ltx_font_bold">both</span> nuScenes 3D LiDAR detection and tracking leaderboard at time of submission.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Modern 3D object detectors, either LiDAR-basedÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, or Camera-basedÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, or Multi-ModalÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> 3D object detectors generally rely on BEV view representationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>. These methods adopt dense feature maps or dense anchors, for conducting object prediction in a birdâ€™s eye view (BEV) space. Among these methods, VoxelNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> as the pioneer works discretize point clouds into voxel representation and applies dense convolution to generate BEV heatmaps. SECONDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> accelerates VoxelNet with 3D sparse convolutionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> to extract 3D features. Some Pillar-based detectorsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> collapse the height dimension and utilize 2D CNNs for efficient 3D detection.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Different from dense detectors, point-based 3D detectors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> directly process point clouds via PointNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> and perform grouping or predictions on the sparse representations. Concerning involvement of neighborhood query on point clouds, it becomes time-consuming and unaffordable for large-scale point clouds. Concerning computation and spatial cost, another line of 3D detectors directly predicts objects on sparse point clouds to avoid dense feature construction. SST <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> applies sparse regional attention and avoids downsampling for small-object detection. FSD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> instead further recognize instances directly on sparse representations obtained by SST <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> and SparseConv for long-range detection.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Recent multi-modal detectorsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> follow the similar paradigm of BEV detectors and incorporate the multi-view image features by physical projection or learnable alignments between LiDAR and cameras. TransFusionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> applies cross attention to obtain image features for each object query. Despite various kinds of modal-specific voxel feature encoders, these detectors finally produce dense BEV features for classification and regression at the heatmap level.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2308.04556/assets/x2.png" id="S2.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="528" height="177" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span id="S2.F2.3.1" class="ltx_text ltx_font_bold">Overall architecture of FocalFormer3D</span>. The overall framework comprises two novel components: a multi-stage heatmap encoder network that uses the Hard Instance Probing (HIP) strategy to produce high-recall object queries (candidates), and a deformable transformer decoder network with rescoring mechanism that is responsible for eliminating false positives from the large set of candidates. (a) Following feature extraction from modalities, the map-view features produce a set of multi-stage BEV features and then BEV heatmaps. The positive mask accumulates to exclude the easy positive candidates of prior stages from BEV heatmaps. The left object candidates are chosen and collected according to the response of BEV heatmap in a multi-stage process. (b) A deformable transformer decoder is adapted to effectively handle diverse object queries. The query embedding is enhanced with a box pooling module, which leverages the intermediate object supervision to identify local regions. It refines object queries in a local-scope manner, rather than at a point level. Residual connections and normalization layers have been excluded from the figure for clarity.</figcaption>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We introduce Hard Instance Probing (HIP) for automated identifying hard instances (<span id="S3.p1.1.1" class="ltx_text ltx_font_italic">False Negative</span>s) in SectionÂ <a href="#S3.SS1" title="3.1 Hard Instance Probing (HIP) â€£ 3 Methodology â€£ FocalFormer3D : Focusing on Hard Instance for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>. We then present the implementations for the two main components of FocalFormer3D.
SectionÂ <a href="#S3.SS2" title="3.2 Multi-stage Heatmap Encoder â€£ 3 Methodology â€£ FocalFormer3D : Focusing on Hard Instance for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a> describes our multi-stage heatmap encoder that harvests the <span id="S3.p1.1.2" class="ltx_text ltx_font_italic">False Negative</span>s for producing high-recall initial object candidates following HIP. SectionÂ <a href="#S3.SS3" title="3.3 Box-level Deformable Decoder â€£ 3 Methodology â€£ FocalFormer3D : Focusing on Hard Instance for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a> introduces a box-level deformable decoder network that further distinguishes objects from these candidates.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2308.04556/assets/x3.png" id="S3.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="414" height="324" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span id="S3.F3.3.1" class="ltx_text ltx_font_bold">Hard Instance Probing.</span> We use the symbol â€œGâ€ to indicate the object candidates that are labeled as ground-truth objects during the target assignment process in training. To ensure clarity, we omit numerous negative predictions for detection, given that background takes up most of the images.</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Hard Instance Probing (HIP)</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Real-world applications, such as autonomous driving, require a high level of scene understanding to ensure safe and secure operation. In particular, false negatives in object detection can present severe risks, emphasizing the need for high recall rates. However, accurately identifying objects in complex scenes or when occlusion occurs is challenging in 3D object detection, resulting in many false negative predictions. Unfortunately, few studies have explicitly focused on addressing false negatives in the design of detection heads. Motivated by the cascade-style detectors, we formulate a training pipeline to emulate the process of identifying false negative predictions at inference time.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.7" class="ltx_p"><span id="S3.SS1.p2.7.1" class="ltx_text ltx_font_bold">Formulation of Hard Instance Probing.</span>
Our strategy to identify hard instances operates stage by stage, as illustrated by a toy example in Fig.Â <a href="#S3.F3" title="Figure 3 â€£ 3 Methodology â€£ FocalFormer3D : Focusing on Hard Instance for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Initially, we annotate the ground-truth objects as</p>
<table id="S3.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex1.m1.1" class="ltx_math_unparsed" alttext="\mathcal{O}=\left\{o_{i},i=1,2,...\right\}," display="block"><semantics id="S3.Ex1.m1.1a"><mrow id="S3.Ex1.m1.1b"><mi class="ltx_font_mathcaligraphic" id="S3.Ex1.m1.1.2">ğ’ª</mi><mo id="S3.Ex1.m1.1.3">=</mo><mrow id="S3.Ex1.m1.1.4"><mo id="S3.Ex1.m1.1.4.1">{</mo><msub id="S3.Ex1.m1.1.4.2"><mi id="S3.Ex1.m1.1.4.2.2">o</mi><mi id="S3.Ex1.m1.1.4.2.3">i</mi></msub><mo id="S3.Ex1.m1.1.4.3">,</mo><mi id="S3.Ex1.m1.1.1">i</mi><mo id="S3.Ex1.m1.1.4.4">=</mo><mn id="S3.Ex1.m1.1.4.5">1</mn><mo id="S3.Ex1.m1.1.4.6">,</mo><mn id="S3.Ex1.m1.1.4.7">2</mn><mo id="S3.Ex1.m1.1.4.8">,</mo><mi mathvariant="normal" id="S3.Ex1.m1.1.4.9">â€¦</mi><mo id="S3.Ex1.m1.1.4.10">}</mo></mrow><mo id="S3.Ex1.m1.1.5">,</mo></mrow><annotation encoding="application/x-tex" id="S3.Ex1.m1.1c">\mathcal{O}=\left\{o_{i},i=1,2,...\right\},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS1.p2.2" class="ltx_p">which is the main targets for initial stages. The neural network makes <span id="S3.SS1.p2.2.1" class="ltx_text ltx_font_italic">Positive</span> or <span id="S3.SS1.p2.2.2" class="ltx_text ltx_font_italic">Negative</span> predictions given a set of initial object candidates <math id="S3.SS1.p2.1.m1.1" class="ltx_math_unparsed" alttext="\mathcal{A}=\left\{a_{i},i=1,2,...\right\}," display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mrow id="S3.SS1.p2.1.m1.1b"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.1.m1.1.2">ğ’œ</mi><mo id="S3.SS1.p2.1.m1.1.3">=</mo><mrow id="S3.SS1.p2.1.m1.1.4"><mo id="S3.SS1.p2.1.m1.1.4.1">{</mo><msub id="S3.SS1.p2.1.m1.1.4.2"><mi id="S3.SS1.p2.1.m1.1.4.2.2">a</mi><mi id="S3.SS1.p2.1.m1.1.4.2.3">i</mi></msub><mo id="S3.SS1.p2.1.m1.1.4.3">,</mo><mi id="S3.SS1.p2.1.m1.1.1">i</mi><mo id="S3.SS1.p2.1.m1.1.4.4">=</mo><mn id="S3.SS1.p2.1.m1.1.4.5">1</mn><mo id="S3.SS1.p2.1.m1.1.4.6">,</mo><mn id="S3.SS1.p2.1.m1.1.4.7">2</mn><mo id="S3.SS1.p2.1.m1.1.4.8">,</mo><mi mathvariant="normal" id="S3.SS1.p2.1.m1.1.4.9">â€¦</mi><mo id="S3.SS1.p2.1.m1.1.4.10">}</mo></mrow><mo id="S3.SS1.p2.1.m1.1.5">,</mo></mrow><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">\mathcal{A}=\left\{a_{i},i=1,2,...\right\},</annotation></semantics></math>
which is not limited to anchorsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>, point-based anchorsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, and object queriesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>. Suppose the detected objects (<span id="S3.SS1.p2.2.3" class="ltx_text ltx_font_italic">Positive</span> predictions) at <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">k</annotation></semantics></math>-th stage are</p>
<table id="S3.Ex2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex2.m1.1" class="ltx_math_unparsed" alttext="\mathcal{P}_{k}=\left\{p_{i},i=1,2,...\right\}." display="block"><semantics id="S3.Ex2.m1.1a"><mrow id="S3.Ex2.m1.1b"><msub id="S3.Ex2.m1.1.2"><mi class="ltx_font_mathcaligraphic" id="S3.Ex2.m1.1.2.2">ğ’«</mi><mi id="S3.Ex2.m1.1.2.3">k</mi></msub><mo id="S3.Ex2.m1.1.3">=</mo><mrow id="S3.Ex2.m1.1.4"><mo id="S3.Ex2.m1.1.4.1">{</mo><msub id="S3.Ex2.m1.1.4.2"><mi id="S3.Ex2.m1.1.4.2.2">p</mi><mi id="S3.Ex2.m1.1.4.2.3">i</mi></msub><mo id="S3.Ex2.m1.1.4.3">,</mo><mi id="S3.Ex2.m1.1.1">i</mi><mo id="S3.Ex2.m1.1.4.4">=</mo><mn id="S3.Ex2.m1.1.4.5">1</mn><mo id="S3.Ex2.m1.1.4.6">,</mo><mn id="S3.Ex2.m1.1.4.7">2</mn><mo id="S3.Ex2.m1.1.4.8">,</mo><mi mathvariant="normal" id="S3.Ex2.m1.1.4.9">â€¦</mi><mo id="S3.Ex2.m1.1.4.10">}</mo></mrow><mo lspace="0em" id="S3.Ex2.m1.1.5">.</mo></mrow><annotation encoding="application/x-tex" id="S3.Ex2.m1.1c">\mathcal{P}_{k}=\left\{p_{i},i=1,2,...\right\}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS1.p2.8" class="ltx_p">We are then allowed to classify the ground-truth objects according to their assigned candidates:</p>
<table id="S3.Ex3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex3.m1.1" class="ltx_Math" alttext="\mathcal{O}_{k}^{TP}=\left\{o_{j}\big{|}\exists p_{i}\in\mathcal{P}_{k},\sigma(p_{i},o_{j})&gt;\eta\right\}." display="block"><semantics id="S3.Ex3.m1.1a"><mrow id="S3.Ex3.m1.1.1.1" xref="S3.Ex3.m1.1.1.1.1.cmml"><mrow id="S3.Ex3.m1.1.1.1.1" xref="S3.Ex3.m1.1.1.1.1.cmml"><msubsup id="S3.Ex3.m1.1.1.1.1.4" xref="S3.Ex3.m1.1.1.1.1.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex3.m1.1.1.1.1.4.2.2" xref="S3.Ex3.m1.1.1.1.1.4.2.2.cmml">ğ’ª</mi><mi id="S3.Ex3.m1.1.1.1.1.4.2.3" xref="S3.Ex3.m1.1.1.1.1.4.2.3.cmml">k</mi><mrow id="S3.Ex3.m1.1.1.1.1.4.3" xref="S3.Ex3.m1.1.1.1.1.4.3.cmml"><mi id="S3.Ex3.m1.1.1.1.1.4.3.2" xref="S3.Ex3.m1.1.1.1.1.4.3.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.Ex3.m1.1.1.1.1.4.3.1" xref="S3.Ex3.m1.1.1.1.1.4.3.1.cmml">â€‹</mo><mi id="S3.Ex3.m1.1.1.1.1.4.3.3" xref="S3.Ex3.m1.1.1.1.1.4.3.3.cmml">P</mi></mrow></msubsup><mo id="S3.Ex3.m1.1.1.1.1.3" xref="S3.Ex3.m1.1.1.1.1.3.cmml">=</mo><mrow id="S3.Ex3.m1.1.1.1.1.2.2" xref="S3.Ex3.m1.1.1.1.1.2.3.cmml"><mo id="S3.Ex3.m1.1.1.1.1.2.2.3" xref="S3.Ex3.m1.1.1.1.1.2.3.1.cmml">{</mo><msub id="S3.Ex3.m1.1.1.1.1.1.1.1" xref="S3.Ex3.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.Ex3.m1.1.1.1.1.1.1.1.2" xref="S3.Ex3.m1.1.1.1.1.1.1.1.2.cmml">o</mi><mi id="S3.Ex3.m1.1.1.1.1.1.1.1.3" xref="S3.Ex3.m1.1.1.1.1.1.1.1.3.cmml">j</mi></msub><mo lspace="0em" mathsize="120%" rspace="0.167em" id="S3.Ex3.m1.1.1.1.1.2.2.4" xref="S3.Ex3.m1.1.1.1.1.2.3.1.cmml">|</mo><mrow id="S3.Ex3.m1.1.1.1.1.2.2.2.2" xref="S3.Ex3.m1.1.1.1.1.2.2.2.3.cmml"><mrow id="S3.Ex3.m1.1.1.1.1.2.2.2.1.1" xref="S3.Ex3.m1.1.1.1.1.2.2.2.1.1.cmml"><mrow id="S3.Ex3.m1.1.1.1.1.2.2.2.1.1.2" xref="S3.Ex3.m1.1.1.1.1.2.2.2.1.1.2.cmml"><mo rspace="0.167em" id="S3.Ex3.m1.1.1.1.1.2.2.2.1.1.2.1" xref="S3.Ex3.m1.1.1.1.1.2.2.2.1.1.2.1.cmml">âˆƒ</mo><msub id="S3.Ex3.m1.1.1.1.1.2.2.2.1.1.2.2" xref="S3.Ex3.m1.1.1.1.1.2.2.2.1.1.2.2.cmml"><mi id="S3.Ex3.m1.1.1.1.1.2.2.2.1.1.2.2.2" xref="S3.Ex3.m1.1.1.1.1.2.2.2.1.1.2.2.2.cmml">p</mi><mi id="S3.Ex3.m1.1.1.1.1.2.2.2.1.1.2.2.3" xref="S3.Ex3.m1.1.1.1.1.2.2.2.1.1.2.2.3.cmml">i</mi></msub></mrow><mo id="S3.Ex3.m1.1.1.1.1.2.2.2.1.1.1" xref="S3.Ex3.m1.1.1.1.1.2.2.2.1.1.1.cmml">âˆˆ</mo><msub id="S3.Ex3.m1.1.1.1.1.2.2.2.1.1.3" xref="S3.Ex3.m1.1.1.1.1.2.2.2.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex3.m1.1.1.1.1.2.2.2.1.1.3.2" xref="S3.Ex3.m1.1.1.1.1.2.2.2.1.1.3.2.cmml">ğ’«</mi><mi id="S3.Ex3.m1.1.1.1.1.2.2.2.1.1.3.3" xref="S3.Ex3.m1.1.1.1.1.2.2.2.1.1.3.3.cmml">k</mi></msub></mrow><mo id="S3.Ex3.m1.1.1.1.1.2.2.2.2.3" xref="S3.Ex3.m1.1.1.1.1.2.2.2.3a.cmml">,</mo><mrow id="S3.Ex3.m1.1.1.1.1.2.2.2.2.2" xref="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.cmml"><mrow id="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.2" xref="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.2.cmml"><mi id="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.2.4" xref="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.2.4.cmml">Ïƒ</mi><mo lspace="0em" rspace="0em" id="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.2.3" xref="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.2.3.cmml">â€‹</mo><mrow id="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.2.2.2" xref="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.2.2.2.3" xref="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.2.2.3.cmml">(</mo><msub id="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.1.1.1.1" xref="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.1.1.1.1.cmml"><mi id="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.1.1.1.1.2" xref="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.1.1.1.1.2.cmml">p</mi><mi id="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.1.1.1.1.3" xref="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.2.2.2.4" xref="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.2.2.3.cmml">,</mo><msub id="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.2.2.2.2" xref="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.2.2.2.2.cmml"><mi id="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.2.2.2.2.2" xref="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.2.2.2.2.2.cmml">o</mi><mi id="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.2.2.2.2.3" xref="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.2.2.2.2.3.cmml">j</mi></msub><mo stretchy="false" id="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.2.2.2.5" xref="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.3" xref="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.3.cmml">&gt;</mo><mi id="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.4" xref="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.4.cmml">Î·</mi></mrow></mrow><mo id="S3.Ex3.m1.1.1.1.1.2.2.5" xref="S3.Ex3.m1.1.1.1.1.2.3.1.cmml">}</mo></mrow></mrow><mo lspace="0em" id="S3.Ex3.m1.1.1.1.2" xref="S3.Ex3.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex3.m1.1b"><apply id="S3.Ex3.m1.1.1.1.1.cmml" xref="S3.Ex3.m1.1.1.1"><eq id="S3.Ex3.m1.1.1.1.1.3.cmml" xref="S3.Ex3.m1.1.1.1.1.3"></eq><apply id="S3.Ex3.m1.1.1.1.1.4.cmml" xref="S3.Ex3.m1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.Ex3.m1.1.1.1.1.4.1.cmml" xref="S3.Ex3.m1.1.1.1.1.4">superscript</csymbol><apply id="S3.Ex3.m1.1.1.1.1.4.2.cmml" xref="S3.Ex3.m1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.Ex3.m1.1.1.1.1.4.2.1.cmml" xref="S3.Ex3.m1.1.1.1.1.4">subscript</csymbol><ci id="S3.Ex3.m1.1.1.1.1.4.2.2.cmml" xref="S3.Ex3.m1.1.1.1.1.4.2.2">ğ’ª</ci><ci id="S3.Ex3.m1.1.1.1.1.4.2.3.cmml" xref="S3.Ex3.m1.1.1.1.1.4.2.3">ğ‘˜</ci></apply><apply id="S3.Ex3.m1.1.1.1.1.4.3.cmml" xref="S3.Ex3.m1.1.1.1.1.4.3"><times id="S3.Ex3.m1.1.1.1.1.4.3.1.cmml" xref="S3.Ex3.m1.1.1.1.1.4.3.1"></times><ci id="S3.Ex3.m1.1.1.1.1.4.3.2.cmml" xref="S3.Ex3.m1.1.1.1.1.4.3.2">ğ‘‡</ci><ci id="S3.Ex3.m1.1.1.1.1.4.3.3.cmml" xref="S3.Ex3.m1.1.1.1.1.4.3.3">ğ‘ƒ</ci></apply></apply><apply id="S3.Ex3.m1.1.1.1.1.2.3.cmml" xref="S3.Ex3.m1.1.1.1.1.2.2"><csymbol cd="latexml" id="S3.Ex3.m1.1.1.1.1.2.3.1.cmml" xref="S3.Ex3.m1.1.1.1.1.2.2.3">conditional-set</csymbol><apply id="S3.Ex3.m1.1.1.1.1.1.1.1.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex3.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.Ex3.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.2">ğ‘œ</ci><ci id="S3.Ex3.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.3">ğ‘—</ci></apply><apply id="S3.Ex3.m1.1.1.1.1.2.2.2.3.cmml" xref="S3.Ex3.m1.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.Ex3.m1.1.1.1.1.2.2.2.3a.cmml" xref="S3.Ex3.m1.1.1.1.1.2.2.2.2.3">formulae-sequence</csymbol><apply id="S3.Ex3.m1.1.1.1.1.2.2.2.1.1.cmml" xref="S3.Ex3.m1.1.1.1.1.2.2.2.1.1"><in id="S3.Ex3.m1.1.1.1.1.2.2.2.1.1.1.cmml" xref="S3.Ex3.m1.1.1.1.1.2.2.2.1.1.1"></in><apply id="S3.Ex3.m1.1.1.1.1.2.2.2.1.1.2.cmml" xref="S3.Ex3.m1.1.1.1.1.2.2.2.1.1.2"><exists id="S3.Ex3.m1.1.1.1.1.2.2.2.1.1.2.1.cmml" xref="S3.Ex3.m1.1.1.1.1.2.2.2.1.1.2.1"></exists><apply id="S3.Ex3.m1.1.1.1.1.2.2.2.1.1.2.2.cmml" xref="S3.Ex3.m1.1.1.1.1.2.2.2.1.1.2.2"><csymbol cd="ambiguous" id="S3.Ex3.m1.1.1.1.1.2.2.2.1.1.2.2.1.cmml" xref="S3.Ex3.m1.1.1.1.1.2.2.2.1.1.2.2">subscript</csymbol><ci id="S3.Ex3.m1.1.1.1.1.2.2.2.1.1.2.2.2.cmml" xref="S3.Ex3.m1.1.1.1.1.2.2.2.1.1.2.2.2">ğ‘</ci><ci id="S3.Ex3.m1.1.1.1.1.2.2.2.1.1.2.2.3.cmml" xref="S3.Ex3.m1.1.1.1.1.2.2.2.1.1.2.2.3">ğ‘–</ci></apply></apply><apply id="S3.Ex3.m1.1.1.1.1.2.2.2.1.1.3.cmml" xref="S3.Ex3.m1.1.1.1.1.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.Ex3.m1.1.1.1.1.2.2.2.1.1.3.1.cmml" xref="S3.Ex3.m1.1.1.1.1.2.2.2.1.1.3">subscript</csymbol><ci id="S3.Ex3.m1.1.1.1.1.2.2.2.1.1.3.2.cmml" xref="S3.Ex3.m1.1.1.1.1.2.2.2.1.1.3.2">ğ’«</ci><ci id="S3.Ex3.m1.1.1.1.1.2.2.2.1.1.3.3.cmml" xref="S3.Ex3.m1.1.1.1.1.2.2.2.1.1.3.3">ğ‘˜</ci></apply></apply><apply id="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.Ex3.m1.1.1.1.1.2.2.2.2.2"><gt id="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.3.cmml" xref="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.3"></gt><apply id="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.2.cmml" xref="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.2"><times id="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.2.3.cmml" xref="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.2.3"></times><ci id="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.2.4.cmml" xref="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.2.4">ğœ</ci><interval closure="open" id="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.2.2.3.cmml" xref="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.2.2.2"><apply id="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.1.1.1.1.cmml" xref="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.1.1.1.1.1.cmml" xref="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.1.1.1.1">subscript</csymbol><ci id="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.1.1.1.1.2.cmml" xref="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.1.1.1.1.2">ğ‘</ci><ci id="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.1.1.1.1.3.cmml" xref="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.1.1.1.1.3">ğ‘–</ci></apply><apply id="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.2.2.2.2.cmml" xref="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.2.2.2.2.1.cmml" xref="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.2.2.2.2">subscript</csymbol><ci id="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.2.2.2.2.2.cmml" xref="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.2.2.2.2.2">ğ‘œ</ci><ci id="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.2.2.2.2.3.cmml" xref="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.2.2.2.2.3">ğ‘—</ci></apply></interval></apply><ci id="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.4.cmml" xref="S3.Ex3.m1.1.1.1.1.2.2.2.2.2.4">ğœ‚</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex3.m1.1c">\mathcal{O}_{k}^{TP}=\left\{o_{j}\big{|}\exists p_{i}\in\mathcal{P}_{k},\sigma(p_{i},o_{j})&gt;\eta\right\}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS1.p2.4" class="ltx_p">where an object matching metric <math id="S3.SS1.p2.3.m1.2" class="ltx_Math" alttext="\sigma(\cdot,\cdot)" display="inline"><semantics id="S3.SS1.p2.3.m1.2a"><mrow id="S3.SS1.p2.3.m1.2.3" xref="S3.SS1.p2.3.m1.2.3.cmml"><mi id="S3.SS1.p2.3.m1.2.3.2" xref="S3.SS1.p2.3.m1.2.3.2.cmml">Ïƒ</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.3.m1.2.3.1" xref="S3.SS1.p2.3.m1.2.3.1.cmml">â€‹</mo><mrow id="S3.SS1.p2.3.m1.2.3.3.2" xref="S3.SS1.p2.3.m1.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS1.p2.3.m1.2.3.3.2.1" xref="S3.SS1.p2.3.m1.2.3.3.1.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS1.p2.3.m1.1.1" xref="S3.SS1.p2.3.m1.1.1.cmml">â‹…</mo><mo rspace="0em" id="S3.SS1.p2.3.m1.2.3.3.2.2" xref="S3.SS1.p2.3.m1.2.3.3.1.cmml">,</mo><mo lspace="0em" rspace="0em" id="S3.SS1.p2.3.m1.2.2" xref="S3.SS1.p2.3.m1.2.2.cmml">â‹…</mo><mo stretchy="false" id="S3.SS1.p2.3.m1.2.3.3.2.3" xref="S3.SS1.p2.3.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m1.2b"><apply id="S3.SS1.p2.3.m1.2.3.cmml" xref="S3.SS1.p2.3.m1.2.3"><times id="S3.SS1.p2.3.m1.2.3.1.cmml" xref="S3.SS1.p2.3.m1.2.3.1"></times><ci id="S3.SS1.p2.3.m1.2.3.2.cmml" xref="S3.SS1.p2.3.m1.2.3.2">ğœ</ci><interval closure="open" id="S3.SS1.p2.3.m1.2.3.3.1.cmml" xref="S3.SS1.p2.3.m1.2.3.3.2"><ci id="S3.SS1.p2.3.m1.1.1.cmml" xref="S3.SS1.p2.3.m1.1.1">â‹…</ci><ci id="S3.SS1.p2.3.m1.2.2.cmml" xref="S3.SS1.p2.3.m1.2.2">â‹…</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m1.2c">\sigma(\cdot,\cdot)</annotation></semantics></math> (e.g. Intersection over UnionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> and center distanceÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>) and a predefined threshold <math id="S3.SS1.p2.4.m2.1" class="ltx_Math" alttext="\eta" display="inline"><semantics id="S3.SS1.p2.4.m2.1a"><mi id="S3.SS1.p2.4.m2.1.1" xref="S3.SS1.p2.4.m2.1.1.cmml">Î·</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m2.1b"><ci id="S3.SS1.p2.4.m2.1.1.cmml" xref="S3.SS1.p2.4.m2.1.1">ğœ‚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m2.1c">\eta</annotation></semantics></math>. Thus, the left unmatched targets can be regarded as hard instances:</p>
<table id="S3.Ex4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex4.m1.1" class="ltx_Math" alttext="\mathcal{O}_{k}^{FN}=O-\bigcup_{i=1}^{k}O_{k}^{TP}." display="block"><semantics id="S3.Ex4.m1.1a"><mrow id="S3.Ex4.m1.1.1.1" xref="S3.Ex4.m1.1.1.1.1.cmml"><mrow id="S3.Ex4.m1.1.1.1.1" xref="S3.Ex4.m1.1.1.1.1.cmml"><msubsup id="S3.Ex4.m1.1.1.1.1.2" xref="S3.Ex4.m1.1.1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex4.m1.1.1.1.1.2.2.2" xref="S3.Ex4.m1.1.1.1.1.2.2.2.cmml">ğ’ª</mi><mi id="S3.Ex4.m1.1.1.1.1.2.2.3" xref="S3.Ex4.m1.1.1.1.1.2.2.3.cmml">k</mi><mrow id="S3.Ex4.m1.1.1.1.1.2.3" xref="S3.Ex4.m1.1.1.1.1.2.3.cmml"><mi id="S3.Ex4.m1.1.1.1.1.2.3.2" xref="S3.Ex4.m1.1.1.1.1.2.3.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.Ex4.m1.1.1.1.1.2.3.1" xref="S3.Ex4.m1.1.1.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.Ex4.m1.1.1.1.1.2.3.3" xref="S3.Ex4.m1.1.1.1.1.2.3.3.cmml">N</mi></mrow></msubsup><mo id="S3.Ex4.m1.1.1.1.1.1" xref="S3.Ex4.m1.1.1.1.1.1.cmml">=</mo><mrow id="S3.Ex4.m1.1.1.1.1.3" xref="S3.Ex4.m1.1.1.1.1.3.cmml"><mi id="S3.Ex4.m1.1.1.1.1.3.2" xref="S3.Ex4.m1.1.1.1.1.3.2.cmml">O</mi><mo rspace="0.055em" id="S3.Ex4.m1.1.1.1.1.3.1" xref="S3.Ex4.m1.1.1.1.1.3.1.cmml">âˆ’</mo><mrow id="S3.Ex4.m1.1.1.1.1.3.3" xref="S3.Ex4.m1.1.1.1.1.3.3.cmml"><munderover id="S3.Ex4.m1.1.1.1.1.3.3.1" xref="S3.Ex4.m1.1.1.1.1.3.3.1.cmml"><mo movablelimits="false" id="S3.Ex4.m1.1.1.1.1.3.3.1.2.2" xref="S3.Ex4.m1.1.1.1.1.3.3.1.2.2.cmml">â‹ƒ</mo><mrow id="S3.Ex4.m1.1.1.1.1.3.3.1.2.3" xref="S3.Ex4.m1.1.1.1.1.3.3.1.2.3.cmml"><mi id="S3.Ex4.m1.1.1.1.1.3.3.1.2.3.2" xref="S3.Ex4.m1.1.1.1.1.3.3.1.2.3.2.cmml">i</mi><mo id="S3.Ex4.m1.1.1.1.1.3.3.1.2.3.1" xref="S3.Ex4.m1.1.1.1.1.3.3.1.2.3.1.cmml">=</mo><mn id="S3.Ex4.m1.1.1.1.1.3.3.1.2.3.3" xref="S3.Ex4.m1.1.1.1.1.3.3.1.2.3.3.cmml">1</mn></mrow><mi id="S3.Ex4.m1.1.1.1.1.3.3.1.3" xref="S3.Ex4.m1.1.1.1.1.3.3.1.3.cmml">k</mi></munderover><msubsup id="S3.Ex4.m1.1.1.1.1.3.3.2" xref="S3.Ex4.m1.1.1.1.1.3.3.2.cmml"><mi id="S3.Ex4.m1.1.1.1.1.3.3.2.2.2" xref="S3.Ex4.m1.1.1.1.1.3.3.2.2.2.cmml">O</mi><mi id="S3.Ex4.m1.1.1.1.1.3.3.2.2.3" xref="S3.Ex4.m1.1.1.1.1.3.3.2.2.3.cmml">k</mi><mrow id="S3.Ex4.m1.1.1.1.1.3.3.2.3" xref="S3.Ex4.m1.1.1.1.1.3.3.2.3.cmml"><mi id="S3.Ex4.m1.1.1.1.1.3.3.2.3.2" xref="S3.Ex4.m1.1.1.1.1.3.3.2.3.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.Ex4.m1.1.1.1.1.3.3.2.3.1" xref="S3.Ex4.m1.1.1.1.1.3.3.2.3.1.cmml">â€‹</mo><mi id="S3.Ex4.m1.1.1.1.1.3.3.2.3.3" xref="S3.Ex4.m1.1.1.1.1.3.3.2.3.3.cmml">P</mi></mrow></msubsup></mrow></mrow></mrow><mo lspace="0em" id="S3.Ex4.m1.1.1.1.2" xref="S3.Ex4.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex4.m1.1b"><apply id="S3.Ex4.m1.1.1.1.1.cmml" xref="S3.Ex4.m1.1.1.1"><eq id="S3.Ex4.m1.1.1.1.1.1.cmml" xref="S3.Ex4.m1.1.1.1.1.1"></eq><apply id="S3.Ex4.m1.1.1.1.1.2.cmml" xref="S3.Ex4.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex4.m1.1.1.1.1.2.1.cmml" xref="S3.Ex4.m1.1.1.1.1.2">superscript</csymbol><apply id="S3.Ex4.m1.1.1.1.1.2.2.cmml" xref="S3.Ex4.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex4.m1.1.1.1.1.2.2.1.cmml" xref="S3.Ex4.m1.1.1.1.1.2">subscript</csymbol><ci id="S3.Ex4.m1.1.1.1.1.2.2.2.cmml" xref="S3.Ex4.m1.1.1.1.1.2.2.2">ğ’ª</ci><ci id="S3.Ex4.m1.1.1.1.1.2.2.3.cmml" xref="S3.Ex4.m1.1.1.1.1.2.2.3">ğ‘˜</ci></apply><apply id="S3.Ex4.m1.1.1.1.1.2.3.cmml" xref="S3.Ex4.m1.1.1.1.1.2.3"><times id="S3.Ex4.m1.1.1.1.1.2.3.1.cmml" xref="S3.Ex4.m1.1.1.1.1.2.3.1"></times><ci id="S3.Ex4.m1.1.1.1.1.2.3.2.cmml" xref="S3.Ex4.m1.1.1.1.1.2.3.2">ğ¹</ci><ci id="S3.Ex4.m1.1.1.1.1.2.3.3.cmml" xref="S3.Ex4.m1.1.1.1.1.2.3.3">ğ‘</ci></apply></apply><apply id="S3.Ex4.m1.1.1.1.1.3.cmml" xref="S3.Ex4.m1.1.1.1.1.3"><minus id="S3.Ex4.m1.1.1.1.1.3.1.cmml" xref="S3.Ex4.m1.1.1.1.1.3.1"></minus><ci id="S3.Ex4.m1.1.1.1.1.3.2.cmml" xref="S3.Ex4.m1.1.1.1.1.3.2">ğ‘‚</ci><apply id="S3.Ex4.m1.1.1.1.1.3.3.cmml" xref="S3.Ex4.m1.1.1.1.1.3.3"><apply id="S3.Ex4.m1.1.1.1.1.3.3.1.cmml" xref="S3.Ex4.m1.1.1.1.1.3.3.1"><csymbol cd="ambiguous" id="S3.Ex4.m1.1.1.1.1.3.3.1.1.cmml" xref="S3.Ex4.m1.1.1.1.1.3.3.1">superscript</csymbol><apply id="S3.Ex4.m1.1.1.1.1.3.3.1.2.cmml" xref="S3.Ex4.m1.1.1.1.1.3.3.1"><csymbol cd="ambiguous" id="S3.Ex4.m1.1.1.1.1.3.3.1.2.1.cmml" xref="S3.Ex4.m1.1.1.1.1.3.3.1">subscript</csymbol><union id="S3.Ex4.m1.1.1.1.1.3.3.1.2.2.cmml" xref="S3.Ex4.m1.1.1.1.1.3.3.1.2.2"></union><apply id="S3.Ex4.m1.1.1.1.1.3.3.1.2.3.cmml" xref="S3.Ex4.m1.1.1.1.1.3.3.1.2.3"><eq id="S3.Ex4.m1.1.1.1.1.3.3.1.2.3.1.cmml" xref="S3.Ex4.m1.1.1.1.1.3.3.1.2.3.1"></eq><ci id="S3.Ex4.m1.1.1.1.1.3.3.1.2.3.2.cmml" xref="S3.Ex4.m1.1.1.1.1.3.3.1.2.3.2">ğ‘–</ci><cn type="integer" id="S3.Ex4.m1.1.1.1.1.3.3.1.2.3.3.cmml" xref="S3.Ex4.m1.1.1.1.1.3.3.1.2.3.3">1</cn></apply></apply><ci id="S3.Ex4.m1.1.1.1.1.3.3.1.3.cmml" xref="S3.Ex4.m1.1.1.1.1.3.3.1.3">ğ‘˜</ci></apply><apply id="S3.Ex4.m1.1.1.1.1.3.3.2.cmml" xref="S3.Ex4.m1.1.1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.Ex4.m1.1.1.1.1.3.3.2.1.cmml" xref="S3.Ex4.m1.1.1.1.1.3.3.2">superscript</csymbol><apply id="S3.Ex4.m1.1.1.1.1.3.3.2.2.cmml" xref="S3.Ex4.m1.1.1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.Ex4.m1.1.1.1.1.3.3.2.2.1.cmml" xref="S3.Ex4.m1.1.1.1.1.3.3.2">subscript</csymbol><ci id="S3.Ex4.m1.1.1.1.1.3.3.2.2.2.cmml" xref="S3.Ex4.m1.1.1.1.1.3.3.2.2.2">ğ‘‚</ci><ci id="S3.Ex4.m1.1.1.1.1.3.3.2.2.3.cmml" xref="S3.Ex4.m1.1.1.1.1.3.3.2.2.3">ğ‘˜</ci></apply><apply id="S3.Ex4.m1.1.1.1.1.3.3.2.3.cmml" xref="S3.Ex4.m1.1.1.1.1.3.3.2.3"><times id="S3.Ex4.m1.1.1.1.1.3.3.2.3.1.cmml" xref="S3.Ex4.m1.1.1.1.1.3.3.2.3.1"></times><ci id="S3.Ex4.m1.1.1.1.1.3.3.2.3.2.cmml" xref="S3.Ex4.m1.1.1.1.1.3.3.2.3.2">ğ‘‡</ci><ci id="S3.Ex4.m1.1.1.1.1.3.3.2.3.3.cmml" xref="S3.Ex4.m1.1.1.1.1.3.3.2.3.3">ğ‘ƒ</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex4.m1.1c">\mathcal{O}_{k}^{FN}=O-\bigcup_{i=1}^{k}O_{k}^{TP}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS1.p2.6" class="ltx_p">The training of (<math id="S3.SS1.p2.5.m1.1" class="ltx_Math" alttext="k+1" display="inline"><semantics id="S3.SS1.p2.5.m1.1a"><mrow id="S3.SS1.p2.5.m1.1.1" xref="S3.SS1.p2.5.m1.1.1.cmml"><mi id="S3.SS1.p2.5.m1.1.1.2" xref="S3.SS1.p2.5.m1.1.1.2.cmml">k</mi><mo id="S3.SS1.p2.5.m1.1.1.1" xref="S3.SS1.p2.5.m1.1.1.1.cmml">+</mo><mn id="S3.SS1.p2.5.m1.1.1.3" xref="S3.SS1.p2.5.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m1.1b"><apply id="S3.SS1.p2.5.m1.1.1.cmml" xref="S3.SS1.p2.5.m1.1.1"><plus id="S3.SS1.p2.5.m1.1.1.1.cmml" xref="S3.SS1.p2.5.m1.1.1.1"></plus><ci id="S3.SS1.p2.5.m1.1.1.2.cmml" xref="S3.SS1.p2.5.m1.1.1.2">ğ‘˜</ci><cn type="integer" id="S3.SS1.p2.5.m1.1.1.3.cmml" xref="S3.SS1.p2.5.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m1.1c">k+1</annotation></semantics></math>)-th stages is to detect these targets <math id="S3.SS1.p2.6.m2.1" class="ltx_Math" alttext="\mathcal{O}_{k}^{FN}" display="inline"><semantics id="S3.SS1.p2.6.m2.1a"><msubsup id="S3.SS1.p2.6.m2.1.1" xref="S3.SS1.p2.6.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.6.m2.1.1.2.2" xref="S3.SS1.p2.6.m2.1.1.2.2.cmml">ğ’ª</mi><mi id="S3.SS1.p2.6.m2.1.1.2.3" xref="S3.SS1.p2.6.m2.1.1.2.3.cmml">k</mi><mrow id="S3.SS1.p2.6.m2.1.1.3" xref="S3.SS1.p2.6.m2.1.1.3.cmml"><mi id="S3.SS1.p2.6.m2.1.1.3.2" xref="S3.SS1.p2.6.m2.1.1.3.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.6.m2.1.1.3.1" xref="S3.SS1.p2.6.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS1.p2.6.m2.1.1.3.3" xref="S3.SS1.p2.6.m2.1.1.3.3.cmml">N</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m2.1b"><apply id="S3.SS1.p2.6.m2.1.1.cmml" xref="S3.SS1.p2.6.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m2.1.1.1.cmml" xref="S3.SS1.p2.6.m2.1.1">superscript</csymbol><apply id="S3.SS1.p2.6.m2.1.1.2.cmml" xref="S3.SS1.p2.6.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m2.1.1.2.1.cmml" xref="S3.SS1.p2.6.m2.1.1">subscript</csymbol><ci id="S3.SS1.p2.6.m2.1.1.2.2.cmml" xref="S3.SS1.p2.6.m2.1.1.2.2">ğ’ª</ci><ci id="S3.SS1.p2.6.m2.1.1.2.3.cmml" xref="S3.SS1.p2.6.m2.1.1.2.3">ğ‘˜</ci></apply><apply id="S3.SS1.p2.6.m2.1.1.3.cmml" xref="S3.SS1.p2.6.m2.1.1.3"><times id="S3.SS1.p2.6.m2.1.1.3.1.cmml" xref="S3.SS1.p2.6.m2.1.1.3.1"></times><ci id="S3.SS1.p2.6.m2.1.1.3.2.cmml" xref="S3.SS1.p2.6.m2.1.1.3.2">ğ¹</ci><ci id="S3.SS1.p2.6.m2.1.1.3.3.cmml" xref="S3.SS1.p2.6.m2.1.1.3.3">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m2.1c">\mathcal{O}_{k}^{FN}</annotation></semantics></math> from the object candidates while omitting all prior <span id="S3.SS1.p2.6.1" class="ltx_text ltx_font_italic">Positive</span> object candidates.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">Despite the cascade way mimicking the process of identifying false negative samples, we might collect a number of object candidates across all stages. Thus, a second-stage object-level refinement model is necessary to eliminate any potential false positives.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para ltx_noindent">
<p id="S3.SS1.p4.1" class="ltx_p"><span id="S3.SS1.p4.1.1" class="ltx_text ltx_font_bold">Relation with hard example mining.</span>
The most relevant topic close to our approach is hard example miningÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>, which samples hard examples during training. Recent researchÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>, <a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite> has further explored soft-sampling, such as adjusting the loss distribution to mitigate foreground-background imbalance issues. In contrast, our method operates in stages. Specifically, we use <span id="S3.SS1.p4.1.2" class="ltx_text ltx_font_italic">False Negative</span> predictions from prior stages to guide the subsequent stage of the model toward learning from these challenging objects.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Multi-stage Heatmap Encoder</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The upcoming subsections outline the key implementations of FocalFormer3D as depicted in Fig.Â <a href="#S2.F2" title="Figure 2 â€£ 2 Related Work â€£ FocalFormer3D : Focusing on Hard Instance for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. We begin by detailing the implementation of hard instance probing for BEV detection. This involves using the BEV center heatmap to generate the initial object candidate in a cascade manner.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.3" class="ltx_p"><span id="S3.SS2.p2.3.1" class="ltx_text ltx_font_bold">Preliminary of center heatmap in BEV perception.</span>
In common practiceÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, the objective of the BEV heatmap head is to produce heatmap peaks at the center locations of detected objects. The BEV heatmaps are represented by a tensor <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="S\in\mathbb{R}^{X\times Y\times C}" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">S</mi><mo id="S3.SS2.p2.1.m1.1.1.1" xref="S3.SS2.p2.1.m1.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml"><mi id="S3.SS2.p2.1.m1.1.1.3.2" xref="S3.SS2.p2.1.m1.1.1.3.2.cmml">â„</mi><mrow id="S3.SS2.p2.1.m1.1.1.3.3" xref="S3.SS2.p2.1.m1.1.1.3.3.cmml"><mi id="S3.SS2.p2.1.m1.1.1.3.3.2" xref="S3.SS2.p2.1.m1.1.1.3.3.2.cmml">X</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.1.m1.1.1.3.3.1" xref="S3.SS2.p2.1.m1.1.1.3.3.1.cmml">Ã—</mo><mi id="S3.SS2.p2.1.m1.1.1.3.3.3" xref="S3.SS2.p2.1.m1.1.1.3.3.3.cmml">Y</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.1.m1.1.1.3.3.1a" xref="S3.SS2.p2.1.m1.1.1.3.3.1.cmml">Ã—</mo><mi id="S3.SS2.p2.1.m1.1.1.3.3.4" xref="S3.SS2.p2.1.m1.1.1.3.3.4.cmml">C</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><in id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1"></in><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">ğ‘†</ci><apply id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.3.1.cmml" xref="S3.SS2.p2.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.3.2.cmml" xref="S3.SS2.p2.1.m1.1.1.3.2">â„</ci><apply id="S3.SS2.p2.1.m1.1.1.3.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3"><times id="S3.SS2.p2.1.m1.1.1.3.3.1.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3.1"></times><ci id="S3.SS2.p2.1.m1.1.1.3.3.2.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3.2">ğ‘‹</ci><ci id="S3.SS2.p2.1.m1.1.1.3.3.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3.3">ğ‘Œ</ci><ci id="S3.SS2.p2.1.m1.1.1.3.3.4.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3.4">ğ¶</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">S\in\mathbb{R}^{X\times Y\times C}</annotation></semantics></math>, where <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="X\times Y" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mrow id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml">X</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.2.m2.1.1.1" xref="S3.SS2.p2.2.m2.1.1.1.cmml">Ã—</mo><mi id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml">Y</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><times id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1"></times><ci id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2">ğ‘‹</ci><ci id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3">ğ‘Œ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">X\times Y</annotation></semantics></math> indicates the size of BEV feature map and <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><mi id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><ci id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">ğ¶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">C</annotation></semantics></math> is the number of object categories. The target is achieved by producing 2D Gaussians near the BEV object points, which are obtained by projecting 3D box centers onto the map view. In top views such as Fig.Â <a href="#S3.F4" title="Figure 4 â€£ 3.2 Multi-stage Heatmap Encoder â€£ 3 Methodology â€£ FocalFormer3D : Focusing on Hard Instance for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, objects are more sparsely distributed than in a 2D image. Moreover, it is assumed that objects do not have intra-class overlaps on the birdâ€™s eye view.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">Based on the non-overlapping assumption, excluding prior easy positive candidates from BEV heatmap predictions can be achieved easily. In the following, we illustrate the implementation details of HIP, which utilizes an accumulated positive mask.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para ltx_noindent">
<p id="S3.SS2.p4.1" class="ltx_p"><span id="S3.SS2.p4.1.1" class="ltx_text ltx_font_bold">Positive mask accumulation.</span>
To keep track of all easy positive object candidates of prior stages, we generate a positive mask (PM) on the BEV space for each stage and accumulated them to an accumulated positive mask (APM):</p>
<table id="S3.Ex5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex5.m1.3" class="ltx_Math" alttext="\hat{M}_{k}\in\left\{0,1\right\}^{X\times Y\times C}," display="block"><semantics id="S3.Ex5.m1.3a"><mrow id="S3.Ex5.m1.3.3.1" xref="S3.Ex5.m1.3.3.1.1.cmml"><mrow id="S3.Ex5.m1.3.3.1.1" xref="S3.Ex5.m1.3.3.1.1.cmml"><msub id="S3.Ex5.m1.3.3.1.1.2" xref="S3.Ex5.m1.3.3.1.1.2.cmml"><mover accent="true" id="S3.Ex5.m1.3.3.1.1.2.2" xref="S3.Ex5.m1.3.3.1.1.2.2.cmml"><mi id="S3.Ex5.m1.3.3.1.1.2.2.2" xref="S3.Ex5.m1.3.3.1.1.2.2.2.cmml">M</mi><mo id="S3.Ex5.m1.3.3.1.1.2.2.1" xref="S3.Ex5.m1.3.3.1.1.2.2.1.cmml">^</mo></mover><mi id="S3.Ex5.m1.3.3.1.1.2.3" xref="S3.Ex5.m1.3.3.1.1.2.3.cmml">k</mi></msub><mo id="S3.Ex5.m1.3.3.1.1.1" xref="S3.Ex5.m1.3.3.1.1.1.cmml">âˆˆ</mo><msup id="S3.Ex5.m1.3.3.1.1.3" xref="S3.Ex5.m1.3.3.1.1.3.cmml"><mrow id="S3.Ex5.m1.3.3.1.1.3.2.2" xref="S3.Ex5.m1.3.3.1.1.3.2.1.cmml"><mo id="S3.Ex5.m1.3.3.1.1.3.2.2.1" xref="S3.Ex5.m1.3.3.1.1.3.2.1.cmml">{</mo><mn id="S3.Ex5.m1.1.1" xref="S3.Ex5.m1.1.1.cmml">0</mn><mo id="S3.Ex5.m1.3.3.1.1.3.2.2.2" xref="S3.Ex5.m1.3.3.1.1.3.2.1.cmml">,</mo><mn id="S3.Ex5.m1.2.2" xref="S3.Ex5.m1.2.2.cmml">1</mn><mo id="S3.Ex5.m1.3.3.1.1.3.2.2.3" xref="S3.Ex5.m1.3.3.1.1.3.2.1.cmml">}</mo></mrow><mrow id="S3.Ex5.m1.3.3.1.1.3.3" xref="S3.Ex5.m1.3.3.1.1.3.3.cmml"><mi id="S3.Ex5.m1.3.3.1.1.3.3.2" xref="S3.Ex5.m1.3.3.1.1.3.3.2.cmml">X</mi><mo lspace="0.222em" rspace="0.222em" id="S3.Ex5.m1.3.3.1.1.3.3.1" xref="S3.Ex5.m1.3.3.1.1.3.3.1.cmml">Ã—</mo><mi id="S3.Ex5.m1.3.3.1.1.3.3.3" xref="S3.Ex5.m1.3.3.1.1.3.3.3.cmml">Y</mi><mo lspace="0.222em" rspace="0.222em" id="S3.Ex5.m1.3.3.1.1.3.3.1a" xref="S3.Ex5.m1.3.3.1.1.3.3.1.cmml">Ã—</mo><mi id="S3.Ex5.m1.3.3.1.1.3.3.4" xref="S3.Ex5.m1.3.3.1.1.3.3.4.cmml">C</mi></mrow></msup></mrow><mo id="S3.Ex5.m1.3.3.1.2" xref="S3.Ex5.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex5.m1.3b"><apply id="S3.Ex5.m1.3.3.1.1.cmml" xref="S3.Ex5.m1.3.3.1"><in id="S3.Ex5.m1.3.3.1.1.1.cmml" xref="S3.Ex5.m1.3.3.1.1.1"></in><apply id="S3.Ex5.m1.3.3.1.1.2.cmml" xref="S3.Ex5.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="S3.Ex5.m1.3.3.1.1.2.1.cmml" xref="S3.Ex5.m1.3.3.1.1.2">subscript</csymbol><apply id="S3.Ex5.m1.3.3.1.1.2.2.cmml" xref="S3.Ex5.m1.3.3.1.1.2.2"><ci id="S3.Ex5.m1.3.3.1.1.2.2.1.cmml" xref="S3.Ex5.m1.3.3.1.1.2.2.1">^</ci><ci id="S3.Ex5.m1.3.3.1.1.2.2.2.cmml" xref="S3.Ex5.m1.3.3.1.1.2.2.2">ğ‘€</ci></apply><ci id="S3.Ex5.m1.3.3.1.1.2.3.cmml" xref="S3.Ex5.m1.3.3.1.1.2.3">ğ‘˜</ci></apply><apply id="S3.Ex5.m1.3.3.1.1.3.cmml" xref="S3.Ex5.m1.3.3.1.1.3"><csymbol cd="ambiguous" id="S3.Ex5.m1.3.3.1.1.3.1.cmml" xref="S3.Ex5.m1.3.3.1.1.3">superscript</csymbol><set id="S3.Ex5.m1.3.3.1.1.3.2.1.cmml" xref="S3.Ex5.m1.3.3.1.1.3.2.2"><cn type="integer" id="S3.Ex5.m1.1.1.cmml" xref="S3.Ex5.m1.1.1">0</cn><cn type="integer" id="S3.Ex5.m1.2.2.cmml" xref="S3.Ex5.m1.2.2">1</cn></set><apply id="S3.Ex5.m1.3.3.1.1.3.3.cmml" xref="S3.Ex5.m1.3.3.1.1.3.3"><times id="S3.Ex5.m1.3.3.1.1.3.3.1.cmml" xref="S3.Ex5.m1.3.3.1.1.3.3.1"></times><ci id="S3.Ex5.m1.3.3.1.1.3.3.2.cmml" xref="S3.Ex5.m1.3.3.1.1.3.3.2">ğ‘‹</ci><ci id="S3.Ex5.m1.3.3.1.1.3.3.3.cmml" xref="S3.Ex5.m1.3.3.1.1.3.3.3">ğ‘Œ</ci><ci id="S3.Ex5.m1.3.3.1.1.3.3.4.cmml" xref="S3.Ex5.m1.3.3.1.1.3.3.4">ğ¶</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex5.m1.3c">\hat{M}_{k}\in\left\{0,1\right\}^{X\times Y\times C},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS2.p4.2" class="ltx_p">which is initialized as all zeros.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.10" class="ltx_p">The generation of multi-stage BEV features is accomplished in a cascade manner using a lightweight inversed residual blockÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite> between stages. Multi-stage BEV heatmaps are generated by adding an extra convolution layer. At each stage, we generate the positive mask according to the positive predictions. To emulate the process of identifying <span id="S3.SS2.p5.10.1" class="ltx_text ltx_font_italic">False Negative</span>s, we use a test-time selection strategy that ranks the scores according to BEV heatmap responseÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. Specifically, at the <math id="S3.SS2.p5.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS2.p5.1.m1.1a"><mi id="S3.SS2.p5.1.m1.1.1" xref="S3.SS2.p5.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.m1.1b"><ci id="S3.SS2.p5.1.m1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.m1.1c">k</annotation></semantics></math>-th stage, Top-K selection is performed on the BEV heatmap across all BEV positions and categories, producing a set of object predictions <math id="S3.SS2.p5.2.m2.1" class="ltx_Math" alttext="\mathcal{P}_{k}" display="inline"><semantics id="S3.SS2.p5.2.m2.1a"><msub id="S3.SS2.p5.2.m2.1.1" xref="S3.SS2.p5.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p5.2.m2.1.1.2" xref="S3.SS2.p5.2.m2.1.1.2.cmml">ğ’«</mi><mi id="S3.SS2.p5.2.m2.1.1.3" xref="S3.SS2.p5.2.m2.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.2.m2.1b"><apply id="S3.SS2.p5.2.m2.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.2.m2.1.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p5.2.m2.1.1.2.cmml" xref="S3.SS2.p5.2.m2.1.1.2">ğ’«</ci><ci id="S3.SS2.p5.2.m2.1.1.3.cmml" xref="S3.SS2.p5.2.m2.1.1.3">ğ‘˜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.2.m2.1c">\mathcal{P}_{k}</annotation></semantics></math>. Then the positive mask <math id="S3.SS2.p5.3.m3.2" class="ltx_Math" alttext="M_{k}\in\left\{0,1\right\}^{X\times Y\times C}" display="inline"><semantics id="S3.SS2.p5.3.m3.2a"><mrow id="S3.SS2.p5.3.m3.2.3" xref="S3.SS2.p5.3.m3.2.3.cmml"><msub id="S3.SS2.p5.3.m3.2.3.2" xref="S3.SS2.p5.3.m3.2.3.2.cmml"><mi id="S3.SS2.p5.3.m3.2.3.2.2" xref="S3.SS2.p5.3.m3.2.3.2.2.cmml">M</mi><mi id="S3.SS2.p5.3.m3.2.3.2.3" xref="S3.SS2.p5.3.m3.2.3.2.3.cmml">k</mi></msub><mo id="S3.SS2.p5.3.m3.2.3.1" xref="S3.SS2.p5.3.m3.2.3.1.cmml">âˆˆ</mo><msup id="S3.SS2.p5.3.m3.2.3.3" xref="S3.SS2.p5.3.m3.2.3.3.cmml"><mrow id="S3.SS2.p5.3.m3.2.3.3.2.2" xref="S3.SS2.p5.3.m3.2.3.3.2.1.cmml"><mo id="S3.SS2.p5.3.m3.2.3.3.2.2.1" xref="S3.SS2.p5.3.m3.2.3.3.2.1.cmml">{</mo><mn id="S3.SS2.p5.3.m3.1.1" xref="S3.SS2.p5.3.m3.1.1.cmml">0</mn><mo id="S3.SS2.p5.3.m3.2.3.3.2.2.2" xref="S3.SS2.p5.3.m3.2.3.3.2.1.cmml">,</mo><mn id="S3.SS2.p5.3.m3.2.2" xref="S3.SS2.p5.3.m3.2.2.cmml">1</mn><mo id="S3.SS2.p5.3.m3.2.3.3.2.2.3" xref="S3.SS2.p5.3.m3.2.3.3.2.1.cmml">}</mo></mrow><mrow id="S3.SS2.p5.3.m3.2.3.3.3" xref="S3.SS2.p5.3.m3.2.3.3.3.cmml"><mi id="S3.SS2.p5.3.m3.2.3.3.3.2" xref="S3.SS2.p5.3.m3.2.3.3.3.2.cmml">X</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p5.3.m3.2.3.3.3.1" xref="S3.SS2.p5.3.m3.2.3.3.3.1.cmml">Ã—</mo><mi id="S3.SS2.p5.3.m3.2.3.3.3.3" xref="S3.SS2.p5.3.m3.2.3.3.3.3.cmml">Y</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p5.3.m3.2.3.3.3.1a" xref="S3.SS2.p5.3.m3.2.3.3.3.1.cmml">Ã—</mo><mi id="S3.SS2.p5.3.m3.2.3.3.3.4" xref="S3.SS2.p5.3.m3.2.3.3.3.4.cmml">C</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.3.m3.2b"><apply id="S3.SS2.p5.3.m3.2.3.cmml" xref="S3.SS2.p5.3.m3.2.3"><in id="S3.SS2.p5.3.m3.2.3.1.cmml" xref="S3.SS2.p5.3.m3.2.3.1"></in><apply id="S3.SS2.p5.3.m3.2.3.2.cmml" xref="S3.SS2.p5.3.m3.2.3.2"><csymbol cd="ambiguous" id="S3.SS2.p5.3.m3.2.3.2.1.cmml" xref="S3.SS2.p5.3.m3.2.3.2">subscript</csymbol><ci id="S3.SS2.p5.3.m3.2.3.2.2.cmml" xref="S3.SS2.p5.3.m3.2.3.2.2">ğ‘€</ci><ci id="S3.SS2.p5.3.m3.2.3.2.3.cmml" xref="S3.SS2.p5.3.m3.2.3.2.3">ğ‘˜</ci></apply><apply id="S3.SS2.p5.3.m3.2.3.3.cmml" xref="S3.SS2.p5.3.m3.2.3.3"><csymbol cd="ambiguous" id="S3.SS2.p5.3.m3.2.3.3.1.cmml" xref="S3.SS2.p5.3.m3.2.3.3">superscript</csymbol><set id="S3.SS2.p5.3.m3.2.3.3.2.1.cmml" xref="S3.SS2.p5.3.m3.2.3.3.2.2"><cn type="integer" id="S3.SS2.p5.3.m3.1.1.cmml" xref="S3.SS2.p5.3.m3.1.1">0</cn><cn type="integer" id="S3.SS2.p5.3.m3.2.2.cmml" xref="S3.SS2.p5.3.m3.2.2">1</cn></set><apply id="S3.SS2.p5.3.m3.2.3.3.3.cmml" xref="S3.SS2.p5.3.m3.2.3.3.3"><times id="S3.SS2.p5.3.m3.2.3.3.3.1.cmml" xref="S3.SS2.p5.3.m3.2.3.3.3.1"></times><ci id="S3.SS2.p5.3.m3.2.3.3.3.2.cmml" xref="S3.SS2.p5.3.m3.2.3.3.3.2">ğ‘‹</ci><ci id="S3.SS2.p5.3.m3.2.3.3.3.3.cmml" xref="S3.SS2.p5.3.m3.2.3.3.3.3">ğ‘Œ</ci><ci id="S3.SS2.p5.3.m3.2.3.3.3.4.cmml" xref="S3.SS2.p5.3.m3.2.3.3.3.4">ğ¶</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.3.m3.2c">M_{k}\in\left\{0,1\right\}^{X\times Y\times C}</annotation></semantics></math> records the all the positions of positive predictions by setting <math id="S3.SS2.p5.4.m4.3" class="ltx_Math" alttext="M_{(x,y,c)}=1" display="inline"><semantics id="S3.SS2.p5.4.m4.3a"><mrow id="S3.SS2.p5.4.m4.3.4" xref="S3.SS2.p5.4.m4.3.4.cmml"><msub id="S3.SS2.p5.4.m4.3.4.2" xref="S3.SS2.p5.4.m4.3.4.2.cmml"><mi id="S3.SS2.p5.4.m4.3.4.2.2" xref="S3.SS2.p5.4.m4.3.4.2.2.cmml">M</mi><mrow id="S3.SS2.p5.4.m4.3.3.3.5" xref="S3.SS2.p5.4.m4.3.3.3.4.cmml"><mo stretchy="false" id="S3.SS2.p5.4.m4.3.3.3.5.1" xref="S3.SS2.p5.4.m4.3.3.3.4.cmml">(</mo><mi id="S3.SS2.p5.4.m4.1.1.1.1" xref="S3.SS2.p5.4.m4.1.1.1.1.cmml">x</mi><mo id="S3.SS2.p5.4.m4.3.3.3.5.2" xref="S3.SS2.p5.4.m4.3.3.3.4.cmml">,</mo><mi id="S3.SS2.p5.4.m4.2.2.2.2" xref="S3.SS2.p5.4.m4.2.2.2.2.cmml">y</mi><mo id="S3.SS2.p5.4.m4.3.3.3.5.3" xref="S3.SS2.p5.4.m4.3.3.3.4.cmml">,</mo><mi id="S3.SS2.p5.4.m4.3.3.3.3" xref="S3.SS2.p5.4.m4.3.3.3.3.cmml">c</mi><mo stretchy="false" id="S3.SS2.p5.4.m4.3.3.3.5.4" xref="S3.SS2.p5.4.m4.3.3.3.4.cmml">)</mo></mrow></msub><mo id="S3.SS2.p5.4.m4.3.4.1" xref="S3.SS2.p5.4.m4.3.4.1.cmml">=</mo><mn id="S3.SS2.p5.4.m4.3.4.3" xref="S3.SS2.p5.4.m4.3.4.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.4.m4.3b"><apply id="S3.SS2.p5.4.m4.3.4.cmml" xref="S3.SS2.p5.4.m4.3.4"><eq id="S3.SS2.p5.4.m4.3.4.1.cmml" xref="S3.SS2.p5.4.m4.3.4.1"></eq><apply id="S3.SS2.p5.4.m4.3.4.2.cmml" xref="S3.SS2.p5.4.m4.3.4.2"><csymbol cd="ambiguous" id="S3.SS2.p5.4.m4.3.4.2.1.cmml" xref="S3.SS2.p5.4.m4.3.4.2">subscript</csymbol><ci id="S3.SS2.p5.4.m4.3.4.2.2.cmml" xref="S3.SS2.p5.4.m4.3.4.2.2">ğ‘€</ci><vector id="S3.SS2.p5.4.m4.3.3.3.4.cmml" xref="S3.SS2.p5.4.m4.3.3.3.5"><ci id="S3.SS2.p5.4.m4.1.1.1.1.cmml" xref="S3.SS2.p5.4.m4.1.1.1.1">ğ‘¥</ci><ci id="S3.SS2.p5.4.m4.2.2.2.2.cmml" xref="S3.SS2.p5.4.m4.2.2.2.2">ğ‘¦</ci><ci id="S3.SS2.p5.4.m4.3.3.3.3.cmml" xref="S3.SS2.p5.4.m4.3.3.3.3">ğ‘</ci></vector></apply><cn type="integer" id="S3.SS2.p5.4.m4.3.4.3.cmml" xref="S3.SS2.p5.4.m4.3.4.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.4.m4.3c">M_{(x,y,c)}=1</annotation></semantics></math> for each predicted object <math id="S3.SS2.p5.5.m5.1" class="ltx_Math" alttext="p_{i}\in\mathcal{P}_{k}" display="inline"><semantics id="S3.SS2.p5.5.m5.1a"><mrow id="S3.SS2.p5.5.m5.1.1" xref="S3.SS2.p5.5.m5.1.1.cmml"><msub id="S3.SS2.p5.5.m5.1.1.2" xref="S3.SS2.p5.5.m5.1.1.2.cmml"><mi id="S3.SS2.p5.5.m5.1.1.2.2" xref="S3.SS2.p5.5.m5.1.1.2.2.cmml">p</mi><mi id="S3.SS2.p5.5.m5.1.1.2.3" xref="S3.SS2.p5.5.m5.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS2.p5.5.m5.1.1.1" xref="S3.SS2.p5.5.m5.1.1.1.cmml">âˆˆ</mo><msub id="S3.SS2.p5.5.m5.1.1.3" xref="S3.SS2.p5.5.m5.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p5.5.m5.1.1.3.2" xref="S3.SS2.p5.5.m5.1.1.3.2.cmml">ğ’«</mi><mi id="S3.SS2.p5.5.m5.1.1.3.3" xref="S3.SS2.p5.5.m5.1.1.3.3.cmml">k</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.5.m5.1b"><apply id="S3.SS2.p5.5.m5.1.1.cmml" xref="S3.SS2.p5.5.m5.1.1"><in id="S3.SS2.p5.5.m5.1.1.1.cmml" xref="S3.SS2.p5.5.m5.1.1.1"></in><apply id="S3.SS2.p5.5.m5.1.1.2.cmml" xref="S3.SS2.p5.5.m5.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p5.5.m5.1.1.2.1.cmml" xref="S3.SS2.p5.5.m5.1.1.2">subscript</csymbol><ci id="S3.SS2.p5.5.m5.1.1.2.2.cmml" xref="S3.SS2.p5.5.m5.1.1.2.2">ğ‘</ci><ci id="S3.SS2.p5.5.m5.1.1.2.3.cmml" xref="S3.SS2.p5.5.m5.1.1.2.3">ğ‘–</ci></apply><apply id="S3.SS2.p5.5.m5.1.1.3.cmml" xref="S3.SS2.p5.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p5.5.m5.1.1.3.1.cmml" xref="S3.SS2.p5.5.m5.1.1.3">subscript</csymbol><ci id="S3.SS2.p5.5.m5.1.1.3.2.cmml" xref="S3.SS2.p5.5.m5.1.1.3.2">ğ’«</ci><ci id="S3.SS2.p5.5.m5.1.1.3.3.cmml" xref="S3.SS2.p5.5.m5.1.1.3.3">ğ‘˜</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.5.m5.1c">p_{i}\in\mathcal{P}_{k}</annotation></semantics></math>, where <math id="S3.SS2.p5.6.m6.2" class="ltx_Math" alttext="(x,y)" display="inline"><semantics id="S3.SS2.p5.6.m6.2a"><mrow id="S3.SS2.p5.6.m6.2.3.2" xref="S3.SS2.p5.6.m6.2.3.1.cmml"><mo stretchy="false" id="S3.SS2.p5.6.m6.2.3.2.1" xref="S3.SS2.p5.6.m6.2.3.1.cmml">(</mo><mi id="S3.SS2.p5.6.m6.1.1" xref="S3.SS2.p5.6.m6.1.1.cmml">x</mi><mo id="S3.SS2.p5.6.m6.2.3.2.2" xref="S3.SS2.p5.6.m6.2.3.1.cmml">,</mo><mi id="S3.SS2.p5.6.m6.2.2" xref="S3.SS2.p5.6.m6.2.2.cmml">y</mi><mo stretchy="false" id="S3.SS2.p5.6.m6.2.3.2.3" xref="S3.SS2.p5.6.m6.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.6.m6.2b"><interval closure="open" id="S3.SS2.p5.6.m6.2.3.1.cmml" xref="S3.SS2.p5.6.m6.2.3.2"><ci id="S3.SS2.p5.6.m6.1.1.cmml" xref="S3.SS2.p5.6.m6.1.1">ğ‘¥</ci><ci id="S3.SS2.p5.6.m6.2.2.cmml" xref="S3.SS2.p5.6.m6.2.2">ğ‘¦</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.6.m6.2c">(x,y)</annotation></semantics></math> represents <math id="S3.SS2.p5.7.m7.1" class="ltx_Math" alttext="p_{i}" display="inline"><semantics id="S3.SS2.p5.7.m7.1a"><msub id="S3.SS2.p5.7.m7.1.1" xref="S3.SS2.p5.7.m7.1.1.cmml"><mi id="S3.SS2.p5.7.m7.1.1.2" xref="S3.SS2.p5.7.m7.1.1.2.cmml">p</mi><mi id="S3.SS2.p5.7.m7.1.1.3" xref="S3.SS2.p5.7.m7.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.7.m7.1b"><apply id="S3.SS2.p5.7.m7.1.1.cmml" xref="S3.SS2.p5.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.7.m7.1.1.1.cmml" xref="S3.SS2.p5.7.m7.1.1">subscript</csymbol><ci id="S3.SS2.p5.7.m7.1.1.2.cmml" xref="S3.SS2.p5.7.m7.1.1.2">ğ‘</ci><ci id="S3.SS2.p5.7.m7.1.1.3.cmml" xref="S3.SS2.p5.7.m7.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.7.m7.1c">p_{i}</annotation></semantics></math>â€™s location and <math id="S3.SS2.p5.8.m8.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.SS2.p5.8.m8.1a"><mi id="S3.SS2.p5.8.m8.1.1" xref="S3.SS2.p5.8.m8.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.8.m8.1b"><ci id="S3.SS2.p5.8.m8.1.1.cmml" xref="S3.SS2.p5.8.m8.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.8.m8.1c">c</annotation></semantics></math> is <math id="S3.SS2.p5.9.m9.1" class="ltx_Math" alttext="p_{i}" display="inline"><semantics id="S3.SS2.p5.9.m9.1a"><msub id="S3.SS2.p5.9.m9.1.1" xref="S3.SS2.p5.9.m9.1.1.cmml"><mi id="S3.SS2.p5.9.m9.1.1.2" xref="S3.SS2.p5.9.m9.1.1.2.cmml">p</mi><mi id="S3.SS2.p5.9.m9.1.1.3" xref="S3.SS2.p5.9.m9.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.9.m9.1b"><apply id="S3.SS2.p5.9.m9.1.1.cmml" xref="S3.SS2.p5.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.9.m9.1.1.1.cmml" xref="S3.SS2.p5.9.m9.1.1">subscript</csymbol><ci id="S3.SS2.p5.9.m9.1.1.2.cmml" xref="S3.SS2.p5.9.m9.1.1.2">ğ‘</ci><ci id="S3.SS2.p5.9.m9.1.1.3.cmml" xref="S3.SS2.p5.9.m9.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.9.m9.1c">p_{i}</annotation></semantics></math>â€™s class. The left points are set to <math id="S3.SS2.p5.10.m10.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S3.SS2.p5.10.m10.1a"><mn id="S3.SS2.p5.10.m10.1.1" xref="S3.SS2.p5.10.m10.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.10.m10.1b"><cn type="integer" id="S3.SS2.p5.10.m10.1.1.cmml" xref="S3.SS2.p5.10.m10.1.1">0</cn></annotation-xml></semantics></math> by default.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2308.04556/assets/x4.png" id="S3.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="231" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span id="S3.F4.3.1" class="ltx_text ltx_font_bold">Example visualization for the positive mask. (left) and predicted BEV heatmap (right)</span>. The positive mask is class-aware and we show different categories with different colors for visualization. The masking area for objects of different categories can differ in the pooling-based masking method.</figcaption>
</figure>
<div id="S3.SS2.p6" class="ltx_para">
<p id="S3.SS2.p6.1" class="ltx_p">According to the non-overlapping assumption, the ideal way to indicate the existence of a positive object candidate (represented as a point in the center heatmap) on the mask is by masking the box if there is a matched ground truth box. However, since the ground-truth boxes are not available at inference time, we propose the following masking methods during training:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Point Masking</span>. This method involves no change, where only the center point of the positive candidates is filled.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Pooling-based Masking</span>. In this method, smaller objects fill in the center points while larger objects fill in with a kernel size of <math id="S3.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S3.I1.i2.p1.1.m1.1a"><mrow id="S3.I1.i2.p1.1.m1.1.1" xref="S3.I1.i2.p1.1.m1.1.1.cmml"><mn id="S3.I1.i2.p1.1.m1.1.1.2" xref="S3.I1.i2.p1.1.m1.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.I1.i2.p1.1.m1.1.1.1" xref="S3.I1.i2.p1.1.m1.1.1.1.cmml">Ã—</mo><mn id="S3.I1.i2.p1.1.m1.1.1.3" xref="S3.I1.i2.p1.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.1.m1.1b"><apply id="S3.I1.i2.p1.1.m1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1"><times id="S3.I1.i2.p1.1.m1.1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.I1.i2.p1.1.m1.1.1.2.cmml" xref="S3.I1.i2.p1.1.m1.1.1.2">3</cn><cn type="integer" id="S3.I1.i2.p1.1.m1.1.1.3.cmml" xref="S3.I1.i2.p1.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.1.m1.1c">3\times 3</annotation></semantics></math>.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Box Masking</span>. This method requires an additional box prediction branch and involves filling the internal region of the predicted BEV box.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS2.p7" class="ltx_para">
<p id="S3.SS2.p7.1" class="ltx_p">The accumulated positive mask (APM) for the <math id="S3.SS2.p7.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS2.p7.1.m1.1a"><mi id="S3.SS2.p7.1.m1.1.1" xref="S3.SS2.p7.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.1.m1.1b"><ci id="S3.SS2.p7.1.m1.1.1.cmml" xref="S3.SS2.p7.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.1.m1.1c">k</annotation></semantics></math>-th stage is obtained by simply accumulating prior Positive Masks as follows:</p>
<table id="S3.Ex6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex6.m1.1" class="ltx_Math" alttext="\hat{M}_{k}=\max_{1\leq i\leq k}M_{i}." display="block"><semantics id="S3.Ex6.m1.1a"><mrow id="S3.Ex6.m1.1.1.1" xref="S3.Ex6.m1.1.1.1.1.cmml"><mrow id="S3.Ex6.m1.1.1.1.1" xref="S3.Ex6.m1.1.1.1.1.cmml"><msub id="S3.Ex6.m1.1.1.1.1.2" xref="S3.Ex6.m1.1.1.1.1.2.cmml"><mover accent="true" id="S3.Ex6.m1.1.1.1.1.2.2" xref="S3.Ex6.m1.1.1.1.1.2.2.cmml"><mi id="S3.Ex6.m1.1.1.1.1.2.2.2" xref="S3.Ex6.m1.1.1.1.1.2.2.2.cmml">M</mi><mo id="S3.Ex6.m1.1.1.1.1.2.2.1" xref="S3.Ex6.m1.1.1.1.1.2.2.1.cmml">^</mo></mover><mi id="S3.Ex6.m1.1.1.1.1.2.3" xref="S3.Ex6.m1.1.1.1.1.2.3.cmml">k</mi></msub><mo id="S3.Ex6.m1.1.1.1.1.1" xref="S3.Ex6.m1.1.1.1.1.1.cmml">=</mo><mrow id="S3.Ex6.m1.1.1.1.1.3" xref="S3.Ex6.m1.1.1.1.1.3.cmml"><munder id="S3.Ex6.m1.1.1.1.1.3.1" xref="S3.Ex6.m1.1.1.1.1.3.1.cmml"><mi id="S3.Ex6.m1.1.1.1.1.3.1.2" xref="S3.Ex6.m1.1.1.1.1.3.1.2.cmml">max</mi><mrow id="S3.Ex6.m1.1.1.1.1.3.1.3" xref="S3.Ex6.m1.1.1.1.1.3.1.3.cmml"><mn id="S3.Ex6.m1.1.1.1.1.3.1.3.2" xref="S3.Ex6.m1.1.1.1.1.3.1.3.2.cmml">1</mn><mo id="S3.Ex6.m1.1.1.1.1.3.1.3.3" xref="S3.Ex6.m1.1.1.1.1.3.1.3.3.cmml">â‰¤</mo><mi id="S3.Ex6.m1.1.1.1.1.3.1.3.4" xref="S3.Ex6.m1.1.1.1.1.3.1.3.4.cmml">i</mi><mo id="S3.Ex6.m1.1.1.1.1.3.1.3.5" xref="S3.Ex6.m1.1.1.1.1.3.1.3.5.cmml">â‰¤</mo><mi id="S3.Ex6.m1.1.1.1.1.3.1.3.6" xref="S3.Ex6.m1.1.1.1.1.3.1.3.6.cmml">k</mi></mrow></munder><mo lspace="0.167em" id="S3.Ex6.m1.1.1.1.1.3a" xref="S3.Ex6.m1.1.1.1.1.3.cmml">â¡</mo><msub id="S3.Ex6.m1.1.1.1.1.3.2" xref="S3.Ex6.m1.1.1.1.1.3.2.cmml"><mi id="S3.Ex6.m1.1.1.1.1.3.2.2" xref="S3.Ex6.m1.1.1.1.1.3.2.2.cmml">M</mi><mi id="S3.Ex6.m1.1.1.1.1.3.2.3" xref="S3.Ex6.m1.1.1.1.1.3.2.3.cmml">i</mi></msub></mrow></mrow><mo lspace="0em" id="S3.Ex6.m1.1.1.1.2" xref="S3.Ex6.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex6.m1.1b"><apply id="S3.Ex6.m1.1.1.1.1.cmml" xref="S3.Ex6.m1.1.1.1"><eq id="S3.Ex6.m1.1.1.1.1.1.cmml" xref="S3.Ex6.m1.1.1.1.1.1"></eq><apply id="S3.Ex6.m1.1.1.1.1.2.cmml" xref="S3.Ex6.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex6.m1.1.1.1.1.2.1.cmml" xref="S3.Ex6.m1.1.1.1.1.2">subscript</csymbol><apply id="S3.Ex6.m1.1.1.1.1.2.2.cmml" xref="S3.Ex6.m1.1.1.1.1.2.2"><ci id="S3.Ex6.m1.1.1.1.1.2.2.1.cmml" xref="S3.Ex6.m1.1.1.1.1.2.2.1">^</ci><ci id="S3.Ex6.m1.1.1.1.1.2.2.2.cmml" xref="S3.Ex6.m1.1.1.1.1.2.2.2">ğ‘€</ci></apply><ci id="S3.Ex6.m1.1.1.1.1.2.3.cmml" xref="S3.Ex6.m1.1.1.1.1.2.3">ğ‘˜</ci></apply><apply id="S3.Ex6.m1.1.1.1.1.3.cmml" xref="S3.Ex6.m1.1.1.1.1.3"><apply id="S3.Ex6.m1.1.1.1.1.3.1.cmml" xref="S3.Ex6.m1.1.1.1.1.3.1"><csymbol cd="ambiguous" id="S3.Ex6.m1.1.1.1.1.3.1.1.cmml" xref="S3.Ex6.m1.1.1.1.1.3.1">subscript</csymbol><max id="S3.Ex6.m1.1.1.1.1.3.1.2.cmml" xref="S3.Ex6.m1.1.1.1.1.3.1.2"></max><apply id="S3.Ex6.m1.1.1.1.1.3.1.3.cmml" xref="S3.Ex6.m1.1.1.1.1.3.1.3"><and id="S3.Ex6.m1.1.1.1.1.3.1.3a.cmml" xref="S3.Ex6.m1.1.1.1.1.3.1.3"></and><apply id="S3.Ex6.m1.1.1.1.1.3.1.3b.cmml" xref="S3.Ex6.m1.1.1.1.1.3.1.3"><leq id="S3.Ex6.m1.1.1.1.1.3.1.3.3.cmml" xref="S3.Ex6.m1.1.1.1.1.3.1.3.3"></leq><cn type="integer" id="S3.Ex6.m1.1.1.1.1.3.1.3.2.cmml" xref="S3.Ex6.m1.1.1.1.1.3.1.3.2">1</cn><ci id="S3.Ex6.m1.1.1.1.1.3.1.3.4.cmml" xref="S3.Ex6.m1.1.1.1.1.3.1.3.4">ğ‘–</ci></apply><apply id="S3.Ex6.m1.1.1.1.1.3.1.3c.cmml" xref="S3.Ex6.m1.1.1.1.1.3.1.3"><leq id="S3.Ex6.m1.1.1.1.1.3.1.3.5.cmml" xref="S3.Ex6.m1.1.1.1.1.3.1.3.5"></leq><share href="#S3.Ex6.m1.1.1.1.1.3.1.3.4.cmml" id="S3.Ex6.m1.1.1.1.1.3.1.3d.cmml" xref="S3.Ex6.m1.1.1.1.1.3.1.3"></share><ci id="S3.Ex6.m1.1.1.1.1.3.1.3.6.cmml" xref="S3.Ex6.m1.1.1.1.1.3.1.3.6">ğ‘˜</ci></apply></apply></apply><apply id="S3.Ex6.m1.1.1.1.1.3.2.cmml" xref="S3.Ex6.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.Ex6.m1.1.1.1.1.3.2.1.cmml" xref="S3.Ex6.m1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.Ex6.m1.1.1.1.1.3.2.2.cmml" xref="S3.Ex6.m1.1.1.1.1.3.2.2">ğ‘€</ci><ci id="S3.Ex6.m1.1.1.1.1.3.2.3.cmml" xref="S3.Ex6.m1.1.1.1.1.3.2.3">ğ‘–</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex6.m1.1c">\hat{M}_{k}=\max_{1\leq i\leq k}M_{i}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS2.p7.2" class="ltx_p">By masking the BEV heatmap <math id="S3.SS2.p7.2.m1.1" class="ltx_Math" alttext="S_{k}" display="inline"><semantics id="S3.SS2.p7.2.m1.1a"><msub id="S3.SS2.p7.2.m1.1.1" xref="S3.SS2.p7.2.m1.1.1.cmml"><mi id="S3.SS2.p7.2.m1.1.1.2" xref="S3.SS2.p7.2.m1.1.1.2.cmml">S</mi><mi id="S3.SS2.p7.2.m1.1.1.3" xref="S3.SS2.p7.2.m1.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.2.m1.1b"><apply id="S3.SS2.p7.2.m1.1.1.cmml" xref="S3.SS2.p7.2.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p7.2.m1.1.1.1.cmml" xref="S3.SS2.p7.2.m1.1.1">subscript</csymbol><ci id="S3.SS2.p7.2.m1.1.1.2.cmml" xref="S3.SS2.p7.2.m1.1.1.2">ğ‘†</ci><ci id="S3.SS2.p7.2.m1.1.1.3.cmml" xref="S3.SS2.p7.2.m1.1.1.3">ğ‘˜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.2.m1.1c">S_{k}</annotation></semantics></math> with</p>
<table id="S3.Ex7" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex7.m1.1" class="ltx_Math" alttext="\hat{S}_{k}=S_{k}\cdot(1-\hat{M}_{k})," display="block"><semantics id="S3.Ex7.m1.1a"><mrow id="S3.Ex7.m1.1.1.1" xref="S3.Ex7.m1.1.1.1.1.cmml"><mrow id="S3.Ex7.m1.1.1.1.1" xref="S3.Ex7.m1.1.1.1.1.cmml"><msub id="S3.Ex7.m1.1.1.1.1.3" xref="S3.Ex7.m1.1.1.1.1.3.cmml"><mover accent="true" id="S3.Ex7.m1.1.1.1.1.3.2" xref="S3.Ex7.m1.1.1.1.1.3.2.cmml"><mi id="S3.Ex7.m1.1.1.1.1.3.2.2" xref="S3.Ex7.m1.1.1.1.1.3.2.2.cmml">S</mi><mo id="S3.Ex7.m1.1.1.1.1.3.2.1" xref="S3.Ex7.m1.1.1.1.1.3.2.1.cmml">^</mo></mover><mi id="S3.Ex7.m1.1.1.1.1.3.3" xref="S3.Ex7.m1.1.1.1.1.3.3.cmml">k</mi></msub><mo id="S3.Ex7.m1.1.1.1.1.2" xref="S3.Ex7.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.Ex7.m1.1.1.1.1.1" xref="S3.Ex7.m1.1.1.1.1.1.cmml"><msub id="S3.Ex7.m1.1.1.1.1.1.3" xref="S3.Ex7.m1.1.1.1.1.1.3.cmml"><mi id="S3.Ex7.m1.1.1.1.1.1.3.2" xref="S3.Ex7.m1.1.1.1.1.1.3.2.cmml">S</mi><mi id="S3.Ex7.m1.1.1.1.1.1.3.3" xref="S3.Ex7.m1.1.1.1.1.1.3.3.cmml">k</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.Ex7.m1.1.1.1.1.1.2" xref="S3.Ex7.m1.1.1.1.1.1.2.cmml">â‹…</mo><mrow id="S3.Ex7.m1.1.1.1.1.1.1.1" xref="S3.Ex7.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.Ex7.m1.1.1.1.1.1.1.1.2" xref="S3.Ex7.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.Ex7.m1.1.1.1.1.1.1.1.1" xref="S3.Ex7.m1.1.1.1.1.1.1.1.1.cmml"><mn id="S3.Ex7.m1.1.1.1.1.1.1.1.1.2" xref="S3.Ex7.m1.1.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S3.Ex7.m1.1.1.1.1.1.1.1.1.1" xref="S3.Ex7.m1.1.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><msub id="S3.Ex7.m1.1.1.1.1.1.1.1.1.3" xref="S3.Ex7.m1.1.1.1.1.1.1.1.1.3.cmml"><mover accent="true" id="S3.Ex7.m1.1.1.1.1.1.1.1.1.3.2" xref="S3.Ex7.m1.1.1.1.1.1.1.1.1.3.2.cmml"><mi id="S3.Ex7.m1.1.1.1.1.1.1.1.1.3.2.2" xref="S3.Ex7.m1.1.1.1.1.1.1.1.1.3.2.2.cmml">M</mi><mo id="S3.Ex7.m1.1.1.1.1.1.1.1.1.3.2.1" xref="S3.Ex7.m1.1.1.1.1.1.1.1.1.3.2.1.cmml">^</mo></mover><mi id="S3.Ex7.m1.1.1.1.1.1.1.1.1.3.3" xref="S3.Ex7.m1.1.1.1.1.1.1.1.1.3.3.cmml">k</mi></msub></mrow><mo stretchy="false" id="S3.Ex7.m1.1.1.1.1.1.1.1.3" xref="S3.Ex7.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.Ex7.m1.1.1.1.2" xref="S3.Ex7.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex7.m1.1b"><apply id="S3.Ex7.m1.1.1.1.1.cmml" xref="S3.Ex7.m1.1.1.1"><eq id="S3.Ex7.m1.1.1.1.1.2.cmml" xref="S3.Ex7.m1.1.1.1.1.2"></eq><apply id="S3.Ex7.m1.1.1.1.1.3.cmml" xref="S3.Ex7.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex7.m1.1.1.1.1.3.1.cmml" xref="S3.Ex7.m1.1.1.1.1.3">subscript</csymbol><apply id="S3.Ex7.m1.1.1.1.1.3.2.cmml" xref="S3.Ex7.m1.1.1.1.1.3.2"><ci id="S3.Ex7.m1.1.1.1.1.3.2.1.cmml" xref="S3.Ex7.m1.1.1.1.1.3.2.1">^</ci><ci id="S3.Ex7.m1.1.1.1.1.3.2.2.cmml" xref="S3.Ex7.m1.1.1.1.1.3.2.2">ğ‘†</ci></apply><ci id="S3.Ex7.m1.1.1.1.1.3.3.cmml" xref="S3.Ex7.m1.1.1.1.1.3.3">ğ‘˜</ci></apply><apply id="S3.Ex7.m1.1.1.1.1.1.cmml" xref="S3.Ex7.m1.1.1.1.1.1"><ci id="S3.Ex7.m1.1.1.1.1.1.2.cmml" xref="S3.Ex7.m1.1.1.1.1.1.2">â‹…</ci><apply id="S3.Ex7.m1.1.1.1.1.1.3.cmml" xref="S3.Ex7.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex7.m1.1.1.1.1.1.3.1.cmml" xref="S3.Ex7.m1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.Ex7.m1.1.1.1.1.1.3.2.cmml" xref="S3.Ex7.m1.1.1.1.1.1.3.2">ğ‘†</ci><ci id="S3.Ex7.m1.1.1.1.1.1.3.3.cmml" xref="S3.Ex7.m1.1.1.1.1.1.3.3">ğ‘˜</ci></apply><apply id="S3.Ex7.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex7.m1.1.1.1.1.1.1.1"><minus id="S3.Ex7.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex7.m1.1.1.1.1.1.1.1.1.1"></minus><cn type="integer" id="S3.Ex7.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex7.m1.1.1.1.1.1.1.1.1.2">1</cn><apply id="S3.Ex7.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex7.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex7.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.Ex7.m1.1.1.1.1.1.1.1.1.3">subscript</csymbol><apply id="S3.Ex7.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.Ex7.m1.1.1.1.1.1.1.1.1.3.2"><ci id="S3.Ex7.m1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.Ex7.m1.1.1.1.1.1.1.1.1.3.2.1">^</ci><ci id="S3.Ex7.m1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.Ex7.m1.1.1.1.1.1.1.1.1.3.2.2">ğ‘€</ci></apply><ci id="S3.Ex7.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.Ex7.m1.1.1.1.1.1.1.1.1.3.3">ğ‘˜</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex7.m1.1c">\hat{S}_{k}=S_{k}\cdot(1-\hat{M}_{k}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS2.p7.3" class="ltx_p">we omit prior easy positive regions in the current stage, thus enabling the model to focus on the false negative samples of the prior stage (hard instances). To train the multi-stage heatmap encoder, we adopt Gaussian Focal LossÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> as the training loss function. We sum up the BEV heatmap losses across stages to obtain the final heatmap loss.</p>
</div>
<div id="S3.SS2.p8" class="ltx_para">
<p id="S3.SS2.p8.1" class="ltx_p">During both training and inference, we collect the positive candidates from all stages as the object candidates for the second-stage rescoring as the potential false positive predictions.</p>
</div>
<div id="S3.SS2.p9" class="ltx_para ltx_noindent">
<p id="S3.SS2.p9.1" class="ltx_p"><span id="S3.SS2.p9.1.1" class="ltx_text ltx_font_bold">Discussion on implementation validity for HIP.</span>
Although the HIP strategy is simple, the masking way has two critical criteria that need to be met to ensure valid implementation of HIP:
</p>
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p">Exclusion of prior positive object candidates at the current stage.</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p">Avoidance of removal of potential real objects (false negatives).</p>
</div>
</li>
</ul>
<p id="S3.SS2.p9.2" class="ltx_p">Point masking satisfies both requirements based on the following facts. As the Top-K selection is based on ranking predicted BEV heatmap scores, the hottest response points are automatically excluded when a point is masked. Besides, the design of a class-aware positive mask ensures that non-overlapping assumptions at the intra-class level on the BEV are met.</p>
</div>
<div id="S3.SS2.p10" class="ltx_para">
<p id="S3.SS2.p10.1" class="ltx_p">However, the point masking strategy is less efficient as only one BEV object candidate is excluded for each positive prediction compared with the ideal masking with ground-truth box guidance. Therefore, there is a trade-off between the masking area and the validity of the exclusion operation. We compare all three strategies in TableÂ <a href="#S4.T5" title="Table 5 â€£ 4.4 Ablation Study â€£ 4 Experiments â€£ FocalFormer3D : Focusing on Hard Instance for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> and pooling-based masking performs better than others.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Box-level Deformable Decoder</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The object candidates obtained from the multi-stage heatmap encoder can be treated as positional object queriesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. The recall of initial candidates improves with an increase in the number of collected candidates. However, redundant candidates introduce false positives, thereby necessitating a high level of performance for the following object-level refinement blocks.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">To enhance the efficiency of object query processing, we employ deformable attentionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> instead of computationally intensive modules such as cross attentionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite> or box attentionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite>. Unlike previous methods that used center point features as the query embeddingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>, we model the object candidates as box-level queries. Specifically, Specifically, we introduce object supervision between deformable decoder layers, facilitating relative box prediction.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para ltx_noindent">
<p id="S3.SS3.p3.1" class="ltx_p"><span id="S3.SS3.p3.1.1" class="ltx_text ltx_font_bold">Box-pooling module.</span> To better model the relations between objects and local regions in the regular grid manner, we extract the box context information from the BEV features using simple RoIAlignÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> in the Box-pooling module as Fig.Â <a href="#S2.F2" title="Figure 2 â€£ 2 Related Work â€£ FocalFormer3D : Focusing on Hard Instance for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. In specific, given the intermediate predicted box, each object query extracts <math id="S3.SS3.p3.1.m1.1" class="ltx_Math" alttext="7\times 7" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><mrow id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml"><mn id="S3.SS3.p3.1.m1.1.1.2" xref="S3.SS3.p3.1.m1.1.1.2.cmml">7</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p3.1.m1.1.1.1" xref="S3.SS3.p3.1.m1.1.1.1.cmml">Ã—</mo><mn id="S3.SS3.p3.1.m1.1.1.3" xref="S3.SS3.p3.1.m1.1.1.3.cmml">7</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><apply id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1"><times id="S3.SS3.p3.1.m1.1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1.1"></times><cn type="integer" id="S3.SS3.p3.1.m1.1.1.2.cmml" xref="S3.SS3.p3.1.m1.1.1.2">7</cn><cn type="integer" id="S3.SS3.p3.1.m1.1.1.3.cmml" xref="S3.SS3.p3.1.m1.1.1.3">7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">7\times 7</annotation></semantics></math> feature grid points Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> from the BEV map followed by two MLP layers. The positional encoding is also applied both for queries and all BEV points for extracting positional information. This allows us to update both the content and positional information into the query embedding.
This lightweight module enhances the query feature for the deformable decoder (See TableÂ <a href="#S4.T6" title="Table 6 â€£ 4.4 Ablation Study â€£ 4 Experiments â€£ FocalFormer3D : Focusing on Hard Instance for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>).</p>
</div>
<div id="S3.SS3.p4" class="ltx_para ltx_noindent">
<p id="S3.SS3.p4.4" class="ltx_p"><span id="S3.SS3.p4.4.1" class="ltx_text ltx_font_bold">Decoder implementation.</span>
Following Deformable DETRÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, our model employs 8 heads in all attention modules, including multi-head attention and multi-head deformable attention. The deformable attention utilizes 4 sampling points across 3 scales. To generate three scales of BEV features, we apply <math id="S3.SS3.p4.1.m1.1" class="ltx_math_unparsed" alttext="2\times" display="inline"><semantics id="S3.SS3.p4.1.m1.1a"><mrow id="S3.SS3.p4.1.m1.1b"><mn id="S3.SS3.p4.1.m1.1.1">2</mn><mo lspace="0.222em" id="S3.SS3.p4.1.m1.1.2">Ã—</mo></mrow><annotation encoding="application/x-tex" id="S3.SS3.p4.1.m1.1c">2\times</annotation></semantics></math> and <math id="S3.SS3.p4.2.m2.1" class="ltx_math_unparsed" alttext="4\times" display="inline"><semantics id="S3.SS3.p4.2.m2.1a"><mrow id="S3.SS3.p4.2.m2.1b"><mn id="S3.SS3.p4.2.m2.1.1">4</mn><mo lspace="0.222em" id="S3.SS3.p4.2.m2.1.2">Ã—</mo></mrow><annotation encoding="application/x-tex" id="S3.SS3.p4.2.m2.1c">4\times</annotation></semantics></math> downsampling operations to the original BEV features. The box-pooling module extracts <math id="S3.SS3.p4.3.m3.1" class="ltx_Math" alttext="7\times 7" display="inline"><semantics id="S3.SS3.p4.3.m3.1a"><mrow id="S3.SS3.p4.3.m3.1.1" xref="S3.SS3.p4.3.m3.1.1.cmml"><mn id="S3.SS3.p4.3.m3.1.1.2" xref="S3.SS3.p4.3.m3.1.1.2.cmml">7</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p4.3.m3.1.1.1" xref="S3.SS3.p4.3.m3.1.1.1.cmml">Ã—</mo><mn id="S3.SS3.p4.3.m3.1.1.3" xref="S3.SS3.p4.3.m3.1.1.3.cmml">7</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.3.m3.1b"><apply id="S3.SS3.p4.3.m3.1.1.cmml" xref="S3.SS3.p4.3.m3.1.1"><times id="S3.SS3.p4.3.m3.1.1.1.cmml" xref="S3.SS3.p4.3.m3.1.1.1"></times><cn type="integer" id="S3.SS3.p4.3.m3.1.1.2.cmml" xref="S3.SS3.p4.3.m3.1.1.2">7</cn><cn type="integer" id="S3.SS3.p4.3.m3.1.1.3.cmml" xref="S3.SS3.p4.3.m3.1.1.3">7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.3.m3.1c">7\times 7</annotation></semantics></math> feature grid points within each <span id="S3.SS3.p4.4.2" class="ltx_text ltx_font_italic">rotated</span> BEV box followed by 2 FC layers and adds the object feature to query embedding. We expand the predicted box to <math id="S3.SS3.p4.4.m4.1" class="ltx_math_unparsed" alttext="1.2\times" display="inline"><semantics id="S3.SS3.p4.4.m4.1a"><mrow id="S3.SS3.p4.4.m4.1b"><mn id="S3.SS3.p4.4.m4.1.1">1.2</mn><mo lspace="0.222em" id="S3.SS3.p4.4.m4.1.2">Ã—</mo></mrow><annotation encoding="application/x-tex" id="S3.SS3.p4.4.m4.1c">1.2\times</annotation></semantics></math> size of its original size.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<div id="S3.T1.2" class="ltx_logical-block ltx_pruned_first">
<div id="S3.T1.2.p2" class="ltx_para ltx_noindent ltx_align_center">
<div id="S3.T1.2.p2.8" class="ltx_inline-block ltx_transformed_outer" style="width:667.7pt;height:487pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p id="S3.T1.2.p2.8.8" class="ltx_p"><span id="S3.T1.2.p2.8.8.8" class="ltx_text">
<span id="S3.T1.2.p2.8.8.8.8" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_tbody">
<span id="S3.T1.2.p2.8.8.8.8.9.1" class="ltx_tr">
<span id="S3.T1.2.p2.8.8.8.8.9.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">Methods</span>
<span id="S3.T1.2.p2.8.8.8.8.9.1.2" class="ltx_td ltx_align_center ltx_border_tt">Modality</span>
<span id="S3.T1.2.p2.8.8.8.8.9.1.3" class="ltx_td ltx_align_center ltx_border_tt">mAP</span>
<span id="S3.T1.2.p2.8.8.8.8.9.1.4" class="ltx_td ltx_align_center ltx_border_tt">NDS</span>
<span id="S3.T1.2.p2.8.8.8.8.9.1.5" class="ltx_td ltx_align_center ltx_border_tt">Car</span>
<span id="S3.T1.2.p2.8.8.8.8.9.1.6" class="ltx_td ltx_align_center ltx_border_tt">Truck</span>
<span id="S3.T1.2.p2.8.8.8.8.9.1.7" class="ltx_td ltx_align_center ltx_border_tt">C.V.</span>
<span id="S3.T1.2.p2.8.8.8.8.9.1.8" class="ltx_td ltx_align_center ltx_border_tt">Bus</span>
<span id="S3.T1.2.p2.8.8.8.8.9.1.9" class="ltx_td ltx_align_center ltx_border_tt">Trailer</span>
<span id="S3.T1.2.p2.8.8.8.8.9.1.10" class="ltx_td ltx_align_center ltx_border_tt">Barrier</span>
<span id="S3.T1.2.p2.8.8.8.8.9.1.11" class="ltx_td ltx_align_center ltx_border_tt">Motor.</span>
<span id="S3.T1.2.p2.8.8.8.8.9.1.12" class="ltx_td ltx_align_center ltx_border_tt">Bike</span>
<span id="S3.T1.2.p2.8.8.8.8.9.1.13" class="ltx_td ltx_align_center ltx_border_tt">Ped.</span>
<span id="S3.T1.2.p2.8.8.8.8.9.1.14" class="ltx_td ltx_align_center ltx_border_tt">T.C.</span></span>
<span id="S3.T1.2.p2.8.8.8.8.10.2" class="ltx_tr">
<span id="S3.T1.2.p2.8.8.8.8.10.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt ltx_colspan ltx_colspan_5"><span id="S3.T1.2.p2.8.8.8.8.10.2.1.1" class="ltx_text ltx_font_italic">LiDAR-based 3D Detection</span></span>
<span id="S3.T1.2.p2.8.8.8.8.10.2.2" class="ltx_td ltx_border_tt"></span>
<span id="S3.T1.2.p2.8.8.8.8.10.2.3" class="ltx_td ltx_border_tt"></span>
<span id="S3.T1.2.p2.8.8.8.8.10.2.4" class="ltx_td ltx_border_tt"></span>
<span id="S3.T1.2.p2.8.8.8.8.10.2.5" class="ltx_td ltx_border_tt"></span>
<span id="S3.T1.2.p2.8.8.8.8.10.2.6" class="ltx_td ltx_border_tt"></span>
<span id="S3.T1.2.p2.8.8.8.8.10.2.7" class="ltx_td ltx_border_tt"></span>
<span id="S3.T1.2.p2.8.8.8.8.10.2.8" class="ltx_td ltx_border_tt"></span>
<span id="S3.T1.2.p2.8.8.8.8.10.2.9" class="ltx_td ltx_border_tt"></span>
<span id="S3.T1.2.p2.8.8.8.8.10.2.10" class="ltx_td ltx_border_tt"></span></span>
<span id="S3.T1.2.p2.8.8.8.8.11.3" class="ltx_tr">
<span id="S3.T1.2.p2.8.8.8.8.11.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">PointPillars <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite></span>
<span id="S3.T1.2.p2.8.8.8.8.11.3.2" class="ltx_td ltx_align_center ltx_border_t">L</span>
<span id="S3.T1.2.p2.8.8.8.8.11.3.3" class="ltx_td ltx_align_center ltx_border_t">30.5</span>
<span id="S3.T1.2.p2.8.8.8.8.11.3.4" class="ltx_td ltx_align_center ltx_border_t">45.3</span>
<span id="S3.T1.2.p2.8.8.8.8.11.3.5" class="ltx_td ltx_align_center ltx_border_t">68.4</span>
<span id="S3.T1.2.p2.8.8.8.8.11.3.6" class="ltx_td ltx_align_center ltx_border_t">23.0</span>
<span id="S3.T1.2.p2.8.8.8.8.11.3.7" class="ltx_td ltx_align_center ltx_border_t">4.1</span>
<span id="S3.T1.2.p2.8.8.8.8.11.3.8" class="ltx_td ltx_align_center ltx_border_t">28.2</span>
<span id="S3.T1.2.p2.8.8.8.8.11.3.9" class="ltx_td ltx_align_center ltx_border_t">23.4</span>
<span id="S3.T1.2.p2.8.8.8.8.11.3.10" class="ltx_td ltx_align_center ltx_border_t">38.9</span>
<span id="S3.T1.2.p2.8.8.8.8.11.3.11" class="ltx_td ltx_align_center ltx_border_t">27.4</span>
<span id="S3.T1.2.p2.8.8.8.8.11.3.12" class="ltx_td ltx_align_center ltx_border_t">1.1</span>
<span id="S3.T1.2.p2.8.8.8.8.11.3.13" class="ltx_td ltx_align_center ltx_border_t">59.7</span>
<span id="S3.T1.2.p2.8.8.8.8.11.3.14" class="ltx_td ltx_align_center ltx_border_t">30.8</span></span>
<span id="S3.T1.2.p2.8.8.8.8.12.4" class="ltx_tr">
<span id="S3.T1.2.p2.8.8.8.8.12.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">CBGS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite></span>
<span id="S3.T1.2.p2.8.8.8.8.12.4.2" class="ltx_td ltx_align_center">L</span>
<span id="S3.T1.2.p2.8.8.8.8.12.4.3" class="ltx_td ltx_align_center">52.8</span>
<span id="S3.T1.2.p2.8.8.8.8.12.4.4" class="ltx_td ltx_align_center">63.3</span>
<span id="S3.T1.2.p2.8.8.8.8.12.4.5" class="ltx_td ltx_align_center">81.1</span>
<span id="S3.T1.2.p2.8.8.8.8.12.4.6" class="ltx_td ltx_align_center">48.5</span>
<span id="S3.T1.2.p2.8.8.8.8.12.4.7" class="ltx_td ltx_align_center">10.5</span>
<span id="S3.T1.2.p2.8.8.8.8.12.4.8" class="ltx_td ltx_align_center">54.9</span>
<span id="S3.T1.2.p2.8.8.8.8.12.4.9" class="ltx_td ltx_align_center">42.9</span>
<span id="S3.T1.2.p2.8.8.8.8.12.4.10" class="ltx_td ltx_align_center">65.7</span>
<span id="S3.T1.2.p2.8.8.8.8.12.4.11" class="ltx_td ltx_align_center">51.5</span>
<span id="S3.T1.2.p2.8.8.8.8.12.4.12" class="ltx_td ltx_align_center">22.3</span>
<span id="S3.T1.2.p2.8.8.8.8.12.4.13" class="ltx_td ltx_align_center">80.1</span>
<span id="S3.T1.2.p2.8.8.8.8.12.4.14" class="ltx_td ltx_align_center">70.9</span></span>
<span id="S3.T1.2.p2.8.8.8.8.13.5" class="ltx_tr">
<span id="S3.T1.2.p2.8.8.8.8.13.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">LargeKernel3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite></span>
<span id="S3.T1.2.p2.8.8.8.8.13.5.2" class="ltx_td ltx_align_center">L</span>
<span id="S3.T1.2.p2.8.8.8.8.13.5.3" class="ltx_td ltx_align_center">65.3</span>
<span id="S3.T1.2.p2.8.8.8.8.13.5.4" class="ltx_td ltx_align_center">70.5</span>
<span id="S3.T1.2.p2.8.8.8.8.13.5.5" class="ltx_td ltx_align_center">85.9</span>
<span id="S3.T1.2.p2.8.8.8.8.13.5.6" class="ltx_td ltx_align_center">55.3</span>
<span id="S3.T1.2.p2.8.8.8.8.13.5.7" class="ltx_td ltx_align_center">26.8</span>
<span id="S3.T1.2.p2.8.8.8.8.13.5.8" class="ltx_td ltx_align_center">66.2</span>
<span id="S3.T1.2.p2.8.8.8.8.13.5.9" class="ltx_td ltx_align_center">60.2</span>
<span id="S3.T1.2.p2.8.8.8.8.13.5.10" class="ltx_td ltx_align_center">74.3</span>
<span id="S3.T1.2.p2.8.8.8.8.13.5.11" class="ltx_td ltx_align_center">72.5</span>
<span id="S3.T1.2.p2.8.8.8.8.13.5.12" class="ltx_td ltx_align_center">46.6</span>
<span id="S3.T1.2.p2.8.8.8.8.13.5.13" class="ltx_td ltx_align_center">85.6</span>
<span id="S3.T1.2.p2.8.8.8.8.13.5.14" class="ltx_td ltx_align_center">80.0</span></span>
<span id="S3.T1.2.p2.8.8.8.8.14.6" class="ltx_tr">
<span id="S3.T1.2.p2.8.8.8.8.14.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">TransFusion-L <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite></span>
<span id="S3.T1.2.p2.8.8.8.8.14.6.2" class="ltx_td ltx_align_center">L</span>
<span id="S3.T1.2.p2.8.8.8.8.14.6.3" class="ltx_td ltx_align_center">65.5</span>
<span id="S3.T1.2.p2.8.8.8.8.14.6.4" class="ltx_td ltx_align_center">70.2</span>
<span id="S3.T1.2.p2.8.8.8.8.14.6.5" class="ltx_td ltx_align_center">86.2</span>
<span id="S3.T1.2.p2.8.8.8.8.14.6.6" class="ltx_td ltx_align_center">56.7</span>
<span id="S3.T1.2.p2.8.8.8.8.14.6.7" class="ltx_td ltx_align_center">28.2</span>
<span id="S3.T1.2.p2.8.8.8.8.14.6.8" class="ltx_td ltx_align_center">66.3</span>
<span id="S3.T1.2.p2.8.8.8.8.14.6.9" class="ltx_td ltx_align_center">58.8</span>
<span id="S3.T1.2.p2.8.8.8.8.14.6.10" class="ltx_td ltx_align_center">78.2</span>
<span id="S3.T1.2.p2.8.8.8.8.14.6.11" class="ltx_td ltx_align_center">68.3</span>
<span id="S3.T1.2.p2.8.8.8.8.14.6.12" class="ltx_td ltx_align_center">44.2</span>
<span id="S3.T1.2.p2.8.8.8.8.14.6.13" class="ltx_td ltx_align_center">86.1</span>
<span id="S3.T1.2.p2.8.8.8.8.14.6.14" class="ltx_td ltx_align_center">82.0</span></span>
<span id="S3.T1.2.p2.8.8.8.8.15.7" class="ltx_tr">
<span id="S3.T1.2.p2.8.8.8.8.15.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">PillarNet-34Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite></span>
<span id="S3.T1.2.p2.8.8.8.8.15.7.2" class="ltx_td ltx_align_center">L</span>
<span id="S3.T1.2.p2.8.8.8.8.15.7.3" class="ltx_td ltx_align_center">66.0</span>
<span id="S3.T1.2.p2.8.8.8.8.15.7.4" class="ltx_td ltx_align_center">71.4</span>
<span id="S3.T1.2.p2.8.8.8.8.15.7.5" class="ltx_td ltx_align_center">87.6</span>
<span id="S3.T1.2.p2.8.8.8.8.15.7.6" class="ltx_td ltx_align_center"><span id="S3.T1.2.p2.8.8.8.8.15.7.6.1" class="ltx_text ltx_font_bold">57.5</span></span>
<span id="S3.T1.2.p2.8.8.8.8.15.7.7" class="ltx_td ltx_align_center">27.9</span>
<span id="S3.T1.2.p2.8.8.8.8.15.7.8" class="ltx_td ltx_align_center">63.6</span>
<span id="S3.T1.2.p2.8.8.8.8.15.7.9" class="ltx_td ltx_align_center">63.1</span>
<span id="S3.T1.2.p2.8.8.8.8.15.7.10" class="ltx_td ltx_align_center">77.2</span>
<span id="S3.T1.2.p2.8.8.8.8.15.7.11" class="ltx_td ltx_align_center">70.1</span>
<span id="S3.T1.2.p2.8.8.8.8.15.7.12" class="ltx_td ltx_align_center">42.3</span>
<span id="S3.T1.2.p2.8.8.8.8.15.7.13" class="ltx_td ltx_align_center">87.3</span>
<span id="S3.T1.2.p2.8.8.8.8.15.7.14" class="ltx_td ltx_align_center">83.3</span></span>
<span id="S3.T1.2.p2.8.8.8.8.16.8" class="ltx_tr">
<span id="S3.T1.2.p2.8.8.8.8.16.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">LiDARMultiNetÂ Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite></span>
<span id="S3.T1.2.p2.8.8.8.8.16.8.2" class="ltx_td ltx_align_center">L</span>
<span id="S3.T1.2.p2.8.8.8.8.16.8.3" class="ltx_td ltx_align_center">67.0</span>
<span id="S3.T1.2.p2.8.8.8.8.16.8.4" class="ltx_td ltx_align_center">71.6</span>
<span id="S3.T1.2.p2.8.8.8.8.16.8.5" class="ltx_td ltx_align_center">86.9</span>
<span id="S3.T1.2.p2.8.8.8.8.16.8.6" class="ltx_td ltx_align_center">57.4</span>
<span id="S3.T1.2.p2.8.8.8.8.16.8.7" class="ltx_td ltx_align_center">31.5</span>
<span id="S3.T1.2.p2.8.8.8.8.16.8.8" class="ltx_td ltx_align_center">64.7</span>
<span id="S3.T1.2.p2.8.8.8.8.16.8.9" class="ltx_td ltx_align_center">61.0</span>
<span id="S3.T1.2.p2.8.8.8.8.16.8.10" class="ltx_td ltx_align_center">73.5</span>
<span id="S3.T1.2.p2.8.8.8.8.16.8.11" class="ltx_td ltx_align_center">75.3</span>
<span id="S3.T1.2.p2.8.8.8.8.16.8.12" class="ltx_td ltx_align_center">47.6</span>
<span id="S3.T1.2.p2.8.8.8.8.16.8.13" class="ltx_td ltx_align_center">87.2</span>
<span id="S3.T1.2.p2.8.8.8.8.16.8.14" class="ltx_td ltx_align_center"><span id="S3.T1.2.p2.8.8.8.8.16.8.14.1" class="ltx_text ltx_font_bold">85.1</span></span></span>
<span id="S3.T1.2.p2.8.8.8.8.17.9" class="ltx_tr">
<span id="S3.T1.2.p2.8.8.8.8.17.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S3.T1.2.p2.8.8.8.8.17.9.1.1" class="ltx_text ltx_font_bold">FocalFormer3D</span></span>
<span id="S3.T1.2.p2.8.8.8.8.17.9.2" class="ltx_td ltx_align_center">L</span>
<span id="S3.T1.2.p2.8.8.8.8.17.9.3" class="ltx_td ltx_align_center"><span id="S3.T1.2.p2.8.8.8.8.17.9.3.1" class="ltx_text ltx_font_bold">68.7</span></span>
<span id="S3.T1.2.p2.8.8.8.8.17.9.4" class="ltx_td ltx_align_center"><span id="S3.T1.2.p2.8.8.8.8.17.9.4.1" class="ltx_text ltx_font_bold">72.6</span></span>
<span id="S3.T1.2.p2.8.8.8.8.17.9.5" class="ltx_td ltx_align_center"><span id="S3.T1.2.p2.8.8.8.8.17.9.5.1" class="ltx_text ltx_font_bold">87.2</span></span>
<span id="S3.T1.2.p2.8.8.8.8.17.9.6" class="ltx_td ltx_align_center">57.1</span>
<span id="S3.T1.2.p2.8.8.8.8.17.9.7" class="ltx_td ltx_align_center"><span id="S3.T1.2.p2.8.8.8.8.17.9.7.1" class="ltx_text ltx_font_bold">34.4</span></span>
<span id="S3.T1.2.p2.8.8.8.8.17.9.8" class="ltx_td ltx_align_center"><span id="S3.T1.2.p2.8.8.8.8.17.9.8.1" class="ltx_text ltx_font_bold">69.6</span></span>
<span id="S3.T1.2.p2.8.8.8.8.17.9.9" class="ltx_td ltx_align_center"><span id="S3.T1.2.p2.8.8.8.8.17.9.9.1" class="ltx_text ltx_font_bold">64.9</span></span>
<span id="S3.T1.2.p2.8.8.8.8.17.9.10" class="ltx_td ltx_align_center"><span id="S3.T1.2.p2.8.8.8.8.17.9.10.1" class="ltx_text ltx_font_bold">77.8</span></span>
<span id="S3.T1.2.p2.8.8.8.8.17.9.11" class="ltx_td ltx_align_center"><span id="S3.T1.2.p2.8.8.8.8.17.9.11.1" class="ltx_text ltx_font_bold">76.2</span></span>
<span id="S3.T1.2.p2.8.8.8.8.17.9.12" class="ltx_td ltx_align_center"><span id="S3.T1.2.p2.8.8.8.8.17.9.12.1" class="ltx_text ltx_font_bold">49.6</span></span>
<span id="S3.T1.2.p2.8.8.8.8.17.9.13" class="ltx_td ltx_align_center"><span id="S3.T1.2.p2.8.8.8.8.17.9.13.1" class="ltx_text ltx_font_bold">88.2</span></span>
<span id="S3.T1.2.p2.8.8.8.8.17.9.14" class="ltx_td ltx_align_center">82.3</span></span>
<span id="S3.T1.2.p2.1.1.1.1.1" class="ltx_tr">
<span id="S3.T1.2.p2.1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">CenterPoint <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> <sup id="S3.T1.2.p2.1.1.1.1.1.1.1" class="ltx_sup"><span id="S3.T1.2.p2.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_italic">â€ </span></sup></span>
<span id="S3.T1.2.p2.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t">L</span>
<span id="S3.T1.2.p2.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t">60.3</span>
<span id="S3.T1.2.p2.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t">67.3</span>
<span id="S3.T1.2.p2.1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t">85.2</span>
<span id="S3.T1.2.p2.1.1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_t">53.5</span>
<span id="S3.T1.2.p2.1.1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_t">20.0</span>
<span id="S3.T1.2.p2.1.1.1.1.1.8" class="ltx_td ltx_align_center ltx_border_t">63.6</span>
<span id="S3.T1.2.p2.1.1.1.1.1.9" class="ltx_td ltx_align_center ltx_border_t">56.0</span>
<span id="S3.T1.2.p2.1.1.1.1.1.10" class="ltx_td ltx_align_center ltx_border_t">71.1</span>
<span id="S3.T1.2.p2.1.1.1.1.1.11" class="ltx_td ltx_align_center ltx_border_t">59.5</span>
<span id="S3.T1.2.p2.1.1.1.1.1.12" class="ltx_td ltx_align_center ltx_border_t">30.7</span>
<span id="S3.T1.2.p2.1.1.1.1.1.13" class="ltx_td ltx_align_center ltx_border_t">84.6</span>
<span id="S3.T1.2.p2.1.1.1.1.1.14" class="ltx_td ltx_align_center ltx_border_t">78.4</span></span>
<span id="S3.T1.2.p2.2.2.2.2.2" class="ltx_tr">
<span id="S3.T1.2.p2.2.2.2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MGTANet<sup id="S3.T1.2.p2.2.2.2.2.2.1.1" class="ltx_sup"><span id="S3.T1.2.p2.2.2.2.2.2.1.1.1" class="ltx_text ltx_font_italic">â€ </span></sup>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite></span>
<span id="S3.T1.2.p2.2.2.2.2.2.2" class="ltx_td ltx_align_center">L</span>
<span id="S3.T1.2.p2.2.2.2.2.2.3" class="ltx_td ltx_align_center">67.5</span>
<span id="S3.T1.2.p2.2.2.2.2.2.4" class="ltx_td ltx_align_center">72.7</span>
<span id="S3.T1.2.p2.2.2.2.2.2.5" class="ltx_td ltx_align_center">88.5</span>
<span id="S3.T1.2.p2.2.2.2.2.2.6" class="ltx_td ltx_align_center">59.8</span>
<span id="S3.T1.2.p2.2.2.2.2.2.7" class="ltx_td ltx_align_center">30.6</span>
<span id="S3.T1.2.p2.2.2.2.2.2.8" class="ltx_td ltx_align_center">67.2</span>
<span id="S3.T1.2.p2.2.2.2.2.2.9" class="ltx_td ltx_align_center">61.5</span>
<span id="S3.T1.2.p2.2.2.2.2.2.10" class="ltx_td ltx_align_center">66.3</span>
<span id="S3.T1.2.p2.2.2.2.2.2.11" class="ltx_td ltx_align_center">75.8</span>
<span id="S3.T1.2.p2.2.2.2.2.2.12" class="ltx_td ltx_align_center">52.5</span>
<span id="S3.T1.2.p2.2.2.2.2.2.13" class="ltx_td ltx_align_center">87.3</span>
<span id="S3.T1.2.p2.2.2.2.2.2.14" class="ltx_td ltx_align_center"><span id="S3.T1.2.p2.2.2.2.2.2.14.1" class="ltx_text ltx_font_bold">85.5</span></span></span>
<span id="S3.T1.2.p2.3.3.3.3.3" class="ltx_tr">
<span id="S3.T1.2.p2.3.3.3.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">LargeKernel3D<sup id="S3.T1.2.p2.3.3.3.3.3.1.1" class="ltx_sup"><span id="S3.T1.2.p2.3.3.3.3.3.1.1.1" class="ltx_text ltx_font_italic">â€¡</span></sup>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite></span>
<span id="S3.T1.2.p2.3.3.3.3.3.2" class="ltx_td ltx_align_center">L</span>
<span id="S3.T1.2.p2.3.3.3.3.3.3" class="ltx_td ltx_align_center">68.8</span>
<span id="S3.T1.2.p2.3.3.3.3.3.4" class="ltx_td ltx_align_center">72.8</span>
<span id="S3.T1.2.p2.3.3.3.3.3.5" class="ltx_td ltx_align_center">87.3</span>
<span id="S3.T1.2.p2.3.3.3.3.3.6" class="ltx_td ltx_align_center">59.1</span>
<span id="S3.T1.2.p2.3.3.3.3.3.7" class="ltx_td ltx_align_center">30.2</span>
<span id="S3.T1.2.p2.3.3.3.3.3.8" class="ltx_td ltx_align_center">68.5</span>
<span id="S3.T1.2.p2.3.3.3.3.3.9" class="ltx_td ltx_align_center">65.6</span>
<span id="S3.T1.2.p2.3.3.3.3.3.10" class="ltx_td ltx_align_center">75.0</span>
<span id="S3.T1.2.p2.3.3.3.3.3.11" class="ltx_td ltx_align_center"><span id="S3.T1.2.p2.3.3.3.3.3.11.1" class="ltx_text ltx_font_bold">77.8</span></span>
<span id="S3.T1.2.p2.3.3.3.3.3.12" class="ltx_td ltx_align_center"><span id="S3.T1.2.p2.3.3.3.3.3.12.1" class="ltx_text ltx_font_bold">53.5</span></span>
<span id="S3.T1.2.p2.3.3.3.3.3.13" class="ltx_td ltx_align_center">88.3</span>
<span id="S3.T1.2.p2.3.3.3.3.3.14" class="ltx_td ltx_align_center">82.4</span></span>
<span id="S3.T1.2.p2.4.4.4.4.4" class="ltx_tr">
<span id="S3.T1.2.p2.4.4.4.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S3.T1.2.p2.4.4.4.4.4.1.1" class="ltx_text ltx_font_bold">FocalFormer3D</span> <sup id="S3.T1.2.p2.4.4.4.4.4.1.2" class="ltx_sup"><span id="S3.T1.2.p2.4.4.4.4.4.1.2.1" class="ltx_text ltx_font_italic">â€ </span></sup></span>
<span id="S3.T1.2.p2.4.4.4.4.4.2" class="ltx_td ltx_align_center">L</span>
<span id="S3.T1.2.p2.4.4.4.4.4.3" class="ltx_td ltx_align_center"><span id="S3.T1.2.p2.4.4.4.4.4.3.1" class="ltx_text ltx_font_bold">70.5</span></span>
<span id="S3.T1.2.p2.4.4.4.4.4.4" class="ltx_td ltx_align_center"><span id="S3.T1.2.p2.4.4.4.4.4.4.1" class="ltx_text ltx_font_bold">73.9</span></span>
<span id="S3.T1.2.p2.4.4.4.4.4.5" class="ltx_td ltx_align_center"><span id="S3.T1.2.p2.4.4.4.4.4.5.1" class="ltx_text ltx_font_bold">87.8</span></span>
<span id="S3.T1.2.p2.4.4.4.4.4.6" class="ltx_td ltx_align_center"><span id="S3.T1.2.p2.4.4.4.4.4.6.1" class="ltx_text ltx_font_bold">59.4</span></span>
<span id="S3.T1.2.p2.4.4.4.4.4.7" class="ltx_td ltx_align_center"><span id="S3.T1.2.p2.4.4.4.4.4.7.1" class="ltx_text ltx_font_bold">37.8</span></span>
<span id="S3.T1.2.p2.4.4.4.4.4.8" class="ltx_td ltx_align_center"><span id="S3.T1.2.p2.4.4.4.4.4.8.1" class="ltx_text ltx_font_bold">73.0</span></span>
<span id="S3.T1.2.p2.4.4.4.4.4.9" class="ltx_td ltx_align_center"><span id="S3.T1.2.p2.4.4.4.4.4.9.1" class="ltx_text ltx_font_bold">65.7</span></span>
<span id="S3.T1.2.p2.4.4.4.4.4.10" class="ltx_td ltx_align_center"><span id="S3.T1.2.p2.4.4.4.4.4.10.1" class="ltx_text ltx_font_bold">77.8</span></span>
<span id="S3.T1.2.p2.4.4.4.4.4.11" class="ltx_td ltx_align_center">77.4</span>
<span id="S3.T1.2.p2.4.4.4.4.4.12" class="ltx_td ltx_align_center">52.4</span>
<span id="S3.T1.2.p2.4.4.4.4.4.13" class="ltx_td ltx_align_center"><span id="S3.T1.2.p2.4.4.4.4.4.13.1" class="ltx_text ltx_font_bold">90.0</span></span>
<span id="S3.T1.2.p2.4.4.4.4.4.14" class="ltx_td ltx_align_center">83.4</span></span>
<span id="S3.T1.2.p2.8.8.8.8.18.10" class="ltx_tr">
<span id="S3.T1.2.p2.8.8.8.8.18.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt ltx_colspan ltx_colspan_5"><span id="S3.T1.2.p2.8.8.8.8.18.10.1.1" class="ltx_text ltx_font_italic">Multi-Modal 3D Detection</span></span>
<span id="S3.T1.2.p2.8.8.8.8.18.10.2" class="ltx_td ltx_border_tt"></span>
<span id="S3.T1.2.p2.8.8.8.8.18.10.3" class="ltx_td ltx_border_tt"></span>
<span id="S3.T1.2.p2.8.8.8.8.18.10.4" class="ltx_td ltx_border_tt"></span>
<span id="S3.T1.2.p2.8.8.8.8.18.10.5" class="ltx_td ltx_border_tt"></span>
<span id="S3.T1.2.p2.8.8.8.8.18.10.6" class="ltx_td ltx_border_tt"></span>
<span id="S3.T1.2.p2.8.8.8.8.18.10.7" class="ltx_td ltx_border_tt"></span>
<span id="S3.T1.2.p2.8.8.8.8.18.10.8" class="ltx_td ltx_border_tt"></span>
<span id="S3.T1.2.p2.8.8.8.8.18.10.9" class="ltx_td ltx_border_tt"></span>
<span id="S3.T1.2.p2.8.8.8.8.18.10.10" class="ltx_td ltx_border_tt"></span></span>
<span id="S3.T1.2.p2.8.8.8.8.19.11" class="ltx_tr">
<span id="S3.T1.2.p2.8.8.8.8.19.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">PointPainting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite></span>
<span id="S3.T1.2.p2.8.8.8.8.19.11.2" class="ltx_td ltx_align_center ltx_border_t">L+C</span>
<span id="S3.T1.2.p2.8.8.8.8.19.11.3" class="ltx_td ltx_align_center ltx_border_t">46.4</span>
<span id="S3.T1.2.p2.8.8.8.8.19.11.4" class="ltx_td ltx_align_center ltx_border_t">58.1</span>
<span id="S3.T1.2.p2.8.8.8.8.19.11.5" class="ltx_td ltx_align_center ltx_border_t">77.9</span>
<span id="S3.T1.2.p2.8.8.8.8.19.11.6" class="ltx_td ltx_align_center ltx_border_t">35.8</span>
<span id="S3.T1.2.p2.8.8.8.8.19.11.7" class="ltx_td ltx_align_center ltx_border_t">15.8</span>
<span id="S3.T1.2.p2.8.8.8.8.19.11.8" class="ltx_td ltx_align_center ltx_border_t">36.2</span>
<span id="S3.T1.2.p2.8.8.8.8.19.11.9" class="ltx_td ltx_align_center ltx_border_t">37.3</span>
<span id="S3.T1.2.p2.8.8.8.8.19.11.10" class="ltx_td ltx_align_center ltx_border_t">60.2</span>
<span id="S3.T1.2.p2.8.8.8.8.19.11.11" class="ltx_td ltx_align_center ltx_border_t">41.5</span>
<span id="S3.T1.2.p2.8.8.8.8.19.11.12" class="ltx_td ltx_align_center ltx_border_t">24.1</span>
<span id="S3.T1.2.p2.8.8.8.8.19.11.13" class="ltx_td ltx_align_center ltx_border_t">73.3</span>
<span id="S3.T1.2.p2.8.8.8.8.19.11.14" class="ltx_td ltx_align_center ltx_border_t">62.4</span></span>
<span id="S3.T1.2.p2.8.8.8.8.20.12" class="ltx_tr">
<span id="S3.T1.2.p2.8.8.8.8.20.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">3D-CVF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite></span>
<span id="S3.T1.2.p2.8.8.8.8.20.12.2" class="ltx_td ltx_align_center">L+C</span>
<span id="S3.T1.2.p2.8.8.8.8.20.12.3" class="ltx_td ltx_align_center">52.7</span>
<span id="S3.T1.2.p2.8.8.8.8.20.12.4" class="ltx_td ltx_align_center">62.3</span>
<span id="S3.T1.2.p2.8.8.8.8.20.12.5" class="ltx_td ltx_align_center">83.0</span>
<span id="S3.T1.2.p2.8.8.8.8.20.12.6" class="ltx_td ltx_align_center">45.0</span>
<span id="S3.T1.2.p2.8.8.8.8.20.12.7" class="ltx_td ltx_align_center">15.9</span>
<span id="S3.T1.2.p2.8.8.8.8.20.12.8" class="ltx_td ltx_align_center">48.8</span>
<span id="S3.T1.2.p2.8.8.8.8.20.12.9" class="ltx_td ltx_align_center">49.6</span>
<span id="S3.T1.2.p2.8.8.8.8.20.12.10" class="ltx_td ltx_align_center">65.9</span>
<span id="S3.T1.2.p2.8.8.8.8.20.12.11" class="ltx_td ltx_align_center">51.2</span>
<span id="S3.T1.2.p2.8.8.8.8.20.12.12" class="ltx_td ltx_align_center">30.4</span>
<span id="S3.T1.2.p2.8.8.8.8.20.12.13" class="ltx_td ltx_align_center">74.2</span>
<span id="S3.T1.2.p2.8.8.8.8.20.12.14" class="ltx_td ltx_align_center">62.9</span></span>
<span id="S3.T1.2.p2.8.8.8.8.21.13" class="ltx_tr">
<span id="S3.T1.2.p2.8.8.8.8.21.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MVP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite></span>
<span id="S3.T1.2.p2.8.8.8.8.21.13.2" class="ltx_td ltx_align_center">L+C</span>
<span id="S3.T1.2.p2.8.8.8.8.21.13.3" class="ltx_td ltx_align_center">66.4</span>
<span id="S3.T1.2.p2.8.8.8.8.21.13.4" class="ltx_td ltx_align_center">70.5</span>
<span id="S3.T1.2.p2.8.8.8.8.21.13.5" class="ltx_td ltx_align_center">86.8</span>
<span id="S3.T1.2.p2.8.8.8.8.21.13.6" class="ltx_td ltx_align_center">58.5</span>
<span id="S3.T1.2.p2.8.8.8.8.21.13.7" class="ltx_td ltx_align_center">26.1</span>
<span id="S3.T1.2.p2.8.8.8.8.21.13.8" class="ltx_td ltx_align_center">67.4</span>
<span id="S3.T1.2.p2.8.8.8.8.21.13.9" class="ltx_td ltx_align_center">57.3</span>
<span id="S3.T1.2.p2.8.8.8.8.21.13.10" class="ltx_td ltx_align_center">74.8</span>
<span id="S3.T1.2.p2.8.8.8.8.21.13.11" class="ltx_td ltx_align_center">70.0</span>
<span id="S3.T1.2.p2.8.8.8.8.21.13.12" class="ltx_td ltx_align_center">49.3</span>
<span id="S3.T1.2.p2.8.8.8.8.21.13.13" class="ltx_td ltx_align_center">89.1</span>
<span id="S3.T1.2.p2.8.8.8.8.21.13.14" class="ltx_td ltx_align_center">85.0</span></span>
<span id="S3.T1.2.p2.8.8.8.8.22.14" class="ltx_tr">
<span id="S3.T1.2.p2.8.8.8.8.22.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">FusionPainting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite></span>
<span id="S3.T1.2.p2.8.8.8.8.22.14.2" class="ltx_td ltx_align_center">L+C</span>
<span id="S3.T1.2.p2.8.8.8.8.22.14.3" class="ltx_td ltx_align_center">68.1</span>
<span id="S3.T1.2.p2.8.8.8.8.22.14.4" class="ltx_td ltx_align_center">71.6</span>
<span id="S3.T1.2.p2.8.8.8.8.22.14.5" class="ltx_td ltx_align_center">87.1</span>
<span id="S3.T1.2.p2.8.8.8.8.22.14.6" class="ltx_td ltx_align_center">60.8</span>
<span id="S3.T1.2.p2.8.8.8.8.22.14.7" class="ltx_td ltx_align_center">30.0</span>
<span id="S3.T1.2.p2.8.8.8.8.22.14.8" class="ltx_td ltx_align_center">68.5</span>
<span id="S3.T1.2.p2.8.8.8.8.22.14.9" class="ltx_td ltx_align_center">61.7</span>
<span id="S3.T1.2.p2.8.8.8.8.22.14.10" class="ltx_td ltx_align_center">71.8</span>
<span id="S3.T1.2.p2.8.8.8.8.22.14.11" class="ltx_td ltx_align_center">74.7</span>
<span id="S3.T1.2.p2.8.8.8.8.22.14.12" class="ltx_td ltx_align_center">53.5</span>
<span id="S3.T1.2.p2.8.8.8.8.22.14.13" class="ltx_td ltx_align_center">88.3</span>
<span id="S3.T1.2.p2.8.8.8.8.22.14.14" class="ltx_td ltx_align_center">85.0</span></span>
<span id="S3.T1.2.p2.8.8.8.8.23.15" class="ltx_tr">
<span id="S3.T1.2.p2.8.8.8.8.23.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">TransFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite></span>
<span id="S3.T1.2.p2.8.8.8.8.23.15.2" class="ltx_td ltx_align_center">L+C</span>
<span id="S3.T1.2.p2.8.8.8.8.23.15.3" class="ltx_td ltx_align_center">68.9</span>
<span id="S3.T1.2.p2.8.8.8.8.23.15.4" class="ltx_td ltx_align_center">71.7</span>
<span id="S3.T1.2.p2.8.8.8.8.23.15.5" class="ltx_td ltx_align_center">87.1</span>
<span id="S3.T1.2.p2.8.8.8.8.23.15.6" class="ltx_td ltx_align_center">60.0</span>
<span id="S3.T1.2.p2.8.8.8.8.23.15.7" class="ltx_td ltx_align_center">33.1</span>
<span id="S3.T1.2.p2.8.8.8.8.23.15.8" class="ltx_td ltx_align_center">68.3</span>
<span id="S3.T1.2.p2.8.8.8.8.23.15.9" class="ltx_td ltx_align_center">60.8</span>
<span id="S3.T1.2.p2.8.8.8.8.23.15.10" class="ltx_td ltx_align_center">78.1</span>
<span id="S3.T1.2.p2.8.8.8.8.23.15.11" class="ltx_td ltx_align_center">73.6</span>
<span id="S3.T1.2.p2.8.8.8.8.23.15.12" class="ltx_td ltx_align_center">52.9</span>
<span id="S3.T1.2.p2.8.8.8.8.23.15.13" class="ltx_td ltx_align_center">88.4</span>
<span id="S3.T1.2.p2.8.8.8.8.23.15.14" class="ltx_td ltx_align_center">86.7</span></span>
<span id="S3.T1.2.p2.8.8.8.8.24.16" class="ltx_tr">
<span id="S3.T1.2.p2.8.8.8.8.24.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">BEVFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite></span>
<span id="S3.T1.2.p2.8.8.8.8.24.16.2" class="ltx_td ltx_align_center">L+C</span>
<span id="S3.T1.2.p2.8.8.8.8.24.16.3" class="ltx_td ltx_align_center">69.2</span>
<span id="S3.T1.2.p2.8.8.8.8.24.16.4" class="ltx_td ltx_align_center">71.8</span>
<span id="S3.T1.2.p2.8.8.8.8.24.16.5" class="ltx_td ltx_align_center">88.1</span>
<span id="S3.T1.2.p2.8.8.8.8.24.16.6" class="ltx_td ltx_align_center">60.9</span>
<span id="S3.T1.2.p2.8.8.8.8.24.16.7" class="ltx_td ltx_align_center">34.4</span>
<span id="S3.T1.2.p2.8.8.8.8.24.16.8" class="ltx_td ltx_align_center">69.3</span>
<span id="S3.T1.2.p2.8.8.8.8.24.16.9" class="ltx_td ltx_align_center">62.1</span>
<span id="S3.T1.2.p2.8.8.8.8.24.16.10" class="ltx_td ltx_align_center">78.2</span>
<span id="S3.T1.2.p2.8.8.8.8.24.16.11" class="ltx_td ltx_align_center">72.2</span>
<span id="S3.T1.2.p2.8.8.8.8.24.16.12" class="ltx_td ltx_align_center">52.2</span>
<span id="S3.T1.2.p2.8.8.8.8.24.16.13" class="ltx_td ltx_align_center">89.2</span>
<span id="S3.T1.2.p2.8.8.8.8.24.16.14" class="ltx_td ltx_align_center">85.2</span></span>
<span id="S3.T1.2.p2.8.8.8.8.25.17" class="ltx_tr">
<span id="S3.T1.2.p2.8.8.8.8.25.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">BEVFusion-MIT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite></span>
<span id="S3.T1.2.p2.8.8.8.8.25.17.2" class="ltx_td ltx_align_center">L+C</span>
<span id="S3.T1.2.p2.8.8.8.8.25.17.3" class="ltx_td ltx_align_center">70.2</span>
<span id="S3.T1.2.p2.8.8.8.8.25.17.4" class="ltx_td ltx_align_center">72.9</span>
<span id="S3.T1.2.p2.8.8.8.8.25.17.5" class="ltx_td ltx_align_center"><span id="S3.T1.2.p2.8.8.8.8.25.17.5.1" class="ltx_text ltx_font_bold">88.6</span></span>
<span id="S3.T1.2.p2.8.8.8.8.25.17.6" class="ltx_td ltx_align_center">60.1</span>
<span id="S3.T1.2.p2.8.8.8.8.25.17.7" class="ltx_td ltx_align_center"><span id="S3.T1.2.p2.8.8.8.8.25.17.7.1" class="ltx_text ltx_font_bold">39.3</span></span>
<span id="S3.T1.2.p2.8.8.8.8.25.17.8" class="ltx_td ltx_align_center">69.8</span>
<span id="S3.T1.2.p2.8.8.8.8.25.17.9" class="ltx_td ltx_align_center">63.8</span>
<span id="S3.T1.2.p2.8.8.8.8.25.17.10" class="ltx_td ltx_align_center">80.0</span>
<span id="S3.T1.2.p2.8.8.8.8.25.17.11" class="ltx_td ltx_align_center">74.1</span>
<span id="S3.T1.2.p2.8.8.8.8.25.17.12" class="ltx_td ltx_align_center">51.0</span>
<span id="S3.T1.2.p2.8.8.8.8.25.17.13" class="ltx_td ltx_align_center">89.2</span>
<span id="S3.T1.2.p2.8.8.8.8.25.17.14" class="ltx_td ltx_align_center">86.5</span></span>
<span id="S3.T1.2.p2.8.8.8.8.26.18" class="ltx_tr">
<span id="S3.T1.2.p2.8.8.8.8.26.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">DeepInteraction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite></span>
<span id="S3.T1.2.p2.8.8.8.8.26.18.2" class="ltx_td ltx_align_center">L+C</span>
<span id="S3.T1.2.p2.8.8.8.8.26.18.3" class="ltx_td ltx_align_center">70.8</span>
<span id="S3.T1.2.p2.8.8.8.8.26.18.4" class="ltx_td ltx_align_center">73.4</span>
<span id="S3.T1.2.p2.8.8.8.8.26.18.5" class="ltx_td ltx_align_center">87.9</span>
<span id="S3.T1.2.p2.8.8.8.8.26.18.6" class="ltx_td ltx_align_center">60.2</span>
<span id="S3.T1.2.p2.8.8.8.8.26.18.7" class="ltx_td ltx_align_center">37.5</span>
<span id="S3.T1.2.p2.8.8.8.8.26.18.8" class="ltx_td ltx_align_center">70.8</span>
<span id="S3.T1.2.p2.8.8.8.8.26.18.9" class="ltx_td ltx_align_center">63.8</span>
<span id="S3.T1.2.p2.8.8.8.8.26.18.10" class="ltx_td ltx_align_center"><span id="S3.T1.2.p2.8.8.8.8.26.18.10.1" class="ltx_text ltx_font_bold">80.4</span></span>
<span id="S3.T1.2.p2.8.8.8.8.26.18.11" class="ltx_td ltx_align_center">75.4</span>
<span id="S3.T1.2.p2.8.8.8.8.26.18.12" class="ltx_td ltx_align_center">54.5</span>
<span id="S3.T1.2.p2.8.8.8.8.26.18.13" class="ltx_td ltx_align_center"><span id="S3.T1.2.p2.8.8.8.8.26.18.13.1" class="ltx_text ltx_font_bold">91.7</span></span>
<span id="S3.T1.2.p2.8.8.8.8.26.18.14" class="ltx_td ltx_align_center"><span id="S3.T1.2.p2.8.8.8.8.26.18.14.1" class="ltx_text ltx_font_bold">87.2</span></span></span>
<span id="S3.T1.2.p2.8.8.8.8.27.19" class="ltx_tr">
<span id="S3.T1.2.p2.8.8.8.8.27.19.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S3.T1.2.p2.8.8.8.8.27.19.1.1" class="ltx_text ltx_font_bold">FocalFormer3D</span></span>
<span id="S3.T1.2.p2.8.8.8.8.27.19.2" class="ltx_td ltx_align_center">L+C</span>
<span id="S3.T1.2.p2.8.8.8.8.27.19.3" class="ltx_td ltx_align_center"><span id="S3.T1.2.p2.8.8.8.8.27.19.3.1" class="ltx_text ltx_font_bold">71.6</span></span>
<span id="S3.T1.2.p2.8.8.8.8.27.19.4" class="ltx_td ltx_align_center"><span id="S3.T1.2.p2.8.8.8.8.27.19.4.1" class="ltx_text ltx_font_bold">73.9</span></span>
<span id="S3.T1.2.p2.8.8.8.8.27.19.5" class="ltx_td ltx_align_center">88.5</span>
<span id="S3.T1.2.p2.8.8.8.8.27.19.6" class="ltx_td ltx_align_center"><span id="S3.T1.2.p2.8.8.8.8.27.19.6.1" class="ltx_text ltx_font_bold">61.4</span></span>
<span id="S3.T1.2.p2.8.8.8.8.27.19.7" class="ltx_td ltx_align_center">35.9</span>
<span id="S3.T1.2.p2.8.8.8.8.27.19.8" class="ltx_td ltx_align_center"><span id="S3.T1.2.p2.8.8.8.8.27.19.8.1" class="ltx_text ltx_font_bold">71.7</span></span>
<span id="S3.T1.2.p2.8.8.8.8.27.19.9" class="ltx_td ltx_align_center"><span id="S3.T1.2.p2.8.8.8.8.27.19.9.1" class="ltx_text ltx_font_bold">66.4</span></span>
<span id="S3.T1.2.p2.8.8.8.8.27.19.10" class="ltx_td ltx_align_center">79.3</span>
<span id="S3.T1.2.p2.8.8.8.8.27.19.11" class="ltx_td ltx_align_center"><span id="S3.T1.2.p2.8.8.8.8.27.19.11.1" class="ltx_text ltx_font_bold">80.3</span></span>
<span id="S3.T1.2.p2.8.8.8.8.27.19.12" class="ltx_td ltx_align_center"><span id="S3.T1.2.p2.8.8.8.8.27.19.12.1" class="ltx_text ltx_font_bold">57.1</span></span>
<span id="S3.T1.2.p2.8.8.8.8.27.19.13" class="ltx_td ltx_align_center">89.7</span>
<span id="S3.T1.2.p2.8.8.8.8.27.19.14" class="ltx_td ltx_align_center">85.3</span></span>
<span id="S3.T1.2.p2.5.5.5.5.5" class="ltx_tr">
<span id="S3.T1.2.p2.5.5.5.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">PointAugmenting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> <sup id="S3.T1.2.p2.5.5.5.5.5.1.1" class="ltx_sup"><span id="S3.T1.2.p2.5.5.5.5.5.1.1.1" class="ltx_text ltx_font_italic">â€ </span></sup></span>
<span id="S3.T1.2.p2.5.5.5.5.5.2" class="ltx_td ltx_align_center ltx_border_t">L+C</span>
<span id="S3.T1.2.p2.5.5.5.5.5.3" class="ltx_td ltx_align_center ltx_border_t">66.8</span>
<span id="S3.T1.2.p2.5.5.5.5.5.4" class="ltx_td ltx_align_center ltx_border_t">71.0</span>
<span id="S3.T1.2.p2.5.5.5.5.5.5" class="ltx_td ltx_align_center ltx_border_t">87.5</span>
<span id="S3.T1.2.p2.5.5.5.5.5.6" class="ltx_td ltx_align_center ltx_border_t">57.3</span>
<span id="S3.T1.2.p2.5.5.5.5.5.7" class="ltx_td ltx_align_center ltx_border_t">28.0</span>
<span id="S3.T1.2.p2.5.5.5.5.5.8" class="ltx_td ltx_align_center ltx_border_t">65.2</span>
<span id="S3.T1.2.p2.5.5.5.5.5.9" class="ltx_td ltx_align_center ltx_border_t">60.7</span>
<span id="S3.T1.2.p2.5.5.5.5.5.10" class="ltx_td ltx_align_center ltx_border_t">72.6</span>
<span id="S3.T1.2.p2.5.5.5.5.5.11" class="ltx_td ltx_align_center ltx_border_t">74.3</span>
<span id="S3.T1.2.p2.5.5.5.5.5.12" class="ltx_td ltx_align_center ltx_border_t">50.9</span>
<span id="S3.T1.2.p2.5.5.5.5.5.13" class="ltx_td ltx_align_center ltx_border_t">87.9</span>
<span id="S3.T1.2.p2.5.5.5.5.5.14" class="ltx_td ltx_align_center ltx_border_t">83.6</span></span>
<span id="S3.T1.2.p2.6.6.6.6.6" class="ltx_tr">
<span id="S3.T1.2.p2.6.6.6.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Focals Conv-F <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> <sup id="S3.T1.2.p2.6.6.6.6.6.1.1" class="ltx_sup"><span id="S3.T1.2.p2.6.6.6.6.6.1.1.1" class="ltx_text ltx_font_italic">â€¡</span></sup></span>
<span id="S3.T1.2.p2.6.6.6.6.6.2" class="ltx_td ltx_align_center">L+C</span>
<span id="S3.T1.2.p2.6.6.6.6.6.3" class="ltx_td ltx_align_center">70.1</span>
<span id="S3.T1.2.p2.6.6.6.6.6.4" class="ltx_td ltx_align_center">73.6</span>
<span id="S3.T1.2.p2.6.6.6.6.6.5" class="ltx_td ltx_align_center">87.5</span>
<span id="S3.T1.2.p2.6.6.6.6.6.6" class="ltx_td ltx_align_center">60.0</span>
<span id="S3.T1.2.p2.6.6.6.6.6.7" class="ltx_td ltx_align_center">32.6</span>
<span id="S3.T1.2.p2.6.6.6.6.6.8" class="ltx_td ltx_align_center">69.9</span>
<span id="S3.T1.2.p2.6.6.6.6.6.9" class="ltx_td ltx_align_center">64.0</span>
<span id="S3.T1.2.p2.6.6.6.6.6.10" class="ltx_td ltx_align_center">71.8</span>
<span id="S3.T1.2.p2.6.6.6.6.6.11" class="ltx_td ltx_align_center">81.1</span>
<span id="S3.T1.2.p2.6.6.6.6.6.12" class="ltx_td ltx_align_center">59.2</span>
<span id="S3.T1.2.p2.6.6.6.6.6.13" class="ltx_td ltx_align_center">89.0</span>
<span id="S3.T1.2.p2.6.6.6.6.6.14" class="ltx_td ltx_align_center">85.5</span></span>
<span id="S3.T1.2.p2.7.7.7.7.7" class="ltx_tr">
<span id="S3.T1.2.p2.7.7.7.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">LargeKernel3D-F <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> <sup id="S3.T1.2.p2.7.7.7.7.7.1.1" class="ltx_sup"><span id="S3.T1.2.p2.7.7.7.7.7.1.1.1" class="ltx_text ltx_font_italic">â€¡</span></sup></span>
<span id="S3.T1.2.p2.7.7.7.7.7.2" class="ltx_td ltx_align_center">L+C</span>
<span id="S3.T1.2.p2.7.7.7.7.7.3" class="ltx_td ltx_align_center">71.1</span>
<span id="S3.T1.2.p2.7.7.7.7.7.4" class="ltx_td ltx_align_center">74.2</span>
<span id="S3.T1.2.p2.7.7.7.7.7.5" class="ltx_td ltx_align_center">88.1</span>
<span id="S3.T1.2.p2.7.7.7.7.7.6" class="ltx_td ltx_align_center">60.3</span>
<span id="S3.T1.2.p2.7.7.7.7.7.7" class="ltx_td ltx_align_center">34.3</span>
<span id="S3.T1.2.p2.7.7.7.7.7.8" class="ltx_td ltx_align_center">69.1</span>
<span id="S3.T1.2.p2.7.7.7.7.7.9" class="ltx_td ltx_align_center">66.5</span>
<span id="S3.T1.2.p2.7.7.7.7.7.10" class="ltx_td ltx_align_center">75.5</span>
<span id="S3.T1.2.p2.7.7.7.7.7.11" class="ltx_td ltx_align_center">82.0</span>
<span id="S3.T1.2.p2.7.7.7.7.7.12" class="ltx_td ltx_align_center"><span id="S3.T1.2.p2.7.7.7.7.7.12.1" class="ltx_text ltx_font_bold">60.3</span></span>
<span id="S3.T1.2.p2.7.7.7.7.7.13" class="ltx_td ltx_align_center">89.6</span>
<span id="S3.T1.2.p2.7.7.7.7.7.14" class="ltx_td ltx_align_center">85.7</span></span>
<span id="S3.T1.2.p2.8.8.8.8.8" class="ltx_tr">
<span id="S3.T1.2.p2.8.8.8.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S3.T1.2.p2.8.8.8.8.8.1.1" class="ltx_text ltx_font_bold">FocalFormer3D-F</span> <sup id="S3.T1.2.p2.8.8.8.8.8.1.2" class="ltx_sup"><span id="S3.T1.2.p2.8.8.8.8.8.1.2.1" class="ltx_text ltx_font_italic">â€ </span></sup></span>
<span id="S3.T1.2.p2.8.8.8.8.8.2" class="ltx_td ltx_align_center ltx_border_bb">L+C</span>
<span id="S3.T1.2.p2.8.8.8.8.8.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.2.p2.8.8.8.8.8.3.1" class="ltx_text ltx_font_bold">72.9</span></span>
<span id="S3.T1.2.p2.8.8.8.8.8.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.2.p2.8.8.8.8.8.4.1" class="ltx_text ltx_font_bold">75.0</span></span>
<span id="S3.T1.2.p2.8.8.8.8.8.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.2.p2.8.8.8.8.8.5.1" class="ltx_text ltx_font_bold">88.8</span></span>
<span id="S3.T1.2.p2.8.8.8.8.8.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.2.p2.8.8.8.8.8.6.1" class="ltx_text ltx_font_bold">63.5</span></span>
<span id="S3.T1.2.p2.8.8.8.8.8.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.2.p2.8.8.8.8.8.7.1" class="ltx_text ltx_font_bold">39.0</span></span>
<span id="S3.T1.2.p2.8.8.8.8.8.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.2.p2.8.8.8.8.8.8.1" class="ltx_text ltx_font_bold">73.7</span></span>
<span id="S3.T1.2.p2.8.8.8.8.8.9" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.2.p2.8.8.8.8.8.9.1" class="ltx_text ltx_font_bold">66.9</span></span>
<span id="S3.T1.2.p2.8.8.8.8.8.10" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.2.p2.8.8.8.8.8.10.1" class="ltx_text ltx_font_bold">79.2</span></span>
<span id="S3.T1.2.p2.8.8.8.8.8.11" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.2.p2.8.8.8.8.8.11.1" class="ltx_text ltx_font_bold">81.0</span></span>
<span id="S3.T1.2.p2.8.8.8.8.8.12" class="ltx_td ltx_align_center ltx_border_bb">58.1</span>
<span id="S3.T1.2.p2.8.8.8.8.8.13" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.2.p2.8.8.8.8.8.13.1" class="ltx_text ltx_font_bold">91.1</span></span>
<span id="S3.T1.2.p2.8.8.8.8.8.14" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.2.p2.8.8.8.8.8.14.1" class="ltx_text ltx_font_bold">87.1</span></span></span>
</span>
</span></span></p>
</span></div>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span><span id="S3.T1.8.1" class="ltx_text ltx_font_bold">Performance comparison on the nuScenes 3D detection <span id="S3.T1.8.1.1" class="ltx_text ltx_font_italic">test</span> set.</span> <math id="S3.T1.5.m1.1" class="ltx_Math" alttext="{\dagger}" display="inline"><semantics id="S3.T1.5.m1.1b"><mo id="S3.T1.5.m1.1.1" xref="S3.T1.5.m1.1.1.cmml">â€ </mo><annotation-xml encoding="MathML-Content" id="S3.T1.5.m1.1c"><ci id="S3.T1.5.m1.1.1.cmml" xref="S3.T1.5.m1.1.1">â€ </ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.m1.1d">{\dagger}</annotation></semantics></math> represents using flipping test-time augmentation. <math id="S3.T1.6.m2.1" class="ltx_Math" alttext="{\ddagger}" display="inline"><semantics id="S3.T1.6.m2.1b"><mo id="S3.T1.6.m2.1.1" xref="S3.T1.6.m2.1.1.cmml">â€¡</mo><annotation-xml encoding="MathML-Content" id="S3.T1.6.m2.1c"><ci id="S3.T1.6.m2.1.1.cmml" xref="S3.T1.6.m2.1.1">â€¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.m2.1d">{\ddagger}</annotation></semantics></math> means using both flipping and rotation test-time augmentation. C.V, Motor., Ped. and T.C. are short for construction vehicle, motorcycle, pedestrian, and traffic cones, respectively.
</figcaption>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Model Training</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">The model is trained in two stages. In the first stage, we train the LiDAR backbone using a deformable transformer decoder head, which we refer to as DeformFormer3D (TableÂ <a href="#S4.T4" title="Table 4 â€£ 4.4 Ablation Study â€£ 4 Experiments â€£ FocalFormer3D : Focusing on Hard Instance for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> (a)).
After initializing the weights from DeformFormer3D, we train the FocalFormer3D detector, which consists of a multi-stage heatmap encoder and a box-level deformable decoder. However, during the training of the deformable decoder with bipartite graph matching, we encounter slow convergence issues in the early stagesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. To address this, we generate noisy queries from ground-truth objectsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>, enabling effective training of the model from scratch. Additionally, we improve the training process by excluding matching pairs with a center distance between the prediction and its GT object exceeding 7 meters.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Setup</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">Dataset and metric.</span> We evaluate our approach on <span id="S4.SS1.p1.1.2" class="ltx_text ltx_font_italic">nuScenes</span> and <span id="S4.SS1.p1.1.3" class="ltx_text ltx_font_italic">Waymo</span> 3D detection dataset.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.5" class="ltx_p"><span id="S4.SS1.p2.5.1" class="ltx_text ltx_font_italic">nuScenes Dataset</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite> is a large-scale outdoor dataset. nuScenes contains <math id="S4.SS1.p2.1.m1.2" class="ltx_Math" alttext="1,000" display="inline"><semantics id="S4.SS1.p2.1.m1.2a"><mrow id="S4.SS1.p2.1.m1.2.3.2" xref="S4.SS1.p2.1.m1.2.3.1.cmml"><mn id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">1</mn><mo id="S4.SS1.p2.1.m1.2.3.2.1" xref="S4.SS1.p2.1.m1.2.3.1.cmml">,</mo><mn id="S4.SS1.p2.1.m1.2.2" xref="S4.SS1.p2.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.2b"><list id="S4.SS1.p2.1.m1.2.3.1.cmml" xref="S4.SS1.p2.1.m1.2.3.2"><cn type="integer" id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">1</cn><cn type="integer" id="S4.SS1.p2.1.m1.2.2.cmml" xref="S4.SS1.p2.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.2c">1,000</annotation></semantics></math> scenes of multi-modal data, including <math id="S4.SS1.p2.2.m2.1" class="ltx_Math" alttext="32" display="inline"><semantics id="S4.SS1.p2.2.m2.1a"><mn id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><cn type="integer" id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">32</annotation></semantics></math>-beams LiDAR with <math id="S4.SS1.p2.3.m3.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S4.SS1.p2.3.m3.1a"><mn id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.1b"><cn type="integer" id="S4.SS1.p2.3.m3.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.1c">20</annotation></semantics></math>FPS and <math id="S4.SS1.p2.4.m4.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S4.SS1.p2.4.m4.1a"><mn id="S4.SS1.p2.4.m4.1.1" xref="S4.SS1.p2.4.m4.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.4.m4.1b"><cn type="integer" id="S4.SS1.p2.4.m4.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.4.m4.1c">6</annotation></semantics></math>-view camera images. We mainly evaluate our method on both <span id="S4.SS1.p2.5.2" class="ltx_text ltx_font_italic">LiDAR-only</span> and <span id="S4.SS1.p2.5.3" class="ltx_text ltx_font_italic">LiDAR-Camera fusion</span> settings. The evaluation metrics follow nuScenes official metrics including mean average precision (mAP) and nuScenes detection score (NDS) defined by averaging the matching thresholds of center distance <math id="S4.SS1.p2.5.m5.2" class="ltx_math_unparsed" alttext="\mathbb{D}=\{0.5,1.,2.,4.\}" display="inline"><semantics id="S4.SS1.p2.5.m5.2a"><mrow id="S4.SS1.p2.5.m5.2b"><mi id="S4.SS1.p2.5.m5.2.3">ğ”»</mi><mo id="S4.SS1.p2.5.m5.2.4">=</mo><mrow id="S4.SS1.p2.5.m5.2.5"><mo stretchy="false" id="S4.SS1.p2.5.m5.2.5.1">{</mo><mn id="S4.SS1.p2.5.m5.1.1">0.5</mn><mo id="S4.SS1.p2.5.m5.2.5.2">,</mo><mn id="S4.SS1.p2.5.m5.2.2">1</mn><mo lspace="0em" rspace="0.167em" id="S4.SS1.p2.5.m5.2.5.3">.</mo><mo id="S4.SS1.p2.5.m5.2.5.4">,</mo><mn id="S4.SS1.p2.5.m5.2.5.5">2</mn><mo lspace="0em" rspace="0.167em" id="S4.SS1.p2.5.m5.2.5.6">.</mo><mo id="S4.SS1.p2.5.m5.2.5.7">,</mo><mn id="S4.SS1.p2.5.m5.2.5.8">4</mn><mo lspace="0em" rspace="0.167em" id="S4.SS1.p2.5.m5.2.5.9">.</mo><mo stretchy="false" id="S4.SS1.p2.5.m5.2.5.10">}</mo></mrow></mrow><annotation encoding="application/x-tex" id="S4.SS1.p2.5.m5.2c">\mathbb{D}=\{0.5,1.,2.,4.\}</annotation></semantics></math> (m). For evaluating the quality of object queries, we also introduce the Average Recall (AR) defined by center distance as well. The ablation studies in our research primarily utilize the nuScenes dataset, unless explicitly stated otherwise.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_italic">Waymo Open Dataset</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> has a wider detection range of <math id="S4.SS1.p3.1.m1.1" class="ltx_Math" alttext="150m\times 150m" display="inline"><semantics id="S4.SS1.p3.1.m1.1a"><mrow id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml"><mrow id="S4.SS1.p3.1.m1.1.1.2" xref="S4.SS1.p3.1.m1.1.1.2.cmml"><mrow id="S4.SS1.p3.1.m1.1.1.2.2" xref="S4.SS1.p3.1.m1.1.1.2.2.cmml"><mn id="S4.SS1.p3.1.m1.1.1.2.2.2" xref="S4.SS1.p3.1.m1.1.1.2.2.2.cmml">150</mn><mo lspace="0em" rspace="0em" id="S4.SS1.p3.1.m1.1.1.2.2.1" xref="S4.SS1.p3.1.m1.1.1.2.2.1.cmml">â€‹</mo><mi id="S4.SS1.p3.1.m1.1.1.2.2.3" xref="S4.SS1.p3.1.m1.1.1.2.2.3.cmml">m</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p3.1.m1.1.1.2.1" xref="S4.SS1.p3.1.m1.1.1.2.1.cmml">Ã—</mo><mn id="S4.SS1.p3.1.m1.1.1.2.3" xref="S4.SS1.p3.1.m1.1.1.2.3.cmml">150</mn></mrow><mo lspace="0em" rspace="0em" id="S4.SS1.p3.1.m1.1.1.1" xref="S4.SS1.p3.1.m1.1.1.1.cmml">â€‹</mo><mi id="S4.SS1.p3.1.m1.1.1.3" xref="S4.SS1.p3.1.m1.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><apply id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1"><times id="S4.SS1.p3.1.m1.1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1.1"></times><apply id="S4.SS1.p3.1.m1.1.1.2.cmml" xref="S4.SS1.p3.1.m1.1.1.2"><times id="S4.SS1.p3.1.m1.1.1.2.1.cmml" xref="S4.SS1.p3.1.m1.1.1.2.1"></times><apply id="S4.SS1.p3.1.m1.1.1.2.2.cmml" xref="S4.SS1.p3.1.m1.1.1.2.2"><times id="S4.SS1.p3.1.m1.1.1.2.2.1.cmml" xref="S4.SS1.p3.1.m1.1.1.2.2.1"></times><cn type="integer" id="S4.SS1.p3.1.m1.1.1.2.2.2.cmml" xref="S4.SS1.p3.1.m1.1.1.2.2.2">150</cn><ci id="S4.SS1.p3.1.m1.1.1.2.2.3.cmml" xref="S4.SS1.p3.1.m1.1.1.2.2.3">ğ‘š</ci></apply><cn type="integer" id="S4.SS1.p3.1.m1.1.1.2.3.cmml" xref="S4.SS1.p3.1.m1.1.1.2.3">150</cn></apply><ci id="S4.SS1.p3.1.m1.1.1.3.cmml" xref="S4.SS1.p3.1.m1.1.1.3">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">150m\times 150m</annotation></semantics></math> compared to the nuScenes dataset. Waymo dataset comprises of 798 scenes for training and 202 scenes for validation. The official evaluation metrics used are mean Average Precision (mAP) and mean Average Precision with Heading (mAPH), where the mAP is weighted by the heading accuracy. The mAP and mAPH scores are computed with a 3D Intersection over Union (IoU) threshold of 0.7 for <span id="S4.SS1.p3.1.2" class="ltx_text ltx_font_italic">Vehicle</span> and 0.5 for <span id="S4.SS1.p3.1.3" class="ltx_text ltx_font_italic">Pedestrian</span> and <span id="S4.SS1.p3.1.4" class="ltx_text ltx_font_italic">Cyclist</span>. The evaluation has two difficulty levels: Level 1, for boxes with more than five LiDAR points, and Level 2, for boxes with at least one LiDAR point. Of the two difficulty levels, Level 2 is prioritized as the primary evaluation metric for all experiments.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para ltx_noindent">
<p id="S4.SS1.p4.7" class="ltx_p"><span id="S4.SS1.p4.7.1" class="ltx_text ltx_font_bold">Implementation details.</span>
Our implementation is mainly based on the open-sourced codebase MMDetection3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite>. For the LiDAR backbone, we use CenterPoint-Voxel as the point cloud feature extractor. For the multi-stage heatmap encoder, we apply 3 stages, generating a total of 600 queries by default. Data augmentation includes random double flipping along both <math id="S4.SS1.p4.1.m1.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S4.SS1.p4.1.m1.1a"><mi id="S4.SS1.p4.1.m1.1.1" xref="S4.SS1.p4.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.1.m1.1b"><ci id="S4.SS1.p4.1.m1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1">ğ‘‹</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.1c">X</annotation></semantics></math> and <math id="S4.SS1.p4.2.m2.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S4.SS1.p4.2.m2.1a"><mi id="S4.SS1.p4.2.m2.1.1" xref="S4.SS1.p4.2.m2.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.2.m2.1b"><ci id="S4.SS1.p4.2.m2.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1">ğ‘Œ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.2.m2.1c">Y</annotation></semantics></math> axes, random global rotation between [<math id="S4.SS1.p4.3.m3.1" class="ltx_Math" alttext="-\pi/4" display="inline"><semantics id="S4.SS1.p4.3.m3.1a"><mrow id="S4.SS1.p4.3.m3.1.1" xref="S4.SS1.p4.3.m3.1.1.cmml"><mo id="S4.SS1.p4.3.m3.1.1a" xref="S4.SS1.p4.3.m3.1.1.cmml">âˆ’</mo><mrow id="S4.SS1.p4.3.m3.1.1.2" xref="S4.SS1.p4.3.m3.1.1.2.cmml"><mi id="S4.SS1.p4.3.m3.1.1.2.2" xref="S4.SS1.p4.3.m3.1.1.2.2.cmml">Ï€</mi><mo id="S4.SS1.p4.3.m3.1.1.2.1" xref="S4.SS1.p4.3.m3.1.1.2.1.cmml">/</mo><mn id="S4.SS1.p4.3.m3.1.1.2.3" xref="S4.SS1.p4.3.m3.1.1.2.3.cmml">4</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.3.m3.1b"><apply id="S4.SS1.p4.3.m3.1.1.cmml" xref="S4.SS1.p4.3.m3.1.1"><minus id="S4.SS1.p4.3.m3.1.1.1.cmml" xref="S4.SS1.p4.3.m3.1.1"></minus><apply id="S4.SS1.p4.3.m3.1.1.2.cmml" xref="S4.SS1.p4.3.m3.1.1.2"><divide id="S4.SS1.p4.3.m3.1.1.2.1.cmml" xref="S4.SS1.p4.3.m3.1.1.2.1"></divide><ci id="S4.SS1.p4.3.m3.1.1.2.2.cmml" xref="S4.SS1.p4.3.m3.1.1.2.2">ğœ‹</ci><cn type="integer" id="S4.SS1.p4.3.m3.1.1.2.3.cmml" xref="S4.SS1.p4.3.m3.1.1.2.3">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.3.m3.1c">-\pi/4</annotation></semantics></math>, <math id="S4.SS1.p4.4.m4.1" class="ltx_Math" alttext="\pi/4" display="inline"><semantics id="S4.SS1.p4.4.m4.1a"><mrow id="S4.SS1.p4.4.m4.1.1" xref="S4.SS1.p4.4.m4.1.1.cmml"><mi id="S4.SS1.p4.4.m4.1.1.2" xref="S4.SS1.p4.4.m4.1.1.2.cmml">Ï€</mi><mo id="S4.SS1.p4.4.m4.1.1.1" xref="S4.SS1.p4.4.m4.1.1.1.cmml">/</mo><mn id="S4.SS1.p4.4.m4.1.1.3" xref="S4.SS1.p4.4.m4.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.4.m4.1b"><apply id="S4.SS1.p4.4.m4.1.1.cmml" xref="S4.SS1.p4.4.m4.1.1"><divide id="S4.SS1.p4.4.m4.1.1.1.cmml" xref="S4.SS1.p4.4.m4.1.1.1"></divide><ci id="S4.SS1.p4.4.m4.1.1.2.cmml" xref="S4.SS1.p4.4.m4.1.1.2">ğœ‹</ci><cn type="integer" id="S4.SS1.p4.4.m4.1.1.3.cmml" xref="S4.SS1.p4.4.m4.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.4.m4.1c">\pi/4</annotation></semantics></math>], the random scale of [<math id="S4.SS1.p4.5.m5.1" class="ltx_Math" alttext="0.9" display="inline"><semantics id="S4.SS1.p4.5.m5.1a"><mn id="S4.SS1.p4.5.m5.1.1" xref="S4.SS1.p4.5.m5.1.1.cmml">0.9</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.5.m5.1b"><cn type="float" id="S4.SS1.p4.5.m5.1.1.cmml" xref="S4.SS1.p4.5.m5.1.1">0.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.5.m5.1c">0.9</annotation></semantics></math>, <math id="S4.SS1.p4.6.m6.1" class="ltx_Math" alttext="1.1" display="inline"><semantics id="S4.SS1.p4.6.m6.1a"><mn id="S4.SS1.p4.6.m6.1.1" xref="S4.SS1.p4.6.m6.1.1.cmml">1.1</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.6.m6.1b"><cn type="float" id="S4.SS1.p4.6.m6.1.1.cmml" xref="S4.SS1.p4.6.m6.1.1">1.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.6.m6.1c">1.1</annotation></semantics></math>], and random translation with a standard deviation of <math id="S4.SS1.p4.7.m7.1" class="ltx_Math" alttext="0.5" display="inline"><semantics id="S4.SS1.p4.7.m7.1a"><mn id="S4.SS1.p4.7.m7.1.1" xref="S4.SS1.p4.7.m7.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.7.m7.1b"><cn type="float" id="S4.SS1.p4.7.m7.1.1.cmml" xref="S4.SS1.p4.7.m7.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.7.m7.1c">0.5</annotation></semantics></math> in all axes. All models are trained with a batch size of 16 on eight V100 GPUs. More implementation details are referred to in supplementary files.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Main Results</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.6" class="ltx_p"><span id="S4.SS2.p1.6.1" class="ltx_text ltx_font_bold">nuScenes LiDAR-based 3D object detection.</span>
We evaluate the performance of FocalFormer3D on the nuScenes <span id="S4.SS2.p1.6.2" class="ltx_text ltx_font_italic">test</span> set. As shown in Table <a href="#S3.T1" title="Table 1 â€£ 3.3 Box-level Deformable Decoder â€£ 3 Methodology â€£ FocalFormer3D : Focusing on Hard Instance for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the results demonstrate its superiority over state-of-the-art methods on various evaluation metrics and settings. Our single-model FocalFormer3D achieved <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="68.7" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mn id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">68.7</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><cn type="float" id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">68.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">68.7</annotation></semantics></math> mAP and <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="72.6" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mn id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml">72.6</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><cn type="float" id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">72.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">72.6</annotation></semantics></math> NDS, which surpasses the prior TransFusion-L method by <math id="S4.SS2.p1.3.m3.1" class="ltx_Math" alttext="+3.2" display="inline"><semantics id="S4.SS2.p1.3.m3.1a"><mrow id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml"><mo id="S4.SS2.p1.3.m3.1.1a" xref="S4.SS2.p1.3.m3.1.1.cmml">+</mo><mn id="S4.SS2.p1.3.m3.1.1.2" xref="S4.SS2.p1.3.m3.1.1.2.cmml">3.2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><apply id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1"><plus id="S4.SS2.p1.3.m3.1.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1"></plus><cn type="float" id="S4.SS2.p1.3.m3.1.1.2.cmml" xref="S4.SS2.p1.3.m3.1.1.2">3.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">+3.2</annotation></semantics></math> points on mAP and <math id="S4.SS2.p1.4.m4.1" class="ltx_Math" alttext="+2.4" display="inline"><semantics id="S4.SS2.p1.4.m4.1a"><mrow id="S4.SS2.p1.4.m4.1.1" xref="S4.SS2.p1.4.m4.1.1.cmml"><mo id="S4.SS2.p1.4.m4.1.1a" xref="S4.SS2.p1.4.m4.1.1.cmml">+</mo><mn id="S4.SS2.p1.4.m4.1.1.2" xref="S4.SS2.p1.4.m4.1.1.2.cmml">2.4</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.4.m4.1b"><apply id="S4.SS2.p1.4.m4.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1"><plus id="S4.SS2.p1.4.m4.1.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1"></plus><cn type="float" id="S4.SS2.p1.4.m4.1.1.2.cmml" xref="S4.SS2.p1.4.m4.1.1.2">2.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.4.m4.1c">+2.4</annotation></semantics></math> points on NDS. Notably, even compared with the previous best method that was trained with segmentation-level labels, our method without extra supervision still outperformed LiDARMultiNet by <math id="S4.SS2.p1.5.m5.1" class="ltx_Math" alttext="+1.7" display="inline"><semantics id="S4.SS2.p1.5.m5.1a"><mrow id="S4.SS2.p1.5.m5.1.1" xref="S4.SS2.p1.5.m5.1.1.cmml"><mo id="S4.SS2.p1.5.m5.1.1a" xref="S4.SS2.p1.5.m5.1.1.cmml">+</mo><mn id="S4.SS2.p1.5.m5.1.1.2" xref="S4.SS2.p1.5.m5.1.1.2.cmml">1.7</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.5.m5.1b"><apply id="S4.SS2.p1.5.m5.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1"><plus id="S4.SS2.p1.5.m5.1.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1"></plus><cn type="float" id="S4.SS2.p1.5.m5.1.1.2.cmml" xref="S4.SS2.p1.5.m5.1.1.2">1.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.5.m5.1c">+1.7</annotation></semantics></math> mAP and <math id="S4.SS2.p1.6.m6.1" class="ltx_Math" alttext="+1.0" display="inline"><semantics id="S4.SS2.p1.6.m6.1a"><mrow id="S4.SS2.p1.6.m6.1.1" xref="S4.SS2.p1.6.m6.1.1.cmml"><mo id="S4.SS2.p1.6.m6.1.1a" xref="S4.SS2.p1.6.m6.1.1.cmml">+</mo><mn id="S4.SS2.p1.6.m6.1.1.2" xref="S4.SS2.p1.6.m6.1.1.2.cmml">1.0</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.6.m6.1b"><apply id="S4.SS2.p1.6.m6.1.1.cmml" xref="S4.SS2.p1.6.m6.1.1"><plus id="S4.SS2.p1.6.m6.1.1.1.cmml" xref="S4.SS2.p1.6.m6.1.1"></plus><cn type="float" id="S4.SS2.p1.6.m6.1.1.2.cmml" xref="S4.SS2.p1.6.m6.1.1.2">1.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.6.m6.1c">+1.0</annotation></semantics></math> NDS.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.1" class="ltx_p"><span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_bold">nuScenes multi-modal 3D object detection.</span> We extend our approach to a simple multi-modal variant and demonstrate its generality. Following TransFusionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, we use a pre-trained <span id="S4.SS2.p2.1.2" class="ltx_text ltx_font_italic">ResNet-50</span> model on COCOÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite> and nuImageÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite> dataset as the image model and freeze its weights during training. To reduce computation costs, the input images are downscaled to 1/2 of their original size. Unlike heavy lift-splat-shotÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> camera encoders used in BEVFusionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, the multi-view camera images are projected onto a pre-defined voxel space and fused with LiDAR BEV feature. Additional details are available in the supplementary files. Without test-time augmentation, our simple multi-modal variant model outperforms all other state-of-the-art with less inference time (TableÂ 2). With TTA, FocalFormer3D achieves 72.9 mAP and 75.0 NDS, ranking first among all single-model solutions on the nuScenes benchmark. Interestingly, our model achieves high results for some rare classes such as (<span id="S4.SS2.p2.1.3" class="ltx_text ltx_font_slanted">Trailer</span>, <span id="S4.SS2.p2.1.4" class="ltx_text ltx_font_slanted">Motorcycle</span>, <span id="S4.SS2.p2.1.5" class="ltx_text ltx_font_slanted">Bicycle</span>) compared to other methods.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.p3.1" class="ltx_p"><span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_bold">nuScenes 3D object tracking.</span>
To further demonstrate the versatility, we also extend FocalFormer3D to 3D multi-object tracking (MOT) by using the tracking-by-detection algorithm SimpleTrack. Interested readers can refer to the original paperÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite> for more comprehensive details. As depicted in TableÂ <a href="#S4.T2" title="Table 2 â€£ 4.2 Main Results â€£ 4 Experiments â€£ FocalFormer3D : Focusing on Hard Instance for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, FocalFormer3D gets 2.9 points better than prior state-of-the-art TransFusion-LÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> in LiDAR settings and FocalFormer3D-F achieves 2.1 points over TransFusion in terms of AMOTA. Moreover, our single model FocalFormer3D-F with double-flip testing results performs even better than the BEVFusionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> with model ensembling.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<div id="S4.T2.3.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:367.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(36.4pt,-30.9pt) scale(1.20181999411222,1.20181999411222) ;">
<table id="S4.T2.3.3.3" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.3.3.3.4.1" class="ltx_tr">
<td id="S4.T2.3.3.3.4.1.1" class="ltx_td ltx_align_left ltx_border_tt">Methods</td>
<td id="S4.T2.3.3.3.4.1.2" class="ltx_td ltx_align_center ltx_border_tt">AMOTA</td>
<td id="S4.T2.3.3.3.4.1.3" class="ltx_td ltx_align_center ltx_border_tt">AMOTP</td>
<td id="S4.T2.3.3.3.4.1.4" class="ltx_td ltx_align_center ltx_border_tt">MOTA</td>
<td id="S4.T2.3.3.3.4.1.5" class="ltx_td ltx_align_center ltx_border_tt">IDS</td>
</tr>
<tr id="S4.T2.3.3.3.5.2" class="ltx_tr">
<td id="S4.T2.3.3.3.5.2.1" class="ltx_td ltx_align_left ltx_border_tt" colspan="5"><span id="S4.T2.3.3.3.5.2.1.1" class="ltx_text ltx_font_italic">LiDAR-based 3D Tracking</span></td>
</tr>
<tr id="S4.T2.3.3.3.6.3" class="ltx_tr">
<td id="S4.T2.3.3.3.6.3.1" class="ltx_td ltx_align_left ltx_border_t">AB3DMOTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>
</td>
<td id="S4.T2.3.3.3.6.3.2" class="ltx_td ltx_align_center ltx_border_t">15.1</td>
<td id="S4.T2.3.3.3.6.3.3" class="ltx_td ltx_align_center ltx_border_t">150.1</td>
<td id="S4.T2.3.3.3.6.3.4" class="ltx_td ltx_align_center ltx_border_t">15.4</td>
<td id="S4.T2.3.3.3.6.3.5" class="ltx_td ltx_align_center ltx_border_t">9027</td>
</tr>
<tr id="S4.T2.3.3.3.7.4" class="ltx_tr">
<td id="S4.T2.3.3.3.7.4.1" class="ltx_td ltx_align_left">CenterPointÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>
</td>
<td id="S4.T2.3.3.3.7.4.2" class="ltx_td ltx_align_center">63.8</td>
<td id="S4.T2.3.3.3.7.4.3" class="ltx_td ltx_align_center">55.5</td>
<td id="S4.T2.3.3.3.7.4.4" class="ltx_td ltx_align_center">53.7</td>
<td id="S4.T2.3.3.3.7.4.5" class="ltx_td ltx_align_center">760</td>
</tr>
<tr id="S4.T2.3.3.3.8.5" class="ltx_tr">
<td id="S4.T2.3.3.3.8.5.1" class="ltx_td ltx_align_left">CBMOTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite>
</td>
<td id="S4.T2.3.3.3.8.5.2" class="ltx_td ltx_align_center">64.9</td>
<td id="S4.T2.3.3.3.8.5.3" class="ltx_td ltx_align_center">59.2</td>
<td id="S4.T2.3.3.3.8.5.4" class="ltx_td ltx_align_center">54.5</td>
<td id="S4.T2.3.3.3.8.5.5" class="ltx_td ltx_align_center">557</td>
</tr>
<tr id="S4.T2.3.3.3.9.6" class="ltx_tr">
<td id="S4.T2.3.3.3.9.6.1" class="ltx_td ltx_align_left">OGR3MOTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite>
</td>
<td id="S4.T2.3.3.3.9.6.2" class="ltx_td ltx_align_center">65.6</td>
<td id="S4.T2.3.3.3.9.6.3" class="ltx_td ltx_align_center">62.0</td>
<td id="S4.T2.3.3.3.9.6.4" class="ltx_td ltx_align_center">55.4</td>
<td id="S4.T2.3.3.3.9.6.5" class="ltx_td ltx_align_center"><span id="S4.T2.3.3.3.9.6.5.1" class="ltx_text ltx_font_bold">288</span></td>
</tr>
<tr id="S4.T2.3.3.3.10.7" class="ltx_tr">
<td id="S4.T2.3.3.3.10.7.1" class="ltx_td ltx_align_left">SimpleTrackÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>
</td>
<td id="S4.T2.3.3.3.10.7.2" class="ltx_td ltx_align_center">66.8</td>
<td id="S4.T2.3.3.3.10.7.3" class="ltx_td ltx_align_center">55.0</td>
<td id="S4.T2.3.3.3.10.7.4" class="ltx_td ltx_align_center">56.6</td>
<td id="S4.T2.3.3.3.10.7.5" class="ltx_td ltx_align_center">575</td>
</tr>
<tr id="S4.T2.3.3.3.11.8" class="ltx_tr">
<td id="S4.T2.3.3.3.11.8.1" class="ltx_td ltx_align_left">UVTR-LÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</td>
<td id="S4.T2.3.3.3.11.8.2" class="ltx_td ltx_align_center">67.0</td>
<td id="S4.T2.3.3.3.11.8.3" class="ltx_td ltx_align_center">55.0</td>
<td id="S4.T2.3.3.3.11.8.4" class="ltx_td ltx_align_center">56.6</td>
<td id="S4.T2.3.3.3.11.8.5" class="ltx_td ltx_align_center">774</td>
</tr>
<tr id="S4.T2.3.3.3.12.9" class="ltx_tr">
<td id="S4.T2.3.3.3.12.9.1" class="ltx_td ltx_align_left">TransFusion-LÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>
</td>
<td id="S4.T2.3.3.3.12.9.2" class="ltx_td ltx_align_center">68.6</td>
<td id="S4.T2.3.3.3.12.9.3" class="ltx_td ltx_align_center">52.9</td>
<td id="S4.T2.3.3.3.12.9.4" class="ltx_td ltx_align_center">57.1</td>
<td id="S4.T2.3.3.3.12.9.5" class="ltx_td ltx_align_center">893</td>
</tr>
<tr id="S4.T2.3.3.3.13.10" class="ltx_tr">
<td id="S4.T2.3.3.3.13.10.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T2.3.3.3.13.10.1.1" class="ltx_text ltx_font_bold">FocalFormer3D</span></td>
<td id="S4.T2.3.3.3.13.10.2" class="ltx_td ltx_align_center ltx_border_t">71.5</td>
<td id="S4.T2.3.3.3.13.10.3" class="ltx_td ltx_align_center ltx_border_t">54.9</td>
<td id="S4.T2.3.3.3.13.10.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.3.3.3.13.10.4.1" class="ltx_text ltx_font_bold">60.1</span></td>
<td id="S4.T2.3.3.3.13.10.5" class="ltx_td ltx_align_center ltx_border_t">888</td>
</tr>
<tr id="S4.T2.1.1.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1.1.1" class="ltx_td ltx_align_left"><span id="S4.T2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">FocalFormer3D<sup id="S4.T2.1.1.1.1.1.1.1" class="ltx_sup"><span id="S4.T2.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">â€ </span></sup></span></td>
<td id="S4.T2.1.1.1.1.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.1.1.2.1" class="ltx_text ltx_font_bold">72.1</span></td>
<td id="S4.T2.1.1.1.1.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.1.1.3.1" class="ltx_text ltx_font_bold">47.0</span></td>
<td id="S4.T2.1.1.1.1.4" class="ltx_td ltx_align_center">60.0</td>
<td id="S4.T2.1.1.1.1.5" class="ltx_td ltx_align_center">701</td>
</tr>
<tr id="S4.T2.3.3.3.14.11" class="ltx_tr">
<td id="S4.T2.3.3.3.14.11.1" class="ltx_td ltx_align_left ltx_border_tt" colspan="5"><span id="S4.T2.3.3.3.14.11.1.1" class="ltx_text ltx_font_italic">Multi-Modal 3D Tracking</span></td>
</tr>
<tr id="S4.T2.3.3.3.15.12" class="ltx_tr">
<td id="S4.T2.3.3.3.15.12.1" class="ltx_td ltx_align_left ltx_border_t">UVTR-MultiModalÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</td>
<td id="S4.T2.3.3.3.15.12.2" class="ltx_td ltx_align_center ltx_border_t">70.1</td>
<td id="S4.T2.3.3.3.15.12.3" class="ltx_td ltx_align_center ltx_border_t">68.6</td>
<td id="S4.T2.3.3.3.15.12.4" class="ltx_td ltx_align_center ltx_border_t">61.8</td>
<td id="S4.T2.3.3.3.15.12.5" class="ltx_td ltx_align_center ltx_border_t">941</td>
</tr>
<tr id="S4.T2.3.3.3.16.13" class="ltx_tr">
<td id="S4.T2.3.3.3.16.13.1" class="ltx_td ltx_align_left">TransFusionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>
</td>
<td id="S4.T2.3.3.3.16.13.2" class="ltx_td ltx_align_center">71.8</td>
<td id="S4.T2.3.3.3.16.13.3" class="ltx_td ltx_align_center">55.1</td>
<td id="S4.T2.3.3.3.16.13.4" class="ltx_td ltx_align_center"><span id="S4.T2.3.3.3.16.13.4.1" class="ltx_text ltx_font_bold">60.7</span></td>
<td id="S4.T2.3.3.3.16.13.5" class="ltx_td ltx_align_center">944</td>
</tr>
<tr id="S4.T2.2.2.2.2" class="ltx_tr">
<td id="S4.T2.2.2.2.2.1" class="ltx_td ltx_align_left">BEVFusion-MITÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite><sup id="S4.T2.2.2.2.2.1.1" class="ltx_sup"><span id="S4.T2.2.2.2.2.1.1.1" class="ltx_text ltx_font_italic">â€¡</span></sup>
</td>
<td id="S4.T2.2.2.2.2.2" class="ltx_td ltx_align_center">74.1</td>
<td id="S4.T2.2.2.2.2.3" class="ltx_td ltx_align_center">40.3</td>
<td id="S4.T2.2.2.2.2.4" class="ltx_td ltx_align_center">60.3</td>
<td id="S4.T2.2.2.2.2.5" class="ltx_td ltx_align_center">506</td>
</tr>
<tr id="S4.T2.3.3.3.17.14" class="ltx_tr">
<td id="S4.T2.3.3.3.17.14.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T2.3.3.3.17.14.1.1" class="ltx_text ltx_font_bold">FocalFormer3D-F</span></td>
<td id="S4.T2.3.3.3.17.14.2" class="ltx_td ltx_align_center ltx_border_t">73.9</td>
<td id="S4.T2.3.3.3.17.14.3" class="ltx_td ltx_align_center ltx_border_t">51.4</td>
<td id="S4.T2.3.3.3.17.14.4" class="ltx_td ltx_align_center ltx_border_t">61.8</td>
<td id="S4.T2.3.3.3.17.14.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.3.3.3.17.14.5.1" class="ltx_text ltx_font_bold">824</span></td>
</tr>
<tr id="S4.T2.3.3.3.3" class="ltx_tr">
<td id="S4.T2.3.3.3.3.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T2.3.3.3.3.1.1" class="ltx_text ltx_font_bold">FocalFormer3D-F<sup id="S4.T2.3.3.3.3.1.1.1" class="ltx_sup"><span id="S4.T2.3.3.3.3.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">â€ </span></sup></span></td>
<td id="S4.T2.3.3.3.3.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.3.3.3.3.2.1" class="ltx_text ltx_font_bold">74.6</span></td>
<td id="S4.T2.3.3.3.3.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.3.3.3.3.3.1" class="ltx_text ltx_font_bold">47.3</span></td>
<td id="S4.T2.3.3.3.3.4" class="ltx_td ltx_align_center ltx_border_bb">63.0</td>
<td id="S4.T2.3.3.3.3.5" class="ltx_td ltx_align_center ltx_border_bb">849</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span><span id="S4.T2.11.1" class="ltx_text ltx_font_bold">Performance comparison on nuScenes 3D tracking test set</span>. <sup id="S4.T2.12.2" class="ltx_sup"><span id="S4.T2.12.2.1" class="ltx_text ltx_font_italic">â€ </span></sup> is based on the double-flip testing results in TableÂ <a href="#S3.T1" title="Table 1 â€£ 3.3 Box-level Deformable Decoder â€£ 3 Methodology â€£ FocalFormer3D : Focusing on Hard Instance for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. <sup id="S4.T2.13.3" class="ltx_sup"><span id="S4.T2.13.3.1" class="ltx_text ltx_font_italic">â€¡</span></sup> is based on model ensembling. </figcaption>
</figure>
<div id="S4.SS2.p4" class="ltx_para ltx_noindent">
<p id="S4.SS2.p4.1" class="ltx_p"><span id="S4.SS2.p4.1.1" class="ltx_text ltx_font_bold">Waymo LiDAR 3D object detection.</span>
The results of our single-frame LiDAR 3D detection method on the Waymo dataset are presented in TableÂ <a href="#S4.T3" title="Table 3 â€£ 4.2 Main Results â€£ 4 Experiments â€£ FocalFormer3D : Focusing on Hard Instance for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, alongside the comparison with other approaches. Employing with the same VoxelNet backbone as nuScenes, our method achieves competitive performance without any fine-tuning of the model hyperparameters specifically for the Waymo dataset. Particularly, when compared to TransFusion-L with the same backbone, our method exhibits a +1.1 mAPH improvement.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<div id="S4.T3.8.8" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:277.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(48.0pt,-30.7pt) scale(1.28409211628446,1.28409211628446) ;">
<table id="S4.T3.8.8.8" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.8.8.8.9.1" class="ltx_tr">
<th id="S4.T3.8.8.8.9.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">Methods</th>
<td id="S4.T3.8.8.8.9.1.2" class="ltx_td ltx_align_center ltx_border_tt">mAP</td>
<td id="S4.T3.8.8.8.9.1.3" class="ltx_td ltx_align_center ltx_border_tt">mAPH</td>
<td id="S4.T3.8.8.8.9.1.4" class="ltx_td ltx_align_center ltx_border_tt">Vel.</td>
<td id="S4.T3.8.8.8.9.1.5" class="ltx_td ltx_align_center ltx_border_tt">Ped.</td>
<td id="S4.T3.8.8.8.9.1.6" class="ltx_td ltx_align_center ltx_border_tt">Cyc.</td>
</tr>
<tr id="S4.T3.8.8.8.10.2" class="ltx_tr">
<th id="S4.T3.8.8.8.10.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" colspan="5"><span id="S4.T3.8.8.8.10.2.1.1" class="ltx_text ltx_font_italic">LiDAR-based 3D Detection</span></th>
<td id="S4.T3.8.8.8.10.2.2" class="ltx_td ltx_border_tt"></td>
</tr>
<tr id="S4.T3.1.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">RSN<sup id="S4.T3.1.1.1.1.1.1" class="ltx_sup"><span id="S4.T3.1.1.1.1.1.1.1" class="ltx_text ltx_font_italic">â‹†</span></sup>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite>
</th>
<td id="S4.T3.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t">â€“</td>
<td id="S4.T3.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t">â€“</td>
<td id="S4.T3.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t">65.5</td>
<td id="S4.T3.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t">63.7</td>
<td id="S4.T3.1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_t">â€“</td>
</tr>
<tr id="S4.T3.2.2.2.2" class="ltx_tr">
<th id="S4.T3.2.2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">AFDetV2<sup id="S4.T3.2.2.2.2.1.1" class="ltx_sup"><span id="S4.T3.2.2.2.2.1.1.1" class="ltx_text ltx_font_italic">â‹†</span></sup>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite>
</th>
<td id="S4.T3.2.2.2.2.2" class="ltx_td ltx_align_center">71.0</td>
<td id="S4.T3.2.2.2.2.3" class="ltx_td ltx_align_center">68.8</td>
<td id="S4.T3.2.2.2.2.4" class="ltx_td ltx_align_center">69.2</td>
<td id="S4.T3.2.2.2.2.5" class="ltx_td ltx_align_center">67.0</td>
<td id="S4.T3.2.2.2.2.6" class="ltx_td ltx_align_center">70.1</td>
</tr>
<tr id="S4.T3.3.3.3.3" class="ltx_tr">
<th id="S4.T3.3.3.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">SST<sup id="S4.T3.3.3.3.3.1.1" class="ltx_sup"><span id="S4.T3.3.3.3.3.1.1.1" class="ltx_text ltx_font_italic">â‹†</span></sup>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>
</th>
<td id="S4.T3.3.3.3.3.2" class="ltx_td ltx_align_center">67.8</td>
<td id="S4.T3.3.3.3.3.3" class="ltx_td ltx_align_center">64.6</td>
<td id="S4.T3.3.3.3.3.4" class="ltx_td ltx_align_center">65.1</td>
<td id="S4.T3.3.3.3.3.5" class="ltx_td ltx_align_center">61.7</td>
<td id="S4.T3.3.3.3.3.6" class="ltx_td ltx_align_center">66.9</td>
</tr>
<tr id="S4.T3.4.4.4.4" class="ltx_tr">
<th id="S4.T3.4.4.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">PV-RCNN<sup id="S4.T3.4.4.4.4.1.1" class="ltx_sup"><span id="S4.T3.4.4.4.4.1.1.1" class="ltx_text ltx_font_italic">â‹†</span></sup>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>
</th>
<td id="S4.T3.4.4.4.4.2" class="ltx_td ltx_align_center">66.8</td>
<td id="S4.T3.4.4.4.4.3" class="ltx_td ltx_align_center">63.3</td>
<td id="S4.T3.4.4.4.4.4" class="ltx_td ltx_align_center">68.4</td>
<td id="S4.T3.4.4.4.4.5" class="ltx_td ltx_align_center">65.8</td>
<td id="S4.T3.4.4.4.4.6" class="ltx_td ltx_align_center">68.5</td>
</tr>
<tr id="S4.T3.5.5.5.5" class="ltx_tr">
<th id="S4.T3.5.5.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">PV-RCNN++<sup id="S4.T3.5.5.5.5.1.1" class="ltx_sup"><span id="S4.T3.5.5.5.5.1.1.1" class="ltx_text ltx_font_italic">â‹†</span></sup>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>
</th>
<td id="S4.T3.5.5.5.5.2" class="ltx_td ltx_align_center">71.7</td>
<td id="S4.T3.5.5.5.5.3" class="ltx_td ltx_align_center">69.5</td>
<td id="S4.T3.5.5.5.5.4" class="ltx_td ltx_align_center">70.2</td>
<td id="S4.T3.5.5.5.5.5" class="ltx_td ltx_align_center"><span id="S4.T3.5.5.5.5.5.1" class="ltx_text ltx_font_bold">68.0</span></td>
<td id="S4.T3.5.5.5.5.6" class="ltx_td ltx_align_center">70.2</td>
</tr>
<tr id="S4.T3.6.6.6.6" class="ltx_tr">
<th id="S4.T3.6.6.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">PillarNet-34<sup id="S4.T3.6.6.6.6.1.1" class="ltx_sup"><span id="S4.T3.6.6.6.6.1.1.1" class="ltx_text ltx_font_italic">â‹†</span></sup>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>
</th>
<td id="S4.T3.6.6.6.6.2" class="ltx_td ltx_align_center">71.0</td>
<td id="S4.T3.6.6.6.6.3" class="ltx_td ltx_align_center">68.8</td>
<td id="S4.T3.6.6.6.6.4" class="ltx_td ltx_align_center"><span id="S4.T3.6.6.6.6.4.1" class="ltx_text ltx_font_bold">70.5</span></td>
<td id="S4.T3.6.6.6.6.5" class="ltx_td ltx_align_center">66.2</td>
<td id="S4.T3.6.6.6.6.6" class="ltx_td ltx_align_center">68.7</td>
</tr>
<tr id="S4.T3.7.7.7.7" class="ltx_tr">
<th id="S4.T3.7.7.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">FSD-spconv<sup id="S4.T3.7.7.7.7.1.1" class="ltx_sup"><span id="S4.T3.7.7.7.7.1.1.1" class="ltx_text ltx_font_italic">â‹†</span></sup>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>
</th>
<td id="S4.T3.7.7.7.7.2" class="ltx_td ltx_align_center"><span id="S4.T3.7.7.7.7.2.1" class="ltx_text ltx_font_bold">71.9</span></td>
<td id="S4.T3.7.7.7.7.3" class="ltx_td ltx_align_center"><span id="S4.T3.7.7.7.7.3.1" class="ltx_text ltx_font_bold">69.7</span></td>
<td id="S4.T3.7.7.7.7.4" class="ltx_td ltx_align_center">68.5</td>
<td id="S4.T3.7.7.7.7.5" class="ltx_td ltx_align_center"><span id="S4.T3.7.7.7.7.5.1" class="ltx_text ltx_font_bold">68.0</span></td>
<td id="S4.T3.7.7.7.7.6" class="ltx_td ltx_align_center">72.5</td>
</tr>
<tr id="S4.T3.8.8.8.11.3" class="ltx_tr">
<th id="S4.T3.8.8.8.11.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">CenterPointÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>
</th>
<td id="S4.T3.8.8.8.11.3.2" class="ltx_td ltx_align_center ltx_border_t">69.8</td>
<td id="S4.T3.8.8.8.11.3.3" class="ltx_td ltx_align_center ltx_border_t">67.6</td>
<td id="S4.T3.8.8.8.11.3.4" class="ltx_td ltx_align_center ltx_border_t">73.4</td>
<td id="S4.T3.8.8.8.11.3.5" class="ltx_td ltx_align_center ltx_border_t">65.8</td>
<td id="S4.T3.8.8.8.11.3.6" class="ltx_td ltx_align_center ltx_border_t">68.5</td>
</tr>
<tr id="S4.T3.8.8.8.8" class="ltx_tr">
<th id="S4.T3.8.8.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">TransFusion-L<sup id="S4.T3.8.8.8.8.1.1" class="ltx_sup">âˆ§</sup>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>
</th>
<td id="S4.T3.8.8.8.8.2" class="ltx_td ltx_align_center">70.5</td>
<td id="S4.T3.8.8.8.8.3" class="ltx_td ltx_align_center">67.9</td>
<td id="S4.T3.8.8.8.8.4" class="ltx_td ltx_align_center">66.8</td>
<td id="S4.T3.8.8.8.8.5" class="ltx_td ltx_align_center">66.1</td>
<td id="S4.T3.8.8.8.8.6" class="ltx_td ltx_align_center">70.9</td>
</tr>
<tr id="S4.T3.8.8.8.12.4" class="ltx_tr">
<th id="S4.T3.8.8.8.12.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">FocalFormer3D</th>
<td id="S4.T3.8.8.8.12.4.2" class="ltx_td ltx_align_center ltx_border_bb">71.5</td>
<td id="S4.T3.8.8.8.12.4.3" class="ltx_td ltx_align_center ltx_border_bb">69.0</td>
<td id="S4.T3.8.8.8.12.4.4" class="ltx_td ltx_align_center ltx_border_bb">67.6</td>
<td id="S4.T3.8.8.8.12.4.5" class="ltx_td ltx_align_center ltx_border_bb">66.8</td>
<td id="S4.T3.8.8.8.12.4.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.8.8.8.12.4.6.1" class="ltx_text ltx_font_bold">72.6</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span><span id="S4.T3.16.1" class="ltx_text ltx_font_bold">Performance comparison on the Waymo <span id="S4.T3.16.1.1" class="ltx_text ltx_font_italic">val</span> set.</span> All models inputs single-frame point clouds. The methods marked with <sup id="S4.T3.17.2" class="ltx_sup">âˆ—</sup> indicate the utilization of different point cloud backbones in VoxelNet. The method marked with <sup id="S4.T3.18.3" class="ltx_sup">âˆ§</sup> indicates our reproduction. The evaluation metric used is the LEVEL 2 difficulty, and the results are reported on the full Waymo validation set.</figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Recall Analysis</h3>

<figure id="S4.F5" class="ltx_figure"><img src="/html/2308.04556/assets/x5.png" id="S4.F5.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="81" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Average recall comparisons between initial object predictions and final object prediction centers on the nuScenes <span id="S4.F5.3.1" class="ltx_text ltx_font_italic">val</span> set. The subfigures are shown over center distance thresholds (%) following nuScenes detection metrics. </figcaption>
</figure>
<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.4" class="ltx_p">To diagnose the performance improvements, we compare several recent methods in terms of AR for both stages â€“ initial BEV heatmap predictions and final box predictions in Fig.Â <a href="#S4.F5" title="Figure 5 â€£ 4.3 Recall Analysis â€£ 4 Experiments â€£ FocalFormer3D : Focusing on Hard Instance for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. The metric of AR is computed based on center distance following the nuScenes metrics and different distance thresholds (<em id="S4.SS3.p1.4.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S4.SS3.p1.4.2" class="ltx_text"></span>, <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="0.5m" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mrow id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><mn id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2.cmml">0.5</mn><mo lspace="0em" rspace="0em" id="S4.SS3.p1.1.m1.1.1.1" xref="S4.SS3.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S4.SS3.p1.1.m1.1.1.3" xref="S4.SS3.p1.1.m1.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><times id="S4.SS3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1.1"></times><cn type="float" id="S4.SS3.p1.1.m1.1.1.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2">0.5</cn><ci id="S4.SS3.p1.1.m1.1.1.3.cmml" xref="S4.SS3.p1.1.m1.1.1.3">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">0.5m</annotation></semantics></math>, <math id="S4.SS3.p1.2.m2.1" class="ltx_Math" alttext="1.0m" display="inline"><semantics id="S4.SS3.p1.2.m2.1a"><mrow id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml"><mn id="S4.SS3.p1.2.m2.1.1.2" xref="S4.SS3.p1.2.m2.1.1.2.cmml">1.0</mn><mo lspace="0em" rspace="0em" id="S4.SS3.p1.2.m2.1.1.1" xref="S4.SS3.p1.2.m2.1.1.1.cmml">â€‹</mo><mi id="S4.SS3.p1.2.m2.1.1.3" xref="S4.SS3.p1.2.m2.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><apply id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1"><times id="S4.SS3.p1.2.m2.1.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1.1"></times><cn type="float" id="S4.SS3.p1.2.m2.1.1.2.cmml" xref="S4.SS3.p1.2.m2.1.1.2">1.0</cn><ci id="S4.SS3.p1.2.m2.1.1.3.cmml" xref="S4.SS3.p1.2.m2.1.1.3">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">1.0m</annotation></semantics></math>, <math id="S4.SS3.p1.3.m3.1" class="ltx_Math" alttext="2.0m" display="inline"><semantics id="S4.SS3.p1.3.m3.1a"><mrow id="S4.SS3.p1.3.m3.1.1" xref="S4.SS3.p1.3.m3.1.1.cmml"><mn id="S4.SS3.p1.3.m3.1.1.2" xref="S4.SS3.p1.3.m3.1.1.2.cmml">2.0</mn><mo lspace="0em" rspace="0em" id="S4.SS3.p1.3.m3.1.1.1" xref="S4.SS3.p1.3.m3.1.1.1.cmml">â€‹</mo><mi id="S4.SS3.p1.3.m3.1.1.3" xref="S4.SS3.p1.3.m3.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.3.m3.1b"><apply id="S4.SS3.p1.3.m3.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1"><times id="S4.SS3.p1.3.m3.1.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1.1"></times><cn type="float" id="S4.SS3.p1.3.m3.1.1.2.cmml" xref="S4.SS3.p1.3.m3.1.1.2">2.0</cn><ci id="S4.SS3.p1.3.m3.1.1.3.cmml" xref="S4.SS3.p1.3.m3.1.1.3">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.3.m3.1c">2.0m</annotation></semantics></math>, <math id="S4.SS3.p1.4.m4.1" class="ltx_Math" alttext="4.0m" display="inline"><semantics id="S4.SS3.p1.4.m4.1a"><mrow id="S4.SS3.p1.4.m4.1.1" xref="S4.SS3.p1.4.m4.1.1.cmml"><mn id="S4.SS3.p1.4.m4.1.1.2" xref="S4.SS3.p1.4.m4.1.1.2.cmml">4.0</mn><mo lspace="0em" rspace="0em" id="S4.SS3.p1.4.m4.1.1.1" xref="S4.SS3.p1.4.m4.1.1.1.cmml">â€‹</mo><mi id="S4.SS3.p1.4.m4.1.1.3" xref="S4.SS3.p1.4.m4.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.4.m4.1b"><apply id="S4.SS3.p1.4.m4.1.1.cmml" xref="S4.SS3.p1.4.m4.1.1"><times id="S4.SS3.p1.4.m4.1.1.1.cmml" xref="S4.SS3.p1.4.m4.1.1.1"></times><cn type="float" id="S4.SS3.p1.4.m4.1.1.2.cmml" xref="S4.SS3.p1.4.m4.1.1.2">4.0</cn><ci id="S4.SS3.p1.4.m4.1.1.3.cmml" xref="S4.SS3.p1.4.m4.1.1.3">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.4.m4.1c">4.0m</annotation></semantics></math>), and the mean AR (mAR) are compared.</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2308.04556/assets/x6.png" id="S4.F6.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="268" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span><span id="S4.F6.4.1" class="ltx_text ltx_font_bold">Class-wise recall comparison on nuScenes val set</span> between TransFusion-L and FocalFormer3D in terms of recall values across nuScenes center distance (CD) threshes (0.25/0.5/1.0m) on the nuScenes <span id="S4.F6.5.2" class="ltx_text ltx_font_italic">val</span> set. The red bars are normalized to 100%.</figcaption>
</figure>
<div id="S4.SS3.p2" class="ltx_para ltx_noindent">
<p id="S4.SS3.p2.1" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_bold">Recall comparison on initial object candidates.</span>
Figure <a href="#S4.F5" title="Figure 5 â€£ 4.3 Recall Analysis â€£ 4 Experiments â€£ FocalFormer3D : Focusing on Hard Instance for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> compares the recall of state-of-the-art methods that share the same SparseUNet backbone. With total 200 queries, FocalFormer3D-200P reaches 75.2 mAR, achieving considerable and consistent improvements by <math id="S4.SS3.p2.1.m1.1" class="ltx_Math" alttext="+4.5" display="inline"><semantics id="S4.SS3.p2.1.m1.1a"><mrow id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml"><mo id="S4.SS3.p2.1.m1.1.1a" xref="S4.SS3.p2.1.m1.1.1.cmml">+</mo><mn id="S4.SS3.p2.1.m1.1.1.2" xref="S4.SS3.p2.1.m1.1.1.2.cmml">4.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><apply id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"><plus id="S4.SS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"></plus><cn type="float" id="S4.SS3.p2.1.m1.1.1.2.cmml" xref="S4.SS3.p2.1.m1.1.1.2">4.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">+4.5</annotation></semantics></math> mAR compared with the prior state-of-the-art LiDAR approach TransFusion-L. Surprisingly, our LiDAR-based FocalFormer even achieves better results than the prior multi-modal approach DeepInteraction by 2.6 points in terms of mAR as well. As the query sizes get 600, FocalFormer3D achieves 79.2 mAR, surpassing the fusion approach DeepInteraction by 6.6 points. Further, by incorporating multi-view camera features, our multi-modal version FocalFormer-F gets improved to 80.9 mAR.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para ltx_noindent">
<p id="S4.SS3.p3.1" class="ltx_p"><span id="S4.SS3.p3.1.1" class="ltx_text ltx_font_bold">Recall comparison on final object prediction.</span>
Concerning the final predictions of 3D detectors, most LiDAR and fusion approaches obtain fewer performance improvements as the distance thresholds increase as shown in Fig.Â <a href="#S4.F5" title="Figure 5 â€£ 4.3 Recall Analysis â€£ 4 Experiments â€£ FocalFormer3D : Focusing on Hard Instance for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. This can be explained by higher distance thresholds indicating the performance for the extreme cases of missing detections. The introduction of camera features helps the model see the context in the perspective view, which leads to better performance such as DeepInteraction. However, their final prediction recall falls far behind FocalFormer-F with a large margin of 6.8 points.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para ltx_noindent">
<p id="S4.SS3.p4.1" class="ltx_p"><span id="S4.SS3.p4.1.1" class="ltx_text ltx_font_bold">Class-wise recall comparison.</span>
We compare the class-wise recall analysis for object candidates in Fig.Â <a href="#S4.F6" title="Figure 6 â€£ 4.3 Recall Analysis â€£ 4 Experiments â€£ FocalFormer3D : Focusing on Hard Instance for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> at the category level. The findings highlight the effectiveness of FocalFormer3D in improving the relative recall of initial BEV queries by a relative +10.9% improvement against TransFusion-L. Large objects such as <span id="S4.SS3.p4.1.2" class="ltx_text ltx_font_slanted">Construction Vehicles</span> and <span id="S4.SS3.p4.1.3" class="ltx_text ltx_font_slanted">Trailer</span> get the most improvements so that the predictions of their initial centers are challenging.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Ablation Study</h3>

<div id="S4.SS4.p1" class="ltx_para ltx_noindent">
<p id="S4.SS4.p1.1" class="ltx_p"><span id="S4.SS4.p1.1.1" class="ltx_text ltx_font_bold">HIP query sizes and generation stages.</span>
TableÂ <a href="#S4.T4" title="Table 4 â€£ 4.4 Ablation Study â€£ 4 Experiments â€£ FocalFormer3D : Focusing on Hard Instance for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> ablates the impacts of the number of queries and stages in the multi-stage heatmap encoder. When using the same query size of rough 200, approaches (b), which uses additional one stage of HIP, demonstrates better performance than baseline (a) by a margin of <math id="S4.SS4.p1.1.m1.1" class="ltx_Math" alttext="+0.7" display="inline"><semantics id="S4.SS4.p1.1.m1.1a"><mrow id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml"><mo id="S4.SS4.p1.1.m1.1.1a" xref="S4.SS4.p1.1.m1.1.1.cmml">+</mo><mn id="S4.SS4.p1.1.m1.1.1.2" xref="S4.SS4.p1.1.m1.1.1.2.cmml">0.7</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><apply id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1"><plus id="S4.SS4.p1.1.m1.1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1"></plus><cn type="float" id="S4.SS4.p1.1.m1.1.1.2.cmml" xref="S4.SS4.p1.1.m1.1.1.2">0.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">+0.7</annotation></semantics></math> mAP. When provided with more queries (600), our approach (d) and (e) achieve over 1.1-point improvement in terms of mAP.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.1.1.1" class="ltx_tr">
<th id="S4.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">#</th>
<th id="S4.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"># Stages</th>
<th id="S4.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">#Total Queries</th>
<th id="S4.T4.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">mAP</th>
<th id="S4.T4.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">NDS</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.1.2.1" class="ltx_tr">
<td id="S4.T4.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t">(a)</td>
<td id="S4.T4.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">1</td>
<td id="S4.T4.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">200</td>
<td id="S4.T4.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">65.3</td>
<td id="S4.T4.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">70.5</td>
</tr>
<tr id="S4.T4.1.3.2" class="ltx_tr">
<td id="S4.T4.1.3.2.1" class="ltx_td ltx_align_center">(b)</td>
<td id="S4.T4.1.3.2.2" class="ltx_td ltx_align_center">2</td>
<td id="S4.T4.1.3.2.3" class="ltx_td ltx_align_center">200</td>
<td id="S4.T4.1.3.2.4" class="ltx_td ltx_align_center">66.0</td>
<td id="S4.T4.1.3.2.5" class="ltx_td ltx_align_center">70.7</td>
</tr>
<tr id="S4.T4.1.4.3" class="ltx_tr">
<td id="S4.T4.1.4.3.1" class="ltx_td ltx_align_center">(c)</td>
<td id="S4.T4.1.4.3.2" class="ltx_td ltx_align_center">1</td>
<td id="S4.T4.1.4.3.3" class="ltx_td ltx_align_center">600</td>
<td id="S4.T4.1.4.3.4" class="ltx_td ltx_align_center">65.4</td>
<td id="S4.T4.1.4.3.5" class="ltx_td ltx_align_center">70.5</td>
</tr>
<tr id="S4.T4.1.5.4" class="ltx_tr">
<td id="S4.T4.1.5.4.1" class="ltx_td ltx_align_center">(d)</td>
<td id="S4.T4.1.5.4.2" class="ltx_td ltx_align_center">2</td>
<td id="S4.T4.1.5.4.3" class="ltx_td ltx_align_center">600</td>
<td id="S4.T4.1.5.4.4" class="ltx_td ltx_align_center">66.4</td>
<td id="S4.T4.1.5.4.5" class="ltx_td ltx_align_center">70.9</td>
</tr>
<tr id="S4.T4.1.6.5" class="ltx_tr">
<td id="S4.T4.1.6.5.1" class="ltx_td ltx_align_center ltx_border_bb">(e)</td>
<td id="S4.T4.1.6.5.2" class="ltx_td ltx_align_center ltx_border_bb">3</td>
<td id="S4.T4.1.6.5.3" class="ltx_td ltx_align_center ltx_border_bb">600</td>
<td id="S4.T4.1.6.5.4" class="ltx_td ltx_align_center ltx_border_bb">66.5</td>
<td id="S4.T4.1.6.5.5" class="ltx_td ltx_align_center ltx_border_bb">71.1</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span><span id="S4.T4.3.1" class="ltx_text ltx_font_bold">Effects of numbers of stages and total queries.</span> Here one stage stands for the baseline method without using hard instance probing. </figcaption>
</figure>
<figure id="S4.T5" class="ltx_table">
<table id="S4.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.1.1.1" class="ltx_tr">
<th id="S4.T5.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Mask Type</th>
<th id="S4.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">mAP</th>
<th id="S4.T5.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">NDS</th>
</tr>
<tr id="S4.T5.1.2.2" class="ltx_tr">
<th id="S4.T5.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">None</th>
<th id="S4.T5.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">65.3</th>
<th id="S4.T5.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">70.4</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.1.3.1" class="ltx_tr">
<th id="S4.T5.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Point-based</th>
<td id="S4.T5.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t">65.9</td>
<td id="S4.T5.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">70.5</td>
</tr>
<tr id="S4.T5.1.4.2" class="ltx_tr">
<th id="S4.T5.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Box-based</th>
<td id="S4.T5.1.4.2.2" class="ltx_td ltx_align_center">66.1</td>
<td id="S4.T5.1.4.2.3" class="ltx_td ltx_align_center">70.9</td>
</tr>
<tr id="S4.T5.1.5.3" class="ltx_tr">
<th id="S4.T5.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Pooling-based</th>
<td id="S4.T5.1.5.3.2" class="ltx_td ltx_align_center ltx_border_bb">66.5</td>
<td id="S4.T5.1.5.3.3" class="ltx_td ltx_align_center ltx_border_bb">71.1</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span><span id="S4.T5.3.1" class="ltx_text ltx_font_bold">Effects of various positive mask types.</span> All models adopt the same network except for the masking way.</figcaption>
</figure>
<div id="S4.SS4.p2" class="ltx_para ltx_noindent">
<p id="S4.SS4.p2.3" class="ltx_p"><span id="S4.SS4.p2.3.1" class="ltx_text ltx_font_bold">Positive mask type.</span>
TableÂ <a href="#S4.T5" title="Table 5 â€£ 4.4 Ablation Study â€£ 4 Experiments â€£ FocalFormer3D : Focusing on Hard Instance for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> presents an ablation study on the effectiveness of Hard Instance Probing in terms of various mask types. Specifically, we compare the performance of our method with none masking, point-based masking, and pooling-based masking. The results demonstrate that even with single-point masking, HIP improves the performance of the baseline by a gain of <math id="S4.SS4.p2.1.m1.1" class="ltx_Math" alttext="+0.6" display="inline"><semantics id="S4.SS4.p2.1.m1.1a"><mrow id="S4.SS4.p2.1.m1.1.1" xref="S4.SS4.p2.1.m1.1.1.cmml"><mo id="S4.SS4.p2.1.m1.1.1a" xref="S4.SS4.p2.1.m1.1.1.cmml">+</mo><mn id="S4.SS4.p2.1.m1.1.1.2" xref="S4.SS4.p2.1.m1.1.1.2.cmml">0.6</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.1.m1.1b"><apply id="S4.SS4.p2.1.m1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1"><plus id="S4.SS4.p2.1.m1.1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1"></plus><cn type="float" id="S4.SS4.p2.1.m1.1.1.2.cmml" xref="S4.SS4.p2.1.m1.1.1.2">0.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.1.m1.1c">+0.6</annotation></semantics></math> points in terms of mAP. Furthermore, the pooling-based masking shows the best gain with <math id="S4.SS4.p2.2.m2.1" class="ltx_Math" alttext="+1.2" display="inline"><semantics id="S4.SS4.p2.2.m2.1a"><mrow id="S4.SS4.p2.2.m2.1.1" xref="S4.SS4.p2.2.m2.1.1.cmml"><mo id="S4.SS4.p2.2.m2.1.1a" xref="S4.SS4.p2.2.m2.1.1.cmml">+</mo><mn id="S4.SS4.p2.2.m2.1.1.2" xref="S4.SS4.p2.2.m2.1.1.2.cmml">1.2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.2.m2.1b"><apply id="S4.SS4.p2.2.m2.1.1.cmml" xref="S4.SS4.p2.2.m2.1.1"><plus id="S4.SS4.p2.2.m2.1.1.1.cmml" xref="S4.SS4.p2.2.m2.1.1"></plus><cn type="float" id="S4.SS4.p2.2.m2.1.1.2.cmml" xref="S4.SS4.p2.2.m2.1.1.2">1.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.2.m2.1c">+1.2</annotation></semantics></math> mAP and <math id="S4.SS4.p2.3.m3.1" class="ltx_Math" alttext="+0.7" display="inline"><semantics id="S4.SS4.p2.3.m3.1a"><mrow id="S4.SS4.p2.3.m3.1.1" xref="S4.SS4.p2.3.m3.1.1.cmml"><mo id="S4.SS4.p2.3.m3.1.1a" xref="S4.SS4.p2.3.m3.1.1.cmml">+</mo><mn id="S4.SS4.p2.3.m3.1.1.2" xref="S4.SS4.p2.3.m3.1.1.2.cmml">0.7</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.3.m3.1b"><apply id="S4.SS4.p2.3.m3.1.1.cmml" xref="S4.SS4.p2.3.m3.1.1"><plus id="S4.SS4.p2.3.m3.1.1.1.cmml" xref="S4.SS4.p2.3.m3.1.1"></plus><cn type="float" id="S4.SS4.p2.3.m3.1.1.2.cmml" xref="S4.SS4.p2.3.m3.1.1.2">0.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.3.m3.1c">+0.7</annotation></semantics></math> NDS, outperforming the box-based masking. This can be attributed to two facts. Point or pooling-based masking can already effectively exclude positive objects as the center heatmapÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> only highlights a Gaussian peak. Second, the wrong false positive predictions or predicted boxes might lead to false masking of the ground-truth boxes, resulting in missed detection.</p>
</div>
<figure id="S4.T6" class="ltx_table">
<div id="S4.T6.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:263.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(98.4pt,-59.9pt) scale(1.83176428394444,1.83176428394444) ;">
<table id="S4.T6.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T6.1.1.1.1" class="ltx_tr">
<th id="S4.T6.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" rowspan="2"><span id="S4.T6.1.1.1.1.1.1" class="ltx_text">#</span></th>
<th id="S4.T6.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" rowspan="2"><span id="S4.T6.1.1.1.1.2.1" class="ltx_text">M.S. Heat</span></th>
<th id="S4.T6.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">Refinement Module</th>
<th id="S4.T6.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" rowspan="2"><span id="S4.T6.1.1.1.1.4.1" class="ltx_text">mAP</span></th>
<th id="S4.T6.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" rowspan="2"><span id="S4.T6.1.1.1.1.5.1" class="ltx_text">NDS</span></th>
</tr>
<tr id="S4.T6.1.1.2.2" class="ltx_tr">
<th id="S4.T6.1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">BoxPool</th>
<th id="S4.T6.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">C.A.</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T6.1.1.3.1" class="ltx_tr">
<td id="S4.T6.1.1.3.1.1" class="ltx_td ltx_align_center ltx_border_t">(a)</td>
<td id="S4.T6.1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t">âœ—</td>
<td id="S4.T6.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">âœ—</td>
<td id="S4.T6.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">âœ—</td>
<td id="S4.T6.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">63.1</td>
<td id="S4.T6.1.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t">69.1</td>
</tr>
<tr id="S4.T6.1.1.4.2" class="ltx_tr">
<td id="S4.T6.1.1.4.2.1" class="ltx_td ltx_align_center">(b)</td>
<td id="S4.T6.1.1.4.2.2" class="ltx_td ltx_align_center">âœ“</td>
<td id="S4.T6.1.1.4.2.3" class="ltx_td ltx_align_center">âœ—</td>
<td id="S4.T6.1.1.4.2.4" class="ltx_td ltx_align_center">âœ—</td>
<td id="S4.T6.1.1.4.2.5" class="ltx_td ltx_align_center">63.3</td>
<td id="S4.T6.1.1.4.2.6" class="ltx_td ltx_align_center">69.3</td>
</tr>
<tr id="S4.T6.1.1.5.3" class="ltx_tr">
<td id="S4.T6.1.1.5.3.1" class="ltx_td ltx_align_center">(c)</td>
<td id="S4.T6.1.1.5.3.2" class="ltx_td ltx_align_center">âœ“</td>
<td id="S4.T6.1.1.5.3.3" class="ltx_td ltx_align_center">âœ“</td>
<td id="S4.T6.1.1.5.3.4" class="ltx_td ltx_align_center">âœ—</td>
<td id="S4.T6.1.1.5.3.5" class="ltx_td ltx_align_center">65.1</td>
<td id="S4.T6.1.1.5.3.6" class="ltx_td ltx_align_center">69.9</td>
</tr>
<tr id="S4.T6.1.1.6.4" class="ltx_tr">
<td id="S4.T6.1.1.6.4.1" class="ltx_td ltx_align_center">(d)</td>
<td id="S4.T6.1.1.6.4.2" class="ltx_td ltx_align_center">âœ“</td>
<td id="S4.T6.1.1.6.4.3" class="ltx_td ltx_align_center">âœ—</td>
<td id="S4.T6.1.1.6.4.4" class="ltx_td ltx_align_center">âœ“</td>
<td id="S4.T6.1.1.6.4.5" class="ltx_td ltx_align_center">65.9</td>
<td id="S4.T6.1.1.6.4.6" class="ltx_td ltx_align_center">70.9</td>
</tr>
<tr id="S4.T6.1.1.7.5" class="ltx_tr">
<td id="S4.T6.1.1.7.5.1" class="ltx_td ltx_align_center">(e)</td>
<td id="S4.T6.1.1.7.5.2" class="ltx_td ltx_align_center">âœ“</td>
<td id="S4.T6.1.1.7.5.3" class="ltx_td ltx_align_center">âœ“</td>
<td id="S4.T6.1.1.7.5.4" class="ltx_td ltx_align_center">âœ“</td>
<td id="S4.T6.1.1.7.5.5" class="ltx_td ltx_align_center">66.5</td>
<td id="S4.T6.1.1.7.5.6" class="ltx_td ltx_align_center">71.1</td>
</tr>
<tr id="S4.T6.1.1.8.6" class="ltx_tr">
<td id="S4.T6.1.1.8.6.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">(f)</td>
<td id="S4.T6.1.1.8.6.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">âœ“</td>
<td id="S4.T6.1.1.8.6.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" colspan="2">Rescoring Only</td>
<td id="S4.T6.1.1.8.6.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">66.1</td>
<td id="S4.T6.1.1.8.6.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">68.8</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span><span id="S4.T6.3.1" class="ltx_text ltx_font_bold">Step-by-step improvements made by modules.</span> â€œM.S. Heatâ€ represents the application of the multi-stage heatmap encoder for hard instance probing. â€œC.A.â€ denotes using deformable cross attention for second-stage refinement. â€œBoxPoolâ€ represents the Box-pooling module. The term â€œRescoring Onlyâ€ refers to the model that directly generates box prediction from BEV feature and uses its decoder head to rescore the candidate predictions from heatmap without performing additional bounding box refinement. </figcaption>
</figure>
<div id="S4.SS4.p3" class="ltx_para ltx_noindent">
<p id="S4.SS4.p3.1" class="ltx_p"><span id="S4.SS4.p3.1.1" class="ltx_text ltx_font_bold">Step-by-step module refinement.</span>
We conduct ablation studies on the step-by-step improvements by each module, presented in TableÂ <a href="#S4.T6" title="Table 6 â€£ 4.4 Ablation Study â€£ 4 Experiments â€£ FocalFormer3D : Focusing on Hard Instance for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, to illustrate the component effectiveness within hard instance probing (HIP) pipeline. Initially, without second-stage refinement, we used simple center-based predictionsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> (a), which estimate boxes directly from BEV feature by another convolutional layer.</p>
</div>
<div id="S4.SS4.p4" class="ltx_para">
<p id="S4.SS4.p4.5" class="ltx_p">Despite an improvement in the average recall by over <math id="S4.SS4.p4.1.m1.1" class="ltx_Math" alttext="9" display="inline"><semantics id="S4.SS4.p4.1.m1.1a"><mn id="S4.SS4.p4.1.m1.1.1" xref="S4.SS4.p4.1.m1.1.1.cmml">9</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.p4.1.m1.1b"><cn type="integer" id="S4.SS4.p4.1.m1.1.1.cmml" xref="S4.SS4.p4.1.m1.1.1">9</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p4.1.m1.1c">9</annotation></semantics></math> points in Fig.Â <span id="S4.SS4.p4.5.1" class="ltx_text" style="color:#FF0000;"> 5</span>, we found little improvement of (b) over (a) in performance after using the multi-stage heatmap encoder to generate the object candidates. By applying simple object-level rescoring (c), with RoI-based refinement (using two hidden MLP layers), the performance is boosted to <math id="S4.SS4.p4.2.m2.1" class="ltx_Math" alttext="65.1" display="inline"><semantics id="S4.SS4.p4.2.m2.1a"><mn id="S4.SS4.p4.2.m2.1.1" xref="S4.SS4.p4.2.m2.1.1.cmml">65.1</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.p4.2.m2.1b"><cn type="float" id="S4.SS4.p4.2.m2.1.1.cmml" xref="S4.SS4.p4.2.m2.1.1">65.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p4.2.m2.1c">65.1</annotation></semantics></math> mAP and <math id="S4.SS4.p4.3.m3.1" class="ltx_Math" alttext="69.9" display="inline"><semantics id="S4.SS4.p4.3.m3.1a"><mn id="S4.SS4.p4.3.m3.1.1" xref="S4.SS4.p4.3.m3.1.1.cmml">69.9</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.p4.3.m3.1b"><cn type="float" id="S4.SS4.p4.3.m3.1.1.cmml" xref="S4.SS4.p4.3.m3.1.1">69.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p4.3.m3.1c">69.9</annotation></semantics></math> NDS. Remarkably, our complete box-level deformable decoder (e) further improves the performance by a margin of <math id="S4.SS4.p4.4.m4.1" class="ltx_Math" alttext="+1.4" display="inline"><semantics id="S4.SS4.p4.4.m4.1a"><mrow id="S4.SS4.p4.4.m4.1.1" xref="S4.SS4.p4.4.m4.1.1.cmml"><mo id="S4.SS4.p4.4.m4.1.1a" xref="S4.SS4.p4.4.m4.1.1.cmml">+</mo><mn id="S4.SS4.p4.4.m4.1.1.2" xref="S4.SS4.p4.4.m4.1.1.2.cmml">1.4</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p4.4.m4.1b"><apply id="S4.SS4.p4.4.m4.1.1.cmml" xref="S4.SS4.p4.4.m4.1.1"><plus id="S4.SS4.p4.4.m4.1.1.1.cmml" xref="S4.SS4.p4.4.m4.1.1"></plus><cn type="float" id="S4.SS4.p4.4.m4.1.1.2.cmml" xref="S4.SS4.p4.4.m4.1.1.2">1.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p4.4.m4.1c">+1.4</annotation></semantics></math> mAP and <math id="S4.SS4.p4.5.m5.1" class="ltx_Math" alttext="+1.2" display="inline"><semantics id="S4.SS4.p4.5.m5.1a"><mrow id="S4.SS4.p4.5.m5.1.1" xref="S4.SS4.p4.5.m5.1.1.cmml"><mo id="S4.SS4.p4.5.m5.1.1a" xref="S4.SS4.p4.5.m5.1.1.cmml">+</mo><mn id="S4.SS4.p4.5.m5.1.1.2" xref="S4.SS4.p4.5.m5.1.1.2.cmml">1.2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p4.5.m5.1b"><apply id="S4.SS4.p4.5.m5.1.1.cmml" xref="S4.SS4.p4.5.m5.1.1"><plus id="S4.SS4.p4.5.m5.1.1.1.cmml" xref="S4.SS4.p4.5.m5.1.1"></plus><cn type="float" id="S4.SS4.p4.5.m5.1.1.2.cmml" xref="S4.SS4.p4.5.m5.1.1.2">1.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p4.5.m5.1c">+1.2</annotation></semantics></math> NDS.</p>
</div>
<div id="S4.SS4.p5" class="ltx_para">
<p id="S4.SS4.p5.1" class="ltx_p">To assess the effects of rescoring alone, we perform experiment (f), which excludes the effects of box regression by not using any box or position regression in the object-level refinement module. Despite this, experiment (f) still achieves high center accuracy (<math id="S4.SS4.p5.1.m1.1" class="ltx_Math" alttext="66.1" display="inline"><semantics id="S4.SS4.p5.1.m1.1a"><mn id="S4.SS4.p5.1.m1.1.1" xref="S4.SS4.p5.1.m1.1.1.cmml">66.1</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.p5.1.m1.1b"><cn type="float" id="S4.SS4.p5.1.m1.1.1.cmml" xref="S4.SS4.p5.1.m1.1.1">66.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p5.1.m1.1c">66.1</annotation></semantics></math> mAP) compared to (a). This finding highlights the limitations of the initial ranking of object candidates across stages based solely on BEV heatmap scores. Therefore, it validates the necessity for a second-stage object-level rescoring in the hard instance probing pipeline (Fig.Â <span id="S4.SS4.p5.1.1" class="ltx_text" style="color:#FF0000;"> 3</span>).</p>
</div>
<div id="S4.SS4.p6" class="ltx_para ltx_noindent">
<p id="S4.SS4.p6.1" class="ltx_p"><span id="S4.SS4.p6.1.1" class="ltx_text ltx_font_bold">Latency analysis for model components.</span>
We conduct a latency analysis for FocalFormer3D on the nuScenes dataset. The runtimes are measured on the same V100 GPU machine for comparison.To ensure a fair speed comparison with CenterPointÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, dynamic voxelizationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite> is employed for speed testing of both TransFusion-L and FocalFormer3D. The computation time is mostly taken up by the sparse convolution-based backbone network (VoxelNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>), which takes 78ms. Our multi-stage heatmap encoder takes 13ms to collect queries from the heatmaps across stages, while the box-level deformable decoder head takes 18ms. Note that, the generation of multi-stage heatmaps only takes 5ms, and additional operations such as Top-K selection takes 7ms, indicating potential optimization opportunities for future work.</p>
</div>
<figure id="S4.T7" class="ltx_table">
<table id="S4.T7.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T7.1.1.1" class="ltx_tr">
<th id="S4.T7.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Models/Components</th>
<th id="S4.T7.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Latency</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T7.1.2.1" class="ltx_tr">
<td id="S4.T7.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t">TransFusion-L</td>
<td id="S4.T7.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">93ms</td>
</tr>
<tr id="S4.T7.1.3.2" class="ltx_tr">
<td id="S4.T7.1.3.2.1" class="ltx_td ltx_align_left ltx_border_t">FocalFormer3D</td>
<td id="S4.T7.1.3.2.2" class="ltx_td ltx_align_center ltx_border_t">109ms</td>
</tr>
<tr id="S4.T7.1.4.3" class="ltx_tr">
<td id="S4.T7.1.4.3.1" class="ltx_td ltx_align_left">â€“ VoxelNet backbone</td>
<td id="S4.T7.1.4.3.2" class="ltx_td ltx_align_center">78ms</td>
</tr>
<tr id="S4.T7.1.5.4" class="ltx_tr">
<td id="S4.T7.1.5.4.1" class="ltx_td ltx_align_left">â€“ Multi-stage heatmap encoder</td>
<td id="S4.T7.1.5.4.2" class="ltx_td ltx_align_center">13ms</td>
</tr>
<tr id="S4.T7.1.6.5" class="ltx_tr">
<td id="S4.T7.1.6.5.1" class="ltx_td ltx_align_left ltx_border_bb">â€“ Box-level deformable decoder</td>
<td id="S4.T7.1.6.5.2" class="ltx_td ltx_align_center ltx_border_bb">18ms</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7: </span><span id="S4.T7.3.1" class="ltx_text ltx_font_bold">Latency analysis for model components.</span> Latency is measured on a V100 GPU for reference. </figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this work, we explicitly focus on the fatal problem in autonomous driving, <span id="S5.p1.1.1" class="ltx_text ltx_font_italic">i.e.</span>, false negative detections. We present FocalFormer3D as solution. It progressively probes hard instances and improves prediction recall, via the hard instance probingÂ (HIP). Nontrivial improvements are introduced with limited overhead upon transformer-based 3D detectors. The HIP algorithm enables FocalFormer3D to effectively reduce false negatives in 3D object detection.</p>
</div>
<section id="S5.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Limitation.</h4>

<div id="S5.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px1.p1.1" class="ltx_p">A key limitation is that FocalFormer3Dâ€™s hard instance probing (HIP) relies on the assumption that object centers produce Gaussian-like peaks in the BEV heatmap, which may not hold for camera-based detectors where heatmaps tend to be fan-shaped. Additionally, few studies have explored hard instances in long-range detection, so more research is needed to evaluate HIP in this area. We leave more investigation of hard instance probing as future work.</p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Yin Zhou and Oncel Tuzel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Voxelnet: End-to-end learning for point cloud based 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
AlexÂ H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar
Beijbom.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Pointpillars: Fast encoders for object detection from point clouds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">2018.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Tianwei Yin, Xingyi Zhou, and Philipp Krahenbuhl.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Center-based 3d object detection and tracking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, pages 11784â€“11793, 2021.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Xuyang Bai, Zeyu Hu, Xinge Zhu, Qingqiu Huang, Yilun Chen, Hongbo Fu, and
Chiew-Lan Tai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Transfusion: Robust lidar-camera fusion for 3d object detection with
transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, pages 1080â€“1089, 2022.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Tingting Liang, Hongwei Xie, Kaicheng Yu, Zhongyu Xia, Zhiwei Lin, Yongtao
Wang, Tao Tang, Bing Wang, and Zhi Tang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Bevfusion: A simple and robust lidar-camera fusion framework.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Zhijian Liu, Haotian Tang, Alexander Amini, Xinyu Yang, Huizi Mao, Daniela Rus,
and Song Han.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Bevfusion: Multi-task multi-sensor fusion with unified birdâ€™s-eye
view representation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2205.13542</span><span id="bib.bib6.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Yanwei Li, Yilun Chen, Xiaojuan Qi, Zeming Li, Jian Sun, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Unifying voxel-based representation with transformer for 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">2022.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Yanwei Li, Xiaojuan Qi, Yukang Chen, Liwei Wang, Zeming Li, Jian Sun, and Jiaya
Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Voxel field fusion for 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Yang Jiao, Zequn Jie, Shaoxiang Chen, Jingjing Chen, Lin Ma, and Yu-Gang Jiang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Msmdfusion: A gated multi-scale lidar-camera fusion framework with
multi-depth seeds for 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2209.03102</span><span id="bib.bib9.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Zeyu Yang, Jiaqi Chen, Zhenwei Miao, Wei Li, Xiatian Zhu, and LiÂ Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Deepinteraction: 3d object detection via modality interaction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2208.11112</span><span id="bib.bib10.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Pointrcnn: 3d object proposal generation and detection from point
cloud.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, pages 770â€“779, 2019.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Yilun Chen, Shu Liu, Xiaoyong Shen, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Fast point r-cnn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, pages 9775â€“9784, 2019.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Jiajun Deng, Shaoshuai Shi, Peiwei Li, Wengang Zhou, Yanyong Zhang, and
Houqiang Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Voxel r-cnn: Towards high performance voxel-based 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">AAAI</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, volumeÂ 35, pages 1201â€“1209, 2021.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Zhichao Li, Feng Wang, and Naiyan Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Lidar r-cnn: An efficient and universal 3d object detector.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib14.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
ZeÂ Liu, Zheng Zhang, Yue Cao, Han Hu, and Xin Tong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Group-free 3d object detection via transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, pages 2949â€“2958, 2021.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Zhaowei Cai and Nuno Vasconcelos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Cascade r-cnn: Delving into high quality object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, pages 6154â€“6162, 2018.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Deformable detr: Deformable transformers for end-to-end object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2010.04159</span><span id="bib.bib17.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Feng Li, Hao Zhang, Shilong Liu, Jian Guo, LionelÂ M Ni, and Lei Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Dn-detr: Accelerate detr training by introducing query denoising.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, pages 13619â€“13627, 2022.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Xingyi Zhou, Dequan Wang, and Philipp KrÃ¤henbÃ¼hl.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Objects as points.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1904.07850</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Georgia Gkioxari, Piotr DollÃ¡r, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Mask r-cnn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Faster r-cnn: Towards real-time object detection with region proposal
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Yan Yan, Yuxing Mao, and BoÂ Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Second: Sparsely embedded convolutional detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">2018.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Bin Yang, Wenjie Luo, and Raquel Urtasun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Pixor: Real-time 3d object detection from point clouds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Shaoshuai Shi, Chaoxu Guo, LiÂ Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, and
Hongsheng Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Pv-rcnn: Point-voxel feature set abstraction for 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, pages 10529â€“10538, 2020.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Shaoshuai Shi, LiÂ Jiang, Jiajun Deng, Zhe Wang, Chaoxu Guo, Jianping Shi,
Xiaogang Wang, and Hongsheng Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Pv-rcnn++: Point-voxel feature set abstraction with local vector
representation for 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IJCV</span><span id="bib.bib25.4.2" class="ltx_text" style="font-size:90%;">, pages 1â€“21, 2022.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Zixiang Zhou, Xiangchen Zhao, YuÂ Wang, Panqu Wang, and Hassan Foroosh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Centerformer: Center-based transformer for 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Yukang Chen, Yanwei Li, Xiangyu Zhang, Jian Sun, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Focal sparse convolutional networks for 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, pages 5428â€“5437, 2022.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Yukang Chen, Jianhui Liu, Xiaojuan Qi, Xiangyu Zhang, Jian Sun, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Scaling up kernels in 3d cnns.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2206.10555</span><span id="bib.bib28.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Yukang Chen, Jianhui Liu, Xiangyu Zhang, Xiaojuan Qi, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Voxelnext: Fully sparse voxelnet for 3d object detection and
tracking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Thomas Roddick, Alex Kendall, and Roberto Cipolla.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Orthographic feature transform for monocular 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">British Machine Vision Conference</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Yilun Chen, Shu Liu, Xiaoyong Shen, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Dsgn: Deep stereo geometry network for 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, pages 12536â€“12545, 2020.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Jonah Philion and Sanja Fidler.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Lift, splat, shoot: Encoding images from arbitrary camera rigs by
implicitly unprojecting to 3d.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">, pages 194â€“210. Springer, 2020.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, YuÂ Qiao,
and Jifeng Dai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Bevformer: Learning birdâ€™s-eye-view representation from
multi-camera images via spatiotemporal transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2203.17270</span><span id="bib.bib33.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Yunpeng Zhang, Zheng Zhu, Wenzhao Zheng, Junjie Huang, Guan Huang, Jie Zhou,
and Jiwen Lu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Beverse: Unified perception and prediction in birds-eye-view for
vision-centric autonomous driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2205.09743</span><span id="bib.bib34.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Junjie Huang, Guan Huang, Zheng Zhu, YeÂ Yun, and Dalong Du.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Bevdet: High-performance multi-camera 3d object detection in
bird-eye-view.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2112.11790</span><span id="bib.bib35.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Yingfei Liu, Tiancai Wang, Xiangyu Zhang, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Petr: Position embedding transformation for multi-view 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Visionâ€“ECCV 2022: 17th European Conference, Tel
Aviv, Israel, October 23â€“27, 2022, Proceedings, Part XXVII</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:90%;">, pages 531â€“548.
Springer, 2022.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Yilun Chen, Shijia Huang, Shu Liu, Bei Yu, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Dsgn++: Exploiting visual-spatial relation for stereo-based 3d
detectors.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span id="bib.bib37.4.2" class="ltx_text" style="font-size:90%;">,
2022.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
AJÂ Piergiovanni, Vincent Casser, MichaelÂ S Ryoo, and Anelia Angelova.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">4d-net for learned multi-modal alignment.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib38.5.3" class="ltx_text" style="font-size:90%;">, pages 15435â€“15445, 2021.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Yingwei Li, AdamsÂ Wei Yu, Tianjian Meng, Ben Caine, Jiquan Ngiam, Daiyi Peng,
Junyang Shen, Yifeng Lu, Denny Zhou, QuocÂ V Le, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">Deepfusion: Lidar-camera deep fusion for multi-modal 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib39.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib39.5.3" class="ltx_text" style="font-size:90%;">, pages 17182â€“17191, 2022.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Zehui Chen, Zhenyu Li, Shiquan Zhang, Liangji Fang, Qinhong Jiang, and Feng
Zhao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Autoalignv2: Deformable feature aggregation for dynamic multi-modal
3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib40.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Tianwei Yin, Xingyi Zhou, and Philipp KrÃ¤henbÃ¼hl.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">Multimodal virtual point 3d detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib41.4.2" class="ltx_text" style="font-size:90%;">, 34:16494â€“16507, 2021.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Xuanyao Chen, Tianyuan Zhang, Yue Wang, Yilun Wang, and Hang Zhao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Futr3d: A unified sensor fusion framework for 3d detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2203.10642</span><span id="bib.bib42.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Xiaozhi Chen, Huimin Ma, JiÂ Wan, BoÂ Li, and Tian Xia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Multi-view 3d object detection network for autonomous driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib43.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib43.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Ming Liang, Bin Yang, Yun Chen, Rui Hu, and Raquel Urtasun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Multi-task multi-sensor fusion for 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib44.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib44.5.3" class="ltx_text" style="font-size:90%;">, pages 7345â€“7353, 2019.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Ming Liang, Bin Yang, Shenlong Wang, and Raquel Urtasun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">Deep continuous fusion for multi-sensor 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib45.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib45.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Hongyang Li, Chonghao Sima, Jifeng Dai, Wenhai Wang, Lewei Lu, Huijie Wang,
Enze Xie, Zhiqi Li, Hanming Deng, Hao Tian, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">Delving into the devils of birdâ€™s-eye-view perception: A review,
evaluation and recipe.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2209.05324</span><span id="bib.bib46.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Benjamin Graham, Martin Engelcke, and Laurens vanÂ der Maaten.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">3d semantic segmentation with submanifold sparse convolutional
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text" style="font-size:90%;">2018.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
Guangsheng Shi, Ruifeng Li, and Chao Ma.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">Pillarnet: Real-time and high-performance pillar-based 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib48.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Visionâ€“ECCV 2022: 17th European Conference, Tel
Aviv, Israel, October 23â€“27, 2022, Proceedings, Part X</span><span id="bib.bib48.5.3" class="ltx_text" style="font-size:90%;">, pages 35â€“52.
Springer, 2022.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
Runzhou Ge, Zhuangzhuang Ding, Yihan Hu, YuÂ Wang, Sijia Chen, LiÂ Huang, and
Yuan Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">Afdet: Anchor free one stage 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2006.12671</span><span id="bib.bib49.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
Zetong Yang, Yanan Sun, Shu Liu, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">3dssd: Point-based 3d single stage object detector, 2020.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text" style="font-size:90%;">
Zetong Yang, Yanan Sun, Shu Liu, Xiaoyong Shen, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.2.1" class="ltx_text" style="font-size:90%;">Std: Sparse-to-dense 3d object detector for point cloud.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib51.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib51.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text" style="font-size:90%;">
CharlesÂ R Qi, OrÂ Litany, Kaiming He, and LeonidasÂ J Guibas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.2.1" class="ltx_text" style="font-size:90%;">Deep hough voting for 3d object detection in point clouds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib52.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib52.5.3" class="ltx_text" style="font-size:90%;">, pages 9277â€“9286, 2019.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text" style="font-size:90%;">
CharlesÂ R. Qi, Hao Su, Kaichun Mo, and LeonidasÂ J Guibas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.2.1" class="ltx_text" style="font-size:90%;">Pointnet: Deep learning on point sets for 3d classification and
segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.3.1" class="ltx_text" style="font-size:90%;">2017.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text" style="font-size:90%;">
CharlesÂ R. Qi, LiÂ Yi, Hao Su, and LeonidasÂ J Guibas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.2.1" class="ltx_text" style="font-size:90%;">Pointnet++: Deep hierarchical feature learning on point sets in a
metric space.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib54.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib54.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text" style="font-size:90%;">
Lue Fan, Ziqi Pang, Tianyuan Zhang, Yu-Xiong Wang, Hang Zhao, Feng Wang, Naiyan
Wang, and Zhaoxiang Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.2.1" class="ltx_text" style="font-size:90%;">Embracing Single Stride 3D Object Detector with Sparse Transformer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib55.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib55.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text" style="font-size:90%;">
Lue Fan, Feng Wang, Naiyan Wang, and Zhaoxiang Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.2.1" class="ltx_text" style="font-size:90%;">Fully Sparse 3D Object Detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2207.10035</span><span id="bib.bib56.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text" style="font-size:90%;">
Chunwei Wang, Chao Ma, Ming Zhu, and Xiaokang Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.2.1" class="ltx_text" style="font-size:90%;">Pointaugmenting: Cross-modal augmentation for 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib57.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib57.5.3" class="ltx_text" style="font-size:90%;">, pages 11794â€“11803, 2021.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock"><span id="bib.bib58.1.1" class="ltx_text" style="font-size:90%;">
Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.2.1" class="ltx_text" style="font-size:90%;">Fast r-cnn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib58.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib58.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock"><span id="bib.bib59.1.1" class="ltx_text" style="font-size:90%;">
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander
Kirillov, and Sergey Zagoruyko.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.2.1" class="ltx_text" style="font-size:90%;">End-to-end object detection with transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib59.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib59.5.3" class="ltx_text" style="font-size:90%;">, pages 213â€“229. Springer, 2020.
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock"><span id="bib.bib60.1.1" class="ltx_text" style="font-size:90%;">
Andreas Geiger, Philip Lenz, and Raquel Urtasun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.2.1" class="ltx_text" style="font-size:90%;">Are we ready for autonomous driving? the kitti vision benchmark
suite.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib60.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib60.5.3" class="ltx_text" style="font-size:90%;">, 2012.
</span>
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock"><span id="bib.bib61.1.1" class="ltx_text" style="font-size:90%;">
Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai
Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.2.1" class="ltx_text" style="font-size:90%;">Scalability in perception for autonomous driving: Waymo open dataset.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib61.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib61.5.3" class="ltx_text" style="font-size:90%;">, pages 2446â€“2454, 2020.
</span>
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock"><span id="bib.bib62.1.1" class="ltx_text" style="font-size:90%;">
Holger Caesar, Varun Bankiti, AlexÂ H Lang, Sourabh Vora, VeniceÂ Erin Liong,
Qiang Xu, Anush Krishnan, YuÂ Pan, Giancarlo Baldan, and Oscar Beijbom.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.2.1" class="ltx_text" style="font-size:90%;">nuscenes: A multimodal dataset for autonomous driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib62.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib62.5.3" class="ltx_text" style="font-size:90%;">, pages 11621â€“11631, 2020.
</span>
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock"><span id="bib.bib63.1.1" class="ltx_text" style="font-size:90%;">
Kah-Kay Sung.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.2.1" class="ltx_text" style="font-size:90%;">Learning and example selection for object and pattern detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.3.1" class="ltx_text" style="font-size:90%;">1996.
</span>
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock"><span id="bib.bib64.1.1" class="ltx_text" style="font-size:90%;">
Abhinav Shrivastava, Abhinav Gupta, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.2.1" class="ltx_text" style="font-size:90%;">Training region-based object detectors with online hard example
mining.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib64.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib64.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock"><span id="bib.bib65.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.2.1" class="ltx_text" style="font-size:90%;">Focal loss for dense object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib65.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib65.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock"><span id="bib.bib66.1.1" class="ltx_text" style="font-size:90%;">
Buyu Li, YuÂ Liu, and Xiaogang Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.2.1" class="ltx_text" style="font-size:90%;">Gradient harmonized single-stage detector.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib66.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the AAAI conference on artificial
intelligence</span><span id="bib.bib66.5.3" class="ltx_text" style="font-size:90%;">, volumeÂ 33, pages 8577â€“8584, 2019.
</span>
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock"><span id="bib.bib67.1.1" class="ltx_text" style="font-size:90%;">
Yuhang Cao, Kai Chen, ChenÂ Change Loy, and Dahua Lin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.2.1" class="ltx_text" style="font-size:90%;">Prime sample attention in object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib67.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib67.5.3" class="ltx_text" style="font-size:90%;">, pages 11583â€“11591, 2020.
</span>
</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock"><span id="bib.bib68.1.1" class="ltx_text" style="font-size:90%;">
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh
Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.2.1" class="ltx_text" style="font-size:90%;">Mobilenetv2: Inverted residuals and linear bottlenecks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib68.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib68.5.3" class="ltx_text" style="font-size:90%;">, pages 4510â€“4520, 2018.
</span>
</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock"><span id="bib.bib69.1.1" class="ltx_text" style="font-size:90%;">
Zhuyu Yao, Jiangbo Ai, Boxun Li, and Chi Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.2.1" class="ltx_text" style="font-size:90%;">Efficient detr: improving end-to-end object detector with dense
prior.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2104.01318</span><span id="bib.bib69.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock"><span id="bib.bib70.1.1" class="ltx_text" style="font-size:90%;">
Duy-Kien Nguyen, Jihong Ju, Olaf Booij, MartinÂ R Oswald, and CeesÂ GM Snoek.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.2.1" class="ltx_text" style="font-size:90%;">Boxer: Box-attention for 2d and 3d transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib70.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib70.5.3" class="ltx_text" style="font-size:90%;">, pages 4773â€“4782, 2022.
</span>
</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock"><span id="bib.bib71.1.1" class="ltx_text" style="font-size:90%;">
Benjin Zhu, Zhengkai Jiang, Xiangxin Zhou, Zeming Li, and Gang Yu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.2.1" class="ltx_text" style="font-size:90%;">Class-balanced grouping and sampling for point cloud 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1908.09492</span><span id="bib.bib71.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock"><span id="bib.bib72.1.1" class="ltx_text" style="font-size:90%;">
Dongqiangzi Ye, Zixiang Zhou, Weijia Chen, Yufei Xie, YuÂ Wang, Panqu Wang, and
Hassan Foroosh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.2.1" class="ltx_text" style="font-size:90%;">Lidarmultinet: Towards a unified multi-task network for lidar
perception.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2209.09385</span><span id="bib.bib72.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock"><span id="bib.bib73.1.1" class="ltx_text" style="font-size:90%;">
Junho Koh, Junhyung Lee, Youngwoo Lee, Jaekyum Kim, and JunÂ Won Choi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.2.1" class="ltx_text" style="font-size:90%;">Mgtanet: Encoding sequential lidar points using long short-term
motion-guided temporal attention for 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2212.00442</span><span id="bib.bib73.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock"><span id="bib.bib74.1.1" class="ltx_text" style="font-size:90%;">
Sourabh Vora, AlexÂ H Lang, Bassam Helou, and Oscar Beijbom.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib74.2.1" class="ltx_text" style="font-size:90%;">Pointpainting: Sequential fusion for 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib74.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib74.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib74.5.3" class="ltx_text" style="font-size:90%;">, pages 4604â€“4612, 2020.
</span>
</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock"><span id="bib.bib75.1.1" class="ltx_text" style="font-size:90%;">
JinÂ Hyeok Yoo, Yecheol Kim, Jisong Kim, and JunÂ Won Choi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib75.2.1" class="ltx_text" style="font-size:90%;">3d-cvf: Generating joint camera and lidar features using cross-view
spatial feature fusion for 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib75.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib75.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</span><span id="bib.bib75.5.3" class="ltx_text" style="font-size:90%;">, pages 720â€“736.
Springer, 2020.
</span>
</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock"><span id="bib.bib76.1.1" class="ltx_text" style="font-size:90%;">
Shaoqing Xu, Dingfu Zhou, Jin Fang, Junbo Yin, Zhou Bin, and Liangjun Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib76.2.1" class="ltx_text" style="font-size:90%;">Fusionpainting: Multimodal fusion with adaptive attention for 3d
object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib76.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib76.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2021 IEEE International Intelligent Transportation Systems
Conference (ITSC)</span><span id="bib.bib76.5.3" class="ltx_text" style="font-size:90%;">, pages 3047â€“3054. IEEE, 2021.
</span>
</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock"><span id="bib.bib77.1.1" class="ltx_text" style="font-size:90%;">
Junjie Yan, Yingfei Liu, Jianjian Sun, Fan Jia, Shuailin Li, Tiancai Wang, and
Xiangyu Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib77.2.1" class="ltx_text" style="font-size:90%;">Cross modal transformer via coordinates encoding for 3d object
dectection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib77.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2301.01283</span><span id="bib.bib77.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock"><span id="bib.bib78.1.1" class="ltx_text" style="font-size:90%;">
Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel Ni, and
Harry Shum.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib78.2.1" class="ltx_text" style="font-size:90%;">Dino: Detr with improved denoising anchor boxes for end-to-end object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib78.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib78.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib78.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock"><span id="bib.bib79.1.1" class="ltx_text" style="font-size:90%;">
MMDetection3D Contributors.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib79.2.1" class="ltx_text" style="font-size:90%;">MMDetection3D: OpenMMLab next-generation platform for general 3D
object detection.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/open-mmlab/mmdetection3d" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/open-mmlab/mmdetection3d</a><span id="bib.bib79.3.1" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock"><span id="bib.bib80.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr DollÃ¡r, and CÂ Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib80.2.1" class="ltx_text" style="font-size:90%;">Microsoft coco: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib80.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib80.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib80.5.3" class="ltx_text" style="font-size:90%;">, pages 740â€“755. Springer, 2014.
</span>
</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock"><span id="bib.bib81.1.1" class="ltx_text" style="font-size:90%;">
Ziqi Pang, Zhichao Li, and Naiyan Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib81.2.1" class="ltx_text" style="font-size:90%;">Simpletrack: Understanding and rethinking 3d multi-object tracking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib81.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2111.09621</span><span id="bib.bib81.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock"><span id="bib.bib82.1.1" class="ltx_text" style="font-size:90%;">
Xinshuo Weng, Jianren Wang, David Held, and Kris Kitani.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib82.2.1" class="ltx_text" style="font-size:90%;">3d multi-object tracking: A baseline and new evaluation metrics.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib82.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib82.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IROS</span><span id="bib.bib82.5.3" class="ltx_text" style="font-size:90%;">, pages 10359â€“10366, 2020.
</span>
</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock"><span id="bib.bib83.1.1" class="ltx_text" style="font-size:90%;">
Nuri Benbarka, Jona SchrÃ¶der, and Andreas Zell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib83.2.1" class="ltx_text" style="font-size:90%;">Score refinement for confidence-based 3d multi-object tracking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib83.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib83.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IROS</span><span id="bib.bib83.5.3" class="ltx_text" style="font-size:90%;">, pages 8083â€“8090, 2021.
</span>
</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock"><span id="bib.bib84.1.1" class="ltx_text" style="font-size:90%;">
Jan-Nico Zaech, Dengxin Dai, Alexander Liniger, Martin Danelljan, and LucÂ Van
Gool.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib84.2.1" class="ltx_text" style="font-size:90%;">Learnable online graph representations for 3d multi-object tracking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib84.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib84.4.2" class="ltx_text" style="font-size:90%;">, abs/2104.11747, 2021.
</span>
</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock"><span id="bib.bib85.1.1" class="ltx_text" style="font-size:90%;">
Pei Sun, Weiyue Wang, Yuning Chai, Gamaleldin Elsayed, Alex Bewley, Xiao Zhang,
Cristian Sminchisescu, and Dragomir Anguelov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib85.2.1" class="ltx_text" style="font-size:90%;">Rsn: Range sparse net for efficient, accurate lidar 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib85.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib85.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib85.5.3" class="ltx_text" style="font-size:90%;">, pages 5725â€“5734, 2021.
</span>
</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock"><span id="bib.bib86.1.1" class="ltx_text" style="font-size:90%;">
Yihan Hu, Zhuangzhuang Ding, Runzhou Ge, Wenxin Shao, LiÂ Huang, Kun Li, and
Qiang Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib86.2.1" class="ltx_text" style="font-size:90%;">Afdetv2: Rethinking the necessity of the second stage for object
detection from point clouds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib86.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib86.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the AAAI Conference on Artificial
Intelligence</span><span id="bib.bib86.5.3" class="ltx_text" style="font-size:90%;">, volumeÂ 36, pages 969â€“979, 2022.
</span>
</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock"><span id="bib.bib87.1.1" class="ltx_text" style="font-size:90%;">
Yin Zhou, Pei Sun, YuÂ Zhang, Dragomir Anguelov, Jiyang Gao, Tom Ouyang, James
Guo, Jiquan Ngiam, and Vijay Vasudevan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib87.2.1" class="ltx_text" style="font-size:90%;">End-to-end multi-view fusion for 3d object detection in lidar point
clouds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib87.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib87.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Conference on Robot Learning</span><span id="bib.bib87.5.3" class="ltx_text" style="font-size:90%;">, pages 923â€“932. PMLR, 2020.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="Ax1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">Appendix for FocalFormer3D</h2>

<div id="Ax1.p1" class="ltx_para">
<p id="Ax1.p1.1" class="ltx_p">The supplementary materials for FocalFormer3D is organized as follows:</p>
<ul id="Ax1.I1" class="ltx_itemize">
<li id="Ax1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="Ax1.I1.i1.p1" class="ltx_para">
<p id="Ax1.I1.i1.p1.1" class="ltx_p">Sec.Â <a href="#A1" title="Appendix A Additional Ablation Studies â€£ FocalFormer3D : Focusing on Hard Instance for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a> shows additional ablation studies on decoder head and latency analysis for multi-modal models.</p>
</div>
</li>
<li id="Ax1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="Ax1.I1.i2.p1" class="ltx_para">
<p id="Ax1.I1.i2.p1.1" class="ltx_p">Sec.Â <a href="#A2" title="Appendix B Additional Implementation Details â€£ FocalFormer3D : Focusing on Hard Instance for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a> gives more implementation details including network details, and extension to the multi-modal variant.</p>
</div>
</li>
<li id="Ax1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="Ax1.I1.i3.p1" class="ltx_para">
<p id="Ax1.I1.i3.p1.1" class="ltx_p">Sec.Â <a href="#A3" title="Appendix C Prediction Locality of Second-Stage Refinement â€£ FocalFormer3D : Focusing on Hard Instance for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a> discusses the <span id="Ax1.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">prediction locality</span> for second-stage refinements.</p>
</div>
</li>
<li id="Ax1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="Ax1.I1.i4.p1" class="ltx_para">
<p id="Ax1.I1.i4.p1.1" class="ltx_p">Sec.Â <a href="#A4" title="Appendix D Example Visualization â€£ FocalFormer3D : Focusing on Hard Instance for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a> presents some visual results for multi-stage heatmaps and 3D detection results on birdâ€™s eye view.
</p>
</div>
</li>
</ul>
</div>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Additional Ablation Studies</h2>

<div id="A1.p1" class="ltx_para ltx_noindent">
<p id="A1.p1.1" class="ltx_p"><span id="A1.p1.1.1" class="ltx_text ltx_font_bold">Design of the decoder head.</span> We analyze the capability of the decoder head in processing massive queries in TableÂ <a href="#A1.T8" title="Table 8 â€£ Appendix A Additional Ablation Studies â€£ FocalFormer3D : Focusing on Hard Instance for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. Concerning the type of cross attention, with an increasing number of queries up to 600, the computation time of cross attention moduleÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite> (c) grows faster than deformable oneÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> (e). As a result, more deformable transformer layers can be applied. In our experiments, the transformer decoder head with 6 layers obtains the best performance (66.5 mAP and 71.1 NDS) with a more affordable computation time than the cross attention modules. Furthermore, compared with point-level query embeddingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> (g), our box-level query embedding (f) achieves +0.6 points improvements with <math id="A1.p1.1.m1.1" class="ltx_Math" alttext="3.7ms" display="inline"><semantics id="A1.p1.1.m1.1a"><mrow id="A1.p1.1.m1.1.1" xref="A1.p1.1.m1.1.1.cmml"><mn id="A1.p1.1.m1.1.1.2" xref="A1.p1.1.m1.1.1.2.cmml">3.7</mn><mo lspace="0em" rspace="0em" id="A1.p1.1.m1.1.1.1" xref="A1.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="A1.p1.1.m1.1.1.3" xref="A1.p1.1.m1.1.1.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="A1.p1.1.m1.1.1.1a" xref="A1.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="A1.p1.1.m1.1.1.4" xref="A1.p1.1.m1.1.1.4.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.p1.1.m1.1b"><apply id="A1.p1.1.m1.1.1.cmml" xref="A1.p1.1.m1.1.1"><times id="A1.p1.1.m1.1.1.1.cmml" xref="A1.p1.1.m1.1.1.1"></times><cn type="float" id="A1.p1.1.m1.1.1.2.cmml" xref="A1.p1.1.m1.1.1.2">3.7</cn><ci id="A1.p1.1.m1.1.1.3.cmml" xref="A1.p1.1.m1.1.1.3">ğ‘š</ci><ci id="A1.p1.1.m1.1.1.4.cmml" xref="A1.p1.1.m1.1.1.4">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.1.m1.1c">3.7ms</annotation></semantics></math> computation overhead, demonstrating the effectiveness of box-level query.</p>
</div>
<figure id="A1.T8" class="ltx_table">
<div id="A1.T8.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:240.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(86.8pt,-48.1pt) scale(1.66758845192128,1.66758845192128) ;">
<table id="A1.T8.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T8.1.1.1.1" class="ltx_tr">
<th id="A1.T8.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">#</th>
<th id="A1.T8.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">C.A.</th>
<th id="A1.T8.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">#Q</th>
<th id="A1.T8.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">#Layer</th>
<th id="A1.T8.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">mAP</th>
<th id="A1.T8.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">NDS</th>
<th id="A1.T8.1.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Latency</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T8.1.1.2.1" class="ltx_tr">
<td id="A1.T8.1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t">(a)</td>
<td id="A1.T8.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">Full</td>
<td id="A1.T8.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">200</td>
<td id="A1.T8.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">1</td>
<td id="A1.T8.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">65.8</td>
<td id="A1.T8.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">70.5</td>
<td id="A1.T8.1.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t">7.6ms</td>
</tr>
<tr id="A1.T8.1.1.3.2" class="ltx_tr">
<td id="A1.T8.1.1.3.2.1" class="ltx_td ltx_align_center">(b)</td>
<td id="A1.T8.1.1.3.2.2" class="ltx_td ltx_align_center">Full</td>
<td id="A1.T8.1.1.3.2.3" class="ltx_td ltx_align_center">600</td>
<td id="A1.T8.1.1.3.2.4" class="ltx_td ltx_align_center">1</td>
<td id="A1.T8.1.1.3.2.5" class="ltx_td ltx_align_center">66.1</td>
<td id="A1.T8.1.1.3.2.6" class="ltx_td ltx_align_center">70.9</td>
<td id="A1.T8.1.1.3.2.7" class="ltx_td ltx_align_center">13.1ms</td>
</tr>
<tr id="A1.T8.1.1.4.3" class="ltx_tr">
<td id="A1.T8.1.1.4.3.1" class="ltx_td ltx_align_center">(c)</td>
<td id="A1.T8.1.1.4.3.2" class="ltx_td ltx_align_center">Full</td>
<td id="A1.T8.1.1.4.3.3" class="ltx_td ltx_align_center">600</td>
<td id="A1.T8.1.1.4.3.4" class="ltx_td ltx_align_center">2</td>
<td id="A1.T8.1.1.4.3.5" class="ltx_td ltx_align_center">66.3</td>
<td id="A1.T8.1.1.4.3.6" class="ltx_td ltx_align_center">71.1</td>
<td id="A1.T8.1.1.4.3.7" class="ltx_td ltx_align_center">26.2ms</td>
</tr>
<tr id="A1.T8.1.1.5.4" class="ltx_tr">
<td id="A1.T8.1.1.5.4.1" class="ltx_td ltx_align_center">(d)</td>
<td id="A1.T8.1.1.5.4.2" class="ltx_td ltx_align_center">Deform</td>
<td id="A1.T8.1.1.5.4.3" class="ltx_td ltx_align_center">200</td>
<td id="A1.T8.1.1.5.4.4" class="ltx_td ltx_align_center">6</td>
<td id="A1.T8.1.1.5.4.5" class="ltx_td ltx_align_center">65.9</td>
<td id="A1.T8.1.1.5.4.6" class="ltx_td ltx_align_center">70.8</td>
<td id="A1.T8.1.1.5.4.7" class="ltx_td ltx_align_center">14.8ms</td>
</tr>
<tr id="A1.T8.1.1.6.5" class="ltx_tr">
<td id="A1.T8.1.1.6.5.1" class="ltx_td ltx_align_center">(e)</td>
<td id="A1.T8.1.1.6.5.2" class="ltx_td ltx_align_center">Deform</td>
<td id="A1.T8.1.1.6.5.3" class="ltx_td ltx_align_center">600</td>
<td id="A1.T8.1.1.6.5.4" class="ltx_td ltx_align_center">2</td>
<td id="A1.T8.1.1.6.5.5" class="ltx_td ltx_align_center">66.2</td>
<td id="A1.T8.1.1.6.5.6" class="ltx_td ltx_align_center">70.7</td>
<td id="A1.T8.1.1.6.5.7" class="ltx_td ltx_align_center">7.6ms</td>
</tr>
<tr id="A1.T8.1.1.7.6" class="ltx_tr">
<td id="A1.T8.1.1.7.6.1" class="ltx_td ltx_align_center">(f)</td>
<td id="A1.T8.1.1.7.6.2" class="ltx_td ltx_align_center">Deform</td>
<td id="A1.T8.1.1.7.6.3" class="ltx_td ltx_align_center">600</td>
<td id="A1.T8.1.1.7.6.4" class="ltx_td ltx_align_center">6</td>
<td id="A1.T8.1.1.7.6.5" class="ltx_td ltx_align_center">66.5</td>
<td id="A1.T8.1.1.7.6.6" class="ltx_td ltx_align_center">71.1</td>
<td id="A1.T8.1.1.7.6.7" class="ltx_td ltx_align_center">17.0ms</td>
</tr>
<tr id="A1.T8.1.1.8.7" class="ltx_tr">
<td id="A1.T8.1.1.8.7.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">(g)</td>
<td id="A1.T8.1.1.8.7.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" colspan="3">w/o Box-pooling</td>
<td id="A1.T8.1.1.8.7.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">65.9</td>
<td id="A1.T8.1.1.8.7.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">70.9</td>
<td id="A1.T8.1.1.8.7.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">â€“</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 8: </span><span id="A1.T8.3.1" class="ltx_text ltx_font_bold">Ablation studies for box-level deformable decoder head.</span> â€œC.A.â€ denotes the types of cross attention layers. â€œ# Qâ€ represents the number of used queries. â€œ# Layerâ€ stands for the number of decoder layers. Latency is measured for the transformer decoder head on a V100 GPU for reference.</figcaption>
</figure>
<div id="A1.p2" class="ltx_para ltx_noindent">
<p id="A1.p2.1" class="ltx_p"><span id="A1.p2.1.1" class="ltx_text ltx_font_bold">Latency analysis.</span>
We compare ours with other leading-performance methods in TableÂ <a href="#A1.T9" title="Table 9 â€£ Appendix A Additional Ablation Studies â€£ FocalFormer3D : Focusing on Hard Instance for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>. It shows that FocalFormer-F outperforms the dominating methods, BEVFusionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and DeepInteractionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> in terms of both performance and efficiency.</p>
</div>
<figure id="A1.T9" class="ltx_table">
<table id="A1.T9.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T9.1.1.1" class="ltx_tr">
<th id="A1.T9.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Methods</th>
<th id="A1.T9.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">mAP</th>
<th id="A1.T9.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">NDS</th>
<th id="A1.T9.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Latency</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T9.1.2.1" class="ltx_tr">
<th id="A1.T9.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">BEVFusionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>
</th>
<td id="A1.T9.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">69.2</td>
<td id="A1.T9.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">71.8</td>
<td id="A1.T9.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">1610ms</td>
</tr>
<tr id="A1.T9.1.3.2" class="ltx_tr">
<th id="A1.T9.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">DeepInteractionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>
</th>
<td id="A1.T9.1.3.2.2" class="ltx_td ltx_align_center">70.8</td>
<td id="A1.T9.1.3.2.3" class="ltx_td ltx_align_center">73.4</td>
<td id="A1.T9.1.3.2.4" class="ltx_td ltx_align_center">480ms</td>
</tr>
<tr id="A1.T9.1.4.3" class="ltx_tr">
<th id="A1.T9.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">FocalFormer3D-F (Ours)</th>
<td id="A1.T9.1.4.3.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="A1.T9.1.4.3.2.1" class="ltx_text ltx_font_bold">71.6</span></td>
<td id="A1.T9.1.4.3.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="A1.T9.1.4.3.3.1" class="ltx_text ltx_font_bold">73.9</span></td>
<td id="A1.T9.1.4.3.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="A1.T9.1.4.3.4.1" class="ltx_text ltx_font_bold">363ms</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 9: </span><span id="A1.T9.3.1" class="ltx_text ltx_font_bold">Efficiency comparison with other SOTA methods on nuScenes dataset.</span> Results are shown on nuScenes test set. All methods are tested on a single V100 GPU for reference.</figcaption>
</figure>
<div id="A1.p3" class="ltx_para ltx_noindent">
<p id="A1.p3.1" class="ltx_p"><span id="A1.p3.1.1" class="ltx_text ltx_font_bold">Results on nuScenes val set.</span>
We also report the method comparisons on the nuScenes <span id="A1.p3.1.2" class="ltx_text ltx_font_italic">val</span> set in TableÂ <a href="#A1.T10" title="Table 10 â€£ Appendix A Additional Ablation Studies â€£ FocalFormer3D : Focusing on Hard Instance for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>.</p>
</div>
<figure id="A1.T10" class="ltx_table">
<table id="A1.T10.1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T10.1.1.2.1" class="ltx_tr">
<th id="A1.T10.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Methods</th>
<th id="A1.T10.1.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">mAP</th>
<th id="A1.T10.1.1.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">NDS</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T10.1.1.3.1" class="ltx_tr">
<th id="A1.T10.1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">CBGSÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>
</th>
<td id="A1.T10.1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t">51.4</td>
<td id="A1.T10.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">62.6</td>
</tr>
<tr id="A1.T10.1.1.4.2" class="ltx_tr">
<th id="A1.T10.1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">CenterPointÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>
</th>
<td id="A1.T10.1.1.4.2.2" class="ltx_td ltx_align_center">59.6</td>
<td id="A1.T10.1.1.4.2.3" class="ltx_td ltx_align_center">66.8</td>
</tr>
<tr id="A1.T10.1.1.5.3" class="ltx_tr">
<th id="A1.T10.1.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">LiDARMultiNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite>
</th>
<td id="A1.T10.1.1.5.3.2" class="ltx_td ltx_align_center">63.8</td>
<td id="A1.T10.1.1.5.3.3" class="ltx_td ltx_align_center">69.5</td>
</tr>
<tr id="A1.T10.1.1.1" class="ltx_tr">
<th id="A1.T10.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">TransFusion-L<sup id="A1.T10.1.1.1.1.1" class="ltx_sup">âˆ§</sup>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>
</th>
<td id="A1.T10.1.1.1.2" class="ltx_td ltx_align_center">65.2</td>
<td id="A1.T10.1.1.1.3" class="ltx_td ltx_align_center">70.2</td>
</tr>
<tr id="A1.T10.1.1.6.4" class="ltx_tr">
<th id="A1.T10.1.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">FocalFormer3D (Ours)</th>
<td id="A1.T10.1.1.6.4.2" class="ltx_td ltx_align_center ltx_border_bb">66.5</td>
<td id="A1.T10.1.1.6.4.3" class="ltx_td ltx_align_center ltx_border_bb">71.1</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 10: </span><span id="A1.T10.7.1" class="ltx_text ltx_font_bold">Performance comparison on the nuScenes <span id="A1.T10.7.1.1" class="ltx_text ltx_font_italic">val</span> set.</span> Results marked with <sup id="A1.T10.8.2" class="ltx_sup">âˆ§</sup> indicate our reproduction. The results of other compared methods on the nuScenes <span id="A1.T10.9.3" class="ltx_text ltx_font_italic">val</span> set were obtained from their respective original papers.</figcaption>
</figure>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Additional Implementation Details</h2>

<div id="A2.p1" class="ltx_para ltx_noindent">
<p id="A2.p1.8" class="ltx_p"><span id="A2.p1.8.1" class="ltx_text ltx_font_bold">Model details for nuScenes dataset.</span>
On the nuScenes dataset, the voxel size is set as <math id="A2.p1.1.m1.1" class="ltx_Math" alttext="0.075m\times 0.075m\times 0.2m" display="inline"><semantics id="A2.p1.1.m1.1a"><mrow id="A2.p1.1.m1.1.1" xref="A2.p1.1.m1.1.1.cmml"><mrow id="A2.p1.1.m1.1.1.2" xref="A2.p1.1.m1.1.1.2.cmml"><mrow id="A2.p1.1.m1.1.1.2.2" xref="A2.p1.1.m1.1.1.2.2.cmml"><mrow id="A2.p1.1.m1.1.1.2.2.2" xref="A2.p1.1.m1.1.1.2.2.2.cmml"><mrow id="A2.p1.1.m1.1.1.2.2.2.2" xref="A2.p1.1.m1.1.1.2.2.2.2.cmml"><mn id="A2.p1.1.m1.1.1.2.2.2.2.2" xref="A2.p1.1.m1.1.1.2.2.2.2.2.cmml">0.075</mn><mo lspace="0em" rspace="0em" id="A2.p1.1.m1.1.1.2.2.2.2.1" xref="A2.p1.1.m1.1.1.2.2.2.2.1.cmml">â€‹</mo><mi id="A2.p1.1.m1.1.1.2.2.2.2.3" xref="A2.p1.1.m1.1.1.2.2.2.2.3.cmml">m</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="A2.p1.1.m1.1.1.2.2.2.1" xref="A2.p1.1.m1.1.1.2.2.2.1.cmml">Ã—</mo><mn id="A2.p1.1.m1.1.1.2.2.2.3" xref="A2.p1.1.m1.1.1.2.2.2.3.cmml">0.075</mn></mrow><mo lspace="0em" rspace="0em" id="A2.p1.1.m1.1.1.2.2.1" xref="A2.p1.1.m1.1.1.2.2.1.cmml">â€‹</mo><mi id="A2.p1.1.m1.1.1.2.2.3" xref="A2.p1.1.m1.1.1.2.2.3.cmml">m</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="A2.p1.1.m1.1.1.2.1" xref="A2.p1.1.m1.1.1.2.1.cmml">Ã—</mo><mn id="A2.p1.1.m1.1.1.2.3" xref="A2.p1.1.m1.1.1.2.3.cmml">0.2</mn></mrow><mo lspace="0em" rspace="0em" id="A2.p1.1.m1.1.1.1" xref="A2.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="A2.p1.1.m1.1.1.3" xref="A2.p1.1.m1.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.p1.1.m1.1b"><apply id="A2.p1.1.m1.1.1.cmml" xref="A2.p1.1.m1.1.1"><times id="A2.p1.1.m1.1.1.1.cmml" xref="A2.p1.1.m1.1.1.1"></times><apply id="A2.p1.1.m1.1.1.2.cmml" xref="A2.p1.1.m1.1.1.2"><times id="A2.p1.1.m1.1.1.2.1.cmml" xref="A2.p1.1.m1.1.1.2.1"></times><apply id="A2.p1.1.m1.1.1.2.2.cmml" xref="A2.p1.1.m1.1.1.2.2"><times id="A2.p1.1.m1.1.1.2.2.1.cmml" xref="A2.p1.1.m1.1.1.2.2.1"></times><apply id="A2.p1.1.m1.1.1.2.2.2.cmml" xref="A2.p1.1.m1.1.1.2.2.2"><times id="A2.p1.1.m1.1.1.2.2.2.1.cmml" xref="A2.p1.1.m1.1.1.2.2.2.1"></times><apply id="A2.p1.1.m1.1.1.2.2.2.2.cmml" xref="A2.p1.1.m1.1.1.2.2.2.2"><times id="A2.p1.1.m1.1.1.2.2.2.2.1.cmml" xref="A2.p1.1.m1.1.1.2.2.2.2.1"></times><cn type="float" id="A2.p1.1.m1.1.1.2.2.2.2.2.cmml" xref="A2.p1.1.m1.1.1.2.2.2.2.2">0.075</cn><ci id="A2.p1.1.m1.1.1.2.2.2.2.3.cmml" xref="A2.p1.1.m1.1.1.2.2.2.2.3">ğ‘š</ci></apply><cn type="float" id="A2.p1.1.m1.1.1.2.2.2.3.cmml" xref="A2.p1.1.m1.1.1.2.2.2.3">0.075</cn></apply><ci id="A2.p1.1.m1.1.1.2.2.3.cmml" xref="A2.p1.1.m1.1.1.2.2.3">ğ‘š</ci></apply><cn type="float" id="A2.p1.1.m1.1.1.2.3.cmml" xref="A2.p1.1.m1.1.1.2.3">0.2</cn></apply><ci id="A2.p1.1.m1.1.1.3.cmml" xref="A2.p1.1.m1.1.1.3">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.1.m1.1c">0.075m\times 0.075m\times 0.2m</annotation></semantics></math> and the detection range is set to [<math id="A2.p1.2.m2.1" class="ltx_Math" alttext="-54.0m" display="inline"><semantics id="A2.p1.2.m2.1a"><mrow id="A2.p1.2.m2.1.1" xref="A2.p1.2.m2.1.1.cmml"><mo id="A2.p1.2.m2.1.1a" xref="A2.p1.2.m2.1.1.cmml">âˆ’</mo><mrow id="A2.p1.2.m2.1.1.2" xref="A2.p1.2.m2.1.1.2.cmml"><mn id="A2.p1.2.m2.1.1.2.2" xref="A2.p1.2.m2.1.1.2.2.cmml">54.0</mn><mo lspace="0em" rspace="0em" id="A2.p1.2.m2.1.1.2.1" xref="A2.p1.2.m2.1.1.2.1.cmml">â€‹</mo><mi id="A2.p1.2.m2.1.1.2.3" xref="A2.p1.2.m2.1.1.2.3.cmml">m</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.p1.2.m2.1b"><apply id="A2.p1.2.m2.1.1.cmml" xref="A2.p1.2.m2.1.1"><minus id="A2.p1.2.m2.1.1.1.cmml" xref="A2.p1.2.m2.1.1"></minus><apply id="A2.p1.2.m2.1.1.2.cmml" xref="A2.p1.2.m2.1.1.2"><times id="A2.p1.2.m2.1.1.2.1.cmml" xref="A2.p1.2.m2.1.1.2.1"></times><cn type="float" id="A2.p1.2.m2.1.1.2.2.cmml" xref="A2.p1.2.m2.1.1.2.2">54.0</cn><ci id="A2.p1.2.m2.1.1.2.3.cmml" xref="A2.p1.2.m2.1.1.2.3">ğ‘š</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.2.m2.1c">-54.0m</annotation></semantics></math>, <math id="A2.p1.3.m3.1" class="ltx_Math" alttext="54.0m" display="inline"><semantics id="A2.p1.3.m3.1a"><mrow id="A2.p1.3.m3.1.1" xref="A2.p1.3.m3.1.1.cmml"><mn id="A2.p1.3.m3.1.1.2" xref="A2.p1.3.m3.1.1.2.cmml">54.0</mn><mo lspace="0em" rspace="0em" id="A2.p1.3.m3.1.1.1" xref="A2.p1.3.m3.1.1.1.cmml">â€‹</mo><mi id="A2.p1.3.m3.1.1.3" xref="A2.p1.3.m3.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.p1.3.m3.1b"><apply id="A2.p1.3.m3.1.1.cmml" xref="A2.p1.3.m3.1.1"><times id="A2.p1.3.m3.1.1.1.cmml" xref="A2.p1.3.m3.1.1.1"></times><cn type="float" id="A2.p1.3.m3.1.1.2.cmml" xref="A2.p1.3.m3.1.1.2">54.0</cn><ci id="A2.p1.3.m3.1.1.3.cmml" xref="A2.p1.3.m3.1.1.3">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.3.m3.1c">54.0m</annotation></semantics></math>] along <math id="A2.p1.4.m4.1" class="ltx_Math" alttext="X" display="inline"><semantics id="A2.p1.4.m4.1a"><mi id="A2.p1.4.m4.1.1" xref="A2.p1.4.m4.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="A2.p1.4.m4.1b"><ci id="A2.p1.4.m4.1.1.cmml" xref="A2.p1.4.m4.1.1">ğ‘‹</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.4.m4.1c">X</annotation></semantics></math> and <math id="A2.p1.5.m5.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="A2.p1.5.m5.1a"><mi id="A2.p1.5.m5.1.1" xref="A2.p1.5.m5.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="A2.p1.5.m5.1b"><ci id="A2.p1.5.m5.1.1.cmml" xref="A2.p1.5.m5.1.1">ğ‘Œ</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.5.m5.1c">Y</annotation></semantics></math> axes, and [<math id="A2.p1.6.m6.1" class="ltx_Math" alttext="-5.0m" display="inline"><semantics id="A2.p1.6.m6.1a"><mrow id="A2.p1.6.m6.1.1" xref="A2.p1.6.m6.1.1.cmml"><mo id="A2.p1.6.m6.1.1a" xref="A2.p1.6.m6.1.1.cmml">âˆ’</mo><mrow id="A2.p1.6.m6.1.1.2" xref="A2.p1.6.m6.1.1.2.cmml"><mn id="A2.p1.6.m6.1.1.2.2" xref="A2.p1.6.m6.1.1.2.2.cmml">5.0</mn><mo lspace="0em" rspace="0em" id="A2.p1.6.m6.1.1.2.1" xref="A2.p1.6.m6.1.1.2.1.cmml">â€‹</mo><mi id="A2.p1.6.m6.1.1.2.3" xref="A2.p1.6.m6.1.1.2.3.cmml">m</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.p1.6.m6.1b"><apply id="A2.p1.6.m6.1.1.cmml" xref="A2.p1.6.m6.1.1"><minus id="A2.p1.6.m6.1.1.1.cmml" xref="A2.p1.6.m6.1.1"></minus><apply id="A2.p1.6.m6.1.1.2.cmml" xref="A2.p1.6.m6.1.1.2"><times id="A2.p1.6.m6.1.1.2.1.cmml" xref="A2.p1.6.m6.1.1.2.1"></times><cn type="float" id="A2.p1.6.m6.1.1.2.2.cmml" xref="A2.p1.6.m6.1.1.2.2">5.0</cn><ci id="A2.p1.6.m6.1.1.2.3.cmml" xref="A2.p1.6.m6.1.1.2.3">ğ‘š</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.6.m6.1c">-5.0m</annotation></semantics></math>, <math id="A2.p1.7.m7.1" class="ltx_Math" alttext="3.0m" display="inline"><semantics id="A2.p1.7.m7.1a"><mrow id="A2.p1.7.m7.1.1" xref="A2.p1.7.m7.1.1.cmml"><mn id="A2.p1.7.m7.1.1.2" xref="A2.p1.7.m7.1.1.2.cmml">3.0</mn><mo lspace="0em" rspace="0em" id="A2.p1.7.m7.1.1.1" xref="A2.p1.7.m7.1.1.1.cmml">â€‹</mo><mi id="A2.p1.7.m7.1.1.3" xref="A2.p1.7.m7.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.p1.7.m7.1b"><apply id="A2.p1.7.m7.1.1.cmml" xref="A2.p1.7.m7.1.1"><times id="A2.p1.7.m7.1.1.1.cmml" xref="A2.p1.7.m7.1.1.1"></times><cn type="float" id="A2.p1.7.m7.1.1.2.cmml" xref="A2.p1.7.m7.1.1.2">3.0</cn><ci id="A2.p1.7.m7.1.1.3.cmml" xref="A2.p1.7.m7.1.1.3">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.7.m7.1c">3.0m</annotation></semantics></math>] along <math id="A2.p1.8.m8.1" class="ltx_Math" alttext="Z" display="inline"><semantics id="A2.p1.8.m8.1a"><mi id="A2.p1.8.m8.1.1" xref="A2.p1.8.m8.1.1.cmml">Z</mi><annotation-xml encoding="MathML-Content" id="A2.p1.8.m8.1b"><ci id="A2.p1.8.m8.1.1.cmml" xref="A2.p1.8.m8.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.8.m8.1c">Z</annotation></semantics></math> axis.We follow the common practice of accumulating the past 9 frames to the current frame for both training and validation. We train the LiDAR backbone with the deformable transformer decoder head for 20 epochs. Then, we freeze the pre-trained LiDAR backbones and train the detection head with multi-stage focal heatmaps for another 6 epochs. GT sample augmentation is adopted except for the last 5 epochs. We adopt pooling-based masking for generating Accumulated Positive Mask, where we simply select <span id="A2.p1.8.2" class="ltx_text ltx_font_slanted">Pedestrian</span> and <span id="A2.p1.8.3" class="ltx_text ltx_font_slanted">Traffic Cones</span> as the small objects.</p>
</div>
<div id="A2.p2" class="ltx_para ltx_noindent">
<p id="A2.p2.1" class="ltx_p"><span id="A2.p2.1.1" class="ltx_text ltx_font_bold">Model details for Waymo dataset.</span>
On the Waymo dataset, we simply keep the VoxelNet backbone and FocalFormer3D detector head the same as those used for the nuScenes dataset. The voxel size used for the Waymo dataset is set to <math id="A2.p2.1.m1.1" class="ltx_Math" alttext="0.1m\times 0.1m\times 0.15m" display="inline"><semantics id="A2.p2.1.m1.1a"><mrow id="A2.p2.1.m1.1.1" xref="A2.p2.1.m1.1.1.cmml"><mrow id="A2.p2.1.m1.1.1.2" xref="A2.p2.1.m1.1.1.2.cmml"><mrow id="A2.p2.1.m1.1.1.2.2" xref="A2.p2.1.m1.1.1.2.2.cmml"><mrow id="A2.p2.1.m1.1.1.2.2.2" xref="A2.p2.1.m1.1.1.2.2.2.cmml"><mrow id="A2.p2.1.m1.1.1.2.2.2.2" xref="A2.p2.1.m1.1.1.2.2.2.2.cmml"><mn id="A2.p2.1.m1.1.1.2.2.2.2.2" xref="A2.p2.1.m1.1.1.2.2.2.2.2.cmml">0.1</mn><mo lspace="0em" rspace="0em" id="A2.p2.1.m1.1.1.2.2.2.2.1" xref="A2.p2.1.m1.1.1.2.2.2.2.1.cmml">â€‹</mo><mi id="A2.p2.1.m1.1.1.2.2.2.2.3" xref="A2.p2.1.m1.1.1.2.2.2.2.3.cmml">m</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="A2.p2.1.m1.1.1.2.2.2.1" xref="A2.p2.1.m1.1.1.2.2.2.1.cmml">Ã—</mo><mn id="A2.p2.1.m1.1.1.2.2.2.3" xref="A2.p2.1.m1.1.1.2.2.2.3.cmml">0.1</mn></mrow><mo lspace="0em" rspace="0em" id="A2.p2.1.m1.1.1.2.2.1" xref="A2.p2.1.m1.1.1.2.2.1.cmml">â€‹</mo><mi id="A2.p2.1.m1.1.1.2.2.3" xref="A2.p2.1.m1.1.1.2.2.3.cmml">m</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="A2.p2.1.m1.1.1.2.1" xref="A2.p2.1.m1.1.1.2.1.cmml">Ã—</mo><mn id="A2.p2.1.m1.1.1.2.3" xref="A2.p2.1.m1.1.1.2.3.cmml">0.15</mn></mrow><mo lspace="0em" rspace="0em" id="A2.p2.1.m1.1.1.1" xref="A2.p2.1.m1.1.1.1.cmml">â€‹</mo><mi id="A2.p2.1.m1.1.1.3" xref="A2.p2.1.m1.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.p2.1.m1.1b"><apply id="A2.p2.1.m1.1.1.cmml" xref="A2.p2.1.m1.1.1"><times id="A2.p2.1.m1.1.1.1.cmml" xref="A2.p2.1.m1.1.1.1"></times><apply id="A2.p2.1.m1.1.1.2.cmml" xref="A2.p2.1.m1.1.1.2"><times id="A2.p2.1.m1.1.1.2.1.cmml" xref="A2.p2.1.m1.1.1.2.1"></times><apply id="A2.p2.1.m1.1.1.2.2.cmml" xref="A2.p2.1.m1.1.1.2.2"><times id="A2.p2.1.m1.1.1.2.2.1.cmml" xref="A2.p2.1.m1.1.1.2.2.1"></times><apply id="A2.p2.1.m1.1.1.2.2.2.cmml" xref="A2.p2.1.m1.1.1.2.2.2"><times id="A2.p2.1.m1.1.1.2.2.2.1.cmml" xref="A2.p2.1.m1.1.1.2.2.2.1"></times><apply id="A2.p2.1.m1.1.1.2.2.2.2.cmml" xref="A2.p2.1.m1.1.1.2.2.2.2"><times id="A2.p2.1.m1.1.1.2.2.2.2.1.cmml" xref="A2.p2.1.m1.1.1.2.2.2.2.1"></times><cn type="float" id="A2.p2.1.m1.1.1.2.2.2.2.2.cmml" xref="A2.p2.1.m1.1.1.2.2.2.2.2">0.1</cn><ci id="A2.p2.1.m1.1.1.2.2.2.2.3.cmml" xref="A2.p2.1.m1.1.1.2.2.2.2.3">ğ‘š</ci></apply><cn type="float" id="A2.p2.1.m1.1.1.2.2.2.3.cmml" xref="A2.p2.1.m1.1.1.2.2.2.3">0.1</cn></apply><ci id="A2.p2.1.m1.1.1.2.2.3.cmml" xref="A2.p2.1.m1.1.1.2.2.3">ğ‘š</ci></apply><cn type="float" id="A2.p2.1.m1.1.1.2.3.cmml" xref="A2.p2.1.m1.1.1.2.3">0.15</cn></apply><ci id="A2.p2.1.m1.1.1.3.cmml" xref="A2.p2.1.m1.1.1.3">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p2.1.m1.1c">0.1m\times 0.1m\times 0.15m</annotation></semantics></math>. For the multi-stage heatmap encoder, we use pooling-based masking, selecting <span id="A2.p2.1.2" class="ltx_text ltx_font_slanted">Vehicle</span> as the large object category, and <span id="A2.p2.1.3" class="ltx_text ltx_font_slanted">Pedestrain</span> and <span id="A2.p2.1.4" class="ltx_text ltx_font_slanted">Cyclist</span> as the small object categories. The training process involves two stages, with the model trained for 36 epochs and another 11 epochs trained for the FocalFormer3D detector. We adopt GT sample augmentation during training, except for the last 6 epochs. As the Waymo dataset provides denser point clouds than nuScenes, the models adopt single-frame point cloud inputÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
<div id="A2.p3" class="ltx_para ltx_noindent">
<p id="A2.p3.2" class="ltx_p"><span id="A2.p3.2.1" class="ltx_text ltx_font_bold">Extension to multi-modal fusion model.</span>
We provide more details on the extension of FocalFormer3D to its multi-modal variant. Specifically, the image backbone network utilized is ResNet-50 following TransFusionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Rather than using more heavy camera projection techniques such as Lift-split-shotÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> or BEVFormerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, we project multi-view camera features onto a predefined voxel grid in the 3D spaceÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. The BEV size of the voxel grid is set to <math id="A2.p3.1.m1.1" class="ltx_Math" alttext="180\times 180" display="inline"><semantics id="A2.p3.1.m1.1a"><mrow id="A2.p3.1.m1.1.1" xref="A2.p3.1.m1.1.1.cmml"><mn id="A2.p3.1.m1.1.1.2" xref="A2.p3.1.m1.1.1.2.cmml">180</mn><mo lspace="0.222em" rspace="0.222em" id="A2.p3.1.m1.1.1.1" xref="A2.p3.1.m1.1.1.1.cmml">Ã—</mo><mn id="A2.p3.1.m1.1.1.3" xref="A2.p3.1.m1.1.1.3.cmml">180</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.p3.1.m1.1b"><apply id="A2.p3.1.m1.1.1.cmml" xref="A2.p3.1.m1.1.1"><times id="A2.p3.1.m1.1.1.1.cmml" xref="A2.p3.1.m1.1.1.1"></times><cn type="integer" id="A2.p3.1.m1.1.1.2.cmml" xref="A2.p3.1.m1.1.1.2">180</cn><cn type="integer" id="A2.p3.1.m1.1.1.3.cmml" xref="A2.p3.1.m1.1.1.3">180</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p3.1.m1.1c">180\times 180</annotation></semantics></math>, in line with <math id="A2.p3.2.m2.1" class="ltx_math_unparsed" alttext="8\times" display="inline"><semantics id="A2.p3.2.m2.1a"><mrow id="A2.p3.2.m2.1b"><mn id="A2.p3.2.m2.1.1">8</mn><mo lspace="0.222em" id="A2.p3.2.m2.1.2">Ã—</mo></mrow><annotation encoding="application/x-tex" id="A2.p3.2.m2.1c">8\times</annotation></semantics></math> downsampled BEV features produced by VoxelNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. The height of the voxel grid is fixed at 10.</p>
</div>
<div id="A2.p4" class="ltx_para">
<p id="A2.p4.1" class="ltx_p">To obtain camera features for BEV LiDAR feature, we adopt a cross-attention moduleÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> within each pillar. This module views each BEV pixel feature as the query and the projected camera grid features as both the key and value. The generated camera BEV features are then fused with LiDAR BEV features by an extra convolutional layer. This multi-modal fusion is conducted at each stage for the multi-stage heatmap encoder. We leave the exploration of stronger fusion techniquesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> as future work.</p>
</div>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Prediction Locality of Second-Stage Refinement</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.3" class="ltx_p">Recent 3D detectors have implemented global attention modulesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> or fusion with multi-view cameraÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> to capture larger context information and improve the detection accuracy. However, we observe a limited regression range (named as <span id="A3.p1.3.1" class="ltx_text ltx_font_italic">prediction locality</span>) compared to the initial heatmap prediction. To analyze their second-stage ability to compensate for the missing detection (false negatives), we visualize the distribution of their predicted center shifts <math id="A3.p1.1.m1.2" class="ltx_Math" alttext="\delta=(\delta_{x},\delta_{y})" display="inline"><semantics id="A3.p1.1.m1.2a"><mrow id="A3.p1.1.m1.2.2" xref="A3.p1.1.m1.2.2.cmml"><mi id="A3.p1.1.m1.2.2.4" xref="A3.p1.1.m1.2.2.4.cmml">Î´</mi><mo id="A3.p1.1.m1.2.2.3" xref="A3.p1.1.m1.2.2.3.cmml">=</mo><mrow id="A3.p1.1.m1.2.2.2.2" xref="A3.p1.1.m1.2.2.2.3.cmml"><mo stretchy="false" id="A3.p1.1.m1.2.2.2.2.3" xref="A3.p1.1.m1.2.2.2.3.cmml">(</mo><msub id="A3.p1.1.m1.1.1.1.1.1" xref="A3.p1.1.m1.1.1.1.1.1.cmml"><mi id="A3.p1.1.m1.1.1.1.1.1.2" xref="A3.p1.1.m1.1.1.1.1.1.2.cmml">Î´</mi><mi id="A3.p1.1.m1.1.1.1.1.1.3" xref="A3.p1.1.m1.1.1.1.1.1.3.cmml">x</mi></msub><mo id="A3.p1.1.m1.2.2.2.2.4" xref="A3.p1.1.m1.2.2.2.3.cmml">,</mo><msub id="A3.p1.1.m1.2.2.2.2.2" xref="A3.p1.1.m1.2.2.2.2.2.cmml"><mi id="A3.p1.1.m1.2.2.2.2.2.2" xref="A3.p1.1.m1.2.2.2.2.2.2.cmml">Î´</mi><mi id="A3.p1.1.m1.2.2.2.2.2.3" xref="A3.p1.1.m1.2.2.2.2.2.3.cmml">y</mi></msub><mo stretchy="false" id="A3.p1.1.m1.2.2.2.2.5" xref="A3.p1.1.m1.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A3.p1.1.m1.2b"><apply id="A3.p1.1.m1.2.2.cmml" xref="A3.p1.1.m1.2.2"><eq id="A3.p1.1.m1.2.2.3.cmml" xref="A3.p1.1.m1.2.2.3"></eq><ci id="A3.p1.1.m1.2.2.4.cmml" xref="A3.p1.1.m1.2.2.4">ğ›¿</ci><interval closure="open" id="A3.p1.1.m1.2.2.2.3.cmml" xref="A3.p1.1.m1.2.2.2.2"><apply id="A3.p1.1.m1.1.1.1.1.1.cmml" xref="A3.p1.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="A3.p1.1.m1.1.1.1.1.1.1.cmml" xref="A3.p1.1.m1.1.1.1.1.1">subscript</csymbol><ci id="A3.p1.1.m1.1.1.1.1.1.2.cmml" xref="A3.p1.1.m1.1.1.1.1.1.2">ğ›¿</ci><ci id="A3.p1.1.m1.1.1.1.1.1.3.cmml" xref="A3.p1.1.m1.1.1.1.1.1.3">ğ‘¥</ci></apply><apply id="A3.p1.1.m1.2.2.2.2.2.cmml" xref="A3.p1.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="A3.p1.1.m1.2.2.2.2.2.1.cmml" xref="A3.p1.1.m1.2.2.2.2.2">subscript</csymbol><ci id="A3.p1.1.m1.2.2.2.2.2.2.cmml" xref="A3.p1.1.m1.2.2.2.2.2.2">ğ›¿</ci><ci id="A3.p1.1.m1.2.2.2.2.2.3.cmml" xref="A3.p1.1.m1.2.2.2.2.2.3">ğ‘¦</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.1.m1.2c">\delta=(\delta_{x},\delta_{y})</annotation></semantics></math> in Fig.Â <a href="#A3.F7" title="Figure 7 â€£ Appendix C Prediction Locality of Second-Stage Refinement â€£ FocalFormer3D : Focusing on Hard Instance for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> for several recent leading 3D detectors, including the LiDAR detectors (CenterPointÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, TransFusion-LÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>) and multi-modal detectors (BEVFusionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, DeepInteractionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>). Statistics of center shift (<math id="A3.p1.2.m2.1" class="ltx_Math" alttext="\sigma_{\delta}&lt;0.283m" display="inline"><semantics id="A3.p1.2.m2.1a"><mrow id="A3.p1.2.m2.1.1" xref="A3.p1.2.m2.1.1.cmml"><msub id="A3.p1.2.m2.1.1.2" xref="A3.p1.2.m2.1.1.2.cmml"><mi id="A3.p1.2.m2.1.1.2.2" xref="A3.p1.2.m2.1.1.2.2.cmml">Ïƒ</mi><mi id="A3.p1.2.m2.1.1.2.3" xref="A3.p1.2.m2.1.1.2.3.cmml">Î´</mi></msub><mo id="A3.p1.2.m2.1.1.1" xref="A3.p1.2.m2.1.1.1.cmml">&lt;</mo><mrow id="A3.p1.2.m2.1.1.3" xref="A3.p1.2.m2.1.1.3.cmml"><mn id="A3.p1.2.m2.1.1.3.2" xref="A3.p1.2.m2.1.1.3.2.cmml">0.283</mn><mo lspace="0em" rspace="0em" id="A3.p1.2.m2.1.1.3.1" xref="A3.p1.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="A3.p1.2.m2.1.1.3.3" xref="A3.p1.2.m2.1.1.3.3.cmml">m</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="A3.p1.2.m2.1b"><apply id="A3.p1.2.m2.1.1.cmml" xref="A3.p1.2.m2.1.1"><lt id="A3.p1.2.m2.1.1.1.cmml" xref="A3.p1.2.m2.1.1.1"></lt><apply id="A3.p1.2.m2.1.1.2.cmml" xref="A3.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="A3.p1.2.m2.1.1.2.1.cmml" xref="A3.p1.2.m2.1.1.2">subscript</csymbol><ci id="A3.p1.2.m2.1.1.2.2.cmml" xref="A3.p1.2.m2.1.1.2.2">ğœ</ci><ci id="A3.p1.2.m2.1.1.2.3.cmml" xref="A3.p1.2.m2.1.1.2.3">ğ›¿</ci></apply><apply id="A3.p1.2.m2.1.1.3.cmml" xref="A3.p1.2.m2.1.1.3"><times id="A3.p1.2.m2.1.1.3.1.cmml" xref="A3.p1.2.m2.1.1.3.1"></times><cn type="float" id="A3.p1.2.m2.1.1.3.2.cmml" xref="A3.p1.2.m2.1.1.3.2">0.283</cn><ci id="A3.p1.2.m2.1.1.3.3.cmml" xref="A3.p1.2.m2.1.1.3.3">ğ‘š</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.2.m2.1c">\sigma_{\delta}&lt;0.283m</annotation></semantics></math> illustrate almost all predictions are strongly correlated with their initial positions (generally less than <math id="A3.p1.3.m3.1" class="ltx_Math" alttext="2" display="inline"><semantics id="A3.p1.3.m3.1a"><mn id="A3.p1.3.m3.1.1" xref="A3.p1.3.m3.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="A3.p1.3.m3.1b"><cn type="integer" id="A3.p1.3.m3.1.1.cmml" xref="A3.p1.3.m3.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.3.m3.1c">2</annotation></semantics></math> meters away), especially for LiDAR-only detectors, such as CenterPoint and TransFusion-L.</p>
</div>
<figure id="A3.F7" class="ltx_figure"><img src="/html/2308.04556/assets/x7.png" id="A3.F7.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="432" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Object center shifts <math id="A3.F7.3.m1.2" class="ltx_Math" alttext="(\delta_{x},\delta_{y})" display="inline"><semantics id="A3.F7.3.m1.2b"><mrow id="A3.F7.3.m1.2.2.2" xref="A3.F7.3.m1.2.2.3.cmml"><mo stretchy="false" id="A3.F7.3.m1.2.2.2.3" xref="A3.F7.3.m1.2.2.3.cmml">(</mo><msub id="A3.F7.3.m1.1.1.1.1" xref="A3.F7.3.m1.1.1.1.1.cmml"><mi id="A3.F7.3.m1.1.1.1.1.2" xref="A3.F7.3.m1.1.1.1.1.2.cmml">Î´</mi><mi id="A3.F7.3.m1.1.1.1.1.3" xref="A3.F7.3.m1.1.1.1.1.3.cmml">x</mi></msub><mo id="A3.F7.3.m1.2.2.2.4" xref="A3.F7.3.m1.2.2.3.cmml">,</mo><msub id="A3.F7.3.m1.2.2.2.2" xref="A3.F7.3.m1.2.2.2.2.cmml"><mi id="A3.F7.3.m1.2.2.2.2.2" xref="A3.F7.3.m1.2.2.2.2.2.cmml">Î´</mi><mi id="A3.F7.3.m1.2.2.2.2.3" xref="A3.F7.3.m1.2.2.2.2.3.cmml">y</mi></msub><mo stretchy="false" id="A3.F7.3.m1.2.2.2.5" xref="A3.F7.3.m1.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="A3.F7.3.m1.2c"><interval closure="open" id="A3.F7.3.m1.2.2.3.cmml" xref="A3.F7.3.m1.2.2.2"><apply id="A3.F7.3.m1.1.1.1.1.cmml" xref="A3.F7.3.m1.1.1.1.1"><csymbol cd="ambiguous" id="A3.F7.3.m1.1.1.1.1.1.cmml" xref="A3.F7.3.m1.1.1.1.1">subscript</csymbol><ci id="A3.F7.3.m1.1.1.1.1.2.cmml" xref="A3.F7.3.m1.1.1.1.1.2">ğ›¿</ci><ci id="A3.F7.3.m1.1.1.1.1.3.cmml" xref="A3.F7.3.m1.1.1.1.1.3">ğ‘¥</ci></apply><apply id="A3.F7.3.m1.2.2.2.2.cmml" xref="A3.F7.3.m1.2.2.2.2"><csymbol cd="ambiguous" id="A3.F7.3.m1.2.2.2.2.1.cmml" xref="A3.F7.3.m1.2.2.2.2">subscript</csymbol><ci id="A3.F7.3.m1.2.2.2.2.2.cmml" xref="A3.F7.3.m1.2.2.2.2.2">ğ›¿</ci><ci id="A3.F7.3.m1.2.2.2.2.3.cmml" xref="A3.F7.3.m1.2.2.2.2.3">ğ‘¦</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="A3.F7.3.m1.2d">(\delta_{x},\delta_{y})</annotation></semantics></math> distribution without normalization between initial heatmap response and final object predictions. The unit is a meter.</figcaption>
</figure>
<div id="A3.p2" class="ltx_para">
<p id="A3.p2.2" class="ltx_p">The disparity between small object sizes (usually <math id="A3.p2.1.m1.1" class="ltx_Math" alttext="&lt;5m\times 5m" display="inline"><semantics id="A3.p2.1.m1.1a"><mrow id="A3.p2.1.m1.1.1" xref="A3.p2.1.m1.1.1.cmml"><mi id="A3.p2.1.m1.1.1.2" xref="A3.p2.1.m1.1.1.2.cmml"></mi><mo id="A3.p2.1.m1.1.1.1" xref="A3.p2.1.m1.1.1.1.cmml">&lt;</mo><mrow id="A3.p2.1.m1.1.1.3" xref="A3.p2.1.m1.1.1.3.cmml"><mrow id="A3.p2.1.m1.1.1.3.2" xref="A3.p2.1.m1.1.1.3.2.cmml"><mrow id="A3.p2.1.m1.1.1.3.2.2" xref="A3.p2.1.m1.1.1.3.2.2.cmml"><mn id="A3.p2.1.m1.1.1.3.2.2.2" xref="A3.p2.1.m1.1.1.3.2.2.2.cmml">5</mn><mo lspace="0em" rspace="0em" id="A3.p2.1.m1.1.1.3.2.2.1" xref="A3.p2.1.m1.1.1.3.2.2.1.cmml">â€‹</mo><mi id="A3.p2.1.m1.1.1.3.2.2.3" xref="A3.p2.1.m1.1.1.3.2.2.3.cmml">m</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="A3.p2.1.m1.1.1.3.2.1" xref="A3.p2.1.m1.1.1.3.2.1.cmml">Ã—</mo><mn id="A3.p2.1.m1.1.1.3.2.3" xref="A3.p2.1.m1.1.1.3.2.3.cmml">5</mn></mrow><mo lspace="0em" rspace="0em" id="A3.p2.1.m1.1.1.3.1" xref="A3.p2.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="A3.p2.1.m1.1.1.3.3" xref="A3.p2.1.m1.1.1.3.3.cmml">m</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="A3.p2.1.m1.1b"><apply id="A3.p2.1.m1.1.1.cmml" xref="A3.p2.1.m1.1.1"><lt id="A3.p2.1.m1.1.1.1.cmml" xref="A3.p2.1.m1.1.1.1"></lt><csymbol cd="latexml" id="A3.p2.1.m1.1.1.2.cmml" xref="A3.p2.1.m1.1.1.2">absent</csymbol><apply id="A3.p2.1.m1.1.1.3.cmml" xref="A3.p2.1.m1.1.1.3"><times id="A3.p2.1.m1.1.1.3.1.cmml" xref="A3.p2.1.m1.1.1.3.1"></times><apply id="A3.p2.1.m1.1.1.3.2.cmml" xref="A3.p2.1.m1.1.1.3.2"><times id="A3.p2.1.m1.1.1.3.2.1.cmml" xref="A3.p2.1.m1.1.1.3.2.1"></times><apply id="A3.p2.1.m1.1.1.3.2.2.cmml" xref="A3.p2.1.m1.1.1.3.2.2"><times id="A3.p2.1.m1.1.1.3.2.2.1.cmml" xref="A3.p2.1.m1.1.1.3.2.2.1"></times><cn type="integer" id="A3.p2.1.m1.1.1.3.2.2.2.cmml" xref="A3.p2.1.m1.1.1.3.2.2.2">5</cn><ci id="A3.p2.1.m1.1.1.3.2.2.3.cmml" xref="A3.p2.1.m1.1.1.3.2.2.3">ğ‘š</ci></apply><cn type="integer" id="A3.p2.1.m1.1.1.3.2.3.cmml" xref="A3.p2.1.m1.1.1.3.2.3">5</cn></apply><ci id="A3.p2.1.m1.1.1.3.3.cmml" xref="A3.p2.1.m1.1.1.3.3">ğ‘š</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p2.1.m1.1c">&lt;5m\times 5m</annotation></semantics></math>) and extensive detection range (over <math id="A3.p2.2.m2.1" class="ltx_Math" alttext="100m\times 100m" display="inline"><semantics id="A3.p2.2.m2.1a"><mrow id="A3.p2.2.m2.1.1" xref="A3.p2.2.m2.1.1.cmml"><mrow id="A3.p2.2.m2.1.1.2" xref="A3.p2.2.m2.1.1.2.cmml"><mrow id="A3.p2.2.m2.1.1.2.2" xref="A3.p2.2.m2.1.1.2.2.cmml"><mn id="A3.p2.2.m2.1.1.2.2.2" xref="A3.p2.2.m2.1.1.2.2.2.cmml">100</mn><mo lspace="0em" rspace="0em" id="A3.p2.2.m2.1.1.2.2.1" xref="A3.p2.2.m2.1.1.2.2.1.cmml">â€‹</mo><mi id="A3.p2.2.m2.1.1.2.2.3" xref="A3.p2.2.m2.1.1.2.2.3.cmml">m</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="A3.p2.2.m2.1.1.2.1" xref="A3.p2.2.m2.1.1.2.1.cmml">Ã—</mo><mn id="A3.p2.2.m2.1.1.2.3" xref="A3.p2.2.m2.1.1.2.3.cmml">100</mn></mrow><mo lspace="0em" rspace="0em" id="A3.p2.2.m2.1.1.1" xref="A3.p2.2.m2.1.1.1.cmml">â€‹</mo><mi id="A3.p2.2.m2.1.1.3" xref="A3.p2.2.m2.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="A3.p2.2.m2.1b"><apply id="A3.p2.2.m2.1.1.cmml" xref="A3.p2.2.m2.1.1"><times id="A3.p2.2.m2.1.1.1.cmml" xref="A3.p2.2.m2.1.1.1"></times><apply id="A3.p2.2.m2.1.1.2.cmml" xref="A3.p2.2.m2.1.1.2"><times id="A3.p2.2.m2.1.1.2.1.cmml" xref="A3.p2.2.m2.1.1.2.1"></times><apply id="A3.p2.2.m2.1.1.2.2.cmml" xref="A3.p2.2.m2.1.1.2.2"><times id="A3.p2.2.m2.1.1.2.2.1.cmml" xref="A3.p2.2.m2.1.1.2.2.1"></times><cn type="integer" id="A3.p2.2.m2.1.1.2.2.2.cmml" xref="A3.p2.2.m2.1.1.2.2.2">100</cn><ci id="A3.p2.2.m2.1.1.2.2.3.cmml" xref="A3.p2.2.m2.1.1.2.2.3">ğ‘š</ci></apply><cn type="integer" id="A3.p2.2.m2.1.1.2.3.cmml" xref="A3.p2.2.m2.1.1.2.3">100</cn></apply><ci id="A3.p2.2.m2.1.1.3.cmml" xref="A3.p2.2.m2.1.1.3">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p2.2.m2.1c">100m\times 100m</annotation></semantics></math> meters) limits the efficacy of long-range second-stage refinement, despite the introduction of global operations and perspective camera information. Achieving a balance between long-range modeling and computation efficiency for BEV detection is crucial. FocalFormer3D, as the pioneer in identifying false negatives on the BEV heatmap followed by local-scope rescoring, may provide insights for future network design.</p>
</div>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Example Visualization</h2>

<div id="A4.p1" class="ltx_para ltx_noindent">
<p id="A4.p1.1" class="ltx_p"><span id="A4.p1.1.1" class="ltx_text ltx_font_bold">Example visualization of multi-stage heatmaps and masking.</span>
We present a visual illustration of the multi-stage heatmap encoder process in Fig.Â <a href="#A4.F8" title="Figure 8 â€£ Appendix D Example Visualization â€£ FocalFormer3D : Focusing on Hard Instance for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
<figure id="A4.F8" class="ltx_figure"><img src="/html/2308.04556/assets/x8.png" id="A4.F8.1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="346" height="501" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 8: </span><span id="A4.F8.4.1" class="ltx_text ltx_font_bold">Example visualization of multi-stage heatmap encoder process on the birdâ€™s eye view</span>. The process of identifying false negatives operates stage by stage. We show different categories with different colors for visualization. The top three subfigures display the ground-truth center heatmaps at each stage, highlighting the missed object detections. The two subfigures below display the positive mask that shows positive object predictions. The scene ids are â€4de831d46edf46d084ac2cecf682b11aâ€ and â€825a9083e9fc466ca6fdb4bb75a95449â€ from the nuScenes <span id="A4.F8.5.2" class="ltx_text ltx_font_italic">val</span> set. We recommend zooming in on the figure for best viewing.</figcaption>
</figure>
<div id="A4.p2" class="ltx_para ltx_noindent">
<p id="A4.p2.2" class="ltx_p"><span id="A4.p2.2.1" class="ltx_text ltx_font_bold">Qualitative results.</span>
Fig.Â <a href="#A4.F9" title="Figure 9 â€£ Appendix D Example Visualization â€£ FocalFormer3D : Focusing on Hard Instance for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> shows some visual results and failure cases of FocalFormer3D on the birdâ€™s eye view. Although the average recall AR<sub id="A4.p2.2.2" class="ltx_sub"><span id="A4.p2.2.2.1" class="ltx_text ltx_font_italic">&lt;1.0m</span></sub> reaches over <math id="A4.p2.2.m2.1" class="ltx_Math" alttext="80" display="inline"><semantics id="A4.p2.2.m2.1a"><mn id="A4.p2.2.m2.1.1" xref="A4.p2.2.m2.1.1.cmml">80</mn><annotation-xml encoding="MathML-Content" id="A4.p2.2.m2.1b"><cn type="integer" id="A4.p2.2.m2.1.1.cmml" xref="A4.p2.2.m2.1.1">80</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.p2.2.m2.1c">80</annotation></semantics></math>%, some false negatives are still present due to either large occlusion or insufficient points. Also, despite accurate center prediction, false negatives can arise due to incorrect box orientation. Further exploration of a strong box refinement network is left for future work.</p>
</div>
<figure id="A4.F9" class="ltx_figure"><img src="/html/2308.04556/assets/x9.png" id="A4.F9.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="392" height="422" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 9: </span><span id="A4.F9.3.1" class="ltx_text ltx_font_bold">Visual results and failure cases.</span> The green boxes represent the ground truth objects and the blue ones stand for our predictions. We recommend zooming in on the figure for best viewing.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2308.04555" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2308.04556" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2308.04556">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2308.04556" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2308.04557" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 13:36:06 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
