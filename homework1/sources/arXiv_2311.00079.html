<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2311.00079] Spuriosity Rankings for Free: A Simple Framework for Last Layer Retraining Based on Object Detection</title><meta property="og:description" content="Deep neural networks have exhibited remarkable performance in various domains. However, the reliance of these models on spurious features has raised concerns about their reliability. A promising solution to this proble…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spuriosity Rankings for Free: A Simple Framework for Last Layer Retraining Based on Object Detection">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Spuriosity Rankings for Free: A Simple Framework for Last Layer Retraining Based on Object Detection">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2311.00079">

<!--Generated on Tue Feb 27 22:47:58 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Machine Learning,  ICML">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_pruned_first">
<h1 class="ltx_title ltx_title_document">Spuriosity Rankings for Free: 
<br class="ltx_break">A Simple Framework for Last Layer Retraining Based on Object Detection</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mohammad Azizmalayeri
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Reza Abbasi
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Amir Hosein Haji Mohammad rezaie
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Reihaneh Zohrabi
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mahdi Amiri
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mohammad Taghi Manzuri
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mohammad Hossein Rohban
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Deep neural networks have exhibited remarkable performance in various domains. However, the reliance of these models on spurious features has raised concerns about their reliability. A promising solution to this problem is last-layer retraining, which involves retraining the linear classifier head on a small subset of data without spurious cues. Nevertheless, selecting this subset requires human supervision, which reduces its scalability. Moreover, spurious cues may still exist in the selected subset. As a solution to this problem, we propose a novel ranking framework that leverages an open vocabulary object detection technique to identify images without spurious cues. More specifically, we use the object detector as a measure to score the presence of the target object in the images. Next, the images are sorted based on this score, and the last-layer of the model is retrained on a subset of the data with the highest scores. Our experiments on the ImageNet-1k dataset demonstrate the effectiveness of this ranking framework in sorting images based on spuriousness and using them for last-layer retraining.</p>
</div>
<div class="ltx_keywords">Machine Learning, ICML
</div>
<div id="p2" class="ltx_para">
<br class="ltx_break">
</div>
<figure id="S0.F1" class="ltx_figure"><img src="/html/2311.00079/assets/Figures/Fig1_1.jpg" id="S0.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="251" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>
Spuriosity ranking reveals core object detectability and the presence of spurious cues, distinguishing between clear representations such as a recognizable three-toed sloth, Diamondback, and Cannon (low spurious cues - right) and more ambiguous instances featuring spurious cues like trees with a less prominent three-toed sloth, Diamondbacks in hard-to-distinguish locations, and Cannon barrel (high spurious cues - left).
</figcaption>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">A prominent issue that has recently gained substantial attention is the dependency of deep models on shortcuts <cite class="ltx_cite ltx_citemacro_citep">(Geirhos et al., <a href="#bib.bib7" title="" class="ltx_ref">2020</a>)</cite>.
An illustrative example of such shortcuts is spurious correlation which is statistically informative heuristics for predicting the labels of most examples in the training data. However, they are not actually relevant to the true labeling function, and do not necessarily have a causal relationship with the class label <cite class="ltx_cite ltx_citemacro_citep">(Kirichenko et al., <a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite>. A model that captures these spurious correlations may have high average accuracy on a given test set, but encounters significant accuracy drop on data subsets without these correlations <cite class="ltx_cite ltx_citemacro_citep">(Arjovsky et al., <a href="#bib.bib2" title="" class="ltx_ref">2019</a>; Sagawa et al., <a href="#bib.bib19" title="" class="ltx_ref">2019</a>; Alcorn et al., <a href="#bib.bib1" title="" class="ltx_ref">2019</a>)</cite>.
Theoretical arguments suggest that a bias towards model simplicity contributes to the reliance on spurious features <cite class="ltx_cite ltx_citemacro_citep">(Shah et al., <a href="#bib.bib21" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">A wide range of solutions have been proposed to address the model robustness against spurious correlations. One approach involves optimizing for the worst case group accuracy, which necessitates the annotations indicating the respective group affiliation of each training example <cite class="ltx_cite ltx_citemacro_citep">(Sagawa et al., <a href="#bib.bib19" title="" class="ltx_ref">2019</a>; Zhang et al., <a href="#bib.bib24" title="" class="ltx_ref">2021</a>; Liu et al., <a href="#bib.bib13" title="" class="ltx_ref">2021</a>; Hu et al., <a href="#bib.bib9" title="" class="ltx_ref">2018</a>)</cite>. Additionally, alternative strategies involve leveraging causality <cite class="ltx_cite ltx_citemacro_citep">(Aubin et al., <a href="#bib.bib3" title="" class="ltx_ref">2021</a>; Seo et al., <a href="#bib.bib20" title="" class="ltx_ref">2022</a>)</cite>, and learning invariant latent spaces <cite class="ltx_cite ltx_citemacro_citep">(Arjovsky et al., <a href="#bib.bib2" title="" class="ltx_ref">2019</a>; Robey et al., <a href="#bib.bib17" title="" class="ltx_ref">2021</a>)</cite>. Recent researches have also highlighted the effectiveness of fine-tuning the last layer of neural networks on samples without spurious correlations to the label, to mitigate the model sensitivity to spurious cues <cite class="ltx_cite ltx_citemacro_citep">(Kirichenko et al., <a href="#bib.bib11" title="" class="ltx_ref">2023</a>; Rosenfeld et al., <a href="#bib.bib18" title="" class="ltx_ref">2022</a>; Lee et al., <a href="#bib.bib12" title="" class="ltx_ref">2022</a>)</cite>. However, they often rely on human supervision to determine the presence of spurious features, which may limit their applicability, particularly for large datasets like ImageNet <cite class="ltx_cite ltx_citemacro_citep">(Deng et al., <a href="#bib.bib4" title="" class="ltx_ref">2009</a>)</cite>. For example, in <cite class="ltx_cite ltx_citemacro_citep">(Kirichenko et al., <a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite>, human supervision is necessary to re-weight the spurious features through retraining the last linear layer on a small dataset without spurious correlations between the background and foreground.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To overcome this limitation,
<cite class="ltx_cite ltx_citemacro_citep">(Singla &amp; Feizi, <a href="#bib.bib22" title="" class="ltx_ref">2022</a>; Singla et al., <a href="#bib.bib23" title="" class="ltx_ref">2022</a>)</cite> have introduced an approach that utilizes the neurons of robust models as detectors for visual attributes, enabling identification of the spurious features at a large scale while minimizing the need for human supervision. Moreover, <cite class="ltx_cite ltx_citemacro_citep">(Neuhaus et al., <a href="#bib.bib16" title="" class="ltx_ref">2022</a>)</cite> have proposed an automated pipeline for detecting spurious features using class-wise neural PCA components, which still requires human supervision. Building upon the concept of robust neural features, <cite class="ltx_cite ltx_citemacro_citep">(Moayeri et al., <a href="#bib.bib15" title="" class="ltx_ref">2022</a>)</cite> introduced a novel ranking framework that assesses the prominence of spurious cues within image classes. In order to reduce the impact of spurious features, their approach involves training linear heads on carefully selected subsets of data determined by the ranking.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Despite these notable attempts to reduce the reliance on human supervision, the complete elimination of such supervision remains challenging. The mentioned approaches still require human involvement in annotating feature-class dependencies, indicating that previous works may face limitations when employing a human-in-the-loop strategy. Therefore, it is important to acknowledge that the need for human involvement in these methods introduces scalability concerns. To overcome these challenges and further enhance the autonomy of the system, we propose a novel method for eliminating the need for human supervision in the detection and mitigation of the effect of spurious features.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In this work, we introduce a novel approach to identify the core and spurious features within classes, utilizing an open vocabulary object detection technique <cite class="ltx_cite ltx_citemacro_citep">(Minderer et al., <a href="#bib.bib14" title="" class="ltx_ref">2022</a>)</cite>. By examining the images with the highest and lowest spuriosity ranks, as illustrated in <a href="#S0.F1" title="In Spuriosity Rankings for Free: A Simple Framework for Last Layer Retraining Based on Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a>, we are able to detect images with the high spurious cues and minority subgroups. In our framework, images that clearly have the object, such as a clear identifiable three-toed sloth, receive high scores and are ranked as low spurious. Conversely, images where the spurious cues are present, and the object could not be easily detected, like trees with a less evident three-toed sloth, are assigned low scores and ranked as highly spurious. This approach allows for precise differentiation between images based on their level of spuriousness and object detectability, enabling us to rank images based on their spuriosity without any reliance on human supervision.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Proposed Sorting Approach</h2>

<figure id="S2.F2" class="ltx_figure"><img src="/html/2311.00079/assets/Figures/Fig2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="174" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Schematic representation of our proposed 2-step approach.
1: a) utilizing an open-vocabulary object detector, b) sorting based on detection scores, and c) selecting low-ranked images.
2: a) training a model on the entire dataset and b) retraining the last layer.</figcaption>
</figure>
<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">One of the most effective methods for reducing the reliance of models on spurious features is the last layer re-training approach <cite class="ltx_cite ltx_citemacro_citep">(Kirichenko et al., <a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite>. To leverage this approach effectively, images within a dataset should be ranked based on their spuriosity in order to select a subset that contains low spurious cues for retraining. For large datasets, the existing ranking method <cite class="ltx_cite ltx_citemacro_citep">(Moayeri et al., <a href="#bib.bib15" title="" class="ltx_ref">2022</a>)</cite> requires human supervision, which may not be feasible. Hence, we propose a method for determining feature-class dependencies without requiring human supervision. Our ranking method is based on large-scale pre-trained models that can generalize to out-of-distribution (OOD) samples.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Recent advances have applied language capabilities to object detection <cite class="ltx_cite ltx_citemacro_citep">(Gu et al., <a href="#bib.bib8" title="" class="ltx_ref">2022</a>; Kamath et al., <a href="#bib.bib10" title="" class="ltx_ref">2021</a>; Minderer et al., <a href="#bib.bib14" title="" class="ltx_ref">2022</a>)</cite>, integrating image and text in a shared representation space<cite class="ltx_cite ltx_citemacro_citep">(Frome et al., <a href="#bib.bib6" title="" class="ltx_ref">2013</a>)</cite>. OWL-ViT <cite class="ltx_cite ltx_citemacro_citep">(Minderer et al., <a href="#bib.bib14" title="" class="ltx_ref">2022</a>)</cite>, a cutting-edge method, combines image and text encoders using contrastive learning on a large dataset. Inspired by the CLIP model, OWL-ViT exhibits remarkable generalization and detects objects in open-vocabulary environments. Its generalization capability can help us detect high-spurious samples.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">OWL-ViT architecture consists of a Vision Transformer <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy et al., <a href="#bib.bib5" title="" class="ltx_ref">2021</a>)</cite> for image encoding and a Transformer architecture for text encoding. The model image and text encoders are trained on a large-scale image-text data using contrastive learning. Then, detection heads are added to the model, and it is trained on a medium-sized detection data. OWL-ViT modifies the image encoder for object detection by projecting token representations and using a small MLP to determine the box coordinates. Predicted bounding boxes are ranked based on the confidence scores. These scores are calculated by transforming logits from a fully connected layer with a sigmoid function to obtain class probability scores. Aggregating the scores across classes produces a single score for each predicted bounding box.
Notably, this score serves as a valuable metric for assessing the level of spurious features within an image and can effectively rank images within their respective classes. Specifically, higher scores correspond to lower rankings of spuriosity, indicating a more clear and more identifiable object in the image.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">On this basis, as shown in <a href="#S2.F2" title="In 2 Proposed Sorting Approach ‣ Spuriosity Rankings for Free: A Simple Framework for Last Layer Retraining Based on Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a>, our proposed method utilizes an open-vocabulary object detector to detect and classify objects in images. First, the model assigns detection scores to the objects based on its confidence. The images are then sorted based on these scores to refine the results and prioritize them according to their spuriosity.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">To mitigate the impact of spurious correlations learned by the model during the initial detection process, the last layer of the model is retrained. This technique allows for fine-tuning the parameters in the last layer, enhancing the model ability to recognize meaningful features while disregarding irrelevant or misleading information. This two-step approach provides an efficient and effective solution for addressing spurious correlations in image classification.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">The combination of accurate detection scores, automated sorting based on the spuriosity, and targeted retraining of the last layer ensures improved reliability and robustness in object classification tasks. This technique also enables the determination of the degree of spuriosity in each image, providing valuable insights into the reliability and accuracy of the detected objects. Additionally, the novel sorting technique eliminates human supervision, which is particularly advantageous when working with large datasets.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiments</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">To demonstrate the efficacy of our ranking framework, we conduct several experiments which will be discussed in the following sections.</p>
</div>
<figure id="S3.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S3.F3.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_top" style="width:216.8pt;">
<img src="/html/2311.00079/assets/Figures/100_res.png" id="S3.F3.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S3.F3.2" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_top" style="width:216.8pt;">
<img src="/html/2311.00079/assets/Figures/100_vit.png" id="S3.F3.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
</div>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Impact of Training Subsets on Classification Accuracy for Validation Samples with Varying spuriosity Ranks. The dashed horizontal lines depict the average accuracy levels achieved by each model.</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Effect of spuriosity Rankings</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In this section, we utilize two pre-trained models on the ImageNet-1k (ResNet-50 or DeiT-small) and try to re-train only their last layer as a linear classification head on a subset from the same data from scratch. This process is similar to the method used in <cite class="ltx_cite ltx_citemacro_citep">(Kirichenko et al., <a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite>.
The training data subsets are selected based on our proposed approach for spuriosity rankings. More specifically, we consider four subsets each containing k samples of each class: <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_italic">bot k</span> (lowest spuriosity rank), <span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_italic">mid k</span> (closest to median rank), <span id="S3.SS1.p1.1.3" class="ltx_text ltx_font_italic">top k</span> (highest spuriosity rank), and <span id="S3.SS1.p1.1.4" class="ltx_text ltx_font_italic">rnd k</span> (randomly selected samples). By incorporating these various subsets of data, we aim to capture the impact of our proposed method for ranking images in the last layer re-training method.
For assessing the models’ performance, each model is evaluated on varying spuriosity level test data. For this purpose, we chose the image with the i-th highest spuriosity rank from each class in the test data, resulting in a set of images with varying spurious rankings.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">The evaluation results for k=100 are presented in <a href="#S3.F3" title="In 3 Experiments ‣ Spuriosity Rankings for Free: A Simple Framework for Last Layer Retraining Based on Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a>, indicating that the model trained on <span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_italic">top k</span> data performs better on low-ranked ones, whereas the model trained on <span id="S3.SS1.p2.1.2" class="ltx_text ltx_font_italic">bot k</span> data is more resilient to the accuracy change when assessed on data with varying spurious rankings. It is worth noting that our method achieves similar results to <cite class="ltx_cite ltx_citemacro_citep">(Moayeri et al., <a href="#bib.bib15" title="" class="ltx_ref">2022</a>)</cite>, but ranks images without human supervision. The results for k=50 and k=200 could be found in the appendix.</p>
</div>
<figure id="S3.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S3.F4.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_top" style="width:216.8pt;">
<img src="/html/2311.00079/assets/Figures/Picture1.png" id="S3.F4.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S3.F4.2" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_top" style="width:216.8pt;">
<img src="/html/2311.00079/assets/Figures/Picture2.png" id="S3.F4.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
</div>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Experimental results show <span id="S3.F4.9.1" class="ltx_text ltx_font_italic">top 100</span> models are more robust than <span id="S3.F4.10.2" class="ltx_text ltx_font_italic">bot 100</span> under foreground and background noise for both ResNet and VIT models, with <span id="S3.F4.11.3" class="ltx_text ltx_font_italic">top 100</span> VIT outperforming <span id="S3.F4.12.4" class="ltx_text ltx_font_italic">rnd 100</span> and <span id="S3.F4.13.5" class="ltx_text ltx_font_italic">mid 100</span> models. Results suggest <span id="S3.F4.14.6" class="ltx_text ltx_font_italic">top 100</span> approach enhances ResNet’s robustness on high-level noisy foreground samples.</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Foreground and Background Noise Sensitivity Analysis: Comparing Model Robustness</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.4" class="ltx_p">In this section, we present a sensitivity analysis on the models trained as described in section <a href="#S3.SS1" title="3.1 Effect of spuriosity Rankings ‣ 3 Experiments ‣ Spuriosity Rankings for Free: A Simple Framework for Last Layer Retraining Based on Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a> by adding noise to the foreground or background of the images.
First, we define the foreground region of ImageNet images as the area inside the detection rectangle determined by the OWL-ViT model. We create a binary mask for each sample by assigning a value of 1 to pixels within the foreground region and 0 otherwise. To create the noisy samples, a noise tensor <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">n</annotation></semantics></math> is generated in which the pixel values are independently drawn from <math id="S3.SS2.p1.2.m2.2" class="ltx_Math" alttext="\mathcal{N}(0,1)" display="inline"><semantics id="S3.SS2.p1.2.m2.2a"><mrow id="S3.SS2.p1.2.m2.2.3" xref="S3.SS2.p1.2.m2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.2.m2.2.3.2" xref="S3.SS2.p1.2.m2.2.3.2.cmml">𝒩</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.2.m2.2.3.1" xref="S3.SS2.p1.2.m2.2.3.1.cmml">​</mo><mrow id="S3.SS2.p1.2.m2.2.3.3.2" xref="S3.SS2.p1.2.m2.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS2.p1.2.m2.2.3.3.2.1" xref="S3.SS2.p1.2.m2.2.3.3.1.cmml">(</mo><mn id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">0</mn><mo id="S3.SS2.p1.2.m2.2.3.3.2.2" xref="S3.SS2.p1.2.m2.2.3.3.1.cmml">,</mo><mn id="S3.SS2.p1.2.m2.2.2" xref="S3.SS2.p1.2.m2.2.2.cmml">1</mn><mo stretchy="false" id="S3.SS2.p1.2.m2.2.3.3.2.3" xref="S3.SS2.p1.2.m2.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.2b"><apply id="S3.SS2.p1.2.m2.2.3.cmml" xref="S3.SS2.p1.2.m2.2.3"><times id="S3.SS2.p1.2.m2.2.3.1.cmml" xref="S3.SS2.p1.2.m2.2.3.1"></times><ci id="S3.SS2.p1.2.m2.2.3.2.cmml" xref="S3.SS2.p1.2.m2.2.3.2">𝒩</ci><interval closure="open" id="S3.SS2.p1.2.m2.2.3.3.1.cmml" xref="S3.SS2.p1.2.m2.2.3.3.2"><cn type="integer" id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">0</cn><cn type="integer" id="S3.SS2.p1.2.m2.2.2.cmml" xref="S3.SS2.p1.2.m2.2.2">1</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.2c">\mathcal{N}(0,1)</annotation></semantics></math>. Afterward, the noisy foreground sample <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="\bar{x}_{fg}" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><msub id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml"><mover accent="true" id="S3.SS2.p1.3.m3.1.1.2" xref="S3.SS2.p1.3.m3.1.1.2.cmml"><mi id="S3.SS2.p1.3.m3.1.1.2.2" xref="S3.SS2.p1.3.m3.1.1.2.2.cmml">x</mi><mo id="S3.SS2.p1.3.m3.1.1.2.1" xref="S3.SS2.p1.3.m3.1.1.2.1.cmml">¯</mo></mover><mrow id="S3.SS2.p1.3.m3.1.1.3" xref="S3.SS2.p1.3.m3.1.1.3.cmml"><mi id="S3.SS2.p1.3.m3.1.1.3.2" xref="S3.SS2.p1.3.m3.1.1.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.3.m3.1.1.3.1" xref="S3.SS2.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p1.3.m3.1.1.3.3" xref="S3.SS2.p1.3.m3.1.1.3.3.cmml">g</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><apply id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">subscript</csymbol><apply id="S3.SS2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2"><ci id="S3.SS2.p1.3.m3.1.1.2.1.cmml" xref="S3.SS2.p1.3.m3.1.1.2.1">¯</ci><ci id="S3.SS2.p1.3.m3.1.1.2.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2.2">𝑥</ci></apply><apply id="S3.SS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3"><times id="S3.SS2.p1.3.m3.1.1.3.1.cmml" xref="S3.SS2.p1.3.m3.1.1.3.1"></times><ci id="S3.SS2.p1.3.m3.1.1.3.2.cmml" xref="S3.SS2.p1.3.m3.1.1.3.2">𝑓</ci><ci id="S3.SS2.p1.3.m3.1.1.3.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3.3">𝑔</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">\bar{x}_{fg}</annotation></semantics></math> and noisy background sample <math id="S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="\bar{x}_{bg}" display="inline"><semantics id="S3.SS2.p1.4.m4.1a"><msub id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml"><mover accent="true" id="S3.SS2.p1.4.m4.1.1.2" xref="S3.SS2.p1.4.m4.1.1.2.cmml"><mi id="S3.SS2.p1.4.m4.1.1.2.2" xref="S3.SS2.p1.4.m4.1.1.2.2.cmml">x</mi><mo id="S3.SS2.p1.4.m4.1.1.2.1" xref="S3.SS2.p1.4.m4.1.1.2.1.cmml">¯</mo></mover><mrow id="S3.SS2.p1.4.m4.1.1.3" xref="S3.SS2.p1.4.m4.1.1.3.cmml"><mi id="S3.SS2.p1.4.m4.1.1.3.2" xref="S3.SS2.p1.4.m4.1.1.3.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.4.m4.1.1.3.1" xref="S3.SS2.p1.4.m4.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p1.4.m4.1.1.3.3" xref="S3.SS2.p1.4.m4.1.1.3.3.cmml">g</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><apply id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.4.m4.1.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">subscript</csymbol><apply id="S3.SS2.p1.4.m4.1.1.2.cmml" xref="S3.SS2.p1.4.m4.1.1.2"><ci id="S3.SS2.p1.4.m4.1.1.2.1.cmml" xref="S3.SS2.p1.4.m4.1.1.2.1">¯</ci><ci id="S3.SS2.p1.4.m4.1.1.2.2.cmml" xref="S3.SS2.p1.4.m4.1.1.2.2">𝑥</ci></apply><apply id="S3.SS2.p1.4.m4.1.1.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3"><times id="S3.SS2.p1.4.m4.1.1.3.1.cmml" xref="S3.SS2.p1.4.m4.1.1.3.1"></times><ci id="S3.SS2.p1.4.m4.1.1.3.2.cmml" xref="S3.SS2.p1.4.m4.1.1.3.2">𝑏</ci><ci id="S3.SS2.p1.4.m4.1.1.3.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3.3">𝑔</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">\bar{x}_{bg}</annotation></semantics></math> are:</p>
<table id="S3.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex1.m1.36" class="ltx_Math" alttext="\begin{gathered}\bar{x}_{fg}=x+\alpha\cdot f\left(n\odot m_{x}\right)\\
\bar{x}_{bg}=x+\alpha\cdot f\left(n\odot\left(1-m_{x}\right)\right),\end{gathered}" display="block"><semantics id="S3.Ex1.m1.36a"><mtable displaystyle="true" rowspacing="0pt" id="S3.Ex1.m1.36.36.3"><mtr id="S3.Ex1.m1.36.36.3a"><mtd id="S3.Ex1.m1.36.36.3b"><mrow id="S3.Ex1.m1.35.35.2.34.15.15"><msub id="S3.Ex1.m1.35.35.2.34.15.15.16"><mover accent="true" id="S3.Ex1.m1.1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.cmml"><mi id="S3.Ex1.m1.1.1.1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.1.1.2.cmml">x</mi><mo id="S3.Ex1.m1.1.1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.1.cmml">¯</mo></mover><mrow id="S3.Ex1.m1.2.2.2.2.2.2.1" xref="S3.Ex1.m1.2.2.2.2.2.2.1.cmml"><mi id="S3.Ex1.m1.2.2.2.2.2.2.1.2" xref="S3.Ex1.m1.2.2.2.2.2.2.1.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.2.2.2.2.2.2.1.1" xref="S3.Ex1.m1.2.2.2.2.2.2.1.1.cmml">​</mo><mi id="S3.Ex1.m1.2.2.2.2.2.2.1.3" xref="S3.Ex1.m1.2.2.2.2.2.2.1.3.cmml">g</mi></mrow></msub><mo id="S3.Ex1.m1.3.3.3.3.3.3" xref="S3.Ex1.m1.3.3.3.3.3.3.cmml">=</mo><mrow id="S3.Ex1.m1.35.35.2.34.15.15.15"><mi id="S3.Ex1.m1.4.4.4.4.4.4" xref="S3.Ex1.m1.4.4.4.4.4.4.cmml">x</mi><mo id="S3.Ex1.m1.5.5.5.5.5.5" xref="S3.Ex1.m1.5.5.5.5.5.5.cmml">+</mo><mrow id="S3.Ex1.m1.35.35.2.34.15.15.15.1"><mrow id="S3.Ex1.m1.35.35.2.34.15.15.15.1.3"><mi id="S3.Ex1.m1.6.6.6.6.6.6" xref="S3.Ex1.m1.6.6.6.6.6.6.cmml">α</mi><mo lspace="0.222em" rspace="0.222em" id="S3.Ex1.m1.7.7.7.7.7.7" xref="S3.Ex1.m1.7.7.7.7.7.7.cmml">⋅</mo><mi id="S3.Ex1.m1.8.8.8.8.8.8" xref="S3.Ex1.m1.8.8.8.8.8.8.cmml">f</mi></mrow><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.35.35.2.34.15.15.15.1.2" xref="S3.Ex1.m1.34.34.1.1.1.cmml">​</mo><mrow id="S3.Ex1.m1.35.35.2.34.15.15.15.1.1.1"><mo id="S3.Ex1.m1.9.9.9.9.9.9" xref="S3.Ex1.m1.34.34.1.1.1.cmml">(</mo><mrow id="S3.Ex1.m1.35.35.2.34.15.15.15.1.1.1.1"><mi id="S3.Ex1.m1.10.10.10.10.10.10" xref="S3.Ex1.m1.10.10.10.10.10.10.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="S3.Ex1.m1.11.11.11.11.11.11" xref="S3.Ex1.m1.11.11.11.11.11.11.cmml">⊙</mo><msub id="S3.Ex1.m1.35.35.2.34.15.15.15.1.1.1.1.1"><mi id="S3.Ex1.m1.12.12.12.12.12.12" xref="S3.Ex1.m1.12.12.12.12.12.12.cmml">m</mi><mi id="S3.Ex1.m1.13.13.13.13.13.13.1" xref="S3.Ex1.m1.13.13.13.13.13.13.1.cmml">x</mi></msub></mrow><mo id="S3.Ex1.m1.14.14.14.14.14.14" xref="S3.Ex1.m1.34.34.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mtd></mtr><mtr id="S3.Ex1.m1.36.36.3c"><mtd id="S3.Ex1.m1.36.36.3d"><mrow id="S3.Ex1.m1.36.36.3.35.20.20.20"><mrow id="S3.Ex1.m1.36.36.3.35.20.20.20.1"><msub id="S3.Ex1.m1.36.36.3.35.20.20.20.1.2"><mover accent="true" id="S3.Ex1.m1.15.15.15.1.1.1" xref="S3.Ex1.m1.15.15.15.1.1.1.cmml"><mi id="S3.Ex1.m1.15.15.15.1.1.1.2" xref="S3.Ex1.m1.15.15.15.1.1.1.2.cmml">x</mi><mo id="S3.Ex1.m1.15.15.15.1.1.1.1" xref="S3.Ex1.m1.15.15.15.1.1.1.1.cmml">¯</mo></mover><mrow id="S3.Ex1.m1.16.16.16.2.2.2.1" xref="S3.Ex1.m1.16.16.16.2.2.2.1.cmml"><mi id="S3.Ex1.m1.16.16.16.2.2.2.1.2" xref="S3.Ex1.m1.16.16.16.2.2.2.1.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.16.16.16.2.2.2.1.1" xref="S3.Ex1.m1.16.16.16.2.2.2.1.1.cmml">​</mo><mi id="S3.Ex1.m1.16.16.16.2.2.2.1.3" xref="S3.Ex1.m1.16.16.16.2.2.2.1.3.cmml">g</mi></mrow></msub><mo id="S3.Ex1.m1.17.17.17.3.3.3" xref="S3.Ex1.m1.17.17.17.3.3.3.cmml">=</mo><mrow id="S3.Ex1.m1.36.36.3.35.20.20.20.1.1"><mi id="S3.Ex1.m1.18.18.18.4.4.4" xref="S3.Ex1.m1.18.18.18.4.4.4.cmml">x</mi><mo id="S3.Ex1.m1.19.19.19.5.5.5" xref="S3.Ex1.m1.19.19.19.5.5.5.cmml">+</mo><mrow id="S3.Ex1.m1.36.36.3.35.20.20.20.1.1.1"><mrow id="S3.Ex1.m1.36.36.3.35.20.20.20.1.1.1.3"><mi id="S3.Ex1.m1.20.20.20.6.6.6" xref="S3.Ex1.m1.20.20.20.6.6.6.cmml">α</mi><mo lspace="0.222em" rspace="0.222em" id="S3.Ex1.m1.21.21.21.7.7.7" xref="S3.Ex1.m1.21.21.21.7.7.7.cmml">⋅</mo><mi id="S3.Ex1.m1.22.22.22.8.8.8" xref="S3.Ex1.m1.22.22.22.8.8.8.cmml">f</mi></mrow><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.36.36.3.35.20.20.20.1.1.1.2" xref="S3.Ex1.m1.34.34.1.1.1.cmml">​</mo><mrow id="S3.Ex1.m1.36.36.3.35.20.20.20.1.1.1.1.1"><mo id="S3.Ex1.m1.23.23.23.9.9.9" xref="S3.Ex1.m1.34.34.1.1.1.cmml">(</mo><mrow id="S3.Ex1.m1.36.36.3.35.20.20.20.1.1.1.1.1.1"><mi id="S3.Ex1.m1.24.24.24.10.10.10" xref="S3.Ex1.m1.24.24.24.10.10.10.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="S3.Ex1.m1.25.25.25.11.11.11" xref="S3.Ex1.m1.25.25.25.11.11.11.cmml">⊙</mo><mrow id="S3.Ex1.m1.36.36.3.35.20.20.20.1.1.1.1.1.1.1.1"><mo id="S3.Ex1.m1.26.26.26.12.12.12" xref="S3.Ex1.m1.34.34.1.1.1.cmml">(</mo><mrow id="S3.Ex1.m1.36.36.3.35.20.20.20.1.1.1.1.1.1.1.1.1"><mn id="S3.Ex1.m1.27.27.27.13.13.13" xref="S3.Ex1.m1.27.27.27.13.13.13.cmml">1</mn><mo id="S3.Ex1.m1.28.28.28.14.14.14" xref="S3.Ex1.m1.28.28.28.14.14.14.cmml">−</mo><msub id="S3.Ex1.m1.36.36.3.35.20.20.20.1.1.1.1.1.1.1.1.1.1"><mi id="S3.Ex1.m1.29.29.29.15.15.15" xref="S3.Ex1.m1.29.29.29.15.15.15.cmml">m</mi><mi id="S3.Ex1.m1.30.30.30.16.16.16.1" xref="S3.Ex1.m1.30.30.30.16.16.16.1.cmml">x</mi></msub></mrow><mo id="S3.Ex1.m1.31.31.31.17.17.17" xref="S3.Ex1.m1.34.34.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.Ex1.m1.32.32.32.18.18.18" xref="S3.Ex1.m1.34.34.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.Ex1.m1.33.33.33.19.19.19" xref="S3.Ex1.m1.34.34.1.1.1.cmml">,</mo></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.36b"><apply id="S3.Ex1.m1.34.34.1.1.1.cmml" xref="S3.Ex1.m1.35.35.2.34.15.15.15.1.2"><and id="S3.Ex1.m1.34.34.1.1.1a.cmml" xref="S3.Ex1.m1.35.35.2.34.15.15.15.1.2"></and><apply id="S3.Ex1.m1.34.34.1.1.1b.cmml" xref="S3.Ex1.m1.35.35.2.34.15.15.15.1.2"><eq id="S3.Ex1.m1.3.3.3.3.3.3.cmml" xref="S3.Ex1.m1.3.3.3.3.3.3"></eq><apply id="S3.Ex1.m1.34.34.1.1.1.4.cmml" xref="S3.Ex1.m1.35.35.2.34.15.15.15.1.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.34.34.1.1.1.4.1.cmml" xref="S3.Ex1.m1.35.35.2.34.15.15.15.1.2">subscript</csymbol><apply id="S3.Ex1.m1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1"><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1">¯</ci><ci id="S3.Ex1.m1.1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.2">𝑥</ci></apply><apply id="S3.Ex1.m1.2.2.2.2.2.2.1.cmml" xref="S3.Ex1.m1.2.2.2.2.2.2.1"><times id="S3.Ex1.m1.2.2.2.2.2.2.1.1.cmml" xref="S3.Ex1.m1.2.2.2.2.2.2.1.1"></times><ci id="S3.Ex1.m1.2.2.2.2.2.2.1.2.cmml" xref="S3.Ex1.m1.2.2.2.2.2.2.1.2">𝑓</ci><ci id="S3.Ex1.m1.2.2.2.2.2.2.1.3.cmml" xref="S3.Ex1.m1.2.2.2.2.2.2.1.3">𝑔</ci></apply></apply><apply id="S3.Ex1.m1.34.34.1.1.1.1.cmml" xref="S3.Ex1.m1.35.35.2.34.15.15.15.1.2"><plus id="S3.Ex1.m1.5.5.5.5.5.5.cmml" xref="S3.Ex1.m1.5.5.5.5.5.5"></plus><ci id="S3.Ex1.m1.4.4.4.4.4.4.cmml" xref="S3.Ex1.m1.4.4.4.4.4.4">𝑥</ci><apply id="S3.Ex1.m1.34.34.1.1.1.1.1.cmml" xref="S3.Ex1.m1.35.35.2.34.15.15.15.1.2"><times id="S3.Ex1.m1.34.34.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.35.35.2.34.15.15.15.1.2"></times><apply id="S3.Ex1.m1.34.34.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.35.35.2.34.15.15.15.1.2"><ci id="S3.Ex1.m1.7.7.7.7.7.7.cmml" xref="S3.Ex1.m1.7.7.7.7.7.7">⋅</ci><ci id="S3.Ex1.m1.6.6.6.6.6.6.cmml" xref="S3.Ex1.m1.6.6.6.6.6.6">𝛼</ci><ci id="S3.Ex1.m1.8.8.8.8.8.8.cmml" xref="S3.Ex1.m1.8.8.8.8.8.8">𝑓</ci></apply><apply id="S3.Ex1.m1.34.34.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.35.35.2.34.15.15.15.1.2"><csymbol cd="latexml" id="S3.Ex1.m1.11.11.11.11.11.11.cmml" xref="S3.Ex1.m1.11.11.11.11.11.11">direct-product</csymbol><ci id="S3.Ex1.m1.10.10.10.10.10.10.cmml" xref="S3.Ex1.m1.10.10.10.10.10.10">𝑛</ci><apply id="S3.Ex1.m1.34.34.1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.35.35.2.34.15.15.15.1.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.34.34.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.Ex1.m1.35.35.2.34.15.15.15.1.2">subscript</csymbol><ci id="S3.Ex1.m1.12.12.12.12.12.12.cmml" xref="S3.Ex1.m1.12.12.12.12.12.12">𝑚</ci><ci id="S3.Ex1.m1.13.13.13.13.13.13.1.cmml" xref="S3.Ex1.m1.13.13.13.13.13.13.1">𝑥</ci></apply></apply><apply id="S3.Ex1.m1.34.34.1.1.1.1.1.4.cmml" xref="S3.Ex1.m1.35.35.2.34.15.15.15.1.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.34.34.1.1.1.1.1.4.1.cmml" xref="S3.Ex1.m1.35.35.2.34.15.15.15.1.2">subscript</csymbol><apply id="S3.Ex1.m1.15.15.15.1.1.1.cmml" xref="S3.Ex1.m1.15.15.15.1.1.1"><ci id="S3.Ex1.m1.15.15.15.1.1.1.1.cmml" xref="S3.Ex1.m1.15.15.15.1.1.1.1">¯</ci><ci id="S3.Ex1.m1.15.15.15.1.1.1.2.cmml" xref="S3.Ex1.m1.15.15.15.1.1.1.2">𝑥</ci></apply><apply id="S3.Ex1.m1.16.16.16.2.2.2.1.cmml" xref="S3.Ex1.m1.16.16.16.2.2.2.1"><times id="S3.Ex1.m1.16.16.16.2.2.2.1.1.cmml" xref="S3.Ex1.m1.16.16.16.2.2.2.1.1"></times><ci id="S3.Ex1.m1.16.16.16.2.2.2.1.2.cmml" xref="S3.Ex1.m1.16.16.16.2.2.2.1.2">𝑏</ci><ci id="S3.Ex1.m1.16.16.16.2.2.2.1.3.cmml" xref="S3.Ex1.m1.16.16.16.2.2.2.1.3">𝑔</ci></apply></apply></apply></apply></apply><apply id="S3.Ex1.m1.34.34.1.1.1c.cmml" xref="S3.Ex1.m1.35.35.2.34.15.15.15.1.2"><eq id="S3.Ex1.m1.17.17.17.3.3.3.cmml" xref="S3.Ex1.m1.17.17.17.3.3.3"></eq><share href="#S3.Ex1.m1.34.34.1.1.1.1.cmml" id="S3.Ex1.m1.34.34.1.1.1d.cmml" xref="S3.Ex1.m1.35.35.2.34.15.15.15.1.2"></share><apply id="S3.Ex1.m1.34.34.1.1.1.2.cmml" xref="S3.Ex1.m1.35.35.2.34.15.15.15.1.2"><plus id="S3.Ex1.m1.19.19.19.5.5.5.cmml" xref="S3.Ex1.m1.19.19.19.5.5.5"></plus><ci id="S3.Ex1.m1.18.18.18.4.4.4.cmml" xref="S3.Ex1.m1.18.18.18.4.4.4">𝑥</ci><apply id="S3.Ex1.m1.34.34.1.1.1.2.1.cmml" xref="S3.Ex1.m1.35.35.2.34.15.15.15.1.2"><times id="S3.Ex1.m1.34.34.1.1.1.2.1.2.cmml" xref="S3.Ex1.m1.35.35.2.34.15.15.15.1.2"></times><apply id="S3.Ex1.m1.34.34.1.1.1.2.1.3.cmml" xref="S3.Ex1.m1.35.35.2.34.15.15.15.1.2"><ci id="S3.Ex1.m1.21.21.21.7.7.7.cmml" xref="S3.Ex1.m1.21.21.21.7.7.7">⋅</ci><ci id="S3.Ex1.m1.20.20.20.6.6.6.cmml" xref="S3.Ex1.m1.20.20.20.6.6.6">𝛼</ci><ci id="S3.Ex1.m1.22.22.22.8.8.8.cmml" xref="S3.Ex1.m1.22.22.22.8.8.8">𝑓</ci></apply><apply id="S3.Ex1.m1.34.34.1.1.1.2.1.1.1.1.cmml" xref="S3.Ex1.m1.35.35.2.34.15.15.15.1.2"><csymbol cd="latexml" id="S3.Ex1.m1.25.25.25.11.11.11.cmml" xref="S3.Ex1.m1.25.25.25.11.11.11">direct-product</csymbol><ci id="S3.Ex1.m1.24.24.24.10.10.10.cmml" xref="S3.Ex1.m1.24.24.24.10.10.10">𝑛</ci><apply id="S3.Ex1.m1.34.34.1.1.1.2.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.35.35.2.34.15.15.15.1.2"><minus id="S3.Ex1.m1.28.28.28.14.14.14.cmml" xref="S3.Ex1.m1.28.28.28.14.14.14"></minus><cn type="integer" id="S3.Ex1.m1.27.27.27.13.13.13.cmml" xref="S3.Ex1.m1.27.27.27.13.13.13">1</cn><apply id="S3.Ex1.m1.34.34.1.1.1.2.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.35.35.2.34.15.15.15.1.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.34.34.1.1.1.2.1.1.1.1.1.1.1.3.1.cmml" xref="S3.Ex1.m1.35.35.2.34.15.15.15.1.2">subscript</csymbol><ci id="S3.Ex1.m1.29.29.29.15.15.15.cmml" xref="S3.Ex1.m1.29.29.29.15.15.15">𝑚</ci><ci id="S3.Ex1.m1.30.30.30.16.16.16.1.cmml" xref="S3.Ex1.m1.30.30.30.16.16.16.1">𝑥</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.36c">\begin{gathered}\bar{x}_{fg}=x+\alpha\cdot f\left(n\odot m_{x}\right)\\
\bar{x}_{bg}=x+\alpha\cdot f\left(n\odot\left(1-m_{x}\right)\right),\end{gathered}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS2.p1.13" class="ltx_p">where <math id="S3.SS2.p1.5.m1.1" class="ltx_Math" alttext="\odot" display="inline"><semantics id="S3.SS2.p1.5.m1.1a"><mo id="S3.SS2.p1.5.m1.1.1" xref="S3.SS2.p1.5.m1.1.1.cmml">⊙</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m1.1b"><csymbol cd="latexml" id="S3.SS2.p1.5.m1.1.1.cmml" xref="S3.SS2.p1.5.m1.1.1">direct-product</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m1.1c">\odot</annotation></semantics></math> is the hadamard product, <math id="S3.SS2.p1.6.m2.1" class="ltx_Math" alttext="m_{x}" display="inline"><semantics id="S3.SS2.p1.6.m2.1a"><msub id="S3.SS2.p1.6.m2.1.1" xref="S3.SS2.p1.6.m2.1.1.cmml"><mi id="S3.SS2.p1.6.m2.1.1.2" xref="S3.SS2.p1.6.m2.1.1.2.cmml">m</mi><mi id="S3.SS2.p1.6.m2.1.1.3" xref="S3.SS2.p1.6.m2.1.1.3.cmml">x</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m2.1b"><apply id="S3.SS2.p1.6.m2.1.1.cmml" xref="S3.SS2.p1.6.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.6.m2.1.1.1.cmml" xref="S3.SS2.p1.6.m2.1.1">subscript</csymbol><ci id="S3.SS2.p1.6.m2.1.1.2.cmml" xref="S3.SS2.p1.6.m2.1.1.2">𝑚</ci><ci id="S3.SS2.p1.6.m2.1.1.3.cmml" xref="S3.SS2.p1.6.m2.1.1.3">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m2.1c">m_{x}</annotation></semantics></math> is the binary mask for sample <math id="S3.SS2.p1.7.m3.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS2.p1.7.m3.1a"><mi id="S3.SS2.p1.7.m3.1.1" xref="S3.SS2.p1.7.m3.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.7.m3.1b"><ci id="S3.SS2.p1.7.m3.1.1.cmml" xref="S3.SS2.p1.7.m3.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.7.m3.1c">x</annotation></semantics></math>, <math id="S3.SS2.p1.8.m4.1" class="ltx_math_unparsed" alttext="f(" display="inline"><semantics id="S3.SS2.p1.8.m4.1a"><mrow id="S3.SS2.p1.8.m4.1b"><mi id="S3.SS2.p1.8.m4.1.1">f</mi><mo stretchy="false" id="S3.SS2.p1.8.m4.1.2">(</mo></mrow><annotation encoding="application/x-tex" id="S3.SS2.p1.8.m4.1c">f(</annotation></semantics></math>.<math id="S3.SS2.p1.9.m5.1" class="ltx_Math" alttext=")" display="inline"><semantics id="S3.SS2.p1.9.m5.1a"><mo stretchy="false" id="S3.SS2.p1.9.m5.1.1" xref="S3.SS2.p1.9.m5.1.1.cmml">)</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.9.m5.1b"><ci id="S3.SS2.p1.9.m5.1.1.cmml" xref="S3.SS2.p1.9.m5.1.1">)</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.9.m5.1c">)</annotation></semantics></math> is the noise <math id="S3.SS2.p1.10.m6.1" class="ltx_Math" alttext="\ell_{2}" display="inline"><semantics id="S3.SS2.p1.10.m6.1a"><msub id="S3.SS2.p1.10.m6.1.1" xref="S3.SS2.p1.10.m6.1.1.cmml"><mi mathvariant="normal" id="S3.SS2.p1.10.m6.1.1.2" xref="S3.SS2.p1.10.m6.1.1.2.cmml">ℓ</mi><mn id="S3.SS2.p1.10.m6.1.1.3" xref="S3.SS2.p1.10.m6.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.10.m6.1b"><apply id="S3.SS2.p1.10.m6.1.1.cmml" xref="S3.SS2.p1.10.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.10.m6.1.1.1.cmml" xref="S3.SS2.p1.10.m6.1.1">subscript</csymbol><ci id="S3.SS2.p1.10.m6.1.1.2.cmml" xref="S3.SS2.p1.10.m6.1.1.2">ℓ</ci><cn type="integer" id="S3.SS2.p1.10.m6.1.1.3.cmml" xref="S3.SS2.p1.10.m6.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.10.m6.1c">\ell_{2}</annotation></semantics></math> normalization function, and <math id="S3.SS2.p1.11.m7.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS2.p1.11.m7.1a"><mi id="S3.SS2.p1.11.m7.1.1" xref="S3.SS2.p1.11.m7.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.11.m7.1b"><ci id="S3.SS2.p1.11.m7.1.1.cmml" xref="S3.SS2.p1.11.m7.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.11.m7.1c">\alpha</annotation></semantics></math> is a scalar parameter controlling noise level. <math id="S3.SS2.p1.12.m8.1" class="ltx_Math" alttext="\ell_{2}" display="inline"><semantics id="S3.SS2.p1.12.m8.1a"><msub id="S3.SS2.p1.12.m8.1.1" xref="S3.SS2.p1.12.m8.1.1.cmml"><mi mathvariant="normal" id="S3.SS2.p1.12.m8.1.1.2" xref="S3.SS2.p1.12.m8.1.1.2.cmml">ℓ</mi><mn id="S3.SS2.p1.12.m8.1.1.3" xref="S3.SS2.p1.12.m8.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.12.m8.1b"><apply id="S3.SS2.p1.12.m8.1.1.cmml" xref="S3.SS2.p1.12.m8.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.12.m8.1.1.1.cmml" xref="S3.SS2.p1.12.m8.1.1">subscript</csymbol><ci id="S3.SS2.p1.12.m8.1.1.2.cmml" xref="S3.SS2.p1.12.m8.1.1.2">ℓ</ci><cn type="integer" id="S3.SS2.p1.12.m8.1.1.3.cmml" xref="S3.SS2.p1.12.m8.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.12.m8.1c">\ell_{2}</annotation></semantics></math> normalization is applied to ensure that the magnitude of noise in the foreground and background remains consistent, thereby enabling fair comparisons between them.
We consider three different levels of noise <math id="S3.SS2.p1.13.m9.3" class="ltx_Math" alttext="\alpha\in\{10,100,250\}" display="inline"><semantics id="S3.SS2.p1.13.m9.3a"><mrow id="S3.SS2.p1.13.m9.3.4" xref="S3.SS2.p1.13.m9.3.4.cmml"><mi id="S3.SS2.p1.13.m9.3.4.2" xref="S3.SS2.p1.13.m9.3.4.2.cmml">α</mi><mo id="S3.SS2.p1.13.m9.3.4.1" xref="S3.SS2.p1.13.m9.3.4.1.cmml">∈</mo><mrow id="S3.SS2.p1.13.m9.3.4.3.2" xref="S3.SS2.p1.13.m9.3.4.3.1.cmml"><mo stretchy="false" id="S3.SS2.p1.13.m9.3.4.3.2.1" xref="S3.SS2.p1.13.m9.3.4.3.1.cmml">{</mo><mn id="S3.SS2.p1.13.m9.1.1" xref="S3.SS2.p1.13.m9.1.1.cmml">10</mn><mo id="S3.SS2.p1.13.m9.3.4.3.2.2" xref="S3.SS2.p1.13.m9.3.4.3.1.cmml">,</mo><mn id="S3.SS2.p1.13.m9.2.2" xref="S3.SS2.p1.13.m9.2.2.cmml">100</mn><mo id="S3.SS2.p1.13.m9.3.4.3.2.3" xref="S3.SS2.p1.13.m9.3.4.3.1.cmml">,</mo><mn id="S3.SS2.p1.13.m9.3.3" xref="S3.SS2.p1.13.m9.3.3.cmml">250</mn><mo stretchy="false" id="S3.SS2.p1.13.m9.3.4.3.2.4" xref="S3.SS2.p1.13.m9.3.4.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.13.m9.3b"><apply id="S3.SS2.p1.13.m9.3.4.cmml" xref="S3.SS2.p1.13.m9.3.4"><in id="S3.SS2.p1.13.m9.3.4.1.cmml" xref="S3.SS2.p1.13.m9.3.4.1"></in><ci id="S3.SS2.p1.13.m9.3.4.2.cmml" xref="S3.SS2.p1.13.m9.3.4.2">𝛼</ci><set id="S3.SS2.p1.13.m9.3.4.3.1.cmml" xref="S3.SS2.p1.13.m9.3.4.3.2"><cn type="integer" id="S3.SS2.p1.13.m9.1.1.cmml" xref="S3.SS2.p1.13.m9.1.1">10</cn><cn type="integer" id="S3.SS2.p1.13.m9.2.2.cmml" xref="S3.SS2.p1.13.m9.2.2">100</cn><cn type="integer" id="S3.SS2.p1.13.m9.3.3.cmml" xref="S3.SS2.p1.13.m9.3.3">250</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.13.m9.3c">\alpha\in\{10,100,250\}</annotation></semantics></math> for evaluating the models’ sensitivity. <a href="#S3.F4" title="In 3.1 Effect of spuriosity Rankings ‣ 3 Experiments ‣ Spuriosity Rankings for Free: A Simple Framework for Last Layer Retraining Based on Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4</span></a> shows the results of our experiment, where the <span id="S3.SS2.p1.13.1" class="ltx_text ltx_font_italic">top 100</span> models consistently exhibit greater robustness than the <span id="S3.SS2.p1.13.2" class="ltx_text ltx_font_italic">bot 100</span> models under foreground and background noise for both ResNet and VIT models. The <span id="S3.SS2.p1.13.3" class="ltx_text ltx_font_italic">top 100</span> VIT model outperforms the <span id="S3.SS2.p1.13.4" class="ltx_text ltx_font_italic">rnd 100</span> and <span id="S3.SS2.p1.13.5" class="ltx_text ltx_font_italic">mid 100</span> models, while the <span id="S3.SS2.p1.13.6" class="ltx_text ltx_font_italic">mid 100</span> and <span id="S3.SS2.p1.13.7" class="ltx_text ltx_font_italic">rnd 100</span> ResNet models demonstrate greater robustness due to their ability to learn more robust features of the foreground region in the presence of low-level noise. The results suggest that the <span id="S3.SS2.p1.13.8" class="ltx_text ltx_font_italic">top 100</span> approach enables ResNet to achieve greater robustness on high-level noisy foreground samples, which confirms the effectiveness of our approach.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Model Performance on OOD Data</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">In this experiment, we aim to evaluate the performance of various models on OOD datasets, specifically ImageNet-A. The models used are the same as those trained in section <a href="#S3.SS1" title="3.1 Effect of spuriosity Rankings ‣ 3 Experiments ‣ Spuriosity Rankings for Free: A Simple Framework for Last Layer Retraining Based on Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>, and their accuracy is assessed on both in-distribution and OOD data. The results are presented in <a href="#S3.F5" title="In 3.3 Model Performance on OOD Data ‣ 3 Experiments ‣ Spuriosity Rankings for Free: A Simple Framework for Last Layer Retraining Based on Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5</span></a>, indicating that VIT models outperformed convolutional models on OOD dataset. Moreover, models trained on <span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_italic">bot k</span> achieved higher accuracy on ImageNet-A, possibly due to the similarity in image style between ImageNet-A and the training data for <span id="S3.SS3.p1.1.2" class="ltx_text ltx_font_italic">bot k</span> models. This suggests that the <span id="S3.SS3.p1.1.3" class="ltx_text ltx_font_italic">bot k</span> models were able to effectively capture the underlying patterns and characteristics of the ImageNet-A images, resulting in improved performance on this specific OOD dataset. Additionally, models trained on <span id="S3.SS3.p1.1.4" class="ltx_text ltx_font_italic">mid k</span> and <span id="S3.SS3.p1.1.5" class="ltx_text ltx_font_italic">rnd k</span>, which include different image styles, demonstrate superior overall performance compared to other models in both in-distribution and OOD settings.</p>
</div>
<figure id="S3.F5" class="ltx_figure">
<p id="S3.F5.1.1" class="ltx_p ltx_align_center ltx_align_center"><span id="S3.F5.1.1.1" class="ltx_text"><img src="/html/2311.00079/assets/Figures/ood.png" id="S3.F5.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="200" height="150" alt="Refer to caption"></span></p>
<br class="ltx_break ltx_break">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The performance comparison of different models on OOD dataset (ImageNet-A) and in-distribution setting. VIT models outperform others on OOD data, while the <span id="S3.F5.5.1" class="ltx_text ltx_font_italic">bot k</span> subset models achieve superior results on ImageNet-A; <span id="S3.F5.6.2" class="ltx_text ltx_font_italic">mid k</span> and <span id="S3.F5.7.3" class="ltx_text ltx_font_italic">rnd k</span> subset models demonstrate strong performance across both in-distribution and OOD scenarios.</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In conclusion, our study introduced a novel method for ranking images based on their spuriosity within classes without human supervision. Through comprehensive experiments, we examined the impact of our approach on model training, assessed model sensitivity to foreground and background noise, and evaluated model performance on OOD data. The results demonstrate the efficacy of our method in enhancing model’s robustness and performance in the presence of spurious cues. By eliminating the need for human supervision, our approach presents a promising solution for tackling spurious features and strengthening the robustness of deep neural networks in practical applications.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alcorn et al. (2019)</span>
<span class="ltx_bibblock">
Alcorn, M. A., Li, Q., Gong, Z., Wang, C., Mai, L., Ku, W., and Nguyen, A.

</span>
<span class="ltx_bibblock">Strike (with) a pose: Neural networks are easily fooled by strange
poses of familiar objects.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern
Recognition, CVPR</em>, 2019.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/CVPR.2019.00498</span>.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="http://openaccess.thecvf.com/content_CVPR_2019/html/Alcorn_Strike_With_a_Pose_Neural_Networks_Are_Easily_Fooled_by_CVPR_2019_paper.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://openaccess.thecvf.com/content_CVPR_2019/html/Alcorn_Strike_With_a_Pose_Neural_Networks_Are_Easily_Fooled_by_CVPR_2019_paper.html</a>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arjovsky et al. (2019)</span>
<span class="ltx_bibblock">
Arjovsky, M., Bottou, L., Gulrajani, I., and Lopez-Paz, D.

</span>
<span class="ltx_bibblock">Invariant risk minimization.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/1907.02893, 2019.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://arxiv.org/abs/1907.02893" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1907.02893</a>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aubin et al. (2021)</span>
<span class="ltx_bibblock">
Aubin, B., Slowik, A., Arjovsky, M., Bottou, L., and Lopez-Paz, D.

</span>
<span class="ltx_bibblock">Linear unit-tests for invariance discovery.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2102.10867, 2021.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2102.10867" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2102.10867</a>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al. (2009)</span>
<span class="ltx_bibblock">
Deng, J., Dong, W., Socher, R., Li, L., Li, K., and Fei-Fei, L.

</span>
<span class="ltx_bibblock">Imagenet: A large-scale hierarchical image database.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">2009 IEEE Computer Society Conference on Computer Vision
and Pattern Recognition CVPR</em>, 2009.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/CVPR.2009.5206848</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1109/CVPR.2009.5206848" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/CVPR.2009.5206848</a>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy et al. (2021)</span>
<span class="ltx_bibblock">
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S.,
Uszkoreit, J., and Houlsby, N.

</span>
<span class="ltx_bibblock">An image is worth 16x16 words: Transformers for image recognition at
scale, 2021.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Frome et al. (2013)</span>
<span class="ltx_bibblock">
Frome, A., Corrado, G. S., Shlens, J., Bengio, S., Dean, J., Ranzato, M., and
Mikolov, T.

</span>
<span class="ltx_bibblock">Devise: A deep visual-semantic embedding model.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems,
NeurIPS</em>, 2013.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="https://proceedings.neurips.cc/paper/2013/hash/7cce53cf90577442771720a370c3c723-Abstract.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.neurips.cc/paper/2013/hash/7cce53cf90577442771720a370c3c723-Abstract.html</a>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geirhos et al. (2020)</span>
<span class="ltx_bibblock">
Geirhos, R., Jacobsen, J., Michaelis, C., Zemel, R. S., Brendel, W., Bethge,
M., and Wichmann, F. A.

</span>
<span class="ltx_bibblock">Shortcut learning in deep neural networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2004.07780, 2020.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2004.07780" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2004.07780</a>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et al. (2022)</span>
<span class="ltx_bibblock">
Gu, X., Lin, T., Kuo, W., and Cui, Y.

</span>
<span class="ltx_bibblock">Open-vocabulary object detection via vision and language knowledge
distillation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">The Tenth International Conference on Learning
Representations, ICLR</em>, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=lL3lnMbR4WU" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=lL3lnMbR4WU</a>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2018)</span>
<span class="ltx_bibblock">
Hu, W., Niu, G., Sato, I., and Sugiyama, M.

</span>
<span class="ltx_bibblock">Does distributionally robust supervised learning give robust
classifiers?

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 35th International Conference on Machine
Learning, ICML</em>, 2018.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://proceedings.mlr.press/v80/hu18a.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://proceedings.mlr.press/v80/hu18a.html</a>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kamath et al. (2021)</span>
<span class="ltx_bibblock">
Kamath, A., Singh, M., LeCun, Y., Synnaeve, G., Misra, I., and Carion, N.

</span>
<span class="ltx_bibblock">MDETR - modulated detection for end-to-end multi-modal
understanding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">IEEE/CVF International Conference on Computer Vision,
ICCV</em>, 2021.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/ICCV48922.2021.00180</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1109/ICCV48922.2021.00180" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/ICCV48922.2021.00180</a>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kirichenko et al. (2023)</span>
<span class="ltx_bibblock">
Kirichenko, P., Izmailov, P., and Wilson, A. G.

</span>
<span class="ltx_bibblock">Last layer re-training is sufficient for robustness to spurious
correlations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">The Eleventh International Conference on Learning
Representations, ICLR</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=Zb6c8A-Fghk" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=Zb6c8A-Fghk</a>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2022)</span>
<span class="ltx_bibblock">
Lee, Y., Chen, A. S., Tajwar, F., Kumar, A., Yao, H., Liang, P., and Finn, C.

</span>
<span class="ltx_bibblock">Surgical fine-tuning improves adaptation to distribution shifts.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2210.11466, 2022.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.48550/arXiv.2210.11466</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.48550/arXiv.2210.11466" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2210.11466</a>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2021)</span>
<span class="ltx_bibblock">
Liu, E. Z., Haghgoo, B., Chen, A. S., Raghunathan, A., Koh, P. W., Sagawa, S.,
Liang, P., and Finn, C.

</span>
<span class="ltx_bibblock">Just train twice: Improving group robustness without training group
information.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 38th International Conference on Machine
Learning, ICML</em>, 2021.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://proceedings.mlr.press/v139/liu21f.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://proceedings.mlr.press/v139/liu21f.html</a>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Minderer et al. (2022)</span>
<span class="ltx_bibblock">
Minderer, M., Gritsenko, A. A., Stone, A., Neumann, M., Weissenborn, D.,
Dosovitskiy, A., Mahendran, A., Arnab, A., Dehghani, M., Shen, Z., Wang, X.,
Zhai, X., Kipf, T., and Houlsby, N.

</span>
<span class="ltx_bibblock">Simple open-vocabulary object detection with vision transformers.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision, ECCV</em>, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.48550/arXiv.2205.06230" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2205.06230</a>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moayeri et al. (2022)</span>
<span class="ltx_bibblock">
Moayeri, M., Wang, W., Singla, S., and Feizi, S.

</span>
<span class="ltx_bibblock">Spuriosity rankings: Sorting data for spurious correlation
robustness.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2212.02648, 2022.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.48550/arXiv.2212.02648</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.48550/arXiv.2212.02648" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2212.02648</a>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Neuhaus et al. (2022)</span>
<span class="ltx_bibblock">
Neuhaus, Y., Augustin, M., Boreiko, V., and Hein, M.

</span>
<span class="ltx_bibblock">Spurious features everywhere - large-scale detection of harmful
spurious features in imagenet.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2212.04871, 2022.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.48550/arXiv.2212.04871</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.48550/arXiv.2212.04871" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2212.04871</a>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robey et al. (2021)</span>
<span class="ltx_bibblock">
Robey, A., Pappas, G. J., and Hassani, H.

</span>
<span class="ltx_bibblock">Model-based domain generalization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems,
NeurIPS</em>, 2021.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="https://proceedings.neurips.cc/paper/2021/hash/a8f12d9486cbcc2fe0cfc5352011ad35-Abstract.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.neurips.cc/paper/2021/hash/a8f12d9486cbcc2fe0cfc5352011ad35-Abstract.html</a>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rosenfeld et al. (2022)</span>
<span class="ltx_bibblock">
Rosenfeld, E., Ravikumar, P., and Risteski, A.

</span>
<span class="ltx_bibblock">Domain-adjusted regression or: ERM may already learn features
sufficient for out-of-distribution generalization.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2202.06856, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2202.06856" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2202.06856</a>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sagawa et al. (2019)</span>
<span class="ltx_bibblock">
Sagawa, S., Koh, P. W., Hashimoto, T. B., and Liang, P.

</span>
<span class="ltx_bibblock">Distributionally robust neural networks for group shifts: On the
importance of regularization for worst-case generalization.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/1911.08731, 2019.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://arxiv.org/abs/1911.08731" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1911.08731</a>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Seo et al. (2022)</span>
<span class="ltx_bibblock">
Seo, S., Lee, J., and Han, B.

</span>
<span class="ltx_bibblock">Information-theoretic bias reduction via causal view of spurious
correlation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Thirty-Sixth AAAI Conference on Artificial Intelligence</em>,
2022.

</span>
<span class="ltx_bibblock">ISBN 978-1-57735-876-3.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://ojs.aaai.org/index.php/AAAI/article/view/20115" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ojs.aaai.org/index.php/AAAI/article/view/20115</a>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shah et al. (2020)</span>
<span class="ltx_bibblock">
Shah, H., Tamuly, K., Raghunathan, A., Jain, P., and Netrapalli, P.

</span>
<span class="ltx_bibblock">The pitfalls of simplicity bias in neural networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems,
NeurIPS</em>, 2020.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="https://proceedings.neurips.cc/paper/2020/hash/6cfe0e6127fa25df2a0ef2ae1067d915-Abstract.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.neurips.cc/paper/2020/hash/6cfe0e6127fa25df2a0ef2ae1067d915-Abstract.html</a>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singla &amp; Feizi (2022)</span>
<span class="ltx_bibblock">
Singla, S. and Feizi, S.

</span>
<span class="ltx_bibblock">Salient imagenet: How to discover spurious features in deep learning?

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">The Tenth International Conference on Learning
Representations, ICLR</em>, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=XVPqLyNxSyh" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=XVPqLyNxSyh</a>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singla et al. (2022)</span>
<span class="ltx_bibblock">
Singla, S., Moayeri, M., and Feizi, S.

</span>
<span class="ltx_bibblock">Core risk minimization using salient imagenet.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2203.15566, 2022.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.48550/arXiv.2203.15566</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.48550/arXiv.2203.15566" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2203.15566</a>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2021)</span>
<span class="ltx_bibblock">
Zhang, J., Menon, A. K., Veit, A., Bhojanapalli, S., Kumar, S., and Sra, S.

</span>
<span class="ltx_bibblock">Coping with label shift via distributionally robust optimisation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">9th International Conference on Learning Representations,
ICLR</em>, 2021.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=BtZhsSGNRNi" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=BtZhsSGNRNi</a>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>

<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Experimental Analysis of Training Subsets and spuriosity Ranking on Model Performance</h3>

<div id="A1.SS1.p1" class="ltx_para">
<p id="A1.SS1.p1.1" class="ltx_p">We conducted comprehensive experiments using two specific values of k, k=50 and k=200, to further investigate the impact of training subsets on model’s performance. The models were trained using the same method described in section <a href="#S3.SS1" title="3.1 Effect of spuriosity Rankings ‣ 3 Experiments ‣ Spuriosity Rankings for Free: A Simple Framework for Last Layer Retraining Based on Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>, on the proportion of the data sorted based on spuriosity ranking. For k=50, the evaluation results revealed that the model trained on the <span id="A1.SS1.p1.1.1" class="ltx_text ltx_font_italic">top 50</span> subset exhibited superior performance on low-ranking data, showing its ability to effectively capture and generalize patterns from such samples. On the other hand, the model trained on the <span id="A1.SS1.p1.1.2" class="ltx_text ltx_font_italic">bot 50</span> subset demonstrated increased resilience to accuracy changes when assessed on data with varying spurious rankings. This indicates that the model trained on the <span id="A1.SS1.p1.1.3" class="ltx_text ltx_font_italic">bot 50</span> subset is more robust and less influenced by spurious cues present in the test data.</p>
</div>
<div id="A1.SS1.p2" class="ltx_para">
<p id="A1.SS1.p2.1" class="ltx_p">Similarly, for k=200, we observed a similar trend. The model trained on the <span id="A1.SS1.p2.1.1" class="ltx_text ltx_font_italic">top 200</span> subset displayed better performance on low-ranking data, while the model trained on the <span id="A1.SS1.p2.1.2" class="ltx_text ltx_font_italic">bot 200</span> subset exhibited higher resilience to accuracy changes when evaluated on data with varying spurious rankings.</p>
</div>
<div id="A1.SS1.p3" class="ltx_para">
<p id="A1.SS1.p3.1" class="ltx_p">These findings highlight the impact of different training data subsets and spuriosity rankings on the model’s performance. The models trained on subsets containing low-ranking samples prioritize accuracy on those data points, while the models trained on subsets with high-ranking samples are more robust to changes in spurious correlations. These insights contribute to a deeper understanding of the relationship between training data subsets, spuriosity rankings, and model performance.
The detailed visualizations and analysis of the results for k=50 and k=200 are presented in <a href="#A1.F6" title="In A.1 Experimental Analysis of Training Subsets and spuriosity Ranking on Model Performance ‣ Appendix A Appendix ‣ Spuriosity Rankings for Free: A Simple Framework for Last Layer Retraining Based on Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<figure id="A1.F6" class="ltx_figure"><img src="/html/2311.00079/assets/x1.png" id="A1.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="346" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Impact of training subsets (k=50, 200) on classification accuracy for validation samples with Varying spuriosity ranks. The plot illustrates how different subsets of training data, each containing 50 and 200 samples per class respectively, influence the accuracy of classification. The dashed horizontal lines represent the average accuracy levels achieved by each model
on different data subsets.</figcaption>
</figure>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>
Further Illustrations of spuriosity Rankings</h3>

<div id="A1.SS2.p1" class="ltx_para">
<p id="A1.SS2.p1.1" class="ltx_p">In this part, we present additional examples that highlight the concept of spuriosity rankings, their implications for object detectability, and the presence of spurious cues for some random classes. A wide range of images is shown in <a href="#A1.F7" title="Figure 7 ‣ A.2 Further Illustrations of spuriosity Rankings ‣ Appendix A Appendix ‣ Spuriosity Rankings for Free: A Simple Framework for Last Layer Retraining Based on Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> ranging from clear images with low spuriosity to hard-to-distinguish cases with high spuriosity.</p>
</div>
<figure id="A1.F7" class="ltx_figure"><img src="/html/2311.00079/assets/Figures/Rankings.jpg" id="A1.F7.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="440" height="576" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Examples of Spuriosity Rankings: Illustrating object detectability and spurious cues. The images on the left side demonstrate high spuriosity, showing cases where spurious cues are present, making the object difficult to distinguish. Conversely, the images on the right side exhibit low spurious cues, displaying clear images of the object.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2311.00078" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2311.00079" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2311.00079">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2311.00079" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2311.00080" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 22:47:58 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
