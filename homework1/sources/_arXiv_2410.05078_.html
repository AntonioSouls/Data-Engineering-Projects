<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data</title>
<!--Generated on Mon Oct  7 14:25:00 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.05078v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S1" title="In Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S1.SS0.SSS0.Px1" title="In 1 Introduction â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Main Contributions</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S2" title="In Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Background</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S3" title="In Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S3.SS0.SSS0.Px1" title="In 3 Related Work â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Compression Without Transformers</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S3.SS0.SSS0.Px2" title="In 3 Related Work â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Online Transformers</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S3.SS0.SSS0.Px3" title="In 3 Related Work â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Pre-Trained Transformers</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S4" title="In Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S4.SS0.SSS0.Px1" title="In 4 Methods â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Baselines</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S4.SS0.SSS0.Px2" title="In 4 Methods â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S4.SS0.SSS0.Px3" title="In 4 Methods â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">(No) Tokenization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S4.SS0.SSS0.Px4" title="In 4 Methods â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S4.SS0.SSS0.Px5" title="In 4 Methods â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Training Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S4.SS0.SSS0.Px6" title="In 4 Methods â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Out-of-Distribution Evaluation Datasets</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5" title="In Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.SS0.SSS0.Px1" title="In 5 Results â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Small Transformers Can Be Domain-General Compressors</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.SS0.SSS0.Px2" title="In 5 Results â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">What You See Is What You Get</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.SS0.SSS0.Px3" title="In 5 Results â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Scaling Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.SS0.SSS0.Px4" title="In 5 Results â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Model Size vs.Â Context Size</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.SS0.SSS0.Px5" title="In 5 Results â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Sliding Window</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S6" title="In Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S6.SS0.SSS0.Px1" title="In 6 Discussion â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Limitations</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S7" title="In Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A1" title="In Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Experimental Details</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A1.SS1" title="In Appendix A Experimental Details â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Training Data Sources</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A1.SS1.SSS0.Px1" title="In A.1 Training Data Sources â€£ Appendix A Experimental Details â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Text</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A1.SS1.SSS0.Px2" title="In A.1 Training Data Sources â€£ Appendix A Experimental Details â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Image</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A1.SS1.SSS0.Px3" title="In A.1 Training Data Sources â€£ Appendix A Experimental Details â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Audio</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A1.SS2" title="In Appendix A Experimental Details â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Out-of-Distribution Evaluation Data Sources</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A1.SS2.SSS0.Px1" title="In A.2 Out-of-Distribution Evaluation Data Sources â€£ Appendix A Experimental Details â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Text</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A1.SS2.SSS0.Px2" title="In A.2 Out-of-Distribution Evaluation Data Sources â€£ Appendix A Experimental Details â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Images</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A1.SS2.SSS0.Px3" title="In A.2 Out-of-Distribution Evaluation Data Sources â€£ Appendix A Experimental Details â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Audio</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A1.SS2.SSS0.Px4" title="In A.2 Out-of-Distribution Evaluation Data Sources â€£ Appendix A Experimental Details â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Multimodal Evaluations</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A1.SS3" title="In Appendix A Experimental Details â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Sweeps</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A1.SS3.SSS0.Px1" title="In A.3 Sweeps â€£ Appendix A Experimental Details â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Model Size vs.Â Dataset Size</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A1.SS3.SSS0.Px2" title="In A.3 Sweeps â€£ Appendix A Experimental Details â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title">Model Size vs.Â Context Size</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A1.SS4" title="In Appendix A Experimental Details â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.4 </span>Computational Resources</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A2" title="In Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Additional Results</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A2.SS1" title="In Appendix B Additional Results â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>Compression Ratios</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A2.SS2" title="In Appendix B Additional Results â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.2 </span>Running Times</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A2.SS3" title="In Appendix B Additional Results â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.3 </span>Compressing Model Parameters</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A2.SS4" title="In Appendix B Additional Results â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.4 </span>Scaling Analysis for Multimodal Training</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.5">\correspondingauthor</span>
<p class="ltx_p" id="p1.4"><sup class="ltx_sup" id="p1.4.1">âˆ—</sup>Equal contribution.
Correspondence to {anianr, timgen}@google.com.
<sup class="ltx_sup" id="p1.4.2">1</sup>Chandar Research Lab. MILA - Quebec AI Institute. Polytechnique MontrÃ©al.
<sup class="ltx_sup" id="p1.4.3">2</sup>Google DeepMind.
<math alttext="\dagger" class="ltx_Math" display="inline" id="p1.4.m4.1"><semantics id="p1.4.m4.1a"><mo id="p1.4.m4.1.1" xref="p1.4.m4.1.1.cmml">â€ </mo><annotation-xml encoding="MathML-Content" id="p1.4.m4.1b"><ci id="p1.4.m4.1.1.cmml" xref="p1.4.m4.1.1">â€ </ci></annotation-xml><annotation encoding="application/x-tex" id="p1.4.m4.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="p1.4.m4.1d">â€ </annotation></semantics></math> Work performed while the author was at Google DeepMind.</p>
</div>
<h1 class="ltx_title ltx_title_document">Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">David Heurtel-Depeiges<sup class="ltx_sup" id="id5.2.id1"><span class="ltx_text ltx_font_italic" id="id5.2.id1.1">âˆ—1â€ </span></sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Anian Ruoss<sup class="ltx_sup" id="id6.2.id1"><span class="ltx_text ltx_font_italic" id="id6.2.id1.1">âˆ—2</span></sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Joel Veness<sup class="ltx_sup" id="id7.2.id1"><span class="ltx_text ltx_font_italic" id="id7.2.id1.1">2</span></sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Tim Genewein<sup class="ltx_sup" id="id8.2.id1"><span class="ltx_text ltx_font_italic" id="id8.2.id1.1">2</span></sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id9.id1">Foundation models have recently been shown to be strong data compressors.
However, when accounting for their excessive parameter count, their compression ratios are actually inferior to standard compression algorithms.
Moreover, naively reducing the number of parameters may not necessarily help as it leads to worse predictions and thus weaker compression.
In this paper, we conduct a large-scale empirical study to investigate whether there is a sweet spot where competitive compression ratios with pre-trained vanilla transformers are possible.
To this end, we train families of models on 165GB of raw byte sequences of either text, image, or audio data (and all possible combinations of the three) and then compress 1GB of out-of-distribution (OOD) data from each modality.
We find that relatively small models (i.e., millions of parameters) can outperform standard general-purpose compression algorithms (gzip, LZMA2) and even domain-specific compressors (PNG, JPEGÂ 2000, FLAC) â€” even when factoring in parameter count.
We achieve, e.g., the lowest compression ratio of 0.49 on OOD audio data (vs.Â 0.54 for FLAC).
To study the impact of model- and dataset scale, we conduct extensive ablations and hyperparameter sweeps, and we investigate the effect of unimodal versus multimodal training.
We find that even small models can be trained to perform well on multiple modalities, but, in contrast to previously reported results with large-scale foundation models, transfer to unseen modalities is generally weak.</p>
</div>
<section class="ltx_section" id="S1" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Strong predictive models can straightforwardly be turned into strong lossless compressors, e.g., via arithmetic codingÂ <cite class="ltx_cite ltx_citemacro_citep">(Pasco, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib52" title="">1977</a>; Rissanen, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib58" title="">1976</a>; Witten etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib81" title="">1987</a>)</cite>.
Consequently, large pre-trained foundation models, such as LLMs, achieve very high data compression on their training distributions and beyondÂ <cite class="ltx_cite ltx_citemacro_citep">(DelÃ©tang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib11" title="">2024</a>)</cite>.
However, when factoring in these modelsâ€™ parameter count into the compression ratio, too large models actually perform worse.
For this reason, large foundation models with parameter counts on the order of billions cannot compete with standard compression algorithms such as gzipÂ <cite class="ltx_cite ltx_citemacro_citep">(Deutsch, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib12" title="">1996</a>)</cite> or LZMA2Â <cite class="ltx_cite ltx_citemacro_citep">(Pavlov, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib53" title="">2019</a>)</cite>.
The goal of this paper is thus to investigate whether pre-trained vanilla transformers can achieve compression ratios that are competitive with standard algorithms across a range of data modalities.
This places fairly tight constraints on the maximal model size, leading us to investigate families of relatively small transformers (with millions of parameters).
Note that our aim is not to build a practical transformer-based data compressor, as the computational footprint (running time, memory, FLOPs) of even small models is far beyond standard compressors.
Instead, studying compression via pre-trained models provides insight into the modelsâ€™ <em class="ltx_emph ltx_font_italic" id="S1.p1.1.1">learned</em> inductive biases, e.g., whether they are domain-general, how they depend on the training data composition, and whether there is transfer between modalities.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.3">Recently, <cite class="ltx_cite ltx_citemacro_citet">DelÃ©tang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib11" title="">2024</a>)</cite> stated that â€œlanguage modeling is compressionâ€, pointing out that log-loss minimization is equivalent to optimizing a lossless compression objective. To illustrate this point, the authors used billion-parameter LLMs that were trained exclusively on text (LlamaÂ 2 from <cite class="ltx_cite ltx_citemacro_citet">Touvron etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib72" title="">2023b</a>)</cite> and Chinchilla from <cite class="ltx_cite ltx_citemacro_citet">Hoffmann etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib22" title="">2022</a>)</cite>) to compress <math alttext="1" class="ltx_Math" display="inline" id="S1.p2.1.m1.1"><semantics id="S1.p2.1.m1.1a"><mn id="S1.p2.1.m1.1.1" xref="S1.p2.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S1.p2.1.m1.1b"><cn id="S1.p2.1.m1.1.1.cmml" type="integer" xref="S1.p2.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.1.m1.1c">1</annotation><annotation encoding="application/x-llamapun" id="S1.p2.1.m1.1d">1</annotation></semantics></math>GB of image and audio data from ImageNetÂ <cite class="ltx_cite ltx_citemacro_citep">(Russakovsky etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib59" title="">2015</a>)</cite> and LibriSpeechÂ <cite class="ltx_cite ltx_citemacro_citep">(Panayotov etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib51" title="">2015</a>)</cite>, respectively.
They found that these models compress better than gzip or LZMA2 and even domain-specific compressors such as PNGÂ <cite class="ltx_cite ltx_citemacro_citep">(Boutell, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib5" title="">1997</a>)</cite> and FLACÂ <cite class="ltx_cite ltx_citemacro_citep">(Coalson, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib8" title="">2008</a>)</cite>, but only when parameter counts are not being accounted for.
To see if competitive performance is possible, they also trained small-scale transformers (up to <math alttext="3.2" class="ltx_Math" display="inline" id="S1.p2.2.m2.1"><semantics id="S1.p2.2.m2.1a"><mn id="S1.p2.2.m2.1.1" xref="S1.p2.2.m2.1.1.cmml">3.2</mn><annotation-xml encoding="MathML-Content" id="S1.p2.2.m2.1b"><cn id="S1.p2.2.m2.1.1.cmml" type="float" xref="S1.p2.2.m2.1.1">3.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.2.m2.1c">3.2</annotation><annotation encoding="application/x-llamapun" id="S1.p2.2.m2.1d">3.2</annotation></semantics></math>M parameters) on <math alttext="1" class="ltx_Math" display="inline" id="S1.p2.3.m3.1"><semantics id="S1.p2.3.m3.1a"><mn id="S1.p2.3.m3.1.1" xref="S1.p2.3.m3.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S1.p2.3.m3.1b"><cn id="S1.p2.3.m3.1.1.cmml" type="integer" xref="S1.p2.3.m3.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.3.m3.1c">1</annotation><annotation encoding="application/x-llamapun" id="S1.p2.3.m3.1d">1</annotation></semantics></math>GB of WikipediaÂ <cite class="ltx_cite ltx_citemacro_citep">(Hutter, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib26" title="">2006</a>)</cite>, but found that these models were significantly worse at compressing images and audio data.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="359" id="S1.F1.1.g1" src="x1.png" width="664"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>
Overview of our training and evaluation data pipelines.
We consider three data modalities: text, images, and audio.
From these modalities we create training data mixtures of <math alttext="165" class="ltx_Math" display="inline" id="S1.F1.3.m1.1"><semantics id="S1.F1.3.m1.1b"><mn id="S1.F1.3.m1.1.1" xref="S1.F1.3.m1.1.1.cmml">165</mn><annotation-xml encoding="MathML-Content" id="S1.F1.3.m1.1c"><cn id="S1.F1.3.m1.1.1.cmml" type="integer" xref="S1.F1.3.m1.1.1">165</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.3.m1.1d">165</annotation><annotation encoding="application/x-llamapun" id="S1.F1.3.m1.1e">165</annotation></semantics></math>GB that are either unimodal or multimodal.
After pre-training transformers on each of these datasets, we evaluate their compression ratio (i.e., factoring in modelsâ€™ parameter counts) on each of the three modalities.
If the corresponding modality has not been seen during training, we refer to the evaluation as â€˜out-of-modalityâ€™, otherwise it is â€˜in-modalityâ€™.
Importantly, our evaluation is always performed on out-of-distribution data (different from any of the training data sources), even when it is in-modality.
</figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.2">The obvious open question is whether small transformers pre-trained on large (multimodal) datasets can achieve competitive compression ratios across different modalities and whether there is transfer to unseen modalities, as observed in the large-scale model case.
We therefore conduct an extensive empirical study where we train families of decoder-only transformers on <math alttext="165" class="ltx_Math" display="inline" id="S1.p3.1.m1.1"><semantics id="S1.p3.1.m1.1a"><mn id="S1.p3.1.m1.1.1" xref="S1.p3.1.m1.1.1.cmml">165</mn><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b"><cn id="S1.p3.1.m1.1.1.cmml" type="integer" xref="S1.p3.1.m1.1.1">165</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.1.m1.1c">165</annotation><annotation encoding="application/x-llamapun" id="S1.p3.1.m1.1d">165</annotation></semantics></math>GB of either text, image, or audio data and all combinations of the three. We then use these models (with frozen parameters, i.e., offline training) to compress <math alttext="1" class="ltx_Math" display="inline" id="S1.p3.2.m2.1"><semantics id="S1.p3.2.m2.1a"><mn id="S1.p3.2.m2.1.1" xref="S1.p3.2.m2.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S1.p3.2.m2.1b"><cn id="S1.p3.2.m2.1.1.cmml" type="integer" xref="S1.p3.2.m2.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.2.m2.1c">1</annotation><annotation encoding="application/x-llamapun" id="S1.p3.2.m2.1d">1</annotation></semantics></math>GB of out-of-distribution (OOD) data from all three modalities (see <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S1.F1" title="In 1 Introduction â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">1</span></a>).
We also compare against transformers that are trained purely online, i.e., on the data stream that is being compressed <cite class="ltx_cite ltx_citemacro_citep">(Bellard, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib3" title="">2019</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib4" title="">2021</a>)</cite>, meaning that storage or communication of the transformer weights for decompression is not required (unlike for our pre-trained models).
These online transformers currently achieve state-of-the-art results on the Large Text Compression BenchmarkÂ <cite class="ltx_cite ltx_citemacro_citep">(Mahoney, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib43" title="">2006</a>)</cite>.
Overall we find that small pre-trained transformers achieve competitive compression ratios, as our best models consistently outperform domain-general and domain-specific standard compression algorithms and are on par with the online transformers from <cite class="ltx_cite ltx_citemacro_citet">Bellard (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib4" title="">2021</a>)</cite>.</p>
</div>
<section class="ltx_paragraph" id="S1.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Main Contributions</h4>
<div class="ltx_para" id="S1.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S1.SS0.SSS0.Px1.p1.1">We make the following key contributions:</p>
</div>
<div class="ltx_para" id="S1.SS0.SSS0.Px1.p2">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We conduct a large-scale empirical study (hyperparameter sweeps, ablations) on the compression performance of small transformers trained on raw byte sequences of text, image, and audio data (and all combinations), across various model- and dataset sizes.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We are the first to show that small pre-trained transformers achieve better compression ratios than general-purpose and domain-specific compressors on <math alttext="1" class="ltx_Math" display="inline" id="S1.I1.i2.p1.1.m1.1"><semantics id="S1.I1.i2.p1.1.m1.1a"><mn id="S1.I1.i2.p1.1.m1.1.1" xref="S1.I1.i2.p1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S1.I1.i2.p1.1.m1.1b"><cn id="S1.I1.i2.p1.1.m1.1.1.cmml" type="integer" xref="S1.I1.i2.p1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i2.p1.1.m1.1c">1</annotation><annotation encoding="application/x-llamapun" id="S1.I1.i2.p1.1.m1.1d">1</annotation></semantics></math>GB of out-of-distribution data across different modalities, e.g., 0.49 on audio vs.Â 0.51 for <cite class="ltx_cite ltx_citemacro_citet">Bellard (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib4" title="">2021</a>)</cite> &amp; 0.54 for FLAC.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We show that training on multiple modalities only slightly deteriorates the performance on each individual modality but significantly boosts the compression ratios on multimodal data, as long as all the evaluation modalities are part of the training data mixture.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">We demonstrate that small pre-trained transformers fail to beat standard compressors on unseen data modalities (i.e., modalities they were not trained on), meaning that there is only weak transfer to novel modalities (which is not the case for LLMsÂ <cite class="ltx_cite ltx_citemacro_citep">(DelÃ©tang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib11" title="">2024</a>)</cite>).</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_section" id="S2" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.8">Compression and prediction are â€œtwo sides of the same coinâ€Â <cite class="ltx_cite ltx_citemacro_citep">(MacKay, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib41" title="">2003</a>)</cite>.
This fundamental duality stems directly from Shannonâ€™s celebrated lossless source coding theoremÂ <cite class="ltx_cite ltx_citemacro_citep">(Shannon, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib64" title="">1948</a>)</cite>, which states that there is a well-defined lower bound for encoding data from a probabilistic source.
For any data sequence <math alttext="{x_{1:n}:=x_{1}x_{2}\ldots x_{n}\in\mathcal{X}^{n}}" class="ltx_Math" display="inline" id="S2.p1.1.m1.1"><semantics id="S2.p1.1.m1.1a"><mrow id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml"><msub id="S2.p1.1.m1.1.1.2" xref="S2.p1.1.m1.1.1.2.cmml"><mi id="S2.p1.1.m1.1.1.2.2" xref="S2.p1.1.m1.1.1.2.2.cmml">x</mi><mrow id="S2.p1.1.m1.1.1.2.3" xref="S2.p1.1.m1.1.1.2.3.cmml"><mn id="S2.p1.1.m1.1.1.2.3.2" xref="S2.p1.1.m1.1.1.2.3.2.cmml">1</mn><mo id="S2.p1.1.m1.1.1.2.3.1" lspace="0.278em" rspace="0.278em" xref="S2.p1.1.m1.1.1.2.3.1.cmml">:</mo><mi id="S2.p1.1.m1.1.1.2.3.3" xref="S2.p1.1.m1.1.1.2.3.3.cmml">n</mi></mrow></msub><mo id="S2.p1.1.m1.1.1.3" lspace="0.278em" rspace="0.278em" xref="S2.p1.1.m1.1.1.3.cmml">:=</mo><mrow id="S2.p1.1.m1.1.1.4" xref="S2.p1.1.m1.1.1.4.cmml"><msub id="S2.p1.1.m1.1.1.4.2" xref="S2.p1.1.m1.1.1.4.2.cmml"><mi id="S2.p1.1.m1.1.1.4.2.2" xref="S2.p1.1.m1.1.1.4.2.2.cmml">x</mi><mn id="S2.p1.1.m1.1.1.4.2.3" xref="S2.p1.1.m1.1.1.4.2.3.cmml">1</mn></msub><mo id="S2.p1.1.m1.1.1.4.1" xref="S2.p1.1.m1.1.1.4.1.cmml">â¢</mo><msub id="S2.p1.1.m1.1.1.4.3" xref="S2.p1.1.m1.1.1.4.3.cmml"><mi id="S2.p1.1.m1.1.1.4.3.2" xref="S2.p1.1.m1.1.1.4.3.2.cmml">x</mi><mn id="S2.p1.1.m1.1.1.4.3.3" xref="S2.p1.1.m1.1.1.4.3.3.cmml">2</mn></msub><mo id="S2.p1.1.m1.1.1.4.1a" xref="S2.p1.1.m1.1.1.4.1.cmml">â¢</mo><mi id="S2.p1.1.m1.1.1.4.4" mathvariant="normal" xref="S2.p1.1.m1.1.1.4.4.cmml">â€¦</mi><mo id="S2.p1.1.m1.1.1.4.1b" xref="S2.p1.1.m1.1.1.4.1.cmml">â¢</mo><msub id="S2.p1.1.m1.1.1.4.5" xref="S2.p1.1.m1.1.1.4.5.cmml"><mi id="S2.p1.1.m1.1.1.4.5.2" xref="S2.p1.1.m1.1.1.4.5.2.cmml">x</mi><mi id="S2.p1.1.m1.1.1.4.5.3" xref="S2.p1.1.m1.1.1.4.5.3.cmml">n</mi></msub></mrow><mo id="S2.p1.1.m1.1.1.5" xref="S2.p1.1.m1.1.1.5.cmml">âˆˆ</mo><msup id="S2.p1.1.m1.1.1.6" xref="S2.p1.1.m1.1.1.6.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.p1.1.m1.1.1.6.2" xref="S2.p1.1.m1.1.1.6.2.cmml">ğ’³</mi><mi id="S2.p1.1.m1.1.1.6.3" xref="S2.p1.1.m1.1.1.6.3.cmml">n</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><apply id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1"><and id="S2.p1.1.m1.1.1a.cmml" xref="S2.p1.1.m1.1.1"></and><apply id="S2.p1.1.m1.1.1b.cmml" xref="S2.p1.1.m1.1.1"><csymbol cd="latexml" id="S2.p1.1.m1.1.1.3.cmml" xref="S2.p1.1.m1.1.1.3">assign</csymbol><apply id="S2.p1.1.m1.1.1.2.cmml" xref="S2.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.p1.1.m1.1.1.2.1.cmml" xref="S2.p1.1.m1.1.1.2">subscript</csymbol><ci id="S2.p1.1.m1.1.1.2.2.cmml" xref="S2.p1.1.m1.1.1.2.2">ğ‘¥</ci><apply id="S2.p1.1.m1.1.1.2.3.cmml" xref="S2.p1.1.m1.1.1.2.3"><ci id="S2.p1.1.m1.1.1.2.3.1.cmml" xref="S2.p1.1.m1.1.1.2.3.1">:</ci><cn id="S2.p1.1.m1.1.1.2.3.2.cmml" type="integer" xref="S2.p1.1.m1.1.1.2.3.2">1</cn><ci id="S2.p1.1.m1.1.1.2.3.3.cmml" xref="S2.p1.1.m1.1.1.2.3.3">ğ‘›</ci></apply></apply><apply id="S2.p1.1.m1.1.1.4.cmml" xref="S2.p1.1.m1.1.1.4"><times id="S2.p1.1.m1.1.1.4.1.cmml" xref="S2.p1.1.m1.1.1.4.1"></times><apply id="S2.p1.1.m1.1.1.4.2.cmml" xref="S2.p1.1.m1.1.1.4.2"><csymbol cd="ambiguous" id="S2.p1.1.m1.1.1.4.2.1.cmml" xref="S2.p1.1.m1.1.1.4.2">subscript</csymbol><ci id="S2.p1.1.m1.1.1.4.2.2.cmml" xref="S2.p1.1.m1.1.1.4.2.2">ğ‘¥</ci><cn id="S2.p1.1.m1.1.1.4.2.3.cmml" type="integer" xref="S2.p1.1.m1.1.1.4.2.3">1</cn></apply><apply id="S2.p1.1.m1.1.1.4.3.cmml" xref="S2.p1.1.m1.1.1.4.3"><csymbol cd="ambiguous" id="S2.p1.1.m1.1.1.4.3.1.cmml" xref="S2.p1.1.m1.1.1.4.3">subscript</csymbol><ci id="S2.p1.1.m1.1.1.4.3.2.cmml" xref="S2.p1.1.m1.1.1.4.3.2">ğ‘¥</ci><cn id="S2.p1.1.m1.1.1.4.3.3.cmml" type="integer" xref="S2.p1.1.m1.1.1.4.3.3">2</cn></apply><ci id="S2.p1.1.m1.1.1.4.4.cmml" xref="S2.p1.1.m1.1.1.4.4">â€¦</ci><apply id="S2.p1.1.m1.1.1.4.5.cmml" xref="S2.p1.1.m1.1.1.4.5"><csymbol cd="ambiguous" id="S2.p1.1.m1.1.1.4.5.1.cmml" xref="S2.p1.1.m1.1.1.4.5">subscript</csymbol><ci id="S2.p1.1.m1.1.1.4.5.2.cmml" xref="S2.p1.1.m1.1.1.4.5.2">ğ‘¥</ci><ci id="S2.p1.1.m1.1.1.4.5.3.cmml" xref="S2.p1.1.m1.1.1.4.5.3">ğ‘›</ci></apply></apply></apply><apply id="S2.p1.1.m1.1.1c.cmml" xref="S2.p1.1.m1.1.1"><in id="S2.p1.1.m1.1.1.5.cmml" xref="S2.p1.1.m1.1.1.5"></in><share href="https://arxiv.org/html/2410.05078v1#S2.p1.1.m1.1.1.4.cmml" id="S2.p1.1.m1.1.1d.cmml" xref="S2.p1.1.m1.1.1"></share><apply id="S2.p1.1.m1.1.1.6.cmml" xref="S2.p1.1.m1.1.1.6"><csymbol cd="ambiguous" id="S2.p1.1.m1.1.1.6.1.cmml" xref="S2.p1.1.m1.1.1.6">superscript</csymbol><ci id="S2.p1.1.m1.1.1.6.2.cmml" xref="S2.p1.1.m1.1.1.6.2">ğ’³</ci><ci id="S2.p1.1.m1.1.1.6.3.cmml" xref="S2.p1.1.m1.1.1.6.3">ğ‘›</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">{x_{1:n}:=x_{1}x_{2}\ldots x_{n}\in\mathcal{X}^{n}}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.1.m1.1d">italic_x start_POSTSUBSCRIPT 1 : italic_n end_POSTSUBSCRIPT := italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT â€¦ italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT âˆˆ caligraphic_X start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT</annotation></semantics></math> of length <math alttext="n" class="ltx_Math" display="inline" id="S2.p1.2.m2.1"><semantics id="S2.p1.2.m2.1a"><mi id="S2.p1.2.m2.1.1" xref="S2.p1.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.1b"><ci id="S2.p1.2.m2.1.1.cmml" xref="S2.p1.2.m2.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S2.p1.2.m2.1d">italic_n</annotation></semantics></math> from a finite alphabet <math alttext="\mathcal{X}" class="ltx_Math" display="inline" id="S2.p1.3.m3.1"><semantics id="S2.p1.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S2.p1.3.m3.1.1" xref="S2.p1.3.m3.1.1.cmml">ğ’³</mi><annotation-xml encoding="MathML-Content" id="S2.p1.3.m3.1b"><ci id="S2.p1.3.m3.1.1.cmml" xref="S2.p1.3.m3.1.1">ğ’³</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.3.m3.1c">\mathcal{X}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.3.m3.1d">caligraphic_X</annotation></semantics></math> sampled from a source <math alttext="{\rho:\mathcal{X}^{*}\mapsto(0,1]}" class="ltx_Math" display="inline" id="S2.p1.4.m4.2"><semantics id="S2.p1.4.m4.2a"><mrow id="S2.p1.4.m4.2.3" xref="S2.p1.4.m4.2.3.cmml"><mi id="S2.p1.4.m4.2.3.2" xref="S2.p1.4.m4.2.3.2.cmml">Ï</mi><mo id="S2.p1.4.m4.2.3.1" lspace="0.278em" rspace="0.278em" xref="S2.p1.4.m4.2.3.1.cmml">:</mo><mrow id="S2.p1.4.m4.2.3.3" xref="S2.p1.4.m4.2.3.3.cmml"><msup id="S2.p1.4.m4.2.3.3.2" xref="S2.p1.4.m4.2.3.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.p1.4.m4.2.3.3.2.2" xref="S2.p1.4.m4.2.3.3.2.2.cmml">ğ’³</mi><mo id="S2.p1.4.m4.2.3.3.2.3" xref="S2.p1.4.m4.2.3.3.2.3.cmml">âˆ—</mo></msup><mo id="S2.p1.4.m4.2.3.3.1" stretchy="false" xref="S2.p1.4.m4.2.3.3.1.cmml">â†¦</mo><mrow id="S2.p1.4.m4.2.3.3.3.2" xref="S2.p1.4.m4.2.3.3.3.1.cmml"><mo id="S2.p1.4.m4.2.3.3.3.2.1" stretchy="false" xref="S2.p1.4.m4.2.3.3.3.1.cmml">(</mo><mn id="S2.p1.4.m4.1.1" xref="S2.p1.4.m4.1.1.cmml">0</mn><mo id="S2.p1.4.m4.2.3.3.3.2.2" xref="S2.p1.4.m4.2.3.3.3.1.cmml">,</mo><mn id="S2.p1.4.m4.2.2" xref="S2.p1.4.m4.2.2.cmml">1</mn><mo id="S2.p1.4.m4.2.3.3.3.2.3" stretchy="false" xref="S2.p1.4.m4.2.3.3.3.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.4.m4.2b"><apply id="S2.p1.4.m4.2.3.cmml" xref="S2.p1.4.m4.2.3"><ci id="S2.p1.4.m4.2.3.1.cmml" xref="S2.p1.4.m4.2.3.1">:</ci><ci id="S2.p1.4.m4.2.3.2.cmml" xref="S2.p1.4.m4.2.3.2">ğœŒ</ci><apply id="S2.p1.4.m4.2.3.3.cmml" xref="S2.p1.4.m4.2.3.3"><csymbol cd="latexml" id="S2.p1.4.m4.2.3.3.1.cmml" xref="S2.p1.4.m4.2.3.3.1">maps-to</csymbol><apply id="S2.p1.4.m4.2.3.3.2.cmml" xref="S2.p1.4.m4.2.3.3.2"><csymbol cd="ambiguous" id="S2.p1.4.m4.2.3.3.2.1.cmml" xref="S2.p1.4.m4.2.3.3.2">superscript</csymbol><ci id="S2.p1.4.m4.2.3.3.2.2.cmml" xref="S2.p1.4.m4.2.3.3.2.2">ğ’³</ci><times id="S2.p1.4.m4.2.3.3.2.3.cmml" xref="S2.p1.4.m4.2.3.3.2.3"></times></apply><interval closure="open-closed" id="S2.p1.4.m4.2.3.3.3.1.cmml" xref="S2.p1.4.m4.2.3.3.3.2"><cn id="S2.p1.4.m4.1.1.cmml" type="integer" xref="S2.p1.4.m4.1.1">0</cn><cn id="S2.p1.4.m4.2.2.cmml" type="integer" xref="S2.p1.4.m4.2.2">1</cn></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.4.m4.2c">{\rho:\mathcal{X}^{*}\mapsto(0,1]}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.4.m4.2d">italic_Ï : caligraphic_X start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT â†¦ ( 0 , 1 ]</annotation></semantics></math>, a lossless compressor <math alttext="{c:\mathcal{X}^{*}\mapsto\{0,1\}^{*}}" class="ltx_Math" display="inline" id="S2.p1.5.m5.2"><semantics id="S2.p1.5.m5.2a"><mrow id="S2.p1.5.m5.2.3" xref="S2.p1.5.m5.2.3.cmml"><mi id="S2.p1.5.m5.2.3.2" xref="S2.p1.5.m5.2.3.2.cmml">c</mi><mo id="S2.p1.5.m5.2.3.1" lspace="0.278em" rspace="0.278em" xref="S2.p1.5.m5.2.3.1.cmml">:</mo><mrow id="S2.p1.5.m5.2.3.3" xref="S2.p1.5.m5.2.3.3.cmml"><msup id="S2.p1.5.m5.2.3.3.2" xref="S2.p1.5.m5.2.3.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.p1.5.m5.2.3.3.2.2" xref="S2.p1.5.m5.2.3.3.2.2.cmml">ğ’³</mi><mo id="S2.p1.5.m5.2.3.3.2.3" xref="S2.p1.5.m5.2.3.3.2.3.cmml">âˆ—</mo></msup><mo id="S2.p1.5.m5.2.3.3.1" stretchy="false" xref="S2.p1.5.m5.2.3.3.1.cmml">â†¦</mo><msup id="S2.p1.5.m5.2.3.3.3" xref="S2.p1.5.m5.2.3.3.3.cmml"><mrow id="S2.p1.5.m5.2.3.3.3.2.2" xref="S2.p1.5.m5.2.3.3.3.2.1.cmml"><mo id="S2.p1.5.m5.2.3.3.3.2.2.1" stretchy="false" xref="S2.p1.5.m5.2.3.3.3.2.1.cmml">{</mo><mn id="S2.p1.5.m5.1.1" xref="S2.p1.5.m5.1.1.cmml">0</mn><mo id="S2.p1.5.m5.2.3.3.3.2.2.2" xref="S2.p1.5.m5.2.3.3.3.2.1.cmml">,</mo><mn id="S2.p1.5.m5.2.2" xref="S2.p1.5.m5.2.2.cmml">1</mn><mo id="S2.p1.5.m5.2.3.3.3.2.2.3" stretchy="false" xref="S2.p1.5.m5.2.3.3.3.2.1.cmml">}</mo></mrow><mo id="S2.p1.5.m5.2.3.3.3.3" xref="S2.p1.5.m5.2.3.3.3.3.cmml">âˆ—</mo></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.5.m5.2b"><apply id="S2.p1.5.m5.2.3.cmml" xref="S2.p1.5.m5.2.3"><ci id="S2.p1.5.m5.2.3.1.cmml" xref="S2.p1.5.m5.2.3.1">:</ci><ci id="S2.p1.5.m5.2.3.2.cmml" xref="S2.p1.5.m5.2.3.2">ğ‘</ci><apply id="S2.p1.5.m5.2.3.3.cmml" xref="S2.p1.5.m5.2.3.3"><csymbol cd="latexml" id="S2.p1.5.m5.2.3.3.1.cmml" xref="S2.p1.5.m5.2.3.3.1">maps-to</csymbol><apply id="S2.p1.5.m5.2.3.3.2.cmml" xref="S2.p1.5.m5.2.3.3.2"><csymbol cd="ambiguous" id="S2.p1.5.m5.2.3.3.2.1.cmml" xref="S2.p1.5.m5.2.3.3.2">superscript</csymbol><ci id="S2.p1.5.m5.2.3.3.2.2.cmml" xref="S2.p1.5.m5.2.3.3.2.2">ğ’³</ci><times id="S2.p1.5.m5.2.3.3.2.3.cmml" xref="S2.p1.5.m5.2.3.3.2.3"></times></apply><apply id="S2.p1.5.m5.2.3.3.3.cmml" xref="S2.p1.5.m5.2.3.3.3"><csymbol cd="ambiguous" id="S2.p1.5.m5.2.3.3.3.1.cmml" xref="S2.p1.5.m5.2.3.3.3">superscript</csymbol><set id="S2.p1.5.m5.2.3.3.3.2.1.cmml" xref="S2.p1.5.m5.2.3.3.3.2.2"><cn id="S2.p1.5.m5.1.1.cmml" type="integer" xref="S2.p1.5.m5.1.1">0</cn><cn id="S2.p1.5.m5.2.2.cmml" type="integer" xref="S2.p1.5.m5.2.2">1</cn></set><times id="S2.p1.5.m5.2.3.3.3.3.cmml" xref="S2.p1.5.m5.2.3.3.3.3"></times></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.5.m5.2c">{c:\mathcal{X}^{*}\mapsto\{0,1\}^{*}}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.5.m5.2d">italic_c : caligraphic_X start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT â†¦ { 0 , 1 } start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT</annotation></semantics></math> assigns a code <math alttext="c(x_{1:n})" class="ltx_Math" display="inline" id="S2.p1.6.m6.1"><semantics id="S2.p1.6.m6.1a"><mrow id="S2.p1.6.m6.1.1" xref="S2.p1.6.m6.1.1.cmml"><mi id="S2.p1.6.m6.1.1.3" xref="S2.p1.6.m6.1.1.3.cmml">c</mi><mo id="S2.p1.6.m6.1.1.2" xref="S2.p1.6.m6.1.1.2.cmml">â¢</mo><mrow id="S2.p1.6.m6.1.1.1.1" xref="S2.p1.6.m6.1.1.1.1.1.cmml"><mo id="S2.p1.6.m6.1.1.1.1.2" stretchy="false" xref="S2.p1.6.m6.1.1.1.1.1.cmml">(</mo><msub id="S2.p1.6.m6.1.1.1.1.1" xref="S2.p1.6.m6.1.1.1.1.1.cmml"><mi id="S2.p1.6.m6.1.1.1.1.1.2" xref="S2.p1.6.m6.1.1.1.1.1.2.cmml">x</mi><mrow id="S2.p1.6.m6.1.1.1.1.1.3" xref="S2.p1.6.m6.1.1.1.1.1.3.cmml"><mn id="S2.p1.6.m6.1.1.1.1.1.3.2" xref="S2.p1.6.m6.1.1.1.1.1.3.2.cmml">1</mn><mo id="S2.p1.6.m6.1.1.1.1.1.3.1" lspace="0.278em" rspace="0.278em" xref="S2.p1.6.m6.1.1.1.1.1.3.1.cmml">:</mo><mi id="S2.p1.6.m6.1.1.1.1.1.3.3" xref="S2.p1.6.m6.1.1.1.1.1.3.3.cmml">n</mi></mrow></msub><mo id="S2.p1.6.m6.1.1.1.1.3" stretchy="false" xref="S2.p1.6.m6.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.6.m6.1b"><apply id="S2.p1.6.m6.1.1.cmml" xref="S2.p1.6.m6.1.1"><times id="S2.p1.6.m6.1.1.2.cmml" xref="S2.p1.6.m6.1.1.2"></times><ci id="S2.p1.6.m6.1.1.3.cmml" xref="S2.p1.6.m6.1.1.3">ğ‘</ci><apply id="S2.p1.6.m6.1.1.1.1.1.cmml" xref="S2.p1.6.m6.1.1.1.1"><csymbol cd="ambiguous" id="S2.p1.6.m6.1.1.1.1.1.1.cmml" xref="S2.p1.6.m6.1.1.1.1">subscript</csymbol><ci id="S2.p1.6.m6.1.1.1.1.1.2.cmml" xref="S2.p1.6.m6.1.1.1.1.1.2">ğ‘¥</ci><apply id="S2.p1.6.m6.1.1.1.1.1.3.cmml" xref="S2.p1.6.m6.1.1.1.1.1.3"><ci id="S2.p1.6.m6.1.1.1.1.1.3.1.cmml" xref="S2.p1.6.m6.1.1.1.1.1.3.1">:</ci><cn id="S2.p1.6.m6.1.1.1.1.1.3.2.cmml" type="integer" xref="S2.p1.6.m6.1.1.1.1.1.3.2">1</cn><ci id="S2.p1.6.m6.1.1.1.1.1.3.3.cmml" xref="S2.p1.6.m6.1.1.1.1.1.3.3">ğ‘›</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.6.m6.1c">c(x_{1:n})</annotation><annotation encoding="application/x-llamapun" id="S2.p1.6.m6.1d">italic_c ( italic_x start_POSTSUBSCRIPT 1 : italic_n end_POSTSUBSCRIPT )</annotation></semantics></math>, i.e., a sequence of bits, from which the original sequence is recoverable without loss of information. The goal is to
minimize the expected length: <math alttext="{L_{\rho}:=\mathrm{\mathbb{missing}}{E}_{x\sim\rho}[\ell_{c}(x)]}" class="ltx_Math" display="inline" id="S2.p1.7.m7.2"><semantics id="S2.p1.7.m7.2a"><mrow id="S2.p1.7.m7.2.2" xref="S2.p1.7.m7.2.2.cmml"><msub id="S2.p1.7.m7.2.2.3" xref="S2.p1.7.m7.2.2.3.cmml"><mi id="S2.p1.7.m7.2.2.3.2" xref="S2.p1.7.m7.2.2.3.2.cmml">L</mi><mi id="S2.p1.7.m7.2.2.3.3" xref="S2.p1.7.m7.2.2.3.3.cmml">Ï</mi></msub><mo id="S2.p1.7.m7.2.2.2" lspace="0.278em" rspace="0.278em" xref="S2.p1.7.m7.2.2.2.cmml">:=</mo><mrow id="S2.p1.7.m7.2.2.1" xref="S2.p1.7.m7.2.2.1.cmml"><mi id="S2.p1.7.m7.2.2.1.3" xref="S2.p1.7.m7.2.2.1.3.cmml">missing</mi><mo id="S2.p1.7.m7.2.2.1.2" xref="S2.p1.7.m7.2.2.1.2.cmml">â¢</mo><msub id="S2.p1.7.m7.2.2.1.4" xref="S2.p1.7.m7.2.2.1.4.cmml"><mi id="S2.p1.7.m7.2.2.1.4.2" xref="S2.p1.7.m7.2.2.1.4.2.cmml">E</mi><mrow id="S2.p1.7.m7.2.2.1.4.3" xref="S2.p1.7.m7.2.2.1.4.3.cmml"><mi id="S2.p1.7.m7.2.2.1.4.3.2" xref="S2.p1.7.m7.2.2.1.4.3.2.cmml">x</mi><mo id="S2.p1.7.m7.2.2.1.4.3.1" xref="S2.p1.7.m7.2.2.1.4.3.1.cmml">âˆ¼</mo><mi id="S2.p1.7.m7.2.2.1.4.3.3" xref="S2.p1.7.m7.2.2.1.4.3.3.cmml">Ï</mi></mrow></msub><mo id="S2.p1.7.m7.2.2.1.2a" xref="S2.p1.7.m7.2.2.1.2.cmml">â¢</mo><mrow id="S2.p1.7.m7.2.2.1.1.1" xref="S2.p1.7.m7.2.2.1.1.2.cmml"><mo id="S2.p1.7.m7.2.2.1.1.1.2" stretchy="false" xref="S2.p1.7.m7.2.2.1.1.2.1.cmml">[</mo><mrow id="S2.p1.7.m7.2.2.1.1.1.1" xref="S2.p1.7.m7.2.2.1.1.1.1.cmml"><msub id="S2.p1.7.m7.2.2.1.1.1.1.2" xref="S2.p1.7.m7.2.2.1.1.1.1.2.cmml"><mi id="S2.p1.7.m7.2.2.1.1.1.1.2.2" mathvariant="normal" xref="S2.p1.7.m7.2.2.1.1.1.1.2.2.cmml">â„“</mi><mi id="S2.p1.7.m7.2.2.1.1.1.1.2.3" xref="S2.p1.7.m7.2.2.1.1.1.1.2.3.cmml">c</mi></msub><mo id="S2.p1.7.m7.2.2.1.1.1.1.1" xref="S2.p1.7.m7.2.2.1.1.1.1.1.cmml">â¢</mo><mrow id="S2.p1.7.m7.2.2.1.1.1.1.3.2" xref="S2.p1.7.m7.2.2.1.1.1.1.cmml"><mo id="S2.p1.7.m7.2.2.1.1.1.1.3.2.1" stretchy="false" xref="S2.p1.7.m7.2.2.1.1.1.1.cmml">(</mo><mi id="S2.p1.7.m7.1.1" xref="S2.p1.7.m7.1.1.cmml">x</mi><mo id="S2.p1.7.m7.2.2.1.1.1.1.3.2.2" stretchy="false" xref="S2.p1.7.m7.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.p1.7.m7.2.2.1.1.1.3" stretchy="false" xref="S2.p1.7.m7.2.2.1.1.2.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.7.m7.2b"><apply id="S2.p1.7.m7.2.2.cmml" xref="S2.p1.7.m7.2.2"><csymbol cd="latexml" id="S2.p1.7.m7.2.2.2.cmml" xref="S2.p1.7.m7.2.2.2">assign</csymbol><apply id="S2.p1.7.m7.2.2.3.cmml" xref="S2.p1.7.m7.2.2.3"><csymbol cd="ambiguous" id="S2.p1.7.m7.2.2.3.1.cmml" xref="S2.p1.7.m7.2.2.3">subscript</csymbol><ci id="S2.p1.7.m7.2.2.3.2.cmml" xref="S2.p1.7.m7.2.2.3.2">ğ¿</ci><ci id="S2.p1.7.m7.2.2.3.3.cmml" xref="S2.p1.7.m7.2.2.3.3">ğœŒ</ci></apply><apply id="S2.p1.7.m7.2.2.1.cmml" xref="S2.p1.7.m7.2.2.1"><times id="S2.p1.7.m7.2.2.1.2.cmml" xref="S2.p1.7.m7.2.2.1.2"></times><ci id="S2.p1.7.m7.2.2.1.3.cmml" xref="S2.p1.7.m7.2.2.1.3">missing</ci><apply id="S2.p1.7.m7.2.2.1.4.cmml" xref="S2.p1.7.m7.2.2.1.4"><csymbol cd="ambiguous" id="S2.p1.7.m7.2.2.1.4.1.cmml" xref="S2.p1.7.m7.2.2.1.4">subscript</csymbol><ci id="S2.p1.7.m7.2.2.1.4.2.cmml" xref="S2.p1.7.m7.2.2.1.4.2">ğ¸</ci><apply id="S2.p1.7.m7.2.2.1.4.3.cmml" xref="S2.p1.7.m7.2.2.1.4.3"><csymbol cd="latexml" id="S2.p1.7.m7.2.2.1.4.3.1.cmml" xref="S2.p1.7.m7.2.2.1.4.3.1">similar-to</csymbol><ci id="S2.p1.7.m7.2.2.1.4.3.2.cmml" xref="S2.p1.7.m7.2.2.1.4.3.2">ğ‘¥</ci><ci id="S2.p1.7.m7.2.2.1.4.3.3.cmml" xref="S2.p1.7.m7.2.2.1.4.3.3">ğœŒ</ci></apply></apply><apply id="S2.p1.7.m7.2.2.1.1.2.cmml" xref="S2.p1.7.m7.2.2.1.1.1"><csymbol cd="latexml" id="S2.p1.7.m7.2.2.1.1.2.1.cmml" xref="S2.p1.7.m7.2.2.1.1.1.2">delimited-[]</csymbol><apply id="S2.p1.7.m7.2.2.1.1.1.1.cmml" xref="S2.p1.7.m7.2.2.1.1.1.1"><times id="S2.p1.7.m7.2.2.1.1.1.1.1.cmml" xref="S2.p1.7.m7.2.2.1.1.1.1.1"></times><apply id="S2.p1.7.m7.2.2.1.1.1.1.2.cmml" xref="S2.p1.7.m7.2.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.p1.7.m7.2.2.1.1.1.1.2.1.cmml" xref="S2.p1.7.m7.2.2.1.1.1.1.2">subscript</csymbol><ci id="S2.p1.7.m7.2.2.1.1.1.1.2.2.cmml" xref="S2.p1.7.m7.2.2.1.1.1.1.2.2">â„“</ci><ci id="S2.p1.7.m7.2.2.1.1.1.1.2.3.cmml" xref="S2.p1.7.m7.2.2.1.1.1.1.2.3">ğ‘</ci></apply><ci id="S2.p1.7.m7.1.1.cmml" xref="S2.p1.7.m7.1.1">ğ‘¥</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.7.m7.2c">{L_{\rho}:=\mathrm{\mathbb{missing}}{E}_{x\sim\rho}[\ell_{c}(x)]}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.7.m7.2d">italic_L start_POSTSUBSCRIPT italic_Ï end_POSTSUBSCRIPT := roman_missing italic_E start_POSTSUBSCRIPT italic_x âˆ¼ italic_Ï end_POSTSUBSCRIPT [ roman_â„“ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_x ) ]</annotation></semantics></math> by encoding rare sequences with more bits and frequent sequences with fewer bits.
Shannonâ€™s source coding theorem states that the minimal expected length is lower-bounded by the Shannon entropy of the source: <math alttext="{L_{\rho}\geq H(\rho):=\mathbb{E}_{x\sim\rho}[-\log_{2}\rho(x)]}" class="ltx_Math" display="inline" id="S2.p1.8.m8.3"><semantics id="S2.p1.8.m8.3a"><mrow id="S2.p1.8.m8.3.3" xref="S2.p1.8.m8.3.3.cmml"><msub id="S2.p1.8.m8.3.3.3" xref="S2.p1.8.m8.3.3.3.cmml"><mi id="S2.p1.8.m8.3.3.3.2" xref="S2.p1.8.m8.3.3.3.2.cmml">L</mi><mi id="S2.p1.8.m8.3.3.3.3" xref="S2.p1.8.m8.3.3.3.3.cmml">Ï</mi></msub><mo id="S2.p1.8.m8.3.3.4" xref="S2.p1.8.m8.3.3.4.cmml">â‰¥</mo><mrow id="S2.p1.8.m8.3.3.5" xref="S2.p1.8.m8.3.3.5.cmml"><mi id="S2.p1.8.m8.3.3.5.2" xref="S2.p1.8.m8.3.3.5.2.cmml">H</mi><mo id="S2.p1.8.m8.3.3.5.1" xref="S2.p1.8.m8.3.3.5.1.cmml">â¢</mo><mrow id="S2.p1.8.m8.3.3.5.3.2" xref="S2.p1.8.m8.3.3.5.cmml"><mo id="S2.p1.8.m8.3.3.5.3.2.1" stretchy="false" xref="S2.p1.8.m8.3.3.5.cmml">(</mo><mi id="S2.p1.8.m8.1.1" xref="S2.p1.8.m8.1.1.cmml">Ï</mi><mo id="S2.p1.8.m8.3.3.5.3.2.2" rspace="0.278em" stretchy="false" xref="S2.p1.8.m8.3.3.5.cmml">)</mo></mrow></mrow><mo id="S2.p1.8.m8.3.3.6" rspace="0.278em" xref="S2.p1.8.m8.3.3.6.cmml">:=</mo><mrow id="S2.p1.8.m8.3.3.1" xref="S2.p1.8.m8.3.3.1.cmml"><msub id="S2.p1.8.m8.3.3.1.3" xref="S2.p1.8.m8.3.3.1.3.cmml"><mi id="S2.p1.8.m8.3.3.1.3.2" xref="S2.p1.8.m8.3.3.1.3.2.cmml">ğ”¼</mi><mrow id="S2.p1.8.m8.3.3.1.3.3" xref="S2.p1.8.m8.3.3.1.3.3.cmml"><mi id="S2.p1.8.m8.3.3.1.3.3.2" xref="S2.p1.8.m8.3.3.1.3.3.2.cmml">x</mi><mo id="S2.p1.8.m8.3.3.1.3.3.1" xref="S2.p1.8.m8.3.3.1.3.3.1.cmml">âˆ¼</mo><mi id="S2.p1.8.m8.3.3.1.3.3.3" xref="S2.p1.8.m8.3.3.1.3.3.3.cmml">Ï</mi></mrow></msub><mo id="S2.p1.8.m8.3.3.1.2" xref="S2.p1.8.m8.3.3.1.2.cmml">â¢</mo><mrow id="S2.p1.8.m8.3.3.1.1.1" xref="S2.p1.8.m8.3.3.1.1.2.cmml"><mo id="S2.p1.8.m8.3.3.1.1.1.2" stretchy="false" xref="S2.p1.8.m8.3.3.1.1.2.1.cmml">[</mo><mrow id="S2.p1.8.m8.3.3.1.1.1.1" xref="S2.p1.8.m8.3.3.1.1.1.1.cmml"><mo id="S2.p1.8.m8.3.3.1.1.1.1a" rspace="0.167em" xref="S2.p1.8.m8.3.3.1.1.1.1.cmml">âˆ’</mo><mrow id="S2.p1.8.m8.3.3.1.1.1.1.2" xref="S2.p1.8.m8.3.3.1.1.1.1.2.cmml"><mrow id="S2.p1.8.m8.3.3.1.1.1.1.2.2" xref="S2.p1.8.m8.3.3.1.1.1.1.2.2.cmml"><msub id="S2.p1.8.m8.3.3.1.1.1.1.2.2.1" xref="S2.p1.8.m8.3.3.1.1.1.1.2.2.1.cmml"><mi id="S2.p1.8.m8.3.3.1.1.1.1.2.2.1.2" xref="S2.p1.8.m8.3.3.1.1.1.1.2.2.1.2.cmml">log</mi><mn id="S2.p1.8.m8.3.3.1.1.1.1.2.2.1.3" xref="S2.p1.8.m8.3.3.1.1.1.1.2.2.1.3.cmml">2</mn></msub><mo id="S2.p1.8.m8.3.3.1.1.1.1.2.2a" lspace="0.167em" xref="S2.p1.8.m8.3.3.1.1.1.1.2.2.cmml">â¡</mo><mi id="S2.p1.8.m8.3.3.1.1.1.1.2.2.2" xref="S2.p1.8.m8.3.3.1.1.1.1.2.2.2.cmml">Ï</mi></mrow><mo id="S2.p1.8.m8.3.3.1.1.1.1.2.1" xref="S2.p1.8.m8.3.3.1.1.1.1.2.1.cmml">â¢</mo><mrow id="S2.p1.8.m8.3.3.1.1.1.1.2.3.2" xref="S2.p1.8.m8.3.3.1.1.1.1.2.cmml"><mo id="S2.p1.8.m8.3.3.1.1.1.1.2.3.2.1" stretchy="false" xref="S2.p1.8.m8.3.3.1.1.1.1.2.cmml">(</mo><mi id="S2.p1.8.m8.2.2" xref="S2.p1.8.m8.2.2.cmml">x</mi><mo id="S2.p1.8.m8.3.3.1.1.1.1.2.3.2.2" stretchy="false" xref="S2.p1.8.m8.3.3.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S2.p1.8.m8.3.3.1.1.1.3" stretchy="false" xref="S2.p1.8.m8.3.3.1.1.2.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.8.m8.3b"><apply id="S2.p1.8.m8.3.3.cmml" xref="S2.p1.8.m8.3.3"><and id="S2.p1.8.m8.3.3a.cmml" xref="S2.p1.8.m8.3.3"></and><apply id="S2.p1.8.m8.3.3b.cmml" xref="S2.p1.8.m8.3.3"><geq id="S2.p1.8.m8.3.3.4.cmml" xref="S2.p1.8.m8.3.3.4"></geq><apply id="S2.p1.8.m8.3.3.3.cmml" xref="S2.p1.8.m8.3.3.3"><csymbol cd="ambiguous" id="S2.p1.8.m8.3.3.3.1.cmml" xref="S2.p1.8.m8.3.3.3">subscript</csymbol><ci id="S2.p1.8.m8.3.3.3.2.cmml" xref="S2.p1.8.m8.3.3.3.2">ğ¿</ci><ci id="S2.p1.8.m8.3.3.3.3.cmml" xref="S2.p1.8.m8.3.3.3.3">ğœŒ</ci></apply><apply id="S2.p1.8.m8.3.3.5.cmml" xref="S2.p1.8.m8.3.3.5"><times id="S2.p1.8.m8.3.3.5.1.cmml" xref="S2.p1.8.m8.3.3.5.1"></times><ci id="S2.p1.8.m8.3.3.5.2.cmml" xref="S2.p1.8.m8.3.3.5.2">ğ»</ci><ci id="S2.p1.8.m8.1.1.cmml" xref="S2.p1.8.m8.1.1">ğœŒ</ci></apply></apply><apply id="S2.p1.8.m8.3.3c.cmml" xref="S2.p1.8.m8.3.3"><csymbol cd="latexml" id="S2.p1.8.m8.3.3.6.cmml" xref="S2.p1.8.m8.3.3.6">assign</csymbol><share href="https://arxiv.org/html/2410.05078v1#S2.p1.8.m8.3.3.5.cmml" id="S2.p1.8.m8.3.3d.cmml" xref="S2.p1.8.m8.3.3"></share><apply id="S2.p1.8.m8.3.3.1.cmml" xref="S2.p1.8.m8.3.3.1"><times id="S2.p1.8.m8.3.3.1.2.cmml" xref="S2.p1.8.m8.3.3.1.2"></times><apply id="S2.p1.8.m8.3.3.1.3.cmml" xref="S2.p1.8.m8.3.3.1.3"><csymbol cd="ambiguous" id="S2.p1.8.m8.3.3.1.3.1.cmml" xref="S2.p1.8.m8.3.3.1.3">subscript</csymbol><ci id="S2.p1.8.m8.3.3.1.3.2.cmml" xref="S2.p1.8.m8.3.3.1.3.2">ğ”¼</ci><apply id="S2.p1.8.m8.3.3.1.3.3.cmml" xref="S2.p1.8.m8.3.3.1.3.3"><csymbol cd="latexml" id="S2.p1.8.m8.3.3.1.3.3.1.cmml" xref="S2.p1.8.m8.3.3.1.3.3.1">similar-to</csymbol><ci id="S2.p1.8.m8.3.3.1.3.3.2.cmml" xref="S2.p1.8.m8.3.3.1.3.3.2">ğ‘¥</ci><ci id="S2.p1.8.m8.3.3.1.3.3.3.cmml" xref="S2.p1.8.m8.3.3.1.3.3.3">ğœŒ</ci></apply></apply><apply id="S2.p1.8.m8.3.3.1.1.2.cmml" xref="S2.p1.8.m8.3.3.1.1.1"><csymbol cd="latexml" id="S2.p1.8.m8.3.3.1.1.2.1.cmml" xref="S2.p1.8.m8.3.3.1.1.1.2">delimited-[]</csymbol><apply id="S2.p1.8.m8.3.3.1.1.1.1.cmml" xref="S2.p1.8.m8.3.3.1.1.1.1"><minus id="S2.p1.8.m8.3.3.1.1.1.1.1.cmml" xref="S2.p1.8.m8.3.3.1.1.1.1"></minus><apply id="S2.p1.8.m8.3.3.1.1.1.1.2.cmml" xref="S2.p1.8.m8.3.3.1.1.1.1.2"><times id="S2.p1.8.m8.3.3.1.1.1.1.2.1.cmml" xref="S2.p1.8.m8.3.3.1.1.1.1.2.1"></times><apply id="S2.p1.8.m8.3.3.1.1.1.1.2.2.cmml" xref="S2.p1.8.m8.3.3.1.1.1.1.2.2"><apply id="S2.p1.8.m8.3.3.1.1.1.1.2.2.1.cmml" xref="S2.p1.8.m8.3.3.1.1.1.1.2.2.1"><csymbol cd="ambiguous" id="S2.p1.8.m8.3.3.1.1.1.1.2.2.1.1.cmml" xref="S2.p1.8.m8.3.3.1.1.1.1.2.2.1">subscript</csymbol><log id="S2.p1.8.m8.3.3.1.1.1.1.2.2.1.2.cmml" xref="S2.p1.8.m8.3.3.1.1.1.1.2.2.1.2"></log><cn id="S2.p1.8.m8.3.3.1.1.1.1.2.2.1.3.cmml" type="integer" xref="S2.p1.8.m8.3.3.1.1.1.1.2.2.1.3">2</cn></apply><ci id="S2.p1.8.m8.3.3.1.1.1.1.2.2.2.cmml" xref="S2.p1.8.m8.3.3.1.1.1.1.2.2.2">ğœŒ</ci></apply><ci id="S2.p1.8.m8.2.2.cmml" xref="S2.p1.8.m8.2.2">ğ‘¥</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.8.m8.3c">{L_{\rho}\geq H(\rho):=\mathbb{E}_{x\sim\rho}[-\log_{2}\rho(x)]}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.8.m8.3d">italic_L start_POSTSUBSCRIPT italic_Ï end_POSTSUBSCRIPT â‰¥ italic_H ( italic_Ï ) := blackboard_E start_POSTSUBSCRIPT italic_x âˆ¼ italic_Ï end_POSTSUBSCRIPT [ - roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_Ï ( italic_x ) ]</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.3">If the sourceâ€™s statistics are unknown, good compression becomes a statistical modeling problem, i.e., good compression relies entirely on being able to predict well sequentially.
For any predictorÂ <math alttext="{\pi:\mathcal{X}^{*}\mapsto(0,1]}" class="ltx_Math" display="inline" id="S2.p2.1.m1.2"><semantics id="S2.p2.1.m1.2a"><mrow id="S2.p2.1.m1.2.3" xref="S2.p2.1.m1.2.3.cmml"><mi id="S2.p2.1.m1.2.3.2" xref="S2.p2.1.m1.2.3.2.cmml">Ï€</mi><mo id="S2.p2.1.m1.2.3.1" lspace="0.278em" rspace="0.278em" xref="S2.p2.1.m1.2.3.1.cmml">:</mo><mrow id="S2.p2.1.m1.2.3.3" xref="S2.p2.1.m1.2.3.3.cmml"><msup id="S2.p2.1.m1.2.3.3.2" xref="S2.p2.1.m1.2.3.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.p2.1.m1.2.3.3.2.2" xref="S2.p2.1.m1.2.3.3.2.2.cmml">ğ’³</mi><mo id="S2.p2.1.m1.2.3.3.2.3" xref="S2.p2.1.m1.2.3.3.2.3.cmml">âˆ—</mo></msup><mo id="S2.p2.1.m1.2.3.3.1" stretchy="false" xref="S2.p2.1.m1.2.3.3.1.cmml">â†¦</mo><mrow id="S2.p2.1.m1.2.3.3.3.2" xref="S2.p2.1.m1.2.3.3.3.1.cmml"><mo id="S2.p2.1.m1.2.3.3.3.2.1" stretchy="false" xref="S2.p2.1.m1.2.3.3.3.1.cmml">(</mo><mn id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml">0</mn><mo id="S2.p2.1.m1.2.3.3.3.2.2" xref="S2.p2.1.m1.2.3.3.3.1.cmml">,</mo><mn id="S2.p2.1.m1.2.2" xref="S2.p2.1.m1.2.2.cmml">1</mn><mo id="S2.p2.1.m1.2.3.3.3.2.3" stretchy="false" xref="S2.p2.1.m1.2.3.3.3.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.2b"><apply id="S2.p2.1.m1.2.3.cmml" xref="S2.p2.1.m1.2.3"><ci id="S2.p2.1.m1.2.3.1.cmml" xref="S2.p2.1.m1.2.3.1">:</ci><ci id="S2.p2.1.m1.2.3.2.cmml" xref="S2.p2.1.m1.2.3.2">ğœ‹</ci><apply id="S2.p2.1.m1.2.3.3.cmml" xref="S2.p2.1.m1.2.3.3"><csymbol cd="latexml" id="S2.p2.1.m1.2.3.3.1.cmml" xref="S2.p2.1.m1.2.3.3.1">maps-to</csymbol><apply id="S2.p2.1.m1.2.3.3.2.cmml" xref="S2.p2.1.m1.2.3.3.2"><csymbol cd="ambiguous" id="S2.p2.1.m1.2.3.3.2.1.cmml" xref="S2.p2.1.m1.2.3.3.2">superscript</csymbol><ci id="S2.p2.1.m1.2.3.3.2.2.cmml" xref="S2.p2.1.m1.2.3.3.2.2">ğ’³</ci><times id="S2.p2.1.m1.2.3.3.2.3.cmml" xref="S2.p2.1.m1.2.3.3.2.3"></times></apply><interval closure="open-closed" id="S2.p2.1.m1.2.3.3.3.1.cmml" xref="S2.p2.1.m1.2.3.3.3.2"><cn id="S2.p2.1.m1.1.1.cmml" type="integer" xref="S2.p2.1.m1.1.1">0</cn><cn id="S2.p2.1.m1.2.2.cmml" type="integer" xref="S2.p2.1.m1.2.2">1</cn></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.2c">{\pi:\mathcal{X}^{*}\mapsto(0,1]}</annotation><annotation encoding="application/x-llamapun" id="S2.p2.1.m1.2d">italic_Ï€ : caligraphic_X start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT â†¦ ( 0 , 1 ]</annotation></semantics></math> the expected coding lengthÂ <math alttext="L_{\pi}^{\rho}" class="ltx_Math" display="inline" id="S2.p2.2.m2.1"><semantics id="S2.p2.2.m2.1a"><msubsup id="S2.p2.2.m2.1.1" xref="S2.p2.2.m2.1.1.cmml"><mi id="S2.p2.2.m2.1.1.2.2" xref="S2.p2.2.m2.1.1.2.2.cmml">L</mi><mi id="S2.p2.2.m2.1.1.2.3" xref="S2.p2.2.m2.1.1.2.3.cmml">Ï€</mi><mi id="S2.p2.2.m2.1.1.3" xref="S2.p2.2.m2.1.1.3.cmml">Ï</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.p2.2.m2.1b"><apply id="S2.p2.2.m2.1.1.cmml" xref="S2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.p2.2.m2.1.1.1.cmml" xref="S2.p2.2.m2.1.1">superscript</csymbol><apply id="S2.p2.2.m2.1.1.2.cmml" xref="S2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.p2.2.m2.1.1.2.1.cmml" xref="S2.p2.2.m2.1.1">subscript</csymbol><ci id="S2.p2.2.m2.1.1.2.2.cmml" xref="S2.p2.2.m2.1.1.2.2">ğ¿</ci><ci id="S2.p2.2.m2.1.1.2.3.cmml" xref="S2.p2.2.m2.1.1.2.3">ğœ‹</ci></apply><ci id="S2.p2.2.m2.1.1.3.cmml" xref="S2.p2.2.m2.1.1.3">ğœŒ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.2.m2.1c">L_{\pi}^{\rho}</annotation><annotation encoding="application/x-llamapun" id="S2.p2.2.m2.1d">italic_L start_POSTSUBSCRIPT italic_Ï€ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_Ï end_POSTSUPERSCRIPT</annotation></semantics></math> for data drawn from <math alttext="\rho" class="ltx_Math" display="inline" id="S2.p2.3.m3.1"><semantics id="S2.p2.3.m3.1a"><mi id="S2.p2.3.m3.1.1" xref="S2.p2.3.m3.1.1.cmml">Ï</mi><annotation-xml encoding="MathML-Content" id="S2.p2.3.m3.1b"><ci id="S2.p2.3.m3.1.1.cmml" xref="S2.p2.3.m3.1.1">ğœŒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.3.m3.1c">\rho</annotation><annotation encoding="application/x-llamapun" id="S2.p2.3.m3.1d">italic_Ï</annotation></semantics></math> is at least the cross entropy:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="L_{\pi}^{\rho}\geq\mathbb{E}_{x\sim\rho}[-\log_{2}\pi(x)]=\mathbb{E}_{x\sim%
\rho}\left[-\log_{2}\frac{\pi(x)\rho(x)}{\rho(x)}\right]=H(\rho)+D_{\text{KL}}%
(\rho||\pi)\geq H(\rho)," class="ltx_math_unparsed" display="block" id="S2.Ex1.m1.5"><semantics id="S2.Ex1.m1.5a"><mrow id="S2.Ex1.m1.5b"><msubsup id="S2.Ex1.m1.5.6"><mi id="S2.Ex1.m1.5.6.2.2">L</mi><mi id="S2.Ex1.m1.5.6.2.3">Ï€</mi><mi id="S2.Ex1.m1.5.6.3">Ï</mi></msubsup><mo id="S2.Ex1.m1.5.7">â‰¥</mo><msub id="S2.Ex1.m1.5.8"><mi id="S2.Ex1.m1.5.8.2">ğ”¼</mi><mrow id="S2.Ex1.m1.5.8.3"><mi id="S2.Ex1.m1.5.8.3.2">x</mi><mo id="S2.Ex1.m1.5.8.3.1">âˆ¼</mo><mi id="S2.Ex1.m1.5.8.3.3">Ï</mi></mrow></msub><mrow id="S2.Ex1.m1.5.9"><mo id="S2.Ex1.m1.5.9.1" stretchy="false">[</mo><mo id="S2.Ex1.m1.5.9.2" lspace="0em">âˆ’</mo><msub id="S2.Ex1.m1.5.9.3"><mi id="S2.Ex1.m1.5.9.3.2">log</mi><mn id="S2.Ex1.m1.5.9.3.3">2</mn></msub><mi id="S2.Ex1.m1.5.9.4">Ï€</mi><mrow id="S2.Ex1.m1.5.9.5"><mo id="S2.Ex1.m1.5.9.5.1" stretchy="false">(</mo><mi id="S2.Ex1.m1.4.4">x</mi><mo id="S2.Ex1.m1.5.9.5.2" stretchy="false">)</mo></mrow><mo id="S2.Ex1.m1.5.9.6" stretchy="false">]</mo></mrow><mo id="S2.Ex1.m1.5.10">=</mo><msub id="S2.Ex1.m1.5.11"><mi id="S2.Ex1.m1.5.11.2">ğ”¼</mi><mrow id="S2.Ex1.m1.5.11.3"><mi id="S2.Ex1.m1.5.11.3.2">x</mi><mo id="S2.Ex1.m1.5.11.3.1">âˆ¼</mo><mi id="S2.Ex1.m1.5.11.3.3">Ï</mi></mrow></msub><mrow id="S2.Ex1.m1.5.12"><mo id="S2.Ex1.m1.5.12.1">[</mo><mo id="S2.Ex1.m1.5.12.2" lspace="0em">âˆ’</mo><msub id="S2.Ex1.m1.5.12.3"><mi id="S2.Ex1.m1.5.12.3.2">log</mi><mn id="S2.Ex1.m1.5.12.3.3">2</mn></msub><mfrac id="S2.Ex1.m1.3.3"><mrow id="S2.Ex1.m1.2.2.2"><mi id="S2.Ex1.m1.2.2.2.4">Ï€</mi><mo id="S2.Ex1.m1.2.2.2.3">â¢</mo><mrow id="S2.Ex1.m1.2.2.2.5.2"><mo id="S2.Ex1.m1.2.2.2.5.2.1" stretchy="false">(</mo><mi id="S2.Ex1.m1.1.1.1.1">x</mi><mo id="S2.Ex1.m1.2.2.2.5.2.2" stretchy="false">)</mo></mrow><mo id="S2.Ex1.m1.2.2.2.3a">â¢</mo><mi id="S2.Ex1.m1.2.2.2.6">Ï</mi><mo id="S2.Ex1.m1.2.2.2.3b">â¢</mo><mrow id="S2.Ex1.m1.2.2.2.7.2"><mo id="S2.Ex1.m1.2.2.2.7.2.1" stretchy="false">(</mo><mi id="S2.Ex1.m1.2.2.2.2">x</mi><mo id="S2.Ex1.m1.2.2.2.7.2.2" stretchy="false">)</mo></mrow></mrow><mrow id="S2.Ex1.m1.3.3.3"><mi id="S2.Ex1.m1.3.3.3.3">Ï</mi><mo id="S2.Ex1.m1.3.3.3.2">â¢</mo><mrow id="S2.Ex1.m1.3.3.3.4.2"><mo id="S2.Ex1.m1.3.3.3.4.2.1" stretchy="false">(</mo><mi id="S2.Ex1.m1.3.3.3.1">x</mi><mo id="S2.Ex1.m1.3.3.3.4.2.2" stretchy="false">)</mo></mrow></mrow></mfrac><mo id="S2.Ex1.m1.5.12.4">]</mo></mrow><mo id="S2.Ex1.m1.5.13">=</mo><mi id="S2.Ex1.m1.5.14">H</mi><mrow id="S2.Ex1.m1.5.15"><mo id="S2.Ex1.m1.5.15.1" stretchy="false">(</mo><mi id="S2.Ex1.m1.5.5">Ï</mi><mo id="S2.Ex1.m1.5.15.2" stretchy="false">)</mo></mrow><mo id="S2.Ex1.m1.5.16">+</mo><msub id="S2.Ex1.m1.5.17"><mi id="S2.Ex1.m1.5.17.2">D</mi><mtext id="S2.Ex1.m1.5.17.3">KL</mtext></msub><mrow id="S2.Ex1.m1.5.18"><mo id="S2.Ex1.m1.5.18.1" stretchy="false">(</mo><mi id="S2.Ex1.m1.5.18.2">Ï</mi><mo fence="false" id="S2.Ex1.m1.5.18.3" rspace="0.167em" stretchy="false">|</mo><mo fence="false" id="S2.Ex1.m1.5.18.4" rspace="0.167em" stretchy="false">|</mo><mi id="S2.Ex1.m1.5.18.5">Ï€</mi><mo id="S2.Ex1.m1.5.18.6" stretchy="false">)</mo></mrow><mo id="S2.Ex1.m1.5.19">â‰¥</mo><mi id="S2.Ex1.m1.5.20">H</mi><mrow id="S2.Ex1.m1.5.21"><mo id="S2.Ex1.m1.5.21.1" stretchy="false">(</mo><mi id="S2.Ex1.m1.5.21.2">Ï</mi><mo id="S2.Ex1.m1.5.21.3" stretchy="false">)</mo></mrow><mo id="S2.Ex1.m1.5.22">,</mo></mrow><annotation encoding="application/x-tex" id="S2.Ex1.m1.5c">L_{\pi}^{\rho}\geq\mathbb{E}_{x\sim\rho}[-\log_{2}\pi(x)]=\mathbb{E}_{x\sim%
\rho}\left[-\log_{2}\frac{\pi(x)\rho(x)}{\rho(x)}\right]=H(\rho)+D_{\text{KL}}%
(\rho||\pi)\geq H(\rho),</annotation><annotation encoding="application/x-llamapun" id="S2.Ex1.m1.5d">italic_L start_POSTSUBSCRIPT italic_Ï€ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_Ï end_POSTSUPERSCRIPT â‰¥ blackboard_E start_POSTSUBSCRIPT italic_x âˆ¼ italic_Ï end_POSTSUBSCRIPT [ - roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_Ï€ ( italic_x ) ] = blackboard_E start_POSTSUBSCRIPT italic_x âˆ¼ italic_Ï end_POSTSUBSCRIPT [ - roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT divide start_ARG italic_Ï€ ( italic_x ) italic_Ï ( italic_x ) end_ARG start_ARG italic_Ï ( italic_x ) end_ARG ] = italic_H ( italic_Ï ) + italic_D start_POSTSUBSCRIPT KL end_POSTSUBSCRIPT ( italic_Ï | | italic_Ï€ ) â‰¥ italic_H ( italic_Ï ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.p2.9">which is also lower-bounded by the Shannon entropy of <math alttext="\rho" class="ltx_Math" display="inline" id="S2.p2.4.m1.1"><semantics id="S2.p2.4.m1.1a"><mi id="S2.p2.4.m1.1.1" xref="S2.p2.4.m1.1.1.cmml">Ï</mi><annotation-xml encoding="MathML-Content" id="S2.p2.4.m1.1b"><ci id="S2.p2.4.m1.1.1.cmml" xref="S2.p2.4.m1.1.1">ğœŒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.4.m1.1c">\rho</annotation><annotation encoding="application/x-llamapun" id="S2.p2.4.m1.1d">italic_Ï</annotation></semantics></math>.
A mismatch between <math alttext="\pi" class="ltx_Math" display="inline" id="S2.p2.5.m2.1"><semantics id="S2.p2.5.m2.1a"><mi id="S2.p2.5.m2.1.1" xref="S2.p2.5.m2.1.1.cmml">Ï€</mi><annotation-xml encoding="MathML-Content" id="S2.p2.5.m2.1b"><ci id="S2.p2.5.m2.1.1.cmml" xref="S2.p2.5.m2.1.1">ğœ‹</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.5.m2.1c">\pi</annotation><annotation encoding="application/x-llamapun" id="S2.p2.5.m2.1d">italic_Ï€</annotation></semantics></math> and <math alttext="\rho" class="ltx_Math" display="inline" id="S2.p2.6.m3.1"><semantics id="S2.p2.6.m3.1a"><mi id="S2.p2.6.m3.1.1" xref="S2.p2.6.m3.1.1.cmml">Ï</mi><annotation-xml encoding="MathML-Content" id="S2.p2.6.m3.1b"><ci id="S2.p2.6.m3.1.1.cmml" xref="S2.p2.6.m3.1.1">ğœŒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.6.m3.1c">\rho</annotation><annotation encoding="application/x-llamapun" id="S2.p2.6.m3.1d">italic_Ï</annotation></semantics></math> thus leads to an excess length given by their KL divergence, and minimal coding length (maximal compression) implies <math alttext="\pi=\rho" class="ltx_Math" display="inline" id="S2.p2.7.m4.1"><semantics id="S2.p2.7.m4.1a"><mrow id="S2.p2.7.m4.1.1" xref="S2.p2.7.m4.1.1.cmml"><mi id="S2.p2.7.m4.1.1.2" xref="S2.p2.7.m4.1.1.2.cmml">Ï€</mi><mo id="S2.p2.7.m4.1.1.1" xref="S2.p2.7.m4.1.1.1.cmml">=</mo><mi id="S2.p2.7.m4.1.1.3" xref="S2.p2.7.m4.1.1.3.cmml">Ï</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.7.m4.1b"><apply id="S2.p2.7.m4.1.1.cmml" xref="S2.p2.7.m4.1.1"><eq id="S2.p2.7.m4.1.1.1.cmml" xref="S2.p2.7.m4.1.1.1"></eq><ci id="S2.p2.7.m4.1.1.2.cmml" xref="S2.p2.7.m4.1.1.2">ğœ‹</ci><ci id="S2.p2.7.m4.1.1.3.cmml" xref="S2.p2.7.m4.1.1.3">ğœŒ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.7.m4.1c">\pi=\rho</annotation><annotation encoding="application/x-llamapun" id="S2.p2.7.m4.1d">italic_Ï€ = italic_Ï</annotation></semantics></math> across the whole support of <math alttext="\rho" class="ltx_Math" display="inline" id="S2.p2.8.m5.1"><semantics id="S2.p2.8.m5.1a"><mi id="S2.p2.8.m5.1.1" xref="S2.p2.8.m5.1.1.cmml">Ï</mi><annotation-xml encoding="MathML-Content" id="S2.p2.8.m5.1b"><ci id="S2.p2.8.m5.1.1.cmml" xref="S2.p2.8.m5.1.1">ğœŒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.8.m5.1c">\rho</annotation><annotation encoding="application/x-llamapun" id="S2.p2.8.m5.1d">italic_Ï</annotation></semantics></math>.
Accordingly, some AI researchers have argued that compressing well is fundamentally connected to intelligence (e.g., Chaitinâ€™s famous â€œCompression is Comprehensionâ€Â <cite class="ltx_cite ltx_citemacro_citep">(Chaitin, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib6" title="">2006</a>)</cite>; <cite class="ltx_cite ltx_citemacro_citet">Rathmanner and Hutter (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib56" title="">2011</a>); Grau-Moya etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib21" title="">2024</a>)</cite>), and that building universal compressors will accelerate AI development (cf.Â the Hutter prizeÂ <cite class="ltx_cite ltx_citemacro_citep">(Hutter, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib26" title="">2006</a>)</cite>, an ongoing competition to compress (<math alttext="1" class="ltx_Math" display="inline" id="S2.p2.9.m6.1"><semantics id="S2.p2.9.m6.1a"><mn id="S2.p2.9.m6.1.1" xref="S2.p2.9.m6.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S2.p2.9.m6.1b"><cn id="S2.p2.9.m6.1.1.cmml" type="integer" xref="S2.p2.9.m6.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.9.m6.1c">1</annotation><annotation encoding="application/x-llamapun" id="S2.p2.9.m6.1d">1</annotation></semantics></math>GB of) human knowledge).
The duality between compression and prediction has also led to the (algorithmic) information-theoretic formulation of universal prediction, i.e., Solomonoff inductionÂ <cite class="ltx_cite ltx_citemacro_citep">(Solomonoff, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib68" title="">1964a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib69" title="">b</a>; Li and VitÃ¡nyi, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib37" title="">2019</a>)</cite>, one of two key ingredients for AIXIÂ <cite class="ltx_cite ltx_citemacro_citep">(Legg and Hutter, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib34" title="">2007</a>; Hutter etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib27" title="">2024</a>)</cite>, the theory of artificial superintelligence.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Consequently, <cite class="ltx_cite ltx_citemacro_citet">DelÃ©tang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib11" title="">2024</a>)</cite> argue that lossless compression performance lends itself as a domain-general metric for assessing any predictorâ€™s quality, including foundation models.
They further emphasize that foundation models trained by minimizing log-loss (a.k.a., next-token prediction-error or cross entropy loss) are explicitly trained to minimize the expected coding length:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min_{\pi}L_{\pi}^{\rho}=\min_{\pi}\underbrace{\mathbb{E}_{x\sim\rho}[-\log_{2%
}\pi(x)]}_{\text{``log loss''}}=\min_{\pi}\mathbb{E}_{x\sim\rho}\left[\sum_{i}%
-\log_{2}\pi(x_{i}|x_{&lt;i})\right]." class="ltx_Math" display="block" id="S2.E1.m1.3"><semantics id="S2.E1.m1.3a"><mrow id="S2.E1.m1.3.3.1" xref="S2.E1.m1.3.3.1.1.cmml"><mrow id="S2.E1.m1.3.3.1.1" xref="S2.E1.m1.3.3.1.1.cmml"><mrow id="S2.E1.m1.3.3.1.1.3" xref="S2.E1.m1.3.3.1.1.3.cmml"><munder id="S2.E1.m1.3.3.1.1.3.1" xref="S2.E1.m1.3.3.1.1.3.1.cmml"><mi id="S2.E1.m1.3.3.1.1.3.1.2" xref="S2.E1.m1.3.3.1.1.3.1.2.cmml">min</mi><mi id="S2.E1.m1.3.3.1.1.3.1.3" xref="S2.E1.m1.3.3.1.1.3.1.3.cmml">Ï€</mi></munder><mo id="S2.E1.m1.3.3.1.1.3a" lspace="0.167em" xref="S2.E1.m1.3.3.1.1.3.cmml">â¡</mo><msubsup id="S2.E1.m1.3.3.1.1.3.2" xref="S2.E1.m1.3.3.1.1.3.2.cmml"><mi id="S2.E1.m1.3.3.1.1.3.2.2.2" xref="S2.E1.m1.3.3.1.1.3.2.2.2.cmml">L</mi><mi id="S2.E1.m1.3.3.1.1.3.2.2.3" xref="S2.E1.m1.3.3.1.1.3.2.2.3.cmml">Ï€</mi><mi id="S2.E1.m1.3.3.1.1.3.2.3" xref="S2.E1.m1.3.3.1.1.3.2.3.cmml">Ï</mi></msubsup></mrow><mo id="S2.E1.m1.3.3.1.1.4" xref="S2.E1.m1.3.3.1.1.4.cmml">=</mo><mrow id="S2.E1.m1.3.3.1.1.5" xref="S2.E1.m1.3.3.1.1.5.cmml"><munder id="S2.E1.m1.3.3.1.1.5.1" xref="S2.E1.m1.3.3.1.1.5.1.cmml"><mi id="S2.E1.m1.3.3.1.1.5.1.2" xref="S2.E1.m1.3.3.1.1.5.1.2.cmml">min</mi><mi id="S2.E1.m1.3.3.1.1.5.1.3" xref="S2.E1.m1.3.3.1.1.5.1.3.cmml">Ï€</mi></munder><mo id="S2.E1.m1.3.3.1.1.5a" lspace="0.167em" xref="S2.E1.m1.3.3.1.1.5.cmml">â¡</mo><munder id="S2.E1.m1.3.3.1.1.5.2" xref="S2.E1.m1.3.3.1.1.5.2.cmml"><munder accentunder="true" id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml"><mrow id="S2.E1.m1.2.2.2" xref="S2.E1.m1.2.2.2.cmml"><msub id="S2.E1.m1.2.2.2.4" xref="S2.E1.m1.2.2.2.4.cmml"><mi id="S2.E1.m1.2.2.2.4.2" xref="S2.E1.m1.2.2.2.4.2.cmml">ğ”¼</mi><mrow id="S2.E1.m1.2.2.2.4.3" xref="S2.E1.m1.2.2.2.4.3.cmml"><mi id="S2.E1.m1.2.2.2.4.3.2" xref="S2.E1.m1.2.2.2.4.3.2.cmml">x</mi><mo id="S2.E1.m1.2.2.2.4.3.1" xref="S2.E1.m1.2.2.2.4.3.1.cmml">âˆ¼</mo><mi id="S2.E1.m1.2.2.2.4.3.3" xref="S2.E1.m1.2.2.2.4.3.3.cmml">Ï</mi></mrow></msub><mo id="S2.E1.m1.2.2.2.3" xref="S2.E1.m1.2.2.2.3.cmml">â¢</mo><mrow id="S2.E1.m1.2.2.2.2.1" xref="S2.E1.m1.2.2.2.2.2.cmml"><mo id="S2.E1.m1.2.2.2.2.1.2" stretchy="false" xref="S2.E1.m1.2.2.2.2.2.1.cmml">[</mo><mrow id="S2.E1.m1.2.2.2.2.1.1" xref="S2.E1.m1.2.2.2.2.1.1.cmml"><mo id="S2.E1.m1.2.2.2.2.1.1a" rspace="0.167em" xref="S2.E1.m1.2.2.2.2.1.1.cmml">âˆ’</mo><mrow id="S2.E1.m1.2.2.2.2.1.1.2" xref="S2.E1.m1.2.2.2.2.1.1.2.cmml"><mrow id="S2.E1.m1.2.2.2.2.1.1.2.2" xref="S2.E1.m1.2.2.2.2.1.1.2.2.cmml"><msub id="S2.E1.m1.2.2.2.2.1.1.2.2.1" xref="S2.E1.m1.2.2.2.2.1.1.2.2.1.cmml"><mi id="S2.E1.m1.2.2.2.2.1.1.2.2.1.2" xref="S2.E1.m1.2.2.2.2.1.1.2.2.1.2.cmml">log</mi><mn id="S2.E1.m1.2.2.2.2.1.1.2.2.1.3" xref="S2.E1.m1.2.2.2.2.1.1.2.2.1.3.cmml">2</mn></msub><mo id="S2.E1.m1.2.2.2.2.1.1.2.2a" lspace="0.167em" xref="S2.E1.m1.2.2.2.2.1.1.2.2.cmml">â¡</mo><mi id="S2.E1.m1.2.2.2.2.1.1.2.2.2" xref="S2.E1.m1.2.2.2.2.1.1.2.2.2.cmml">Ï€</mi></mrow><mo id="S2.E1.m1.2.2.2.2.1.1.2.1" xref="S2.E1.m1.2.2.2.2.1.1.2.1.cmml">â¢</mo><mrow id="S2.E1.m1.2.2.2.2.1.1.2.3.2" xref="S2.E1.m1.2.2.2.2.1.1.2.cmml"><mo id="S2.E1.m1.2.2.2.2.1.1.2.3.2.1" stretchy="false" xref="S2.E1.m1.2.2.2.2.1.1.2.cmml">(</mo><mi id="S2.E1.m1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.cmml">x</mi><mo id="S2.E1.m1.2.2.2.2.1.1.2.3.2.2" stretchy="false" xref="S2.E1.m1.2.2.2.2.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E1.m1.2.2.2.2.1.3" stretchy="false" xref="S2.E1.m1.2.2.2.2.2.1.cmml">]</mo></mrow></mrow><mo id="S2.E1.m1.2.2.3" xref="S2.E1.m1.2.2.3.cmml">âŸ</mo></munder><mtext id="S2.E1.m1.3.3.1.1.5.2.2" xref="S2.E1.m1.3.3.1.1.5.2.2a.cmml">â€œlog lossâ€</mtext></munder></mrow><mo id="S2.E1.m1.3.3.1.1.6" xref="S2.E1.m1.3.3.1.1.6.cmml">=</mo><mrow id="S2.E1.m1.3.3.1.1.1" xref="S2.E1.m1.3.3.1.1.1.cmml"><mrow id="S2.E1.m1.3.3.1.1.1.3" xref="S2.E1.m1.3.3.1.1.1.3.cmml"><munder id="S2.E1.m1.3.3.1.1.1.3.1" xref="S2.E1.m1.3.3.1.1.1.3.1.cmml"><mi id="S2.E1.m1.3.3.1.1.1.3.1.2" xref="S2.E1.m1.3.3.1.1.1.3.1.2.cmml">min</mi><mi id="S2.E1.m1.3.3.1.1.1.3.1.3" xref="S2.E1.m1.3.3.1.1.1.3.1.3.cmml">Ï€</mi></munder><mo id="S2.E1.m1.3.3.1.1.1.3a" lspace="0.167em" xref="S2.E1.m1.3.3.1.1.1.3.cmml">â¡</mo><msub id="S2.E1.m1.3.3.1.1.1.3.2" xref="S2.E1.m1.3.3.1.1.1.3.2.cmml"><mi id="S2.E1.m1.3.3.1.1.1.3.2.2" xref="S2.E1.m1.3.3.1.1.1.3.2.2.cmml">ğ”¼</mi><mrow id="S2.E1.m1.3.3.1.1.1.3.2.3" xref="S2.E1.m1.3.3.1.1.1.3.2.3.cmml"><mi id="S2.E1.m1.3.3.1.1.1.3.2.3.2" xref="S2.E1.m1.3.3.1.1.1.3.2.3.2.cmml">x</mi><mo id="S2.E1.m1.3.3.1.1.1.3.2.3.1" xref="S2.E1.m1.3.3.1.1.1.3.2.3.1.cmml">âˆ¼</mo><mi id="S2.E1.m1.3.3.1.1.1.3.2.3.3" xref="S2.E1.m1.3.3.1.1.1.3.2.3.3.cmml">Ï</mi></mrow></msub></mrow><mo id="S2.E1.m1.3.3.1.1.1.2" xref="S2.E1.m1.3.3.1.1.1.2.cmml">â¢</mo><mrow id="S2.E1.m1.3.3.1.1.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.2.cmml"><mo id="S2.E1.m1.3.3.1.1.1.1.1.2" xref="S2.E1.m1.3.3.1.1.1.1.2.1.cmml">[</mo><mrow id="S2.E1.m1.3.3.1.1.1.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.1.1.cmml"><munder id="S2.E1.m1.3.3.1.1.1.1.1.1.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.cmml"><mo id="S2.E1.m1.3.3.1.1.1.1.1.1.3.2" lspace="0em" movablelimits="false" rspace="0em" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.2.cmml">âˆ‘</mo><mi id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.cmml">i</mi></munder><mo id="S2.E1.m1.3.3.1.1.1.1.1.1.2" lspace="0em" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2.cmml">âˆ’</mo><mrow id="S2.E1.m1.3.3.1.1.1.1.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.cmml"><mrow id="S2.E1.m1.3.3.1.1.1.1.1.1.1.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.cmml"><msub id="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.1" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.1.cmml"><mi id="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.1.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.1.2.cmml">log</mi><mn id="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.1.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.1.3.cmml">2</mn></msub><mo id="S2.E1.m1.3.3.1.1.1.1.1.1.1.3a" lspace="0.167em" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.cmml">â¡</mo><mi id="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.2.cmml">Ï€</mi></mrow><mo id="S2.E1.m1.3.3.1.1.1.1.1.1.1.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.2.cmml">â¢</mo><mrow id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.2.cmml">x</mi><mi id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo fence="false" id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml">|</mo><msub id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2.cmml">x</mi><mrow id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.cmml"><mi id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.2.cmml"></mi><mo id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.1" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.1.cmml">&lt;</mo><mi id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.3.cmml">i</mi></mrow></msub></mrow><mo id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E1.m1.3.3.1.1.1.1.1.3" xref="S2.E1.m1.3.3.1.1.1.1.2.1.cmml">]</mo></mrow></mrow></mrow><mo id="S2.E1.m1.3.3.1.2" lspace="0em" xref="S2.E1.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.3b"><apply id="S2.E1.m1.3.3.1.1.cmml" xref="S2.E1.m1.3.3.1"><and id="S2.E1.m1.3.3.1.1a.cmml" xref="S2.E1.m1.3.3.1"></and><apply id="S2.E1.m1.3.3.1.1b.cmml" xref="S2.E1.m1.3.3.1"><eq id="S2.E1.m1.3.3.1.1.4.cmml" xref="S2.E1.m1.3.3.1.1.4"></eq><apply id="S2.E1.m1.3.3.1.1.3.cmml" xref="S2.E1.m1.3.3.1.1.3"><apply id="S2.E1.m1.3.3.1.1.3.1.cmml" xref="S2.E1.m1.3.3.1.1.3.1"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.3.1.1.cmml" xref="S2.E1.m1.3.3.1.1.3.1">subscript</csymbol><min id="S2.E1.m1.3.3.1.1.3.1.2.cmml" xref="S2.E1.m1.3.3.1.1.3.1.2"></min><ci id="S2.E1.m1.3.3.1.1.3.1.3.cmml" xref="S2.E1.m1.3.3.1.1.3.1.3">ğœ‹</ci></apply><apply id="S2.E1.m1.3.3.1.1.3.2.cmml" xref="S2.E1.m1.3.3.1.1.3.2"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.3.2.1.cmml" xref="S2.E1.m1.3.3.1.1.3.2">superscript</csymbol><apply id="S2.E1.m1.3.3.1.1.3.2.2.cmml" xref="S2.E1.m1.3.3.1.1.3.2"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.3.2.2.1.cmml" xref="S2.E1.m1.3.3.1.1.3.2">subscript</csymbol><ci id="S2.E1.m1.3.3.1.1.3.2.2.2.cmml" xref="S2.E1.m1.3.3.1.1.3.2.2.2">ğ¿</ci><ci id="S2.E1.m1.3.3.1.1.3.2.2.3.cmml" xref="S2.E1.m1.3.3.1.1.3.2.2.3">ğœ‹</ci></apply><ci id="S2.E1.m1.3.3.1.1.3.2.3.cmml" xref="S2.E1.m1.3.3.1.1.3.2.3">ğœŒ</ci></apply></apply><apply id="S2.E1.m1.3.3.1.1.5.cmml" xref="S2.E1.m1.3.3.1.1.5"><apply id="S2.E1.m1.3.3.1.1.5.1.cmml" xref="S2.E1.m1.3.3.1.1.5.1"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.5.1.1.cmml" xref="S2.E1.m1.3.3.1.1.5.1">subscript</csymbol><min id="S2.E1.m1.3.3.1.1.5.1.2.cmml" xref="S2.E1.m1.3.3.1.1.5.1.2"></min><ci id="S2.E1.m1.3.3.1.1.5.1.3.cmml" xref="S2.E1.m1.3.3.1.1.5.1.3">ğœ‹</ci></apply><apply id="S2.E1.m1.3.3.1.1.5.2.cmml" xref="S2.E1.m1.3.3.1.1.5.2"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.5.2.1.cmml" xref="S2.E1.m1.3.3.1.1.5.2">subscript</csymbol><apply id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2"><ci id="S2.E1.m1.2.2.3.cmml" xref="S2.E1.m1.2.2.3">âŸ</ci><apply id="S2.E1.m1.2.2.2.cmml" xref="S2.E1.m1.2.2.2"><times id="S2.E1.m1.2.2.2.3.cmml" xref="S2.E1.m1.2.2.2.3"></times><apply id="S2.E1.m1.2.2.2.4.cmml" xref="S2.E1.m1.2.2.2.4"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.2.4.1.cmml" xref="S2.E1.m1.2.2.2.4">subscript</csymbol><ci id="S2.E1.m1.2.2.2.4.2.cmml" xref="S2.E1.m1.2.2.2.4.2">ğ”¼</ci><apply id="S2.E1.m1.2.2.2.4.3.cmml" xref="S2.E1.m1.2.2.2.4.3"><csymbol cd="latexml" id="S2.E1.m1.2.2.2.4.3.1.cmml" xref="S2.E1.m1.2.2.2.4.3.1">similar-to</csymbol><ci id="S2.E1.m1.2.2.2.4.3.2.cmml" xref="S2.E1.m1.2.2.2.4.3.2">ğ‘¥</ci><ci id="S2.E1.m1.2.2.2.4.3.3.cmml" xref="S2.E1.m1.2.2.2.4.3.3">ğœŒ</ci></apply></apply><apply id="S2.E1.m1.2.2.2.2.2.cmml" xref="S2.E1.m1.2.2.2.2.1"><csymbol cd="latexml" id="S2.E1.m1.2.2.2.2.2.1.cmml" xref="S2.E1.m1.2.2.2.2.1.2">delimited-[]</csymbol><apply id="S2.E1.m1.2.2.2.2.1.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1"><minus id="S2.E1.m1.2.2.2.2.1.1.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1"></minus><apply id="S2.E1.m1.2.2.2.2.1.1.2.cmml" xref="S2.E1.m1.2.2.2.2.1.1.2"><times id="S2.E1.m1.2.2.2.2.1.1.2.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1.2.1"></times><apply id="S2.E1.m1.2.2.2.2.1.1.2.2.cmml" xref="S2.E1.m1.2.2.2.2.1.1.2.2"><apply id="S2.E1.m1.2.2.2.2.1.1.2.2.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1.2.2.1"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.2.2.1.1.2.2.1.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1.2.2.1">subscript</csymbol><log id="S2.E1.m1.2.2.2.2.1.1.2.2.1.2.cmml" xref="S2.E1.m1.2.2.2.2.1.1.2.2.1.2"></log><cn id="S2.E1.m1.2.2.2.2.1.1.2.2.1.3.cmml" type="integer" xref="S2.E1.m1.2.2.2.2.1.1.2.2.1.3">2</cn></apply><ci id="S2.E1.m1.2.2.2.2.1.1.2.2.2.cmml" xref="S2.E1.m1.2.2.2.2.1.1.2.2.2">ğœ‹</ci></apply><ci id="S2.E1.m1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1">ğ‘¥</ci></apply></apply></apply></apply></apply><ci id="S2.E1.m1.3.3.1.1.5.2.2a.cmml" xref="S2.E1.m1.3.3.1.1.5.2.2"><mtext id="S2.E1.m1.3.3.1.1.5.2.2.cmml" mathsize="70%" xref="S2.E1.m1.3.3.1.1.5.2.2">â€œlog lossâ€</mtext></ci></apply></apply></apply><apply id="S2.E1.m1.3.3.1.1c.cmml" xref="S2.E1.m1.3.3.1"><eq id="S2.E1.m1.3.3.1.1.6.cmml" xref="S2.E1.m1.3.3.1.1.6"></eq><share href="https://arxiv.org/html/2410.05078v1#S2.E1.m1.3.3.1.1.5.cmml" id="S2.E1.m1.3.3.1.1d.cmml" xref="S2.E1.m1.3.3.1"></share><apply id="S2.E1.m1.3.3.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1"><times id="S2.E1.m1.3.3.1.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.1.2"></times><apply id="S2.E1.m1.3.3.1.1.1.3.cmml" xref="S2.E1.m1.3.3.1.1.1.3"><apply id="S2.E1.m1.3.3.1.1.1.3.1.cmml" xref="S2.E1.m1.3.3.1.1.1.3.1"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.1.3.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1.3.1">subscript</csymbol><min id="S2.E1.m1.3.3.1.1.1.3.1.2.cmml" xref="S2.E1.m1.3.3.1.1.1.3.1.2"></min><ci id="S2.E1.m1.3.3.1.1.1.3.1.3.cmml" xref="S2.E1.m1.3.3.1.1.1.3.1.3">ğœ‹</ci></apply><apply id="S2.E1.m1.3.3.1.1.1.3.2.cmml" xref="S2.E1.m1.3.3.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.1.3.2.1.cmml" xref="S2.E1.m1.3.3.1.1.1.3.2">subscript</csymbol><ci id="S2.E1.m1.3.3.1.1.1.3.2.2.cmml" xref="S2.E1.m1.3.3.1.1.1.3.2.2">ğ”¼</ci><apply id="S2.E1.m1.3.3.1.1.1.3.2.3.cmml" xref="S2.E1.m1.3.3.1.1.1.3.2.3"><csymbol cd="latexml" id="S2.E1.m1.3.3.1.1.1.3.2.3.1.cmml" xref="S2.E1.m1.3.3.1.1.1.3.2.3.1">similar-to</csymbol><ci id="S2.E1.m1.3.3.1.1.1.3.2.3.2.cmml" xref="S2.E1.m1.3.3.1.1.1.3.2.3.2">ğ‘¥</ci><ci id="S2.E1.m1.3.3.1.1.1.3.2.3.3.cmml" xref="S2.E1.m1.3.3.1.1.1.3.2.3.3">ğœŒ</ci></apply></apply></apply><apply id="S2.E1.m1.3.3.1.1.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1"><csymbol cd="latexml" id="S2.E1.m1.3.3.1.1.1.1.2.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1"><minus id="S2.E1.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2"></minus><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3">subscript</csymbol><sum id="S2.E1.m1.3.3.1.1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.2"></sum><ci id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3">ğ‘–</ci></apply><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1"><times id="S2.E1.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.2"></times><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.3"><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.1"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.1">subscript</csymbol><log id="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.1.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.1.2"></log><cn id="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.1.3.cmml" type="integer" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.1.3">2</cn></apply><ci id="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.3.2">ğœ‹</ci></apply><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1">conditional</csymbol><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.2">ğ‘¥</ci><ci id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.3">ğ‘–</ci></apply><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2">ğ‘¥</ci><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3"><lt id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.1"></lt><csymbol cd="latexml" id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.2">absent</csymbol><ci id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.3">ğ‘–</ci></apply></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.3c">\min_{\pi}L_{\pi}^{\rho}=\min_{\pi}\underbrace{\mathbb{E}_{x\sim\rho}[-\log_{2%
}\pi(x)]}_{\text{``log loss''}}=\min_{\pi}\mathbb{E}_{x\sim\rho}\left[\sum_{i}%
-\log_{2}\pi(x_{i}|x_{&lt;i})\right].</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.3d">roman_min start_POSTSUBSCRIPT italic_Ï€ end_POSTSUBSCRIPT italic_L start_POSTSUBSCRIPT italic_Ï€ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_Ï end_POSTSUPERSCRIPT = roman_min start_POSTSUBSCRIPT italic_Ï€ end_POSTSUBSCRIPT underâŸ start_ARG blackboard_E start_POSTSUBSCRIPT italic_x âˆ¼ italic_Ï end_POSTSUBSCRIPT [ - roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_Ï€ ( italic_x ) ] end_ARG start_POSTSUBSCRIPT â€œlog lossâ€ end_POSTSUBSCRIPT = roman_min start_POSTSUBSCRIPT italic_Ï€ end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_x âˆ¼ italic_Ï end_POSTSUBSCRIPT [ âˆ‘ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_Ï€ ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT &lt; italic_i end_POSTSUBSCRIPT ) ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.p3.2">Note that the problem of constructing the actual codes that achieve (near) minimal expected code length given a predictor is largely solved in information theory, with gold-standard algorithms such as Huffman codingÂ <cite class="ltx_cite ltx_citemacro_citep">(Huffman, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib25" title="">1952</a>)</cite>, arithmetic codingÂ <cite class="ltx_cite ltx_citemacro_citep">(Pasco, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib52" title="">1977</a>; Rissanen, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib58" title="">1976</a>; Witten etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib81" title="">1987</a>)</cite>, or asymmetric numeral systemsÂ <cite class="ltx_cite ltx_citemacro_citep">(Duda, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib14" title="">2009</a>)</cite>.
The latter two compress strings online by iteratively converting them into a single binary number with increasing precision (see <cite class="ltx_cite ltx_citemacro_citet">DelÃ©tang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib11" title="">2024</a>)</cite> for an illustration or ChapterÂ 2 in <cite class="ltx_cite ltx_citemacro_citet">Hutter etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib27" title="">2024</a>)</cite>).
Arithmetic coding is an example of an online compression algorithm since it only requires a single pass through the data and compresses on the fly (unlike offline compressors, such as Huffman coding, that require multiple passes through the data).
Both our models and <cite class="ltx_cite ltx_citemacro_citet">Bellard (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib4" title="">2021</a>)</cite>, which we compare against, use arithmetic coding and compress online.
However, the difference is that we pre-train our predictor, i.e., we perform <em class="ltx_emph ltx_font_italic" id="S2.p3.2.1">offline training</em> on a dataset and then freeze its parameters (non-adaptive arithmetic coding), whereas <cite class="ltx_cite ltx_citemacro_citet">Bellard (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib4" title="">2021</a>)</cite> performs <em class="ltx_emph ltx_font_italic" id="S2.p3.2.2">online adaptation</em> of the model parameters on the data stream that is being compressed (adaptive arithmetic coding).
As a result, and unlike our compressors, <cite class="ltx_cite ltx_citemacro_citet">Bellard (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib4" title="">2021</a>)</cite> does not communicate the trained weights for decompression but only the model architecture and training algorithm (i.e., the model parameters do not need to be factored into the compression ratio).</p>
</div>
</section>
<section class="ltx_section" id="S3" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Related Work</h2>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Compression Without Transformers</h4>
<div class="ltx_para" id="S3.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px1.p1.1">Using neural networks as predictors for lossless compression has been extensively studied, both in conjunction with arithmetic codingÂ <cite class="ltx_cite ltx_citemacro_citep">(Lehtokangas etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib35" title="">1993</a>; Schmidhuber and Heil, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib62" title="">1994</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib63" title="">1996</a>; Mahoney, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib42" title="">2000</a>; Mikolov, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib47" title="">2012</a>; Knoll, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib32" title="">2014</a>; vanÂ den Oord and Schrauwen, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib75" title="">2014</a>; Cox, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib10" title="">2016</a>; Schiopu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib61" title="">2018</a>; Goyal etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib19" title="">2019</a>; Liu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib39" title="">2019</a>; Mentzer etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib45" title="">2019</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib46" title="">2020</a>; Schiopu and Munteanu, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib60" title="">2020</a>; Rhee etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib57" title="">2022</a>)</cite> and with asymmetric numeral systemsÂ <cite class="ltx_cite ltx_citemacro_citep">(Hoogeboom etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib23" title="">2019</a>; Kingma etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib31" title="">2019</a>; Townsend etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib73" title="">2019</a>; Barzen etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib2" title="">2022</a>)</cite>.
Lossy neural compression has been achieved, e.g., by overfitting tiny networks to individual data points and transmitting the model weights rather than the original dataÂ <cite class="ltx_cite ltx_citemacro_citep">(Dupont etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib15" title="">2021</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib16" title="">2022</a>; Chen etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib7" title="">2021</a>; Ladune etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib33" title="">2023</a>; Kim etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib29" title="">2023</a>)</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Online Transformers</h4>
<div class="ltx_para" id="S3.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px2.p1.1">Most of the above approaches use a separate training set to pre-train models that are then used to compress a test set.
Alternatively, the model can also be trained from scratch on the data stream that is being compressedÂ <cite class="ltx_cite ltx_citemacro_citep">(Bellard, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib3" title="">2019</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib4" title="">2021</a>; Goyal etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib20" title="">2020</a>; Mao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib44" title="">2022</a>)</cite>.
The main advantage of these adaptive online compressors is that they are (quasi) parameterless (since they are initialized from scratch when compressing a new stream of data), meaning that the model size does not explicitly affect the compression ratio, even for relatively large models (though it implicitly affects the training performance, e.g., large models train more slowly meaning that larger chunks of the initial data stream are only weakly compressed).
The transformer-based adaptive online compressor of <cite class="ltx_cite ltx_citemacro_citet">Bellard (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib4" title="">2021</a>)</cite> is currently state-of-the-art on the Large Text Compression BenchmarkÂ <cite class="ltx_cite ltx_citemacro_citep">(Mahoney, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib43" title="">2006</a>)</cite>, and our evaluation (in <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5" title="5 Results â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">5</span></a>) shows that our best models are on par across all modalities.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Pre-Trained Transformers</h4>
<div class="ltx_para" id="S3.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px3.p1.1">Most closely related to our work is the line of research by <cite class="ltx_cite ltx_citemacro_citet">Valmeekam etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib74" title="">2023</a>); DelÃ©tang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib11" title="">2024</a>); Huang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib24" title="">2024</a>); Li etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib38" title="">2024</a>); Mittu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib50" title="">2024</a>)</cite>, which investigates lossless compression via arithmetic coding with pre-trained foundation models, i.e., the Llama modelsÂ <cite class="ltx_cite ltx_citemacro_citep">(Touvron etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib71" title="">2023a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib72" title="">b</a>; Dubey etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib13" title="">2024</a>)</cite> and ChinchillaÂ <cite class="ltx_cite ltx_citemacro_citep">(Hoffmann etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib22" title="">2022</a>)</cite>.
<cite class="ltx_cite ltx_citemacro_citet">DelÃ©tang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib11" title="">2024</a>)</cite>, in particular, also report good compression rates on unseen modalities (LLMs trained only on text compress images and audio data well).
However, these studies differ from our work as they do not take the model size into account for the compression ratios, except for <cite class="ltx_cite ltx_citemacro_citet">DelÃ©tang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib11" title="">2024</a>)</cite>, who report both â€œrawâ€ and â€œadjustedâ€ compression ratios and find that LLMs are not competitive in terms of adjusted (i.e., the actual) compression ratios.
To the best of our knowledge, our paper is the first to systematically investigate the use of appropriately sized pre-trained transformers for multimodal lossless compression in a regime where competitive performance w.r.t.Â standard compression algorithms is possible.
In this regime, our study is the most comprehensive in that it also investigates multimodal training and cross-modal transfer of pre-trained transformers.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Methods</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We now describe our experimental setup (with additional details, e.g., sweeps, in <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A1" title="Appendix A Experimental Details â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Appendix</span>Â <span class="ltx_text ltx_ref_tag">A</span></a>).</p>
</div>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Baselines</h4>
<div class="ltx_para" id="S4.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px1.p1.1">We compare to various standard compressors, both general-purpose, i.e., gzipÂ <cite class="ltx_cite ltx_citemacro_citep">(Deutsch, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib12" title="">1996</a>)</cite> and LZMA2Â <cite class="ltx_cite ltx_citemacro_citep">(Pavlov, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib53" title="">2019</a>)</cite>, and domain-specific, i.e., FLACÂ <cite class="ltx_cite ltx_citemacro_citep">(Coalson, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib8" title="">2008</a>)</cite> for audio data and PNGÂ <cite class="ltx_cite ltx_citemacro_citep">(Boutell, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib5" title="">1997</a>)</cite> and lossless JPEGÂ 2000Â <cite class="ltx_cite ltx_citemacro_citep">(Skodras etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib67" title="">2001</a>)</cite> for images.
Both gzip and LZMA2 (which is used by the 7zip software) are based on Huffman codingÂ <cite class="ltx_cite ltx_citemacro_citep">(Huffman, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib25" title="">1952</a>)</cite> and the Lempel-Ziv-Welch algorithmÂ <cite class="ltx_cite ltx_citemacro_citep">(Welch, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib79" title="">1984</a>)</cite>.
We use the default parameters for gzip, LZMA2, and JPEGÂ 2000, compression level 12 for FLAC, and instruct PNG to find the optimal encoder settings.
We also compare to the online transformer from <cite class="ltx_cite ltx_citemacro_citet">Bellard (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib4" title="">2021</a>)</cite>, with the default v3.3 parameters, which is the current state-of-the-art on the Large Text Compression BenchmarkÂ <cite class="ltx_cite ltx_citemacro_citep">(Mahoney, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib43" title="">2006</a>)</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Models</h4>
<div class="ltx_para" id="S4.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px2.p1.8">We focus on decoder-only transformersÂ <cite class="ltx_cite ltx_citemacro_citep">(Vaswani etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib76" title="">2017</a>)</cite> with SwiGLU activationsÂ <cite class="ltx_cite ltx_citemacro_citep">(Shazeer, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib66" title="">2020</a>)</cite> and post-layer normalization.
Unless otherwise noted, we use <math alttext="8" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.1.m1.1"><semantics id="S4.SS0.SSS0.Px2.p1.1.m1.1a"><mn id="S4.SS0.SSS0.Px2.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.1.m1.1b"><cn id="S4.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.1.m1.1c">8</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.1.m1.1d">8</annotation></semantics></math> heads, an embedding dimension of <math alttext="64" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.2.m2.1"><semantics id="S4.SS0.SSS0.Px2.p1.2.m2.1a"><mn id="S4.SS0.SSS0.Px2.p1.2.m2.1.1" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.2.m2.1b"><cn id="S4.SS0.SSS0.Px2.p1.2.m2.1.1.cmml" type="integer" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.2.m2.1c">64</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.2.m2.1d">64</annotation></semantics></math>, a context size of <math alttext="4096" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.3.m3.1"><semantics id="S4.SS0.SSS0.Px2.p1.3.m3.1a"><mn id="S4.SS0.SSS0.Px2.p1.3.m3.1.1" xref="S4.SS0.SSS0.Px2.p1.3.m3.1.1.cmml">4096</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.3.m3.1b"><cn id="S4.SS0.SSS0.Px2.p1.3.m3.1.1.cmml" type="integer" xref="S4.SS0.SSS0.Px2.p1.3.m3.1.1">4096</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.3.m3.1c">4096</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.3.m3.1d">4096</annotation></semantics></math> (bytes), and sliding windows without overlap or memory (full details in <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A1.SS3" title="A.3 Sweeps â€£ Appendix A Experimental Details â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">A.3</span></a>).
We train our models with the Adam optimizerÂ <cite class="ltx_cite ltx_citemacro_citep">(Kingma and Ba, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib30" title="">2015</a>)</cite> for <math alttext="2.5" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.4.m4.1"><semantics id="S4.SS0.SSS0.Px2.p1.4.m4.1a"><mn id="S4.SS0.SSS0.Px2.p1.4.m4.1.1" xref="S4.SS0.SSS0.Px2.p1.4.m4.1.1.cmml">2.5</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.4.m4.1b"><cn id="S4.SS0.SSS0.Px2.p1.4.m4.1.1.cmml" type="float" xref="S4.SS0.SSS0.Px2.p1.4.m4.1.1">2.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.4.m4.1c">2.5</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.4.m4.1d">2.5</annotation></semantics></math> million steps with a batch size of <math alttext="32" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.5.m5.1"><semantics id="S4.SS0.SSS0.Px2.p1.5.m5.1a"><mn id="S4.SS0.SSS0.Px2.p1.5.m5.1.1" xref="S4.SS0.SSS0.Px2.p1.5.m5.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.5.m5.1b"><cn id="S4.SS0.SSS0.Px2.p1.5.m5.1.1.cmml" type="integer" xref="S4.SS0.SSS0.Px2.p1.5.m5.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.5.m5.1c">32</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.5.m5.1d">32</annotation></semantics></math>, which, for <math alttext="165" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.6.m6.1"><semantics id="S4.SS0.SSS0.Px2.p1.6.m6.1a"><mn id="S4.SS0.SSS0.Px2.p1.6.m6.1.1" xref="S4.SS0.SSS0.Px2.p1.6.m6.1.1.cmml">165</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.6.m6.1b"><cn id="S4.SS0.SSS0.Px2.p1.6.m6.1.1.cmml" type="integer" xref="S4.SS0.SSS0.Px2.p1.6.m6.1.1">165</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.6.m6.1c">165</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.6.m6.1d">165</annotation></semantics></math>GB of data, roughly corresponds to <math alttext="2" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.7.m7.1"><semantics id="S4.SS0.SSS0.Px2.p1.7.m7.1a"><mn id="S4.SS0.SSS0.Px2.p1.7.m7.1.1" xref="S4.SS0.SSS0.Px2.p1.7.m7.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.7.m7.1b"><cn id="S4.SS0.SSS0.Px2.p1.7.m7.1.1.cmml" type="integer" xref="S4.SS0.SSS0.Px2.p1.7.m7.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.7.m7.1c">2</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.7.m7.1d">2</annotation></semantics></math> epochs.
Due to the duality of compression and prediction, we minimize the standard (sequential) <math alttext="\log" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.8.m8.1"><semantics id="S4.SS0.SSS0.Px2.p1.8.m8.1a"><mi id="S4.SS0.SSS0.Px2.p1.8.m8.1.1" xref="S4.SS0.SSS0.Px2.p1.8.m8.1.1.cmml">log</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.8.m8.1b"><log id="S4.SS0.SSS0.Px2.p1.8.m8.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.8.m8.1.1"></log></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.8.m8.1c">\log</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.8.m8.1d">roman_log</annotation></semantics></math>-lossÂ (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S2.E1" title="In 2 Background â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Eq.</span>Â <span class="ltx_text ltx_ref_tag">1</span></a>) during training, which is a maximum-compression objective (see <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S2" title="2 Background â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">(No) Tokenization</h4>
<div class="ltx_para" id="S4.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px3.p1.1">Tokenization is a commonly-used, <em class="ltx_emph ltx_font_italic" id="S4.SS0.SSS0.Px3.p1.1.1">domain-specific</em> pre-compression step to boost transformersâ€™ performance by increasing their vocabulary size in order to fit more information into their limited context windowÂ <cite class="ltx_cite ltx_citemacro_citep">(Lester etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib36" title="">2024</a>)</cite>, i.e., increased information density at the cost of increased entropy.
However, since our goal is to be domain-general, we do not use tokenization and instead feed our models directly with byte streams (we still have to choose how to flatten images and how to sample audio signals, which are minimal domain-specific preprocessing steps).</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Evaluation</h4>
<div class="ltx_para" id="S4.SS0.SSS0.Px4.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px4.p1.7">To evaluate performance, we compute the compression ratio (lower is better):</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathrm{compression\leavevmode\nobreak\ ratio}:=\frac{\mathrm{size\leavevmode%
\nobreak\ of\leavevmode\nobreak\ compressed\leavevmode\nobreak\ data}+\mathrm{%
size\leavevmode\nobreak\ of\leavevmode\nobreak\ compressor}}{\mathrm{size%
\leavevmode\nobreak\ of\leavevmode\nobreak\ uncompressed\leavevmode\nobreak\ %
data}}," class="ltx_Math" display="block" id="S4.E2.m1.1"><semantics id="S4.E2.m1.1a"><mrow id="S4.E2.m1.1.1.1" xref="S4.E2.m1.1.1.1.1.cmml"><mrow id="S4.E2.m1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.cmml"><mrow id="S4.E2.m1.1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.2.cmml"><mi id="S4.E2.m1.1.1.1.1.2.2" xref="S4.E2.m1.1.1.1.1.2.2.cmml">compression</mi><mo id="S4.E2.m1.1.1.1.1.2.1" lspace="0.500em" xref="S4.E2.m1.1.1.1.1.2.1.cmml">â¢</mo><mi id="S4.E2.m1.1.1.1.1.2.3" xref="S4.E2.m1.1.1.1.1.2.3.cmml">ratio</mi></mrow><mo id="S4.E2.m1.1.1.1.1.1" lspace="0.278em" rspace="0.278em" xref="S4.E2.m1.1.1.1.1.1.cmml">:=</mo><mfrac id="S4.E2.m1.1.1.1.1.3" xref="S4.E2.m1.1.1.1.1.3.cmml"><mrow id="S4.E2.m1.1.1.1.1.3.2" xref="S4.E2.m1.1.1.1.1.3.2.cmml"><mrow id="S4.E2.m1.1.1.1.1.3.2.2" xref="S4.E2.m1.1.1.1.1.3.2.2.cmml"><mi id="S4.E2.m1.1.1.1.1.3.2.2.2" xref="S4.E2.m1.1.1.1.1.3.2.2.2.cmml">size</mi><mo id="S4.E2.m1.1.1.1.1.3.2.2.1" lspace="0.500em" xref="S4.E2.m1.1.1.1.1.3.2.2.1.cmml">â¢</mo><mi id="S4.E2.m1.1.1.1.1.3.2.2.3" xref="S4.E2.m1.1.1.1.1.3.2.2.3.cmml">of</mi><mo id="S4.E2.m1.1.1.1.1.3.2.2.1a" lspace="0.500em" xref="S4.E2.m1.1.1.1.1.3.2.2.1.cmml">â¢</mo><mi id="S4.E2.m1.1.1.1.1.3.2.2.4" xref="S4.E2.m1.1.1.1.1.3.2.2.4.cmml">compressed</mi><mo id="S4.E2.m1.1.1.1.1.3.2.2.1b" lspace="0.500em" xref="S4.E2.m1.1.1.1.1.3.2.2.1.cmml">â¢</mo><mi id="S4.E2.m1.1.1.1.1.3.2.2.5" xref="S4.E2.m1.1.1.1.1.3.2.2.5.cmml">data</mi></mrow><mo id="S4.E2.m1.1.1.1.1.3.2.1" xref="S4.E2.m1.1.1.1.1.3.2.1.cmml">+</mo><mrow id="S4.E2.m1.1.1.1.1.3.2.3" xref="S4.E2.m1.1.1.1.1.3.2.3.cmml"><mi id="S4.E2.m1.1.1.1.1.3.2.3.2" xref="S4.E2.m1.1.1.1.1.3.2.3.2.cmml">size</mi><mo id="S4.E2.m1.1.1.1.1.3.2.3.1" lspace="0.500em" xref="S4.E2.m1.1.1.1.1.3.2.3.1.cmml">â¢</mo><mi id="S4.E2.m1.1.1.1.1.3.2.3.3" xref="S4.E2.m1.1.1.1.1.3.2.3.3.cmml">of</mi><mo id="S4.E2.m1.1.1.1.1.3.2.3.1a" lspace="0.500em" xref="S4.E2.m1.1.1.1.1.3.2.3.1.cmml">â¢</mo><mi id="S4.E2.m1.1.1.1.1.3.2.3.4" xref="S4.E2.m1.1.1.1.1.3.2.3.4.cmml">compressor</mi></mrow></mrow><mrow id="S4.E2.m1.1.1.1.1.3.3" xref="S4.E2.m1.1.1.1.1.3.3.cmml"><mi id="S4.E2.m1.1.1.1.1.3.3.2" xref="S4.E2.m1.1.1.1.1.3.3.2.cmml">size</mi><mo id="S4.E2.m1.1.1.1.1.3.3.1" lspace="0.500em" xref="S4.E2.m1.1.1.1.1.3.3.1.cmml">â¢</mo><mi id="S4.E2.m1.1.1.1.1.3.3.3" xref="S4.E2.m1.1.1.1.1.3.3.3.cmml">of</mi><mo id="S4.E2.m1.1.1.1.1.3.3.1a" lspace="0.500em" xref="S4.E2.m1.1.1.1.1.3.3.1.cmml">â¢</mo><mi id="S4.E2.m1.1.1.1.1.3.3.4" xref="S4.E2.m1.1.1.1.1.3.3.4.cmml">uncompressed</mi><mo id="S4.E2.m1.1.1.1.1.3.3.1b" lspace="0.500em" xref="S4.E2.m1.1.1.1.1.3.3.1.cmml">â¢</mo><mi id="S4.E2.m1.1.1.1.1.3.3.5" xref="S4.E2.m1.1.1.1.1.3.3.5.cmml">data</mi></mrow></mfrac></mrow><mo id="S4.E2.m1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.1b"><apply id="S4.E2.m1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1"><csymbol cd="latexml" id="S4.E2.m1.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.1">assign</csymbol><apply id="S4.E2.m1.1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.2"><times id="S4.E2.m1.1.1.1.1.2.1.cmml" xref="S4.E2.m1.1.1.1.1.2.1"></times><ci id="S4.E2.m1.1.1.1.1.2.2.cmml" xref="S4.E2.m1.1.1.1.1.2.2">compression</ci><ci id="S4.E2.m1.1.1.1.1.2.3.cmml" xref="S4.E2.m1.1.1.1.1.2.3">ratio</ci></apply><apply id="S4.E2.m1.1.1.1.1.3.cmml" xref="S4.E2.m1.1.1.1.1.3"><divide id="S4.E2.m1.1.1.1.1.3.1.cmml" xref="S4.E2.m1.1.1.1.1.3"></divide><apply id="S4.E2.m1.1.1.1.1.3.2.cmml" xref="S4.E2.m1.1.1.1.1.3.2"><plus id="S4.E2.m1.1.1.1.1.3.2.1.cmml" xref="S4.E2.m1.1.1.1.1.3.2.1"></plus><apply id="S4.E2.m1.1.1.1.1.3.2.2.cmml" xref="S4.E2.m1.1.1.1.1.3.2.2"><times id="S4.E2.m1.1.1.1.1.3.2.2.1.cmml" xref="S4.E2.m1.1.1.1.1.3.2.2.1"></times><ci id="S4.E2.m1.1.1.1.1.3.2.2.2.cmml" xref="S4.E2.m1.1.1.1.1.3.2.2.2">size</ci><ci id="S4.E2.m1.1.1.1.1.3.2.2.3.cmml" xref="S4.E2.m1.1.1.1.1.3.2.2.3">of</ci><ci id="S4.E2.m1.1.1.1.1.3.2.2.4.cmml" xref="S4.E2.m1.1.1.1.1.3.2.2.4">compressed</ci><ci id="S4.E2.m1.1.1.1.1.3.2.2.5.cmml" xref="S4.E2.m1.1.1.1.1.3.2.2.5">data</ci></apply><apply id="S4.E2.m1.1.1.1.1.3.2.3.cmml" xref="S4.E2.m1.1.1.1.1.3.2.3"><times id="S4.E2.m1.1.1.1.1.3.2.3.1.cmml" xref="S4.E2.m1.1.1.1.1.3.2.3.1"></times><ci id="S4.E2.m1.1.1.1.1.3.2.3.2.cmml" xref="S4.E2.m1.1.1.1.1.3.2.3.2">size</ci><ci id="S4.E2.m1.1.1.1.1.3.2.3.3.cmml" xref="S4.E2.m1.1.1.1.1.3.2.3.3">of</ci><ci id="S4.E2.m1.1.1.1.1.3.2.3.4.cmml" xref="S4.E2.m1.1.1.1.1.3.2.3.4">compressor</ci></apply></apply><apply id="S4.E2.m1.1.1.1.1.3.3.cmml" xref="S4.E2.m1.1.1.1.1.3.3"><times id="S4.E2.m1.1.1.1.1.3.3.1.cmml" xref="S4.E2.m1.1.1.1.1.3.3.1"></times><ci id="S4.E2.m1.1.1.1.1.3.3.2.cmml" xref="S4.E2.m1.1.1.1.1.3.3.2">size</ci><ci id="S4.E2.m1.1.1.1.1.3.3.3.cmml" xref="S4.E2.m1.1.1.1.1.3.3.3">of</ci><ci id="S4.E2.m1.1.1.1.1.3.3.4.cmml" xref="S4.E2.m1.1.1.1.1.3.3.4">uncompressed</ci><ci id="S4.E2.m1.1.1.1.1.3.3.5.cmml" xref="S4.E2.m1.1.1.1.1.3.3.5">data</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.1c">\mathrm{compression\leavevmode\nobreak\ ratio}:=\frac{\mathrm{size\leavevmode%
\nobreak\ of\leavevmode\nobreak\ compressed\leavevmode\nobreak\ data}+\mathrm{%
size\leavevmode\nobreak\ of\leavevmode\nobreak\ compressor}}{\mathrm{size%
\leavevmode\nobreak\ of\leavevmode\nobreak\ uncompressed\leavevmode\nobreak\ %
data}},</annotation><annotation encoding="application/x-llamapun" id="S4.E2.m1.1d">roman_compression roman_ratio := divide start_ARG roman_size roman_of roman_compressed roman_data + roman_size roman_of roman_compressor end_ARG start_ARG roman_size roman_of roman_uncompressed roman_data end_ARG ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS0.SSS0.Px4.p1.6">which accounts for the model size and is equivalent to the â€œadjusted compression rateâ€ of <cite class="ltx_cite ltx_citemacro_citet">DelÃ©tang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib11" title="">2024</a>)</cite>.
We always evaluate on <math alttext="1" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px4.p1.1.m1.1"><semantics id="S4.SS0.SSS0.Px4.p1.1.m1.1a"><mn id="S4.SS0.SSS0.Px4.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px4.p1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px4.p1.1.m1.1b"><cn id="S4.SS0.SSS0.Px4.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS0.SSS0.Px4.p1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px4.p1.1.m1.1c">1</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px4.p1.1.m1.1d">1</annotation></semantics></math>GB of out-of-distribution data, i.e., <math alttext="\mathrm{size\leavevmode\nobreak\ of\leavevmode\nobreak\ uncompressed%
\leavevmode\nobreak\ data}=1\mathrm{GB}" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px4.p1.2.m2.1"><semantics id="S4.SS0.SSS0.Px4.p1.2.m2.1a"><mrow id="S4.SS0.SSS0.Px4.p1.2.m2.1.1" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.cmml"><mrow id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.cmml"><mi id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.2" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.2.cmml">size</mi><mo id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.1" lspace="0.500em" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.1.cmml">â¢</mo><mi id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.3" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.3.cmml">of</mi><mo id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.1a" lspace="0.500em" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.1.cmml">â¢</mo><mi id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.4" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.4.cmml">uncompressed</mi><mo id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.1b" lspace="0.500em" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.1.cmml">â¢</mo><mi id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.5" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.5.cmml">data</mi></mrow><mo id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.1" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.1.cmml">=</mo><mrow id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3.cmml"><mn id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3.2" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3.2.cmml">1</mn><mo id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3.1" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3.1.cmml">â¢</mo><mi id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3.3" mathvariant="normal" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3.3.cmml">G</mi><mo id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3.1a" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3.1.cmml">â¢</mo><mi id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3.4" mathvariant="normal" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3.4.cmml">B</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px4.p1.2.m2.1b"><apply id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.cmml" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1"><eq id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.1.cmml" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.1"></eq><apply id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.cmml" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2"><times id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.1.cmml" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.1"></times><ci id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.2.cmml" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.2">size</ci><ci id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.3.cmml" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.3">of</ci><ci id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.4.cmml" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.4">uncompressed</ci><ci id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.5.cmml" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.5">data</ci></apply><apply id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3.cmml" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3"><times id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3.1.cmml" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3.1"></times><cn id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3.2.cmml" type="integer" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3.2">1</cn><ci id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3.3.cmml" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3.3">G</ci><ci id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3.4.cmml" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3.4">B</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px4.p1.2.m2.1c">\mathrm{size\leavevmode\nobreak\ of\leavevmode\nobreak\ uncompressed%
\leavevmode\nobreak\ data}=1\mathrm{GB}</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px4.p1.2.m2.1d">roman_size roman_of roman_uncompressed roman_data = 1 roman_G roman_B</annotation></semantics></math>.
As <cite class="ltx_cite ltx_citemacro_citet">DelÃ©tang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib11" title="">2024</a>)</cite>, we compute the size of the compressor by encoding the model weights with <span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px4.p1.6.1">float16</span> (2 bytes per parameter) since this level of quantization does not significantly affect performanceÂ <cite class="ltx_cite ltx_citemacro_citep">(Tao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib70" title="">2022</a>)</cite> and is standard for model inference.
As a result, our model sizes range from <math alttext="0.8" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px4.p1.3.m3.1"><semantics id="S4.SS0.SSS0.Px4.p1.3.m3.1a"><mn id="S4.SS0.SSS0.Px4.p1.3.m3.1.1" xref="S4.SS0.SSS0.Px4.p1.3.m3.1.1.cmml">0.8</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px4.p1.3.m3.1b"><cn id="S4.SS0.SSS0.Px4.p1.3.m3.1.1.cmml" type="float" xref="S4.SS0.SSS0.Px4.p1.3.m3.1.1">0.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px4.p1.3.m3.1c">0.8</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px4.p1.3.m3.1d">0.8</annotation></semantics></math>MB to <math alttext="40.3" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px4.p1.4.m4.1"><semantics id="S4.SS0.SSS0.Px4.p1.4.m4.1a"><mn id="S4.SS0.SSS0.Px4.p1.4.m4.1.1" xref="S4.SS0.SSS0.Px4.p1.4.m4.1.1.cmml">40.3</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px4.p1.4.m4.1b"><cn id="S4.SS0.SSS0.Px4.p1.4.m4.1.1.cmml" type="float" xref="S4.SS0.SSS0.Px4.p1.4.m4.1.1">40.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px4.p1.4.m4.1c">40.3</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px4.p1.4.m4.1d">40.3</annotation></semantics></math>MB.
Note that, similar to <cite class="ltx_cite ltx_citemacro_citet">DelÃ©tang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib11" title="">2024</a>)</cite>, we do not compress the model parameters, since naive approaches (e.g., compressing them with gzip) do not significantly decrease the model size (only by around <math alttext="7" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px4.p1.5.m5.1"><semantics id="S4.SS0.SSS0.Px4.p1.5.m5.1a"><mn id="S4.SS0.SSS0.Px4.p1.5.m5.1.1" xref="S4.SS0.SSS0.Px4.p1.5.m5.1.1.cmml">7</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px4.p1.5.m5.1b"><cn id="S4.SS0.SSS0.Px4.p1.5.m5.1.1.cmml" type="integer" xref="S4.SS0.SSS0.Px4.p1.5.m5.1.1">7</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px4.p1.5.m5.1c">7</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px4.p1.5.m5.1d">7</annotation></semantics></math>%, which corresponds to a decrease in compression ratio of only <math alttext="0.002821" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px4.p1.6.m6.1"><semantics id="S4.SS0.SSS0.Px4.p1.6.m6.1a"><mn id="S4.SS0.SSS0.Px4.p1.6.m6.1.1" xref="S4.SS0.SSS0.Px4.p1.6.m6.1.1.cmml">0.002821</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px4.p1.6.m6.1b"><cn id="S4.SS0.SSS0.Px4.p1.6.m6.1.1.cmml" type="float" xref="S4.SS0.SSS0.Px4.p1.6.m6.1.1">0.002821</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px4.p1.6.m6.1c">0.002821</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px4.p1.6.m6.1d">0.002821</annotation></semantics></math> for our largest model).
However, as a result, the compression ratio we report is technically an upper bound, which could be improved by (losslessly) compressing the parameters (though with limited room for improvement in our regime, even in the best case).</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px5">
<h4 class="ltx_title ltx_title_paragraph">Training Datasets</h4>
<div class="ltx_para" id="S4.SS0.SSS0.Px5.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px5.p1.13">A key point of our investigation is to evaluate how well pre-trained transformers can compress data from different modalities â€” both if the modality was or was not part of the training data (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S1.F1" title="In 1 Introduction â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">1</span></a> visualizes our data collection process).
We create three different unimodal training datasets with audio, images, and text data, and four multimodal training sets (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A1.SS1" title="A.1 Training Data Sources â€£ Appendix A Experimental Details â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">A.1</span></a> describes the datasets in full detail).
This yields seven pre-training datasets in total, each consisting of <math alttext="165" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px5.p1.1.m1.1"><semantics id="S4.SS0.SSS0.Px5.p1.1.m1.1a"><mn id="S4.SS0.SSS0.Px5.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px5.p1.1.m1.1.1.cmml">165</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.1.m1.1b"><cn id="S4.SS0.SSS0.Px5.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS0.SSS0.Px5.p1.1.m1.1.1">165</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.1.m1.1c">165</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px5.p1.1.m1.1d">165</annotation></semantics></math>GB of data: (i) <math alttext="165" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px5.p1.2.m2.1"><semantics id="S4.SS0.SSS0.Px5.p1.2.m2.1a"><mn id="S4.SS0.SSS0.Px5.p1.2.m2.1.1" xref="S4.SS0.SSS0.Px5.p1.2.m2.1.1.cmml">165</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.2.m2.1b"><cn id="S4.SS0.SSS0.Px5.p1.2.m2.1.1.cmml" type="integer" xref="S4.SS0.SSS0.Px5.p1.2.m2.1.1">165</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.2.m2.1c">165</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px5.p1.2.m2.1d">165</annotation></semantics></math>GB of audio; (ii) <math alttext="165" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px5.p1.3.m3.1"><semantics id="S4.SS0.SSS0.Px5.p1.3.m3.1a"><mn id="S4.SS0.SSS0.Px5.p1.3.m3.1.1" xref="S4.SS0.SSS0.Px5.p1.3.m3.1.1.cmml">165</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.3.m3.1b"><cn id="S4.SS0.SSS0.Px5.p1.3.m3.1.1.cmml" type="integer" xref="S4.SS0.SSS0.Px5.p1.3.m3.1.1">165</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.3.m3.1c">165</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px5.p1.3.m3.1d">165</annotation></semantics></math>GB of images; (iii) <math alttext="165" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px5.p1.4.m4.1"><semantics id="S4.SS0.SSS0.Px5.p1.4.m4.1a"><mn id="S4.SS0.SSS0.Px5.p1.4.m4.1.1" xref="S4.SS0.SSS0.Px5.p1.4.m4.1.1.cmml">165</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.4.m4.1b"><cn id="S4.SS0.SSS0.Px5.p1.4.m4.1.1.cmml" type="integer" xref="S4.SS0.SSS0.Px5.p1.4.m4.1.1">165</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.4.m4.1c">165</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px5.p1.4.m4.1d">165</annotation></semantics></math>GB of text; (iv) <math alttext="82.5" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px5.p1.5.m5.1"><semantics id="S4.SS0.SSS0.Px5.p1.5.m5.1a"><mn id="S4.SS0.SSS0.Px5.p1.5.m5.1.1" xref="S4.SS0.SSS0.Px5.p1.5.m5.1.1.cmml">82.5</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.5.m5.1b"><cn id="S4.SS0.SSS0.Px5.p1.5.m5.1.1.cmml" type="float" xref="S4.SS0.SSS0.Px5.p1.5.m5.1.1">82.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.5.m5.1c">82.5</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px5.p1.5.m5.1d">82.5</annotation></semantics></math>GB of audio and <math alttext="82.5" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px5.p1.6.m6.1"><semantics id="S4.SS0.SSS0.Px5.p1.6.m6.1a"><mn id="S4.SS0.SSS0.Px5.p1.6.m6.1.1" xref="S4.SS0.SSS0.Px5.p1.6.m6.1.1.cmml">82.5</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.6.m6.1b"><cn id="S4.SS0.SSS0.Px5.p1.6.m6.1.1.cmml" type="float" xref="S4.SS0.SSS0.Px5.p1.6.m6.1.1">82.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.6.m6.1c">82.5</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px5.p1.6.m6.1d">82.5</annotation></semantics></math>GB of images; (v) <math alttext="82.5" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px5.p1.7.m7.1"><semantics id="S4.SS0.SSS0.Px5.p1.7.m7.1a"><mn id="S4.SS0.SSS0.Px5.p1.7.m7.1.1" xref="S4.SS0.SSS0.Px5.p1.7.m7.1.1.cmml">82.5</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.7.m7.1b"><cn id="S4.SS0.SSS0.Px5.p1.7.m7.1.1.cmml" type="float" xref="S4.SS0.SSS0.Px5.p1.7.m7.1.1">82.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.7.m7.1c">82.5</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px5.p1.7.m7.1d">82.5</annotation></semantics></math>GB of audio and <math alttext="82.5" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px5.p1.8.m8.1"><semantics id="S4.SS0.SSS0.Px5.p1.8.m8.1a"><mn id="S4.SS0.SSS0.Px5.p1.8.m8.1.1" xref="S4.SS0.SSS0.Px5.p1.8.m8.1.1.cmml">82.5</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.8.m8.1b"><cn id="S4.SS0.SSS0.Px5.p1.8.m8.1.1.cmml" type="float" xref="S4.SS0.SSS0.Px5.p1.8.m8.1.1">82.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.8.m8.1c">82.5</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px5.p1.8.m8.1d">82.5</annotation></semantics></math>GB of text; (vi) <math alttext="82.5" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px5.p1.9.m9.1"><semantics id="S4.SS0.SSS0.Px5.p1.9.m9.1a"><mn id="S4.SS0.SSS0.Px5.p1.9.m9.1.1" xref="S4.SS0.SSS0.Px5.p1.9.m9.1.1.cmml">82.5</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.9.m9.1b"><cn id="S4.SS0.SSS0.Px5.p1.9.m9.1.1.cmml" type="float" xref="S4.SS0.SSS0.Px5.p1.9.m9.1.1">82.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.9.m9.1c">82.5</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px5.p1.9.m9.1d">82.5</annotation></semantics></math>GB of images and <math alttext="82.5" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px5.p1.10.m10.1"><semantics id="S4.SS0.SSS0.Px5.p1.10.m10.1a"><mn id="S4.SS0.SSS0.Px5.p1.10.m10.1.1" xref="S4.SS0.SSS0.Px5.p1.10.m10.1.1.cmml">82.5</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.10.m10.1b"><cn id="S4.SS0.SSS0.Px5.p1.10.m10.1.1.cmml" type="float" xref="S4.SS0.SSS0.Px5.p1.10.m10.1.1">82.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.10.m10.1c">82.5</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px5.p1.10.m10.1d">82.5</annotation></semantics></math>GB of text; and (vii) <math alttext="55" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px5.p1.11.m11.1"><semantics id="S4.SS0.SSS0.Px5.p1.11.m11.1a"><mn id="S4.SS0.SSS0.Px5.p1.11.m11.1.1" xref="S4.SS0.SSS0.Px5.p1.11.m11.1.1.cmml">55</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.11.m11.1b"><cn id="S4.SS0.SSS0.Px5.p1.11.m11.1.1.cmml" type="integer" xref="S4.SS0.SSS0.Px5.p1.11.m11.1.1">55</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.11.m11.1c">55</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px5.p1.11.m11.1d">55</annotation></semantics></math>GB audio, <math alttext="55" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px5.p1.12.m12.1"><semantics id="S4.SS0.SSS0.Px5.p1.12.m12.1a"><mn id="S4.SS0.SSS0.Px5.p1.12.m12.1.1" xref="S4.SS0.SSS0.Px5.p1.12.m12.1.1.cmml">55</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.12.m12.1b"><cn id="S4.SS0.SSS0.Px5.p1.12.m12.1.1.cmml" type="integer" xref="S4.SS0.SSS0.Px5.p1.12.m12.1.1">55</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.12.m12.1c">55</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px5.p1.12.m12.1d">55</annotation></semantics></math>GB of images, and <math alttext="55" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px5.p1.13.m13.1"><semantics id="S4.SS0.SSS0.Px5.p1.13.m13.1a"><mn id="S4.SS0.SSS0.Px5.p1.13.m13.1.1" xref="S4.SS0.SSS0.Px5.p1.13.m13.1.1.cmml">55</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.13.m13.1b"><cn id="S4.SS0.SSS0.Px5.p1.13.m13.1.1.cmml" type="integer" xref="S4.SS0.SSS0.Px5.p1.13.m13.1.1">55</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.13.m13.1c">55</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px5.p1.13.m13.1d">55</annotation></semantics></math>GB text.
By training our models on all seven training data mixtures, we can investigate <em class="ltx_emph ltx_font_italic" id="S4.SS0.SSS0.Px5.p1.13.1">in-modality</em> and <em class="ltx_emph ltx_font_italic" id="S4.SS0.SSS0.Px5.p1.13.2">out-of-modality</em> compression ratios.
For example, for a model trained on the text dataset, the in-modality compression ratio can be determined by evaluating on text, while audio or image data provide out-of-modality compression ratios.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px6">
<h4 class="ltx_title ltx_title_paragraph">Out-of-Distribution Evaluation Datasets</h4>
<div class="ltx_para" id="S4.SS0.SSS0.Px6.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px6.p1.1">To mimic the setting for which standard compression algorithms were developed (and thereby ensure a fair comparison), where the compressor is programmed with only minimal statistical assumptions about the data (with stronger assumptions for domain-specific compressors), we evaluate on unseen, out-of-distribution (OOD) datasets for each modality and not on in-distribution held-out datasets (as commonly done in machine learning).
To do so, we create a single OOD dataset of <math alttext="1" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px6.p1.1.m1.1"><semantics id="S4.SS0.SSS0.Px6.p1.1.m1.1a"><mn id="S4.SS0.SSS0.Px6.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px6.p1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px6.p1.1.m1.1b"><cn id="S4.SS0.SSS0.Px6.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS0.SSS0.Px6.p1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px6.p1.1.m1.1c">1</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px6.p1.1.m1.1d">1</annotation></semantics></math>GB for each modality (full details in <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A1.SS2" title="A.2 Out-of-Distribution Evaluation Data Sources â€£ Appendix A Experimental Details â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">A.2</span></a>).</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section, we present our extensive experimental evaluation (additional results in <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A2" title="Appendix B Additional Results â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Appendix</span>Â <span class="ltx_text ltx_ref_tag">B</span></a>).
Unless otherwise noted, we report the best results over two hyperparameter sweeps (described in <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A1.SS3" title="A.3 Sweeps â€£ Appendix A Experimental Details â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">A.3</span></a>): (i) over the model- and dataset sizes, and (ii) over the model- and context sizes.</p>
</div>
<figure class="ltx_figure" id="S5.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="516" id="S5.F2.1.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>
Small pre-trained transformers can be domain-general compressors (panels correspond to evaluation data mixtures, bars to training data mixtures).
On every out-of-distribution evaluation data mixture, our method (i.e., the bars) outperforms standard compression algorithms (all horizontal lines except for â€˜Bellardâ€™) and is on par with Bellardâ€™s online adaptive transformers (the dark blue line) â€” as long as the evaluation modality was included in the training data mixture.
For unseen modalities we observe very little cross-modal transfer (which is different from observations made with foundation models <cite class="ltx_cite ltx_citemacro_cite">DelÃ©tang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib11" title="">2024</a>)</cite>).
Unimodal training leads to models that are good for their respective modality, but multimodal training yields models that perform almost as well as the unimodal models across all their training modalities (despite seeing a lot less data per modality than the unimodal models), i.e., one can trade off a small amount of performance on each individual modality in return for a strong domain-general compressor via multimodal training (gray bar).
</figcaption>
</figure>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Small Transformers Can Be Domain-General Compressors</h4>
<div class="ltx_para" id="S5.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px1.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.F2" title="In 5 Results â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">2</span></a> shows the best compression ratio attained on each of the seven out-of-distribution evaluation datasets when training a model on each of the seven training data mixtures (we report the best-performing model from our two sweeps for each training-evaluation pair).
We observe that transformers can achieve state-of-the-art in-modality compression ratios, regardless of the concrete composition of the training mixture, outperforming standard compression algorithms (even domain-specific ones) in all cases where all evaluation modalities are part of the training mixture.
In these cases, transformers thus learn the prototypical statistical patterns related to that modality during pre-training.
Importantly, by comparing models trained on unimodal vs.Â multimodal data, we observe that multimodal training only slightly decreases the compression performance compared to the unimodal models on their respective modalities (despite only having half or a third amount of data from that modality).
This means that it is possible to trade off a small amount of performance on each individual modality to obtain a very strong domain-general compressor via multimodal training (the gray bar in <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.F2" title="In 5 Results â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
<figure class="ltx_figure" id="S5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="245" id="S5.F3.1.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>
What you see is what you get.
Each panel visualizes the compression ratios for one of our modalities when training models on varying dataset mixtures and sizes.
Although one can replace a large proportion of a unimodal training dataset with a multimodal training mixture and not incur a significant loss on the original modality, transformers (at our tested model sizes) do not exhibit improved transfer from the out-of-modality data (i.e., the multimodal models are worse than the unimodal ones, even when trained on much more data from that particular modality).
The upshot is that the multimodal training data does not hurt much (note the scale of the y-axis), but leads to significantly improved multimodal compression performance as shown inÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.F2" title="In 5 Results â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">2</span></a>.
</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">What You See Is What You Get</h4>
<div class="ltx_para" id="S5.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px2.p1.13">While <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.F2" title="In 5 Results â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">2</span></a> shows that substituting half or two thirds of the training set with data from other modalities only leads to a small performance loss compared to the unimodally trained models, it is unclear whether simply training on a smaller amount of unimodal data (i.e., decreasing the unimodal training dataset size to, e.g., <math alttext="82.5" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px2.p1.1.m1.1"><semantics id="S5.SS0.SSS0.Px2.p1.1.m1.1a"><mn id="S5.SS0.SSS0.Px2.p1.1.m1.1.1" xref="S5.SS0.SSS0.Px2.p1.1.m1.1.1.cmml">82.5</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.1.m1.1b"><cn id="S5.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" type="float" xref="S5.SS0.SSS0.Px2.p1.1.m1.1.1">82.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.1.m1.1c">82.5</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px2.p1.1.m1.1d">82.5</annotation></semantics></math>GB and not substituting <math alttext="82.5" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px2.p1.2.m2.1"><semantics id="S5.SS0.SSS0.Px2.p1.2.m2.1a"><mn id="S5.SS0.SSS0.Px2.p1.2.m2.1.1" xref="S5.SS0.SSS0.Px2.p1.2.m2.1.1.cmml">82.5</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.2.m2.1b"><cn id="S5.SS0.SSS0.Px2.p1.2.m2.1.1.cmml" type="float" xref="S5.SS0.SSS0.Px2.p1.2.m2.1.1">82.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.2.m2.1c">82.5</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px2.p1.2.m2.1d">82.5</annotation></semantics></math>GB with data from another modality) would give the same performance, or whether there is some transfer between modalities (as suggested by <cite class="ltx_cite ltx_citemacro_citet">Mirchandani etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib48" title="">2023</a>)</cite>) that compensates for the smaller amount of data per individual modality.
To investigate this, we run an ablation where we subdivide each of our seven training sets into <math alttext="5" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px2.p1.3.m3.1"><semantics id="S5.SS0.SSS0.Px2.p1.3.m3.1a"><mn id="S5.SS0.SSS0.Px2.p1.3.m3.1.1" xref="S5.SS0.SSS0.Px2.p1.3.m3.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.3.m3.1b"><cn id="S5.SS0.SSS0.Px2.p1.3.m3.1.1.cmml" type="integer" xref="S5.SS0.SSS0.Px2.p1.3.m3.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.3.m3.1c">5</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px2.p1.3.m3.1d">5</annotation></semantics></math> different sizes: <math alttext="20\%" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px2.p1.4.m4.1"><semantics id="S5.SS0.SSS0.Px2.p1.4.m4.1a"><mrow id="S5.SS0.SSS0.Px2.p1.4.m4.1.1" xref="S5.SS0.SSS0.Px2.p1.4.m4.1.1.cmml"><mn id="S5.SS0.SSS0.Px2.p1.4.m4.1.1.2" xref="S5.SS0.SSS0.Px2.p1.4.m4.1.1.2.cmml">20</mn><mo id="S5.SS0.SSS0.Px2.p1.4.m4.1.1.1" xref="S5.SS0.SSS0.Px2.p1.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.4.m4.1b"><apply id="S5.SS0.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.4.m4.1.1"><csymbol cd="latexml" id="S5.SS0.SSS0.Px2.p1.4.m4.1.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.4.m4.1.1.1">percent</csymbol><cn id="S5.SS0.SSS0.Px2.p1.4.m4.1.1.2.cmml" type="integer" xref="S5.SS0.SSS0.Px2.p1.4.m4.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.4.m4.1c">20\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px2.p1.4.m4.1d">20 %</annotation></semantics></math>, <math alttext="40\%" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px2.p1.5.m5.1"><semantics id="S5.SS0.SSS0.Px2.p1.5.m5.1a"><mrow id="S5.SS0.SSS0.Px2.p1.5.m5.1.1" xref="S5.SS0.SSS0.Px2.p1.5.m5.1.1.cmml"><mn id="S5.SS0.SSS0.Px2.p1.5.m5.1.1.2" xref="S5.SS0.SSS0.Px2.p1.5.m5.1.1.2.cmml">40</mn><mo id="S5.SS0.SSS0.Px2.p1.5.m5.1.1.1" xref="S5.SS0.SSS0.Px2.p1.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.5.m5.1b"><apply id="S5.SS0.SSS0.Px2.p1.5.m5.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.5.m5.1.1"><csymbol cd="latexml" id="S5.SS0.SSS0.Px2.p1.5.m5.1.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.5.m5.1.1.1">percent</csymbol><cn id="S5.SS0.SSS0.Px2.p1.5.m5.1.1.2.cmml" type="integer" xref="S5.SS0.SSS0.Px2.p1.5.m5.1.1.2">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.5.m5.1c">40\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px2.p1.5.m5.1d">40 %</annotation></semantics></math>, <math alttext="60\%" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px2.p1.6.m6.1"><semantics id="S5.SS0.SSS0.Px2.p1.6.m6.1a"><mrow id="S5.SS0.SSS0.Px2.p1.6.m6.1.1" xref="S5.SS0.SSS0.Px2.p1.6.m6.1.1.cmml"><mn id="S5.SS0.SSS0.Px2.p1.6.m6.1.1.2" xref="S5.SS0.SSS0.Px2.p1.6.m6.1.1.2.cmml">60</mn><mo id="S5.SS0.SSS0.Px2.p1.6.m6.1.1.1" xref="S5.SS0.SSS0.Px2.p1.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.6.m6.1b"><apply id="S5.SS0.SSS0.Px2.p1.6.m6.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.6.m6.1.1"><csymbol cd="latexml" id="S5.SS0.SSS0.Px2.p1.6.m6.1.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.6.m6.1.1.1">percent</csymbol><cn id="S5.SS0.SSS0.Px2.p1.6.m6.1.1.2.cmml" type="integer" xref="S5.SS0.SSS0.Px2.p1.6.m6.1.1.2">60</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.6.m6.1c">60\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px2.p1.6.m6.1d">60 %</annotation></semantics></math>, <math alttext="80\%" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px2.p1.7.m7.1"><semantics id="S5.SS0.SSS0.Px2.p1.7.m7.1a"><mrow id="S5.SS0.SSS0.Px2.p1.7.m7.1.1" xref="S5.SS0.SSS0.Px2.p1.7.m7.1.1.cmml"><mn id="S5.SS0.SSS0.Px2.p1.7.m7.1.1.2" xref="S5.SS0.SSS0.Px2.p1.7.m7.1.1.2.cmml">80</mn><mo id="S5.SS0.SSS0.Px2.p1.7.m7.1.1.1" xref="S5.SS0.SSS0.Px2.p1.7.m7.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.7.m7.1b"><apply id="S5.SS0.SSS0.Px2.p1.7.m7.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.7.m7.1.1"><csymbol cd="latexml" id="S5.SS0.SSS0.Px2.p1.7.m7.1.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.7.m7.1.1.1">percent</csymbol><cn id="S5.SS0.SSS0.Px2.p1.7.m7.1.1.2.cmml" type="integer" xref="S5.SS0.SSS0.Px2.p1.7.m7.1.1.2">80</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.7.m7.1c">80\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px2.p1.7.m7.1d">80 %</annotation></semantics></math>, and <math alttext="100\%" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px2.p1.8.m8.1"><semantics id="S5.SS0.SSS0.Px2.p1.8.m8.1a"><mrow id="S5.SS0.SSS0.Px2.p1.8.m8.1.1" xref="S5.SS0.SSS0.Px2.p1.8.m8.1.1.cmml"><mn id="S5.SS0.SSS0.Px2.p1.8.m8.1.1.2" xref="S5.SS0.SSS0.Px2.p1.8.m8.1.1.2.cmml">100</mn><mo id="S5.SS0.SSS0.Px2.p1.8.m8.1.1.1" xref="S5.SS0.SSS0.Px2.p1.8.m8.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.8.m8.1b"><apply id="S5.SS0.SSS0.Px2.p1.8.m8.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.8.m8.1.1"><csymbol cd="latexml" id="S5.SS0.SSS0.Px2.p1.8.m8.1.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.8.m8.1.1.1">percent</csymbol><cn id="S5.SS0.SSS0.Px2.p1.8.m8.1.1.2.cmml" type="integer" xref="S5.SS0.SSS0.Px2.p1.8.m8.1.1.2">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.8.m8.1c">100\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px2.p1.8.m8.1d">100 %</annotation></semantics></math> of the respective dataset (uniformly subsampled).
We train a series of models (sweeping over their number of layers; see <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A1.SS3" title="A.3 Sweeps â€£ Appendix A Experimental Details â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">A.3</span></a>) on each dataset mixture and each dataset size, and then evaluate as before.
<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.F3" title="In Small Transformers Can Be Domain-General Compressors â€£ 5 Results â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">3</span></a> shows that, for our models and datasets, there is little transfer between modalities.
For all cases of audio, text, and (less clearly) images, it is better to train on a smaller unimodal dataset to get the best unimodal performance, as opposed to training on a much larger multimodal dataset.
For example, training on a pure text dataset of <math alttext="33" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px2.p1.9.m9.1"><semantics id="S5.SS0.SSS0.Px2.p1.9.m9.1a"><mn id="S5.SS0.SSS0.Px2.p1.9.m9.1.1" xref="S5.SS0.SSS0.Px2.p1.9.m9.1.1.cmml">33</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.9.m9.1b"><cn id="S5.SS0.SSS0.Px2.p1.9.m9.1.1.cmml" type="integer" xref="S5.SS0.SSS0.Px2.p1.9.m9.1.1">33</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.9.m9.1c">33</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px2.p1.9.m9.1d">33</annotation></semantics></math>GB (<math alttext="20\%" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px2.p1.10.m10.1"><semantics id="S5.SS0.SSS0.Px2.p1.10.m10.1a"><mrow id="S5.SS0.SSS0.Px2.p1.10.m10.1.1" xref="S5.SS0.SSS0.Px2.p1.10.m10.1.1.cmml"><mn id="S5.SS0.SSS0.Px2.p1.10.m10.1.1.2" xref="S5.SS0.SSS0.Px2.p1.10.m10.1.1.2.cmml">20</mn><mo id="S5.SS0.SSS0.Px2.p1.10.m10.1.1.1" xref="S5.SS0.SSS0.Px2.p1.10.m10.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.10.m10.1b"><apply id="S5.SS0.SSS0.Px2.p1.10.m10.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.10.m10.1.1"><csymbol cd="latexml" id="S5.SS0.SSS0.Px2.p1.10.m10.1.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.10.m10.1.1.1">percent</csymbol><cn id="S5.SS0.SSS0.Px2.p1.10.m10.1.1.2.cmml" type="integer" xref="S5.SS0.SSS0.Px2.p1.10.m10.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.10.m10.1c">20\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px2.p1.10.m10.1d">20 %</annotation></semantics></math> of <math alttext="165" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px2.p1.11.m11.1"><semantics id="S5.SS0.SSS0.Px2.p1.11.m11.1a"><mn id="S5.SS0.SSS0.Px2.p1.11.m11.1.1" xref="S5.SS0.SSS0.Px2.p1.11.m11.1.1.cmml">165</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.11.m11.1b"><cn id="S5.SS0.SSS0.Px2.p1.11.m11.1.1.cmml" type="integer" xref="S5.SS0.SSS0.Px2.p1.11.m11.1.1">165</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.11.m11.1c">165</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px2.p1.11.m11.1d">165</annotation></semantics></math>GB) outperforms training on a dataset consisting of <math alttext="82.5" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px2.p1.12.m12.1"><semantics id="S5.SS0.SSS0.Px2.p1.12.m12.1a"><mn id="S5.SS0.SSS0.Px2.p1.12.m12.1.1" xref="S5.SS0.SSS0.Px2.p1.12.m12.1.1.cmml">82.5</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.12.m12.1b"><cn id="S5.SS0.SSS0.Px2.p1.12.m12.1.1.cmml" type="float" xref="S5.SS0.SSS0.Px2.p1.12.m12.1.1">82.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.12.m12.1c">82.5</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px2.p1.12.m12.1d">82.5</annotation></semantics></math>GB (i.e., more than twice as much) text and of <math alttext="82.5" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px2.p1.13.m13.1"><semantics id="S5.SS0.SSS0.Px2.p1.13.m13.1a"><mn id="S5.SS0.SSS0.Px2.p1.13.m13.1.1" xref="S5.SS0.SSS0.Px2.p1.13.m13.1.1.cmml">82.5</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.13.m13.1b"><cn id="S5.SS0.SSS0.Px2.p1.13.m13.1.1.cmml" type="float" xref="S5.SS0.SSS0.Px2.p1.13.m13.1.1">82.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.13.m13.1c">82.5</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px2.p1.13.m13.1d">82.5</annotation></semantics></math>GB images/audio.</p>
</div>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="259" id="S5.F4.1.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>
Simultaneously scaling training dataset and model size (for unimodal training- and evaluation data).
The colors indicate the model size, and the lines correspond to different dataset sizes (<math alttext="20\%" class="ltx_Math" display="inline" id="S5.F4.8.m1.1"><semantics id="S5.F4.8.m1.1b"><mrow id="S5.F4.8.m1.1.1" xref="S5.F4.8.m1.1.1.cmml"><mn id="S5.F4.8.m1.1.1.2" xref="S5.F4.8.m1.1.1.2.cmml">20</mn><mo id="S5.F4.8.m1.1.1.1" xref="S5.F4.8.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.F4.8.m1.1c"><apply id="S5.F4.8.m1.1.1.cmml" xref="S5.F4.8.m1.1.1"><csymbol cd="latexml" id="S5.F4.8.m1.1.1.1.cmml" xref="S5.F4.8.m1.1.1.1">percent</csymbol><cn id="S5.F4.8.m1.1.1.2.cmml" type="integer" xref="S5.F4.8.m1.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.8.m1.1d">20\%</annotation><annotation encoding="application/x-llamapun" id="S5.F4.8.m1.1e">20 %</annotation></semantics></math>, <math alttext="40\%" class="ltx_Math" display="inline" id="S5.F4.9.m2.1"><semantics id="S5.F4.9.m2.1b"><mrow id="S5.F4.9.m2.1.1" xref="S5.F4.9.m2.1.1.cmml"><mn id="S5.F4.9.m2.1.1.2" xref="S5.F4.9.m2.1.1.2.cmml">40</mn><mo id="S5.F4.9.m2.1.1.1" xref="S5.F4.9.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.F4.9.m2.1c"><apply id="S5.F4.9.m2.1.1.cmml" xref="S5.F4.9.m2.1.1"><csymbol cd="latexml" id="S5.F4.9.m2.1.1.1.cmml" xref="S5.F4.9.m2.1.1.1">percent</csymbol><cn id="S5.F4.9.m2.1.1.2.cmml" type="integer" xref="S5.F4.9.m2.1.1.2">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.9.m2.1d">40\%</annotation><annotation encoding="application/x-llamapun" id="S5.F4.9.m2.1e">40 %</annotation></semantics></math>, <math alttext="60\%" class="ltx_Math" display="inline" id="S5.F4.10.m3.1"><semantics id="S5.F4.10.m3.1b"><mrow id="S5.F4.10.m3.1.1" xref="S5.F4.10.m3.1.1.cmml"><mn id="S5.F4.10.m3.1.1.2" xref="S5.F4.10.m3.1.1.2.cmml">60</mn><mo id="S5.F4.10.m3.1.1.1" xref="S5.F4.10.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.F4.10.m3.1c"><apply id="S5.F4.10.m3.1.1.cmml" xref="S5.F4.10.m3.1.1"><csymbol cd="latexml" id="S5.F4.10.m3.1.1.1.cmml" xref="S5.F4.10.m3.1.1.1">percent</csymbol><cn id="S5.F4.10.m3.1.1.2.cmml" type="integer" xref="S5.F4.10.m3.1.1.2">60</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.10.m3.1d">60\%</annotation><annotation encoding="application/x-llamapun" id="S5.F4.10.m3.1e">60 %</annotation></semantics></math>, <math alttext="80\%" class="ltx_Math" display="inline" id="S5.F4.11.m4.1"><semantics id="S5.F4.11.m4.1b"><mrow id="S5.F4.11.m4.1.1" xref="S5.F4.11.m4.1.1.cmml"><mn id="S5.F4.11.m4.1.1.2" xref="S5.F4.11.m4.1.1.2.cmml">80</mn><mo id="S5.F4.11.m4.1.1.1" xref="S5.F4.11.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.F4.11.m4.1c"><apply id="S5.F4.11.m4.1.1.cmml" xref="S5.F4.11.m4.1.1"><csymbol cd="latexml" id="S5.F4.11.m4.1.1.1.cmml" xref="S5.F4.11.m4.1.1.1">percent</csymbol><cn id="S5.F4.11.m4.1.1.2.cmml" type="integer" xref="S5.F4.11.m4.1.1.2">80</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.11.m4.1d">80\%</annotation><annotation encoding="application/x-llamapun" id="S5.F4.11.m4.1e">80 %</annotation></semantics></math>, and <math alttext="100\%" class="ltx_Math" display="inline" id="S5.F4.12.m5.1"><semantics id="S5.F4.12.m5.1b"><mrow id="S5.F4.12.m5.1.1" xref="S5.F4.12.m5.1.1.cmml"><mn id="S5.F4.12.m5.1.1.2" xref="S5.F4.12.m5.1.1.2.cmml">100</mn><mo id="S5.F4.12.m5.1.1.1" xref="S5.F4.12.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.F4.12.m5.1c"><apply id="S5.F4.12.m5.1.1.cmml" xref="S5.F4.12.m5.1.1"><csymbol cd="latexml" id="S5.F4.12.m5.1.1.1.cmml" xref="S5.F4.12.m5.1.1.1">percent</csymbol><cn id="S5.F4.12.m5.1.1.2.cmml" type="integer" xref="S5.F4.12.m5.1.1.2">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.12.m5.1d">100\%</annotation><annotation encoding="application/x-llamapun" id="S5.F4.12.m5.1e">100 %</annotation></semantics></math>).
We always train for <math alttext="2" class="ltx_Math" display="inline" id="S5.F4.13.m6.1"><semantics id="S5.F4.13.m6.1b"><mn id="S5.F4.13.m6.1.1" xref="S5.F4.13.m6.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S5.F4.13.m6.1c"><cn id="S5.F4.13.m6.1.1.cmml" type="integer" xref="S5.F4.13.m6.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.13.m6.1d">2</annotation><annotation encoding="application/x-llamapun" id="S5.F4.13.m6.1e">2</annotation></semantics></math> epochs, regardless of dataset size, i.e., smaller datasets require fewer FLOPS.
As expected, increasing the number of parameters and the dataset size boosts compression (at the cost of increased training FLOPS).
Note that our out-of-distribution evaluation makes models more prone to overfitting, as seen, e.g., for our largest models on images, making scaling more complex than traditionally observed LLM scaling laws.
</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Scaling Analysis</h4>
<div class="ltx_para" id="S5.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px3.p1.1">Since there is a non-trivial relationship between model size and dataset size, we perform a scaling analysis on both of these factors (details in <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A1.SS3" title="A.3 Sweeps â€£ Appendix A Experimental Details â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">A.3</span></a>).
<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.F4" title="In What You See Is What You Get â€£ 5 Results â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">4</span></a> shows trends akin to the scaling laws observed for LLMsÂ <cite class="ltx_cite ltx_citemacro_citep">(Kaplan etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib28" title="">2020</a>)</cite>, which state that better prediction (in our case compression) is only possible by scaling both models and datasets, in a particular way.
Note that, different to traditional scaling laws for models trained on internet-scale datasets, the distribution shift in our evaluation makes it easier for the model to overfit to the training distribution.
However, as the number of parameters and the training flops of our small models increase, the adjusted compression ratio improves, eventually beating standard compression algorithms.
We do observe gradual overfitting on the image dataset for our models trained only on images.
However, this phenomenon can be mitigated by including other modalities in the training mixture (see <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A2.F1" title="In B.4 Scaling Analysis for Multimodal Training â€£ Appendix B Additional Results â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">A1</span></a>).</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Model Size vs.Â Context Size</h4>
<figure class="ltx_figure" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="257" id="S5.F5.1.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>
Relationship between context- and model size.
Given a certain training compute budget (in FLOPS), one can either increase the context size (measured in bytes) or the model size, leading to a non-trivial trade-off.
Our results show that this trade-off is highly modality-dependent (also note the different scales on the y-axis, meaning that the magnitude of the effect varies significantly with modality).
For text, shorter context sizes and larger models are beneficial (indicating the importance of short-term dependencies for our data and model scale).
For images, larger context is generally beneficial, which makes sense, given that a single image consists of <math alttext="512\cdot 512\cdot 3=786432" class="ltx_Math" display="inline" id="S5.F5.3.m1.1"><semantics id="S5.F5.3.m1.1b"><mrow id="S5.F5.3.m1.1.1" xref="S5.F5.3.m1.1.1.cmml"><mrow id="S5.F5.3.m1.1.1.2" xref="S5.F5.3.m1.1.1.2.cmml"><mn id="S5.F5.3.m1.1.1.2.2" xref="S5.F5.3.m1.1.1.2.2.cmml">512</mn><mo id="S5.F5.3.m1.1.1.2.1" lspace="0.222em" rspace="0.222em" xref="S5.F5.3.m1.1.1.2.1.cmml">â‹…</mo><mn id="S5.F5.3.m1.1.1.2.3" xref="S5.F5.3.m1.1.1.2.3.cmml">512</mn><mo id="S5.F5.3.m1.1.1.2.1b" lspace="0.222em" rspace="0.222em" xref="S5.F5.3.m1.1.1.2.1.cmml">â‹…</mo><mn id="S5.F5.3.m1.1.1.2.4" xref="S5.F5.3.m1.1.1.2.4.cmml">3</mn></mrow><mo id="S5.F5.3.m1.1.1.1" xref="S5.F5.3.m1.1.1.1.cmml">=</mo><mn id="S5.F5.3.m1.1.1.3" xref="S5.F5.3.m1.1.1.3.cmml">786432</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.F5.3.m1.1c"><apply id="S5.F5.3.m1.1.1.cmml" xref="S5.F5.3.m1.1.1"><eq id="S5.F5.3.m1.1.1.1.cmml" xref="S5.F5.3.m1.1.1.1"></eq><apply id="S5.F5.3.m1.1.1.2.cmml" xref="S5.F5.3.m1.1.1.2"><ci id="S5.F5.3.m1.1.1.2.1.cmml" xref="S5.F5.3.m1.1.1.2.1">â‹…</ci><cn id="S5.F5.3.m1.1.1.2.2.cmml" type="integer" xref="S5.F5.3.m1.1.1.2.2">512</cn><cn id="S5.F5.3.m1.1.1.2.3.cmml" type="integer" xref="S5.F5.3.m1.1.1.2.3">512</cn><cn id="S5.F5.3.m1.1.1.2.4.cmml" type="integer" xref="S5.F5.3.m1.1.1.2.4">3</cn></apply><cn id="S5.F5.3.m1.1.1.3.cmml" type="integer" xref="S5.F5.3.m1.1.1.3">786432</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F5.3.m1.1d">512\cdot 512\cdot 3=786432</annotation><annotation encoding="application/x-llamapun" id="S5.F5.3.m1.1e">512 â‹… 512 â‹… 3 = 786432</annotation></semantics></math> bytes, which far exceeds our modelsâ€™ contexts, i.e., models with larger context can process larger fractions of an image.
Finally, for audio data the relationship is complex with intermediate context length and larger models performing better (though the reverse is true for short context length).
</figcaption>
</figure>
<div class="ltx_para" id="S5.SS0.SSS0.Px4.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px4.p1.2">The previous two experiments investigated the impact of training dataset size and model size, which revealed a complex, â€œscaling lawâ€-like, relationship between the two factors and the overall training budget in FLOPS.
In this experiment, we investigate the impact of the length of the context window.
Since the context window length has a large impact on the overall FLOPS footprint (attention scales quadratically with the input sequence length), we also vary the size of our models to explore whether there is a sweet spot in terms of training compute budget allocation (details in <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A1.SS3" title="A.3 Sweeps â€£ Appendix A Experimental Details â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">A.3</span></a>).
<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.F5" title="In Model Size vs. Context Size â€£ 5 Results â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">5</span></a> shows that the optimal trade-off strongly depends on the data modality.
The top performing models for text have a context window less than or equal to <math alttext="2048" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px4.p1.1.m1.1"><semantics id="S5.SS0.SSS0.Px4.p1.1.m1.1a"><mn id="S5.SS0.SSS0.Px4.p1.1.m1.1.1" xref="S5.SS0.SSS0.Px4.p1.1.m1.1.1.cmml">2048</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px4.p1.1.m1.1b"><cn id="S5.SS0.SSS0.Px4.p1.1.m1.1.1.cmml" type="integer" xref="S5.SS0.SSS0.Px4.p1.1.m1.1.1">2048</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px4.p1.1.m1.1c">2048</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px4.p1.1.m1.1d">2048</annotation></semantics></math> bytes, indicating that short term dependencies are more important than long ones in this case.
For images, the best compromise overall is to choose a larger context window of <math alttext="8192" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px4.p1.2.m2.1"><semantics id="S5.SS0.SSS0.Px4.p1.2.m2.1a"><mn id="S5.SS0.SSS0.Px4.p1.2.m2.1.1" xref="S5.SS0.SSS0.Px4.p1.2.m2.1.1.cmml">8192</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px4.p1.2.m2.1b"><cn id="S5.SS0.SSS0.Px4.p1.2.m2.1.1.cmml" type="integer" xref="S5.SS0.SSS0.Px4.p1.2.m2.1.1">8192</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px4.p1.2.m2.1c">8192</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px4.p1.2.m2.1d">8192</annotation></semantics></math>, which means decreasing the model size.
For audio data, the trade-off is even more complex.
Overall these results highlight the difficulty of tuning architectures to achieve best performance across many modalities.</p>
</div>
<figure class="ltx_figure ltx_align_floatright" id="S5.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="329" id="S5.F6.1.g1" src="x6.png" width="332"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>
Impact of the sliding window overlap (for unimodal training and evaluation).
Having overlapping context windows only leads to marginal performance increases (most significant for images) in our experiments but comes at a huge cost in terms of computational efficiency.
</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px5">
<h4 class="ltx_title ltx_title_paragraph">Sliding Window</h4>
<div class="ltx_para" id="S5.SS0.SSS0.Px5.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px5.p1.5">In all experiments so far we used a sliding window without overlap to process the evaluation byte streams, i.e., we completely fill a whole context window, process it, and then slide it forward by the size of the context window to process the next chunk of data.
This means that bytes early in the context window are not conditioned on a lot of data (in theory, conditioning on more data should help with prediction and thus compression, which may well be exploitable by transformersâ€™ in-context learning abilitiesÂ <cite class="ltx_cite ltx_citemacro_citep">(Genewein etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib18" title="">2023</a>; Ge etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib17" title="">2024</a>)</cite>).
However, sliding the context window with more overlap requires more forward passes to process the same amount of data, which significantly increases the computational cost with increasing overlap.
With no overlap processing <math alttext="4096" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px5.p1.1.m1.1"><semantics id="S5.SS0.SSS0.Px5.p1.1.m1.1a"><mn id="S5.SS0.SSS0.Px5.p1.1.m1.1.1" xref="S5.SS0.SSS0.Px5.p1.1.m1.1.1.cmml">4096</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px5.p1.1.m1.1b"><cn id="S5.SS0.SSS0.Px5.p1.1.m1.1.1.cmml" type="integer" xref="S5.SS0.SSS0.Px5.p1.1.m1.1.1">4096</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px5.p1.1.m1.1c">4096</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px5.p1.1.m1.1d">4096</annotation></semantics></math> bytes with a context window of <math alttext="4096" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px5.p1.2.m2.1"><semantics id="S5.SS0.SSS0.Px5.p1.2.m2.1a"><mn id="S5.SS0.SSS0.Px5.p1.2.m2.1.1" xref="S5.SS0.SSS0.Px5.p1.2.m2.1.1.cmml">4096</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px5.p1.2.m2.1b"><cn id="S5.SS0.SSS0.Px5.p1.2.m2.1.1.cmml" type="integer" xref="S5.SS0.SSS0.Px5.p1.2.m2.1.1">4096</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px5.p1.2.m2.1c">4096</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px5.p1.2.m2.1d">4096</annotation></semantics></math> takes a single forward pass.
In the most extreme case of maximal overlap it would take <math alttext="4095" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px5.p1.3.m3.1"><semantics id="S5.SS0.SSS0.Px5.p1.3.m3.1a"><mn id="S5.SS0.SSS0.Px5.p1.3.m3.1.1" xref="S5.SS0.SSS0.Px5.p1.3.m3.1.1.cmml">4095</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px5.p1.3.m3.1b"><cn id="S5.SS0.SSS0.Px5.p1.3.m3.1.1.cmml" type="integer" xref="S5.SS0.SSS0.Px5.p1.3.m3.1.1">4095</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px5.p1.3.m3.1c">4095</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px5.p1.3.m3.1d">4095</annotation></semantics></math> forward passes, where the context window is moved by a single byte each time (though each prediction could be conditioned on the full <math alttext="4095" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px5.p1.4.m4.1"><semantics id="S5.SS0.SSS0.Px5.p1.4.m4.1a"><mn id="S5.SS0.SSS0.Px5.p1.4.m4.1.1" xref="S5.SS0.SSS0.Px5.p1.4.m4.1.1.cmml">4095</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px5.p1.4.m4.1b"><cn id="S5.SS0.SSS0.Px5.p1.4.m4.1.1.cmml" type="integer" xref="S5.SS0.SSS0.Px5.p1.4.m4.1.1">4095</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px5.p1.4.m4.1c">4095</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px5.p1.4.m4.1d">4095</annotation></semantics></math> preceding bytes).
In our final experiment, we investigate the effect of different overlaps between context windows.
<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.F6" title="In Model Size vs. Context Size â€£ 5 Results â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">6</span></a> shows that for our data and model sizes, increasing the overlap window (for a context length of 4096) has relatively little effect.
The strongest effect is observed for image data, which makes sense given that 4096 bytes only corresponds to a small fraction of an image and there are obvious long-range dependencies between channels of the same image.
Beyond an overlap of <math alttext="2048" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px5.p1.5.m5.1"><semantics id="S5.SS0.SSS0.Px5.p1.5.m5.1a"><mn id="S5.SS0.SSS0.Px5.p1.5.m5.1.1" xref="S5.SS0.SSS0.Px5.p1.5.m5.1.1.cmml">2048</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px5.p1.5.m5.1b"><cn id="S5.SS0.SSS0.Px5.p1.5.m5.1.1.cmml" type="integer" xref="S5.SS0.SSS0.Px5.p1.5.m5.1.1">2048</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px5.p1.5.m5.1c">2048</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px5.p1.5.m5.1d">2048</annotation></semantics></math> we do not see much benefit of further increasing the overlap window in our experiments.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Discussion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.2">The main goal of our work is to investigate whether pre-trained transformers can be competitive with standard compressors, even when taking their parameter size into account.
In contrast to previous work, this places our models into a relatively small regime, where it is unclear whether models will learn well from large datasets at all and have non-trivial out-of-distribution and cross-modality transfer.
This could partly be countered by training larger models and then subsequently compressing the model parameters themselves.
We chose not to do this in our case since naive lossless compression of model parameters leads to a <math alttext="10\%" class="ltx_Math" display="inline" id="S6.p1.1.m1.1"><semantics id="S6.p1.1.m1.1a"><mrow id="S6.p1.1.m1.1.1" xref="S6.p1.1.m1.1.1.cmml"><mn id="S6.p1.1.m1.1.1.2" xref="S6.p1.1.m1.1.1.2.cmml">10</mn><mo id="S6.p1.1.m1.1.1.1" xref="S6.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.p1.1.m1.1b"><apply id="S6.p1.1.m1.1.1.cmml" xref="S6.p1.1.m1.1.1"><csymbol cd="latexml" id="S6.p1.1.m1.1.1.1.cmml" xref="S6.p1.1.m1.1.1.1">percent</csymbol><cn id="S6.p1.1.m1.1.1.2.cmml" type="integer" xref="S6.p1.1.m1.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.1.m1.1c">10\%</annotation><annotation encoding="application/x-llamapun" id="S6.p1.1.m1.1d">10 %</annotation></semantics></math> reduction at best (see <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A2.T3" title="In B.3 Compressing Model Parameters â€£ Appendix B Additional Results â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">A3</span></a>), and even best-case scenarios would only lead to marginal improvements in compression ratio given the size of our largest models.
For very large (e.g., foundation) models, compressing weights to achieve competitive compression ratios may be interesting, though it will be necessary to use lossy weight compression techniquesÂ <cite class="ltx_cite ltx_citemacro_citep">(Tao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib70" title="">2022</a>)</cite>, which lead to non-trivial trade-offs between high (lossy) compression and maintaining strong predictor performance, i.e., the two summands in the numerator of <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S4.E2" title="In Evaluation â€£ 4 Methods â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Eq.</span>Â <span class="ltx_text ltx_ref_tag">2</span></a>.
Exploring these trade-offs is an interesting direction for future research but beyond the scope of our work.
Another way to allow for larger models would be to simply evaluate on a larger test set.
We deliberately chose to use <math alttext="1" class="ltx_Math" display="inline" id="S6.p1.2.m2.1"><semantics id="S6.p1.2.m2.1a"><mn id="S6.p1.2.m2.1.1" xref="S6.p1.2.m2.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S6.p1.2.m2.1b"><cn id="S6.p1.2.m2.1.1.cmml" type="integer" xref="S6.p1.2.m2.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.2.m2.1c">1</annotation><annotation encoding="application/x-llamapun" id="S6.p1.2.m2.1d">1</annotation></semantics></math>GB of test data as a regime where standard compression algorithms are hard to beat.
Additionally, evaluations on larger test data, and in settings where model parameters are not taken into account have previously conductedÂ <cite class="ltx_cite ltx_citemacro_citep">(DelÃ©tang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib11" title="">2024</a>; Valmeekam etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib74" title="">2023</a>; Li etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib38" title="">2024</a>)</cite> (where significant amounts of cross-domain transfer have also been found, unlike in our experiments).</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">Note that, similar to <cite class="ltx_cite ltx_citemacro_citet">Xue etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib82" title="">2022</a>)</cite>, we do not use a tokenizer, which has two reasons.
First, tokenizers are typically pre-trained per modality, and we want to rule out bad cross-modality transfer resulting from a bad tokenizer.
Second, tokenization acts as a pre-trained â€œpre-compressionâ€ step (<cite class="ltx_cite ltx_citemacro_citet">DelÃ©tang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib11" title="">2024</a>)</cite> make a similar comment).
This pre-compression increases information density in the context window at the cost of increasing entropy, which can make the prediction problem harder: <cite class="ltx_cite ltx_citemacro_citet">Lester etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib36" title="">2024</a>)</cite> even show that when using a strong neural-based pre-compressor (together with arithmetic coding) to train LLMs, training performance can collapse catastrophically.</p>
</div>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Limitations</h4>
<div class="ltx_para" id="S6.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S6.SS0.SSS0.Px1.p1.1">All our claims regarding the universality of our compressors (or the lack thereof) are limited to the model size regime and the particular modalities and datasets we studied.
We cannot rule out that there are cases where even in-modality transfer is weak (e.g., when using another out-of-disribution image evaluation dataset with very different statistics), or that there may be cases of non-trivial cross-modal transfer (which we have not observed).
Similarly, our claims regarding outperforming standard compression algorithms are limited to our experiments. We cannot rule out that there are datasets (such as spreadsheet data, or code, which, technically, are both text) where no pre-trained transformer outperforms, e.g., LZMA2 (in fact, we think its plausible that such datasets can be constructed synthetically).
Finally, note that the goal of our study is not to build a practical transformer-based universal compressor to compete with standard compressors in terms of computational footprint.
As <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A2.T2" title="In B.2 Running Times â€£ Appendix B Additional Results â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">A2</span></a> shows, our models are orders of magnitude slower for encoding data (and have significantly larger memory- and FLOPS-demands), and they are about three times slower than Bellardâ€™s online adaptive transformer.
This is only the forward-pass cost, which can be done for a whole context window at once (without overlap).
If our models were used do decode, which has to be performed token-by-token to obtain the correct conditioning, our running time demands would be even worse, making our models clearly uncompetitive in that sense.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this paper we have shown that it is possible to use pre-trained vanilla transformers as competitive â€œzero-shotâ€ compressors on out-of-distribution evaluation data, where competitive means achieving better compression ratios than both domain-general and domain-specific standard compression algorithms.
We found this to be true for text, images, and audio data, and for all possible combinations of the three â€” but only as long as the corresponding modalities have been seen during training.
We further found that, despite their relatively small size, our models have the capacity to train on multiple modalities, and then compress these well, without losing much performance compared to a purely unimodal model.
On the other hand, we found that even multimodal training does not lead to the emergence of a universal compression ability that would yield strong compression performance on unseen modalities.
This is in contrast to observations made by <cite class="ltx_cite ltx_citemacro_citet">DelÃ©tang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib11" title="">2024</a>)</cite> on LLMs and indicates that there is a qualitative difference between small and (very) large models, even when the small models are trained on large amounts of data.
Overall our results suggest that small transformers can be pre-trained to recognize and exploit statistical regularities on par and even better than hand-crafted standard compressors and current state-of-the-art adaptive online neural compressors, but we do not observe the emergence of a general compression ability with our model sizes.</p>
</div>
</section>
<section class="ltx_section" id="Sx1" lang="en">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">We thank
Benjamin Beyret,
Emilien Dupont,
Daniele Calandriello,
GrÃ©goire DelÃ©tang,
Jordi Grau-Moya,
Li Kevin Wenliang,
Laurent Orseau,
Marcus Hutter,
Matthew Aitchison,
Sarath Chandar,
Satinder Baveja,
Theophane Weber,
Zhengdong Wang,
and Zoubin Ghahramani
for their helpful feedback and insightful discussions.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib" lang="en">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ardila etÂ al. (2020)</span>
<span class="ltx_bibblock">
R.Â Ardila, M.Â Branson, K.Â Davis, M.Â Kohler, J.Â Meyer, M.Â Henretty, R.Â Morais,
L.Â Saunders, F.Â M. Tyers, and G.Â Weber.

</span>
<span class="ltx_bibblock">Common voice: A massively-multilingual speech corpus.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">LREC</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barzen etÂ al. (2022)</span>
<span class="ltx_bibblock">
B.Â L.Â C. Barzen, F.Â Glazov, J.Â Geistert, and T.Â Sikora.

</span>
<span class="ltx_bibblock">Accelerated deep lossless image coding with unified paralleleized
GPU coding architecture.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">PCS</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bellard (2019)</span>
<span class="ltx_bibblock">
F.Â Bellard.

</span>
<span class="ltx_bibblock">Lossless data compression with neural networks.

</span>
<span class="ltx_bibblock">Technical report, Amarisoft, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bellard (2021)</span>
<span class="ltx_bibblock">
F.Â Bellard.

</span>
<span class="ltx_bibblock">NNCP v2: Lossless data compression with transformer.

</span>
<span class="ltx_bibblock">Technical report, Amarisoft, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Boutell (1997)</span>
<span class="ltx_bibblock">
T.Â Boutell.

</span>
<span class="ltx_bibblock">PNG (portable network graphics) specification version 1.0.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">RFC</em>, 1997.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chaitin (2006)</span>
<span class="ltx_bibblock">
G.Â J. Chaitin.

</span>
<span class="ltx_bibblock">The limits of reason.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Scientific American</em>, 2006.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2021)</span>
<span class="ltx_bibblock">
H.Â Chen, B.Â He, H.Â Wang, Y.Â Ren, S.Â Lim, and A.Â Shrivastava.

</span>
<span class="ltx_bibblock">Nerv: Neural representations for videos.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">NeurIPS</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Coalson (2008)</span>
<span class="ltx_bibblock">
J.Â Coalson.

</span>
<span class="ltx_bibblock">Free lossless audio codec, 2008.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://xiph.org/flac" title="">https://xiph.org/flac</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cohan etÂ al. (2018)</span>
<span class="ltx_bibblock">
A.Â Cohan, F.Â Dernoncourt, D.Â S. Kim, T.Â Bui, S.Â Kim, W.Â Chang, and N.Â Goharian.

</span>
<span class="ltx_bibblock">A discourse-aware attention model for abstractive summarization of
long documents.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">NAACL-HLT (2)</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cox (2016)</span>
<span class="ltx_bibblock">
D.Â Cox.

</span>
<span class="ltx_bibblock">Syntactically informed text compression with recurrent neural
networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv:1608.02893</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DelÃ©tang etÂ al. (2024)</span>
<span class="ltx_bibblock">
G.Â DelÃ©tang, A.Â Ruoss, P.Â Duquenne, E.Â Catt, T.Â Genewein, C.Â Mattern,
J.Â Grau-Moya, L.Â K. Wenliang, M.Â Aitchison, L.Â Orseau, M.Â Hutter, and
J.Â Veness.

</span>
<span class="ltx_bibblock">Language modeling is compression.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">ICLR</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deutsch (1996)</span>
<span class="ltx_bibblock">
P.Â Deutsch.

</span>
<span class="ltx_bibblock">GZIP file format specification version 4.3.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">RFC</em>, 1996.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dubey etÂ al. (2024)</span>
<span class="ltx_bibblock">
A.Â Dubey, A.Â Jauhri, A.Â Pandey, A.Â Kadian, A.Â Al-Dahle, A.Â Letman, A.Â Mathur,
A.Â Schelten, A.Â Yang, A.Â Fan, A.Â Goyal, A.Â Hartshorn, A.Â Yang, A.Â Mitra,
A.Â Sravankumar, A.Â Korenev, A.Â Hinsvark, A.Â Rao, A.Â Zhang, A.Â Rodriguez,
A.Â Gregerson, A.Â Spataru, B.Â RoziÃ¨re, B.Â Biron, B.Â Tang, B.Â Chern,
C.Â Caucheteux, C.Â Nayak, C.Â Bi, C.Â Marra, C.Â McConnell, C.Â Keller, C.Â Touret,
C.Â Wu, C.Â Wong, C.Â C. Ferrer, C.Â Nikolaidis, D.Â Allonsius, D.Â Song, D.Â Pintz,
D.Â Livshits, D.Â Esiobu, D.Â Choudhary, D.Â Mahajan, D.Â Garcia-Olano,
D.Â Perino, D.Â Hupkes, E.Â Lakomkin, E.Â AlBadawy, E.Â Lobanova, E.Â Dinan, E.Â M.
Smith, F.Â Radenovic, F.Â Zhang, G.Â Synnaeve, G.Â Lee, G.Â L. Anderson, G.Â Nail,
G.Â Mialon, G.Â Pang, G.Â Cucurell, H.Â Nguyen, H.Â Korevaar, H.Â Xu, H.Â Touvron,
I.Â Zarov, I.Â A. Ibarra, I.Â M. Kloumann, I.Â Misra, I.Â Evtimov, J.Â Copet,
J.Â Lee, J.Â Geffert, J.Â Vranes, J.Â Park, J.Â Mahadeokar, J.Â Shah, J.Â vanÂ der
Linde, J.Â Billock, J.Â Hong, J.Â Lee, J.Â Fu, J.Â Chi, J.Â Huang, J.Â Liu, J.Â Wang,
J.Â Yu, J.Â Bitton, J.Â Spisak, J.Â Park, J.Â Rocca, J.Â Johnstun, J.Â Saxe, J.Â Jia,
K.Â V. Alwala, K.Â Upasani, K.Â Plawiak, K.Â Li, K.Â Heafield, K.Â Stone, and
etÂ al.

</span>
<span class="ltx_bibblock">The llama 3 herd of models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv:2407.21783</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Duda (2009)</span>
<span class="ltx_bibblock">
J.Â Duda.

</span>
<span class="ltx_bibblock">Asymmetric numeral systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv:0902.0271</em>, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dupont etÂ al. (2021)</span>
<span class="ltx_bibblock">
E.Â Dupont, A.Â Golinski, M.Â Alizadeh, Y.Â W. Teh, and A.Â Doucet.

</span>
<span class="ltx_bibblock">COIN: compression with implicit neural representations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">arXiv:2103.03123</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dupont etÂ al. (2022)</span>
<span class="ltx_bibblock">
E.Â Dupont, H.Â Loya, M.Â Alizadeh, A.Â Golinski, Y.Â W. Teh, and A.Â Doucet.

</span>
<span class="ltx_bibblock">COIN++: neural compression across modalities.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Trans. Mach. Learn. Res.</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ge etÂ al. (2024)</span>
<span class="ltx_bibblock">
T.Â Ge, J.Â Hu, L.Â Wang, X.Â Wang, S.Â Chen, and F.Â Wei.

</span>
<span class="ltx_bibblock">In-context autoencoder for context compression in a large language
model.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">ICLR</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Genewein etÂ al. (2023)</span>
<span class="ltx_bibblock">
T.Â Genewein, G.Â DelÃ©tang, A.Â Ruoss, L.Â K. Wenliang, E.Â Catt,
V.Â Dutordoir, J.Â Grau-Moya, L.Â Orseau, M.Â Hutter, and J.Â Veness.

</span>
<span class="ltx_bibblock">Memory-based meta-learning on non-stationary distributions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">ICML</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal etÂ al. (2019)</span>
<span class="ltx_bibblock">
M.Â Goyal, K.Â Tatwawadi, S.Â Chandak, and I.Â Ochoa.

</span>
<span class="ltx_bibblock">Deepzip: Lossless data compression using recurrent neural networks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">DCC</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal etÂ al. (2020)</span>
<span class="ltx_bibblock">
M.Â Goyal, K.Â Tatwawadi, S.Â Chandak, and I.Â Ochoa.

</span>
<span class="ltx_bibblock">Dzip: Improved general-purpose lossless compression based on novel
neural network modeling.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">DCC</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grau-Moya etÂ al. (2024)</span>
<span class="ltx_bibblock">
J.Â Grau-Moya, T.Â Genewein, M.Â Hutter, L.Â Orseau, G.Â DelÃ©tang, E.Â Catt,
A.Â Ruoss, L.Â K. Wenliang, C.Â Mattern, M.Â Aitchison, and J.Â Veness.

</span>
<span class="ltx_bibblock">Learning universal predictors.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">ICML</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoffmann etÂ al. (2022)</span>
<span class="ltx_bibblock">
J.Â Hoffmann, S.Â Borgeaud, A.Â Mensch, E.Â Buchatskaya, T.Â Cai, E.Â Rutherford,
D.Â deÂ LasÂ Casas, L.Â A. Hendricks, J.Â Welbl, A.Â Clark, T.Â Hennigan, E.Â Noland,
K.Â Millican, G.Â vanÂ den Driessche, B.Â Damoc, A.Â Guy, S.Â Osindero,
K.Â Simonyan, E.Â Elsen, J.Â W. Rae, O.Â Vinyals, and L.Â Sifre.

</span>
<span class="ltx_bibblock">Training compute-optimal large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arxiv:2203.15556</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoogeboom etÂ al. (2019)</span>
<span class="ltx_bibblock">
E.Â Hoogeboom, J.Â W.Â T. Peters, R.Â vanÂ den Berg, and M.Â Welling.

</span>
<span class="ltx_bibblock">Integer discrete flows and lossless compression.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">NeurIPS</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang etÂ al. (2024)</span>
<span class="ltx_bibblock">
Y.Â Huang, J.Â Zhang, Z.Â Shan, and J.Â He.

</span>
<span class="ltx_bibblock">Compression represents intelligence linearly.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv:2404.09937</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huffman (1952)</span>
<span class="ltx_bibblock">
D.Â A. Huffman.

</span>
<span class="ltx_bibblock">A method for the construction of minimum-redundancy codes.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the IRE</em>, 1952.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hutter (2006)</span>
<span class="ltx_bibblock">
M.Â Hutter.

</span>
<span class="ltx_bibblock">500â€™000â‚¬Â prize for compressing human knowledge, 2006.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://prize.hutter1.net" title="">http://prize.hutter1.net</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hutter etÂ al. (2024)</span>
<span class="ltx_bibblock">
M.Â Hutter, D.Â Quarel, and E.Â Catt.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">An Introduction to Universal Artificial Intelligence</em>.

</span>
<span class="ltx_bibblock">Chapman &amp; Hall, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaplan etÂ al. (2020)</span>
<span class="ltx_bibblock">
J.Â Kaplan, S.Â McCandlish, T.Â Henighan, T.Â B. Brown, B.Â Chess, R.Â Child,
S.Â Gray, A.Â Radford, J.Â Wu, and D.Â Amodei.

</span>
<span class="ltx_bibblock">Scaling laws for neural language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">arXiv:2001.08361</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim etÂ al. (2023)</span>
<span class="ltx_bibblock">
H.Â Kim, M.Â Bauer, L.Â Theis, J.Â R. Schwarz, and E.Â Dupont.

</span>
<span class="ltx_bibblock">C3: high-performance and low-complexity neural compression from a
single image or video.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">arXiv:2312.02753</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma and Ba (2015)</span>
<span class="ltx_bibblock">
D.Â P. Kingma and J.Â Ba.

</span>
<span class="ltx_bibblock">Adam: A method for stochastic optimization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">ICLR (Poster)</em>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma etÂ al. (2019)</span>
<span class="ltx_bibblock">
F.Â H. Kingma, P.Â Abbeel, and J.Â Ho.

</span>
<span class="ltx_bibblock">Bit-swap: Recursive bits-back coding for lossless compression with
hierarchical latent variables.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">ICML</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Knoll (2014)</span>
<span class="ltx_bibblock">
B.Â Knoll.

</span>
<span class="ltx_bibblock">CMIX, 2014.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://www.byronknoll.com/cmix.html" title="">http://www.byronknoll.com/cmix.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ladune etÂ al. (2023)</span>
<span class="ltx_bibblock">
T.Â Ladune, P.Â Philippe, F.Â Henry, G.Â Clare, and T.Â Leguay.

</span>
<span class="ltx_bibblock">COOL-CHIC: coordinate-based low complexity hierarchical image
codec.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">ICCV</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Legg and Hutter (2007)</span>
<span class="ltx_bibblock">
S.Â Legg and M.Â Hutter.

</span>
<span class="ltx_bibblock">Universal intelligence: A definition of machine intelligence.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Minds Mach.</em>, 2007.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lehtokangas etÂ al. (1993)</span>
<span class="ltx_bibblock">
M.Â Lehtokangas, J.Â Saarinen, P.Â Huuhtanen, and K.Â Kaski.

</span>
<span class="ltx_bibblock">Neural network optimization tool based on predictive MDL principle
for time series prediction.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">ICTAI</em>, 1993.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lester etÂ al. (2024)</span>
<span class="ltx_bibblock">
B.Â Lester, J.Â Lee, A.Â Alemi, J.Â Pennington, A.Â Roberts, J.Â Sohl-Dickstein,
and N.Â Constant.

</span>
<span class="ltx_bibblock">Training llms over neurally compressed text.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">arxiv:2404.03626</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and VitÃ¡nyi (2019)</span>
<span class="ltx_bibblock">
M.Â Li and P.Â M.Â B. VitÃ¡nyi.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">An Introduction to Kolmogorov Complexity and Its Applications,
4th Edition</em>.

</span>
<span class="ltx_bibblock">Springer, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2024)</span>
<span class="ltx_bibblock">
Z.Â Li, C.Â Huang, X.Â Wang, H.Â Hu, C.Â Wyeth, D.Â Bu, Q.Â Yu, W.Â Gao, X.Â Liu, and
M.Â Li.

</span>
<span class="ltx_bibblock">Understanding is compression.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">arxiv:2407.07723</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2019)</span>
<span class="ltx_bibblock">
Q.Â Liu, Y.Â Xu, and Z.Â Li.

</span>
<span class="ltx_bibblock">DecMac: A deep context model for high efficiency arithmetic
coding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">ICAIIC</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2015)</span>
<span class="ltx_bibblock">
Z.Â Liu, P.Â Luo, X.Â Wang, and X.Â Tang.

</span>
<span class="ltx_bibblock">Deep learning face attributes in the wild.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">ICCV</em>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MacKay (2003)</span>
<span class="ltx_bibblock">
D.Â J.Â C. MacKay.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Information theory, inference, and learning algorithms</em>.

</span>
<span class="ltx_bibblock">Cambridge University Press, 2003.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mahoney (2000)</span>
<span class="ltx_bibblock">
M.Â V. Mahoney.

</span>
<span class="ltx_bibblock">Fast text compression with neural networks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">FLAIRS</em>, 2000.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mahoney (2006)</span>
<span class="ltx_bibblock">
M.Â V. Mahoney.

</span>
<span class="ltx_bibblock">Large text compression benchmark, 2006.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.mattmahoney.net/dc/text.html" title="">https://www.mattmahoney.net/dc/text.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mao etÂ al. (2022)</span>
<span class="ltx_bibblock">
Y.Â Mao, Y.Â Cui, T.Â Kuo, and C.Â J. Xue.

</span>
<span class="ltx_bibblock">TRACE: A fast transformer-based general-purpose lossless
compressor.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">WWW</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mentzer etÂ al. (2019)</span>
<span class="ltx_bibblock">
F.Â Mentzer, E.Â Agustsson, M.Â Tschannen, R.Â Timofte, and L.Â V. Gool.

</span>
<span class="ltx_bibblock">Practical full resolution learned lossless image compression.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">CVPR</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mentzer etÂ al. (2020)</span>
<span class="ltx_bibblock">
F.Â Mentzer, L.Â V. Gool, and M.Â Tschannen.

</span>
<span class="ltx_bibblock">Learning better lossless compression using lossy compression.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">CVPR</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mikolov (2012)</span>
<span class="ltx_bibblock">
T.Â Mikolov.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Statistical Language Models Based on Neural Networks</em>.

</span>
<span class="ltx_bibblock">PhD thesis, Brno Universtiy of Technology, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mirchandani etÂ al. (2023)</span>
<span class="ltx_bibblock">
S.Â Mirchandani, F.Â Xia, P.Â Florence, B.Â Ichter, D.Â Driess, M.Â G. Arenas,
K.Â Rao, D.Â Sadigh, and A.Â Zeng.

</span>
<span class="ltx_bibblock">Large language models as general pattern machines.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">CoRL</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mishra etÂ al. (2022)</span>
<span class="ltx_bibblock">
S.Â Mishra, D.Â Khashabi, C.Â Baral, and H.Â Hajishirzi.

</span>
<span class="ltx_bibblock">Cross-task generalization via natural language crowdsourcing
instructions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">ACL (1)</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mittu etÂ al. (2024)</span>
<span class="ltx_bibblock">
F.Â Mittu, Y.Â Bu, A.Â Gupta, A.Â Devireddy, A.Â E. Ozdarendeli, A.Â Singh, and
G.Â Anumanchipalli.

</span>
<span class="ltx_bibblock">Finezip : Pushing the limits of large language models for practical
lossless text compression.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">arXiv:2409.17141</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Panayotov etÂ al. (2015)</span>
<span class="ltx_bibblock">
V.Â Panayotov, G.Â Chen, D.Â Povey, and S.Â Khudanpur.

</span>
<span class="ltx_bibblock">Librispeech: An ASR corpus based on public domain audio books.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">ICASSP</em>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pasco (1977)</span>
<span class="ltx_bibblock">
R.Â C. Pasco.

</span>
<span class="ltx_bibblock">Source coding algorithms for fast data compression (ph.d. thesis
abstr.).

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">IEEE Trans. Inf. Theory</em>, 1977.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pavlov (2019)</span>
<span class="ltx_bibblock">
I.Â Pavlov.

</span>
<span class="ltx_bibblock">7z Format, 2019.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://www.7-zip.org/7z.html" title="">http://www.7-zip.org/7z.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pot etÂ al. (2019)</span>
<span class="ltx_bibblock">
E.Â Pot, A.Â Mohiuddin, P.Â Ruyssen, M.Â Michalski, R.Â S.Â J. Simsa, and M.Â Wicke.

</span>
<span class="ltx_bibblock">TensorFlow Datasets, a collection of ready-to-use datasets, 2019.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.tensorflow.org/datasets" title="">https://www.tensorflow.org/datasets</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rae etÂ al. (2020)</span>
<span class="ltx_bibblock">
J.Â W. Rae, A.Â Potapenko, S.Â M. Jayakumar, C.Â Hillier, and T.Â P. Lillicrap.

</span>
<span class="ltx_bibblock">Compressive transformers for long-range sequence modelling.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">ICLR</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rathmanner and Hutter (2011)</span>
<span class="ltx_bibblock">
S.Â Rathmanner and M.Â Hutter.

</span>
<span class="ltx_bibblock">A philosophical treatise of universal induction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">Entropy</em>, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rhee etÂ al. (2022)</span>
<span class="ltx_bibblock">
H.Â Rhee, Y.Â I. Jang, S.Â Kim, and N.Â I. Cho.

</span>
<span class="ltx_bibblock">LC-FDNet: Learned lossless image compression with frequency
decomposition network.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">CVPR</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rissanen (1976)</span>
<span class="ltx_bibblock">
J.Â Rissanen.

</span>
<span class="ltx_bibblock">Generalized kraft inequality and arithmetic coding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">IBM J. Res. Dev.</em>, 1976.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Russakovsky etÂ al. (2015)</span>
<span class="ltx_bibblock">
O.Â Russakovsky, J.Â Deng, H.Â Su, J.Â Krause, S.Â Satheesh, S.Â Ma, Z.Â Huang,
A.Â Karpathy, A.Â Khosla, M.Â S. Bernstein, A.Â C. Berg, and L.Â Fei-Fei.

</span>
<span class="ltx_bibblock">Imagenet large scale visual recognition challenge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">Int. J. Comput. Vis.</em>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schiopu and Munteanu (2020)</span>
<span class="ltx_bibblock">
I.Â Schiopu and A.Â Munteanu.

</span>
<span class="ltx_bibblock">Deep-learning-based lossless image coding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">IEEE Trans. Circuits Syst. Video Technol.</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schiopu etÂ al. (2018)</span>
<span class="ltx_bibblock">
I.Â Schiopu, Y.Â Liu, and A.Â Munteanu.

</span>
<span class="ltx_bibblock">CNN-based prediction for lossless coding of photographic images.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">PCS</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schmidhuber and Heil (1994)</span>
<span class="ltx_bibblock">
J.Â Schmidhuber and S.Â Heil.

</span>
<span class="ltx_bibblock">Predictive coding with neural nets: Application to text compression.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">NIPS</em>, pages 1047â€“1054. MIT Press, 1994.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schmidhuber and Heil (1996)</span>
<span class="ltx_bibblock">
J.Â Schmidhuber and S.Â Heil.

</span>
<span class="ltx_bibblock">Sequential neural text compression.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">IEEE Trans. Neural Networks</em>, 1996.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shannon (1948)</span>
<span class="ltx_bibblock">
C.Â E. Shannon.

</span>
<span class="ltx_bibblock">A mathematical theory of communication.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">Bell Syst. Tech. J.</em>, 1948.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharma etÂ al. (2019)</span>
<span class="ltx_bibblock">
E.Â Sharma, C.Â Li, and L.Â Wang.

</span>
<span class="ltx_bibblock">BIGPATENT: A large-scale dataset for abstractive and coherent
summarization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">ACL (1)</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shazeer (2020)</span>
<span class="ltx_bibblock">
N.Â Shazeer.

</span>
<span class="ltx_bibblock">GLU variants improve transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">arXiv:2002.05202</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Skodras etÂ al. (2001)</span>
<span class="ltx_bibblock">
A.Â Skodras, C.Â A. Christopoulos, and T.Â Ebrahimi.

</span>
<span class="ltx_bibblock">The JPEG 2000 still image compression standard.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">IEEE Signal Process. Mag.</em>, 2001.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Solomonoff (1964a)</span>
<span class="ltx_bibblock">
R.Â J. Solomonoff.

</span>
<span class="ltx_bibblock">A formal theory of inductive inference. part I.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">Inf. Control.</em>, 1964a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Solomonoff (1964b)</span>
<span class="ltx_bibblock">
R.Â J. Solomonoff.

</span>
<span class="ltx_bibblock">A formal theory of inductive inference. part II.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">Inf. Control.</em>, 1964b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tao etÂ al. (2022)</span>
<span class="ltx_bibblock">
C.Â Tao, L.Â Hou, W.Â Zhang, L.Â Shang, X.Â Jiang, Q.Â Liu, P.Â Luo, and N.Â Wong.

</span>
<span class="ltx_bibblock">Compression of generative pre-trained language models via
quantization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">ACL (1)</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron etÂ al. (2023a)</span>
<span class="ltx_bibblock">
H.Â Touvron, T.Â Lavril, G.Â Izacard, X.Â Martinet, M.Â Lachaux, T.Â Lacroix,
B.Â RoziÃ¨re, N.Â Goyal, E.Â Hambro, F.Â Azhar, A.Â Rodriguez, A.Â Joulin,
E.Â Grave, and G.Â Lample.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">arXiv:2302.13971</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron etÂ al. (2023b)</span>
<span class="ltx_bibblock">
H.Â Touvron, L.Â Martin, K.Â Stone, P.Â Albert, A.Â Almahairi, Y.Â Babaei,
N.Â Bashlykov, S.Â Batra, P.Â Bhargava, S.Â Bhosale, D.Â Bikel, L.Â Blecher,
C.Â Canton-Ferrer, M.Â Chen, G.Â Cucurull, D.Â Esiobu, J.Â Fernandes, J.Â Fu,
W.Â Fu, B.Â Fuller, C.Â Gao, V.Â Goswami, N.Â Goyal, A.Â Hartshorn, S.Â Hosseini,
R.Â Hou, H.Â Inan, M.Â Kardas, V.Â Kerkez, M.Â Khabsa, I.Â Kloumann, A.Â Korenev,
P.Â S. Koura, M.Â Lachaux, T.Â Lavril, J.Â Lee, D.Â Liskovich, Y.Â Lu, Y.Â Mao,
X.Â Martinet, T.Â Mihaylov, P.Â Mishra, I.Â Molybog, Y.Â Nie, A.Â Poulton,
J.Â Reizenstein, R.Â Rungta, K.Â Saladi, A.Â Schelten, R.Â Silva, E.Â M. Smith,
R.Â Subramanian, X.Â E. Tan, B.Â Tang, R.Â Taylor, A.Â Williams, J.Â X. Kuan,
P.Â Xu, Z.Â Yan, I.Â Zarov, Y.Â Zhang, A.Â Fan, M.Â Kambadur, S.Â Narang,
A.Â Rodriguez, R.Â Stojnic, S.Â Edunov, and T.Â Scialom.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">arXiv:2307.09288</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Townsend etÂ al. (2019)</span>
<span class="ltx_bibblock">
J.Â Townsend, T.Â Bird, and D.Â Barber.

</span>
<span class="ltx_bibblock">Practical lossless compression with latent variables using bits back
coding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">ICLR (Poster)</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Valmeekam etÂ al. (2023)</span>
<span class="ltx_bibblock">
C.Â S.Â K. Valmeekam, K.Â Narayanan, D.Â Kalathil, J.Â Chamberland, and
S.Â Shakkottai.

</span>
<span class="ltx_bibblock">Llmzip: Lossless text compression using large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib74.1.1">arXiv:2306.04050</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">vanÂ den Oord and Schrauwen (2014)</span>
<span class="ltx_bibblock">
A.Â vanÂ den Oord and B.Â Schrauwen.

</span>
<span class="ltx_bibblock">The student-t mixture as a natural image patch prior with application
to image compression.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib75.1.1">J. Mach. Learn. Res.</em>, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani etÂ al. (2017)</span>
<span class="ltx_bibblock">
A.Â Vaswani, N.Â Shazeer, N.Â Parmar, J.Â Uszkoreit, L.Â Jones, A.Â N. Gomez,
L.Â Kaiser, and I.Â Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib76.1.1">NIPS</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">VÃ¶lske etÂ al. (2017)</span>
<span class="ltx_bibblock">
M.Â VÃ¶lske, M.Â Potthast, S.Â Syed, and B.Â Stein.

</span>
<span class="ltx_bibblock">TL;DR: Mining Reddit to learn automatic summarization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib77.1.1">Proceedings of the Workshop on New Frontiers in
Summarization</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2022)</span>
<span class="ltx_bibblock">
Y.Â Wang, S.Â Mishra, P.Â Alipoormolabashi, Y.Â Kordi, A.Â Mirzaei, A.Â Naik,
A.Â Ashok, A.Â S. Dhanasekaran, A.Â Arunkumar, D.Â Stap, E.Â Pathak,
G.Â Karamanolakis, H.Â G. Lai, I.Â Purohit, I.Â Mondal, J.Â Anderson, K.Â Kuznia,
K.Â Doshi, K.Â K. Pal, M.Â Patel, M.Â Moradshahi, M.Â Parmar, M.Â Purohit,
N.Â Varshney, P.Â R. Kaza, P.Â Verma, R.Â S. Puri, R.Â Karia, S.Â Doshi, S.Â K.
Sampat, S.Â Mishra, S.Â R. A, S.Â Patro, T.Â Dixit, and X.Â Shen.

</span>
<span class="ltx_bibblock">Super-naturalinstructions: Generalization via declarative
instructions on 1600+ NLP tasks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib78.1.1">EMNLP</em>, pages 5085â€“5109. Association for Computational
Linguistics, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Welch (1984)</span>
<span class="ltx_bibblock">
T.Â A. Welch.

</span>
<span class="ltx_bibblock">A technique for high-performance data compression.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib79.1.1">Computer</em>, 1984.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wikimedia (2023)</span>
<span class="ltx_bibblock">
Wikimedia.

</span>
<span class="ltx_bibblock">Wikimedia downloads, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://dumps.wikimedia.org" title="">https://dumps.wikimedia.org</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Witten etÂ al. (1987)</span>
<span class="ltx_bibblock">
I.Â H. Witten, R.Â M. Neal, and J.Â G. Cleary.

</span>
<span class="ltx_bibblock">Arithmetic coding for data compression.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib81.1.1">Commun. ACM</em>, 1987.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue etÂ al. (2022)</span>
<span class="ltx_bibblock">
L.Â Xue, A.Â Barua, N.Â Constant, R.Â Al-Rfou, S.Â Narang, M.Â Kale, A.Â Roberts,
and C.Â Raffel.

</span>
<span class="ltx_bibblock">Byt5: Towards a token-free future with pre-trained byte-to-byte
models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib82.1.1">Trans. Assoc. Comput. Linguistics</em>, 2022.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1" lang="en">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Experimental Details</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Training Data Sources</h3>
<div class="ltx_para" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1">We source all of our data from the following open-source TensorFlow datasetsÂ <cite class="ltx_cite ltx_citemacro_citep">(Pot etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib54" title="">2019</a>)</cite>:</p>
</div>
<section class="ltx_paragraph" id="A1.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Text</h4>
<div class="ltx_para" id="A1.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="A1.SS1.SSS0.Px1.p1.6">Since most of TensorFlowâ€™s text datasets are quite small, we concatenate the following five datasets into a single collection of <math alttext="165" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px1.p1.1.m1.1"><semantics id="A1.SS1.SSS0.Px1.p1.1.m1.1a"><mn id="A1.SS1.SSS0.Px1.p1.1.m1.1.1" xref="A1.SS1.SSS0.Px1.p1.1.m1.1.1.cmml">165</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS0.Px1.p1.1.m1.1b"><cn id="A1.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" type="integer" xref="A1.SS1.SSS0.Px1.p1.1.m1.1.1">165</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS0.Px1.p1.1.m1.1c">165</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.SSS0.Px1.p1.1.m1.1d">165</annotation></semantics></math>GB:
(i) <em class="ltx_emph ltx_font_italic" id="A1.SS1.SSS0.Px1.p1.6.1">Wikipedia</em>Â <cite class="ltx_cite ltx_citemacro_citep">(Wikimedia, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib80" title="">2023</a>)</cite>, the filtered UTF-8 encoded text from an XML dump from 2023-06-01, containing all languages but predominantly English and western languages (<math alttext="113.9" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px1.p1.2.m2.1"><semantics id="A1.SS1.SSS0.Px1.p1.2.m2.1a"><mn id="A1.SS1.SSS0.Px1.p1.2.m2.1.1" xref="A1.SS1.SSS0.Px1.p1.2.m2.1.1.cmml">113.9</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS0.Px1.p1.2.m2.1b"><cn id="A1.SS1.SSS0.Px1.p1.2.m2.1.1.cmml" type="float" xref="A1.SS1.SSS0.Px1.p1.2.m2.1.1">113.9</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS0.Px1.p1.2.m2.1c">113.9</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.SSS0.Px1.p1.2.m2.1d">113.9</annotation></semantics></math>GB);
(ii) <em class="ltx_emph ltx_font_italic" id="A1.SS1.SSS0.Px1.p1.6.2">PG-19</em>Â <cite class="ltx_cite ltx_citemacro_citep">(Rae etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib55" title="">2020</a>)</cite>, books from the Project Gutenberg, also encoded in UTF-8 (<math alttext="9.4" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px1.p1.3.m3.1"><semantics id="A1.SS1.SSS0.Px1.p1.3.m3.1a"><mn id="A1.SS1.SSS0.Px1.p1.3.m3.1.1" xref="A1.SS1.SSS0.Px1.p1.3.m3.1.1.cmml">9.4</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS0.Px1.p1.3.m3.1b"><cn id="A1.SS1.SSS0.Px1.p1.3.m3.1.1.cmml" type="float" xref="A1.SS1.SSS0.Px1.p1.3.m3.1.1">9.4</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS0.Px1.p1.3.m3.1c">9.4</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.SSS0.Px1.p1.3.m3.1d">9.4</annotation></semantics></math>GB);
(iii) <em class="ltx_emph ltx_font_italic" id="A1.SS1.SSS0.Px1.p1.6.3">Big Patent</em>Â <cite class="ltx_cite ltx_citemacro_citep">(Sharma etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib65" title="">2019</a>)</cite>, a dataset of patents in English (<math alttext="30.2" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px1.p1.4.m4.1"><semantics id="A1.SS1.SSS0.Px1.p1.4.m4.1a"><mn id="A1.SS1.SSS0.Px1.p1.4.m4.1.1" xref="A1.SS1.SSS0.Px1.p1.4.m4.1.1.cmml">30.2</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS0.Px1.p1.4.m4.1b"><cn id="A1.SS1.SSS0.Px1.p1.4.m4.1.1.cmml" type="float" xref="A1.SS1.SSS0.Px1.p1.4.m4.1.1">30.2</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS0.Px1.p1.4.m4.1c">30.2</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.SSS0.Px1.p1.4.m4.1d">30.2</annotation></semantics></math>GB);
(iv) <em class="ltx_emph ltx_font_italic" id="A1.SS1.SSS0.Px1.p1.6.4">Scientific Papers</em>Â <cite class="ltx_cite ltx_citemacro_citep">(Cohan etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib9" title="">2018</a>)</cite>, from arXiv and PubMed, containing the raw text including the LaTeX code (<math alttext="8.1" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px1.p1.5.m5.1"><semantics id="A1.SS1.SSS0.Px1.p1.5.m5.1a"><mn id="A1.SS1.SSS0.Px1.p1.5.m5.1.1" xref="A1.SS1.SSS0.Px1.p1.5.m5.1.1.cmml">8.1</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS0.Px1.p1.5.m5.1b"><cn id="A1.SS1.SSS0.Px1.p1.5.m5.1.1.cmml" type="float" xref="A1.SS1.SSS0.Px1.p1.5.m5.1.1">8.1</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS0.Px1.p1.5.m5.1c">8.1</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.SSS0.Px1.p1.5.m5.1d">8.1</annotation></semantics></math>GB);
and (v) <em class="ltx_emph ltx_font_italic" id="A1.SS1.SSS0.Px1.p1.6.5">Natural Instructions</em>Â <cite class="ltx_cite ltx_citemacro_citep">(Mishra etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib49" title="">2022</a>; Wang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib78" title="">2022</a>)</cite>, tasks formulated in English covering different domains and lanugages (<math alttext="4.1" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px1.p1.6.m6.1"><semantics id="A1.SS1.SSS0.Px1.p1.6.m6.1a"><mn id="A1.SS1.SSS0.Px1.p1.6.m6.1.1" xref="A1.SS1.SSS0.Px1.p1.6.m6.1.1.cmml">4.1</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS0.Px1.p1.6.m6.1b"><cn id="A1.SS1.SSS0.Px1.p1.6.m6.1.1.cmml" type="float" xref="A1.SS1.SSS0.Px1.p1.6.m6.1.1">4.1</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS0.Px1.p1.6.m6.1c">4.1</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.SSS0.Px1.p1.6.m6.1d">4.1</annotation></semantics></math>GB).</p>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Image</h4>
<div class="ltx_para" id="A1.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="A1.SS1.SSS0.Px2.p1.2">We collect a subset of <math alttext="165" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px2.p1.1.m1.1"><semantics id="A1.SS1.SSS0.Px2.p1.1.m1.1a"><mn id="A1.SS1.SSS0.Px2.p1.1.m1.1.1" xref="A1.SS1.SSS0.Px2.p1.1.m1.1.1.cmml">165</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS0.Px2.p1.1.m1.1b"><cn id="A1.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" type="integer" xref="A1.SS1.SSS0.Px2.p1.1.m1.1.1">165</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS0.Px2.p1.1.m1.1c">165</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.SSS0.Px2.p1.1.m1.1d">165</annotation></semantics></math>GB of the ImageNet datasetÂ <cite class="ltx_cite ltx_citemacro_citep">(Russakovsky etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib59" title="">2015</a>)</cite>, uniformly sampled across the <math alttext="1000" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px2.p1.2.m2.1"><semantics id="A1.SS1.SSS0.Px2.p1.2.m2.1a"><mn id="A1.SS1.SSS0.Px2.p1.2.m2.1.1" xref="A1.SS1.SSS0.Px2.p1.2.m2.1.1.cmml">1000</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS0.Px2.p1.2.m2.1b"><cn id="A1.SS1.SSS0.Px2.p1.2.m2.1.1.cmml" type="integer" xref="A1.SS1.SSS0.Px2.p1.2.m2.1.1">1000</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS0.Px2.p1.2.m2.1c">1000</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.SSS0.Px2.p1.2.m2.1d">1000</annotation></semantics></math>Â classes, which contains <span class="ltx_text ltx_number" id="A1.SS1.SSS0.Px2.p1.2.1">14â€‰197â€‰122</span> annotated images (of varying resolutions) from the WordNet hierarchy.
We decode the images into RGB arrays (three <span class="ltx_text ltx_font_typewriter" id="A1.SS1.SSS0.Px2.p1.2.2">uint8</span> channels), flatten them, and concatenate them into a byte stream of flattened images.
As a consequence, we ignore image boundaries when sampling from this data source (i.e., sequences are not guaranteed to start or end at the start or end of an image).</p>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Audio</h4>
<div class="ltx_para" id="A1.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="A1.SS1.SSS0.Px3.p1.1">We create a subset of <math alttext="165" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px3.p1.1.m1.1"><semantics id="A1.SS1.SSS0.Px3.p1.1.m1.1a"><mn id="A1.SS1.SSS0.Px3.p1.1.m1.1.1" xref="A1.SS1.SSS0.Px3.p1.1.m1.1.1.cmml">165</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS0.Px3.p1.1.m1.1b"><cn id="A1.SS1.SSS0.Px3.p1.1.m1.1.1.cmml" type="integer" xref="A1.SS1.SSS0.Px3.p1.1.m1.1.1">165</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS0.Px3.p1.1.m1.1c">165</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.SSS0.Px3.p1.1.m1.1d">165</annotation></semantics></math>GB from the Common Voice datasetÂ <cite class="ltx_cite ltx_citemacro_citep">(Ardila etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib1" title="">2020</a>)</cite>, a multilingual dataset of voice recordings.
We downsample the dataset from 48 kHz to 16 kHz and encode the waveform as <span class="ltx_text ltx_font_typewriter" id="A1.SS1.SSS0.Px3.p1.1.1">int16</span>, i.e., with two bytes per sample.
As for images, we concatenate all individual audio samples into a single byte stream.
Accordingly, there is no guarantee that a sequence sampled from our dataset starts or ends at the beginning of a recording.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Out-of-Distribution Evaluation Data Sources</h3>
<div class="ltx_para" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1">We source all of our data from the following open-source TensorFlow datasetsÂ <cite class="ltx_cite ltx_citemacro_citep">(Pot etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib54" title="">2019</a>)</cite>:</p>
</div>
<section class="ltx_paragraph" id="A1.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Text</h4>
<div class="ltx_para" id="A1.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="A1.SS2.SSS0.Px1.p1.2">We consider a <math alttext="1" class="ltx_Math" display="inline" id="A1.SS2.SSS0.Px1.p1.1.m1.1"><semantics id="A1.SS2.SSS0.Px1.p1.1.m1.1a"><mn id="A1.SS2.SSS0.Px1.p1.1.m1.1.1" xref="A1.SS2.SSS0.Px1.p1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="A1.SS2.SSS0.Px1.p1.1.m1.1b"><cn id="A1.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" type="integer" xref="A1.SS2.SSS0.Px1.p1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.SSS0.Px1.p1.1.m1.1c">1</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.SSS0.Px1.p1.1.m1.1d">1</annotation></semantics></math>GB subset of the <em class="ltx_emph ltx_font_italic" id="A1.SS2.SSS0.Px1.p1.2.1">Reddit</em> datasetÂ <cite class="ltx_cite ltx_citemacro_citep">(VÃ¶lske etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib77" title="">2017</a>)</cite>, which contains <math alttext="3.8" class="ltx_Math" display="inline" id="A1.SS2.SSS0.Px1.p1.2.m2.1"><semantics id="A1.SS2.SSS0.Px1.p1.2.m2.1a"><mn id="A1.SS2.SSS0.Px1.p1.2.m2.1.1" xref="A1.SS2.SSS0.Px1.p1.2.m2.1.1.cmml">3.8</mn><annotation-xml encoding="MathML-Content" id="A1.SS2.SSS0.Px1.p1.2.m2.1b"><cn id="A1.SS2.SSS0.Px1.p1.2.m2.1.1.cmml" type="float" xref="A1.SS2.SSS0.Px1.p1.2.m2.1.1">3.8</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.SSS0.Px1.p1.2.m2.1c">3.8</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.SSS0.Px1.p1.2.m2.1d">3.8</annotation></semantics></math>
million Reddit posts encoded in UTF-8.</p>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Images</h4>
<div class="ltx_para" id="A1.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="A1.SS2.SSS0.Px2.p1.2">We create a <math alttext="1" class="ltx_Math" display="inline" id="A1.SS2.SSS0.Px2.p1.1.m1.1"><semantics id="A1.SS2.SSS0.Px2.p1.1.m1.1a"><mn id="A1.SS2.SSS0.Px2.p1.1.m1.1.1" xref="A1.SS2.SSS0.Px2.p1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="A1.SS2.SSS0.Px2.p1.1.m1.1b"><cn id="A1.SS2.SSS0.Px2.p1.1.m1.1.1.cmml" type="integer" xref="A1.SS2.SSS0.Px2.p1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.SSS0.Px2.p1.1.m1.1c">1</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.SSS0.Px2.p1.1.m1.1d">1</annotation></semantics></math>GB subset of the CelebA HQ datasetÂ <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib40" title="">2015</a>)</cite> with a resolution of <math alttext="512\times 512" class="ltx_Math" display="inline" id="A1.SS2.SSS0.Px2.p1.2.m2.1"><semantics id="A1.SS2.SSS0.Px2.p1.2.m2.1a"><mrow id="A1.SS2.SSS0.Px2.p1.2.m2.1.1" xref="A1.SS2.SSS0.Px2.p1.2.m2.1.1.cmml"><mn id="A1.SS2.SSS0.Px2.p1.2.m2.1.1.2" xref="A1.SS2.SSS0.Px2.p1.2.m2.1.1.2.cmml">512</mn><mo id="A1.SS2.SSS0.Px2.p1.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="A1.SS2.SSS0.Px2.p1.2.m2.1.1.1.cmml">Ã—</mo><mn id="A1.SS2.SSS0.Px2.p1.2.m2.1.1.3" xref="A1.SS2.SSS0.Px2.p1.2.m2.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.SSS0.Px2.p1.2.m2.1b"><apply id="A1.SS2.SSS0.Px2.p1.2.m2.1.1.cmml" xref="A1.SS2.SSS0.Px2.p1.2.m2.1.1"><times id="A1.SS2.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="A1.SS2.SSS0.Px2.p1.2.m2.1.1.1"></times><cn id="A1.SS2.SSS0.Px2.p1.2.m2.1.1.2.cmml" type="integer" xref="A1.SS2.SSS0.Px2.p1.2.m2.1.1.2">512</cn><cn id="A1.SS2.SSS0.Px2.p1.2.m2.1.1.3.cmml" type="integer" xref="A1.SS2.SSS0.Px2.p1.2.m2.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.SSS0.Px2.p1.2.m2.1c">512\times 512</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.SSS0.Px2.p1.2.m2.1d">512 Ã— 512</annotation></semantics></math>.
We process the images in the same way as for our image training set, i.e., flattening and concatenation, and we subsample uniformly across classes of CelebA.</p>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS2.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Audio</h4>
<div class="ltx_para" id="A1.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="A1.SS2.SSS0.Px3.p1.2">We use <math alttext="1" class="ltx_Math" display="inline" id="A1.SS2.SSS0.Px3.p1.1.m1.1"><semantics id="A1.SS2.SSS0.Px3.p1.1.m1.1a"><mn id="A1.SS2.SSS0.Px3.p1.1.m1.1.1" xref="A1.SS2.SSS0.Px3.p1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="A1.SS2.SSS0.Px3.p1.1.m1.1b"><cn id="A1.SS2.SSS0.Px3.p1.1.m1.1.1.cmml" type="integer" xref="A1.SS2.SSS0.Px3.p1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.SSS0.Px3.p1.1.m1.1c">1</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.SSS0.Px3.p1.1.m1.1d">1</annotation></semantics></math>GB from the <em class="ltx_emph ltx_font_italic" id="A1.SS2.SSS0.Px3.p1.2.1">LibriSpeech</em>Â <cite class="ltx_cite ltx_citemacro_citep">(Panayotov etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib51" title="">2015</a>)</cite> dataset, which contains roughly <math alttext="1000" class="ltx_Math" display="inline" id="A1.SS2.SSS0.Px3.p1.2.m2.1"><semantics id="A1.SS2.SSS0.Px3.p1.2.m2.1a"><mn id="A1.SS2.SSS0.Px3.p1.2.m2.1.1" xref="A1.SS2.SSS0.Px3.p1.2.m2.1.1.cmml">1000</mn><annotation-xml encoding="MathML-Content" id="A1.SS2.SSS0.Px3.p1.2.m2.1b"><cn id="A1.SS2.SSS0.Px3.p1.2.m2.1.1.cmml" type="integer" xref="A1.SS2.SSS0.Px3.p1.2.m2.1.1">1000</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.SSS0.Px3.p1.2.m2.1c">1000</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.SSS0.Px3.p1.2.m2.1d">1000</annotation></semantics></math> hours of English speech data derived from audiobooks that have been segmented and aligned in the LibriVox project.
The data is already in 16kHz (with a sample size of 2 bytes), and we simply concatenate samples into a single byte stream.</p>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS2.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Multimodal Evaluations</h4>
<div class="ltx_para" id="A1.SS2.SSS0.Px4.p1">
<p class="ltx_p" id="A1.SS2.SSS0.Px4.p1.1">For our evaluations on multimodal data, we use the unimodal evaluations on <math alttext="1" class="ltx_Math" display="inline" id="A1.SS2.SSS0.Px4.p1.1.m1.1"><semantics id="A1.SS2.SSS0.Px4.p1.1.m1.1a"><mn id="A1.SS2.SSS0.Px4.p1.1.m1.1.1" xref="A1.SS2.SSS0.Px4.p1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="A1.SS2.SSS0.Px4.p1.1.m1.1b"><cn id="A1.SS2.SSS0.Px4.p1.1.m1.1.1.cmml" type="integer" xref="A1.SS2.SSS0.Px4.p1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.SSS0.Px4.p1.1.m1.1c">1</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.SSS0.Px4.p1.1.m1.1d">1</annotation></semantics></math>GB of data as described above and average the results accordingly (both for our models but also all standard compression algorithms, and Bellardâ€™s online adaptive transformer), either over two or three evaluations depending on the evaluation mixture composition.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Sweeps</h3>
<section class="ltx_paragraph" id="A1.SS3.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Model Size vs.Â Dataset Size</h4>
<div class="ltx_para" id="A1.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="A1.SS3.SSS0.Px1.p1.16">The experiment to investigate the impact of training dataset- and model size, with results shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.F4" title="In What You See Is What You Get â€£ 5 Results â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">4</span></a>, used the following model parameters.
Dataset sizes were <math alttext="20\%" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.1.m1.1"><semantics id="A1.SS3.SSS0.Px1.p1.1.m1.1a"><mrow id="A1.SS3.SSS0.Px1.p1.1.m1.1.1" xref="A1.SS3.SSS0.Px1.p1.1.m1.1.1.cmml"><mn id="A1.SS3.SSS0.Px1.p1.1.m1.1.1.2" xref="A1.SS3.SSS0.Px1.p1.1.m1.1.1.2.cmml">20</mn><mo id="A1.SS3.SSS0.Px1.p1.1.m1.1.1.1" xref="A1.SS3.SSS0.Px1.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px1.p1.1.m1.1b"><apply id="A1.SS3.SSS0.Px1.p1.1.m1.1.1.cmml" xref="A1.SS3.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="latexml" id="A1.SS3.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="A1.SS3.SSS0.Px1.p1.1.m1.1.1.1">percent</csymbol><cn id="A1.SS3.SSS0.Px1.p1.1.m1.1.1.2.cmml" type="integer" xref="A1.SS3.SSS0.Px1.p1.1.m1.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px1.p1.1.m1.1c">20\%</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px1.p1.1.m1.1d">20 %</annotation></semantics></math>, <math alttext="40\%" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.2.m2.1"><semantics id="A1.SS3.SSS0.Px1.p1.2.m2.1a"><mrow id="A1.SS3.SSS0.Px1.p1.2.m2.1.1" xref="A1.SS3.SSS0.Px1.p1.2.m2.1.1.cmml"><mn id="A1.SS3.SSS0.Px1.p1.2.m2.1.1.2" xref="A1.SS3.SSS0.Px1.p1.2.m2.1.1.2.cmml">40</mn><mo id="A1.SS3.SSS0.Px1.p1.2.m2.1.1.1" xref="A1.SS3.SSS0.Px1.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px1.p1.2.m2.1b"><apply id="A1.SS3.SSS0.Px1.p1.2.m2.1.1.cmml" xref="A1.SS3.SSS0.Px1.p1.2.m2.1.1"><csymbol cd="latexml" id="A1.SS3.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="A1.SS3.SSS0.Px1.p1.2.m2.1.1.1">percent</csymbol><cn id="A1.SS3.SSS0.Px1.p1.2.m2.1.1.2.cmml" type="integer" xref="A1.SS3.SSS0.Px1.p1.2.m2.1.1.2">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px1.p1.2.m2.1c">40\%</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px1.p1.2.m2.1d">40 %</annotation></semantics></math>, <math alttext="60\%" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.3.m3.1"><semantics id="A1.SS3.SSS0.Px1.p1.3.m3.1a"><mrow id="A1.SS3.SSS0.Px1.p1.3.m3.1.1" xref="A1.SS3.SSS0.Px1.p1.3.m3.1.1.cmml"><mn id="A1.SS3.SSS0.Px1.p1.3.m3.1.1.2" xref="A1.SS3.SSS0.Px1.p1.3.m3.1.1.2.cmml">60</mn><mo id="A1.SS3.SSS0.Px1.p1.3.m3.1.1.1" xref="A1.SS3.SSS0.Px1.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px1.p1.3.m3.1b"><apply id="A1.SS3.SSS0.Px1.p1.3.m3.1.1.cmml" xref="A1.SS3.SSS0.Px1.p1.3.m3.1.1"><csymbol cd="latexml" id="A1.SS3.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="A1.SS3.SSS0.Px1.p1.3.m3.1.1.1">percent</csymbol><cn id="A1.SS3.SSS0.Px1.p1.3.m3.1.1.2.cmml" type="integer" xref="A1.SS3.SSS0.Px1.p1.3.m3.1.1.2">60</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px1.p1.3.m3.1c">60\%</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px1.p1.3.m3.1d">60 %</annotation></semantics></math>, <math alttext="80\%" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.4.m4.1"><semantics id="A1.SS3.SSS0.Px1.p1.4.m4.1a"><mrow id="A1.SS3.SSS0.Px1.p1.4.m4.1.1" xref="A1.SS3.SSS0.Px1.p1.4.m4.1.1.cmml"><mn id="A1.SS3.SSS0.Px1.p1.4.m4.1.1.2" xref="A1.SS3.SSS0.Px1.p1.4.m4.1.1.2.cmml">80</mn><mo id="A1.SS3.SSS0.Px1.p1.4.m4.1.1.1" xref="A1.SS3.SSS0.Px1.p1.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px1.p1.4.m4.1b"><apply id="A1.SS3.SSS0.Px1.p1.4.m4.1.1.cmml" xref="A1.SS3.SSS0.Px1.p1.4.m4.1.1"><csymbol cd="latexml" id="A1.SS3.SSS0.Px1.p1.4.m4.1.1.1.cmml" xref="A1.SS3.SSS0.Px1.p1.4.m4.1.1.1">percent</csymbol><cn id="A1.SS3.SSS0.Px1.p1.4.m4.1.1.2.cmml" type="integer" xref="A1.SS3.SSS0.Px1.p1.4.m4.1.1.2">80</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px1.p1.4.m4.1c">80\%</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px1.p1.4.m4.1d">80 %</annotation></semantics></math>, and <math alttext="100\%" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.5.m5.1"><semantics id="A1.SS3.SSS0.Px1.p1.5.m5.1a"><mrow id="A1.SS3.SSS0.Px1.p1.5.m5.1.1" xref="A1.SS3.SSS0.Px1.p1.5.m5.1.1.cmml"><mn id="A1.SS3.SSS0.Px1.p1.5.m5.1.1.2" xref="A1.SS3.SSS0.Px1.p1.5.m5.1.1.2.cmml">100</mn><mo id="A1.SS3.SSS0.Px1.p1.5.m5.1.1.1" xref="A1.SS3.SSS0.Px1.p1.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px1.p1.5.m5.1b"><apply id="A1.SS3.SSS0.Px1.p1.5.m5.1.1.cmml" xref="A1.SS3.SSS0.Px1.p1.5.m5.1.1"><csymbol cd="latexml" id="A1.SS3.SSS0.Px1.p1.5.m5.1.1.1.cmml" xref="A1.SS3.SSS0.Px1.p1.5.m5.1.1.1">percent</csymbol><cn id="A1.SS3.SSS0.Px1.p1.5.m5.1.1.2.cmml" type="integer" xref="A1.SS3.SSS0.Px1.p1.5.m5.1.1.2">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px1.p1.5.m5.1c">100\%</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px1.p1.5.m5.1d">100 %</annotation></semantics></math> of the full <math alttext="165" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.6.m6.1"><semantics id="A1.SS3.SSS0.Px1.p1.6.m6.1a"><mn id="A1.SS3.SSS0.Px1.p1.6.m6.1.1" xref="A1.SS3.SSS0.Px1.p1.6.m6.1.1.cmml">165</mn><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px1.p1.6.m6.1b"><cn id="A1.SS3.SSS0.Px1.p1.6.m6.1.1.cmml" type="integer" xref="A1.SS3.SSS0.Px1.p1.6.m6.1.1">165</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px1.p1.6.m6.1c">165</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px1.p1.6.m6.1d">165</annotation></semantics></math>GB for each training set mixture (uni- and multimodal).
All models used a context size of <math alttext="4096" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.7.m7.1"><semantics id="A1.SS3.SSS0.Px1.p1.7.m7.1a"><mn id="A1.SS3.SSS0.Px1.p1.7.m7.1.1" xref="A1.SS3.SSS0.Px1.p1.7.m7.1.1.cmml">4096</mn><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px1.p1.7.m7.1b"><cn id="A1.SS3.SSS0.Px1.p1.7.m7.1.1.cmml" type="integer" xref="A1.SS3.SSS0.Px1.p1.7.m7.1.1">4096</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px1.p1.7.m7.1c">4096</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px1.p1.7.m7.1d">4096</annotation></semantics></math>, <math alttext="8" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.8.m8.1"><semantics id="A1.SS3.SSS0.Px1.p1.8.m8.1a"><mn id="A1.SS3.SSS0.Px1.p1.8.m8.1.1" xref="A1.SS3.SSS0.Px1.p1.8.m8.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px1.p1.8.m8.1b"><cn id="A1.SS3.SSS0.Px1.p1.8.m8.1.1.cmml" type="integer" xref="A1.SS3.SSS0.Px1.p1.8.m8.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px1.p1.8.m8.1c">8</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px1.p1.8.m8.1d">8</annotation></semantics></math> attention heads per layer, a widening factor of <math alttext="4" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.9.m9.1"><semantics id="A1.SS3.SSS0.Px1.p1.9.m9.1a"><mn id="A1.SS3.SSS0.Px1.p1.9.m9.1.1" xref="A1.SS3.SSS0.Px1.p1.9.m9.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px1.p1.9.m9.1b"><cn id="A1.SS3.SSS0.Px1.p1.9.m9.1.1.cmml" type="integer" xref="A1.SS3.SSS0.Px1.p1.9.m9.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px1.p1.9.m9.1c">4</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px1.p1.9.m9.1d">4</annotation></semantics></math> and the number of layers was either <math alttext="2" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.10.m10.1"><semantics id="A1.SS3.SSS0.Px1.p1.10.m10.1a"><mn id="A1.SS3.SSS0.Px1.p1.10.m10.1.1" xref="A1.SS3.SSS0.Px1.p1.10.m10.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px1.p1.10.m10.1b"><cn id="A1.SS3.SSS0.Px1.p1.10.m10.1.1.cmml" type="integer" xref="A1.SS3.SSS0.Px1.p1.10.m10.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px1.p1.10.m10.1c">2</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px1.p1.10.m10.1d">2</annotation></semantics></math>, <math alttext="4" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.11.m11.1"><semantics id="A1.SS3.SSS0.Px1.p1.11.m11.1a"><mn id="A1.SS3.SSS0.Px1.p1.11.m11.1.1" xref="A1.SS3.SSS0.Px1.p1.11.m11.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px1.p1.11.m11.1b"><cn id="A1.SS3.SSS0.Px1.p1.11.m11.1.1.cmml" type="integer" xref="A1.SS3.SSS0.Px1.p1.11.m11.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px1.p1.11.m11.1c">4</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px1.p1.11.m11.1d">4</annotation></semantics></math>, <math alttext="6" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.12.m12.1"><semantics id="A1.SS3.SSS0.Px1.p1.12.m12.1a"><mn id="A1.SS3.SSS0.Px1.p1.12.m12.1.1" xref="A1.SS3.SSS0.Px1.p1.12.m12.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px1.p1.12.m12.1b"><cn id="A1.SS3.SSS0.Px1.p1.12.m12.1.1.cmml" type="integer" xref="A1.SS3.SSS0.Px1.p1.12.m12.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px1.p1.12.m12.1c">6</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px1.p1.12.m12.1d">6</annotation></semantics></math>, <math alttext="8" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.13.m13.1"><semantics id="A1.SS3.SSS0.Px1.p1.13.m13.1a"><mn id="A1.SS3.SSS0.Px1.p1.13.m13.1.1" xref="A1.SS3.SSS0.Px1.p1.13.m13.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px1.p1.13.m13.1b"><cn id="A1.SS3.SSS0.Px1.p1.13.m13.1.1.cmml" type="integer" xref="A1.SS3.SSS0.Px1.p1.13.m13.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px1.p1.13.m13.1c">8</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px1.p1.13.m13.1d">8</annotation></semantics></math>, or <math alttext="10" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.14.m14.1"><semantics id="A1.SS3.SSS0.Px1.p1.14.m14.1a"><mn id="A1.SS3.SSS0.Px1.p1.14.m14.1.1" xref="A1.SS3.SSS0.Px1.p1.14.m14.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px1.p1.14.m14.1b"><cn id="A1.SS3.SSS0.Px1.p1.14.m14.1.1.cmml" type="integer" xref="A1.SS3.SSS0.Px1.p1.14.m14.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px1.p1.14.m14.1c">10</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px1.p1.14.m14.1d">10</annotation></semantics></math>.
Models were trained with a batch size of <math alttext="32" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.15.m15.1"><semantics id="A1.SS3.SSS0.Px1.p1.15.m15.1a"><mn id="A1.SS3.SSS0.Px1.p1.15.m15.1.1" xref="A1.SS3.SSS0.Px1.p1.15.m15.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px1.p1.15.m15.1b"><cn id="A1.SS3.SSS0.Px1.p1.15.m15.1.1.cmml" type="integer" xref="A1.SS3.SSS0.Px1.p1.15.m15.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px1.p1.15.m15.1c">32</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px1.p1.15.m15.1d">32</annotation></semantics></math>.
The learning rate was <math alttext="1\text{\times}{10}^{-4}" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.16.m16.3"><semantics id="A1.SS3.SSS0.Px1.p1.16.m16.3a"><mrow id="A1.SS3.SSS0.Px1.p1.16.m16.3.3.3" xref="A1.SS3.SSS0.Px1.p1.16.m16.3.3.3.cmml"><mn id="A1.SS3.SSS0.Px1.p1.16.m16.1.1.1.1.1.1.1" xref="A1.SS3.SSS0.Px1.p1.16.m16.3.3.3.cmml">1</mn><mtext id="A1.SS3.SSS0.Px1.p1.16.m16.2.2.2.2.2.2.2" xref="A1.SS3.SSS0.Px1.p1.16.m16.3.3.3.cmml">Ã—</mtext><msup id="A1.SS3.SSS0.Px1.p1.16.m16.3.3.3.3.3.3.3" xref="A1.SS3.SSS0.Px1.p1.16.m16.3.3.3.cmml"><mn id="A1.SS3.SSS0.Px1.p1.16.m16.3.3.3.3.3.3.3.2" xref="A1.SS3.SSS0.Px1.p1.16.m16.3.3.3.cmml">10</mn><mrow id="A1.SS3.SSS0.Px1.p1.16.m16.3.3.3.3.3.3.3.3.2" xref="A1.SS3.SSS0.Px1.p1.16.m16.3.3.3.cmml"><mo id="A1.SS3.SSS0.Px1.p1.16.m16.3.3.3.3.3.3.3.3.2a" xref="A1.SS3.SSS0.Px1.p1.16.m16.3.3.3.cmml">âˆ’</mo><mn id="A1.SS3.SSS0.Px1.p1.16.m16.3.3.3.3.3.3.3.3.2.2" xref="A1.SS3.SSS0.Px1.p1.16.m16.3.3.3.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px1.p1.16.m16.3b"><csymbol cd="latexml" id="A1.SS3.SSS0.Px1.p1.16.m16.3.3.3.cmml" xref="A1.SS3.SSS0.Px1.p1.16.m16.3.3.3">1E-4</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px1.p1.16.m16.3c">1\text{\times}{10}^{-4}</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px1.p1.16.m16.3d">start_ARG 1 end_ARG start_ARG times end_ARG start_ARG power start_ARG 10 end_ARG start_ARG - 4 end_ARG end_ARG</annotation></semantics></math>, and a sinusoid positional encoding was used.</p>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS3.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Model Size vs.Â Context Size</h4>
<div class="ltx_para" id="A1.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="A1.SS3.SSS0.Px2.p1.12"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.F5" title="In Model Size vs. Context Size â€£ 5 Results â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">5</span></a> in the main paper shows the relationship between context length and model size.
For this experiment we performed a large-scale sweep with the goal of covering a good range of training FLOPS budget with models that make various trade-offs between model size and context length (given the same model size, compute demand increases with increasing context length).
The main question was whether there is a qualitatively similar relationship across parameters, and whether there is a clear sweet spot â€” see the main paper for results and discussion.
For our sweep we used the same model parameters as in the previous paragraph (the training data size was always at <math alttext="100\%" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px2.p1.1.m1.1"><semantics id="A1.SS3.SSS0.Px2.p1.1.m1.1a"><mrow id="A1.SS3.SSS0.Px2.p1.1.m1.1.1" xref="A1.SS3.SSS0.Px2.p1.1.m1.1.1.cmml"><mn id="A1.SS3.SSS0.Px2.p1.1.m1.1.1.2" xref="A1.SS3.SSS0.Px2.p1.1.m1.1.1.2.cmml">100</mn><mo id="A1.SS3.SSS0.Px2.p1.1.m1.1.1.1" xref="A1.SS3.SSS0.Px2.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px2.p1.1.m1.1b"><apply id="A1.SS3.SSS0.Px2.p1.1.m1.1.1.cmml" xref="A1.SS3.SSS0.Px2.p1.1.m1.1.1"><csymbol cd="latexml" id="A1.SS3.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="A1.SS3.SSS0.Px2.p1.1.m1.1.1.1">percent</csymbol><cn id="A1.SS3.SSS0.Px2.p1.1.m1.1.1.2.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.1.m1.1.1.2">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px2.p1.1.m1.1c">100\%</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px2.p1.1.m1.1d">100 %</annotation></semantics></math>) and sweep over the following four context sizes (with training batch size in brackets): <math alttext="{[1024\leavevmode\nobreak\ (128),2048\leavevmode\nobreak\ (64),4096\leavevmode%
\nobreak\ (32),8192\leavevmode\nobreak\ (16)]}" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px2.p1.2.m2.8"><semantics id="A1.SS3.SSS0.Px2.p1.2.m2.8a"><mrow id="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4" xref="A1.SS3.SSS0.Px2.p1.2.m2.8.8.5.cmml"><mo id="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.5" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.2.m2.8.8.5.cmml">[</mo><mrow id="A1.SS3.SSS0.Px2.p1.2.m2.5.5.1.1" xref="A1.SS3.SSS0.Px2.p1.2.m2.5.5.1.1.cmml"><mn id="A1.SS3.SSS0.Px2.p1.2.m2.5.5.1.1.2" xref="A1.SS3.SSS0.Px2.p1.2.m2.5.5.1.1.2.cmml">1024</mn><mo id="A1.SS3.SSS0.Px2.p1.2.m2.5.5.1.1.1" lspace="0.500em" xref="A1.SS3.SSS0.Px2.p1.2.m2.5.5.1.1.1.cmml">â¢</mo><mrow id="A1.SS3.SSS0.Px2.p1.2.m2.5.5.1.1.3.2" xref="A1.SS3.SSS0.Px2.p1.2.m2.5.5.1.1.cmml"><mo id="A1.SS3.SSS0.Px2.p1.2.m2.5.5.1.1.3.2.1" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.2.m2.5.5.1.1.cmml">(</mo><mn id="A1.SS3.SSS0.Px2.p1.2.m2.1.1" xref="A1.SS3.SSS0.Px2.p1.2.m2.1.1.cmml">128</mn><mo id="A1.SS3.SSS0.Px2.p1.2.m2.5.5.1.1.3.2.2" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.2.m2.5.5.1.1.cmml">)</mo></mrow></mrow><mo id="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.6" xref="A1.SS3.SSS0.Px2.p1.2.m2.8.8.5.cmml">,</mo><mrow id="A1.SS3.SSS0.Px2.p1.2.m2.6.6.2.2" xref="A1.SS3.SSS0.Px2.p1.2.m2.6.6.2.2.cmml"><mn id="A1.SS3.SSS0.Px2.p1.2.m2.6.6.2.2.2" xref="A1.SS3.SSS0.Px2.p1.2.m2.6.6.2.2.2.cmml">2048</mn><mo id="A1.SS3.SSS0.Px2.p1.2.m2.6.6.2.2.1" lspace="0.500em" xref="A1.SS3.SSS0.Px2.p1.2.m2.6.6.2.2.1.cmml">â¢</mo><mrow id="A1.SS3.SSS0.Px2.p1.2.m2.6.6.2.2.3.2" xref="A1.SS3.SSS0.Px2.p1.2.m2.6.6.2.2.cmml"><mo id="A1.SS3.SSS0.Px2.p1.2.m2.6.6.2.2.3.2.1" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.2.m2.6.6.2.2.cmml">(</mo><mn id="A1.SS3.SSS0.Px2.p1.2.m2.2.2" xref="A1.SS3.SSS0.Px2.p1.2.m2.2.2.cmml">64</mn><mo id="A1.SS3.SSS0.Px2.p1.2.m2.6.6.2.2.3.2.2" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.2.m2.6.6.2.2.cmml">)</mo></mrow></mrow><mo id="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.7" xref="A1.SS3.SSS0.Px2.p1.2.m2.8.8.5.cmml">,</mo><mrow id="A1.SS3.SSS0.Px2.p1.2.m2.7.7.3.3" xref="A1.SS3.SSS0.Px2.p1.2.m2.7.7.3.3.cmml"><mn id="A1.SS3.SSS0.Px2.p1.2.m2.7.7.3.3.2" xref="A1.SS3.SSS0.Px2.p1.2.m2.7.7.3.3.2.cmml">4096</mn><mo id="A1.SS3.SSS0.Px2.p1.2.m2.7.7.3.3.1" lspace="0.500em" xref="A1.SS3.SSS0.Px2.p1.2.m2.7.7.3.3.1.cmml">â¢</mo><mrow id="A1.SS3.SSS0.Px2.p1.2.m2.7.7.3.3.3.2" xref="A1.SS3.SSS0.Px2.p1.2.m2.7.7.3.3.cmml"><mo id="A1.SS3.SSS0.Px2.p1.2.m2.7.7.3.3.3.2.1" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.2.m2.7.7.3.3.cmml">(</mo><mn id="A1.SS3.SSS0.Px2.p1.2.m2.3.3" xref="A1.SS3.SSS0.Px2.p1.2.m2.3.3.cmml">32</mn><mo id="A1.SS3.SSS0.Px2.p1.2.m2.7.7.3.3.3.2.2" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.2.m2.7.7.3.3.cmml">)</mo></mrow></mrow><mo id="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.8" xref="A1.SS3.SSS0.Px2.p1.2.m2.8.8.5.cmml">,</mo><mrow id="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.4" xref="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.4.cmml"><mn id="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.4.2" xref="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.4.2.cmml">8192</mn><mo id="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.4.1" lspace="0.500em" xref="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.4.1.cmml">â¢</mo><mrow id="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.4.3.2" xref="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.4.cmml"><mo id="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.4.3.2.1" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.4.cmml">(</mo><mn id="A1.SS3.SSS0.Px2.p1.2.m2.4.4" xref="A1.SS3.SSS0.Px2.p1.2.m2.4.4.cmml">16</mn><mo id="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.4.3.2.2" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.4.cmml">)</mo></mrow></mrow><mo id="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.9" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.2.m2.8.8.5.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px2.p1.2.m2.8b"><list id="A1.SS3.SSS0.Px2.p1.2.m2.8.8.5.cmml" xref="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4"><apply id="A1.SS3.SSS0.Px2.p1.2.m2.5.5.1.1.cmml" xref="A1.SS3.SSS0.Px2.p1.2.m2.5.5.1.1"><times id="A1.SS3.SSS0.Px2.p1.2.m2.5.5.1.1.1.cmml" xref="A1.SS3.SSS0.Px2.p1.2.m2.5.5.1.1.1"></times><cn id="A1.SS3.SSS0.Px2.p1.2.m2.5.5.1.1.2.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.2.m2.5.5.1.1.2">1024</cn><cn id="A1.SS3.SSS0.Px2.p1.2.m2.1.1.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.2.m2.1.1">128</cn></apply><apply id="A1.SS3.SSS0.Px2.p1.2.m2.6.6.2.2.cmml" xref="A1.SS3.SSS0.Px2.p1.2.m2.6.6.2.2"><times id="A1.SS3.SSS0.Px2.p1.2.m2.6.6.2.2.1.cmml" xref="A1.SS3.SSS0.Px2.p1.2.m2.6.6.2.2.1"></times><cn id="A1.SS3.SSS0.Px2.p1.2.m2.6.6.2.2.2.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.2.m2.6.6.2.2.2">2048</cn><cn id="A1.SS3.SSS0.Px2.p1.2.m2.2.2.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.2.m2.2.2">64</cn></apply><apply id="A1.SS3.SSS0.Px2.p1.2.m2.7.7.3.3.cmml" xref="A1.SS3.SSS0.Px2.p1.2.m2.7.7.3.3"><times id="A1.SS3.SSS0.Px2.p1.2.m2.7.7.3.3.1.cmml" xref="A1.SS3.SSS0.Px2.p1.2.m2.7.7.3.3.1"></times><cn id="A1.SS3.SSS0.Px2.p1.2.m2.7.7.3.3.2.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.2.m2.7.7.3.3.2">4096</cn><cn id="A1.SS3.SSS0.Px2.p1.2.m2.3.3.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.2.m2.3.3">32</cn></apply><apply id="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.4.cmml" xref="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.4"><times id="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.4.1.cmml" xref="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.4.1"></times><cn id="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.4.2.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.2.m2.8.8.4.4.2">8192</cn><cn id="A1.SS3.SSS0.Px2.p1.2.m2.4.4.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.2.m2.4.4">16</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px2.p1.2.m2.8c">{[1024\leavevmode\nobreak\ (128),2048\leavevmode\nobreak\ (64),4096\leavevmode%
\nobreak\ (32),8192\leavevmode\nobreak\ (16)]}</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px2.p1.2.m2.8d">[ 1024 ( 128 ) , 2048 ( 64 ) , 4096 ( 32 ) , 8192 ( 16 ) ]</annotation></semantics></math>.
For each context size we train five models (XS, S, M, L, and XL) on all three unimodal datasets, respectively.
Each model has a different combination of embedding dimension and number of layers for each different context size.
The XS models have embedding dimensions <math alttext="{[112,96,80,64]}" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px2.p1.3.m3.4"><semantics id="A1.SS3.SSS0.Px2.p1.3.m3.4a"><mrow id="A1.SS3.SSS0.Px2.p1.3.m3.4.5.2" xref="A1.SS3.SSS0.Px2.p1.3.m3.4.5.1.cmml"><mo id="A1.SS3.SSS0.Px2.p1.3.m3.4.5.2.1" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.3.m3.4.5.1.cmml">[</mo><mn id="A1.SS3.SSS0.Px2.p1.3.m3.1.1" xref="A1.SS3.SSS0.Px2.p1.3.m3.1.1.cmml">112</mn><mo id="A1.SS3.SSS0.Px2.p1.3.m3.4.5.2.2" xref="A1.SS3.SSS0.Px2.p1.3.m3.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.3.m3.2.2" xref="A1.SS3.SSS0.Px2.p1.3.m3.2.2.cmml">96</mn><mo id="A1.SS3.SSS0.Px2.p1.3.m3.4.5.2.3" xref="A1.SS3.SSS0.Px2.p1.3.m3.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.3.m3.3.3" xref="A1.SS3.SSS0.Px2.p1.3.m3.3.3.cmml">80</mn><mo id="A1.SS3.SSS0.Px2.p1.3.m3.4.5.2.4" xref="A1.SS3.SSS0.Px2.p1.3.m3.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.3.m3.4.4" xref="A1.SS3.SSS0.Px2.p1.3.m3.4.4.cmml">64</mn><mo id="A1.SS3.SSS0.Px2.p1.3.m3.4.5.2.5" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.3.m3.4.5.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px2.p1.3.m3.4b"><list id="A1.SS3.SSS0.Px2.p1.3.m3.4.5.1.cmml" xref="A1.SS3.SSS0.Px2.p1.3.m3.4.5.2"><cn id="A1.SS3.SSS0.Px2.p1.3.m3.1.1.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.3.m3.1.1">112</cn><cn id="A1.SS3.SSS0.Px2.p1.3.m3.2.2.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.3.m3.2.2">96</cn><cn id="A1.SS3.SSS0.Px2.p1.3.m3.3.3.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.3.m3.3.3">80</cn><cn id="A1.SS3.SSS0.Px2.p1.3.m3.4.4.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.3.m3.4.4">64</cn></list></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px2.p1.3.m3.4c">{[112,96,80,64]}</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px2.p1.3.m3.4d">[ 112 , 96 , 80 , 64 ]</annotation></semantics></math> and numbers of layers <math alttext="{[11,7,5,3]}" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px2.p1.4.m4.4"><semantics id="A1.SS3.SSS0.Px2.p1.4.m4.4a"><mrow id="A1.SS3.SSS0.Px2.p1.4.m4.4.5.2" xref="A1.SS3.SSS0.Px2.p1.4.m4.4.5.1.cmml"><mo id="A1.SS3.SSS0.Px2.p1.4.m4.4.5.2.1" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.4.m4.4.5.1.cmml">[</mo><mn id="A1.SS3.SSS0.Px2.p1.4.m4.1.1" xref="A1.SS3.SSS0.Px2.p1.4.m4.1.1.cmml">11</mn><mo id="A1.SS3.SSS0.Px2.p1.4.m4.4.5.2.2" xref="A1.SS3.SSS0.Px2.p1.4.m4.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.4.m4.2.2" xref="A1.SS3.SSS0.Px2.p1.4.m4.2.2.cmml">7</mn><mo id="A1.SS3.SSS0.Px2.p1.4.m4.4.5.2.3" xref="A1.SS3.SSS0.Px2.p1.4.m4.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.4.m4.3.3" xref="A1.SS3.SSS0.Px2.p1.4.m4.3.3.cmml">5</mn><mo id="A1.SS3.SSS0.Px2.p1.4.m4.4.5.2.4" xref="A1.SS3.SSS0.Px2.p1.4.m4.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.4.m4.4.4" xref="A1.SS3.SSS0.Px2.p1.4.m4.4.4.cmml">3</mn><mo id="A1.SS3.SSS0.Px2.p1.4.m4.4.5.2.5" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.4.m4.4.5.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px2.p1.4.m4.4b"><list id="A1.SS3.SSS0.Px2.p1.4.m4.4.5.1.cmml" xref="A1.SS3.SSS0.Px2.p1.4.m4.4.5.2"><cn id="A1.SS3.SSS0.Px2.p1.4.m4.1.1.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.4.m4.1.1">11</cn><cn id="A1.SS3.SSS0.Px2.p1.4.m4.2.2.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.4.m4.2.2">7</cn><cn id="A1.SS3.SSS0.Px2.p1.4.m4.3.3.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.4.m4.3.3">5</cn><cn id="A1.SS3.SSS0.Px2.p1.4.m4.4.4.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.4.m4.4.4">3</cn></list></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px2.p1.4.m4.4c">{[11,7,5,3]}</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px2.p1.4.m4.4d">[ 11 , 7 , 5 , 3 ]</annotation></semantics></math> for the different context sizes respectively (i.e., wider and deeper models for shorter contexts and more narrow and more shallow models for long context size).
The S models have embedding dimensions <math alttext="{[192,160,112,96]}" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px2.p1.5.m5.4"><semantics id="A1.SS3.SSS0.Px2.p1.5.m5.4a"><mrow id="A1.SS3.SSS0.Px2.p1.5.m5.4.5.2" xref="A1.SS3.SSS0.Px2.p1.5.m5.4.5.1.cmml"><mo id="A1.SS3.SSS0.Px2.p1.5.m5.4.5.2.1" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.5.m5.4.5.1.cmml">[</mo><mn id="A1.SS3.SSS0.Px2.p1.5.m5.1.1" xref="A1.SS3.SSS0.Px2.p1.5.m5.1.1.cmml">192</mn><mo id="A1.SS3.SSS0.Px2.p1.5.m5.4.5.2.2" xref="A1.SS3.SSS0.Px2.p1.5.m5.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.5.m5.2.2" xref="A1.SS3.SSS0.Px2.p1.5.m5.2.2.cmml">160</mn><mo id="A1.SS3.SSS0.Px2.p1.5.m5.4.5.2.3" xref="A1.SS3.SSS0.Px2.p1.5.m5.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.5.m5.3.3" xref="A1.SS3.SSS0.Px2.p1.5.m5.3.3.cmml">112</mn><mo id="A1.SS3.SSS0.Px2.p1.5.m5.4.5.2.4" xref="A1.SS3.SSS0.Px2.p1.5.m5.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.5.m5.4.4" xref="A1.SS3.SSS0.Px2.p1.5.m5.4.4.cmml">96</mn><mo id="A1.SS3.SSS0.Px2.p1.5.m5.4.5.2.5" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.5.m5.4.5.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px2.p1.5.m5.4b"><list id="A1.SS3.SSS0.Px2.p1.5.m5.4.5.1.cmml" xref="A1.SS3.SSS0.Px2.p1.5.m5.4.5.2"><cn id="A1.SS3.SSS0.Px2.p1.5.m5.1.1.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.5.m5.1.1">192</cn><cn id="A1.SS3.SSS0.Px2.p1.5.m5.2.2.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.5.m5.2.2">160</cn><cn id="A1.SS3.SSS0.Px2.p1.5.m5.3.3.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.5.m5.3.3">112</cn><cn id="A1.SS3.SSS0.Px2.p1.5.m5.4.4.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.5.m5.4.4">96</cn></list></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px2.p1.5.m5.4c">{[192,160,112,96]}</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px2.p1.5.m5.4d">[ 192 , 160 , 112 , 96 ]</annotation></semantics></math> and numbers of layers <math alttext="{[10,8,6,4]}" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px2.p1.6.m6.4"><semantics id="A1.SS3.SSS0.Px2.p1.6.m6.4a"><mrow id="A1.SS3.SSS0.Px2.p1.6.m6.4.5.2" xref="A1.SS3.SSS0.Px2.p1.6.m6.4.5.1.cmml"><mo id="A1.SS3.SSS0.Px2.p1.6.m6.4.5.2.1" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.6.m6.4.5.1.cmml">[</mo><mn id="A1.SS3.SSS0.Px2.p1.6.m6.1.1" xref="A1.SS3.SSS0.Px2.p1.6.m6.1.1.cmml">10</mn><mo id="A1.SS3.SSS0.Px2.p1.6.m6.4.5.2.2" xref="A1.SS3.SSS0.Px2.p1.6.m6.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.6.m6.2.2" xref="A1.SS3.SSS0.Px2.p1.6.m6.2.2.cmml">8</mn><mo id="A1.SS3.SSS0.Px2.p1.6.m6.4.5.2.3" xref="A1.SS3.SSS0.Px2.p1.6.m6.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.6.m6.3.3" xref="A1.SS3.SSS0.Px2.p1.6.m6.3.3.cmml">6</mn><mo id="A1.SS3.SSS0.Px2.p1.6.m6.4.5.2.4" xref="A1.SS3.SSS0.Px2.p1.6.m6.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.6.m6.4.4" xref="A1.SS3.SSS0.Px2.p1.6.m6.4.4.cmml">4</mn><mo id="A1.SS3.SSS0.Px2.p1.6.m6.4.5.2.5" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.6.m6.4.5.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px2.p1.6.m6.4b"><list id="A1.SS3.SSS0.Px2.p1.6.m6.4.5.1.cmml" xref="A1.SS3.SSS0.Px2.p1.6.m6.4.5.2"><cn id="A1.SS3.SSS0.Px2.p1.6.m6.1.1.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.6.m6.1.1">10</cn><cn id="A1.SS3.SSS0.Px2.p1.6.m6.2.2.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.6.m6.2.2">8</cn><cn id="A1.SS3.SSS0.Px2.p1.6.m6.3.3.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.6.m6.3.3">6</cn><cn id="A1.SS3.SSS0.Px2.p1.6.m6.4.4.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.6.m6.4.4">4</cn></list></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px2.p1.6.m6.4c">{[10,8,6,4]}</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px2.p1.6.m6.4d">[ 10 , 8 , 6 , 4 ]</annotation></semantics></math>.
The M models have embedding dimensions <math alttext="{[224,192,144,112]}" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px2.p1.7.m7.4"><semantics id="A1.SS3.SSS0.Px2.p1.7.m7.4a"><mrow id="A1.SS3.SSS0.Px2.p1.7.m7.4.5.2" xref="A1.SS3.SSS0.Px2.p1.7.m7.4.5.1.cmml"><mo id="A1.SS3.SSS0.Px2.p1.7.m7.4.5.2.1" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.7.m7.4.5.1.cmml">[</mo><mn id="A1.SS3.SSS0.Px2.p1.7.m7.1.1" xref="A1.SS3.SSS0.Px2.p1.7.m7.1.1.cmml">224</mn><mo id="A1.SS3.SSS0.Px2.p1.7.m7.4.5.2.2" xref="A1.SS3.SSS0.Px2.p1.7.m7.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.7.m7.2.2" xref="A1.SS3.SSS0.Px2.p1.7.m7.2.2.cmml">192</mn><mo id="A1.SS3.SSS0.Px2.p1.7.m7.4.5.2.3" xref="A1.SS3.SSS0.Px2.p1.7.m7.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.7.m7.3.3" xref="A1.SS3.SSS0.Px2.p1.7.m7.3.3.cmml">144</mn><mo id="A1.SS3.SSS0.Px2.p1.7.m7.4.5.2.4" xref="A1.SS3.SSS0.Px2.p1.7.m7.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.7.m7.4.4" xref="A1.SS3.SSS0.Px2.p1.7.m7.4.4.cmml">112</mn><mo id="A1.SS3.SSS0.Px2.p1.7.m7.4.5.2.5" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.7.m7.4.5.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px2.p1.7.m7.4b"><list id="A1.SS3.SSS0.Px2.p1.7.m7.4.5.1.cmml" xref="A1.SS3.SSS0.Px2.p1.7.m7.4.5.2"><cn id="A1.SS3.SSS0.Px2.p1.7.m7.1.1.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.7.m7.1.1">224</cn><cn id="A1.SS3.SSS0.Px2.p1.7.m7.2.2.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.7.m7.2.2">192</cn><cn id="A1.SS3.SSS0.Px2.p1.7.m7.3.3.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.7.m7.3.3">144</cn><cn id="A1.SS3.SSS0.Px2.p1.7.m7.4.4.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.7.m7.4.4">112</cn></list></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px2.p1.7.m7.4c">{[224,192,144,112]}</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px2.p1.7.m7.4d">[ 224 , 192 , 144 , 112 ]</annotation></semantics></math> and numbers of layers <math alttext="{[12,9,7,5]}" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px2.p1.8.m8.4"><semantics id="A1.SS3.SSS0.Px2.p1.8.m8.4a"><mrow id="A1.SS3.SSS0.Px2.p1.8.m8.4.5.2" xref="A1.SS3.SSS0.Px2.p1.8.m8.4.5.1.cmml"><mo id="A1.SS3.SSS0.Px2.p1.8.m8.4.5.2.1" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.8.m8.4.5.1.cmml">[</mo><mn id="A1.SS3.SSS0.Px2.p1.8.m8.1.1" xref="A1.SS3.SSS0.Px2.p1.8.m8.1.1.cmml">12</mn><mo id="A1.SS3.SSS0.Px2.p1.8.m8.4.5.2.2" xref="A1.SS3.SSS0.Px2.p1.8.m8.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.8.m8.2.2" xref="A1.SS3.SSS0.Px2.p1.8.m8.2.2.cmml">9</mn><mo id="A1.SS3.SSS0.Px2.p1.8.m8.4.5.2.3" xref="A1.SS3.SSS0.Px2.p1.8.m8.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.8.m8.3.3" xref="A1.SS3.SSS0.Px2.p1.8.m8.3.3.cmml">7</mn><mo id="A1.SS3.SSS0.Px2.p1.8.m8.4.5.2.4" xref="A1.SS3.SSS0.Px2.p1.8.m8.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.8.m8.4.4" xref="A1.SS3.SSS0.Px2.p1.8.m8.4.4.cmml">5</mn><mo id="A1.SS3.SSS0.Px2.p1.8.m8.4.5.2.5" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.8.m8.4.5.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px2.p1.8.m8.4b"><list id="A1.SS3.SSS0.Px2.p1.8.m8.4.5.1.cmml" xref="A1.SS3.SSS0.Px2.p1.8.m8.4.5.2"><cn id="A1.SS3.SSS0.Px2.p1.8.m8.1.1.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.8.m8.1.1">12</cn><cn id="A1.SS3.SSS0.Px2.p1.8.m8.2.2.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.8.m8.2.2">9</cn><cn id="A1.SS3.SSS0.Px2.p1.8.m8.3.3.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.8.m8.3.3">7</cn><cn id="A1.SS3.SSS0.Px2.p1.8.m8.4.4.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.8.m8.4.4">5</cn></list></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px2.p1.8.m8.4c">{[12,9,7,5]}</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px2.p1.8.m8.4d">[ 12 , 9 , 7 , 5 ]</annotation></semantics></math>.
The L models have embedding dimensions <math alttext="{[272,240,176,144]}" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px2.p1.9.m9.4"><semantics id="A1.SS3.SSS0.Px2.p1.9.m9.4a"><mrow id="A1.SS3.SSS0.Px2.p1.9.m9.4.5.2" xref="A1.SS3.SSS0.Px2.p1.9.m9.4.5.1.cmml"><mo id="A1.SS3.SSS0.Px2.p1.9.m9.4.5.2.1" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.9.m9.4.5.1.cmml">[</mo><mn id="A1.SS3.SSS0.Px2.p1.9.m9.1.1" xref="A1.SS3.SSS0.Px2.p1.9.m9.1.1.cmml">272</mn><mo id="A1.SS3.SSS0.Px2.p1.9.m9.4.5.2.2" xref="A1.SS3.SSS0.Px2.p1.9.m9.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.9.m9.2.2" xref="A1.SS3.SSS0.Px2.p1.9.m9.2.2.cmml">240</mn><mo id="A1.SS3.SSS0.Px2.p1.9.m9.4.5.2.3" xref="A1.SS3.SSS0.Px2.p1.9.m9.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.9.m9.3.3" xref="A1.SS3.SSS0.Px2.p1.9.m9.3.3.cmml">176</mn><mo id="A1.SS3.SSS0.Px2.p1.9.m9.4.5.2.4" xref="A1.SS3.SSS0.Px2.p1.9.m9.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.9.m9.4.4" xref="A1.SS3.SSS0.Px2.p1.9.m9.4.4.cmml">144</mn><mo id="A1.SS3.SSS0.Px2.p1.9.m9.4.5.2.5" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.9.m9.4.5.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px2.p1.9.m9.4b"><list id="A1.SS3.SSS0.Px2.p1.9.m9.4.5.1.cmml" xref="A1.SS3.SSS0.Px2.p1.9.m9.4.5.2"><cn id="A1.SS3.SSS0.Px2.p1.9.m9.1.1.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.9.m9.1.1">272</cn><cn id="A1.SS3.SSS0.Px2.p1.9.m9.2.2.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.9.m9.2.2">240</cn><cn id="A1.SS3.SSS0.Px2.p1.9.m9.3.3.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.9.m9.3.3">176</cn><cn id="A1.SS3.SSS0.Px2.p1.9.m9.4.4.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.9.m9.4.4">144</cn></list></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px2.p1.9.m9.4c">{[272,240,176,144]}</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px2.p1.9.m9.4d">[ 272 , 240 , 176 , 144 ]</annotation></semantics></math> and numbers of layers <math alttext="{[13,10,8,5]}" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px2.p1.10.m10.4"><semantics id="A1.SS3.SSS0.Px2.p1.10.m10.4a"><mrow id="A1.SS3.SSS0.Px2.p1.10.m10.4.5.2" xref="A1.SS3.SSS0.Px2.p1.10.m10.4.5.1.cmml"><mo id="A1.SS3.SSS0.Px2.p1.10.m10.4.5.2.1" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.10.m10.4.5.1.cmml">[</mo><mn id="A1.SS3.SSS0.Px2.p1.10.m10.1.1" xref="A1.SS3.SSS0.Px2.p1.10.m10.1.1.cmml">13</mn><mo id="A1.SS3.SSS0.Px2.p1.10.m10.4.5.2.2" xref="A1.SS3.SSS0.Px2.p1.10.m10.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.10.m10.2.2" xref="A1.SS3.SSS0.Px2.p1.10.m10.2.2.cmml">10</mn><mo id="A1.SS3.SSS0.Px2.p1.10.m10.4.5.2.3" xref="A1.SS3.SSS0.Px2.p1.10.m10.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.10.m10.3.3" xref="A1.SS3.SSS0.Px2.p1.10.m10.3.3.cmml">8</mn><mo id="A1.SS3.SSS0.Px2.p1.10.m10.4.5.2.4" xref="A1.SS3.SSS0.Px2.p1.10.m10.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.10.m10.4.4" xref="A1.SS3.SSS0.Px2.p1.10.m10.4.4.cmml">5</mn><mo id="A1.SS3.SSS0.Px2.p1.10.m10.4.5.2.5" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.10.m10.4.5.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px2.p1.10.m10.4b"><list id="A1.SS3.SSS0.Px2.p1.10.m10.4.5.1.cmml" xref="A1.SS3.SSS0.Px2.p1.10.m10.4.5.2"><cn id="A1.SS3.SSS0.Px2.p1.10.m10.1.1.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.10.m10.1.1">13</cn><cn id="A1.SS3.SSS0.Px2.p1.10.m10.2.2.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.10.m10.2.2">10</cn><cn id="A1.SS3.SSS0.Px2.p1.10.m10.3.3.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.10.m10.3.3">8</cn><cn id="A1.SS3.SSS0.Px2.p1.10.m10.4.4.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.10.m10.4.4">5</cn></list></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px2.p1.10.m10.4c">{[13,10,8,5]}</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px2.p1.10.m10.4d">[ 13 , 10 , 8 , 5 ]</annotation></semantics></math>.
The XL models have embedding dimensions <math alttext="{[320,304,240,160]}" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px2.p1.11.m11.4"><semantics id="A1.SS3.SSS0.Px2.p1.11.m11.4a"><mrow id="A1.SS3.SSS0.Px2.p1.11.m11.4.5.2" xref="A1.SS3.SSS0.Px2.p1.11.m11.4.5.1.cmml"><mo id="A1.SS3.SSS0.Px2.p1.11.m11.4.5.2.1" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.11.m11.4.5.1.cmml">[</mo><mn id="A1.SS3.SSS0.Px2.p1.11.m11.1.1" xref="A1.SS3.SSS0.Px2.p1.11.m11.1.1.cmml">320</mn><mo id="A1.SS3.SSS0.Px2.p1.11.m11.4.5.2.2" xref="A1.SS3.SSS0.Px2.p1.11.m11.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.11.m11.2.2" xref="A1.SS3.SSS0.Px2.p1.11.m11.2.2.cmml">304</mn><mo id="A1.SS3.SSS0.Px2.p1.11.m11.4.5.2.3" xref="A1.SS3.SSS0.Px2.p1.11.m11.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.11.m11.3.3" xref="A1.SS3.SSS0.Px2.p1.11.m11.3.3.cmml">240</mn><mo id="A1.SS3.SSS0.Px2.p1.11.m11.4.5.2.4" xref="A1.SS3.SSS0.Px2.p1.11.m11.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.11.m11.4.4" xref="A1.SS3.SSS0.Px2.p1.11.m11.4.4.cmml">160</mn><mo id="A1.SS3.SSS0.Px2.p1.11.m11.4.5.2.5" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.11.m11.4.5.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px2.p1.11.m11.4b"><list id="A1.SS3.SSS0.Px2.p1.11.m11.4.5.1.cmml" xref="A1.SS3.SSS0.Px2.p1.11.m11.4.5.2"><cn id="A1.SS3.SSS0.Px2.p1.11.m11.1.1.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.11.m11.1.1">320</cn><cn id="A1.SS3.SSS0.Px2.p1.11.m11.2.2.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.11.m11.2.2">304</cn><cn id="A1.SS3.SSS0.Px2.p1.11.m11.3.3.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.11.m11.3.3">240</cn><cn id="A1.SS3.SSS0.Px2.p1.11.m11.4.4.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.11.m11.4.4">160</cn></list></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px2.p1.11.m11.4c">{[320,304,240,160]}</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px2.p1.11.m11.4d">[ 320 , 304 , 240 , 160 ]</annotation></semantics></math> and numbers of layers <math alttext="{[12,9,7,6]}" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px2.p1.12.m12.4"><semantics id="A1.SS3.SSS0.Px2.p1.12.m12.4a"><mrow id="A1.SS3.SSS0.Px2.p1.12.m12.4.5.2" xref="A1.SS3.SSS0.Px2.p1.12.m12.4.5.1.cmml"><mo id="A1.SS3.SSS0.Px2.p1.12.m12.4.5.2.1" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.12.m12.4.5.1.cmml">[</mo><mn id="A1.SS3.SSS0.Px2.p1.12.m12.1.1" xref="A1.SS3.SSS0.Px2.p1.12.m12.1.1.cmml">12</mn><mo id="A1.SS3.SSS0.Px2.p1.12.m12.4.5.2.2" xref="A1.SS3.SSS0.Px2.p1.12.m12.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.12.m12.2.2" xref="A1.SS3.SSS0.Px2.p1.12.m12.2.2.cmml">9</mn><mo id="A1.SS3.SSS0.Px2.p1.12.m12.4.5.2.3" xref="A1.SS3.SSS0.Px2.p1.12.m12.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.12.m12.3.3" xref="A1.SS3.SSS0.Px2.p1.12.m12.3.3.cmml">7</mn><mo id="A1.SS3.SSS0.Px2.p1.12.m12.4.5.2.4" xref="A1.SS3.SSS0.Px2.p1.12.m12.4.5.1.cmml">,</mo><mn id="A1.SS3.SSS0.Px2.p1.12.m12.4.4" xref="A1.SS3.SSS0.Px2.p1.12.m12.4.4.cmml">6</mn><mo id="A1.SS3.SSS0.Px2.p1.12.m12.4.5.2.5" stretchy="false" xref="A1.SS3.SSS0.Px2.p1.12.m12.4.5.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS0.Px2.p1.12.m12.4b"><list id="A1.SS3.SSS0.Px2.p1.12.m12.4.5.1.cmml" xref="A1.SS3.SSS0.Px2.p1.12.m12.4.5.2"><cn id="A1.SS3.SSS0.Px2.p1.12.m12.1.1.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.12.m12.1.1">12</cn><cn id="A1.SS3.SSS0.Px2.p1.12.m12.2.2.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.12.m12.2.2">9</cn><cn id="A1.SS3.SSS0.Px2.p1.12.m12.3.3.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.12.m12.3.3">7</cn><cn id="A1.SS3.SSS0.Px2.p1.12.m12.4.4.cmml" type="integer" xref="A1.SS3.SSS0.Px2.p1.12.m12.4.4">6</cn></list></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS0.Px2.p1.12.m12.4c">{[12,9,7,6]}</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS0.Px2.p1.12.m12.4d">[ 12 , 9 , 7 , 6 ]</annotation></semantics></math>.
The main goal with these settings is to create families of models that have roughly the same demand in terms of FLOPS (iso-FLOPS) but very different trade-offs in terms of model- and context size.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>Computational Resources</h3>
<div class="ltx_para" id="A1.SS4.p1">
<p class="ltx_p" id="A1.SS4.p1.1">We trained every model on 16 NVIDIA A100 GPUs from our internal cluster.
We trained 315 models in total, yielding a computational footprint of 5040 A100s.
We ran Bellardâ€™s code on an NVIDIA GeForce RTX 4090 GPU with a 24-core Intel i9-13900KF CPU @ 3Ghz.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A2" lang="en">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Additional Results</h2>
<section class="ltx_subsection" id="A2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Compression Ratios</h3>
<figure class="ltx_table" id="A2.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table A1: </span>
Best compression ratios for each compressor.
This table shows the same results as <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.F2" title="In 5 Results â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">2</span></a> but as precise numerical values to facilitate detailed comparison.
</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A2.T1.1" style="width:390.3pt;height:155.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-8.7pt,3.5pt) scale(0.957125589040112,0.957125589040112) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A2.T1.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T1.1.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="A2.T1.1.1.1.1.1"></th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="7" id="A2.T1.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A2.T1.1.1.1.1.2.1">Out-of-Distribution Compression Ratio</span></td>
</tr>
<tr class="ltx_tr" id="A2.T1.1.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T1.1.1.2.2.1"><span class="ltx_text ltx_font_bold" id="A2.T1.1.1.2.2.1.1">Evaluation Modality</span></th>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T1.1.1.2.2.2"><span class="ltx_text ltx_font_bold" id="A2.T1.1.1.2.2.2.1">Ours</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T1.1.1.2.2.3"><span class="ltx_text ltx_font_bold" id="A2.T1.1.1.2.2.3.1">Bellard</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T1.1.1.2.2.4"><span class="ltx_text ltx_font_bold" id="A2.T1.1.1.2.2.4.1">gzip</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T1.1.1.2.2.5"><span class="ltx_text ltx_font_bold" id="A2.T1.1.1.2.2.5.1">LZMA2</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T1.1.1.2.2.6"><span class="ltx_text ltx_font_bold" id="A2.T1.1.1.2.2.6.1">FLAC</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T1.1.1.2.2.7"><span class="ltx_text ltx_font_bold" id="A2.T1.1.1.2.2.7.1">PNG</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t" id="A2.T1.1.1.2.2.8"><span class="ltx_text ltx_font_bold" id="A2.T1.1.1.2.2.8.1">JPEGÂ 2000</span></td>
</tr>
<tr class="ltx_tr" id="A2.T1.1.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T1.1.1.3.3.1">Audio</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T1.1.1.3.3.2"><span class="ltx_text ltx_font_bold" id="A2.T1.1.1.3.3.2.1">0.487</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T1.1.1.3.3.3">0.509</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T1.1.1.3.3.4">0.813</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T1.1.1.3.3.5">0.699</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T1.1.1.3.3.6">0.538</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T1.1.1.3.3.7">-</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t" id="A2.T1.1.1.3.3.8">-</td>
</tr>
<tr class="ltx_tr" id="A2.T1.1.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T1.1.1.4.4.1">Image</th>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.4.4.2">0.285</td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.4.4.3"><span class="ltx_text ltx_font_bold" id="A2.T1.1.1.4.4.3.1">0.281</span></td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.4.4.4">0.698</td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.4.4.5">0.545</td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.4.4.6">-</td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.4.4.7">0.426</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A2.T1.1.1.4.4.8">0.390</td>
</tr>
<tr class="ltx_tr" id="A2.T1.1.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T1.1.1.5.5.1">Text</th>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.5.5.2">0.217</td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.5.5.3"><span class="ltx_text ltx_font_bold" id="A2.T1.1.1.5.5.3.1">0.204</span></td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.5.5.4">0.394</td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.5.5.5">0.286</td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.5.5.6">-</td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.5.5.7">-</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A2.T1.1.1.5.5.8">-</td>
</tr>
<tr class="ltx_tr" id="A2.T1.1.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T1.1.1.6.6.1">Audio + Image</th>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.6.6.2"><span class="ltx_text ltx_font_bold" id="A2.T1.1.1.6.6.2.1">0.393</span></td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.6.6.3">0.395</td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.6.6.4">0.756</td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.6.6.5">0.622</td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.6.6.6">-</td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.6.6.7">-</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A2.T1.1.1.6.6.8">-</td>
</tr>
<tr class="ltx_tr" id="A2.T1.1.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T1.1.1.7.7.1">Audio + Text</th>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.7.7.2">0.362</td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.7.7.3"><span class="ltx_text ltx_font_bold" id="A2.T1.1.1.7.7.3.1">0.357</span></td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.7.7.4">0.604</td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.7.7.5">0.493</td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.7.7.6">-</td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.7.7.7">-</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A2.T1.1.1.7.7.8">-</td>
</tr>
<tr class="ltx_tr" id="A2.T1.1.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T1.1.1.8.8.1">Image + Text</th>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.8.8.2">0.270</td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.8.8.3"><span class="ltx_text ltx_font_bold" id="A2.T1.1.1.8.8.3.1">0.243</span></td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.8.8.4">0.546</td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.8.8.5">0.415</td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.8.8.6">-</td>
<td class="ltx_td ltx_align_right" id="A2.T1.1.1.8.8.7">-</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A2.T1.1.1.8.8.8">-</td>
</tr>
<tr class="ltx_tr" id="A2.T1.1.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A2.T1.1.1.9.9.1">Audio + Image + Text</th>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A2.T1.1.1.9.9.2">0.349</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A2.T1.1.1.9.9.3"><span class="ltx_text ltx_font_bold" id="A2.T1.1.1.9.9.3.1">0.331</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A2.T1.1.1.9.9.4">0.635</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A2.T1.1.1.9.9.5">0.510</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A2.T1.1.1.9.9.6">-</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A2.T1.1.1.9.9.7">-</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb" id="A2.T1.1.1.9.9.8">-</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para" id="A2.SS1.p1">
<p class="ltx_p" id="A2.SS1.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A2.T1" title="In B.1 Compression Ratios â€£ Appendix B Additional Results â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">A1</span></a> shows the optimal compression ratios that each of the compressors achieve on all of the different evaluation modalities (note that all evaluations are on out-of-distribution data).
The same values as shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.F2" title="In 5 Results â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">2</span></a> in the main paper and given here as precise numerical values for completeness.</p>
</div>
</section>
<section class="ltx_subsection" id="A2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Running Times</h3>
<figure class="ltx_table" id="A2.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table A2: </span>Running times to compress <math alttext="1" class="ltx_Math" display="inline" id="A2.T2.2.m1.1"><semantics id="A2.T2.2.m1.1b"><mn id="A2.T2.2.m1.1.1" xref="A2.T2.2.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="A2.T2.2.m1.1c"><cn id="A2.T2.2.m1.1.1.cmml" type="integer" xref="A2.T2.2.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T2.2.m1.1d">1</annotation><annotation encoding="application/x-llamapun" id="A2.T2.2.m1.1e">1</annotation></semantics></math>GB of data for all compressors used in our study. Note that we use the best model per modality, which have different sizes and thus different running times.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A2.T2.3" style="width:390.3pt;height:88pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-4.5pt,1.0pt) scale(0.977333018201536,0.977333018201536) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A2.T2.3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A2.T2.3.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="A2.T2.3.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="7" id="A2.T2.3.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A2.T2.3.1.1.1.2.1">Running Times [s]</span></th>
</tr>
<tr class="ltx_tr" id="A2.T2.3.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" id="A2.T2.3.1.2.2.1"><span class="ltx_text ltx_font_bold" id="A2.T2.3.1.2.2.1.1">Evaluation Modality</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A2.T2.3.1.2.2.2"><span class="ltx_text ltx_font_bold" id="A2.T2.3.1.2.2.2.1">Ours</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A2.T2.3.1.2.2.3"><span class="ltx_text ltx_font_bold" id="A2.T2.3.1.2.2.3.1">Bellard</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A2.T2.3.1.2.2.4"><span class="ltx_text ltx_font_bold" id="A2.T2.3.1.2.2.4.1">gzip</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A2.T2.3.1.2.2.5"><span class="ltx_text ltx_font_bold" id="A2.T2.3.1.2.2.5.1">LZMA2</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A2.T2.3.1.2.2.6"><span class="ltx_text ltx_font_bold" id="A2.T2.3.1.2.2.6.1">FLAC</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A2.T2.3.1.2.2.7"><span class="ltx_text ltx_font_bold" id="A2.T2.3.1.2.2.7.1">PNG</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A2.T2.3.1.2.2.8"><span class="ltx_text ltx_font_bold" id="A2.T2.3.1.2.2.8.1">JPEGÂ 2000</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T2.3.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T2.3.1.3.1.1">Audio</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T2.3.1.3.1.2"><span class="ltx_text ltx_number" id="A2.T2.3.1.3.1.2.1">305â€‰609</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T2.3.1.3.1.3"><span class="ltx_text ltx_number" id="A2.T2.3.1.3.1.3.1">101â€‰178</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T2.3.1.3.1.4">55</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T2.3.1.3.1.5">524</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T2.3.1.3.1.6">169</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T2.3.1.3.1.7">-</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t" id="A2.T2.3.1.3.1.8">-</td>
</tr>
<tr class="ltx_tr" id="A2.T2.3.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T2.3.1.4.2.1">Image</th>
<td class="ltx_td ltx_align_right" id="A2.T2.3.1.4.2.2"><span class="ltx_text ltx_number" id="A2.T2.3.1.4.2.2.1">222â€‰065</span></td>
<td class="ltx_td ltx_align_right" id="A2.T2.3.1.4.2.3"><span class="ltx_text ltx_number" id="A2.T2.3.1.4.2.3.1">103â€‰391</span></td>
<td class="ltx_td ltx_align_right" id="A2.T2.3.1.4.2.4">47</td>
<td class="ltx_td ltx_align_right" id="A2.T2.3.1.4.2.5">436</td>
<td class="ltx_td ltx_align_right" id="A2.T2.3.1.4.2.6">174</td>
<td class="ltx_td ltx_align_right" id="A2.T2.3.1.4.2.7">495</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A2.T2.3.1.4.2.8">99</td>
</tr>
<tr class="ltx_tr" id="A2.T2.3.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A2.T2.3.1.5.3.1">Text</th>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A2.T2.3.1.5.3.2"><span class="ltx_text ltx_number" id="A2.T2.3.1.5.3.2.1">452â€‰355</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A2.T2.3.1.5.3.3"><span class="ltx_text ltx_number" id="A2.T2.3.1.5.3.3.1">100â€‰657</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A2.T2.3.1.5.3.4">102</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A2.T2.3.1.5.3.5">881</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A2.T2.3.1.5.3.6">184</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A2.T2.3.1.5.3.7">-</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb" id="A2.T2.3.1.5.3.8">-</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para" id="A2.SS2.p1">
<p class="ltx_p" id="A2.SS2.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A2.T2" title="In B.2 Running Times â€£ Appendix B Additional Results â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">A2</span></a> shows the wall-clock running times in seconds for compressing <math alttext="1" class="ltx_Math" display="inline" id="A2.SS2.p1.1.m1.1"><semantics id="A2.SS2.p1.1.m1.1a"><mn id="A2.SS2.p1.1.m1.1.1" xref="A2.SS2.p1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="A2.SS2.p1.1.m1.1b"><cn id="A2.SS2.p1.1.m1.1.1.cmml" type="integer" xref="A2.SS2.p1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p1.1.m1.1c">1</annotation><annotation encoding="application/x-llamapun" id="A2.SS2.p1.1.m1.1d">1</annotation></semantics></math>GB of data from each of the three modalities for our models, Bellardâ€™s online adaptive transformer <cite class="ltx_cite ltx_citemacro_citep">(Bellard, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#bib.bib4" title="">2021</a>)</cite>, and the standard compression algorithms used in our work.
As the table clearly shows, our models and Bellardâ€™s model are orders of magnitudes slower (let alone the increased computational demand and GPU requirements).
Note that running times for our models differ, because we pick the best model per modality, which are models of different sizes.</p>
</div>
</section>
<section class="ltx_subsection" id="A2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.3 </span>Compressing Model Parameters</h3>
<figure class="ltx_table" id="A2.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table A3: </span>
Compression ratios for model parameters.
We losslessly compress the trained model parameters with standard compressors.
For each modality we choose the best-performing model.
As is shown, the maximal compression is <math alttext="11\%" class="ltx_Math" display="inline" id="A2.T3.2.m1.1"><semantics id="A2.T3.2.m1.1b"><mrow id="A2.T3.2.m1.1.1" xref="A2.T3.2.m1.1.1.cmml"><mn id="A2.T3.2.m1.1.1.2" xref="A2.T3.2.m1.1.1.2.cmml">11</mn><mo id="A2.T3.2.m1.1.1.1" xref="A2.T3.2.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.T3.2.m1.1c"><apply id="A2.T3.2.m1.1.1.cmml" xref="A2.T3.2.m1.1.1"><csymbol cd="latexml" id="A2.T3.2.m1.1.1.1.cmml" xref="A2.T3.2.m1.1.1.1">percent</csymbol><cn id="A2.T3.2.m1.1.1.2.cmml" type="integer" xref="A2.T3.2.m1.1.1.2">11</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.2.m1.1d">11\%</annotation><annotation encoding="application/x-llamapun" id="A2.T3.2.m1.1e">11 %</annotation></semantics></math>, which would affect the overall compression ratio on the corresponding evaluation data only very marginally.
</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A2.T3.3" style="width:195.1pt;height:120.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(10.1pt,-6.2pt) scale(1.11501282825398,1.11501282825398) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A2.T3.3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A2.T3.3.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="A2.T3.3.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="A2.T3.3.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A2.T3.3.1.1.1.2.1">Model Parameter</span></th>
</tr>
<tr class="ltx_tr" id="A2.T3.3.1.2.2">
<th class="ltx_td ltx_th ltx_th_row" id="A2.T3.3.1.2.2.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="2" id="A2.T3.3.1.2.2.2"><span class="ltx_text ltx_font_bold" id="A2.T3.3.1.2.2.2.1">Compression Ratio</span></th>
</tr>
<tr class="ltx_tr" id="A2.T3.3.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" id="A2.T3.3.1.3.3.1"><span class="ltx_text ltx_font_bold" id="A2.T3.3.1.3.3.1.1">Evaluation Modality</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A2.T3.3.1.3.3.2"><span class="ltx_text ltx_font_bold" id="A2.T3.3.1.3.3.2.1">gzip</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A2.T3.3.1.3.3.3"><span class="ltx_text ltx_font_bold" id="A2.T3.3.1.3.3.3.1">LZMA2</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T3.3.1.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T3.3.1.4.1.1">Audio</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T3.3.1.4.1.2">0.93</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t" id="A2.T3.3.1.4.1.3">0.90</td>
</tr>
<tr class="ltx_tr" id="A2.T3.3.1.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T3.3.1.5.2.1">Image</th>
<td class="ltx_td ltx_align_right" id="A2.T3.3.1.5.2.2">0.93</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A2.T3.3.1.5.2.3">0.90</td>
</tr>
<tr class="ltx_tr" id="A2.T3.3.1.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A2.T3.3.1.6.3.1">Text</th>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A2.T3.3.1.6.3.2">0.92</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb" id="A2.T3.3.1.6.3.3">0.89</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para" id="A2.SS3.p1">
<p class="ltx_p" id="A2.SS3.p1.1">Throughout our paper we report compression rates that take uncompressed model parameters into account. As discussed in the main paper, compression ratios could be improved by also compressing model parameters. However, as <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A2.T3" title="In B.3 Compressing Model Parameters â€£ Appendix B Additional Results â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">A3</span></a> shows, naively compressing model parameters with a lossless compressor does not lead to much compression, which would translate into very marginal gains on the overall compression ratio. While it is possible to investigate more sophisticated compression schemes, in particular lossy compression of network weights (though this opens the problem of having to solve a trade-off between increasing weight compression and maintaining compression performance), this is beyond the scope of our paper. Accordingly, our compression rates can be understood as (somewhat) conservative estimates that give (in our case fairly tight) upper bounds on compression performance. The topic of compressing network weights to achieve competitive compression ratios would be of greater significance in a regime where models are significantly larger than ours (but the evaluation data stays roughly at the same size).</p>
</div>
</section>
<section class="ltx_subsection" id="A2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.4 </span>Scaling Analysis for Multimodal Training</h3>
<figure class="ltx_figure" id="A2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="259" id="A2.F1.1.g1" src="x7.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure A1: </span>Similar to <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.F4" title="In What You See Is What You Get â€£ 5 Results â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">4</span></a> in the main paper, but here the models are trained on a uniform mixture over all three modalities (<math alttext="55" class="ltx_Math" display="inline" id="A2.F1.9.m1.1"><semantics id="A2.F1.9.m1.1b"><mn id="A2.F1.9.m1.1.1" xref="A2.F1.9.m1.1.1.cmml">55</mn><annotation-xml encoding="MathML-Content" id="A2.F1.9.m1.1c"><cn id="A2.F1.9.m1.1.1.cmml" type="integer" xref="A2.F1.9.m1.1.1">55</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.F1.9.m1.1d">55</annotation><annotation encoding="application/x-llamapun" id="A2.F1.9.m1.1e">55</annotation></semantics></math>GB per modality).
The plot shows compression performance evaluated on the unimodal datasets as training progresses for various model- and training set sizes (models are different colors, each line is a different training set size of either <math alttext="20\%" class="ltx_Math" display="inline" id="A2.F1.10.m2.1"><semantics id="A2.F1.10.m2.1b"><mrow id="A2.F1.10.m2.1.1" xref="A2.F1.10.m2.1.1.cmml"><mn id="A2.F1.10.m2.1.1.2" xref="A2.F1.10.m2.1.1.2.cmml">20</mn><mo id="A2.F1.10.m2.1.1.1" xref="A2.F1.10.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.F1.10.m2.1c"><apply id="A2.F1.10.m2.1.1.cmml" xref="A2.F1.10.m2.1.1"><csymbol cd="latexml" id="A2.F1.10.m2.1.1.1.cmml" xref="A2.F1.10.m2.1.1.1">percent</csymbol><cn id="A2.F1.10.m2.1.1.2.cmml" type="integer" xref="A2.F1.10.m2.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.F1.10.m2.1d">20\%</annotation><annotation encoding="application/x-llamapun" id="A2.F1.10.m2.1e">20 %</annotation></semantics></math>, <math alttext="40\%" class="ltx_Math" display="inline" id="A2.F1.11.m3.1"><semantics id="A2.F1.11.m3.1b"><mrow id="A2.F1.11.m3.1.1" xref="A2.F1.11.m3.1.1.cmml"><mn id="A2.F1.11.m3.1.1.2" xref="A2.F1.11.m3.1.1.2.cmml">40</mn><mo id="A2.F1.11.m3.1.1.1" xref="A2.F1.11.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.F1.11.m3.1c"><apply id="A2.F1.11.m3.1.1.cmml" xref="A2.F1.11.m3.1.1"><csymbol cd="latexml" id="A2.F1.11.m3.1.1.1.cmml" xref="A2.F1.11.m3.1.1.1">percent</csymbol><cn id="A2.F1.11.m3.1.1.2.cmml" type="integer" xref="A2.F1.11.m3.1.1.2">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.F1.11.m3.1d">40\%</annotation><annotation encoding="application/x-llamapun" id="A2.F1.11.m3.1e">40 %</annotation></semantics></math>, <math alttext="60\%" class="ltx_Math" display="inline" id="A2.F1.12.m4.1"><semantics id="A2.F1.12.m4.1b"><mrow id="A2.F1.12.m4.1.1" xref="A2.F1.12.m4.1.1.cmml"><mn id="A2.F1.12.m4.1.1.2" xref="A2.F1.12.m4.1.1.2.cmml">60</mn><mo id="A2.F1.12.m4.1.1.1" xref="A2.F1.12.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.F1.12.m4.1c"><apply id="A2.F1.12.m4.1.1.cmml" xref="A2.F1.12.m4.1.1"><csymbol cd="latexml" id="A2.F1.12.m4.1.1.1.cmml" xref="A2.F1.12.m4.1.1.1">percent</csymbol><cn id="A2.F1.12.m4.1.1.2.cmml" type="integer" xref="A2.F1.12.m4.1.1.2">60</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.F1.12.m4.1d">60\%</annotation><annotation encoding="application/x-llamapun" id="A2.F1.12.m4.1e">60 %</annotation></semantics></math>, <math alttext="80\%" class="ltx_Math" display="inline" id="A2.F1.13.m5.1"><semantics id="A2.F1.13.m5.1b"><mrow id="A2.F1.13.m5.1.1" xref="A2.F1.13.m5.1.1.cmml"><mn id="A2.F1.13.m5.1.1.2" xref="A2.F1.13.m5.1.1.2.cmml">80</mn><mo id="A2.F1.13.m5.1.1.1" xref="A2.F1.13.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.F1.13.m5.1c"><apply id="A2.F1.13.m5.1.1.cmml" xref="A2.F1.13.m5.1.1"><csymbol cd="latexml" id="A2.F1.13.m5.1.1.1.cmml" xref="A2.F1.13.m5.1.1.1">percent</csymbol><cn id="A2.F1.13.m5.1.1.2.cmml" type="integer" xref="A2.F1.13.m5.1.1.2">80</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.F1.13.m5.1d">80\%</annotation><annotation encoding="application/x-llamapun" id="A2.F1.13.m5.1e">80 %</annotation></semantics></math>, and <math alttext="100\%" class="ltx_Math" display="inline" id="A2.F1.14.m6.1"><semantics id="A2.F1.14.m6.1b"><mrow id="A2.F1.14.m6.1.1" xref="A2.F1.14.m6.1.1.cmml"><mn id="A2.F1.14.m6.1.1.2" xref="A2.F1.14.m6.1.1.2.cmml">100</mn><mo id="A2.F1.14.m6.1.1.1" xref="A2.F1.14.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.F1.14.m6.1c"><apply id="A2.F1.14.m6.1.1.cmml" xref="A2.F1.14.m6.1.1"><csymbol cd="latexml" id="A2.F1.14.m6.1.1.1.cmml" xref="A2.F1.14.m6.1.1.1">percent</csymbol><cn id="A2.F1.14.m6.1.1.2.cmml" type="integer" xref="A2.F1.14.m6.1.1.2">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.F1.14.m6.1d">100\%</annotation><annotation encoding="application/x-llamapun" id="A2.F1.14.m6.1e">100 %</annotation></semantics></math>).
We always train for <math alttext="2" class="ltx_Math" display="inline" id="A2.F1.15.m7.1"><semantics id="A2.F1.15.m7.1b"><mn id="A2.F1.15.m7.1.1" xref="A2.F1.15.m7.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="A2.F1.15.m7.1c"><cn id="A2.F1.15.m7.1.1.cmml" type="integer" xref="A2.F1.15.m7.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.F1.15.m7.1d">2</annotation><annotation encoding="application/x-llamapun" id="A2.F1.15.m7.1e">2</annotation></semantics></math> epochs, regardless of dataset size, i.e., smaller datasets require fewer FLOPS.
In contrast to <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.F4" title="In What You See Is What You Get â€£ 5 Results â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">4</span></a>, where models are trained on unimodal data, we observe no overfitting, e.g., on images, even for the largest models tested.
Note, however, that the compression ratios are slightly worse than for unimodal training, which is in line with our other expriments that show small losses when training on multimodal data.
</figcaption>
</figure>
<div class="ltx_para" id="A2.SS4.p1">
<p class="ltx_p" id="A2.SS4.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A2.F1" title="In B.4 Scaling Analysis for Multimodal Training â€£ Appendix B Additional Results â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">A1</span></a> shows the results of simultaneously scaling dataset- and model size across training.
In contrast to the similar <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.F4" title="In What You See Is What You Get â€£ 5 Results â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">4</span></a> in the main paper, where models were trained on unimodal data, <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#A2.F1" title="In B.4 Scaling Analysis for Multimodal Training â€£ Appendix B Additional Results â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">A1</span></a> shows models trained on multimodal data (i.e., the uniform mixture across all three modalities, with <math alttext="55" class="ltx_Math" display="inline" id="A2.SS4.p1.1.m1.1"><semantics id="A2.SS4.p1.1.m1.1a"><mn id="A2.SS4.p1.1.m1.1.1" xref="A2.SS4.p1.1.m1.1.1.cmml">55</mn><annotation-xml encoding="MathML-Content" id="A2.SS4.p1.1.m1.1b"><cn id="A2.SS4.p1.1.m1.1.1.cmml" type="integer" xref="A2.SS4.p1.1.m1.1.1">55</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.SS4.p1.1.m1.1c">55</annotation><annotation encoding="application/x-llamapun" id="A2.SS4.p1.1.m1.1d">55</annotation></semantics></math>GB per modality).
The multimodal training mixture acts as a regularizer, which can clearly be seen by the lack of overfitting of the largest models on images.
Compare this against the unimodal training results in <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.F4" title="In What You See Is What You Get â€£ 5 Results â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">4</span></a> where overfitting can be observed.
In line with our other main results in <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.F2" title="In 5 Results â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">2</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.05078v1#S5.F3" title="In Small Transformers Can Be Domain-General Compressors â€£ 5 Results â€£ Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">3</span></a>, the overall compression ratios are slightly worse for the models trained on multimodal data compared to unimodal training.</p>
</div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Oct  7 14:25:00 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
