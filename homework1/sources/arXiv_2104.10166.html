<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2104.10166] Evaluating the Immediate Applicability of Pose Estimation for Sign Language Recognition</title><meta property="og:description" content="Sign languages are visual languages produced by the movement of the hands, face, and body.
In this paper, we evaluate representations based on skeleton poses, as these are explainable, person-independent, privacy-prese‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Evaluating the Immediate Applicability of Pose Estimation for Sign Language Recognition">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Evaluating the Immediate Applicability of Pose Estimation for Sign Language Recognition">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2104.10166">

<!--Generated on Sat Mar  9 02:59:33 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Evaluating the Immediate Applicability of
<br class="ltx_break">Pose Estimation for Sign Language Recognition</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Amit Moryossef<sup id="id1.1.id1" class="ltx_sup">1,2</sup> ‚ÄÑ‚ÄÑ
Ioannis Tsochantaridis<sup id="id2.2.id2" class="ltx_sup">2</sup> ‚ÄÑ‚ÄÑ 
<br class="ltx_break">Joe Dinn<sup id="id3.3.id3" class="ltx_sup">3</sup> ‚ÄÑ‚ÄÑ
Necati Cihan Camg√∂z<sup id="id4.4.id4" class="ltx_sup">3</sup> ‚ÄÑ‚ÄÑ
Richard Bowden<sup id="id5.5.id5" class="ltx_sup">3</sup> ‚ÄÑ‚ÄÑ
Tao Jiang<sup id="id6.6.id6" class="ltx_sup">3</sup> 
<br class="ltx_break">Annette Rios<sup id="id7.7.id7" class="ltx_sup">4</sup> ‚ÄÑ‚ÄÑ
Mathias M√ºller<sup id="id8.8.id8" class="ltx_sup">4</sup> ‚ÄÑ‚ÄÑ
Sarah Ebling<sup id="id9.9.id9" class="ltx_sup">4</sup> ‚ÄÑ‚ÄÑ

<br class="ltx_break"><span id="id10.10.id10" class="ltx_text ltx_font_typewriter">amitmoryossef@gmail.com, ioannis@google.com</span> 
<br class="ltx_break"><span id="id11.11.id11" class="ltx_text ltx_font_typewriter">{j.dinn, n.camgoz, r.bowden, t.jiang}@surrey.ac.uk</span> 
<br class="ltx_break"><span id="id12.12.id12" class="ltx_text ltx_font_typewriter">{rios, mmueller, ebling}@cl.uzh.ch</span> 
<br class="ltx_break"><sup id="id13.13.id13" class="ltx_sup">1</sup>Bar Ilan University
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> <sup id="id14.1.id1" class="ltx_sup">2</sup>Google 
<br class="ltx_break"><sup id="id15.2.id2" class="ltx_sup">3</sup>University of Surrey
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> <sup id="id16.1.id1" class="ltx_sup">4</sup>University of Zurich
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id17.id1" class="ltx_p">Sign languages are visual languages produced by the movement of the hands, face, and body.
In this paper, we evaluate representations based on skeleton poses, as these are explainable, person-independent, privacy-preserving, low-dimensional representations. Basically, skeletal representations generalize over an individual‚Äôs appearance and background, allowing us to focus on the recognition of motion. But how much information is lost by the skeletal representation? We perform two independent studies using two state-of-the-art pose estimation systems. We analyze the applicability of the pose estimation systems to sign language recognition by evaluating the failure cases of the recognition models. Importantly, this allows us to characterize the current limitations of skeletal pose estimation approaches in sign language recognition.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Sign languages are visual languages produced by the movement of the hands, face, and body.
As languages that rely on visual communication, recordings are in video form.
Current state-of-the-art sign language processing systems rely on the video to model tasks such as sign language recognition (SLR) and sign language translation (SLT). However, using the raw video signal is computationally expensive and can lead to overfitting and person dependence.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In an attempt to abstract over the video information, skeleton poses have been suggested as an explainable, person-independent, privacy-preserving, and low-dimensional representation that provides the signer body pose and information on how it changes over time.
Theoretically, skeletal poses contain all the relevant information required to understand signs produced in videos, except for interactions with elements in space (for example, a mug or a table).</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The recording of accurate human skeleton poses is difficult and often intrusive, requiring signers to wear specialized and expensive motion capture hardware. Fortunately, advances in computer vision now allow the estimation of human skeleton poses directly from videos.
However, as these estimation systems were not specifically designed with sign language in mind, we currently do not understand their suitability for use in processing sign languages both in recognition or production.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this paper, we evaluate two pose estimation systems and demonstrate their suitability (and limitations) for SLR by conducting two independent studies on the CVPR21 ChaLearn challenge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>.
Because we perform no pretraining of the skeletal model, the final results are considerably lower than potential end-to-end approaches (¬ß<a href="#S3" title="3 Experiments ‚Ä£ Evaluating the Immediate Applicability of Pose Estimation for Sign Language Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). The results demonstrate that the skeletal representation loses considerable information. To better understand why, we evaluate our approaches (¬ß<a href="#S4" title="4 Results ‚Ä£ Evaluating the Immediate Applicability of Pose Estimation for Sign Language Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>), categorize their failure cases (¬ß<a href="#S5" title="5 Analysis ‚Ä£ Evaluating the Immediate Applicability of Pose Estimation for Sign Language Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>), and conclude by characterizing the attributes a pose estimation system should have to be applicable for SLR (¬ß<a href="#S6" title="6 Conclusions ‚Ä£ Evaluating the Immediate Applicability of Pose Estimation for Sign Language Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>).</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Pose Estimation</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Pose estimation is the task of detecting human figures in images and videos to determine where various joints are present in an image. This area has been thoroughly researched
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
with objectives varying from the predicting of 2D/3D poses to a selection of
a small specific set of landmarks or a dense mesh of a person. Vogler <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> showed that the face pose correlates with facial non-manual features.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">OpenPose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> was the first multi-person system to jointly detect human body, hand, facial, and foot keypoints (135 keypoints in total) in 2D on single images.
While this model can estimate the full pose directly from an image in a single inference, a pipeline approach is also suggested where
first, the body pose is estimated and then independently the hands and face pose by acquiring higher-resolution crops around those areas.
Building on the slow pipeline approach, a single-network whole-body OpenPose model has been proposed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>,
which is faster and more accurate for the case of obtaining all keypoints.
Additionally, with multiple recording angles, OpenPose also offers keypoint triangulation to reconstruct the pose in 3D.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">DensePose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> takes a different approach.
Instead of classifying for every keypoint which pixel is most likely, similar to semantic segmentation, each pixel is classified as belonging to a body part.
Then, for each pixel, knowing the body part, the system predicts where that pixel is on the body part relative to a 2D projection of a representative body model.
This approach results in reconstructing the full-body mesh and allows sampling to find specific keypoints similar to OpenPose.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p">MediaPipe Holistic <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> attempts to solve the 3D pose estimation problem directly by taking a similar approach to
OpenPose, having a pipeline system to estimate the body and then the face and hands.
It uses a dense mesh model for the face pose containing 468 points, but resorts to skeletal joints for the body and hands. Unlike OpenPose, the poses are estimated using regression rather than classification and are estimated in 3D rather than 2D.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Sign Language Recognition</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Sign language recognition (SLR) is the task of recognizing a sign or a sequence of signs from a video.
This task has been attempted both with computer vision models, assuming the input is the raw video, and with poses, assuming the video has been processed with a pose estimation tool.</p>
</div>
<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Video to Sign</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.1" class="ltx_p">Camg√∂z et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> formulate this problem as one of translation.
They encode each video frame using AlexNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, initialized using weights that were trained on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.
Then they apply a GRU encoder-decoder architecture with Luong Attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> to generate the signs.
In a follow-up work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, they use a transformer encoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> to replace the GRU and use Connectionist Temporal Classification (CTC) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> to decode the signs.
They show a slight improvement with this approach over the previous one.</p>
</div>
<div id="S2.SS2.SSS1.p2" class="ltx_para">
<p id="S2.SS2.SSS1.p2.1" class="ltx_p">Adaloglou et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> perform a comparative experimental assessment of computer vision-based methods for the SLR task.
They implement various approaches from previous research <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> and test
them on multiple datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> either for isolated sign recognition or continuous sign recognition.
They conclude that 3D convolutional models outperform models using only recurrent networks because they better capture temporal information and that convolutional models are more scalable given the restricted receptive field, which results from their ‚Äúsliding window‚Äù technique.</p>
</div>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Pose to Sign</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.1" class="ltx_p">Upper body poses have been widely used as a feature for computational sign language research <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, due to their signer-invariant representation capabilities. They have been included into recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, translation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, or detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> frameworks, either in raw coordinate form or as linguistically meaningful symbols extracted from joint coordinates <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.</p>
</div>
<div id="S2.SS2.SSS2.p2" class="ltx_para">
<p id="S2.SS2.SSS2.p2.1" class="ltx_p">Before the deep learning era, most sign language systems utilized specialized sensors, such as Kinect <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, to estimate signers pose in real-time <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. There have also been attempts to train models on sign language data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> which extract low-resolution skeletons, i.e., few joints. However, these approaches suffered from noisy estimations and had deficient hand joint resolution.</p>
</div>
<div id="S2.SS2.SSS2.p3" class="ltx_para">
<p id="S2.SS2.SSS2.p3.1" class="ltx_p">As with any subfield of computer vision, human pose estimation also improved with the introduction of deep learning-based approaches. Open source, general-purpose human pose estimation models, such as convolutional pose machines <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> and their predecessor OpenPose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, became widely used in sign language research. Ko et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> utilized a transformer-based translation based purely on skeletal information. Albanie et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> proposed using pose estimates to recognize co-articulated signs. They further used the pose estimates to train knowledge distillation networks and learn meaningful representations for downstream tasks.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiments</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">To evaluate whether pose estimation models are applicable for SLR, we participated in the CVPR21 ChaLearn challenge for person-independent isolated SLR on the Ankara University Turkish Sign Language (AUTSL) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> dataset.
Even though the dataset includes Kinect pose estimations, Kinect poses have not been made available for the challenge.
We processed the dataset using two pose estimation tools: 1. OpenPose Single-Network Whole-Body Pose Estimation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>; and 2. MediaPipe Holistic <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>; and made the data available via an open-source sign language datasets repository <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">We approach the recognition task with two independent experiments performed by different teams unaware of the other team‚Äôs work throughout the validation stage.
In the validation stage, each team focussed on one pose estimation approach, and in the test stage, both teams got access to both pose estimation outputs. We eventually submitted three systems: 1. based on <em id="S3.p2.1.1" class="ltx_emph ltx_font_italic">OpenPose</em> poses; 2. based on <em id="S3.p2.1.2" class="ltx_emph ltx_font_italic">Holistic</em> poses; 3. based on both <em id="S3.p2.1.3" class="ltx_emph ltx_font_italic">OpenPose</em> and <em id="S3.p2.1.4" class="ltx_emph ltx_font_italic">Holistic</em> poses combined (concatenated).</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Team 1</h3>

<figure id="S3.F1" class="ltx_figure"><img src="/html/2104.10166/assets/x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="460" height="626" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Diagram of <em id="S3.F1.2.1" class="ltx_emph ltx_font_italic">Team 1</em>‚Äôs model with one subnetwork (in green). (KE: Keypoint extraction, PE: Positional encoding, FF: feed forward)</figcaption>
</figure>
<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p"><em id="S3.SS1.p1.1.1" class="ltx_emph ltx_font_italic">Team 1</em> worked with OpenPose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> pose estimation output and used the SLR transformer architecture from Camg√∂z et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. The model takes as input a series of feature vectors, in this case, human upper body skeletal coordinates extracted from the video frames. These are each projected to a lower dimension hidden state vector. The size of the hidden state remains constant throughout the subsequent operations. A sinusoidal positional encoding is added to provide temporal information. This is then passed to a subnetwork consisting of a multiheaded self-attention layer, followed by a feedforward layer. After each of these layers, the output is added to the input and normalized. This subnetwork can be repeated any number of times. Finally, the output is fed to a linear layer and softmax to give probabilities for each class (Figure <a href="#S3.F1" title="Figure 1 ‚Ä£ 3.1 Team 1 ‚Ä£ 3 Experiments ‚Ä£ Evaluating the Immediate Applicability of Pose Estimation for Sign Language Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">The model is trained using CTC loss. This is designed to allow the output to be invariant to alignment; however, this is not a significant concern when there should only be one output symbol. The final prediction is obtained via CTC beam search decoding, collapsing multiple same class outputs into one. As the model is trained to predict a single class per video, it does not predict different classes within a sequence.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">The number of layers, heads, hidden size, and dropout rate affect the model complexity. There is, therefore, a tradeoff between sufficient complexity to model the data and overfitting.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">Additionally, as a baseline, the pose estimation keypoints were replaced with the output of three off-the-shelf image-based frame feature extractors, giving us small dense representations for each frame. Three extractors were used: 1. EfficientNet-B7 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>; 2. I3D trained on Kinetics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>; and 3. I3D trained on BSL1K <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2104.10166/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="460" height="642" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Diagram of <em id="S3.F2.2.1" class="ltx_emph ltx_font_italic">Team 2</em>‚Äôs model. (KE: Keypoint extraction)</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Team 2</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p"><em id="S3.SS2.p1.1.1" class="ltx_emph ltx_font_italic">Team 2</em> worked with the MediaPipe Holistic <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> pose estimation system output.
From the 543 landmarks, the face mesh was removed which consists of 468 landmarks and the remaining 75 landmarks were used for the body and hands.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">A standard sequence classification architecture was used.
The model takes as input a series of feature vectors, constructed from a flat vector representation of the pose concatenated with the 2D angle and length of every limb, using the <em id="S3.SS2.p2.1.1" class="ltx_emph ltx_font_italic">pose-format</em><span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://github.com/AmitMY/pose-format" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/AmitMY/pose-format</a></span></span></span> library.
These representations are subjected to a 20% dropout, normalized using 1D batch normalization, and are projected to a lower dimension hidden state vector (512 dimensions).
This is then passed to a two-layer BiLSTM with hidden dimension 256, followed by a max-pooling operation to obtain a single representation vector per video.
Finally, the output is fed to a linear layer and softmax to give probabilities for each class (Figure <a href="#S3.F2" title="Figure 2 ‚Ä£ 3.1 Team 1 ‚Ä£ 3 Experiments ‚Ä£ Evaluating the Immediate Applicability of Pose Estimation for Sign Language Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">The model is trained using cross-entropy loss with the Adam optimizer (with default parameters) and a batch size of 512 on a single GPU. <em id="S3.SS2.p3.1.1" class="ltx_emph ltx_font_italic">No</em> data augmentation or frame dropout is applied at training time, except for horizontal frame flip to account for left-handed signers in the dataset.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Table <a href="#S4.T1" title="Table 1 ‚Ä£ 4 Results ‚Ä£ Evaluating the Immediate Applicability of Pose Estimation for Sign Language Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows our teams‚Äô results on the validation set. We note that both teams‚Äô approaches using pose estimation performed similarly, with validation accuracy ranging between 80% and 85%. It rules out trivial errors and implementation issues that, despite working independently, and with two separate pose estimation tools, both teams achieve similar evaluation scores.
Furthermore, from a comparison between the pose estimation based systems (80-85%) and the pretrained image feature extractors (38-68%), we can see that pose estimation features do indeed generalize better to the nature of the challenge, including unseen signers and backgrounds.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<td id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">Team 1</td>
<td id="S4.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">Team 2</td>
</tr>
<tr id="S4.T1.1.2.2" class="ltx_tr">
<th id="S4.T1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">EfficientNet-B7</th>
<td id="S4.T1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">38.80%</td>
<td id="S4.T1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">‚Äî</td>
</tr>
<tr id="S4.T1.1.3.3" class="ltx_tr">
<th id="S4.T1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">I3D (Kinetics)</th>
<td id="S4.T1.1.3.3.2" class="ltx_td ltx_align_center">47.46%</td>
<td id="S4.T1.1.3.3.3" class="ltx_td ltx_align_center">‚Äî</td>
</tr>
<tr id="S4.T1.1.4.4" class="ltx_tr">
<th id="S4.T1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">I3D (BSL1K)</th>
<td id="S4.T1.1.4.4.2" class="ltx_td ltx_align_center">68.65%</td>
<td id="S4.T1.1.4.4.3" class="ltx_td ltx_align_center">‚Äî</td>
</tr>
<tr id="S4.T1.1.5.5" class="ltx_tr">
<th id="S4.T1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">OpenPose</th>
<td id="S4.T1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_t">83.25%</td>
<td id="S4.T1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_t">79.99%</td>
</tr>
<tr id="S4.T1.1.6.6" class="ltx_tr">
<th id="S4.T1.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Holistic</th>
<td id="S4.T1.1.6.6.2" class="ltx_td ltx_align_center">85.63%</td>
<td id="S4.T1.1.6.6.3" class="ltx_td ltx_align_center">82.14%</td>
</tr>
<tr id="S4.T1.1.7.7" class="ltx_tr">
<th id="S4.T1.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">OpenPose+Holistic</th>
<td id="S4.T1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">84.16%</td>
<td id="S4.T1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">82.89%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Results evaluated on the validation set with various frame-level features.</figcaption>
</figure>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">We submitted <em id="S4.p2.1.1" class="ltx_emph ltx_font_italic">Team 2</em>‚Äôs test set predictions to the official challenge evaluation.
On the test set, both <em id="S4.p2.1.2" class="ltx_emph ltx_font_italic">OpenPose</em> and <em id="S4.p2.1.3" class="ltx_emph ltx_font_italic">Holistic</em> performed <span id="S4.p2.1.4" class="ltx_text ltx_font_bold">equally well</span> despite making different predictions, each with 78.35% test set accuracy. However, our combined system, which was trained using both pose estimations, achieves 81.93% test set accuracy.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Analysis</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">The interpretability of skeletal poses allows us to assess them qualitatively using visualisation.
We manually review our model‚Äôs failure cases and categorize them into two main categories: hands interaction and hand-face interaction.</p>
</div>
<section id="S5.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Hands Interaction</h5>

<div id="S5.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px1.p1.1" class="ltx_p">When there exists an interaction between both hands, or one hand occludes the other from the camera‚Äôs view, we often fail to estimate the pose of one of the hands (Figure <a href="#S5.F3" title="Figure 3 ‚Ä£ Hands Interaction ‚Ä£ 5 Analysis ‚Ä£ Evaluating the Immediate Applicability of Pose Estimation for Sign Language Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) or estimate it incorrectly such that the interaction is not clearly shown (Figure <a href="#S5.F4" title="Figure 4 ‚Ä£ Hands Interaction ‚Ä£ 5 Analysis ‚Ä£ Evaluating the Immediate Applicability of Pose Estimation for Sign Language Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>).</p>
</div>
<figure id="S5.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2104.10166/assets/figures/holistic_failure/signer1_sample391_label47_frame21_color.png" id="S5.F3.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="293" height="293" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2104.10166/assets/figures/holistic_failure/signer1_sample391_label47_frame21_pose.png" id="S5.F3.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="293" height="293" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Example of hands interaction, where the pose estimation fails for one of the hands (Holistic).</figcaption>
</figure>
<figure id="S5.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2104.10166/assets/figures/holistic_failure/signer25_sample184_label47_frame26_color.png" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="293" height="293" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2104.10166/assets/figures/holistic_failure/signer25_sample184_label47_frame26_pose.png" id="S5.F4.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="293" height="293" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Example of hands interaction, where the pose estimation does not reflect the existing interaction (Holistic).</figcaption>
</figure>
</section>
<section id="S5.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Hand-Face Interaction</h5>

<div id="S5.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px2.p1.1" class="ltx_p">When there exists an interaction between a hand and the face, or one hand overlaps with the face from the camera‚Äôs angle, we often fail to estimate the pose of the interacting hand (Figure <a href="#S5.F5" title="Figure 5 ‚Ä£ Hand-Face Interaction ‚Ä£ 5 Analysis ‚Ä£ Evaluating the Immediate Applicability of Pose Estimation for Sign Language Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>).</p>
</div>
<figure id="S5.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2104.10166/assets/figures/holistic_failure/signer16_sample304_label172_frame22_color.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="293" height="293" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2104.10166/assets/figures/holistic_failure/signer16_sample304_label172_frame22_pose.png" id="S5.F5.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="293" height="293" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Example of hand-face interaction, where the pose estimation fails for the interacting hand (Holistic).</figcaption>
</figure>
<div id="S5.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S5.SS0.SSS0.Px2.p2.1" class="ltx_p">These cases of missed interactions between the different body parts often lose the essence of the sign, where the interaction and the hand shape are the main distinguishing features for those signs, and thus hinder the model‚Äôs ability to extract meaningful information from the pose that is relevant to the sign.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Presence or absence of hand pose</h5>

<div id="S5.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px3.p1.1" class="ltx_p">We describe a number of failure cases of Holistic pose estimation above. Many of them mean that keypoints for the hands are not available at all, since Holistic can omit them if it fails to detect the hand. As a complementary quantitative analysis, we correlate prediction outcomes with the average number of frames where hand pose was present (Figure <a href="#S5.F6" title="Figure 6 ‚Ä£ Presence or absence of hand pose ‚Ä£ 5 Analysis ‚Ä£ Evaluating the Immediate Applicability of Pose Estimation for Sign Language Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>).</p>
</div>
<figure id="S5.F6" class="ltx_figure"><img src="/html/2104.10166/assets/figures/plot_confidence_all_classes_conf_per_frame_holistic_percent.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_img_square" width="538" height="446" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Distribution of percent of frames containing the Holistic pose estimation of the dominant hand in each validation sample, grouped by whether the final prediction of our model was correct.</figcaption>
</figure>
<div id="S5.SS0.SSS0.Px3.p2" class="ltx_para">
<p id="S5.SS0.SSS0.Px3.p2.1" class="ltx_p">We find that on average, for all correct predictions the percentage of frames that do contain hand keypoints (85.13%) is significantly higher<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>We tested for a significant difference of the mean values with a Wilcoxon rank-sum test <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, <math id="footnote2.m1.1" class="ltx_Math" alttext="p&lt;0.0001" display="inline"><semantics id="footnote2.m1.1b"><mrow id="footnote2.m1.1.1" xref="footnote2.m1.1.1.cmml"><mi id="footnote2.m1.1.1.2" xref="footnote2.m1.1.1.2.cmml">p</mi><mo id="footnote2.m1.1.1.1" xref="footnote2.m1.1.1.1.cmml">&lt;</mo><mn id="footnote2.m1.1.1.3" xref="footnote2.m1.1.1.3.cmml">0.0001</mn></mrow><annotation-xml encoding="MathML-Content" id="footnote2.m1.1c"><apply id="footnote2.m1.1.1.cmml" xref="footnote2.m1.1.1"><lt id="footnote2.m1.1.1.1.cmml" xref="footnote2.m1.1.1.1"></lt><ci id="footnote2.m1.1.1.2.cmml" xref="footnote2.m1.1.1.2">ùëù</ci><cn type="float" id="footnote2.m1.1.1.3.cmml" xref="footnote2.m1.1.1.3">0.0001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote2.m1.1d">p&lt;0.0001</annotation></semantics></math>.</span></span></span> than for all incorrect predictions (79.78%). This is in line with our qualitative analysis.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Although many teams outperformed our models that use only off-the-shelf skeletal representations, with the best submission reaching 98.4% test set accuracy, it is unclear how well such approaches will generalise to other datasets. Our initial questions related to how good skeletal representations are for recognition, given their natural ability to generalise. However, performance in the ChaLearn challenge suggests that despite their benefits, considerable information is lost in the skeletal representation that must be represented in the image domain. A qualitative analysis of our models‚Äô failure cases shows that pose estimation tools suffer from shortcomings when body parts interact. We conclude that pose estimation tools are not immediately applicable for the use in sign language recognition ‚Äì the current representations are not sufficiently expressive, and that further improvements with regard to interacting body parts is crucial for their applicability.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Nikolas Adaloglou, Theocharis Chatzis, Ilias Papastratis, Andreas Stergioulas,
Georgios¬†Th Papadopoulos, Vassia Zacharopoulou, George¬†J Xydopoulos, Klimnis
Atzakas, Dimitris Papazachariou, and Petros Daras.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">A comprehensive study on sign language recognition methods.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2007.12530</span><span id="bib.bib1.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Samuel Albanie, G√ºl Varol, Liliane Momeni, Triantafyllos Afouras, Joon¬†Son
Chung, Neil Fox, and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">BSL-1K: Scaling up co-articulated sign language recognition using
mouthing cues.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Necati¬†Cihan Camg√∂z, Simon Hadfield, Oscar Koller, and Richard Bowden.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Subunets: End-to-end hand shape and continuous sign language
recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2017 IEEE International Conference on Computer Vision
(ICCV)</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, pages 3075‚Äì3084. IEEE, 2017.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Necati¬†Cihan Camg√∂z, Simon Hadfield, Oscar Koller, Hermann Ney, and Richard
Bowden.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Neural sign language translation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, pages 7784‚Äì7793, 2018.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Necati¬†Cihan Camg√∂z, Oscar Koller, Simon Hadfield, and Richard Bowden.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Multi-channel transformers for multi-articulatory sign language
translation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, pages 301‚Äì319,
2020.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Necati¬†Cihan Camg√∂z, Oscar Koller, Simon Hadfield, and Richard Bowden.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Sign language transformers: Joint end-to-end sign language
recognition and translation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, pages 10023‚Äì10033, 2020.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Z. Cao, G. Hidalgo Martinez, T. Simon, S. Wei, and Y.¬†A. Sheikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Openpose: Realtime multi-person 2d pose estimation using part
affinity fields.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span id="bib.bib7.4.2" class="ltx_text" style="font-size:90%;">,
2019.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Realtime multi-person 2d pose estimation using part affinity fields.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Joao Carreira and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Quo vadis, action recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">A new model and the kinetics dataset. CoRR, abs/1705.07750</span><span id="bib.bib9.4.2" class="ltx_text" style="font-size:90%;">,
2(3):1, 2017.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Xiujuan Chai, Guang Li, Yushun Lin, Zhihao Xu, Yili Tang, Xilin Chen, and Ming
Zhou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Sign Language Recognition and Translation with Kinect.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the International Conference on Automatic Face
and Gesture Recognition (FG)</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
James Charles, Tomas Pfister, Mark Everingham, and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Automatic and Efficient Human Pose Estimation for Sign Language
Videos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Computer Vision (IJCV)</span><span id="bib.bib11.4.2" class="ltx_text" style="font-size:90%;">, 110(1), 2014.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Yu Chen, Chunhua Shen, Xiu-Shen Wei, Lingqiao Liu, and Jian Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Adversarial posenet: A structure-aware convolutional network for
human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, pages 1212‚Äì1221, 2017.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Helen Cooper and Richard Bowden.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Sign Language Recognition using Linguistically Derived Sub-Units.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 4th Workshop on the Representation and
Processing of Sign Languages: Corpora and Sign Language Technologies</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, 2010.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Helen Cooper, Brian Holt, and Richard Bowden.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Sign Language Recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Visual Analysis of Humans</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">. Springer, 2011.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Runpeng Cui, Hu Liu, and Changshui Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">A deep neural framework for continuous sign language recognition by
iterative training.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Multimedia</span><span id="bib.bib15.4.2" class="ltx_text" style="font-size:90%;">, 21(7):1880‚Äì1891, 2019.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Imagenet: A large-scale hierarchical image database.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2009 IEEE conference on computer vision and pattern
recognition</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, pages 248‚Äì255. Ieee, 2009.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Biyi Fang, Jillian Co, and Mi Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">DeepASL: Enabling Ubiquitous and Non-Intrusive Word and
Sentence-Level Sign Language Translation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the ACM Conference on Embedded Networked
Sensor Systems (SenSys)</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Alex Graves, Santiago Fern√°ndez, Faustino Gomez, and J√ºrgen
Schmidhuber.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Connectionist temporal classification: labelling unsegmented sequence
data with recurrent neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 23rd international conference on Machine
learning</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, pages 369‚Äì376, 2006.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Ivan Grishchenko and Valentin Bazarevsky.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Mediapipe holistic.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://ai.googleblog.com/2020/12/mediapipe-holistic-simultaneous-face.html" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://ai.googleblog.com/2020/12/mediapipe-holistic-simultaneous-face.html</a><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">,
2020.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Rƒ±za¬†Alp G√ºler, Natalia Neverova, and Iasonas Kokkinos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Densepose: Dense human pose estimation in the wild.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, pages 7297‚Äì7306, 2018.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Gines Hidalgo, Yaadhav Raaj, Haroon Idrees, Donglai Xiang, Hanbyul Joo, Tomas
Simon, and Yaser Sheikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Single-network whole-body pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Jie Huang, Wengang Zhou, Qilin Zhang, Houqiang Li, and Weiping Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Video-based sign language recognition without temporal segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the AAAI Conference on Artificial
Intelligence</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, volume¬†32, 2018.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Sang-Ki Ko, Chang¬†Jo Kim, Hyedong Jung, and Choongsang Cho.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Neural Sign Language Translation based on Human Keypoint
Estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Applied Sciences</span><span id="bib.bib23.4.2" class="ltx_text" style="font-size:90%;">, 9(13), 2019.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Alex Krizhevsky, Ilya Sutskever, and Geoffrey¬†E Hinton.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Imagenet classification with deep convolutional neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, pages
1097‚Äì1105, 2012.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Thang Luong, Hieu Pham, and Christopher¬†D. Manning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Effective approaches to attention-based neural machine translation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 2015 Conference on Empirical Methods in
Natural Language Processing</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, pages 1412‚Äì1421, Lisbon, Portugal, Sept. 2015.
Association for Computational Linguistics.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Marcos Luzardo, Matti Karppa, Jorma Laaksonen, and Tommi Jantunen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Head Pose Estimation for Sign Language Video.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Image Analysis</span><span id="bib.bib26.4.2" class="ltx_text" style="font-size:90%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Amit Moryossef.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Sign language datasets.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/sign-language-processing/datasets" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/sign-language-processing/datasets</a><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Amit Moryossef, Ioannis Tsochantaridis, Roee¬†Yosef Aharoni, Sarah Ebling, and
Srini Narayanan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Real-time sign-language detection using human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">2020.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Tomas Pfister, James Charles, Mark Everingham, and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Automatic and Efficient Long Term Arm and Hand Tracking for
Continuous Sign Language TV Broadcasts.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the British Machine Vision Conference (BMVC)</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">,
2012.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Leonid Pishchulin, Arjun Jain, Mykhaylo Andriluka, Thorsten Thorm¬†√§ hlen,
and Bernt Schiele.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Articulated people detection and pose estimation: Reshaping the
future.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2012 IEEE Conference on Computer Vision and Pattern
Recognition</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, pages 3178‚Äì3185. IEEE, 2012.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
J. Shotton, Andrew Fitzgibbon, M. Cook, Toby Sharp, M. Finocchio, R. Moore, A.
Kipman, and A. Blake.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Real-time Human Pose Recognition in Parts from Single Depth Images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, 2011.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Tomas Simon, Hanbyul Joo, Iain Matthews, and Yaser Sheikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Hand keypoint detection in single images using multiview
bootstrapping.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Ozge¬†Mercanoglu Sincan, Julio C.¬†S. Jacques Junior, Sergio Escalera, and
Hacer¬†Yalim Keles.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Chalearn LAP large scale signer independent isolated sign language
recognition challenge: Design, results and future research.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) Workshops</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Ozge¬†Mercanoglu Sincan and Hacer¬†Yalim Keles.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Autsl: A large scale multi-modal turkish sign language dataset and
baseline methods.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Access</span><span id="bib.bib34.4.2" class="ltx_text" style="font-size:90%;">, 8:181340‚Äì181355, 2020.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Mingxing Tan and Quoc Le.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Efficientnet: Rethinking model scaling for convolutional neural
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:90%;">, pages
6105‚Äì6114. PMLR, 2019.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Hamid Vaezi¬†Joze and Oscar Koller.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Ms-asl: A large-scale data set and benchmark for understanding
american sign language.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">The British Machine Vision Conference (BMVC)</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:90%;">, September
2019.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan¬†N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Attention is all you need.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib37.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib37.5.3" class="ltx_text" style="font-size:90%;">, pages
5998‚Äì6008, 2017.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Christian Vogler and Siome Goldenstein.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Analysis of facial expressions in american sign language.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc, of the 3rd Int. Conf. on Universal Access in
Human-Computer Interaction, Springer</span><span id="bib.bib38.5.3" class="ltx_text" style="font-size:90%;">, 2005.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Ulrich Von¬†Agris and Karl-Friedrich Kraiss.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">Towards a video corpus for signer-independent continuous sign
language recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Gesture in Human-Computer Interaction and Simulation, Lisbon,
Portugal, May</span><span id="bib.bib39.4.2" class="ltx_text" style="font-size:90%;">, 11, 2007.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Shih-En Wei, Varun Ramakrishna, Takeo Kanade, and Yaser Sheikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Convolutional pose machines.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib40.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib40.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Shih-En Wei, Varun Ramakrishna, Takeo Kanade, and Yaser Sheikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">Convolutional Pose Machines.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib41.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib41.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Frank Wilcoxon.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Individual comparisons by ranking methods.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib42.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Breakthroughs in statistics</span><span id="bib.bib42.5.3" class="ltx_text" style="font-size:90%;">, pages 196‚Äì202. Springer, 1992.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Zahoor Zafrulla, Helene Brashear, Thad Starner, Harley Hamilton, and Peter
Presti.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">American Sign Language Recognition with the Kinect.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib43.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the ACM International Conference on Multimodal
Interfaces (ICMI)</span><span id="bib.bib43.5.3" class="ltx_text" style="font-size:90%;">, 2011.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2104.10165" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2104.10166" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2104.10166">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2104.10166" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2104.10167" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  9 02:59:33 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
