<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2304.11463] OmniLabel: A Challenging Benchmark for Language-Based Object Detection</title><meta property="og:description" content="Language-based object detection is a promising direction towards building a natural interface to describe objects in images that goes far beyond plain category names. While recent methods show great progress in that di…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="OmniLabel: A Challenging Benchmark for Language-Based Object Detection">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="OmniLabel: A Challenging Benchmark for Language-Based Object Detection">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2304.11463">

<!--Generated on Thu Feb 29 12:49:29 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">OmniLabel: A Challenging Benchmark for Language-Based Object Detection</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Samuel Schulter<sup id="id1.1.id1" class="ltx_sup">1, †</sup>  Vijay Kumar B G<sup id="id2.2.id2" class="ltx_sup">1</sup>  Yumin Suh<sup id="id3.3.id3" class="ltx_sup">1</sup>  Konstantinos M. Dafnis<sup id="id4.4.id4" class="ltx_sup">2,*</sup>  
<br class="ltx_break">Zhixing Zhang<sup id="id5.5.id5" class="ltx_sup">2,*</sup>  Shiyu Zhao<sup id="id6.6.id6" class="ltx_sup">2,*</sup>  Dimitris Metaxas<sup id="id7.7.id7" class="ltx_sup">2</sup>  
<br class="ltx_break"><sup id="id8.8.id8" class="ltx_sup">†</sup> project lead  <sup id="id9.9.id9" class="ltx_sup">*</sup> equal technical contribution, alphabetic order 
<br class="ltx_break"><sup id="id10.10.id10" class="ltx_sup">1</sup> NEC Laboratories America  <sup id="id11.11.id11" class="ltx_sup">2</sup> Rutgers University
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id12.id1" class="ltx_p">Language-based object detection is a promising direction towards building a natural interface to describe objects in images that goes far beyond plain category names. While recent methods show great progress in that direction, proper evaluation is lacking. With OmniLabel, we propose a novel task definition, dataset, and evaluation metric. The task subsumes standard- and open-vocabulary detection as well as referring expressions. With more than 28K unique object descriptions on over 25K images, OmniLabel provides a challenging benchmark with diverse and complex object descriptions in a naturally open-vocabulary setting. Moreover, a key differentiation to existing benchmarks is that our object descriptions can refer to one, multiple or even no object, hence, providing negative examples in free-form text. The proposed evaluation handles the large label space and judges performance via a modified average precision metric, which we validate by evaluating strong language-based baselines. OmniLabel indeed provides a challenging test bed for future research on language-based detection. Visit the project website at <a target="_blank" href="https://www.omnilabel.org" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.omnilabel.org</a></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">A nuanced understanding of the rich semantics of the world around us is a key ability in the visual perception system of humans. Identifying objects from a description like “person wearing blue-and-white striped T-shirt standing next to the traffic sign” feels easy, because humans understand the composition of object category names, attributes, actions, and spatial or semantic relations between objects. When automated, this same ability can improve and enable a plethora of applications in robotics, autonomous vehicles, navigation, retail, etc.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2304.11463/assets/figures/teaser_positioning_v2.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="120" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.4.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.5.2" class="ltx_text" style="font-size:90%;">
<span id="S1.F1.5.2.1" class="ltx_text ltx_font_bold">(Left)</span> OmniLabel extends upon standard detection, open-vocabulary detection, as well as referring expressions, subsuming these tasks as special cases.
<span id="S1.F1.5.2.2" class="ltx_text ltx_font_bold">(Right)</span> The OmniLabel dataset is semantically rich with diverse free-form text descriptions of objects. Moreover, with object descriptions referring to multiple instances, dedicated negative descriptions, and a novel evaluation metric, our benchmark poses a challenging task for language-based detectors.
</span></figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">With the recent advances in vision &amp; language models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>, along with extensions towards object localization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>, a comprehensive evaluation benchmark is needed. However, existing ones fall short in various aspects. While object detection datasets significantly increased the label space over time (from 20 in Pascal <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> to 1200 in LVIS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>), a fixed label space is assumed. The zero-shot <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> and open-vocabulary detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> settings drop the fixed-labelspace assumption, but corresponding benchmarks only evaluate simple category names, neglecting more complex descriptions. Referring expression datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> probe models with free-form text descriptions of objects. However, the corresponding dataset annotations and metrics do not allow for a comprehensive evaluation of models.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">We introduce a novel benchmark called OmniLabel with the goal to comprehensively probe models for their ability to understand complex, free-form textual descriptions of objects and to locate the corresponding instances. This requires a novel task definition and evaluation metric, which we propose in <a href="#S3" title="3 Benchmark and Evaluation Metric ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3</span></a>. Our evaluation benchmark does not assume a fixed label space (unlike standard detection), uses complex object descriptions beyond plain category names (unlike open-vocabulary detection), and evaluates true detection ability with descriptions referring to zero, one or more instances in a given image (unlike referring expressions). A unique aspect of our benchmark are the descriptions that refer to zero instances, which pose a challenge to existing methods as hard negative examples. <a href="#S1.F1" title="In 1 Introduction ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a> positions our OmniLabel benchmark.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<table id="S1.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S1.T1.2.1.1" class="ltx_tr">
<th id="S1.T1.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" style="padding:1.5pt 3.0pt;">Dataset</th>
<th id="S1.T1.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding:1.5pt 3.0pt;">
<div id="S1.T1.2.1.1.2.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.9pt;height:41.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:41.2pt;transform:translate(-16.14pt,-15.17pt) rotate(-90deg) ;">
<p id="S1.T1.2.1.1.2.1.1" class="ltx_p"># images</p>
</span></div>
</th>
<th id="S1.T1.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding:1.5pt 3.0pt;">
<div id="S1.T1.2.1.1.3.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:42.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:42.1pt;transform:translate(-17.6pt,-17.6pt) rotate(-90deg) ;">
<p id="S1.T1.2.1.1.3.1.1" class="ltx_p">Free-form</p>
</span></div>
</th>
<th id="S1.T1.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding:1.5pt 3.0pt;">
<div id="S1.T1.2.1.1.4.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.9pt;height:57.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:57.7pt;transform:translate(-24.42pt,-23.44pt) rotate(-90deg) ;">
<p id="S1.T1.2.1.1.4.1.1" class="ltx_p">Descr. length</p>
</span></div>
</th>
<th id="S1.T1.2.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding:1.5pt 3.0pt;">
<div id="S1.T1.2.1.1.5.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.9pt;height:69.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:69.8pt;transform:translate(-30.44pt,-29.47pt) rotate(-90deg) ;">
<p id="S1.T1.2.1.1.5.1.1" class="ltx_p"># unique nouns</p>
</span></div>
</th>
<th id="S1.T1.2.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding:1.5pt 3.0pt;">
<div id="S1.T1.2.1.1.6.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.9pt;height:74.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:74.8pt;transform:translate(-32.93pt,-31.96pt) rotate(-90deg) ;">
<p id="S1.T1.2.1.1.6.1.1" class="ltx_p">Open-vocabulary</p>
</span></div>
</th>
<th id="S1.T1.2.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding:1.5pt 3.0pt;">
<div id="S1.T1.2.1.1.7.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:63.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:63.9pt;transform:translate(-28.5pt,-28.5pt) rotate(-90deg) ;">
<p id="S1.T1.2.1.1.7.1.1" class="ltx_p">Multi-Instance</p>
</span></div>
</th>
<th id="S1.T1.2.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding:1.5pt 3.0pt;">
<div id="S1.T1.2.1.1.8.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.8pt;height:38.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:38.1pt;transform:translate(-14.64pt,-13.67pt) rotate(-90deg) ;">
<p id="S1.T1.2.1.1.8.1.1" class="ltx_p">Negative</p>
</span></div>
</th>
<th id="S1.T1.2.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding:1.5pt 3.0pt;">
<div id="S1.T1.2.1.1.9.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:47.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:47.1pt;transform:translate(-20.07pt,-20.07pt) rotate(-90deg) ;">
<p id="S1.T1.2.1.1.9.1.1" class="ltx_p">Evaluation</p>
</span></div>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S1.T1.2.2.1" class="ltx_tr">
<th id="S1.T1.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" style="padding:1.5pt 3.0pt;">LVIS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>
</th>
<td id="S1.T1.2.2.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding:1.5pt 3.0pt;">5K</td>
<td id="S1.T1.2.2.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding:1.5pt 3.0pt;"><span id="S1.T1.2.2.1.3.1" class="ltx_text" style="color:#D2323C;">✗</span></td>
<td id="S1.T1.2.2.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding:1.5pt 3.0pt;">–</td>
<td id="S1.T1.2.2.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding:1.5pt 3.0pt;">1.2K</td>
<td id="S1.T1.2.2.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding:1.5pt 3.0pt;"><span id="S1.T1.2.2.1.6.1" class="ltx_text" style="color:#D2323C;">✗</span></td>
<td id="S1.T1.2.2.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding:1.5pt 3.0pt;"><span id="S1.T1.2.2.1.7.1" class="ltx_text" style="color:#32D23C;">✓</span></td>
<td id="S1.T1.2.2.1.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding:1.5pt 3.0pt;"><span id="S1.T1.2.2.1.8.1" class="ltx_text" style="color:#32D23C;">✓</span></td>
<td id="S1.T1.2.2.1.9" class="ltx_td ltx_align_center ltx_border_tt" style="padding:1.5pt 3.0pt;">AP</td>
</tr>
<tr id="S1.T1.2.3.2" class="ltx_tr">
<th id="S1.T1.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:1.5pt 3.0pt;">ODinW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>
</th>
<td id="S1.T1.2.3.2.2" class="ltx_td ltx_align_center" style="padding:1.5pt 3.0pt;">27.3k</td>
<td id="S1.T1.2.3.2.3" class="ltx_td ltx_align_center" style="padding:1.5pt 3.0pt;"><span id="S1.T1.2.3.2.3.1" class="ltx_text" style="color:#D2323C;">✗</span></td>
<td id="S1.T1.2.3.2.4" class="ltx_td ltx_align_center" style="padding:1.5pt 3.0pt;">–</td>
<td id="S1.T1.2.3.2.5" class="ltx_td ltx_align_center" style="padding:1.5pt 3.0pt;">0.3K</td>
<td id="S1.T1.2.3.2.6" class="ltx_td ltx_align_center" style="padding:1.5pt 3.0pt;"><span id="S1.T1.2.3.2.6.1" class="ltx_text" style="color:#32D23C;">✓</span></td>
<td id="S1.T1.2.3.2.7" class="ltx_td ltx_align_center" style="padding:1.5pt 3.0pt;"><span id="S1.T1.2.3.2.7.1" class="ltx_text" style="color:#32D23C;">✓</span></td>
<td id="S1.T1.2.3.2.8" class="ltx_td ltx_align_center" style="padding:1.5pt 3.0pt;"><span id="S1.T1.2.3.2.8.1" class="ltx_text" style="color:#32D23C;">✓</span></td>
<td id="S1.T1.2.3.2.9" class="ltx_td ltx_align_center" style="padding:1.5pt 3.0pt;">AP</td>
</tr>
<tr id="S1.T1.2.4.3" class="ltx_tr">
<th id="S1.T1.2.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:1.5pt 3.0pt;">RefCOCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>
</th>
<td id="S1.T1.2.4.3.2" class="ltx_td ltx_align_center" style="padding:1.5pt 3.0pt;">4.3K</td>
<td id="S1.T1.2.4.3.3" class="ltx_td ltx_align_center" style="padding:1.5pt 3.0pt;"><span id="S1.T1.2.4.3.3.1" class="ltx_text" style="color:#32D23C;">✓</span></td>
<td id="S1.T1.2.4.3.4" class="ltx_td ltx_align_center" style="padding:1.5pt 3.0pt;">4.5</td>
<td id="S1.T1.2.4.3.5" class="ltx_td ltx_align_center" style="padding:1.5pt 3.0pt;">3.5K</td>
<td id="S1.T1.2.4.3.6" class="ltx_td ltx_align_center" style="padding:1.5pt 3.0pt;"><span id="S1.T1.2.4.3.6.1" class="ltx_text" style="color:#32D23C;">✓</span></td>
<td id="S1.T1.2.4.3.7" class="ltx_td ltx_align_center" style="padding:1.5pt 3.0pt;"><span id="S1.T1.2.4.3.7.1" class="ltx_text" style="color:#D2323C;">✗</span></td>
<td id="S1.T1.2.4.3.8" class="ltx_td ltx_align_center" style="padding:1.5pt 3.0pt;"><span id="S1.T1.2.4.3.8.1" class="ltx_text" style="color:#D2323C;">✗</span></td>
<td id="S1.T1.2.4.3.9" class="ltx_td ltx_align_center" style="padding:1.5pt 3.0pt;">P</td>
</tr>
<tr id="S1.T1.2.5.4" class="ltx_tr">
<th id="S1.T1.2.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:1.5pt 3.0pt;">Flickr30k <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>
</th>
<td id="S1.T1.2.5.4.2" class="ltx_td ltx_align_center" style="padding:1.5pt 3.0pt;">1.0K</td>
<td id="S1.T1.2.5.4.3" class="ltx_td ltx_align_center" style="padding:1.5pt 3.0pt;"><span id="S1.T1.2.5.4.3.1" class="ltx_text" style="color:#32D23C;">✓</span></td>
<td id="S1.T1.2.5.4.4" class="ltx_td ltx_align_center" style="padding:1.5pt 3.0pt;">2.4</td>
<td id="S1.T1.2.5.4.5" class="ltx_td ltx_align_center" style="padding:1.5pt 3.0pt;">1.9K</td>
<td id="S1.T1.2.5.4.6" class="ltx_td ltx_align_center" style="padding:1.5pt 3.0pt;"><span id="S1.T1.2.5.4.6.1" class="ltx_text" style="color:#32D23C;">✓</span></td>
<td id="S1.T1.2.5.4.7" class="ltx_td ltx_align_center" style="padding:1.5pt 3.0pt;"><span id="S1.T1.2.5.4.7.1" class="ltx_text" style="color:#D2323C;">✗</span></td>
<td id="S1.T1.2.5.4.8" class="ltx_td ltx_align_center" style="padding:1.5pt 3.0pt;"><span id="S1.T1.2.5.4.8.1" class="ltx_text" style="color:#D2323C;">✗</span></td>
<td id="S1.T1.2.5.4.9" class="ltx_td ltx_align_center" style="padding:1.5pt 3.0pt;">R</td>
</tr>
<tr id="S1.T1.2.6.5" class="ltx_tr">
<th id="S1.T1.2.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:1.5pt 3.0pt;">PhraseCut <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>
</th>
<td id="S1.T1.2.6.5.2" class="ltx_td ltx_align_center" style="padding:1.5pt 3.0pt;">2.9K</td>
<td id="S1.T1.2.6.5.3" class="ltx_td ltx_align_center" style="padding:1.5pt 3.0pt;"><span id="S1.T1.2.6.5.3.1" class="ltx_text" style="color:#32D23C;">✓</span></td>
<td id="S1.T1.2.6.5.4" class="ltx_td ltx_align_center" style="padding:1.5pt 3.0pt;">2.0</td>
<td id="S1.T1.2.6.5.5" class="ltx_td ltx_align_center" style="padding:1.5pt 3.0pt;">1.5K</td>
<td id="S1.T1.2.6.5.6" class="ltx_td ltx_align_center" style="padding:1.5pt 3.0pt;"><span id="S1.T1.2.6.5.6.1" class="ltx_text" style="color:#32D23C;">✓</span></td>
<td id="S1.T1.2.6.5.7" class="ltx_td ltx_align_center" style="padding:1.5pt 3.0pt;"><span id="S1.T1.2.6.5.7.1" class="ltx_text" style="color:#32D23C;">✓</span></td>
<td id="S1.T1.2.6.5.8" class="ltx_td ltx_align_center" style="padding:1.5pt 3.0pt;"><span id="S1.T1.2.6.5.8.1" class="ltx_text" style="color:#D2323C;">✗</span></td>
<td id="S1.T1.2.6.5.9" class="ltx_td ltx_align_center" style="padding:1.5pt 3.0pt;">IoU</td>
</tr>
<tr id="S1.T1.2.7.6" class="ltx_tr">
<th id="S1.T1.2.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding:1.5pt 3.0pt;">
<span id="S1.T1.2.7.6.1.1" class="ltx_ERROR undefined">\rowcolor</span>green!5
<span id="S1.T1.2.7.6.1.2" class="ltx_text ltx_font_bold">OmniLabel</span>
</th>
<td id="S1.T1.2.7.6.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1.5pt 3.0pt;">12.2K</td>
<td id="S1.T1.2.7.6.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1.5pt 3.0pt;"><span id="S1.T1.2.7.6.3.1" class="ltx_text" style="color:#32D23C;">✓</span></td>
<td id="S1.T1.2.7.6.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1.5pt 3.0pt;">5.6</td>
<td id="S1.T1.2.7.6.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1.5pt 3.0pt;">4.6K</td>
<td id="S1.T1.2.7.6.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1.5pt 3.0pt;"><span id="S1.T1.2.7.6.6.1" class="ltx_text" style="color:#32D23C;">✓</span></td>
<td id="S1.T1.2.7.6.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1.5pt 3.0pt;"><span id="S1.T1.2.7.6.7.1" class="ltx_text" style="color:#32D23C;">✓</span></td>
<td id="S1.T1.2.7.6.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1.5pt 3.0pt;"><span id="S1.T1.2.7.6.8.1" class="ltx_text" style="color:#32D23C;">✓</span></td>
<td id="S1.T1.2.7.6.9" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1.5pt 3.0pt;">AP</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S1.T1.5.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S1.T1.6.2" class="ltx_text" style="font-size:90%;">
Comparing OmniLabel to existing benchmarks: On 12.2K images, OmniLabel provides free-form text descriptions of objects with an average description length of 5.6 words, covering 4.6K unique nouns. Each description can refer to multiple objects, or no object, <em id="S1.T1.6.2.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.T1.6.2.2" class="ltx_text"></span>, a negative example, an important factor in our evaluation. (Numbers are computed on validation sets. P: Precision, R: Recall, AP: Average Precision, IoU: Intersection over Union)
</span></figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To build this evaluation benchmark, we collected a set of novel annotations upon existing object detection datasets. We augment the existing plain category names with novel free-form text descriptions of objects. Our specific annotation process (<a href="#S4" title="4 Dataset Collection ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4</span></a>) increases the difficulty of the task by ensuring that at least one of the following conditions is true:</p>
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Multiple instances of the same underlying object category are present in the same image</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">One object description can refer to multiple objects</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(c)</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">An image contains a negative object description, which refers to no object but is related to the image’s semantics</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(d)</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">Descriptions do not use the original category name</p>
</div>
</li>
</ol>
<p id="S1.p4.2" class="ltx_p"><a href="#S1.T1" title="In 1 Introduction ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a> highlights the key differences of OmniLabel to existing benchmarks: The diversity in the free-form text descriptions and the evaluation as an object detection task, including multiple instances per description as well as negative object descriptions. The numbers in the table reflect our <em id="S1.p4.2.1" class="ltx_emph ltx_font_italic">public validation set</em>, which is roughly the same size as our <em id="S1.p4.2.2" class="ltx_emph ltx_font_italic">private test set</em>. <a href="#S1.F2" title="In 1 Introduction ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a> provides examples of the dataset.</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2304.11463/assets/figures/examples.jpg" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="299" height="458" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S1.F2.3.2" class="ltx_text" style="font-size:90%;">
Examples of the OmniLabel ground truth annotations. Positive descriptions (above each image) can refer to one or multiple instances. Negative descriptions (below each image) are semantically related to the image but refer to no object.
More examples in <a href="#A4" title="Appendix D Examples of Dataset Samples ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">D</span></a>
</span></figcaption>
</figure>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">We also evaluate recent language-based detectors on our benchmark, including RegionCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>, Detic <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, MDETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, GLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> and FIBER <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. Summarized in <a href="#S6.SS2" title="6.2 Results ‣ 6 Baselines ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">6.2</span></a>, our key observation is that the proposed benchmark is difficult for all methods, and the evaluation metric is more stringent than in prior benchmarks. Negative object descriptions pose the biggest challenge to current methods.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">We summarize our contributions as follows:</p>
<ol id="S1.I2" class="ltx_enumerate">
<li id="S1.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span> 
<div id="S1.I2.i1.p1" class="ltx_para">
<p id="S1.I2.i1.p1.1" class="ltx_p">A novel benchmark to unify standard detection, open-vocabulary detection and referring expressions
</p>
</div>
</li>
<li id="S1.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span> 
<div id="S1.I2.i2.p1" class="ltx_para">
<p id="S1.I2.i2.p1.1" class="ltx_p">A data annotation process to collect diverse and complex free-form text descriptions of objects, including negative examples</p>
</div>
</li>
<li id="S1.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(c)</span> 
<div id="S1.I2.i3.p1" class="ltx_para">
<p id="S1.I2.i3.p1.1" class="ltx_p">A comprehensive evaluation metric that handles the virtually infinite label space</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">To position our proposed OmniLabel benchmark, we relate it to existing tasks and focus on the corresponding benchmark datasets.</p>
</div>
<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Object Detection:</h4>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">Localizing and categorizing objects is a long-standing and important task with many applications. An enormous amount of datasets fueled this research. Besides datasets for specific use cases, like face <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, pedestrians <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> or driving scenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, the most popular ones are general-purpose: Pascal VOC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, MS COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, Objects-365 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, OpenImages <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> or LVIS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. These datasets also reflect the evolution of size, both in number of images and, more relevant here, the number of categories. In the same order as above, the label space sizes are 20, 80, 365, 600 and 1203. These datasets lead to significant progress in the past years on neural network architectures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib58" title="" class="ltx_ref">58</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite> as well as robustness and scaling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>. Still, the limitations over OmniLabel are obvious: All detection datasets assume a fixed labelspace, do not provide an open-vocabulary setting or free-form object detections.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Referring Expressions:</h4>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">Instead of a limited and fixed set of category names, the motivation in referring expressions is to refer to objects with natural language.
The most popular benchmark is the series of RefCOCO/g/+ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>. While RefCOCO/g <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> often contains long and redundant descriptions, RefCOCO/+ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> limited the referring phrases with a specific annotation process involving a two-player game. The RefCOCO+ extension restricted annotators to use spatial references (<em id="S2.SS0.SSS0.Px2.p1.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.SS0.SSS0.Px2.p1.1.2" class="ltx_text"></span>, “man on left”), which was likely over-used because all of RefCOCO/g/+ assume each phrase to refer to exactly one instance. In contrast, OmniLabel explicitly asks annotators to pick two or more instances to describe in many images. PhraseCut <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> also collects templated expressions that refer to multiple instances and also provides segmentation masks. However, OmniLabel still has more instances per object description and uses free-form descriptions. Moreover, none of the existing referring expression datasets provides negative examples.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Visual Grounding:</h4>

<div id="S2.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px3.p1.1" class="ltx_p">While the task of referring expressions is to localize the main subject of the phrase, visual grounding aims at localizing each object of the phrase, <em id="S2.SS0.SSS0.Px3.p1.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S2.SS0.SSS0.Px3.p1.1.2" class="ltx_text"></span>, grounding the text in the image. Benchmarks include Flickr30k <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> or Visual Genomes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, which have often been used also for general object-centric pre-training for vision &amp; language models like GLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, MDETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, SIMLA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, ALBEF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.
OmniLabel addresses a different task that is more related to referring expressions. The annotation costs for grounding are also typically higher since all objects mentioned in a phrase need an associated bounding box, which often leads to noisy ground truth. In contrast, the annotation process for OmniLabel can easily be built upon existing detection datasets with high-quality bounding boxes.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Open-Vocabulary Object Detection:</h4>

<div id="S2.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px4.p1.1" class="ltx_p">Aside from using natural language as object descriptions, scaling the label space of object detectors becomes infeasible with a standard supervised approach. This sparked work on the zero-shot setting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, where a set of base categories is available at training time, but novel (or unseen) categories are added at test time. While Bansal <em id="S2.SS0.SSS0.Px4.p1.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS0.SSS0.Px4.p1.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> introduced the first work on zero-shot detection, later works relaxed the setting to open-vocabulary <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>, where annotations other than bounding boxes can be leveraged that may include the novel categories, <em id="S2.SS0.SSS0.Px4.p1.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.SS0.SSS0.Px4.p1.1.4" class="ltx_text"></span>, image captions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>. The recent success of large V&amp;L models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> surged interest in open-vocabulary detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>. However, benchmarks for this setting are lacking. Most existing work evaluates on standard detection datasets, COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> and LVIS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, by separating categories into base and novel. Most recently, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> introduces the ODinW benchmark which combines 35 standard detection datasets to setup an open-vocabulary challenge. Still, all benchmarks use a rather limited set of simple category names. In contrast, OmniLabel provides higher complexity with object descriptions being free-form text and, with this, a larger number of unique words (and nouns) which poses a naturally open-vocabulary setting since every description is effectively unique.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2304.11463/assets/figures/figure_task_and_eval.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="685" height="181" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.24.12.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S2.F3.22.11" class="ltx_text" style="font-size:90%;">Illustration of the proposed task and evaluation. Given an image <math id="S2.F3.12.1.m1.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S2.F3.12.1.m1.1b"><mi id="S2.F3.12.1.m1.1.1" xref="S2.F3.12.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S2.F3.12.1.m1.1c"><ci id="S2.F3.12.1.m1.1.1.cmml" xref="S2.F3.12.1.m1.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F3.12.1.m1.1d">I</annotation></semantics></math> and a set of object descriptions <math id="S2.F3.13.2.m2.4" class="ltx_Math" alttext="\left[d^{1},d^{2},\ldots,d^{5}\right]" display="inline"><semantics id="S2.F3.13.2.m2.4b"><mrow id="S2.F3.13.2.m2.4.4.3" xref="S2.F3.13.2.m2.4.4.4.cmml"><mo id="S2.F3.13.2.m2.4.4.3.4" xref="S2.F3.13.2.m2.4.4.4.cmml">[</mo><msup id="S2.F3.13.2.m2.2.2.1.1" xref="S2.F3.13.2.m2.2.2.1.1.cmml"><mi id="S2.F3.13.2.m2.2.2.1.1.2" xref="S2.F3.13.2.m2.2.2.1.1.2.cmml">d</mi><mn id="S2.F3.13.2.m2.2.2.1.1.3" xref="S2.F3.13.2.m2.2.2.1.1.3.cmml">1</mn></msup><mo id="S2.F3.13.2.m2.4.4.3.5" xref="S2.F3.13.2.m2.4.4.4.cmml">,</mo><msup id="S2.F3.13.2.m2.3.3.2.2" xref="S2.F3.13.2.m2.3.3.2.2.cmml"><mi id="S2.F3.13.2.m2.3.3.2.2.2" xref="S2.F3.13.2.m2.3.3.2.2.2.cmml">d</mi><mn id="S2.F3.13.2.m2.3.3.2.2.3" xref="S2.F3.13.2.m2.3.3.2.2.3.cmml">2</mn></msup><mo id="S2.F3.13.2.m2.4.4.3.6" xref="S2.F3.13.2.m2.4.4.4.cmml">,</mo><mi mathvariant="normal" id="S2.F3.13.2.m2.1.1" xref="S2.F3.13.2.m2.1.1.cmml">…</mi><mo id="S2.F3.13.2.m2.4.4.3.7" xref="S2.F3.13.2.m2.4.4.4.cmml">,</mo><msup id="S2.F3.13.2.m2.4.4.3.3" xref="S2.F3.13.2.m2.4.4.3.3.cmml"><mi id="S2.F3.13.2.m2.4.4.3.3.2" xref="S2.F3.13.2.m2.4.4.3.3.2.cmml">d</mi><mn id="S2.F3.13.2.m2.4.4.3.3.3" xref="S2.F3.13.2.m2.4.4.3.3.3.cmml">5</mn></msup><mo id="S2.F3.13.2.m2.4.4.3.8" xref="S2.F3.13.2.m2.4.4.4.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.F3.13.2.m2.4c"><list id="S2.F3.13.2.m2.4.4.4.cmml" xref="S2.F3.13.2.m2.4.4.3"><apply id="S2.F3.13.2.m2.2.2.1.1.cmml" xref="S2.F3.13.2.m2.2.2.1.1"><csymbol cd="ambiguous" id="S2.F3.13.2.m2.2.2.1.1.1.cmml" xref="S2.F3.13.2.m2.2.2.1.1">superscript</csymbol><ci id="S2.F3.13.2.m2.2.2.1.1.2.cmml" xref="S2.F3.13.2.m2.2.2.1.1.2">𝑑</ci><cn type="integer" id="S2.F3.13.2.m2.2.2.1.1.3.cmml" xref="S2.F3.13.2.m2.2.2.1.1.3">1</cn></apply><apply id="S2.F3.13.2.m2.3.3.2.2.cmml" xref="S2.F3.13.2.m2.3.3.2.2"><csymbol cd="ambiguous" id="S2.F3.13.2.m2.3.3.2.2.1.cmml" xref="S2.F3.13.2.m2.3.3.2.2">superscript</csymbol><ci id="S2.F3.13.2.m2.3.3.2.2.2.cmml" xref="S2.F3.13.2.m2.3.3.2.2.2">𝑑</ci><cn type="integer" id="S2.F3.13.2.m2.3.3.2.2.3.cmml" xref="S2.F3.13.2.m2.3.3.2.2.3">2</cn></apply><ci id="S2.F3.13.2.m2.1.1.cmml" xref="S2.F3.13.2.m2.1.1">…</ci><apply id="S2.F3.13.2.m2.4.4.3.3.cmml" xref="S2.F3.13.2.m2.4.4.3.3"><csymbol cd="ambiguous" id="S2.F3.13.2.m2.4.4.3.3.1.cmml" xref="S2.F3.13.2.m2.4.4.3.3">superscript</csymbol><ci id="S2.F3.13.2.m2.4.4.3.3.2.cmml" xref="S2.F3.13.2.m2.4.4.3.3.2">𝑑</ci><cn type="integer" id="S2.F3.13.2.m2.4.4.3.3.3.cmml" xref="S2.F3.13.2.m2.4.4.3.3.3">5</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S2.F3.13.2.m2.4d">\left[d^{1},d^{2},\ldots,d^{5}\right]</annotation></semantics></math>, the model outputs a set of predictions. Each prediction <math id="S2.F3.14.3.m3.1" class="ltx_Math" alttext="p^{l}" display="inline"><semantics id="S2.F3.14.3.m3.1b"><msup id="S2.F3.14.3.m3.1.1" xref="S2.F3.14.3.m3.1.1.cmml"><mi id="S2.F3.14.3.m3.1.1.2" xref="S2.F3.14.3.m3.1.1.2.cmml">p</mi><mi id="S2.F3.14.3.m3.1.1.3" xref="S2.F3.14.3.m3.1.1.3.cmml">l</mi></msup><annotation-xml encoding="MathML-Content" id="S2.F3.14.3.m3.1c"><apply id="S2.F3.14.3.m3.1.1.cmml" xref="S2.F3.14.3.m3.1.1"><csymbol cd="ambiguous" id="S2.F3.14.3.m3.1.1.1.cmml" xref="S2.F3.14.3.m3.1.1">superscript</csymbol><ci id="S2.F3.14.3.m3.1.1.2.cmml" xref="S2.F3.14.3.m3.1.1.2">𝑝</ci><ci id="S2.F3.14.3.m3.1.1.3.cmml" xref="S2.F3.14.3.m3.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F3.14.3.m3.1d">p^{l}</annotation></semantics></math> is a triplet, consisting of a bounding box <math id="S2.F3.15.4.m4.1" class="ltx_Math" alttext="b^{l}" display="inline"><semantics id="S2.F3.15.4.m4.1b"><msup id="S2.F3.15.4.m4.1.1" xref="S2.F3.15.4.m4.1.1.cmml"><mi id="S2.F3.15.4.m4.1.1.2" xref="S2.F3.15.4.m4.1.1.2.cmml">b</mi><mi id="S2.F3.15.4.m4.1.1.3" xref="S2.F3.15.4.m4.1.1.3.cmml">l</mi></msup><annotation-xml encoding="MathML-Content" id="S2.F3.15.4.m4.1c"><apply id="S2.F3.15.4.m4.1.1.cmml" xref="S2.F3.15.4.m4.1.1"><csymbol cd="ambiguous" id="S2.F3.15.4.m4.1.1.1.cmml" xref="S2.F3.15.4.m4.1.1">superscript</csymbol><ci id="S2.F3.15.4.m4.1.1.2.cmml" xref="S2.F3.15.4.m4.1.1.2">𝑏</ci><ci id="S2.F3.15.4.m4.1.1.3.cmml" xref="S2.F3.15.4.m4.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F3.15.4.m4.1d">b^{l}</annotation></semantics></math> (blue boxes), a confidence score <math id="S2.F3.16.5.m5.1" class="ltx_Math" alttext="s^{l}" display="inline"><semantics id="S2.F3.16.5.m5.1b"><msup id="S2.F3.16.5.m5.1.1" xref="S2.F3.16.5.m5.1.1.cmml"><mi id="S2.F3.16.5.m5.1.1.2" xref="S2.F3.16.5.m5.1.1.2.cmml">s</mi><mi id="S2.F3.16.5.m5.1.1.3" xref="S2.F3.16.5.m5.1.1.3.cmml">l</mi></msup><annotation-xml encoding="MathML-Content" id="S2.F3.16.5.m5.1c"><apply id="S2.F3.16.5.m5.1.1.cmml" xref="S2.F3.16.5.m5.1.1"><csymbol cd="ambiguous" id="S2.F3.16.5.m5.1.1.1.cmml" xref="S2.F3.16.5.m5.1.1">superscript</csymbol><ci id="S2.F3.16.5.m5.1.1.2.cmml" xref="S2.F3.16.5.m5.1.1.2">𝑠</ci><ci id="S2.F3.16.5.m5.1.1.3.cmml" xref="S2.F3.16.5.m5.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F3.16.5.m5.1d">s^{l}</annotation></semantics></math> (not shown) and an index <math id="S2.F3.17.6.m6.1" class="ltx_Math" alttext="g^{l}" display="inline"><semantics id="S2.F3.17.6.m6.1b"><msup id="S2.F3.17.6.m6.1.1" xref="S2.F3.17.6.m6.1.1.cmml"><mi id="S2.F3.17.6.m6.1.1.2" xref="S2.F3.17.6.m6.1.1.2.cmml">g</mi><mi id="S2.F3.17.6.m6.1.1.3" xref="S2.F3.17.6.m6.1.1.3.cmml">l</mi></msup><annotation-xml encoding="MathML-Content" id="S2.F3.17.6.m6.1c"><apply id="S2.F3.17.6.m6.1.1.cmml" xref="S2.F3.17.6.m6.1.1"><csymbol cd="ambiguous" id="S2.F3.17.6.m6.1.1.1.cmml" xref="S2.F3.17.6.m6.1.1">superscript</csymbol><ci id="S2.F3.17.6.m6.1.1.2.cmml" xref="S2.F3.17.6.m6.1.1.2">𝑔</ci><ci id="S2.F3.17.6.m6.1.1.3.cmml" xref="S2.F3.17.6.m6.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F3.17.6.m6.1d">g^{l}</annotation></semantics></math>, which links the prediction to one of the object descriptions <math id="S2.F3.18.7.m7.1" class="ltx_Math" alttext="d^{*}" display="inline"><semantics id="S2.F3.18.7.m7.1b"><msup id="S2.F3.18.7.m7.1.1" xref="S2.F3.18.7.m7.1.1.cmml"><mi id="S2.F3.18.7.m7.1.1.2" xref="S2.F3.18.7.m7.1.1.2.cmml">d</mi><mo id="S2.F3.18.7.m7.1.1.3" xref="S2.F3.18.7.m7.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S2.F3.18.7.m7.1c"><apply id="S2.F3.18.7.m7.1.1.cmml" xref="S2.F3.18.7.m7.1.1"><csymbol cd="ambiguous" id="S2.F3.18.7.m7.1.1.1.cmml" xref="S2.F3.18.7.m7.1.1">superscript</csymbol><ci id="S2.F3.18.7.m7.1.1.2.cmml" xref="S2.F3.18.7.m7.1.1.2">𝑑</ci><times id="S2.F3.18.7.m7.1.1.3.cmml" xref="S2.F3.18.7.m7.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F3.18.7.m7.1d">d^{*}</annotation></semantics></math> (the figure groups predicted boxes by <math id="S2.F3.19.8.m8.1" class="ltx_Math" alttext="g^{l}" display="inline"><semantics id="S2.F3.19.8.m8.1b"><msup id="S2.F3.19.8.m8.1.1" xref="S2.F3.19.8.m8.1.1.cmml"><mi id="S2.F3.19.8.m8.1.1.2" xref="S2.F3.19.8.m8.1.1.2.cmml">g</mi><mi id="S2.F3.19.8.m8.1.1.3" xref="S2.F3.19.8.m8.1.1.3.cmml">l</mi></msup><annotation-xml encoding="MathML-Content" id="S2.F3.19.8.m8.1c"><apply id="S2.F3.19.8.m8.1.1.cmml" xref="S2.F3.19.8.m8.1.1"><csymbol cd="ambiguous" id="S2.F3.19.8.m8.1.1.1.cmml" xref="S2.F3.19.8.m8.1.1">superscript</csymbol><ci id="S2.F3.19.8.m8.1.1.2.cmml" xref="S2.F3.19.8.m8.1.1.2">𝑔</ci><ci id="S2.F3.19.8.m8.1.1.3.cmml" xref="S2.F3.19.8.m8.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F3.19.8.m8.1d">g^{l}</annotation></semantics></math>). For evaluation, predictions and ground truth (green boxes) are matched based on location (intersection-over-union) as well as the indices <math id="S2.F3.20.9.m9.1" class="ltx_Math" alttext="g^{l}" display="inline"><semantics id="S2.F3.20.9.m9.1b"><msup id="S2.F3.20.9.m9.1.1" xref="S2.F3.20.9.m9.1.1.cmml"><mi id="S2.F3.20.9.m9.1.1.2" xref="S2.F3.20.9.m9.1.1.2.cmml">g</mi><mi id="S2.F3.20.9.m9.1.1.3" xref="S2.F3.20.9.m9.1.1.3.cmml">l</mi></msup><annotation-xml encoding="MathML-Content" id="S2.F3.20.9.m9.1c"><apply id="S2.F3.20.9.m9.1.1.cmml" xref="S2.F3.20.9.m9.1.1"><csymbol cd="ambiguous" id="S2.F3.20.9.m9.1.1.1.cmml" xref="S2.F3.20.9.m9.1.1">superscript</csymbol><ci id="S2.F3.20.9.m9.1.1.2.cmml" xref="S2.F3.20.9.m9.1.1.2">𝑔</ci><ci id="S2.F3.20.9.m9.1.1.3.cmml" xref="S2.F3.20.9.m9.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F3.20.9.m9.1d">g^{l}</annotation></semantics></math>. In contrast to standard detection, note that one object can be matched with multiple (but different!) descriptions. For example, the woman holding the green umbrella is matched with both <math id="S2.F3.21.10.m10.1" class="ltx_Math" alttext="d^{1}" display="inline"><semantics id="S2.F3.21.10.m10.1b"><msup id="S2.F3.21.10.m10.1.1" xref="S2.F3.21.10.m10.1.1.cmml"><mi id="S2.F3.21.10.m10.1.1.2" xref="S2.F3.21.10.m10.1.1.2.cmml">d</mi><mn id="S2.F3.21.10.m10.1.1.3" xref="S2.F3.21.10.m10.1.1.3.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="S2.F3.21.10.m10.1c"><apply id="S2.F3.21.10.m10.1.1.cmml" xref="S2.F3.21.10.m10.1.1"><csymbol cd="ambiguous" id="S2.F3.21.10.m10.1.1.1.cmml" xref="S2.F3.21.10.m10.1.1">superscript</csymbol><ci id="S2.F3.21.10.m10.1.1.2.cmml" xref="S2.F3.21.10.m10.1.1.2">𝑑</ci><cn type="integer" id="S2.F3.21.10.m10.1.1.3.cmml" xref="S2.F3.21.10.m10.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F3.21.10.m10.1d">d^{1}</annotation></semantics></math> and <math id="S2.F3.22.11.m11.1" class="ltx_Math" alttext="d^{5}" display="inline"><semantics id="S2.F3.22.11.m11.1b"><msup id="S2.F3.22.11.m11.1.1" xref="S2.F3.22.11.m11.1.1.cmml"><mi id="S2.F3.22.11.m11.1.1.2" xref="S2.F3.22.11.m11.1.1.2.cmml">d</mi><mn id="S2.F3.22.11.m11.1.1.3" xref="S2.F3.22.11.m11.1.1.3.cmml">5</mn></msup><annotation-xml encoding="MathML-Content" id="S2.F3.22.11.m11.1c"><apply id="S2.F3.22.11.m11.1.1.cmml" xref="S2.F3.22.11.m11.1.1"><csymbol cd="ambiguous" id="S2.F3.22.11.m11.1.1.1.cmml" xref="S2.F3.22.11.m11.1.1">superscript</csymbol><ci id="S2.F3.22.11.m11.1.1.2.cmml" xref="S2.F3.22.11.m11.1.1.2">𝑑</ci><cn type="integer" id="S2.F3.22.11.m11.1.1.3.cmml" xref="S2.F3.22.11.m11.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F3.22.11.m11.1d">d^{5}</annotation></semantics></math>. While there is no category-wise grouping as in standard detection, average precision (AP) metrics are computed for other groups, like “plain categories”, “free-form object descriptions”, and others. The final metric is the harmonic mean between AP values for “plain categories” and “free-form object descriptions”.</span></figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Benchmark and Evaluation Metric</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">This section provides a formal definition of the benchmark task and the corresponding evaluation metric. An illustration of both is given in <a href="#S2.F3" title="In Open-Vocabulary Object Detection: ‣ 2 Related Work ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Benchmark Task</h3>

<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Input:</h4>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p1.12" class="ltx_p">Given a regular RGB image <math id="S3.SS1.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="I_{i}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.1.m1.1a"><msub id="S3.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml">I</mi><mi id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.1.m1.1b"><apply id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2">𝐼</ci><ci id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.1.m1.1c">I_{i}</annotation></semantics></math> along with a label space <math id="S3.SS1.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="D_{i}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.2.m2.1a"><msub id="S3.SS1.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.2" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.2.cmml">D</mi><mi id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.2.m2.1b"><apply id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.2">𝐷</ci><ci id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.2.m2.1c">D_{i}</annotation></semantics></math>, the task for model <math id="S3.SS1.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.3.m3.1a"><mi id="S3.SS1.SSS0.Px1.p1.3.m3.1.1" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.3.m3.1b"><ci id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.3.m3.1c">M</annotation></semantics></math> is to output object predictions <math id="S3.SS1.SSS0.Px1.p1.4.m4.1" class="ltx_Math" alttext="P_{i}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.4.m4.1a"><msub id="S3.SS1.SSS0.Px1.p1.4.m4.1.1" xref="S3.SS1.SSS0.Px1.p1.4.m4.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p1.4.m4.1.1.2" xref="S3.SS1.SSS0.Px1.p1.4.m4.1.1.2.cmml">P</mi><mi id="S3.SS1.SSS0.Px1.p1.4.m4.1.1.3" xref="S3.SS1.SSS0.Px1.p1.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.4.m4.1b"><apply id="S3.SS1.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.4.m4.1.1.2">𝑃</ci><ci id="S3.SS1.SSS0.Px1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.4.m4.1c">P_{i}</annotation></semantics></math> according to the label space <math id="S3.SS1.SSS0.Px1.p1.5.m5.1" class="ltx_Math" alttext="D_{i}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.5.m5.1a"><msub id="S3.SS1.SSS0.Px1.p1.5.m5.1.1" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.2" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1.2.cmml">D</mi><mi id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.3" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.5.m5.1b"><apply id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1.2">𝐷</ci><ci id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.5.m5.1c">D_{i}</annotation></semantics></math>. The subscript in <math id="S3.SS1.SSS0.Px1.p1.6.m6.1" class="ltx_Math" alttext="D_{i}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.6.m6.1a"><msub id="S3.SS1.SSS0.Px1.p1.6.m6.1.1" xref="S3.SS1.SSS0.Px1.p1.6.m6.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p1.6.m6.1.1.2" xref="S3.SS1.SSS0.Px1.p1.6.m6.1.1.2.cmml">D</mi><mi id="S3.SS1.SSS0.Px1.p1.6.m6.1.1.3" xref="S3.SS1.SSS0.Px1.p1.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.6.m6.1b"><apply id="S3.SS1.SSS0.Px1.p1.6.m6.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.6.m6.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.6.m6.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.1.1.2">𝐷</ci><ci id="S3.SS1.SSS0.Px1.p1.6.m6.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.6.m6.1c">D_{i}</annotation></semantics></math> indicates that both content and size <math id="S3.SS1.SSS0.Px1.p1.7.m7.1" class="ltx_Math" alttext="|D_{i}|" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.7.m7.1a"><mrow id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.1" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.2.cmml"><mo stretchy="false" id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.1.2" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.2.1.cmml">|</mo><msub id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.1.1" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.1.1.2" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.1.1.2.cmml">D</mi><mi id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.1.1.3" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.1.3" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.2.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.7.m7.1b"><apply id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.1"><abs id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.2.1.cmml" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.1.2"></abs><apply id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.1.1.2">𝐷</ci><ci id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.1.1.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.7.m7.1c">|D_{i}|</annotation></semantics></math> vary for each image <math id="S3.SS1.SSS0.Px1.p1.8.m8.1" class="ltx_Math" alttext="I_{i}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.8.m8.1a"><msub id="S3.SS1.SSS0.Px1.p1.8.m8.1.1" xref="S3.SS1.SSS0.Px1.p1.8.m8.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p1.8.m8.1.1.2" xref="S3.SS1.SSS0.Px1.p1.8.m8.1.1.2.cmml">I</mi><mi id="S3.SS1.SSS0.Px1.p1.8.m8.1.1.3" xref="S3.SS1.SSS0.Px1.p1.8.m8.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.8.m8.1b"><apply id="S3.SS1.SSS0.Px1.p1.8.m8.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.8.m8.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.8.m8.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.8.m8.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.8.m8.1.1.2">𝐼</ci><ci id="S3.SS1.SSS0.Px1.p1.8.m8.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.8.m8.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.8.m8.1c">I_{i}</annotation></semantics></math>. The label space <math id="S3.SS1.SSS0.Px1.p1.9.m9.2" class="ltx_Math" alttext="D_{i}=\left[d_{i}^{k}\right]_{k=1}^{|D_{i}|}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.9.m9.2a"><mrow id="S3.SS1.SSS0.Px1.p1.9.m9.2.2" xref="S3.SS1.SSS0.Px1.p1.9.m9.2.2.cmml"><msub id="S3.SS1.SSS0.Px1.p1.9.m9.2.2.3" xref="S3.SS1.SSS0.Px1.p1.9.m9.2.2.3.cmml"><mi id="S3.SS1.SSS0.Px1.p1.9.m9.2.2.3.2" xref="S3.SS1.SSS0.Px1.p1.9.m9.2.2.3.2.cmml">D</mi><mi id="S3.SS1.SSS0.Px1.p1.9.m9.2.2.3.3" xref="S3.SS1.SSS0.Px1.p1.9.m9.2.2.3.3.cmml">i</mi></msub><mo id="S3.SS1.SSS0.Px1.p1.9.m9.2.2.2" xref="S3.SS1.SSS0.Px1.p1.9.m9.2.2.2.cmml">=</mo><msubsup id="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1" xref="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.cmml"><mrow id="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.1.1" xref="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.1.2.cmml"><mo id="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.1.1.2" xref="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.1.2.1.cmml">[</mo><msubsup id="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.1.1.1" xref="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.1.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.1.1.1.2.2" xref="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.1.1.1.2.2.cmml">d</mi><mi id="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.1.1.1.2.3" xref="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.1.1.1.2.3.cmml">i</mi><mi id="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.1.1.1.3" xref="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.1.1.1.3.cmml">k</mi></msubsup><mo id="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.1.1.3" xref="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.1.2.1.cmml">]</mo></mrow><mrow id="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.3" xref="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.3.cmml"><mi id="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.3.2" xref="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.3.2.cmml">k</mi><mo id="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.3.1" xref="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.3.1.cmml">=</mo><mn id="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.3.3" xref="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.3.3.cmml">1</mn></mrow><mrow id="S3.SS1.SSS0.Px1.p1.9.m9.1.1.1.1" xref="S3.SS1.SSS0.Px1.p1.9.m9.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS1.SSS0.Px1.p1.9.m9.1.1.1.1.2" xref="S3.SS1.SSS0.Px1.p1.9.m9.1.1.1.2.1.cmml">|</mo><msub id="S3.SS1.SSS0.Px1.p1.9.m9.1.1.1.1.1" xref="S3.SS1.SSS0.Px1.p1.9.m9.1.1.1.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p1.9.m9.1.1.1.1.1.2" xref="S3.SS1.SSS0.Px1.p1.9.m9.1.1.1.1.1.2.cmml">D</mi><mi id="S3.SS1.SSS0.Px1.p1.9.m9.1.1.1.1.1.3" xref="S3.SS1.SSS0.Px1.p1.9.m9.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS1.SSS0.Px1.p1.9.m9.1.1.1.1.3" xref="S3.SS1.SSS0.Px1.p1.9.m9.1.1.1.2.1.cmml">|</mo></mrow></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.9.m9.2b"><apply id="S3.SS1.SSS0.Px1.p1.9.m9.2.2.cmml" xref="S3.SS1.SSS0.Px1.p1.9.m9.2.2"><eq id="S3.SS1.SSS0.Px1.p1.9.m9.2.2.2.cmml" xref="S3.SS1.SSS0.Px1.p1.9.m9.2.2.2"></eq><apply id="S3.SS1.SSS0.Px1.p1.9.m9.2.2.3.cmml" xref="S3.SS1.SSS0.Px1.p1.9.m9.2.2.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.9.m9.2.2.3.1.cmml" xref="S3.SS1.SSS0.Px1.p1.9.m9.2.2.3">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.9.m9.2.2.3.2.cmml" xref="S3.SS1.SSS0.Px1.p1.9.m9.2.2.3.2">𝐷</ci><ci id="S3.SS1.SSS0.Px1.p1.9.m9.2.2.3.3.cmml" xref="S3.SS1.SSS0.Px1.p1.9.m9.2.2.3.3">𝑖</ci></apply><apply id="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.cmml" xref="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1">superscript</csymbol><apply id="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1">subscript</csymbol><apply id="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.1.1"><csymbol cd="latexml" id="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.1.2.1.cmml" xref="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.1.1.1">superscript</csymbol><apply id="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.1.1.1.2.1.cmml" xref="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.1.1.1.2.2.cmml" xref="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.1.1.1.2.2">𝑑</ci><ci id="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.1.1.1.2.3.cmml" xref="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.1.1.1.2.3">𝑖</ci></apply><ci id="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.1.1.1.3">𝑘</ci></apply></apply><apply id="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.3"><eq id="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.3.1.cmml" xref="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.3.1"></eq><ci id="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.3.2.cmml" xref="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.3.2">𝑘</ci><cn type="integer" id="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.3.3.cmml" xref="S3.SS1.SSS0.Px1.p1.9.m9.2.2.1.1.3.3">1</cn></apply></apply><apply id="S3.SS1.SSS0.Px1.p1.9.m9.1.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.9.m9.1.1.1.1"><abs id="S3.SS1.SSS0.Px1.p1.9.m9.1.1.1.2.1.cmml" xref="S3.SS1.SSS0.Px1.p1.9.m9.1.1.1.1.2"></abs><apply id="S3.SS1.SSS0.Px1.p1.9.m9.1.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.9.m9.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.9.m9.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.9.m9.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.9.m9.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.9.m9.1.1.1.1.1.2">𝐷</ci><ci id="S3.SS1.SSS0.Px1.p1.9.m9.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.9.m9.1.1.1.1.1.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.9.m9.2c">D_{i}=\left[d_{i}^{k}\right]_{k=1}^{|D_{i}|}</annotation></semantics></math> consists of <math id="S3.SS1.SSS0.Px1.p1.10.m10.1" class="ltx_Math" alttext="|D_{i}|" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.10.m10.1a"><mrow id="S3.SS1.SSS0.Px1.p1.10.m10.1.1.1" xref="S3.SS1.SSS0.Px1.p1.10.m10.1.1.2.cmml"><mo stretchy="false" id="S3.SS1.SSS0.Px1.p1.10.m10.1.1.1.2" xref="S3.SS1.SSS0.Px1.p1.10.m10.1.1.2.1.cmml">|</mo><msub id="S3.SS1.SSS0.Px1.p1.10.m10.1.1.1.1" xref="S3.SS1.SSS0.Px1.p1.10.m10.1.1.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p1.10.m10.1.1.1.1.2" xref="S3.SS1.SSS0.Px1.p1.10.m10.1.1.1.1.2.cmml">D</mi><mi id="S3.SS1.SSS0.Px1.p1.10.m10.1.1.1.1.3" xref="S3.SS1.SSS0.Px1.p1.10.m10.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS1.SSS0.Px1.p1.10.m10.1.1.1.3" xref="S3.SS1.SSS0.Px1.p1.10.m10.1.1.2.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.10.m10.1b"><apply id="S3.SS1.SSS0.Px1.p1.10.m10.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.10.m10.1.1.1"><abs id="S3.SS1.SSS0.Px1.p1.10.m10.1.1.2.1.cmml" xref="S3.SS1.SSS0.Px1.p1.10.m10.1.1.1.2"></abs><apply id="S3.SS1.SSS0.Px1.p1.10.m10.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.10.m10.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.10.m10.1.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.10.m10.1.1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.10.m10.1.1.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.10.m10.1.1.1.1.2">𝐷</ci><ci id="S3.SS1.SSS0.Px1.p1.10.m10.1.1.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.10.m10.1.1.1.1.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.10.m10.1c">|D_{i}|</annotation></semantics></math> elements, <math id="S3.SS1.SSS0.Px1.p1.11.m11.1" class="ltx_Math" alttext="d_{i}^{k}\in D_{i}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.11.m11.1a"><mrow id="S3.SS1.SSS0.Px1.p1.11.m11.1.1" xref="S3.SS1.SSS0.Px1.p1.11.m11.1.1.cmml"><msubsup id="S3.SS1.SSS0.Px1.p1.11.m11.1.1.2" xref="S3.SS1.SSS0.Px1.p1.11.m11.1.1.2.cmml"><mi id="S3.SS1.SSS0.Px1.p1.11.m11.1.1.2.2.2" xref="S3.SS1.SSS0.Px1.p1.11.m11.1.1.2.2.2.cmml">d</mi><mi id="S3.SS1.SSS0.Px1.p1.11.m11.1.1.2.2.3" xref="S3.SS1.SSS0.Px1.p1.11.m11.1.1.2.2.3.cmml">i</mi><mi id="S3.SS1.SSS0.Px1.p1.11.m11.1.1.2.3" xref="S3.SS1.SSS0.Px1.p1.11.m11.1.1.2.3.cmml">k</mi></msubsup><mo id="S3.SS1.SSS0.Px1.p1.11.m11.1.1.1" xref="S3.SS1.SSS0.Px1.p1.11.m11.1.1.1.cmml">∈</mo><msub id="S3.SS1.SSS0.Px1.p1.11.m11.1.1.3" xref="S3.SS1.SSS0.Px1.p1.11.m11.1.1.3.cmml"><mi id="S3.SS1.SSS0.Px1.p1.11.m11.1.1.3.2" xref="S3.SS1.SSS0.Px1.p1.11.m11.1.1.3.2.cmml">D</mi><mi id="S3.SS1.SSS0.Px1.p1.11.m11.1.1.3.3" xref="S3.SS1.SSS0.Px1.p1.11.m11.1.1.3.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.11.m11.1b"><apply id="S3.SS1.SSS0.Px1.p1.11.m11.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.11.m11.1.1"><in id="S3.SS1.SSS0.Px1.p1.11.m11.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.11.m11.1.1.1"></in><apply id="S3.SS1.SSS0.Px1.p1.11.m11.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.11.m11.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.11.m11.1.1.2.1.cmml" xref="S3.SS1.SSS0.Px1.p1.11.m11.1.1.2">superscript</csymbol><apply id="S3.SS1.SSS0.Px1.p1.11.m11.1.1.2.2.cmml" xref="S3.SS1.SSS0.Px1.p1.11.m11.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.11.m11.1.1.2.2.1.cmml" xref="S3.SS1.SSS0.Px1.p1.11.m11.1.1.2">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.11.m11.1.1.2.2.2.cmml" xref="S3.SS1.SSS0.Px1.p1.11.m11.1.1.2.2.2">𝑑</ci><ci id="S3.SS1.SSS0.Px1.p1.11.m11.1.1.2.2.3.cmml" xref="S3.SS1.SSS0.Px1.p1.11.m11.1.1.2.2.3">𝑖</ci></apply><ci id="S3.SS1.SSS0.Px1.p1.11.m11.1.1.2.3.cmml" xref="S3.SS1.SSS0.Px1.p1.11.m11.1.1.2.3">𝑘</ci></apply><apply id="S3.SS1.SSS0.Px1.p1.11.m11.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.11.m11.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.11.m11.1.1.3.1.cmml" xref="S3.SS1.SSS0.Px1.p1.11.m11.1.1.3">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.11.m11.1.1.3.2.cmml" xref="S3.SS1.SSS0.Px1.p1.11.m11.1.1.3.2">𝐷</ci><ci id="S3.SS1.SSS0.Px1.p1.11.m11.1.1.3.3.cmml" xref="S3.SS1.SSS0.Px1.p1.11.m11.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.11.m11.1c">d_{i}^{k}\in D_{i}</annotation></semantics></math>, each of which is called an “object description”. Our object descriptions comprise a combination of plain category names (as in detection) as well as newly-collected free-form text descriptions of objects, see <a href="#S4" title="4 Dataset Collection ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4</span></a>. Being free-form text effectively makes each description unique. While we could define a common label space as the union of all descriptions, this results in a huge label space and poses hard computational challenges on models that tightly fuse image and text, like MDETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. Instead, we vary the label space and each <math id="S3.SS1.SSS0.Px1.p1.12.m12.1" class="ltx_Math" alttext="D_{i}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.12.m12.1a"><msub id="S3.SS1.SSS0.Px1.p1.12.m12.1.1" xref="S3.SS1.SSS0.Px1.p1.12.m12.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p1.12.m12.1.1.2" xref="S3.SS1.SSS0.Px1.p1.12.m12.1.1.2.cmml">D</mi><mi id="S3.SS1.SSS0.Px1.p1.12.m12.1.1.3" xref="S3.SS1.SSS0.Px1.p1.12.m12.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.12.m12.1b"><apply id="S3.SS1.SSS0.Px1.p1.12.m12.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.12.m12.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.12.m12.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.12.m12.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.12.m12.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.12.m12.1.1.2">𝐷</ci><ci id="S3.SS1.SSS0.Px1.p1.12.m12.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.12.m12.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.12.m12.1c">D_{i}</annotation></semantics></math> contains both positive (referring to an object in the image) and negative (related to image content but no related objects) descriptions. Examples of free-form object descriptions are given in <a href="#S1.F2" title="In 1 Introduction ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Output:</h4>

<div id="S3.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p1.18" class="ltx_p">Model <math id="S3.SS1.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.1.m1.1a"><mi id="S3.SS1.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.1.m1.1b"><ci id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.1.m1.1c">M</annotation></semantics></math> must output a set of triplets <math id="S3.SS1.SSS0.Px2.p1.2.m2.2" class="ltx_Math" alttext="P_{i}=\left[p_{i}^{l}\right]_{l=1}^{|P_{i}|}" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.2.m2.2a"><mrow id="S3.SS1.SSS0.Px2.p1.2.m2.2.2" xref="S3.SS1.SSS0.Px2.p1.2.m2.2.2.cmml"><msub id="S3.SS1.SSS0.Px2.p1.2.m2.2.2.3" xref="S3.SS1.SSS0.Px2.p1.2.m2.2.2.3.cmml"><mi id="S3.SS1.SSS0.Px2.p1.2.m2.2.2.3.2" xref="S3.SS1.SSS0.Px2.p1.2.m2.2.2.3.2.cmml">P</mi><mi id="S3.SS1.SSS0.Px2.p1.2.m2.2.2.3.3" xref="S3.SS1.SSS0.Px2.p1.2.m2.2.2.3.3.cmml">i</mi></msub><mo id="S3.SS1.SSS0.Px2.p1.2.m2.2.2.2" xref="S3.SS1.SSS0.Px2.p1.2.m2.2.2.2.cmml">=</mo><msubsup id="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1" xref="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.cmml"><mrow id="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.1.1" xref="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.1.2.cmml"><mo id="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.1.1.2" xref="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.1.2.1.cmml">[</mo><msubsup id="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.1.1.1" xref="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.1.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.1.1.1.2.2" xref="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.1.1.1.2.2.cmml">p</mi><mi id="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.1.1.1.2.3" xref="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.1.1.1.2.3.cmml">i</mi><mi id="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.1.1.1.3" xref="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.1.1.1.3.cmml">l</mi></msubsup><mo id="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.1.1.3" xref="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.1.2.1.cmml">]</mo></mrow><mrow id="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.3" xref="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.3.cmml"><mi id="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.3.2" xref="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.3.2.cmml">l</mi><mo id="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.3.1" xref="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.3.1.cmml">=</mo><mn id="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.3.3" xref="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.3.3.cmml">1</mn></mrow><mrow id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.1.1" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.1.1.2" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.1.2.1.cmml">|</mo><msub id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.1.1.1" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.1.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.1.1.1.2" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.1.1.1.2.cmml">P</mi><mi id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.1.1.1.3" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.1.1.3" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.1.2.1.cmml">|</mo></mrow></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.2.m2.2b"><apply id="S3.SS1.SSS0.Px2.p1.2.m2.2.2.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.2.2"><eq id="S3.SS1.SSS0.Px2.p1.2.m2.2.2.2.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.2.2.2"></eq><apply id="S3.SS1.SSS0.Px2.p1.2.m2.2.2.3.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.2.2.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.2.m2.2.2.3.1.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.2.2.3">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.2.m2.2.2.3.2.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.2.2.3.2">𝑃</ci><ci id="S3.SS1.SSS0.Px2.p1.2.m2.2.2.3.3.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.2.2.3.3">𝑖</ci></apply><apply id="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1">superscript</csymbol><apply id="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1">subscript</csymbol><apply id="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.1.1"><csymbol cd="latexml" id="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.1.2.1.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.1.1.1">superscript</csymbol><apply id="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.1.1.1.2.1.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.1.1.1.2.2.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.1.1.1.2.2">𝑝</ci><ci id="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.1.1.1.2.3.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.1.1.1.2.3">𝑖</ci></apply><ci id="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.1.1.1.3">𝑙</ci></apply></apply><apply id="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.3"><eq id="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.3.1.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.3.1"></eq><ci id="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.3.2.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.3.2">𝑙</ci><cn type="integer" id="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.3.3.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.2.2.1.1.3.3">1</cn></apply></apply><apply id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.1.1"><abs id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.1.2.1.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.1.1.2"></abs><apply id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.1.1.1.2">𝑃</ci><ci id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.1.1.1.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.2.m2.2c">P_{i}=\left[p_{i}^{l}\right]_{l=1}^{|P_{i}|}</annotation></semantics></math> for image <math id="S3.SS1.SSS0.Px2.p1.3.m3.1" class="ltx_Math" alttext="I_{i}" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.3.m3.1a"><msub id="S3.SS1.SSS0.Px2.p1.3.m3.1.1" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p1.3.m3.1.1.2" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1.2.cmml">I</mi><mi id="S3.SS1.SSS0.Px2.p1.3.m3.1.1.3" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.3.m3.1b"><apply id="S3.SS1.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.3.m3.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.3.m3.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1.2">𝐼</ci><ci id="S3.SS1.SSS0.Px2.p1.3.m3.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.3.m3.1c">I_{i}</annotation></semantics></math> and label space <math id="S3.SS1.SSS0.Px2.p1.4.m4.1" class="ltx_Math" alttext="D_{i}" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.4.m4.1a"><msub id="S3.SS1.SSS0.Px2.p1.4.m4.1.1" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.2" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.2.cmml">D</mi><mi id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.4.m4.1b"><apply id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.2">𝐷</ci><ci id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.4.m4.1c">D_{i}</annotation></semantics></math>. Each triplet <math id="S3.SS1.SSS0.Px2.p1.5.m5.3" class="ltx_Math" alttext="p_{i}^{l}=(b_{i}^{l},s_{i}^{l},g_{i}^{l})" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.5.m5.3a"><mrow id="S3.SS1.SSS0.Px2.p1.5.m5.3.3" xref="S3.SS1.SSS0.Px2.p1.5.m5.3.3.cmml"><msubsup id="S3.SS1.SSS0.Px2.p1.5.m5.3.3.5" xref="S3.SS1.SSS0.Px2.p1.5.m5.3.3.5.cmml"><mi id="S3.SS1.SSS0.Px2.p1.5.m5.3.3.5.2.2" xref="S3.SS1.SSS0.Px2.p1.5.m5.3.3.5.2.2.cmml">p</mi><mi id="S3.SS1.SSS0.Px2.p1.5.m5.3.3.5.2.3" xref="S3.SS1.SSS0.Px2.p1.5.m5.3.3.5.2.3.cmml">i</mi><mi id="S3.SS1.SSS0.Px2.p1.5.m5.3.3.5.3" xref="S3.SS1.SSS0.Px2.p1.5.m5.3.3.5.3.cmml">l</mi></msubsup><mo id="S3.SS1.SSS0.Px2.p1.5.m5.3.3.4" xref="S3.SS1.SSS0.Px2.p1.5.m5.3.3.4.cmml">=</mo><mrow id="S3.SS1.SSS0.Px2.p1.5.m5.3.3.3.3" xref="S3.SS1.SSS0.Px2.p1.5.m5.3.3.3.4.cmml"><mo stretchy="false" id="S3.SS1.SSS0.Px2.p1.5.m5.3.3.3.3.4" xref="S3.SS1.SSS0.Px2.p1.5.m5.3.3.3.4.cmml">(</mo><msubsup id="S3.SS1.SSS0.Px2.p1.5.m5.1.1.1.1.1" xref="S3.SS1.SSS0.Px2.p1.5.m5.1.1.1.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p1.5.m5.1.1.1.1.1.2.2" xref="S3.SS1.SSS0.Px2.p1.5.m5.1.1.1.1.1.2.2.cmml">b</mi><mi id="S3.SS1.SSS0.Px2.p1.5.m5.1.1.1.1.1.2.3" xref="S3.SS1.SSS0.Px2.p1.5.m5.1.1.1.1.1.2.3.cmml">i</mi><mi id="S3.SS1.SSS0.Px2.p1.5.m5.1.1.1.1.1.3" xref="S3.SS1.SSS0.Px2.p1.5.m5.1.1.1.1.1.3.cmml">l</mi></msubsup><mo id="S3.SS1.SSS0.Px2.p1.5.m5.3.3.3.3.5" xref="S3.SS1.SSS0.Px2.p1.5.m5.3.3.3.4.cmml">,</mo><msubsup id="S3.SS1.SSS0.Px2.p1.5.m5.2.2.2.2.2" xref="S3.SS1.SSS0.Px2.p1.5.m5.2.2.2.2.2.cmml"><mi id="S3.SS1.SSS0.Px2.p1.5.m5.2.2.2.2.2.2.2" xref="S3.SS1.SSS0.Px2.p1.5.m5.2.2.2.2.2.2.2.cmml">s</mi><mi id="S3.SS1.SSS0.Px2.p1.5.m5.2.2.2.2.2.2.3" xref="S3.SS1.SSS0.Px2.p1.5.m5.2.2.2.2.2.2.3.cmml">i</mi><mi id="S3.SS1.SSS0.Px2.p1.5.m5.2.2.2.2.2.3" xref="S3.SS1.SSS0.Px2.p1.5.m5.2.2.2.2.2.3.cmml">l</mi></msubsup><mo id="S3.SS1.SSS0.Px2.p1.5.m5.3.3.3.3.6" xref="S3.SS1.SSS0.Px2.p1.5.m5.3.3.3.4.cmml">,</mo><msubsup id="S3.SS1.SSS0.Px2.p1.5.m5.3.3.3.3.3" xref="S3.SS1.SSS0.Px2.p1.5.m5.3.3.3.3.3.cmml"><mi id="S3.SS1.SSS0.Px2.p1.5.m5.3.3.3.3.3.2.2" xref="S3.SS1.SSS0.Px2.p1.5.m5.3.3.3.3.3.2.2.cmml">g</mi><mi id="S3.SS1.SSS0.Px2.p1.5.m5.3.3.3.3.3.2.3" xref="S3.SS1.SSS0.Px2.p1.5.m5.3.3.3.3.3.2.3.cmml">i</mi><mi id="S3.SS1.SSS0.Px2.p1.5.m5.3.3.3.3.3.3" xref="S3.SS1.SSS0.Px2.p1.5.m5.3.3.3.3.3.3.cmml">l</mi></msubsup><mo stretchy="false" id="S3.SS1.SSS0.Px2.p1.5.m5.3.3.3.3.7" xref="S3.SS1.SSS0.Px2.p1.5.m5.3.3.3.4.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.5.m5.3b"><apply id="S3.SS1.SSS0.Px2.p1.5.m5.3.3.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.3.3"><eq id="S3.SS1.SSS0.Px2.p1.5.m5.3.3.4.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.3.3.4"></eq><apply id="S3.SS1.SSS0.Px2.p1.5.m5.3.3.5.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.3.3.5"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.5.m5.3.3.5.1.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.3.3.5">superscript</csymbol><apply id="S3.SS1.SSS0.Px2.p1.5.m5.3.3.5.2.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.3.3.5"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.5.m5.3.3.5.2.1.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.3.3.5">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.5.m5.3.3.5.2.2.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.3.3.5.2.2">𝑝</ci><ci id="S3.SS1.SSS0.Px2.p1.5.m5.3.3.5.2.3.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.3.3.5.2.3">𝑖</ci></apply><ci id="S3.SS1.SSS0.Px2.p1.5.m5.3.3.5.3.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.3.3.5.3">𝑙</ci></apply><vector id="S3.SS1.SSS0.Px2.p1.5.m5.3.3.3.4.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.3.3.3.3"><apply id="S3.SS1.SSS0.Px2.p1.5.m5.1.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.5.m5.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.1.1.1.1.1">superscript</csymbol><apply id="S3.SS1.SSS0.Px2.p1.5.m5.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.5.m5.1.1.1.1.1.2.1.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.5.m5.1.1.1.1.1.2.2.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.1.1.1.1.1.2.2">𝑏</ci><ci id="S3.SS1.SSS0.Px2.p1.5.m5.1.1.1.1.1.2.3.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.1.1.1.1.1.2.3">𝑖</ci></apply><ci id="S3.SS1.SSS0.Px2.p1.5.m5.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.1.1.1.1.1.3">𝑙</ci></apply><apply id="S3.SS1.SSS0.Px2.p1.5.m5.2.2.2.2.2.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.5.m5.2.2.2.2.2.1.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.2.2.2.2.2">superscript</csymbol><apply id="S3.SS1.SSS0.Px2.p1.5.m5.2.2.2.2.2.2.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.5.m5.2.2.2.2.2.2.1.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.2.2.2.2.2">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.5.m5.2.2.2.2.2.2.2.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.2.2.2.2.2.2.2">𝑠</ci><ci id="S3.SS1.SSS0.Px2.p1.5.m5.2.2.2.2.2.2.3.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.2.2.2.2.2.2.3">𝑖</ci></apply><ci id="S3.SS1.SSS0.Px2.p1.5.m5.2.2.2.2.2.3.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.2.2.2.2.2.3">𝑙</ci></apply><apply id="S3.SS1.SSS0.Px2.p1.5.m5.3.3.3.3.3.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.5.m5.3.3.3.3.3.1.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.3.3.3.3.3">superscript</csymbol><apply id="S3.SS1.SSS0.Px2.p1.5.m5.3.3.3.3.3.2.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.5.m5.3.3.3.3.3.2.1.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.3.3.3.3.3">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.5.m5.3.3.3.3.3.2.2.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.3.3.3.3.3.2.2">𝑔</ci><ci id="S3.SS1.SSS0.Px2.p1.5.m5.3.3.3.3.3.2.3.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.3.3.3.3.3.2.3">𝑖</ci></apply><ci id="S3.SS1.SSS0.Px2.p1.5.m5.3.3.3.3.3.3.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.3.3.3.3.3.3">𝑙</ci></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.5.m5.3c">p_{i}^{l}=(b_{i}^{l},s_{i}^{l},g_{i}^{l})</annotation></semantics></math> consists of a bounding box <math id="S3.SS1.SSS0.Px2.p1.6.m6.1" class="ltx_Math" alttext="b" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.6.m6.1a"><mi id="S3.SS1.SSS0.Px2.p1.6.m6.1.1" xref="S3.SS1.SSS0.Px2.p1.6.m6.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.6.m6.1b"><ci id="S3.SS1.SSS0.Px2.p1.6.m6.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.6.m6.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.6.m6.1c">b</annotation></semantics></math>, a confidence score <math id="S3.SS1.SSS0.Px2.p1.7.m7.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.7.m7.1a"><mi id="S3.SS1.SSS0.Px2.p1.7.m7.1.1" xref="S3.SS1.SSS0.Px2.p1.7.m7.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.7.m7.1b"><ci id="S3.SS1.SSS0.Px2.p1.7.m7.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.7.m7.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.7.m7.1c">s</annotation></semantics></math>, and an index <math id="S3.SS1.SSS0.Px2.p1.8.m8.1" class="ltx_Math" alttext="g" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.8.m8.1a"><mi id="S3.SS1.SSS0.Px2.p1.8.m8.1.1" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.8.m8.1b"><ci id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1">𝑔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.8.m8.1c">g</annotation></semantics></math> linking the prediction to an object description in <math id="S3.SS1.SSS0.Px2.p1.9.m9.1" class="ltx_Math" alttext="D_{i}" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.9.m9.1a"><msub id="S3.SS1.SSS0.Px2.p1.9.m9.1.1" xref="S3.SS1.SSS0.Px2.p1.9.m9.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p1.9.m9.1.1.2" xref="S3.SS1.SSS0.Px2.p1.9.m9.1.1.2.cmml">D</mi><mi id="S3.SS1.SSS0.Px2.p1.9.m9.1.1.3" xref="S3.SS1.SSS0.Px2.p1.9.m9.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.9.m9.1b"><apply id="S3.SS1.SSS0.Px2.p1.9.m9.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.9.m9.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.9.m9.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.9.m9.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.9.m9.1.1.2">𝐷</ci><ci id="S3.SS1.SSS0.Px2.p1.9.m9.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.9.m9.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.9.m9.1c">D_{i}</annotation></semantics></math>. A bounding box <math id="S3.SS1.SSS0.Px2.p1.10.m10.1" class="ltx_Math" alttext="b_{i}^{l}" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.10.m10.1a"><msubsup id="S3.SS1.SSS0.Px2.p1.10.m10.1.1" xref="S3.SS1.SSS0.Px2.p1.10.m10.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p1.10.m10.1.1.2.2" xref="S3.SS1.SSS0.Px2.p1.10.m10.1.1.2.2.cmml">b</mi><mi id="S3.SS1.SSS0.Px2.p1.10.m10.1.1.2.3" xref="S3.SS1.SSS0.Px2.p1.10.m10.1.1.2.3.cmml">i</mi><mi id="S3.SS1.SSS0.Px2.p1.10.m10.1.1.3" xref="S3.SS1.SSS0.Px2.p1.10.m10.1.1.3.cmml">l</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.10.m10.1b"><apply id="S3.SS1.SSS0.Px2.p1.10.m10.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.10.m10.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.10.m10.1.1">superscript</csymbol><apply id="S3.SS1.SSS0.Px2.p1.10.m10.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.10.m10.1.1.2.1.cmml" xref="S3.SS1.SSS0.Px2.p1.10.m10.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.10.m10.1.1.2.2.cmml" xref="S3.SS1.SSS0.Px2.p1.10.m10.1.1.2.2">𝑏</ci><ci id="S3.SS1.SSS0.Px2.p1.10.m10.1.1.2.3.cmml" xref="S3.SS1.SSS0.Px2.p1.10.m10.1.1.2.3">𝑖</ci></apply><ci id="S3.SS1.SSS0.Px2.p1.10.m10.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.10.m10.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.10.m10.1c">b_{i}^{l}</annotation></semantics></math> consists of 4 coordinates in the image space to define the extent of an object, as in standard object detection. The confidence of a model’s prediction is expressed by the real-valued scalar <math id="S3.SS1.SSS0.Px2.p1.11.m11.1" class="ltx_Math" alttext="s_{i}^{l}" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.11.m11.1a"><msubsup id="S3.SS1.SSS0.Px2.p1.11.m11.1.1" xref="S3.SS1.SSS0.Px2.p1.11.m11.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p1.11.m11.1.1.2.2" xref="S3.SS1.SSS0.Px2.p1.11.m11.1.1.2.2.cmml">s</mi><mi id="S3.SS1.SSS0.Px2.p1.11.m11.1.1.2.3" xref="S3.SS1.SSS0.Px2.p1.11.m11.1.1.2.3.cmml">i</mi><mi id="S3.SS1.SSS0.Px2.p1.11.m11.1.1.3" xref="S3.SS1.SSS0.Px2.p1.11.m11.1.1.3.cmml">l</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.11.m11.1b"><apply id="S3.SS1.SSS0.Px2.p1.11.m11.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.11.m11.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.11.m11.1.1">superscript</csymbol><apply id="S3.SS1.SSS0.Px2.p1.11.m11.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.11.m11.1.1.2.1.cmml" xref="S3.SS1.SSS0.Px2.p1.11.m11.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.11.m11.1.1.2.2.cmml" xref="S3.SS1.SSS0.Px2.p1.11.m11.1.1.2.2">𝑠</ci><ci id="S3.SS1.SSS0.Px2.p1.11.m11.1.1.2.3.cmml" xref="S3.SS1.SSS0.Px2.p1.11.m11.1.1.2.3">𝑖</ci></apply><ci id="S3.SS1.SSS0.Px2.p1.11.m11.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.11.m11.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.11.m11.1c">s_{i}^{l}</annotation></semantics></math>. Finally, the index <math id="S3.SS1.SSS0.Px2.p1.12.m12.1" class="ltx_Math" alttext="g_{i}^{l}" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.12.m12.1a"><msubsup id="S3.SS1.SSS0.Px2.p1.12.m12.1.1" xref="S3.SS1.SSS0.Px2.p1.12.m12.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p1.12.m12.1.1.2.2" xref="S3.SS1.SSS0.Px2.p1.12.m12.1.1.2.2.cmml">g</mi><mi id="S3.SS1.SSS0.Px2.p1.12.m12.1.1.2.3" xref="S3.SS1.SSS0.Px2.p1.12.m12.1.1.2.3.cmml">i</mi><mi id="S3.SS1.SSS0.Px2.p1.12.m12.1.1.3" xref="S3.SS1.SSS0.Px2.p1.12.m12.1.1.3.cmml">l</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.12.m12.1b"><apply id="S3.SS1.SSS0.Px2.p1.12.m12.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.12.m12.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.12.m12.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.12.m12.1.1">superscript</csymbol><apply id="S3.SS1.SSS0.Px2.p1.12.m12.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.12.m12.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.12.m12.1.1.2.1.cmml" xref="S3.SS1.SSS0.Px2.p1.12.m12.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.12.m12.1.1.2.2.cmml" xref="S3.SS1.SSS0.Px2.p1.12.m12.1.1.2.2">𝑔</ci><ci id="S3.SS1.SSS0.Px2.p1.12.m12.1.1.2.3.cmml" xref="S3.SS1.SSS0.Px2.p1.12.m12.1.1.2.3">𝑖</ci></apply><ci id="S3.SS1.SSS0.Px2.p1.12.m12.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.12.m12.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.12.m12.1c">g_{i}^{l}</annotation></semantics></math> is in the range of <math id="S3.SS1.SSS0.Px2.p1.13.m13.2" class="ltx_Math" alttext="\left[1,|D_{i}|\right]" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.13.m13.2a"><mrow id="S3.SS1.SSS0.Px2.p1.13.m13.2.2.1" xref="S3.SS1.SSS0.Px2.p1.13.m13.2.2.2.cmml"><mo id="S3.SS1.SSS0.Px2.p1.13.m13.2.2.1.2" xref="S3.SS1.SSS0.Px2.p1.13.m13.2.2.2.cmml">[</mo><mn id="S3.SS1.SSS0.Px2.p1.13.m13.1.1" xref="S3.SS1.SSS0.Px2.p1.13.m13.1.1.cmml">1</mn><mo id="S3.SS1.SSS0.Px2.p1.13.m13.2.2.1.3" xref="S3.SS1.SSS0.Px2.p1.13.m13.2.2.2.cmml">,</mo><mrow id="S3.SS1.SSS0.Px2.p1.13.m13.2.2.1.1.1" xref="S3.SS1.SSS0.Px2.p1.13.m13.2.2.1.1.2.cmml"><mo stretchy="false" id="S3.SS1.SSS0.Px2.p1.13.m13.2.2.1.1.1.2" xref="S3.SS1.SSS0.Px2.p1.13.m13.2.2.1.1.2.1.cmml">|</mo><msub id="S3.SS1.SSS0.Px2.p1.13.m13.2.2.1.1.1.1" xref="S3.SS1.SSS0.Px2.p1.13.m13.2.2.1.1.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p1.13.m13.2.2.1.1.1.1.2" xref="S3.SS1.SSS0.Px2.p1.13.m13.2.2.1.1.1.1.2.cmml">D</mi><mi id="S3.SS1.SSS0.Px2.p1.13.m13.2.2.1.1.1.1.3" xref="S3.SS1.SSS0.Px2.p1.13.m13.2.2.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS1.SSS0.Px2.p1.13.m13.2.2.1.1.1.3" xref="S3.SS1.SSS0.Px2.p1.13.m13.2.2.1.1.2.1.cmml">|</mo></mrow><mo id="S3.SS1.SSS0.Px2.p1.13.m13.2.2.1.4" xref="S3.SS1.SSS0.Px2.p1.13.m13.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.13.m13.2b"><interval closure="closed" id="S3.SS1.SSS0.Px2.p1.13.m13.2.2.2.cmml" xref="S3.SS1.SSS0.Px2.p1.13.m13.2.2.1"><cn type="integer" id="S3.SS1.SSS0.Px2.p1.13.m13.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.13.m13.1.1">1</cn><apply id="S3.SS1.SSS0.Px2.p1.13.m13.2.2.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.13.m13.2.2.1.1.1"><abs id="S3.SS1.SSS0.Px2.p1.13.m13.2.2.1.1.2.1.cmml" xref="S3.SS1.SSS0.Px2.p1.13.m13.2.2.1.1.1.2"></abs><apply id="S3.SS1.SSS0.Px2.p1.13.m13.2.2.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.13.m13.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.13.m13.2.2.1.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.13.m13.2.2.1.1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.13.m13.2.2.1.1.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.13.m13.2.2.1.1.1.1.2">𝐷</ci><ci id="S3.SS1.SSS0.Px2.p1.13.m13.2.2.1.1.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.13.m13.2.2.1.1.1.1.3">𝑖</ci></apply></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.13.m13.2c">\left[1,|D_{i}|\right]</annotation></semantics></math> and indicates that the prediction <math id="S3.SS1.SSS0.Px2.p1.14.m14.1" class="ltx_Math" alttext="p_{i}^{l}" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.14.m14.1a"><msubsup id="S3.SS1.SSS0.Px2.p1.14.m14.1.1" xref="S3.SS1.SSS0.Px2.p1.14.m14.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p1.14.m14.1.1.2.2" xref="S3.SS1.SSS0.Px2.p1.14.m14.1.1.2.2.cmml">p</mi><mi id="S3.SS1.SSS0.Px2.p1.14.m14.1.1.2.3" xref="S3.SS1.SSS0.Px2.p1.14.m14.1.1.2.3.cmml">i</mi><mi id="S3.SS1.SSS0.Px2.p1.14.m14.1.1.3" xref="S3.SS1.SSS0.Px2.p1.14.m14.1.1.3.cmml">l</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.14.m14.1b"><apply id="S3.SS1.SSS0.Px2.p1.14.m14.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.14.m14.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.14.m14.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.14.m14.1.1">superscript</csymbol><apply id="S3.SS1.SSS0.Px2.p1.14.m14.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.14.m14.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.14.m14.1.1.2.1.cmml" xref="S3.SS1.SSS0.Px2.p1.14.m14.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.14.m14.1.1.2.2.cmml" xref="S3.SS1.SSS0.Px2.p1.14.m14.1.1.2.2">𝑝</ci><ci id="S3.SS1.SSS0.Px2.p1.14.m14.1.1.2.3.cmml" xref="S3.SS1.SSS0.Px2.p1.14.m14.1.1.2.3">𝑖</ci></apply><ci id="S3.SS1.SSS0.Px2.p1.14.m14.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.14.m14.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.14.m14.1c">p_{i}^{l}</annotation></semantics></math> localizes one object instance of the description <math id="S3.SS1.SSS0.Px2.p1.15.m15.1" class="ltx_Math" alttext="g_{i}^{l}" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.15.m15.1a"><msubsup id="S3.SS1.SSS0.Px2.p1.15.m15.1.1" xref="S3.SS1.SSS0.Px2.p1.15.m15.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p1.15.m15.1.1.2.2" xref="S3.SS1.SSS0.Px2.p1.15.m15.1.1.2.2.cmml">g</mi><mi id="S3.SS1.SSS0.Px2.p1.15.m15.1.1.2.3" xref="S3.SS1.SSS0.Px2.p1.15.m15.1.1.2.3.cmml">i</mi><mi id="S3.SS1.SSS0.Px2.p1.15.m15.1.1.3" xref="S3.SS1.SSS0.Px2.p1.15.m15.1.1.3.cmml">l</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.15.m15.1b"><apply id="S3.SS1.SSS0.Px2.p1.15.m15.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.15.m15.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.15.m15.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.15.m15.1.1">superscript</csymbol><apply id="S3.SS1.SSS0.Px2.p1.15.m15.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.15.m15.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.15.m15.1.1.2.1.cmml" xref="S3.SS1.SSS0.Px2.p1.15.m15.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.15.m15.1.1.2.2.cmml" xref="S3.SS1.SSS0.Px2.p1.15.m15.1.1.2.2">𝑔</ci><ci id="S3.SS1.SSS0.Px2.p1.15.m15.1.1.2.3.cmml" xref="S3.SS1.SSS0.Px2.p1.15.m15.1.1.2.3">𝑖</ci></apply><ci id="S3.SS1.SSS0.Px2.p1.15.m15.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.15.m15.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.15.m15.1c">g_{i}^{l}</annotation></semantics></math> of the label space <math id="S3.SS1.SSS0.Px2.p1.16.m16.1" class="ltx_Math" alttext="D_{i}" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.16.m16.1a"><msub id="S3.SS1.SSS0.Px2.p1.16.m16.1.1" xref="S3.SS1.SSS0.Px2.p1.16.m16.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p1.16.m16.1.1.2" xref="S3.SS1.SSS0.Px2.p1.16.m16.1.1.2.cmml">D</mi><mi id="S3.SS1.SSS0.Px2.p1.16.m16.1.1.3" xref="S3.SS1.SSS0.Px2.p1.16.m16.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.16.m16.1b"><apply id="S3.SS1.SSS0.Px2.p1.16.m16.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.16.m16.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.16.m16.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.16.m16.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.16.m16.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.16.m16.1.1.2">𝐷</ci><ci id="S3.SS1.SSS0.Px2.p1.16.m16.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.16.m16.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.16.m16.1c">D_{i}</annotation></semantics></math>. Note that multiple predictions <math id="S3.SS1.SSS0.Px2.p1.17.m17.1" class="ltx_Math" alttext="p_{i}^{l}" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.17.m17.1a"><msubsup id="S3.SS1.SSS0.Px2.p1.17.m17.1.1" xref="S3.SS1.SSS0.Px2.p1.17.m17.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p1.17.m17.1.1.2.2" xref="S3.SS1.SSS0.Px2.p1.17.m17.1.1.2.2.cmml">p</mi><mi id="S3.SS1.SSS0.Px2.p1.17.m17.1.1.2.3" xref="S3.SS1.SSS0.Px2.p1.17.m17.1.1.2.3.cmml">i</mi><mi id="S3.SS1.SSS0.Px2.p1.17.m17.1.1.3" xref="S3.SS1.SSS0.Px2.p1.17.m17.1.1.3.cmml">l</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.17.m17.1b"><apply id="S3.SS1.SSS0.Px2.p1.17.m17.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.17.m17.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.17.m17.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.17.m17.1.1">superscript</csymbol><apply id="S3.SS1.SSS0.Px2.p1.17.m17.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.17.m17.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.17.m17.1.1.2.1.cmml" xref="S3.SS1.SSS0.Px2.p1.17.m17.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.17.m17.1.1.2.2.cmml" xref="S3.SS1.SSS0.Px2.p1.17.m17.1.1.2.2">𝑝</ci><ci id="S3.SS1.SSS0.Px2.p1.17.m17.1.1.2.3.cmml" xref="S3.SS1.SSS0.Px2.p1.17.m17.1.1.2.3">𝑖</ci></apply><ci id="S3.SS1.SSS0.Px2.p1.17.m17.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.17.m17.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.17.m17.1c">p_{i}^{l}</annotation></semantics></math> can point to the same object description <math id="S3.SS1.SSS0.Px2.p1.18.m18.1" class="ltx_Math" alttext="d_{i}^{k}" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.18.m18.1a"><msubsup id="S3.SS1.SSS0.Px2.p1.18.m18.1.1" xref="S3.SS1.SSS0.Px2.p1.18.m18.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p1.18.m18.1.1.2.2" xref="S3.SS1.SSS0.Px2.p1.18.m18.1.1.2.2.cmml">d</mi><mi id="S3.SS1.SSS0.Px2.p1.18.m18.1.1.2.3" xref="S3.SS1.SSS0.Px2.p1.18.m18.1.1.2.3.cmml">i</mi><mi id="S3.SS1.SSS0.Px2.p1.18.m18.1.1.3" xref="S3.SS1.SSS0.Px2.p1.18.m18.1.1.3.cmml">k</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.18.m18.1b"><apply id="S3.SS1.SSS0.Px2.p1.18.m18.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.18.m18.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.18.m18.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.18.m18.1.1">superscript</csymbol><apply id="S3.SS1.SSS0.Px2.p1.18.m18.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.18.m18.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.18.m18.1.1.2.1.cmml" xref="S3.SS1.SSS0.Px2.p1.18.m18.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.18.m18.1.1.2.2.cmml" xref="S3.SS1.SSS0.Px2.p1.18.m18.1.1.2.2">𝑑</ci><ci id="S3.SS1.SSS0.Px2.p1.18.m18.1.1.2.3.cmml" xref="S3.SS1.SSS0.Px2.p1.18.m18.1.1.2.3">𝑖</ci></apply><ci id="S3.SS1.SSS0.Px2.p1.18.m18.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.18.m18.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.18.m18.1c">d_{i}^{k}</annotation></semantics></math>.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Difference to object detection benchmarks:</h4>

<div id="S3.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px3.p1.1" class="ltx_p">The main difference is the label space, which is more complex (with natural text object descriptions, often unseen during training) as well as dynamic (size of label space changes for every test image). Standard object detectors fail this task because of their fixed label space assumption.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Difference to referring expression benchmarks:</h4>

<div id="S3.SS1.SSS0.Px4.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px4.p1.1" class="ltx_p">While the task definition is similar, the key difference is in the corresponding data. First, object descriptions <math id="S3.SS1.SSS0.Px4.p1.1.m1.1" class="ltx_Math" alttext="D_{i}" display="inline"><semantics id="S3.SS1.SSS0.Px4.p1.1.m1.1a"><msub id="S3.SS1.SSS0.Px4.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.cmml"><mi id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.2" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.2.cmml">D</mi><mi id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px4.p1.1.m1.1b"><apply id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.2">𝐷</ci><ci id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px4.p1.1.m1.1c">D_{i}</annotation></semantics></math> in our benchmark range from plain categories (like in standard detection) to highly specific descriptions. Second, each description can refer to zero, one, or multiple instances in the image. All referring expression datasets assume the presence of the object described by the text and, hence, do not contain negative examples that refer to zero instances, an important aspect of standard detection evaluation. Moreover, only one referring expression dataset (<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>) refers to more than a single instance per image.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Evaluation Metric</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">To evaluate a model <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">M</annotation></semantics></math> on our task, we propose a modified version of the object detection evaluation metric, average precision (AP) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. This modification is necessary to account for our novel object descriptions that make the label space virtually infinite in size, and that are different for each image. The following list summarizes the changes:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.2" class="ltx_p">While AP is computed for each category separately (and then averaged) in standard detection, this initial grouping is omitted in OmniLabel. Due to the high specificity of the object descriptions, many of these “groups” would then consist of only a single object instance in the whole dataset. This can make the metric less robust. However, to ensure that our metric considers the predicted semantic categories, we adjust the matching between prediction and ground truth. While in standard detection the matching is based purely on the bounding boxes via intersection-over-union (since categories are already grouped), we include the index <math id="S3.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="g_{i}^{l}" display="inline"><semantics id="S3.I1.i1.p1.1.m1.1a"><msubsup id="S3.I1.i1.p1.1.m1.1.1" xref="S3.I1.i1.p1.1.m1.1.1.cmml"><mi id="S3.I1.i1.p1.1.m1.1.1.2.2" xref="S3.I1.i1.p1.1.m1.1.1.2.2.cmml">g</mi><mi id="S3.I1.i1.p1.1.m1.1.1.2.3" xref="S3.I1.i1.p1.1.m1.1.1.2.3.cmml">i</mi><mi id="S3.I1.i1.p1.1.m1.1.1.3" xref="S3.I1.i1.p1.1.m1.1.1.3.cmml">l</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.1.m1.1b"><apply id="S3.I1.i1.p1.1.m1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I1.i1.p1.1.m1.1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1">superscript</csymbol><apply id="S3.I1.i1.p1.1.m1.1.1.2.cmml" xref="S3.I1.i1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I1.i1.p1.1.m1.1.1.2.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.I1.i1.p1.1.m1.1.1.2.2.cmml" xref="S3.I1.i1.p1.1.m1.1.1.2.2">𝑔</ci><ci id="S3.I1.i1.p1.1.m1.1.1.2.3.cmml" xref="S3.I1.i1.p1.1.m1.1.1.2.3">𝑖</ci></apply><ci id="S3.I1.i1.p1.1.m1.1.1.3.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.1.m1.1c">g_{i}^{l}</annotation></semantics></math> that links a prediction with the object descriptions in <math id="S3.I1.i1.p1.2.m2.1" class="ltx_Math" alttext="D_{i}" display="inline"><semantics id="S3.I1.i1.p1.2.m2.1a"><msub id="S3.I1.i1.p1.2.m2.1.1" xref="S3.I1.i1.p1.2.m2.1.1.cmml"><mi id="S3.I1.i1.p1.2.m2.1.1.2" xref="S3.I1.i1.p1.2.m2.1.1.2.cmml">D</mi><mi id="S3.I1.i1.p1.2.m2.1.1.3" xref="S3.I1.i1.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.2.m2.1b"><apply id="S3.I1.i1.p1.2.m2.1.1.cmml" xref="S3.I1.i1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.I1.i1.p1.2.m2.1.1.1.cmml" xref="S3.I1.i1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.I1.i1.p1.2.m2.1.1.2.cmml" xref="S3.I1.i1.p1.2.m2.1.1.2">𝐷</ci><ci id="S3.I1.i1.p1.2.m2.1.1.3.cmml" xref="S3.I1.i1.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.2.m2.1c">D_{i}</annotation></semantics></math>, see above in <a href="#S3.SS1" title="3.1 Benchmark Task ‣ 3 Benchmark and Evaluation Metric ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.1</span></a>. Specifically, a prediction is matched to a ground truth only if the prediction and the ground truth point to the same object description (semantics) and the predicted and ground truth bounding boxes overlap sufficiently (localization).</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">Standard detection ground truth exclusively assigns each object instance one semantic category. In contrast, our task requires multi-label predictions. For instance, “person” and “woman in red shirt” can refer to the same object. This needs to be considered in the matching process of the evaluation. In contrast to standard detection, one ground truth box can be correctly matched with multiple predictions if the match happens via different object descriptions (recall index <math id="S3.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="g_{i}^{l}" display="inline"><semantics id="S3.I1.i2.p1.1.m1.1a"><msubsup id="S3.I1.i2.p1.1.m1.1.1" xref="S3.I1.i2.p1.1.m1.1.1.cmml"><mi id="S3.I1.i2.p1.1.m1.1.1.2.2" xref="S3.I1.i2.p1.1.m1.1.1.2.2.cmml">g</mi><mi id="S3.I1.i2.p1.1.m1.1.1.2.3" xref="S3.I1.i2.p1.1.m1.1.1.2.3.cmml">i</mi><mi id="S3.I1.i2.p1.1.m1.1.1.3" xref="S3.I1.i2.p1.1.m1.1.1.3.cmml">l</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.1.m1.1b"><apply id="S3.I1.i2.p1.1.m1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I1.i2.p1.1.m1.1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1">superscript</csymbol><apply id="S3.I1.i2.p1.1.m1.1.1.2.cmml" xref="S3.I1.i2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I1.i2.p1.1.m1.1.1.2.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.I1.i2.p1.1.m1.1.1.2.2.cmml" xref="S3.I1.i2.p1.1.m1.1.1.2.2">𝑔</ci><ci id="S3.I1.i2.p1.1.m1.1.1.2.3.cmml" xref="S3.I1.i2.p1.1.m1.1.1.2.3">𝑖</ci></apply><ci id="S3.I1.i2.p1.1.m1.1.1.3.cmml" xref="S3.I1.i2.p1.1.m1.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.1.m1.1c">g_{i}^{l}</annotation></semantics></math> above).</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">Our object descriptions <math id="S3.I1.i3.p1.1.m1.1" class="ltx_Math" alttext="D_{i}" display="inline"><semantics id="S3.I1.i3.p1.1.m1.1a"><msub id="S3.I1.i3.p1.1.m1.1.1" xref="S3.I1.i3.p1.1.m1.1.1.cmml"><mi id="S3.I1.i3.p1.1.m1.1.1.2" xref="S3.I1.i3.p1.1.m1.1.1.2.cmml">D</mi><mi id="S3.I1.i3.p1.1.m1.1.1.3" xref="S3.I1.i3.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.1.m1.1b"><apply id="S3.I1.i3.p1.1.m1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I1.i3.p1.1.m1.1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.I1.i3.p1.1.m1.1.1.2.cmml" xref="S3.I1.i3.p1.1.m1.1.1.2">𝐷</ci><ci id="S3.I1.i3.p1.1.m1.1.1.3.cmml" xref="S3.I1.i3.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.1.m1.1c">D_{i}</annotation></semantics></math> contain both plain category names (like “car” or “person” from standard detection) as well as complex free-form text (like “blue sports car parked near left sidewalk”). We want our metric to give equal importance to both types. Due to the different number of ground truth instances, we first compute AP for both types separately and then take the harmonic mean. Different from the arithmetic mean, the harmonic mean requires good results on both types to achieve a high number on the final metric.</p>
</div>
</li>
</ul>
<p id="S3.SS2.p1.2" class="ltx_p">We implemented this evaluation protocol in Python and released it at <a target="_blank" href="https://github.com/samschulter/omnilabeltools" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/samschulter/omnilabeltools</a></p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Dataset Collection</h2>

<figure id="S4.F4" class="ltx_figure"><img src="/html/2304.11463/assets/figures/anno_process.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="299" height="513" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.3.2" class="ltx_text" style="font-size:90%;">A step-by-step summary of our data collection and annotation process. <a href="#S4" title="4 Dataset Collection ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4</span></a> describes each step in detail. The key benefits of our annotation process are: (1) reuse of existing object detection datasets with annotated categories allows for a balanced sampling and consequently a set of diverse object description. (2) Contrary to existing free-form text benchmarks, we collect negative descriptions, which are related to an image but do not refer to any object.</span></figcaption>
</figure>
<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">To establish our novel evaluation benchmark, we need images that annotate objects with bounding boxes and corresponding free-form text descriptions. To do so, we define a multi-step annotation and verification process. <a href="#S4.F4" title="In 4 Dataset Collection ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a> and the following paragraphs describe the process.</p>
</div>
<section id="S4.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Existing datasets:</h4>

<div id="S4.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p1.1" class="ltx_p">We start with the validation/test sets of COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, Objects-365 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, and OpenImages-V5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, which not only saves annotation cost for obtaining bounding boxes, but also helps collecting <em id="S4.SS0.SSS0.Px1.p1.1.1" class="ltx_emph ltx_font_italic">diverse</em> object descriptions. By leveraging the (super-)category information when sampling images, we force annotators to provide descriptions also for rare categories. Otherwise, annotators will quickly pick simple and common categories to describe. And although reusing datasets may exclude some categories that were not annotated, the object descriptions we collect often include additional categories. For example, while a “bottle cap” is not part of the original categories, a description like “bottle with red cap” requires to understand “bottle cap”.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Sample image / (super)category pairs:</h4>

<div id="S4.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px2.p1.1" class="ltx_p">To encourage a diverse distribution of categories and images, we propose a strategy to randomly sample pairs of images and (super) categories. We first filter all possible pairs based on the following criteria: (a) At least two instances of a (super) category need to be present in the image. (b) For super-categories, at least two different sub-categories need to be present in the image. (c) To collect descriptions that focus on the object’s appearance, relations and actions, we reject pairs with more than 10 instances, if the larger side of any instances’ bounding box is smaller than 80 pixels, if the average over the bounding box’s largest overlap with any other box is larger than 50%, or if any instance is flagged as covering a crowd of objects (“iscrowd”). Finally, we pick a random subset of the filtered pairs for annotation with free-form descriptions.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Collect object descriptions:</h4>

<div id="S4.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px3.p1.3" class="ltx_p">All initial object descriptions are collected with Amazon Mechanical Turk (AMT). Given an image/(super-)category pair, we draw the bounding boxes of that (super-)category’s instances and request annotators to pick a subset of the instances and provide a text description that only matches their selection. Specifically, for <em id="S4.SS0.SSS0.Px3.p1.3.1" class="ltx_emph ltx_font_italic">i</em>mage/category pairs with <math id="S4.SS0.SSS0.Px3.p1.1.m1.1" class="ltx_Math" alttext="N=2" display="inline"><semantics id="S4.SS0.SSS0.Px3.p1.1.m1.1a"><mrow id="S4.SS0.SSS0.Px3.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.cmml"><mi id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.2" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.2.cmml">N</mi><mo id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.3" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p1.1.m1.1b"><apply id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1"><eq id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1"></eq><ci id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.2.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.2">𝑁</ci><cn type="integer" id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.3.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p1.1.m1.1c">N=2</annotation></semantics></math> possible instances, we ask to pick exactly one. If <math id="S4.SS0.SSS0.Px3.p1.2.m2.1" class="ltx_Math" alttext="N&gt;2" display="inline"><semantics id="S4.SS0.SSS0.Px3.p1.2.m2.1a"><mrow id="S4.SS0.SSS0.Px3.p1.2.m2.1.1" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.1.cmml"><mi id="S4.SS0.SSS0.Px3.p1.2.m2.1.1.2" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.1.2.cmml">N</mi><mo id="S4.SS0.SSS0.Px3.p1.2.m2.1.1.1" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.1.1.cmml">&gt;</mo><mn id="S4.SS0.SSS0.Px3.p1.2.m2.1.1.3" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p1.2.m2.1b"><apply id="S4.SS0.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.1"><gt id="S4.SS0.SSS0.Px3.p1.2.m2.1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.1.1"></gt><ci id="S4.SS0.SSS0.Px3.p1.2.m2.1.1.2.cmml" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.1.2">𝑁</ci><cn type="integer" id="S4.SS0.SSS0.Px3.p1.2.m2.1.1.3.cmml" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p1.2.m2.1c">N&gt;2</annotation></semantics></math>, we ask to select at least 2 but at most <math id="S4.SS0.SSS0.Px3.p1.3.m3.1" class="ltx_Math" alttext="N-1" display="inline"><semantics id="S4.SS0.SSS0.Px3.p1.3.m3.1a"><mrow id="S4.SS0.SSS0.Px3.p1.3.m3.1.1" xref="S4.SS0.SSS0.Px3.p1.3.m3.1.1.cmml"><mi id="S4.SS0.SSS0.Px3.p1.3.m3.1.1.2" xref="S4.SS0.SSS0.Px3.p1.3.m3.1.1.2.cmml">N</mi><mo id="S4.SS0.SSS0.Px3.p1.3.m3.1.1.1" xref="S4.SS0.SSS0.Px3.p1.3.m3.1.1.1.cmml">−</mo><mn id="S4.SS0.SSS0.Px3.p1.3.m3.1.1.3" xref="S4.SS0.SSS0.Px3.p1.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p1.3.m3.1b"><apply id="S4.SS0.SSS0.Px3.p1.3.m3.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.3.m3.1.1"><minus id="S4.SS0.SSS0.Px3.p1.3.m3.1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.3.m3.1.1.1"></minus><ci id="S4.SS0.SSS0.Px3.p1.3.m3.1.1.2.cmml" xref="S4.SS0.SSS0.Px3.p1.3.m3.1.1.2">𝑁</ci><cn type="integer" id="S4.SS0.SSS0.Px3.p1.3.m3.1.1.3.cmml" xref="S4.SS0.SSS0.Px3.p1.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p1.3.m3.1c">N-1</annotation></semantics></math> instances. This ensures that if an object description uses the category name itself, additional text is needed to distinguish the instances. For <em id="S4.SS0.SSS0.Px3.p1.3.2" class="ltx_emph ltx_font_italic">i</em>mage/super-category pairs, we ask to select at least one instance, but avoid using the category names themselves in the descriptions. This encourages higher-level descriptions like “edible item” for all objects of the super-category “food”. Finally, we ran a semi-automatic profanity check <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> on the collected text. We manually inspected 500 descriptions with the highest probability of containing profane language, but did not need to discard any description.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Verification:</h4>

<div id="S4.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px4.p1.1" class="ltx_p">To ensure high quality descriptions, we again use AMT to verify the selection of bounding boxes from the previous step. We provide annotators the originally highlighted bounding boxes and the newly collected description and request to select the objects for which the description applies. We only keep descriptions for which both selections (initial and verification) are equal, which is about 80% of all descriptions.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Collect negative descriptions:</h4>

<div id="S4.SS0.SSS0.Px5.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px5.p1.1" class="ltx_p">As described earlier, a key aspect of our benchmark are negative object descriptions. These descriptions are related to an image, but do not actually refer to any object. To collect such descriptions, we leverage the already-collected free-form object descriptions with their underlying (super-)category information. Hence, a sample &amp; verify approach is suitable, where, for each image/(super-)category pair, we randomly sample 5 object descriptions from the same (super-)category but a different image and ask 2 AMT annotators to confirm that the given description does not refer to any object. We then only keep negative descriptions with 2 confirmations, which was about 30% in our case.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px6" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Quality check:</h4>

<div id="S4.SS0.SSS0.Px6.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px6.p1.1" class="ltx_p">Finally, we perform a manual quality check. We fix misspellings and ambiguous descriptions when possible. If the meaning of the description changed, we keep the positive description, but discard all negative associations to other images. If an object description is entirely wrong, we discard it, which was the case for about 10% of the remaining descriptions.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px7" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Annotators:</h4>

<div id="S4.SS0.SSS0.Px7.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px7.p1.1" class="ltx_p">In total, 263 different annotators from AMT provided inputs for our annotations. For the three tasks using AMT (generating descriptions, verifying descriptions, and verifying negative descriptions), we had 54, 71, and 235 annotators, respectively.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.6.7.1" class="ltx_tr">
<td id="S4.T2.6.7.1.1" class="ltx_td" style="padding:0.6pt 3.5pt;"></td>
<th id="S4.T2.6.7.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column" style="padding:0.6pt 3.5pt;">
<div id="S4.T2.6.7.1.2.1" class="ltx_inline-block ltx_transformed_outer" style="width:54.1pt;height:8pt;vertical-align:-2.0pt;"><span class="ltx_transformed_inner" style="width:54.1pt;transform:translate(0pt,3pt) rotate(-0deg) ;">
<p id="S4.T2.6.7.1.2.1.1" class="ltx_p"><span id="S4.T2.6.7.1.2.1.1.1" class="ltx_text" style="font-size:80%;">RefCOCO/g/+</span></p>
</span></div>
</th>
<th id="S4.T2.6.7.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column" style="padding:0.6pt 3.5pt;">
<div id="S4.T2.6.7.1.3.1" class="ltx_inline-block ltx_transformed_outer" style="width:32.6pt;height:5.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:32.6pt;transform:translate(0pt,0pt) rotate(-0deg) ;">
<p id="S4.T2.6.7.1.3.1.1" class="ltx_p"><span id="S4.T2.6.7.1.3.1.1.1" class="ltx_text" style="font-size:80%;">Flickr30k</span></p>
</span></div>
</th>
<th id="S4.T2.6.7.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column" style="padding:0.6pt 3.5pt;">
<div id="S4.T2.6.7.1.4.1" class="ltx_inline-block ltx_transformed_outer" style="width:37.1pt;height:5.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:37.1pt;transform:translate(0pt,0pt) rotate(-0deg) ;">
<p id="S4.T2.6.7.1.4.1.1" class="ltx_p"><span id="S4.T2.6.7.1.4.1.1.1" class="ltx_text" style="font-size:80%;">PhraseCut</span></p>
</span></div>
</th>
<th id="S4.T2.6.7.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column" style="padding:0.6pt 3.5pt;">
<div id="S4.T2.6.7.1.5.1" class="ltx_inline-block ltx_transformed_outer" style="width:39.0pt;height:5.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:39.0pt;transform:translate(0pt,0pt) rotate(-0deg) ;">
<p id="S4.T2.6.7.1.5.1.1" class="ltx_p"><span id="S4.T2.6.7.1.5.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">OmniLabel</span></p>
</span></div>
</th>
</tr>
<tr id="S4.T2.6.8.2" class="ltx_tr">
<td id="S4.T2.6.8.2.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding:0.6pt 3.5pt;"><span id="S4.T2.6.8.2.1.1" class="ltx_text" style="font-size:80%;"># images</span></td>
<td id="S4.T2.6.8.2.2" class="ltx_td ltx_align_left ltx_border_tt" style="padding:0.6pt 3.5pt;"><span id="S4.T2.6.8.2.2.1" class="ltx_text" style="font-size:80%;">4.3K</span></td>
<td id="S4.T2.6.8.2.3" class="ltx_td ltx_align_left ltx_border_tt" style="padding:0.6pt 3.5pt;"><span id="S4.T2.6.8.2.3.1" class="ltx_text" style="font-size:80%;">1.0K</span></td>
<td id="S4.T2.6.8.2.4" class="ltx_td ltx_align_left ltx_border_tt" style="padding:0.6pt 3.5pt;"><span id="S4.T2.6.8.2.4.1" class="ltx_text" style="font-size:80%;">2.9K</span></td>
<td id="S4.T2.6.8.2.5" class="ltx_td ltx_align_left ltx_border_tt" style="padding:0.6pt 3.5pt;"><span id="S4.T2.6.8.2.5.1" class="ltx_text" style="font-size:80%;">12.2K</span></td>
</tr>
<tr id="S4.T2.6.9.3" class="ltx_tr">
<td id="S4.T2.6.9.3.1" class="ltx_td ltx_align_left" style="padding:0.6pt 3.5pt;"><span id="S4.T2.6.9.3.1.1" class="ltx_text" style="font-size:80%;"># descr.</span></td>
<td id="S4.T2.6.9.3.2" class="ltx_td ltx_align_left" style="padding:0.6pt 3.5pt;"><span id="S4.T2.6.9.3.2.1" class="ltx_text" style="font-size:80%;">26.5K</span></td>
<td id="S4.T2.6.9.3.3" class="ltx_td ltx_align_left" style="padding:0.6pt 3.5pt;"><span id="S4.T2.6.9.3.3.1" class="ltx_text" style="font-size:80%;">11.3K</span></td>
<td id="S4.T2.6.9.3.4" class="ltx_td ltx_align_left" style="padding:0.6pt 3.5pt;"><span id="S4.T2.6.9.3.4.1" class="ltx_text" style="font-size:80%;">19.5K</span></td>
<td id="S4.T2.6.9.3.5" class="ltx_td ltx_align_left" style="padding:0.6pt 3.5pt;">
<span id="S4.T2.6.9.3.5.1" class="ltx_text" style="font-size:80%;">15.8K </span><span id="S4.T2.6.9.3.5.2" class="ltx_text" style="font-size:50%;">(16.8K)</span>
</td>
</tr>
<tr id="S4.T2.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1" class="ltx_td ltx_align_left" style="padding:0.6pt 3.5pt;">
<span id="S4.T2.1.1.1.1" class="ltx_text" style="font-size:80%;">  </span><span id="S4.T2.1.1.1.2" class="ltx_text" style="font-size:80%;"> # pos</span>
</td>
<td id="S4.T2.1.1.2" class="ltx_td ltx_align_left" style="padding:0.6pt 3.5pt;"><span id="S4.T2.1.1.2.1" class="ltx_text" style="font-size:80%;">26.5K</span></td>
<td id="S4.T2.1.1.3" class="ltx_td ltx_align_left" style="padding:0.6pt 3.5pt;"><span id="S4.T2.1.1.3.1" class="ltx_text" style="font-size:80%;">11.3K</span></td>
<td id="S4.T2.1.1.4" class="ltx_td ltx_align_left" style="padding:0.6pt 3.5pt;"><span id="S4.T2.1.1.4.1" class="ltx_text" style="font-size:80%;">19.5K</span></td>
<td id="S4.T2.1.1.5" class="ltx_td ltx_align_left" style="padding:0.6pt 3.5pt;"><span id="S4.T2.1.1.5.1" class="ltx_text" style="font-size:80%;">11.7K</span></td>
</tr>
<tr id="S4.T2.2.2" class="ltx_tr">
<td id="S4.T2.2.2.1" class="ltx_td ltx_align_left" style="padding:0.6pt 3.5pt;">
<span id="S4.T2.2.2.1.1" class="ltx_text" style="font-size:80%;">  </span><span id="S4.T2.2.2.1.2" class="ltx_text" style="font-size:80%;"> # neg</span>
</td>
<td id="S4.T2.2.2.2" class="ltx_td ltx_align_left" style="padding:0.6pt 3.5pt;"><span id="S4.T2.2.2.2.1" class="ltx_text" style="font-size:80%;">0</span></td>
<td id="S4.T2.2.2.3" class="ltx_td ltx_align_left" style="padding:0.6pt 3.5pt;"><span id="S4.T2.2.2.3.1" class="ltx_text" style="font-size:80%;">0</span></td>
<td id="S4.T2.2.2.4" class="ltx_td ltx_align_left" style="padding:0.6pt 3.5pt;"><span id="S4.T2.2.2.4.1" class="ltx_text" style="font-size:80%;">0</span></td>
<td id="S4.T2.2.2.5" class="ltx_td ltx_align_left" style="padding:0.6pt 3.5pt;"><span id="S4.T2.2.2.5.1" class="ltx_text" style="font-size:80%;">9.4K</span></td>
</tr>
<tr id="S4.T2.6.10.4" class="ltx_tr">
<td id="S4.T2.6.10.4.1" class="ltx_td ltx_align_left" style="padding:0.6pt 3.5pt;"><span id="S4.T2.6.10.4.1.1" class="ltx_text" style="font-size:80%;"># boxes</span></td>
<td id="S4.T2.6.10.4.2" class="ltx_td ltx_align_left" style="padding:0.6pt 3.5pt;"><span id="S4.T2.6.10.4.2.1" class="ltx_text" style="font-size:80%;">10.2K</span></td>
<td id="S4.T2.6.10.4.3" class="ltx_td ltx_align_left" style="padding:0.6pt 3.5pt;"><span id="S4.T2.6.10.4.3.1" class="ltx_text" style="font-size:80%;">4.6K</span></td>
<td id="S4.T2.6.10.4.4" class="ltx_td ltx_align_left" style="padding:0.6pt 3.5pt;"><span id="S4.T2.6.10.4.4.1" class="ltx_text" style="font-size:80%;">32.1K</span></td>
<td id="S4.T2.6.10.4.5" class="ltx_td ltx_align_left" style="padding:0.6pt 3.5pt;">
<span id="S4.T2.6.10.4.5.1" class="ltx_text" style="font-size:80%;">20.4K </span><span id="S4.T2.6.10.4.5.2" class="ltx_text" style="font-size:50%;">(165.7K)</span>
</td>
</tr>
<tr id="S4.T2.6.6" class="ltx_tr">
<td id="S4.T2.6.6.5" class="ltx_td ltx_align_left ltx_border_bb" style="padding:0.6pt 3.5pt;"><span id="S4.T2.6.6.5.1" class="ltx_text" style="font-size:80%;"># boxes/descr</span></td>
<td id="S4.T2.3.3.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding:0.6pt 3.5pt;">
<span id="S4.T2.3.3.1.1" class="ltx_text" style="font-size:80%;">1.0</span><math id="S4.T2.3.3.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.3.3.1.m1.1a"><mo mathsize="80%" id="S4.T2.3.3.1.m1.1.1" xref="S4.T2.3.3.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.1.m1.1b"><csymbol cd="latexml" id="S4.T2.3.3.1.m1.1.1.cmml" xref="S4.T2.3.3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.1.m1.1c">\pm</annotation></semantics></math><span id="S4.T2.3.3.1.2" class="ltx_text" style="font-size:80%;">0.0</span>
</td>
<td id="S4.T2.4.4.2" class="ltx_td ltx_align_left ltx_border_bb" style="padding:0.6pt 3.5pt;">
<span id="S4.T2.4.4.2.1" class="ltx_text" style="font-size:80%;">1.0</span><math id="S4.T2.4.4.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.4.4.2.m1.1a"><mo mathsize="80%" id="S4.T2.4.4.2.m1.1.1" xref="S4.T2.4.4.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.2.m1.1b"><csymbol cd="latexml" id="S4.T2.4.4.2.m1.1.1.cmml" xref="S4.T2.4.4.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.2.m1.1c">\pm</annotation></semantics></math><span id="S4.T2.4.4.2.2" class="ltx_text" style="font-size:80%;">0.0</span>
</td>
<td id="S4.T2.5.5.3" class="ltx_td ltx_align_left ltx_border_bb" style="padding:0.6pt 3.5pt;">
<span id="S4.T2.5.5.3.1" class="ltx_text" style="font-size:80%;">1.6</span><math id="S4.T2.5.5.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.5.5.3.m1.1a"><mo mathsize="80%" id="S4.T2.5.5.3.m1.1.1" xref="S4.T2.5.5.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.3.m1.1b"><csymbol cd="latexml" id="S4.T2.5.5.3.m1.1.1.cmml" xref="S4.T2.5.5.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.3.m1.1c">\pm</annotation></semantics></math><span id="S4.T2.5.5.3.2" class="ltx_text" style="font-size:80%;">1.6</span>
</td>
<td id="S4.T2.6.6.4" class="ltx_td ltx_align_left ltx_border_bb" style="padding:0.6pt 3.5pt;">
<span id="S4.T2.6.6.4.1" class="ltx_text" style="font-size:80%;">1.7</span><math id="S4.T2.6.6.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.6.6.4.m1.1a"><mo mathsize="80%" id="S4.T2.6.6.4.m1.1.1" xref="S4.T2.6.6.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.6.6.4.m1.1b"><csymbol cd="latexml" id="S4.T2.6.6.4.m1.1.1.cmml" xref="S4.T2.6.6.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.6.4.m1.1c">\pm</annotation></semantics></math><span id="S4.T2.6.6.4.2" class="ltx_text" style="font-size:80%;">1.0</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.19.1.1" class="ltx_text" style="font-size:113%;">Table 2</span>: </span><span id="S4.T2.20.2" class="ltx_text" style="font-size:113%;">
Basic statistics of the validation sets of OmniLabel, the combination of RefCOCO/g/+ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>, Flickr30k <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> and PhraseCut <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>.
All numbers are based only on free-form object descriptions. Numbers in parenthesis in the last column indicate statistics with plain categories included
</span></figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Dataset Analysis</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This section analyzes various statistics of OmniLabel and compares them with other related datasets. For all datasets, we analyze the corresponding validation sets.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Basic statistics</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p"><a href="#S4.T2" title="In Annotators: ‣ 4 Dataset Collection ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">2</span></a> summarizes key numbers of our OmniLabel dataset in comparison with prior benchmarks on referring expressions or visual grounding, specifically, the combination of RefCOCO/g/+ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>, Flickr30k <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> and PhraseCut <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>.
The key takeaways are: (a) The existence of negative objet descriptions (# neg). Like in standard detection, where categories not present in an image are considered negative and are still evaluated, OmniLabel provides free-form object descriptions that are related to the image but do not refer to any object. (b) The number of bounding boxes per description is higher than for any other dataset, which adds to the difficulty of the benchmark.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Analysis of free-form object descriptions</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">While OmniLabel also contains plain categories like in standard object detection, our focus for this analysis is on the free-form object descriptions.</p>
</div>
<figure id="S5.F5" class="ltx_figure"><img src="/html/2304.11463/assets/figures/histogram_pos-tag-descriptions_incl-plain-True.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="299" height="299" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S5.F5.3.2" class="ltx_text" style="font-size:90%;">
Grouping of the words in our object descriptions into relevant part-of-speech tags. We show the count of unique words (left) and the distribution of words (right). The statistics are computed from a random subset of 10K descriptions of each dataset.
</span></figcaption>
</figure>
<section id="S5.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Part-Of-Speech (POS) tagging:</h4>

<div id="S5.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS2.SSS0.Px1.p1.3" class="ltx_p">To analyze the content and the diversity of our object descriptions, <a href="#S5.F5" title="In 5.2 Analysis of free-form object descriptions ‣ 5 Dataset Analysis ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a> shows an analysis of the words when grouped by part-of-speech tagging. On the left, we have the number of unique words (not counting multiple occurrences) based on a random subset of 10K descriptions. For adjectives, verbs and particularly nouns, OmniLabel covers more unique words, attributing to its diversity. On the right, we have the distribution of (non-unique) words among the different POS tags. We observe a more uniform distribution than other datasets, indicating longer descriptions (see below) that are closer to sentences, rather than short phrases or single words. On average, we have 2.04 <math id="S5.SS2.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.SS2.SSS0.Px1.p1.1.m1.1a"><mo id="S5.SS2.SSS0.Px1.p1.1.m1.1.1" xref="S5.SS2.SSS0.Px1.p1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px1.p1.1.m1.1b"><csymbol cd="latexml" id="S5.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S5.SS2.SSS0.Px1.p1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px1.p1.1.m1.1c">\pm</annotation></semantics></math> 0.90 nouns, 0.62 <math id="S5.SS2.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.SS2.SSS0.Px1.p1.2.m2.1a"><mo id="S5.SS2.SSS0.Px1.p1.2.m2.1.1" xref="S5.SS2.SSS0.Px1.p1.2.m2.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px1.p1.2.m2.1b"><csymbol cd="latexml" id="S5.SS2.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S5.SS2.SSS0.Px1.p1.2.m2.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px1.p1.2.m2.1c">\pm</annotation></semantics></math> 0.71 adjectives and 0.43 <math id="S5.SS2.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.SS2.SSS0.Px1.p1.3.m3.1a"><mo id="S5.SS2.SSS0.Px1.p1.3.m3.1.1" xref="S5.SS2.SSS0.Px1.p1.3.m3.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px1.p1.3.m3.1b"><csymbol cd="latexml" id="S5.SS2.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S5.SS2.SSS0.Px1.p1.3.m3.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px1.p1.3.m3.1c">\pm</annotation></semantics></math> 0.61 verbs per object description.</p>
</div>
</section>
<section id="S5.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Description lengths:</h4>

<div id="S5.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS2.SSS0.Px2.p1.1" class="ltx_p"><a href="#S6.F6" title="In 6.1 Models ‣ 6 Baselines ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">6</span></a> confirms our assumption from above that object descriptions in OmniLabel contain more words than other datasets.</p>
</div>
</section>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Baselines</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Beyond statistics of the collected annotations, we also evaluate recent language-based object detection models on OmniLabel with our novel evaluation metric.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Models</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">Our evaluation aims to encompass a wide range of models, and we select them based on their performance on (a) standard detection benchmarks (like LVIS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> and COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>), and (b) tasks like Phrase Grounding and Referring Expression Compression. For models that primarily focus on open-vocabulary detection via large-scale pre-training, we utilize RegionCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> and Detic <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>. For models that are designed for text-conditioned detection with state-of-the-art performance on visual grounding, we use MDETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, GLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, and FIBER <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.
We present a brief summary of each of these models.</p>
</div>
<figure id="S6.F6" class="ltx_figure"><img src="/html/2304.11463/assets/figures/histograms_description_lengths_words-incl-plain-True.png" id="S6.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="225" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S6.F6.3.2" class="ltx_text" style="font-size:90%;">Histograms of description lengths (in number of words) for OmniLabel and three other datasets.</span></figcaption>
</figure>
<div id="S6.SS1.p2" class="ltx_para ltx_noindent">
<p id="S6.SS1.p2.1" class="ltx_p"><span id="S6.SS1.p2.1.1" class="ltx_text ltx_font_bold">RegionCLIP</span> is an open-vocabulary object detector based on Faster RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>. It adopts pretrained CLIP’s visual encoder (ResNet-50) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> as the backbone and is finetuned with image-text pairs from the Internet (e.g. CC3M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>). Thus, RegionCLIP is expected to get lower performance on our benchmark, compared to other baselines trained with detection and visual grounding datasets.</p>
</div>
<div id="S6.SS1.p3" class="ltx_para ltx_noindent">
<p id="S6.SS1.p3.1" class="ltx_p"><span id="S6.SS1.p3.1.1" class="ltx_text ltx_font_bold">Detic</span> is an open-vocabulary object detector that relies on CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> embeddings to encode class names. It utilizes a combination of box-level and image-level annotations, with a loss function that is weakly-supervised (modified Federated Loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>). For the results presented, we utilized Swin-Base <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> as the backbone.</p>
</div>
<div id="S6.SS1.p4" class="ltx_para ltx_noindent">
<p id="S6.SS1.p4.1" class="ltx_p"><span id="S6.SS1.p4.1.1" class="ltx_text ltx_font_bold">MDETR</span> is an end-to-end modulated detector that can detect objects for a given free-form text query, with a tight coupling between image and text modalities. The model is based on DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and trained with a combination of different visual grounding datasets (GoldG).</p>
</div>
<div id="S6.SS1.p5" class="ltx_para ltx_noindent">
<p id="S6.SS1.p5.1" class="ltx_p"><span id="S6.SS1.p5.1.1" class="ltx_text ltx_font_bold">GLIP</span> is a large scale visual grounding model that is trained with a combination of detection annotations, visual grounding data and image-text pairs. We evaluate its two variants, GLIP-T and GLIP-L. GLIP-T adopts Swin-Tiny <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> as the backbone and is trained with Objects365 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, GoldG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, CC3M, and SBU <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>. GLIP-L adopts Swin-Large <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> as the backbone and is trained with several detection datasets (including Objects365, OpenImages <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, and Visual Genome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>), GoldG, CC12M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, SBU, and additional 24M image-text pairs collected from the Internet.</p>
</div>
<div id="S6.SS1.p6" class="ltx_para ltx_noindent">
<p id="S6.SS1.p6.1" class="ltx_p"><span id="S6.SS1.p6.1.1" class="ltx_text ltx_font_bold">FIBER-B</span> introduces a two-stage pretraining strategy, from coarse- to fine-grained data, with image-text and image-text-box annotations, respectively. It generally follows the model design and the training protocol of GLIP, but adopts Swin-Base <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> as the backbone.</p>
</div>
<figure id="S6.T3" class="ltx_table">
<table id="S6.T3.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T3.2.1.1" class="ltx_tr">
<th id="S6.T3.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" style="padding:0.6pt 4.0pt;">
<div id="S6.T3.2.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:7.0pt;height:24.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:24.3pt;transform:translate(-8.62pt,-7.84pt) rotate(-90deg) ;">
<p id="S6.T3.2.1.1.1.1.1" class="ltx_p"><span id="S6.T3.2.1.1.1.1.1.1" class="ltx_text" style="font-size:80%;">Images</span></p>
</span></div>
</th>
<th id="S6.T3.2.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" style="padding:0.6pt 4.0pt;">
<div id="S6.T3.2.1.1.2.1" class="ltx_inline-block ltx_transformed_outer" style="width:5.6pt;height:27.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:27.1pt;transform:translate(-10.78pt,-10.78pt) rotate(-90deg) ;">
<p id="S6.T3.2.1.1.2.1.1" class="ltx_p"><span id="S6.T3.2.1.1.2.1.1.1" class="ltx_text" style="font-size:80%;">Method</span></p>
</span></div>
</th>
<th id="S6.T3.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding:0.6pt 4.0pt;">
<div id="S6.T3.2.1.1.3.1" class="ltx_inline-block ltx_transformed_outer" style="width:5.5pt;height:11.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:11.4pt;transform:translate(-2.99pt,-2.99pt) rotate(-90deg) ;">
<p id="S6.T3.2.1.1.3.1.1" class="ltx_p"><span id="S6.T3.2.1.1.3.1.1.1" class="ltx_text" style="font-size:80%;">AP</span></p>
</span></div>
</th>
<th id="S6.T3.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding:0.6pt 4.0pt;">
<div id="S6.T3.2.1.1.4.1" class="ltx_inline-block ltx_transformed_outer" style="width:7.0pt;height:32.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:32.3pt;transform:translate(-12.66pt,-11.88pt) rotate(-90deg) ;">
<p id="S6.T3.2.1.1.4.1.1" class="ltx_p"><span id="S6.T3.2.1.1.4.1.1.1" class="ltx_text" style="font-size:80%;">AP-categ</span></p>
</span></div>
</th>
<th id="S6.T3.2.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding:0.6pt 4.0pt;">
<div id="S6.T3.2.1.1.5.1" class="ltx_inline-block ltx_transformed_outer" style="width:5.6pt;height:32pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:32.0pt;transform:translate(-13.2pt,-13.2pt) rotate(-90deg) ;">
<p id="S6.T3.2.1.1.5.1.1" class="ltx_p"><span id="S6.T3.2.1.1.5.1.1.1" class="ltx_text" style="font-size:80%;">AP-descr</span></p>
</span></div>
</th>
<th id="S6.T3.2.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding:0.6pt 4.0pt;">
<div id="S6.T3.2.1.1.6.1" class="ltx_inline-block ltx_transformed_outer" style="width:7.1pt;height:46.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:46.4pt;transform:translate(-19.67pt,-18.89pt) rotate(-90deg) ;">
<p id="S6.T3.2.1.1.6.1.1" class="ltx_p"><span id="S6.T3.2.1.1.6.1.1.1" class="ltx_text" style="font-size:80%;">AP-descr-pos</span></p>
</span></div>
</th>
<th id="S6.T3.2.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding:0.6pt 4.0pt;">
<div id="S6.T3.2.1.1.7.1" class="ltx_inline-block ltx_transformed_outer" style="width:5.6pt;height:39.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:39.1pt;transform:translate(-16.76pt,-16.76pt) rotate(-90deg) ;">
<p id="S6.T3.2.1.1.7.1.1" class="ltx_p"><span id="S6.T3.2.1.1.7.1.1.1" class="ltx_text" style="font-size:80%;">AP-descr-S</span></p>
</span></div>
</th>
<th id="S6.T3.2.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding:0.6pt 4.0pt;">
<div id="S6.T3.2.1.1.8.1" class="ltx_inline-block ltx_transformed_outer" style="width:5.6pt;height:42pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:42.0pt;transform:translate(-18.2pt,-18.2pt) rotate(-90deg) ;">
<p id="S6.T3.2.1.1.8.1.1" class="ltx_p"><span id="S6.T3.2.1.1.8.1.1.1" class="ltx_text" style="font-size:80%;">AP-descr-M</span></p>
</span></div>
</th>
<th id="S6.T3.2.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding:0.6pt 4.0pt;">
<div id="S6.T3.2.1.1.9.1" class="ltx_inline-block ltx_transformed_outer" style="width:5.6pt;height:39.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:39.6pt;transform:translate(-17.03pt,-17.03pt) rotate(-90deg) ;">
<p id="S6.T3.2.1.1.9.1.1" class="ltx_p"><span id="S6.T3.2.1.1.9.1.1.1" class="ltx_text" style="font-size:80%;">AP-descr-L</span></p>
</span></div>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T3.2.2.1" class="ltx_tr">
<th id="S6.T3.2.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" style="padding:0.6pt 4.0pt;" rowspan="6"><span id="S6.T3.2.2.1.1.1" class="ltx_text" style="font-size:80%;">
<span id="S6.T3.2.2.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:5.6pt;height:10.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:10.4pt;transform:translate(-2.44pt,-2.44pt) rotate(-90deg) ;">
<span id="S6.T3.2.2.1.1.1.1.1" class="ltx_p">All</span>
</span></span></span></th>
<th id="S6.T3.2.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" style="padding:0.6pt 4.0pt;">
<span id="S6.T3.2.2.1.2.1" class="ltx_text" style="font-size:80%;">RegionCLIP </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T3.2.2.1.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib53" title="" class="ltx_ref">53</a><span id="S6.T3.2.2.1.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S6.T3.2.2.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.2.1.3.1" class="ltx_text" style="font-size:80%;">2.7</span></td>
<td id="S6.T3.2.2.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.2.1.4.1" class="ltx_text" style="font-size:80%;">2.7</span></td>
<td id="S6.T3.2.2.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.2.1.5.1" class="ltx_text" style="font-size:80%;">2.6</span></td>
<td id="S6.T3.2.2.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.2.1.6.1" class="ltx_text" style="font-size:80%;">3.2</span></td>
<td id="S6.T3.2.2.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.2.1.7.1" class="ltx_text" style="font-size:80%;">3.6</span></td>
<td id="S6.T3.2.2.1.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.2.1.8.1" class="ltx_text" style="font-size:80%;">2.7</span></td>
<td id="S6.T3.2.2.1.9" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.2.1.9.1" class="ltx_text" style="font-size:80%;">2.3</span></td>
</tr>
<tr id="S6.T3.2.3.2" class="ltx_tr">
<th id="S6.T3.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.6pt 4.0pt;">
<span id="S6.T3.2.3.2.1.1" class="ltx_text" style="font-size:80%;">Detic </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T3.2.3.2.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib55" title="" class="ltx_ref">55</a><span id="S6.T3.2.3.2.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S6.T3.2.3.2.2" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.3.2.2.1" class="ltx_text" style="font-size:80%;">8.0</span></td>
<td id="S6.T3.2.3.2.3" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.3.2.3.1" class="ltx_text" style="font-size:80%;">15.6</span></td>
<td id="S6.T3.2.3.2.4" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.3.2.4.1" class="ltx_text" style="font-size:80%;">5.4</span></td>
<td id="S6.T3.2.3.2.5" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.3.2.5.1" class="ltx_text" style="font-size:80%;">8.0</span></td>
<td id="S6.T3.2.3.2.6" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.3.2.6.1" class="ltx_text" style="font-size:80%;">5.7</span></td>
<td id="S6.T3.2.3.2.7" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.3.2.7.1" class="ltx_text" style="font-size:80%;">5.4</span></td>
<td id="S6.T3.2.3.2.8" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.3.2.8.1" class="ltx_text" style="font-size:80%;">6.2</span></td>
</tr>
<tr id="S6.T3.2.4.3" class="ltx_tr">
<th id="S6.T3.2.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding:0.6pt 4.0pt;">
<span id="S6.T3.2.4.3.1.1" class="ltx_text" style="font-size:80%;">MDETR </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T3.2.4.3.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib20" title="" class="ltx_ref">20</a><span id="S6.T3.2.4.3.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S6.T3.2.4.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.4.3.2.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S6.T3.2.4.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.4.3.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S6.T3.2.4.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.4.3.4.1" class="ltx_text" style="font-size:80%;">4.7</span></td>
<td id="S6.T3.2.4.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.4.3.5.1" class="ltx_text" style="font-size:80%;">9.1</span></td>
<td id="S6.T3.2.4.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.4.3.6.1" class="ltx_text" style="font-size:80%;">6.4</span></td>
<td id="S6.T3.2.4.3.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.4.3.7.1" class="ltx_text" style="font-size:80%;">4.6</span></td>
<td id="S6.T3.2.4.3.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.4.3.8.1" class="ltx_text" style="font-size:80%;">4.0</span></td>
</tr>
<tr id="S6.T3.2.5.4" class="ltx_tr">
<th id="S6.T3.2.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.6pt 4.0pt;">
<span id="S6.T3.2.5.4.1.1" class="ltx_text" style="font-size:80%;">GLIP-T </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T3.2.5.4.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib29" title="" class="ltx_ref">29</a><span id="S6.T3.2.5.4.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S6.T3.2.5.4.2" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.5.4.2.1" class="ltx_text" style="font-size:80%;">19.3</span></td>
<td id="S6.T3.2.5.4.3" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.5.4.3.1" class="ltx_text" style="font-size:80%;">23.6</span></td>
<td id="S6.T3.2.5.4.4" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.5.4.4.1" class="ltx_text" style="font-size:80%;">16.4</span></td>
<td id="S6.T3.2.5.4.5" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.5.4.5.1" class="ltx_text" style="font-size:80%;">25.8</span></td>
<td id="S6.T3.2.5.4.6" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.5.4.6.1" class="ltx_text" style="font-size:80%;">29.4</span></td>
<td id="S6.T3.2.5.4.7" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.5.4.7.1" class="ltx_text" style="font-size:80%;">14.8</span></td>
<td id="S6.T3.2.5.4.8" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.5.4.8.1" class="ltx_text" style="font-size:80%;">8.2</span></td>
</tr>
<tr id="S6.T3.2.6.5" class="ltx_tr">
<th id="S6.T3.2.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.6pt 4.0pt;">
<span id="S6.T3.2.6.5.1.1" class="ltx_text" style="font-size:80%;">GLIP-L </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T3.2.6.5.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib29" title="" class="ltx_ref">29</a><span id="S6.T3.2.6.5.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S6.T3.2.6.5.2" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.6.5.2.1" class="ltx_text" style="font-size:80%;">25.8</span></td>
<td id="S6.T3.2.6.5.3" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.6.5.3.1" class="ltx_text" style="font-size:80%;">32.9</span></td>
<td id="S6.T3.2.6.5.4" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.6.5.4.1" class="ltx_text" style="font-size:80%;">21.2</span></td>
<td id="S6.T3.2.6.5.5" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.6.5.5.1" class="ltx_text" style="font-size:80%;">33.2</span></td>
<td id="S6.T3.2.6.5.6" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.6.5.6.1" class="ltx_text" style="font-size:80%;">37.7</span></td>
<td id="S6.T3.2.6.5.7" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.6.5.7.1" class="ltx_text" style="font-size:80%;">18.9</span></td>
<td id="S6.T3.2.6.5.8" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.6.5.8.1" class="ltx_text" style="font-size:80%;">10.8</span></td>
</tr>
<tr id="S6.T3.2.7.6" class="ltx_tr">
<th id="S6.T3.2.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.6pt 4.0pt;">
<span id="S6.T3.2.7.6.1.1" class="ltx_text" style="font-size:80%;">FIBER-B </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T3.2.7.6.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib12" title="" class="ltx_ref">12</a><span id="S6.T3.2.7.6.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S6.T3.2.7.6.2" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.7.6.2.1" class="ltx_text" style="font-size:80%;">25.7</span></td>
<td id="S6.T3.2.7.6.3" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.7.6.3.1" class="ltx_text" style="font-size:80%;">30.3</span></td>
<td id="S6.T3.2.7.6.4" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.7.6.4.1" class="ltx_text" style="font-size:80%;">22.3</span></td>
<td id="S6.T3.2.7.6.5" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.7.6.5.1" class="ltx_text" style="font-size:80%;">34.8</span></td>
<td id="S6.T3.2.7.6.6" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.7.6.6.1" class="ltx_text" style="font-size:80%;">38.6</span></td>
<td id="S6.T3.2.7.6.7" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.7.6.7.1" class="ltx_text" style="font-size:80%;">19.5</span></td>
<td id="S6.T3.2.7.6.8" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.7.6.8.1" class="ltx_text" style="font-size:80%;">12.4</span></td>
</tr>
<tr id="S6.T3.2.8.7" class="ltx_tr">
<th id="S6.T3.2.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding:0.6pt 4.0pt;" rowspan="6"><span id="S6.T3.2.8.7.1.1" class="ltx_text" style="font-size:80%;">
<span id="S6.T3.2.8.7.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:5.5pt;height:24pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:24.0pt;transform:translate(-9.27pt,-9.27pt) rotate(-90deg) ;">
<span id="S6.T3.2.8.7.1.1.1.1" class="ltx_p">COCO</span>
</span></span></span></th>
<th id="S6.T3.2.8.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.8.7.2.1" class="ltx_text" style="font-size:80%;">RegionCLIP</span></th>
<td id="S6.T3.2.8.7.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.8.7.3.1" class="ltx_text" style="font-size:80%;">4.1</span></td>
<td id="S6.T3.2.8.7.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.8.7.4.1" class="ltx_text" style="font-size:80%;">5.1</span></td>
<td id="S6.T3.2.8.7.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.8.7.5.1" class="ltx_text" style="font-size:80%;">3.5</span></td>
<td id="S6.T3.2.8.7.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.8.7.6.1" class="ltx_text" style="font-size:80%;">5.1</span></td>
<td id="S6.T3.2.8.7.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.8.7.7.1" class="ltx_text" style="font-size:80%;">6.1</span></td>
<td id="S6.T3.2.8.7.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.8.7.8.1" class="ltx_text" style="font-size:80%;">3.3</span></td>
<td id="S6.T3.2.8.7.9" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.8.7.9.1" class="ltx_text" style="font-size:80%;">4.1</span></td>
</tr>
<tr id="S6.T3.2.9.8" class="ltx_tr">
<th id="S6.T3.2.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.9.8.1.1" class="ltx_text" style="font-size:80%;">Detic</span></th>
<td id="S6.T3.2.9.8.2" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.9.8.2.1" class="ltx_text" style="font-size:80%;">8.3</span></td>
<td id="S6.T3.2.9.8.3" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.9.8.3.1" class="ltx_text" style="font-size:80%;">43.1</span></td>
<td id="S6.T3.2.9.8.4" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.9.8.4.1" class="ltx_text" style="font-size:80%;">4.6</span></td>
<td id="S6.T3.2.9.8.5" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.9.8.5.1" class="ltx_text" style="font-size:80%;">9.9</span></td>
<td id="S6.T3.2.9.8.6" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.9.8.6.1" class="ltx_text" style="font-size:80%;">10.2</span></td>
<td id="S6.T3.2.9.8.7" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.9.8.7.1" class="ltx_text" style="font-size:80%;">3.5</span></td>
<td id="S6.T3.2.9.8.8" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.9.8.8.1" class="ltx_text" style="font-size:80%;">7.2</span></td>
</tr>
<tr id="S6.T3.2.10.9" class="ltx_tr">
<th id="S6.T3.2.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.10.9.1.1" class="ltx_text" style="font-size:80%;">MDETR</span></th>
<td id="S6.T3.2.10.9.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.10.9.2.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S6.T3.2.10.9.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.10.9.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S6.T3.2.10.9.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.10.9.4.1" class="ltx_text" style="font-size:80%;">13.2</span></td>
<td id="S6.T3.2.10.9.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.10.9.5.1" class="ltx_text" style="font-size:80%;">31.6</span></td>
<td id="S6.T3.2.10.9.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.10.9.6.1" class="ltx_text" style="font-size:80%;">15.4</span></td>
<td id="S6.T3.2.10.9.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.10.9.7.1" class="ltx_text" style="font-size:80%;">13.5</span></td>
<td id="S6.T3.2.10.9.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.10.9.8.1" class="ltx_text" style="font-size:80%;">12.4</span></td>
</tr>
<tr id="S6.T3.2.11.10" class="ltx_tr">
<th id="S6.T3.2.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.11.10.1.1" class="ltx_text" style="font-size:80%;">GLIP-T</span></th>
<td id="S6.T3.2.11.10.2" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.11.10.2.1" class="ltx_text" style="font-size:80%;">18.7</span></td>
<td id="S6.T3.2.11.10.3" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.11.10.3.1" class="ltx_text" style="font-size:80%;">45.7</span></td>
<td id="S6.T3.2.11.10.4" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.11.10.4.1" class="ltx_text" style="font-size:80%;">11.7</span></td>
<td id="S6.T3.2.11.10.5" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.11.10.5.1" class="ltx_text" style="font-size:80%;">31.2</span></td>
<td id="S6.T3.2.11.10.6" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.11.10.6.1" class="ltx_text" style="font-size:80%;">27.0</span></td>
<td id="S6.T3.2.11.10.7" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.11.10.7.1" class="ltx_text" style="font-size:80%;">10.9</span></td>
<td id="S6.T3.2.11.10.8" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.11.10.8.1" class="ltx_text" style="font-size:80%;">10.2</span></td>
</tr>
<tr id="S6.T3.2.12.11" class="ltx_tr">
<th id="S6.T3.2.12.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.12.11.1.1" class="ltx_text" style="font-size:80%;">GLIP-L</span></th>
<td id="S6.T3.2.12.11.2" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.12.11.2.1" class="ltx_text" style="font-size:80%;">21.8</span></td>
<td id="S6.T3.2.12.11.3" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.12.11.3.1" class="ltx_text" style="font-size:80%;">50.4</span></td>
<td id="S6.T3.2.12.11.4" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.12.11.4.1" class="ltx_text" style="font-size:80%;">13.9</span></td>
<td id="S6.T3.2.12.11.5" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.12.11.5.1" class="ltx_text" style="font-size:80%;">36.8</span></td>
<td id="S6.T3.2.12.11.6" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.12.11.6.1" class="ltx_text" style="font-size:80%;">28.9</span></td>
<td id="S6.T3.2.12.11.7" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.12.11.7.1" class="ltx_text" style="font-size:80%;">12.9</span></td>
<td id="S6.T3.2.12.11.8" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.12.11.8.1" class="ltx_text" style="font-size:80%;">11.5</span></td>
</tr>
<tr id="S6.T3.2.13.12" class="ltx_tr">
<th id="S6.T3.2.13.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.13.12.1.1" class="ltx_text" style="font-size:80%;">FIBER-B</span></th>
<td id="S6.T3.2.13.12.2" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.13.12.2.1" class="ltx_text" style="font-size:80%;">22.2</span></td>
<td id="S6.T3.2.13.12.3" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.13.12.3.1" class="ltx_text" style="font-size:80%;">49.6</span></td>
<td id="S6.T3.2.13.12.4" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.13.12.4.1" class="ltx_text" style="font-size:80%;">14.3</span></td>
<td id="S6.T3.2.13.12.5" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.13.12.5.1" class="ltx_text" style="font-size:80%;">38.8</span></td>
<td id="S6.T3.2.13.12.6" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.13.12.6.1" class="ltx_text" style="font-size:80%;">31.3</span></td>
<td id="S6.T3.2.13.12.7" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.13.12.7.1" class="ltx_text" style="font-size:80%;">12.7</span></td>
<td id="S6.T3.2.13.12.8" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.13.12.8.1" class="ltx_text" style="font-size:80%;">14.2</span></td>
</tr>
<tr id="S6.T3.2.14.13" class="ltx_tr">
<th id="S6.T3.2.14.13.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding:0.6pt 4.0pt;" rowspan="6"><span id="S6.T3.2.14.13.1.1" class="ltx_text" style="font-size:80%;">
<span id="S6.T3.2.14.13.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:7.1pt;height:41.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:41.6pt;transform:translate(-17.24pt,-16.47pt) rotate(-90deg) ;">
<span id="S6.T3.2.14.13.1.1.1.1" class="ltx_p">Objects-365</span>
</span></span></span></th>
<th id="S6.T3.2.14.13.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.14.13.2.1" class="ltx_text" style="font-size:80%;">RegionCLIP</span></th>
<td id="S6.T3.2.14.13.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.14.13.3.1" class="ltx_text" style="font-size:80%;">3.6</span></td>
<td id="S6.T3.2.14.13.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.14.13.4.1" class="ltx_text" style="font-size:80%;">3.6</span></td>
<td id="S6.T3.2.14.13.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.14.13.5.1" class="ltx_text" style="font-size:80%;">3.6</span></td>
<td id="S6.T3.2.14.13.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.14.13.6.1" class="ltx_text" style="font-size:80%;">4.1</span></td>
<td id="S6.T3.2.14.13.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.14.13.7.1" class="ltx_text" style="font-size:80%;">5.0</span></td>
<td id="S6.T3.2.14.13.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.14.13.8.1" class="ltx_text" style="font-size:80%;">3.5</span></td>
<td id="S6.T3.2.14.13.9" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.14.13.9.1" class="ltx_text" style="font-size:80%;">3.0</span></td>
</tr>
<tr id="S6.T3.2.15.14" class="ltx_tr">
<th id="S6.T3.2.15.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.15.14.1.1" class="ltx_text" style="font-size:80%;">Detic</span></th>
<td id="S6.T3.2.15.14.2" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.15.14.2.1" class="ltx_text" style="font-size:80%;">9.1</span></td>
<td id="S6.T3.2.15.14.3" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.15.14.3.1" class="ltx_text" style="font-size:80%;">21.6</span></td>
<td id="S6.T3.2.15.14.4" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.15.14.4.1" class="ltx_text" style="font-size:80%;">5.7</span></td>
<td id="S6.T3.2.15.14.5" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.15.14.5.1" class="ltx_text" style="font-size:80%;">8.4</span></td>
<td id="S6.T3.2.15.14.6" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.15.14.6.1" class="ltx_text" style="font-size:80%;">6.6</span></td>
<td id="S6.T3.2.15.14.7" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.15.14.7.1" class="ltx_text" style="font-size:80%;">5.9</span></td>
<td id="S6.T3.2.15.14.8" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.15.14.8.1" class="ltx_text" style="font-size:80%;">6.9</span></td>
</tr>
<tr id="S6.T3.2.16.15" class="ltx_tr">
<th id="S6.T3.2.16.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.16.15.1.1" class="ltx_text" style="font-size:80%;">MDETR</span></th>
<td id="S6.T3.2.16.15.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.16.15.2.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S6.T3.2.16.15.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.16.15.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S6.T3.2.16.15.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.16.15.4.1" class="ltx_text" style="font-size:80%;">3.2</span></td>
<td id="S6.T3.2.16.15.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.16.15.5.1" class="ltx_text" style="font-size:80%;">5.9</span></td>
<td id="S6.T3.2.16.15.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.16.15.6.1" class="ltx_text" style="font-size:80%;">3.0</span></td>
<td id="S6.T3.2.16.15.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.16.15.7.1" class="ltx_text" style="font-size:80%;">3.2</span></td>
<td id="S6.T3.2.16.15.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.16.15.8.1" class="ltx_text" style="font-size:80%;">2.7</span></td>
</tr>
<tr id="S6.T3.2.17.16" class="ltx_tr">
<th id="S6.T3.2.17.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.17.16.1.1" class="ltx_text" style="font-size:80%;">GLIP-T</span></th>
<td id="S6.T3.2.17.16.2" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.17.16.2.1" class="ltx_text" style="font-size:80%;">22.6</span></td>
<td id="S6.T3.2.17.16.3" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.17.16.3.1" class="ltx_text" style="font-size:80%;">30.0</span></td>
<td id="S6.T3.2.17.16.4" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.17.16.4.1" class="ltx_text" style="font-size:80%;">18.1</span></td>
<td id="S6.T3.2.17.16.5" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.17.16.5.1" class="ltx_text" style="font-size:80%;">26.9</span></td>
<td id="S6.T3.2.17.16.6" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.17.16.6.1" class="ltx_text" style="font-size:80%;">34.2</span></td>
<td id="S6.T3.2.17.16.7" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.17.16.7.1" class="ltx_text" style="font-size:80%;">16.0</span></td>
<td id="S6.T3.2.17.16.8" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.17.16.8.1" class="ltx_text" style="font-size:80%;">9.1</span></td>
</tr>
<tr id="S6.T3.2.18.17" class="ltx_tr">
<th id="S6.T3.2.18.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.18.17.1.1" class="ltx_text" style="font-size:80%;">GLIP-L</span></th>
<td id="S6.T3.2.18.17.2" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.18.17.2.1" class="ltx_text" style="font-size:80%;">29.3</span></td>
<td id="S6.T3.2.18.17.3" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.18.17.3.1" class="ltx_text" style="font-size:80%;">37.5</span></td>
<td id="S6.T3.2.18.17.4" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.18.17.4.1" class="ltx_text" style="font-size:80%;">24.0</span></td>
<td id="S6.T3.2.18.17.5" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.18.17.5.1" class="ltx_text" style="font-size:80%;">35.2</span></td>
<td id="S6.T3.2.18.17.6" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.18.17.6.1" class="ltx_text" style="font-size:80%;">44.5</span></td>
<td id="S6.T3.2.18.17.7" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.18.17.7.1" class="ltx_text" style="font-size:80%;">20.5</span></td>
<td id="S6.T3.2.18.17.8" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.18.17.8.1" class="ltx_text" style="font-size:80%;">11.8</span></td>
</tr>
<tr id="S6.T3.2.19.18" class="ltx_tr">
<th id="S6.T3.2.19.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.19.18.1.1" class="ltx_text" style="font-size:80%;">FIBER-B</span></th>
<td id="S6.T3.2.19.18.2" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.19.18.2.1" class="ltx_text" style="font-size:80%;">30.8</span></td>
<td id="S6.T3.2.19.18.3" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.19.18.3.1" class="ltx_text" style="font-size:80%;">37.9</span></td>
<td id="S6.T3.2.19.18.4" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.19.18.4.1" class="ltx_text" style="font-size:80%;">25.9</span></td>
<td id="S6.T3.2.19.18.5" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.19.18.5.1" class="ltx_text" style="font-size:80%;">38.2</span></td>
<td id="S6.T3.2.19.18.6" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.19.18.6.1" class="ltx_text" style="font-size:80%;">44.7</span></td>
<td id="S6.T3.2.19.18.7" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.19.18.7.1" class="ltx_text" style="font-size:80%;">22.5</span></td>
<td id="S6.T3.2.19.18.8" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.19.18.8.1" class="ltx_text" style="font-size:80%;">14.1</span></td>
</tr>
<tr id="S6.T3.2.20.19" class="ltx_tr">
<th id="S6.T3.2.20.19.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" style="padding:0.6pt 4.0pt;" rowspan="6"><span id="S6.T3.2.20.19.1.1" class="ltx_text" style="font-size:80%;">
<span id="S6.T3.2.20.19.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:7.0pt;height:54pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:54.0pt;transform:translate(-23.51pt,-22.73pt) rotate(-90deg) ;">
<span id="S6.T3.2.20.19.1.1.1.1" class="ltx_p">OpenImages v5</span>
</span></span></span></th>
<th id="S6.T3.2.20.19.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.20.19.2.1" class="ltx_text" style="font-size:80%;">RegionCLIP</span></th>
<td id="S6.T3.2.20.19.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.20.19.3.1" class="ltx_text" style="font-size:80%;">2.3</span></td>
<td id="S6.T3.2.20.19.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.20.19.4.1" class="ltx_text" style="font-size:80%;">2.1</span></td>
<td id="S6.T3.2.20.19.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.20.19.5.1" class="ltx_text" style="font-size:80%;">2.7</span></td>
<td id="S6.T3.2.20.19.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.20.19.6.1" class="ltx_text" style="font-size:80%;">2.9</span></td>
<td id="S6.T3.2.20.19.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.20.19.7.1" class="ltx_text" style="font-size:80%;">3.4</span></td>
<td id="S6.T3.2.20.19.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.20.19.8.1" class="ltx_text" style="font-size:80%;">2.7</span></td>
<td id="S6.T3.2.20.19.9" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.20.19.9.1" class="ltx_text" style="font-size:80%;">2.0</span></td>
</tr>
<tr id="S6.T3.2.21.20" class="ltx_tr">
<th id="S6.T3.2.21.20.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.21.20.1.1" class="ltx_text" style="font-size:80%;">Detic</span></th>
<td id="S6.T3.2.21.20.2" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.21.20.2.1" class="ltx_text" style="font-size:80%;">6.4</span></td>
<td id="S6.T3.2.21.20.3" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.21.20.3.1" class="ltx_text" style="font-size:80%;">8.1</span></td>
<td id="S6.T3.2.21.20.4" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.21.20.4.1" class="ltx_text" style="font-size:80%;">5.4</span></td>
<td id="S6.T3.2.21.20.5" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.21.20.5.1" class="ltx_text" style="font-size:80%;">6.9</span></td>
<td id="S6.T3.2.21.20.6" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.21.20.6.1" class="ltx_text" style="font-size:80%;">5.4</span></td>
<td id="S6.T3.2.21.20.7" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.21.20.7.1" class="ltx_text" style="font-size:80%;">5.6</span></td>
<td id="S6.T3.2.21.20.8" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.21.20.8.1" class="ltx_text" style="font-size:80%;">5.8</span></td>
</tr>
<tr id="S6.T3.2.22.21" class="ltx_tr">
<th id="S6.T3.2.22.21.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.22.21.1.1" class="ltx_text" style="font-size:80%;">MDETR</span></th>
<td id="S6.T3.2.22.21.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.22.21.2.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S6.T3.2.22.21.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.22.21.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S6.T3.2.22.21.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.22.21.4.1" class="ltx_text" style="font-size:80%;">6.1</span></td>
<td id="S6.T3.2.22.21.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.22.21.5.1" class="ltx_text" style="font-size:80%;">10.6</span></td>
<td id="S6.T3.2.22.21.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.22.21.6.1" class="ltx_text" style="font-size:80%;">9.6</span></td>
<td id="S6.T3.2.22.21.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.22.21.7.1" class="ltx_text" style="font-size:80%;">5.7</span></td>
<td id="S6.T3.2.22.21.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.22.21.8.1" class="ltx_text" style="font-size:80%;">4.1</span></td>
</tr>
<tr id="S6.T3.2.23.22" class="ltx_tr">
<th id="S6.T3.2.23.22.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.23.22.1.1" class="ltx_text" style="font-size:80%;">GLIP-T</span></th>
<td id="S6.T3.2.23.22.2" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.23.22.2.1" class="ltx_text" style="font-size:80%;">17.6</span></td>
<td id="S6.T3.2.23.22.3" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.23.22.3.1" class="ltx_text" style="font-size:80%;">20.0</span></td>
<td id="S6.T3.2.23.22.4" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.23.22.4.1" class="ltx_text" style="font-size:80%;">15.7</span></td>
<td id="S6.T3.2.23.22.5" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.23.22.5.1" class="ltx_text" style="font-size:80%;">24.4</span></td>
<td id="S6.T3.2.23.22.6" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.23.22.6.1" class="ltx_text" style="font-size:80%;">25.8</span></td>
<td id="S6.T3.2.23.22.7" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.23.22.7.1" class="ltx_text" style="font-size:80%;">14.9</span></td>
<td id="S6.T3.2.23.22.8" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.23.22.8.1" class="ltx_text" style="font-size:80%;">7.5</span></td>
</tr>
<tr id="S6.T3.2.24.23" class="ltx_tr">
<th id="S6.T3.2.24.23.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.24.23.1.1" class="ltx_text" style="font-size:80%;">GLIP-L</span></th>
<td id="S6.T3.2.24.23.2" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.24.23.2.1" class="ltx_text" style="font-size:80%;">25.7</span></td>
<td id="S6.T3.2.24.23.3" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.24.23.3.1" class="ltx_text" style="font-size:80%;">35.8</span></td>
<td id="S6.T3.2.24.23.4" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.24.23.4.1" class="ltx_text" style="font-size:80%;">20.1</span></td>
<td id="S6.T3.2.24.23.5" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.24.23.5.1" class="ltx_text" style="font-size:80%;">31.2</span></td>
<td id="S6.T3.2.24.23.6" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.24.23.6.1" class="ltx_text" style="font-size:80%;">33.3</span></td>
<td id="S6.T3.2.24.23.7" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.24.23.7.1" class="ltx_text" style="font-size:80%;">18.7</span></td>
<td id="S6.T3.2.24.23.8" class="ltx_td ltx_align_center" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.24.23.8.1" class="ltx_text" style="font-size:80%;">10.3</span></td>
</tr>
<tr id="S6.T3.2.25.24" class="ltx_tr">
<th id="S6.T3.2.25.24.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.25.24.1.1" class="ltx_text" style="font-size:80%;">FIBER-B</span></th>
<td id="S6.T3.2.25.24.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.25.24.2.1" class="ltx_text" style="font-size:80%;">22.0</span></td>
<td id="S6.T3.2.25.24.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.25.24.3.1" class="ltx_text" style="font-size:80%;">24.4</span></td>
<td id="S6.T3.2.25.24.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.25.24.4.1" class="ltx_text" style="font-size:80%;">20.1</span></td>
<td id="S6.T3.2.25.24.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.25.24.5.1" class="ltx_text" style="font-size:80%;">30.9</span></td>
<td id="S6.T3.2.25.24.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.25.24.6.1" class="ltx_text" style="font-size:80%;">34.1</span></td>
<td id="S6.T3.2.25.24.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.25.24.7.1" class="ltx_text" style="font-size:80%;">18.5</span></td>
<td id="S6.T3.2.25.24.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.6pt 4.0pt;"><span id="S6.T3.2.25.24.8.1" class="ltx_text" style="font-size:80%;">10.5</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S6.T3.6.1.1" class="ltx_text" style="font-size:113%;">Table 3</span>: </span><span id="S6.T3.7.2" class="ltx_text" style="font-size:113%;">
Evaluation of language-based detection baselines on the OmniLabel benchmark with the metric described in <a href="#S3.SS2" title="3.2 Evaluation Metric ‣ 3 Benchmark and Evaluation Metric ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>. The final AP value is the geometric mean of only plain categories (AP-categ) and free-form descriptions (AP-descr). AP-descr-pos evaluates on only positive descriptions, clearly showcasing the impact of negative descriptions. AP-descr-S/M/L evaluate descriptions of different length (up to 3 words, 4 to 8, and more than 8)
</span></figcaption>
</figure>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Results</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">We run two experiments with the above described models. The first one focuses on a detailed analysis of our new metric (see <a href="#S3.SS2" title="3.2 Evaluation Metric ‣ 3 Benchmark and Evaluation Metric ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>) on the OmniLabel dataset. The second experiment compares our metric on three different datasets.</p>
</div>
<section id="S6.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Analysis on the OmniLabel dataset – <a href="#S6.T3" title="In 6.1 Models ‣ 6 Baselines ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">3</span></a>:</h4>

<div id="S6.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S6.SS2.SSS0.Px1.p1.1" class="ltx_p">The first observation we make is that nearly all methods achieve higher accuracy on plain object categories (AP-categ) compared to free-form text descriptions (AP-descr).
One can also clearly see the effect of using the geometric mean for the final metric (AP), when averaging over plain categories (AP-categ) and free-form descriptions (AP-descr). This effect is more pronounced for COCO images.
</p>
</div>
<div id="S6.SS2.SSS0.Px1.p2" class="ltx_para">
<p id="S6.SS2.SSS0.Px1.p2.1" class="ltx_p">A key takeaway message from <a href="#S6.T3" title="In 6.1 Models ‣ 6 Baselines ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">3</span></a> is the impact of negative descriptions. The performance gap between including negative descriptions in the label space (AP-descr) and excluding them (AP-descr-pos) is significant. The biggest gap can be observed for COCO images, which is because this part of the dataset contains the most negative descriptions relative to the number of images (due to our data collection process),
see <a href="#A2" title="Appendix B Additional information on data collection ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">B</span></a>).
Another observation we get from <a href="#S6.T3" title="In 6.1 Models ‣ 6 Baselines ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">3</span></a> is that accuracy correlates negatively with description length. AP values are in general higher for shorter descriptions (AP-descr-S, up to three words) than for longer descriptions (AP-descr-L, more than 8 words).</p>
</div>
<div id="S6.SS2.SSS0.Px1.p3" class="ltx_para">
<p id="S6.SS2.SSS0.Px1.p3.1" class="ltx_p">Finally, we can see that GLIP-T/L and FIBER-B achieve the best results on OmniLabel. MDETR achieves reasonable results when only considering positive descriptions (AP-descr-pos) but fails when negatives are added (AP-descr), likely due to the specific training algorithm. Also, we did not report results of MDETR for AP or AP-categ due to the significant runtime induced by the large labelspace and MDETR’s model design. As expected, Detic is good on plain categories (AP-categ) but underperforms on free-form descriptions (AP-descr). RegionCLIP’s lower performance is likely due to a combination of a weaker backbone and the training data.</p>
</div>
<figure id="S6.T4" class="ltx_table">
<table id="S6.T4.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T4.2.1.1" class="ltx_tr">
<th id="S6.T4.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.65pt 5.0pt;"><span id="S6.T4.2.1.1.1.1" class="ltx_text" style="font-size:90%;">Method</span></th>
<td id="S6.T4.2.1.1.2" class="ltx_td ltx_align_center" style="padding:0.65pt 5.0pt;" colspan="2"><span id="S6.T4.2.1.1.2.1" class="ltx_text" style="font-size:90%;">OmniLabel</span></td>
<td id="S6.T4.2.1.1.3" class="ltx_td ltx_align_center" style="padding:0.65pt 5.0pt;"><span id="S6.T4.2.1.1.3.1" class="ltx_text" style="font-size:90%;">RefCOCOg</span></td>
<td id="S6.T4.2.1.1.4" class="ltx_td ltx_align_center" style="padding:0.65pt 5.0pt;"><span id="S6.T4.2.1.1.4.1" class="ltx_text" style="font-size:90%;">PhraseCut</span></td>
</tr>
<tr id="S6.T4.2.2.2" class="ltx_tr">
<th id="S6.T4.2.2.2.1" class="ltx_td ltx_th ltx_th_row" style="padding:0.65pt 5.0pt;"></th>
<td id="S6.T4.2.2.2.2" class="ltx_td ltx_align_center" style="padding:0.65pt 5.0pt;"><span id="S6.T4.2.2.2.2.1" class="ltx_text" style="font-size:80%;">descr</span></td>
<td id="S6.T4.2.2.2.3" class="ltx_td ltx_align_center" style="padding:0.65pt 5.0pt;"><span id="S6.T4.2.2.2.3.1" class="ltx_text" style="font-size:80%;">descr-pos</span></td>
<td id="S6.T4.2.2.2.4" class="ltx_td ltx_align_center" style="padding:0.65pt 5.0pt;"><span id="S6.T4.2.2.2.4.1" class="ltx_text" style="font-size:80%;">descr</span></td>
<td id="S6.T4.2.2.2.5" class="ltx_td ltx_align_center" style="padding:0.65pt 5.0pt;"><span id="S6.T4.2.2.2.5.1" class="ltx_text" style="font-size:80%;">descr</span></td>
</tr>
<tr id="S6.T4.2.3.3" class="ltx_tr">
<th id="S6.T4.2.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" style="padding:0.65pt 5.0pt;">
<span id="S6.T4.2.3.3.1.1" class="ltx_text" style="font-size:90%;">RegionCLIP </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T4.2.3.3.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib53" title="" class="ltx_ref">53</a><span id="S6.T4.2.3.3.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S6.T4.2.3.3.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.65pt 5.0pt;"><span id="S6.T4.2.3.3.2.1" class="ltx_text" style="font-size:90%;">2.6</span></td>
<td id="S6.T4.2.3.3.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.65pt 5.0pt;"><span id="S6.T4.2.3.3.3.1" class="ltx_text" style="font-size:90%;">3.2</span></td>
<td id="S6.T4.2.3.3.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.65pt 5.0pt;"><span id="S6.T4.2.3.3.4.1" class="ltx_text" style="font-size:90%;">1.1</span></td>
<td id="S6.T4.2.3.3.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.65pt 5.0pt;"><span id="S6.T4.2.3.3.5.1" class="ltx_text" style="font-size:90%;">2.2</span></td>
</tr>
<tr id="S6.T4.2.4.4" class="ltx_tr">
<th id="S6.T4.2.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.65pt 5.0pt;">
<span id="S6.T4.2.4.4.1.1" class="ltx_text" style="font-size:90%;">Detic </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T4.2.4.4.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib55" title="" class="ltx_ref">55</a><span id="S6.T4.2.4.4.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S6.T4.2.4.4.2" class="ltx_td ltx_align_center" style="padding:0.65pt 5.0pt;"><span id="S6.T4.2.4.4.2.1" class="ltx_text" style="font-size:90%;">5.4</span></td>
<td id="S6.T4.2.4.4.3" class="ltx_td ltx_align_center" style="padding:0.65pt 5.0pt;"><span id="S6.T4.2.4.4.3.1" class="ltx_text" style="font-size:90%;">8.0</span></td>
<td id="S6.T4.2.4.4.4" class="ltx_td ltx_align_center" style="padding:0.65pt 5.0pt;"><span id="S6.T4.2.4.4.4.1" class="ltx_text" style="font-size:90%;">6.8</span></td>
<td id="S6.T4.2.4.4.5" class="ltx_td ltx_align_center" style="padding:0.65pt 5.0pt;"><span id="S6.T4.2.4.4.5.1" class="ltx_text" style="font-size:90%;">6.8</span></td>
</tr>
<tr id="S6.T4.2.5.5" class="ltx_tr">
<th id="S6.T4.2.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding:0.65pt 5.0pt;">
<span id="S6.T4.2.5.5.1.1" class="ltx_text" style="font-size:90%;">GLIP-T </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T4.2.5.5.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib29" title="" class="ltx_ref">29</a><span id="S6.T4.2.5.5.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S6.T4.2.5.5.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.65pt 5.0pt;"><span id="S6.T4.2.5.5.2.1" class="ltx_text" style="font-size:90%;">16.4</span></td>
<td id="S6.T4.2.5.5.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.65pt 5.0pt;"><span id="S6.T4.2.5.5.3.1" class="ltx_text" style="font-size:90%;">25.8</span></td>
<td id="S6.T4.2.5.5.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.65pt 5.0pt;"><span id="S6.T4.2.5.5.4.1" class="ltx_text" style="font-size:90%;">32.1</span></td>
<td id="S6.T4.2.5.5.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.65pt 5.0pt;"><span id="S6.T4.2.5.5.5.1" class="ltx_text" style="font-size:90%;">23.9</span></td>
</tr>
<tr id="S6.T4.2.6.6" class="ltx_tr">
<th id="S6.T4.2.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.65pt 5.0pt;">
<span id="S6.T4.2.6.6.1.1" class="ltx_text" style="font-size:90%;">GLIP-L </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T4.2.6.6.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib29" title="" class="ltx_ref">29</a><span id="S6.T4.2.6.6.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S6.T4.2.6.6.2" class="ltx_td ltx_align_center" style="padding:0.65pt 5.0pt;"><span id="S6.T4.2.6.6.2.1" class="ltx_text" style="font-size:90%;">21.2</span></td>
<td id="S6.T4.2.6.6.3" class="ltx_td ltx_align_center" style="padding:0.65pt 5.0pt;"><span id="S6.T4.2.6.6.3.1" class="ltx_text" style="font-size:90%;">33.2</span></td>
<td id="S6.T4.2.6.6.4" class="ltx_td ltx_align_center" style="padding:0.65pt 5.0pt;"><span id="S6.T4.2.6.6.4.1" class="ltx_text" style="font-size:90%;">33.4</span></td>
<td id="S6.T4.2.6.6.5" class="ltx_td ltx_align_center" style="padding:0.65pt 5.0pt;"><span id="S6.T4.2.6.6.5.1" class="ltx_text" style="font-size:90%;">29.3</span></td>
</tr>
<tr id="S6.T4.2.7.7" class="ltx_tr">
<th id="S6.T4.2.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding:0.65pt 5.0pt;">
<span id="S6.T4.2.7.7.1.1" class="ltx_text" style="font-size:90%;">FIBER-B </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T4.2.7.7.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib12" title="" class="ltx_ref">12</a><span id="S6.T4.2.7.7.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S6.T4.2.7.7.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.65pt 5.0pt;"><span id="S6.T4.2.7.7.2.1" class="ltx_text" style="font-size:90%;">22.3</span></td>
<td id="S6.T4.2.7.7.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.65pt 5.0pt;"><span id="S6.T4.2.7.7.3.1" class="ltx_text" style="font-size:90%;">34.8</span></td>
<td id="S6.T4.2.7.7.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.65pt 5.0pt;"><span id="S6.T4.2.7.7.4.1" class="ltx_text" style="font-size:90%;">33.0</span></td>
<td id="S6.T4.2.7.7.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.65pt 5.0pt;"><span id="S6.T4.2.7.7.5.1" class="ltx_text" style="font-size:90%;">27.4</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 4: </span>
Comparing our evaluation metric (<a href="#S3.SS2" title="3.2 Evaluation Metric ‣ 3 Benchmark and Evaluation Metric ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>) for all models on three different datasets. OmniLabel poses a more difficult challenge, specifically when negative descriptions are included (descr vs. descr-pos)
</figcaption>
</figure>
</section>
<section id="S6.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Evaluation metric across different datasets – <a href="#S6.T4" title="In Analysis on the OmniLabel dataset – Tab. 3: ‣ 6.2 Results ‣ 6 Baselines ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">4</span></a>:</h4>

<div id="S6.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S6.SS2.SSS0.Px2.p1.1" class="ltx_p">We compare all models on three datasets (OmniLabel, RefCOCOg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> and PhraseCut <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>). We make two main observations: First, OmniLabel is a more difficult benchmark, particularly because of negative descriptions. Second, the proposed evaluation metric from <a href="#S3.SS2" title="3.2 Evaluation Metric ‣ 3 Benchmark and Evaluation Metric ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.2</span></a> is more stringent than the one used in RefCOCO/g/+. For instance, FIBER-B on RefCOCOg (val) achieves 87.1% accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> compared to the 33.0 from <a href="#S6.T4" title="In Analysis on the OmniLabel dataset – Tab. 3: ‣ 6.2 Results ‣ 6 Baselines ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
</section>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusions</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">OmniLabel presents a novel benchmark for evaluating language-based object detectors. A key innovations is the annotation process, which (a) encourages free-form text descriptions of objects that are complex and diverse, (b) ensures collecting difficult examples with multiple instances of the same underlying category present in the images, and (c) provides negative free-form descriptions that are related but not present in an image. Moreover, OmniLabel defines a novel task setting and a corresponding evaluation metric.
Our analysis of the dataset shows that we could indeed collect object descriptions that are diverse and contain more unique nouns, verbs and adjectives than existing benchmarks. Also, evaluating recent language-based object detectors confirmed the level of difficulty that OmniLabel poses to these models.
We hope that our contributions in providing a challenging benchmark help progress the field towards robust object detectors that understand semantically rich and complex descriptions of objects.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Zeynep Akata, Florent Perronnin, Zaid Harchaoui, and Cordelia Schmid.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Label-embedding for attribute-based classification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Zeynep Akata, Scott Reed, Daniel Walter, Honglak Lee, and Bernt Schiele.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Evaluation of output embeddings for fine-grained image
classification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Mykhaylo Andriluka, Stefan Roth, and Bernt Schiele.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">People-tracking-by-detection and people-detection-by-tracking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, 2008.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Ankan Bansal, Karan Sikka, Gaurav Sharma, Rama Chellappa, and Ajay Divakaran.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Zero-shot object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, pages 384–400, 2018.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Zhaowei Cai, Gukyeong Kwon, Avinash Ravichandran, Erhan Bas, Zhuowen Tu, Rahul
Bhotika, and Stefano Soatto.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">X-DETR: A versatile architecture for instance-wise
vision-language tasks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander
Kirillov, and Sergey Zagoruyko.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">End-to-end object detection with transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Conceptual 1M: Pushing web-scale image-text pre-training to
recognize long-tail visual concepts.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr
Dollár, and C. Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Microsoft coco captions: Data collection and evaluation server.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1504.00325</span><span id="bib.bib8.4.2" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Universal Dependencies contributors.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Universal POS tags.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">https://universaldependencies.org/u/pos/.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler,
Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">The cityscapes dataset for semantic urban scene understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Piotr Dollár, Christian Wojek, Bernt Schiele, and Pietro Perona.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Pedestrian detection: An evaluation of the state of the art.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE TPAMI</span><span id="bib.bib11.4.2" class="ltx_text" style="font-size:90%;">, 34(4):743–761, 2012.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Zi-Yi Dou, Aishwarya Kamath, Zhe Gan, Pengchuan Zhang, Jianfeng Wang, Linjie
Li, Zicheng Liu, Ce Liu, Yann LeCun, Nanyun Peng, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Coarse-to-Fine Vision-Language Pre-training with Fusion in the
Backbone.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Mark Everingham, Luc Van Gool, Christopher K. I. Williams, John Winn., and
Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">The Pascal Visual Object Classes (VOC) Challenge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IJCV</span><span id="bib.bib13.4.2" class="ltx_text" style="font-size:90%;">, 88(2):303–338, June 2010.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Explosion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">spaCy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">https://spacy.io.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Andreas Geiger, Philip Lenz, and Raquel Urtasun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Are we ready for Autonomous Driving? The KITTI Vision Benchmark
Suite.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, 2012.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Scaling Open-Vocabulary Image Segmentation with Image-Level Labels.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Open-vocabulary Object Detection via Vision and Language Knowledge
Distillation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Agrim Gupta, Piotr Dollár, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">LVIS: A Dataset for Large Vocabulary Instance Segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V.
Le, Yunhsuan Sung, Zhen Li, and Tom Duerig.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Scaling Up Visual and Vision-Language Representation Learning With
Noisy Text Supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICML</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and
Nicolas Carion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">MDETR–Modulated Detection for End-to-End Multi-Modal
Understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Zaid Khan, Vijay Kumar B.G., Xiang Yu, Samuel Schulter, Manmohan Chandraker,
and Yun Fu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Single-Stream Multi-Level Alignment for Vision-Language Pretraining
.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua
Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Visual genome: Connecting language and vision using crowdsourced
dense image annotations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IJCV</span><span id="bib.bib22.4.2" class="ltx_text" style="font-size:90%;">, 123:32–73, 2017.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi
Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander
Kolesnikov, Tom Duerig, and Vittorio Ferrari.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">The open images dataset v4: Unified image classification, object
detection, and visual relationship detection at scale.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IJCV</span><span id="bib.bib23.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Christoph H. Lampert, Hannes Nickisch, and Stefan Harmeling.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Learning To Detect Unseen Object Classes by Between-Class Attribute
Transfer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, 2009.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Christoph H. Lampert, Hannes Nickisch, and Stefan Harmeling.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Attribute-Based Classification for Zero-Shot Visual Object
Categorization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE TPAMI</span><span id="bib.bib25.4.2" class="ltx_text" style="font-size:90%;">, 36(3):453–465, 2014.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Boyi Li, Kilian Q. Weinberger, Serge Belongie, Vladlen Koltun, and René
Ranftl.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Language-driven Semantic Segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Chunyuan Li, Haotian Liu, Liunian Harold Li, Pengchuan Zhang, Jyoti Aneja,
Jianwei Yang, Ping Jin, Houdong Hu, Zicheng Liu, Yong Jae Lee, and Jianfeng
Gao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">ELEVATER: A Benchmark and Toolkit for Evaluating Language-Augmented
Visual Models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Deepak Gotmare, Shafiq Joty,
Caiming Xiong, and Steven Hoi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Align before fuse: Vision and language representation learning with
momentum distillation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li,
Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang,
and Jianfeng Gao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Grounded Language-Image Pre-training.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Chuang Lin, Peize Sun, Yi Jiang, Ping Luo, Lizhen Qu, Gholamreza Haffari,
Zehuan Yuan, and Jianfei Cai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Learning object-language alignments for open-vocabulary object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan,
and Serge Belongie.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Feature Pyramid Networks for Object Detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C. Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Microsoft COCO: Common Objects in Context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed,
Cheng-Yang Fu, and Alexander C. Berg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">SSD: Single shot multibox detector.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
Baining Guo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Swin Transformer: Hierarchical Vision Transformer using Shifted
Windows.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF international conference on
computer vision</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">, pages 10012–10022, 2021.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L. Yuille, and
Kevin Murphy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Generation and comprehension of unambiguous object descriptions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bulò, and Peter Kontschieder.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">The Mapillary Vistas Dataset for Semantic Understanding of Street
Scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Vicente Ordonez, Girish Kulkarni, and Tamara Berg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Im2text: Describing images using 1 million captioned photographs.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib37.4.2" class="ltx_text" style="font-size:90%;">, 24, 2011.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Bryan A. Plummer, Liwei Wang, Christopher M. Cervantes, Juan C. Caicedo, Julia
Hockenmaier, and Svetlana Lazebnik.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Flickr30k entities: Collecting region-to-phrase correspondences for
richer image-to-sentence models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IJCV</span><span id="bib.bib38.4.2" class="ltx_text" style="font-size:90%;">, 123(1):74–93, 2017.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
Gretchen Krueger, and Ilya Sutskever.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">Learning transferable visual models from natural language
supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib39.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICML</span><span id="bib.bib39.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Joseph Redmon and Ali Farhadi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">YOLO9000: Better, Faster, Stronger.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib40.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib40.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">Faster R-CNN: Towards Real-Time Object Detection with Region
Proposal Networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib41.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NIPS</span><span id="bib.bib41.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Jing Li, Xiangyu
Zhang, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Objects365: A Large-scale, High-quality Dataset for Object
Detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib42.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib42.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Conceptual captions: A cleaned, hypernymed, image alt-text dataset
for automatic image captioning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib43.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ACL</span><span id="bib.bib43.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Chenyun Wu, Zhe Lin, Scott Cohen, Trung Bui, and Subhransu Maji.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">PhraseCut: Language-based Image Segmentation in the Wild.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib44.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib44.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Shuo Yang, Ping Luo, Chen Change Loy, and Xiaoou Tang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">WIDER FACE: A Face Detection Benchmark.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib45.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib45.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Fisher Yu, Wenqi Xian, Yingying Chen, Fangchen Liu, Mike Liao, Vashisht
Madhavan, and Trevor Darrell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">BDD100K: A Diverse Driving Video Database with Scalable Annotation
Tooling.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv:1805.04687</span><span id="bib.bib46.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Licheng Yu, Patrick Poirson, Shan Yang, Alexander C. Berg, and Tamara L. Berg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">Modeling Context in Referring Expressions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib47.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib47.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, and Shih-Fu Chang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">Open-Vocabulary Object Detection Using Captions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib48.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib48.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M. Ni, and
Heung-Yeung Shum.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">Dino: Detr with improved denoising anchor boxes for end-to-end object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib49.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib49.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun Chen, Liunian Harold Li,
Xiyang Dai, Lijuan Wang, Lu Yuan, Jenq-Neng Hwang, and Jianfeng Gao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">GLIPv2: Unifying Localization and Vision-Language Understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib50.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib50.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text" style="font-size:90%;">
Shiyu Zhao, Zhixing Zhang, Samuel Schulter, Long Zhao, Vijay Kumar B. G,
Anastasis Stathopoulos, Manmohan Chandraker, and Dimitris Metaxas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.2.1" class="ltx_text" style="font-size:90%;">Exploiting Unlabeled Data with Vision and Language Models for Object
Detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib51.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib51.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text" style="font-size:90%;">
Xiangyun Zhao, Samuel Schulter, Gaurav Sharma, Yi-Hsuan Tsai, Manmohan
Chandraker, and Ying Wu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.2.1" class="ltx_text" style="font-size:90%;">Object Detection with a Unified Label Space from Multiple Datasets
.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib52.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib52.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text" style="font-size:90%;">
Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella,
Liunian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.2.1" class="ltx_text" style="font-size:90%;">RegionCLIP: Region-based Language-Image Pretraining.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib53.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib53.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text" style="font-size:90%;">
Victor Zhou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.2.1" class="ltx_text" style="font-size:90%;">profanity-check.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/vzhou842/profanity-check" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/vzhou842/profanity-check</a><span id="bib.bib54.3.1" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.4.1" class="ltx_text" style="font-size:90%;">Accessed: 2023-03-06.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text" style="font-size:90%;">
Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp Krähenbühl, and
Ishan Misra.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.2.1" class="ltx_text" style="font-size:90%;">Detecting Twenty-thousand Classes using Image-level Supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib55.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib55.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text" style="font-size:90%;">
Xingyi Zhou, Vladlen Koltun, and Philipp Krähenbühl.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.2.1" class="ltx_text" style="font-size:90%;">Probabilistic two-stage detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2103.07461</span><span id="bib.bib56.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text" style="font-size:90%;">
Xingyi Zhou, Vladlen Koltun, and Philipp Krähenbühl.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.2.1" class="ltx_text" style="font-size:90%;">Simple multi-dataset detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib57.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib57.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock"><span id="bib.bib58.1.1" class="ltx_text" style="font-size:90%;">
Xingyi Zhou, Dequan Wang, and Philipp Krähenbühl.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.2.1" class="ltx_text" style="font-size:90%;">Objects as points.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib58.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1904.07850</span><span id="bib.bib58.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock"><span id="bib.bib59.1.1" class="ltx_text" style="font-size:90%;">
Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.2.1" class="ltx_text" style="font-size:90%;">Deformable DETR: Deformable Transformers for End-to-End Object
Detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib59.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib59.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Additional dataset analysis</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">We further analyze the object descriptions we collected for the OmniLabel benchmark in the following paragraphs.</p>
</div>
<section id="A1.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Part-Of-Speech (POS) tags:</h4>

<div id="A1.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="A1.SS0.SSS0.Px1.p1.1" class="ltx_p">In <a href="#S5.SS2" title="5.2 Analysis of free-form object descriptions ‣ 5 Dataset Analysis ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">5.2</span></a>,
we analyze object descriptions by their POS tagging. To get POS tags, we use the <span id="A1.SS0.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_typewriter">spacy</span> toolbox <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, which categorizes each word into one of 17 UPOS tags <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, out of which we selected the 6 most relevant tags for
<a href="#S5.F5" title="In 5.2 Analysis of free-form object descriptions ‣ 5 Dataset Analysis ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a>:</p>
<ul id="A1.I1" class="ltx_itemize">
<li id="A1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i1.p1" class="ltx_para">
<p id="A1.I1.i1.p1.1" class="ltx_p">ADJ: adjective</p>
</div>
</li>
<li id="A1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i2.p1" class="ltx_para">
<p id="A1.I1.i2.p1.1" class="ltx_p">ADP: adposition</p>
</div>
</li>
<li id="A1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i3.p1" class="ltx_para">
<p id="A1.I1.i3.p1.1" class="ltx_p">DET: determiner</p>
</div>
</li>
<li id="A1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i4.p1" class="ltx_para">
<p id="A1.I1.i4.p1.1" class="ltx_p">NOUN: noun</p>
</div>
</li>
<li id="A1.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i5.p1" class="ltx_para">
<p id="A1.I1.i5.p1.1" class="ltx_p">PROPN: proper noun</p>
</div>
</li>
<li id="A1.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i6.p1" class="ltx_para">
<p id="A1.I1.i6.p1.1" class="ltx_p">VERB: verb</p>
</div>
</li>
</ul>
<p id="A1.SS0.SSS0.Px1.p1.2" class="ltx_p"><a href="#A1.F7" title="In Types of language understanding: ‣ Appendix A Additional dataset analysis ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">7</span></a> shows word clouds for the tags NOUN, VERB and ADJ, collected from a random subset of 5K object descriptions.</p>
</div>
</section>
<section id="A1.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Types of language understanding:</h4>

<div id="A1.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="A1.SS0.SSS0.Px2.p1.1" class="ltx_p">To further analyze the our object descriptions, we manually tagged a random subset of 500 descriptions with what type of language understanding they require:</p>
<ul id="A1.I2" class="ltx_itemize">
<li id="A1.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i1.p1" class="ltx_para">
<p id="A1.I2.i1.p1.1" class="ltx_p"><span id="A1.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">“categories”</span>: The description contains one or more object category names</p>
</div>
</li>
<li id="A1.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i2.p1" class="ltx_para">
<p id="A1.I2.i2.p1.1" class="ltx_p"><span id="A1.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">“spatial relations”</span>: Example: “left to”, “behind”</p>
</div>
</li>
<li id="A1.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i3.p1" class="ltx_para">
<p id="A1.I2.i3.p1.1" class="ltx_p"><span id="A1.I2.i3.p1.1.1" class="ltx_text ltx_font_bold">“attributes”</span>: Attribute of objects, <em id="A1.I2.i3.p1.1.2" class="ltx_emph ltx_font_italic">e.g</em>.<span id="A1.I2.i3.p1.1.3" class="ltx_text"></span>, color or material</p>
</div>
</li>
<li id="A1.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i4.p1" class="ltx_para">
<p id="A1.I2.i4.p1.1" class="ltx_p"><span id="A1.I2.i4.p1.1.1" class="ltx_text ltx_font_bold">“(external) knowledge or reasoning”</span>: Knowledge beyond the image content</p>
</div>
</li>
<li id="A1.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i5.p1" class="ltx_para">
<p id="A1.I2.i5.p1.1" class="ltx_p"><span id="A1.I2.i5.p1.1.1" class="ltx_text ltx_font_bold">“functional relations”</span>: Describing objects by their functionality, <em id="A1.I2.i5.p1.1.2" class="ltx_emph ltx_font_italic">e.g</em>.<span id="A1.I2.i5.p1.1.3" class="ltx_text"></span>: “edible item” or “areas to sit on”</p>
</div>
</li>
<li id="A1.I2.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i6.p1" class="ltx_para">
<p id="A1.I2.i6.p1.1" class="ltx_p"><span id="A1.I2.i6.p1.1.1" class="ltx_text ltx_font_bold">“actions”</span>: Any action an object can perform, “person jumping”, “parked car”</p>
</div>
</li>
<li id="A1.I2.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i7.p1" class="ltx_para">
<p id="A1.I2.i7.p1.1" class="ltx_p"><span id="A1.I2.i7.p1.1.1" class="ltx_text ltx_font_bold">“numeracy”</span>: Descriptions that require reasoning about numbers, like counting or understanding the time</p>
</div>
</li>
</ul>
<p id="A1.SS0.SSS0.Px2.p1.2" class="ltx_p"><a href="#A1.F8" title="In Types of language understanding: ‣ Appendix A Additional dataset analysis ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">8</span></a> shows the results of our manual tagging efforts as the percentage of description that were tagged with one of the above types. Note that one description can be tagged with multiple categories. For example, the description “A black cat jumping onto the chair on the left” would get tags for “attribute” (black), “categories” (cat, chair), “action” (jumping), and “spatial relations” (on the left).</p>
</div>
<figure id="A1.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A1.F7.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.11463/assets/figures_supp/wordcloud_pos_NOUN.png" id="A1.F7.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="685" height="294" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A1.F7.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A1.F7.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.11463/assets/figures_supp/wordcloud_pos_VERB.png" id="A1.F7.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="685" height="294" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A1.F7.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A1.F7.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.11463/assets/figures_supp/wordcloud_pos_ADJ.png" id="A1.F7.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="685" height="294" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A1.F7.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A1.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="A1.F7.3.2" class="ltx_text" style="font-size:90%;">Word clouds of nouns (a), verbs (b) and adjectives (c) collected from a subset of 5K object descriptions.</span></figcaption>
</figure>
<div id="A1.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="A1.SS0.SSS0.Px2.p2.1" class="ltx_p">We can see from <a href="#A1.F8" title="In Types of language understanding: ‣ Appendix A Additional dataset analysis ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">8</span></a> that more than 80% of object description include some category name, which is expected. Note that the number of unique nouns is not limited to a fixed label space. In fact, the validation set of OmniLabel has 4.6K unique nouns, a lot more than existing benchmarks,
see <a href="#S1.T1" title="In 1 Introduction ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a>.
Besides category names, close to 40% of object descriptions require an understanding of attributes, spatial relations, and external knowledge or reasoning for correct localization of objects. And finally, understanding of functional capabilities, actions and numeracy is needed for 5-10% of the descriptions. In <a href="#A4" title="Appendix D Examples of Dataset Samples ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">D</span></a>, we provide visual examples for each of the above groups.</p>
</div>
<figure id="A1.F8" class="ltx_figure"><img src="/html/2304.11463/assets/figures_supp/histogram_tagging_object_descriptions.png" id="A1.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="228" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A1.F8.2.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="A1.F8.3.2" class="ltx_text" style="font-size:90%;">
We manually tagged a random subset of 500 object descriptions with the types of language understanding needed to localize the referred instances correctly. The plot shows the percentage of object descriptions tagged for each type. Each description can be tagged with multiple types.
</span></figcaption>
</figure>
</section>
<section id="A1.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Distribution of number of boxes per description:</h4>

<div id="A1.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="A1.SS0.SSS0.Px3.p1.1" class="ltx_p">One aspect that differentiates our OmniLabel dataset from prior benchmarks is the number of instances (bounding boxes) that are referred to by one object description. As we can see in <a href="#A1.F9" title="In Distribution of number of boxes per description: ‣ Appendix A Additional dataset analysis ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">9</span></a>, for both RefCOCO/g/+ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> and Flickr30k <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> all descriptions refer to exactly one instance in the image. PhraseCut <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> and OmniLabel allow multiple instances per description, while OmniLabel shows a lower bias towards referring to one instance.</p>
</div>
<figure id="A1.F9" class="ltx_figure"><img src="/html/2304.11463/assets/figures_supp/histograms_num_boxes_per_description.png" id="A1.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="225" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A1.F9.2.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="A1.F9.3.2" class="ltx_text" style="font-size:90%;">
Distribution of object descriptions referring to different number of instances in the image.
</span></figcaption>
</figure>
</section>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Additional information on data collection</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p"><a href="#S4" title="4 Dataset Collection ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4</span></a>
describes our data collection process. One aspect of this process is that we start from object detection datasets with existing annotations of bounding boxes and corresponding semantic categories.
On COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, semantic annotations contain a category name along with a grouping into super-categories. For Objects-365 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> and OpenImages <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, we manually grouped categories into super-categories.</p>
</div>
<figure id="A2.F10" class="ltx_figure"><img src="/html/2304.11463/assets/figures_supp/piechart_description_types_mpl.png" id="A2.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="225" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A2.F10.2.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span id="A2.F10.3.2" class="ltx_text" style="font-size:90%;">
Pie chart showing the distribution of object descriptions grouped into plain categories and free-form descriptions collected based on standard categories (FF-Class) and super-categories (FF-SuperClass).
</span></figcaption>
</figure>
<div id="A2.p2" class="ltx_para">
<p id="A2.p2.1" class="ltx_p">We leverage this semantic annotation when selecting images for annotation with free-form object descriptions. Specifically, we sample pairs of images and (super-) categories for annotation that fulfill some constraints
(enough instances available, see <a href="#S4" title="4 Dataset Collection ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4</span></a>).
<a href="#A2.F10" title="In Appendix B Additional information on data collection ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">10</span></a> shows the distribution of object descriptions over their origin:</p>
<ul id="A2.I1" class="ltx_itemize">
<li id="A2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I1.i1.p1" class="ltx_para">
<p id="A2.I1.i1.p1.1" class="ltx_p">Plain: Original categories of the underlying dataset</p>
</div>
</li>
<li id="A2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I1.i2.p1" class="ltx_para">
<p id="A2.I1.i2.p1.1" class="ltx_p">FF-Class: Free-form descriptions based on categories</p>
</div>
</li>
<li id="A2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I1.i3.p1" class="ltx_para">
<p id="A2.I1.i3.p1.1" class="ltx_p">FF-SuperClass: Free-form descriptions based on super-categories</p>
</div>
</li>
</ul>
<p id="A2.p2.2" class="ltx_p">The intuition behind sampling based on different types of categories is to collect object descriptions that go beyond using the original category names along with additional context to specify subsets of object instances. And indeed, we found that 45.3% of the “FF-Class” descriptions use the underlying category name, while only 10.8% of the “FF-SuperClass” descriptions use the super-category name and only 5.3% of the “FF-SuperClass” descriptions use any of the subclass names.</p>
</div>
<figure id="A2.F11" class="ltx_figure"><img src="/html/2304.11463/assets/figures_supp/histogram_num_negatives_per_image.png" id="A2.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="225" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A2.F11.2.1.1" class="ltx_text" style="font-size:90%;">Figure 11</span>: </span><span id="A2.F11.3.2" class="ltx_text" style="font-size:90%;">
Distribution of images with different number of negative descriptions. The title of each sub-plot indicates the subset of images that were inspected. Images from COCO have a significantly different distribution to Objects-365 and OpenImages-v5 due to our annotation schedule, see text.
</span></figcaption>
</figure>
<section id="A2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Collection of negative object descriptions</h4>

<div id="A2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="A2.SS0.SSS0.Px1.p1.1" class="ltx_p">A major claim in our paper is the existence of negative descriptions. These are object descriptions that are semantically related to an image, but do not refer to any object. For any given image, we collect such negative descriptions by first randomly sampling collected positive descriptions from other images that contain the same (super-) category. Then, the randomly selected descriptions are manually verified by human annotators to not refer to any object in the image. The semantic relation to the image we obtain from the sampling process makes these negative descriptions difficult distractors. <a href="#A4.F21" title="In Appendix D Examples of Dataset Samples ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figs.</span> <span class="ltx_text ltx_ref_tag">21</span></a> and <a href="#A4.F22" title="Figure 22 ‣ Appendix D Examples of Dataset Samples ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">22</span></a> in <a href="#A4" title="Appendix D Examples of Dataset Samples ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">D</span></a> show several examples.</p>
</div>
<div id="A2.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="A2.SS0.SSS0.Px1.p2.1" class="ltx_p">Finally, <a href="#A2.F11" title="In Appendix B Additional information on data collection ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">11</span></a> shows a distribution of the number of negatives per image, for all images of the dataset as well as for the set of images coming from the three datasets we used for annotation, COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, Objects-365 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, and OpenImages-v5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. The figure shows a significantly different distribution for COCO compared to the other datasets. The absolute numbers of negatives are different given the number of images per dataset, see title of sub-plots. Still, there are two reasons for this stark difference and both relate to our annotation process. First, we collected negative descriptions only for 50% of the images in Objects-365 and OpenImages-v5<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>This might change in the future when we collect more data</span></span></span>. Second, we found that the verification rate of negative descriptions
(see <a href="#S4" title="4 Dataset Collection ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4</span></a>)
is clearly higher for COCO (around 45%) compared to Objects-365 (around 25%) and OpenImages-v5 (around 16%). We suspect the number of underlying object categories to cause this difference in the verification rates, but this aspect needs further investigation.</p>
</div>
<div id="A2.SS0.SSS0.Px1.p3" class="ltx_para">
<p id="A2.SS0.SSS0.Px1.p3.1" class="ltx_p">Nevertheless, the total number of negative descriptions in OmniLabel is currently around 10K, sufficient to make a clearly noticeable impact in the evaluation of models. This can be seen from our evaluation
in <a href="#S6.T3" title="In 6.1 Models ‣ 6 Baselines ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">3</span></a>,
specifically when looking at the difference between AP-descr and AP-descr-pos. The difference between these metrics is that AP-descr-pos does not evaluate on negative descriptions. Given that we observe significantly higher numbers for AP-descr-pos, particularly for COCO images, we can safely conclude that negative descriptions pose a significant challenge to current language-based models.</p>
</div>
</section>
<section id="A2.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Annotation interface:</h4>

<div id="A2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="A2.SS0.SSS0.Px2.p1.1" class="ltx_p">We provide screenshots of the annotation interface for the three tasks we rely on human annotators,
see also <a href="#S4.F4" title="In 4 Dataset Collection ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>:</p>
<ol id="A2.I2" class="ltx_enumerate">
<li id="A2.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span> 
<div id="A2.I2.i1.p1" class="ltx_para">
<p id="A2.I2.i1.p1.1" class="ltx_p">“collect object descriptions” (<a href="#A4.F13" title="In Appendix D Examples of Dataset Samples ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">13</span></a>)</p>
</div>
</li>
<li id="A2.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span> 
<div id="A2.I2.i2.p1" class="ltx_para">
<p id="A2.I2.i2.p1.1" class="ltx_p">“Verification of descriptions” (<a href="#A4.F14" title="In Appendix D Examples of Dataset Samples ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">14</span></a>)</p>
</div>
</li>
<li id="A2.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(c)</span> 
<div id="A2.I2.i3.p1" class="ltx_para">
<p id="A2.I2.i3.p1.1" class="ltx_p">“Collection of negative descriptions” (<a href="#A4.F15" title="In Appendix D Examples of Dataset Samples ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">15</span></a>).</p>
</div>
</li>
</ol>
</div>
</section>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Code and Dataset</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p">Along with the dataset, we built a Python-based toolkit to visualize samples from the dataset and to evaluate prediction results.
The toolbox is publicly released at <a target="_blank" href="https://github.com/samschulter/omnilabeltools" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/samschulter/omnilabeltools</a> and includes a Jupyter notebook <span id="A3.p1.1.1" class="ltx_text ltx_font_typewriter">omnilabel_demo.ipynb</span> demonstrating the use of the library. The last cell in the notebook runs the evaluation with dummy predictions. The final metric,
as described in <a href="#S3.SS2" title="3.2 Evaluation Metric ‣ 3 Benchmark and Evaluation Metric ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>,
is the harmonic mean between AP for plain and freeform-text object descriptions. <a href="#A3.F12" title="In Appendix C Code and Dataset ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">12</span></a> illustrates the impact of using the harmonic mean over the arithmetic mean.</p>
</div>
<figure id="A3.F12" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A3.F12.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.11463/assets/figures_supp/mean_arithmetic.png" id="A3.F12.sf1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="150" height="133" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A3.F12.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A3.F12.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.11463/assets/figures_supp/mean_harmonic.png" id="A3.F12.sf2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="160" height="133" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A3.F12.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A3.F12.2.1.1" class="ltx_text" style="font-size:90%;">Figure 12</span>: </span><span id="A3.F12.3.2" class="ltx_text" style="font-size:90%;">Difference between (a) arithmetic and (b) harmonic mean when averaging two values. The preferred choice in our metric to average AP of plain and freeform-text object descriptions is the harmonic mean, because it encourages good performance on both types of object descriptions. This is apparent from the low values in both the upper left and lower right corners in (b).</span></figcaption>
</figure>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Examples of Dataset Samples</h2>

<div id="A4.p1" class="ltx_para">
<p id="A4.p1.1" class="ltx_p">Finally, we visualize some examples of our datasets. First, <a href="#A4.F16" title="In Appendix D Examples of Dataset Samples ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figs.</span> <span class="ltx_text ltx_ref_tag">16</span></a>, <a href="#A4.F17" title="Figure 17 ‣ Appendix D Examples of Dataset Samples ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">17</span></a>, <a href="#A4.F18" title="Figure 18 ‣ Appendix D Examples of Dataset Samples ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">18</span></a>, <a href="#A4.F19" title="Figure 19 ‣ Appendix D Examples of Dataset Samples ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">19</span></a> and <a href="#A4.F20" title="Figure 20 ‣ Appendix D Examples of Dataset Samples ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">20</span></a> showcase interesting positive examples that highlight the different types of required language understanding as described above in <a href="#A1" title="Appendix A Additional dataset analysis ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">A</span></a>. Second, <a href="#A4.F21" title="In Appendix D Examples of Dataset Samples ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figs.</span> <span class="ltx_text ltx_ref_tag">21</span></a> and <a href="#A4.F22" title="Figure 22 ‣ Appendix D Examples of Dataset Samples ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">22</span></a> show difficult negative object descriptions that are related to the image content but do not actually refer to any object. These negative descriptions pose a significant challenge to current language-based detection models. See the corresponding captions for more details.</p>
</div>
<figure id="A4.F13" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F13.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.11463/assets/figures_supp/amt_description_collection_01.jpeg" id="A4.F13.sf1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="685" height="950" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F13.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F13.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.11463/assets/figures_supp/amt_description_collection_02.jpeg" id="A4.F13.sf2.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="685" height="917" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F13.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F13.2.1.1" class="ltx_text" style="font-size:90%;">Figure 13</span>: </span><span id="A4.F13.3.2" class="ltx_text" style="font-size:90%;">
Two examples of our annotation interface to collect object descriptions. Annotators pick a subset of the bounding boxes by clicking the corresponding checkboxes and write a freeform text description. Note that the selection has some constraints,
as described in <a href="#S4" title="4 Dataset Collection ‣ OmniLabel: A Challenging Benchmark for Language-Based Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4</span></a>.
</span></figcaption>
</figure>
<figure id="A4.F14" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F14.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.11463/assets/figures_supp/amt_description_verification_01.jpeg" id="A4.F14.sf1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="685" height="986" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F14.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F14.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.11463/assets/figures_supp/amt_description_verification_02.jpeg" id="A4.F14.sf2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="685" height="797" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F14.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F14.2.1.1" class="ltx_text" style="font-size:90%;">Figure 14</span>: </span><span id="A4.F14.3.2" class="ltx_text" style="font-size:90%;">
Two examples of our annotation interface to verify collected object descriptions. Annotators are given the image and and a description and need to pick the matching bounding boxes by clicking the corresponding checkboxes.
</span></figcaption>
</figure>
<figure id="A4.F15" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F15.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.11463/assets/figures_supp/amt_negative_verification_01.jpeg" id="A4.F15.sf1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="685" height="848" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F15.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F15.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.11463/assets/figures_supp/amt_negative_verification_02.jpeg" id="A4.F15.sf2.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="685" height="946" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F15.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F15.3.1.1" class="ltx_text" style="font-size:90%;">Figure 15</span>: </span><span id="A4.F15.4.2" class="ltx_text" style="font-size:90%;">
Two examples of our annotation interface to verify <em id="A4.F15.4.2.1" class="ltx_emph ltx_font_italic">negative</em> object descriptions. Annotators are given an image and and a description and are asked if the object refers to any object in the image or not.
</span></figcaption>
</figure>
<figure id="A4.F16" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F16.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.11463/assets/figures_supp_examples/000000001378-000000001773.jpeg" id="A4.F16.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="685" height="548" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F16.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F16.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.11463/assets/figures_supp_examples/000000001246-000000002008.jpeg" id="A4.F16.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="685" height="493" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F16.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F16.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.11463/assets/figures_supp_examples/000000001300-000000002044.jpeg" id="A4.F16.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="685" height="490" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F16.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F16.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.11463/assets/figures_supp_examples/000000001302-000000002057.jpeg" id="A4.F16.sf4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="685" height="548" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F16.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F16.3.1.1" class="ltx_text" style="font-size:90%;">Figure 16</span>: </span><span id="A4.F16.4.2" class="ltx_text" style="font-size:90%;">
Examples of <span id="A4.F16.4.2.1" class="ltx_text ltx_font_bold">positive object descriptions</span> requiring different types of language understanding (we only highlight a subset): categories (“adults”, “benches”, “woman”, “skirts”, “people”, “surfboard”) and actions (“sitting”, “wearing”, “holding”).
</span></figcaption>
</figure>
<figure id="A4.F17" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F17.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.11463/assets/figures_supp_examples/000000001540-000000001441.jpeg" id="A4.F17.sf1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="617" height="878" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F17.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F17.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.11463/assets/figures_supp_examples/000000001396-000000001992.jpeg" id="A4.F17.sf2.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="617" height="864" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F17.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F17.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.11463/assets/figures_supp_examples/000000001572-000000001784.jpeg" id="A4.F17.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="685" height="548" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F17.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F17.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.11463/assets/figures_supp_examples/000000014053-000000015432.jpeg" id="A4.F17.sf4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="685" height="703" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F17.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F17.3.1.1" class="ltx_text" style="font-size:90%;">Figure 17</span>: </span><span id="A4.F17.4.2" class="ltx_text" style="font-size:90%;">
Examples of <span id="A4.F17.4.2.1" class="ltx_text ltx_font_bold">positive object descriptions</span> requiring different types of language understanding (we only highlight a subset): spatial (“second to bottom”, “closer to”, “right side”) and functional relations (“to drink from”).
</span></figcaption>
</figure>
<figure id="A4.F18" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F18.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.11463/assets/figures_supp_examples/000000010742-000000013701.jpeg" id="A4.F18.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="685" height="536" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F18.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F18.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.11463/assets/figures_supp_examples/000000001341-000000001239.jpeg" id="A4.F18.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="685" height="548" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F18.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F18.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.11463/assets/figures_supp_examples/000000015779-000000018869.jpeg" id="A4.F18.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="685" height="492" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F18.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F18.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.11463/assets/figures_supp_examples/000000001379-000000001260.jpeg" id="A4.F18.sf4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="685" height="492" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F18.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F18.3.1.1" class="ltx_text" style="font-size:90%;">Figure 18</span>: </span><span id="A4.F18.4.2" class="ltx_text" style="font-size:90%;">
Examples of <span id="A4.F18.4.2.1" class="ltx_text ltx_font_bold">positive object descriptions</span> requiring different types of language understanding (we only highlight a subset): attributes (“white”, “dark in color”, “green”) and numeracy (“one thirty-two”).
</span></figcaption>
</figure>
<figure id="A4.F19" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F19.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.11463/assets/figures_supp_examples/000000021749-000000025462.jpeg" id="A4.F19.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="685" height="483" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F19.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F19.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.11463/assets/figures_supp_examples/000000017333-000000022661.jpeg" id="A4.F19.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="685" height="548" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F19.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F19.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.11463/assets/figures_supp_examples/000000001476-000000002632.jpeg" id="A4.F19.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="685" height="546" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F19.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F19.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.11463/assets/figures_supp_examples/000000001351-000000001249.jpeg" id="A4.F19.sf4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="685" height="420" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F19.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F19.3.1.1" class="ltx_text" style="font-size:90%;">Figure 19</span>: </span><span id="A4.F19.4.2" class="ltx_text" style="font-size:90%;">
Examples of <span id="A4.F19.4.2.1" class="ltx_text ltx_font_bold">positive object descriptions</span> requiring different types of language understanding (we only highlight a subset): numeracy (“six dots”) and external knowledge or reasoning (“devices with screens”, “meant to run on the ground”, “mario character”).
</span></figcaption>
</figure>
<figure id="A4.F20" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F20.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.11463/assets/figures_supp_examples/000000001239-000000002462.jpeg" id="A4.F20.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="685" height="548" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F20.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F20.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.11463/assets/figures_supp_examples/000000016272-000000018387.jpeg" id="A4.F20.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="685" height="482" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F20.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F20.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.11463/assets/figures_supp_examples/000000009946-000000012562.jpeg" id="A4.F20.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="685" height="536" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F20.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F20.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.11463/assets/figures_supp_examples/000000010587-000000016520.jpeg" id="A4.F20.sf4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="685" height="331" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F20.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F20.3.1.1" class="ltx_text" style="font-size:90%;">Figure 20</span>: </span><span id="A4.F20.4.2" class="ltx_text" style="font-size:90%;">
Examples of <span id="A4.F20.4.2.1" class="ltx_text ltx_font_bold">positive object descriptions</span> requiring different types of language understanding (we only highlight a subset): external knowledge or reasoning (“container with alcohol”, “Coke logo”, “HSBC sign”, “numbered buttons”).
</span></figcaption>
</figure>
<figure id="A4.F21" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F21.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.11463/assets/figures_supp_examples_neg/0000001260.jpeg" id="A4.F21.sf1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="685" height="680" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F21.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F21.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.11463/assets/figures_supp_examples_neg/0000001282.jpeg" id="A4.F21.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="685" height="540" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F21.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F21.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.11463/assets/figures_supp_examples_neg/0000001290.jpeg" id="A4.F21.sf3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="685" height="571" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F21.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F21.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.11463/assets/figures_supp_examples_neg/0000001357.jpeg" id="A4.F21.sf4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="685" height="727" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F21.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F21.3.1.1" class="ltx_text" style="font-size:90%;">Figure 21</span>: </span><span id="A4.F21.4.2" class="ltx_text" style="font-size:90%;">
Examples of difficult <span id="A4.F21.4.2.1" class="ltx_text ltx_font_bold">negative object descriptions</span>, which are listed below the respective images. Note that for positive descriptions, we only show the freeform-text descriptions and omit the plain categories to avoid cuttered visualizations in the image.
</span></figcaption>
</figure>
<figure id="A4.F22" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F22.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.11463/assets/figures_supp_examples_neg/0000001473.jpeg" id="A4.F22.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="685" height="536" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F22.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F22.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.11463/assets/figures_supp_examples_neg/0000001672.jpeg" id="A4.F22.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="685" height="508" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F22.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F22.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.11463/assets/figures_supp_examples_neg/0000001963.jpeg" id="A4.F22.sf3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="685" height="628" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F22.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F22.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.11463/assets/figures_supp_examples_neg/0000002052.jpeg" id="A4.F22.sf4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="685" height="536" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F22.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F22.3.1.1" class="ltx_text" style="font-size:90%;">Figure 22</span>: </span><span id="A4.F22.4.2" class="ltx_text" style="font-size:90%;">
Examples of difficult <span id="A4.F22.4.2.1" class="ltx_text ltx_font_bold">negative object descriptions</span>, which are listed below the respective images. Note that for positive descriptions, we only show the freeform-text descriptions and omit the plain categories to avoid cuttered visualizations in the image.
</span></figcaption>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2304.11462" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2304.11463" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2304.11463">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2304.11463" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2304.11464" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 12:49:29 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
