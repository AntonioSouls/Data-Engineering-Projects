<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2212.05598] Recurrent Vision Transformers for Object Detection with Event Cameras</title><meta property="og:description" content="We present Recurrent Vision Transformers (RVTs), a novel backbone for object detection with event cameras.
Event cameras provide visual information with sub-millisecond latency at a high-dynamic range and with strong r…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Recurrent Vision Transformers for Object Detection with Event Cameras">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Recurrent Vision Transformers for Object Detection with Event Cameras">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2212.05598">

<!--Generated on Fri Mar  1 11:33:30 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\useunder</span>
<p id="p1.2" class="ltx_p"><span id="p1.2.1" class="ltx_text ltx_ulem_uline"></span><span id="p1.2.2" class="ltx_ERROR undefined">\ul</span>












</p>
</div>
<h1 class="ltx_title ltx_title_document">Recurrent Vision Transformers for Object Detection with Event Cameras</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mathias Gehrig and Davide Scaramuzza
<br class="ltx_break">
Robotics and Perception Group, University of Zurich, Switzerland
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.2" class="ltx_p">We present Recurrent Vision Transformers (RVTs), a novel backbone for object detection with event cameras.
Event cameras provide visual information with sub-millisecond latency at a high-dynamic range and with strong robustness against motion blur.
These unique properties offer great potential for low-latency object detection and tracking in time-critical scenarios.
Prior work in event-based vision has achieved outstanding detection performance but at the cost of substantial inference time, typically beyond 40 milliseconds.
By revisiting the high-level design of recurrent vision backbones, we reduce inference time by a factor of 6 while retaining similar performance.
To achieve this, we explore a multi-stage design that utilizes three key concepts in each stage:
First, a convolutional prior that can be regarded as a conditional positional embedding.
Second, local and dilated global self-attention for spatial feature interaction.
Third, recurrent temporal feature aggregation to minimize latency while retaining temporal information.
RVTs can be trained from scratch to reach state-of-the-art performance on event-based object detection - achieving an mAP of 47.2% on the Gen1 automotive dataset. At the same time, RVTs offer fast inference (<math id="id1.1.m1.1" class="ltx_Math" alttext="&lt;12" display="inline"><semantics id="id1.1.m1.1a"><mrow id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mi id="id1.1.m1.1.1.2" xref="id1.1.m1.1.1.2.cmml"></mi><mo id="id1.1.m1.1.1.1" xref="id1.1.m1.1.1.1.cmml">&lt;</mo><mn id="id1.1.m1.1.1.3" xref="id1.1.m1.1.1.3.cmml">12</mn></mrow><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><lt id="id1.1.m1.1.1.1.cmml" xref="id1.1.m1.1.1.1"></lt><csymbol cd="latexml" id="id1.1.m1.1.1.2.cmml" xref="id1.1.m1.1.1.2">absent</csymbol><cn type="integer" id="id1.1.m1.1.1.3.cmml" xref="id1.1.m1.1.1.3">12</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">&lt;12</annotation></semantics></math> ms on a T4 GPU) and favorable parameter efficiency (<math id="id2.2.m2.1" class="ltx_math_unparsed" alttext="5\times" display="inline"><semantics id="id2.2.m2.1a"><mrow id="id2.2.m2.1b"><mn id="id2.2.m2.1.1">5</mn><mo lspace="0.222em" id="id2.2.m2.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="id2.2.m2.1c">5\times</annotation></semantics></math> fewer than prior art).
Our study brings new insights into effective design choices that can be fruitful for research beyond event-based vision.</p>
</div>
<div id="p2" class="ltx_para ltx_noindent">
<p id="p2.1" class="ltx_p"><span id="p2.1.1" class="ltx_text ltx_font_bold">Code</span>: <a target="_blank" href="https://github.com/uzh-rpg/RVT" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/uzh-rpg/RVT</a></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Time matters for object detection.
In 30 milliseconds, a human can run 0.3 meters, a car on public roads covers up to 1 meter, and a train can travel over 2 meters.
Yet, during this time, an ordinary camera captures only a single frame.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Frame-based sensors must strike a balance between latency and bandwidth.
Given a fixed bandwidth, a frame-based camera must trade-off camera resolution and frame rate.
However, in highly dynamic scenes, reducing the resolution or the frame rate may come at the cost of missing essential scene details, and, in safety-critical scenarios like automotive, this may even cause fatalities.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In recent years, event cameras have emerged as alternative sensor that offers a different trade-off.
Instead of counterbalancing bandwidth requirements and perceptual latency, they provide visual information at sub-millisecond latency but sacrifice absolute intensity information.
Instead of capturing intensity images, event cameras measure changes in intensity at the time they occur.
This results in a stream of events, which encode time, location, and polarity of brightness changes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.
The main advantages of event cameras are their sub-millisecond latency, very high dynamic range (<math id="S1.p3.1.m1.1" class="ltx_Math" alttext="&gt;120" display="inline"><semantics id="S1.p3.1.m1.1a"><mrow id="S1.p3.1.m1.1.1" xref="S1.p3.1.m1.1.1.cmml"><mi id="S1.p3.1.m1.1.1.2" xref="S1.p3.1.m1.1.1.2.cmml"></mi><mo id="S1.p3.1.m1.1.1.1" xref="S1.p3.1.m1.1.1.1.cmml">&gt;</mo><mn id="S1.p3.1.m1.1.1.3" xref="S1.p3.1.m1.1.1.3.cmml">120</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b"><apply id="S1.p3.1.m1.1.1.cmml" xref="S1.p3.1.m1.1.1"><gt id="S1.p3.1.m1.1.1.1.cmml" xref="S1.p3.1.m1.1.1.1"></gt><csymbol cd="latexml" id="S1.p3.1.m1.1.1.2.cmml" xref="S1.p3.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S1.p3.1.m1.1.1.3.cmml" xref="S1.p3.1.m1.1.1.3">120</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.1.m1.1c">&gt;120</annotation></semantics></math> dB), strong robustness to motion blur, and ability to provide events asynchronously in a continuous manner.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2212.05598/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="229" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.3.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Detection performance vs inference time<span id="S1.F1.4.2.1" class="ltx_text ltx_font_medium"> of our RVT models on the 1 Mpx detection dataset using a T4 GPU. The circle areas are proportional to the model size.</span></span></figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this work, we aim to utilize these outstanding properties of event cameras for object detection in time-critical scenarios.
Therefore, our objective is to design an approach that reduces the processing latency as much as possible while maintaining high performance.
This is challenging because event cameras asynchronously trigger binary events that are spread of pixel space and time.
Hence, we need to develop detection algorithms that can continuously associate features in the spatio-temporal domain while simultaneously satisfying strict latency requirements.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Recent work has shown that dynamic graph neural networks (GNNs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> and sparse neural networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> can theoretically achieve low latency inference for event-based object detection.
Yet, to achieve this in practical scenarios they either require specialized hardware or their detection performance needs to be improved.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">An alternative thread of research approaches the problem from the view of conventional, dense neural network designs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.
These methods show impressive performance on event-based object detection, especially when using temporal recurrence in their architectures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.
Still, the processing latency of these approaches remains beyond 40 milliseconds such that the low-latency aspect of event cameras cannot be fully leveraged.
This raises the question: How can we achieve both high accuracy and efficiency without requiring specialized hardware?</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">We notice that common design choices yield a suboptimal trade-off between performance and compute.
For example, prior work uses expensive convolutional LSTM (Conv-LSTM) cells <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> extensively in their feature extraction stage <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> or relies on heavy backbones such as the VGG architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.
Sparse neural networks instead struggle to model global mixing of features which is crucial to correctly locate and classify large objects in the scene.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">To achieve our main objective, we fundamentally revisit the design of vision backbones for event-based object detection.
In particular, we take inspiration from neural network design for conventional frame-based object detection and combine them with ideas that have proven successful in the event-based vision literature.
Our study deliberately focuses on <em id="S1.p8.1.1" class="ltx_emph ltx_font_italic">macro</em> design of the object detection backbone to identify key components for both high performance and fast inference on GPUs.
The resulting neural network is based on a single block that is repeated four times to form a multi-stage hierarchical backbone that can be used with off-the-shelf detection frameworks.</p>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p">We identify three key components that enable an excellent trade-off between detection performance and inference time.
First, we find that interleaved local- and global self-attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> is ideally suited to mix both local and global features while offering linear complexity in the input resolution.
Second, this attention mechanism is most effective when preceded by a simple convolution that also downsamples the spatial resolution from the previous stage.
This convolution effectively provides a strong prior about the grid-structure of the pixel array and also acts as a conditional positional embedding for the transformer layers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.
Third, temporal recurrence is paramount to achieve strong detection performance with events.
Differently from prior work, we find that Conv-LSTM cells can be replaced by plain LSTM cells <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> that operate on each feature separately<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>equivalent to <math id="footnote1.m1.1" class="ltx_Math" alttext="1\times 1" display="inline"><semantics id="footnote1.m1.1b"><mrow id="footnote1.m1.1.1" xref="footnote1.m1.1.1.cmml"><mn id="footnote1.m1.1.1.2" xref="footnote1.m1.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="footnote1.m1.1.1.1" xref="footnote1.m1.1.1.1.cmml">×</mo><mn id="footnote1.m1.1.1.3" xref="footnote1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="footnote1.m1.1c"><apply id="footnote1.m1.1.1.cmml" xref="footnote1.m1.1.1"><times id="footnote1.m1.1.1.1.cmml" xref="footnote1.m1.1.1.1"></times><cn type="integer" id="footnote1.m1.1.1.2.cmml" xref="footnote1.m1.1.1.2">1</cn><cn type="integer" id="footnote1.m1.1.1.3.cmml" xref="footnote1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote1.m1.1d">1\times 1</annotation></semantics></math> kernel in a Conv-LSTM cell</span></span></span>.
By doing so, we dramatically reduce the number of parameters and latency but also slightly improve the overall performance.
Our full framework achieves competitive performance and higher efficiency compared to state-of-the-art methods.
Specifically, we reduce parameter count (from 100M to 18.5 M) and inference time (from 72 ms to 12 ms)
up to a factor of 6 compared to prior art <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.
At the same time, we train our networks from scratch, showing that these benefits do not originate from large-scale pretraining.</p>
</div>
<div id="S1.p10" class="ltx_para">
<p id="S1.p10.1" class="ltx_p">Our paper can be summarized as follows:
(1) We re-examine predominant design choices in event-based object detection pipelines and reveal a set of key enablers for high performance in event-based object detection.
(2) We propose a simple, composable stage design that unifies the crucial building blocks in a compact way. We build a 4-stage hierarchical backbone that is fast, lightweight and still offers performance comparable to the best reported so far.
(3) We achieve state-of-the-art object detection performance of 47.2% mAP on the Gen1 detection dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and a highly competitive 47.4% mAP on the 1 Mpx detection dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> while training the proposed architecture from scratch.
In addition, we also provide insights into effective data augmentation techniques that contribute to these results.</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2212.05598/assets/x2.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="133" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.3.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S1.F2.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Overview<span id="S1.F2.4.2.1" class="ltx_text ltx_font_medium"> of the unrolled computation graph of our multi-stage recurrent backbone. Events are processed into a tensor representation before they are used as input to the first stage. Each stage also reuses the LSTM states (c: cell, h: hidden) from the previous timestep. Finally, the detection framework interfaces with the backbone from the second stage onwards. Specifically, the hidden states of the LSTMs are used as features for the detection framework.</span></span></figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Object Detection for Event Cameras</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Object detection in the event camera literature can be broadly classified into three emerging research directions.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Recent work explores <span id="S2.SS1.p2.1.1" class="ltx_text ltx_font_bold">graph neural networks</span> to dynamically construct a spatio-temporal graph <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>.
New nodes and node edges are established by sub-sampling events and finding existing nodes that are close in space-time.
The main challenge is to design the architecture such that information can propagate over vast distances in the space-time volume.
This is relevant, for example, when large objects move slowly with respect to the camera.
Furthermore, aggressive sub-sampling of events can lead to the removal of potentially crucial information, but is often required to maintain low-latency inference.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">A second line of work employs <span id="S2.SS1.p3.1.1" class="ltx_text ltx_font_bold">spiking neural networks</span> (SNNs) that propagate information sparsely within the network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.
SNNs are closely related to dense recurrent neural networks (RNNs) in that each spiking neuron has an internal state that is propagated in time.
Differently from RNNs, neurons in SNNs only emit spikes whenever a threshold is reached.
This spike generation mechanism is not differentiable, which leads to substantial difficulties in optimizing these networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>.
One workaround is to avoid the aforementioned threshold and instead propagate features throughout the receptive field <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> .
The downside of this mechanism is that the sparse-processing property is lost within deeper layers of the network.
Overall, the design and training of SNNs still requires fundamental investigation before competitive performance can be reached.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p">A third research direction is concerned with exploring <span id="S2.SS1.p4.1.1" class="ltx_text ltx_font_bold">dense neural networks</span> for object detection with event cameras.
The first step is the creation of a dense tensor (event representation) that enables compatibility with dense operations such as convolutions.
Early work directly uses a single event representation generated from a short temporal window of events to infer detections <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>.
These approaches discard relevant information from beyond the considered temporal window such that detecting slowly moving objects becomes difficult or impossible.
Followup work addresses this issue by incorporating recurrent neural network layers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> which drastically improved the detection performance.
We follow this line of work but revamp dominant architecture choices to build a canonical framework that is fast, lightweight and highly performant.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Vision Transformers for Spatio-Temporal Data</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The success of attention-based models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> in NLP has inspired the exploration of transformer-based architectures in computer vision <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.
Attention-based models have recently also been explored in video classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> where the models are applied directly to a set of frames.
While these approaches have shown promising results in spatio-temporal modelling, they are optimized for offline processing of stored video data.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">In event-based vision, attention-based components have found applications in classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> and image reconstruction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>, and monocular depth estimation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, but their use in object detection has yet to be investigated.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2212.05598/assets/x3.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="210" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.7.3.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S2.F3.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">RVT block structure<span id="S2.F3.4.2.2" class="ltx_text ltx_font_medium">. The input is convolved with kernel size <math id="S2.F3.3.1.1.m1.1" class="ltx_Math" alttext="k\times k" display="inline"><semantics id="S2.F3.3.1.1.m1.1b"><mrow id="S2.F3.3.1.1.m1.1.1" xref="S2.F3.3.1.1.m1.1.1.cmml"><mi id="S2.F3.3.1.1.m1.1.1.2" xref="S2.F3.3.1.1.m1.1.1.2.cmml">k</mi><mo lspace="0.222em" rspace="0.222em" id="S2.F3.3.1.1.m1.1.1.1" xref="S2.F3.3.1.1.m1.1.1.1.cmml">×</mo><mi id="S2.F3.3.1.1.m1.1.1.3" xref="S2.F3.3.1.1.m1.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.F3.3.1.1.m1.1c"><apply id="S2.F3.3.1.1.m1.1.1.cmml" xref="S2.F3.3.1.1.m1.1.1"><times id="S2.F3.3.1.1.m1.1.1.1.cmml" xref="S2.F3.3.1.1.m1.1.1.1"></times><ci id="S2.F3.3.1.1.m1.1.1.2.cmml" xref="S2.F3.3.1.1.m1.1.1.2">𝑘</ci><ci id="S2.F3.3.1.1.m1.1.1.3.cmml" xref="S2.F3.3.1.1.m1.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F3.3.1.1.m1.1d">k\times k</annotation></semantics></math> and stride <math id="S2.F3.4.2.2.m2.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S2.F3.4.2.2.m2.1b"><mi id="S2.F3.4.2.2.m2.1.1" xref="S2.F3.4.2.2.m2.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S2.F3.4.2.2.m2.1c"><ci id="S2.F3.4.2.2.m2.1.1.cmml" xref="S2.F3.4.2.2.m2.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F3.4.2.2.m2.1d">s</annotation></semantics></math>. Block-SA applies self-attention in local windows while Grid-SA is a global operation using dilated attention. Finally, each block ends with an LSTM that reuses the (cell- and hidden) states from the previous timestep. The LSTM is applied to each feature separately. Normalization and activation layers are omitted for conciseness.</span></span></figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Our object detection approach is designed to process a stream of events sequentially as they arrive.
Incoming events are first processed into tensors that represent events in space and time.
In every timestep, our network takes a new event representation as input as well as the previous states of the recurrent neural network layers.
After each pass through the backbone, the output of the RNNs are used as input to the detection framework.
The following sections elaborate on each one of these steps.
Fig. <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Recurrent Vision Transformers for Object Detection with Event Cameras" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows an overview of the RVT architecture.</p>
</div>
<section id="S3.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Event Processing</h5>

<div id="S3.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px1.p1.4" class="ltx_p">Each pixel of an event camera can independently trigger an event when a significant log brightness change occurs.
An event can be positive or negative depending on the sign of the brightness change.
We characterize an event with polarity <math id="S3.SS0.SSS0.Px1.p1.1.m1.2" class="ltx_Math" alttext="p_{k}\in\{0,1\}" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.1.m1.2a"><mrow id="S3.SS0.SSS0.Px1.p1.1.m1.2.3" xref="S3.SS0.SSS0.Px1.p1.1.m1.2.3.cmml"><msub id="S3.SS0.SSS0.Px1.p1.1.m1.2.3.2" xref="S3.SS0.SSS0.Px1.p1.1.m1.2.3.2.cmml"><mi id="S3.SS0.SSS0.Px1.p1.1.m1.2.3.2.2" xref="S3.SS0.SSS0.Px1.p1.1.m1.2.3.2.2.cmml">p</mi><mi id="S3.SS0.SSS0.Px1.p1.1.m1.2.3.2.3" xref="S3.SS0.SSS0.Px1.p1.1.m1.2.3.2.3.cmml">k</mi></msub><mo id="S3.SS0.SSS0.Px1.p1.1.m1.2.3.1" xref="S3.SS0.SSS0.Px1.p1.1.m1.2.3.1.cmml">∈</mo><mrow id="S3.SS0.SSS0.Px1.p1.1.m1.2.3.3.2" xref="S3.SS0.SSS0.Px1.p1.1.m1.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS0.SSS0.Px1.p1.1.m1.2.3.3.2.1" xref="S3.SS0.SSS0.Px1.p1.1.m1.2.3.3.1.cmml">{</mo><mn id="S3.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.cmml">0</mn><mo id="S3.SS0.SSS0.Px1.p1.1.m1.2.3.3.2.2" xref="S3.SS0.SSS0.Px1.p1.1.m1.2.3.3.1.cmml">,</mo><mn id="S3.SS0.SSS0.Px1.p1.1.m1.2.2" xref="S3.SS0.SSS0.Px1.p1.1.m1.2.2.cmml">1</mn><mo stretchy="false" id="S3.SS0.SSS0.Px1.p1.1.m1.2.3.3.2.3" xref="S3.SS0.SSS0.Px1.p1.1.m1.2.3.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.1.m1.2b"><apply id="S3.SS0.SSS0.Px1.p1.1.m1.2.3.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.2.3"><in id="S3.SS0.SSS0.Px1.p1.1.m1.2.3.1.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.2.3.1"></in><apply id="S3.SS0.SSS0.Px1.p1.1.m1.2.3.2.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.2.3.2"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px1.p1.1.m1.2.3.2.1.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.2.3.2">subscript</csymbol><ci id="S3.SS0.SSS0.Px1.p1.1.m1.2.3.2.2.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.2.3.2.2">𝑝</ci><ci id="S3.SS0.SSS0.Px1.p1.1.m1.2.3.2.3.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.2.3.2.3">𝑘</ci></apply><set id="S3.SS0.SSS0.Px1.p1.1.m1.2.3.3.1.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.2.3.3.2"><cn type="integer" id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1">0</cn><cn type="integer" id="S3.SS0.SSS0.Px1.p1.1.m1.2.2.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.2.2">1</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.1.m1.2c">p_{k}\in\{0,1\}</annotation></semantics></math> as a tuple <math id="S3.SS0.SSS0.Px1.p1.2.m2.4" class="ltx_Math" alttext="e_{k}=(x_{k},y_{k},t_{k},p_{k})" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.2.m2.4a"><mrow id="S3.SS0.SSS0.Px1.p1.2.m2.4.4" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.cmml"><msub id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.6" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.6.cmml"><mi id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.6.2" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.6.2.cmml">e</mi><mi id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.6.3" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.6.3.cmml">k</mi></msub><mo id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.5" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.5.cmml">=</mo><mrow id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.5.cmml"><mo stretchy="false" id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.5" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.5.cmml">(</mo><msub id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.1" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.1.cmml"><mi id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.1.2" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.1.2.cmml">x</mi><mi id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.1.3" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.1.3.cmml">k</mi></msub><mo id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.6" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.5.cmml">,</mo><msub id="S3.SS0.SSS0.Px1.p1.2.m2.2.2.2.2.2" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.2.2.2.2.cmml"><mi id="S3.SS0.SSS0.Px1.p1.2.m2.2.2.2.2.2.2" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.2.2.2.2.2.cmml">y</mi><mi id="S3.SS0.SSS0.Px1.p1.2.m2.2.2.2.2.2.3" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.2.2.2.2.3.cmml">k</mi></msub><mo id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.7" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.5.cmml">,</mo><msub id="S3.SS0.SSS0.Px1.p1.2.m2.3.3.3.3.3" xref="S3.SS0.SSS0.Px1.p1.2.m2.3.3.3.3.3.cmml"><mi id="S3.SS0.SSS0.Px1.p1.2.m2.3.3.3.3.3.2" xref="S3.SS0.SSS0.Px1.p1.2.m2.3.3.3.3.3.2.cmml">t</mi><mi id="S3.SS0.SSS0.Px1.p1.2.m2.3.3.3.3.3.3" xref="S3.SS0.SSS0.Px1.p1.2.m2.3.3.3.3.3.3.cmml">k</mi></msub><mo id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.8" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.5.cmml">,</mo><msub id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.4" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.4.cmml"><mi id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.4.2" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.4.2.cmml">p</mi><mi id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.4.3" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.4.3.cmml">k</mi></msub><mo stretchy="false" id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.9" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.5.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.2.m2.4b"><apply id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4"><eq id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.5.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.5"></eq><apply id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.6.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.6"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.6.1.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.6">subscript</csymbol><ci id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.6.2.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.6.2">𝑒</ci><ci id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.6.3.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.6.3">𝑘</ci></apply><vector id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.5.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4"><apply id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.1">subscript</csymbol><ci id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.1.2">𝑥</ci><ci id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.1.3.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.1.3">𝑘</ci></apply><apply id="S3.SS0.SSS0.Px1.p1.2.m2.2.2.2.2.2.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px1.p1.2.m2.2.2.2.2.2.1.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS0.SSS0.Px1.p1.2.m2.2.2.2.2.2.2.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.2.2.2.2.2">𝑦</ci><ci id="S3.SS0.SSS0.Px1.p1.2.m2.2.2.2.2.2.3.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.2.2.2.2.3">𝑘</ci></apply><apply id="S3.SS0.SSS0.Px1.p1.2.m2.3.3.3.3.3.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px1.p1.2.m2.3.3.3.3.3.1.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.3.3.3.3.3">subscript</csymbol><ci id="S3.SS0.SSS0.Px1.p1.2.m2.3.3.3.3.3.2.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.3.3.3.3.3.2">𝑡</ci><ci id="S3.SS0.SSS0.Px1.p1.2.m2.3.3.3.3.3.3.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.3.3.3.3.3.3">𝑘</ci></apply><apply id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.4.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.4"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.4.1.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.4">subscript</csymbol><ci id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.4.2.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.4.2">𝑝</ci><ci id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.4.3.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.4.3">𝑘</ci></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.2.m2.4c">e_{k}=(x_{k},y_{k},t_{k},p_{k})</annotation></semantics></math> that occurs at pixel <math id="S3.SS0.SSS0.Px1.p1.3.m3.2" class="ltx_Math" alttext="(x_{k},y_{k})" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.3.m3.2a"><mrow id="S3.SS0.SSS0.Px1.p1.3.m3.2.2.2" xref="S3.SS0.SSS0.Px1.p1.3.m3.2.2.3.cmml"><mo stretchy="false" id="S3.SS0.SSS0.Px1.p1.3.m3.2.2.2.3" xref="S3.SS0.SSS0.Px1.p1.3.m3.2.2.3.cmml">(</mo><msub id="S3.SS0.SSS0.Px1.p1.3.m3.1.1.1.1" xref="S3.SS0.SSS0.Px1.p1.3.m3.1.1.1.1.cmml"><mi id="S3.SS0.SSS0.Px1.p1.3.m3.1.1.1.1.2" xref="S3.SS0.SSS0.Px1.p1.3.m3.1.1.1.1.2.cmml">x</mi><mi id="S3.SS0.SSS0.Px1.p1.3.m3.1.1.1.1.3" xref="S3.SS0.SSS0.Px1.p1.3.m3.1.1.1.1.3.cmml">k</mi></msub><mo id="S3.SS0.SSS0.Px1.p1.3.m3.2.2.2.4" xref="S3.SS0.SSS0.Px1.p1.3.m3.2.2.3.cmml">,</mo><msub id="S3.SS0.SSS0.Px1.p1.3.m3.2.2.2.2" xref="S3.SS0.SSS0.Px1.p1.3.m3.2.2.2.2.cmml"><mi id="S3.SS0.SSS0.Px1.p1.3.m3.2.2.2.2.2" xref="S3.SS0.SSS0.Px1.p1.3.m3.2.2.2.2.2.cmml">y</mi><mi id="S3.SS0.SSS0.Px1.p1.3.m3.2.2.2.2.3" xref="S3.SS0.SSS0.Px1.p1.3.m3.2.2.2.2.3.cmml">k</mi></msub><mo stretchy="false" id="S3.SS0.SSS0.Px1.p1.3.m3.2.2.2.5" xref="S3.SS0.SSS0.Px1.p1.3.m3.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.3.m3.2b"><interval closure="open" id="S3.SS0.SSS0.Px1.p1.3.m3.2.2.3.cmml" xref="S3.SS0.SSS0.Px1.p1.3.m3.2.2.2"><apply id="S3.SS0.SSS0.Px1.p1.3.m3.1.1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px1.p1.3.m3.1.1.1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.3.m3.1.1.1.1">subscript</csymbol><ci id="S3.SS0.SSS0.Px1.p1.3.m3.1.1.1.1.2.cmml" xref="S3.SS0.SSS0.Px1.p1.3.m3.1.1.1.1.2">𝑥</ci><ci id="S3.SS0.SSS0.Px1.p1.3.m3.1.1.1.1.3.cmml" xref="S3.SS0.SSS0.Px1.p1.3.m3.1.1.1.1.3">𝑘</ci></apply><apply id="S3.SS0.SSS0.Px1.p1.3.m3.2.2.2.2.cmml" xref="S3.SS0.SSS0.Px1.p1.3.m3.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px1.p1.3.m3.2.2.2.2.1.cmml" xref="S3.SS0.SSS0.Px1.p1.3.m3.2.2.2.2">subscript</csymbol><ci id="S3.SS0.SSS0.Px1.p1.3.m3.2.2.2.2.2.cmml" xref="S3.SS0.SSS0.Px1.p1.3.m3.2.2.2.2.2">𝑦</ci><ci id="S3.SS0.SSS0.Px1.p1.3.m3.2.2.2.2.3.cmml" xref="S3.SS0.SSS0.Px1.p1.3.m3.2.2.2.2.3">𝑘</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.3.m3.2c">(x_{k},y_{k})</annotation></semantics></math> at time <math id="S3.SS0.SSS0.Px1.p1.4.m4.1" class="ltx_Math" alttext="t_{k}" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.4.m4.1a"><msub id="S3.SS0.SSS0.Px1.p1.4.m4.1.1" xref="S3.SS0.SSS0.Px1.p1.4.m4.1.1.cmml"><mi id="S3.SS0.SSS0.Px1.p1.4.m4.1.1.2" xref="S3.SS0.SSS0.Px1.p1.4.m4.1.1.2.cmml">t</mi><mi id="S3.SS0.SSS0.Px1.p1.4.m4.1.1.3" xref="S3.SS0.SSS0.Px1.p1.4.m4.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.4.m4.1b"><apply id="S3.SS0.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px1.p1.4.m4.1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS0.SSS0.Px1.p1.4.m4.1.1.2.cmml" xref="S3.SS0.SSS0.Px1.p1.4.m4.1.1.2">𝑡</ci><ci id="S3.SS0.SSS0.Px1.p1.4.m4.1.1.3.cmml" xref="S3.SS0.SSS0.Px1.p1.4.m4.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.4.m4.1c">t_{k}</annotation></semantics></math>.
Modern event cameras can produce 10s of millions of events per second which renders event-by-event processing out of reach on conventional processing units.</p>
</div>
<div id="S3.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S3.SS0.SSS0.Px1.p2.1" class="ltx_p">In this work we opt for a very simple preprocessing step to enable compatibility with convolutional neural network layers which, as we will show later in Sec. <a href="#S4.SS2" title="4.2 Ablation Studies ‣ 4 Experiments ‣ Recurrent Vision Transformers for Object Detection with Event Cameras" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>, are an important contributor to the performance of our model.</p>
</div>
<div id="S3.SS0.SSS0.Px1.p3" class="ltx_para">
<p id="S3.SS0.SSS0.Px1.p3.5" class="ltx_p">Our preprocessing step starts with the creation of a 4-dimensional tensor <math id="S3.SS0.SSS0.Px1.p3.1.m1.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S3.SS0.SSS0.Px1.p3.1.m1.1a"><mi id="S3.SS0.SSS0.Px1.p3.1.m1.1.1" xref="S3.SS0.SSS0.Px1.p3.1.m1.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p3.1.m1.1b"><ci id="S3.SS0.SSS0.Px1.p3.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p3.1.m1.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p3.1.m1.1c">E</annotation></semantics></math>.
The first dimension consists of two components and represents the polarity. The second dimension has <math id="S3.SS0.SSS0.Px1.p3.2.m2.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS0.SSS0.Px1.p3.2.m2.1a"><mi id="S3.SS0.SSS0.Px1.p3.2.m2.1.1" xref="S3.SS0.SSS0.Px1.p3.2.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p3.2.m2.1b"><ci id="S3.SS0.SSS0.Px1.p3.2.m2.1.1.cmml" xref="S3.SS0.SSS0.Px1.p3.2.m2.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p3.2.m2.1c">T</annotation></semantics></math> components and is associated with <math id="S3.SS0.SSS0.Px1.p3.3.m3.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS0.SSS0.Px1.p3.3.m3.1a"><mi id="S3.SS0.SSS0.Px1.p3.3.m3.1.1" xref="S3.SS0.SSS0.Px1.p3.3.m3.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p3.3.m3.1b"><ci id="S3.SS0.SSS0.Px1.p3.3.m3.1.1.cmml" xref="S3.SS0.SSS0.Px1.p3.3.m3.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p3.3.m3.1c">T</annotation></semantics></math> discretization steps of time. The 3rd and 4th dimension represent height and width of the event camera.
We process set of events <math id="S3.SS0.SSS0.Px1.p3.4.m4.1" class="ltx_Math" alttext="\mathcal{E}" display="inline"><semantics id="S3.SS0.SSS0.Px1.p3.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS0.SSS0.Px1.p3.4.m4.1.1" xref="S3.SS0.SSS0.Px1.p3.4.m4.1.1.cmml">ℰ</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p3.4.m4.1b"><ci id="S3.SS0.SSS0.Px1.p3.4.m4.1.1.cmml" xref="S3.SS0.SSS0.Px1.p3.4.m4.1.1">ℰ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p3.4.m4.1c">\mathcal{E}</annotation></semantics></math> within a time duration <math id="S3.SS0.SSS0.Px1.p3.5.m5.2" class="ltx_Math" alttext="[t_{a},t_{b})" display="inline"><semantics id="S3.SS0.SSS0.Px1.p3.5.m5.2a"><mrow id="S3.SS0.SSS0.Px1.p3.5.m5.2.2.2" xref="S3.SS0.SSS0.Px1.p3.5.m5.2.2.3.cmml"><mo stretchy="false" id="S3.SS0.SSS0.Px1.p3.5.m5.2.2.2.3" xref="S3.SS0.SSS0.Px1.p3.5.m5.2.2.3.cmml">[</mo><msub id="S3.SS0.SSS0.Px1.p3.5.m5.1.1.1.1" xref="S3.SS0.SSS0.Px1.p3.5.m5.1.1.1.1.cmml"><mi id="S3.SS0.SSS0.Px1.p3.5.m5.1.1.1.1.2" xref="S3.SS0.SSS0.Px1.p3.5.m5.1.1.1.1.2.cmml">t</mi><mi id="S3.SS0.SSS0.Px1.p3.5.m5.1.1.1.1.3" xref="S3.SS0.SSS0.Px1.p3.5.m5.1.1.1.1.3.cmml">a</mi></msub><mo id="S3.SS0.SSS0.Px1.p3.5.m5.2.2.2.4" xref="S3.SS0.SSS0.Px1.p3.5.m5.2.2.3.cmml">,</mo><msub id="S3.SS0.SSS0.Px1.p3.5.m5.2.2.2.2" xref="S3.SS0.SSS0.Px1.p3.5.m5.2.2.2.2.cmml"><mi id="S3.SS0.SSS0.Px1.p3.5.m5.2.2.2.2.2" xref="S3.SS0.SSS0.Px1.p3.5.m5.2.2.2.2.2.cmml">t</mi><mi id="S3.SS0.SSS0.Px1.p3.5.m5.2.2.2.2.3" xref="S3.SS0.SSS0.Px1.p3.5.m5.2.2.2.2.3.cmml">b</mi></msub><mo stretchy="false" id="S3.SS0.SSS0.Px1.p3.5.m5.2.2.2.5" xref="S3.SS0.SSS0.Px1.p3.5.m5.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p3.5.m5.2b"><interval closure="closed-open" id="S3.SS0.SSS0.Px1.p3.5.m5.2.2.3.cmml" xref="S3.SS0.SSS0.Px1.p3.5.m5.2.2.2"><apply id="S3.SS0.SSS0.Px1.p3.5.m5.1.1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p3.5.m5.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px1.p3.5.m5.1.1.1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p3.5.m5.1.1.1.1">subscript</csymbol><ci id="S3.SS0.SSS0.Px1.p3.5.m5.1.1.1.1.2.cmml" xref="S3.SS0.SSS0.Px1.p3.5.m5.1.1.1.1.2">𝑡</ci><ci id="S3.SS0.SSS0.Px1.p3.5.m5.1.1.1.1.3.cmml" xref="S3.SS0.SSS0.Px1.p3.5.m5.1.1.1.1.3">𝑎</ci></apply><apply id="S3.SS0.SSS0.Px1.p3.5.m5.2.2.2.2.cmml" xref="S3.SS0.SSS0.Px1.p3.5.m5.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px1.p3.5.m5.2.2.2.2.1.cmml" xref="S3.SS0.SSS0.Px1.p3.5.m5.2.2.2.2">subscript</csymbol><ci id="S3.SS0.SSS0.Px1.p3.5.m5.2.2.2.2.2.cmml" xref="S3.SS0.SSS0.Px1.p3.5.m5.2.2.2.2.2">𝑡</ci><ci id="S3.SS0.SSS0.Px1.p3.5.m5.2.2.2.2.3.cmml" xref="S3.SS0.SSS0.Px1.p3.5.m5.2.2.2.2.3">𝑏</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p3.5.m5.2c">[t_{a},t_{b})</annotation></semantics></math> the following way:</p>
<table id="S7.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.Ex1.m1.4" class="ltx_Math" alttext="\displaystyle E(p,\tau,x,y)" display="inline"><semantics id="S3.Ex1.m1.4a"><mrow id="S3.Ex1.m1.4.5" xref="S3.Ex1.m1.4.5.cmml"><mi id="S3.Ex1.m1.4.5.2" xref="S3.Ex1.m1.4.5.2.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.4.5.1" xref="S3.Ex1.m1.4.5.1.cmml">​</mo><mrow id="S3.Ex1.m1.4.5.3.2" xref="S3.Ex1.m1.4.5.3.1.cmml"><mo stretchy="false" id="S3.Ex1.m1.4.5.3.2.1" xref="S3.Ex1.m1.4.5.3.1.cmml">(</mo><mi id="S3.Ex1.m1.1.1" xref="S3.Ex1.m1.1.1.cmml">p</mi><mo id="S3.Ex1.m1.4.5.3.2.2" xref="S3.Ex1.m1.4.5.3.1.cmml">,</mo><mi id="S3.Ex1.m1.2.2" xref="S3.Ex1.m1.2.2.cmml">τ</mi><mo id="S3.Ex1.m1.4.5.3.2.3" xref="S3.Ex1.m1.4.5.3.1.cmml">,</mo><mi id="S3.Ex1.m1.3.3" xref="S3.Ex1.m1.3.3.cmml">x</mi><mo id="S3.Ex1.m1.4.5.3.2.4" xref="S3.Ex1.m1.4.5.3.1.cmml">,</mo><mi id="S3.Ex1.m1.4.4" xref="S3.Ex1.m1.4.4.cmml">y</mi><mo stretchy="false" id="S3.Ex1.m1.4.5.3.2.5" xref="S3.Ex1.m1.4.5.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.4b"><apply id="S3.Ex1.m1.4.5.cmml" xref="S3.Ex1.m1.4.5"><times id="S3.Ex1.m1.4.5.1.cmml" xref="S3.Ex1.m1.4.5.1"></times><ci id="S3.Ex1.m1.4.5.2.cmml" xref="S3.Ex1.m1.4.5.2">𝐸</ci><vector id="S3.Ex1.m1.4.5.3.1.cmml" xref="S3.Ex1.m1.4.5.3.2"><ci id="S3.Ex1.m1.1.1.cmml" xref="S3.Ex1.m1.1.1">𝑝</ci><ci id="S3.Ex1.m1.2.2.cmml" xref="S3.Ex1.m1.2.2">𝜏</ci><ci id="S3.Ex1.m1.3.3.cmml" xref="S3.Ex1.m1.3.3">𝑥</ci><ci id="S3.Ex1.m1.4.4.cmml" xref="S3.Ex1.m1.4.4">𝑦</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.4c">\displaystyle E(p,\tau,x,y)</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.Ex1.m2.1" class="ltx_Math" alttext="\displaystyle=\sum_{e_{k}\in\mathcal{E}}\delta(p-p_{k})\delta(x-x_{k},y-y_{k})\delta(\tau-\tau_{k})," display="inline"><semantics id="S3.Ex1.m2.1a"><mrow id="S3.Ex1.m2.1.1.1" xref="S3.Ex1.m2.1.1.1.1.cmml"><mrow id="S3.Ex1.m2.1.1.1.1" xref="S3.Ex1.m2.1.1.1.1.cmml"><mi id="S3.Ex1.m2.1.1.1.1.6" xref="S3.Ex1.m2.1.1.1.1.6.cmml"></mi><mo id="S3.Ex1.m2.1.1.1.1.5" xref="S3.Ex1.m2.1.1.1.1.5.cmml">=</mo><mrow id="S3.Ex1.m2.1.1.1.1.4" xref="S3.Ex1.m2.1.1.1.1.4.cmml"><mstyle displaystyle="true" id="S3.Ex1.m2.1.1.1.1.4.5" xref="S3.Ex1.m2.1.1.1.1.4.5.cmml"><munder id="S3.Ex1.m2.1.1.1.1.4.5a" xref="S3.Ex1.m2.1.1.1.1.4.5.cmml"><mo movablelimits="false" id="S3.Ex1.m2.1.1.1.1.4.5.2" xref="S3.Ex1.m2.1.1.1.1.4.5.2.cmml">∑</mo><mrow id="S3.Ex1.m2.1.1.1.1.4.5.3" xref="S3.Ex1.m2.1.1.1.1.4.5.3.cmml"><msub id="S3.Ex1.m2.1.1.1.1.4.5.3.2" xref="S3.Ex1.m2.1.1.1.1.4.5.3.2.cmml"><mi id="S3.Ex1.m2.1.1.1.1.4.5.3.2.2" xref="S3.Ex1.m2.1.1.1.1.4.5.3.2.2.cmml">e</mi><mi id="S3.Ex1.m2.1.1.1.1.4.5.3.2.3" xref="S3.Ex1.m2.1.1.1.1.4.5.3.2.3.cmml">k</mi></msub><mo id="S3.Ex1.m2.1.1.1.1.4.5.3.1" xref="S3.Ex1.m2.1.1.1.1.4.5.3.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S3.Ex1.m2.1.1.1.1.4.5.3.3" xref="S3.Ex1.m2.1.1.1.1.4.5.3.3.cmml">ℰ</mi></mrow></munder></mstyle><mrow id="S3.Ex1.m2.1.1.1.1.4.4" xref="S3.Ex1.m2.1.1.1.1.4.4.cmml"><mi id="S3.Ex1.m2.1.1.1.1.4.4.6" xref="S3.Ex1.m2.1.1.1.1.4.4.6.cmml">δ</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m2.1.1.1.1.4.4.5" xref="S3.Ex1.m2.1.1.1.1.4.4.5.cmml">​</mo><mrow id="S3.Ex1.m2.1.1.1.1.1.1.1.1" xref="S3.Ex1.m2.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.Ex1.m2.1.1.1.1.1.1.1.1.2" xref="S3.Ex1.m2.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.Ex1.m2.1.1.1.1.1.1.1.1.1" xref="S3.Ex1.m2.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.Ex1.m2.1.1.1.1.1.1.1.1.1.2" xref="S3.Ex1.m2.1.1.1.1.1.1.1.1.1.2.cmml">p</mi><mo id="S3.Ex1.m2.1.1.1.1.1.1.1.1.1.1" xref="S3.Ex1.m2.1.1.1.1.1.1.1.1.1.1.cmml">−</mo><msub id="S3.Ex1.m2.1.1.1.1.1.1.1.1.1.3" xref="S3.Ex1.m2.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.Ex1.m2.1.1.1.1.1.1.1.1.1.3.2" xref="S3.Ex1.m2.1.1.1.1.1.1.1.1.1.3.2.cmml">p</mi><mi id="S3.Ex1.m2.1.1.1.1.1.1.1.1.1.3.3" xref="S3.Ex1.m2.1.1.1.1.1.1.1.1.1.3.3.cmml">k</mi></msub></mrow><mo stretchy="false" id="S3.Ex1.m2.1.1.1.1.1.1.1.1.3" xref="S3.Ex1.m2.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.Ex1.m2.1.1.1.1.4.4.5a" xref="S3.Ex1.m2.1.1.1.1.4.4.5.cmml">​</mo><mi id="S3.Ex1.m2.1.1.1.1.4.4.7" xref="S3.Ex1.m2.1.1.1.1.4.4.7.cmml">δ</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m2.1.1.1.1.4.4.5b" xref="S3.Ex1.m2.1.1.1.1.4.4.5.cmml">​</mo><mrow id="S3.Ex1.m2.1.1.1.1.3.3.3.2" xref="S3.Ex1.m2.1.1.1.1.3.3.3.3.cmml"><mo stretchy="false" id="S3.Ex1.m2.1.1.1.1.3.3.3.2.3" xref="S3.Ex1.m2.1.1.1.1.3.3.3.3.cmml">(</mo><mrow id="S3.Ex1.m2.1.1.1.1.2.2.2.1.1" xref="S3.Ex1.m2.1.1.1.1.2.2.2.1.1.cmml"><mi id="S3.Ex1.m2.1.1.1.1.2.2.2.1.1.2" xref="S3.Ex1.m2.1.1.1.1.2.2.2.1.1.2.cmml">x</mi><mo id="S3.Ex1.m2.1.1.1.1.2.2.2.1.1.1" xref="S3.Ex1.m2.1.1.1.1.2.2.2.1.1.1.cmml">−</mo><msub id="S3.Ex1.m2.1.1.1.1.2.2.2.1.1.3" xref="S3.Ex1.m2.1.1.1.1.2.2.2.1.1.3.cmml"><mi id="S3.Ex1.m2.1.1.1.1.2.2.2.1.1.3.2" xref="S3.Ex1.m2.1.1.1.1.2.2.2.1.1.3.2.cmml">x</mi><mi id="S3.Ex1.m2.1.1.1.1.2.2.2.1.1.3.3" xref="S3.Ex1.m2.1.1.1.1.2.2.2.1.1.3.3.cmml">k</mi></msub></mrow><mo id="S3.Ex1.m2.1.1.1.1.3.3.3.2.4" xref="S3.Ex1.m2.1.1.1.1.3.3.3.3.cmml">,</mo><mrow id="S3.Ex1.m2.1.1.1.1.3.3.3.2.2" xref="S3.Ex1.m2.1.1.1.1.3.3.3.2.2.cmml"><mi id="S3.Ex1.m2.1.1.1.1.3.3.3.2.2.2" xref="S3.Ex1.m2.1.1.1.1.3.3.3.2.2.2.cmml">y</mi><mo id="S3.Ex1.m2.1.1.1.1.3.3.3.2.2.1" xref="S3.Ex1.m2.1.1.1.1.3.3.3.2.2.1.cmml">−</mo><msub id="S3.Ex1.m2.1.1.1.1.3.3.3.2.2.3" xref="S3.Ex1.m2.1.1.1.1.3.3.3.2.2.3.cmml"><mi id="S3.Ex1.m2.1.1.1.1.3.3.3.2.2.3.2" xref="S3.Ex1.m2.1.1.1.1.3.3.3.2.2.3.2.cmml">y</mi><mi id="S3.Ex1.m2.1.1.1.1.3.3.3.2.2.3.3" xref="S3.Ex1.m2.1.1.1.1.3.3.3.2.2.3.3.cmml">k</mi></msub></mrow><mo stretchy="false" id="S3.Ex1.m2.1.1.1.1.3.3.3.2.5" xref="S3.Ex1.m2.1.1.1.1.3.3.3.3.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.Ex1.m2.1.1.1.1.4.4.5c" xref="S3.Ex1.m2.1.1.1.1.4.4.5.cmml">​</mo><mi id="S3.Ex1.m2.1.1.1.1.4.4.8" xref="S3.Ex1.m2.1.1.1.1.4.4.8.cmml">δ</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m2.1.1.1.1.4.4.5d" xref="S3.Ex1.m2.1.1.1.1.4.4.5.cmml">​</mo><mrow id="S3.Ex1.m2.1.1.1.1.4.4.4.1" xref="S3.Ex1.m2.1.1.1.1.4.4.4.1.1.cmml"><mo stretchy="false" id="S3.Ex1.m2.1.1.1.1.4.4.4.1.2" xref="S3.Ex1.m2.1.1.1.1.4.4.4.1.1.cmml">(</mo><mrow id="S3.Ex1.m2.1.1.1.1.4.4.4.1.1" xref="S3.Ex1.m2.1.1.1.1.4.4.4.1.1.cmml"><mi id="S3.Ex1.m2.1.1.1.1.4.4.4.1.1.2" xref="S3.Ex1.m2.1.1.1.1.4.4.4.1.1.2.cmml">τ</mi><mo id="S3.Ex1.m2.1.1.1.1.4.4.4.1.1.1" xref="S3.Ex1.m2.1.1.1.1.4.4.4.1.1.1.cmml">−</mo><msub id="S3.Ex1.m2.1.1.1.1.4.4.4.1.1.3" xref="S3.Ex1.m2.1.1.1.1.4.4.4.1.1.3.cmml"><mi id="S3.Ex1.m2.1.1.1.1.4.4.4.1.1.3.2" xref="S3.Ex1.m2.1.1.1.1.4.4.4.1.1.3.2.cmml">τ</mi><mi id="S3.Ex1.m2.1.1.1.1.4.4.4.1.1.3.3" xref="S3.Ex1.m2.1.1.1.1.4.4.4.1.1.3.3.cmml">k</mi></msub></mrow><mo stretchy="false" id="S3.Ex1.m2.1.1.1.1.4.4.4.1.3" xref="S3.Ex1.m2.1.1.1.1.4.4.4.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.Ex1.m2.1.1.1.2" xref="S3.Ex1.m2.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m2.1b"><apply id="S3.Ex1.m2.1.1.1.1.cmml" xref="S3.Ex1.m2.1.1.1"><eq id="S3.Ex1.m2.1.1.1.1.5.cmml" xref="S3.Ex1.m2.1.1.1.1.5"></eq><csymbol cd="latexml" id="S3.Ex1.m2.1.1.1.1.6.cmml" xref="S3.Ex1.m2.1.1.1.1.6">absent</csymbol><apply id="S3.Ex1.m2.1.1.1.1.4.cmml" xref="S3.Ex1.m2.1.1.1.1.4"><apply id="S3.Ex1.m2.1.1.1.1.4.5.cmml" xref="S3.Ex1.m2.1.1.1.1.4.5"><csymbol cd="ambiguous" id="S3.Ex1.m2.1.1.1.1.4.5.1.cmml" xref="S3.Ex1.m2.1.1.1.1.4.5">subscript</csymbol><sum id="S3.Ex1.m2.1.1.1.1.4.5.2.cmml" xref="S3.Ex1.m2.1.1.1.1.4.5.2"></sum><apply id="S3.Ex1.m2.1.1.1.1.4.5.3.cmml" xref="S3.Ex1.m2.1.1.1.1.4.5.3"><in id="S3.Ex1.m2.1.1.1.1.4.5.3.1.cmml" xref="S3.Ex1.m2.1.1.1.1.4.5.3.1"></in><apply id="S3.Ex1.m2.1.1.1.1.4.5.3.2.cmml" xref="S3.Ex1.m2.1.1.1.1.4.5.3.2"><csymbol cd="ambiguous" id="S3.Ex1.m2.1.1.1.1.4.5.3.2.1.cmml" xref="S3.Ex1.m2.1.1.1.1.4.5.3.2">subscript</csymbol><ci id="S3.Ex1.m2.1.1.1.1.4.5.3.2.2.cmml" xref="S3.Ex1.m2.1.1.1.1.4.5.3.2.2">𝑒</ci><ci id="S3.Ex1.m2.1.1.1.1.4.5.3.2.3.cmml" xref="S3.Ex1.m2.1.1.1.1.4.5.3.2.3">𝑘</ci></apply><ci id="S3.Ex1.m2.1.1.1.1.4.5.3.3.cmml" xref="S3.Ex1.m2.1.1.1.1.4.5.3.3">ℰ</ci></apply></apply><apply id="S3.Ex1.m2.1.1.1.1.4.4.cmml" xref="S3.Ex1.m2.1.1.1.1.4.4"><times id="S3.Ex1.m2.1.1.1.1.4.4.5.cmml" xref="S3.Ex1.m2.1.1.1.1.4.4.5"></times><ci id="S3.Ex1.m2.1.1.1.1.4.4.6.cmml" xref="S3.Ex1.m2.1.1.1.1.4.4.6">𝛿</ci><apply id="S3.Ex1.m2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m2.1.1.1.1.1.1.1.1"><minus id="S3.Ex1.m2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m2.1.1.1.1.1.1.1.1.1.1"></minus><ci id="S3.Ex1.m2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m2.1.1.1.1.1.1.1.1.1.2">𝑝</ci><apply id="S3.Ex1.m2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m2.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex1.m2.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.Ex1.m2.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.Ex1.m2.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.Ex1.m2.1.1.1.1.1.1.1.1.1.3.2">𝑝</ci><ci id="S3.Ex1.m2.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.Ex1.m2.1.1.1.1.1.1.1.1.1.3.3">𝑘</ci></apply></apply><ci id="S3.Ex1.m2.1.1.1.1.4.4.7.cmml" xref="S3.Ex1.m2.1.1.1.1.4.4.7">𝛿</ci><interval closure="open" id="S3.Ex1.m2.1.1.1.1.3.3.3.3.cmml" xref="S3.Ex1.m2.1.1.1.1.3.3.3.2"><apply id="S3.Ex1.m2.1.1.1.1.2.2.2.1.1.cmml" xref="S3.Ex1.m2.1.1.1.1.2.2.2.1.1"><minus id="S3.Ex1.m2.1.1.1.1.2.2.2.1.1.1.cmml" xref="S3.Ex1.m2.1.1.1.1.2.2.2.1.1.1"></minus><ci id="S3.Ex1.m2.1.1.1.1.2.2.2.1.1.2.cmml" xref="S3.Ex1.m2.1.1.1.1.2.2.2.1.1.2">𝑥</ci><apply id="S3.Ex1.m2.1.1.1.1.2.2.2.1.1.3.cmml" xref="S3.Ex1.m2.1.1.1.1.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.Ex1.m2.1.1.1.1.2.2.2.1.1.3.1.cmml" xref="S3.Ex1.m2.1.1.1.1.2.2.2.1.1.3">subscript</csymbol><ci id="S3.Ex1.m2.1.1.1.1.2.2.2.1.1.3.2.cmml" xref="S3.Ex1.m2.1.1.1.1.2.2.2.1.1.3.2">𝑥</ci><ci id="S3.Ex1.m2.1.1.1.1.2.2.2.1.1.3.3.cmml" xref="S3.Ex1.m2.1.1.1.1.2.2.2.1.1.3.3">𝑘</ci></apply></apply><apply id="S3.Ex1.m2.1.1.1.1.3.3.3.2.2.cmml" xref="S3.Ex1.m2.1.1.1.1.3.3.3.2.2"><minus id="S3.Ex1.m2.1.1.1.1.3.3.3.2.2.1.cmml" xref="S3.Ex1.m2.1.1.1.1.3.3.3.2.2.1"></minus><ci id="S3.Ex1.m2.1.1.1.1.3.3.3.2.2.2.cmml" xref="S3.Ex1.m2.1.1.1.1.3.3.3.2.2.2">𝑦</ci><apply id="S3.Ex1.m2.1.1.1.1.3.3.3.2.2.3.cmml" xref="S3.Ex1.m2.1.1.1.1.3.3.3.2.2.3"><csymbol cd="ambiguous" id="S3.Ex1.m2.1.1.1.1.3.3.3.2.2.3.1.cmml" xref="S3.Ex1.m2.1.1.1.1.3.3.3.2.2.3">subscript</csymbol><ci id="S3.Ex1.m2.1.1.1.1.3.3.3.2.2.3.2.cmml" xref="S3.Ex1.m2.1.1.1.1.3.3.3.2.2.3.2">𝑦</ci><ci id="S3.Ex1.m2.1.1.1.1.3.3.3.2.2.3.3.cmml" xref="S3.Ex1.m2.1.1.1.1.3.3.3.2.2.3.3">𝑘</ci></apply></apply></interval><ci id="S3.Ex1.m2.1.1.1.1.4.4.8.cmml" xref="S3.Ex1.m2.1.1.1.1.4.4.8">𝛿</ci><apply id="S3.Ex1.m2.1.1.1.1.4.4.4.1.1.cmml" xref="S3.Ex1.m2.1.1.1.1.4.4.4.1"><minus id="S3.Ex1.m2.1.1.1.1.4.4.4.1.1.1.cmml" xref="S3.Ex1.m2.1.1.1.1.4.4.4.1.1.1"></minus><ci id="S3.Ex1.m2.1.1.1.1.4.4.4.1.1.2.cmml" xref="S3.Ex1.m2.1.1.1.1.4.4.4.1.1.2">𝜏</ci><apply id="S3.Ex1.m2.1.1.1.1.4.4.4.1.1.3.cmml" xref="S3.Ex1.m2.1.1.1.1.4.4.4.1.1.3"><csymbol cd="ambiguous" id="S3.Ex1.m2.1.1.1.1.4.4.4.1.1.3.1.cmml" xref="S3.Ex1.m2.1.1.1.1.4.4.4.1.1.3">subscript</csymbol><ci id="S3.Ex1.m2.1.1.1.1.4.4.4.1.1.3.2.cmml" xref="S3.Ex1.m2.1.1.1.1.4.4.4.1.1.3.2">𝜏</ci><ci id="S3.Ex1.m2.1.1.1.1.4.4.4.1.1.3.3.cmml" xref="S3.Ex1.m2.1.1.1.1.4.4.4.1.1.3.3">𝑘</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m2.1c">\displaystyle=\sum_{e_{k}\in\mathcal{E}}\delta(p-p_{k})\delta(x-x_{k},y-y_{k})\delta(\tau-\tau_{k}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.Ex2.m1.1" class="ltx_Math" alttext="\displaystyle\tau_{k}" display="inline"><semantics id="S3.Ex2.m1.1a"><msub id="S3.Ex2.m1.1.1" xref="S3.Ex2.m1.1.1.cmml"><mi id="S3.Ex2.m1.1.1.2" xref="S3.Ex2.m1.1.1.2.cmml">τ</mi><mi id="S3.Ex2.m1.1.1.3" xref="S3.Ex2.m1.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.Ex2.m1.1b"><apply id="S3.Ex2.m1.1.1.cmml" xref="S3.Ex2.m1.1.1"><csymbol cd="ambiguous" id="S3.Ex2.m1.1.1.1.cmml" xref="S3.Ex2.m1.1.1">subscript</csymbol><ci id="S3.Ex2.m1.1.1.2.cmml" xref="S3.Ex2.m1.1.1.2">𝜏</ci><ci id="S3.Ex2.m1.1.1.3.cmml" xref="S3.Ex2.m1.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex2.m1.1c">\displaystyle\tau_{k}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.Ex2.m2.1" class="ltx_Math" alttext="\displaystyle=\left\lfloor\frac{t_{k}-t_{a}}{t_{b}-t_{a}}\cdot T\right\rfloor" display="inline"><semantics id="S3.Ex2.m2.1a"><mrow id="S3.Ex2.m2.1.1" xref="S3.Ex2.m2.1.1.cmml"><mi id="S3.Ex2.m2.1.1.3" xref="S3.Ex2.m2.1.1.3.cmml"></mi><mo id="S3.Ex2.m2.1.1.2" xref="S3.Ex2.m2.1.1.2.cmml">=</mo><mrow id="S3.Ex2.m2.1.1.1.1" xref="S3.Ex2.m2.1.1.1.2.cmml"><mo id="S3.Ex2.m2.1.1.1.1.2" xref="S3.Ex2.m2.1.1.1.2.1.cmml">⌊</mo><mrow id="S3.Ex2.m2.1.1.1.1.1" xref="S3.Ex2.m2.1.1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.Ex2.m2.1.1.1.1.1.2" xref="S3.Ex2.m2.1.1.1.1.1.2.cmml"><mfrac id="S3.Ex2.m2.1.1.1.1.1.2a" xref="S3.Ex2.m2.1.1.1.1.1.2.cmml"><mrow id="S3.Ex2.m2.1.1.1.1.1.2.2" xref="S3.Ex2.m2.1.1.1.1.1.2.2.cmml"><msub id="S3.Ex2.m2.1.1.1.1.1.2.2.2" xref="S3.Ex2.m2.1.1.1.1.1.2.2.2.cmml"><mi id="S3.Ex2.m2.1.1.1.1.1.2.2.2.2" xref="S3.Ex2.m2.1.1.1.1.1.2.2.2.2.cmml">t</mi><mi id="S3.Ex2.m2.1.1.1.1.1.2.2.2.3" xref="S3.Ex2.m2.1.1.1.1.1.2.2.2.3.cmml">k</mi></msub><mo id="S3.Ex2.m2.1.1.1.1.1.2.2.1" xref="S3.Ex2.m2.1.1.1.1.1.2.2.1.cmml">−</mo><msub id="S3.Ex2.m2.1.1.1.1.1.2.2.3" xref="S3.Ex2.m2.1.1.1.1.1.2.2.3.cmml"><mi id="S3.Ex2.m2.1.1.1.1.1.2.2.3.2" xref="S3.Ex2.m2.1.1.1.1.1.2.2.3.2.cmml">t</mi><mi id="S3.Ex2.m2.1.1.1.1.1.2.2.3.3" xref="S3.Ex2.m2.1.1.1.1.1.2.2.3.3.cmml">a</mi></msub></mrow><mrow id="S3.Ex2.m2.1.1.1.1.1.2.3" xref="S3.Ex2.m2.1.1.1.1.1.2.3.cmml"><msub id="S3.Ex2.m2.1.1.1.1.1.2.3.2" xref="S3.Ex2.m2.1.1.1.1.1.2.3.2.cmml"><mi id="S3.Ex2.m2.1.1.1.1.1.2.3.2.2" xref="S3.Ex2.m2.1.1.1.1.1.2.3.2.2.cmml">t</mi><mi id="S3.Ex2.m2.1.1.1.1.1.2.3.2.3" xref="S3.Ex2.m2.1.1.1.1.1.2.3.2.3.cmml">b</mi></msub><mo id="S3.Ex2.m2.1.1.1.1.1.2.3.1" xref="S3.Ex2.m2.1.1.1.1.1.2.3.1.cmml">−</mo><msub id="S3.Ex2.m2.1.1.1.1.1.2.3.3" xref="S3.Ex2.m2.1.1.1.1.1.2.3.3.cmml"><mi id="S3.Ex2.m2.1.1.1.1.1.2.3.3.2" xref="S3.Ex2.m2.1.1.1.1.1.2.3.3.2.cmml">t</mi><mi id="S3.Ex2.m2.1.1.1.1.1.2.3.3.3" xref="S3.Ex2.m2.1.1.1.1.1.2.3.3.3.cmml">a</mi></msub></mrow></mfrac></mstyle><mo lspace="0.222em" rspace="0.222em" id="S3.Ex2.m2.1.1.1.1.1.1" xref="S3.Ex2.m2.1.1.1.1.1.1.cmml">⋅</mo><mi id="S3.Ex2.m2.1.1.1.1.1.3" xref="S3.Ex2.m2.1.1.1.1.1.3.cmml">T</mi></mrow><mo id="S3.Ex2.m2.1.1.1.1.3" xref="S3.Ex2.m2.1.1.1.2.1.cmml">⌋</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex2.m2.1b"><apply id="S3.Ex2.m2.1.1.cmml" xref="S3.Ex2.m2.1.1"><eq id="S3.Ex2.m2.1.1.2.cmml" xref="S3.Ex2.m2.1.1.2"></eq><csymbol cd="latexml" id="S3.Ex2.m2.1.1.3.cmml" xref="S3.Ex2.m2.1.1.3">absent</csymbol><apply id="S3.Ex2.m2.1.1.1.2.cmml" xref="S3.Ex2.m2.1.1.1.1"><floor id="S3.Ex2.m2.1.1.1.2.1.cmml" xref="S3.Ex2.m2.1.1.1.1.2"></floor><apply id="S3.Ex2.m2.1.1.1.1.1.cmml" xref="S3.Ex2.m2.1.1.1.1.1"><ci id="S3.Ex2.m2.1.1.1.1.1.1.cmml" xref="S3.Ex2.m2.1.1.1.1.1.1">⋅</ci><apply id="S3.Ex2.m2.1.1.1.1.1.2.cmml" xref="S3.Ex2.m2.1.1.1.1.1.2"><divide id="S3.Ex2.m2.1.1.1.1.1.2.1.cmml" xref="S3.Ex2.m2.1.1.1.1.1.2"></divide><apply id="S3.Ex2.m2.1.1.1.1.1.2.2.cmml" xref="S3.Ex2.m2.1.1.1.1.1.2.2"><minus id="S3.Ex2.m2.1.1.1.1.1.2.2.1.cmml" xref="S3.Ex2.m2.1.1.1.1.1.2.2.1"></minus><apply id="S3.Ex2.m2.1.1.1.1.1.2.2.2.cmml" xref="S3.Ex2.m2.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.Ex2.m2.1.1.1.1.1.2.2.2.1.cmml" xref="S3.Ex2.m2.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.Ex2.m2.1.1.1.1.1.2.2.2.2.cmml" xref="S3.Ex2.m2.1.1.1.1.1.2.2.2.2">𝑡</ci><ci id="S3.Ex2.m2.1.1.1.1.1.2.2.2.3.cmml" xref="S3.Ex2.m2.1.1.1.1.1.2.2.2.3">𝑘</ci></apply><apply id="S3.Ex2.m2.1.1.1.1.1.2.2.3.cmml" xref="S3.Ex2.m2.1.1.1.1.1.2.2.3"><csymbol cd="ambiguous" id="S3.Ex2.m2.1.1.1.1.1.2.2.3.1.cmml" xref="S3.Ex2.m2.1.1.1.1.1.2.2.3">subscript</csymbol><ci id="S3.Ex2.m2.1.1.1.1.1.2.2.3.2.cmml" xref="S3.Ex2.m2.1.1.1.1.1.2.2.3.2">𝑡</ci><ci id="S3.Ex2.m2.1.1.1.1.1.2.2.3.3.cmml" xref="S3.Ex2.m2.1.1.1.1.1.2.2.3.3">𝑎</ci></apply></apply><apply id="S3.Ex2.m2.1.1.1.1.1.2.3.cmml" xref="S3.Ex2.m2.1.1.1.1.1.2.3"><minus id="S3.Ex2.m2.1.1.1.1.1.2.3.1.cmml" xref="S3.Ex2.m2.1.1.1.1.1.2.3.1"></minus><apply id="S3.Ex2.m2.1.1.1.1.1.2.3.2.cmml" xref="S3.Ex2.m2.1.1.1.1.1.2.3.2"><csymbol cd="ambiguous" id="S3.Ex2.m2.1.1.1.1.1.2.3.2.1.cmml" xref="S3.Ex2.m2.1.1.1.1.1.2.3.2">subscript</csymbol><ci id="S3.Ex2.m2.1.1.1.1.1.2.3.2.2.cmml" xref="S3.Ex2.m2.1.1.1.1.1.2.3.2.2">𝑡</ci><ci id="S3.Ex2.m2.1.1.1.1.1.2.3.2.3.cmml" xref="S3.Ex2.m2.1.1.1.1.1.2.3.2.3">𝑏</ci></apply><apply id="S3.Ex2.m2.1.1.1.1.1.2.3.3.cmml" xref="S3.Ex2.m2.1.1.1.1.1.2.3.3"><csymbol cd="ambiguous" id="S3.Ex2.m2.1.1.1.1.1.2.3.3.1.cmml" xref="S3.Ex2.m2.1.1.1.1.1.2.3.3">subscript</csymbol><ci id="S3.Ex2.m2.1.1.1.1.1.2.3.3.2.cmml" xref="S3.Ex2.m2.1.1.1.1.1.2.3.3.2">𝑡</ci><ci id="S3.Ex2.m2.1.1.1.1.1.2.3.3.3.cmml" xref="S3.Ex2.m2.1.1.1.1.1.2.3.3.3">𝑎</ci></apply></apply></apply><ci id="S3.Ex2.m2.1.1.1.1.1.3.cmml" xref="S3.Ex2.m2.1.1.1.1.1.3">𝑇</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex2.m2.1c">\displaystyle=\left\lfloor\frac{t_{k}-t_{a}}{t_{b}-t_{a}}\cdot T\right\rfloor</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS0.SSS0.Px1.p3.8" class="ltx_p">In words, we create <math id="S3.SS0.SSS0.Px1.p3.6.m1.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS0.SSS0.Px1.p3.6.m1.1a"><mi id="S3.SS0.SSS0.Px1.p3.6.m1.1.1" xref="S3.SS0.SSS0.Px1.p3.6.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p3.6.m1.1b"><ci id="S3.SS0.SSS0.Px1.p3.6.m1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p3.6.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p3.6.m1.1c">T</annotation></semantics></math> 2-channel frames where each pixel contains the number of positive or negative events within one of the <math id="S3.SS0.SSS0.Px1.p3.7.m2.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS0.SSS0.Px1.p3.7.m2.1a"><mi id="S3.SS0.SSS0.Px1.p3.7.m2.1.1" xref="S3.SS0.SSS0.Px1.p3.7.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p3.7.m2.1b"><ci id="S3.SS0.SSS0.Px1.p3.7.m2.1.1.cmml" xref="S3.SS0.SSS0.Px1.p3.7.m2.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p3.7.m2.1c">T</annotation></semantics></math> temporal frames.
As a final step, we flatten the polarity and time dimension to retrieve a 3-dimensional tensor with shape <math id="S3.SS0.SSS0.Px1.p3.8.m3.3" class="ltx_Math" alttext="(2T,H,W)" display="inline"><semantics id="S3.SS0.SSS0.Px1.p3.8.m3.3a"><mrow id="S3.SS0.SSS0.Px1.p3.8.m3.3.3.1" xref="S3.SS0.SSS0.Px1.p3.8.m3.3.3.2.cmml"><mo stretchy="false" id="S3.SS0.SSS0.Px1.p3.8.m3.3.3.1.2" xref="S3.SS0.SSS0.Px1.p3.8.m3.3.3.2.cmml">(</mo><mrow id="S3.SS0.SSS0.Px1.p3.8.m3.3.3.1.1" xref="S3.SS0.SSS0.Px1.p3.8.m3.3.3.1.1.cmml"><mn id="S3.SS0.SSS0.Px1.p3.8.m3.3.3.1.1.2" xref="S3.SS0.SSS0.Px1.p3.8.m3.3.3.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p3.8.m3.3.3.1.1.1" xref="S3.SS0.SSS0.Px1.p3.8.m3.3.3.1.1.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px1.p3.8.m3.3.3.1.1.3" xref="S3.SS0.SSS0.Px1.p3.8.m3.3.3.1.1.3.cmml">T</mi></mrow><mo id="S3.SS0.SSS0.Px1.p3.8.m3.3.3.1.3" xref="S3.SS0.SSS0.Px1.p3.8.m3.3.3.2.cmml">,</mo><mi id="S3.SS0.SSS0.Px1.p3.8.m3.1.1" xref="S3.SS0.SSS0.Px1.p3.8.m3.1.1.cmml">H</mi><mo id="S3.SS0.SSS0.Px1.p3.8.m3.3.3.1.4" xref="S3.SS0.SSS0.Px1.p3.8.m3.3.3.2.cmml">,</mo><mi id="S3.SS0.SSS0.Px1.p3.8.m3.2.2" xref="S3.SS0.SSS0.Px1.p3.8.m3.2.2.cmml">W</mi><mo stretchy="false" id="S3.SS0.SSS0.Px1.p3.8.m3.3.3.1.5" xref="S3.SS0.SSS0.Px1.p3.8.m3.3.3.2.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p3.8.m3.3b"><vector id="S3.SS0.SSS0.Px1.p3.8.m3.3.3.2.cmml" xref="S3.SS0.SSS0.Px1.p3.8.m3.3.3.1"><apply id="S3.SS0.SSS0.Px1.p3.8.m3.3.3.1.1.cmml" xref="S3.SS0.SSS0.Px1.p3.8.m3.3.3.1.1"><times id="S3.SS0.SSS0.Px1.p3.8.m3.3.3.1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p3.8.m3.3.3.1.1.1"></times><cn type="integer" id="S3.SS0.SSS0.Px1.p3.8.m3.3.3.1.1.2.cmml" xref="S3.SS0.SSS0.Px1.p3.8.m3.3.3.1.1.2">2</cn><ci id="S3.SS0.SSS0.Px1.p3.8.m3.3.3.1.1.3.cmml" xref="S3.SS0.SSS0.Px1.p3.8.m3.3.3.1.1.3">𝑇</ci></apply><ci id="S3.SS0.SSS0.Px1.p3.8.m3.1.1.cmml" xref="S3.SS0.SSS0.Px1.p3.8.m3.1.1">𝐻</ci><ci id="S3.SS0.SSS0.Px1.p3.8.m3.2.2.cmml" xref="S3.SS0.SSS0.Px1.p3.8.m3.2.2">𝑊</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p3.8.m3.3c">(2T,H,W)</annotation></semantics></math> to directly enable compatibility with 2D convolutions.
We implement the presented algorithm with byte tensors to save memory and bandwidth.
Other, more sophisticated representations are possible <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib58" title="" class="ltx_ref">58</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, but their thorough evaluation is not our focus.</p>
</div>
</section>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Mixing Spatial and Temporal Features</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The main difficulty of object detection with event cameras is that at any given time, the neural network should be able to efficiently
(1) extract local- and global task-relevant features in pixel space because objects can cover both very small regions or large portions of the field of view;
(2) extract features from very recent events (e.g. moving edges) as well as events from several seconds ago. This is necessary because some objects are moving slowly with respect to the camera such that they generate very few events over time.
These observations motivate us to investigate transformer layers for spatial feature extraction and recurrent neural networks for efficient temporal feature extraction.
Fig. <a href="#S2.F3" title="Figure 3 ‣ 2.2 Vision Transformers for Spatio-Temporal Data ‣ 2 Related Work ‣ Recurrent Vision Transformers for Object Detection with Event Cameras" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates the components of a single stage.</p>
</div>
<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Spatial Feature Extraction</h5>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p1.1" class="ltx_p">The spatial feature extraction stage should incorporate a prior about the fact that pixels are arranged in a 2D grid as early as possible in the computation graph.
We enable this by using a convolution with overlapping kernels on the input features that at the same time spatially downsamples the input or features from the previous stage.
This convolution also endows our model with a conditional positional embedding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> such that we do not require absolute <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> or relative <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> positional embeddings.
Our ablation study in Sec. <a href="#S4.SS2" title="4.2 Ablation Studies ‣ 4 Experiments ‣ Recurrent Vision Transformers for Object Detection with Event Cameras" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a> shows that overlapping kernels lead to a substantial boost in detection performance.</p>
</div>
<div id="S3.SS1.SSS0.Px1.p2" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p2.6" class="ltx_p">In a subsequent step, the resulting features are transformed through multi-axis self-attention.
We quickly summarize the steps but refer to Tu et. al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> for an elaborate explanation.
Multi-axis attention consists of two stages using self-attention.
The first stage performs local feature interaction while the second stage enables dilated global feature mixing.
More specifically, the features are first grouped locally into non-overlapping windows:
Let <math id="S3.SS1.SSS0.Px1.p2.1.m1.1" class="ltx_Math" alttext="X\in\mathbb{R}^{H\times W\times C}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p2.1.m1.1a"><mrow id="S3.SS1.SSS0.Px1.p2.1.m1.1.1" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p2.1.m1.1.1.2" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1.2.cmml">X</mi><mo id="S3.SS1.SSS0.Px1.p2.1.m1.1.1.1" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS1.SSS0.Px1.p2.1.m1.1.1.3" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1.3.cmml"><mi id="S3.SS1.SSS0.Px1.p2.1.m1.1.1.3.2" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.SSS0.Px1.p2.1.m1.1.1.3.3" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1.3.3.cmml"><mi id="S3.SS1.SSS0.Px1.p2.1.m1.1.1.3.3.2" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1.3.3.2.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS0.Px1.p2.1.m1.1.1.3.3.1" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.SS1.SSS0.Px1.p2.1.m1.1.1.3.3.3" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1.3.3.3.cmml">W</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS0.Px1.p2.1.m1.1.1.3.3.1a" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.SS1.SSS0.Px1.p2.1.m1.1.1.3.3.4" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1.3.3.4.cmml">C</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p2.1.m1.1b"><apply id="S3.SS1.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1"><in id="S3.SS1.SSS0.Px1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1.1"></in><ci id="S3.SS1.SSS0.Px1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1.2">𝑋</ci><apply id="S3.SS1.SSS0.Px1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p2.1.m1.1.1.3.1.cmml" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS1.SSS0.Px1.p2.1.m1.1.1.3.2.cmml" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1.3.2">ℝ</ci><apply id="S3.SS1.SSS0.Px1.p2.1.m1.1.1.3.3.cmml" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1.3.3"><times id="S3.SS1.SSS0.Px1.p2.1.m1.1.1.3.3.1.cmml" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1.3.3.1"></times><ci id="S3.SS1.SSS0.Px1.p2.1.m1.1.1.3.3.2.cmml" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1.3.3.2">𝐻</ci><ci id="S3.SS1.SSS0.Px1.p2.1.m1.1.1.3.3.3.cmml" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1.3.3.3">𝑊</ci><ci id="S3.SS1.SSS0.Px1.p2.1.m1.1.1.3.3.4.cmml" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1.3.3.4">𝐶</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p2.1.m1.1c">X\in\mathbb{R}^{H\times W\times C}</annotation></semantics></math> be the input feature tensor.
We reshape the tensor to a shape <math id="S3.SS1.SSS0.Px1.p2.2.m2.3" class="ltx_Math" alttext="(\frac{H}{P}\times\frac{W}{P},P\times P,C)" display="inline"><semantics id="S3.SS1.SSS0.Px1.p2.2.m2.3a"><mrow id="S3.SS1.SSS0.Px1.p2.2.m2.3.3.2" xref="S3.SS1.SSS0.Px1.p2.2.m2.3.3.3.cmml"><mo stretchy="false" id="S3.SS1.SSS0.Px1.p2.2.m2.3.3.2.3" xref="S3.SS1.SSS0.Px1.p2.2.m2.3.3.3.cmml">(</mo><mrow id="S3.SS1.SSS0.Px1.p2.2.m2.2.2.1.1" xref="S3.SS1.SSS0.Px1.p2.2.m2.2.2.1.1.cmml"><mfrac id="S3.SS1.SSS0.Px1.p2.2.m2.2.2.1.1.2" xref="S3.SS1.SSS0.Px1.p2.2.m2.2.2.1.1.2.cmml"><mi id="S3.SS1.SSS0.Px1.p2.2.m2.2.2.1.1.2.2" xref="S3.SS1.SSS0.Px1.p2.2.m2.2.2.1.1.2.2.cmml">H</mi><mi id="S3.SS1.SSS0.Px1.p2.2.m2.2.2.1.1.2.3" xref="S3.SS1.SSS0.Px1.p2.2.m2.2.2.1.1.2.3.cmml">P</mi></mfrac><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS0.Px1.p2.2.m2.2.2.1.1.1" xref="S3.SS1.SSS0.Px1.p2.2.m2.2.2.1.1.1.cmml">×</mo><mfrac id="S3.SS1.SSS0.Px1.p2.2.m2.2.2.1.1.3" xref="S3.SS1.SSS0.Px1.p2.2.m2.2.2.1.1.3.cmml"><mi id="S3.SS1.SSS0.Px1.p2.2.m2.2.2.1.1.3.2" xref="S3.SS1.SSS0.Px1.p2.2.m2.2.2.1.1.3.2.cmml">W</mi><mi id="S3.SS1.SSS0.Px1.p2.2.m2.2.2.1.1.3.3" xref="S3.SS1.SSS0.Px1.p2.2.m2.2.2.1.1.3.3.cmml">P</mi></mfrac></mrow><mo id="S3.SS1.SSS0.Px1.p2.2.m2.3.3.2.4" xref="S3.SS1.SSS0.Px1.p2.2.m2.3.3.3.cmml">,</mo><mrow id="S3.SS1.SSS0.Px1.p2.2.m2.3.3.2.2" xref="S3.SS1.SSS0.Px1.p2.2.m2.3.3.2.2.cmml"><mi id="S3.SS1.SSS0.Px1.p2.2.m2.3.3.2.2.2" xref="S3.SS1.SSS0.Px1.p2.2.m2.3.3.2.2.2.cmml">P</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS0.Px1.p2.2.m2.3.3.2.2.1" xref="S3.SS1.SSS0.Px1.p2.2.m2.3.3.2.2.1.cmml">×</mo><mi id="S3.SS1.SSS0.Px1.p2.2.m2.3.3.2.2.3" xref="S3.SS1.SSS0.Px1.p2.2.m2.3.3.2.2.3.cmml">P</mi></mrow><mo id="S3.SS1.SSS0.Px1.p2.2.m2.3.3.2.5" xref="S3.SS1.SSS0.Px1.p2.2.m2.3.3.3.cmml">,</mo><mi id="S3.SS1.SSS0.Px1.p2.2.m2.1.1" xref="S3.SS1.SSS0.Px1.p2.2.m2.1.1.cmml">C</mi><mo stretchy="false" id="S3.SS1.SSS0.Px1.p2.2.m2.3.3.2.6" xref="S3.SS1.SSS0.Px1.p2.2.m2.3.3.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p2.2.m2.3b"><vector id="S3.SS1.SSS0.Px1.p2.2.m2.3.3.3.cmml" xref="S3.SS1.SSS0.Px1.p2.2.m2.3.3.2"><apply id="S3.SS1.SSS0.Px1.p2.2.m2.2.2.1.1.cmml" xref="S3.SS1.SSS0.Px1.p2.2.m2.2.2.1.1"><times id="S3.SS1.SSS0.Px1.p2.2.m2.2.2.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p2.2.m2.2.2.1.1.1"></times><apply id="S3.SS1.SSS0.Px1.p2.2.m2.2.2.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p2.2.m2.2.2.1.1.2"><divide id="S3.SS1.SSS0.Px1.p2.2.m2.2.2.1.1.2.1.cmml" xref="S3.SS1.SSS0.Px1.p2.2.m2.2.2.1.1.2"></divide><ci id="S3.SS1.SSS0.Px1.p2.2.m2.2.2.1.1.2.2.cmml" xref="S3.SS1.SSS0.Px1.p2.2.m2.2.2.1.1.2.2">𝐻</ci><ci id="S3.SS1.SSS0.Px1.p2.2.m2.2.2.1.1.2.3.cmml" xref="S3.SS1.SSS0.Px1.p2.2.m2.2.2.1.1.2.3">𝑃</ci></apply><apply id="S3.SS1.SSS0.Px1.p2.2.m2.2.2.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p2.2.m2.2.2.1.1.3"><divide id="S3.SS1.SSS0.Px1.p2.2.m2.2.2.1.1.3.1.cmml" xref="S3.SS1.SSS0.Px1.p2.2.m2.2.2.1.1.3"></divide><ci id="S3.SS1.SSS0.Px1.p2.2.m2.2.2.1.1.3.2.cmml" xref="S3.SS1.SSS0.Px1.p2.2.m2.2.2.1.1.3.2">𝑊</ci><ci id="S3.SS1.SSS0.Px1.p2.2.m2.2.2.1.1.3.3.cmml" xref="S3.SS1.SSS0.Px1.p2.2.m2.2.2.1.1.3.3">𝑃</ci></apply></apply><apply id="S3.SS1.SSS0.Px1.p2.2.m2.3.3.2.2.cmml" xref="S3.SS1.SSS0.Px1.p2.2.m2.3.3.2.2"><times id="S3.SS1.SSS0.Px1.p2.2.m2.3.3.2.2.1.cmml" xref="S3.SS1.SSS0.Px1.p2.2.m2.3.3.2.2.1"></times><ci id="S3.SS1.SSS0.Px1.p2.2.m2.3.3.2.2.2.cmml" xref="S3.SS1.SSS0.Px1.p2.2.m2.3.3.2.2.2">𝑃</ci><ci id="S3.SS1.SSS0.Px1.p2.2.m2.3.3.2.2.3.cmml" xref="S3.SS1.SSS0.Px1.p2.2.m2.3.3.2.2.3">𝑃</ci></apply><ci id="S3.SS1.SSS0.Px1.p2.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px1.p2.2.m2.1.1">𝐶</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p2.2.m2.3c">(\frac{H}{P}\times\frac{W}{P},P\times P,C)</annotation></semantics></math> where <math id="S3.SS1.SSS0.Px1.p2.3.m3.1" class="ltx_Math" alttext="P\times P" display="inline"><semantics id="S3.SS1.SSS0.Px1.p2.3.m3.1a"><mrow id="S3.SS1.SSS0.Px1.p2.3.m3.1.1" xref="S3.SS1.SSS0.Px1.p2.3.m3.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p2.3.m3.1.1.2" xref="S3.SS1.SSS0.Px1.p2.3.m3.1.1.2.cmml">P</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS0.Px1.p2.3.m3.1.1.1" xref="S3.SS1.SSS0.Px1.p2.3.m3.1.1.1.cmml">×</mo><mi id="S3.SS1.SSS0.Px1.p2.3.m3.1.1.3" xref="S3.SS1.SSS0.Px1.p2.3.m3.1.1.3.cmml">P</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p2.3.m3.1b"><apply id="S3.SS1.SSS0.Px1.p2.3.m3.1.1.cmml" xref="S3.SS1.SSS0.Px1.p2.3.m3.1.1"><times id="S3.SS1.SSS0.Px1.p2.3.m3.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p2.3.m3.1.1.1"></times><ci id="S3.SS1.SSS0.Px1.p2.3.m3.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p2.3.m3.1.1.2">𝑃</ci><ci id="S3.SS1.SSS0.Px1.p2.3.m3.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p2.3.m3.1.1.3">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p2.3.m3.1c">P\times P</annotation></semantics></math> is the window size in which multi-head self-attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> is applied.
This <em id="S3.SS1.SSS0.Px1.p2.6.1" class="ltx_emph ltx_font_italic">block attention</em> (Block-SA in Fig. <a href="#S2.F3" title="Figure 3 ‣ 2.2 Vision Transformers for Spatio-Temporal Data ‣ 2 Related Work ‣ Recurrent Vision Transformers for Object Detection with Event Cameras" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) is used to model local interactions.
As a next step, we would ideally be able to extract features globally.
One straightforward way to achieve this would be applying self-attention on the whole feature map.
Unfortunately, global self-attention has quadratic complexity in the number of features.
Instead, we use <em id="S3.SS1.SSS0.Px1.p2.6.2" class="ltx_emph ltx_font_italic">grid attention</em> (Grid-SA in Fig. <a href="#S2.F3" title="Figure 3 ‣ 2.2 Vision Transformers for Spatio-Temporal Data ‣ 2 Related Work ‣ Recurrent Vision Transformers for Object Detection with Event Cameras" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>).
Grid attention partitions the feature maps into a grid of shape <math id="S3.SS1.SSS0.Px1.p2.4.m4.3" class="ltx_Math" alttext="(G\times G,\frac{H}{G}\times\frac{W}{G},C)" display="inline"><semantics id="S3.SS1.SSS0.Px1.p2.4.m4.3a"><mrow id="S3.SS1.SSS0.Px1.p2.4.m4.3.3.2" xref="S3.SS1.SSS0.Px1.p2.4.m4.3.3.3.cmml"><mo stretchy="false" id="S3.SS1.SSS0.Px1.p2.4.m4.3.3.2.3" xref="S3.SS1.SSS0.Px1.p2.4.m4.3.3.3.cmml">(</mo><mrow id="S3.SS1.SSS0.Px1.p2.4.m4.2.2.1.1" xref="S3.SS1.SSS0.Px1.p2.4.m4.2.2.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p2.4.m4.2.2.1.1.2" xref="S3.SS1.SSS0.Px1.p2.4.m4.2.2.1.1.2.cmml">G</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS0.Px1.p2.4.m4.2.2.1.1.1" xref="S3.SS1.SSS0.Px1.p2.4.m4.2.2.1.1.1.cmml">×</mo><mi id="S3.SS1.SSS0.Px1.p2.4.m4.2.2.1.1.3" xref="S3.SS1.SSS0.Px1.p2.4.m4.2.2.1.1.3.cmml">G</mi></mrow><mo id="S3.SS1.SSS0.Px1.p2.4.m4.3.3.2.4" xref="S3.SS1.SSS0.Px1.p2.4.m4.3.3.3.cmml">,</mo><mrow id="S3.SS1.SSS0.Px1.p2.4.m4.3.3.2.2" xref="S3.SS1.SSS0.Px1.p2.4.m4.3.3.2.2.cmml"><mfrac id="S3.SS1.SSS0.Px1.p2.4.m4.3.3.2.2.2" xref="S3.SS1.SSS0.Px1.p2.4.m4.3.3.2.2.2.cmml"><mi id="S3.SS1.SSS0.Px1.p2.4.m4.3.3.2.2.2.2" xref="S3.SS1.SSS0.Px1.p2.4.m4.3.3.2.2.2.2.cmml">H</mi><mi id="S3.SS1.SSS0.Px1.p2.4.m4.3.3.2.2.2.3" xref="S3.SS1.SSS0.Px1.p2.4.m4.3.3.2.2.2.3.cmml">G</mi></mfrac><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS0.Px1.p2.4.m4.3.3.2.2.1" xref="S3.SS1.SSS0.Px1.p2.4.m4.3.3.2.2.1.cmml">×</mo><mfrac id="S3.SS1.SSS0.Px1.p2.4.m4.3.3.2.2.3" xref="S3.SS1.SSS0.Px1.p2.4.m4.3.3.2.2.3.cmml"><mi id="S3.SS1.SSS0.Px1.p2.4.m4.3.3.2.2.3.2" xref="S3.SS1.SSS0.Px1.p2.4.m4.3.3.2.2.3.2.cmml">W</mi><mi id="S3.SS1.SSS0.Px1.p2.4.m4.3.3.2.2.3.3" xref="S3.SS1.SSS0.Px1.p2.4.m4.3.3.2.2.3.3.cmml">G</mi></mfrac></mrow><mo id="S3.SS1.SSS0.Px1.p2.4.m4.3.3.2.5" xref="S3.SS1.SSS0.Px1.p2.4.m4.3.3.3.cmml">,</mo><mi id="S3.SS1.SSS0.Px1.p2.4.m4.1.1" xref="S3.SS1.SSS0.Px1.p2.4.m4.1.1.cmml">C</mi><mo stretchy="false" id="S3.SS1.SSS0.Px1.p2.4.m4.3.3.2.6" xref="S3.SS1.SSS0.Px1.p2.4.m4.3.3.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p2.4.m4.3b"><vector id="S3.SS1.SSS0.Px1.p2.4.m4.3.3.3.cmml" xref="S3.SS1.SSS0.Px1.p2.4.m4.3.3.2"><apply id="S3.SS1.SSS0.Px1.p2.4.m4.2.2.1.1.cmml" xref="S3.SS1.SSS0.Px1.p2.4.m4.2.2.1.1"><times id="S3.SS1.SSS0.Px1.p2.4.m4.2.2.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p2.4.m4.2.2.1.1.1"></times><ci id="S3.SS1.SSS0.Px1.p2.4.m4.2.2.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p2.4.m4.2.2.1.1.2">𝐺</ci><ci id="S3.SS1.SSS0.Px1.p2.4.m4.2.2.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p2.4.m4.2.2.1.1.3">𝐺</ci></apply><apply id="S3.SS1.SSS0.Px1.p2.4.m4.3.3.2.2.cmml" xref="S3.SS1.SSS0.Px1.p2.4.m4.3.3.2.2"><times id="S3.SS1.SSS0.Px1.p2.4.m4.3.3.2.2.1.cmml" xref="S3.SS1.SSS0.Px1.p2.4.m4.3.3.2.2.1"></times><apply id="S3.SS1.SSS0.Px1.p2.4.m4.3.3.2.2.2.cmml" xref="S3.SS1.SSS0.Px1.p2.4.m4.3.3.2.2.2"><divide id="S3.SS1.SSS0.Px1.p2.4.m4.3.3.2.2.2.1.cmml" xref="S3.SS1.SSS0.Px1.p2.4.m4.3.3.2.2.2"></divide><ci id="S3.SS1.SSS0.Px1.p2.4.m4.3.3.2.2.2.2.cmml" xref="S3.SS1.SSS0.Px1.p2.4.m4.3.3.2.2.2.2">𝐻</ci><ci id="S3.SS1.SSS0.Px1.p2.4.m4.3.3.2.2.2.3.cmml" xref="S3.SS1.SSS0.Px1.p2.4.m4.3.3.2.2.2.3">𝐺</ci></apply><apply id="S3.SS1.SSS0.Px1.p2.4.m4.3.3.2.2.3.cmml" xref="S3.SS1.SSS0.Px1.p2.4.m4.3.3.2.2.3"><divide id="S3.SS1.SSS0.Px1.p2.4.m4.3.3.2.2.3.1.cmml" xref="S3.SS1.SSS0.Px1.p2.4.m4.3.3.2.2.3"></divide><ci id="S3.SS1.SSS0.Px1.p2.4.m4.3.3.2.2.3.2.cmml" xref="S3.SS1.SSS0.Px1.p2.4.m4.3.3.2.2.3.2">𝑊</ci><ci id="S3.SS1.SSS0.Px1.p2.4.m4.3.3.2.2.3.3.cmml" xref="S3.SS1.SSS0.Px1.p2.4.m4.3.3.2.2.3.3">𝐺</ci></apply></apply><ci id="S3.SS1.SSS0.Px1.p2.4.m4.1.1.cmml" xref="S3.SS1.SSS0.Px1.p2.4.m4.1.1">𝐶</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p2.4.m4.3c">(G\times G,\frac{H}{G}\times\frac{W}{G},C)</annotation></semantics></math> using a <math id="S3.SS1.SSS0.Px1.p2.5.m5.1" class="ltx_Math" alttext="G\times G" display="inline"><semantics id="S3.SS1.SSS0.Px1.p2.5.m5.1a"><mrow id="S3.SS1.SSS0.Px1.p2.5.m5.1.1" xref="S3.SS1.SSS0.Px1.p2.5.m5.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p2.5.m5.1.1.2" xref="S3.SS1.SSS0.Px1.p2.5.m5.1.1.2.cmml">G</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS0.Px1.p2.5.m5.1.1.1" xref="S3.SS1.SSS0.Px1.p2.5.m5.1.1.1.cmml">×</mo><mi id="S3.SS1.SSS0.Px1.p2.5.m5.1.1.3" xref="S3.SS1.SSS0.Px1.p2.5.m5.1.1.3.cmml">G</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p2.5.m5.1b"><apply id="S3.SS1.SSS0.Px1.p2.5.m5.1.1.cmml" xref="S3.SS1.SSS0.Px1.p2.5.m5.1.1"><times id="S3.SS1.SSS0.Px1.p2.5.m5.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p2.5.m5.1.1.1"></times><ci id="S3.SS1.SSS0.Px1.p2.5.m5.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p2.5.m5.1.1.2">𝐺</ci><ci id="S3.SS1.SSS0.Px1.p2.5.m5.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p2.5.m5.1.1.3">𝐺</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p2.5.m5.1c">G\times G</annotation></semantics></math> uniform grid.
The resulting windows are of size <math id="S3.SS1.SSS0.Px1.p2.6.m6.1" class="ltx_Math" alttext="\frac{H}{G}\times\frac{W}{G}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p2.6.m6.1a"><mrow id="S3.SS1.SSS0.Px1.p2.6.m6.1.1" xref="S3.SS1.SSS0.Px1.p2.6.m6.1.1.cmml"><mfrac id="S3.SS1.SSS0.Px1.p2.6.m6.1.1.2" xref="S3.SS1.SSS0.Px1.p2.6.m6.1.1.2.cmml"><mi id="S3.SS1.SSS0.Px1.p2.6.m6.1.1.2.2" xref="S3.SS1.SSS0.Px1.p2.6.m6.1.1.2.2.cmml">H</mi><mi id="S3.SS1.SSS0.Px1.p2.6.m6.1.1.2.3" xref="S3.SS1.SSS0.Px1.p2.6.m6.1.1.2.3.cmml">G</mi></mfrac><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS0.Px1.p2.6.m6.1.1.1" xref="S3.SS1.SSS0.Px1.p2.6.m6.1.1.1.cmml">×</mo><mfrac id="S3.SS1.SSS0.Px1.p2.6.m6.1.1.3" xref="S3.SS1.SSS0.Px1.p2.6.m6.1.1.3.cmml"><mi id="S3.SS1.SSS0.Px1.p2.6.m6.1.1.3.2" xref="S3.SS1.SSS0.Px1.p2.6.m6.1.1.3.2.cmml">W</mi><mi id="S3.SS1.SSS0.Px1.p2.6.m6.1.1.3.3" xref="S3.SS1.SSS0.Px1.p2.6.m6.1.1.3.3.cmml">G</mi></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p2.6.m6.1b"><apply id="S3.SS1.SSS0.Px1.p2.6.m6.1.1.cmml" xref="S3.SS1.SSS0.Px1.p2.6.m6.1.1"><times id="S3.SS1.SSS0.Px1.p2.6.m6.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p2.6.m6.1.1.1"></times><apply id="S3.SS1.SSS0.Px1.p2.6.m6.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p2.6.m6.1.1.2"><divide id="S3.SS1.SSS0.Px1.p2.6.m6.1.1.2.1.cmml" xref="S3.SS1.SSS0.Px1.p2.6.m6.1.1.2"></divide><ci id="S3.SS1.SSS0.Px1.p2.6.m6.1.1.2.2.cmml" xref="S3.SS1.SSS0.Px1.p2.6.m6.1.1.2.2">𝐻</ci><ci id="S3.SS1.SSS0.Px1.p2.6.m6.1.1.2.3.cmml" xref="S3.SS1.SSS0.Px1.p2.6.m6.1.1.2.3">𝐺</ci></apply><apply id="S3.SS1.SSS0.Px1.p2.6.m6.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p2.6.m6.1.1.3"><divide id="S3.SS1.SSS0.Px1.p2.6.m6.1.1.3.1.cmml" xref="S3.SS1.SSS0.Px1.p2.6.m6.1.1.3"></divide><ci id="S3.SS1.SSS0.Px1.p2.6.m6.1.1.3.2.cmml" xref="S3.SS1.SSS0.Px1.p2.6.m6.1.1.3.2">𝑊</ci><ci id="S3.SS1.SSS0.Px1.p2.6.m6.1.1.3.3.cmml" xref="S3.SS1.SSS0.Px1.p2.6.m6.1.1.3.3">𝐺</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p2.6.m6.1c">\frac{H}{G}\times\frac{W}{G}</annotation></semantics></math>.
Self-attention is then applied to these windows which corresponds to global, dilated mixing of features.</p>
</div>
<div id="S3.SS1.SSS0.Px1.p3" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p3.1" class="ltx_p">We study alternative designs as part of our architecture in the ablation studies in Sec. <a href="#S4.SS2" title="4.2 Ablation Studies ‣ 4 Experiments ‣ Recurrent Vision Transformers for Object Detection with Event Cameras" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Temporal Feature Extraction</h5>

<div id="S3.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p1.2" class="ltx_p">We opt for temporal feature aggregation with LSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> cells at the end of the stage.
Differently from prior work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> we find that temporal and spatial feature aggregation can be completely separated.
This means that we use plain LSTM cells such that the states of the LSTMs do not interact with each other.
By avoiding Conv-LSTM units <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, we can drastically reduce the computational complexity and parameter count.
I.e. a Conv-LSTM with kernel size <math id="S3.SS1.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="k\times k" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.1.m1.1a"><mrow id="S3.SS1.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.2" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.2.cmml">k</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.1" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.1.cmml">×</mo><mi id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.1.m1.1b"><apply id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1"><times id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.1"></times><ci id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.2">𝑘</ci><ci id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.1.m1.1c">k\times k</annotation></semantics></math> and stride 1 demands <math id="S3.SS1.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="k^{2}" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.2.m2.1a"><msup id="S3.SS1.SSS0.Px2.p1.2.m2.1.1" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.2" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.2.cmml">k</mi><mn id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.2.m2.1b"><apply id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1">superscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.2">𝑘</ci><cn type="integer" id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.2.m2.1c">k^{2}</annotation></semantics></math> the number of parameters and compute compared to the original LSTM cell.
We examine this aspect in the experimental Sec. <a href="#S4.SS2" title="4.2 Ablation Studies ‣ 4 Experiments ‣ Recurrent Vision Transformers for Object Detection with Event Cameras" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Model Details</h5>

<div id="S3.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px3.p1.1" class="ltx_p">We apply LayerNorm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> before and LayerScale <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> after each attention and MLP module, and add a residual connection after each module.
We found that LayerScale enables a wider range of learning rates.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Hierarchical Multi-Stage Design</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We compose multiple RVT blocks together to form a multi-stage hierarchical backbone.
The overall architecture is shown in Fig. <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Recurrent Vision Transformers for Object Detection with Event Cameras" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">At first, a local temporal slice of events is processed into a 2D tensor format as formulated in the beginning of this section.
Subsequently, each stage takes the previous features as input and optionally uses the LSTM state from the last timestep to compute features for the next stage.
By saving the LSTM states for the following timestep, each recurrent stage can retain temporal information for the whole feature map.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">We follow prior work and use features from the second to the fourth stage for the object detection framework.
To do so, we reshape the hidden states of the LSTMs into 2D feature maps.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<figure id="S4.T1" class="ltx_table">
<div id="S4.T1.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:84.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-30.3pt,11.8pt) scale(0.781296687047261,0.781296687047261) ;">
<table id="S4.T1.4.4" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.4.4.5.1" class="ltx_tr">
<th id="S4.T1.4.4.5.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S4.T1.4.4.5.1.2" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></th>
<th id="S4.T1.4.4.5.1.3" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></th>
<th id="S4.T1.4.4.5.1.4" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></th>
<th id="S4.T1.4.4.5.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3">Channels</th>
</tr>
<tr id="S4.T1.4.4.6.2" class="ltx_tr">
<th id="S4.T1.4.4.6.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row">Stage</th>
<th id="S4.T1.4.4.6.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_column">Size</th>
<th id="S4.T1.4.4.6.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column">Kernel</th>
<th id="S4.T1.4.4.6.2.4" class="ltx_td ltx_align_left ltx_th ltx_th_column">Stride</th>
<th id="S4.T1.4.4.6.2.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">RVT-T</th>
<th id="S4.T1.4.4.6.2.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">RVT-S</th>
<th id="S4.T1.4.4.6.2.7" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_t">RVT-B</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">S1</th>
<td id="S4.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_t"><math id="S4.T1.1.1.1.1.m1.1" class="ltx_Math" alttext="1/4" display="inline"><semantics id="S4.T1.1.1.1.1.m1.1a"><mrow id="S4.T1.1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.1.m1.1.1.cmml"><mn id="S4.T1.1.1.1.1.m1.1.1.2" xref="S4.T1.1.1.1.1.m1.1.1.2.cmml">1</mn><mo id="S4.T1.1.1.1.1.m1.1.1.1" xref="S4.T1.1.1.1.1.m1.1.1.1.cmml">/</mo><mn id="S4.T1.1.1.1.1.m1.1.1.3" xref="S4.T1.1.1.1.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.m1.1b"><apply id="S4.T1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.m1.1.1"><divide id="S4.T1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T1.1.1.1.1.m1.1.1.1"></divide><cn type="integer" id="S4.T1.1.1.1.1.m1.1.1.2.cmml" xref="S4.T1.1.1.1.1.m1.1.1.2">1</cn><cn type="integer" id="S4.T1.1.1.1.1.m1.1.1.3.cmml" xref="S4.T1.1.1.1.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.m1.1c">1/4</annotation></semantics></math></td>
<td id="S4.T1.1.1.1.3" class="ltx_td ltx_align_left ltx_border_t">7</td>
<td id="S4.T1.1.1.1.4" class="ltx_td ltx_align_left ltx_border_t">4</td>
<td id="S4.T1.1.1.1.5" class="ltx_td ltx_align_left ltx_border_t">32</td>
<td id="S4.T1.1.1.1.6" class="ltx_td ltx_align_left ltx_border_t">48</td>
<td id="S4.T1.1.1.1.7" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">64</td>
</tr>
<tr id="S4.T1.2.2.2" class="ltx_tr">
<th id="S4.T1.2.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">S2</th>
<td id="S4.T1.2.2.2.1" class="ltx_td ltx_align_left"><math id="S4.T1.2.2.2.1.m1.1" class="ltx_Math" alttext="1/8" display="inline"><semantics id="S4.T1.2.2.2.1.m1.1a"><mrow id="S4.T1.2.2.2.1.m1.1.1" xref="S4.T1.2.2.2.1.m1.1.1.cmml"><mn id="S4.T1.2.2.2.1.m1.1.1.2" xref="S4.T1.2.2.2.1.m1.1.1.2.cmml">1</mn><mo id="S4.T1.2.2.2.1.m1.1.1.1" xref="S4.T1.2.2.2.1.m1.1.1.1.cmml">/</mo><mn id="S4.T1.2.2.2.1.m1.1.1.3" xref="S4.T1.2.2.2.1.m1.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.1.m1.1b"><apply id="S4.T1.2.2.2.1.m1.1.1.cmml" xref="S4.T1.2.2.2.1.m1.1.1"><divide id="S4.T1.2.2.2.1.m1.1.1.1.cmml" xref="S4.T1.2.2.2.1.m1.1.1.1"></divide><cn type="integer" id="S4.T1.2.2.2.1.m1.1.1.2.cmml" xref="S4.T1.2.2.2.1.m1.1.1.2">1</cn><cn type="integer" id="S4.T1.2.2.2.1.m1.1.1.3.cmml" xref="S4.T1.2.2.2.1.m1.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.1.m1.1c">1/8</annotation></semantics></math></td>
<td id="S4.T1.2.2.2.3" class="ltx_td ltx_align_left">3</td>
<td id="S4.T1.2.2.2.4" class="ltx_td ltx_align_left">2</td>
<td id="S4.T1.2.2.2.5" class="ltx_td ltx_align_left">64</td>
<td id="S4.T1.2.2.2.6" class="ltx_td ltx_align_left">96</td>
<td id="S4.T1.2.2.2.7" class="ltx_td ltx_nopad_r ltx_align_left">128</td>
</tr>
<tr id="S4.T1.3.3.3" class="ltx_tr">
<th id="S4.T1.3.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">S3</th>
<td id="S4.T1.3.3.3.1" class="ltx_td ltx_align_left"><math id="S4.T1.3.3.3.1.m1.1" class="ltx_Math" alttext="1/16" display="inline"><semantics id="S4.T1.3.3.3.1.m1.1a"><mrow id="S4.T1.3.3.3.1.m1.1.1" xref="S4.T1.3.3.3.1.m1.1.1.cmml"><mn id="S4.T1.3.3.3.1.m1.1.1.2" xref="S4.T1.3.3.3.1.m1.1.1.2.cmml">1</mn><mo id="S4.T1.3.3.3.1.m1.1.1.1" xref="S4.T1.3.3.3.1.m1.1.1.1.cmml">/</mo><mn id="S4.T1.3.3.3.1.m1.1.1.3" xref="S4.T1.3.3.3.1.m1.1.1.3.cmml">16</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.1.m1.1b"><apply id="S4.T1.3.3.3.1.m1.1.1.cmml" xref="S4.T1.3.3.3.1.m1.1.1"><divide id="S4.T1.3.3.3.1.m1.1.1.1.cmml" xref="S4.T1.3.3.3.1.m1.1.1.1"></divide><cn type="integer" id="S4.T1.3.3.3.1.m1.1.1.2.cmml" xref="S4.T1.3.3.3.1.m1.1.1.2">1</cn><cn type="integer" id="S4.T1.3.3.3.1.m1.1.1.3.cmml" xref="S4.T1.3.3.3.1.m1.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.1.m1.1c">1/16</annotation></semantics></math></td>
<td id="S4.T1.3.3.3.3" class="ltx_td ltx_align_left">3</td>
<td id="S4.T1.3.3.3.4" class="ltx_td ltx_align_left">2</td>
<td id="S4.T1.3.3.3.5" class="ltx_td ltx_align_left">128</td>
<td id="S4.T1.3.3.3.6" class="ltx_td ltx_align_left">192</td>
<td id="S4.T1.3.3.3.7" class="ltx_td ltx_nopad_r ltx_align_left">256</td>
</tr>
<tr id="S4.T1.4.4.4" class="ltx_tr">
<th id="S4.T1.4.4.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">S4</th>
<td id="S4.T1.4.4.4.1" class="ltx_td ltx_align_left ltx_border_bb"><math id="S4.T1.4.4.4.1.m1.1" class="ltx_Math" alttext="1/32" display="inline"><semantics id="S4.T1.4.4.4.1.m1.1a"><mrow id="S4.T1.4.4.4.1.m1.1.1" xref="S4.T1.4.4.4.1.m1.1.1.cmml"><mn id="S4.T1.4.4.4.1.m1.1.1.2" xref="S4.T1.4.4.4.1.m1.1.1.2.cmml">1</mn><mo id="S4.T1.4.4.4.1.m1.1.1.1" xref="S4.T1.4.4.4.1.m1.1.1.1.cmml">/</mo><mn id="S4.T1.4.4.4.1.m1.1.1.3" xref="S4.T1.4.4.4.1.m1.1.1.3.cmml">32</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.4.1.m1.1b"><apply id="S4.T1.4.4.4.1.m1.1.1.cmml" xref="S4.T1.4.4.4.1.m1.1.1"><divide id="S4.T1.4.4.4.1.m1.1.1.1.cmml" xref="S4.T1.4.4.4.1.m1.1.1.1"></divide><cn type="integer" id="S4.T1.4.4.4.1.m1.1.1.2.cmml" xref="S4.T1.4.4.4.1.m1.1.1.2">1</cn><cn type="integer" id="S4.T1.4.4.4.1.m1.1.1.3.cmml" xref="S4.T1.4.4.4.1.m1.1.1.3">32</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.4.1.m1.1c">1/32</annotation></semantics></math></td>
<td id="S4.T1.4.4.4.3" class="ltx_td ltx_align_left ltx_border_bb">3</td>
<td id="S4.T1.4.4.4.4" class="ltx_td ltx_align_left ltx_border_bb">2</td>
<td id="S4.T1.4.4.4.5" class="ltx_td ltx_align_left ltx_border_bb">256</td>
<td id="S4.T1.4.4.4.6" class="ltx_td ltx_align_left ltx_border_bb">384</td>
<td id="S4.T1.4.4.4.7" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb">512</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.7.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.8.2" class="ltx_text ltx_font_bold" style="font-size:90%;">RVT parameters and architecture variation<span id="S4.T1.8.2.1" class="ltx_text ltx_font_medium">. All model variants share the same parameter set except the number of channels per stage. Each stage initially applies a 2D convolution with kernel and stride as indicated in the table.</span></span></figcaption>
</figure>
<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We conduct ablations and evaluation our model on the Gen1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and 1 Mpx <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> event camera datasets.
We train three variants of our model on both datasets: the base model RVT-B and its small and tiny variants RVT-S and RVT-T.
Parameter details for the models are shown in Tab. <a href="#S4.T1" title="Table 1 ‣ 4 Experiments ‣ Recurrent Vision Transformers for Object Detection with Event Cameras" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Setup</h3>

<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Implementation Details</h5>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p1.1" class="ltx_p">We initialize all layers randomly except LayerScale which is initialized to 1e-5 for each module.
Our models are trained with mixed precision for 400k iterations with the ADAM optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> using a OneCycle learning rate schedule <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> with a linear decay from a maximum learning rate.
We use a mixed batching strategy that applies backpropagation through time (BPTT) to half of the samples of the batch and truncated BPTT (TBPTT) to the other half.
More details regarding this batching strategy can be found in the supplementary material.
Our data augmentation includes random horizontal flipping, zooming in and zooming out.
More details on data augmentation are available in Sec. <a href="#S4.SS2" title="4.2 Ablation Studies ‣ 4 Experiments ‣ Recurrent Vision Transformers for Object Detection with Event Cameras" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a> and the supplementary material.
To construct event representations, we consider 50 ms time windows that are discretized into <math id="S4.SS1.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="T=10" display="inline"><semantics id="S4.SS1.SSS0.Px1.p1.1.m1.1a"><mrow id="S4.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml">T</mi><mo id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.1" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.1.m1.1b"><apply id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1"><eq id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.1"></eq><ci id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2">𝑇</ci><cn type="integer" id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.1.m1.1c">T=10</annotation></semantics></math> bins.
Finally, we use the YOLOX framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, which includes the IOU loss, class loss and regression loss.
These losses are averaged both over the batch and sequence length for each optimization step.</p>
</div>
<div id="S4.SS1.SSS0.Px1.p2" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p2.1" class="ltx_p">To compare against prior work on the Gen1 dataset, we train our models with a batch size of 8, sequence length of 21, and learning rate of 2e-4.
The training takes approximately 2 days on a single A100 GPU.</p>
</div>
<div id="S4.SS1.SSS0.Px1.p3" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p3.1" class="ltx_p">On the 1 Mpx dataset, we train with a batch size of 24, sequence length of 5, and learning rate of 3.5e-4.
The training takes approximately 3 days on two A100 GPUs.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Datasets</h5>

<div id="S4.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p1.1" class="ltx_p">The Gen1 Automotive Detection dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> consists of 39 hours of event camera recordings at a resolution of <math id="S4.SS1.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="304\times 240" display="inline"><semantics id="S4.SS1.SSS0.Px2.p1.1.m1.1a"><mrow id="S4.SS1.SSS0.Px2.p1.1.m1.1.1" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.cmml"><mn id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.2" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.2.cmml">304</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.1" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.3" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.3.cmml">240</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px2.p1.1.m1.1b"><apply id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1"><times id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.2">304</cn><cn type="integer" id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.3.cmml" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.3">240</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px2.p1.1.m1.1c">304\times 240</annotation></semantics></math>.
In total, the Gen1 dataset contains 228k car and 28k pedestrian bounding boxes available at 1, 2 or 4 Hz.
We follow the evaluation protocol of prior work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> and remove bounding boxes with a side length of less than 10 pixels and a diagonal of less than 30 pixels.</p>
</div>
<div id="S4.SS1.SSS0.Px2.p2" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p2.2" class="ltx_p">The 1 MPx dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> also features driving scenarios but provides recordings at a higher resolution of <math id="S4.SS1.SSS0.Px2.p2.1.m1.1" class="ltx_Math" alttext="720\times 1280" display="inline"><semantics id="S4.SS1.SSS0.Px2.p2.1.m1.1a"><mrow id="S4.SS1.SSS0.Px2.p2.1.m1.1.1" xref="S4.SS1.SSS0.Px2.p2.1.m1.1.1.cmml"><mn id="S4.SS1.SSS0.Px2.p2.1.m1.1.1.2" xref="S4.SS1.SSS0.Px2.p2.1.m1.1.1.2.cmml">720</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.SSS0.Px2.p2.1.m1.1.1.1" xref="S4.SS1.SSS0.Px2.p2.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS1.SSS0.Px2.p2.1.m1.1.1.3" xref="S4.SS1.SSS0.Px2.p2.1.m1.1.1.3.cmml">1280</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px2.p2.1.m1.1b"><apply id="S4.SS1.SSS0.Px2.p2.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px2.p2.1.m1.1.1"><times id="S4.SS1.SSS0.Px2.p2.1.m1.1.1.1.cmml" xref="S4.SS1.SSS0.Px2.p2.1.m1.1.1.1"></times><cn type="integer" id="S4.SS1.SSS0.Px2.p2.1.m1.1.1.2.cmml" xref="S4.SS1.SSS0.Px2.p2.1.m1.1.1.2">720</cn><cn type="integer" id="S4.SS1.SSS0.Px2.p2.1.m1.1.1.3.cmml" xref="S4.SS1.SSS0.Px2.p2.1.m1.1.1.3">1280</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px2.p2.1.m1.1c">720\times 1280</annotation></semantics></math> over a period of several months at day and night.
It consists of approximately 15 hours of event data labeled at a frequency of 30 or 60 Hz with a total amount of 25 million bounding box labels for three classes (car, pedestrian, and two-wheeler).
We follow the evaluation protocol of prior work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.
That is, we remove bounding boxes with a side length of less than 20 pixels and a diagonal of less than 60 pixels and halve the input resolution to nHD resolution (<math id="S4.SS1.SSS0.Px2.p2.2.m2.1" class="ltx_Math" alttext="640\times 360" display="inline"><semantics id="S4.SS1.SSS0.Px2.p2.2.m2.1a"><mrow id="S4.SS1.SSS0.Px2.p2.2.m2.1.1" xref="S4.SS1.SSS0.Px2.p2.2.m2.1.1.cmml"><mn id="S4.SS1.SSS0.Px2.p2.2.m2.1.1.2" xref="S4.SS1.SSS0.Px2.p2.2.m2.1.1.2.cmml">640</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.SSS0.Px2.p2.2.m2.1.1.1" xref="S4.SS1.SSS0.Px2.p2.2.m2.1.1.1.cmml">×</mo><mn id="S4.SS1.SSS0.Px2.p2.2.m2.1.1.3" xref="S4.SS1.SSS0.Px2.p2.2.m2.1.1.3.cmml">360</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px2.p2.2.m2.1b"><apply id="S4.SS1.SSS0.Px2.p2.2.m2.1.1.cmml" xref="S4.SS1.SSS0.Px2.p2.2.m2.1.1"><times id="S4.SS1.SSS0.Px2.p2.2.m2.1.1.1.cmml" xref="S4.SS1.SSS0.Px2.p2.2.m2.1.1.1"></times><cn type="integer" id="S4.SS1.SSS0.Px2.p2.2.m2.1.1.2.cmml" xref="S4.SS1.SSS0.Px2.p2.2.m2.1.1.2">640</cn><cn type="integer" id="S4.SS1.SSS0.Px2.p2.2.m2.1.1.3.cmml" xref="S4.SS1.SSS0.Px2.p2.2.m2.1.1.3">360</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px2.p2.2.m2.1c">640\times 360</annotation></semantics></math>).
We provide qualitative examples of this dataset together with predictions of our base model in Fig. <a href="#S4.F4" title="Figure 4 ‣ 4.2.2 Data Augmentation ‣ 4.2 Ablation Studies ‣ 4 Experiments ‣ Recurrent Vision Transformers for Object Detection with Event Cameras" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div id="S4.SS1.SSS0.Px2.p3" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p3.1" class="ltx_p">For both datasets, mean average precision (mAP) is the main metric <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> that we consider.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Ablation Studies</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">This section examines the two main contributors to the final performance of the proposed model.
First, we investigate key components and design choices of the proposed backbone.
Second, we study the influence of different data augmentation techniques that are compatible with our sequential problem setting.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Unless stated otherwise, the ablation studies are performed on the Gen1 validation set using the best performing model after 400k iterations.
To reduce the training time, we use BPTT with a sequence length of 11 instead 21.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.2.3.1" class="ltx_tr">
<th id="S4.T2.2.3.1.1" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></th>
<th id="S4.T2.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="2">Gen1</th>
<th id="S4.T2.2.3.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">1 Mpx</th>
<th id="S4.T2.2.3.1.4" class="ltx_td ltx_nopad_r ltx_th ltx_th_column ltx_border_tt"></th>
</tr>
<tr id="S4.T2.2.2" class="ltx_tr">
<th id="S4.T2.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column">Block-type</th>
<th id="S4.T2.2.2.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">mAP</th>
<th id="S4.T2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">AP<sub id="S4.T2.1.1.1.1" class="ltx_sub"><span id="S4.T2.1.1.1.1.1" class="ltx_text ltx_font_italic">50</span></sub>
</th>
<th id="S4.T2.2.2.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">mAP</th>
<th id="S4.T2.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">AP<sub id="S4.T2.2.2.2.1" class="ltx_sub"><span id="S4.T2.2.2.2.1.1" class="ltx_text ltx_font_italic">50</span></sub>
</th>
<th id="S4.T2.2.2.6" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column">Params (M)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.2.4.1" class="ltx_tr">
<td id="S4.T2.2.4.1.1" class="ltx_td ltx_align_left ltx_border_t">
<span id="S4.T2.2.4.1.1.1" class="ltx_ERROR undefined">\ul</span>multi-axis</td>
<td id="S4.T2.2.4.1.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T2.2.4.1.2.1" class="ltx_text ltx_font_bold">47.6</span></td>
<td id="S4.T2.2.4.1.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T2.2.4.1.3.1" class="ltx_text ltx_font_bold">70.1</span></td>
<td id="S4.T2.2.4.1.4" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T2.2.4.1.4.1" class="ltx_text ltx_font_bold">46.0</span></td>
<td id="S4.T2.2.4.1.5" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T2.2.4.1.5.1" class="ltx_text ltx_font_bold">72.3</span></td>
<td id="S4.T2.2.4.1.6" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">18.5</td>
</tr>
<tr id="S4.T2.2.5.2" class="ltx_tr">
<td id="S4.T2.2.5.2.1" class="ltx_td ltx_align_left">Swin</td>
<td id="S4.T2.2.5.2.2" class="ltx_td ltx_align_left">46.7</td>
<td id="S4.T2.2.5.2.3" class="ltx_td ltx_align_left">68.7</td>
<td id="S4.T2.2.5.2.4" class="ltx_td ltx_align_left">44.4</td>
<td id="S4.T2.2.5.2.5" class="ltx_td ltx_align_left">71.7</td>
<td id="S4.T2.2.5.2.6" class="ltx_td ltx_nopad_r ltx_align_left">18.5</td>
</tr>
<tr id="S4.T2.2.6.3" class="ltx_tr">
<td id="S4.T2.2.6.3.1" class="ltx_td ltx_align_left ltx_border_bb">ConvNeXt</td>
<td id="S4.T2.2.6.3.2" class="ltx_td ltx_align_left ltx_border_bb">45.5</td>
<td id="S4.T2.2.6.3.3" class="ltx_td ltx_align_left ltx_border_bb">65.8</td>
<td id="S4.T2.2.6.3.4" class="ltx_td ltx_align_left ltx_border_bb">42.3</td>
<td id="S4.T2.2.6.3.5" class="ltx_td ltx_align_left ltx_border_bb">70.6</td>
<td id="S4.T2.2.6.3.6" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb">18.7</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.5.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.6.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Spatial Aggregation<span id="S4.T2.6.2.1" class="ltx_text ltx_font_medium">. Multi-axis attention leads to the best results on both the Gen1 and 1 Mpx dataset.</span></span></figcaption>
</figure>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Model Components</h4>

<section id="S4.SS2.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Spatial Interaction</h5>

<div id="S4.SS2.SSS1.Px1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.Px1.p1.1" class="ltx_p">In Tab. <a href="#S4.T2" title="Table 2 ‣ 4.2 Ablation Studies ‣ 4 Experiments ‣ Recurrent Vision Transformers for Object Detection with Event Cameras" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we study different spatial aggregation techniques.
For a fair comparison, we keep the LSTM and convolutional downsampling layers identical and only exchange the attention and MLP modules.
We compare multi-axis attention with ConvNext blocks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> and Swin transformer blocks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>.
ConvNext is a convolutional neural network architecture that has shown competitive performance with transformer-based models on a wide range of tasks, including object detection.
We use the default kernel size of <math id="S4.SS2.SSS1.Px1.p1.1.m1.1" class="ltx_Math" alttext="7\times 7" display="inline"><semantics id="S4.SS2.SSS1.Px1.p1.1.m1.1a"><mrow id="S4.SS2.SSS1.Px1.p1.1.m1.1.1" xref="S4.SS2.SSS1.Px1.p1.1.m1.1.1.cmml"><mn id="S4.SS2.SSS1.Px1.p1.1.m1.1.1.2" xref="S4.SS2.SSS1.Px1.p1.1.m1.1.1.2.cmml">7</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.SSS1.Px1.p1.1.m1.1.1.1" xref="S4.SS2.SSS1.Px1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS2.SSS1.Px1.p1.1.m1.1.1.3" xref="S4.SS2.SSS1.Px1.p1.1.m1.1.1.3.cmml">7</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.Px1.p1.1.m1.1b"><apply id="S4.SS2.SSS1.Px1.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS1.Px1.p1.1.m1.1.1"><times id="S4.SS2.SSS1.Px1.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS1.Px1.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS2.SSS1.Px1.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS1.Px1.p1.1.m1.1.1.2">7</cn><cn type="integer" id="S4.SS2.SSS1.Px1.p1.1.m1.1.1.3.cmml" xref="S4.SS2.SSS1.Px1.p1.1.m1.1.1.3">7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.Px1.p1.1.m1.1c">7\times 7</annotation></semantics></math> as originally suggested and place three ConvNeXt blocks in each stage to approximately match the number of parameters of the reference model.
Swin, instead, is an attention-based model that applies local self-attention in windows that interact with each other through cyclic shifting.</p>
</div>
<div id="S4.SS2.SSS1.Px1.p2" class="ltx_para">
<p id="S4.SS2.SSS1.Px1.p2.1" class="ltx_p">We find that our Swin variant achieves better performance than the ConvNext variant, however, both are outperformed by multi-axis self-attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> on both the Gen1 and 1 Mpx dataset.
This experiment suggests that global interaction at every stage (multi-axis) is advantageous to purely local interaction (Swin, ConvNext).</p>
</div>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.2.2" class="ltx_tr">
<th id="S4.T3.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Conv. kernel type</th>
<th id="S4.T3.2.2.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">mAP</th>
<th id="S4.T3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">AP<sub id="S4.T3.1.1.1.1" class="ltx_sub"><span id="S4.T3.1.1.1.1.1" class="ltx_text ltx_font_italic">50</span></sub>
</th>
<th id="S4.T3.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">AP<sub id="S4.T3.2.2.2.1" class="ltx_sub"><span id="S4.T3.2.2.2.1.1" class="ltx_text ltx_font_italic">75</span></sub>
</th>
<th id="S4.T3.2.2.5" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt">Params (M)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.2.3.1" class="ltx_tr">
<td id="S4.T3.2.3.1.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T3.2.3.1.1.1" class="ltx_text ltx_ulem_uline">overlapping</span></td>
<td id="S4.T3.2.3.1.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T3.2.3.1.2.1" class="ltx_text ltx_font_bold">47.6</span></td>
<td id="S4.T3.2.3.1.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T3.2.3.1.3.1" class="ltx_text ltx_font_bold">70.1</span></td>
<td id="S4.T3.2.3.1.4" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T3.2.3.1.4.1" class="ltx_text ltx_font_bold">52.6</span></td>
<td id="S4.T3.2.3.1.5" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">18.5</td>
</tr>
<tr id="S4.T3.2.4.2" class="ltx_tr">
<td id="S4.T3.2.4.2.1" class="ltx_td ltx_align_left ltx_border_bb">non-overlapping</td>
<td id="S4.T3.2.4.2.2" class="ltx_td ltx_align_left ltx_border_bb">46.1</td>
<td id="S4.T3.2.4.2.3" class="ltx_td ltx_align_left ltx_border_bb">68.6</td>
<td id="S4.T3.2.4.2.4" class="ltx_td ltx_align_left ltx_border_bb">50.5</td>
<td id="S4.T3.2.4.2.5" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb"><span id="S4.T3.2.4.2.5.1" class="ltx_text ltx_font_bold">17.6</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.5.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S4.T3.6.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Downsampling Strategy<span id="S4.T3.6.2.1" class="ltx_text ltx_font_medium">. The usage of overlapping kernels leads to higher performance at the expense of a slight increase in the number of parameters.</span></span></figcaption>
</figure>
</section>
<section id="S4.SS2.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Convolutional Downsampling</h5>

<div id="S4.SS2.SSS1.Px2.p1" class="ltx_para">
<p id="S4.SS2.SSS1.Px2.p1.1" class="ltx_p">The original vision transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> architecture does not perform local feature interaction with convolutional layers.
Some popular hierarchical counterparts also choose to apply downsample features without overlapping kernels <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>.
In Tab. <a href="#S4.T3" title="Table 3 ‣ Spatial Interaction ‣ 4.2.1 Model Components ‣ 4.2 Ablation Studies ‣ 4 Experiments ‣ Recurrent Vision Transformers for Object Detection with Event Cameras" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we compare overlapping and non-overlapping convolutional kernels in both the input layer (patch embedding) and feature downsampling stage.
While non-overlapping convolutions reduce the number of parameters, they cause a substantial drop in performance.
Consequently, we choose overlapping kernels in all stages of the network.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.2.2" class="ltx_tr">
<th id="S4.T4.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">LSTM kernel size</th>
<th id="S4.T4.2.2.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">mAP</th>
<th id="S4.T4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">AP<sub id="S4.T4.1.1.1.1" class="ltx_sub"><span id="S4.T4.1.1.1.1.1" class="ltx_text ltx_font_italic">50</span></sub>
</th>
<th id="S4.T4.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">AP<sub id="S4.T4.2.2.2.1" class="ltx_sub"><span id="S4.T4.2.2.2.1.1" class="ltx_text ltx_font_italic">75</span></sub>
</th>
<th id="S4.T4.2.2.5" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt">Params (M)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.3.3" class="ltx_tr">
<th id="S4.T4.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T4.3.3.1.1" class="ltx_text ltx_ulem_uline"><math id="S4.T4.3.3.1.1.m1.1" class="ltx_Math" alttext="1\times 1" display="inline"><semantics id="S4.T4.3.3.1.1.m1.1a"><mrow id="S4.T4.3.3.1.1.m1.1.1" xref="S4.T4.3.3.1.1.m1.1.1.cmml"><mn id="S4.T4.3.3.1.1.m1.1.1.2" xref="S4.T4.3.3.1.1.m1.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T4.3.3.1.1.m1.1.1.1" xref="S4.T4.3.3.1.1.m1.1.1.1.cmml">×</mo><mn id="S4.T4.3.3.1.1.m1.1.1.3" xref="S4.T4.3.3.1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.3.3.1.1.m1.1b"><apply id="S4.T4.3.3.1.1.m1.1.1.cmml" xref="S4.T4.3.3.1.1.m1.1.1"><times id="S4.T4.3.3.1.1.m1.1.1.1.cmml" xref="S4.T4.3.3.1.1.m1.1.1.1"></times><cn type="integer" id="S4.T4.3.3.1.1.m1.1.1.2.cmml" xref="S4.T4.3.3.1.1.m1.1.1.2">1</cn><cn type="integer" id="S4.T4.3.3.1.1.m1.1.1.3.cmml" xref="S4.T4.3.3.1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.3.1.1.m1.1c">1\times 1</annotation></semantics></math></span></th>
<td id="S4.T4.3.3.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T4.3.3.2.1" class="ltx_text ltx_font_bold">47.6</span></td>
<td id="S4.T4.3.3.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T4.3.3.3.1" class="ltx_text ltx_font_bold">70.1</span></td>
<td id="S4.T4.3.3.4" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T4.3.3.4.1" class="ltx_text ltx_font_bold">52.6</span></td>
<td id="S4.T4.3.3.5" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t"><span id="S4.T4.3.3.5.1" class="ltx_text ltx_font_bold">18.5</span></td>
</tr>
<tr id="S4.T4.4.4" class="ltx_tr">
<th id="S4.T4.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><math id="S4.T4.4.4.1.m1.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S4.T4.4.4.1.m1.1a"><mrow id="S4.T4.4.4.1.m1.1.1" xref="S4.T4.4.4.1.m1.1.1.cmml"><mn id="S4.T4.4.4.1.m1.1.1.2" xref="S4.T4.4.4.1.m1.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T4.4.4.1.m1.1.1.1" xref="S4.T4.4.4.1.m1.1.1.1.cmml">×</mo><mn id="S4.T4.4.4.1.m1.1.1.3" xref="S4.T4.4.4.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.4.4.1.m1.1b"><apply id="S4.T4.4.4.1.m1.1.1.cmml" xref="S4.T4.4.4.1.m1.1.1"><times id="S4.T4.4.4.1.m1.1.1.1.cmml" xref="S4.T4.4.4.1.m1.1.1.1"></times><cn type="integer" id="S4.T4.4.4.1.m1.1.1.2.cmml" xref="S4.T4.4.4.1.m1.1.1.2">3</cn><cn type="integer" id="S4.T4.4.4.1.m1.1.1.3.cmml" xref="S4.T4.4.4.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.4.4.1.m1.1c">3\times 3</annotation></semantics></math></th>
<td id="S4.T4.4.4.2" class="ltx_td ltx_align_left">46.5</td>
<td id="S4.T4.4.4.3" class="ltx_td ltx_align_left">69.0</td>
<td id="S4.T4.4.4.4" class="ltx_td ltx_align_left">51.4</td>
<td id="S4.T4.4.4.5" class="ltx_td ltx_nopad_r ltx_align_left">40.8</td>
</tr>
<tr id="S4.T4.5.5" class="ltx_tr">
<th id="S4.T4.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">
<math id="S4.T4.5.5.1.m1.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S4.T4.5.5.1.m1.1a"><mrow id="S4.T4.5.5.1.m1.1.1" xref="S4.T4.5.5.1.m1.1.1.cmml"><mn id="S4.T4.5.5.1.m1.1.1.2" xref="S4.T4.5.5.1.m1.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T4.5.5.1.m1.1.1.1" xref="S4.T4.5.5.1.m1.1.1.1.cmml">×</mo><mn id="S4.T4.5.5.1.m1.1.1.3" xref="S4.T4.5.5.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.5.5.1.m1.1b"><apply id="S4.T4.5.5.1.m1.1.1.cmml" xref="S4.T4.5.5.1.m1.1.1"><times id="S4.T4.5.5.1.m1.1.1.1.cmml" xref="S4.T4.5.5.1.m1.1.1.1"></times><cn type="integer" id="S4.T4.5.5.1.m1.1.1.2.cmml" xref="S4.T4.5.5.1.m1.1.1.2">3</cn><cn type="integer" id="S4.T4.5.5.1.m1.1.1.3.cmml" xref="S4.T4.5.5.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.5.5.1.m1.1c">3\times 3</annotation></semantics></math> depth-sep</th>
<td id="S4.T4.5.5.2" class="ltx_td ltx_align_left ltx_border_bb">46.3</td>
<td id="S4.T4.5.5.3" class="ltx_td ltx_align_left ltx_border_bb">67.2</td>
<td id="S4.T4.5.5.4" class="ltx_td ltx_align_left ltx_border_bb">51.2</td>
<td id="S4.T4.5.5.5" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb">18.6</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.10.2.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S4.T4.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;">LSTM kernel size<span id="S4.T4.7.1.1" class="ltx_text ltx_font_medium">. Conv-LSTM variants do not outperform the feature specific (<math id="S4.T4.7.1.1.m1.1" class="ltx_Math" alttext="1\times 1" display="inline"><semantics id="S4.T4.7.1.1.m1.1b"><mrow id="S4.T4.7.1.1.m1.1.1" xref="S4.T4.7.1.1.m1.1.1.cmml"><mn id="S4.T4.7.1.1.m1.1.1.2" xref="S4.T4.7.1.1.m1.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T4.7.1.1.m1.1.1.1" xref="S4.T4.7.1.1.m1.1.1.1.cmml">×</mo><mn id="S4.T4.7.1.1.m1.1.1.3" xref="S4.T4.7.1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.7.1.1.m1.1c"><apply id="S4.T4.7.1.1.m1.1.1.cmml" xref="S4.T4.7.1.1.m1.1.1"><times id="S4.T4.7.1.1.m1.1.1.1.cmml" xref="S4.T4.7.1.1.m1.1.1.1"></times><cn type="integer" id="S4.T4.7.1.1.m1.1.1.2.cmml" xref="S4.T4.7.1.1.m1.1.1.2">1</cn><cn type="integer" id="S4.T4.7.1.1.m1.1.1.3.cmml" xref="S4.T4.7.1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.7.1.1.m1.1d">1\times 1</annotation></semantics></math>) LSTM.</span></span></figcaption>
</figure>
</section>
<section id="S4.SS2.SSS1.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">LSTM with Convolutions</h5>

<div id="S4.SS2.SSS1.Px3.p1" class="ltx_para">
<p id="S4.SS2.SSS1.Px3.p1.1" class="ltx_p">Prior state-of-the-art approaches on object detection with event cameras heavily rely on convolutional LSTM cells <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.
We revisit this design choice and experiment with plain LSTM cells and a depthwise separable Conv-LSTM variant <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>.
The depthwise separable Conv-LSTM first applies a depthwise separable convolution on both the input and hidden state before a point-wise (<math id="S4.SS2.SSS1.Px3.p1.1.m1.1" class="ltx_Math" alttext="1\times 1" display="inline"><semantics id="S4.SS2.SSS1.Px3.p1.1.m1.1a"><mrow id="S4.SS2.SSS1.Px3.p1.1.m1.1.1" xref="S4.SS2.SSS1.Px3.p1.1.m1.1.1.cmml"><mn id="S4.SS2.SSS1.Px3.p1.1.m1.1.1.2" xref="S4.SS2.SSS1.Px3.p1.1.m1.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.SSS1.Px3.p1.1.m1.1.1.1" xref="S4.SS2.SSS1.Px3.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS2.SSS1.Px3.p1.1.m1.1.1.3" xref="S4.SS2.SSS1.Px3.p1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.Px3.p1.1.m1.1b"><apply id="S4.SS2.SSS1.Px3.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS1.Px3.p1.1.m1.1.1"><times id="S4.SS2.SSS1.Px3.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS1.Px3.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS2.SSS1.Px3.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS1.Px3.p1.1.m1.1.1.2">1</cn><cn type="integer" id="S4.SS2.SSS1.Px3.p1.1.m1.1.1.3.cmml" xref="S4.SS2.SSS1.Px3.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.Px3.p1.1.m1.1c">1\times 1</annotation></semantics></math>) convolution is applied.
Our results in Tab. <a href="#S4.T4" title="Table 4 ‣ Convolutional Downsampling ‣ 4.2.1 Model Components ‣ 4.2 Ablation Studies ‣ 4 Experiments ‣ Recurrent Vision Transformers for Object Detection with Event Cameras" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> suggest that plain LSTM cells are sufficient in our model and even outperform both variations.
This is to some degree surprising because both variants are a strict superset of the plain LSTM.
We decide to use a plain LSTM cell based on these observations.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<table id="S4.T5.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.2.2" class="ltx_tr">
<th id="S4.T5.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">S1</th>
<th id="S4.T5.2.2.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">S2</th>
<th id="S4.T5.2.2.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">S3</th>
<th id="S4.T5.2.2.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">S4</th>
<th id="S4.T5.2.2.7" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">mAP</th>
<th id="S4.T5.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">AP<sub id="S4.T5.1.1.1.1" class="ltx_sub"><span id="S4.T5.1.1.1.1.1" class="ltx_text ltx_font_italic">50</span></sub>
</th>
<th id="S4.T5.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">AP<sub id="S4.T5.2.2.2.1" class="ltx_sub"><span id="S4.T5.2.2.2.1.1" class="ltx_text ltx_font_italic">75</span></sub>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.2.3.1" class="ltx_tr">
<td id="S4.T5.2.3.1.1" class="ltx_td ltx_border_t"></td>
<td id="S4.T5.2.3.1.2" class="ltx_td ltx_border_t"></td>
<td id="S4.T5.2.3.1.3" class="ltx_td ltx_border_t"></td>
<td id="S4.T5.2.3.1.4" class="ltx_td ltx_border_t"></td>
<td id="S4.T5.2.3.1.5" class="ltx_td ltx_align_left ltx_border_t">32.0</td>
<td id="S4.T5.2.3.1.6" class="ltx_td ltx_align_left ltx_border_t">54.8</td>
<td id="S4.T5.2.3.1.7" class="ltx_td ltx_align_left ltx_border_t">31.4</td>
</tr>
<tr id="S4.T5.2.4.2" class="ltx_tr">
<td id="S4.T5.2.4.2.1" class="ltx_td"></td>
<td id="S4.T5.2.4.2.2" class="ltx_td"></td>
<td id="S4.T5.2.4.2.3" class="ltx_td"></td>
<td id="S4.T5.2.4.2.4" class="ltx_td ltx_align_left">✓</td>
<td id="S4.T5.2.4.2.5" class="ltx_td ltx_align_left">39.8</td>
<td id="S4.T5.2.4.2.6" class="ltx_td ltx_align_left">63.5</td>
<td id="S4.T5.2.4.2.7" class="ltx_td ltx_align_left">41.6</td>
</tr>
<tr id="S4.T5.2.5.3" class="ltx_tr">
<td id="S4.T5.2.5.3.1" class="ltx_td"></td>
<td id="S4.T5.2.5.3.2" class="ltx_td"></td>
<td id="S4.T5.2.5.3.3" class="ltx_td ltx_align_left">✓</td>
<td id="S4.T5.2.5.3.4" class="ltx_td ltx_align_left">✓</td>
<td id="S4.T5.2.5.3.5" class="ltx_td ltx_align_left">44.2</td>
<td id="S4.T5.2.5.3.6" class="ltx_td ltx_align_left">68.4</td>
<td id="S4.T5.2.5.3.7" class="ltx_td ltx_align_left">47.5</td>
</tr>
<tr id="S4.T5.2.6.4" class="ltx_tr">
<td id="S4.T5.2.6.4.1" class="ltx_td"></td>
<td id="S4.T5.2.6.4.2" class="ltx_td ltx_align_left">✓</td>
<td id="S4.T5.2.6.4.3" class="ltx_td ltx_align_left">✓</td>
<td id="S4.T5.2.6.4.4" class="ltx_td ltx_align_left">✓</td>
<td id="S4.T5.2.6.4.5" class="ltx_td ltx_align_left">46.9</td>
<td id="S4.T5.2.6.4.6" class="ltx_td ltx_align_left">70.0</td>
<td id="S4.T5.2.6.4.7" class="ltx_td ltx_align_left">50.8</td>
</tr>
<tr id="S4.T5.2.7.5" class="ltx_tr">
<td id="S4.T5.2.7.5.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T5.2.7.5.1.1" class="ltx_text ltx_ulem_uline">✓</span></td>
<td id="S4.T5.2.7.5.2" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T5.2.7.5.2.1" class="ltx_text ltx_ulem_uline">✓</span></td>
<td id="S4.T5.2.7.5.3" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T5.2.7.5.3.1" class="ltx_text ltx_ulem_uline">✓</span></td>
<td id="S4.T5.2.7.5.4" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T5.2.7.5.4.1" class="ltx_text ltx_ulem_uline">✓</span></td>
<td id="S4.T5.2.7.5.5" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T5.2.7.5.5.1" class="ltx_text ltx_font_bold">47.6</span></td>
<td id="S4.T5.2.7.5.6" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T5.2.7.5.6.1" class="ltx_text ltx_font_bold">70.1</span></td>
<td id="S4.T5.2.7.5.7" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T5.2.7.5.7.1" class="ltx_text ltx_font_bold">52.6</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T5.5.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="S4.T5.6.2" class="ltx_text ltx_font_bold" style="font-size:90%;">LSTM placement<span id="S4.T5.6.2.1" class="ltx_text ltx_font_medium">. LSTM cells contribute to the overall performance even in the early stages.</span></span></figcaption>
</figure>
<figure id="S4.T6" class="ltx_table">
<table id="S4.T6.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T6.4.5.1" class="ltx_tr">
<td id="S4.T6.4.5.1.1" class="ltx_td ltx_border_tt"></td>
<td id="S4.T6.4.5.1.2" class="ltx_td ltx_border_tt"></td>
<td id="S4.T6.4.5.1.3" class="ltx_td ltx_border_tt"></td>
<td id="S4.T6.4.5.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2">Gen1</td>
<td id="S4.T6.4.5.1.5" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">1 Mpx</td>
<td id="S4.T6.4.5.1.6" class="ltx_td ltx_nopad_r ltx_border_tt"></td>
</tr>
<tr id="S4.T6.4.6.2" class="ltx_tr">
<td id="S4.T6.4.6.2.1" class="ltx_td ltx_align_left">Method</td>
<td id="S4.T6.4.6.2.2" class="ltx_td ltx_align_left">Backbone</td>
<td id="S4.T6.4.6.2.3" class="ltx_td ltx_align_left">Detection Head</td>
<td id="S4.T6.4.6.2.4" class="ltx_td ltx_align_left ltx_border_t">mAP</td>
<td id="S4.T6.4.6.2.5" class="ltx_td ltx_align_left ltx_border_t">Time (ms)</td>
<td id="S4.T6.4.6.2.6" class="ltx_td ltx_align_left ltx_border_t">mAP</td>
<td id="S4.T6.4.6.2.7" class="ltx_td ltx_align_left ltx_border_t">Time (ms)</td>
<td id="S4.T6.4.6.2.8" class="ltx_td ltx_nopad_r ltx_align_left">Params (M)</td>
</tr>
<tr id="S4.T6.4.7.3" class="ltx_tr">
<td id="S4.T6.4.7.3.1" class="ltx_td ltx_align_left ltx_border_t">NVS-S <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>
</td>
<td id="S4.T6.4.7.3.2" class="ltx_td ltx_align_left ltx_border_t">GNN</td>
<td id="S4.T6.4.7.3.3" class="ltx_td ltx_align_left ltx_border_t">YOLOv1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>
</td>
<td id="S4.T6.4.7.3.4" class="ltx_td ltx_align_left ltx_border_t">8.6</td>
<td id="S4.T6.4.7.3.5" class="ltx_td ltx_align_left ltx_border_t">-</td>
<td id="S4.T6.4.7.3.6" class="ltx_td ltx_align_left ltx_border_t">-</td>
<td id="S4.T6.4.7.3.7" class="ltx_td ltx_align_left ltx_border_t">-</td>
<td id="S4.T6.4.7.3.8" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">0.9</td>
</tr>
<tr id="S4.T6.4.8.4" class="ltx_tr">
<td id="S4.T6.4.8.4.1" class="ltx_td ltx_align_left">Asynet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>
</td>
<td id="S4.T6.4.8.4.2" class="ltx_td ltx_align_left">Sparse CNN</td>
<td id="S4.T6.4.8.4.3" class="ltx_td ltx_align_left">YOLOv1</td>
<td id="S4.T6.4.8.4.4" class="ltx_td ltx_align_left">14.5</td>
<td id="S4.T6.4.8.4.5" class="ltx_td ltx_align_left">-</td>
<td id="S4.T6.4.8.4.6" class="ltx_td ltx_align_left">-</td>
<td id="S4.T6.4.8.4.7" class="ltx_td ltx_align_left">-</td>
<td id="S4.T6.4.8.4.8" class="ltx_td ltx_nopad_r ltx_align_left">11.4</td>
</tr>
<tr id="S4.T6.4.9.5" class="ltx_tr">
<td id="S4.T6.4.9.5.1" class="ltx_td ltx_align_left">AEGNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>
</td>
<td id="S4.T6.4.9.5.2" class="ltx_td ltx_align_left">GNN</td>
<td id="S4.T6.4.9.5.3" class="ltx_td ltx_align_left">YOLOv1</td>
<td id="S4.T6.4.9.5.4" class="ltx_td ltx_align_left">16.3</td>
<td id="S4.T6.4.9.5.5" class="ltx_td ltx_align_left">-</td>
<td id="S4.T6.4.9.5.6" class="ltx_td ltx_align_left">-</td>
<td id="S4.T6.4.9.5.7" class="ltx_td ltx_align_left">-</td>
<td id="S4.T6.4.9.5.8" class="ltx_td ltx_nopad_r ltx_align_left">20.0</td>
</tr>
<tr id="S4.T6.4.10.6" class="ltx_tr">
<td id="S4.T6.4.10.6.1" class="ltx_td ltx_align_left">Spiking DenseNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>
</td>
<td id="S4.T6.4.10.6.2" class="ltx_td ltx_align_left">SNN</td>
<td id="S4.T6.4.10.6.3" class="ltx_td ltx_align_left">SSD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>
</td>
<td id="S4.T6.4.10.6.4" class="ltx_td ltx_align_left">18.9</td>
<td id="S4.T6.4.10.6.5" class="ltx_td ltx_align_left">-</td>
<td id="S4.T6.4.10.6.6" class="ltx_td ltx_align_left">-</td>
<td id="S4.T6.4.10.6.7" class="ltx_td ltx_align_left">-</td>
<td id="S4.T6.4.10.6.8" class="ltx_td ltx_nopad_r ltx_align_left">8.2</td>
</tr>
<tr id="S4.T6.1.1" class="ltx_tr">
<td id="S4.T6.1.1.2" class="ltx_td ltx_align_left">Inception + SSD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
</td>
<td id="S4.T6.1.1.3" class="ltx_td ltx_align_left">CNN</td>
<td id="S4.T6.1.1.4" class="ltx_td ltx_align_left">SSD</td>
<td id="S4.T6.1.1.5" class="ltx_td ltx_align_left">30.1</td>
<td id="S4.T6.1.1.6" class="ltx_td ltx_align_left">19.4</td>
<td id="S4.T6.1.1.7" class="ltx_td ltx_align_left">34.0</td>
<td id="S4.T6.1.1.8" class="ltx_td ltx_align_left">45.2</td>
<td id="S4.T6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">
<math id="S4.T6.1.1.1.m1.1" class="ltx_Math" alttext="&gt;60" display="inline"><semantics id="S4.T6.1.1.1.m1.1a"><mrow id="S4.T6.1.1.1.m1.1.1" xref="S4.T6.1.1.1.m1.1.1.cmml"><mi id="S4.T6.1.1.1.m1.1.1.2" xref="S4.T6.1.1.1.m1.1.1.2.cmml"></mi><mo id="S4.T6.1.1.1.m1.1.1.1" xref="S4.T6.1.1.1.m1.1.1.1.cmml">&gt;</mo><mn id="S4.T6.1.1.1.m1.1.1.3" xref="S4.T6.1.1.1.m1.1.1.3.cmml">60</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.1.1.1.m1.1b"><apply id="S4.T6.1.1.1.m1.1.1.cmml" xref="S4.T6.1.1.1.m1.1.1"><gt id="S4.T6.1.1.1.m1.1.1.1.cmml" xref="S4.T6.1.1.1.m1.1.1.1"></gt><csymbol cd="latexml" id="S4.T6.1.1.1.m1.1.1.2.cmml" xref="S4.T6.1.1.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S4.T6.1.1.1.m1.1.1.3.cmml" xref="S4.T6.1.1.1.m1.1.1.3">60</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.1.1.1.m1.1c">&gt;60</annotation></semantics></math>*</td>
</tr>
<tr id="S4.T6.2.2" class="ltx_tr">
<td id="S4.T6.2.2.2" class="ltx_td ltx_align_left">RRC-Events <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</td>
<td id="S4.T6.2.2.3" class="ltx_td ltx_align_left">CNN</td>
<td id="S4.T6.2.2.4" class="ltx_td ltx_align_left">YOLOv3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>
</td>
<td id="S4.T6.2.2.5" class="ltx_td ltx_align_left">30.7</td>
<td id="S4.T6.2.2.6" class="ltx_td ltx_align_left">21.5</td>
<td id="S4.T6.2.2.7" class="ltx_td ltx_align_left">34.3</td>
<td id="S4.T6.2.2.8" class="ltx_td ltx_align_left">46.4</td>
<td id="S4.T6.2.2.1" class="ltx_td ltx_nopad_r ltx_align_left">
<math id="S4.T6.2.2.1.m1.1" class="ltx_Math" alttext="&gt;100" display="inline"><semantics id="S4.T6.2.2.1.m1.1a"><mrow id="S4.T6.2.2.1.m1.1.1" xref="S4.T6.2.2.1.m1.1.1.cmml"><mi id="S4.T6.2.2.1.m1.1.1.2" xref="S4.T6.2.2.1.m1.1.1.2.cmml"></mi><mo id="S4.T6.2.2.1.m1.1.1.1" xref="S4.T6.2.2.1.m1.1.1.1.cmml">&gt;</mo><mn id="S4.T6.2.2.1.m1.1.1.3" xref="S4.T6.2.2.1.m1.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.2.2.1.m1.1b"><apply id="S4.T6.2.2.1.m1.1.1.cmml" xref="S4.T6.2.2.1.m1.1.1"><gt id="S4.T6.2.2.1.m1.1.1.1.cmml" xref="S4.T6.2.2.1.m1.1.1.1"></gt><csymbol cd="latexml" id="S4.T6.2.2.1.m1.1.1.2.cmml" xref="S4.T6.2.2.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S4.T6.2.2.1.m1.1.1.3.cmml" xref="S4.T6.2.2.1.m1.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.2.2.1.m1.1c">&gt;100</annotation></semantics></math>*</td>
</tr>
<tr id="S4.T6.4.11.7" class="ltx_tr">
<td id="S4.T6.4.11.7.1" class="ltx_td ltx_align_left">MatrixLSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</td>
<td id="S4.T6.4.11.7.2" class="ltx_td ltx_align_left">RNN + CNN</td>
<td id="S4.T6.4.11.7.3" class="ltx_td ltx_align_left">YOLOv3</td>
<td id="S4.T6.4.11.7.4" class="ltx_td ltx_align_left">31.0</td>
<td id="S4.T6.4.11.7.5" class="ltx_td ltx_align_left">-</td>
<td id="S4.T6.4.11.7.6" class="ltx_td ltx_align_left">-</td>
<td id="S4.T6.4.11.7.7" class="ltx_td ltx_align_left">-</td>
<td id="S4.T6.4.11.7.8" class="ltx_td ltx_nopad_r ltx_align_left">61.5</td>
</tr>
<tr id="S4.T6.3.3" class="ltx_tr">
<td id="S4.T6.3.3.2" class="ltx_td ltx_align_left">YOLOv3 Events <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>
</td>
<td id="S4.T6.3.3.3" class="ltx_td ltx_align_left">CNN</td>
<td id="S4.T6.3.3.4" class="ltx_td ltx_align_left">YOLOv3</td>
<td id="S4.T6.3.3.5" class="ltx_td ltx_align_left">31.2</td>
<td id="S4.T6.3.3.6" class="ltx_td ltx_align_left">22.3</td>
<td id="S4.T6.3.3.7" class="ltx_td ltx_align_left">34.6</td>
<td id="S4.T6.3.3.8" class="ltx_td ltx_align_left">49.4</td>
<td id="S4.T6.3.3.1" class="ltx_td ltx_nopad_r ltx_align_left">
<math id="S4.T6.3.3.1.m1.1" class="ltx_Math" alttext="&gt;60" display="inline"><semantics id="S4.T6.3.3.1.m1.1a"><mrow id="S4.T6.3.3.1.m1.1.1" xref="S4.T6.3.3.1.m1.1.1.cmml"><mi id="S4.T6.3.3.1.m1.1.1.2" xref="S4.T6.3.3.1.m1.1.1.2.cmml"></mi><mo id="S4.T6.3.3.1.m1.1.1.1" xref="S4.T6.3.3.1.m1.1.1.1.cmml">&gt;</mo><mn id="S4.T6.3.3.1.m1.1.1.3" xref="S4.T6.3.3.1.m1.1.1.3.cmml">60</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.3.3.1.m1.1b"><apply id="S4.T6.3.3.1.m1.1.1.cmml" xref="S4.T6.3.3.1.m1.1.1"><gt id="S4.T6.3.3.1.m1.1.1.1.cmml" xref="S4.T6.3.3.1.m1.1.1.1"></gt><csymbol cd="latexml" id="S4.T6.3.3.1.m1.1.1.2.cmml" xref="S4.T6.3.3.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S4.T6.3.3.1.m1.1.1.3.cmml" xref="S4.T6.3.3.1.m1.1.1.3">60</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.3.3.1.m1.1c">&gt;60</annotation></semantics></math>*</td>
</tr>
<tr id="S4.T6.4.12.8" class="ltx_tr">
<td id="S4.T6.4.12.8.1" class="ltx_td ltx_align_left">RED <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>
</td>
<td id="S4.T6.4.12.8.2" class="ltx_td ltx_align_left">CNN + RNN</td>
<td id="S4.T6.4.12.8.3" class="ltx_td ltx_align_left">SSD</td>
<td id="S4.T6.4.12.8.4" class="ltx_td ltx_align_left">40.0</td>
<td id="S4.T6.4.12.8.5" class="ltx_td ltx_align_left">16.7</td>
<td id="S4.T6.4.12.8.6" class="ltx_td ltx_align_left">43.0</td>
<td id="S4.T6.4.12.8.7" class="ltx_td ltx_align_left">39.3</td>
<td id="S4.T6.4.12.8.8" class="ltx_td ltx_nopad_r ltx_align_left">24.1</td>
</tr>
<tr id="S4.T6.4.4" class="ltx_tr">
<td id="S4.T6.4.4.2" class="ltx_td ltx_align_left">ASTMNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>
</td>
<td id="S4.T6.4.4.3" class="ltx_td ltx_align_left">(T)CNN + RNN</td>
<td id="S4.T6.4.4.4" class="ltx_td ltx_align_left">SSD</td>
<td id="S4.T6.4.4.5" class="ltx_td ltx_align_left">
<span id="S4.T6.4.4.5.1" class="ltx_ERROR undefined">\ul</span>46.7</td>
<td id="S4.T6.4.4.6" class="ltx_td ltx_align_left">35.6</td>
<td id="S4.T6.4.4.7" class="ltx_td ltx_align_left"><span id="S4.T6.4.4.7.1" class="ltx_text ltx_font_bold">48.3</span></td>
<td id="S4.T6.4.4.8" class="ltx_td ltx_align_left">72.3</td>
<td id="S4.T6.4.4.1" class="ltx_td ltx_nopad_r ltx_align_left">
<math id="S4.T6.4.4.1.m1.1" class="ltx_Math" alttext="&gt;100" display="inline"><semantics id="S4.T6.4.4.1.m1.1a"><mrow id="S4.T6.4.4.1.m1.1.1" xref="S4.T6.4.4.1.m1.1.1.cmml"><mi id="S4.T6.4.4.1.m1.1.1.2" xref="S4.T6.4.4.1.m1.1.1.2.cmml"></mi><mo id="S4.T6.4.4.1.m1.1.1.1" xref="S4.T6.4.4.1.m1.1.1.1.cmml">&gt;</mo><mn id="S4.T6.4.4.1.m1.1.1.3" xref="S4.T6.4.4.1.m1.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.4.4.1.m1.1b"><apply id="S4.T6.4.4.1.m1.1.1.cmml" xref="S4.T6.4.4.1.m1.1.1"><gt id="S4.T6.4.4.1.m1.1.1.1.cmml" xref="S4.T6.4.4.1.m1.1.1.1"></gt><csymbol cd="latexml" id="S4.T6.4.4.1.m1.1.1.2.cmml" xref="S4.T6.4.4.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S4.T6.4.4.1.m1.1.1.3.cmml" xref="S4.T6.4.4.1.m1.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.4.4.1.m1.1c">&gt;100</annotation></semantics></math>*</td>
</tr>
<tr id="S4.T6.4.13.9" class="ltx_tr">
<td id="S4.T6.4.13.9.1" class="ltx_td ltx_align_left"><span id="S4.T6.4.13.9.1.1" class="ltx_text ltx_font_bold">RVT-B (ours)</span></td>
<td id="S4.T6.4.13.9.2" class="ltx_td ltx_align_left">Transformer + RNN</td>
<td id="S4.T6.4.13.9.3" class="ltx_td ltx_align_left">YOLOX <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>
</td>
<td id="S4.T6.4.13.9.4" class="ltx_td ltx_align_left"><span id="S4.T6.4.13.9.4.1" class="ltx_text ltx_font_bold">47.2</span></td>
<td id="S4.T6.4.13.9.5" class="ltx_td ltx_align_left"><span id="S4.T6.4.13.9.5.1" class="ltx_text ltx_font_bold">10.2 (3.7)</span></td>
<td id="S4.T6.4.13.9.6" class="ltx_td ltx_align_left">
<span id="S4.T6.4.13.9.6.1" class="ltx_ERROR undefined">\ul</span>47.4</td>
<td id="S4.T6.4.13.9.7" class="ltx_td ltx_align_left"><span id="S4.T6.4.13.9.7.1" class="ltx_text ltx_font_bold">11.9 (6.1)</span></td>
<td id="S4.T6.4.13.9.8" class="ltx_td ltx_nopad_r ltx_align_left">18.5</td>
</tr>
<tr id="S4.T6.4.14.10" class="ltx_tr">
<td id="S4.T6.4.14.10.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T6.4.14.10.1.1" class="ltx_text ltx_font_bold">RVT-S (ours)</span></td>
<td id="S4.T6.4.14.10.2" class="ltx_td ltx_align_left ltx_border_t">Transformer + RNN</td>
<td id="S4.T6.4.14.10.3" class="ltx_td ltx_align_left ltx_border_t">YOLOX</td>
<td id="S4.T6.4.14.10.4" class="ltx_td ltx_align_left ltx_border_t">46.5</td>
<td id="S4.T6.4.14.10.5" class="ltx_td ltx_align_left ltx_border_t">9.5 (3.0)</td>
<td id="S4.T6.4.14.10.6" class="ltx_td ltx_align_left ltx_border_t">44.1</td>
<td id="S4.T6.4.14.10.7" class="ltx_td ltx_align_left ltx_border_t">10.1 (5.0)</td>
<td id="S4.T6.4.14.10.8" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">9.9</td>
</tr>
<tr id="S4.T6.4.15.11" class="ltx_tr">
<td id="S4.T6.4.15.11.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T6.4.15.11.1.1" class="ltx_text ltx_font_bold">RVT-T (ours)</span></td>
<td id="S4.T6.4.15.11.2" class="ltx_td ltx_align_left ltx_border_bb">Transformer + RNN</td>
<td id="S4.T6.4.15.11.3" class="ltx_td ltx_align_left ltx_border_bb">YOLOX</td>
<td id="S4.T6.4.15.11.4" class="ltx_td ltx_align_left ltx_border_bb">44.1</td>
<td id="S4.T6.4.15.11.5" class="ltx_td ltx_align_left ltx_border_bb">9.4 (2.3)</td>
<td id="S4.T6.4.15.11.6" class="ltx_td ltx_align_left ltx_border_bb">41.5</td>
<td id="S4.T6.4.15.11.7" class="ltx_td ltx_align_left ltx_border_bb">9.5 (3.5)</td>
<td id="S4.T6.4.15.11.8" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb">4.4</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T6.15.3.1" class="ltx_text" style="font-size:90%;">Table 6</span>: </span><span id="S4.T6.8.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Comparisons on test sets of Gen1 and 1 Mpx datasets<span id="S4.T6.8.2.3" class="ltx_text ltx_font_medium">. Best results in </span>bold<span id="S4.T6.8.2.2" class="ltx_text ltx_font_medium"> and second best <span id="S4.T6.8.2.2.1" class="ltx_ERROR undefined">\ul</span>underlined. Brackets <math id="S4.T6.7.1.1.m1.1" class="ltx_Math" alttext="(\cdot)" display="inline"><semantics id="S4.T6.7.1.1.m1.1b"><mrow id="S4.T6.7.1.1.m1.1.2.2"><mo stretchy="false" id="S4.T6.7.1.1.m1.1.2.2.1">(</mo><mo lspace="0em" rspace="0em" id="S4.T6.7.1.1.m1.1.1" xref="S4.T6.7.1.1.m1.1.1.cmml">⋅</mo><mo stretchy="false" id="S4.T6.7.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.7.1.1.m1.1c"><ci id="S4.T6.7.1.1.m1.1.1.cmml" xref="S4.T6.7.1.1.m1.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.7.1.1.m1.1d">(\cdot)</annotation></semantics></math> in runtime indicate the inference time with JIT-compiled code using <span id="S4.T6.8.2.2.2" class="ltx_text ltx_font_typewriter">torch.compile</span>. A star <sup id="S4.T6.8.2.2.3" class="ltx_sup">∗</sup> suggests that this information was not directly available and estimated based on the publications. Runtime is measured in milliseconds for a batch size of 1. We used a T4 GPU for RVT to compare against indicated timings in prior work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> on comparable GPUs (Titan Xp).</span></span></figcaption>
</figure>
</section>
<section id="S4.SS2.SSS1.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">LSTM Placement</h5>

<div id="S4.SS2.SSS1.Px4.p1" class="ltx_para">
<p id="S4.SS2.SSS1.Px4.p1.1" class="ltx_p">In this ablation we study the influence of using temporal recurrence only in a subset of stages or not at all.
For all comparisons, we leave the model exactly the same but reset the states of the LSTMs at selected stages in each timestep.
This way, we can simulate the absence of recurrent layers while keeping the number of parameters constant in the comparisons.</p>
</div>
<div id="S4.SS2.SSS1.Px4.p2" class="ltx_para">
<p id="S4.SS2.SSS1.Px4.p2.1" class="ltx_p">The results in Tab. <a href="#S4.T5" title="Table 5 ‣ LSTM with Convolutions ‣ 4.2.1 Model Components ‣ 4.2 Ablation Studies ‣ 4 Experiments ‣ Recurrent Vision Transformers for Object Detection with Event Cameras" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> suggest that using no recurrence at all leads to a drastic decline of detection performance.
Enabling the LSTMs in each stage, starting from the fourth consistently leads to enhanced performance.
Surprisingly, we find that adding an LSTM to the first stage also leads to improvements, albeit the increase in mAP is not large.
In general, this experiment suggests that the detection framework benefits from features that have been augmented with temporal information.
Based on our observations, we decide to keep the LSTM also in the first stage.</p>
</div>
<figure id="S4.T7" class="ltx_table">
<table id="S4.T7.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T7.2.2" class="ltx_tr">
<th id="S4.T7.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">h-flip</th>
<th id="S4.T7.2.2.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">zoom-in</th>
<th id="S4.T7.2.2.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt">zoom-out</th>
<th id="S4.T7.2.2.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">mAP</th>
<th id="S4.T7.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">AP<sub id="S4.T7.1.1.1.1" class="ltx_sub"><span id="S4.T7.1.1.1.1.1" class="ltx_text ltx_font_italic">50</span></sub>
</th>
<th id="S4.T7.2.2.2" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt">AP<sub id="S4.T7.2.2.2.1" class="ltx_sub"><span id="S4.T7.2.2.2.1.1" class="ltx_text ltx_font_italic">75</span></sub>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T7.2.3.1" class="ltx_tr">
<th id="S4.T7.2.3.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="S4.T7.2.3.1.2" class="ltx_td ltx_border_t"></td>
<td id="S4.T7.2.3.1.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<th id="S4.T7.2.3.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">38.1</th>
<td id="S4.T7.2.3.1.5" class="ltx_td ltx_align_left ltx_border_t">59.5</td>
<td id="S4.T7.2.3.1.6" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">41.1</td>
</tr>
<tr id="S4.T7.2.4.2" class="ltx_tr">
<th id="S4.T7.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">✓</th>
<td id="S4.T7.2.4.2.2" class="ltx_td"></td>
<td id="S4.T7.2.4.2.3" class="ltx_td ltx_border_r"></td>
<th id="S4.T7.2.4.2.4" class="ltx_td ltx_align_left ltx_th ltx_th_row">41.6</th>
<td id="S4.T7.2.4.2.5" class="ltx_td ltx_align_left">63.5</td>
<td id="S4.T7.2.4.2.6" class="ltx_td ltx_nopad_r ltx_align_left">45.5</td>
</tr>
<tr id="S4.T7.2.5.3" class="ltx_tr">
<th id="S4.T7.2.5.3.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S4.T7.2.5.3.2" class="ltx_td ltx_align_left">✓</td>
<td id="S4.T7.2.5.3.3" class="ltx_td ltx_border_r"></td>
<th id="S4.T7.2.5.3.4" class="ltx_td ltx_align_left ltx_th ltx_th_row">45.8</th>
<td id="S4.T7.2.5.3.5" class="ltx_td ltx_align_left">67.8</td>
<td id="S4.T7.2.5.3.6" class="ltx_td ltx_nopad_r ltx_align_left">49.8</td>
</tr>
<tr id="S4.T7.2.6.4" class="ltx_tr">
<th id="S4.T7.2.6.4.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S4.T7.2.6.4.2" class="ltx_td"></td>
<td id="S4.T7.2.6.4.3" class="ltx_td ltx_align_left ltx_border_r">✓</td>
<th id="S4.T7.2.6.4.4" class="ltx_td ltx_align_left ltx_th ltx_th_row">44.1</th>
<td id="S4.T7.2.6.4.5" class="ltx_td ltx_align_left">65.7</td>
<td id="S4.T7.2.6.4.6" class="ltx_td ltx_nopad_r ltx_align_left">48.4</td>
</tr>
<tr id="S4.T7.2.7.5" class="ltx_tr">
<th id="S4.T7.2.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S4.T7.2.7.5.1.1" class="ltx_text ltx_ulem_uline">✓</span></th>
<td id="S4.T7.2.7.5.2" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T7.2.7.5.2.1" class="ltx_text ltx_ulem_uline">✓</span></td>
<td id="S4.T7.2.7.5.3" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r"><span id="S4.T7.2.7.5.3.1" class="ltx_text ltx_ulem_uline">✓</span></td>
<th id="S4.T7.2.7.5.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S4.T7.2.7.5.4.1" class="ltx_text ltx_font_bold">47.6</span></th>
<td id="S4.T7.2.7.5.5" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T7.2.7.5.5.1" class="ltx_text ltx_font_bold">70.1</span></td>
<td id="S4.T7.2.7.5.6" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb"><span id="S4.T7.2.7.5.6.1" class="ltx_text ltx_font_bold">52.6</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T7.5.1.1" class="ltx_text" style="font-size:90%;">Table 7</span>: </span><span id="S4.T7.6.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Data Augmentation<span id="S4.T7.6.2.1" class="ltx_text ltx_font_medium">. Data augmentation consistently improves the results.</span></span></figcaption>
</figure>
</section>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Data Augmentation</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">While data augmentation is not directly related to the model itself, it greatly influences the final result as we will illustrate next.
Here, we investigate three data augmentation techniques that are suitable for object detection on spatio-temporal data:
Random (1) horizontal flipping, (2) zoom-in, and (3) zoom-out.</p>
</div>
<div id="S4.SS2.SSS2.p2" class="ltx_para">
<p id="S4.SS2.SSS2.p2.1" class="ltx_p">Zoom-in augmentation randomly selects crops that contain at least one full bounding box at the final timestep of the BPTT sequence (i.e. during training).
This crop is then applied to the rest of the sequence before the crops are rescaled to the default resolution.
This procedure ensures that we have at least a single label to compute the loss function while maintaining the same resolution during training.</p>
</div>
<div id="S4.SS2.SSS2.p3" class="ltx_para">
<p id="S4.SS2.SSS2.p3.1" class="ltx_p">Zoom-out augmentation resizes the full input to a lower resolution and randomly places the downscaled input in a zero-tensor initialized at the default resolution.
This procedure is then applied in an identical way to the remaining BPTT sequence.</p>
</div>
<div id="S4.SS2.SSS2.p4" class="ltx_para">
<p id="S4.SS2.SSS2.p4.1" class="ltx_p">Table <a href="#S4.T7" title="Table 7 ‣ LSTM Placement ‣ 4.2.1 Model Components ‣ 4.2 Ablation Studies ‣ 4 Experiments ‣ Recurrent Vision Transformers for Object Detection with Event Cameras" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows that our model is performing poorly if no data augmentation is applied.
Overall, we find that data augmentation is important to combat overfittig not only on the Gen1 sequence but also on the 1 Mpx dataset.
The most effective augmentation is zoom-in, followed by zoom-out and horizontal flipping.
Based on these results, we decide to apply all data augmentation techniques.
We report the specific hyperparameters in the supplementary material.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2212.05598/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="146" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.3.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Predictions on the 1 Mpx dataset<span id="S4.F4.4.2.1" class="ltx_text ltx_font_medium">. All examples are thematically picked to illustrate the behaviour of the model in different scenarios. (d) shows a scenario in which the model can still partially detect objects in absence of events due the temporal memory.</span></span></figcaption>
</figure>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Benchmark Comparisons</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">In this section, we compare our proposed neural network architecture against prior work on both the Gen1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and 1 Mpx dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> and summarize the results in Tab. <a href="#S4.T6" title="Table 6 ‣ LSTM with Convolutions ‣ 4.2.1 Model Components ‣ 4.2 Ablation Studies ‣ 4 Experiments ‣ Recurrent Vision Transformers for Object Detection with Event Cameras" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.
We train three models, a base model (RVT-B) with approximately 18.5 million parameters, a small variant (RVT-S) with 9.9 million parameters, and a tiny model (RVT-T) with 4.4 million parameters by adapting the channel dimensions in each stage.
Their architectural hyperparameters are outlined in Tab. <a href="#S4.T1" title="Table 1 ‣ 4 Experiments ‣ Recurrent Vision Transformers for Object Detection with Event Cameras" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
To compare with prior work, we choose the models based on their best performance on the validation set and evaluate them on the test set.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">From Tab. <a href="#S4.T6" title="Table 6 ‣ LSTM with Convolutions ‣ 4.2.1 Model Components ‣ 4.2 Ablation Studies ‣ 4 Experiments ‣ Recurrent Vision Transformers for Object Detection with Event Cameras" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> we can draw multiple conclusions.
First, we observe that models using recurrent layers consistently outperform other approaches, both sparse (GNNs, SNNs) and dense feed-forward models without recurrent layers (Inception+SSD, RRC-Events, YOLOv3 Events) by an mAP of more than 10 on both datasets.
One notable exception is MatrixLSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> which applies LSTM cells directly at the input.
In contrast, RED <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> and ASTMNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> employ recurrent layers only in deeper layers.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">Our base model achieves a new state-of-the-art performance of 47.2 mAP on the Gen1 dataset and 47.4 mAP on the 1 Mpx dataset.
ASTMNet claims comparable results on both datasets albeit at the cost of using a much larger backbone and increased inference time.
The RED model, also reports favorable results, but achieves 7.2 lower mAP on the Gen1 and 4.4 lower mAP on the 1 Mpx dataset compared to our model.
Finally, our tiny model is amongst the smallest in our comparison.
Still, it achieves 4.1 higher mAP on the Gen1 dataset than the RED model while using 5 times fewer parameters.</p>
</div>
<section id="S4.SS3.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Inference Time</h5>

<div id="S4.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px1.p1.1" class="ltx_p">We also compute the inference time of our model on a T4 GPU with a batch size of 1.
Unfortunately, both RED and ASTMNet are not open source such that we cannot directly compare inference time on the same GPU model.
Instead, we use the timings provided by the authors that conducted their timing experiments on comparable GPUs (e.g. Titan Xp).
We report the timing results of our models in Tab. <a href="#S4.T6" title="Table 6 ‣ LSTM with Convolutions ‣ 4.2.1 Model Components ‣ 4.2 Ablation Studies ‣ 4 Experiments ‣ Recurrent Vision Transformers for Object Detection with Event Cameras" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> and also visualize them in Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Recurrent Vision Transformers for Object Detection with Event Cameras" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S4.SS3.SSS0.Px1.p2" class="ltx_para">
<p id="S4.SS3.SSS0.Px1.p2.2" class="ltx_p">To compare against prior work we first compute the inference time in PyTorch eager mode.
In eager mode, our base model achieves an inference time of 10.2 milliseconds (ms) on the Gen1 dataset (<math id="S4.SS3.SSS0.Px1.p2.1.m1.1" class="ltx_Math" alttext="304\times 240" display="inline"><semantics id="S4.SS3.SSS0.Px1.p2.1.m1.1a"><mrow id="S4.SS3.SSS0.Px1.p2.1.m1.1.1" xref="S4.SS3.SSS0.Px1.p2.1.m1.1.1.cmml"><mn id="S4.SS3.SSS0.Px1.p2.1.m1.1.1.2" xref="S4.SS3.SSS0.Px1.p2.1.m1.1.1.2.cmml">304</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.SSS0.Px1.p2.1.m1.1.1.1" xref="S4.SS3.SSS0.Px1.p2.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS3.SSS0.Px1.p2.1.m1.1.1.3" xref="S4.SS3.SSS0.Px1.p2.1.m1.1.1.3.cmml">240</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS0.Px1.p2.1.m1.1b"><apply id="S4.SS3.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S4.SS3.SSS0.Px1.p2.1.m1.1.1"><times id="S4.SS3.SSS0.Px1.p2.1.m1.1.1.1.cmml" xref="S4.SS3.SSS0.Px1.p2.1.m1.1.1.1"></times><cn type="integer" id="S4.SS3.SSS0.Px1.p2.1.m1.1.1.2.cmml" xref="S4.SS3.SSS0.Px1.p2.1.m1.1.1.2">304</cn><cn type="integer" id="S4.SS3.SSS0.Px1.p2.1.m1.1.1.3.cmml" xref="S4.SS3.SSS0.Px1.p2.1.m1.1.1.3">240</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px1.p2.1.m1.1c">304\times 240</annotation></semantics></math> resolution).
This implies a latency reduction of 6 ms compared to RED and over 3 times lower inference time than ASTMNet.
On the the 1 Mpx dataset, at a resolution of <math id="S4.SS3.SSS0.Px1.p2.2.m2.1" class="ltx_Math" alttext="640\times 360" display="inline"><semantics id="S4.SS3.SSS0.Px1.p2.2.m2.1a"><mrow id="S4.SS3.SSS0.Px1.p2.2.m2.1.1" xref="S4.SS3.SSS0.Px1.p2.2.m2.1.1.cmml"><mn id="S4.SS3.SSS0.Px1.p2.2.m2.1.1.2" xref="S4.SS3.SSS0.Px1.p2.2.m2.1.1.2.cmml">640</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.SSS0.Px1.p2.2.m2.1.1.1" xref="S4.SS3.SSS0.Px1.p2.2.m2.1.1.1.cmml">×</mo><mn id="S4.SS3.SSS0.Px1.p2.2.m2.1.1.3" xref="S4.SS3.SSS0.Px1.p2.2.m2.1.1.3.cmml">360</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS0.Px1.p2.2.m2.1b"><apply id="S4.SS3.SSS0.Px1.p2.2.m2.1.1.cmml" xref="S4.SS3.SSS0.Px1.p2.2.m2.1.1"><times id="S4.SS3.SSS0.Px1.p2.2.m2.1.1.1.cmml" xref="S4.SS3.SSS0.Px1.p2.2.m2.1.1.1"></times><cn type="integer" id="S4.SS3.SSS0.Px1.p2.2.m2.1.1.2.cmml" xref="S4.SS3.SSS0.Px1.p2.2.m2.1.1.2">640</cn><cn type="integer" id="S4.SS3.SSS0.Px1.p2.2.m2.1.1.3.cmml" xref="S4.SS3.SSS0.Px1.p2.2.m2.1.1.3">360</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px1.p2.2.m2.1c">640\times 360</annotation></semantics></math>, our base model takes 11.9 ms for a forward pass, which is 3 times faster than RED and over 5 times faster than ASTMNet.</p>
</div>
<div id="S4.SS3.SSS0.Px1.p3" class="ltx_para">
<p id="S4.SS3.SSS0.Px1.p3.1" class="ltx_p">Even on a T4 GPU, most of the inference time is framework overhead.
To overcome this partially, we use the JIT compilation feature <span id="S4.SS3.SSS0.Px1.p3.1.1" class="ltx_text ltx_font_typewriter">torch.compile</span> of PyTorch 2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>.
As Tab. <a href="#S4.T6" title="Table 6 ‣ LSTM with Convolutions ‣ 4.2.1 Model Components ‣ 4.2 Ablation Studies ‣ 4 Experiments ‣ Recurrent Vision Transformers for Object Detection with Event Cameras" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows, this almost halves the inference time for RVT-B on the 1 Mpx dataset and reduces the inference time by a factor of 2.7 on the Gen1 dataset.
As expected, the small and tiny models benefit even more from JIT compilation.
For example, RVT-T only takes 2.3 ms for a forward pass on Gen1 and 3.5 ms on 1 Mpx.
On a RTX 3090 GPU, RVT-B completes a forward pass in 2.8 ms on the 1 Mpx dataset, which shows the potential for low-latency inference if power consumption is less of a concern.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion and Limitations</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We use a very simple event representation which does not leverage the full potential of event-based data. For example, we only have a weak prior on the order of events because we process the temporal dimension directly with fully connected layers. Recent work has shown substantial gains by introducing temporal convolutions in early layers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. Efficient low-level processing of event data is still an open research problem that we have not addressed in this work.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Our approach currently only uses event streams to detect objects.
Frames yield complementary information that, when properly incorporated, will yield significantly enhanced detection performance.
For example, in Fig. <a href="#S4.F4" title="Figure 4 ‣ 4.2.2 Data Augmentation ‣ 4.2 Ablation Studies ‣ 4 Experiments ‣ Recurrent Vision Transformers for Object Detection with Event Cameras" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> (d) we can see that our model can retain information over some period when no events are available.
Still, the memory of the network will fade and detection performance deteriorates.
High quality frames even at low frame-rate could provide the missing complementary information.
Hence, we believe that a multi-modal extension of our method on a suitable dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> is a promising next step.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We introduced a novel backbone architecture for object detection with event cameras.
The architecture consists of a stage design that is repeatedly applied to create a multi-stage hierarchical neural network.
Each stage compactly incorporates convolutional priors, local- and sparse global attention and recurrent feature aggregation.
Our experiments highlight that recurrent vision transformers can be trained from scratch to reach state-of-the-art performance
in object detection with event cameras.
The resulting canonical stage-design is directly compatible with existing detection frameworks, and paves the way to low-latency object detection with event cameras on conventional hardware.
Nonetheless, we hope that this work also inspires novel designs in future neuromorphic systems.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Acknowledgment</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">This work was supported by the National Centre of Competence in Research (NCCR) Robotics (grant agreement No. 51NF40-185543) through the Swiss National Science Foundation (SNSF), and the European Research Council (ERC) under grant agreement No. 864042 (AGILEFLIGHT).</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lučić, and
Cordelia Schmid.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Vivit: A video vision transformer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. Conf. Comput. Vis. (ICCV)</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Raymond Baldwin, Ruixu Liu, Mohammed Mutlaq Almatrafi, Vijayan K Asari, and
Keigo Hirakawa.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Time-ordered recent event (TORE) volumes for event cameras.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Trans. Pattern Anal. Mach. Intell.</span><span id="bib.bib2.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Sami Barchid, Jose Mennesson, and Chaabane Djeraba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Bina-rep event frames: A simple and effective representation for
event-based cameras.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Int. Conf. Image Process. (ICIP)</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Guillaume Bellec, Franz Scherr, Anand Subramoney, Elias Hajek, Darjan Salaj,
Robert Legenstein, and Wolfgang Maass.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">A solution to the learning dilemma for recurrent networks of spiking
neurons.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Nature Communications</span><span id="bib.bib4.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Gedas Bertasius, Heng Wang, and Lorenzo Torresani.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Is space-time attention all you need for video understanding?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. Int. Conf. Mach. Learning (ICML)</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Marco Cannici, Marco Ciccone, Andrea Romanoni, and Matteo Matteucci.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Asynchronous convolutional networks for object detection in
neuromorphic cameras.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conf. Comput. Vis. Pattern Recog. Workshops (CVPRW)</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">,
2019.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Nicholas F. Y. Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Pseudo-labels for supervised learning on dynamic vision sensor data,
applied to object detection under ego-motion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conf. Comput. Vis. Pattern Recog. Workshops (CVPRW)</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">,
2018.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei,
Huaxia Xia, and Chunhua Shen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Twins: Revisiting the design of spatial attention in vision
transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Conf. Neural Inf. Process. Syst. (NeurIPS)</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xiaolin Wei, Huaxia Xia, and
Chunhua Shen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Conditional positional encodings for vision transformers, 2021.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Loïc Cordone, Benoît Miramond, and Philippe Thierion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Object detection with spiking neural networks on automotive event
data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. Joint Conf. Neural Netw. (IJCNN)</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Pierre de Tournemire, Davide Nitti, Etienne Perot, Davide Migliore, and Amos
Sironi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">A large scale event-based detection dataset for automotive, 2020.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">An image is worth 16x16 words: Transformers for image recognition at
scale.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. Conf. Learn. Representations (ICLR)</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Christoph Feichtenhofer, Haoqi Fan, Yanghao Li, and Kaiming He.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Masked autoencoders as spatiotemporal learners.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv e-prints</span><span id="bib.bib13.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Guillermo Gallego, Tobi Delbruck, Garrick Orchard, Chiara Bartolozzi, Brian
Taba, Andrea Censi, Stefan Leutenegger, Andrew Davison, Jörg Conradt,
Kostas Daniilidis, and Davide Scaramuzza.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Event-based vision: A survey.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Trans. Pattern Anal. Mach. Intell.</span><span id="bib.bib14.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Yolox: Exceeding yolo series in 2021, 2021.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Daniel Gehrig, Antonio Loquercio, Konstantinos G. Derpanis, and Davide
Scaramuzza.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">End-to-end learning of representations for asynchronous event-based
data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. Conf. Comput. Vis. (ICCV)</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Mathias Gehrig, Willem Aarents, Daniel Gehrig, and Davide Scaramuzza.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Dsec: A stereo event camera dataset for driving scenarios.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Robot. Autom. Lett.</span><span id="bib.bib17.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Sepp Hochreiter and Jürgen Schmidhuber.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Long short-term memory.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Neural Computation</span><span id="bib.bib18.4.2" class="ltx_text" style="font-size:90%;">, 1997.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Massimiliano Iacono, Stefan Weber, Arren Glover, and Chiara Bartolozzi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Towards event-driven object detection with off-the-shelf deep
learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE/RSJ Int. Conf. Intell. Robot. Syst. (IROS)</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Zhuangyi Jiang, Pengfei Xia, Kai Huang, Walter Stechele, Guang Chen, Zhenshan
Bing, and Alois Knoll.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Mixed frame-/event-driven fast pedestrian detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Int. Conf. Robot. Autom. (ICRA)</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Diederik P. Kingma and Jimmy L. Ba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Adam: A method for stochastic optimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. Conf. Learn. Representations (ICLR)</span><span id="bib.bib21.4.2" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Chankyu Lee, Syed Shakib Sarwar, Priyadarshini Panda, Gopalakrishnan
Srinivasan, and Kaushik Roy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Enabling spike-based backpropagation for training deep neural network
architectures.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Front. Neurosci.</span><span id="bib.bib22.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Jun Haeng Lee, Tobi Delbruck, and Michael Pfeiffer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Training deep spiking neural networks using backpropagation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Front. Neurosci.</span><span id="bib.bib23.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Layer normalization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv e-prints</span><span id="bib.bib24.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Jianing Li, Siwei Dong, Zhaofei Yu, Yonghong Tian, and Tiejun Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Event-based vision enhanced: A joint detection framework in
autonomous driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2019 IEEE International Conference on Multimedia and Expo
(ICME)</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Jianing Li, Jia Li, Lin Zhu, Xijie Xiang, Tiejun Huang, and Yonghong Tian.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Asynchronous spatio-temporal memory network for continuous
event-based object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Trans. Image Process.</span><span id="bib.bib26.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Yijin Li, Han Zhou, Bangbang Yang, Ye Zhang, Zhaopeng Cui, Hujun Bao, and
Guofeng Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Graph-based asynchronous event processing for rapid object
recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. Conf. Comput. Vis. (ICCV)</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, October 2021.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Neural scene flow fields for space-time view synthesis of dynamic
scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C. Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Microsoft COCO: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Eur. Conf. Comput. Vis. (ECCV)</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, pages 740–755. 2014.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed,
Cheng-Yang Fu, and Alexander C. Berg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">SSD: Single shot MultiBox detector.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Eur. Conf. Comput. Vis. (ECCV)</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Xu Liu, Jianing Li, Xiaopeng Fan, and Yonghong Tian.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Event-based monocular dense depth estimation with recurrent
transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv e-prints</span><span id="bib.bib31.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
Baining Guo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Swin transformer: Hierarchical vision transformer using shifted
windows.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. Conf. Comput. Vis. (ICCV)</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell,
and Saining Xie.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">A convnet for the 2020s.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</span><span id="bib.bib33.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Nico Messikommer, Daniel Gehrig, Antonio Loquercio, and Davide Scaramuzza.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Event-based asynchronous sparse convolutional networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Eur. Conf. Comput. Vis. (ECCV)</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Anton Mitrokhin, Zhiyuan Hua, Cornelia Fermuller, and Yiannis Aloimonos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Learning visual motion segmentation using event surfaces.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:90%;">, June 2020.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Emre O. Neftci, Hesham Mostafa, and Friedemann Zenke.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Surrogate gradient learning in spiking neural networks: Bringing the
power of gradient-based optimization to spiking neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Signal Processing Magazine</span><span id="bib.bib36.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan
Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith
Chintala.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Pytorch: An imperative style, high-performance deep learning library.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib37.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Conf. Neural Inf. Process. Syst. (NeurIPS)</span><span id="bib.bib37.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Etienne Perot, Pierre de Tournemire, Davide Nitti, Jonathan Masci, and Amos
Sironi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Learning to detect objects with a 1 megapixel event camera.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text" style="font-size:90%;">In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin,
editors, </span><span id="bib.bib38.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Conf. Neural Inf. Process. Syst. (NeurIPS)</span><span id="bib.bib38.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Andreas Pfeuffer and Klaus Dietmayer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">Separable convolutional LSTMs for faster video segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib39.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Intelligent Transportation Systems Conference
(ITSC)</span><span id="bib.bib39.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">You only look once: Unified, real-time object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib40.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</span><span id="bib.bib40.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Joseph Redmon and Ali Farhadi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">Yolov3: An incremental improvement, 2018.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Alberto Sabater, Luis Montesano, and Ana C. Murillo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Event transformer. a sparse-aware solution for efficient event data
processing.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib42.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conf. Comput. Vis. Pattern Recog. Workshops (CVPRW)</span><span id="bib.bib42.5.3" class="ltx_text" style="font-size:90%;">,
2022.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Simon Schaefer, Daniel Gehrig, and Davide Scaramuzza.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Aegnn: Asynchronous event-based graph neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib43.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</span><span id="bib.bib43.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and
Wang-chun Woo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Convolutional LSTM network: A machine learning approach for
precipitation nowcasting.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib44.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Conf. Neural Inf. Process. Syst. (NeurIPS)</span><span id="bib.bib44.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Sumit Bam Shrestha and Garrick Orchard.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">SLAYER: Spike layer error reassignment in time.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib45.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Conf. Neural Inf. Process. Syst. (NeurIPS)</span><span id="bib.bib45.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Leslie N. Smith and Nicholay Topin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">Super-convergence: Very fast training of neural networks using large
learning rates, 2017.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Aboozar Taherkhani, Ammar Belatreche, Yuhua Li, Georgina Cosma, Liam P.
Maguire, and T.M. McGinnity.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">A review of learning in biologically plausible spiking neural
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Neural Netw.</span><span id="bib.bib47.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
Zhan Tong, Yibing Song, Jue Wang, and Limin Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">VideoMAE: Masked autoencoders are data-efficient learners for
self-supervised video pre-training.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib48.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Conf. Neural Inf. Process. Syst. (NeurIPS)</span><span id="bib.bib48.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and
Hervé Jégou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">Going deeper with image transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib49.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. Conf. Comput. Vis. (ICCV)</span><span id="bib.bib49.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan
Bovik, and Yinxiao Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">MaxViT: Multi-axis vision transformer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib50.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Eur. Conf. Comput. Vis. (ECCV)</span><span id="bib.bib50.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text" style="font-size:90%;">
Stepan Tulyakov, Francois Fleuret, Martin Kiefel, Peter Gehler, and Michael
Hirsch.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.2.1" class="ltx_text" style="font-size:90%;">Learning an event sequence embedding for dense event-based deep
stereo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib51.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. Conf. Comput. Vis. (ICCV)</span><span id="bib.bib51.5.3" class="ltx_text" style="font-size:90%;">, October 2019.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text" style="font-size:90%;">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.2.1" class="ltx_text" style="font-size:90%;">Attention is all you need.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.3.1" class="ltx_text" style="font-size:90%;">In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S.
Vishwanathan, and R. Garnett, editors, </span><span id="bib.bib52.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Conf. Neural Inf. Process. Syst.
(NeurIPS)</span><span id="bib.bib52.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text" style="font-size:90%;">
Zuowen Wang, Yuhuang Hu, and Shih-Chii Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.2.1" class="ltx_text" style="font-size:90%;">Exploiting spatial sparsity for event cameras with visual
transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib53.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Int. Conf. Image Process. (ICIP)</span><span id="bib.bib53.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text" style="font-size:90%;">
Wenming Weng, Yueyi Zhang, and Zhiwei Xiong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.2.1" class="ltx_text" style="font-size:90%;">Event-based video reconstruction using transformer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib54.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. Conf. Comput. Vis. (ICCV)</span><span id="bib.bib54.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text" style="font-size:90%;">
Man Yao, Huanhuan Gao, Guangshe Zhao, Dingheng Wang, Yihan Lin, Zhaoxu Yang,
and Guoqi Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.2.1" class="ltx_text" style="font-size:90%;">Temporal-wise attention spiking neural networks for event streams
classification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. Conf. Comput. Vis. (ICCV)</span><span id="bib.bib55.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text" style="font-size:90%;">
Friedemann Zenke and Surya Ganguli.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.2.1" class="ltx_text" style="font-size:90%;">SuperSpike: Supervised learning in multilayer spiking neural
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Neural Computation</span><span id="bib.bib56.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text" style="font-size:90%;">
Jiqing Zhang, Bo Dong, Haiwei Zhang, Jianchuan Ding, Felix Heide, Baocai Yin,
and Xin Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.2.1" class="ltx_text" style="font-size:90%;">Spiking transformers for event-based single object tracking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib57.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</span><span id="bib.bib57.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock"><span id="bib.bib58.1.1" class="ltx_text" style="font-size:90%;">
Alex Zihao Zhu, Liangzhe Yuan, Kenneth Chaney, and Kostas Daniilidis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.2.1" class="ltx_text" style="font-size:90%;">Unsupervised event-based learning of optical flow, depth, and
egomotion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib58.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</span><span id="bib.bib58.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2212.05596" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2212.05598" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2212.05598">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2212.05598" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2212.05599" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 11:33:30 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
