<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2309.11851] DEYOv3: DETR with YOLO for Real-time Object Detection</title><meta property="og:description" content="Recently, end-to-end object detectors have gained significant attention from the research community due to their outstanding performance. However, DETR typically relies on supervised pretraining of the backbone on Imag…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="DEYOv3: DETR with YOLO for Real-time Object Detection">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="DEYOv3: DETR with YOLO for Real-time Object Detection">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2309.11851">

<!--Generated on Wed Feb 28 05:00:39 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">DEYOv3: DETR with YOLO for Real-time Object Detection</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Haodong Ouyang
<br class="ltx_break">Southwest Minzu University
<br class="ltx_break">Chengdu, China
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">ouyanghaodong@stu.swun.edu.cn</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Recently, end-to-end object detectors have gained significant attention from the research community due to their outstanding performance. However, DETR typically relies on supervised pretraining of the backbone on ImageNet, which limits the practical application of DETR and the design of the backbone, affecting the model’s potential generalization ability. In this paper, we propose a new training method called step-by-step training. Specifically, in the first stage, the one-to-many pre-trained YOLO detector is used to initialize the end-to-end detector. In the second stage, the backbone and encoder are consistent with the DETR-like model, but only the detector needs to be trained from scratch. Due to this training method, the object detector does not need the additional dataset (ImageNet) to train the backbone, which makes the design of the backbone more flexible and dramatically reduces the training cost of the detector, which is helpful for the practical application of the object detector. At the same time, compared with the DETR-like model, the step-by-step training method can achieve higher accuracy than the traditional training method of the DETR-like model. With the aid of this novel training method, we propose a brand-new end-to-end real-time object detection model called DEYOv3. DEYOv3-N achieves 41.1% on COCO <span id="id2.id1.1" class="ltx_text ltx_font_typewriter">val2017</span> and 270 FPS on T4 GPU, while DEYOv3-L achieves 51.3% AP and 102 FPS. Without the use of additional training data, DEYOv3 surpasses all existing real-time object detectors in terms of both speed and accuracy. It is worth noting that for models of N, S, and M scales, the training on the COCO dataset can be completed using a single 24GB RTX3090 GPU. Code will be released at <a target="_blank" href="https://github.com/ouyanghaodong/DEYOv3" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/ouyanghaodong/DEYOv3</a>.</p>
</div>
<figure id="S0.F1" class="ltx_figure"><img src="/html/2309.11851/assets/1.png" id="S0.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="386" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S0.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S0.F1.3.2" class="ltx_text" style="font-size:90%;">DEYOv3 surpasses other real-time object detectors in terms of speed and accuracy, establishing itself as a SOTA solution. Notably, all detectors were trained solely on the COCO dataset without using any additional datasets.</span></figcaption>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Object detection is a fundamental task in computer vision, aiming at accurately locating and recognizing multiple different classes of objects from images or videos. Object detection is the basis of many computer vision applications, including intelligent driving, video surveillance, face recognition, object tracking, etc. In recent years, deep learning methods, especially methods based on convolutional neural networks (CNN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>]</cite>, have made significant progress in object detection tasks and have become mainstream technical means. Real-time object detection is an essential topic in object detection, aiming at the task of quickly and accurately detecting and identifying objects in images or videos in real-time scenes. Compared with traditional object detection methods, real-time object detection requires faster processing speed and the ability to detect objects in real-time or near real-time. Existing real-time detectors generally adopt CNN-based architecture, which provides a good balance between accuracy and speed. Among them, one of the representatives of real-time detectors is YOLO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>, <a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>, <a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite> (YOLO Only Look Once). After years of development, YOLO has developed into a series of fast models with good performance.</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2309.11851/assets/2.png" id="S1.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="550" height="63" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.3.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S1.F2.4.2" class="ltx_text" style="font-size:90%;">We showcase two distinct training strategies: DETR Strategy and Step-by-step Training. The DETR Strategy heavily relies on the ImageNet pre-trained backbone, while Step-by-step Training eliminates the need for additional datasets. Instead, it initializes DEYOv3’s parameters by leveraging the pre-trained detector’s backbone and multi-scale layers.</span></figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Traditional detectors often require post-processing with non-maximum suppression (NMS). The effectiveness of NMS can be influenced by the chosen IoU threshold, which may lead to significant variations in the detection results. In crowded scenes, it can become a performance bottleneck for classical detectors and introduce inference latency for real-time detection. The Detection Transformer (DETR) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite> proposes an innovative transformer-based <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite> object detector that leverages a transformer encoder-decoder framework that eliminates the manual components of NMS and instead utilizes the Hungarian loss to predict one-to-one sets of objects, enabling end-to-end optimization. Despite numerous works to improve DETR in recent years, the issue of high computational cost has remained unresolved, limiting the practical application and underutilization of its advantages. This means that although the object detection process is simplified, the high computational cost of the DETR model makes it challenging to achieve real-time object detection. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite> re-evaluate DETR, reduce unnecessary computational redundancy in the DETR encoder, and propose the first end-to-end object detector, RT-DETR, fully exploiting the advantages of an end-to-end detection pipeline.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">However, DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite> typically relies on supervised pretraining of the backbone on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite>, as well as random initialization of the transformer encoder and decoder. If one intends to use a new backbone, a pre-trained backbone needs to be selected from ImageNet. Alternatively, after designing the backbone, it must be pre-trained on ImageNet before training DETR. This limits the design of the backbone and significantly increases training costs. Moreover, the performance and effectiveness of the model heavily depend on the dataset used for pretraining. If the current task’s dataset differs significantly from the ImageNet, fine-tuning DETR may not fully adapt to specific tasks, resulting in degraded performance and limiting the robustness and generalization of DETR. To enhance the practicality of the DETR model, we propose a novel training method called step-by-step training. Specifically, in the first stage of training, we perform pre-training for one-to-many matching using YOLO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>, <a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>, <a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite>. In the second stage of training, we utilize the backbone and neck of YOLO to initialize the backbone and encoder of the real-time end-to-end detector, while the decoder is randomly initialized for fine-tuning the one-to-one matching. Our training does not require additional datasets; only an object detection dataset is needed to complete the two stages of training. Additionally, due to the high-quality one-to-many matching pretraining performed by the multi-scale layers in the first stage, our method achieves higher accuracy without affecting inference time compared to the DETR training approach.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Furthermore, we propose a novel real-time object detection model, DEYOv3, based on a stepwise training approach. DEYOv3 eliminates the need for NMS, ensuring that the inference speed of the detector remains unaffected and stable. DEYOv3-N achieves 41.1% AP on COCO <span id="S1.p4.1.1" class="ltx_text ltx_font_typewriter">val2017</span> and runs at 270 FPS on NVIDIA Tesla T4 GPU, while DEYOv3-L achieves 51.3% AP and 102 FPS. Without using additional training data, DEYOv3 outperforms all real-time detectors of similar scale in terms of both speed and accuracy, establishing itself as the new SOTA for real-time object detection.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The main contributions of this paper can be summarized as follows: (1) we propose a novel training method, step-by-step training, for the DETR model. Compared to traditional DETR training methods, it eliminates the need for additional datasets for pretraining and enables the model to achieve higher accuracy; (2) leveraging step-by-step training, we develop DEYOv3, the state-of-the-art real-time object detector; (3) we conduct a series of ablation experiments to thoroughly analyze DEYOv3 and discuss its potential as a feasible design approach for future large-scale object detection models.</p>
</div>
<figure id="S1.F3" class="ltx_figure"><img src="/html/2309.11851/assets/3.png" id="S1.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="253" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F3.3.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S1.F3.4.2" class="ltx_text" style="font-size:90%;">We eliminated the encoder usage and instead employed the multi-scale features {P3, P4, P5} provided by the neck. Following feature projection, these features were utilized as input for the encoder while simultaneously generating candidate bounding boxes and filtering them through the query selector. Subsequently, this information was passed into a decoder with an auxiliary prediction head, enabling iterative optimization for generating bounding boxes and scores.</span></figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Step-by-step Training</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Due to the Hungarian matching employed by DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite> for directly predicting one-to-one sets of objects, as well as the quadratic relationship between the complexity of the DETR decoder and sequence length, DETR achieves query numbers several orders of magnitude lower than classical detectors. We believe that this situation leads to relatively fewer supervisory signals during training for DETR, making it challenging to obtain good performance when training from scratch and thus highly reliant on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite> pretraining of the backbone. Furthermore, the multi-scale layers of the ImageNet pretraining have not been effectively pretrained specifically for the object detection task.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">To address these issues, we divide the training process of DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite> into two stages. Through extensive engineering tests, the computer vision community has thoroughly validated the generality and practicality of YOLO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>, <a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>, <a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite>. These assessments include testing and evaluating YOLO in various real-world scenarios to assess its performance across different datasets, object categories, and environmental conditions. Based on these extensive engineering tests, we believe that even without using additional datasets, YOLO can still perform well in handling complex scenes, multi-object detection, and real-time applications while demonstrating excellent generalization and practicality. Undoubtedly, training YOLO from scratch is the optimal choice for its training strategy. Therefore, in the first stage of training, we initially train a powerful YOLO object detection model to provide high-quality pretraining for the backbone and multi-scale layers of the DEYOv3 model. Thanks to the one-to-many matching and the capability of YOLO to provide thousands of queries, a large amount of supervised signals enables the network to learn rich feature representations, thus providing better initial feature expression capabilities for the second stage of training.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">As the first stage provides high-quality pretraining for the backbone and neck of the DEYOv3 model, in the second stage of training, only the decoder needs to be trained from scratch, further accelerating the convergence speed of the model. Additionally, compared to the multi-scale layers pre-trained on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite>, the neck pre-trained in the first stage can provide higher-quality features. Compared to the traditional training method of DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite>, our training strategy enables our model to exhibit better performance.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>DEYOv3</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Model Overview</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Based on the unreleased yolov8-rtdetr by Ultralytics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>, we construct the DEYOv3 model. As YOLO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>, <a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>, <a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite> and DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite> share the same backbone and neck, it is straightforward to present the DEYO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>, <a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite> paradigm in a lightweight manner for real-time object detection. Figure <a href="#S1.F3" title="Figure 3 ‣ 1 Introduction ‣ DEYOv3: DETR with YOLO for Real-time Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates the overall structure of DEYOv3. DEYOv3 utilizes YOLOv8 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite> as a one-to-many branch of the model. YOLOv8 is an improved version of YOLOv5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite> introduced by Ultralytics, consisting of a backbone, a neck structure composed of Feature Pyramid Network (FPN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a>]</cite> and Path Aggregation Network (PAN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite>, and a head that outputs predictions at three scales. The one-to-one branch of DEYOv3 employs a decoder similar to DINO, different from the previous DEYO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>, <a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite> model, which does not use a transformer as an encoder like DETR. Instead, it encodes multi-scale features using a simple neck structure and feature projection method. The structure of feature projection is straightforward, consisting of a simple 1x1 convolution. This design makes DEYOv3 more lightweight during the encoding phase, improving the model’s runtime efficiency. Additionally, DEYOv3’s decoder adopts static query and dynamic initialization methods for anchor bounding boxes. Furthermore, DEYOv3 introduces an additional CDN (Contrastive Denoising Training) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite> branch.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<div id="S3.T1.11.11" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:472.0pt;height:451.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-83.8pt,80.2pt) scale(0.737891603467109,0.737891603467109) ;">
<table id="S3.T1.11.11.11" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.11.11.11.11" class="ltx_tr">
<td id="S3.T1.11.11.11.11.12" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T1.11.11.11.11.12.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S3.T1.11.11.11.11.13" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.11.11.11.11.13.1" class="ltx_text ltx_font_bold">Backbone</span></td>
<td id="S3.T1.11.11.11.11.14" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.11.11.11.11.14.1" class="ltx_text ltx_font_bold">Epochs</span></td>
<td id="S3.T1.11.11.11.11.15" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.11.11.11.11.15.1" class="ltx_text ltx_font_bold">#Params (M)</span></td>
<td id="S3.T1.11.11.11.11.16" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.11.11.11.11.16.1" class="ltx_text ltx_font_bold">GFLOPs</span></td>
<td id="S3.T1.4.4.4.4.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.4.4.4.4.4.4" class="ltx_text ltx_font_bold">FPS<sub id="S3.T1.4.4.4.4.4.4.1" class="ltx_sub"><span id="S3.T1.4.4.4.4.4.4.1.1" class="ltx_text ltx_font_medium ltx_font_italic">b</span></sub><sub id="S3.T1.4.4.4.4.4.4.2" class="ltx_sub"><span id="S3.T1.4.4.4.4.4.4.2.1" class="ltx_text ltx_font_medium ltx_font_italic">s</span></sub><sub id="S3.T1.4.4.4.4.4.4.3" class="ltx_sub"><span id="S3.T1.4.4.4.4.4.4.3.1" class="ltx_text ltx_font_medium">=</span></sub><sub id="S3.T1.4.4.4.4.4.4.4" class="ltx_sub"><span id="S3.T1.4.4.4.4.4.4.4.1" class="ltx_text ltx_font_medium">1</span></sub></span></td>
<td id="S3.T1.11.11.11.11.17" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.11.11.11.11.17.1" class="ltx_text ltx_font_bold">AP</span></td>
<td id="S3.T1.6.6.6.6.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.6.6.6.6.6.2" class="ltx_text ltx_font_bold">AP<sub id="S3.T1.6.6.6.6.6.2.1" class="ltx_sub"><span id="S3.T1.6.6.6.6.6.2.1.1" class="ltx_text ltx_font_medium">5</span></sub><sub id="S3.T1.6.6.6.6.6.2.2" class="ltx_sub"><span id="S3.T1.6.6.6.6.6.2.2.1" class="ltx_text ltx_font_medium">0</span></sub></span></td>
<td id="S3.T1.8.8.8.8.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.8.8.8.8.8.2" class="ltx_text ltx_font_bold">AP<sub id="S3.T1.8.8.8.8.8.2.1" class="ltx_sub"><span id="S3.T1.8.8.8.8.8.2.1.1" class="ltx_text ltx_font_medium">7</span></sub><sub id="S3.T1.8.8.8.8.8.2.2" class="ltx_sub"><span id="S3.T1.8.8.8.8.8.2.2.1" class="ltx_text ltx_font_medium">5</span></sub></span></td>
<td id="S3.T1.9.9.9.9.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.9.9.9.9.9.1" class="ltx_text ltx_font_bold">AP<sub id="S3.T1.9.9.9.9.9.1.1" class="ltx_sub"><span id="S3.T1.9.9.9.9.9.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">S</span></sub></span></td>
<td id="S3.T1.10.10.10.10.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.10.10.10.10.10.1" class="ltx_text ltx_font_bold">AP<sub id="S3.T1.10.10.10.10.10.1.1" class="ltx_sub"><span id="S3.T1.10.10.10.10.10.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">M</span></sub></span></td>
<td id="S3.T1.11.11.11.11.11" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.11.11.11.11.11.1" class="ltx_text ltx_font_bold">AP<sub id="S3.T1.11.11.11.11.11.1.1" class="ltx_sub"><span id="S3.T1.11.11.11.11.11.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">L</span></sub></span></td>
</tr>
<tr id="S3.T1.11.11.11.12.1" class="ltx_tr" style="background-color:#E6E6E6;">
<td id="S3.T1.11.11.11.12.1.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T1.11.11.11.12.1.1.1" class="ltx_text ltx_font_italic" style="background-color:#E6E6E6;">Real-time Object Detectors</span></td>
<td id="S3.T1.11.11.11.12.1.2" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.11.11.11.12.1.3" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.11.11.11.12.1.4" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.11.11.11.12.1.5" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.11.11.11.12.1.6" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.11.11.11.12.1.7" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.11.11.11.12.1.8" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.11.11.11.12.1.9" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.11.11.11.12.1.10" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.11.11.11.12.1.11" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.11.11.11.12.1.12" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S3.T1.11.11.11.13.2" class="ltx_tr">
<td id="S3.T1.11.11.11.13.2.1" class="ltx_td ltx_align_left">YOLOv5-N <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite>
</td>
<td id="S3.T1.11.11.11.13.2.2" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.11.11.11.13.2.3" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.11.11.11.13.2.4" class="ltx_td ltx_align_center">2</td>
<td id="S3.T1.11.11.11.13.2.5" class="ltx_td ltx_align_center">5</td>
<td id="S3.T1.11.11.11.13.2.6" class="ltx_td ltx_align_center">76</td>
<td id="S3.T1.11.11.11.13.2.7" class="ltx_td ltx_align_center">28.0</td>
<td id="S3.T1.11.11.11.13.2.8" class="ltx_td ltx_align_center">46.2</td>
<td id="S3.T1.11.11.11.13.2.9" class="ltx_td ltx_align_center">29.2</td>
<td id="S3.T1.11.11.11.13.2.10" class="ltx_td ltx_align_center">14.1</td>
<td id="S3.T1.11.11.11.13.2.11" class="ltx_td ltx_align_center">32.2</td>
<td id="S3.T1.11.11.11.13.2.12" class="ltx_td ltx_align_center">36.7</td>
</tr>
<tr id="S3.T1.11.11.11.14.3" class="ltx_tr">
<td id="S3.T1.11.11.11.14.3.1" class="ltx_td ltx_align_left">YOLOv5-S <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite>
</td>
<td id="S3.T1.11.11.11.14.3.2" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.11.11.11.14.3.3" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.11.11.11.14.3.4" class="ltx_td ltx_align_center">7</td>
<td id="S3.T1.11.11.11.14.3.5" class="ltx_td ltx_align_center">17</td>
<td id="S3.T1.11.11.11.14.3.6" class="ltx_td ltx_align_center">73</td>
<td id="S3.T1.11.11.11.14.3.7" class="ltx_td ltx_align_center">37.4</td>
<td id="S3.T1.11.11.11.14.3.8" class="ltx_td ltx_align_center">57.2</td>
<td id="S3.T1.11.11.11.14.3.9" class="ltx_td ltx_align_center">40.2</td>
<td id="S3.T1.11.11.11.14.3.10" class="ltx_td ltx_align_center">21.1</td>
<td id="S3.T1.11.11.11.14.3.11" class="ltx_td ltx_align_center">42.3</td>
<td id="S3.T1.11.11.11.14.3.12" class="ltx_td ltx_align_center">49.0</td>
</tr>
<tr id="S3.T1.11.11.11.15.4" class="ltx_tr">
<td id="S3.T1.11.11.11.15.4.1" class="ltx_td ltx_align_left">YOLOv5-M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite>
</td>
<td id="S3.T1.11.11.11.15.4.2" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.11.11.11.15.4.3" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.11.11.11.15.4.4" class="ltx_td ltx_align_center">21</td>
<td id="S3.T1.11.11.11.15.4.5" class="ltx_td ltx_align_center">49</td>
<td id="S3.T1.11.11.11.15.4.6" class="ltx_td ltx_align_center">64</td>
<td id="S3.T1.11.11.11.15.4.7" class="ltx_td ltx_align_center">45.4</td>
<td id="S3.T1.11.11.11.15.4.8" class="ltx_td ltx_align_center">64.4</td>
<td id="S3.T1.11.11.11.15.4.9" class="ltx_td ltx_align_center">48.9</td>
<td id="S3.T1.11.11.11.15.4.10" class="ltx_td ltx_align_center">27.8</td>
<td id="S3.T1.11.11.11.15.4.11" class="ltx_td ltx_align_center">50.4</td>
<td id="S3.T1.11.11.11.15.4.12" class="ltx_td ltx_align_center">58.1</td>
</tr>
<tr id="S3.T1.11.11.11.16.5" class="ltx_tr">
<td id="S3.T1.11.11.11.16.5.1" class="ltx_td ltx_align_left">YOLOv5-L <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite>
</td>
<td id="S3.T1.11.11.11.16.5.2" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.11.11.11.16.5.3" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.11.11.11.16.5.4" class="ltx_td ltx_align_center">47</td>
<td id="S3.T1.11.11.11.16.5.5" class="ltx_td ltx_align_center">109</td>
<td id="S3.T1.11.11.11.16.5.6" class="ltx_td ltx_align_center">57</td>
<td id="S3.T1.11.11.11.16.5.7" class="ltx_td ltx_align_center">49.0</td>
<td id="S3.T1.11.11.11.16.5.8" class="ltx_td ltx_align_center">67.6</td>
<td id="S3.T1.11.11.11.16.5.9" class="ltx_td ltx_align_center">53.1</td>
<td id="S3.T1.11.11.11.16.5.10" class="ltx_td ltx_align_center">31.8</td>
<td id="S3.T1.11.11.11.16.5.11" class="ltx_td ltx_align_center">54.4</td>
<td id="S3.T1.11.11.11.16.5.12" class="ltx_td ltx_align_center">62.3</td>
</tr>
<tr id="S3.T1.11.11.11.17.6" class="ltx_tr">
<td id="S3.T1.11.11.11.17.6.1" class="ltx_td ltx_align_left">YOLOv5-X <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite>
</td>
<td id="S3.T1.11.11.11.17.6.2" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.11.11.11.17.6.3" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.11.11.11.17.6.4" class="ltx_td ltx_align_center">87</td>
<td id="S3.T1.11.11.11.17.6.5" class="ltx_td ltx_align_center">206</td>
<td id="S3.T1.11.11.11.17.6.6" class="ltx_td ltx_align_center">44</td>
<td id="S3.T1.11.11.11.17.6.7" class="ltx_td ltx_align_center">50.7</td>
<td id="S3.T1.11.11.11.17.6.8" class="ltx_td ltx_align_center">68.9</td>
<td id="S3.T1.11.11.11.17.6.9" class="ltx_td ltx_align_center">54.6</td>
<td id="S3.T1.11.11.11.17.6.10" class="ltx_td ltx_align_center">33.8</td>
<td id="S3.T1.11.11.11.17.6.11" class="ltx_td ltx_align_center">55.7</td>
<td id="S3.T1.11.11.11.17.6.12" class="ltx_td ltx_align_center">65.0</td>
</tr>
<tr id="S3.T1.11.11.11.18.7" class="ltx_tr">
<td id="S3.T1.11.11.11.18.7.1" class="ltx_td ltx_align_left">YOLOv8-N <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>
</td>
<td id="S3.T1.11.11.11.18.7.2" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.11.11.11.18.7.3" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.11.11.11.18.7.4" class="ltx_td ltx_align_center">3</td>
<td id="S3.T1.11.11.11.18.7.5" class="ltx_td ltx_align_center">9</td>
<td id="S3.T1.11.11.11.18.7.6" class="ltx_td ltx_align_center">159</td>
<td id="S3.T1.11.11.11.18.7.7" class="ltx_td ltx_align_center">37.3</td>
<td id="S3.T1.11.11.11.18.7.8" class="ltx_td ltx_align_center">52.5</td>
<td id="S3.T1.11.11.11.18.7.9" class="ltx_td ltx_align_center">40.5</td>
<td id="S3.T1.11.11.11.18.7.10" class="ltx_td ltx_align_center">18.6</td>
<td id="S3.T1.11.11.11.18.7.11" class="ltx_td ltx_align_center">41.0</td>
<td id="S3.T1.11.11.11.18.7.12" class="ltx_td ltx_align_center">53.5</td>
</tr>
<tr id="S3.T1.11.11.11.19.8" class="ltx_tr">
<td id="S3.T1.11.11.11.19.8.1" class="ltx_td ltx_align_left">YOLOv8-S <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>
</td>
<td id="S3.T1.11.11.11.19.8.2" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.11.11.11.19.8.3" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.11.11.11.19.8.4" class="ltx_td ltx_align_center">11</td>
<td id="S3.T1.11.11.11.19.8.5" class="ltx_td ltx_align_center">29</td>
<td id="S3.T1.11.11.11.19.8.6" class="ltx_td ltx_align_center">139</td>
<td id="S3.T1.11.11.11.19.8.7" class="ltx_td ltx_align_center">44.9</td>
<td id="S3.T1.11.11.11.19.8.8" class="ltx_td ltx_align_center">61.8</td>
<td id="S3.T1.11.11.11.19.8.9" class="ltx_td ltx_align_center">48.6</td>
<td id="S3.T1.11.11.11.19.8.10" class="ltx_td ltx_align_center">25.7</td>
<td id="S3.T1.11.11.11.19.8.11" class="ltx_td ltx_align_center">49.9</td>
<td id="S3.T1.11.11.11.19.8.12" class="ltx_td ltx_align_center">61.0</td>
</tr>
<tr id="S3.T1.11.11.11.20.9" class="ltx_tr">
<td id="S3.T1.11.11.11.20.9.1" class="ltx_td ltx_align_left">YOLOv8-M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>
</td>
<td id="S3.T1.11.11.11.20.9.2" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.11.11.11.20.9.3" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.11.11.11.20.9.4" class="ltx_td ltx_align_center">26</td>
<td id="S3.T1.11.11.11.20.9.5" class="ltx_td ltx_align_center">79</td>
<td id="S3.T1.11.11.11.20.9.6" class="ltx_td ltx_align_center">103</td>
<td id="S3.T1.11.11.11.20.9.7" class="ltx_td ltx_align_center">50.2</td>
<td id="S3.T1.11.11.11.20.9.8" class="ltx_td ltx_align_center">67.2</td>
<td id="S3.T1.11.11.11.20.9.9" class="ltx_td ltx_align_center">54.6</td>
<td id="S3.T1.11.11.11.20.9.10" class="ltx_td ltx_align_center">32.0</td>
<td id="S3.T1.11.11.11.20.9.11" class="ltx_td ltx_align_center">55.8</td>
<td id="S3.T1.11.11.11.20.9.12" class="ltx_td ltx_align_center">66.4</td>
</tr>
<tr id="S3.T1.11.11.11.21.10" class="ltx_tr">
<td id="S3.T1.11.11.11.21.10.1" class="ltx_td ltx_align_left">YOLOv8-L <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>
</td>
<td id="S3.T1.11.11.11.21.10.2" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.11.11.11.21.10.3" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.11.11.11.21.10.4" class="ltx_td ltx_align_center">44</td>
<td id="S3.T1.11.11.11.21.10.5" class="ltx_td ltx_align_center">165</td>
<td id="S3.T1.11.11.11.21.10.6" class="ltx_td ltx_align_center">78</td>
<td id="S3.T1.11.11.11.21.10.7" class="ltx_td ltx_align_center">52.9</td>
<td id="S3.T1.11.11.11.21.10.8" class="ltx_td ltx_align_center">69.8</td>
<td id="S3.T1.11.11.11.21.10.9" class="ltx_td ltx_align_center">57.5</td>
<td id="S3.T1.11.11.11.21.10.10" class="ltx_td ltx_align_center">35.3</td>
<td id="S3.T1.11.11.11.21.10.11" class="ltx_td ltx_align_center">58.3</td>
<td id="S3.T1.11.11.11.21.10.12" class="ltx_td ltx_align_center">69.8</td>
</tr>
<tr id="S3.T1.11.11.11.22.11" class="ltx_tr">
<td id="S3.T1.11.11.11.22.11.1" class="ltx_td ltx_align_left">YOLOv8-X <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>
</td>
<td id="S3.T1.11.11.11.22.11.2" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.11.11.11.22.11.3" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.11.11.11.22.11.4" class="ltx_td ltx_align_center">68</td>
<td id="S3.T1.11.11.11.22.11.5" class="ltx_td ltx_align_center">258</td>
<td id="S3.T1.11.11.11.22.11.6" class="ltx_td ltx_align_center">59</td>
<td id="S3.T1.11.11.11.22.11.7" class="ltx_td ltx_align_center">53.9</td>
<td id="S3.T1.11.11.11.22.11.8" class="ltx_td ltx_align_center">71.0</td>
<td id="S3.T1.11.11.11.22.11.9" class="ltx_td ltx_align_center">58.7</td>
<td id="S3.T1.11.11.11.22.11.10" class="ltx_td ltx_align_center">35.7</td>
<td id="S3.T1.11.11.11.22.11.11" class="ltx_td ltx_align_center">59.3</td>
<td id="S3.T1.11.11.11.22.11.12" class="ltx_td ltx_align_center">70.7</td>
</tr>
<tr id="S3.T1.11.11.11.23.12" class="ltx_tr" style="background-color:#E6E6E6;">
<td id="S3.T1.11.11.11.23.12.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T1.11.11.11.23.12.1.1" class="ltx_text ltx_font_italic" style="background-color:#E6E6E6;">End-to-end Object Detectors</span></td>
<td id="S3.T1.11.11.11.23.12.2" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.11.11.11.23.12.3" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.11.11.11.23.12.4" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.11.11.11.23.12.5" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.11.11.11.23.12.6" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.11.11.11.23.12.7" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.11.11.11.23.12.8" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.11.11.11.23.12.9" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.11.11.11.23.12.10" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.11.11.11.23.12.11" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.11.11.11.23.12.12" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S3.T1.11.11.11.24.13" class="ltx_tr">
<td id="S3.T1.11.11.11.24.13.1" class="ltx_td ltx_align_left">DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite>
</td>
<td id="S3.T1.11.11.11.24.13.2" class="ltx_td ltx_align_center">R50</td>
<td id="S3.T1.11.11.11.24.13.3" class="ltx_td ltx_align_center">500</td>
<td id="S3.T1.11.11.11.24.13.4" class="ltx_td ltx_align_center">41</td>
<td id="S3.T1.11.11.11.24.13.5" class="ltx_td ltx_align_center">187</td>
<td id="S3.T1.11.11.11.24.13.6" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.11.11.11.24.13.7" class="ltx_td ltx_align_center">43.3</td>
<td id="S3.T1.11.11.11.24.13.8" class="ltx_td ltx_align_center">63.1</td>
<td id="S3.T1.11.11.11.24.13.9" class="ltx_td ltx_align_center">45.9</td>
<td id="S3.T1.11.11.11.24.13.10" class="ltx_td ltx_align_center">22.5</td>
<td id="S3.T1.11.11.11.24.13.11" class="ltx_td ltx_align_center">47.3</td>
<td id="S3.T1.11.11.11.24.13.12" class="ltx_td ltx_align_center">61.1</td>
</tr>
<tr id="S3.T1.11.11.11.25.14" class="ltx_tr">
<td id="S3.T1.11.11.11.25.14.1" class="ltx_td ltx_align_left">Anchor-DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite>
</td>
<td id="S3.T1.11.11.11.25.14.2" class="ltx_td ltx_align_center">R50</td>
<td id="S3.T1.11.11.11.25.14.3" class="ltx_td ltx_align_center">50</td>
<td id="S3.T1.11.11.11.25.14.4" class="ltx_td ltx_align_center">39</td>
<td id="S3.T1.11.11.11.25.14.5" class="ltx_td ltx_align_center">172</td>
<td id="S3.T1.11.11.11.25.14.6" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.11.11.11.25.14.7" class="ltx_td ltx_align_center">44.2</td>
<td id="S3.T1.11.11.11.25.14.8" class="ltx_td ltx_align_center">64.7</td>
<td id="S3.T1.11.11.11.25.14.9" class="ltx_td ltx_align_center">47.7</td>
<td id="S3.T1.11.11.11.25.14.10" class="ltx_td ltx_align_center">23.7</td>
<td id="S3.T1.11.11.11.25.14.11" class="ltx_td ltx_align_center">49.5</td>
<td id="S3.T1.11.11.11.25.14.12" class="ltx_td ltx_align_center">62.3</td>
</tr>
<tr id="S3.T1.11.11.11.26.15" class="ltx_tr">
<td id="S3.T1.11.11.11.26.15.1" class="ltx_td ltx_align_left">Conditional-DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite>
</td>
<td id="S3.T1.11.11.11.26.15.2" class="ltx_td ltx_align_center">R50</td>
<td id="S3.T1.11.11.11.26.15.3" class="ltx_td ltx_align_center">108</td>
<td id="S3.T1.11.11.11.26.15.4" class="ltx_td ltx_align_center">44</td>
<td id="S3.T1.11.11.11.26.15.5" class="ltx_td ltx_align_center">195</td>
<td id="S3.T1.11.11.11.26.15.6" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.11.11.11.26.15.7" class="ltx_td ltx_align_center">45.1</td>
<td id="S3.T1.11.11.11.26.15.8" class="ltx_td ltx_align_center">65.4</td>
<td id="S3.T1.11.11.11.26.15.9" class="ltx_td ltx_align_center">48.5</td>
<td id="S3.T1.11.11.11.26.15.10" class="ltx_td ltx_align_center">25.3</td>
<td id="S3.T1.11.11.11.26.15.11" class="ltx_td ltx_align_center">49.0</td>
<td id="S3.T1.11.11.11.26.15.12" class="ltx_td ltx_align_center">62.2</td>
</tr>
<tr id="S3.T1.11.11.11.27.16" class="ltx_tr">
<td id="S3.T1.11.11.11.27.16.1" class="ltx_td ltx_align_left">Efficient-DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite>
</td>
<td id="S3.T1.11.11.11.27.16.2" class="ltx_td ltx_align_center">R50</td>
<td id="S3.T1.11.11.11.27.16.3" class="ltx_td ltx_align_center">36</td>
<td id="S3.T1.11.11.11.27.16.4" class="ltx_td ltx_align_center">35</td>
<td id="S3.T1.11.11.11.27.16.5" class="ltx_td ltx_align_center">210</td>
<td id="S3.T1.11.11.11.27.16.6" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.11.11.11.27.16.7" class="ltx_td ltx_align_center">45.1</td>
<td id="S3.T1.11.11.11.27.16.8" class="ltx_td ltx_align_center">63.1</td>
<td id="S3.T1.11.11.11.27.16.9" class="ltx_td ltx_align_center">49.1</td>
<td id="S3.T1.11.11.11.27.16.10" class="ltx_td ltx_align_center">28.3</td>
<td id="S3.T1.11.11.11.27.16.11" class="ltx_td ltx_align_center">48.4</td>
<td id="S3.T1.11.11.11.27.16.12" class="ltx_td ltx_align_center">59.0</td>
</tr>
<tr id="S3.T1.11.11.11.28.17" class="ltx_tr">
<td id="S3.T1.11.11.11.28.17.1" class="ltx_td ltx_align_left">SMCA-DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>]</cite>
</td>
<td id="S3.T1.11.11.11.28.17.2" class="ltx_td ltx_align_center">R50</td>
<td id="S3.T1.11.11.11.28.17.3" class="ltx_td ltx_align_center">108</td>
<td id="S3.T1.11.11.11.28.17.4" class="ltx_td ltx_align_center">40</td>
<td id="S3.T1.11.11.11.28.17.5" class="ltx_td ltx_align_center">152</td>
<td id="S3.T1.11.11.11.28.17.6" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.11.11.11.28.17.7" class="ltx_td ltx_align_center">45.6</td>
<td id="S3.T1.11.11.11.28.17.8" class="ltx_td ltx_align_center">65.5</td>
<td id="S3.T1.11.11.11.28.17.9" class="ltx_td ltx_align_center">49.1</td>
<td id="S3.T1.11.11.11.28.17.10" class="ltx_td ltx_align_center">25.9</td>
<td id="S3.T1.11.11.11.28.17.11" class="ltx_td ltx_align_center">49.3</td>
<td id="S3.T1.11.11.11.28.17.12" class="ltx_td ltx_align_center">62.6</td>
</tr>
<tr id="S3.T1.11.11.11.29.18" class="ltx_tr">
<td id="S3.T1.11.11.11.29.18.1" class="ltx_td ltx_align_left">Deformable-DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite>
</td>
<td id="S3.T1.11.11.11.29.18.2" class="ltx_td ltx_align_center">R50</td>
<td id="S3.T1.11.11.11.29.18.3" class="ltx_td ltx_align_center">50</td>
<td id="S3.T1.11.11.11.29.18.4" class="ltx_td ltx_align_center">40</td>
<td id="S3.T1.11.11.11.29.18.5" class="ltx_td ltx_align_center">173</td>
<td id="S3.T1.11.11.11.29.18.6" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.11.11.11.29.18.7" class="ltx_td ltx_align_center">46.2</td>
<td id="S3.T1.11.11.11.29.18.8" class="ltx_td ltx_align_center">65.2</td>
<td id="S3.T1.11.11.11.29.18.9" class="ltx_td ltx_align_center">50.0</td>
<td id="S3.T1.11.11.11.29.18.10" class="ltx_td ltx_align_center">28.8</td>
<td id="S3.T1.11.11.11.29.18.11" class="ltx_td ltx_align_center">49.2</td>
<td id="S3.T1.11.11.11.29.18.12" class="ltx_td ltx_align_center">61.7</td>
</tr>
<tr id="S3.T1.11.11.11.30.19" class="ltx_tr">
<td id="S3.T1.11.11.11.30.19.1" class="ltx_td ltx_align_left">DAB-Deformable-DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite>
</td>
<td id="S3.T1.11.11.11.30.19.2" class="ltx_td ltx_align_center">R50</td>
<td id="S3.T1.11.11.11.30.19.3" class="ltx_td ltx_align_center">50</td>
<td id="S3.T1.11.11.11.30.19.4" class="ltx_td ltx_align_center">48</td>
<td id="S3.T1.11.11.11.30.19.5" class="ltx_td ltx_align_center">195</td>
<td id="S3.T1.11.11.11.30.19.6" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.11.11.11.30.19.7" class="ltx_td ltx_align_center">46.9</td>
<td id="S3.T1.11.11.11.30.19.8" class="ltx_td ltx_align_center">66.0</td>
<td id="S3.T1.11.11.11.30.19.9" class="ltx_td ltx_align_center">50.8</td>
<td id="S3.T1.11.11.11.30.19.10" class="ltx_td ltx_align_center">30.1</td>
<td id="S3.T1.11.11.11.30.19.11" class="ltx_td ltx_align_center">50.4</td>
<td id="S3.T1.11.11.11.30.19.12" class="ltx_td ltx_align_center">62.5</td>
</tr>
<tr id="S3.T1.11.11.11.31.20" class="ltx_tr">
<td id="S3.T1.11.11.11.31.20.1" class="ltx_td ltx_align_left">DN-Deformable-DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite>
</td>
<td id="S3.T1.11.11.11.31.20.2" class="ltx_td ltx_align_center">R50</td>
<td id="S3.T1.11.11.11.31.20.3" class="ltx_td ltx_align_center">50</td>
<td id="S3.T1.11.11.11.31.20.4" class="ltx_td ltx_align_center">48</td>
<td id="S3.T1.11.11.11.31.20.5" class="ltx_td ltx_align_center">195</td>
<td id="S3.T1.11.11.11.31.20.6" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.11.11.11.31.20.7" class="ltx_td ltx_align_center">49.5</td>
<td id="S3.T1.11.11.11.31.20.8" class="ltx_td ltx_align_center">67.6</td>
<td id="S3.T1.11.11.11.31.20.9" class="ltx_td ltx_align_center">53.8</td>
<td id="S3.T1.11.11.11.31.20.10" class="ltx_td ltx_align_center">31.3</td>
<td id="S3.T1.11.11.11.31.20.11" class="ltx_td ltx_align_center">52.6</td>
<td id="S3.T1.11.11.11.31.20.12" class="ltx_td ltx_align_center">65.4</td>
</tr>
<tr id="S3.T1.11.11.11.32.21" class="ltx_tr">
<td id="S3.T1.11.11.11.32.21.1" class="ltx_td ltx_align_left">DINO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>
</td>
<td id="S3.T1.11.11.11.32.21.2" class="ltx_td ltx_align_center">R50</td>
<td id="S3.T1.11.11.11.32.21.3" class="ltx_td ltx_align_center">36</td>
<td id="S3.T1.11.11.11.32.21.4" class="ltx_td ltx_align_center">47</td>
<td id="S3.T1.11.11.11.32.21.5" class="ltx_td ltx_align_center">279</td>
<td id="S3.T1.11.11.11.32.21.6" class="ltx_td ltx_align_center">5</td>
<td id="S3.T1.11.11.11.32.21.7" class="ltx_td ltx_align_center">50.9</td>
<td id="S3.T1.11.11.11.32.21.8" class="ltx_td ltx_align_center">69.0</td>
<td id="S3.T1.11.11.11.32.21.9" class="ltx_td ltx_align_center">55.3</td>
<td id="S3.T1.11.11.11.32.21.10" class="ltx_td ltx_align_center">34.6</td>
<td id="S3.T1.11.11.11.32.21.11" class="ltx_td ltx_align_center">54.1</td>
<td id="S3.T1.11.11.11.32.21.12" class="ltx_td ltx_align_center">64.6</td>
</tr>
<tr id="S3.T1.11.11.11.33.22" class="ltx_tr" style="background-color:#E6E6E6;">
<td id="S3.T1.11.11.11.33.22.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T1.11.11.11.33.22.1.1" class="ltx_text ltx_font_italic" style="background-color:#E6E6E6;">Real-time End-to-end Object Detectors</span></td>
<td id="S3.T1.11.11.11.33.22.2" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.11.11.11.33.22.3" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.11.11.11.33.22.4" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.11.11.11.33.22.5" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.11.11.11.33.22.6" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.11.11.11.33.22.7" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.11.11.11.33.22.8" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.11.11.11.33.22.9" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.11.11.11.33.22.10" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.11.11.11.33.22.11" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.11.11.11.33.22.12" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S3.T1.11.11.11.34.23" class="ltx_tr">
<td id="S3.T1.11.11.11.34.23.1" class="ltx_td ltx_align_left">RT-DETR-R18 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite>
</td>
<td id="S3.T1.11.11.11.34.23.2" class="ltx_td ltx_align_center">R18</td>
<td id="S3.T1.11.11.11.34.23.3" class="ltx_td ltx_align_center">72</td>
<td id="S3.T1.11.11.11.34.23.4" class="ltx_td ltx_align_center">20</td>
<td id="S3.T1.11.11.11.34.23.5" class="ltx_td ltx_align_center">60</td>
<td id="S3.T1.11.11.11.34.23.6" class="ltx_td ltx_align_center">213</td>
<td id="S3.T1.11.11.11.34.23.7" class="ltx_td ltx_align_center">46.5</td>
<td id="S3.T1.11.11.11.34.23.8" class="ltx_td ltx_align_center">63.8</td>
<td id="S3.T1.11.11.11.34.23.9" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.11.11.11.34.23.10" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.11.11.11.34.23.11" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.11.11.11.34.23.12" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S3.T1.11.11.11.35.24" class="ltx_tr">
<td id="S3.T1.11.11.11.35.24.1" class="ltx_td ltx_align_left">RT-DETR-R34 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite>
</td>
<td id="S3.T1.11.11.11.35.24.2" class="ltx_td ltx_align_center">R34</td>
<td id="S3.T1.11.11.11.35.24.3" class="ltx_td ltx_align_center">72</td>
<td id="S3.T1.11.11.11.35.24.4" class="ltx_td ltx_align_center">31</td>
<td id="S3.T1.11.11.11.35.24.5" class="ltx_td ltx_align_center">92</td>
<td id="S3.T1.11.11.11.35.24.6" class="ltx_td ltx_align_center">154</td>
<td id="S3.T1.11.11.11.35.24.7" class="ltx_td ltx_align_center">48.9</td>
<td id="S3.T1.11.11.11.35.24.8" class="ltx_td ltx_align_center">66.8</td>
<td id="S3.T1.11.11.11.35.24.9" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.11.11.11.35.24.10" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.11.11.11.35.24.11" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.11.11.11.35.24.12" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S3.T1.11.11.11.36.25" class="ltx_tr">
<td id="S3.T1.11.11.11.36.25.1" class="ltx_td ltx_align_left">RT-DETR-R50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite>
</td>
<td id="S3.T1.11.11.11.36.25.2" class="ltx_td ltx_align_center">R50</td>
<td id="S3.T1.11.11.11.36.25.3" class="ltx_td ltx_align_center">72</td>
<td id="S3.T1.11.11.11.36.25.4" class="ltx_td ltx_align_center">36</td>
<td id="S3.T1.11.11.11.36.25.5" class="ltx_td ltx_align_center">100</td>
<td id="S3.T1.11.11.11.36.25.6" class="ltx_td ltx_align_center">107</td>
<td id="S3.T1.11.11.11.36.25.7" class="ltx_td ltx_align_center">53.1</td>
<td id="S3.T1.11.11.11.36.25.8" class="ltx_td ltx_align_center">71.3</td>
<td id="S3.T1.11.11.11.36.25.9" class="ltx_td ltx_align_center">57.7</td>
<td id="S3.T1.11.11.11.36.25.10" class="ltx_td ltx_align_center">34.8</td>
<td id="S3.T1.11.11.11.36.25.11" class="ltx_td ltx_align_center">58.0</td>
<td id="S3.T1.11.11.11.36.25.12" class="ltx_td ltx_align_center">70.0</td>
</tr>
<tr id="S3.T1.11.11.11.37.26" class="ltx_tr">
<td id="S3.T1.11.11.11.37.26.1" class="ltx_td ltx_align_left">RT-DETR-R101 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite>
</td>
<td id="S3.T1.11.11.11.37.26.2" class="ltx_td ltx_align_center">R101</td>
<td id="S3.T1.11.11.11.37.26.3" class="ltx_td ltx_align_center">72</td>
<td id="S3.T1.11.11.11.37.26.4" class="ltx_td ltx_align_center">42</td>
<td id="S3.T1.11.11.11.37.26.5" class="ltx_td ltx_align_center">136</td>
<td id="S3.T1.11.11.11.37.26.6" class="ltx_td ltx_align_center">70</td>
<td id="S3.T1.11.11.11.37.26.7" class="ltx_td ltx_align_center">54.3</td>
<td id="S3.T1.11.11.11.37.26.8" class="ltx_td ltx_align_center">72.7</td>
<td id="S3.T1.11.11.11.37.26.9" class="ltx_td ltx_align_center">58.6</td>
<td id="S3.T1.11.11.11.37.26.10" class="ltx_td ltx_align_center">36.0</td>
<td id="S3.T1.11.11.11.37.26.11" class="ltx_td ltx_align_center">58.8</td>
<td id="S3.T1.11.11.11.37.26.12" class="ltx_td ltx_align_center">72.1</td>
</tr>
<tr id="S3.T1.11.11.11.38.27" class="ltx_tr">
<td id="S3.T1.11.11.11.38.27.1" class="ltx_td ltx_align_left">RT-DETR-L <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite>
</td>
<td id="S3.T1.11.11.11.38.27.2" class="ltx_td ltx_align_center">HGNetv2</td>
<td id="S3.T1.11.11.11.38.27.3" class="ltx_td ltx_align_center">72</td>
<td id="S3.T1.11.11.11.38.27.4" class="ltx_td ltx_align_center">32</td>
<td id="S3.T1.11.11.11.38.27.5" class="ltx_td ltx_align_center">110</td>
<td id="S3.T1.11.11.11.38.27.6" class="ltx_td ltx_align_center">115</td>
<td id="S3.T1.11.11.11.38.27.7" class="ltx_td ltx_align_center">53.0</td>
<td id="S3.T1.11.11.11.38.27.8" class="ltx_td ltx_align_center">71.6</td>
<td id="S3.T1.11.11.11.38.27.9" class="ltx_td ltx_align_center">57.3</td>
<td id="S3.T1.11.11.11.38.27.10" class="ltx_td ltx_align_center">34.6</td>
<td id="S3.T1.11.11.11.38.27.11" class="ltx_td ltx_align_center">57.3</td>
<td id="S3.T1.11.11.11.38.27.12" class="ltx_td ltx_align_center">71.2</td>
</tr>
<tr id="S3.T1.11.11.11.39.28" class="ltx_tr">
<td id="S3.T1.11.11.11.39.28.1" class="ltx_td ltx_align_left">RT-DETR-X <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite>
</td>
<td id="S3.T1.11.11.11.39.28.2" class="ltx_td ltx_align_center">HGNetv2</td>
<td id="S3.T1.11.11.11.39.28.3" class="ltx_td ltx_align_center">72</td>
<td id="S3.T1.11.11.11.39.28.4" class="ltx_td ltx_align_center">67</td>
<td id="S3.T1.11.11.11.39.28.5" class="ltx_td ltx_align_center">234</td>
<td id="S3.T1.11.11.11.39.28.6" class="ltx_td ltx_align_center">73</td>
<td id="S3.T1.11.11.11.39.28.7" class="ltx_td ltx_align_center">54.8</td>
<td id="S3.T1.11.11.11.39.28.8" class="ltx_td ltx_align_center">73.1</td>
<td id="S3.T1.11.11.11.39.28.9" class="ltx_td ltx_align_center">59.4</td>
<td id="S3.T1.11.11.11.39.28.10" class="ltx_td ltx_align_center">35.7</td>
<td id="S3.T1.11.11.11.39.28.11" class="ltx_td ltx_align_center">59.6</td>
<td id="S3.T1.11.11.11.39.28.12" class="ltx_td ltx_align_center">72.9</td>
</tr>
<tr id="S3.T1.11.11.11.40.29" class="ltx_tr" style="background-color:#FFE6E6;">
<td id="S3.T1.11.11.11.40.29.1" class="ltx_td ltx_align_left"><span id="S3.T1.11.11.11.40.29.1.1" class="ltx_text ltx_font_italic" style="background-color:#FFE6E6;">No Extra Training Data</span></td>
<td id="S3.T1.11.11.11.40.29.2" class="ltx_td"></td>
<td id="S3.T1.11.11.11.40.29.3" class="ltx_td"></td>
<td id="S3.T1.11.11.11.40.29.4" class="ltx_td"></td>
<td id="S3.T1.11.11.11.40.29.5" class="ltx_td"></td>
<td id="S3.T1.11.11.11.40.29.6" class="ltx_td"></td>
<td id="S3.T1.11.11.11.40.29.7" class="ltx_td"></td>
<td id="S3.T1.11.11.11.40.29.8" class="ltx_td"></td>
<td id="S3.T1.11.11.11.40.29.9" class="ltx_td"></td>
<td id="S3.T1.11.11.11.40.29.10" class="ltx_td"></td>
<td id="S3.T1.11.11.11.40.29.11" class="ltx_td"></td>
<td id="S3.T1.11.11.11.40.29.12" class="ltx_td"></td>
</tr>
<tr id="S3.T1.11.11.11.41.30" class="ltx_tr">
<td id="S3.T1.11.11.11.41.30.1" class="ltx_td ltx_align_left">DEYOv3-N</td>
<td id="S3.T1.11.11.11.41.30.2" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.11.11.11.41.30.3" class="ltx_td ltx_align_center">72</td>
<td id="S3.T1.11.11.11.41.30.4" class="ltx_td ltx_align_center">10</td>
<td id="S3.T1.11.11.11.41.30.5" class="ltx_td ltx_align_center">17</td>
<td id="S3.T1.11.11.11.41.30.6" class="ltx_td ltx_align_center">270</td>
<td id="S3.T1.11.11.11.41.30.7" class="ltx_td ltx_align_center">41.1</td>
<td id="S3.T1.11.11.11.41.30.8" class="ltx_td ltx_align_center">57.4</td>
<td id="S3.T1.11.11.11.41.30.9" class="ltx_td ltx_align_center">44.2</td>
<td id="S3.T1.11.11.11.41.30.10" class="ltx_td ltx_align_center">23.0</td>
<td id="S3.T1.11.11.11.41.30.11" class="ltx_td ltx_align_center">44.2</td>
<td id="S3.T1.11.11.11.41.30.12" class="ltx_td ltx_align_center">56.6</td>
</tr>
<tr id="S3.T1.11.11.11.42.31" class="ltx_tr">
<td id="S3.T1.11.11.11.42.31.1" class="ltx_td ltx_align_left">DEYOv3-S</td>
<td id="S3.T1.11.11.11.42.31.2" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.11.11.11.42.31.3" class="ltx_td ltx_align_center">72</td>
<td id="S3.T1.11.11.11.42.31.4" class="ltx_td ltx_align_center">16</td>
<td id="S3.T1.11.11.11.42.31.5" class="ltx_td ltx_align_center">33</td>
<td id="S3.T1.11.11.11.42.31.6" class="ltx_td ltx_align_center">220</td>
<td id="S3.T1.11.11.11.42.31.7" class="ltx_td ltx_align_center">45.8</td>
<td id="S3.T1.11.11.11.42.31.8" class="ltx_td ltx_align_center">63.1</td>
<td id="S3.T1.11.11.11.42.31.9" class="ltx_td ltx_align_center">49.4</td>
<td id="S3.T1.11.11.11.42.31.10" class="ltx_td ltx_align_center">27.3</td>
<td id="S3.T1.11.11.11.42.31.11" class="ltx_td ltx_align_center">49.7</td>
<td id="S3.T1.11.11.11.42.31.12" class="ltx_td ltx_align_center">61.4</td>
</tr>
<tr id="S3.T1.11.11.11.43.32" class="ltx_tr">
<td id="S3.T1.11.11.11.43.32.1" class="ltx_td ltx_align_left">DEYOv3-M</td>
<td id="S3.T1.11.11.11.43.32.2" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.11.11.11.43.32.3" class="ltx_td ltx_align_center">72</td>
<td id="S3.T1.11.11.11.43.32.4" class="ltx_td ltx_align_center">30</td>
<td id="S3.T1.11.11.11.43.32.5" class="ltx_td ltx_align_center">76</td>
<td id="S3.T1.11.11.11.43.32.6" class="ltx_td ltx_align_center">140</td>
<td id="S3.T1.11.11.11.43.32.7" class="ltx_td ltx_align_center">49.1</td>
<td id="S3.T1.11.11.11.43.32.8" class="ltx_td ltx_align_center">66.9</td>
<td id="S3.T1.11.11.11.43.32.9" class="ltx_td ltx_align_center">53.2</td>
<td id="S3.T1.11.11.11.43.32.10" class="ltx_td ltx_align_center">31.0</td>
<td id="S3.T1.11.11.11.43.32.11" class="ltx_td ltx_align_center">53.3</td>
<td id="S3.T1.11.11.11.43.32.12" class="ltx_td ltx_align_center">65.2</td>
</tr>
<tr id="S3.T1.11.11.11.44.33" class="ltx_tr">
<td id="S3.T1.11.11.11.44.33.1" class="ltx_td ltx_align_left ltx_border_b">DEYOv3-L</td>
<td id="S3.T1.11.11.11.44.33.2" class="ltx_td ltx_align_center ltx_border_b">–</td>
<td id="S3.T1.11.11.11.44.33.3" class="ltx_td ltx_align_center ltx_border_b">72</td>
<td id="S3.T1.11.11.11.44.33.4" class="ltx_td ltx_align_center ltx_border_b">46</td>
<td id="S3.T1.11.11.11.44.33.5" class="ltx_td ltx_align_center ltx_border_b">152</td>
<td id="S3.T1.11.11.11.44.33.6" class="ltx_td ltx_align_center ltx_border_b">102</td>
<td id="S3.T1.11.11.11.44.33.7" class="ltx_td ltx_align_center ltx_border_b">51.3</td>
<td id="S3.T1.11.11.11.44.33.8" class="ltx_td ltx_align_center ltx_border_b">69.1</td>
<td id="S3.T1.11.11.11.44.33.9" class="ltx_td ltx_align_center ltx_border_b">55.5</td>
<td id="S3.T1.11.11.11.44.33.10" class="ltx_td ltx_align_center ltx_border_b">35.4</td>
<td id="S3.T1.11.11.11.44.33.11" class="ltx_td ltx_align_center ltx_border_b">55.5</td>
<td id="S3.T1.11.11.11.44.33.12" class="ltx_td ltx_align_center ltx_border_b">66.1</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.13.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S3.T1.14.2" class="ltx_text" style="font-size:90%;">Main results. Real-time detectors and our DEYOv3 utilize a consistent input size of 640, while end-to-end detectors employ an input size of (800, 1333).
The end-to-end speed results are reported on a T4 GPU with TensorRT FP16, following the method proposed in RT-DETR. We do not test the speed of DETRs, as they are not real time detectors.</span></figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>One-to-many Branch</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">DEYOv3 adopts YOLOv8 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite> as the one-to-many branch of the model to accommodate our step-by-step training method. YOLOv8 builds upon the success of previous YOLO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>, <a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>, <a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite> versions and introduces new features and improvements to enhance performance and flexibility further. The three multi-scale layers of YOLOv8 provide the one-to-one branch with up to 8400 queries, which can be used to generate proposal bounding boxes and serve as the key and value for the decoder. Unlike DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite>, YOLO benefits from the one-to-many training approach, which allows these queries to be more thoroughly supervised during the first-stage training. As a result, a powerful neck is trained to provide multi-scale information to the decoder, enabling the model to achieve better performance.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Efficient Encoder</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The encoder of DEYOv3 differs from DETR in that it does not employ a transformer as the encoder. Instead, DEYOv3 utilizes YOLO’s pre-trained neck in the first stage to encode multi-scale features, which are then fed into feature projection to align them to the hidden dimension. In a broad sense, the entire neck and feature projection can be considered as the encoder of DEYOv3. This implementation aligns well with our step-by-step training method. Due to the rich features obtained by the neck during efficient pre-training in the first stage, these features can provide efficient initialization for the encoder in the second stage. As a result, the encoder can offer high-quality key, value, and proposal bounding box information to the decoder. Compared to DETR’s randomly initialized multi-scale layers and encoder, DEYOv3’s encoder achieves exceptional speed while simultaneously maintaining performance. We can describe this process as follows:</p>
<table id="S6.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.Ex1.m1.3" class="ltx_Math" alttext="\displaystyle{\color[rgb]{0,0,0}S_{1}=Proj(P_{3},P_{4},P_{5})}" display="inline"><semantics id="S3.Ex1.m1.3a"><mrow id="S3.Ex1.m1.3.3" xref="S3.Ex1.m1.3.3.cmml"><msub id="S3.Ex1.m1.3.3.5" xref="S3.Ex1.m1.3.3.5.cmml"><mi mathcolor="#000000" id="S3.Ex1.m1.3.3.5.2" xref="S3.Ex1.m1.3.3.5.2.cmml">S</mi><mn mathcolor="#000000" id="S3.Ex1.m1.3.3.5.3" xref="S3.Ex1.m1.3.3.5.3.cmml">1</mn></msub><mo mathcolor="#000000" id="S3.Ex1.m1.3.3.4" xref="S3.Ex1.m1.3.3.4.cmml">=</mo><mrow id="S3.Ex1.m1.3.3.3" xref="S3.Ex1.m1.3.3.3.cmml"><mi mathcolor="#000000" id="S3.Ex1.m1.3.3.3.5" xref="S3.Ex1.m1.3.3.3.5.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.3.3.3.4" xref="S3.Ex1.m1.3.3.3.4.cmml">​</mo><mi mathcolor="#000000" id="S3.Ex1.m1.3.3.3.6" xref="S3.Ex1.m1.3.3.3.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.3.3.3.4a" xref="S3.Ex1.m1.3.3.3.4.cmml">​</mo><mi mathcolor="#000000" id="S3.Ex1.m1.3.3.3.7" xref="S3.Ex1.m1.3.3.3.7.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.3.3.3.4b" xref="S3.Ex1.m1.3.3.3.4.cmml">​</mo><mi mathcolor="#000000" id="S3.Ex1.m1.3.3.3.8" xref="S3.Ex1.m1.3.3.3.8.cmml">j</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.3.3.3.4c" xref="S3.Ex1.m1.3.3.3.4.cmml">​</mo><mrow id="S3.Ex1.m1.3.3.3.3.3" xref="S3.Ex1.m1.3.3.3.3.4.cmml"><mo mathcolor="#000000" stretchy="false" id="S3.Ex1.m1.3.3.3.3.3.4" xref="S3.Ex1.m1.3.3.3.3.4.cmml">(</mo><msub id="S3.Ex1.m1.1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.cmml"><mi mathcolor="#000000" id="S3.Ex1.m1.1.1.1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.1.1.2.cmml">P</mi><mn mathcolor="#000000" id="S3.Ex1.m1.1.1.1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.1.1.1.3.cmml">3</mn></msub><mo mathcolor="#000000" id="S3.Ex1.m1.3.3.3.3.3.5" xref="S3.Ex1.m1.3.3.3.3.4.cmml">,</mo><msub id="S3.Ex1.m1.2.2.2.2.2.2" xref="S3.Ex1.m1.2.2.2.2.2.2.cmml"><mi mathcolor="#000000" id="S3.Ex1.m1.2.2.2.2.2.2.2" xref="S3.Ex1.m1.2.2.2.2.2.2.2.cmml">P</mi><mn mathcolor="#000000" id="S3.Ex1.m1.2.2.2.2.2.2.3" xref="S3.Ex1.m1.2.2.2.2.2.2.3.cmml">4</mn></msub><mo mathcolor="#000000" id="S3.Ex1.m1.3.3.3.3.3.6" xref="S3.Ex1.m1.3.3.3.3.4.cmml">,</mo><msub id="S3.Ex1.m1.3.3.3.3.3.3" xref="S3.Ex1.m1.3.3.3.3.3.3.cmml"><mi mathcolor="#000000" id="S3.Ex1.m1.3.3.3.3.3.3.2" xref="S3.Ex1.m1.3.3.3.3.3.3.2.cmml">P</mi><mn mathcolor="#000000" id="S3.Ex1.m1.3.3.3.3.3.3.3" xref="S3.Ex1.m1.3.3.3.3.3.3.3.cmml">5</mn></msub><mo mathcolor="#000000" stretchy="false" id="S3.Ex1.m1.3.3.3.3.3.7" xref="S3.Ex1.m1.3.3.3.3.4.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.3b"><apply id="S3.Ex1.m1.3.3.cmml" xref="S3.Ex1.m1.3.3"><eq id="S3.Ex1.m1.3.3.4.cmml" xref="S3.Ex1.m1.3.3.4"></eq><apply id="S3.Ex1.m1.3.3.5.cmml" xref="S3.Ex1.m1.3.3.5"><csymbol cd="ambiguous" id="S3.Ex1.m1.3.3.5.1.cmml" xref="S3.Ex1.m1.3.3.5">subscript</csymbol><ci id="S3.Ex1.m1.3.3.5.2.cmml" xref="S3.Ex1.m1.3.3.5.2">𝑆</ci><cn type="integer" id="S3.Ex1.m1.3.3.5.3.cmml" xref="S3.Ex1.m1.3.3.5.3">1</cn></apply><apply id="S3.Ex1.m1.3.3.3.cmml" xref="S3.Ex1.m1.3.3.3"><times id="S3.Ex1.m1.3.3.3.4.cmml" xref="S3.Ex1.m1.3.3.3.4"></times><ci id="S3.Ex1.m1.3.3.3.5.cmml" xref="S3.Ex1.m1.3.3.3.5">𝑃</ci><ci id="S3.Ex1.m1.3.3.3.6.cmml" xref="S3.Ex1.m1.3.3.3.6">𝑟</ci><ci id="S3.Ex1.m1.3.3.3.7.cmml" xref="S3.Ex1.m1.3.3.3.7">𝑜</ci><ci id="S3.Ex1.m1.3.3.3.8.cmml" xref="S3.Ex1.m1.3.3.3.8">𝑗</ci><vector id="S3.Ex1.m1.3.3.3.3.4.cmml" xref="S3.Ex1.m1.3.3.3.3.3"><apply id="S3.Ex1.m1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.2">𝑃</ci><cn type="integer" id="S3.Ex1.m1.1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.3">3</cn></apply><apply id="S3.Ex1.m1.2.2.2.2.2.2.cmml" xref="S3.Ex1.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.2.2.2.2.2.2.1.cmml" xref="S3.Ex1.m1.2.2.2.2.2.2">subscript</csymbol><ci id="S3.Ex1.m1.2.2.2.2.2.2.2.cmml" xref="S3.Ex1.m1.2.2.2.2.2.2.2">𝑃</ci><cn type="integer" id="S3.Ex1.m1.2.2.2.2.2.2.3.cmml" xref="S3.Ex1.m1.2.2.2.2.2.2.3">4</cn></apply><apply id="S3.Ex1.m1.3.3.3.3.3.3.cmml" xref="S3.Ex1.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.3.3.3.3.3.3.1.cmml" xref="S3.Ex1.m1.3.3.3.3.3.3">subscript</csymbol><ci id="S3.Ex1.m1.3.3.3.3.3.3.2.cmml" xref="S3.Ex1.m1.3.3.3.3.3.3.2">𝑃</ci><cn type="integer" id="S3.Ex1.m1.3.3.3.3.3.3.3.cmml" xref="S3.Ex1.m1.3.3.3.3.3.3.3">5</cn></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.3c">\displaystyle{\color[rgb]{0,0,0}S_{1}=Proj(P_{3},P_{4},P_{5})}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E1.m1.1" class="ltx_Math" alttext="\displaystyle{\color[rgb]{0,0,0}S_{2}=Concat(S_{1})}" display="inline"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><msub id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><mi mathcolor="#000000" id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml">S</mi><mn mathcolor="#000000" id="S3.E1.m1.1.1.3.3" xref="S3.E1.m1.1.1.3.3.cmml">2</mn></msub><mo mathcolor="#000000" id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml"><mi mathcolor="#000000" id="S3.E1.m1.1.1.1.3" xref="S3.E1.m1.1.1.1.3.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.2.cmml">​</mo><mi mathcolor="#000000" id="S3.E1.m1.1.1.1.4" xref="S3.E1.m1.1.1.1.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.2a" xref="S3.E1.m1.1.1.1.2.cmml">​</mo><mi mathcolor="#000000" id="S3.E1.m1.1.1.1.5" xref="S3.E1.m1.1.1.1.5.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.2b" xref="S3.E1.m1.1.1.1.2.cmml">​</mo><mi mathcolor="#000000" id="S3.E1.m1.1.1.1.6" xref="S3.E1.m1.1.1.1.6.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.2c" xref="S3.E1.m1.1.1.1.2.cmml">​</mo><mi mathcolor="#000000" id="S3.E1.m1.1.1.1.7" xref="S3.E1.m1.1.1.1.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.2d" xref="S3.E1.m1.1.1.1.2.cmml">​</mo><mi mathcolor="#000000" id="S3.E1.m1.1.1.1.8" xref="S3.E1.m1.1.1.1.8.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.2e" xref="S3.E1.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml"><mo mathcolor="#000000" stretchy="false" id="S3.E1.m1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml"><mi mathcolor="#000000" id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.2.cmml">S</mi><mn mathcolor="#000000" id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.3.cmml">1</mn></msub><mo mathcolor="#000000" stretchy="false" id="S3.E1.m1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><eq id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2"></eq><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2">𝑆</ci><cn type="integer" id="S3.E1.m1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.3.3">2</cn></apply><apply id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><times id="S3.E1.m1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.2"></times><ci id="S3.E1.m1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.3">𝐶</ci><ci id="S3.E1.m1.1.1.1.4.cmml" xref="S3.E1.m1.1.1.1.4">𝑜</ci><ci id="S3.E1.m1.1.1.1.5.cmml" xref="S3.E1.m1.1.1.1.5">𝑛</ci><ci id="S3.E1.m1.1.1.1.6.cmml" xref="S3.E1.m1.1.1.1.6">𝑐</ci><ci id="S3.E1.m1.1.1.1.7.cmml" xref="S3.E1.m1.1.1.1.7">𝑎</ci><ci id="S3.E1.m1.1.1.1.8.cmml" xref="S3.E1.m1.1.1.1.8">𝑡</ci><apply id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2">𝑆</ci><cn type="integer" id="S3.E1.m1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\displaystyle{\color[rgb]{0,0,0}S_{2}=Concat(S_{1})}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
<tbody id="S3.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.Ex2.m1.1" class="ltx_Math" alttext="\displaystyle{\color[rgb]{0,0,0}Q=K=V=S_{2}}" display="inline"><semantics id="S3.Ex2.m1.1a"><mrow id="S3.Ex2.m1.1.1" xref="S3.Ex2.m1.1.1.cmml"><mi mathcolor="#000000" id="S3.Ex2.m1.1.1.2" xref="S3.Ex2.m1.1.1.2.cmml">Q</mi><mo mathcolor="#000000" id="S3.Ex2.m1.1.1.3" xref="S3.Ex2.m1.1.1.3.cmml">=</mo><mi mathcolor="#000000" id="S3.Ex2.m1.1.1.4" xref="S3.Ex2.m1.1.1.4.cmml">K</mi><mo mathcolor="#000000" id="S3.Ex2.m1.1.1.5" xref="S3.Ex2.m1.1.1.5.cmml">=</mo><mi mathcolor="#000000" id="S3.Ex2.m1.1.1.6" xref="S3.Ex2.m1.1.1.6.cmml">V</mi><mo mathcolor="#000000" id="S3.Ex2.m1.1.1.7" xref="S3.Ex2.m1.1.1.7.cmml">=</mo><msub id="S3.Ex2.m1.1.1.8" xref="S3.Ex2.m1.1.1.8.cmml"><mi mathcolor="#000000" id="S3.Ex2.m1.1.1.8.2" xref="S3.Ex2.m1.1.1.8.2.cmml">S</mi><mn mathcolor="#000000" id="S3.Ex2.m1.1.1.8.3" xref="S3.Ex2.m1.1.1.8.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex2.m1.1b"><apply id="S3.Ex2.m1.1.1.cmml" xref="S3.Ex2.m1.1.1"><and id="S3.Ex2.m1.1.1a.cmml" xref="S3.Ex2.m1.1.1"></and><apply id="S3.Ex2.m1.1.1b.cmml" xref="S3.Ex2.m1.1.1"><eq id="S3.Ex2.m1.1.1.3.cmml" xref="S3.Ex2.m1.1.1.3"></eq><ci id="S3.Ex2.m1.1.1.2.cmml" xref="S3.Ex2.m1.1.1.2">𝑄</ci><ci id="S3.Ex2.m1.1.1.4.cmml" xref="S3.Ex2.m1.1.1.4">𝐾</ci></apply><apply id="S3.Ex2.m1.1.1c.cmml" xref="S3.Ex2.m1.1.1"><eq id="S3.Ex2.m1.1.1.5.cmml" xref="S3.Ex2.m1.1.1.5"></eq><share href="#S3.Ex2.m1.1.1.4.cmml" id="S3.Ex2.m1.1.1d.cmml" xref="S3.Ex2.m1.1.1"></share><ci id="S3.Ex2.m1.1.1.6.cmml" xref="S3.Ex2.m1.1.1.6">𝑉</ci></apply><apply id="S3.Ex2.m1.1.1e.cmml" xref="S3.Ex2.m1.1.1"><eq id="S3.Ex2.m1.1.1.7.cmml" xref="S3.Ex2.m1.1.1.7"></eq><share href="#S3.Ex2.m1.1.1.6.cmml" id="S3.Ex2.m1.1.1f.cmml" xref="S3.Ex2.m1.1.1"></share><apply id="S3.Ex2.m1.1.1.8.cmml" xref="S3.Ex2.m1.1.1.8"><csymbol cd="ambiguous" id="S3.Ex2.m1.1.1.8.1.cmml" xref="S3.Ex2.m1.1.1.8">subscript</csymbol><ci id="S3.Ex2.m1.1.1.8.2.cmml" xref="S3.Ex2.m1.1.1.8.2">𝑆</ci><cn type="integer" id="S3.Ex2.m1.1.1.8.3.cmml" xref="S3.Ex2.m1.1.1.8.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex2.m1.1c">\displaystyle{\color[rgb]{0,0,0}Q=K=V=S_{2}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>One-to-one Branch</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">The decoder of DEYOv3 adopts a similar architecture to DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite>, utilizing self-attention in transformers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite> to capture relationships between different queries, thereby establishing score disparities to suppress redundant bounding boxes. In each level of the decoder, the queries are progressively refined, resulting in a one-to-one set of objects. This design greatly simplifies the object detection process in DEYOv3 and eliminates the reliance on non-maximum suppression (NMS). Additionally, thanks to the global awareness provided by the transformer decoder, similar to DETR, DEYOv3 exhibits improved classification capability. Furthermore, due to the high-quality initialization provided by the first-stage training, even with supervision on only a few hundred queries in the one-to-one branch, the model can converge rapidly and achieve better performance.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2309.11851/assets/4.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="328" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.8.3.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.4.2" class="ltx_text" style="font-size:90%;">Convergence curves of various methods trained on CrowdHuman. Please note that we calculate the AP<sub id="S3.F4.4.2.1" class="ltx_sub">5</sub><sub id="S3.F4.4.2.2" class="ltx_sub">0</sub> using the tools of YOLOv8 in this experiment.</span></figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Setups</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">COCO</span> To evaluate the performance of our method in object detection tasks, we conducted experiments on the widely used Microsoft COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite>. We trained the DEYOv3 using the <span id="S4.SS1.p1.1.2" class="ltx_text ltx_font_typewriter">train2017</span> and evaluated the performance using the <span id="S4.SS1.p1.1.3" class="ltx_text ltx_font_typewriter">val2017</span>. In our experiments, we did not utilize any additional training data.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">CrowdHuman</span> To evaluate the end-to-end effectiveness of DEYOv3 in dense detection compared to classical detectors, we conducted experiments on CrowdHuman <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>. The CrowdHuman consists of 15,000 images for training, 4,370 images for validation, and 5,000 images for testing. We utilized the provided full-body annotations in the dataset and evaluated on the validation set. In terms of optimizer-related parameters, we adopted the same settings as the COCO.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">Implementation Details</span> In the first stage of training DEYOv3, we used a 6-layer transformer decoder with hidden feature dimensions of 256. We trained the detector from scratch following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite> strategy and hyperparameters. In the second stage of training, we further trained our detector using the AdamW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite> optimizer. The learning rate was set to 0.0001, the learning rate for the backbone was set to 0.00001, and the weight decay was set to 0.0001. The data augmentation strategy in the second stage was similar to the first stage, including random color distortion, translation, flipping, resizing, and other operations. However, unlike the first stage of data augmentation, in the second stage of training, we turned off mosaic data augmentation and stopped using gray filling to pad image borders.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<div id="S4.T2.2.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:213.6pt;height:53.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-1.8pt,0.5pt) scale(0.983075944312667,0.983075944312667) ;">
<table id="S4.T2.2.2.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.2.2.2.2" class="ltx_tr">
<th id="S4.T2.2.2.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">Model</th>
<th id="S4.T2.2.2.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t">Epochs</th>
<th id="S4.T2.2.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AP<sub id="S4.T2.2.2.2.2.2.1" class="ltx_sub">5</sub><sub id="S4.T2.2.2.2.2.2.2" class="ltx_sub">0</sub>
</th>
<th id="S4.T2.2.2.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">mMR</th>
<th id="S4.T2.2.2.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Recall</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.2.2.2.3.1" class="ltx_tr">
<th id="S4.T2.2.2.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">YOLOv8-N</th>
<th id="S4.T2.2.2.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">300</th>
<td id="S4.T2.2.2.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t">83.0</td>
<td id="S4.T2.2.2.2.3.1.4" class="ltx_td ltx_align_center ltx_border_t">49.9</td>
<td id="S4.T2.2.2.2.3.1.5" class="ltx_td ltx_align_center ltx_border_t">84.6</td>
</tr>
<tr id="S4.T2.2.2.2.4.2" class="ltx_tr" style="background-color:#F2F2F2;">
<th id="S4.T2.2.2.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b"><span id="S4.T2.2.2.2.4.2.1.1" class="ltx_text" style="background-color:#F2F2F2;">DEYOv3-N</span></th>
<th id="S4.T2.2.2.2.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b"><span id="S4.T2.2.2.2.4.2.2.1" class="ltx_text" style="background-color:#F2F2F2;">72</span></th>
<td id="S4.T2.2.2.2.4.2.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.2.2.2.4.2.3.1" class="ltx_text" style="background-color:#F2F2F2;">87.1</span></td>
<td id="S4.T2.2.2.2.4.2.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.2.2.2.4.2.4.1" class="ltx_text" style="background-color:#F2F2F2;">49.3</span></td>
<td id="S4.T2.2.2.2.4.2.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.2.2.2.4.2.5.1" class="ltx_text" style="background-color:#F2F2F2;">95.0</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.4.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.5.2" class="ltx_text" style="font-size:90%;">Perfermance on CrowdHuman (full body).</span></figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Main Results</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We compared the scaled DEYOv3 with YOLOv5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite>, YOLOv8 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>, and RT-DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite> in Table <a href="#S3.T1" title="Table 1 ‣ 3.1 Model Overview ‣ 3 DEYOv3 ‣ DEYOv3: DETR with YOLO for Real-time Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Compared to YOLOv8, DEYOv3 demonstrates a significant improvement in accuracy by 3.8% / 0.9% AP at scales N and S while achieving a 70% / 59% increase in FPS. At scales M and L, DEYOv3 continues to exhibit a better trade-off between accuracy and speed. Additionally, compared to non-real-time end-to-end detectors, DEYOv3 showcases remarkable speed advantages. Meanwhile, DEYOv3-N has demonstrated a remarkable increase of 4.1 AP compared to YOLOv8-N in dense detection scenarios of CrowdHuman <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2309.11851/assets/5.png" id="S4.F5.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="177" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.3.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.4.2" class="ltx_text" style="font-size:90%;">The exploration of the ideal performance of YOLOv8 and DEYOv3. Here, reference represents the model’s performance under standard inference, while ideal represents where we perform one-to-one matching between labels and inference results to distinguish true positives (TP) from false positives (FP), thereby achieving the purpose of measuring the ideal performance of the model. Ideal+ builds upon ideal by eliminating classification errors and further examines the model’s ability to localize objects. Experiments were conducted on DEYOv3 and YOLOv8 in Figures (a) and (b), respectively. Figure (c) compares the capabilities of the one-to-many branch and one-to-one branch in generating candidate boxes. In (a), the bounding box from the last layer of the decoder in DEYOv3 was used, while in (c), the bounding boxes of DEYOv3 were derived from the bounding boxes after being filtered through the query selector.</span></figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Ablation Study</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">In Fig. <a href="#S3.F4" title="Figure 4 ‣ 3.4 One-to-one Branch ‣ 3 DEYOv3 ‣ DEYOv3: DETR with YOLO for Real-time Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we present the convergence curves of YOLOv8-N trained from scratch on CrowdHuman <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>, as well as DEYOv3-N trained with three different training strategies: YOLO Strategy, DETR Strategy, and Step-by-step Training. Even with training from scratch, YOLOv8-N <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite> achieves the good performance of 82.6 AP without relying on additional datasets, thanks to the rich supervision signals provided by one-to-many training. However, DEYOv3 trained with the same training strategies and limited supervision signals due to one-to-one matching only achieves a performance of 78.3 AP. Furthermore, initializing DEYOv3’s backbone with YOLOv8-N-CLS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite> pre-trained on ImageNet and using the training strategy of DETR only yields a performance of 72.1 AP.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<div id="S4.T3.2.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:213.6pt;height:50.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-7.9pt,1.9pt) scale(0.931294849805904,0.931294849805904) ;">
<table id="S4.T3.2.2.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.2.2.2.2" class="ltx_tr">
<th id="S4.T3.2.2.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">Model</th>
<th id="S4.T3.2.2.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t">Epochs</th>
<th id="S4.T3.2.2.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Backbone</th>
<th id="S4.T3.2.2.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Neck</th>
<th id="S4.T3.2.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AP<sub id="S4.T3.2.2.2.2.2.1" class="ltx_sub">5</sub><sub id="S4.T3.2.2.2.2.2.2" class="ltx_sub">0</sub>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.2.2.2.3.1" class="ltx_tr">
<th id="S4.T3.2.2.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">DEYOv3-N</th>
<th id="S4.T3.2.2.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">72</th>
<td id="S4.T3.2.2.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S4.T3.2.2.2.3.1.4" class="ltx_td ltx_border_t"></td>
<td id="S4.T3.2.2.2.3.1.5" class="ltx_td ltx_align_center ltx_border_t">68.3</td>
</tr>
<tr id="S4.T3.2.2.2.4.2" class="ltx_tr">
<th id="S4.T3.2.2.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">DEYOv3-N</th>
<th id="S4.T3.2.2.2.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b">72</th>
<td id="S4.T3.2.2.2.4.2.3" class="ltx_td ltx_align_center ltx_border_b">✓</td>
<td id="S4.T3.2.2.2.4.2.4" class="ltx_td ltx_align_center ltx_border_b">✓</td>
<td id="S4.T3.2.2.2.4.2.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T3.2.2.2.4.2.5.1" class="ltx_text ltx_font_bold">87.1</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.4.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S4.T3.5.2" class="ltx_text" style="font-size:90%;">Results of the ablation study on step-by-step training.</span></figcaption>
</figure>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">In Table <a href="#S4.T3" title="Table 3 ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ DEYOv3: DETR with YOLO for Real-time Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we analyze the necessity of the high-quality multi-scale features provided by the neck pre-trained in the first stage. When using the backbone with step-by-step training without initializing the neck, the model’s performance decreases significantly by 18.8 AP, reaching a minimum of 68.3 AP. This clearly demonstrates that the key to achieving good performance in DEYOv3 lies not in a more powerful backbone compared to ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite> pretraining but instead in the utilization of a Neck pre-trained in the first stage, which provides high-quality multi-scale features for the decoder.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Analysis</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">In Table <a href="#S4.T4" title="Table 4 ‣ 4.4 Analysis ‣ 4 Experiments ‣ DEYOv3: DETR with YOLO for Real-time Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we observed that the model scaling strategy of YOLOv8 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite> is not applicable to DEYOv3, as the output dimensions of the Neck do not match the hidden dimensions of the decoder. The use of an X-scale model as the one-to-many branch in DEYOv3 not only slows down the inference speed but also fails to improve the AP. Although DEYOv3 falls short of achieving optimal performance at larger scales, this actually highlights the tremendous potential of the DEYOv3 model. We believe that a specifically designed one-to-many branch or a more powerful feature projection module can elevate the performance of DEYOv3 to new heights.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<div id="S4.T4.4.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:223.6pt;height:51.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-43.6pt,10.1pt) scale(0.719584786716349,0.719584786716349) ;">
<table id="S4.T4.4.4.4" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.4.4.4.4" class="ltx_tr">
<th id="S4.T4.4.4.4.4.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Model Size</th>
<th id="S4.T4.4.4.4.4.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Neck</th>
<th id="S4.T4.4.4.4.4.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Hidden Dimension</th>
<th id="S4.T4.4.4.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">FPS<sub id="S4.T4.4.4.4.4.4.1" class="ltx_sub"><span id="S4.T4.4.4.4.4.4.1.1" class="ltx_text ltx_font_italic">b</span></sub><sub id="S4.T4.4.4.4.4.4.2" class="ltx_sub"><span id="S4.T4.4.4.4.4.4.2.1" class="ltx_text ltx_font_italic">s</span></sub><sub id="S4.T4.4.4.4.4.4.3" class="ltx_sub">=</sub><sub id="S4.T4.4.4.4.4.4.4" class="ltx_sub">1</sub>
</th>
<th id="S4.T4.4.4.4.4.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AP</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.4.4.4.5.1" class="ltx_tr">
<td id="S4.T4.4.4.4.5.1.1" class="ltx_td ltx_align_left ltx_border_t">L</td>
<td id="S4.T4.4.4.4.5.1.2" class="ltx_td ltx_align_center ltx_border_t">(256, 512, 512)</td>
<td id="S4.T4.4.4.4.5.1.3" class="ltx_td ltx_align_center ltx_border_t">256</td>
<td id="S4.T4.4.4.4.5.1.4" class="ltx_td ltx_align_center ltx_border_t">102</td>
<td id="S4.T4.4.4.4.5.1.5" class="ltx_td ltx_align_center ltx_border_t">51.3</td>
</tr>
<tr id="S4.T4.4.4.4.6.2" class="ltx_tr">
<td id="S4.T4.4.4.4.6.2.1" class="ltx_td ltx_align_left">L</td>
<td id="S4.T4.4.4.4.6.2.2" class="ltx_td ltx_align_center">(256, 512, 512)</td>
<td id="S4.T4.4.4.4.6.2.3" class="ltx_td ltx_align_center">512</td>
<td id="S4.T4.4.4.4.6.2.4" class="ltx_td ltx_align_center">85</td>
<td id="S4.T4.4.4.4.6.2.5" class="ltx_td ltx_align_center">51.6</td>
</tr>
<tr id="S4.T4.4.4.4.7.3" class="ltx_tr">
<td id="S4.T4.4.4.4.7.3.1" class="ltx_td ltx_align_left ltx_border_b">X</td>
<td id="S4.T4.4.4.4.7.3.2" class="ltx_td ltx_align_center ltx_border_b">(256, 512, 1024)</td>
<td id="S4.T4.4.4.4.7.3.3" class="ltx_td ltx_align_center ltx_border_b">256</td>
<td id="S4.T4.4.4.4.7.3.4" class="ltx_td ltx_align_center ltx_border_b">75</td>
<td id="S4.T4.4.4.4.7.3.5" class="ltx_td ltx_align_center ltx_border_b">51.3</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.6.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S4.T4.7.2" class="ltx_text" style="font-size:90%;">The ablation study on model scaling, we demonstrate the impact of different output dimensions of multi-scale layers and hidden layer dimensions on the final performance.</span></figcaption>
</figure>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">According to the results shown in Fig. <a href="#S4.F5" title="Figure 5 ‣ 4.2 Main Results ‣ 4 Experiments ‣ DEYOv3: DETR with YOLO for Real-time Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we observed that DEYOv3 outperforms YOLOv8 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite> in terms of performance. This can be attributed to the stronger classification capability of DEYOv3’s decoder. Although YOLOv8 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite> can provide higher-quality candidate boxes, it is challenging to translate them into effective performance. In other words, DEYOv3’s decoder exhibits a higher ability to improve classification accuracy, thereby achieving better performance in object detection tasks.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<div id="S4.T5.2.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:213.6pt;height:51.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-5.7pt,1.4pt) scale(0.949108114566157,0.949108114566157) ;">
<table id="S4.T5.2.2.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.2.2.2.2" class="ltx_tr">
<th id="S4.T5.2.2.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">Model</th>
<th id="S4.T5.2.2.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t">Epochs</th>
<th id="S4.T5.2.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AP<sub id="S4.T5.2.2.2.2.2.1" class="ltx_sub">5</sub><sub id="S4.T5.2.2.2.2.2.2" class="ltx_sub">0</sub>
</th>
<th id="S4.T5.2.2.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">mMR</th>
<th id="S4.T5.2.2.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Recall</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.2.2.2.3.1" class="ltx_tr">
<th id="S4.T5.2.2.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">DEYOv3-N</th>
<th id="S4.T5.2.2.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">72</th>
<td id="S4.T5.2.2.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t">87.1</td>
<td id="S4.T5.2.2.2.3.1.4" class="ltx_td ltx_align_center ltx_border_t">49.3</td>
<td id="S4.T5.2.2.2.3.1.5" class="ltx_td ltx_align_center ltx_border_t">95.0</td>
</tr>
<tr id="S4.T5.2.2.2.4.2" class="ltx_tr" style="background-color:#F2F2F2;">
<th id="S4.T5.2.2.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b"><span id="S4.T5.2.2.2.4.2.1.1" class="ltx_text" style="background-color:#F2F2F2;">DEYOv3-N+</span></th>
<th id="S4.T5.2.2.2.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b"><span id="S4.T5.2.2.2.4.2.2.1" class="ltx_text" style="background-color:#F2F2F2;">300</span></th>
<td id="S4.T5.2.2.2.4.2.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T5.2.2.2.4.2.3.1" class="ltx_text" style="background-color:#F2F2F2;">86.7</span></td>
<td id="S4.T5.2.2.2.4.2.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T5.2.2.2.4.2.4.1" class="ltx_text" style="background-color:#F2F2F2;">49.3</span></td>
<td id="S4.T5.2.2.2.4.2.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T5.2.2.2.4.2.5.1" class="ltx_text" style="background-color:#F2F2F2;">93.4</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T5.4.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="S4.T5.5.2" class="ltx_text" style="font-size:90%;">The comparison of results between DEYOv3 and its improved version DEYOv3+ on CrowdHuman.</span></figcaption>
</figure>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2309.11851/assets/6.png" id="S4.F6.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="269" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.3.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.4.2" class="ltx_text" style="font-size:90%;">The architecture of the improved DEYOv3 model follows the design of DEYOv2, which allows the decoder to obtain higher-quality proposal bounding boxes.</span></figcaption>
</figure>
<figure id="S4.T6" class="ltx_table">
<div id="S4.T6.2.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:223.6pt;height:85.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-52.3pt,20.1pt) scale(0.681451126459319,0.681451126459319) ;">
<table id="S4.T6.2.2.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T6.2.2.2.2" class="ltx_tr">
<th id="S4.T6.2.2.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">Row</th>
<th id="S4.T6.2.2.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t">Model</th>
<th id="S4.T6.2.2.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t">epochs</th>
<th id="S4.T6.2.2.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">o2m</th>
<th id="S4.T6.2.2.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">o2o</th>
<th id="S4.T6.2.2.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">pad</th>
<th id="S4.T6.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AP<sub id="S4.T6.1.1.1.1.1.1" class="ltx_sub"><span id="S4.T6.1.1.1.1.1.1.1" class="ltx_text ltx_font_italic">50(o2m)</span></sub>
</th>
<th id="S4.T6.2.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AP<sub id="S4.T6.2.2.2.2.2.1" class="ltx_sub"><span id="S4.T6.2.2.2.2.2.1.1" class="ltx_text ltx_font_italic">50(o2o)</span></sub>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T6.2.2.2.3.1" class="ltx_tr">
<th id="S4.T6.2.2.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">1</th>
<th id="S4.T6.2.2.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">YOLOv8-N</th>
<th id="S4.T6.2.2.2.3.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">300</th>
<td id="S4.T6.2.2.2.3.1.4" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S4.T6.2.2.2.3.1.5" class="ltx_td ltx_border_t"></td>
<td id="S4.T6.2.2.2.3.1.6" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S4.T6.2.2.2.3.1.7" class="ltx_td ltx_align_center ltx_border_t">83.0</td>
<td id="S4.T6.2.2.2.3.1.8" class="ltx_td ltx_align_center ltx_border_t">–</td>
</tr>
<tr id="S4.T6.2.2.2.4.2" class="ltx_tr">
<th id="S4.T6.2.2.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">2</th>
<th id="S4.T6.2.2.2.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">YOLOv8-N</th>
<th id="S4.T6.2.2.2.4.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_row">300</th>
<td id="S4.T6.2.2.2.4.2.4" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T6.2.2.2.4.2.5" class="ltx_td"></td>
<td id="S4.T6.2.2.2.4.2.6" class="ltx_td"></td>
<td id="S4.T6.2.2.2.4.2.7" class="ltx_td ltx_align_center">83.0</td>
<td id="S4.T6.2.2.2.4.2.8" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S4.T6.2.2.2.5.3" class="ltx_tr">
<th id="S4.T6.2.2.2.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">3</th>
<th id="S4.T6.2.2.2.5.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">DEYOv3-N</th>
<th id="S4.T6.2.2.2.5.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_row">72</th>
<td id="S4.T6.2.2.2.5.3.4" class="ltx_td"></td>
<td id="S4.T6.2.2.2.5.3.5" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T6.2.2.2.5.3.6" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T6.2.2.2.5.3.7" class="ltx_td ltx_align_center">–</td>
<td id="S4.T6.2.2.2.5.3.8" class="ltx_td ltx_align_center">86.7</td>
</tr>
<tr id="S4.T6.2.2.2.6.4" class="ltx_tr">
<th id="S4.T6.2.2.2.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">4</th>
<th id="S4.T6.2.2.2.6.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">DEYOv3-N</th>
<th id="S4.T6.2.2.2.6.4.3" class="ltx_td ltx_align_center ltx_th ltx_th_row">72</th>
<td id="S4.T6.2.2.2.6.4.4" class="ltx_td"></td>
<td id="S4.T6.2.2.2.6.4.5" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T6.2.2.2.6.4.6" class="ltx_td"></td>
<td id="S4.T6.2.2.2.6.4.7" class="ltx_td ltx_align_center">–</td>
<td id="S4.T6.2.2.2.6.4.8" class="ltx_td ltx_align_center">87.1</td>
</tr>
<tr id="S4.T6.2.2.2.7.5" class="ltx_tr">
<th id="S4.T6.2.2.2.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">5</th>
<th id="S4.T6.2.2.2.7.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">DEYOv3-N</th>
<th id="S4.T6.2.2.2.7.5.3" class="ltx_td ltx_align_center ltx_th ltx_th_row">72</th>
<td id="S4.T6.2.2.2.7.5.4" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T6.2.2.2.7.5.5" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T6.2.2.2.7.5.6" class="ltx_td"></td>
<td id="S4.T6.2.2.2.7.5.7" class="ltx_td ltx_align_center">84.1</td>
<td id="S4.T6.2.2.2.7.5.8" class="ltx_td ltx_align_center">86.7</td>
</tr>
<tr id="S4.T6.2.2.2.8.6" class="ltx_tr">
<th id="S4.T6.2.2.2.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">6</th>
<th id="S4.T6.2.2.2.8.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b">  DEYOv3-N+</th>
<th id="S4.T6.2.2.2.8.6.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b">300</th>
<td id="S4.T6.2.2.2.8.6.4" class="ltx_td ltx_align_center ltx_border_b">✓</td>
<td id="S4.T6.2.2.2.8.6.5" class="ltx_td ltx_align_center ltx_border_b">✓</td>
<td id="S4.T6.2.2.2.8.6.6" class="ltx_td ltx_border_b"></td>
<td id="S4.T6.2.2.2.8.6.7" class="ltx_td ltx_align_center ltx_border_b">84.2</td>
<td id="S4.T6.2.2.2.8.6.8" class="ltx_td ltx_align_center ltx_border_b">86.7</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T6.4.1.1" class="ltx_text" style="font-size:90%;">Table 6</span>: </span><span id="S4.T6.5.2" class="ltx_text" style="font-size:90%;">More ablation study on DEYOv3, where pad refers to filling the image borders with gray color.</span></figcaption>
</figure>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Revisiting the DEYO</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">To enhance the quality of candidate boxes generated by DEYOv3’s decoder, we further integrated DETR with YOLO. Specifically, the improved DEYOv3 incorporates a one-to-many branch to generate candidate boxes. Additionally, we introduced DEYOv2’s rank feature <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite>, greedy matching, and query filter. Apart from architectural improvements, we conducted joint training in the second stage to ensure that the one-to-many branch provides high-quality candidate boxes. We transformed the original prediction for one-to-one object sets into a multi-objective optimization problem. Specifically, while using one-to-one matching to supervise the one-to-one branch, we employed a loss function based on one-to-many matching to supervise the one-to-one branch. This process can be described as follows:</p>
<table id="S6.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S4.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S4.E2.m1.1" class="ltx_Math" alttext="\displaystyle{\color[rgb]{0,0,0}L_{total}=L_{o2m}+L_{o2o}}" display="inline"><semantics id="S4.E2.m1.1a"><mrow id="S4.E2.m1.1.1" xref="S4.E2.m1.1.1.cmml"><msub id="S4.E2.m1.1.1.2" xref="S4.E2.m1.1.1.2.cmml"><mi mathcolor="#000000" id="S4.E2.m1.1.1.2.2" xref="S4.E2.m1.1.1.2.2.cmml">L</mi><mrow id="S4.E2.m1.1.1.2.3" xref="S4.E2.m1.1.1.2.3.cmml"><mi mathcolor="#000000" id="S4.E2.m1.1.1.2.3.2" xref="S4.E2.m1.1.1.2.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.2.3.1" xref="S4.E2.m1.1.1.2.3.1.cmml">​</mo><mi mathcolor="#000000" id="S4.E2.m1.1.1.2.3.3" xref="S4.E2.m1.1.1.2.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.2.3.1a" xref="S4.E2.m1.1.1.2.3.1.cmml">​</mo><mi mathcolor="#000000" id="S4.E2.m1.1.1.2.3.4" xref="S4.E2.m1.1.1.2.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.2.3.1b" xref="S4.E2.m1.1.1.2.3.1.cmml">​</mo><mi mathcolor="#000000" id="S4.E2.m1.1.1.2.3.5" xref="S4.E2.m1.1.1.2.3.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.2.3.1c" xref="S4.E2.m1.1.1.2.3.1.cmml">​</mo><mi mathcolor="#000000" id="S4.E2.m1.1.1.2.3.6" xref="S4.E2.m1.1.1.2.3.6.cmml">l</mi></mrow></msub><mo mathcolor="#000000" id="S4.E2.m1.1.1.1" xref="S4.E2.m1.1.1.1.cmml">=</mo><mrow id="S4.E2.m1.1.1.3" xref="S4.E2.m1.1.1.3.cmml"><msub id="S4.E2.m1.1.1.3.2" xref="S4.E2.m1.1.1.3.2.cmml"><mi mathcolor="#000000" id="S4.E2.m1.1.1.3.2.2" xref="S4.E2.m1.1.1.3.2.2.cmml">L</mi><mrow id="S4.E2.m1.1.1.3.2.3" xref="S4.E2.m1.1.1.3.2.3.cmml"><mi mathcolor="#000000" id="S4.E2.m1.1.1.3.2.3.2" xref="S4.E2.m1.1.1.3.2.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.2.3.1" xref="S4.E2.m1.1.1.3.2.3.1.cmml">​</mo><mn mathcolor="#000000" id="S4.E2.m1.1.1.3.2.3.3" xref="S4.E2.m1.1.1.3.2.3.3.cmml">2</mn><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.2.3.1a" xref="S4.E2.m1.1.1.3.2.3.1.cmml">​</mo><mi mathcolor="#000000" id="S4.E2.m1.1.1.3.2.3.4" xref="S4.E2.m1.1.1.3.2.3.4.cmml">m</mi></mrow></msub><mo mathcolor="#000000" id="S4.E2.m1.1.1.3.1" xref="S4.E2.m1.1.1.3.1.cmml">+</mo><msub id="S4.E2.m1.1.1.3.3" xref="S4.E2.m1.1.1.3.3.cmml"><mi mathcolor="#000000" id="S4.E2.m1.1.1.3.3.2" xref="S4.E2.m1.1.1.3.3.2.cmml">L</mi><mrow id="S4.E2.m1.1.1.3.3.3" xref="S4.E2.m1.1.1.3.3.3.cmml"><mi mathcolor="#000000" id="S4.E2.m1.1.1.3.3.3.2" xref="S4.E2.m1.1.1.3.3.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.3.3.1" xref="S4.E2.m1.1.1.3.3.3.1.cmml">​</mo><mn mathcolor="#000000" id="S4.E2.m1.1.1.3.3.3.3" xref="S4.E2.m1.1.1.3.3.3.3.cmml">2</mn><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.3.3.1a" xref="S4.E2.m1.1.1.3.3.3.1.cmml">​</mo><mi mathcolor="#000000" id="S4.E2.m1.1.1.3.3.3.4" xref="S4.E2.m1.1.1.3.3.3.4.cmml">o</mi></mrow></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.1b"><apply id="S4.E2.m1.1.1.cmml" xref="S4.E2.m1.1.1"><eq id="S4.E2.m1.1.1.1.cmml" xref="S4.E2.m1.1.1.1"></eq><apply id="S4.E2.m1.1.1.2.cmml" xref="S4.E2.m1.1.1.2"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.2.1.cmml" xref="S4.E2.m1.1.1.2">subscript</csymbol><ci id="S4.E2.m1.1.1.2.2.cmml" xref="S4.E2.m1.1.1.2.2">𝐿</ci><apply id="S4.E2.m1.1.1.2.3.cmml" xref="S4.E2.m1.1.1.2.3"><times id="S4.E2.m1.1.1.2.3.1.cmml" xref="S4.E2.m1.1.1.2.3.1"></times><ci id="S4.E2.m1.1.1.2.3.2.cmml" xref="S4.E2.m1.1.1.2.3.2">𝑡</ci><ci id="S4.E2.m1.1.1.2.3.3.cmml" xref="S4.E2.m1.1.1.2.3.3">𝑜</ci><ci id="S4.E2.m1.1.1.2.3.4.cmml" xref="S4.E2.m1.1.1.2.3.4">𝑡</ci><ci id="S4.E2.m1.1.1.2.3.5.cmml" xref="S4.E2.m1.1.1.2.3.5">𝑎</ci><ci id="S4.E2.m1.1.1.2.3.6.cmml" xref="S4.E2.m1.1.1.2.3.6">𝑙</ci></apply></apply><apply id="S4.E2.m1.1.1.3.cmml" xref="S4.E2.m1.1.1.3"><plus id="S4.E2.m1.1.1.3.1.cmml" xref="S4.E2.m1.1.1.3.1"></plus><apply id="S4.E2.m1.1.1.3.2.cmml" xref="S4.E2.m1.1.1.3.2"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.3.2.1.cmml" xref="S4.E2.m1.1.1.3.2">subscript</csymbol><ci id="S4.E2.m1.1.1.3.2.2.cmml" xref="S4.E2.m1.1.1.3.2.2">𝐿</ci><apply id="S4.E2.m1.1.1.3.2.3.cmml" xref="S4.E2.m1.1.1.3.2.3"><times id="S4.E2.m1.1.1.3.2.3.1.cmml" xref="S4.E2.m1.1.1.3.2.3.1"></times><ci id="S4.E2.m1.1.1.3.2.3.2.cmml" xref="S4.E2.m1.1.1.3.2.3.2">𝑜</ci><cn type="integer" id="S4.E2.m1.1.1.3.2.3.3.cmml" xref="S4.E2.m1.1.1.3.2.3.3">2</cn><ci id="S4.E2.m1.1.1.3.2.3.4.cmml" xref="S4.E2.m1.1.1.3.2.3.4">𝑚</ci></apply></apply><apply id="S4.E2.m1.1.1.3.3.cmml" xref="S4.E2.m1.1.1.3.3"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.3.3.1.cmml" xref="S4.E2.m1.1.1.3.3">subscript</csymbol><ci id="S4.E2.m1.1.1.3.3.2.cmml" xref="S4.E2.m1.1.1.3.3.2">𝐿</ci><apply id="S4.E2.m1.1.1.3.3.3.cmml" xref="S4.E2.m1.1.1.3.3.3"><times id="S4.E2.m1.1.1.3.3.3.1.cmml" xref="S4.E2.m1.1.1.3.3.3.1"></times><ci id="S4.E2.m1.1.1.3.3.3.2.cmml" xref="S4.E2.m1.1.1.3.3.3.2">𝑜</ci><cn type="integer" id="S4.E2.m1.1.1.3.3.3.3.cmml" xref="S4.E2.m1.1.1.3.3.3.3">2</cn><ci id="S4.E2.m1.1.1.3.3.3.4.cmml" xref="S4.E2.m1.1.1.3.3.3.4">𝑜</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.1c">\displaystyle{\color[rgb]{0,0,0}L_{total}=L_{o2m}+L_{o2o}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.1" class="ltx_p">However, despite our efforts to improve DEYOv3, its performance did not reach the desired level compared to DEYOv2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite>. This may be attributed to the insufficient feature representation provided without the use of an encoder in DEYOv3, which fails to suppress redundant bounding boxes effectively. As shown in Fig. <a href="#S4.F6" title="Figure 6 ‣ 4.4 Analysis ‣ 4 Experiments ‣ DEYOv3: DETR with YOLO for Real-time Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, the model’s improvement on candidate boxes is limited at larger scales, while the enhancement in classification capability surpasses the improvement in candidate box quality. We believe that with a well-designed scale, a balance can be achieved between the quality of candidate boxes and the strength of classification features, thereby overcoming the bottleneck caused by candidate boxes and further enhancing the performance of DEYOv3.</p>
</div>
<div id="S4.SS5.p3" class="ltx_para">
<p id="S4.SS5.p3.1" class="ltx_p">We conducted additional ablation experiments in Table <a href="#S4.T6" title="Table 6 ‣ 4.4 Analysis ‣ 4 Experiments ‣ DEYOv3: DETR with YOLO for Real-time Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. The results in rows 1, 2, and 3 indicate that training the o2m branch without using gray padding to fill the image borders ultimately improves the performance of DEYOv3, suggesting significant room for improvement in DEYOv3 on COCO. The results in rows 4, 5, and 6 demonstrate that joint training can effectively accommodate both the one-to-one branch and the one-to-many branch.</p>
</div>
<div id="S4.SS5.p4" class="ltx_para">
<p id="S4.SS5.p4.1" class="ltx_p">Why don’t we use NMS to suppress redundant bounding boxes, which can significantly reduce the requirements for the model’s classification capability? However, we found that this approach still limits the detector’s performance due to the bottleneck caused by NMS. As shown in Fig. <a href="#S4.F7" title="Figure 7 ‣ 4.5 Revisiting the DEYO ‣ 4 Experiments ‣ DEYOv3: DETR with YOLO for Real-time Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, We found that the padded queries contribute nothing to the final results, indicating that the candidate boxes filtered by NMS are effective queries. In the decoding stage, there is no way to compensate for the performance loss caused by NMS incorrectly removing boxes. Furthermore, the latency of NMS inference is unstable, which significantly affects the speed of the detector. Query filter can avoid these issues and potentially achieve ideal+ theoretical performance under ideal conditions.</p>
</div>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2309.11851/assets/8.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="419" height="199" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S4.F7.3.2" class="ltx_text" style="font-size:90%;">Padding queries do not contribute anything to the final performance.</span></figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Related Work</h2>

<figure id="S5.F8" class="ltx_figure"><img src="/html/2309.11851/assets/7.jpg" id="S5.F8.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="228" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F8.3.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S5.F8.4.2" class="ltx_text" style="font-size:90%;">As shown in the figure, from the results, it intuitively shows that DEYOv3-N achieves better performance with lower computational cost.
</span></figcaption>
</figure>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>You Only Look Once</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Over the years, the YOLO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>, <a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>, <a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite> series has been one of the best single-stage real-time object detector categories. YOLO transforms the object detection task into a regression problem, predicting the positions and categories of multiple objects in a single forward pass, achieving high-speed object detection. After years of development, YOLO has developed into a series of fast models with good performance. Anchor-based YOLO methods include YOLOv4 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite>, YOLOv5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite>, and YOLOv7 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite>, while anchor-free methods are YOLOX <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite>, YOLOv6 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite>, and YOLOv8 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>. Considering the performance of these detectors, anchor-free methods perform as well as anchor-based methods, and anchor boxes are no longer the main factor limiting the development of YOLO. However, all YOLO variants generate many redundant bounding boxes, which NMS must filter out during the prediction stage, which has a significant impact on the accuracy and speed of the detector and conflicts with the design theory of real-time object detectors.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>End-to-end Object Detectors</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Carion et al. proposed an end-to-end object detector based on transformers, named DETR (DEtection TRansformer) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite>, which has attracted significant attention from researchers due to its end-to-end nature in object detection. Specifically, DETR eliminates the anchor and NMS components in traditional detection pipelines and adopts a bipartite graph matching label assignment method to directly predict one-to-one sets of objects. This strategy greatly simplifies the object detection process and alleviates the performance bottleneck caused by NMS. However, DETR suffers from slow convergence speed and query ambiguity issues. To address these problems, several variants of DETR have been proposed, such as Deformable-DET <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite>, Conditional-DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite>, Anchor-DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite>, DAB-DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite>, DN-DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite>, and DINO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>. Deformable-DETR enhances the efficiency of attention mechanisms and accelerates training convergence by utilizing multi-scale features. Conditional-DETR and Anchor-DETR reduce the optimization difficulty of queries. DAB-DETR introduces 4D reference points and optimizes predicted boxes layer by layer. DN-DETR speeds up training convergence by introducing query denoising. DINO improves upon previous work and achieves state-of-the-art results. However, the aforementioned improvements do not address the issue of high computational cost in DETR. RT-DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite> designs an efficient hybrid encoder to replace the original transformer encoder, reducing unnecessary computational redundancy in the DETR encoder and proposing the first end-to-end object detector.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>DETR with YOLO</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">DEYO (DETR with YOLO) addresses the issues of DETR from a novel perspective. By combining the strengths of both classical detectors and the DETR-like model, DEYO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite> achieved state-of-the-art performance at the time. However, DEYO heavily relies on NMS to filter out redundant boxes from the classical detector to avoid impacting the optimization of one-to-one matching. DEYOv2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite> overcomes this dependency on NMS by adopting greedy matching and ranking features. It is the first complete end-to-end model that combines the advantages of classical detectors and query-based object detection models.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We propose a new training method called step-by-step training, and leveraging this method, we introduce a novel real-time end-to-end detector named DEYOv3. DEYOv3 surpasses all existing real-time detectors without the need for additional training data.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">We hope this work will find wide application in practical scenarios and inspire researchers. However, this novel detector design also brings new challenges, particularly in large-scale model design and overcoming localization deficiencies. We believe that the DEYOv3 model has tremendous potential, similar to DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite>, and we look forward to future work successfully addressing these challenges faced by DEYOv3.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib1.5.5.1" class="ltx_text" style="font-size:90%;">Bochkovskiy et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib1.7.1" class="ltx_text" style="font-size:90%;">
Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.8.1" class="ltx_text" style="font-size:90%;">Yolov4: Optimal speed and accuracy of object detection.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib1.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib1.10.2" class="ltx_text" style="font-size:90%;">, abs/2004.10934, 2020.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib2.5.5.1" class="ltx_text" style="font-size:90%;">Carion et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib2.7.1" class="ltx_text" style="font-size:90%;">
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander
Kirillov, and Sergey Zagoruyko.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.8.1" class="ltx_text" style="font-size:90%;">End-to-end object detection with transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib2.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Computer Vision – ECCV 2020</em><span id="bib.bib2.11.3" class="ltx_text" style="font-size:90%;">, pages 213–229, Cham, 2020.
Springer International Publishing.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib3.5.5.1" class="ltx_text" style="font-size:90%;">Deng et al. [2009]</span></span>
<span class="ltx_bibblock"><span id="bib.bib3.7.1" class="ltx_text" style="font-size:90%;">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, K. Li, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.8.1" class="ltx_text" style="font-size:90%;">Imagenet: A large-scale hierarchical image database.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib3.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">2009 IEEE Conference on Computer Vision and Pattern
Recognition</em><span id="bib.bib3.10.2" class="ltx_text" style="font-size:90%;">, pages 248–255, 2009.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib4.5.5.1" class="ltx_text" style="font-size:90%;">Gao et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib4.7.1" class="ltx_text" style="font-size:90%;">
Peng Gao, Minghang Zheng, Xiaogang Wang, Jifeng Dai, and Hongsheng Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.8.1" class="ltx_text" style="font-size:90%;">Fast convergence of detr with spatially modulated co-attention.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib4.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">2021 IEEE/CVF International Conference on Computer Vision
(ICCV)</em><span id="bib.bib4.10.2" class="ltx_text" style="font-size:90%;">, pages 3601–3610, 2021.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib5.5.5.1" class="ltx_text" style="font-size:90%;">Ge et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib5.7.1" class="ltx_text" style="font-size:90%;">
Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.8.1" class="ltx_text" style="font-size:90%;">Yolox: Exceeding yolo series in 2021.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib5.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib5.10.2" class="ltx_text" style="font-size:90%;">, abs/2107.08430, 2021.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib6.3.3.1" class="ltx_text" style="font-size:90%;">[6]</span></span>
<span class="ltx_bibblock"><span id="bib.bib6.5.1" class="ltx_text" style="font-size:90%;">
Jocher Glenn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.6.1" class="ltx_text" style="font-size:90%;">Yolov8.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/ultralytics/ultralytics/tree/main" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/ultralytics/ultralytics/tree/main</a><span id="bib.bib6.7.1" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib7.4.4.1" class="ltx_text" style="font-size:90%;">Glenn [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib7.6.1" class="ltx_text" style="font-size:90%;">
Jocher Glenn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.7.1" class="ltx_text" style="font-size:90%;">Yolov5 release v7.0.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.8.1" class="ltx_text" style="font-size:90%;">2022.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/ultralytics/yolov5/tree/v7.0" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/ultralytics/yolov5/tree/v7.0</a><span id="bib.bib7.9.1" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib8.4.4.1" class="ltx_text" style="font-size:90%;">Kingma and Ba [2014]</span></span>
<span class="ltx_bibblock"><span id="bib.bib8.6.1" class="ltx_text" style="font-size:90%;">
Diederik P. Kingma and Jimmy Ba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.7.1" class="ltx_text" style="font-size:90%;">Adam: A method for stochastic optimization.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib8.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">CoRR</em><span id="bib.bib8.9.2" class="ltx_text" style="font-size:90%;">, abs/1412.6980, 2014.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib9.5.5.1" class="ltx_text" style="font-size:90%;">LeCun et al. [1998]</span></span>
<span class="ltx_bibblock"><span id="bib.bib9.7.1" class="ltx_text" style="font-size:90%;">
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.8.1" class="ltx_text" style="font-size:90%;">Gradient-based learning applied to document recognition.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib9.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. IEEE</em><span id="bib.bib9.10.2" class="ltx_text" style="font-size:90%;">, 86:2278–2324, 1998.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib10.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2022a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib10.7.1" class="ltx_text" style="font-size:90%;">
Chuyin Li, Lu Li, Hongliang Jiang, Kaiheng Weng, Yifei Geng, L. Li, Zaidan Ke,
Qingyuan Li, Meng Cheng, Weiqiang Nie, Yiduo Li, Bo Zhang, Yufei Liang,
Linyuan Zhou, Xiaoming Xu, Xiangxiang Chu, Xiaoming Wei, and Xiaolin Wei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.8.1" class="ltx_text" style="font-size:90%;">Yolov6: A single-stage object detection framework for industrial
applications.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib10.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib10.10.2" class="ltx_text" style="font-size:90%;">, abs/2209.02976, 2022a.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib11.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2022b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib11.7.1" class="ltx_text" style="font-size:90%;">
Feng Li, Hao Zhang, Shi guang Liu, Jian Guo, Lionel Ming shuan Ni, and Lei
Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.8.1" class="ltx_text" style="font-size:90%;">Dn-detr: Accelerate detr training by introducing query denoising.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib11.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">2022 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</em><span id="bib.bib11.10.2" class="ltx_text" style="font-size:90%;">, pages 13609–13617, 2022b.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib12.5.5.1" class="ltx_text" style="font-size:90%;">Lin et al. [2014]</span></span>
<span class="ltx_bibblock"><span id="bib.bib12.7.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C. Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.8.1" class="ltx_text" style="font-size:90%;">Microsoft coco: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib12.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</em><span id="bib.bib12.11.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib13.5.5.1" class="ltx_text" style="font-size:90%;">Lin et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib13.7.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Piotr Dollár, Ross B. Girshick, Kaiming He, Bharath
Hariharan, and Serge J. Belongie.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.8.1" class="ltx_text" style="font-size:90%;">Feature pyramid networks for object detection.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib13.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">2017 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</em><span id="bib.bib13.10.2" class="ltx_text" style="font-size:90%;">, pages 936–944, 2016.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib14.4.4.1" class="ltx_text" style="font-size:90%;">[14]</span></span>
<span class="ltx_bibblock"><span id="bib.bib14.6.1" class="ltx_text" style="font-size:90%;">
Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, and
Lei Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.7.1" class="ltx_text" style="font-size:90%;">Dab-detr: Dynamic anchor boxes are better queries for detr.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib14.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations</em><span id="bib.bib14.10.3" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib15.5.5.1" class="ltx_text" style="font-size:90%;">Liu et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib15.7.1" class="ltx_text" style="font-size:90%;">
Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.8.1" class="ltx_text" style="font-size:90%;">Path aggregation network for instance segmentation.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib15.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">2018 IEEE/CVF Conference on Computer Vision and Pattern
Recognition</em><span id="bib.bib15.10.2" class="ltx_text" style="font-size:90%;">, pages 8759–8768, 2018.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib16.5.5.1" class="ltx_text" style="font-size:90%;">Lv et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib16.7.1" class="ltx_text" style="font-size:90%;">
Wenyu Lv, Shangliang Xu, Yian Zhao, Guanzhong Wang, Jinman Wei, Cheng Cui,
Yuning Du, Qingqing Dang, and Yi Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.8.1" class="ltx_text" style="font-size:90%;">Detrs beat yolos on real-time object detection.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib16.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib16.10.2" class="ltx_text" style="font-size:90%;">, abs/2304.08069, 2023.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib17.5.5.1" class="ltx_text" style="font-size:90%;">Meng et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib17.7.1" class="ltx_text" style="font-size:90%;">
Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan, Lei
Sun, and Jingdong Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.8.1" class="ltx_text" style="font-size:90%;">Conditional detr for fast training convergence.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib17.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">2021 IEEE/CVF International Conference on Computer Vision
(ICCV)</em><span id="bib.bib17.10.2" class="ltx_text" style="font-size:90%;">, pages 3631–3640, 2021.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib18.4.4.1" class="ltx_text" style="font-size:90%;">Ouyang [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib18.6.1" class="ltx_text" style="font-size:90%;">
Hao Ouyang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.7.1" class="ltx_text" style="font-size:90%;">Deyo: Detr with yolo for step-by-step object detection.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib18.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib18.9.2" class="ltx_text" style="font-size:90%;">, abs/2211.06588, 2022.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib19.4.4.1" class="ltx_text" style="font-size:90%;">Ouyang [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib19.6.1" class="ltx_text" style="font-size:90%;">
Hao Ouyang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.7.1" class="ltx_text" style="font-size:90%;">Deyov2: Rank feature with greedy matching for end-to-end object
detection.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib19.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib19.9.2" class="ltx_text" style="font-size:90%;">, abs/2306.09165, 2023.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib20.4.4.1" class="ltx_text" style="font-size:90%;">Redmon and Farhadi [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib20.6.1" class="ltx_text" style="font-size:90%;">
Joseph Redmon and Ali Farhadi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.7.1" class="ltx_text" style="font-size:90%;">Yolo9000: Better, faster, stronger.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib20.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">2017 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</em><span id="bib.bib20.9.2" class="ltx_text" style="font-size:90%;">, pages 6517–6525, 2016.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib21.4.4.1" class="ltx_text" style="font-size:90%;">Redmon and Farhadi [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib21.6.1" class="ltx_text" style="font-size:90%;">
Joseph Redmon and Ali Farhadi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.7.1" class="ltx_text" style="font-size:90%;">Yolov3: An incremental improvement.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib21.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib21.9.2" class="ltx_text" style="font-size:90%;">, abs/1804.02767, 2018.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib22.5.5.1" class="ltx_text" style="font-size:90%;">Redmon et al. [2015]</span></span>
<span class="ltx_bibblock"><span id="bib.bib22.7.1" class="ltx_text" style="font-size:90%;">
Joseph Redmon, Santosh Kumar Divvala, Ross B. Girshick, and Ali Farhadi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.8.1" class="ltx_text" style="font-size:90%;">You only look once: Unified, real-time object detection.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib22.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">2016 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</em><span id="bib.bib22.10.2" class="ltx_text" style="font-size:90%;">, pages 779–788, 2015.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib23.5.5.1" class="ltx_text" style="font-size:90%;">Roh et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib23.7.1" class="ltx_text" style="font-size:90%;">
Byung-Seok Roh, Jaewoong Shin, Wuhyun Shin, and Saehoon Kim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.8.1" class="ltx_text" style="font-size:90%;">Sparse detr: Efficient end-to-end object detection with learnable
sparsity.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib23.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib23.10.2" class="ltx_text" style="font-size:90%;">, abs/2111.14330, 2021.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib24.5.5.1" class="ltx_text" style="font-size:90%;">Shao et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib24.7.1" class="ltx_text" style="font-size:90%;">
Shuai Shao, Zijian Zhao, Boxun Li, Tete Xiao, Gang Yu, Xiangyu Zhang, and Jian
Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.8.1" class="ltx_text" style="font-size:90%;">Crowdhuman: A benchmark for detecting human in a crowd.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib24.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib24.10.2" class="ltx_text" style="font-size:90%;">, abs/1805.00123, 2018.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib25.5.5.1" class="ltx_text" style="font-size:90%;">Vaswani et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib25.7.1" class="ltx_text" style="font-size:90%;">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.8.1" class="ltx_text" style="font-size:90%;">Attention is all you need.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib25.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</em><span id="bib.bib25.10.2" class="ltx_text" style="font-size:90%;">, 30, 2017.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib26.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2022a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib26.7.1" class="ltx_text" style="font-size:90%;">
Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.8.1" class="ltx_text" style="font-size:90%;">Yolov7: Trainable bag-of-freebies sets new state-of-the-art for
real-time object detectors.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib26.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib26.10.2" class="ltx_text" style="font-size:90%;">, abs/2207.02696, 2022a.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib27.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2022b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib27.7.1" class="ltx_text" style="font-size:90%;">
Yingming Wang, X. Zhang, Tong Yang, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.8.1" class="ltx_text" style="font-size:90%;">Anchor detr: Query design for transformer-based detector.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib27.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib27.10.2" class="ltx_text" style="font-size:90%;">, abs/2109.07107, 2022b.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib28.5.5.1" class="ltx_text" style="font-size:90%;">Zhang et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib28.7.1" class="ltx_text" style="font-size:90%;">
Hao Zhang, Feng Li, Siyi Liu, Lei Zhang, Hang Su, Jun-Juan Zhu, Lionel Ming
shuan Ni, and Heung yeung Shum.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.8.1" class="ltx_text" style="font-size:90%;">Dino: Detr with improved denoising anchor boxes for end-to-end object
detection.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib28.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib28.10.2" class="ltx_text" style="font-size:90%;">, abs/2203.03605, 2022.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib29.4.4.1" class="ltx_text" style="font-size:90%;">[29]</span></span>
<span class="ltx_bibblock"><span id="bib.bib29.6.1" class="ltx_text" style="font-size:90%;">
Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.7.1" class="ltx_text" style="font-size:90%;">Deformable detr: Deformable transformers for end-to-end object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib29.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations</em><span id="bib.bib29.10.3" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2309.11850" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2309.11851" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2309.11851">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2309.11851" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2309.11852" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 05:00:39 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
