<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2108.01819] Transfer Learning for Pose Estimation of Illustrated Characters</title><meta property="og:description" content="Human pose information is a critical component in many downstream image processing tasks, such as activity recognition and motion tracking. Likewise, a pose estimator for the illustrated character domain would provide …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Transfer Learning for Pose Estimation of Illustrated Characters">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Transfer Learning for Pose Estimation of Illustrated Characters">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2108.01819">

<!--Generated on Tue Mar 19 07:53:34 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Transfer Learning for Pose Estimation of Illustrated Characters</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Shuhong Chen
<br class="ltx_break">University of Maryland - College Park
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">shuhong@cs.umd.edu</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Matthias Zwicker
<br class="ltx_break">University of Maryland - College Park
<br class="ltx_break"><span id="id2.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">zwicker@cs.umd.edu</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id3.id1" class="ltx_p">Human pose information is a critical component in many downstream image processing tasks, such as activity recognition and motion tracking. Likewise, a pose estimator for the illustrated character domain would provide a valuable prior for assistive content creation tasks, such as reference pose retrieval and automatic character animation. But while modern data-driven techniques have substantially improved pose estimation performance on natural images, little work has been done for illustrations. In our work, we bridge this domain gap by efficiently transfer-learning from both domain-specific and task-specific source models. Additionally, we upgrade and expand an existing illustrated pose estimation dataset, and introduce two new datasets for classification and segmentation subtasks. We then apply the resultant state-of-the-art character pose estimator to solve the novel task of pose-guided illustration retrieval. All data, models, and code will be made publicly available.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Human pose estimation is a foundational computer vision task with many real-world applications, such as activity recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, 3D reconstruction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, motion tracking <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, virtual try-on <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, person re-identification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, etc. The generic formulation is to find, in a given image containing people, the positions and orientations of body parts; typically, this means locating landmark and joint keypoints on 2D images, or regressing for bone transformations in 3D.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The usefulness of pose estimation is not limited to the natural image domain; in particular, we focus on the domain of illustrated characters. As pose-guided motion retargeting of realistic humans rapidly advances <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, there is growing potential for automatic pose-guided animation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, a traditionally labor-intensive task for both 2D and 3D artists. Pose information may also serve as a valuable prior in illustration colorization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>, keyframe interpolation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, 3D character reconstruction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and rigging <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, etc.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">With deep computer vision, we have been able to leverage large-scale datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> to train robust estimators of human pose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. However, little work has been done to solve pose estimation for illustrated characters. Previous pose estimation work on illustrations by Khungurn <em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S1.p3.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> presented a 2D keypoint detector, but relied on a publicly-unavailable synthetic dataset and an ImageNet-trained backbone. In addition, the dataset they collected for supervision lacked variation, and was missing keypoints and bounding boxes required for evaluation under the more modern COCO standard <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Facing these challenges, we constructed a 2D keypoint detector with state-of-the-art performance on illustrated characters, built upon domain-specific components and efficient transfer learning architectures. We demonstrate the effectiveness of our methods by implementing a novel illustration retrieval system. Summarizing, we contribute:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">A state-of-the-art pose estimator for illustrated characters, transfer-learned from both domain-specific and task-specific source models. Despite the absence of synthetic supervision, we outperform previous work by 10-20% PDJ@20 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">An application of our proposed pose estimator to solve the novel task of pose-guided character illustration retrieval.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Datasets for our model and its components, including: an updated COCO-compliant version of Khungurn <em id="S1.I1.i3.p1.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S1.I1.i3.p1.1.2" class="ltx_text"></span>’s <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> pose dataset with 2x the number of samples and more diverse poses; a novel 1062-class Danbooru <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> tagging rulebook; and a character segmentation dataset 20x larger than those currently available.</p>
</div>
</li>
</ul>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2108.01819/assets/figs/schematic.png" id="S1.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="226" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>A schematic outlining our two transfer learning architectures: feature concatenation, and feature matching. Note that source feature specificity is with respect to the target; i.e. task-specific means “related to pose estimation” and domain-specific means “related to illustrations”. Feature converters and matchers are convolutional networks that learn to mimic or re-appropriate pretrained features, respectively. While both designs require the pretrained Mask R-CNN components during training, feature matching discards them during inference, instead relying on the trained matcher network. “BCE” refers to binary cross-entropy loss.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>The Illustration Domain</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Though there has been work on caricatures and cartoons <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, we focus on anime/manga-style drawings where characters tend to be less abstract. While there is work for more traditional problems like lineart cleaning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> and sketch extraction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, more recent studies include sketch colorization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>, illustration segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>, painting relighting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>, image-to-image translation with photos <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, and keyframe interpolation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Available models for illustrated tasks typically rely on small manually-collected datasets. For example, the AniSeg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> character segmenter is trained on less than <math id="S2.SS1.p2.1.m1.2" class="ltx_Math" alttext="1{,}000" display="inline"><semantics id="S2.SS1.p2.1.m1.2a"><mrow id="S2.SS1.p2.1.m1.2.3.2" xref="S2.SS1.p2.1.m1.2.3.1.cmml"><mn id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml">1</mn><mo id="S2.SS1.p2.1.m1.2.3.2.1" xref="S2.SS1.p2.1.m1.2.3.1.cmml">,</mo><mn id="S2.SS1.p2.1.m1.2.2" xref="S2.SS1.p2.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.2b"><list id="S2.SS1.p2.1.m1.2.3.1.cmml" xref="S2.SS1.p2.1.m1.2.3.2"><cn type="integer" id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1">1</cn><cn type="integer" id="S2.SS1.p2.1.m1.2.2.cmml" xref="S2.SS1.p2.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.2c">1{,}000</annotation></semantics></math> examples. While larger datasets are becoming available (e.g. Danbooru <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> now with 4.2m tagged illustrations), the labels are noisy and long-tailed, leading to poor model performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. Works requiring pose information may use synthetic renders of anime-style 3D models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, but the models are usually not publicly available. In this work, we present a cleaner tag classification task, a large character segmentation dataset, and an upgraded COCO keypoint dataset; these will all be made available upon publication, and may serve as a valuable prior for other tasks.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Transfer Learning &amp; Domain Adaptation</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Transfer learning and domain adaptation have been defined somewhat inconsistently throughout the vision and natural language processing literature <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, though generally the former is considered broader than the latter. In this paper, we use the terms interchangeably, referring to methods that leverage information from a number of related source domains and tasks, to a specific target domain and task. Typically, much more data is available for the source than the target, motivating us to transfer useful related source knowledge in the absence of sufficient target data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>. For deep networks, the simplest practice is to pretrain a model on source data, and fine-tune its parameters on target data; however, various techniques have been studied that work with different levels of target data availability.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Much of the transfer learning work in vision focuses on extreme cases with significantly limited target domain data, with emphasis around the task of image classification. In the few-shot learning case, we may be given as few as ten (or even one) samples from the target, inviting methods that embed prototypical target data into a space learned through prior source knowledge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>. In particular, it is common to align parameters of feature extractors across domains, by directly minimizing pairwise feature distances or by adversarial domain discrimination <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>. If the source and target are similar enough, it is possible to perform domain adaptation in the complete absence of labeled target data. This can be achieved by matching statistical properties of extracted features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, or by converting inputs between domains through cycle-consistent image translation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Pose Estimation</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">With the availability of large-scale human pose datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, the vision community has recently been able to make great strides in pose estimation. A naive baseline was demonstrated by Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, which extended their detection and segmentation framework to predict single-pixel masks of joint locations. Other work such as RMPE take an approach tailored to pose estimation, deploying spatial transformer networks with pose-guided NMS and region proposal <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. Around the same time, OpenPose proposed part affinity fields as a bottom-up alternative to the more common heatmap representation of joints <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Human pose estimation work continues to make headway, extending beyond keypoint localization to include dense body part labels <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> and 3D pose estimation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Pose Estimation Transfer</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">Most transfer learning for pose estimation adapts from synthetically-rendered data to natural images. For example, by using mocaps and 3D human models, SURREAL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> provides 6 million frames of synthetic video, complete with a variety of datatypes (2D/3D pose, RGB, depth, optical flow, body parts, etc.). CNNs may be able to directly generalize pose from synthesized images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, and can further close the domain gap using other priors like motion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. Outside of synthetic-to-real, Cao <em id="S2.SS4.p1.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS4.p1.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> explore domain adaptation for quadruped animal pose estimation, achieving generalization from human pose through adversarial domain discrimination with pseudo-label training.</p>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<p id="S2.SS4.p2.1" class="ltx_p">The closest prior work to our topic was done by Khungurn <em id="S2.SS4.p2.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS4.p2.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, who collected a modest AnimeDrawingsDataset (ADD) of 2k character illustrations with joint keypoints, and a larger synthetic dataset of 1 million frames rendered from MikuMikuDance (MMD) 3D models and mocaps. Unfortunately, the MMD dataset is not publicly available, and ADD contains mostly standard forward-facing poses. In addition, ADD is missing bounding boxes and several face keypoints, which are necessary for evaluation under the modern COCO standard <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. We remedy these issues by training a bounding box detector from our new character segmentation dataset, labeling missing annotations in ADD, and labeling 2k additional samples in more varied poses.</p>
</div>
<div id="S2.SS4.p3" class="ltx_para">
<p id="S2.SS4.p3.1" class="ltx_p">Khungurn <em id="S2.SS4.p3.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS4.p3.1.2" class="ltx_text"></span> perform transfer from an ImageNet-pretrained GoogLeNet backbone <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> and synthetic MMD data. In the absence of MMD, we instead transfer from a stronger backbone trained on a new illustration-specific classification task, as well as from a task-specific model pretrained on COCO keypoints. We use our subtask models and data to implement a number of transfer techniques, from naive fine-tuning to adversarial domain discrimination. In doing so, we significantly outperform Khungurn <em id="S2.SS4.p3.1.3" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS4.p3.1.4" class="ltx_text"></span> on their reported metrics by 10-20%.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method &amp; Architectures</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We provide motivation and architecture details for two variants of our proposed pose estimator (feature concatenation and feature matching), as well as two submodules critical for their success (a class-balanced tagger backbone and a character segmentation model). Architectures for baseline comparison models are described in Sec. <a href="#S5.SS1" title="5.1 Pose Estimation Transfer ‣ 5 Experiments ‣ Transfer Learning for Pose Estimation of Illustrated Characters" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Pose Estimation Transfer Model</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We present two versions of our final model: feature concatenation, and feature matching. In this section, we assume that region proposals are given by a separate segmentation model (Sec. <a href="#S3.SS3" title="3.3 Character Segmentation &amp; Bounding Boxes ‣ 3 Method &amp; Architectures ‣ Transfer Learning for Pose Estimation of Illustrated Characters" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>), and that the domain-specific backbone is already available (Sec. <a href="#S3.SS2" title="3.2 ResNet Tagger ‣ 3 Method &amp; Architectures ‣ Transfer Learning for Pose Estimation of Illustrated Characters" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>); here, we focus on combining source features to predict keypoints (Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Transfer Learning for Pose Estimation of Illustrated Characters" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">The goal is to perform transfer simultaneously from both a domain-specific classification backbone (Sec <a href="#S3.SS2" title="3.2 ResNet Tagger ‣ 3 Method &amp; Architectures ‣ Transfer Learning for Pose Estimation of Illustrated Characters" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>) and a task-specific keypoint model (Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>). Here, we chose Mask R-CNN as it showed significantly better out-of-the-box generalization to illustrations than OpenPose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> (Tab. <a href="#S3.T1" title="Table 1 ‣ 3.3 Character Segmentation &amp; Bounding Boxes ‣ 3 Method &amp; Architectures ‣ Transfer Learning for Pose Estimation of Illustrated Characters" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). Taking into account that the task-specific model already achieves mediocre performance on the target domain, the feature concatenation model simply stacks features from both sources (Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Transfer Learning for Pose Estimation of Illustrated Characters" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). In order to perform the concatenation, it learns shallow feature converters for each source to decrease the feature channel count and allow bilinear sampling to a common higher resolution. The combined features are fed to the head, consisting of a shallow converter and two ResNet blocks.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">The final output is a stack of 25 heatmaps, 17 for COCO keypoints and 8 for auxiliary appendage midpoints (following Khungurn <em id="S3.SS1.p3.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S3.SS1.p3.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>). We apply pixel-wise binary cross-entropy loss on each heatmap, targeting a normal distribution centered on the ground-truth keypoint location with standard deviation proportional to the keypoint’s COCO OKS sigma <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>; the sigmas for auxiliary midpoints are averaged from endpoints of the body part. At inference, we gaussian-smooth the heatmaps and take the maximum pixel value index as the keypoint prediction.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">Although feature concatenation produces the best results (Tab. <a href="#S3.T1" title="Table 1 ‣ 3.3 Character Segmentation &amp; Bounding Boxes ‣ 3 Method &amp; Architectures ‣ Transfer Learning for Pose Estimation of Illustrated Characters" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>), it is very inefficient. At inference, it must maintain the parameters of both source models, and run both forward models for each prediction; Mask R-CNN is particularly expensive in this regard. We thus also provide a feature matching model, inspired by the methods used in Luo <em id="S3.SS1.p4.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S3.SS1.p4.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. As shown in Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Transfer Learning for Pose Estimation of Illustrated Characters" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we simultaneously train an additional matching network that predicts features from the expensive task-specific model using features from the domain-specific model. Though matching may be optimized with self-supervision signals such as contrastive loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>, we found that feature-wise mean-squared error is suitable. Given the matcher, the pretrained Mask R-CNN still helps training, but is not necessary at inference. Despite its simplicity, feature matching retains most performance benefits from both source models, while also being significantly lighter and faster than the concatenation architecture.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>ResNet Tagger</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The domain-specific backbone for our model (Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Transfer Learning for Pose Estimation of Illustrated Characters" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) is a pretrained ResNet50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> fine-tuned as an illustration tagger. The tagging task is equivalent to multi-label classification, in this case predicting the labels applied to an image by the Danbooru imageboard moderators <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. The 392k unique tags cover topics including colors, clothing, interactions, composition, and even copyright metainfo.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Khungurn <em id="S3.SS2.p2.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S3.SS2.p2.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> use an ImageNet-trained GoogLeNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> backbone for their illustrated pose estimator, but we find that Danbooru fine-tuning significantly boosts transfer performance. There are publicly-available Danbooru taggers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, but both their classification performance and feature learning capabilities are hindered by uninformative target tags and severe class imbalance. By alleviating these issues, we achieve significantly better transfer to pose estimation.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">Most available Danbooru taggers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> take a coarse approach to defining classes, simply predicting the several thousand (6-7k) most frequent tags. However, many of these tags represent contextual information not present in the image; e.g. neon_genesis_evangelion (name of a franchise), or alternate_costume (fanmade/non-canon clothes). We instead only allow tags explicitly describing the image (clothing, body parts, etc.). Selecting tags by frequency also introduces tag redundancy and annotator disagreement. There are many high-frequency tags that share similar concepts, but are annotated inconsistently; e.g. hand_in_hair, adjusting_hair, and hair_tucking have vague wiki definitions for taggers, and many color tags are subjective (aqua_hair vs. blue_hair). To address these challenges, we survey Danbooru wikis to manually develop a rulebook of tag groups that defines more explicit and less redundant classes.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.8" class="ltx_p">Danbooru tag frequencies form a long-tailed distribution, posing a severe class imbalance problem. In addition to filtering out under-tagged images (detailed in Sec. <a href="#S4.SS2" title="4.2 ResNet Tagger Data ‣ 4 Data Collection ‣ Transfer Learning for Pose Estimation of Illustrated Characters" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>), we implement an inverse square-root frequency reweighing scheme to emphasize the learning of less-frequent classes. More formally, the loss on a sample is:</p>
<table id="S8.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E1.m1.2" class="ltx_Math" alttext="\displaystyle\mathcal{L}(y,\hat{y})" display="inline"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.3" xref="S3.E1.m1.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.2.3.2" xref="S3.E1.m1.2.3.2.cmml">ℒ</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.3.1" xref="S3.E1.m1.2.3.1.cmml">​</mo><mrow id="S3.E1.m1.2.3.3.2" xref="S3.E1.m1.2.3.3.1.cmml"><mo stretchy="false" id="S3.E1.m1.2.3.3.2.1" xref="S3.E1.m1.2.3.3.1.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">y</mi><mo id="S3.E1.m1.2.3.3.2.2" xref="S3.E1.m1.2.3.3.1.cmml">,</mo><mover accent="true" id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml"><mi id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.2.cmml">y</mi><mo id="S3.E1.m1.2.2.1" xref="S3.E1.m1.2.2.1.cmml">^</mo></mover><mo stretchy="false" id="S3.E1.m1.2.3.3.2.3" xref="S3.E1.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.3.cmml" xref="S3.E1.m1.2.3"><times id="S3.E1.m1.2.3.1.cmml" xref="S3.E1.m1.2.3.1"></times><ci id="S3.E1.m1.2.3.2.cmml" xref="S3.E1.m1.2.3.2">ℒ</ci><interval closure="open" id="S3.E1.m1.2.3.3.1.cmml" xref="S3.E1.m1.2.3.3.2"><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝑦</ci><apply id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2"><ci id="S3.E1.m1.2.2.1.cmml" xref="S3.E1.m1.2.2.1">^</ci><ci id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.2">𝑦</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">\displaystyle\mathcal{L}(y,\hat{y})</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E1.m2.3" class="ltx_Math" alttext="\displaystyle=\frac{1}{C}\sum_{i=0}^{C-1}w_{i}(y_{i})BCE(y_{i},\hat{y}_{i})" display="inline"><semantics id="S3.E1.m2.3a"><mrow id="S3.E1.m2.3.3" xref="S3.E1.m2.3.3.cmml"><mi id="S3.E1.m2.3.3.5" xref="S3.E1.m2.3.3.5.cmml"></mi><mo id="S3.E1.m2.3.3.4" xref="S3.E1.m2.3.3.4.cmml">=</mo><mrow id="S3.E1.m2.3.3.3" xref="S3.E1.m2.3.3.3.cmml"><mstyle displaystyle="true" id="S3.E1.m2.3.3.3.5" xref="S3.E1.m2.3.3.3.5.cmml"><mfrac id="S3.E1.m2.3.3.3.5a" xref="S3.E1.m2.3.3.3.5.cmml"><mn id="S3.E1.m2.3.3.3.5.2" xref="S3.E1.m2.3.3.3.5.2.cmml">1</mn><mi id="S3.E1.m2.3.3.3.5.3" xref="S3.E1.m2.3.3.3.5.3.cmml">C</mi></mfrac></mstyle><mo lspace="0em" rspace="0em" id="S3.E1.m2.3.3.3.4" xref="S3.E1.m2.3.3.3.4.cmml">​</mo><mrow id="S3.E1.m2.3.3.3.3" xref="S3.E1.m2.3.3.3.3.cmml"><mstyle displaystyle="true" id="S3.E1.m2.3.3.3.3.4" xref="S3.E1.m2.3.3.3.3.4.cmml"><munderover id="S3.E1.m2.3.3.3.3.4a" xref="S3.E1.m2.3.3.3.3.4.cmml"><mo movablelimits="false" id="S3.E1.m2.3.3.3.3.4.2.2" xref="S3.E1.m2.3.3.3.3.4.2.2.cmml">∑</mo><mrow id="S3.E1.m2.3.3.3.3.4.2.3" xref="S3.E1.m2.3.3.3.3.4.2.3.cmml"><mi id="S3.E1.m2.3.3.3.3.4.2.3.2" xref="S3.E1.m2.3.3.3.3.4.2.3.2.cmml">i</mi><mo id="S3.E1.m2.3.3.3.3.4.2.3.1" xref="S3.E1.m2.3.3.3.3.4.2.3.1.cmml">=</mo><mn id="S3.E1.m2.3.3.3.3.4.2.3.3" xref="S3.E1.m2.3.3.3.3.4.2.3.3.cmml">0</mn></mrow><mrow id="S3.E1.m2.3.3.3.3.4.3" xref="S3.E1.m2.3.3.3.3.4.3.cmml"><mi id="S3.E1.m2.3.3.3.3.4.3.2" xref="S3.E1.m2.3.3.3.3.4.3.2.cmml">C</mi><mo id="S3.E1.m2.3.3.3.3.4.3.1" xref="S3.E1.m2.3.3.3.3.4.3.1.cmml">−</mo><mn id="S3.E1.m2.3.3.3.3.4.3.3" xref="S3.E1.m2.3.3.3.3.4.3.3.cmml">1</mn></mrow></munderover></mstyle><mrow id="S3.E1.m2.3.3.3.3.3" xref="S3.E1.m2.3.3.3.3.3.cmml"><msub id="S3.E1.m2.3.3.3.3.3.5" xref="S3.E1.m2.3.3.3.3.3.5.cmml"><mi id="S3.E1.m2.3.3.3.3.3.5.2" xref="S3.E1.m2.3.3.3.3.3.5.2.cmml">w</mi><mi id="S3.E1.m2.3.3.3.3.3.5.3" xref="S3.E1.m2.3.3.3.3.3.5.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m2.3.3.3.3.3.4" xref="S3.E1.m2.3.3.3.3.3.4.cmml">​</mo><mrow id="S3.E1.m2.1.1.1.1.1.1.1" xref="S3.E1.m2.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m2.1.1.1.1.1.1.1.2" xref="S3.E1.m2.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E1.m2.1.1.1.1.1.1.1.1" xref="S3.E1.m2.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m2.1.1.1.1.1.1.1.1.2" xref="S3.E1.m2.1.1.1.1.1.1.1.1.2.cmml">y</mi><mi id="S3.E1.m2.1.1.1.1.1.1.1.1.3" xref="S3.E1.m2.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.E1.m2.1.1.1.1.1.1.1.3" xref="S3.E1.m2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m2.3.3.3.3.3.4a" xref="S3.E1.m2.3.3.3.3.3.4.cmml">​</mo><mi id="S3.E1.m2.3.3.3.3.3.6" xref="S3.E1.m2.3.3.3.3.3.6.cmml">B</mi><mo lspace="0em" rspace="0em" id="S3.E1.m2.3.3.3.3.3.4b" xref="S3.E1.m2.3.3.3.3.3.4.cmml">​</mo><mi id="S3.E1.m2.3.3.3.3.3.7" xref="S3.E1.m2.3.3.3.3.3.7.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.E1.m2.3.3.3.3.3.4c" xref="S3.E1.m2.3.3.3.3.3.4.cmml">​</mo><mi id="S3.E1.m2.3.3.3.3.3.8" xref="S3.E1.m2.3.3.3.3.3.8.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.E1.m2.3.3.3.3.3.4d" xref="S3.E1.m2.3.3.3.3.3.4.cmml">​</mo><mrow id="S3.E1.m2.3.3.3.3.3.3.2" xref="S3.E1.m2.3.3.3.3.3.3.3.cmml"><mo stretchy="false" id="S3.E1.m2.3.3.3.3.3.3.2.3" xref="S3.E1.m2.3.3.3.3.3.3.3.cmml">(</mo><msub id="S3.E1.m2.2.2.2.2.2.2.1.1" xref="S3.E1.m2.2.2.2.2.2.2.1.1.cmml"><mi id="S3.E1.m2.2.2.2.2.2.2.1.1.2" xref="S3.E1.m2.2.2.2.2.2.2.1.1.2.cmml">y</mi><mi id="S3.E1.m2.2.2.2.2.2.2.1.1.3" xref="S3.E1.m2.2.2.2.2.2.2.1.1.3.cmml">i</mi></msub><mo id="S3.E1.m2.3.3.3.3.3.3.2.4" xref="S3.E1.m2.3.3.3.3.3.3.3.cmml">,</mo><msub id="S3.E1.m2.3.3.3.3.3.3.2.2" xref="S3.E1.m2.3.3.3.3.3.3.2.2.cmml"><mover accent="true" id="S3.E1.m2.3.3.3.3.3.3.2.2.2" xref="S3.E1.m2.3.3.3.3.3.3.2.2.2.cmml"><mi id="S3.E1.m2.3.3.3.3.3.3.2.2.2.2" xref="S3.E1.m2.3.3.3.3.3.3.2.2.2.2.cmml">y</mi><mo id="S3.E1.m2.3.3.3.3.3.3.2.2.2.1" xref="S3.E1.m2.3.3.3.3.3.3.2.2.2.1.cmml">^</mo></mover><mi id="S3.E1.m2.3.3.3.3.3.3.2.2.3" xref="S3.E1.m2.3.3.3.3.3.3.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="S3.E1.m2.3.3.3.3.3.3.2.5" xref="S3.E1.m2.3.3.3.3.3.3.3.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m2.3b"><apply id="S3.E1.m2.3.3.cmml" xref="S3.E1.m2.3.3"><eq id="S3.E1.m2.3.3.4.cmml" xref="S3.E1.m2.3.3.4"></eq><csymbol cd="latexml" id="S3.E1.m2.3.3.5.cmml" xref="S3.E1.m2.3.3.5">absent</csymbol><apply id="S3.E1.m2.3.3.3.cmml" xref="S3.E1.m2.3.3.3"><times id="S3.E1.m2.3.3.3.4.cmml" xref="S3.E1.m2.3.3.3.4"></times><apply id="S3.E1.m2.3.3.3.5.cmml" xref="S3.E1.m2.3.3.3.5"><divide id="S3.E1.m2.3.3.3.5.1.cmml" xref="S3.E1.m2.3.3.3.5"></divide><cn type="integer" id="S3.E1.m2.3.3.3.5.2.cmml" xref="S3.E1.m2.3.3.3.5.2">1</cn><ci id="S3.E1.m2.3.3.3.5.3.cmml" xref="S3.E1.m2.3.3.3.5.3">𝐶</ci></apply><apply id="S3.E1.m2.3.3.3.3.cmml" xref="S3.E1.m2.3.3.3.3"><apply id="S3.E1.m2.3.3.3.3.4.cmml" xref="S3.E1.m2.3.3.3.3.4"><csymbol cd="ambiguous" id="S3.E1.m2.3.3.3.3.4.1.cmml" xref="S3.E1.m2.3.3.3.3.4">superscript</csymbol><apply id="S3.E1.m2.3.3.3.3.4.2.cmml" xref="S3.E1.m2.3.3.3.3.4"><csymbol cd="ambiguous" id="S3.E1.m2.3.3.3.3.4.2.1.cmml" xref="S3.E1.m2.3.3.3.3.4">subscript</csymbol><sum id="S3.E1.m2.3.3.3.3.4.2.2.cmml" xref="S3.E1.m2.3.3.3.3.4.2.2"></sum><apply id="S3.E1.m2.3.3.3.3.4.2.3.cmml" xref="S3.E1.m2.3.3.3.3.4.2.3"><eq id="S3.E1.m2.3.3.3.3.4.2.3.1.cmml" xref="S3.E1.m2.3.3.3.3.4.2.3.1"></eq><ci id="S3.E1.m2.3.3.3.3.4.2.3.2.cmml" xref="S3.E1.m2.3.3.3.3.4.2.3.2">𝑖</ci><cn type="integer" id="S3.E1.m2.3.3.3.3.4.2.3.3.cmml" xref="S3.E1.m2.3.3.3.3.4.2.3.3">0</cn></apply></apply><apply id="S3.E1.m2.3.3.3.3.4.3.cmml" xref="S3.E1.m2.3.3.3.3.4.3"><minus id="S3.E1.m2.3.3.3.3.4.3.1.cmml" xref="S3.E1.m2.3.3.3.3.4.3.1"></minus><ci id="S3.E1.m2.3.3.3.3.4.3.2.cmml" xref="S3.E1.m2.3.3.3.3.4.3.2">𝐶</ci><cn type="integer" id="S3.E1.m2.3.3.3.3.4.3.3.cmml" xref="S3.E1.m2.3.3.3.3.4.3.3">1</cn></apply></apply><apply id="S3.E1.m2.3.3.3.3.3.cmml" xref="S3.E1.m2.3.3.3.3.3"><times id="S3.E1.m2.3.3.3.3.3.4.cmml" xref="S3.E1.m2.3.3.3.3.3.4"></times><apply id="S3.E1.m2.3.3.3.3.3.5.cmml" xref="S3.E1.m2.3.3.3.3.3.5"><csymbol cd="ambiguous" id="S3.E1.m2.3.3.3.3.3.5.1.cmml" xref="S3.E1.m2.3.3.3.3.3.5">subscript</csymbol><ci id="S3.E1.m2.3.3.3.3.3.5.2.cmml" xref="S3.E1.m2.3.3.3.3.3.5.2">𝑤</ci><ci id="S3.E1.m2.3.3.3.3.3.5.3.cmml" xref="S3.E1.m2.3.3.3.3.3.5.3">𝑖</ci></apply><apply id="S3.E1.m2.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m2.1.1.1.1.1.1.1.1.2">𝑦</ci><ci id="S3.E1.m2.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m2.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><ci id="S3.E1.m2.3.3.3.3.3.6.cmml" xref="S3.E1.m2.3.3.3.3.3.6">𝐵</ci><ci id="S3.E1.m2.3.3.3.3.3.7.cmml" xref="S3.E1.m2.3.3.3.3.3.7">𝐶</ci><ci id="S3.E1.m2.3.3.3.3.3.8.cmml" xref="S3.E1.m2.3.3.3.3.3.8">𝐸</ci><interval closure="open" id="S3.E1.m2.3.3.3.3.3.3.3.cmml" xref="S3.E1.m2.3.3.3.3.3.3.2"><apply id="S3.E1.m2.2.2.2.2.2.2.1.1.cmml" xref="S3.E1.m2.2.2.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.E1.m2.2.2.2.2.2.2.1.1.1.cmml" xref="S3.E1.m2.2.2.2.2.2.2.1.1">subscript</csymbol><ci id="S3.E1.m2.2.2.2.2.2.2.1.1.2.cmml" xref="S3.E1.m2.2.2.2.2.2.2.1.1.2">𝑦</ci><ci id="S3.E1.m2.2.2.2.2.2.2.1.1.3.cmml" xref="S3.E1.m2.2.2.2.2.2.2.1.1.3">𝑖</ci></apply><apply id="S3.E1.m2.3.3.3.3.3.3.2.2.cmml" xref="S3.E1.m2.3.3.3.3.3.3.2.2"><csymbol cd="ambiguous" id="S3.E1.m2.3.3.3.3.3.3.2.2.1.cmml" xref="S3.E1.m2.3.3.3.3.3.3.2.2">subscript</csymbol><apply id="S3.E1.m2.3.3.3.3.3.3.2.2.2.cmml" xref="S3.E1.m2.3.3.3.3.3.3.2.2.2"><ci id="S3.E1.m2.3.3.3.3.3.3.2.2.2.1.cmml" xref="S3.E1.m2.3.3.3.3.3.3.2.2.2.1">^</ci><ci id="S3.E1.m2.3.3.3.3.3.3.2.2.2.2.cmml" xref="S3.E1.m2.3.3.3.3.3.3.2.2.2.2">𝑦</ci></apply><ci id="S3.E1.m2.3.3.3.3.3.3.2.2.3.cmml" xref="S3.E1.m2.3.3.3.3.3.3.2.2.3">𝑖</ci></apply></interval></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m2.3c">\displaystyle=\frac{1}{C}\sum_{i=0}^{C-1}w_{i}(y_{i})BCE(y_{i},\hat{y}_{i})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E2.m1.1" class="ltx_Math" alttext="\displaystyle w_{i}(z)" display="inline"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.2" xref="S3.E2.m1.1.2.cmml"><msub id="S3.E2.m1.1.2.2" xref="S3.E2.m1.1.2.2.cmml"><mi id="S3.E2.m1.1.2.2.2" xref="S3.E2.m1.1.2.2.2.cmml">w</mi><mi id="S3.E2.m1.1.2.2.3" xref="S3.E2.m1.1.2.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.2.1" xref="S3.E2.m1.1.2.1.cmml">​</mo><mrow id="S3.E2.m1.1.2.3.2" xref="S3.E2.m1.1.2.cmml"><mo stretchy="false" id="S3.E2.m1.1.2.3.2.1" xref="S3.E2.m1.1.2.cmml">(</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">z</mi><mo stretchy="false" id="S3.E2.m1.1.2.3.2.2" xref="S3.E2.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.2.cmml" xref="S3.E2.m1.1.2"><times id="S3.E2.m1.1.2.1.cmml" xref="S3.E2.m1.1.2.1"></times><apply id="S3.E2.m1.1.2.2.cmml" xref="S3.E2.m1.1.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.2.2.1.cmml" xref="S3.E2.m1.1.2.2">subscript</csymbol><ci id="S3.E2.m1.1.2.2.2.cmml" xref="S3.E2.m1.1.2.2.2">𝑤</ci><ci id="S3.E2.m1.1.2.2.3.cmml" xref="S3.E2.m1.1.2.2.3">𝑖</ci></apply><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">𝑧</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\displaystyle w_{i}(z)</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E2.m2.1" class="ltx_Math" alttext="\displaystyle=\frac{1}{2}\left(\frac{z}{r_{i}}+\frac{1-z}{1-r_{i}}\right)" display="inline"><semantics id="S3.E2.m2.1a"><mrow id="S3.E2.m2.1.1" xref="S3.E2.m2.1.1.cmml"><mi id="S3.E2.m2.1.1.3" xref="S3.E2.m2.1.1.3.cmml"></mi><mo id="S3.E2.m2.1.1.2" xref="S3.E2.m2.1.1.2.cmml">=</mo><mrow id="S3.E2.m2.1.1.1" xref="S3.E2.m2.1.1.1.cmml"><mstyle displaystyle="true" id="S3.E2.m2.1.1.1.3" xref="S3.E2.m2.1.1.1.3.cmml"><mfrac id="S3.E2.m2.1.1.1.3a" xref="S3.E2.m2.1.1.1.3.cmml"><mn id="S3.E2.m2.1.1.1.3.2" xref="S3.E2.m2.1.1.1.3.2.cmml">1</mn><mn id="S3.E2.m2.1.1.1.3.3" xref="S3.E2.m2.1.1.1.3.3.cmml">2</mn></mfrac></mstyle><mo lspace="0em" rspace="0em" id="S3.E2.m2.1.1.1.2" xref="S3.E2.m2.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m2.1.1.1.1.1" xref="S3.E2.m2.1.1.1.1.1.1.cmml"><mo id="S3.E2.m2.1.1.1.1.1.2" xref="S3.E2.m2.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m2.1.1.1.1.1.1" xref="S3.E2.m2.1.1.1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.E2.m2.1.1.1.1.1.1.2" xref="S3.E2.m2.1.1.1.1.1.1.2.cmml"><mfrac id="S3.E2.m2.1.1.1.1.1.1.2a" xref="S3.E2.m2.1.1.1.1.1.1.2.cmml"><mi id="S3.E2.m2.1.1.1.1.1.1.2.2" xref="S3.E2.m2.1.1.1.1.1.1.2.2.cmml">z</mi><msub id="S3.E2.m2.1.1.1.1.1.1.2.3" xref="S3.E2.m2.1.1.1.1.1.1.2.3.cmml"><mi id="S3.E2.m2.1.1.1.1.1.1.2.3.2" xref="S3.E2.m2.1.1.1.1.1.1.2.3.2.cmml">r</mi><mi id="S3.E2.m2.1.1.1.1.1.1.2.3.3" xref="S3.E2.m2.1.1.1.1.1.1.2.3.3.cmml">i</mi></msub></mfrac></mstyle><mo id="S3.E2.m2.1.1.1.1.1.1.1" xref="S3.E2.m2.1.1.1.1.1.1.1.cmml">+</mo><mstyle displaystyle="true" id="S3.E2.m2.1.1.1.1.1.1.3" xref="S3.E2.m2.1.1.1.1.1.1.3.cmml"><mfrac id="S3.E2.m2.1.1.1.1.1.1.3a" xref="S3.E2.m2.1.1.1.1.1.1.3.cmml"><mrow id="S3.E2.m2.1.1.1.1.1.1.3.2" xref="S3.E2.m2.1.1.1.1.1.1.3.2.cmml"><mn id="S3.E2.m2.1.1.1.1.1.1.3.2.2" xref="S3.E2.m2.1.1.1.1.1.1.3.2.2.cmml">1</mn><mo id="S3.E2.m2.1.1.1.1.1.1.3.2.1" xref="S3.E2.m2.1.1.1.1.1.1.3.2.1.cmml">−</mo><mi id="S3.E2.m2.1.1.1.1.1.1.3.2.3" xref="S3.E2.m2.1.1.1.1.1.1.3.2.3.cmml">z</mi></mrow><mrow id="S3.E2.m2.1.1.1.1.1.1.3.3" xref="S3.E2.m2.1.1.1.1.1.1.3.3.cmml"><mn id="S3.E2.m2.1.1.1.1.1.1.3.3.2" xref="S3.E2.m2.1.1.1.1.1.1.3.3.2.cmml">1</mn><mo id="S3.E2.m2.1.1.1.1.1.1.3.3.1" xref="S3.E2.m2.1.1.1.1.1.1.3.3.1.cmml">−</mo><msub id="S3.E2.m2.1.1.1.1.1.1.3.3.3" xref="S3.E2.m2.1.1.1.1.1.1.3.3.3.cmml"><mi id="S3.E2.m2.1.1.1.1.1.1.3.3.3.2" xref="S3.E2.m2.1.1.1.1.1.1.3.3.3.2.cmml">r</mi><mi id="S3.E2.m2.1.1.1.1.1.1.3.3.3.3" xref="S3.E2.m2.1.1.1.1.1.1.3.3.3.3.cmml">i</mi></msub></mrow></mfrac></mstyle></mrow><mo id="S3.E2.m2.1.1.1.1.1.3" xref="S3.E2.m2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m2.1b"><apply id="S3.E2.m2.1.1.cmml" xref="S3.E2.m2.1.1"><eq id="S3.E2.m2.1.1.2.cmml" xref="S3.E2.m2.1.1.2"></eq><csymbol cd="latexml" id="S3.E2.m2.1.1.3.cmml" xref="S3.E2.m2.1.1.3">absent</csymbol><apply id="S3.E2.m2.1.1.1.cmml" xref="S3.E2.m2.1.1.1"><times id="S3.E2.m2.1.1.1.2.cmml" xref="S3.E2.m2.1.1.1.2"></times><apply id="S3.E2.m2.1.1.1.3.cmml" xref="S3.E2.m2.1.1.1.3"><divide id="S3.E2.m2.1.1.1.3.1.cmml" xref="S3.E2.m2.1.1.1.3"></divide><cn type="integer" id="S3.E2.m2.1.1.1.3.2.cmml" xref="S3.E2.m2.1.1.1.3.2">1</cn><cn type="integer" id="S3.E2.m2.1.1.1.3.3.cmml" xref="S3.E2.m2.1.1.1.3.3">2</cn></apply><apply id="S3.E2.m2.1.1.1.1.1.1.cmml" xref="S3.E2.m2.1.1.1.1.1"><plus id="S3.E2.m2.1.1.1.1.1.1.1.cmml" xref="S3.E2.m2.1.1.1.1.1.1.1"></plus><apply id="S3.E2.m2.1.1.1.1.1.1.2.cmml" xref="S3.E2.m2.1.1.1.1.1.1.2"><divide id="S3.E2.m2.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m2.1.1.1.1.1.1.2"></divide><ci id="S3.E2.m2.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m2.1.1.1.1.1.1.2.2">𝑧</ci><apply id="S3.E2.m2.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m2.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E2.m2.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E2.m2.1.1.1.1.1.1.2.3">subscript</csymbol><ci id="S3.E2.m2.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E2.m2.1.1.1.1.1.1.2.3.2">𝑟</ci><ci id="S3.E2.m2.1.1.1.1.1.1.2.3.3.cmml" xref="S3.E2.m2.1.1.1.1.1.1.2.3.3">𝑖</ci></apply></apply><apply id="S3.E2.m2.1.1.1.1.1.1.3.cmml" xref="S3.E2.m2.1.1.1.1.1.1.3"><divide id="S3.E2.m2.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m2.1.1.1.1.1.1.3"></divide><apply id="S3.E2.m2.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m2.1.1.1.1.1.1.3.2"><minus id="S3.E2.m2.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E2.m2.1.1.1.1.1.1.3.2.1"></minus><cn type="integer" id="S3.E2.m2.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E2.m2.1.1.1.1.1.1.3.2.2">1</cn><ci id="S3.E2.m2.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E2.m2.1.1.1.1.1.1.3.2.3">𝑧</ci></apply><apply id="S3.E2.m2.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m2.1.1.1.1.1.1.3.3"><minus id="S3.E2.m2.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E2.m2.1.1.1.1.1.1.3.3.1"></minus><cn type="integer" id="S3.E2.m2.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E2.m2.1.1.1.1.1.1.3.3.2">1</cn><apply id="S3.E2.m2.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E2.m2.1.1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E2.m2.1.1.1.1.1.1.3.3.3.1.cmml" xref="S3.E2.m2.1.1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S3.E2.m2.1.1.1.1.1.1.3.3.3.2.cmml" xref="S3.E2.m2.1.1.1.1.1.1.3.3.3.2">𝑟</ci><ci id="S3.E2.m2.1.1.1.1.1.1.3.3.3.3.cmml" xref="S3.E2.m2.1.1.1.1.1.1.3.3.3.3">𝑖</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m2.1c">\displaystyle=\frac{1}{2}\left(\frac{z}{r_{i}}+\frac{1-z}{1-r_{i}}\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
<tbody id="S3.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E3.m1.1" class="ltx_Math" alttext="\displaystyle r_{i}" display="inline"><semantics id="S3.E3.m1.1a"><msub id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml"><mi id="S3.E3.m1.1.1.2" xref="S3.E3.m1.1.1.2.cmml">r</mi><mi id="S3.E3.m1.1.1.3" xref="S3.E3.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1">subscript</csymbol><ci id="S3.E3.m1.1.1.2.cmml" xref="S3.E3.m1.1.1.2">𝑟</ci><ci id="S3.E3.m1.1.1.3.cmml" xref="S3.E3.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">\displaystyle r_{i}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E3.m2.1" class="ltx_Math" alttext="\displaystyle=\frac{\sqrt{N_{i}}}{\sqrt{N_{i}}+\sqrt{N-N_{i}}}" display="inline"><semantics id="S3.E3.m2.1a"><mrow id="S3.E3.m2.1.1" xref="S3.E3.m2.1.1.cmml"><mi id="S3.E3.m2.1.1.2" xref="S3.E3.m2.1.1.2.cmml"></mi><mo id="S3.E3.m2.1.1.1" xref="S3.E3.m2.1.1.1.cmml">=</mo><mstyle displaystyle="true" id="S3.E3.m2.1.1.3" xref="S3.E3.m2.1.1.3.cmml"><mfrac id="S3.E3.m2.1.1.3a" xref="S3.E3.m2.1.1.3.cmml"><msqrt id="S3.E3.m2.1.1.3.2" xref="S3.E3.m2.1.1.3.2.cmml"><msub id="S3.E3.m2.1.1.3.2.2" xref="S3.E3.m2.1.1.3.2.2.cmml"><mi id="S3.E3.m2.1.1.3.2.2.2" xref="S3.E3.m2.1.1.3.2.2.2.cmml">N</mi><mi id="S3.E3.m2.1.1.3.2.2.3" xref="S3.E3.m2.1.1.3.2.2.3.cmml">i</mi></msub></msqrt><mrow id="S3.E3.m2.1.1.3.3" xref="S3.E3.m2.1.1.3.3.cmml"><msqrt id="S3.E3.m2.1.1.3.3.2" xref="S3.E3.m2.1.1.3.3.2.cmml"><msub id="S3.E3.m2.1.1.3.3.2.2" xref="S3.E3.m2.1.1.3.3.2.2.cmml"><mi id="S3.E3.m2.1.1.3.3.2.2.2" xref="S3.E3.m2.1.1.3.3.2.2.2.cmml">N</mi><mi id="S3.E3.m2.1.1.3.3.2.2.3" xref="S3.E3.m2.1.1.3.3.2.2.3.cmml">i</mi></msub></msqrt><mo id="S3.E3.m2.1.1.3.3.1" xref="S3.E3.m2.1.1.3.3.1.cmml">+</mo><msqrt id="S3.E3.m2.1.1.3.3.3" xref="S3.E3.m2.1.1.3.3.3.cmml"><mrow id="S3.E3.m2.1.1.3.3.3.2" xref="S3.E3.m2.1.1.3.3.3.2.cmml"><mi id="S3.E3.m2.1.1.3.3.3.2.2" xref="S3.E3.m2.1.1.3.3.3.2.2.cmml">N</mi><mo id="S3.E3.m2.1.1.3.3.3.2.1" xref="S3.E3.m2.1.1.3.3.3.2.1.cmml">−</mo><msub id="S3.E3.m2.1.1.3.3.3.2.3" xref="S3.E3.m2.1.1.3.3.3.2.3.cmml"><mi id="S3.E3.m2.1.1.3.3.3.2.3.2" xref="S3.E3.m2.1.1.3.3.3.2.3.2.cmml">N</mi><mi id="S3.E3.m2.1.1.3.3.3.2.3.3" xref="S3.E3.m2.1.1.3.3.3.2.3.3.cmml">i</mi></msub></mrow></msqrt></mrow></mfrac></mstyle></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m2.1b"><apply id="S3.E3.m2.1.1.cmml" xref="S3.E3.m2.1.1"><eq id="S3.E3.m2.1.1.1.cmml" xref="S3.E3.m2.1.1.1"></eq><csymbol cd="latexml" id="S3.E3.m2.1.1.2.cmml" xref="S3.E3.m2.1.1.2">absent</csymbol><apply id="S3.E3.m2.1.1.3.cmml" xref="S3.E3.m2.1.1.3"><divide id="S3.E3.m2.1.1.3.1.cmml" xref="S3.E3.m2.1.1.3"></divide><apply id="S3.E3.m2.1.1.3.2.cmml" xref="S3.E3.m2.1.1.3.2"><root id="S3.E3.m2.1.1.3.2a.cmml" xref="S3.E3.m2.1.1.3.2"></root><apply id="S3.E3.m2.1.1.3.2.2.cmml" xref="S3.E3.m2.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E3.m2.1.1.3.2.2.1.cmml" xref="S3.E3.m2.1.1.3.2.2">subscript</csymbol><ci id="S3.E3.m2.1.1.3.2.2.2.cmml" xref="S3.E3.m2.1.1.3.2.2.2">𝑁</ci><ci id="S3.E3.m2.1.1.3.2.2.3.cmml" xref="S3.E3.m2.1.1.3.2.2.3">𝑖</ci></apply></apply><apply id="S3.E3.m2.1.1.3.3.cmml" xref="S3.E3.m2.1.1.3.3"><plus id="S3.E3.m2.1.1.3.3.1.cmml" xref="S3.E3.m2.1.1.3.3.1"></plus><apply id="S3.E3.m2.1.1.3.3.2.cmml" xref="S3.E3.m2.1.1.3.3.2"><root id="S3.E3.m2.1.1.3.3.2a.cmml" xref="S3.E3.m2.1.1.3.3.2"></root><apply id="S3.E3.m2.1.1.3.3.2.2.cmml" xref="S3.E3.m2.1.1.3.3.2.2"><csymbol cd="ambiguous" id="S3.E3.m2.1.1.3.3.2.2.1.cmml" xref="S3.E3.m2.1.1.3.3.2.2">subscript</csymbol><ci id="S3.E3.m2.1.1.3.3.2.2.2.cmml" xref="S3.E3.m2.1.1.3.3.2.2.2">𝑁</ci><ci id="S3.E3.m2.1.1.3.3.2.2.3.cmml" xref="S3.E3.m2.1.1.3.3.2.2.3">𝑖</ci></apply></apply><apply id="S3.E3.m2.1.1.3.3.3.cmml" xref="S3.E3.m2.1.1.3.3.3"><root id="S3.E3.m2.1.1.3.3.3a.cmml" xref="S3.E3.m2.1.1.3.3.3"></root><apply id="S3.E3.m2.1.1.3.3.3.2.cmml" xref="S3.E3.m2.1.1.3.3.3.2"><minus id="S3.E3.m2.1.1.3.3.3.2.1.cmml" xref="S3.E3.m2.1.1.3.3.3.2.1"></minus><ci id="S3.E3.m2.1.1.3.3.3.2.2.cmml" xref="S3.E3.m2.1.1.3.3.3.2.2">𝑁</ci><apply id="S3.E3.m2.1.1.3.3.3.2.3.cmml" xref="S3.E3.m2.1.1.3.3.3.2.3"><csymbol cd="ambiguous" id="S3.E3.m2.1.1.3.3.3.2.3.1.cmml" xref="S3.E3.m2.1.1.3.3.3.2.3">subscript</csymbol><ci id="S3.E3.m2.1.1.3.3.3.2.3.2.cmml" xref="S3.E3.m2.1.1.3.3.3.2.3.2">𝑁</ci><ci id="S3.E3.m2.1.1.3.3.3.2.3.3.cmml" xref="S3.E3.m2.1.1.3.3.3.2.3.3">𝑖</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m2.1c">\displaystyle=\frac{\sqrt{N_{i}}}{\sqrt{N_{i}}+\sqrt{N-N_{i}}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p4.7" class="ltx_p">where <math id="S3.SS2.p4.1.m1.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.SS2.p4.1.m1.1a"><mi id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><ci id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">C</annotation></semantics></math> is the number of classes, <math id="S3.SS2.p4.2.m2.2" class="ltx_Math" alttext="\hat{y}\in[0,1]^{C}" display="inline"><semantics id="S3.SS2.p4.2.m2.2a"><mrow id="S3.SS2.p4.2.m2.2.3" xref="S3.SS2.p4.2.m2.2.3.cmml"><mover accent="true" id="S3.SS2.p4.2.m2.2.3.2" xref="S3.SS2.p4.2.m2.2.3.2.cmml"><mi id="S3.SS2.p4.2.m2.2.3.2.2" xref="S3.SS2.p4.2.m2.2.3.2.2.cmml">y</mi><mo id="S3.SS2.p4.2.m2.2.3.2.1" xref="S3.SS2.p4.2.m2.2.3.2.1.cmml">^</mo></mover><mo id="S3.SS2.p4.2.m2.2.3.1" xref="S3.SS2.p4.2.m2.2.3.1.cmml">∈</mo><msup id="S3.SS2.p4.2.m2.2.3.3" xref="S3.SS2.p4.2.m2.2.3.3.cmml"><mrow id="S3.SS2.p4.2.m2.2.3.3.2.2" xref="S3.SS2.p4.2.m2.2.3.3.2.1.cmml"><mo stretchy="false" id="S3.SS2.p4.2.m2.2.3.3.2.2.1" xref="S3.SS2.p4.2.m2.2.3.3.2.1.cmml">[</mo><mn id="S3.SS2.p4.2.m2.1.1" xref="S3.SS2.p4.2.m2.1.1.cmml">0</mn><mo id="S3.SS2.p4.2.m2.2.3.3.2.2.2" xref="S3.SS2.p4.2.m2.2.3.3.2.1.cmml">,</mo><mn id="S3.SS2.p4.2.m2.2.2" xref="S3.SS2.p4.2.m2.2.2.cmml">1</mn><mo stretchy="false" id="S3.SS2.p4.2.m2.2.3.3.2.2.3" xref="S3.SS2.p4.2.m2.2.3.3.2.1.cmml">]</mo></mrow><mi id="S3.SS2.p4.2.m2.2.3.3.3" xref="S3.SS2.p4.2.m2.2.3.3.3.cmml">C</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.2b"><apply id="S3.SS2.p4.2.m2.2.3.cmml" xref="S3.SS2.p4.2.m2.2.3"><in id="S3.SS2.p4.2.m2.2.3.1.cmml" xref="S3.SS2.p4.2.m2.2.3.1"></in><apply id="S3.SS2.p4.2.m2.2.3.2.cmml" xref="S3.SS2.p4.2.m2.2.3.2"><ci id="S3.SS2.p4.2.m2.2.3.2.1.cmml" xref="S3.SS2.p4.2.m2.2.3.2.1">^</ci><ci id="S3.SS2.p4.2.m2.2.3.2.2.cmml" xref="S3.SS2.p4.2.m2.2.3.2.2">𝑦</ci></apply><apply id="S3.SS2.p4.2.m2.2.3.3.cmml" xref="S3.SS2.p4.2.m2.2.3.3"><csymbol cd="ambiguous" id="S3.SS2.p4.2.m2.2.3.3.1.cmml" xref="S3.SS2.p4.2.m2.2.3.3">superscript</csymbol><interval closure="closed" id="S3.SS2.p4.2.m2.2.3.3.2.1.cmml" xref="S3.SS2.p4.2.m2.2.3.3.2.2"><cn type="integer" id="S3.SS2.p4.2.m2.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1">0</cn><cn type="integer" id="S3.SS2.p4.2.m2.2.2.cmml" xref="S3.SS2.p4.2.m2.2.2">1</cn></interval><ci id="S3.SS2.p4.2.m2.2.3.3.3.cmml" xref="S3.SS2.p4.2.m2.2.3.3.3">𝐶</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.2c">\hat{y}\in[0,1]^{C}</annotation></semantics></math> is the prediction, <math id="S3.SS2.p4.3.m3.2" class="ltx_Math" alttext="y\in\{0,1\}^{C}" display="inline"><semantics id="S3.SS2.p4.3.m3.2a"><mrow id="S3.SS2.p4.3.m3.2.3" xref="S3.SS2.p4.3.m3.2.3.cmml"><mi id="S3.SS2.p4.3.m3.2.3.2" xref="S3.SS2.p4.3.m3.2.3.2.cmml">y</mi><mo id="S3.SS2.p4.3.m3.2.3.1" xref="S3.SS2.p4.3.m3.2.3.1.cmml">∈</mo><msup id="S3.SS2.p4.3.m3.2.3.3" xref="S3.SS2.p4.3.m3.2.3.3.cmml"><mrow id="S3.SS2.p4.3.m3.2.3.3.2.2" xref="S3.SS2.p4.3.m3.2.3.3.2.1.cmml"><mo stretchy="false" id="S3.SS2.p4.3.m3.2.3.3.2.2.1" xref="S3.SS2.p4.3.m3.2.3.3.2.1.cmml">{</mo><mn id="S3.SS2.p4.3.m3.1.1" xref="S3.SS2.p4.3.m3.1.1.cmml">0</mn><mo id="S3.SS2.p4.3.m3.2.3.3.2.2.2" xref="S3.SS2.p4.3.m3.2.3.3.2.1.cmml">,</mo><mn id="S3.SS2.p4.3.m3.2.2" xref="S3.SS2.p4.3.m3.2.2.cmml">1</mn><mo stretchy="false" id="S3.SS2.p4.3.m3.2.3.3.2.2.3" xref="S3.SS2.p4.3.m3.2.3.3.2.1.cmml">}</mo></mrow><mi id="S3.SS2.p4.3.m3.2.3.3.3" xref="S3.SS2.p4.3.m3.2.3.3.3.cmml">C</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.3.m3.2b"><apply id="S3.SS2.p4.3.m3.2.3.cmml" xref="S3.SS2.p4.3.m3.2.3"><in id="S3.SS2.p4.3.m3.2.3.1.cmml" xref="S3.SS2.p4.3.m3.2.3.1"></in><ci id="S3.SS2.p4.3.m3.2.3.2.cmml" xref="S3.SS2.p4.3.m3.2.3.2">𝑦</ci><apply id="S3.SS2.p4.3.m3.2.3.3.cmml" xref="S3.SS2.p4.3.m3.2.3.3"><csymbol cd="ambiguous" id="S3.SS2.p4.3.m3.2.3.3.1.cmml" xref="S3.SS2.p4.3.m3.2.3.3">superscript</csymbol><set id="S3.SS2.p4.3.m3.2.3.3.2.1.cmml" xref="S3.SS2.p4.3.m3.2.3.3.2.2"><cn type="integer" id="S3.SS2.p4.3.m3.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1">0</cn><cn type="integer" id="S3.SS2.p4.3.m3.2.2.cmml" xref="S3.SS2.p4.3.m3.2.2">1</cn></set><ci id="S3.SS2.p4.3.m3.2.3.3.3.cmml" xref="S3.SS2.p4.3.m3.2.3.3.3">𝐶</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.3.m3.2c">y\in\{0,1\}^{C}</annotation></semantics></math> is the ground truth label, <math id="S3.SS2.p4.4.m4.1" class="ltx_Math" alttext="BCE" display="inline"><semantics id="S3.SS2.p4.4.m4.1a"><mrow id="S3.SS2.p4.4.m4.1.1" xref="S3.SS2.p4.4.m4.1.1.cmml"><mi id="S3.SS2.p4.4.m4.1.1.2" xref="S3.SS2.p4.4.m4.1.1.2.cmml">B</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.4.m4.1.1.1" xref="S3.SS2.p4.4.m4.1.1.1.cmml">​</mo><mi id="S3.SS2.p4.4.m4.1.1.3" xref="S3.SS2.p4.4.m4.1.1.3.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.4.m4.1.1.1a" xref="S3.SS2.p4.4.m4.1.1.1.cmml">​</mo><mi id="S3.SS2.p4.4.m4.1.1.4" xref="S3.SS2.p4.4.m4.1.1.4.cmml">E</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.4.m4.1b"><apply id="S3.SS2.p4.4.m4.1.1.cmml" xref="S3.SS2.p4.4.m4.1.1"><times id="S3.SS2.p4.4.m4.1.1.1.cmml" xref="S3.SS2.p4.4.m4.1.1.1"></times><ci id="S3.SS2.p4.4.m4.1.1.2.cmml" xref="S3.SS2.p4.4.m4.1.1.2">𝐵</ci><ci id="S3.SS2.p4.4.m4.1.1.3.cmml" xref="S3.SS2.p4.4.m4.1.1.3">𝐶</ci><ci id="S3.SS2.p4.4.m4.1.1.4.cmml" xref="S3.SS2.p4.4.m4.1.1.4">𝐸</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.4.m4.1c">BCE</annotation></semantics></math> is binary cross entropy loss, <math id="S3.SS2.p4.5.m5.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS2.p4.5.m5.1a"><mi id="S3.SS2.p4.5.m5.1.1" xref="S3.SS2.p4.5.m5.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.5.m5.1b"><ci id="S3.SS2.p4.5.m5.1.1.cmml" xref="S3.SS2.p4.5.m5.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.5.m5.1c">N</annotation></semantics></math> is the total number of samples, and <math id="S3.SS2.p4.6.m6.1" class="ltx_Math" alttext="N_{i}" display="inline"><semantics id="S3.SS2.p4.6.m6.1a"><msub id="S3.SS2.p4.6.m6.1.1" xref="S3.SS2.p4.6.m6.1.1.cmml"><mi id="S3.SS2.p4.6.m6.1.1.2" xref="S3.SS2.p4.6.m6.1.1.2.cmml">N</mi><mi id="S3.SS2.p4.6.m6.1.1.3" xref="S3.SS2.p4.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.6.m6.1b"><apply id="S3.SS2.p4.6.m6.1.1.cmml" xref="S3.SS2.p4.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.6.m6.1.1.1.cmml" xref="S3.SS2.p4.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.p4.6.m6.1.1.2.cmml" xref="S3.SS2.p4.6.m6.1.1.2">𝑁</ci><ci id="S3.SS2.p4.6.m6.1.1.3.cmml" xref="S3.SS2.p4.6.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.6.m6.1c">N_{i}</annotation></semantics></math> is the number of positive samples in the <math id="S3.SS2.p4.7.m7.1" class="ltx_Math" alttext="i^{\text{th}}" display="inline"><semantics id="S3.SS2.p4.7.m7.1a"><msup id="S3.SS2.p4.7.m7.1.1" xref="S3.SS2.p4.7.m7.1.1.cmml"><mi id="S3.SS2.p4.7.m7.1.1.2" xref="S3.SS2.p4.7.m7.1.1.2.cmml">i</mi><mtext id="S3.SS2.p4.7.m7.1.1.3" xref="S3.SS2.p4.7.m7.1.1.3a.cmml">th</mtext></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.7.m7.1b"><apply id="S3.SS2.p4.7.m7.1.1.cmml" xref="S3.SS2.p4.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.7.m7.1.1.1.cmml" xref="S3.SS2.p4.7.m7.1.1">superscript</csymbol><ci id="S3.SS2.p4.7.m7.1.1.2.cmml" xref="S3.SS2.p4.7.m7.1.1.2">𝑖</ci><ci id="S3.SS2.p4.7.m7.1.1.3a.cmml" xref="S3.SS2.p4.7.m7.1.1.3"><mtext mathsize="70%" id="S3.SS2.p4.7.m7.1.1.3.cmml" xref="S3.SS2.p4.7.m7.1.1.3">th</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.7.m7.1c">i^{\text{th}}</annotation></semantics></math> class. We found that plain inverse frequency weighing caused numerical instability in training, necessitating the square root.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Character Segmentation &amp; Bounding Boxes</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">In order to produce bounding boxes around each subject in the image, we first train an illustrated character segmenter. As we assume one subject per image, we can derive a bounding box by enclosing the thresholded segmentation output. The single-subject assumption also removes the need for region proposal and NMS infrastructure present in available illustrated segmenters <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, so that our model may focus on producing clean segmentations only. Our segmentation model is based on DeepLabv3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, with three additional layers at the end of the head for finer segmentations at the input image resolution. We initialize with pretrained DeepLabv3 weights from PyTorch <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>, and fine-tune the full model using pixel-wise binary cross-entropy loss.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Model</td>
<td id="S3.T1.1.1.1.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">OKS@50</td>
<td id="S3.T1.1.1.1.3" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">OKS@75</td>
<td id="S3.T1.1.1.1.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">PCKh@50</td>
<td id="S3.T1.1.1.1.5" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">PDJ@20</td>
<td id="S3.T1.1.1.1.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">PCPm@50</td>
<td id="S3.T1.1.1.1.7" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">params</td>
<td id="S3.T1.1.1.1.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">ms/img</td>
</tr>
<tr id="S3.T1.1.2.2" class="ltx_tr">
<td id="S3.T1.1.2.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">Feature Concatenation (+new data)</td>
<td id="S3.T1.1.2.2.2" class="ltx_td ltx_align_right ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T1.1.2.2.2.1" class="ltx_text ltx_font_bold">0.8982</span></td>
<td id="S3.T1.1.2.2.3" class="ltx_td ltx_align_right ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T1.1.2.2.3.1" class="ltx_text ltx_font_bold">0.7930</span></td>
<td id="S3.T1.1.2.2.4" class="ltx_td ltx_align_right ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T1.1.2.2.4.1" class="ltx_text ltx_font_bold">0.7866</span></td>
<td id="S3.T1.1.2.2.5" class="ltx_td ltx_align_right ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">0.8403</td>
<td id="S3.T1.1.2.2.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">0.8551</td>
<td id="S3.T1.1.2.2.7" class="ltx_td ltx_align_right ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">86.8m</td>
<td id="S3.T1.1.2.2.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">217.7</td>
</tr>
<tr id="S3.T1.1.3.3" class="ltx_tr">
<td id="S3.T1.1.3.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">Feature Concatenation</td>
<td id="S3.T1.1.3.3.2" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">0.8827</td>
<td id="S3.T1.1.3.3.3" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">0.7723</td>
<td id="S3.T1.1.3.3.4" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">0.7762</td>
<td id="S3.T1.1.3.3.5" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">0.8282</td>
<td id="S3.T1.1.3.3.6" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">0.8435</td>
<td id="S3.T1.1.3.3.7" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">86.8m</td>
<td id="S3.T1.1.3.3.8" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">217.7</td>
</tr>
<tr id="S3.T1.1.4.4" class="ltx_tr">
<td id="S3.T1.1.4.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">Feature Matching (+new data)</td>
<td id="S3.T1.1.4.4.2" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">0.8953</td>
<td id="S3.T1.1.4.4.3" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">0.7907</td>
<td id="S3.T1.1.4.4.4" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">0.7851</td>
<td id="S3.T1.1.4.4.5" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T1.1.4.4.5.1" class="ltx_text ltx_font_bold">0.8423</span></td>
<td id="S3.T1.1.4.4.6" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T1.1.4.4.6.1" class="ltx_text ltx_font_bold">0.8599</span></td>
<td id="S3.T1.1.4.4.7" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">9.9m</td>
<td id="S3.T1.1.4.4.8" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">147.8</td>
</tr>
<tr id="S3.T1.1.5.5" class="ltx_tr">
<td id="S3.T1.1.5.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">Feature Matching</td>
<td id="S3.T1.1.5.5.2" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">0.8769</td>
<td id="S3.T1.1.5.5.3" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">0.7680</td>
<td id="S3.T1.1.5.5.4" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">0.7675</td>
<td id="S3.T1.1.5.5.5" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">0.8251</td>
<td id="S3.T1.1.5.5.6" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">0.8343</td>
<td id="S3.T1.1.5.5.7" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">9.9m</td>
<td id="S3.T1.1.5.5.8" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">147.8</td>
</tr>
<tr id="S3.T1.1.6.6" class="ltx_tr">
<td id="S3.T1.1.6.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Task Fine-tuning Only</td>
<td id="S3.T1.1.6.6.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.8026</td>
<td id="S3.T1.1.6.6.3" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.6481</td>
<td id="S3.T1.1.6.6.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.7032</td>
<td id="S3.T1.1.6.6.5" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.7666</td>
<td id="S3.T1.1.6.6.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.7446</td>
<td id="S3.T1.1.6.6.7" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">77.5m</td>
<td id="S3.T1.1.6.6.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">174.5</td>
</tr>
<tr id="S3.T1.1.7.7" class="ltx_tr">
<td id="S3.T1.1.7.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">Domain Features Only</td>
<td id="S3.T1.1.7.7.2" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T1.1.7.7.2.1" class="ltx_text ltx_font_bold">0.8607</span></td>
<td id="S3.T1.1.7.7.3" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T1.1.7.7.3.1" class="ltx_text ltx_font_bold">0.7467</span></td>
<td id="S3.T1.1.7.7.4" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">0.7444</td>
<td id="S3.T1.1.7.7.5" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">0.8076</td>
<td id="S3.T1.1.7.7.6" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T1.1.7.7.6.1" class="ltx_text ltx_font_bold">0.8215</span></td>
<td id="S3.T1.1.7.7.7" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">9.6m</td>
<td id="S3.T1.1.7.7.8" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">143.7</td>
</tr>
<tr id="S3.T1.1.8.8" class="ltx_tr">
<td id="S3.T1.1.8.8.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">Task Fine-tuning w/ Domain Features</td>
<td id="S3.T1.1.8.8.2" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">0.8548</td>
<td id="S3.T1.1.8.8.3" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">0.7209</td>
<td id="S3.T1.1.8.8.4" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T1.1.8.8.4.1" class="ltx_text ltx_font_bold">0.7544</span></td>
<td id="S3.T1.1.8.8.5" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T1.1.8.8.5.1" class="ltx_text ltx_font_bold">0.8181</span></td>
<td id="S3.T1.1.8.8.6" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">0.8084</td>
<td id="S3.T1.1.8.8.7" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">41.1m</td>
<td id="S3.T1.1.8.8.8" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">147.8</td>
</tr>
<tr id="S3.T1.1.9.9" class="ltx_tr">
<td id="S3.T1.1.9.9.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">Adversarial (DeepFashion2)</td>
<td id="S3.T1.1.9.9.2" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">0.8321</td>
<td id="S3.T1.1.9.9.3" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">0.6804</td>
<td id="S3.T1.1.9.9.4" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">0.7108</td>
<td id="S3.T1.1.9.9.5" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">0.7823</td>
<td id="S3.T1.1.9.9.6" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">0.7778</td>
<td id="S3.T1.1.9.9.7" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">9.9m</td>
<td id="S3.T1.1.9.9.8" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">147.8</td>
</tr>
<tr id="S3.T1.1.10.10" class="ltx_tr">
<td id="S3.T1.1.10.10.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">Adversarial (COCO)</td>
<td id="S3.T1.1.10.10.2" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">0.8065</td>
<td id="S3.T1.1.10.10.3" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">0.6362</td>
<td id="S3.T1.1.10.10.4" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">0.6788</td>
<td id="S3.T1.1.10.10.5" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">0.7607</td>
<td id="S3.T1.1.10.10.6" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">0.7350</td>
<td id="S3.T1.1.10.10.7" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">9.9m</td>
<td id="S3.T1.1.10.10.8" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">147.8</td>
</tr>
<tr id="S3.T1.1.11.11" class="ltx_tr">
<td id="S3.T1.1.11.11.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Task-Pretrained (R-CNN)</td>
<td id="S3.T1.1.11.11.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T1.1.11.11.2.1" class="ltx_text ltx_font_bold">0.7584</span></td>
<td id="S3.T1.1.11.11.3" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T1.1.11.11.3.1" class="ltx_text ltx_font_bold">0.6724</span></td>
<td id="S3.T1.1.11.11.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T1.1.11.11.4.1" class="ltx_text ltx_font_bold">0.6960</span></td>
<td id="S3.T1.1.11.11.5" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T1.1.11.11.5.1" class="ltx_text ltx_font_bold">0.7357</span></td>
<td id="S3.T1.1.11.11.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T1.1.11.11.6.1" class="ltx_text ltx_font_bold">0.6679</span></td>
<td id="S3.T1.1.11.11.7" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">77.5m</td>
<td id="S3.T1.1.11.11.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">174.5</td>
</tr>
<tr id="S3.T1.1.12.12" class="ltx_tr">
<td id="S3.T1.1.12.12.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">Task-Pretrained (OpenPose)</td>
<td id="S3.T1.1.12.12.2" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">0.4922</td>
<td id="S3.T1.1.12.12.3" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">0.4222</td>
<td id="S3.T1.1.12.12.4" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">0.4447</td>
<td id="S3.T1.1.12.12.5" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">0.4796</td>
<td id="S3.T1.1.12.12.6" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">0.4381</td>
<td id="S3.T1.1.12.12.7" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">52.3m</td>
<td id="S3.T1.1.12.12.8" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">128.2</td>
</tr>
<tr id="S3.T1.1.13.13" class="ltx_tr">
<td id="S3.T1.1.13.13.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Ours (equiv. to feat. concat.)</td>
<td id="S3.T1.1.13.13.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T1.1.13.13.2.1" class="ltx_text ltx_font_bold">0.8827</span></td>
<td id="S3.T1.1.13.13.3" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T1.1.13.13.3.1" class="ltx_text ltx_font_bold">0.7723</span></td>
<td id="S3.T1.1.13.13.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T1.1.13.13.4.1" class="ltx_text ltx_font_bold">0.7762</span></td>
<td id="S3.T1.1.13.13.5" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T1.1.13.13.5.1" class="ltx_text ltx_font_bold">0.8282</span></td>
<td id="S3.T1.1.13.13.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T1.1.13.13.6.1" class="ltx_text ltx_font_bold">0.8435</span></td>
<td id="S3.T1.1.13.13.7" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">86.8m</td>
<td id="S3.T1.1.13.13.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">217.7</td>
</tr>
<tr id="S3.T1.1.14.14" class="ltx_tr">
<td id="S3.T1.1.14.14.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">RF5 Backbone</td>
<td id="S3.T1.1.14.14.2" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">0.8547</td>
<td id="S3.T1.1.14.14.3" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">0.7358</td>
<td id="S3.T1.1.14.14.4" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">0.7427</td>
<td id="S3.T1.1.14.14.5" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">0.8015</td>
<td id="S3.T1.1.14.14.6" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">0.8005</td>
<td id="S3.T1.1.14.14.7" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">86.8m</td>
<td id="S3.T1.1.14.14.8" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">217.7</td>
</tr>
<tr id="S3.T1.1.15.15" class="ltx_tr">
<td id="S3.T1.1.15.15.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">ImageNet-pretrained Backbone</td>
<td id="S3.T1.1.15.15.2" class="ltx_td ltx_align_right ltx_border_b" style="padding-left:5.0pt;padding-right:5.0pt;">0.8218</td>
<td id="S3.T1.1.15.15.3" class="ltx_td ltx_align_right ltx_border_b" style="padding-left:5.0pt;padding-right:5.0pt;">0.6919</td>
<td id="S3.T1.1.15.15.4" class="ltx_td ltx_align_right ltx_border_b" style="padding-left:5.0pt;padding-right:5.0pt;">0.7060</td>
<td id="S3.T1.1.15.15.5" class="ltx_td ltx_align_right ltx_border_b" style="padding-left:5.0pt;padding-right:5.0pt;">0.7649</td>
<td id="S3.T1.1.15.15.6" class="ltx_td ltx_align_right ltx_border_b ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">0.7571</td>
<td id="S3.T1.1.15.15.7" class="ltx_td ltx_align_right ltx_border_b" style="padding-left:5.0pt;padding-right:5.0pt;">86.8m</td>
<td id="S3.T1.1.15.15.8" class="ltx_td ltx_align_right ltx_border_b ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">217.7</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Performance of different architectures and ablations described in Sec. <a href="#S5.SS1" title="5.1 Pose Estimation Transfer ‣ 5 Experiments ‣ Transfer Learning for Pose Estimation of Illustrated Characters" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>. Note that the parameter count and speed are measured in inference mode with batch size one; “m” refers to “millions of parameters”.</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Data Collection</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Unless mentioned otherwise, we train with random image rotation, translation, scaling, flipping, and recoloring.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Pose Data</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We extend the AnimeDrawingsDataset (ADD), first collected by Khungurn <em id="S4.SS1.p1.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S4.SS1.p1.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. The original dataset had 2000 illustrated full-body single-character images from Danbooru, each annotated with joint keypoints. However, ADD did not follow the now popularized COCO standard <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>; in particular, it was missing facial keypoints (eyes and ears) and bounding boxes. In order to evaluate and compare with modern pose estimators, we manually labeled the missing keypoints using an open-source COCO annotator <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> and automatically generated bounding boxes using the character segmenter described in Sec. <a href="#S3.SS3" title="3.3 Character Segmentation &amp; Bounding Boxes ‣ 3 Method &amp; Architectures ‣ Transfer Learning for Pose Estimation of Illustrated Characters" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>. We also manually remove 57 images with multiple characters, or without the full body in view.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">In addition, we improve the diversity of poses in ADD by collecting an additional 2043 samples. A major weakness of ADD is its lack of backwards-facing characters; only 5.45% of the entire 2k dataset had a back-related Danbooru tag (e.g. back, from_behind, looking_back, etc.). We specifically filtered for back-related images when annotating, resulting in a total of 850 in the updated dataset (21.25%). We also selected for other notably under-represented poses, like difficult leg tags (soles, bent_over, leg_up, crossed_legs, squatting, kneeling, etc.), arm tags (stretch, arms_up, hands_clasped, etc.), and lying tags (on_side, on_stomach).</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">Our final updated dataset contains 4000 illustrated character images with all 17 COCO keypoints and bounding boxes. We designate 3200 images for training (previously 1373), 313 for validation (previously 97), and 487 for testing (same as original ADD). For each input image, we first scale and crop such that the bounding box is centered and padded by at least 10% of the edge length on all sides. We then perform augmentations; flips require swapping left-right keypoints, and full 360-degree rotations are allowed.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>ResNet Tagger Data</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Our ResNet50 tagger is trained on a new subset of the 512px SFW Danbooru2019 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. The original dataset contains 2.83m images with over 390k tags, but after filtering and retagging we arrive at 837k images with 1062 classes. The new classes are derived from manually-selected union rules over 2027 raw tags, as described in Sec. <a href="#S3.SS2" title="3.2 ResNet Tagger ‣ 3 Method &amp; Architectures ‣ Transfer Learning for Pose Estimation of Illustrated Characters" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>; the rulebook has 314 body-part, 545 clothing, and 203 miscellaneous (e.g. image composition) classes.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">To combat the class imbalance problem described in Sec. <a href="#S3.SS2" title="3.2 ResNet Tagger ‣ 3 Method &amp; Architectures ‣ Transfer Learning for Pose Estimation of Illustrated Characters" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>, we also rigorously filtered the dataset. We remove all images that are not single-person (solo, 1girl, or 1boy), are comics (comic, 4koma, doujinshi, etc.), or are smaller than 512px. Most critically, we remove all images with less than 12 positive tags; these images are very likely under-tagged, and would have introduced many false-negatives to the ground truth. The final subset of 837k images has significantly reduced class imbalance (median class frequency 0.38%, minimum 0.04%) compared to the datasets of available taggers (median 0.07%, min 0.01%) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">We split the dataset 80-10-10 train-val-test. As some tags are color-sensitive, we do not jitter the hue; similarly as some tags are orientation-sensitive, we allow up to 15-degree rotations and horizontal flips only.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Character Segmentation Data</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">To obtain character bounding boxes, we train a character segmentation model and enclose output regions at 0.5 threshold (Sec. <a href="#S3.SS3" title="3.3 Character Segmentation &amp; Bounding Boxes ‣ 3 Method &amp; Architectures ‣ Transfer Learning for Pose Estimation of Illustrated Characters" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>). The inputs to our segmentation system are augmented composites of RGBA foregrounds (with transparent backgrounds) onto RGB backgrounds; the synthetic ground truth is the foreground alpha. The available AniSeg dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> has only 945 images, with manually-labeled segmentations that are not pixel-perfectly aligned. We thus collect our own larger synthetic compositing dataset. Our background images are a mix of illustrated scenery (5.8k Danbooru images with scenery and no_humans tag) and stock textures (2.3k scraped <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> from the Pixiv Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>). We collect single-character foreground images from Danbooru with the transparent_background tag; 18.5k samples are used, after filtering images with text, non-transparency, or more than one connected component in the alpha channel. Counting each foreground as a single sample, this makes our new dataset roughly 20x larger than AniSeg. The foregrounds and backgrounds are randomly paired for compositing during training, with 5% chance of having no foreground. We hold out 2048 deterministic foreground-background pairs for validation and testing (1024 each).</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">keypoint</th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"></th>
<th id="S4.T2.1.1.1.3" class="ltx_td ltx_th ltx_th_column ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"></th>
<th id="S4.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" colspan="2">OKS@50</th>
<th id="S4.T2.1.1.1.5" class="ltx_td ltx_th ltx_th_column ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"></th>
<th id="S4.T2.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" colspan="2">OKS@75</th>
<th id="S4.T2.1.1.1.7" class="ltx_td ltx_th ltx_th_column ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"></th>
<th id="S4.T2.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" colspan="2">PCKh@50</th>
<th id="S4.T2.1.1.1.9" class="ltx_td ltx_th ltx_th_column ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"></th>
<th id="S4.T2.1.1.1.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" colspan="2">PDJ@20</th>
<th id="S4.T2.1.1.1.11" class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"></th>
<th id="S4.T2.1.1.1.12" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" colspan="4">PDJ@20 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.2.1" class="ltx_tr">
<td id="S4.T2.1.2.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">nose</td>
<td id="S4.T2.1.2.1.2" class="ltx_td ltx_border_r ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.2.1.3" class="ltx_td ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.2.1.4" class="ltx_td ltx_align_right ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">0.9466</td>
<td id="S4.T2.1.2.1.5" class="ltx_td ltx_align_right ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">(+0.4%)</td>
<td id="S4.T2.1.2.1.6" class="ltx_td ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.2.1.7" class="ltx_td ltx_align_right ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">0.8419</td>
<td id="S4.T2.1.2.1.8" class="ltx_td ltx_align_right ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">(+3.8%)</td>
<td id="S4.T2.1.2.1.9" class="ltx_td ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.2.1.10" class="ltx_td ltx_align_right ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">0.9918</td>
<td id="S4.T2.1.2.1.11" class="ltx_td ltx_align_right ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">(+0.2%)</td>
<td id="S4.T2.1.2.1.12" class="ltx_td ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.2.1.13" class="ltx_td ltx_align_right ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">0.9897</td>
<td id="S4.T2.1.2.1.14" class="ltx_td ltx_align_right ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">(+0.2%)</td>
<td id="S4.T2.1.2.1.15" class="ltx_td ltx_border_r ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.2.1.16" class="ltx_td ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.2.1.17" class="ltx_td ltx_align_right ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">0.794</td>
<td id="S4.T2.1.2.1.18" class="ltx_td ltx_align_right ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">(+24.7%)</td>
<td id="S4.T2.1.2.1.19" class="ltx_td ltx_border_r ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
</tr>
<tr id="S4.T2.1.3.2" class="ltx_tr">
<td id="S4.T2.1.3.2.1" class="ltx_td ltx_align_left ltx_border_l" style="padding-left:3.0pt;padding-right:3.0pt;">eyes</td>
<td id="S4.T2.1.3.2.2" class="ltx_td ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.3.2.3" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.3.2.4" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">0.9795</td>
<td id="S4.T2.1.3.2.5" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">(+1.1%)</td>
<td id="S4.T2.1.3.2.6" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.3.2.7" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">0.9363</td>
<td id="S4.T2.1.3.2.8" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">(+4.3%)</td>
<td id="S4.T2.1.3.2.9" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.3.2.10" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">0.9928</td>
<td id="S4.T2.1.3.2.11" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">(+0.0%)</td>
<td id="S4.T2.1.3.2.12" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.3.2.13" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">0.9928</td>
<td id="S4.T2.1.3.2.14" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">(+0.1%)</td>
<td id="S4.T2.1.3.2.15" class="ltx_td ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.3.2.16" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.3.2.17" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">*0.890</td>
<td id="S4.T2.1.3.2.18" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">(+11.6%)</td>
<td id="S4.T2.1.3.2.19" class="ltx_td ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
</tr>
<tr id="S4.T2.1.4.3" class="ltx_tr">
<td id="S4.T2.1.4.3.1" class="ltx_td ltx_align_left ltx_border_l" style="padding-left:3.0pt;padding-right:3.0pt;">ears</td>
<td id="S4.T2.1.4.3.2" class="ltx_td ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.4.3.3" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.4.3.4" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">0.9589</td>
<td id="S4.T2.1.4.3.5" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">(+1.3%)</td>
<td id="S4.T2.1.4.3.6" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.4.3.7" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">0.8573</td>
<td id="S4.T2.1.4.3.8" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">(+0.8%)</td>
<td id="S4.T2.1.4.3.9" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.4.3.10" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">0.9836</td>
<td id="S4.T2.1.4.3.11" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">(+0.1%)</td>
<td id="S4.T2.1.4.3.12" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.4.3.13" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">0.9795</td>
<td id="S4.T2.1.4.3.14" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">(-0.2%)</td>
<td id="S4.T2.1.4.3.15" class="ltx_td ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.4.3.16" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.4.3.17" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">*0.890</td>
<td id="S4.T2.1.4.3.18" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">(+10.1%)</td>
<td id="S4.T2.1.4.3.19" class="ltx_td ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
</tr>
<tr id="S4.T2.1.5.4" class="ltx_tr">
<td id="S4.T2.1.5.4.1" class="ltx_td ltx_align_left ltx_border_l" style="padding-left:3.0pt;padding-right:3.0pt;">shoulders</td>
<td id="S4.T2.1.5.4.2" class="ltx_td ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.5.4.3" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.5.4.4" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">0.9825</td>
<td id="S4.T2.1.5.4.5" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">(+2.8%)</td>
<td id="S4.T2.1.5.4.6" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.5.4.7" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">0.9240</td>
<td id="S4.T2.1.5.4.8" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">(+1.8%)</td>
<td id="S4.T2.1.5.4.9" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.5.4.10" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">0.8973</td>
<td id="S4.T2.1.5.4.11" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">(+2.6%)</td>
<td id="S4.T2.1.5.4.12" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.5.4.13" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">0.9343</td>
<td id="S4.T2.1.5.4.14" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">(+2.0%)</td>
<td id="S4.T2.1.5.4.15" class="ltx_td ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.5.4.16" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.5.4.17" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">*0.786</td>
<td id="S4.T2.1.5.4.18" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">(+18.9%)</td>
<td id="S4.T2.1.5.4.19" class="ltx_td ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
</tr>
<tr id="S4.T2.1.6.5" class="ltx_tr">
<td id="S4.T2.1.6.5.1" class="ltx_td ltx_align_left ltx_border_l" style="padding-left:3.0pt;padding-right:3.0pt;">elbows</td>
<td id="S4.T2.1.6.5.2" class="ltx_td ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.6.5.3" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.6.5.4" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">0.8655</td>
<td id="S4.T2.1.6.5.5" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">(+3.8%)</td>
<td id="S4.T2.1.6.5.6" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.6.5.7" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">0.7320</td>
<td id="S4.T2.1.6.5.8" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">(+6.4%)</td>
<td id="S4.T2.1.6.5.9" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.6.5.10" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">0.7290</td>
<td id="S4.T2.1.6.5.11" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">(+5.7%)</td>
<td id="S4.T2.1.6.5.12" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.6.5.13" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">0.7916</td>
<td id="S4.T2.1.6.5.14" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">(+4.2%)</td>
<td id="S4.T2.1.6.5.15" class="ltx_td ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.6.5.16" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.6.5.17" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">0.641</td>
<td id="S4.T2.1.6.5.18" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">(+23.5%)</td>
<td id="S4.T2.1.6.5.19" class="ltx_td ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
</tr>
<tr id="S4.T2.1.7.6" class="ltx_tr">
<td id="S4.T2.1.7.6.1" class="ltx_td ltx_align_left ltx_border_l" style="padding-left:3.0pt;padding-right:3.0pt;">wrists</td>
<td id="S4.T2.1.7.6.2" class="ltx_td ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.7.6.3" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.7.6.4" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">0.7341</td>
<td id="S4.T2.1.7.6.5" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">(+2.0%)</td>
<td id="S4.T2.1.7.6.6" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.7.6.7" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">0.5657</td>
<td id="S4.T2.1.7.6.8" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">(+2.4%)</td>
<td id="S4.T2.1.7.6.9" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.7.6.10" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">0.6263</td>
<td id="S4.T2.1.7.6.11" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">(+1.2%)</td>
<td id="S4.T2.1.7.6.12" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.7.6.13" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">0.6961</td>
<td id="S4.T2.1.7.6.14" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">(+1.5%)</td>
<td id="S4.T2.1.7.6.15" class="ltx_td ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.7.6.16" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.7.6.17" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">0.503</td>
<td id="S4.T2.1.7.6.18" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">(+38.4%)</td>
<td id="S4.T2.1.7.6.19" class="ltx_td ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
</tr>
<tr id="S4.T2.1.8.7" class="ltx_tr">
<td id="S4.T2.1.8.7.1" class="ltx_td ltx_align_left ltx_border_l" style="padding-left:3.0pt;padding-right:3.0pt;">hips</td>
<td id="S4.T2.1.8.7.2" class="ltx_td ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.8.7.3" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.8.7.4" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">0.9630</td>
<td id="S4.T2.1.8.7.5" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">(+0.0%)</td>
<td id="S4.T2.1.8.7.6" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.8.7.7" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">0.8686</td>
<td id="S4.T2.1.8.7.8" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">(+2.8%)</td>
<td id="S4.T2.1.8.7.9" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.8.7.10" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">0.6704</td>
<td id="S4.T2.1.8.7.11" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">(-1.1%)</td>
<td id="S4.T2.1.8.7.12" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.8.7.13" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">0.7854</td>
<td id="S4.T2.1.8.7.14" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">(+0.7%)</td>
<td id="S4.T2.1.8.7.15" class="ltx_td ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.8.7.16" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.8.7.17" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">*0.786</td>
<td id="S4.T2.1.8.7.18" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">(-0.1%)</td>
<td id="S4.T2.1.8.7.19" class="ltx_td ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
</tr>
<tr id="S4.T2.1.9.8" class="ltx_tr">
<td id="S4.T2.1.9.8.1" class="ltx_td ltx_align_left ltx_border_l" style="padding-left:3.0pt;padding-right:3.0pt;">knees</td>
<td id="S4.T2.1.9.8.2" class="ltx_td ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.9.8.3" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.9.8.4" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">0.8686</td>
<td id="S4.T2.1.9.8.5" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">(+2.8%)</td>
<td id="S4.T2.1.9.8.6" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.9.8.7" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">0.7444</td>
<td id="S4.T2.1.9.8.8" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">(+2.5%)</td>
<td id="S4.T2.1.9.8.9" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.9.8.10" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">0.6643</td>
<td id="S4.T2.1.9.8.11" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">(+2.9%)</td>
<td id="S4.T2.1.9.8.12" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.9.8.13" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">0.7577</td>
<td id="S4.T2.1.9.8.14" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">(+3.4%)</td>
<td id="S4.T2.1.9.8.15" class="ltx_td ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.9.8.16" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.9.8.17" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">0.610</td>
<td id="S4.T2.1.9.8.18" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">(+24.2%)</td>
<td id="S4.T2.1.9.8.19" class="ltx_td ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
</tr>
<tr id="S4.T2.1.10.9" class="ltx_tr">
<td id="S4.T2.1.10.9.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l" style="padding-left:3.0pt;padding-right:3.0pt;">ankles</td>
<td id="S4.T2.1.10.9.2" class="ltx_td ltx_border_b ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.10.9.3" class="ltx_td ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.10.9.4" class="ltx_td ltx_align_right ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;">0.8090</td>
<td id="S4.T2.1.10.9.5" class="ltx_td ltx_align_right ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;">(+1.3%)</td>
<td id="S4.T2.1.10.9.6" class="ltx_td ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.10.9.7" class="ltx_td ltx_align_right ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;">0.6910</td>
<td id="S4.T2.1.10.9.8" class="ltx_td ltx_align_right ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;">(-0.3%)</td>
<td id="S4.T2.1.10.9.9" class="ltx_td ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.10.9.10" class="ltx_td ltx_align_right ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;">0.6263</td>
<td id="S4.T2.1.10.9.11" class="ltx_td ltx_align_right ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;">(+1.0%)</td>
<td id="S4.T2.1.10.9.12" class="ltx_td ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.10.9.13" class="ltx_td ltx_align_right ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;">0.7105</td>
<td id="S4.T2.1.10.9.14" class="ltx_td ltx_align_right ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;">(+1.8%)</td>
<td id="S4.T2.1.10.9.15" class="ltx_td ltx_border_b ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.10.9.16" class="ltx_td ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S4.T2.1.10.9.17" class="ltx_td ltx_align_right ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;">0.596</td>
<td id="S4.T2.1.10.9.18" class="ltx_td ltx_align_right ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;">(+19.2%)</td>
<td id="S4.T2.1.10.9.19" class="ltx_td ltx_border_b ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Keypoint breakdown of our most performant “feature concatenation” model trained on our extended ADD dataset. In the center, we list the relative improvement of each metric when training on additional data. On the right, we display the PDJ@20 from Khungurn <em id="S4.T2.4.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S4.T2.5.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, and report the relative difference from our best model. *Note that due to keypoint incompatibilities, we fill missing keypoint results from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> using the most similar keypoints reported: “head” for eyes and ears, and “body” for shoulders and hips.</figcaption>
</figure>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Model</th>
<th id="S4.T3.1.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">F-1</th>
<th id="S4.T3.1.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">pre.</th>
<th id="S4.T3.1.1.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">rec.</th>
<th id="S4.T3.1.1.1.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_t">IoU</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.2.1" class="ltx_tr">
<th id="S4.T3.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">Ours</th>
<td id="S4.T3.1.2.1.2" class="ltx_td ltx_align_right ltx_border_tt"><span id="S4.T3.1.2.1.2.1" class="ltx_text ltx_font_bold">0.9472</span></td>
<td id="S4.T3.1.2.1.3" class="ltx_td ltx_align_right ltx_border_tt"><span id="S4.T3.1.2.1.3.1" class="ltx_text ltx_font_bold">0.9427</span></td>
<td id="S4.T3.1.2.1.4" class="ltx_td ltx_align_right ltx_border_tt"><span id="S4.T3.1.2.1.4.1" class="ltx_text ltx_font_bold">0.9576</span></td>
<td id="S4.T3.1.2.1.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt"><span id="S4.T3.1.2.1.5.1" class="ltx_text ltx_font_bold">0.9326</span></td>
</tr>
<tr id="S4.T3.1.3.2" class="ltx_tr">
<th id="S4.T3.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">YAAS SOLOv2</th>
<td id="S4.T3.1.3.2.2" class="ltx_td ltx_align_right">0.9061</td>
<td id="S4.T3.1.3.2.3" class="ltx_td ltx_align_right">0.9003</td>
<td id="S4.T3.1.3.2.4" class="ltx_td ltx_align_right">0.9379</td>
<td id="S4.T3.1.3.2.5" class="ltx_td ltx_align_right ltx_border_r">0.9077</td>
</tr>
<tr id="S4.T3.1.4.3" class="ltx_tr">
<th id="S4.T3.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">YAAS CondInst</th>
<td id="S4.T3.1.4.3.2" class="ltx_td ltx_align_right">0.8866</td>
<td id="S4.T3.1.4.3.3" class="ltx_td ltx_align_right">0.8824</td>
<td id="S4.T3.1.4.3.4" class="ltx_td ltx_align_right">0.8999</td>
<td id="S4.T3.1.4.3.5" class="ltx_td ltx_align_right ltx_border_r">0.9158</td>
</tr>
<tr id="S4.T3.1.5.4" class="ltx_tr">
<th id="S4.T3.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r">AniSeg</th>
<td id="S4.T3.1.5.4.2" class="ltx_td ltx_align_right ltx_border_b">0.5857</td>
<td id="S4.T3.1.5.4.3" class="ltx_td ltx_align_right ltx_border_b">0.5877</td>
<td id="S4.T3.1.5.4.4" class="ltx_td ltx_align_right ltx_border_b">0.5954</td>
<td id="S4.T3.1.5.4.5" class="ltx_td ltx_align_right ltx_border_b ltx_border_r">0.6651</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparison of our character segmentation and bounding box performance, described in Sec. <a href="#S5.SS3" title="5.3 Character Segmentation &amp; Bounding Boxes ‣ 5 Experiments ‣ Transfer Learning for Pose Estimation of Illustrated Characters" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3</span></a>.</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We used PyTorch <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> wrapped in Lightning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>; some models use the R101-FPN keypoint detection R-CNN from Detectron2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>. All models can be trained with a single GTX1080ti (11GB VRAM). Unless otherwise mentioned, we trained models using the Adam <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> optimizer, with 0.001 learning rate and batch size 32, for <math id="S5.p1.1.m1.2" class="ltx_Math" alttext="1{,}000" display="inline"><semantics id="S5.p1.1.m1.2a"><mrow id="S5.p1.1.m1.2.3.2" xref="S5.p1.1.m1.2.3.1.cmml"><mn id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml">1</mn><mo id="S5.p1.1.m1.2.3.2.1" xref="S5.p1.1.m1.2.3.1.cmml">,</mo><mn id="S5.p1.1.m1.2.2" xref="S5.p1.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.2b"><list id="S5.p1.1.m1.2.3.1.cmml" xref="S5.p1.1.m1.2.3.2"><cn type="integer" id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1">1</cn><cn type="integer" id="S5.p1.1.m1.2.2.cmml" xref="S5.p1.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.2c">1{,}000</annotation></semantics></math> epochs.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">The ResNet backbone is trained on the Danbooru tag classification task using our new manual tagging rulebook (Sec. <a href="#S4.SS2" title="4.2 ResNet Tagger Data ‣ 4 Data Collection ‣ Transfer Learning for Pose Estimation of Illustrated Characters" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>). The character segmenter used for bounding boxes is trained with our new character segmentation dataset (Sec. <a href="#S4.SS3" title="4.3 Character Segmentation Data ‣ 4 Data Collection ‣ Transfer Learning for Pose Estimation of Illustrated Characters" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>). Using the previous two submodules, we train the pose estimator using our upgraded version of the ADD dataset (Sec. <a href="#S4.SS1" title="4.1 Pose Data ‣ 4 Data Collection ‣ Transfer Learning for Pose Estimation of Illustrated Characters" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>). All data and code will be released upon publication.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Pose Estimation Transfer</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Table <a href="#S3.T1" title="Table 1 ‣ 3.3 Character Segmentation &amp; Bounding Boxes ‣ 3 Method &amp; Architectures ‣ Transfer Learning for Pose Estimation of Illustrated Characters" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the performance of different architectures. We report COCO OKS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, PCKh and PCPm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, and PDJ (for comparison with Khungurn <em id="S5.SS1.p1.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S5.SS1.p1.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>). From the top four rows, we see that our proposed feature concatenation and matching models perform the best out overall, and that the addition of our new data increases performance. We also observe that while concatenation performs marginally better than matching, matching is 8.8x more parameter efficient and one-third faster at inference.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">The second group of Table <a href="#S3.T1" title="Table 1 ‣ 3.3 Character Segmentation &amp; Bounding Boxes ‣ 3 Method &amp; Architectures ‣ Transfer Learning for Pose Estimation of Illustrated Characters" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows other architectures, roughly in order of method complexity. Here, as in Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Transfer Learning for Pose Estimation of Illustrated Characters" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, “task” source features refer to Mask R-CNN pose estimation features, and “domain” source features refer to illustration features extracted by our ResNet50 tag classifier.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p"><span id="S5.SS1.p3.1.1" class="ltx_text ltx_font_bold">“Task Fine-tuning Only”</span> fine-tunes the pretrained Mask R-CNN head with its frozen default backbone; the last head layer is re-initialized to accommodate auxiliary appendage keypoints. This is vanilla transfer by fine-tuning a task-specific source network on a small task-specific target domain dataset.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p"><span id="S5.SS1.p4.1.1" class="ltx_text ltx_font_bold">“Domain Features Only”</span> is our frozen ResNet50 backbone with a keypoint head. This is vanilla transfer by adding a new task head to a domain-specific source network.</p>
</div>
<div id="S5.SS1.p5" class="ltx_para">
<p id="S5.SS1.p5.1" class="ltx_p"><span id="S5.SS1.p5.1.1" class="ltx_text ltx_font_bold">“Task Fine-tuning w/ Domain Features”</span> fine-tunes the pretrained Mask R-CNN head as above, but replaces the R-CNN backbone with our frozen ResNet50 backbone. This is a naive method of incorporating both sources, attempting to adapt the task source’s pretrained prediction component to new domain features.</p>
</div>
<div id="S5.SS1.p6" class="ltx_para">
<p id="S5.SS1.p6.1" class="ltx_p"><span id="S5.SS1.p6.1.1" class="ltx_text ltx_font_bold">“Adversarial (DeepFashion2)”</span> reuses the feature matching architecture, but performs adversarial domain discrimination instead of MSE matching. The discriminator is a shallow 2-layer convnet, trained to separate Mask R-CNN features of randomly sampled DeepFashion2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> images from ResNet features of Danbooru illustrations. As the feature maps to discriminate are spatial, we are careful to employ only 1x1 kernels in the discriminator; otherwise, the discriminator could pick up intrinsic anatomical differences. The matching network now fools the discriminator by adversarially aligning the feature distributions.</p>
</div>
<div id="S5.SS1.p7" class="ltx_para">
<p id="S5.SS1.p7.1" class="ltx_p"><span id="S5.SS1.p7.1.1" class="ltx_text ltx_font_bold">“Adversarial (COCO)”</span> is the same adversarial architecture as above, but using COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> images containing people instead of Deepfashion2.</p>
</div>
<figure id="S5.F2" class="ltx_figure"><img src="/html/2108.01819/assets/figs/retrieval_figure.png" id="S5.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="433" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Pose-based retrieval. From left to right, we show the query image (descriptor distance zero) followed by its five nearest neighbors (duplicate and NSFW images removed). Each illustration is annotated with its Danbooru ID, descriptor distance to the query, and the predicted bounding box with COCO keypoints. Please see supplementary materials for full artist attribution and additional examples.</figcaption>
</figure>
<div id="S5.SS1.p8" class="ltx_para">
<p id="S5.SS1.p8.1" class="ltx_p">While domain-features-only is the cheapest architecture overall, it is only slightly more efficient than feature matching, and loses all benefits of task-specific transfer. However, the performance drop from feature concatenation to domain-features-only and task-with-domain-features is not very large (2-3% OKS@50); meanwhile, there is a wide gap to task-fine-tuning-only. This shows that the domain-specific ResNet50 backbone trained on our new body-tag rulebook provides much more predictive power than the task-specific pretrained Mask R-CNN.</p>
</div>
<div id="S5.SS1.p9" class="ltx_para">
<p id="S5.SS1.p9.1" class="ltx_p">It is important to note that the adversarial models exhibited significant instability during training. After extensive hyperparameter tuning, the best DeepFashion2 model returns NaN loss at epoch 795, and the best COCO model fails at epoch 354; all other models safely exited at epoch <math id="S5.SS1.p9.1.m1.2" class="ltx_Math" alttext="1{,}000" display="inline"><semantics id="S5.SS1.p9.1.m1.2a"><mrow id="S5.SS1.p9.1.m1.2.3.2" xref="S5.SS1.p9.1.m1.2.3.1.cmml"><mn id="S5.SS1.p9.1.m1.1.1" xref="S5.SS1.p9.1.m1.1.1.cmml">1</mn><mo id="S5.SS1.p9.1.m1.2.3.2.1" xref="S5.SS1.p9.1.m1.2.3.1.cmml">,</mo><mn id="S5.SS1.p9.1.m1.2.2" xref="S5.SS1.p9.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p9.1.m1.2b"><list id="S5.SS1.p9.1.m1.2.3.1.cmml" xref="S5.SS1.p9.1.m1.2.3.2"><cn type="integer" id="S5.SS1.p9.1.m1.1.1.cmml" xref="S5.SS1.p9.1.m1.1.1">1</cn><cn type="integer" id="S5.SS1.p9.1.m1.2.2.cmml" xref="S5.SS1.p9.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p9.1.m1.2c">1{,}000</annotation></semantics></math>. DeepFashion2 likely outperforms COCO because the image composition is much more similar to that of Danbooru; images are typically single-person portraits with most of the body in view. Adversarial losses are notoriously difficult to optimize, and in our case destabilized training so as to perform worse than not having been used at all.</p>
</div>
<div id="S5.SS1.p10" class="ltx_para">
<p id="S5.SS1.p10.1" class="ltx_p">The fourth group of Table <a href="#S3.T1" title="Table 1 ‣ 3.3 Character Segmentation &amp; Bounding Boxes ‣ 3 Method &amp; Architectures ‣ Transfer Learning for Pose Estimation of Illustrated Characters" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows out-of-the-box generalization to illustrations for Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> and OpenPose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. We use Mask R-CNN as our task-specific source, as it is less-overfit to natural images than OpenPose.</p>
</div>
<div id="S5.SS1.p11" class="ltx_para">
<p id="S5.SS1.p11.1" class="ltx_p">Table <a href="#S4.T2" title="Table 2 ‣ 4.3 Character Segmentation Data ‣ 4 Data Collection ‣ Transfer Learning for Pose Estimation of Illustrated Characters" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> gives a keypoint breakdown and comparison with Khungurn <em id="S5.SS1.p11.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S5.SS1.p11.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. The results demonstrate that training on our additional more varied data improves the overall model performance; this is especially true for appendage keypoints, which are more variable than the head and torso. We also see significant improvement from results reported in Khungurn <em id="S5.SS1.p11.1.3" class="ltx_emph ltx_font_italic">et al</em>. The exception is the hips, for which we compare to their “body” keypoint at the navel. While this is not a direct comparison, our PDJ on hips is nevertheless low relative to other keypoints. This is because PDJ does not account for the intrinsic ambiguity of the hips; looking at the OKS, which accounts for annotator disagreement, we see that hip performance is actually quite high.</p>
</div>
<div id="S5.SS1.p12" class="ltx_para">
<p id="S5.SS1.p12.1" class="ltx_p">An important caveat is that the metrics are generally not comparable with those reported in human pose estimation. COCO OKS, for example, was designed using annotator disagreement on natural images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>; however, illustrated character proportions deviate widely from the standard human form (i.e. bigger head and eyes). Characters also tend to take up more screen space proportional to body size (i.e. big hair and clothing), leading to looser thresholds normalized by bounding box size.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>ResNet Tagger Backbone</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">We train our ResNet50 tagger backbone to produce illustration-specific source features (Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Transfer Learning for Pose Estimation of Illustrated Characters" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). Taking into account the class imbalance, we accumulate gradients for an effective batch size of 512. Considering the minimum (0.04%) and median (0.38%) class frequencies, we may expect the smallest class to appear 0.2 times per batch, and the median class to appear 1.9 times per batch.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">To demonstrate the effectiveness of our tag rulebook and class reweighing strategy, we report performance on pose estimation using two other ResNet50 backbones: the RF5 tagger <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, and the default ImageNet-pretrained ResNet50 from PyTorch <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>. While there are several Danbooru taggers available <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, we chose to compare our backbone to the RF5 tagger <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> because it is the most architecturally similar to our ResNet50, and relatively better-documented. The backbones all share the same architecture and parameter count, and are all placed into our feature concatenation transfer model for the ablation.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">The backbone ablation results are shown in the last three rows of Table <a href="#S3.T1" title="Table 1 ‣ 3.3 Character Segmentation &amp; Bounding Boxes ‣ 3 Method &amp; Architectures ‣ Transfer Learning for Pose Estimation of Illustrated Characters" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. As expected, a classifier trained with our novel body-part-specific tagging rulebook and class-balancing techniques significantly improves transfer to pose estimation. Note that our tagger also outperforms RF5 at classification (on shared target classes); please refer to the supplementary materials for more details.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Character Segmentation &amp; Bounding Boxes</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">We compare the segmentation and bounding box performance of our system with that of publicly-available models. AniSeg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> is a Faster-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, and YAAS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite> provides SOLOv2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> and CondInst <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> models. These detectors may detect more than one character, and their bounding boxes are not necessarily tight around segmentations; for simplicity, we union all predicted segmentations of an image, and redraw a tight bounding box around the union. We evaluate all models on the same test set described in Sec. <a href="#S4.SS3" title="4.3 Character Segmentation Data ‣ 4 Data Collection ‣ Transfer Learning for Pose Estimation of Illustrated Characters" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>. Table <a href="#S4.T3" title="Table 3 ‣ 4.3 Character Segmentation Data ‣ 4 Data Collection ‣ Transfer Learning for Pose Estimation of Illustrated Characters" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows that training with our new 20x larger dataset outperforms available models in both mean F-1 (segmentation) and IoU (bounding boxes); we thus use it in our pipeline for bounding box prediction.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Application: Pose-guided Retrieval</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">An immediate application of our illustrated pose estimator is a pose-guided character retrieval system. We construct a proof-of-concept retriever that takes a query character (or user-specified keypoints and bounding box) and searches for illustrated characters in a similar pose. This system can serve as a useful search tool for artists, who often use reference drawings while illustrating.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Our pose retriever performs a simple nearest-neighbor search. The support images consist of single-character Danbooru illustrations with the full_body tag. Using our best-performing model, we extract bounding boxes and keypoint locations for each character, normalize the keypoints by the longest bounding box dimension, and finally store the pairwise euclidean distances between the normalized keypoints. This process ensures the pairwise-distance descriptor is invariant to translation, rotation, and image scale. At inference, we extract the descriptor from the query, and find the euclidean k-nearest neighbors from the support set.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">In practice, we compute descriptors using all 25 predicted keypoints (17 COCO and 8 additional appendage midpoints). This makes the descriptor 300-dimensional (25 choose 2), which is generally too large for tree-based nearest neighbors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. However, since our support set consists of 136k points, we are still able to brute force search in reasonable time. Empirically, each query takes about 0.1341s for keypoint extraction (GPU) and 0.0638s for search (CPU).</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p">To demonstrate the effectiveness of our pose estimator, we present several query results in Fig. <a href="#S5.F2" title="Figure 2 ‣ 5.1 Pose Estimation Transfer ‣ 5 Experiments ‣ Transfer Learning for Pose Estimation of Illustrated Characters" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>; while there is no ground-truth to measure quantitative performance, qualitative inspection suggests that our model works well. We can retrieve reasonably similar illustrations for standard poses as shown in the first row, as well as more difficult poses for which illustrators would want references. Note that while our system has no awareness of perspective, it is able to effectively leverage keypoint cues to retrieve similarly foreshortened views in the last row. For more examples, please refer to our supplementary materials.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion &amp; Future Work</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">While we may continue to improve the transfer performance through methods like pseudo-labeling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> or cycle-consistent image translation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, we can also begin extending our work to multi-character detection and pose estimation. While it is possible to construct a naive instance-based segmentation and keypoint estimation dataset by compositing background-removed ADD samples, we cannot expect a system trained on such data to perform well in-the-wild. Character interactions in illustrations are often much more complex than human interactions in real life, with much more frequent physical contact. For example, Danbooru has 43.6k images tagged with holding_hands and 59.1k with hugging, already accounting for 2.8% of the entire dataset. Simply compositing independent characters together would not be able to model the intricacies of the illustration domain; we would again need to expand our datasets with annotated instances of character interactions.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">As a fundamental vision task, pose estimation also provides a valuable prior for numerous other novel applications in the illustrated domain. Our pose estimator opens the door to pose-guided retargeting for automatic character animation, better keyframe interpolation, pose-aware illustration colorization, 3D character reconstruction, etc.</p>
</div>
<div id="S7.p3" class="ltx_para">
<p id="S7.p3.1" class="ltx_p">In conclusion, we demonstrate state-of-the-art pose estimation on the illustrated character domain, by leveraging both domain-specific and task-specific source models. Our model significantly outperforms prior art <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> despite the absence of synthetic supervision, thanks to successful transfer from our new illustration tagging subtask focused on classifying body-related tags. In addition, we provide a single-region proposer trained on a novel character segmentation dataset 20x larger than those currently available, as well as an updated illustration pose estimation dataset with twice the number of samples in more diverse poses. Our model performance allows for the novel task of pose-guided character illustration retrieval, and paves the way for future applications in the illustrated domain.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and Bernt Schiele.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">2d human pose estimation: New benchmark and state of the art
analysis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE CVPR (CVPR)</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, June 2014.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Anonymous, Danbooru community, and Gwern Branwen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Danbooru2020: A large-scale crowdsourced and tagged anime
illustration dataset.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.gwern.net/Danbooru2020" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://www.gwern.net/Danbooru2020</a><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">, January 2021.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Matthew Baas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Danbooru2018 pretrained resnet models for pytorch.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://rf5.github.io" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://rf5.github.io</a><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">, July 2019.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Justin Brooks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">COCO Annotator.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/jsbroks/coco-annotator/" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/jsbroks/coco-annotator/</a><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Philip Buchanan, Ramakrishnan Mukundan, and Michael Doggett.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Automatic single-view character model reconstruction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the International Symposium on Sketch-Based
Interfaces and Modeling</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, pages 5–14, 2013.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Lars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa, Andreas
Mueller, Olivier Grisel, Vlad Niculae, Peter Prettenhofer, Alexandre
Gramfort, Jaques Grobler, Robert Layton, Jake VanderPlas, Arnaud Joly, Brian
Holt, and Gaël Varoquaux.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">API design for machine learning software: experiences from the
scikit-learn project.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECML PKDD Workshop: Languages for Data Mining and Machine
Learning</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, pages 108–122, 2013.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Kaidi Cao, Jing Liao, and Lu Yuan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Carigans: Unpaired photo-to-caricature translation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1811.00222</span><span id="bib.bib7.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, and Yaser Sheikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Openpose: realtime multi-person 2d pose estimation using part
affinity fields.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE transactions on pattern analysis and machine intelligence</span><span id="bib.bib8.4.2" class="ltx_text" style="font-size:90%;">,
43(1):172–186, 2019.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Rethinking atrous convolution for semantic image segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1706.05587</span><span id="bib.bib9.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Hal Daume.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Domain adaptation vs. transfer learning, Nov 2007.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Carl Doersch and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Sim2real transfer learning for 3d human pose estimation: motion to
the rescue.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1907.02499</span><span id="bib.bib11.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Haoye Dong, Xiaodan Liang, Xiaohui Shen, Bochao Wang, Hanjiang Lai, Jia Zhu,
Zhiting Hu, and Jian Yin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Towards multi-pose guided virtual try-on network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF ICCV</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, pages 9026–9035, 2019.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Nandaka et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Pixivutil2.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/Nandaka/PixivUtil2" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/Nandaka/PixivUtil2</a><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
William Falcon et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Pytorch lightning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">GitHub. Note:
https://github.com/PyTorchLightning/pytorch-lightning</span><span id="bib.bib14.4.2" class="ltx_text" style="font-size:90%;">, 3, 2019.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Hao-Shu Fang, Shuqin Xie, Yu-Wing Tai, and Cewu Lu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Rmpe: Regional multi-person pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE ICCV</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, pages 2334–2343, 2017.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Oran Gafni, Oron Ashual, and Lior Wolf.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Single-shot freestyle dance reenactment.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2012.01158</span><span id="bib.bib16.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Yuying Ge, Ruimao Zhang, Xiaogang Wang, Xiaoou Tang, and Ping Luo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Deepfashion2: A versatile benchmark for detection, pose estimation,
segmentation and re-identification of clothing images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF CVPR</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, pages 5337–5345, 2019.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Rıza Alp Güler, Natalia Neverova, and Iasonas Kokkinos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Densepose: Dense human pose estimation in the wild.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE CVPR</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, pages 7297–7306, 2018.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Koichi Hamada, Kentaro Tachibana, Tianqi Li, Hiroto Honda, and Yusuke Uchida.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Full-body high-resolution anime generation with progressive
structure-conditional generative adversarial networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision
(ECCV) Workshops</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, pages 0–0, 2018.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Mask r-cnn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE ICCV</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, pages 2961–2969, 2017.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE CVPR</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, pages 770–778, 2016.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate
Saenko, Alexei Efros, and Trevor Darrell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Cycada: Cycle-consistent adversarial domain adaptation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICML</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, pages 1989–1998. PMLR, 2018.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Zeng Huang, Yuanlu Xu, Christoph Lassner, Hao Li, and Tony Tung.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Arch: Animatable reconstruction of clothed humans.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF CVPR</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, pages 3093–3102, 2020.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Human3.6m: Large scale datasets and predictive methods for 3d human
sensing in natural environments.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span id="bib.bib24.4.2" class="ltx_text" style="font-size:90%;">,
36(7):1325–1339, jul 2014.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Pramook Khungurn and Derek Chou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Pose estimation of anime/manga characters: a case for synthetic data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 1st International Workshop on coMics
ANalysis, Processing and Understanding</span><span id="bib.bib25.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Junho Kim, Minjae Kim, Hyeonwoo Kang, and Kwanghee Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">U-gat-it: unsupervised generative attentional networks with adaptive
layer-instance normalization for image-to-image translation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1907.10830</span><span id="bib.bib26.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Kichang Kim, Rachmadani Haryono, and Arthur Guo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Deepdanbooru.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/KichangKim/DeepDanbooru" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/KichangKim/DeepDanbooru</a><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Diederik P Kingma and Jimmy Ba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Adam: A method for stochastic optimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1412.6980</span><span id="bib.bib28.4.2" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Christoph Lassner, Javier Romero, Martin Kiefel, Federica Bogo, Michael J
Black, and Peter V Gehler.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Unite the people: Closing the loop between 3d and 2d human
representations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE CVPR</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, pages 6050–6059, 2017.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Chen Li and Gim Hee Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">From synthetic to real: Unsupervised domain adaptation for animal
pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2103.14843</span><span id="bib.bib30.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Chengze Li, Xueting Liu, and Tien-Tsin Wong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Deep extraction of manga structural lines.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ACM Transactions on Graphics (TOG)</span><span id="bib.bib31.4.2" class="ltx_text" style="font-size:90%;">, 36(4):1–12, 2017.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Jerry Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Pixiv dataset.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/jerryli27/pixiv_dataset" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/jerryli27/pixiv_dataset</a><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Jerry Li and Tazik Shahjahan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Aniseg.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/jerryli27/AniSeg" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/jerryli27/AniSeg</a><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Microsoft coco: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European conference on computer vision</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">, pages 740–755.
Springer, 2014.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Zelun Luo, Yuliang Zou, Judy Hoffman, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Label efficient learning of transferable representations across
domains and tasks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1712.00123</span><span id="bib.bib35.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Jiaxu Miao, Yu Wu, Ping Liu, Yuhang Ding, and Yi Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Pose-guided feature alignment for occluded person re-identification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF ICCV</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:90%;">, pages 542–551, 2019.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Gyeongsik Moon and Kyoung Mu Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">I2l-meshnet: Image-to-lixel prediction network for accurate 3d human
pose and mesh estimation from a single rgb image.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2008.03713</span><span id="bib.bib37.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Tewodros Legesse Munea, Yalew Zelalem Jembre, Halefom Tekle Weldegebriel,
Longbiao Chen, Chenxi Huang, and Chenhui Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">The progress of human pose estimation: a survey and taxonomy of
models applied in 2d human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Access</span><span id="bib.bib38.4.2" class="ltx_text" style="font-size:90%;">, 8:133330–133348, 2020.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan
Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith
Chintala.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">Pytorch: An imperative style, high-performance deep learning library.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text" style="font-size:90%;">In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, </span><span id="bib.bib39.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural
Information Processing Systems 32</span><span id="bib.bib39.5.3" class="ltx_text" style="font-size:90%;">, pages 8024–8035. Curran Associates,
Inc., 2019.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Omid Poursaeed, Vladimir Kim, Eli Shechtman, Jun Saito, and Serge Belongie.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Neural puppet: Generative layered cartoon characters.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib40.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF WACV</span><span id="bib.bib40.5.3" class="ltx_text" style="font-size:90%;">, pages 3346–3356, 2020.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">Faster r-cnn: Towards real-time object detection with region proposal
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1506.01497</span><span id="bib.bib41.4.2" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Jamie Shotton, Andrew Fitzgibbon, Mat Cook, Toby Sharp, Mark Finocchio, Richard
Moore, Alex Kipman, and Andrew Blake.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Real-time human pose recognition in parts from single depth images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib42.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR 2011</span><span id="bib.bib42.5.3" class="ltx_text" style="font-size:90%;">, pages 1297–1304. Ieee, 2011.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Edgar Simo-Serra, Satoshi Iizuka, and Hiroshi Ishikawa.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Mastering sketching: adversarial augmentation for structured
prediction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ACM Transactions on Graphics (TOG)</span><span id="bib.bib43.4.2" class="ltx_text" style="font-size:90%;">, 37(1):1–13, 2018.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Li Siyao, Shiyu Zhao, Weijiang Yu, Wenxiu Sun, Dimitris N Metaxas, Chen Change
Loy, and Ziwei Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Deep animation video interpolation in the wild.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2104.02495</span><span id="bib.bib44.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Baochen Sun and Kate Saenko.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">Deep coral: Correlation alignment for deep domain adaptation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib45.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European conference on computer vision</span><span id="bib.bib45.5.3" class="ltx_text" style="font-size:90%;">, pages 443–450.
Springer, 2016.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir
Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">Going deeper with convolutions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib46.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE CVPR</span><span id="bib.bib46.5.3" class="ltx_text" style="font-size:90%;">, pages 1–9, 2015.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Zhi Tian, Chunhua Shen, and Hao Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">Conditional convolutions for instance segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2003.05664</span><span id="bib.bib47.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">Adversarial discriminative domain adaptation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib48.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE CVPR</span><span id="bib.bib48.5.3" class="ltx_text" style="font-size:90%;">, pages 7167–7176, 2017.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
Gul Varol, Javier Romero, Xavier Martin, Naureen Mahmood, Michael J Black, Ivan
Laptev, and Cordelia Schmid.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">Learning from synthetic humans.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib49.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE CVPR</span><span id="bib.bib49.5.3" class="ltx_text" style="font-size:90%;">, pages 109–117, 2017.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
Mei Wang and Weihong Deng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">Deep visual domain adaptation: A survey.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Neurocomputing</span><span id="bib.bib50.4.2" class="ltx_text" style="font-size:90%;">, 312:135–153, 2018.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text" style="font-size:90%;">
Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chunhua Shen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.2.1" class="ltx_text" style="font-size:90%;">Solov2: Dynamic and fast instance segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib51.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text" style="font-size:90%;">
Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.2.1" class="ltx_text" style="font-size:90%;">Generalizing from a few examples: A survey on few-shot learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ACM Computing Surveys (CSUR)</span><span id="bib.bib52.4.2" class="ltx_text" style="font-size:90%;">, 53(3):1–34, 2020.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text" style="font-size:90%;">
Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.2.1" class="ltx_text" style="font-size:90%;">Detectron2.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/facebookresearch/detectron2" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/facebookresearch/detectron2</a><span id="bib.bib53.3.1" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text" style="font-size:90%;">
Xiangyu Xu, Hao Chen, Francesc Moreno-Noguer, László A Jeni, and
Fernando De la Torre.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.2.1" class="ltx_text" style="font-size:90%;">3d human shape and pose from a single low-resolution image with
self-supervised learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib54.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</span><span id="bib.bib54.5.3" class="ltx_text" style="font-size:90%;">, pages 284–300.
Springer, 2020.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text" style="font-size:90%;">
Zhan Xu, Yang Zhou, Evangelos Kalogerakis, Chris Landreth, and Karan Singh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.2.1" class="ltx_text" style="font-size:90%;">Rignet: Neural rigging for articulated characters.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2005.00559</span><span id="bib.bib55.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text" style="font-size:90%;">
Lvmin Zhang, Yi Ji, and Chunping Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.2.1" class="ltx_text" style="font-size:90%;">Danbooregion: An illustration region dataset.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib56.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV (13)</span><span id="bib.bib56.5.3" class="ltx_text" style="font-size:90%;">, pages 137–154, 2020.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text" style="font-size:90%;">
Lvmin Zhang, Chengze Li, Tien-Tsin Wong, Yi Ji, and Chunping Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.2.1" class="ltx_text" style="font-size:90%;">Two-stage sketch colorization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ACM Transactions on Graphics (TOG)</span><span id="bib.bib57.4.2" class="ltx_text" style="font-size:90%;">, 37(6):1–14, 2018.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock"><span id="bib.bib58.1.1" class="ltx_text" style="font-size:90%;">
Lvmin Zhang, Edgar Simo-Serra, Yi Ji, and Chunping Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.2.1" class="ltx_text" style="font-size:90%;">Generating digital painting lighting effects via rgb-space geometry.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ACM Transactions on Graphics (TOG)</span><span id="bib.bib58.4.2" class="ltx_text" style="font-size:90%;">, 39(2):1–13, 2020.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock"><span id="bib.bib59.1.1" class="ltx_text" style="font-size:90%;">
zymk9.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.2.1" class="ltx_text" style="font-size:90%;">Yet-another-anime-segmenter.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/zymk9/Yet-Another-Anime-Segmenter" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/zymk9/Yet-Another-Anime-Segmenter</a><span id="bib.bib59.3.1" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
</ul>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Supplementary Materials</h2>

<section id="S8.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.1 </span>Tagger Classification Comparison</h3>

<div id="S8.SS1.p1" class="ltx_para">
<p id="S8.SS1.p1.1" class="ltx_p">In the main paper, we show that our tagger (trained on our new tag rulebook with class-balanced weighing) significantly improves transfer to pose estimation. Here, we show classification results in comparison to the RF5 Danbooru tagger <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, a publicly-available model with the same ResNet50 architecture. RF5 predicts the presence of the top 6000 most common tags in the dataset; 1207 of these are present in our new rulebook, and can be used to predict 1032 of the 1062 total new classes. As we can see from Table <a href="#S8.T4" title="Table 4 ‣ 8.1 Tagger Classification Comparison ‣ 8 Supplementary Materials ‣ Transfer Learning for Pose Estimation of Illustrated Characters" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> below, our model performs much better at classifying the same tags.</p>
</div>
<figure id="S8.T4" class="ltx_table">
<table id="S8.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S8.T4.1.1.1" class="ltx_tr">
<th id="S8.T4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">Model</th>
<th id="S8.T4.1.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">Ours</th>
<th id="S8.T4.1.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">RF5</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S8.T4.1.2.1" class="ltx_tr">
<th id="S8.T4.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;">F-2</th>
<td id="S8.T4.1.2.1.2" class="ltx_td ltx_align_right ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;">0.4744</td>
<td id="S8.T4.1.2.1.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;">0.2297</td>
</tr>
<tr id="S8.T4.1.3.2" class="ltx_tr">
<th id="S8.T4.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding-left:10.0pt;padding-right:10.0pt;">precision</th>
<td id="S8.T4.1.3.2.2" class="ltx_td ltx_align_right" style="padding-left:10.0pt;padding-right:10.0pt;">0.3022</td>
<td id="S8.T4.1.3.2.3" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:10.0pt;padding-right:10.0pt;">0.1238</td>
</tr>
<tr id="S8.T4.1.4.3" class="ltx_tr">
<th id="S8.T4.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding-left:10.0pt;padding-right:10.0pt;">recall</th>
<td id="S8.T4.1.4.3.2" class="ltx_td ltx_align_right" style="padding-left:10.0pt;padding-right:10.0pt;">0.5786</td>
<td id="S8.T4.1.4.3.3" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:10.0pt;padding-right:10.0pt;">0.3360</td>
</tr>
<tr id="S8.T4.1.5.4" class="ltx_tr">
<th id="S8.T4.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding-left:10.0pt;padding-right:10.0pt;">accuracy</th>
<td id="S8.T4.1.5.4.2" class="ltx_td ltx_align_right" style="padding-left:10.0pt;padding-right:10.0pt;">0.9760</td>
<td id="S8.T4.1.5.4.3" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:10.0pt;padding-right:10.0pt;">0.9496</td>
</tr>
<tr id="S8.T4.1.6.5" class="ltx_tr">
<th id="S8.T4.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">F-1</th>
<td id="S8.T4.1.6.5.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">0.4249</td>
<td id="S8.T4.1.6.5.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">0.1910</td>
</tr>
<tr id="S8.T4.1.7.6" class="ltx_tr">
<th id="S8.T4.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding-left:10.0pt;padding-right:10.0pt;">precision</th>
<td id="S8.T4.1.7.6.2" class="ltx_td ltx_align_right" style="padding-left:10.0pt;padding-right:10.0pt;">0.4236</td>
<td id="S8.T4.1.7.6.3" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:10.0pt;padding-right:10.0pt;">0.1898</td>
</tr>
<tr id="S8.T4.1.8.7" class="ltx_tr">
<th id="S8.T4.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding-left:10.0pt;padding-right:10.0pt;">recall</th>
<td id="S8.T4.1.8.7.2" class="ltx_td ltx_align_right" style="padding-left:10.0pt;padding-right:10.0pt;">0.4458</td>
<td id="S8.T4.1.8.7.3" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:10.0pt;padding-right:10.0pt;">0.2235</td>
</tr>
<tr id="S8.T4.1.9.8" class="ltx_tr">
<th id="S8.T4.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r" style="padding-left:10.0pt;padding-right:10.0pt;">accuracy</th>
<td id="S8.T4.1.9.8.2" class="ltx_td ltx_align_right ltx_border_b" style="padding-left:10.0pt;padding-right:10.0pt;">0.9851</td>
<td id="S8.T4.1.9.8.3" class="ltx_td ltx_align_right ltx_border_b ltx_border_r" style="padding-left:10.0pt;padding-right:10.0pt;">0.9727</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Comparison of our Danbooru tagger to RF5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. Metrics are calculated using per-class optimal thresholds for either F-1 or F-2, and averaged across all classes shared between models. Note that this means F-1 and F-2 cannot be directly calculated from their respective precision and recall statistics in the table.</figcaption>
</figure>
</section>
<section id="S8.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.2 </span>Pose Retrieval Additional Results</h3>

<div id="S8.SS2.p1" class="ltx_para">
<p id="S8.SS2.p1.1" class="ltx_p">We display several more pose-based illustration retrieval results in Fig. <a href="#S8.F3" title="Figure 3 ‣ 8.2 Pose Retrieval Additional Results ‣ 8 Supplementary Materials ‣ Transfer Learning for Pose Estimation of Illustrated Characters" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>; the images are taken from the Danbooru dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. The first two rows show challenging sitting positions, on which our model still performs well qualitatively. Despite the differences in orientation, our rotation-invariant descriptor is still able to identify the poses as similar. Rows 3-5 show some more standard poses. Notice that in row 4, the first and second neighbors are variations of the same character in the same pose; it is very common to find a set of such variations uploaded to Danbooru together, and our model may help identify them. In the last two rows, we show failure cases of our model, where incorrect predictions on the query result in neighbors with different poses.</p>
</div>
<div id="S8.SS2.p2" class="ltx_para">
<p id="S8.SS2.p2.1" class="ltx_p">We also provide artist attributions for the figures in the main paper and this supplementary document. Danbooru URLs are at <span id="S8.SS2.p2.1.1" class="ltx_text ltx_font_typewriter">https://danbooru.donmai.us/posts/DANBOORU_ID</span>. Artist websites are tracked from the illustration’s Danbooru page to the best of our ability. Please note the links might not be SFW.</p>
</div>
<figure id="S8.F3" class="ltx_figure"><img src="/html/2108.01819/assets/figs/retrieval_figure_supp.png" id="S8.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="598" height="758" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Additional pose-based retrieval results. From left to right, we show the query image (descriptor distance zero) followed by its five nearest neighbors (duplicate and NSFW images removed). Each illustration is annotated with its Danbooru ID, descriptor distance to the query, and the predicted bounding box with COCO keypoints.</figcaption>
</figure>
<figure id="S8.T5" class="ltx_table">
<table id="S8.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S8.T5.1.1.1" class="ltx_tr">
<th id="S8.T5.1.1.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Row,Col</th>
<th id="S8.T5.1.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Danbooru ID</th>
<th id="S8.T5.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Artist Handle</th>
<th id="S8.T5.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Artist URL</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S8.T5.1.2.1" class="ltx_tr">
<th id="S8.T5.1.2.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">0,0</th>
<th id="S8.T5.1.2.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">2467866</th>
<td id="S8.T5.1.2.1.3" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">露茶</td>
<td id="S8.T5.1.2.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/263905</td>
</tr>
<tr id="S8.T5.1.3.2" class="ltx_tr">
<th id="S8.T5.1.3.2.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">0,1</th>
<th id="S8.T5.1.3.2.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">3550832</th>
<td id="S8.T5.1.3.2.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Goｰ1</td>
<td id="S8.T5.1.3.2.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://twitter.com/go_1tk</td>
</tr>
<tr id="S8.T5.1.4.3" class="ltx_tr">
<th id="S8.T5.1.4.3.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">0,2</th>
<th id="S8.T5.1.4.3.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">2686368</th>
<td id="S8.T5.1.4.3.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">あれっくす</td>
<td id="S8.T5.1.4.3.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://twitter.com/alexmaster55</td>
</tr>
<tr id="S8.T5.1.5.4" class="ltx_tr">
<th id="S8.T5.1.5.4.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">0,3</th>
<th id="S8.T5.1.5.4.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">2690117</th>
<td id="S8.T5.1.5.4.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">クラモリ</td>
<td id="S8.T5.1.5.4.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/1209275</td>
</tr>
<tr id="S8.T5.1.6.5" class="ltx_tr">
<th id="S8.T5.1.6.5.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">0,4</th>
<th id="S8.T5.1.6.5.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">2924224</th>
<td id="S8.T5.1.6.5.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">紅葉(くれは)＠お仕事募集中</td>
<td id="S8.T5.1.6.5.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/6006540</td>
</tr>
<tr id="S8.T5.1.7.6" class="ltx_tr">
<th id="S8.T5.1.7.6.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">0,5</th>
<th id="S8.T5.1.7.6.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">3159104</th>
<td id="S8.T5.1.7.6.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">みの字</td>
<td id="S8.T5.1.7.6.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/1523486</td>
</tr>
<tr id="S8.T5.1.8.7" class="ltx_tr">
<th id="S8.T5.1.8.7.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">1,0</th>
<th id="S8.T5.1.8.7.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">2407777</th>
<td id="S8.T5.1.8.7.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">唯川</td>
<td id="S8.T5.1.8.7.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://twitter.com/yUikw</td>
</tr>
<tr id="S8.T5.1.9.8" class="ltx_tr">
<th id="S8.T5.1.9.8.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">1,1</th>
<th id="S8.T5.1.9.8.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">3410315</th>
<td id="S8.T5.1.9.8.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">楠本みや</td>
<td id="S8.T5.1.9.8.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/6211628</td>
</tr>
<tr id="S8.T5.1.10.9" class="ltx_tr">
<th id="S8.T5.1.10.9.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">1,2</th>
<th id="S8.T5.1.10.9.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">2939081</th>
<td id="S8.T5.1.10.9.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">SeNMU</td>
<td id="S8.T5.1.10.9.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/65308</td>
</tr>
<tr id="S8.T5.1.11.10" class="ltx_tr">
<th id="S8.T5.1.11.10.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">1,3</th>
<th id="S8.T5.1.11.10.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">3561420</th>
<td id="S8.T5.1.11.10.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">なもり@ゆるゆり20巻でた</td>
<td id="S8.T5.1.11.10.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://twitter.com/_namori_</td>
</tr>
<tr id="S8.T5.1.12.11" class="ltx_tr">
<th id="S8.T5.1.12.11.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">1,4</th>
<th id="S8.T5.1.12.11.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">1698324</th>
<td id="S8.T5.1.12.11.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">ratryu</td>
<td id="S8.T5.1.12.11.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/3892817</td>
</tr>
<tr id="S8.T5.1.13.12" class="ltx_tr">
<th id="S8.T5.1.13.12.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">1,5</th>
<th id="S8.T5.1.13.12.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">3671428</th>
<td id="S8.T5.1.13.12.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">ぼや野</td>
<td id="S8.T5.1.13.12.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/1263092</td>
</tr>
<tr id="S8.T5.1.14.13" class="ltx_tr">
<th id="S8.T5.1.14.13.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">2,0</th>
<th id="S8.T5.1.14.13.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">3314044</th>
<td id="S8.T5.1.14.13.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">九条だんぼ</td>
<td id="S8.T5.1.14.13.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://twitter.com/_Dan_ball</td>
</tr>
<tr id="S8.T5.1.15.14" class="ltx_tr">
<th id="S8.T5.1.15.14.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">2,1</th>
<th id="S8.T5.1.15.14.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">120497</th>
<td id="S8.T5.1.15.14.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">劉祥</td>
<td id="S8.T5.1.15.14.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/22017</td>
</tr>
<tr id="S8.T5.1.16.15" class="ltx_tr">
<th id="S8.T5.1.16.15.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">2,2</th>
<th id="S8.T5.1.16.15.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">3669958</th>
<td id="S8.T5.1.16.15.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">もやし</td>
<td id="S8.T5.1.16.15.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://twitter.com/moyashi_mou2</td>
</tr>
<tr id="S8.T5.1.17.16" class="ltx_tr">
<th id="S8.T5.1.17.16.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">2,3</th>
<th id="S8.T5.1.17.16.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">2803142</th>
<td id="S8.T5.1.17.16.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">エノキドォ</td>
<td id="S8.T5.1.17.16.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/4535430</td>
</tr>
<tr id="S8.T5.1.18.17" class="ltx_tr">
<th id="S8.T5.1.18.17.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">2,4</th>
<th id="S8.T5.1.18.17.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">114867</th>
<td id="S8.T5.1.18.17.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">なまもななせ・海通信</td>
<td id="S8.T5.1.18.17.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/1167548</td>
</tr>
<tr id="S8.T5.1.19.18" class="ltx_tr">
<th id="S8.T5.1.19.18.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">2,5</th>
<th id="S8.T5.1.19.18.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">696477</th>
<td id="S8.T5.1.19.18.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">みぞれまじ</td>
<td id="S8.T5.1.19.18.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/1502612</td>
</tr>
<tr id="S8.T5.1.20.19" class="ltx_tr">
<th id="S8.T5.1.20.19.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">3,0</th>
<th id="S8.T5.1.20.19.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">3509383</th>
<td id="S8.T5.1.20.19.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">和菓子</td>
<td id="S8.T5.1.20.19.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/13748172</td>
</tr>
<tr id="S8.T5.1.21.20" class="ltx_tr">
<th id="S8.T5.1.21.20.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">3,1</th>
<th id="S8.T5.1.21.20.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">1803361</th>
<td id="S8.T5.1.21.20.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">アチャコ</td>
<td id="S8.T5.1.21.20.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/1302618</td>
</tr>
<tr id="S8.T5.1.22.21" class="ltx_tr">
<th id="S8.T5.1.22.21.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">3,2</th>
<th id="S8.T5.1.22.21.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">2669609</th>
<td id="S8.T5.1.22.21.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Seedkeng</td>
<td id="S8.T5.1.22.21.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/11039166</td>
</tr>
<tr id="S8.T5.1.23.22" class="ltx_tr">
<th id="S8.T5.1.23.22.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">3,3</th>
<th id="S8.T5.1.23.22.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">2997827</th>
<td id="S8.T5.1.23.22.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">吉崎 観音</td>
<td id="S8.T5.1.23.22.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://twitter.com/yosRRX</td>
</tr>
<tr id="S8.T5.1.24.23" class="ltx_tr">
<th id="S8.T5.1.24.23.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">3,4</th>
<th id="S8.T5.1.24.23.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">3470661</th>
<td id="S8.T5.1.24.23.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">テイク</td>
<td id="S8.T5.1.24.23.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/2096681</td>
</tr>
<tr id="S8.T5.1.25.24" class="ltx_tr">
<th id="S8.T5.1.25.24.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_b ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">3,5</th>
<th id="S8.T5.1.25.24.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">2975316</th>
<td id="S8.T5.1.25.24.3" class="ltx_td ltx_align_left ltx_border_b" style="padding-left:5.0pt;padding-right:5.0pt;">いたたたた</td>
<td id="S8.T5.1.25.24.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://twitter.com/itatatata6</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>Artist attribution for Figure <a href="#S5.F2" title="Figure 2 ‣ 5.1 Pose Estimation Transfer ‣ 5 Experiments ‣ Transfer Learning for Pose Estimation of Illustrated Characters" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> of the main paper.</figcaption>
</figure>
<figure id="S8.T6" class="ltx_table">
<table id="S8.T6.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S8.T6.1.1.1" class="ltx_tr">
<th id="S8.T6.1.1.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Row,Col</th>
<th id="S8.T6.1.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Danbooru ID</th>
<th id="S8.T6.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Artist Handle</th>
<th id="S8.T6.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Artist URL</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S8.T6.1.2.1" class="ltx_tr">
<th id="S8.T6.1.2.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">0,0</th>
<th id="S8.T6.1.2.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">2953032</th>
<td id="S8.T6.1.2.1.3" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">Dev@プロフィールを確認!!</td>
<td id="S8.T6.1.2.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/857300</td>
</tr>
<tr id="S8.T6.1.3.2" class="ltx_tr">
<th id="S8.T6.1.3.2.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">0,1</th>
<th id="S8.T6.1.3.2.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">2910278</th>
<td id="S8.T6.1.3.2.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">shrimpえびちゃん</td>
<td id="S8.T6.1.3.2.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/3989209</td>
</tr>
<tr id="S8.T6.1.4.3" class="ltx_tr">
<th id="S8.T6.1.4.3.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">0,2</th>
<th id="S8.T6.1.4.3.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">2104903</th>
<td id="S8.T6.1.4.3.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">枚方ガルダイン</td>
<td id="S8.T6.1.4.3.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://twitter.com/hgd_AG</td>
</tr>
<tr id="S8.T6.1.5.4" class="ltx_tr">
<th id="S8.T6.1.5.4.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">0,3</th>
<th id="S8.T6.1.5.4.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">714536</th>
<td id="S8.T6.1.5.4.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">steward B</td>
<td id="S8.T6.1.5.4.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/2212889</td>
</tr>
<tr id="S8.T6.1.6.5" class="ltx_tr">
<th id="S8.T6.1.6.5.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">0,4</th>
<th id="S8.T6.1.6.5.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">2690308</th>
<td id="S8.T6.1.6.5.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">aken@お仕事募集中</td>
<td id="S8.T6.1.6.5.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/196317</td>
</tr>
<tr id="S8.T6.1.7.6" class="ltx_tr">
<th id="S8.T6.1.7.6.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">0,5</th>
<th id="S8.T6.1.7.6.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">2260893</th>
<td id="S8.T6.1.7.6.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">ABBB</td>
<td id="S8.T6.1.7.6.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/4066325</td>
</tr>
<tr id="S8.T6.1.8.7" class="ltx_tr">
<th id="S8.T6.1.8.7.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">1,0</th>
<th id="S8.T6.1.8.7.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">3014412</th>
<td id="S8.T6.1.8.7.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Disco</td>
<td id="S8.T6.1.8.7.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/14377769</td>
</tr>
<tr id="S8.T6.1.9.8" class="ltx_tr">
<th id="S8.T6.1.9.8.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">1,1</th>
<th id="S8.T6.1.9.8.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">2239511</th>
<td id="S8.T6.1.9.8.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">千里GAN（改名）</td>
<td id="S8.T6.1.9.8.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://twitter.com/oshirase_gan</td>
</tr>
<tr id="S8.T6.1.10.9" class="ltx_tr">
<th id="S8.T6.1.10.9.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">1,2</th>
<th id="S8.T6.1.10.9.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">3657552</th>
<td id="S8.T6.1.10.9.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">塩soda</td>
<td id="S8.T6.1.10.9.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://twitter.com/kurau3</td>
</tr>
<tr id="S8.T6.1.11.10" class="ltx_tr">
<th id="S8.T6.1.11.10.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">1,3</th>
<th id="S8.T6.1.11.10.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">3587211</th>
<td id="S8.T6.1.11.10.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">せんちゃ</td>
<td id="S8.T6.1.11.10.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/3388329</td>
</tr>
<tr id="S8.T6.1.12.11" class="ltx_tr">
<th id="S8.T6.1.12.11.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">1,4</th>
<th id="S8.T6.1.12.11.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">3478391</th>
<td id="S8.T6.1.12.11.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">ぺろんちょ</td>
<td id="S8.T6.1.12.11.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/2689378</td>
</tr>
<tr id="S8.T6.1.13.12" class="ltx_tr">
<th id="S8.T6.1.13.12.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">1,5</th>
<th id="S8.T6.1.13.12.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">2925591</th>
<td id="S8.T6.1.13.12.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">米山舞</td>
<td id="S8.T6.1.13.12.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/1554775</td>
</tr>
<tr id="S8.T6.1.14.13" class="ltx_tr">
<th id="S8.T6.1.14.13.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">2,0</th>
<th id="S8.T6.1.14.13.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">2444625</th>
<td id="S8.T6.1.14.13.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Pack</td>
<td id="S8.T6.1.14.13.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/6069171</td>
</tr>
<tr id="S8.T6.1.15.14" class="ltx_tr">
<th id="S8.T6.1.15.14.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">2,1</th>
<th id="S8.T6.1.15.14.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">2772952</th>
<td id="S8.T6.1.15.14.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">しずまよしのり</td>
<td id="S8.T6.1.15.14.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://twitter.com/M_ars</td>
</tr>
<tr id="S8.T6.1.16.15" class="ltx_tr">
<th id="S8.T6.1.16.15.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">2,2</th>
<th id="S8.T6.1.16.15.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">2847265</th>
<td id="S8.T6.1.16.15.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">千羽ミハト(Mihato)</td>
<td id="S8.T6.1.16.15.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/485524</td>
</tr>
<tr id="S8.T6.1.17.16" class="ltx_tr">
<th id="S8.T6.1.17.16.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">2,3</th>
<th id="S8.T6.1.17.16.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">670776</th>
<td id="S8.T6.1.17.16.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">芹ざわ_お仕事募集中</td>
<td id="S8.T6.1.17.16.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/1156416</td>
</tr>
<tr id="S8.T6.1.18.17" class="ltx_tr">
<th id="S8.T6.1.18.17.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">2,4</th>
<th id="S8.T6.1.18.17.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">2559537</th>
<td id="S8.T6.1.18.17.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">infukun</td>
<td id="S8.T6.1.18.17.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/5065896</td>
</tr>
<tr id="S8.T6.1.19.18" class="ltx_tr">
<th id="S8.T6.1.19.18.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">2,5</th>
<th id="S8.T6.1.19.18.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">2655733</th>
<td id="S8.T6.1.19.18.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">QBASE</td>
<td id="S8.T6.1.19.18.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/246248</td>
</tr>
<tr id="S8.T6.1.20.19" class="ltx_tr">
<th id="S8.T6.1.20.19.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">3,0</th>
<th id="S8.T6.1.20.19.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">2468752</th>
<td id="S8.T6.1.20.19.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">アモニット</td>
<td id="S8.T6.1.20.19.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/1621147</td>
</tr>
<tr id="S8.T6.1.21.20" class="ltx_tr">
<th id="S8.T6.1.21.20.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">3,1</th>
<th id="S8.T6.1.21.20.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">777932</th>
<td id="S8.T6.1.21.20.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">ヒエラポリスとパムッカレ</td>
<td id="S8.T6.1.21.20.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/873649</td>
</tr>
<tr id="S8.T6.1.22.21" class="ltx_tr">
<th id="S8.T6.1.22.21.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">3,2</th>
<th id="S8.T6.1.22.21.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">961167</th>
<td id="S8.T6.1.22.21.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">ヒエラポリスとパムッカレ</td>
<td id="S8.T6.1.22.21.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/873649</td>
</tr>
<tr id="S8.T6.1.23.22" class="ltx_tr">
<th id="S8.T6.1.23.22.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">3,3</th>
<th id="S8.T6.1.23.22.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">2868154</th>
<td id="S8.T6.1.23.22.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">菟_</td>
<td id="S8.T6.1.23.22.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/3621731</td>
</tr>
<tr id="S8.T6.1.24.23" class="ltx_tr">
<th id="S8.T6.1.24.23.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">3,4</th>
<th id="S8.T6.1.24.23.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">2723355</th>
<td id="S8.T6.1.24.23.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">9Ro</td>
<td id="S8.T6.1.24.23.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://twitter.com/RINGDDINGDONGX2</td>
</tr>
<tr id="S8.T6.1.25.24" class="ltx_tr">
<th id="S8.T6.1.25.24.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">3,5</th>
<th id="S8.T6.1.25.24.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">2982683</th>
<td id="S8.T6.1.25.24.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">dairi</td>
<td id="S8.T6.1.25.24.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/4920496</td>
</tr>
<tr id="S8.T6.1.26.25" class="ltx_tr">
<th id="S8.T6.1.26.25.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">4,0</th>
<th id="S8.T6.1.26.25.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">3450286</th>
<td id="S8.T6.1.26.25.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">さとうぽて</td>
<td id="S8.T6.1.26.25.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://twitter.com/mrcosmoov</td>
</tr>
<tr id="S8.T6.1.27.26" class="ltx_tr">
<th id="S8.T6.1.27.26.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">4,1</th>
<th id="S8.T6.1.27.26.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">1376016</th>
<td id="S8.T6.1.27.26.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">トレス</td>
<td id="S8.T6.1.27.26.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/2056112</td>
</tr>
<tr id="S8.T6.1.28.27" class="ltx_tr">
<th id="S8.T6.1.28.27.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">4,2</th>
<th id="S8.T6.1.28.27.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">2968950</th>
<td id="S8.T6.1.28.27.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">LM7</td>
<td id="S8.T6.1.28.27.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/420928</td>
</tr>
<tr id="S8.T6.1.29.28" class="ltx_tr">
<th id="S8.T6.1.29.28.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">4,3</th>
<th id="S8.T6.1.29.28.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">3204663</th>
<td id="S8.T6.1.29.28.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Cowfee@home</td>
<td id="S8.T6.1.29.28.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://twitter.com/cowfee_desu</td>
</tr>
<tr id="S8.T6.1.30.29" class="ltx_tr">
<th id="S8.T6.1.30.29.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">4,4</th>
<th id="S8.T6.1.30.29.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">2176604</th>
<td id="S8.T6.1.30.29.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">ゴールデン＠お仕事募集中</td>
<td id="S8.T6.1.30.29.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/8607687</td>
</tr>
<tr id="S8.T6.1.31.30" class="ltx_tr">
<th id="S8.T6.1.31.30.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">4,5</th>
<th id="S8.T6.1.31.30.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">892947</th>
<td id="S8.T6.1.31.30.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">mizuki</td>
<td id="S8.T6.1.31.30.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/1050881</td>
</tr>
<tr id="S8.T6.1.32.31" class="ltx_tr">
<th id="S8.T6.1.32.31.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">5,0</th>
<th id="S8.T6.1.32.31.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">3561627</th>
<td id="S8.T6.1.32.31.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">keti</td>
<td id="S8.T6.1.32.31.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/4705322</td>
</tr>
<tr id="S8.T6.1.33.32" class="ltx_tr">
<th id="S8.T6.1.33.32.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">5,1</th>
<th id="S8.T6.1.33.32.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">3006738</th>
<td id="S8.T6.1.33.32.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">ささ吉</td>
<td id="S8.T6.1.33.32.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/7187584</td>
</tr>
<tr id="S8.T6.1.34.33" class="ltx_tr">
<th id="S8.T6.1.34.33.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">5,2</th>
<th id="S8.T6.1.34.33.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">2818640</th>
<td id="S8.T6.1.34.33.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">しろもる</td>
<td id="S8.T6.1.34.33.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/19858643</td>
</tr>
<tr id="S8.T6.1.35.34" class="ltx_tr">
<th id="S8.T6.1.35.34.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">5,3</th>
<th id="S8.T6.1.35.34.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">3673940</th>
<td id="S8.T6.1.35.34.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Jiujiu girl</td>
<td id="S8.T6.1.35.34.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/1221354</td>
</tr>
<tr id="S8.T6.1.36.35" class="ltx_tr">
<th id="S8.T6.1.36.35.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">5,4</th>
<th id="S8.T6.1.36.35.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">3566370</th>
<td id="S8.T6.1.36.35.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">２番目のむ～みん</td>
<td id="S8.T6.1.36.35.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/8186490</td>
</tr>
<tr id="S8.T6.1.37.36" class="ltx_tr">
<th id="S8.T6.1.37.36.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">5,5</th>
<th id="S8.T6.1.37.36.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">3617376</th>
<td id="S8.T6.1.37.36.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">まかだみぁ</td>
<td id="S8.T6.1.37.36.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/2782928</td>
</tr>
<tr id="S8.T6.1.38.37" class="ltx_tr">
<th id="S8.T6.1.38.37.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">6,0</th>
<th id="S8.T6.1.38.37.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">1170740</th>
<td id="S8.T6.1.38.37.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">黒星紅白</td>
<td id="S8.T6.1.38.37.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/178217</td>
</tr>
<tr id="S8.T6.1.39.38" class="ltx_tr">
<th id="S8.T6.1.39.38.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">6,1</th>
<th id="S8.T6.1.39.38.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">2889129</th>
<td id="S8.T6.1.39.38.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Anmi@画集発売中</td>
<td id="S8.T6.1.39.38.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/212801</td>
</tr>
<tr id="S8.T6.1.40.39" class="ltx_tr">
<th id="S8.T6.1.40.39.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">6,2</th>
<th id="S8.T6.1.40.39.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">591979</th>
<td id="S8.T6.1.40.39.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">wk.</td>
<td id="S8.T6.1.40.39.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://www.pixiv.net/en/users/142914</td>
</tr>
<tr id="S8.T6.1.41.40" class="ltx_tr">
<th id="S8.T6.1.41.40.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">6,3</th>
<th id="S8.T6.1.41.40.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">1974764</th>
<td id="S8.T6.1.41.40.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">katsuragi (osaka8o2)</td>
<td id="S8.T6.1.41.40.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://danbooru.donmai.us/artists/108387</td>
</tr>
<tr id="S8.T6.1.42.41" class="ltx_tr">
<th id="S8.T6.1.42.41.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">6,4</th>
<th id="S8.T6.1.42.41.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">3389838</th>
<td id="S8.T6.1.42.41.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">るっぴ</td>
<td id="S8.T6.1.42.41.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://twitter.com/gouya_yu0508</td>
</tr>
<tr id="S8.T6.1.43.42" class="ltx_tr">
<th id="S8.T6.1.43.42.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_b ltx_border_l" style="padding-left:5.0pt;padding-right:5.0pt;">6,5</th>
<th id="S8.T6.1.43.42.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">3357418</th>
<td id="S8.T6.1.43.42.3" class="ltx_td ltx_align_left ltx_border_b" style="padding-left:5.0pt;padding-right:5.0pt;">guitaro (yabasaki taro)</td>
<td id="S8.T6.1.43.42.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">https://danbooru.donmai.us/artists/181484</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span>Artist attribution for Figure <a href="#S8.F3" title="Figure 3 ‣ 8.2 Pose Retrieval Additional Results ‣ 8 Supplementary Materials ‣ Transfer Learning for Pose Estimation of Illustrated Characters" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> in the supplementary materials.</figcaption>
</figure>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2108.01818" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2108.01819" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2108.01819">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2108.01819" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2108.01820" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar 19 07:53:34 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
