<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2006.05927] Recent Advances in 3D Object and Hand Pose Estimation</title><meta property="og:description" content="3D object and hand pose estimation have huge potentials for Augmented Reality, to enable tangible interfaces, natural interfaces, and blurring the boundaries between the real and virtual worlds. In this chapter, we pre…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Recent Advances in 3D Object and Hand Pose Estimation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Recent Advances in 3D Object and Hand Pose Estimation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2006.05927">

<!--Generated on Thu Mar  7 06:41:29 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Recent Advances in 3D Object and Hand Pose Estimation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Vincent Lepetit
<br class="ltx_break"><a href="mailto:vincent.lepetit@enpc.fr" title="" class="ltx_ref ltx_href">vincent.lepetit@enpc.fr</a>
<br class="ltx_break">LIGM, Ecole des Ponts, Univ Gustave Eiffel, CNRS, Marne-la-Vallée, France
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">3D object and hand pose estimation have huge potentials for Augmented Reality, to enable tangible interfaces, natural interfaces, and blurring the boundaries between the real and virtual worlds. In this chapter, we present the recent developments for 3D object and hand pose estimation using cameras, and discuss their abilities and limitations and the possible future development of the field.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>3D Object and Hand Pose Estimation for Augmented Reality</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">An Augmented Reality system should not be limited to the visualization of
virtual elements integrated to the real world, it should also <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">perceive</span>
the user and the real world surrounding the user. As even early Augmented Reality
applications demonstrated, this is required to provide rich user experience, and
unlock new possibilities. For example, the magic book application developed by
Billinghurst and Kazo <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx7" title="" class="ltx_ref">Billinghurst <span class="ltx_text ltx_font_italic">et al.</span>, 2001</a>]</cite> and illustrated by
Figure <a href="#S1.F1" title="Figure 1 ‣ 1 3D Object and Hand Pose Estimation for Augmented Reality ‣ Recent Advances in 3D Object and Hand Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, shows the importance of being able to manipulate
real objects in Augmented Reality. It featured a real book, from which virtual
objects would pop up, and these virtual objects could be manipulated by the user
using a small real “shovel”.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">This early system relied on visual markers on the real book and the shovel.
These markers would become the core of the popular ARToolkit, and were essential
to robustly estimate the locations and orientations of objects in the 3D space,
a geometric information required for the proper integration of the virtual
objects with the real book, and their manipulation by the shovel.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<table id="S1.F1.2.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S1.F1.2.2.2" class="ltx_tr">
<td id="S1.F1.1.1.1.1" class="ltx_td ltx_align_center"><img src="/html/2006.05927/assets/images/magicbook_ar_view.png" id="S1.F1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="240" height="164" alt="Refer to caption"></td>
<td id="S1.F1.2.2.2.2" class="ltx_td ltx_align_center"><img src="/html/2006.05927/assets/images/magicbook_external_view.png" id="S1.F1.2.2.2.2.g1" class="ltx_graphics ltx_img_landscape" width="240" height="162" alt="Refer to caption"></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span> Augmented Reality (left) and external (right)
views of the early Magic book developed by Kazo and Billinghurst. This
visionary experiment shows the importance of real object manipulation in AR
applications.</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Visual markers, however, require a modification of the real world, which is not
always possible nor desirable in practice. Similarly, magnetic sensors have also
been used to perceive the spatial positions of real objects, in order to
integrate them to the augmented world. However, they share the same drawbacks
as visual markers.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Being able to perceive the user’s hands is also very important, as the hands can
act as an interface between the user’s intention and the virtual world. Haptic
gloves or various types of joysticks have been used to capture the hands’
locations and motions. In particular, input pads are still popular with current
augmented and virtual reality platforms, as they ensure robustness and accuracy
for the perception of the user’s actions. Entirely getting rid of such hardware
is extremely desirable, as it makes Augmented Reality user interfaces intuitive
and natural.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Researchers have thus been developing Computer Vision approaches to perceiving
the real world using simple cameras and without having to engineer the real
objects with markers or sensors, or the user’s hands with gloves or handles.
Such perception problem also appears in fields such as robotics, and the
scientific literature on this topic is extremely rich. The goal of this chapter
is to introduce the reader to 3D object and hand perception based on cameras for
Augmented Reality applications, and to the scientific literature on the topic,
with a focus on the most recent techniques.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">In the following, we will first detail the problem of 3D pose estimation,
explain why it is difficult, and review early approaches to motivate the use of
Machine Learning techniques. After a brief introduction to Deep Learning, we
will discuss the literature on 3D pose estimation for objects, hands, and hands
manipulating objects through representative works.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Formalization</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In the rest of this chapter, we will define the 3D pose of a rigid object as its
3D location and orientation in some coordinate system, as shown in
Figure <a href="#S2.F2" title="Figure 2 ‣ 2 Formalization ‣ Recent Advances in 3D Object and Hand Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. In the case of Augmented Reality, this coordinate system
should be directly related to the headset, or to the tablet or smartphone,
depending on the visualization system, <span id="S2.p1.1.1" class="ltx_text ltx_font_italic">and</span> to the object. This is thus
different from Simultaneous Localization and Mapping (SLAM), where the 3D pose
of the camera can be estimated in an arbitrary coordinate system as long as it
is consistent over time.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">In general, this 3D pose has thus 6 degrees of freedom: 3 for the 3D
translation, and 3 for the 3D orientation, which can be represented for example
with a 3D rotation matrix or a quaternion. Because of these 6 degrees of
freedom, the 3D pose is also called ’6D pose’ in the literature, the terms ’3D
pose’ and ’6D pose’ thus refer to the same notion.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Articulated objects, such as scissors, or even more deformable objects such as
clothes or sheets of papers have also been considered in the
literature <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx81" title="" class="ltx_ref">Salzmann and Fua, 2010</a>, <a href="#bib.bibx72" title="" class="ltx_ref">Pumarola <span class="ltx_text ltx_font_italic">et al.</span>, 2018</a>]</cite>. Their positions and shapes in space have
many more degrees of freedom, and specific representations of these poses have
been developed. Estimating these values from images robustly remains very
challenging.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">The 3D pose of the hand is also very complex, since we would like to also
consider the positions of the individual fingers. This is by contrast with
gesture recognition for example, which aims at assigning a distinctive label
such as ’open’ or ’close’ to a certain hand pose. Instead, we would like to
estimate continuous values in the 3D space. One option among others to
represent the 3D pose of a hand is to consider the 3D locations of each joint in
some coordinate system. Given a hand model with bone length and rotation angles
for all DoF of the joints, forward kinematics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx27" title="" class="ltx_ref">Gustus <span class="ltx_text ltx_font_italic">et al.</span>, 2012</a>]</cite> can be used to
calculate the 3D joint locations. Reciprocally, inverse
kinematics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx96" title="" class="ltx_ref">Tolani <span class="ltx_text ltx_font_italic">et al.</span>, 2000</a>]</cite> can be applied to obtain joint angles and bone length
from the 3D joint locations. In addition, one may also want to estimate the
shapes of the user’s hands, such as their sizes or the thickness of the fingers.</p>
</div>
<figure id="S2.F2" class="ltx_figure">
<table id="S2.F2.2.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.F2.2.2.2" class="ltx_tr">
<td id="S2.F2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r"><img src="/html/2006.05927/assets/images/object_pose.png" id="S2.F2.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="287" height="161" alt="Refer to caption"></td>
<td id="S2.F2.2.2.2.2" class="ltx_td ltx_align_center"><img src="/html/2006.05927/assets/images/hand_pose.png" id="S2.F2.2.2.2.2.g1" class="ltx_graphics ltx_img_landscape" width="287" height="162" alt="Refer to caption"></td>
</tr>
<tr id="S2.F2.2.2.3.1" class="ltx_tr">
<td id="S2.F2.2.2.3.1.1" class="ltx_td ltx_align_center ltx_border_r">(a)</td>
<td id="S2.F2.2.2.3.1.2" class="ltx_td ltx_align_center">(b)</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span> (a) The 3D pose of a rigid object can be defined as
a 3D rigid motion between a coordinate system attached to the object and
another coordinate system, for example one attached to an Augmented Reality
headset. (b) One way to define the 3D pose of a hand can be defined as a
rigid motion between a coordinate system attached to one of the joints (for
example the wrist) and another coordinate system, plus the 3D locations of
the joints in the first coordinate system. 3D rigid motions can be defined
as the composition of a 3D rotation and a 3D translation. </figcaption>
</figure>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">To be useful for Augmented Reality, 3D pose estimation has to run continuously,
in real-time. When using computer vision, it means that it should be done from
a single image, or stereo images captured at the same time, or RGB-D images that
provide both color and depth data, maybe also relying on the previous images to
guide or stabilize the 3D poses estimations. Cameras with large fields of view
help, as they limit the risk of the target objects to leave the field of view
compared to more standard cameras.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">Stereo camera rigs, made of two or more cameras, provide additional spatial
information that can be exploited for estimating the 3D pose, but make the
device more complex and more expensive. RGB-D cameras are currently a good
trade-off, as they also provide, in addition to a color image, 3D information in
the form of a depth map, <span id="S2.p6.1.1" class="ltx_text ltx_font_italic">i.e.</span>a depth value for each pixel, or at least most of
the pixels of the color image. “Structured light” RGB-D cameras measure depth
by projecting a known pattern in infrared light and capturing the projection
with an infrared camera. Depth can then be estimated from the deformation of
the pattern. “Time-of-flight” cameras are based on pulsed light sources and
measure the time a light pulse takes to travel from the emitter to the scene and
come back after reflection.</p>
</div>
<div id="S2.p7" class="ltx_para">
<p id="S2.p7.1" class="ltx_p">With structured-light technology, depth data can be missing at some image
locations, especially along the silhouettes of objects, or on dark or specular
regions. This technology also struggles to work outdoor, because of the ambient
infrared sunlight. Time-of-flight cameras is also disturbed outdoor, as the
high intensity of sunlight causes a quick saturation of the sensor pixels.
Multiple reflections produced by concave shapes can also affect the
time-of-flight.</p>
</div>
<div id="S2.p8" class="ltx_para">
<p id="S2.p8.1" class="ltx_p">But despite these drawbacks, RGB-D cameras remain however very useful in
practice when they can be used, and algorithms using depth maps perform much
better than algorithms relying only on color information—even though the gap
is decreasing in recent research.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Challenges of 3D Pose Estimation using Computer Vision</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">There are many challenges that need to be tackled in practice when estimating
the 3D poses of objects and hands from images. We list below some of these
challenges, illustrated in Figure <a href="#S3.F3" title="Figure 3 ‣ 3 Challenges of 3D Pose Estimation using Computer Vision ‣ Recent Advances in 3D Object and Hand Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S3.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S3.F3.3.3" class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.F3.3.3.3" class="ltx_tr">
<td id="S3.F3.1.1.1.1" class="ltx_td ltx_align_center"><span id="S3.F3.1.1.1.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;"><img src="/html/2006.05927/assets/images/hand_dof.png" id="S3.F3.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="110" height="120" alt="Refer to caption"></span></td>
<td id="S3.F3.2.2.2.2" class="ltx_td ltx_align_center"><img src="/html/2006.05927/assets/images/hand_object_occlusion.png" id="S3.F3.2.2.2.2.g1" class="ltx_graphics ltx_img_square" width="147" height="120" alt="Refer to caption"></td>
<td id="S3.F3.3.3.3.3" class="ltx_td ltx_align_center"><span id="S3.F3.3.3.3.3.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;"><img src="/html/2006.05927/assets/images/egocentric_hand.png" id="S3.F3.3.3.3.3.1.g1" class="ltx_graphics ltx_img_landscape" width="164" height="120" alt="Refer to caption"></span></td>
</tr>
<tr id="S3.F3.3.3.4.1" class="ltx_tr">
<td id="S3.F3.3.3.4.1.1" class="ltx_td ltx_align_center">(a)</td>
<td id="S3.F3.3.3.4.1.2" class="ltx_td ltx_align_center">(b)</td>
<td id="S3.F3.3.3.4.1.3" class="ltx_td ltx_align_center">(c)</td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S3.F3.7.7" class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.F3.7.7.4" class="ltx_tr">
<th id="S3.F3.5.5.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">
<img src="/html/2006.05927/assets/images/duck2.png" id="S3.F3.4.4.1.1.g1" class="ltx_graphics ltx_img_landscape" width="151" height="108" alt="Refer to caption">
<img src="/html/2006.05927/assets/images/duck1.png" id="S3.F3.5.5.2.2.g2" class="ltx_graphics ltx_img_landscape" width="156" height="108" alt="Refer to caption">
</th>
<th id="S3.F3.7.7.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">
<img src="/html/2006.05927/assets/images/ambiguity1.png" id="S3.F3.6.6.3.3.g1" class="ltx_graphics ltx_img_square" width="108" height="108" alt="Refer to caption">
<img src="/html/2006.05927/assets/images/ambiguity2.png" id="S3.F3.7.7.4.4.g2" class="ltx_graphics ltx_img_square" width="108" height="108" alt="Refer to caption">
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.F3.7.7.5.1" class="ltx_tr">
<td id="S3.F3.7.7.5.1.1" class="ltx_td ltx_align_center">(d)</td>
<td id="S3.F3.7.7.5.1.2" class="ltx_td ltx_align_center">(e)</td>
</tr>
</tbody>
</table>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span> Some challenges when estimating the 3D poses
of objects and hands. (a) Degrees of freedom. The human hand is highly
articulated, and many parameters have to be estimated to correctly represent
its pose. (b) Occlusions. In this case, the hand partially occludes the
object, and the object partially occludes the hand. (c) Self-occlusions can
also occur in the case of hands, especially in egocentric views. (d)
Illuminations. Like the rubber duck in these examples, the appearance of
objects can vary dramatically with illumination. (e) Ambiguities. Many
manufactured objects have symmetric or almost symmetric shapes, which may
make pose estimation ambiguous (object and images from the T-Less
dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx38" title="" class="ltx_ref">Hodan <span class="ltx_text ltx_font_italic">et al.</span>, 2016</a>]</cite>).</figcaption>
</figure>
<section id="S3.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">High degrees of freedom. </h4>

<div id="S3.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px1.p1.1" class="ltx_p">As mentioned above, the pose of a rigid
object can be represented with 6 scalar values, which is already a large number
of degrees of freedom to estimate. The 3D pose of a human hand lives in an even
much higher dimensional space, as it is often represented with about 32 degrees
of freedom. The risk of making an error when estimating the pose increases with
this number of degrees of freedom.</p>
</div>
</section>
<section id="S3.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Occlusions. </h4>

<div id="S3.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px2.p1.1" class="ltx_p">Occlusions of the target objects or hands, even
partial, often disturb pose estimation algorithms. It can be difficult to
identify which parts are visible and which parts are occluded, and the presence
of occlusions may make pose estimation completely fail, or be very inaccurate.
Occlusions often happen in practice. For example, when a hand manipulates an
object, both the hand and the object are usually partially occluded.</p>
</div>
</section>
<section id="S3.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Cluttered background. </h4>

<div id="S3.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px3.p1.1" class="ltx_p">Objects in the background can act as
distractors, especially if they look like target objects, or have similar parts.</p>
</div>
</section>
<section id="S3.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Changing illumination conditions.</h4>

<div id="S3.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px4.p1.1" class="ltx_p">In practice, illumination cannot
be controlled, and will change the appearance of the objects, not only because
the lighting is different but also because shadows can be cast on them. This
requires robust algorithms. Moreover, cameras may struggle to keep a good
balance between bright glares and dark areas, and using high dynamic range
cameras becomes appealing under such conditions. Moreover, sunlight may make
depth cameras fail as discussed above.</p>
</div>
</section>
<section id="S3.SS0.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Material and Textures.</h4>

<div id="S3.SS0.SSS0.Px5.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px5.p1.1" class="ltx_p">Early approaches to pose estimation relied on
the presence of texture or pattern on the objects’ surfaces, because stable
features can be detected and matched relatively easily and efficiently on
patterns. However, in practice, many objects lack textures, making such approach
fail. Non-Lambertian surfaces such as metal and glass make the appearance of
objects change with the objects’ poses because of specularities and reflections
appearing on the objects’ surfaces. Transparent objects are also of course
problematic, as they do not appear clearly in images.</p>
</div>
</section>
<section id="S3.SS0.SSS0.Px6" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Ambiguities.</h4>

<div id="S3.SS0.SSS0.Px6.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px6.p1.1" class="ltx_p">Many manufactured objects are symmetrical or almost
symmetrical, or have repetitive patterns. This generates possible ambiguities
when estimating the poses of these objects that need to be handled explicitly.</p>
</div>
</section>
<section id="S3.SS0.SSS0.Px7" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">3D Model Requirement. </h4>

<div id="S3.SS0.SSS0.Px7.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px7.p1.1" class="ltx_p">Most of the existing algorithms for pose
estimation assume the knowledge of some 3D models of the target objects. Such
3D models provide very useful geometric constraints, which can be exploited to
estimate the objects’ 3D poses. However, building such 3D models takes time and
expertise, and in practice, a 3D model is not readily available. Hand pose
estimation suffers much less from this problem, since the 3D geometry of hands
remains very similar from one person to another. Some recent works have also
considered the estimation of the objects’ geometry together with their
poses <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx24" title="" class="ltx_ref">Grabner <span class="ltx_text ltx_font_italic">et al.</span>, 2018</a>]</cite>. These works are still limited to a few object
categories, such as chairs or cars, but are very interesting for future
developments.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Early Approaches to 3D Pose Estimation and Their Limits</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">3D pose estimation from images has a long history in computer vision. Early
methods were based on simple image features, such as edges or corners. Most
importantly, they strongly relied on some prior knowledge about the 3D pose, to
guide the pose estimation in the high-dimensional space of possible poses. This
prior may come from the poses estimated for the previous images, or could be
provided by a human operator.</p>
</div>
<section id="S4.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">3D tracking methods. </h4>

<div id="S4.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p1.1" class="ltx_p">For example, pioneer works such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx29" title="" class="ltx_ref">Harris and Stennett, 1990</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx53" title="" class="ltx_ref">Lowe, 1991</a>]</cite> describe
the object of interest as a set of 3D geometric primitives such as lines and
conics, which were matched with contours in the image to find a 3D pose estimate
by solving an optimization problem to find the 3D pose that reprojects the 3D
geometric primitives to the matched image contours. The entire process is
computationally light, and careful implementations were able to achieve high
frame rates with computers that would appear primitive to us.</p>
</div>
<div id="S4.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p2.1" class="ltx_p">Unfortunately, edge-based approaches are quite unreliable in practice. Matching
the reprojection of the 3D primitives with image contours is difficult to
achieve: 3D primitives do not necessarily appear as strong image contours,
except for carefully chosen objects. As a result, these primitives are likely to
be matched with the wrong image contours, especially in case of cluttered
background. Incorrect matches will also occur in case of partial occlusions.
Introducing robust estimators into the optimization problem <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">Drummond and Cipolla, 2002</a>]</cite>
helps, but it appears that it is impossible in general to be robust to such
mismatches in edge-based pose estimation: Most of the time, a match between two
contours provides only limited constraints on the pose parameters, as the two
contours can “slide” along each other and still be a good match. Contours thus
do not provide reliable constraints for pose estimation.</p>
</div>
<div id="S4.SS0.SSS0.Px1.p3" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p3.1" class="ltx_p">Relying on feature points <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx30" title="" class="ltx_ref">Harris and Stephens, 1988</a>]</cite> instead of contours provides more
constraints as point-to-point matching does not have the “sliding ambiguity”
of contour matching. Feature point matching has been used for 3D object tracking
with some success, for example in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx102" title="" class="ltx_ref">Vacchetti <span class="ltx_text ltx_font_italic">et al.</span>, 2004</a>]</cite>. However, such approach
assumes the presence of feature points that can be detected on the object’s
surface, which is not true for all the objects.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">The importance of detection methods. </h4>

<div id="S4.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px2.p1.5" class="ltx_p">The two approaches described above assume that prior knowledge on the object
pose is available to guide the matching process between contours or points.
Typically, the object pose estimated at time <math id="S4.SS0.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S4.SS0.SSS0.Px2.p1.1.m1.1a"><mi id="S4.SS0.SSS0.Px2.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.1.m1.1b"><ci id="S4.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.1.m1.1c">t</annotation></semantics></math> is exploited to estimate the
pose at time <math id="S4.SS0.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="t+1" display="inline"><semantics id="S4.SS0.SSS0.Px2.p1.2.m2.1a"><mrow id="S4.SS0.SSS0.Px2.p1.2.m2.1.1" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1.cmml"><mi id="S4.SS0.SSS0.Px2.p1.2.m2.1.1.2" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1.2.cmml">t</mi><mo id="S4.SS0.SSS0.Px2.p1.2.m2.1.1.1" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1.1.cmml">+</mo><mn id="S4.SS0.SSS0.Px2.p1.2.m2.1.1.3" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.2.m2.1b"><apply id="S4.SS0.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1"><plus id="S4.SS0.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1.1"></plus><ci id="S4.SS0.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1.2">𝑡</ci><cn type="integer" id="S4.SS0.SSS0.Px2.p1.2.m2.1.1.3.cmml" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.2.m2.1c">t+1</annotation></semantics></math>. In practice, this makes such approaches fragile, because the
poses at time <math id="S4.SS0.SSS0.Px2.p1.3.m3.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S4.SS0.SSS0.Px2.p1.3.m3.1a"><mi id="S4.SS0.SSS0.Px2.p1.3.m3.1.1" xref="S4.SS0.SSS0.Px2.p1.3.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.3.m3.1b"><ci id="S4.SS0.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.3.m3.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.3.m3.1c">t</annotation></semantics></math> and <math id="S4.SS0.SSS0.Px2.p1.4.m4.1" class="ltx_Math" alttext="t+1" display="inline"><semantics id="S4.SS0.SSS0.Px2.p1.4.m4.1a"><mrow id="S4.SS0.SSS0.Px2.p1.4.m4.1.1" xref="S4.SS0.SSS0.Px2.p1.4.m4.1.1.cmml"><mi id="S4.SS0.SSS0.Px2.p1.4.m4.1.1.2" xref="S4.SS0.SSS0.Px2.p1.4.m4.1.1.2.cmml">t</mi><mo id="S4.SS0.SSS0.Px2.p1.4.m4.1.1.1" xref="S4.SS0.SSS0.Px2.p1.4.m4.1.1.1.cmml">+</mo><mn id="S4.SS0.SSS0.Px2.p1.4.m4.1.1.3" xref="S4.SS0.SSS0.Px2.p1.4.m4.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.4.m4.1b"><apply id="S4.SS0.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.4.m4.1.1"><plus id="S4.SS0.SSS0.Px2.p1.4.m4.1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.4.m4.1.1.1"></plus><ci id="S4.SS0.SSS0.Px2.p1.4.m4.1.1.2.cmml" xref="S4.SS0.SSS0.Px2.p1.4.m4.1.1.2">𝑡</ci><cn type="integer" id="S4.SS0.SSS0.Px2.p1.4.m4.1.1.3.cmml" xref="S4.SS0.SSS0.Px2.p1.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.4.m4.1c">t+1</annotation></semantics></math> can be very different if the object moves fast, or
because the pose estimated at time <math id="S4.SS0.SSS0.Px2.p1.5.m5.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S4.SS0.SSS0.Px2.p1.5.m5.1a"><mi id="S4.SS0.SSS0.Px2.p1.5.m5.1.1" xref="S4.SS0.SSS0.Px2.p1.5.m5.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.5.m5.1b"><ci id="S4.SS0.SSS0.Px2.p1.5.m5.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.5.m5.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.5.m5.1c">t</annotation></semantics></math> can be wrong.</p>
</div>
<div id="S4.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S4.SS0.SSS0.Px2.p2.1" class="ltx_p">Being able to estimate the 3D pose of an object without relying too much on
prior knowledge is therefore very important in practice. As we will see, this
does not mean that methods based on strong prior knowledge on the pose are not
useful. In fact, they tend to be much faster and/or more accurate than
“detection methods”, which are more robust. A natural solution is thus to
combine both, and this is still true in the modern era of object and pose
estimation.</p>
</div>
<div id="S4.SS0.SSS0.Px2.p3" class="ltx_para">
<p id="S4.SS0.SSS0.Px2.p3.1" class="ltx_p">One early popular method for object pose estimation without pose prior was also
based on feature points, often referred to as keypoints in this context. This
requires the ability to match keypoints between an input image, and a reference
image of the target object, which is captured offline and in which the 3D pose
and the 3D shape of the object is known. By using geometry constraints, it is
then possible to estimate the object’s 3D pose. However, wide baseline point
matching is much more difficult than short baseline matching used by tracking
methods. SIFT keypoints and descriptors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx54" title="" class="ltx_ref">Lowe, 2001</a>, <a href="#bib.bibx55" title="" class="ltx_ref">Lowe, 2004</a>]</cite> were a
breakthrough that made many computer vision applications possible, including 3D
pose estimation. They were followed by faster methods, including
SURF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">Bay <span class="ltx_text ltx_font_italic">et al.</span>, 2008</a>]</cite> and ORB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx79" title="" class="ltx_ref">Rublee <span class="ltx_text ltx_font_italic">et al.</span>, 2011</a>]</cite>.</p>
</div>
<div id="S4.SS0.SSS0.Px2.p4" class="ltx_para">
<p id="S4.SS0.SSS0.Px2.p4.1" class="ltx_p">As for tracking methods based on feature points, the limitation of
keypoint-based detection and pose estimation is that it is limited to objects
exhibiting enough keypoints, which is not the case in general. This approach was
still very successful for Augmented Reality in magazines for example, where the
“object” is an image printed on paper—so it has a simple planar
geometry—and selected to guarantee that the approach will work well by
exhibiting enough keypoints <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx45" title="" class="ltx_ref">Kim <span class="ltx_text ltx_font_italic">et al.</span>, 2010</a>]</cite>.</p>
</div>
<div id="S4.SS0.SSS0.Px2.p5" class="ltx_para">
<p id="S4.SS0.SSS0.Px2.p5.1" class="ltx_p">To be able to detect and estimate the 3D pose of objects with almost no
keypoints, sometimes referred to as “texture-less” objects, some works
attempted to use “templates”: The templates of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx36" title="" class="ltx_ref">Hinterstoisser <span class="ltx_text ltx_font_italic">et al.</span>, 2012b</a>]</cite> aim at
capturing the possible appearances of the object by discretizing the 3D pose
space, and representing the object’s appearance for each discretized pose by a
template covering the full object, in a way that is robust to lighting
variations. Then, by scanning the input images looking for templates, the target
object can be detected in 2D and its pose estimated based on the template that
matches best the object appearance. However, such approach requires the
creation of many templates, and is poorly robust to occlusions.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Conclusion. </h4>

<div id="S4.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px3.p1.1" class="ltx_p">We focused in this section on object pose estimation rather than hand pose
estimation, however the conclusion would be the same. Early approaches were
based on handcrafted methods to extract features from images, with the goal of
estimating the 3D pose from these features. This is however very challenging to
do, and almost doomed to fail in the general case. Since then, Machine
Learning-based methods have been shown to be more adapted, even if they also
come with their drawbacks, and will be discussed in the rest of this chapter.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Machine Learning and Deep Learning</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Fundamentally, 3D pose estimation of objects or hands can be seen as a mapping
from an image to a representation of the pose. The input space of this mapping
is thus the space of possible images, which is an incredibly large space: For
example, a RGB VGA image is made of almost 1 million values of pixel values.
Not many fields deal with 1 million dimension data! The output space is much
smaller, since it is made of 6 values for the pose of a rigid object, or a few
tens for the pose of a hand. The natures of the input and output spaces of the
mapping sought in pose estimation are therefore very different, which makes this
mapping very complex. From this point of view, we can understand that it is
very difficult to hope for a pure “algorithmic” approach to code this mapping.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">This is why Machine Learning techniques, which use data to improve algorithms,
have become successful for pose estimation problems, and computer vision
problems in general. Because they are data-driven, they can find automatically
an appropriate mapping, by contrast with previous approaches that required
hardcoding mostly based on intuition, which can be correct or wrong.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Many Machine Learning methods exist, and Random Forests, also called Random
Decision Forests or Randomized Trees, were an early popular method in the
context of 3D pose estimation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx48" title="" class="ltx_ref">Lepetit <span class="ltx_text ltx_font_italic">et al.</span>, 2005</a>, <a href="#bib.bibx84" title="" class="ltx_ref">Shotton <span class="ltx_text ltx_font_italic">et al.</span>, 2013</a>, <a href="#bib.bibx9" title="" class="ltx_ref">Brachmann <span class="ltx_text ltx_font_italic">et al.</span>, 2014</a>]</cite>. Random
Forests can be efficiently applied to image patches and discrete (multi-class)
and continuous (regression) problems, which makes them flexible and suitable to
fast, possibly real-time, applications.</p>
</div>
<section id="S5.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Deep Learning. </h4>

<div id="S5.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px1.p1.1" class="ltx_p">For multiple reasons, Deep Learning, a Machine Learning technique, took over the
other methods almost entirely during the last years in many scientific fields,
including 3D pose estimation. It is very flexible, and in fact, it has been
known for a long time that any continuous mapping can be approximated by
two-layer networks, as finely as wanted <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx40" title="" class="ltx_ref">Hornik <span class="ltx_text ltx_font_italic">et al.</span>, 1989</a>, <a href="#bib.bibx70" title="" class="ltx_ref">Pinkus, 1999</a>]</cite>. In practice,
networks with more than two layers tend to generalize better, and to need
dramatically less parameters than two-layer networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx17" title="" class="ltx_ref">Eldan and Shamir, 2016</a>]</cite>, which makes
them a tool of choice for computer vision problems.</p>
</div>
<div id="S5.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S5.SS0.SSS0.Px1.p2.1" class="ltx_p">Many resources can now be easily found to learn the fundamentals of Deep
Learning, for example <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx22" title="" class="ltx_ref">Goodfellow <span class="ltx_text ltx_font_italic">et al.</span>, 2016</a>]</cite>. To stay brief, we can say here that
a Deep Network can be defined as a composition of functions (“layers”). These
functions may depend on parameters, which need to be estimated for the network
to perform well. Almost any function can be used as layer, as long as it is
useful to solve the problem at hand, and if it is <em id="S5.SS0.SSS0.Px1.p2.1.1" class="ltx_emph ltx_font_italic">differentiable</em>. This
differentiable property is indeed required to find good values for the
parameters by solving an optimization problem.</p>
</div>
<figure id="S5.F4" class="ltx_figure"><img src="/html/2006.05927/assets/images/basic_dn.png" id="S5.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="169" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span> A basic Deep Network architecture applied to
hand pose prediction. <span id="S5.F4.10.1" class="ltx_text ltx_font_sansserif">C1</span>, <span id="S5.F4.11.2" class="ltx_text ltx_font_sansserif">C2</span>, and <span id="S5.F4.12.3" class="ltx_text ltx_font_sansserif">C3</span> are
convolutional layers, <span id="S5.F4.13.4" class="ltx_text ltx_font_sansserif">P1</span> and <span id="S5.F4.14.5" class="ltx_text ltx_font_sansserif">P2</span> are pooling layers, and
<span id="S5.F4.15.6" class="ltx_text ltx_font_sansserif">FC1</span>, <span id="S5.F4.16.7" class="ltx_text ltx_font_sansserif">FC2</span>, and <span id="S5.F4.17.8" class="ltx_text ltx_font_sansserif">FC3</span> are fully connected layers. The
numbers in each bar indicates either: The number and size of the linear
filters in case of the convolutional layers, the size of the pooling region
for the pooling layers, and the size of the output vector in case of the
fully connected layers.</figcaption>
</figure>
</section>
<section id="S5.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Deep Network Training.</h4>

<div id="S5.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px2.p1.2" class="ltx_p">For example, if we want to make a network <math id="S5.SS0.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="F" display="inline"><semantics id="S5.SS0.SSS0.Px2.p1.1.m1.1a"><mi id="S5.SS0.SSS0.Px2.p1.1.m1.1.1" xref="S5.SS0.SSS0.Px2.p1.1.m1.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.1.m1.1b"><ci id="S5.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.1.m1.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.1.m1.1c">F</annotation></semantics></math> predict the pose of a hand visible
in an image, one way to find good values <math id="S5.SS0.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="\hat{\Theta}" display="inline"><semantics id="S5.SS0.SSS0.Px2.p1.2.m2.1a"><mover accent="true" id="S5.SS0.SSS0.Px2.p1.2.m2.1.1" xref="S5.SS0.SSS0.Px2.p1.2.m2.1.1.cmml"><mi mathvariant="normal" id="S5.SS0.SSS0.Px2.p1.2.m2.1.1.2" xref="S5.SS0.SSS0.Px2.p1.2.m2.1.1.2.cmml">Θ</mi><mo id="S5.SS0.SSS0.Px2.p1.2.m2.1.1.1" xref="S5.SS0.SSS0.Px2.p1.2.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.2.m2.1b"><apply id="S5.SS0.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.2.m2.1.1"><ci id="S5.SS0.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.2.m2.1.1.1">^</ci><ci id="S5.SS0.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S5.SS0.SSS0.Px2.p1.2.m2.1.1.2">Θ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.2.m2.1c">\hat{\Theta}</annotation></semantics></math> for the network
parameters is to solve the following optimization problem:</p>
<table id="S5.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.E1.m1.4" class="ltx_Math" alttext="\begin{array}[]{l}\hat{\Theta}=\arg\min_{\Theta}\mathcal{L}(\Theta)\&gt;\text{ with}\\[5.69046pt]
\mathcal{L}(\Theta)=\frac{1}{N}\sum\limits_{i=1}^{N}\|F(I_{i};\Theta)-{\bf e}_{i}\|^{2}\&gt;,\end{array}" display="block"><semantics id="S5.E1.m1.4a"><mtable displaystyle="true" rowspacing="0pt" id="S5.E1.m1.4.4" xref="S5.E1.m1.4.4.cmml"><mtr id="S5.E1.m1.4.4a" xref="S5.E1.m1.4.4.cmml"><mtd class="ltx_align_left" columnalign="left" id="S5.E1.m1.4.4b" xref="S5.E1.m1.4.4.cmml"><mrow id="S5.E1.m1.1.1.1.1.1" xref="S5.E1.m1.1.1.1.1.1.cmml"><mover accent="true" id="S5.E1.m1.1.1.1.1.1.3" xref="S5.E1.m1.1.1.1.1.1.3.cmml"><mi mathvariant="normal" id="S5.E1.m1.1.1.1.1.1.3.2" xref="S5.E1.m1.1.1.1.1.1.3.2.cmml">Θ</mi><mo id="S5.E1.m1.1.1.1.1.1.3.1" xref="S5.E1.m1.1.1.1.1.1.3.1.cmml">^</mo></mover><mo id="S5.E1.m1.1.1.1.1.1.2" xref="S5.E1.m1.1.1.1.1.1.2.cmml">=</mo><mrow id="S5.E1.m1.1.1.1.1.1.4" xref="S5.E1.m1.1.1.1.1.1.4.cmml"><mrow id="S5.E1.m1.1.1.1.1.1.4.2" xref="S5.E1.m1.1.1.1.1.1.4.2.cmml"><mi id="S5.E1.m1.1.1.1.1.1.4.2.1" xref="S5.E1.m1.1.1.1.1.1.4.2.1.cmml">arg</mi><mo lspace="0.167em" id="S5.E1.m1.1.1.1.1.1.4.2a" xref="S5.E1.m1.1.1.1.1.1.4.2.cmml">⁡</mo><mrow id="S5.E1.m1.1.1.1.1.1.4.2.2" xref="S5.E1.m1.1.1.1.1.1.4.2.2.cmml"><msub id="S5.E1.m1.1.1.1.1.1.4.2.2.1" xref="S5.E1.m1.1.1.1.1.1.4.2.2.1.cmml"><mi id="S5.E1.m1.1.1.1.1.1.4.2.2.1.2" xref="S5.E1.m1.1.1.1.1.1.4.2.2.1.2.cmml">min</mi><mi mathvariant="normal" id="S5.E1.m1.1.1.1.1.1.4.2.2.1.3" xref="S5.E1.m1.1.1.1.1.1.4.2.2.1.3.cmml">Θ</mi></msub><mo lspace="0.167em" id="S5.E1.m1.1.1.1.1.1.4.2.2a" xref="S5.E1.m1.1.1.1.1.1.4.2.2.cmml">⁡</mo><mi class="ltx_font_mathcaligraphic" id="S5.E1.m1.1.1.1.1.1.4.2.2.2" xref="S5.E1.m1.1.1.1.1.1.4.2.2.2.cmml">ℒ</mi></mrow></mrow><mo lspace="0em" rspace="0em" id="S5.E1.m1.1.1.1.1.1.4.1" xref="S5.E1.m1.1.1.1.1.1.4.1.cmml">​</mo><mrow id="S5.E1.m1.1.1.1.1.1.4.3.2" xref="S5.E1.m1.1.1.1.1.1.4.cmml"><mo stretchy="false" id="S5.E1.m1.1.1.1.1.1.4.3.2.1" xref="S5.E1.m1.1.1.1.1.1.4.cmml">(</mo><mi mathvariant="normal" id="S5.E1.m1.1.1.1.1.1.1" xref="S5.E1.m1.1.1.1.1.1.1.cmml">Θ</mi><mo stretchy="false" id="S5.E1.m1.1.1.1.1.1.4.3.2.2" xref="S5.E1.m1.1.1.1.1.1.4.cmml">)</mo></mrow><mo lspace="0.220em" rspace="0em" id="S5.E1.m1.1.1.1.1.1.4.1a" xref="S5.E1.m1.1.1.1.1.1.4.1.cmml">​</mo><mtext id="S5.E1.m1.1.1.1.1.1.4.4" xref="S5.E1.m1.1.1.1.1.1.4.4a.cmml"> with</mtext></mrow></mrow></mtd></mtr><mtr id="S5.E1.m1.4.4c" xref="S5.E1.m1.4.4.cmml"><mtd class="ltx_align_left" columnalign="left" id="S5.E1.m1.4.4d" xref="S5.E1.m1.4.4.cmml"><mrow id="S5.E1.m1.4.4.4.3.3.3" xref="S5.E1.m1.4.4.4.3.3.3.1.cmml"><mrow id="S5.E1.m1.4.4.4.3.3.3.1" xref="S5.E1.m1.4.4.4.3.3.3.1.cmml"><mrow id="S5.E1.m1.4.4.4.3.3.3.1.3" xref="S5.E1.m1.4.4.4.3.3.3.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.E1.m1.4.4.4.3.3.3.1.3.2" xref="S5.E1.m1.4.4.4.3.3.3.1.3.2.cmml">ℒ</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.4.4.4.3.3.3.1.3.1" xref="S5.E1.m1.4.4.4.3.3.3.1.3.1.cmml">​</mo><mrow id="S5.E1.m1.4.4.4.3.3.3.1.3.3.2" xref="S5.E1.m1.4.4.4.3.3.3.1.3.cmml"><mo stretchy="false" id="S5.E1.m1.4.4.4.3.3.3.1.3.3.2.1" xref="S5.E1.m1.4.4.4.3.3.3.1.3.cmml">(</mo><mi mathvariant="normal" id="S5.E1.m1.2.2.2.1.1.1" xref="S5.E1.m1.2.2.2.1.1.1.cmml">Θ</mi><mo stretchy="false" id="S5.E1.m1.4.4.4.3.3.3.1.3.3.2.2" xref="S5.E1.m1.4.4.4.3.3.3.1.3.cmml">)</mo></mrow></mrow><mo id="S5.E1.m1.4.4.4.3.3.3.1.2" xref="S5.E1.m1.4.4.4.3.3.3.1.2.cmml">=</mo><mrow id="S5.E1.m1.4.4.4.3.3.3.1.1" xref="S5.E1.m1.4.4.4.3.3.3.1.1.cmml"><mstyle displaystyle="false" id="S5.E1.m1.4.4.4.3.3.3.1.1.3" xref="S5.E1.m1.4.4.4.3.3.3.1.1.3.cmml"><mfrac id="S5.E1.m1.4.4.4.3.3.3.1.1.3a" xref="S5.E1.m1.4.4.4.3.3.3.1.1.3.cmml"><mn id="S5.E1.m1.4.4.4.3.3.3.1.1.3.2" xref="S5.E1.m1.4.4.4.3.3.3.1.1.3.2.cmml">1</mn><mi id="S5.E1.m1.4.4.4.3.3.3.1.1.3.3" xref="S5.E1.m1.4.4.4.3.3.3.1.1.3.3.cmml">N</mi></mfrac></mstyle><mo lspace="0em" rspace="0em" id="S5.E1.m1.4.4.4.3.3.3.1.1.2" xref="S5.E1.m1.4.4.4.3.3.3.1.1.2.cmml">​</mo><mrow id="S5.E1.m1.4.4.4.3.3.3.1.1.1" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.cmml"><mstyle displaystyle="false" id="S5.E1.m1.4.4.4.3.3.3.1.1.1.2" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.2.cmml"><munderover id="S5.E1.m1.4.4.4.3.3.3.1.1.1.2a" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.2.cmml"><mo movablelimits="false" id="S5.E1.m1.4.4.4.3.3.3.1.1.1.2.2.2" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.2.2.2.cmml">∑</mo><mrow id="S5.E1.m1.4.4.4.3.3.3.1.1.1.2.2.3" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.2.2.3.cmml"><mi id="S5.E1.m1.4.4.4.3.3.3.1.1.1.2.2.3.2" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.2.2.3.2.cmml">i</mi><mo id="S5.E1.m1.4.4.4.3.3.3.1.1.1.2.2.3.1" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.2.2.3.1.cmml">=</mo><mn id="S5.E1.m1.4.4.4.3.3.3.1.1.1.2.2.3.3" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S5.E1.m1.4.4.4.3.3.3.1.1.1.2.3" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.2.3.cmml">N</mi></munderover></mstyle><msup id="S5.E1.m1.4.4.4.3.3.3.1.1.1.1" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.cmml"><mrow id="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.2" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.cmml"><mrow id="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.1" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.1.cmml"><mi id="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.1.3" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.1.3.cmml">F</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.1.2" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.1.1.1" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.1.1.1.2" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.1.1.2.cmml">(</mo><msub id="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.1.1.1.1" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.1.1.1.1.2" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml">I</mi><mi id="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.1.1.1.1.3" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.1.1.1.3" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.1.1.2.cmml">;</mo><mi mathvariant="normal" id="S5.E1.m1.3.3.3.2.2.2" xref="S5.E1.m1.3.3.3.2.2.2.cmml">Θ</mi><mo stretchy="false" id="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.1.1.1.4" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.2" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.2.cmml">−</mo><msub id="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.3" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.3.cmml"><mi id="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.3.2" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.3.2.cmml">𝐞</mi><mi id="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.3.3" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo stretchy="false" id="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.3" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.3" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.3.cmml">2</mn></msup></mrow></mrow></mrow><mo id="S5.E1.m1.4.4.4.3.3.3.2" xref="S5.E1.m1.4.4.4.3.3.3.1.cmml">,</mo></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S5.E1.m1.4b"><matrix id="S5.E1.m1.4.4.cmml" xref="S5.E1.m1.4.4"><matrixrow id="S5.E1.m1.4.4a.cmml" xref="S5.E1.m1.4.4"><apply id="S5.E1.m1.1.1.1.1.1.cmml" xref="S5.E1.m1.1.1.1.1.1"><eq id="S5.E1.m1.1.1.1.1.1.2.cmml" xref="S5.E1.m1.1.1.1.1.1.2"></eq><apply id="S5.E1.m1.1.1.1.1.1.3.cmml" xref="S5.E1.m1.1.1.1.1.1.3"><ci id="S5.E1.m1.1.1.1.1.1.3.1.cmml" xref="S5.E1.m1.1.1.1.1.1.3.1">^</ci><ci id="S5.E1.m1.1.1.1.1.1.3.2.cmml" xref="S5.E1.m1.1.1.1.1.1.3.2">Θ</ci></apply><apply id="S5.E1.m1.1.1.1.1.1.4.cmml" xref="S5.E1.m1.1.1.1.1.1.4"><times id="S5.E1.m1.1.1.1.1.1.4.1.cmml" xref="S5.E1.m1.1.1.1.1.1.4.1"></times><apply id="S5.E1.m1.1.1.1.1.1.4.2.cmml" xref="S5.E1.m1.1.1.1.1.1.4.2"><arg id="S5.E1.m1.1.1.1.1.1.4.2.1.cmml" xref="S5.E1.m1.1.1.1.1.1.4.2.1"></arg><apply id="S5.E1.m1.1.1.1.1.1.4.2.2.cmml" xref="S5.E1.m1.1.1.1.1.1.4.2.2"><apply id="S5.E1.m1.1.1.1.1.1.4.2.2.1.cmml" xref="S5.E1.m1.1.1.1.1.1.4.2.2.1"><csymbol cd="ambiguous" id="S5.E1.m1.1.1.1.1.1.4.2.2.1.1.cmml" xref="S5.E1.m1.1.1.1.1.1.4.2.2.1">subscript</csymbol><min id="S5.E1.m1.1.1.1.1.1.4.2.2.1.2.cmml" xref="S5.E1.m1.1.1.1.1.1.4.2.2.1.2"></min><ci id="S5.E1.m1.1.1.1.1.1.4.2.2.1.3.cmml" xref="S5.E1.m1.1.1.1.1.1.4.2.2.1.3">Θ</ci></apply><ci id="S5.E1.m1.1.1.1.1.1.4.2.2.2.cmml" xref="S5.E1.m1.1.1.1.1.1.4.2.2.2">ℒ</ci></apply></apply><ci id="S5.E1.m1.1.1.1.1.1.1.cmml" xref="S5.E1.m1.1.1.1.1.1.1">Θ</ci><ci id="S5.E1.m1.1.1.1.1.1.4.4a.cmml" xref="S5.E1.m1.1.1.1.1.1.4.4"><mtext id="S5.E1.m1.1.1.1.1.1.4.4.cmml" xref="S5.E1.m1.1.1.1.1.1.4.4"> with</mtext></ci></apply></apply></matrixrow><matrixrow id="S5.E1.m1.4.4b.cmml" xref="S5.E1.m1.4.4"><apply id="S5.E1.m1.4.4.4.3.3.3.1.cmml" xref="S5.E1.m1.4.4.4.3.3.3"><eq id="S5.E1.m1.4.4.4.3.3.3.1.2.cmml" xref="S5.E1.m1.4.4.4.3.3.3.1.2"></eq><apply id="S5.E1.m1.4.4.4.3.3.3.1.3.cmml" xref="S5.E1.m1.4.4.4.3.3.3.1.3"><times id="S5.E1.m1.4.4.4.3.3.3.1.3.1.cmml" xref="S5.E1.m1.4.4.4.3.3.3.1.3.1"></times><ci id="S5.E1.m1.4.4.4.3.3.3.1.3.2.cmml" xref="S5.E1.m1.4.4.4.3.3.3.1.3.2">ℒ</ci><ci id="S5.E1.m1.2.2.2.1.1.1.cmml" xref="S5.E1.m1.2.2.2.1.1.1">Θ</ci></apply><apply id="S5.E1.m1.4.4.4.3.3.3.1.1.cmml" xref="S5.E1.m1.4.4.4.3.3.3.1.1"><times id="S5.E1.m1.4.4.4.3.3.3.1.1.2.cmml" xref="S5.E1.m1.4.4.4.3.3.3.1.1.2"></times><apply id="S5.E1.m1.4.4.4.3.3.3.1.1.3.cmml" xref="S5.E1.m1.4.4.4.3.3.3.1.1.3"><divide id="S5.E1.m1.4.4.4.3.3.3.1.1.3.1.cmml" xref="S5.E1.m1.4.4.4.3.3.3.1.1.3"></divide><cn type="integer" id="S5.E1.m1.4.4.4.3.3.3.1.1.3.2.cmml" xref="S5.E1.m1.4.4.4.3.3.3.1.1.3.2">1</cn><ci id="S5.E1.m1.4.4.4.3.3.3.1.1.3.3.cmml" xref="S5.E1.m1.4.4.4.3.3.3.1.1.3.3">𝑁</ci></apply><apply id="S5.E1.m1.4.4.4.3.3.3.1.1.1.cmml" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1"><apply id="S5.E1.m1.4.4.4.3.3.3.1.1.1.2.cmml" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.2"><csymbol cd="ambiguous" id="S5.E1.m1.4.4.4.3.3.3.1.1.1.2.1.cmml" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.2">superscript</csymbol><apply id="S5.E1.m1.4.4.4.3.3.3.1.1.1.2.2.cmml" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.2"><csymbol cd="ambiguous" id="S5.E1.m1.4.4.4.3.3.3.1.1.1.2.2.1.cmml" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.2">subscript</csymbol><sum id="S5.E1.m1.4.4.4.3.3.3.1.1.1.2.2.2.cmml" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.2.2.2"></sum><apply id="S5.E1.m1.4.4.4.3.3.3.1.1.1.2.2.3.cmml" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.2.2.3"><eq id="S5.E1.m1.4.4.4.3.3.3.1.1.1.2.2.3.1.cmml" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.2.2.3.1"></eq><ci id="S5.E1.m1.4.4.4.3.3.3.1.1.1.2.2.3.2.cmml" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.2.2.3.2">𝑖</ci><cn type="integer" id="S5.E1.m1.4.4.4.3.3.3.1.1.1.2.2.3.3.cmml" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S5.E1.m1.4.4.4.3.3.3.1.1.1.2.3.cmml" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.2.3">𝑁</ci></apply><apply id="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.cmml" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.2.cmml" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.1">superscript</csymbol><apply id="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.2.cmml" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1"><csymbol cd="latexml" id="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.2.1.cmml" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.2">norm</csymbol><apply id="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.cmml" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1"><minus id="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.2.cmml" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.2"></minus><apply id="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.1.cmml" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.1"><times id="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.1.2.cmml" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.1.2"></times><ci id="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.1.3.cmml" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.1.3">𝐹</ci><list id="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.1.1.2.cmml" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.1.1.1"><apply id="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.1.1.1.1.2">𝐼</ci><ci id="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><ci id="S5.E1.m1.3.3.3.2.2.2.cmml" xref="S5.E1.m1.3.3.3.2.2.2">Θ</ci></list></apply><apply id="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.3.cmml" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.3.1.cmml" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.3.2.cmml" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.3.2">𝐞</ci><ci id="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.3.3.cmml" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply></apply><cn type="integer" id="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.3.cmml" xref="S5.E1.m1.4.4.4.3.3.3.1.1.1.1.3">2</cn></apply></apply></apply></apply></matrixrow></matrix></annotation-xml><annotation encoding="application/x-tex" id="S5.E1.m1.4c">\begin{array}[]{l}\hat{\Theta}=\arg\min_{\Theta}\mathcal{L}(\Theta)\&gt;\text{ with}\\[5.69046pt]
\mathcal{L}(\Theta)=\frac{1}{N}\sum\limits_{i=1}^{N}\|F(I_{i};\Theta)-{\bf e}_{i}\|^{2}\&gt;,\end{array}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S5.SS0.SSS0.Px2.p1.8" class="ltx_p">where <math id="S5.SS0.SSS0.Px2.p1.3.m1.1" class="ltx_math_unparsed" alttext="\{(I_{1},{\bf e}_{1}),..,(I_{N},{\bf e}_{N})\}" display="inline"><semantics id="S5.SS0.SSS0.Px2.p1.3.m1.1a"><mrow id="S5.SS0.SSS0.Px2.p1.3.m1.1b"><mo stretchy="false" id="S5.SS0.SSS0.Px2.p1.3.m1.1.1">{</mo><mrow id="S5.SS0.SSS0.Px2.p1.3.m1.1.2"><mo stretchy="false" id="S5.SS0.SSS0.Px2.p1.3.m1.1.2.1">(</mo><msub id="S5.SS0.SSS0.Px2.p1.3.m1.1.2.2"><mi id="S5.SS0.SSS0.Px2.p1.3.m1.1.2.2.2">I</mi><mn id="S5.SS0.SSS0.Px2.p1.3.m1.1.2.2.3">1</mn></msub><mo id="S5.SS0.SSS0.Px2.p1.3.m1.1.2.3">,</mo><msub id="S5.SS0.SSS0.Px2.p1.3.m1.1.2.4"><mi id="S5.SS0.SSS0.Px2.p1.3.m1.1.2.4.2">𝐞</mi><mn id="S5.SS0.SSS0.Px2.p1.3.m1.1.2.4.3">1</mn></msub><mo stretchy="false" id="S5.SS0.SSS0.Px2.p1.3.m1.1.2.5">)</mo></mrow><mo id="S5.SS0.SSS0.Px2.p1.3.m1.1.3">,</mo><mo lspace="0em" rspace="0.0835em" id="S5.SS0.SSS0.Px2.p1.3.m1.1.4">.</mo><mo lspace="0.0835em" rspace="0.167em" id="S5.SS0.SSS0.Px2.p1.3.m1.1.5">.</mo><mo id="S5.SS0.SSS0.Px2.p1.3.m1.1.6">,</mo><mrow id="S5.SS0.SSS0.Px2.p1.3.m1.1.7"><mo stretchy="false" id="S5.SS0.SSS0.Px2.p1.3.m1.1.7.1">(</mo><msub id="S5.SS0.SSS0.Px2.p1.3.m1.1.7.2"><mi id="S5.SS0.SSS0.Px2.p1.3.m1.1.7.2.2">I</mi><mi id="S5.SS0.SSS0.Px2.p1.3.m1.1.7.2.3">N</mi></msub><mo id="S5.SS0.SSS0.Px2.p1.3.m1.1.7.3">,</mo><msub id="S5.SS0.SSS0.Px2.p1.3.m1.1.7.4"><mi id="S5.SS0.SSS0.Px2.p1.3.m1.1.7.4.2">𝐞</mi><mi id="S5.SS0.SSS0.Px2.p1.3.m1.1.7.4.3">N</mi></msub><mo stretchy="false" id="S5.SS0.SSS0.Px2.p1.3.m1.1.7.5">)</mo></mrow><mo stretchy="false" id="S5.SS0.SSS0.Px2.p1.3.m1.1.8">}</mo></mrow><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.3.m1.1c">\{(I_{1},{\bf e}_{1}),..,(I_{N},{\bf e}_{N})\}</annotation></semantics></math> is a <em id="S5.SS0.SSS0.Px2.p1.8.1" class="ltx_emph ltx_font_italic">training set</em> containing
pairs made of images <math id="S5.SS0.SSS0.Px2.p1.4.m2.1" class="ltx_Math" alttext="I_{i}" display="inline"><semantics id="S5.SS0.SSS0.Px2.p1.4.m2.1a"><msub id="S5.SS0.SSS0.Px2.p1.4.m2.1.1" xref="S5.SS0.SSS0.Px2.p1.4.m2.1.1.cmml"><mi id="S5.SS0.SSS0.Px2.p1.4.m2.1.1.2" xref="S5.SS0.SSS0.Px2.p1.4.m2.1.1.2.cmml">I</mi><mi id="S5.SS0.SSS0.Px2.p1.4.m2.1.1.3" xref="S5.SS0.SSS0.Px2.p1.4.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.4.m2.1b"><apply id="S5.SS0.SSS0.Px2.p1.4.m2.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.4.m2.1.1"><csymbol cd="ambiguous" id="S5.SS0.SSS0.Px2.p1.4.m2.1.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.4.m2.1.1">subscript</csymbol><ci id="S5.SS0.SSS0.Px2.p1.4.m2.1.1.2.cmml" xref="S5.SS0.SSS0.Px2.p1.4.m2.1.1.2">𝐼</ci><ci id="S5.SS0.SSS0.Px2.p1.4.m2.1.1.3.cmml" xref="S5.SS0.SSS0.Px2.p1.4.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.4.m2.1c">I_{i}</annotation></semantics></math> and the corresponding poses <math id="S5.SS0.SSS0.Px2.p1.5.m3.1" class="ltx_Math" alttext="{\bf e}_{i}" display="inline"><semantics id="S5.SS0.SSS0.Px2.p1.5.m3.1a"><msub id="S5.SS0.SSS0.Px2.p1.5.m3.1.1" xref="S5.SS0.SSS0.Px2.p1.5.m3.1.1.cmml"><mi id="S5.SS0.SSS0.Px2.p1.5.m3.1.1.2" xref="S5.SS0.SSS0.Px2.p1.5.m3.1.1.2.cmml">𝐞</mi><mi id="S5.SS0.SSS0.Px2.p1.5.m3.1.1.3" xref="S5.SS0.SSS0.Px2.p1.5.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.5.m3.1b"><apply id="S5.SS0.SSS0.Px2.p1.5.m3.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.5.m3.1.1"><csymbol cd="ambiguous" id="S5.SS0.SSS0.Px2.p1.5.m3.1.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.5.m3.1.1">subscript</csymbol><ci id="S5.SS0.SSS0.Px2.p1.5.m3.1.1.2.cmml" xref="S5.SS0.SSS0.Px2.p1.5.m3.1.1.2">𝐞</ci><ci id="S5.SS0.SSS0.Px2.p1.5.m3.1.1.3.cmml" xref="S5.SS0.SSS0.Px2.p1.5.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.5.m3.1c">{\bf e}_{i}</annotation></semantics></math>, where the
<math id="S5.SS0.SSS0.Px2.p1.6.m4.1" class="ltx_Math" alttext="{\bf e}_{i}" display="inline"><semantics id="S5.SS0.SSS0.Px2.p1.6.m4.1a"><msub id="S5.SS0.SSS0.Px2.p1.6.m4.1.1" xref="S5.SS0.SSS0.Px2.p1.6.m4.1.1.cmml"><mi id="S5.SS0.SSS0.Px2.p1.6.m4.1.1.2" xref="S5.SS0.SSS0.Px2.p1.6.m4.1.1.2.cmml">𝐞</mi><mi id="S5.SS0.SSS0.Px2.p1.6.m4.1.1.3" xref="S5.SS0.SSS0.Px2.p1.6.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.6.m4.1b"><apply id="S5.SS0.SSS0.Px2.p1.6.m4.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.6.m4.1.1"><csymbol cd="ambiguous" id="S5.SS0.SSS0.Px2.p1.6.m4.1.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.6.m4.1.1">subscript</csymbol><ci id="S5.SS0.SSS0.Px2.p1.6.m4.1.1.2.cmml" xref="S5.SS0.SSS0.Px2.p1.6.m4.1.1.2">𝐞</ci><ci id="S5.SS0.SSS0.Px2.p1.6.m4.1.1.3.cmml" xref="S5.SS0.SSS0.Px2.p1.6.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.6.m4.1c">{\bf e}_{i}</annotation></semantics></math> are vectors that contain, for example, the 3D locations of the joints of
the hand as was done in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx62" title="" class="ltx_ref">Oberweger <span class="ltx_text ltx_font_italic">et al.</span>, 2017</a>]</cite>. Function <math id="S5.SS0.SSS0.Px2.p1.7.m5.1" class="ltx_Math" alttext="\mathcal{L}" display="inline"><semantics id="S5.SS0.SSS0.Px2.p1.7.m5.1a"><mi class="ltx_font_mathcaligraphic" id="S5.SS0.SSS0.Px2.p1.7.m5.1.1" xref="S5.SS0.SSS0.Px2.p1.7.m5.1.1.cmml">ℒ</mi><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.7.m5.1b"><ci id="S5.SS0.SSS0.Px2.p1.7.m5.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.7.m5.1.1">ℒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.7.m5.1c">\mathcal{L}</annotation></semantics></math> is called a loss
function. Optimizing the network parameters <math id="S5.SS0.SSS0.Px2.p1.8.m6.1" class="ltx_Math" alttext="\Theta" display="inline"><semantics id="S5.SS0.SSS0.Px2.p1.8.m6.1a"><mi mathvariant="normal" id="S5.SS0.SSS0.Px2.p1.8.m6.1.1" xref="S5.SS0.SSS0.Px2.p1.8.m6.1.1.cmml">Θ</mi><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.8.m6.1b"><ci id="S5.SS0.SSS0.Px2.p1.8.m6.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.8.m6.1.1">Θ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.8.m6.1c">\Theta</annotation></semantics></math> is called training. Many
optimization algorithms and tricks have been proposed to solve problems like
Eq. (<a href="#S5.E1" title="In Deep Network Training. ‣ 5 Machine Learning and Deep Learning ‣ Recent Advances in 3D Object and Hand Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>), and many software libraries exist to make the implementation
of the network creation and its training an easy task.</p>
</div>
<div id="S5.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S5.SS0.SSS0.Px2.p2.1" class="ltx_p">Because optimization algorithms for network training are based on gradient
descent, any differentiable loss function can be used in principle, which makes
Deep Learning a very flexible approach, as it can be adapted easily to the
problem at hand.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Supervised Training and the Requirement for Annotated Training Sets. </h4>

<div id="S5.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px3.p1.1" class="ltx_p">Training a network by using a loss function like the one in Eq. <a href="#S5.E1" title="In Deep Network Training. ‣ 5 Machine Learning and Deep Learning ‣ Recent Advances in 3D Object and Hand Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> is
called supervised training, because we assume the availability of an annotated
dataset of images. Supervised training tends to perform very well, and it is
used very often in practical applications.</p>
</div>
<div id="S5.SS0.SSS0.Px3.p2" class="ltx_para">
<p id="S5.SS0.SSS0.Px3.p2.1" class="ltx_p">This is however probably the main drawback of Deep Learning-based 3D pose
estimation. While early methods relied only on a 3D model, and for some of them
only on a small number of images of the object, modern approaches based on Deep
Learning require a large dataset of images of the target objects, annotated with
the ground truth 3D poses</p>
</div>
<div id="S5.SS0.SSS0.Px3.p3" class="ltx_para">
<p id="S5.SS0.SSS0.Px3.p3.1" class="ltx_p">Such datasets are already available (see Section <a href="#S6" title="6 Datasets ‣ Recent Advances in 3D Object and Hand Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>), but
they are useful mostly for evaluation and comparison purposes. Applying current
methods to new objects requires creating a dataset for these new objects, and
this is a cumbersome task.</p>
</div>
<div id="S5.SS0.SSS0.Px3.p4" class="ltx_para">
<p id="S5.SS0.SSS0.Px3.p4.1" class="ltx_p">It is also possible to use synthetic data for training, by generating images
using Computer Graphics techniques. This is used in many works, often with
special to take into account the differences between real and synthetic images,
as we will see below.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Datasets</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Datasets have become an important aspect of 3D pose estimation, for training,
evaluating, and comparing methods. We describe some for object, hand, and
hand+object pose estimation below.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Datasets for Object Pose Estimation</h3>

<figure id="S6.F5" class="ltx_figure">
<table id="S6.F5.4.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.F5.4.4.4" class="ltx_tr">
<td id="S6.F5.1.1.1.1" class="ltx_td ltx_align_center"><img src="/html/2006.05927/assets/images/linemod.png" id="S6.F5.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="137" height="96" alt="Refer to caption"></td>
<td id="S6.F5.2.2.2.2" class="ltx_td ltx_align_center"><img src="/html/2006.05927/assets/images/occl_full_frame_1165.png" id="S6.F5.2.2.2.2.g1" class="ltx_graphics ltx_img_landscape" width="128" height="96" alt="Refer to caption"></td>
<td id="S6.F5.3.3.3.3" class="ltx_td ltx_align_center"><img src="/html/2006.05927/assets/images/ycb_full_frame_50_1389.png" id="S6.F5.3.3.3.3.g1" class="ltx_graphics ltx_img_landscape" width="128" height="96" alt="Refer to caption"></td>
<td id="S6.F5.4.4.4.4" class="ltx_td ltx_align_center"><img src="/html/2006.05927/assets/images/tless.png" id="S6.F5.4.4.4.4.g1" class="ltx_graphics ltx_img_landscape" width="140" height="96" alt="Refer to caption"></td>
</tr>
<tr id="S6.F5.4.4.5.1" class="ltx_tr">
<td id="S6.F5.4.4.5.1.1" class="ltx_td ltx_align_center">(a)</td>
<td id="S6.F5.4.4.5.1.2" class="ltx_td ltx_align_center">(b)</td>
<td id="S6.F5.4.4.5.1.3" class="ltx_td ltx_align_center">(c)</td>
<td id="S6.F5.4.4.5.1.4" class="ltx_td ltx_align_center">(d)</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span> Images from popular datasets for 3D object
pose estimation. (a) LineMOD; (b) Occluded LineMOD; (c) YCB-Video; (d)
T-Less.</figcaption>
</figure>
<section id="S6.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">LineMOD Dataset.</h4>

<div id="S6.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S6.SS1.SSS0.Px1.p1.1" class="ltx_p">The LineMOD dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx36" title="" class="ltx_ref">Hinterstoisser <span class="ltx_text ltx_font_italic">et al.</span>, 2012b</a>]</cite>
predates most machine learning approaches and as such, it is not divided into a
training and a test sets. It is made of 15 small objects, such as a camera, a
lamp, and a cup. For each object, it offers a set of 1200 RGB-D images of the
object surrounded by clutter. The other objects are often visible in the
clutter, but only the 3D pose of the target object is provided for each set.
The 3D models of the objects are also provided.</p>
</div>
</section>
<section id="S6.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Occluded LineMOD Dataset. </h4>

<div id="S6.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S6.SS1.SSS0.Px2.p1.1" class="ltx_p">The Occluded LineMOD dataset
was created by the authors of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">Brachmann <span class="ltx_text ltx_font_italic">et al.</span>, 2014</a>]</cite> from LineMOD by annotating the 3D poses of
the objects belonging to the dataset but originally not annotated because they
were considered as part of the clutter. This results into a sequence of 1215
frames, each frame labeled with the 3D poses of eight objects in totla, as well
as the objects’ masks. The objects show severe occlusions, which makes pose
estimation challenging.</p>
</div>
</section>
<section id="S6.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">YCB-Video Dataset. </h4>

<div id="S6.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S6.SS1.SSS0.Px3.p1.1" class="ltx_p">This dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx106" title="" class="ltx_ref">Xiang <span class="ltx_text ltx_font_italic">et al.</span>, 2018a</a>]</cite> consists of 92 video
sequences, where 12 sequences are used for testing and the remaining 80
sequences for training. In addition, the dataset contains 80k synthetically
rendered images, which can be used for training as well. There are 21 “daily
life” objects in the dataset, from cereal boxes to scissors or plates. These
objects were selected from the YCB dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">Calli <span class="ltx_text ltx_font_italic">et al.</span>, 2017</a>]</cite> and are available for
purchase. The dataset is captured with two different RGB-D sensors. The test
images are challenging due to the presence of significant image noise, different
illumination levels, and large occlusions. Each image is annotated with the 3D
object poses, as well as the objects’ masks.</p>
</div>
</section>
<section id="S6.SS1.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">T-Less Dataset. </h4>

<div id="S6.SS1.SSS0.Px4.p1" class="ltx_para">
<p id="S6.SS1.SSS0.Px4.p1.1" class="ltx_p">The T-Less dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx38" title="" class="ltx_ref">Hodan <span class="ltx_text ltx_font_italic">et al.</span>, 2016</a>]</cite> is made from 30
“industry-relevant” objects. These objects have no discriminative color nor
texture. They present different types of symmetries and similarities between
them, making pose estimation often almost ambiguous. The images were captured
using three synchronized sensors: two RGB-D cameras, one structured-light based
and one time-of-flight based, and one high-resolution RGB camera. The test
images (10K from each sensor) are from 20 scenes with increasing difficulties,
with partial occlusions and contacts between objects. This dataset remains
extremely challenging.</p>
</div>
</section>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Datasets for Hand Pose Estimation</h3>

<section id="S6.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Early datasets.</h4>

<div id="S6.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S6.SS2.SSS0.Px1.p1.1" class="ltx_p">The NYU dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx97" title="" class="ltx_ref">Tompson <span class="ltx_text ltx_font_italic">et al.</span>, 2014</a>]</cite> contains over 72k training and 8k test RGB-D
images data, captured from three different viewpoints using a structured-light
camera. The images were annotated with 3D joint locations with a semi-automated
approach, by using a standard 3D hand tracking algorithm reinitialized manually
in case of failure. The ICVL dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx91" title="" class="ltx_ref">Tang <span class="ltx_text ltx_font_italic">et al.</span>, 2014</a>]</cite> contains over 180k training
depth frames showing various hand poses, and two test sequences with each
approximately 700 frames, all captured with a time-of-flight camera. The depth
images have a high quality with hardly any missing depth values and sharp
outlines with little noise. Unfortunately, the hand pose variability of this
dataset is limited compared to other datasets, and annotations are rather
inaccurate <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx89" title="" class="ltx_ref">Supancic <span class="ltx_text ltx_font_italic">et al.</span>, 2015</a>]</cite>. The MSRA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx86" title="" class="ltx_ref">Sun <span class="ltx_text ltx_font_italic">et al.</span>, 2015</a>]</cite> contains about 76k
depth frames, captured using a time-of-flight camera from nine different
subjects.</p>
</div>
</section>
<section id="S6.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">BigHands Dataset. </h4>

<div id="S6.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S6.SS2.SSS0.Px2.p1.1" class="ltx_p">The BigHands dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx109" title="" class="ltx_ref">Yuan <span class="ltx_text ltx_font_italic">et al.</span>, 2017</a>]</cite> contains an impressive 2.2 millions RGB-D
images captured with a structured-light camera. The dataset was automatically
annotated by using six 6D electromagnetic sensors and inverse kinematics to
provide 21 3D joint locations per frame. A significant part is made of
egocentric views. The depth images have a high quality with hardly any missing
depth values, and sharp outlines with little noise. The labels are sometimes
inaccurate because of the annotation process, but the dataset has a large hand
pose variability, from ten users.</p>
</div>
</section>
<section id="S6.SS2.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">CMU Panoptic Hand Dataset. </h4>

<div id="S6.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S6.SS2.SSS0.Px3.p1.1" class="ltx_p">The CMU Panoptic hand dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx85" title="" class="ltx_ref">Simon <span class="ltx_text ltx_font_italic">et al.</span>, 2017</a>]</cite> is made of 15k real and synthetic
RGB images, from a third-person point of view. The real images were recorded in
the CMU’s Panoptic studio and annotated by the method proposed in the paper,
based on multiple views. The annotations are only in 2D but can still be useful
for multi-view pose estimation.</p>
</div>
</section>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Datasets for Object and Hand Pose Estimation</h3>

<figure id="S6.F6" class="ltx_figure">
<table id="S6.F6.6.6" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.F6.3.3.3" class="ltx_tr">
<td id="S6.F6.1.1.1.1" class="ltx_td ltx_align_center"><img src="/html/2006.05927/assets/images/bighand.png" id="S6.F6.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="180" height="122" alt="Refer to caption"></td>
<td id="S6.F6.2.2.2.2" class="ltx_td ltx_align_center"><img src="/html/2006.05927/assets/images/ganerated_sample.png" id="S6.F6.2.2.2.2.g1" class="ltx_graphics ltx_img_square" width="120" height="121" alt="Refer to caption"></td>
<td id="S6.F6.3.3.3.3" class="ltx_td ltx_align_center"><img src="/html/2006.05927/assets/images/fhad.png" id="S6.F6.3.3.3.3.g1" class="ltx_graphics ltx_img_landscape" width="180" height="123" alt="Refer to caption"></td>
</tr>
<tr id="S6.F6.6.6.7.1" class="ltx_tr">
<td id="S6.F6.6.6.7.1.1" class="ltx_td ltx_align_center">(a)</td>
<td id="S6.F6.6.6.7.1.2" class="ltx_td ltx_align_center">(b)</td>
<td id="S6.F6.6.6.7.1.3" class="ltx_td ltx_align_center">(c)</td>
</tr>
<tr id="S6.F6.6.6.6" class="ltx_tr">
<td id="S6.F6.4.4.4.1" class="ltx_td ltx_align_center"><img src="/html/2006.05927/assets/images/obman.png" id="S6.F6.4.4.4.1.g1" class="ltx_graphics ltx_img_landscape" width="180" height="138" alt="Refer to caption"></td>
<td id="S6.F6.5.5.5.2" class="ltx_td ltx_align_center"><img src="/html/2006.05927/assets/images/freihand.png" id="S6.F6.5.5.5.2.g1" class="ltx_graphics ltx_img_landscape" width="180" height="136" alt="Refer to caption"></td>
<td id="S6.F6.6.6.6.3" class="ltx_td ltx_align_center"><img src="/html/2006.05927/assets/images/ho3d.png" id="S6.F6.6.6.6.3.g1" class="ltx_graphics ltx_img_landscape" width="180" height="122" alt="Refer to caption"></td>
</tr>
<tr id="S6.F6.6.6.8.2" class="ltx_tr">
<td id="S6.F6.6.6.8.2.1" class="ltx_td ltx_align_center">(d)</td>
<td id="S6.F6.6.6.8.2.2" class="ltx_td ltx_align_center">(e)</td>
<td id="S6.F6.6.6.8.2.3" class="ltx_td ltx_align_center">(f)</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span> Images from popular datasets for 3D hand
(a) and hand+object (b-f) pose estimation. (a) BigHand; (b)
GANerated hand dataset; (c) First-Person Hand Action dataset; (d) Obman
dataset; (e) FreiHAND dataset; (f) HO-3D dataset.</figcaption>
</figure>
<section id="S6.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">GANerated Hand Dataset. </h4>

<div id="S6.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S6.SS3.SSS0.Px1.p1.1" class="ltx_p">The GANerated hand dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx60" title="" class="ltx_ref">Mueller <span class="ltx_text ltx_font_italic">et al.</span>, 2018b</a>]</cite> is a large dataset made of 330K
synthetic images of hands, sometimes holding an object, in front of a random
background. The images are annotated with the 3D poses of the hand. The images
were made more realistic by extending CycleGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx111" title="" class="ltx_ref">Zhu <span class="ltx_text ltx_font_italic">et al.</span>, 2017</a>]</cite>.</p>
</div>
</section>
<section id="S6.SS3.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">First-Person Hand Action dataset. </h4>

<div id="S6.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S6.SS3.SSS0.Px2.p1.1" class="ltx_p">The First-Person Hand Action dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx19" title="" class="ltx_ref">Garcia-Hernando <span class="ltx_text ltx_font_italic">et al.</span>, 2018</a>]</cite> provides a
dataset of hand and object interactions with 3D annotations for both
hand joints and object pose. They used a motion capture system made
of magnetic sensors attached to the user’s hand and to the
object in order to obtain hand 3D pose annotations in RGB-D video
sequences. Unfortunately, this changes the appearance of the hand in
the color images as the sensors and the tape attaching them are
visible, but the dataset proposes a large number of frames under
various conditions (more than 100K egocentric views of 6 subjects
doing 45 different types of daily-life activities).</p>
</div>
</section>
<section id="S6.SS3.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">ObMan Dataset. </h4>

<div id="S6.SS3.SSS0.Px3.p1" class="ltx_para">
<p id="S6.SS3.SSS0.Px3.p1.1" class="ltx_p">Very recently, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx31" title="" class="ltx_ref">Hasson <span class="ltx_text ltx_font_italic">et al.</span>, 2019a</a>]</cite> introduced ObMan, a large dataset of
images of hands grasping objects. The images are synthetic, but the grasps are
generated using an algorithm from robotics and the grasps still look realistic.
The dataset provides the 3D poses and shapes of the hand as well as the object
shapes.</p>
</div>
</section>
<section id="S6.SS3.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">FreiHand Dataset. </h4>

<div id="S6.SS3.SSS0.Px4.p1" class="ltx_para">
<p id="S6.SS3.SSS0.Px4.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx113" title="" class="ltx_ref">Zimmermann <span class="ltx_text ltx_font_italic">et al.</span>, 2019</a>]</cite> proposed a multi-view RGB dataset, FreiHAND, which
includes hand-object interactions and provides the 3D poses and shapes of the
hand. It relies on a green-screen background environment so that it is easy to
change the background for training purposes.</p>
</div>
</section>
<section id="S6.SS3.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">HO-3D Dataset. </h4>

<div id="S6.SS3.SSS0.Px5.p1" class="ltx_para">
<p id="S6.SS3.SSS0.Px5.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx28" title="" class="ltx_ref">Hampali <span class="ltx_text ltx_font_italic">et al.</span>, 2020</a>]</cite> proposed a method to automatically annotate video sequences
captured with one or more RGB-D cameras with the object and hand poses and
shapes. This results in a dataset made of 75,000 real RGB-D images, from 10
different objects and 10 different users. The objects come from the YCB
dataset (see Section <a href="#S6.SS1" title="6.1 Datasets for Object Pose Estimation ‣ 6 Datasets ‣ Recent Advances in 3D Object and Hand Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.1</span></a>). The backgrounds are complex,
and the mutual occlusions are often large, which makes the pose estimation
realistic but very challenging.</p>
</div>
</section>
</section>
<section id="S6.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4 </span>Metrics</h3>

<div id="S6.SS4.p1" class="ltx_para">
<p id="S6.SS4.p1.1" class="ltx_p">Metrics are important to evaluate and compare methods. Many metrics exist, and
we describe here only the main ones for object pose estimation. Discussions on
metrics for 3D object pose estimation can be found in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx38" title="" class="ltx_ref">Hodan <span class="ltx_text ltx_font_italic">et al.</span>, 2016</a>]</cite> and
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx11" title="" class="ltx_ref">Brégier <span class="ltx_text ltx_font_italic">et al.</span>, 2018</a>]</cite>.</p>
</div>
<section id="S6.SS4.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">ADD, ADI, ADD-S, and the 6D Pose metrics.</h4>

<div id="S6.SS4.SSS0.Px1.p1" class="ltx_para">
<p id="S6.SS4.SSS0.Px1.p1.8" class="ltx_p">The ADD metric <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx35" title="" class="ltx_ref">Hinterstoisser <span class="ltx_text ltx_font_italic">et al.</span>, 2012a</a>]</cite> calculates the average distance in 3D
between the model points, after applying the ground truth pose and the predicted
pose. This can be formalized as:</p>
<table id="S6.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S6.E2.m1.6" class="ltx_Math" alttext="\text{ADD}=\frac{1}{|\mathcal{V}|}\sum_{{\bf M}\in\mathcal{V}}{\|\text{Tr}({\bf M};\hat{{\bf p}})-\text{Tr}({\bf M};\bar{{\bf p}})\|_{2}}\&gt;," display="block"><semantics id="S6.E2.m1.6a"><mrow id="S6.E2.m1.6.6.1" xref="S6.E2.m1.6.6.1.1.cmml"><mrow id="S6.E2.m1.6.6.1.1" xref="S6.E2.m1.6.6.1.1.cmml"><mtext id="S6.E2.m1.6.6.1.1.3" xref="S6.E2.m1.6.6.1.1.3a.cmml">ADD</mtext><mo id="S6.E2.m1.6.6.1.1.2" xref="S6.E2.m1.6.6.1.1.2.cmml">=</mo><mrow id="S6.E2.m1.6.6.1.1.1" xref="S6.E2.m1.6.6.1.1.1.cmml"><mfrac id="S6.E2.m1.1.1" xref="S6.E2.m1.1.1.cmml"><mn id="S6.E2.m1.1.1.3" xref="S6.E2.m1.1.1.3.cmml">1</mn><mrow id="S6.E2.m1.1.1.1.3" xref="S6.E2.m1.1.1.1.2.cmml"><mo stretchy="false" id="S6.E2.m1.1.1.1.3.1" xref="S6.E2.m1.1.1.1.2.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S6.E2.m1.1.1.1.1" xref="S6.E2.m1.1.1.1.1.cmml">𝒱</mi><mo stretchy="false" id="S6.E2.m1.1.1.1.3.2" xref="S6.E2.m1.1.1.1.2.1.cmml">|</mo></mrow></mfrac><mo lspace="0em" rspace="0em" id="S6.E2.m1.6.6.1.1.1.2" xref="S6.E2.m1.6.6.1.1.1.2.cmml">​</mo><mrow id="S6.E2.m1.6.6.1.1.1.1" xref="S6.E2.m1.6.6.1.1.1.1.cmml"><munder id="S6.E2.m1.6.6.1.1.1.1.2" xref="S6.E2.m1.6.6.1.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S6.E2.m1.6.6.1.1.1.1.2.2" xref="S6.E2.m1.6.6.1.1.1.1.2.2.cmml">∑</mo><mrow id="S6.E2.m1.6.6.1.1.1.1.2.3" xref="S6.E2.m1.6.6.1.1.1.1.2.3.cmml"><mi id="S6.E2.m1.6.6.1.1.1.1.2.3.2" xref="S6.E2.m1.6.6.1.1.1.1.2.3.2.cmml">𝐌</mi><mo id="S6.E2.m1.6.6.1.1.1.1.2.3.1" xref="S6.E2.m1.6.6.1.1.1.1.2.3.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S6.E2.m1.6.6.1.1.1.1.2.3.3" xref="S6.E2.m1.6.6.1.1.1.1.2.3.3.cmml">𝒱</mi></mrow></munder><msub id="S6.E2.m1.6.6.1.1.1.1.1" xref="S6.E2.m1.6.6.1.1.1.1.1.cmml"><mrow id="S6.E2.m1.6.6.1.1.1.1.1.1.1" xref="S6.E2.m1.6.6.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S6.E2.m1.6.6.1.1.1.1.1.1.1.2" xref="S6.E2.m1.6.6.1.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S6.E2.m1.6.6.1.1.1.1.1.1.1.1" xref="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.cmml"><mrow id="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.2" xref="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.2.cmml"><mtext id="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.2.2" xref="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.2.2a.cmml">Tr</mtext><mo lspace="0em" rspace="0em" id="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.2.1" xref="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.2.1.cmml">​</mo><mrow id="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.2.3.2" xref="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.2.3.1.cmml"><mo stretchy="false" id="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.2.3.2.1" xref="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.2.3.1.cmml">(</mo><mi id="S6.E2.m1.2.2" xref="S6.E2.m1.2.2.cmml">𝐌</mi><mo id="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.2.3.2.2" xref="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.2.3.1.cmml">;</mo><mover accent="true" id="S6.E2.m1.3.3" xref="S6.E2.m1.3.3.cmml"><mi id="S6.E2.m1.3.3.2" xref="S6.E2.m1.3.3.2.cmml">𝐩</mi><mo id="S6.E2.m1.3.3.1" xref="S6.E2.m1.3.3.1.cmml">^</mo></mover><mo stretchy="false" id="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.2.3.2.3" xref="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.2.3.1.cmml">)</mo></mrow></mrow><mo id="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.1" xref="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.1.cmml">−</mo><mrow id="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.3" xref="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.3.cmml"><mtext id="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.3.2" xref="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.3.2a.cmml">Tr</mtext><mo lspace="0em" rspace="0em" id="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.3.1" xref="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.3.1.cmml">​</mo><mrow id="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.3.3.2" xref="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.3.3.1.cmml"><mo stretchy="false" id="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.3.3.2.1" xref="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.3.3.1.cmml">(</mo><mi id="S6.E2.m1.4.4" xref="S6.E2.m1.4.4.cmml">𝐌</mi><mo id="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.3.3.2.2" xref="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.3.3.1.cmml">;</mo><mover accent="true" id="S6.E2.m1.5.5" xref="S6.E2.m1.5.5.cmml"><mi id="S6.E2.m1.5.5.2" xref="S6.E2.m1.5.5.2.cmml">𝐩</mi><mo id="S6.E2.m1.5.5.1" xref="S6.E2.m1.5.5.1.cmml">¯</mo></mover><mo stretchy="false" id="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.3.3.2.3" xref="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.3.3.1.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S6.E2.m1.6.6.1.1.1.1.1.1.1.3" xref="S6.E2.m1.6.6.1.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S6.E2.m1.6.6.1.1.1.1.1.3" xref="S6.E2.m1.6.6.1.1.1.1.1.3.cmml">2</mn></msub></mrow></mrow></mrow><mo id="S6.E2.m1.6.6.1.2" xref="S6.E2.m1.6.6.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.E2.m1.6b"><apply id="S6.E2.m1.6.6.1.1.cmml" xref="S6.E2.m1.6.6.1"><eq id="S6.E2.m1.6.6.1.1.2.cmml" xref="S6.E2.m1.6.6.1.1.2"></eq><ci id="S6.E2.m1.6.6.1.1.3a.cmml" xref="S6.E2.m1.6.6.1.1.3"><mtext id="S6.E2.m1.6.6.1.1.3.cmml" xref="S6.E2.m1.6.6.1.1.3">ADD</mtext></ci><apply id="S6.E2.m1.6.6.1.1.1.cmml" xref="S6.E2.m1.6.6.1.1.1"><times id="S6.E2.m1.6.6.1.1.1.2.cmml" xref="S6.E2.m1.6.6.1.1.1.2"></times><apply id="S6.E2.m1.1.1.cmml" xref="S6.E2.m1.1.1"><divide id="S6.E2.m1.1.1.2.cmml" xref="S6.E2.m1.1.1"></divide><cn type="integer" id="S6.E2.m1.1.1.3.cmml" xref="S6.E2.m1.1.1.3">1</cn><apply id="S6.E2.m1.1.1.1.2.cmml" xref="S6.E2.m1.1.1.1.3"><abs id="S6.E2.m1.1.1.1.2.1.cmml" xref="S6.E2.m1.1.1.1.3.1"></abs><ci id="S6.E2.m1.1.1.1.1.cmml" xref="S6.E2.m1.1.1.1.1">𝒱</ci></apply></apply><apply id="S6.E2.m1.6.6.1.1.1.1.cmml" xref="S6.E2.m1.6.6.1.1.1.1"><apply id="S6.E2.m1.6.6.1.1.1.1.2.cmml" xref="S6.E2.m1.6.6.1.1.1.1.2"><csymbol cd="ambiguous" id="S6.E2.m1.6.6.1.1.1.1.2.1.cmml" xref="S6.E2.m1.6.6.1.1.1.1.2">subscript</csymbol><sum id="S6.E2.m1.6.6.1.1.1.1.2.2.cmml" xref="S6.E2.m1.6.6.1.1.1.1.2.2"></sum><apply id="S6.E2.m1.6.6.1.1.1.1.2.3.cmml" xref="S6.E2.m1.6.6.1.1.1.1.2.3"><in id="S6.E2.m1.6.6.1.1.1.1.2.3.1.cmml" xref="S6.E2.m1.6.6.1.1.1.1.2.3.1"></in><ci id="S6.E2.m1.6.6.1.1.1.1.2.3.2.cmml" xref="S6.E2.m1.6.6.1.1.1.1.2.3.2">𝐌</ci><ci id="S6.E2.m1.6.6.1.1.1.1.2.3.3.cmml" xref="S6.E2.m1.6.6.1.1.1.1.2.3.3">𝒱</ci></apply></apply><apply id="S6.E2.m1.6.6.1.1.1.1.1.cmml" xref="S6.E2.m1.6.6.1.1.1.1.1"><csymbol cd="ambiguous" id="S6.E2.m1.6.6.1.1.1.1.1.2.cmml" xref="S6.E2.m1.6.6.1.1.1.1.1">subscript</csymbol><apply id="S6.E2.m1.6.6.1.1.1.1.1.1.2.cmml" xref="S6.E2.m1.6.6.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S6.E2.m1.6.6.1.1.1.1.1.1.2.1.cmml" xref="S6.E2.m1.6.6.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.cmml" xref="S6.E2.m1.6.6.1.1.1.1.1.1.1.1"><minus id="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.1.cmml" xref="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.1"></minus><apply id="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.2.cmml" xref="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.2"><times id="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.2.1.cmml" xref="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.2.1"></times><ci id="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.2.2a.cmml" xref="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.2.2"><mtext id="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.2.2.cmml" xref="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.2.2">Tr</mtext></ci><list id="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.2.3.2"><ci id="S6.E2.m1.2.2.cmml" xref="S6.E2.m1.2.2">𝐌</ci><apply id="S6.E2.m1.3.3.cmml" xref="S6.E2.m1.3.3"><ci id="S6.E2.m1.3.3.1.cmml" xref="S6.E2.m1.3.3.1">^</ci><ci id="S6.E2.m1.3.3.2.cmml" xref="S6.E2.m1.3.3.2">𝐩</ci></apply></list></apply><apply id="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.3.cmml" xref="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.3"><times id="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.3.1.cmml" xref="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.3.1"></times><ci id="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.3.2a.cmml" xref="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.3.2"><mtext id="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.3.2.cmml" xref="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.3.2">Tr</mtext></ci><list id="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S6.E2.m1.6.6.1.1.1.1.1.1.1.1.3.3.2"><ci id="S6.E2.m1.4.4.cmml" xref="S6.E2.m1.4.4">𝐌</ci><apply id="S6.E2.m1.5.5.cmml" xref="S6.E2.m1.5.5"><ci id="S6.E2.m1.5.5.1.cmml" xref="S6.E2.m1.5.5.1">¯</ci><ci id="S6.E2.m1.5.5.2.cmml" xref="S6.E2.m1.5.5.2">𝐩</ci></apply></list></apply></apply></apply><cn type="integer" id="S6.E2.m1.6.6.1.1.1.1.1.3.cmml" xref="S6.E2.m1.6.6.1.1.1.1.1.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E2.m1.6c">\text{ADD}=\frac{1}{|\mathcal{V}|}\sum_{{\bf M}\in\mathcal{V}}{\|\text{Tr}({\bf M};\hat{{\bf p}})-\text{Tr}({\bf M};\bar{{\bf p}})\|_{2}}\&gt;,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S6.SS4.SSS0.Px1.p1.7" class="ltx_p">where <math id="S6.SS4.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{V}" display="inline"><semantics id="S6.SS4.SSS0.Px1.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S6.SS4.SSS0.Px1.p1.1.m1.1.1" xref="S6.SS4.SSS0.Px1.p1.1.m1.1.1.cmml">𝒱</mi><annotation-xml encoding="MathML-Content" id="S6.SS4.SSS0.Px1.p1.1.m1.1b"><ci id="S6.SS4.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S6.SS4.SSS0.Px1.p1.1.m1.1.1">𝒱</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.SSS0.Px1.p1.1.m1.1c">\mathcal{V}</annotation></semantics></math> is the set of the object’s vertices, <math id="S6.SS4.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="\hat{{\bf p}}" display="inline"><semantics id="S6.SS4.SSS0.Px1.p1.2.m2.1a"><mover accent="true" id="S6.SS4.SSS0.Px1.p1.2.m2.1.1" xref="S6.SS4.SSS0.Px1.p1.2.m2.1.1.cmml"><mi id="S6.SS4.SSS0.Px1.p1.2.m2.1.1.2" xref="S6.SS4.SSS0.Px1.p1.2.m2.1.1.2.cmml">𝐩</mi><mo id="S6.SS4.SSS0.Px1.p1.2.m2.1.1.1" xref="S6.SS4.SSS0.Px1.p1.2.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S6.SS4.SSS0.Px1.p1.2.m2.1b"><apply id="S6.SS4.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S6.SS4.SSS0.Px1.p1.2.m2.1.1"><ci id="S6.SS4.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S6.SS4.SSS0.Px1.p1.2.m2.1.1.1">^</ci><ci id="S6.SS4.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S6.SS4.SSS0.Px1.p1.2.m2.1.1.2">𝐩</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.SSS0.Px1.p1.2.m2.1c">\hat{{\bf p}}</annotation></semantics></math> the estimated
pose and <math id="S6.SS4.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="\bar{{\bf p}}" display="inline"><semantics id="S6.SS4.SSS0.Px1.p1.3.m3.1a"><mover accent="true" id="S6.SS4.SSS0.Px1.p1.3.m3.1.1" xref="S6.SS4.SSS0.Px1.p1.3.m3.1.1.cmml"><mi id="S6.SS4.SSS0.Px1.p1.3.m3.1.1.2" xref="S6.SS4.SSS0.Px1.p1.3.m3.1.1.2.cmml">𝐩</mi><mo id="S6.SS4.SSS0.Px1.p1.3.m3.1.1.1" xref="S6.SS4.SSS0.Px1.p1.3.m3.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="S6.SS4.SSS0.Px1.p1.3.m3.1b"><apply id="S6.SS4.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S6.SS4.SSS0.Px1.p1.3.m3.1.1"><ci id="S6.SS4.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="S6.SS4.SSS0.Px1.p1.3.m3.1.1.1">¯</ci><ci id="S6.SS4.SSS0.Px1.p1.3.m3.1.1.2.cmml" xref="S6.SS4.SSS0.Px1.p1.3.m3.1.1.2">𝐩</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.SSS0.Px1.p1.3.m3.1c">\bar{{\bf p}}</annotation></semantics></math> the ground truth pose, and <math id="S6.SS4.SSS0.Px1.p1.4.m4.2" class="ltx_Math" alttext="\text{Tr}({\bf M};{\bf p})" display="inline"><semantics id="S6.SS4.SSS0.Px1.p1.4.m4.2a"><mrow id="S6.SS4.SSS0.Px1.p1.4.m4.2.3" xref="S6.SS4.SSS0.Px1.p1.4.m4.2.3.cmml"><mtext id="S6.SS4.SSS0.Px1.p1.4.m4.2.3.2" xref="S6.SS4.SSS0.Px1.p1.4.m4.2.3.2a.cmml">Tr</mtext><mo lspace="0em" rspace="0em" id="S6.SS4.SSS0.Px1.p1.4.m4.2.3.1" xref="S6.SS4.SSS0.Px1.p1.4.m4.2.3.1.cmml">​</mo><mrow id="S6.SS4.SSS0.Px1.p1.4.m4.2.3.3.2" xref="S6.SS4.SSS0.Px1.p1.4.m4.2.3.3.1.cmml"><mo stretchy="false" id="S6.SS4.SSS0.Px1.p1.4.m4.2.3.3.2.1" xref="S6.SS4.SSS0.Px1.p1.4.m4.2.3.3.1.cmml">(</mo><mi id="S6.SS4.SSS0.Px1.p1.4.m4.1.1" xref="S6.SS4.SSS0.Px1.p1.4.m4.1.1.cmml">𝐌</mi><mo id="S6.SS4.SSS0.Px1.p1.4.m4.2.3.3.2.2" xref="S6.SS4.SSS0.Px1.p1.4.m4.2.3.3.1.cmml">;</mo><mi id="S6.SS4.SSS0.Px1.p1.4.m4.2.2" xref="S6.SS4.SSS0.Px1.p1.4.m4.2.2.cmml">𝐩</mi><mo stretchy="false" id="S6.SS4.SSS0.Px1.p1.4.m4.2.3.3.2.3" xref="S6.SS4.SSS0.Px1.p1.4.m4.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.SS4.SSS0.Px1.p1.4.m4.2b"><apply id="S6.SS4.SSS0.Px1.p1.4.m4.2.3.cmml" xref="S6.SS4.SSS0.Px1.p1.4.m4.2.3"><times id="S6.SS4.SSS0.Px1.p1.4.m4.2.3.1.cmml" xref="S6.SS4.SSS0.Px1.p1.4.m4.2.3.1"></times><ci id="S6.SS4.SSS0.Px1.p1.4.m4.2.3.2a.cmml" xref="S6.SS4.SSS0.Px1.p1.4.m4.2.3.2"><mtext id="S6.SS4.SSS0.Px1.p1.4.m4.2.3.2.cmml" xref="S6.SS4.SSS0.Px1.p1.4.m4.2.3.2">Tr</mtext></ci><list id="S6.SS4.SSS0.Px1.p1.4.m4.2.3.3.1.cmml" xref="S6.SS4.SSS0.Px1.p1.4.m4.2.3.3.2"><ci id="S6.SS4.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S6.SS4.SSS0.Px1.p1.4.m4.1.1">𝐌</ci><ci id="S6.SS4.SSS0.Px1.p1.4.m4.2.2.cmml" xref="S6.SS4.SSS0.Px1.p1.4.m4.2.2">𝐩</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.SSS0.Px1.p1.4.m4.2c">\text{Tr}({\bf M};{\bf p})</annotation></semantics></math> the
rigid transformation in <math id="S6.SS4.SSS0.Px1.p1.5.m5.1" class="ltx_Math" alttext="{\bf p}" display="inline"><semantics id="S6.SS4.SSS0.Px1.p1.5.m5.1a"><mi id="S6.SS4.SSS0.Px1.p1.5.m5.1.1" xref="S6.SS4.SSS0.Px1.p1.5.m5.1.1.cmml">𝐩</mi><annotation-xml encoding="MathML-Content" id="S6.SS4.SSS0.Px1.p1.5.m5.1b"><ci id="S6.SS4.SSS0.Px1.p1.5.m5.1.1.cmml" xref="S6.SS4.SSS0.Px1.p1.5.m5.1.1">𝐩</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.SSS0.Px1.p1.5.m5.1c">{\bf p}</annotation></semantics></math> applied to 3D point <math id="S6.SS4.SSS0.Px1.p1.6.m6.1" class="ltx_Math" alttext="{\bf M}" display="inline"><semantics id="S6.SS4.SSS0.Px1.p1.6.m6.1a"><mi id="S6.SS4.SSS0.Px1.p1.6.m6.1.1" xref="S6.SS4.SSS0.Px1.p1.6.m6.1.1.cmml">𝐌</mi><annotation-xml encoding="MathML-Content" id="S6.SS4.SSS0.Px1.p1.6.m6.1b"><ci id="S6.SS4.SSS0.Px1.p1.6.m6.1.1.cmml" xref="S6.SS4.SSS0.Px1.p1.6.m6.1.1">𝐌</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.SSS0.Px1.p1.6.m6.1c">{\bf M}</annotation></semantics></math>. In the 6D Pose
metric, a pose is considered when the ADD metric is less than <math id="S6.SS4.SSS0.Px1.p1.7.m7.1" class="ltx_Math" alttext="10\%" display="inline"><semantics id="S6.SS4.SSS0.Px1.p1.7.m7.1a"><mrow id="S6.SS4.SSS0.Px1.p1.7.m7.1.1" xref="S6.SS4.SSS0.Px1.p1.7.m7.1.1.cmml"><mn id="S6.SS4.SSS0.Px1.p1.7.m7.1.1.2" xref="S6.SS4.SSS0.Px1.p1.7.m7.1.1.2.cmml">10</mn><mo id="S6.SS4.SSS0.Px1.p1.7.m7.1.1.1" xref="S6.SS4.SSS0.Px1.p1.7.m7.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS4.SSS0.Px1.p1.7.m7.1b"><apply id="S6.SS4.SSS0.Px1.p1.7.m7.1.1.cmml" xref="S6.SS4.SSS0.Px1.p1.7.m7.1.1"><csymbol cd="latexml" id="S6.SS4.SSS0.Px1.p1.7.m7.1.1.1.cmml" xref="S6.SS4.SSS0.Px1.p1.7.m7.1.1.1">percent</csymbol><cn type="integer" id="S6.SS4.SSS0.Px1.p1.7.m7.1.1.2.cmml" xref="S6.SS4.SSS0.Px1.p1.7.m7.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.SSS0.Px1.p1.7.m7.1c">10\%</annotation></semantics></math> of the
object’s diameter.</p>
</div>
<div id="S6.SS4.SSS0.Px1.p2" class="ltx_para">
<p id="S6.SS4.SSS0.Px1.p2.1" class="ltx_p">For the objects with ambiguous poses due to symmetries, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx35" title="" class="ltx_ref">Hinterstoisser <span class="ltx_text ltx_font_italic">et al.</span>, 2012a</a>]</cite>
replaces the ADD metric by the ADI metric, also referred to as the ADD-S metric
in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx107" title="" class="ltx_ref">Xiang <span class="ltx_text ltx_font_italic">et al.</span>, 2018b</a>]</cite>, computed as follows:</p>
<table id="S6.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S6.E3.m1.4" class="ltx_Math" alttext="\text{ADD-S}=\frac{1}{|\mathcal{V}|}\sum_{{\bf M}_{1}\in\mathcal{V}}{\min_{{\bf M}_{2}\in\mathcal{V}}{\|\text{Tr}({\bf M}_{1};\hat{{\bf p}})-\text{Tr}({\bf M}_{2};\bar{{\bf p}})\|_{2}}}\&gt;," display="block"><semantics id="S6.E3.m1.4a"><mrow id="S6.E3.m1.4.4.1" xref="S6.E3.m1.4.4.1.1.cmml"><mrow id="S6.E3.m1.4.4.1.1" xref="S6.E3.m1.4.4.1.1.cmml"><mtext id="S6.E3.m1.4.4.1.1.3" xref="S6.E3.m1.4.4.1.1.3a.cmml">ADD-S</mtext><mo id="S6.E3.m1.4.4.1.1.2" xref="S6.E3.m1.4.4.1.1.2.cmml">=</mo><mrow id="S6.E3.m1.4.4.1.1.1" xref="S6.E3.m1.4.4.1.1.1.cmml"><mfrac id="S6.E3.m1.1.1" xref="S6.E3.m1.1.1.cmml"><mn id="S6.E3.m1.1.1.3" xref="S6.E3.m1.1.1.3.cmml">1</mn><mrow id="S6.E3.m1.1.1.1.3" xref="S6.E3.m1.1.1.1.2.cmml"><mo stretchy="false" id="S6.E3.m1.1.1.1.3.1" xref="S6.E3.m1.1.1.1.2.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S6.E3.m1.1.1.1.1" xref="S6.E3.m1.1.1.1.1.cmml">𝒱</mi><mo stretchy="false" id="S6.E3.m1.1.1.1.3.2" xref="S6.E3.m1.1.1.1.2.1.cmml">|</mo></mrow></mfrac><mo lspace="0em" rspace="0em" id="S6.E3.m1.4.4.1.1.1.2" xref="S6.E3.m1.4.4.1.1.1.2.cmml">​</mo><mrow id="S6.E3.m1.4.4.1.1.1.1" xref="S6.E3.m1.4.4.1.1.1.1.cmml"><munder id="S6.E3.m1.4.4.1.1.1.1.2" xref="S6.E3.m1.4.4.1.1.1.1.2.cmml"><mo movablelimits="false" id="S6.E3.m1.4.4.1.1.1.1.2.2" xref="S6.E3.m1.4.4.1.1.1.1.2.2.cmml">∑</mo><mrow id="S6.E3.m1.4.4.1.1.1.1.2.3" xref="S6.E3.m1.4.4.1.1.1.1.2.3.cmml"><msub id="S6.E3.m1.4.4.1.1.1.1.2.3.2" xref="S6.E3.m1.4.4.1.1.1.1.2.3.2.cmml"><mi id="S6.E3.m1.4.4.1.1.1.1.2.3.2.2" xref="S6.E3.m1.4.4.1.1.1.1.2.3.2.2.cmml">𝐌</mi><mn id="S6.E3.m1.4.4.1.1.1.1.2.3.2.3" xref="S6.E3.m1.4.4.1.1.1.1.2.3.2.3.cmml">1</mn></msub><mo id="S6.E3.m1.4.4.1.1.1.1.2.3.1" xref="S6.E3.m1.4.4.1.1.1.1.2.3.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S6.E3.m1.4.4.1.1.1.1.2.3.3" xref="S6.E3.m1.4.4.1.1.1.1.2.3.3.cmml">𝒱</mi></mrow></munder><mrow id="S6.E3.m1.4.4.1.1.1.1.1" xref="S6.E3.m1.4.4.1.1.1.1.1.cmml"><munder id="S6.E3.m1.4.4.1.1.1.1.1.2" xref="S6.E3.m1.4.4.1.1.1.1.1.2.cmml"><mi id="S6.E3.m1.4.4.1.1.1.1.1.2.2" xref="S6.E3.m1.4.4.1.1.1.1.1.2.2.cmml">min</mi><mrow id="S6.E3.m1.4.4.1.1.1.1.1.2.3" xref="S6.E3.m1.4.4.1.1.1.1.1.2.3.cmml"><msub id="S6.E3.m1.4.4.1.1.1.1.1.2.3.2" xref="S6.E3.m1.4.4.1.1.1.1.1.2.3.2.cmml"><mi id="S6.E3.m1.4.4.1.1.1.1.1.2.3.2.2" xref="S6.E3.m1.4.4.1.1.1.1.1.2.3.2.2.cmml">𝐌</mi><mn id="S6.E3.m1.4.4.1.1.1.1.1.2.3.2.3" xref="S6.E3.m1.4.4.1.1.1.1.1.2.3.2.3.cmml">2</mn></msub><mo id="S6.E3.m1.4.4.1.1.1.1.1.2.3.1" xref="S6.E3.m1.4.4.1.1.1.1.1.2.3.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S6.E3.m1.4.4.1.1.1.1.1.2.3.3" xref="S6.E3.m1.4.4.1.1.1.1.1.2.3.3.cmml">𝒱</mi></mrow></munder><mo id="S6.E3.m1.4.4.1.1.1.1.1a" xref="S6.E3.m1.4.4.1.1.1.1.1.cmml">⁡</mo><msub id="S6.E3.m1.4.4.1.1.1.1.1.1" xref="S6.E3.m1.4.4.1.1.1.1.1.1.cmml"><mrow id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.2" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.cmml"><mrow id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml"><mtext id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.3" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.3a.cmml">Tr</mtext><mo lspace="0em" rspace="0em" id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.2" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.cmml">(</mo><msub id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">𝐌</mi><mn id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.cmml">;</mo><mover accent="true" id="S6.E3.m1.2.2" xref="S6.E3.m1.2.2.cmml"><mi id="S6.E3.m1.2.2.2" xref="S6.E3.m1.2.2.2.cmml">𝐩</mi><mo id="S6.E3.m1.2.2.1" xref="S6.E3.m1.2.2.1.cmml">^</mo></mover><mo stretchy="false" id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.4" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.cmml">−</mo><mrow id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.cmml"><mtext id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.3" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.3a.cmml">Tr</mtext><mo lspace="0em" rspace="0em" id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.2" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.cmml">​</mo><mrow id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.1.1" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.1.2.cmml"><mo stretchy="false" id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.1.1.2" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.1.2.cmml">(</mo><msub id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.1.1.1" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.1.1.1.cmml"><mi id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.1.1.1.2" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.1.1.1.2.cmml">𝐌</mi><mn id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.1.1.1.3" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.1.1.1.3.cmml">2</mn></msub><mo id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.1.1.3" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.1.2.cmml">;</mo><mover accent="true" id="S6.E3.m1.3.3" xref="S6.E3.m1.3.3.cmml"><mi id="S6.E3.m1.3.3.2" xref="S6.E3.m1.3.3.2.cmml">𝐩</mi><mo id="S6.E3.m1.3.3.1" xref="S6.E3.m1.3.3.1.cmml">¯</mo></mover><mo stretchy="false" id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.1.1.4" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.1.2.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.3" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S6.E3.m1.4.4.1.1.1.1.1.1.3" xref="S6.E3.m1.4.4.1.1.1.1.1.1.3.cmml">2</mn></msub></mrow></mrow></mrow></mrow><mo id="S6.E3.m1.4.4.1.2" xref="S6.E3.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.E3.m1.4b"><apply id="S6.E3.m1.4.4.1.1.cmml" xref="S6.E3.m1.4.4.1"><eq id="S6.E3.m1.4.4.1.1.2.cmml" xref="S6.E3.m1.4.4.1.1.2"></eq><ci id="S6.E3.m1.4.4.1.1.3a.cmml" xref="S6.E3.m1.4.4.1.1.3"><mtext id="S6.E3.m1.4.4.1.1.3.cmml" xref="S6.E3.m1.4.4.1.1.3">ADD-S</mtext></ci><apply id="S6.E3.m1.4.4.1.1.1.cmml" xref="S6.E3.m1.4.4.1.1.1"><times id="S6.E3.m1.4.4.1.1.1.2.cmml" xref="S6.E3.m1.4.4.1.1.1.2"></times><apply id="S6.E3.m1.1.1.cmml" xref="S6.E3.m1.1.1"><divide id="S6.E3.m1.1.1.2.cmml" xref="S6.E3.m1.1.1"></divide><cn type="integer" id="S6.E3.m1.1.1.3.cmml" xref="S6.E3.m1.1.1.3">1</cn><apply id="S6.E3.m1.1.1.1.2.cmml" xref="S6.E3.m1.1.1.1.3"><abs id="S6.E3.m1.1.1.1.2.1.cmml" xref="S6.E3.m1.1.1.1.3.1"></abs><ci id="S6.E3.m1.1.1.1.1.cmml" xref="S6.E3.m1.1.1.1.1">𝒱</ci></apply></apply><apply id="S6.E3.m1.4.4.1.1.1.1.cmml" xref="S6.E3.m1.4.4.1.1.1.1"><apply id="S6.E3.m1.4.4.1.1.1.1.2.cmml" xref="S6.E3.m1.4.4.1.1.1.1.2"><csymbol cd="ambiguous" id="S6.E3.m1.4.4.1.1.1.1.2.1.cmml" xref="S6.E3.m1.4.4.1.1.1.1.2">subscript</csymbol><sum id="S6.E3.m1.4.4.1.1.1.1.2.2.cmml" xref="S6.E3.m1.4.4.1.1.1.1.2.2"></sum><apply id="S6.E3.m1.4.4.1.1.1.1.2.3.cmml" xref="S6.E3.m1.4.4.1.1.1.1.2.3"><in id="S6.E3.m1.4.4.1.1.1.1.2.3.1.cmml" xref="S6.E3.m1.4.4.1.1.1.1.2.3.1"></in><apply id="S6.E3.m1.4.4.1.1.1.1.2.3.2.cmml" xref="S6.E3.m1.4.4.1.1.1.1.2.3.2"><csymbol cd="ambiguous" id="S6.E3.m1.4.4.1.1.1.1.2.3.2.1.cmml" xref="S6.E3.m1.4.4.1.1.1.1.2.3.2">subscript</csymbol><ci id="S6.E3.m1.4.4.1.1.1.1.2.3.2.2.cmml" xref="S6.E3.m1.4.4.1.1.1.1.2.3.2.2">𝐌</ci><cn type="integer" id="S6.E3.m1.4.4.1.1.1.1.2.3.2.3.cmml" xref="S6.E3.m1.4.4.1.1.1.1.2.3.2.3">1</cn></apply><ci id="S6.E3.m1.4.4.1.1.1.1.2.3.3.cmml" xref="S6.E3.m1.4.4.1.1.1.1.2.3.3">𝒱</ci></apply></apply><apply id="S6.E3.m1.4.4.1.1.1.1.1.cmml" xref="S6.E3.m1.4.4.1.1.1.1.1"><apply id="S6.E3.m1.4.4.1.1.1.1.1.2.cmml" xref="S6.E3.m1.4.4.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S6.E3.m1.4.4.1.1.1.1.1.2.1.cmml" xref="S6.E3.m1.4.4.1.1.1.1.1.2">subscript</csymbol><min id="S6.E3.m1.4.4.1.1.1.1.1.2.2.cmml" xref="S6.E3.m1.4.4.1.1.1.1.1.2.2"></min><apply id="S6.E3.m1.4.4.1.1.1.1.1.2.3.cmml" xref="S6.E3.m1.4.4.1.1.1.1.1.2.3"><in id="S6.E3.m1.4.4.1.1.1.1.1.2.3.1.cmml" xref="S6.E3.m1.4.4.1.1.1.1.1.2.3.1"></in><apply id="S6.E3.m1.4.4.1.1.1.1.1.2.3.2.cmml" xref="S6.E3.m1.4.4.1.1.1.1.1.2.3.2"><csymbol cd="ambiguous" id="S6.E3.m1.4.4.1.1.1.1.1.2.3.2.1.cmml" xref="S6.E3.m1.4.4.1.1.1.1.1.2.3.2">subscript</csymbol><ci id="S6.E3.m1.4.4.1.1.1.1.1.2.3.2.2.cmml" xref="S6.E3.m1.4.4.1.1.1.1.1.2.3.2.2">𝐌</ci><cn type="integer" id="S6.E3.m1.4.4.1.1.1.1.1.2.3.2.3.cmml" xref="S6.E3.m1.4.4.1.1.1.1.1.2.3.2.3">2</cn></apply><ci id="S6.E3.m1.4.4.1.1.1.1.1.2.3.3.cmml" xref="S6.E3.m1.4.4.1.1.1.1.1.2.3.3">𝒱</ci></apply></apply><apply id="S6.E3.m1.4.4.1.1.1.1.1.1.cmml" xref="S6.E3.m1.4.4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S6.E3.m1.4.4.1.1.1.1.1.1.2.cmml" xref="S6.E3.m1.4.4.1.1.1.1.1.1">subscript</csymbol><apply id="S6.E3.m1.4.4.1.1.1.1.1.1.1.2.cmml" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S6.E3.m1.4.4.1.1.1.1.1.1.1.2.1.cmml" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.cmml" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1"><minus id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.cmml" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3"></minus><apply id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1"><times id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.2"></times><ci id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.3a.cmml" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.3"><mtext id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.3">Tr</mtext></ci><list id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1"><apply id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.2">𝐌</ci><cn type="integer" id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.3">1</cn></apply><apply id="S6.E3.m1.2.2.cmml" xref="S6.E3.m1.2.2"><ci id="S6.E3.m1.2.2.1.cmml" xref="S6.E3.m1.2.2.1">^</ci><ci id="S6.E3.m1.2.2.2.cmml" xref="S6.E3.m1.2.2.2">𝐩</ci></apply></list></apply><apply id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.cmml" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2"><times id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.2"></times><ci id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.3a.cmml" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.3"><mtext id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.3">Tr</mtext></ci><list id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.1.2.cmml" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.1.1"><apply id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.1.1.1.cmml" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.1.1.1"><csymbol cd="ambiguous" id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.1.1.1.1.cmml" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.1.1.1">subscript</csymbol><ci id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.1.1.1.2.cmml" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.1.1.1.2">𝐌</ci><cn type="integer" id="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.1.1.1.3.cmml" xref="S6.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.1.1.1.3">2</cn></apply><apply id="S6.E3.m1.3.3.cmml" xref="S6.E3.m1.3.3"><ci id="S6.E3.m1.3.3.1.cmml" xref="S6.E3.m1.3.3.1">¯</ci><ci id="S6.E3.m1.3.3.2.cmml" xref="S6.E3.m1.3.3.2">𝐩</ci></apply></list></apply></apply></apply><cn type="integer" id="S6.E3.m1.4.4.1.1.1.1.1.1.3.cmml" xref="S6.E3.m1.4.4.1.1.1.1.1.1.3">2</cn></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E3.m1.4c">\text{ADD-S}=\frac{1}{|\mathcal{V}|}\sum_{{\bf M}_{1}\in\mathcal{V}}{\min_{{\bf M}_{2}\in\mathcal{V}}{\|\text{Tr}({\bf M}_{1};\hat{{\bf p}})-\text{Tr}({\bf M}_{2};\bar{{\bf p}})\|_{2}}}\&gt;,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S6.SS4.SSS0.Px1.p2.2" class="ltx_p">which averages the distances from points after applying the predicted pose to
the <em id="S6.SS4.SSS0.Px1.p2.2.1" class="ltx_emph ltx_font_italic">closest</em> points under the ground truth pose. The advantage of this
metric is that it is indeed equal to zero when the pose is retrieved up to a
symmetry, even if it does not exploit the symmetries of the object.</p>
</div>
</section>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Modern Approaches to 3D Object Pose Estimation</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">Over the past years, many authors realise that Deep Learning is a powerful tool
for 3D object pose estimation from images. We discuss here the development of
Deep Learning applied to 3D object pose estimation over time. This development
was and is still extremely fast, with improving accuracy, robustness, and
computation times. We present this development through several milestone
methods, but much more methods could also be included here.</p>
</div>
<figure id="S7.F7" class="ltx_figure"><img src="/html/2006.05927/assets/images/bb8.png" id="S7.F7.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="174" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 7: </span> Some 3D object pose estimation methods predict the pose
by first predicting the 2D reprojections <math id="S7.F7.7.m1.1" class="ltx_Math" alttext="{\bf m}_{i}" display="inline"><semantics id="S7.F7.7.m1.1b"><msub id="S7.F7.7.m1.1.1" xref="S7.F7.7.m1.1.1.cmml"><mi id="S7.F7.7.m1.1.1.2" xref="S7.F7.7.m1.1.1.2.cmml">𝐦</mi><mi id="S7.F7.7.m1.1.1.3" xref="S7.F7.7.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S7.F7.7.m1.1c"><apply id="S7.F7.7.m1.1.1.cmml" xref="S7.F7.7.m1.1.1"><csymbol cd="ambiguous" id="S7.F7.7.m1.1.1.1.cmml" xref="S7.F7.7.m1.1.1">subscript</csymbol><ci id="S7.F7.7.m1.1.1.2.cmml" xref="S7.F7.7.m1.1.1.2">𝐦</ci><ci id="S7.F7.7.m1.1.1.3.cmml" xref="S7.F7.7.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.F7.7.m1.1d">{\bf m}_{i}</annotation></semantics></math> of some 3D points <math id="S7.F7.8.m2.1" class="ltx_Math" alttext="{\bf M}_{i}" display="inline"><semantics id="S7.F7.8.m2.1b"><msub id="S7.F7.8.m2.1.1" xref="S7.F7.8.m2.1.1.cmml"><mi id="S7.F7.8.m2.1.1.2" xref="S7.F7.8.m2.1.1.2.cmml">𝐌</mi><mi id="S7.F7.8.m2.1.1.3" xref="S7.F7.8.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S7.F7.8.m2.1c"><apply id="S7.F7.8.m2.1.1.cmml" xref="S7.F7.8.m2.1.1"><csymbol cd="ambiguous" id="S7.F7.8.m2.1.1.1.cmml" xref="S7.F7.8.m2.1.1">subscript</csymbol><ci id="S7.F7.8.m2.1.1.2.cmml" xref="S7.F7.8.m2.1.1.2">𝐌</ci><ci id="S7.F7.8.m2.1.1.3.cmml" xref="S7.F7.8.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.F7.8.m2.1d">{\bf M}_{i}</annotation></semantics></math>,
and then computing the 3D rotation and translation from the 2D-3D
correspondences between the <math id="S7.F7.9.m3.1" class="ltx_Math" alttext="{\bf m}_{i}" display="inline"><semantics id="S7.F7.9.m3.1b"><msub id="S7.F7.9.m3.1.1" xref="S7.F7.9.m3.1.1.cmml"><mi id="S7.F7.9.m3.1.1.2" xref="S7.F7.9.m3.1.1.2.cmml">𝐦</mi><mi id="S7.F7.9.m3.1.1.3" xref="S7.F7.9.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S7.F7.9.m3.1c"><apply id="S7.F7.9.m3.1.1.cmml" xref="S7.F7.9.m3.1.1"><csymbol cd="ambiguous" id="S7.F7.9.m3.1.1.1.cmml" xref="S7.F7.9.m3.1.1">subscript</csymbol><ci id="S7.F7.9.m3.1.1.2.cmml" xref="S7.F7.9.m3.1.1.2">𝐦</ci><ci id="S7.F7.9.m3.1.1.3.cmml" xref="S7.F7.9.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.F7.9.m3.1d">{\bf m}_{i}</annotation></semantics></math> and <math id="S7.F7.10.m4.1" class="ltx_Math" alttext="{\bf M}_{i}" display="inline"><semantics id="S7.F7.10.m4.1b"><msub id="S7.F7.10.m4.1.1" xref="S7.F7.10.m4.1.1.cmml"><mi id="S7.F7.10.m4.1.1.2" xref="S7.F7.10.m4.1.1.2.cmml">𝐌</mi><mi id="S7.F7.10.m4.1.1.3" xref="S7.F7.10.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S7.F7.10.m4.1c"><apply id="S7.F7.10.m4.1.1.cmml" xref="S7.F7.10.m4.1.1"><csymbol cd="ambiguous" id="S7.F7.10.m4.1.1.1.cmml" xref="S7.F7.10.m4.1.1">subscript</csymbol><ci id="S7.F7.10.m4.1.1.2.cmml" xref="S7.F7.10.m4.1.1.2">𝐌</ci><ci id="S7.F7.10.m4.1.1.3.cmml" xref="S7.F7.10.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.F7.10.m4.1d">{\bf M}_{i}</annotation></semantics></math> points using a P<math id="S7.F7.11.m5.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S7.F7.11.m5.1b"><mi id="S7.F7.11.m5.1.1" xref="S7.F7.11.m5.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S7.F7.11.m5.1c"><ci id="S7.F7.11.m5.1.1.cmml" xref="S7.F7.11.m5.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.F7.11.m5.1d">n</annotation></semantics></math>P
algorithm.</figcaption>
</figure>
<section id="S7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1 </span>BB8</h3>

<div id="S7.SS1.p1" class="ltx_para">
<p id="S7.SS1.p1.1" class="ltx_p">One of the first Deep Learning methods for 3D object pose estimation is probably
BB8 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx73" title="" class="ltx_ref">Rad and Lepetit, 2017</a>]</cite>. As it is the first method we describe, we will present it in
some details.</p>
</div>
<div id="S7.SS1.p2" class="ltx_para">
<p id="S7.SS1.p2.1" class="ltx_p">This method proceeds in three steps.
It first detect the target objects in 2D using coarse object segmentation. It
then applies a Deep Network on each image window centered on detected
objects. Instead of predicting the 3D pose of the detected objects in the form
of a 3D translation and a 3D rotation, it predict the 2D projections of the
corners of the object’s bounding box, and compute the 3D pose from these 2D-3D
correspondences with a P<math id="S7.SS1.p2.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S7.SS1.p2.1.m1.1a"><mi id="S7.SS1.p2.1.m1.1.1" xref="S7.SS1.p2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S7.SS1.p2.1.m1.1b"><ci id="S7.SS1.p2.1.m1.1.1.cmml" xref="S7.SS1.p2.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p2.1.m1.1c">n</annotation></semantics></math>P algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx18" title="" class="ltx_ref">Gao <span class="ltx_text ltx_font_italic">et al.</span>, 2003</a>]</cite>—hence the name for the
method, from the 8 corners of the bounding box, as illustrating in
Figure <a href="#S7.F7" title="Figure 7 ‣ 7 Modern Approaches to 3D Object Pose Estimation ‣ Recent Advances in 3D Object and Hand Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. Compared to the direct prediction of the pose, this
avoids the need for a meta-parameter to balance the translation and rotation
terms. It also tends to make network optimization easier. This representation
was used in some later works.</p>
</div>
<div id="S7.SS1.p3" class="ltx_para">
<p id="S7.SS1.p3.1" class="ltx_p">Since it is the first Deep Learning method for pose estimation we describe, we
will detail the loss function (see Section <a href="#S5" title="5 Machine Learning and Deep Learning ‣ Recent Advances in 3D Object and Hand Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>) used to train the
network. This loss function <math id="S7.SS1.p3.1.m1.1" class="ltx_Math" alttext="\mathcal{L}" display="inline"><semantics id="S7.SS1.p3.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S7.SS1.p3.1.m1.1.1" xref="S7.SS1.p3.1.m1.1.1.cmml">ℒ</mi><annotation-xml encoding="MathML-Content" id="S7.SS1.p3.1.m1.1b"><ci id="S7.SS1.p3.1.m1.1.1.cmml" xref="S7.SS1.p3.1.m1.1.1">ℒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p3.1.m1.1c">\mathcal{L}</annotation></semantics></math> is a least-squares error between the
predicted 2D points and the expected ones, and can be written as:</p>
<table id="S7.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S7.E4.m1.6" class="ltx_Math" alttext="\mathcal{L}(\Theta)=\frac{1}{8}\sum_{(W,{\bf p})\in\mathcal{T}}\&gt;\sum_{i}\|\text{Proj}_{\bf p}({\bf M}_{i})-F(W;\Theta)_{i}\|^{2}\&gt;," display="block"><semantics id="S7.E4.m1.6a"><mrow id="S7.E4.m1.6.6.1" xref="S7.E4.m1.6.6.1.1.cmml"><mrow id="S7.E4.m1.6.6.1.1" xref="S7.E4.m1.6.6.1.1.cmml"><mrow id="S7.E4.m1.6.6.1.1.3" xref="S7.E4.m1.6.6.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S7.E4.m1.6.6.1.1.3.2" xref="S7.E4.m1.6.6.1.1.3.2.cmml">ℒ</mi><mo lspace="0em" rspace="0em" id="S7.E4.m1.6.6.1.1.3.1" xref="S7.E4.m1.6.6.1.1.3.1.cmml">​</mo><mrow id="S7.E4.m1.6.6.1.1.3.3.2" xref="S7.E4.m1.6.6.1.1.3.cmml"><mo stretchy="false" id="S7.E4.m1.6.6.1.1.3.3.2.1" xref="S7.E4.m1.6.6.1.1.3.cmml">(</mo><mi mathvariant="normal" id="S7.E4.m1.3.3" xref="S7.E4.m1.3.3.cmml">Θ</mi><mo stretchy="false" id="S7.E4.m1.6.6.1.1.3.3.2.2" xref="S7.E4.m1.6.6.1.1.3.cmml">)</mo></mrow></mrow><mo id="S7.E4.m1.6.6.1.1.2" xref="S7.E4.m1.6.6.1.1.2.cmml">=</mo><mrow id="S7.E4.m1.6.6.1.1.1" xref="S7.E4.m1.6.6.1.1.1.cmml"><mfrac id="S7.E4.m1.6.6.1.1.1.3" xref="S7.E4.m1.6.6.1.1.1.3.cmml"><mn id="S7.E4.m1.6.6.1.1.1.3.2" xref="S7.E4.m1.6.6.1.1.1.3.2.cmml">1</mn><mn id="S7.E4.m1.6.6.1.1.1.3.3" xref="S7.E4.m1.6.6.1.1.1.3.3.cmml">8</mn></mfrac><mo lspace="0em" rspace="0em" id="S7.E4.m1.6.6.1.1.1.2" xref="S7.E4.m1.6.6.1.1.1.2.cmml">​</mo><mrow id="S7.E4.m1.6.6.1.1.1.1" xref="S7.E4.m1.6.6.1.1.1.1.cmml"><munder id="S7.E4.m1.6.6.1.1.1.1.2" xref="S7.E4.m1.6.6.1.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S7.E4.m1.6.6.1.1.1.1.2.2" xref="S7.E4.m1.6.6.1.1.1.1.2.2.cmml">∑</mo><mrow id="S7.E4.m1.2.2.2" xref="S7.E4.m1.2.2.2.cmml"><mrow id="S7.E4.m1.2.2.2.4.2" xref="S7.E4.m1.2.2.2.4.1.cmml"><mo stretchy="false" id="S7.E4.m1.2.2.2.4.2.1" xref="S7.E4.m1.2.2.2.4.1.cmml">(</mo><mi id="S7.E4.m1.1.1.1.1" xref="S7.E4.m1.1.1.1.1.cmml">W</mi><mo id="S7.E4.m1.2.2.2.4.2.2" xref="S7.E4.m1.2.2.2.4.1.cmml">,</mo><mi id="S7.E4.m1.2.2.2.2" xref="S7.E4.m1.2.2.2.2.cmml">𝐩</mi><mo stretchy="false" id="S7.E4.m1.2.2.2.4.2.3" xref="S7.E4.m1.2.2.2.4.1.cmml">)</mo></mrow><mo id="S7.E4.m1.2.2.2.3" xref="S7.E4.m1.2.2.2.3.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S7.E4.m1.2.2.2.5" xref="S7.E4.m1.2.2.2.5.cmml">𝒯</mi></mrow></munder><mrow id="S7.E4.m1.6.6.1.1.1.1.1" xref="S7.E4.m1.6.6.1.1.1.1.1.cmml"><munder id="S7.E4.m1.6.6.1.1.1.1.1.2" xref="S7.E4.m1.6.6.1.1.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S7.E4.m1.6.6.1.1.1.1.1.2.2" xref="S7.E4.m1.6.6.1.1.1.1.1.2.2.cmml">∑</mo><mi id="S7.E4.m1.6.6.1.1.1.1.1.2.3" xref="S7.E4.m1.6.6.1.1.1.1.1.2.3.cmml">i</mi></munder><msup id="S7.E4.m1.6.6.1.1.1.1.1.1" xref="S7.E4.m1.6.6.1.1.1.1.1.1.cmml"><mrow id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.2" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.cmml"><mrow id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.3" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.cmml"><mtext id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.2" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.2a.cmml">Proj</mtext><mi id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.3" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.3.cmml">𝐩</mi></msub><mo lspace="0em" rspace="0em" id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.1" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">𝐌</mi><mi id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.2" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.2.cmml">−</mo><mrow id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3.2" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3.1" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3.1.cmml">​</mo><msub id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3.3" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3.3.cmml"><mrow id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3.3.2.2" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3.3.2.1.cmml"><mo stretchy="false" id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3.3.2.2.1" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3.3.2.1.cmml">(</mo><mi id="S7.E4.m1.4.4" xref="S7.E4.m1.4.4.cmml">W</mi><mo id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3.3.2.2.2" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3.3.2.1.cmml">;</mo><mi mathvariant="normal" id="S7.E4.m1.5.5" xref="S7.E4.m1.5.5.cmml">Θ</mi><mo stretchy="false" id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3.3.2.2.3" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3.3.2.1.cmml">)</mo></mrow><mi id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3.3.3" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3.3.3.cmml">i</mi></msub></mrow></mrow><mo stretchy="false" id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.3" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S7.E4.m1.6.6.1.1.1.1.1.1.3" xref="S7.E4.m1.6.6.1.1.1.1.1.1.3.cmml">2</mn></msup></mrow></mrow></mrow></mrow><mo id="S7.E4.m1.6.6.1.2" xref="S7.E4.m1.6.6.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S7.E4.m1.6b"><apply id="S7.E4.m1.6.6.1.1.cmml" xref="S7.E4.m1.6.6.1"><eq id="S7.E4.m1.6.6.1.1.2.cmml" xref="S7.E4.m1.6.6.1.1.2"></eq><apply id="S7.E4.m1.6.6.1.1.3.cmml" xref="S7.E4.m1.6.6.1.1.3"><times id="S7.E4.m1.6.6.1.1.3.1.cmml" xref="S7.E4.m1.6.6.1.1.3.1"></times><ci id="S7.E4.m1.6.6.1.1.3.2.cmml" xref="S7.E4.m1.6.6.1.1.3.2">ℒ</ci><ci id="S7.E4.m1.3.3.cmml" xref="S7.E4.m1.3.3">Θ</ci></apply><apply id="S7.E4.m1.6.6.1.1.1.cmml" xref="S7.E4.m1.6.6.1.1.1"><times id="S7.E4.m1.6.6.1.1.1.2.cmml" xref="S7.E4.m1.6.6.1.1.1.2"></times><apply id="S7.E4.m1.6.6.1.1.1.3.cmml" xref="S7.E4.m1.6.6.1.1.1.3"><divide id="S7.E4.m1.6.6.1.1.1.3.1.cmml" xref="S7.E4.m1.6.6.1.1.1.3"></divide><cn type="integer" id="S7.E4.m1.6.6.1.1.1.3.2.cmml" xref="S7.E4.m1.6.6.1.1.1.3.2">1</cn><cn type="integer" id="S7.E4.m1.6.6.1.1.1.3.3.cmml" xref="S7.E4.m1.6.6.1.1.1.3.3">8</cn></apply><apply id="S7.E4.m1.6.6.1.1.1.1.cmml" xref="S7.E4.m1.6.6.1.1.1.1"><apply id="S7.E4.m1.6.6.1.1.1.1.2.cmml" xref="S7.E4.m1.6.6.1.1.1.1.2"><csymbol cd="ambiguous" id="S7.E4.m1.6.6.1.1.1.1.2.1.cmml" xref="S7.E4.m1.6.6.1.1.1.1.2">subscript</csymbol><sum id="S7.E4.m1.6.6.1.1.1.1.2.2.cmml" xref="S7.E4.m1.6.6.1.1.1.1.2.2"></sum><apply id="S7.E4.m1.2.2.2.cmml" xref="S7.E4.m1.2.2.2"><in id="S7.E4.m1.2.2.2.3.cmml" xref="S7.E4.m1.2.2.2.3"></in><interval closure="open" id="S7.E4.m1.2.2.2.4.1.cmml" xref="S7.E4.m1.2.2.2.4.2"><ci id="S7.E4.m1.1.1.1.1.cmml" xref="S7.E4.m1.1.1.1.1">𝑊</ci><ci id="S7.E4.m1.2.2.2.2.cmml" xref="S7.E4.m1.2.2.2.2">𝐩</ci></interval><ci id="S7.E4.m1.2.2.2.5.cmml" xref="S7.E4.m1.2.2.2.5">𝒯</ci></apply></apply><apply id="S7.E4.m1.6.6.1.1.1.1.1.cmml" xref="S7.E4.m1.6.6.1.1.1.1.1"><apply id="S7.E4.m1.6.6.1.1.1.1.1.2.cmml" xref="S7.E4.m1.6.6.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S7.E4.m1.6.6.1.1.1.1.1.2.1.cmml" xref="S7.E4.m1.6.6.1.1.1.1.1.2">subscript</csymbol><sum id="S7.E4.m1.6.6.1.1.1.1.1.2.2.cmml" xref="S7.E4.m1.6.6.1.1.1.1.1.2.2"></sum><ci id="S7.E4.m1.6.6.1.1.1.1.1.2.3.cmml" xref="S7.E4.m1.6.6.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S7.E4.m1.6.6.1.1.1.1.1.1.cmml" xref="S7.E4.m1.6.6.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S7.E4.m1.6.6.1.1.1.1.1.1.2.cmml" xref="S7.E4.m1.6.6.1.1.1.1.1.1">superscript</csymbol><apply id="S7.E4.m1.6.6.1.1.1.1.1.1.1.2.cmml" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S7.E4.m1.6.6.1.1.1.1.1.1.1.2.1.cmml" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.cmml" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1"><minus id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.2.cmml" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.2"></minus><apply id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.cmml" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1"><times id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2"></times><apply id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.2a.cmml" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.2"><mtext id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.2">Proj</mtext></ci><ci id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.3">𝐩</ci></apply><apply id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.1.1.2">𝐌</ci><ci id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply></apply><apply id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3.cmml" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3"><times id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3.1"></times><ci id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3.2">𝐹</ci><apply id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3.3">subscript</csymbol><list id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3.3.2.1.cmml" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3.3.2.2"><ci id="S7.E4.m1.4.4.cmml" xref="S7.E4.m1.4.4">𝑊</ci><ci id="S7.E4.m1.5.5.cmml" xref="S7.E4.m1.5.5">Θ</ci></list><ci id="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S7.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3.3.3">𝑖</ci></apply></apply></apply></apply><cn type="integer" id="S7.E4.m1.6.6.1.1.1.1.1.1.3.cmml" xref="S7.E4.m1.6.6.1.1.1.1.1.1.3">2</cn></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.E4.m1.6c">\mathcal{L}(\Theta)=\frac{1}{8}\sum_{(W,{\bf p})\in\mathcal{T}}\&gt;\sum_{i}\|\text{Proj}_{\bf p}({\bf M}_{i})-F(W;\Theta)_{i}\|^{2}\&gt;,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S7.SS1.p3.14" class="ltx_p">where <math id="S7.SS1.p3.2.m1.1" class="ltx_Math" alttext="F" display="inline"><semantics id="S7.SS1.p3.2.m1.1a"><mi id="S7.SS1.p3.2.m1.1.1" xref="S7.SS1.p3.2.m1.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S7.SS1.p3.2.m1.1b"><ci id="S7.SS1.p3.2.m1.1.1.cmml" xref="S7.SS1.p3.2.m1.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p3.2.m1.1c">F</annotation></semantics></math> denotes the trained network and <math id="S7.SS1.p3.3.m2.1" class="ltx_Math" alttext="\Theta" display="inline"><semantics id="S7.SS1.p3.3.m2.1a"><mi mathvariant="normal" id="S7.SS1.p3.3.m2.1.1" xref="S7.SS1.p3.3.m2.1.1.cmml">Θ</mi><annotation-xml encoding="MathML-Content" id="S7.SS1.p3.3.m2.1b"><ci id="S7.SS1.p3.3.m2.1.1.cmml" xref="S7.SS1.p3.3.m2.1.1">Θ</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p3.3.m2.1c">\Theta</annotation></semantics></math> its parameters. <math id="S7.SS1.p3.4.m3.1" class="ltx_Math" alttext="\mathcal{T}" display="inline"><semantics id="S7.SS1.p3.4.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S7.SS1.p3.4.m3.1.1" xref="S7.SS1.p3.4.m3.1.1.cmml">𝒯</mi><annotation-xml encoding="MathML-Content" id="S7.SS1.p3.4.m3.1b"><ci id="S7.SS1.p3.4.m3.1.1.cmml" xref="S7.SS1.p3.4.m3.1.1">𝒯</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p3.4.m3.1c">\mathcal{T}</annotation></semantics></math> is a
training set made of image windows <math id="S7.SS1.p3.5.m4.1" class="ltx_Math" alttext="W" display="inline"><semantics id="S7.SS1.p3.5.m4.1a"><mi id="S7.SS1.p3.5.m4.1.1" xref="S7.SS1.p3.5.m4.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S7.SS1.p3.5.m4.1b"><ci id="S7.SS1.p3.5.m4.1.1.cmml" xref="S7.SS1.p3.5.m4.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p3.5.m4.1c">W</annotation></semantics></math> containing a target object under a pose
<math id="S7.SS1.p3.6.m5.1" class="ltx_Math" alttext="{\bf p}" display="inline"><semantics id="S7.SS1.p3.6.m5.1a"><mi id="S7.SS1.p3.6.m5.1.1" xref="S7.SS1.p3.6.m5.1.1.cmml">𝐩</mi><annotation-xml encoding="MathML-Content" id="S7.SS1.p3.6.m5.1b"><ci id="S7.SS1.p3.6.m5.1.1.cmml" xref="S7.SS1.p3.6.m5.1.1">𝐩</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p3.6.m5.1c">{\bf p}</annotation></semantics></math>. The <math id="S7.SS1.p3.7.m6.1" class="ltx_Math" alttext="{\bf M}_{i}" display="inline"><semantics id="S7.SS1.p3.7.m6.1a"><msub id="S7.SS1.p3.7.m6.1.1" xref="S7.SS1.p3.7.m6.1.1.cmml"><mi id="S7.SS1.p3.7.m6.1.1.2" xref="S7.SS1.p3.7.m6.1.1.2.cmml">𝐌</mi><mi id="S7.SS1.p3.7.m6.1.1.3" xref="S7.SS1.p3.7.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S7.SS1.p3.7.m6.1b"><apply id="S7.SS1.p3.7.m6.1.1.cmml" xref="S7.SS1.p3.7.m6.1.1"><csymbol cd="ambiguous" id="S7.SS1.p3.7.m6.1.1.1.cmml" xref="S7.SS1.p3.7.m6.1.1">subscript</csymbol><ci id="S7.SS1.p3.7.m6.1.1.2.cmml" xref="S7.SS1.p3.7.m6.1.1.2">𝐌</ci><ci id="S7.SS1.p3.7.m6.1.1.3.cmml" xref="S7.SS1.p3.7.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p3.7.m6.1c">{\bf M}_{i}</annotation></semantics></math> are the 3D coordinates of the corners of the bounding box
of for this object, in the object coordinate system. <math id="S7.SS1.p3.8.m7.3" class="ltx_Math" alttext="\text{Proj}_{{\bf e},{\bf t}}({\bf M})" display="inline"><semantics id="S7.SS1.p3.8.m7.3a"><mrow id="S7.SS1.p3.8.m7.3.4" xref="S7.SS1.p3.8.m7.3.4.cmml"><msub id="S7.SS1.p3.8.m7.3.4.2" xref="S7.SS1.p3.8.m7.3.4.2.cmml"><mtext id="S7.SS1.p3.8.m7.3.4.2.2" xref="S7.SS1.p3.8.m7.3.4.2.2a.cmml">Proj</mtext><mrow id="S7.SS1.p3.8.m7.2.2.2.4" xref="S7.SS1.p3.8.m7.2.2.2.3.cmml"><mi id="S7.SS1.p3.8.m7.1.1.1.1" xref="S7.SS1.p3.8.m7.1.1.1.1.cmml">𝐞</mi><mo id="S7.SS1.p3.8.m7.2.2.2.4.1" xref="S7.SS1.p3.8.m7.2.2.2.3.cmml">,</mo><mi id="S7.SS1.p3.8.m7.2.2.2.2" xref="S7.SS1.p3.8.m7.2.2.2.2.cmml">𝐭</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S7.SS1.p3.8.m7.3.4.1" xref="S7.SS1.p3.8.m7.3.4.1.cmml">​</mo><mrow id="S7.SS1.p3.8.m7.3.4.3.2" xref="S7.SS1.p3.8.m7.3.4.cmml"><mo stretchy="false" id="S7.SS1.p3.8.m7.3.4.3.2.1" xref="S7.SS1.p3.8.m7.3.4.cmml">(</mo><mi id="S7.SS1.p3.8.m7.3.3" xref="S7.SS1.p3.8.m7.3.3.cmml">𝐌</mi><mo stretchy="false" id="S7.SS1.p3.8.m7.3.4.3.2.2" xref="S7.SS1.p3.8.m7.3.4.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.p3.8.m7.3b"><apply id="S7.SS1.p3.8.m7.3.4.cmml" xref="S7.SS1.p3.8.m7.3.4"><times id="S7.SS1.p3.8.m7.3.4.1.cmml" xref="S7.SS1.p3.8.m7.3.4.1"></times><apply id="S7.SS1.p3.8.m7.3.4.2.cmml" xref="S7.SS1.p3.8.m7.3.4.2"><csymbol cd="ambiguous" id="S7.SS1.p3.8.m7.3.4.2.1.cmml" xref="S7.SS1.p3.8.m7.3.4.2">subscript</csymbol><ci id="S7.SS1.p3.8.m7.3.4.2.2a.cmml" xref="S7.SS1.p3.8.m7.3.4.2.2"><mtext id="S7.SS1.p3.8.m7.3.4.2.2.cmml" xref="S7.SS1.p3.8.m7.3.4.2.2">Proj</mtext></ci><list id="S7.SS1.p3.8.m7.2.2.2.3.cmml" xref="S7.SS1.p3.8.m7.2.2.2.4"><ci id="S7.SS1.p3.8.m7.1.1.1.1.cmml" xref="S7.SS1.p3.8.m7.1.1.1.1">𝐞</ci><ci id="S7.SS1.p3.8.m7.2.2.2.2.cmml" xref="S7.SS1.p3.8.m7.2.2.2.2">𝐭</ci></list></apply><ci id="S7.SS1.p3.8.m7.3.3.cmml" xref="S7.SS1.p3.8.m7.3.3">𝐌</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p3.8.m7.3c">\text{Proj}_{{\bf e},{\bf t}}({\bf M})</annotation></semantics></math>
projects the 3D point <math id="S7.SS1.p3.9.m8.1" class="ltx_Math" alttext="{\bf M}" display="inline"><semantics id="S7.SS1.p3.9.m8.1a"><mi id="S7.SS1.p3.9.m8.1.1" xref="S7.SS1.p3.9.m8.1.1.cmml">𝐌</mi><annotation-xml encoding="MathML-Content" id="S7.SS1.p3.9.m8.1b"><ci id="S7.SS1.p3.9.m8.1.1.cmml" xref="S7.SS1.p3.9.m8.1.1">𝐌</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p3.9.m8.1c">{\bf M}</annotation></semantics></math> on the image from the pose defined by <math id="S7.SS1.p3.10.m9.1" class="ltx_Math" alttext="{\bf e}" display="inline"><semantics id="S7.SS1.p3.10.m9.1a"><mi id="S7.SS1.p3.10.m9.1.1" xref="S7.SS1.p3.10.m9.1.1.cmml">𝐞</mi><annotation-xml encoding="MathML-Content" id="S7.SS1.p3.10.m9.1b"><ci id="S7.SS1.p3.10.m9.1.1.cmml" xref="S7.SS1.p3.10.m9.1.1">𝐞</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p3.10.m9.1c">{\bf e}</annotation></semantics></math> and
<math id="S7.SS1.p3.11.m10.1" class="ltx_Math" alttext="{\bf t}" display="inline"><semantics id="S7.SS1.p3.11.m10.1a"><mi id="S7.SS1.p3.11.m10.1.1" xref="S7.SS1.p3.11.m10.1.1.cmml">𝐭</mi><annotation-xml encoding="MathML-Content" id="S7.SS1.p3.11.m10.1b"><ci id="S7.SS1.p3.11.m10.1.1.cmml" xref="S7.SS1.p3.11.m10.1.1">𝐭</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p3.11.m10.1c">{\bf t}</annotation></semantics></math>. <math id="S7.SS1.p3.12.m11.2" class="ltx_Math" alttext="F(W;\Theta)_{i}" display="inline"><semantics id="S7.SS1.p3.12.m11.2a"><mrow id="S7.SS1.p3.12.m11.2.3" xref="S7.SS1.p3.12.m11.2.3.cmml"><mi id="S7.SS1.p3.12.m11.2.3.2" xref="S7.SS1.p3.12.m11.2.3.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S7.SS1.p3.12.m11.2.3.1" xref="S7.SS1.p3.12.m11.2.3.1.cmml">​</mo><msub id="S7.SS1.p3.12.m11.2.3.3" xref="S7.SS1.p3.12.m11.2.3.3.cmml"><mrow id="S7.SS1.p3.12.m11.2.3.3.2.2" xref="S7.SS1.p3.12.m11.2.3.3.2.1.cmml"><mo stretchy="false" id="S7.SS1.p3.12.m11.2.3.3.2.2.1" xref="S7.SS1.p3.12.m11.2.3.3.2.1.cmml">(</mo><mi id="S7.SS1.p3.12.m11.1.1" xref="S7.SS1.p3.12.m11.1.1.cmml">W</mi><mo id="S7.SS1.p3.12.m11.2.3.3.2.2.2" xref="S7.SS1.p3.12.m11.2.3.3.2.1.cmml">;</mo><mi mathvariant="normal" id="S7.SS1.p3.12.m11.2.2" xref="S7.SS1.p3.12.m11.2.2.cmml">Θ</mi><mo stretchy="false" id="S7.SS1.p3.12.m11.2.3.3.2.2.3" xref="S7.SS1.p3.12.m11.2.3.3.2.1.cmml">)</mo></mrow><mi id="S7.SS1.p3.12.m11.2.3.3.3" xref="S7.SS1.p3.12.m11.2.3.3.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.p3.12.m11.2b"><apply id="S7.SS1.p3.12.m11.2.3.cmml" xref="S7.SS1.p3.12.m11.2.3"><times id="S7.SS1.p3.12.m11.2.3.1.cmml" xref="S7.SS1.p3.12.m11.2.3.1"></times><ci id="S7.SS1.p3.12.m11.2.3.2.cmml" xref="S7.SS1.p3.12.m11.2.3.2">𝐹</ci><apply id="S7.SS1.p3.12.m11.2.3.3.cmml" xref="S7.SS1.p3.12.m11.2.3.3"><csymbol cd="ambiguous" id="S7.SS1.p3.12.m11.2.3.3.1.cmml" xref="S7.SS1.p3.12.m11.2.3.3">subscript</csymbol><list id="S7.SS1.p3.12.m11.2.3.3.2.1.cmml" xref="S7.SS1.p3.12.m11.2.3.3.2.2"><ci id="S7.SS1.p3.12.m11.1.1.cmml" xref="S7.SS1.p3.12.m11.1.1">𝑊</ci><ci id="S7.SS1.p3.12.m11.2.2.cmml" xref="S7.SS1.p3.12.m11.2.2">Θ</ci></list><ci id="S7.SS1.p3.12.m11.2.3.3.3.cmml" xref="S7.SS1.p3.12.m11.2.3.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p3.12.m11.2c">F(W;\Theta)_{i}</annotation></semantics></math> returns the two components of the output of <math id="S7.SS1.p3.13.m12.1" class="ltx_Math" alttext="F" display="inline"><semantics id="S7.SS1.p3.13.m12.1a"><mi id="S7.SS1.p3.13.m12.1.1" xref="S7.SS1.p3.13.m12.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S7.SS1.p3.13.m12.1b"><ci id="S7.SS1.p3.13.m12.1.1.cmml" xref="S7.SS1.p3.13.m12.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p3.13.m12.1c">F</annotation></semantics></math>
corresponding to the predicted 2D coordinates of the <math id="S7.SS1.p3.14.m13.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S7.SS1.p3.14.m13.1a"><mi id="S7.SS1.p3.14.m13.1.1" xref="S7.SS1.p3.14.m13.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S7.SS1.p3.14.m13.1b"><ci id="S7.SS1.p3.14.m13.1.1.cmml" xref="S7.SS1.p3.14.m13.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p3.14.m13.1c">i</annotation></semantics></math>-th corner.</p>
</div>
<section id="S7.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">The problem of symmetrical and “almost symmetrical” objects. </h4>

<div id="S7.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S7.SS1.SSS0.Px1.p1.1" class="ltx_p">Predicting the 3D pose of objects a standard least-squares problem, using a
standard representation of the pose or point reprojections as in BB8, yields to
large errors on symmetrical objects, such as many objects in the T-Less
dataset (Figure <a href="#S6.F5" title="Figure 5 ‣ 6.1 Datasets for Object Pose Estimation ‣ 6 Datasets ‣ Recent Advances in 3D Object and Hand Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>(d)). This dataset is made of
manufactured objects that are not only similar to each other, but also have one
axis of rotational symmetry. Some objects are not perfectly symmetrical but only
because of small details, like a screw.</p>
</div>
<div id="S7.SS1.SSS0.Px1.p2" class="ltx_para">
<p id="S7.SS1.SSS0.Px1.p2.1" class="ltx_p">The approach described above fails on these objects because it tries to learn a
mapping from the image space to the pose space. Since two images of a
symmetrical object under two different poses look identical, the image-pose
correspondence is in fact a one-to-many relationship.</p>
</div>
<div id="S7.SS1.SSS0.Px1.p3" class="ltx_para">
<p id="S7.SS1.SSS0.Px1.p3.1" class="ltx_p">For objects that are perfectly symmetrical, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx73" title="" class="ltx_ref">Rad and Lepetit, 2017</a>]</cite> proposed a solution
that we will not describe here, as simpler solutions have been proposed since.
Objects that are “almost symmetrical” can also disturb pose prediction, as the
mapping from the image to the 3D pose is difficult to learn even though this is
a one-to-one mapping. Most recent methods ignore the problem and consider that
these objects are actually perfectly symmetrical and consider a pose recovered
up to the symmetries as correct. For such object, BB8 proposes to first consider
these objects as symmetrical, and then to train a classifier (also a Deep
Network) to predict which pose is actually the correct one. For example, for an
object with a rotational symmetry of <math id="S7.SS1.SSS0.Px1.p3.1.m1.1" class="ltx_Math" alttext="180\degree" display="inline"><semantics id="S7.SS1.SSS0.Px1.p3.1.m1.1a"><mrow id="S7.SS1.SSS0.Px1.p3.1.m1.1.1" xref="S7.SS1.SSS0.Px1.p3.1.m1.1.1.cmml"><mn id="S7.SS1.SSS0.Px1.p3.1.m1.1.1.2" xref="S7.SS1.SSS0.Px1.p3.1.m1.1.1.2.cmml">180</mn><mo lspace="0em" rspace="0em" id="S7.SS1.SSS0.Px1.p3.1.m1.1.1.1" xref="S7.SS1.SSS0.Px1.p3.1.m1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S7.SS1.SSS0.Px1.p3.1.m1.1.1.3" xref="S7.SS1.SSS0.Px1.p3.1.m1.1.1.3.cmml">°</mi></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.SSS0.Px1.p3.1.m1.1b"><apply id="S7.SS1.SSS0.Px1.p3.1.m1.1.1.cmml" xref="S7.SS1.SSS0.Px1.p3.1.m1.1.1"><times id="S7.SS1.SSS0.Px1.p3.1.m1.1.1.1.cmml" xref="S7.SS1.SSS0.Px1.p3.1.m1.1.1.1"></times><cn type="integer" id="S7.SS1.SSS0.Px1.p3.1.m1.1.1.2.cmml" xref="S7.SS1.SSS0.Px1.p3.1.m1.1.1.2">180</cn><ci id="S7.SS1.SSS0.Px1.p3.1.m1.1.1.3.cmml" xref="S7.SS1.SSS0.Px1.p3.1.m1.1.1.3">°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.SSS0.Px1.p3.1.m1.1c">180\degree</annotation></semantics></math>, there are two possible poses
in general, and the classifier has to decide between 2 classes. This is a much
simpler problem than predicting 6 degrees-of-freedom for the pose, and the
classifier can focus on the small details that break the symmetry.</p>
</div>
<figure id="S7.F8" class="ltx_figure"><img src="/html/2006.05927/assets/images/bb8_ref.png" id="S7.F8.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="190" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 8: </span> Pose refinement in BB8. Given a first
pose estimate, shown by the blue bounding box, BB8 generates a color
rendering of the object. A network is trained to predict an update for the
object pose given the input image and this rendering, to get a better
estimate shown by the red bounding box. This process can be iterated.</figcaption>
</figure>
</section>
<section id="S7.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Refinement step. </h4>

<div id="S7.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S7.SS1.SSS0.Px2.p1.1" class="ltx_p">The method described above provides an estimate for the 3D pose of an object
using only feedforward computation by a network from the input image to the 3D
pose. It is relatively natural to aim at refining this estimate, which is a step
also present in many other methods. In BB8, this is performed using a method
similar to the one proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx61" title="" class="ltx_ref">Oberweger <span class="ltx_text ltx_font_italic">et al.</span>, 2015</a>]</cite> for hand detection in depth
images. A network is trained to improve the prediction of the 2D projections by
comparing the input image and a rendering of the object for the initial pose
estimate, as illustrated in Figure <a href="#S7.F8" title="Figure 8 ‣ The problem of symmetrical and “almost symmetrical” objects. ‣ 7.1 BB8 ‣ 7 Modern Approaches to 3D Object Pose Estimation ‣ Recent Advances in 3D Object and Hand Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. Such refinement can
be iterated multiple times to improve the pose estimate.</p>
</div>
<div id="S7.SS1.SSS0.Px2.p2" class="ltx_para">
<p id="S7.SS1.SSS0.Px2.p2.1" class="ltx_p">One may wonder why the refinement step, in BB8 but also in more recent works
such as DeepIM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx107" title="" class="ltx_ref">Xiang <span class="ltx_text ltx_font_italic">et al.</span>, 2018b</a>]</cite> or DPOD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx110" title="" class="ltx_ref">Zakharov <span class="ltx_text ltx_font_italic">et al.</span>, 2019</a>]</cite>, can improve pose
accuracy while it is (apparently) trained with the same data as the initial pose
prediction. This can be understood by looking more closely at how the network
predicting the update is trained. The input part of one training sample is made
of a regular input image, plus an rendered image for a pose close to the pose
for input image, and the output part is the difference between the two poses.
In practice, the pose for the rendered image is taken as the ground truth pose
for the real image plus some random noise. In other words, from one sample of
the original dataset trained to predict the network providing the first
estimate, it is possible to generate a virtually infinite number of samples for
training the refinement network, by simply adding noise to the ground truth
pose. The refinement network is thus trained with much more samples than the
first network.</p>
</div>
</section>
<section id="S7.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Data augmentation. </h4>

<div id="S7.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S7.SS1.SSS0.Px3.p1.1" class="ltx_p">Data augmentation, <em id="S7.SS1.SSS0.Px3.p1.1.1" class="ltx_emph ltx_font_italic">i.e.</em> generating additional training images from the
available ones, is often critical in Deep Learning. In the case of BB8, the
objects’ silhouettes are extracted from the original training images, which can
be done using the ground truth poses and the objects’ 3D models. The background
is replaced by a patch extracted from a randomly picked image from the ImageNet
dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx80" title="" class="ltx_ref">Russakovsky <span class="ltx_text ltx_font_italic">et al.</span>, 2015</a>]</cite>. Note that this procedure removes context by
randomly replacing the surroundings of the objects. Some information is thus
lost, as context could be useful for pose estimation.</p>
</div>
</section>
</section>
<section id="S7.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2 </span>SSD-6D</h3>

<div id="S7.SS2.p1" class="ltx_para">
<p id="S7.SS2.p1.1" class="ltx_p">BB8 relies on two separate networks to first detect the target objects in 2D and
then predict their 3D poses, plus a third one if refinement is performed.
Instead, as shown in Figure <a href="#S7.F9" title="Figure 9 ‣ 7.2 SSD-6D ‣ 7 Modern Approaches to 3D Object Pose Estimation ‣ Recent Advances in 3D Object and Hand Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>(a), SSD-6D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx44" title="" class="ltx_ref">Kehl <span class="ltx_text ltx_font_italic">et al.</span>, 2017</a>]</cite>
extends a deep architecture (the SSD architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx52" title="" class="ltx_ref">Liu <span class="ltx_text ltx_font_italic">et al.</span>, 2016</a>]</cite>) developed for 2D
object detection to 3D pose estimation (referred in the paper as 6D estimation).
SSD had already been extended to pose estimation in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx71" title="" class="ltx_ref">Poirson <span class="ltx_text ltx_font_italic">et al.</span>, 2016</a>]</cite>, but SS6-6D
performs full 3D pose prediction.</p>
</div>
<figure id="S7.F9" class="ltx_figure">
<table id="S7.F9.2.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S7.F9.2.2.2" class="ltx_tr">
<td id="S7.F9.1.1.1.1" class="ltx_td ltx_align_center"><img src="/html/2006.05927/assets/images/ssd6d.png" id="S7.F9.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="291" height="138" alt="Refer to caption"></td>
<td id="S7.F9.2.2.2.2" class="ltx_td ltx_align_center"><img src="/html/2006.05927/assets/images/yolo6d.png" id="S7.F9.2.2.2.2.g1" class="ltx_graphics ltx_img_landscape" width="243" height="138" alt="Refer to caption"></td>
</tr>
<tr id="S7.F9.2.2.3.1" class="ltx_tr">
<td id="S7.F9.2.2.3.1.1" class="ltx_td ltx_align_center">(a)</td>
<td id="S7.F9.2.2.3.1.2" class="ltx_td ltx_align_center">(b)</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 9: </span> (a) The architecture of SSD, extended by SSD-6D to
predict 3D poses. For each bounding box corresponding to a detected object,
a discretized 3D pose is also predicted by the network. Image
from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx44" title="" class="ltx_ref">Kehl <span class="ltx_text ltx_font_italic">et al.</span>, 2017</a>]</cite>. (b) The architecture of YOLO-6D. Image
from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx93" title="" class="ltx_ref">Tekin <span class="ltx_text ltx_font_italic">et al.</span>, 2018</a>]</cite>.</figcaption>
</figure>
<div id="S7.SS2.p2" class="ltx_para">
<p id="S7.SS2.p2.1" class="ltx_p">This prediction is done by first discretizing the pose space. Each discretized
pose is considered as a class, to turn the pose prediction into a classification
problem rather than a regression one, as this was performing better in the
authors’ experience. This discretization was done on the 3D pose decomposition
into direction of view over a half-sphere and in-plane rotation. The 3D
translation can be computed from the 2D bounding box. To deal with symmetrical
objects, views on the half-sphere corresponding to identical appearance for the
object are merged into the same class.</p>
</div>
<div id="S7.SS2.p3" class="ltx_para">
<p id="S7.SS2.p3.1" class="ltx_p">A single network is therefore trained to perform both 2D object detection and 3D
pose prediction, using a single loss function made of a weighted sum of
different terms. The weights are hyperparameters and have to be tuned, which can
be difficult. However, having a single network is an elegant solution, and more
importantly for practical applications, this allows to save computation times:
Image feature extraction, the slowest part of the network, is performed only
once even if it is used to predict the 2D bounding boxes and the 3D pose
estimation.</p>
</div>
<div id="S7.SS2.p4" class="ltx_para">
<p id="S7.SS2.p4.1" class="ltx_p">A refinement procedure is then run on each object detection to improve the pose
estimate predicted by the classifier. SSD-6D relies on a method inspired by an
early approach to 3D object tracking based on edges <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">Drummond and Cipolla, 2002</a>]</cite>. Data
augmentation was done by rendering synthetic views of the objects using their 3D
models over images from COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx51" title="" class="ltx_ref">Lin <span class="ltx_text ltx_font_italic">et al.</span>, 2014</a>]</cite>, which helps but only to some extend
as the differences between the real and synthetic images (the “domain gap”)
remain high.</p>
</div>
</section>
<section id="S7.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3 </span>YOLO-6D</h3>

<div id="S7.SS3.p1" class="ltx_para">
<p id="S7.SS3.p1.1" class="ltx_p">The method proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx93" title="" class="ltx_ref">Tekin <span class="ltx_text ltx_font_italic">et al.</span>, 2018</a>]</cite>, sometimes referred as YOLO-6D, is
relatively similar to SSD-6D, but makes different choices that makes it faster
and more accurate. As shown in Figure <a href="#S7.F9" title="Figure 9 ‣ 7.2 SSD-6D ‣ 7 Modern Approaches to 3D Object Pose Estimation ‣ Recent Advances in 3D Object and Hand Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>(b), it relies on
the YOLO architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx76" title="" class="ltx_ref">Redmon <span class="ltx_text ltx_font_italic">et al.</span>, 2016</a>, <a href="#bib.bibx75" title="" class="ltx_ref">Redmon and Farhadi, 2017</a>]</cite> for 2D object detection, and
predicts the 3D object poses in a form similar to the one of BB8. The authors
report that training the network using this pose representation was much simpler
than when using quaternions to represent the 3D rotation. As YOLO is much
faster and as they do not discretize the pose space, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx93" title="" class="ltx_ref">Tekin <span class="ltx_text ltx_font_italic">et al.</span>, 2018</a>]</cite> also reports
much better performance than SSD-6D in terms of both computation times and
accuracy.</p>
</div>
</section>
<section id="S7.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.4 </span>PoseCNN</h3>

<figure id="S7.F10" class="ltx_figure"><img src="/html/2006.05927/assets/images/posecnn.png" id="S7.F10.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="281" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 10: </span> The architecture of PoseCNN. Image
from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx107" title="" class="ltx_ref">Xiang <span class="ltx_text ltx_font_italic">et al.</span>, 2018b</a>]</cite>.</figcaption>
</figure>
<div id="S7.SS4.p1" class="ltx_para">
<p id="S7.SS4.p1.1" class="ltx_p">PoseCNN is a method proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx107" title="" class="ltx_ref">Xiang <span class="ltx_text ltx_font_italic">et al.</span>, 2018b</a>]</cite>. It relies on a relatively
complex architecture, based on the idea of decoupling the 3D pose estimation
task into different sub-tasks. As shown in Figure <a href="#S7.F10" title="Figure 10 ‣ 7.4 PoseCNN ‣ 7 Modern Approaches to 3D Object Pose Estimation ‣ Recent Advances in 3D Object and Hand Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>, this
architecture predicts for each pixel in the input image 1) the object label, 2)
a unit vector towards the 2D object center, and 3) the 3D distance between the
object center and the camera center, plus 4) 2D bounding boxes for the objects
and 5) a 3D rotation for each bounding box in the form of a quaternion. From
1), 2), and 3), it is possible to compute the 3D translation vector for each
visible object, which is combined with the 3D rotation to obtain the full 3D
pose for each visible object.</p>
</div>
<div id="S7.SS4.p2" class="ltx_para">
<p id="S7.SS4.p2.1" class="ltx_p">Maybe the most interesting contribution of PoseCNN is the loss function. It
uses the ADD metric as the loss (see Section <a href="#S6.SS4" title="6.4 Metrics ‣ 6 Datasets ‣ Recent Advances in 3D Object and Hand Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.4</span></a>), and even more
interestingly, the paper shows that the ADD-S metric, used to evaluate the pose
accuracy for symmetrical objects (see Section <a href="#S6.SS1" title="6.1 Datasets for Object Pose Estimation ‣ 6 Datasets ‣ Recent Advances in 3D Object and Hand Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.1</span></a>), can be
used as a loss function to deal with symmetrical objects. The ADI metric makes
use of the <math id="S7.SS4.p2.1.m1.1" class="ltx_Math" alttext="\min" display="inline"><semantics id="S7.SS4.p2.1.m1.1a"><mi id="S7.SS4.p2.1.m1.1.1" xref="S7.SS4.p2.1.m1.1.1.cmml">min</mi><annotation-xml encoding="MathML-Content" id="S7.SS4.p2.1.m1.1b"><min id="S7.SS4.p2.1.m1.1.1.cmml" xref="S7.SS4.p2.1.m1.1.1"></min></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.p2.1.m1.1c">\min</annotation></semantics></math> operator, which makes it maybe an unconventional loss
function, but it is still differentiable and can be used to train a network.
This results in an elegant and efficient way of handling object symmetries.</p>
</div>
<div id="S7.SS4.p3" class="ltx_para">
<p id="S7.SS4.p3.1" class="ltx_p">Another contribution of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx107" title="" class="ltx_ref">Xiang <span class="ltx_text ltx_font_italic">et al.</span>, 2018b</a>]</cite> is the introduction of the YCB-Video
dataset (see Section <a href="#S6.SS1" title="6.1 Datasets for Object Pose Estimation ‣ 6 Datasets ‣ Recent Advances in 3D Object and Hand Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.1</span></a>).</p>
</div>
</section>
<section id="S7.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.5 </span>DeepIM</h3>

<figure id="S7.F11" class="ltx_figure"><img src="/html/2006.05927/assets/images/deepim.png" id="S7.F11.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="195" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 11: </span> The DeepIM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx107" title="" class="ltx_ref">Xiang <span class="ltx_text ltx_font_italic">et al.</span>, 2018b</a>]</cite> refinement
architecture. It is similar to the one of
BB8 (Figure <a href="#S7.F8" title="Figure 8 ‣ The problem of symmetrical and “almost symmetrical” objects. ‣ 7.1 BB8 ‣ 7 Modern Approaches to 3D Object Pose Estimation ‣ Recent Advances in 3D Object and Hand Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>) but predicts a pose update in a
carefully chosen coordinate system for better generalization. </figcaption>
</figure>
<div id="S7.SS5.p1" class="ltx_para">
<p id="S7.SS5.p1.1" class="ltx_p">DeepIM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx50" title="" class="ltx_ref">Li <span class="ltx_text ltx_font_italic">et al.</span>, 2018</a>]</cite> proposes a refinement step that resembles the one in BB8 (see
the discussion on refinement steps in Section <a href="#S7.SS1" title="7.1 BB8 ‣ 7 Modern Approaches to 3D Object Pose Estimation ‣ Recent Advances in 3D Object and Hand Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7.1</span></a>), but
with several main differences. As BB8, it trains a network to compare the input
image with a rendering of an object that has already been detected, and for
which a pose estimate is already available. The network outputs an update for
the 3D object pose that will improve the pose estimate, and this process can be
iterated until convergence. The first difference with BB8 is that the rendered
image has a high-resolution and the input image is centered on the object and
upscaled to the same resolution. This allows a higher precision when predicting
the pose update. The loss function also includes terms to make the network
output the flow and the object mask, to introduce regularization.</p>
</div>
<div id="S7.SS5.p2" class="ltx_para">
<p id="S7.SS5.p2.1" class="ltx_p">The second difference is more fundamental, as it introduces a coordinate system
well suited to define the rotation update that must be predicted by the network.
The authors first remark that predicting the rotation in the camera coordinate
system because this rotation would also <em id="S7.SS5.p2.1.1" class="ltx_emph ltx_font_italic">translate</em> the object. They thus
set the center of rotation to the object center. For the axes of the coordinate
system, the authors remark that using those of the object’s 3D model is not a
good option, as they are arbitrary and this would force the network to learn
them for each object. They therefore propose to use the axes of the camera
coordinate system, which makes the network generalize much better. The
translation update is predicted as a 2D translation on the image plane, plus a
delta along the z axis of the camera in a log-scale. To learn this update,
DeepIM uses the same loss function as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx107" title="" class="ltx_ref">Xiang <span class="ltx_text ltx_font_italic">et al.</span>, 2018b</a>]</cite> (ADD, or ADD-S for
symmetrical objects). This approach is even shown to generalize to unseen
objects, but this is admittedly demonstrated in the paper only on very simple
object renderings.</p>
</div>
</section>
<section id="S7.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.6 </span>Augmented Autoencoders</h3>

<figure id="S7.F12" class="ltx_figure"><img src="/html/2006.05927/assets/images/augmented_autoencoders.png" id="S7.F12.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="242" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 12: </span> Autoencoders trained to learn an
image embedding. This embedding is robust to nuisances such as background and
illumination changes, and the possible rotations for the object in the input
image can be retrieved based on this embedding. Image from
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx88" title="" class="ltx_ref">Sundermeyer <span class="ltx_text ltx_font_italic">et al.</span>, 2019</a>]</cite>.</figcaption>
</figure>
<div id="S7.SS6.p1" class="ltx_para">
<p id="S7.SS6.p1.1" class="ltx_p">The most interesting aspect of the Augmented Autoencoders method proposed
in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx88" title="" class="ltx_ref">Sundermeyer <span class="ltx_text ltx_font_italic">et al.</span>, 2019</a>]</cite> is the way it deals with symmetrical objects. It
proposes to first learn an embedding that can be computed from an image of the
object. This embedding should be robust to imaging artefacts (illumination,
background, etc.) and depends only on the object’s appearance: The embeddings
for two images of the object under ambiguous rotations should thus be the same.
At run-time, the embedding for an input image can then be mapped to all the
possible rotations. This is done efficiently by creating a codebook offline, by
sampling views around the target objects, and associating the embeddings of
these views to the corresponding rotations. The translation can be recovered
from the object bounding box’ 2D location and scale.</p>
</div>
<div id="S7.SS6.p2" class="ltx_para">
<p id="S7.SS6.p2.1" class="ltx_p">To learn to compute the embeddings, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx88" title="" class="ltx_ref">Sundermeyer <span class="ltx_text ltx_font_italic">et al.</span>, 2019</a>]</cite> relies on an
autoencoder architecture, shown in Figure <a href="#S7.F12" title="Figure 12 ‣ 7.6 Augmented Autoencoders ‣ 7 Modern Approaches to 3D Object Pose Estimation ‣ Recent Advances in 3D Object and Hand Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>. An
Encoder network predicts an embedding from an image of an object with different
image nuisances; a Decoder network takes the predicted embedding as input and
should generate the image of the object under the same rotation but without the
nuisances. The encoder and decoder are trained together on synthetic images of
the objects, to which nuisances are added. The loss function encourages the
composition of the two networks to output the original synthetic images, despite
using the images with nuisances as input. Another motivation for this
autoencoder architecture is to learn to be robust to nuisances, even though a
more standard approach outputting the 3D pose would probably do the same when
trained on the same images.</p>
</div>
</section>
<section id="S7.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.7 </span>Robustness to Partial Occlusions: <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx63" title="" class="ltx_ref">Oberweger <span class="ltx_text ltx_font_italic">et al.</span>, 2018</a>]</cite>,
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx41" title="" class="ltx_ref">Hu <span class="ltx_text ltx_font_italic">et al.</span>, 2019</a>]</cite>, PVNet</h3>

<figure id="S7.F13" class="ltx_figure"><img src="/html/2006.05927/assets/images/segmentation_hu.png" id="S7.F13.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="197" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 13: </span> Being robust to large occlusions. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx41" title="" class="ltx_ref">Hu <span class="ltx_text ltx_font_italic">et al.</span>, 2019</a>]</cite> makes
predictions for the 2D reprojections of the objects’ 3D bounding boxes from
many image locations. Image locations that are not occluded will result in
good predictions, the predictions made from occluded image locations can be
filtered out using a robust estimation of the pose. Image
from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx41" title="" class="ltx_ref">Hu <span class="ltx_text ltx_font_italic">et al.</span>, 2019</a>]</cite>.</figcaption>
</figure>
<div id="S7.SS7.p1" class="ltx_para">
<p id="S7.SS7.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx63" title="" class="ltx_ref">Oberweger <span class="ltx_text ltx_font_italic">et al.</span>, 2018</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx41" title="" class="ltx_ref">Hu <span class="ltx_text ltx_font_italic">et al.</span>, 2019</a>]</cite>, and PVNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx68" title="" class="ltx_ref">Peng <span class="ltx_text ltx_font_italic">et al.</span>, 2019</a>]</cite> developed almost in
parallel similar methods that provide accurate 3D poses even under large
partial occlusions. Indeed, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx63" title="" class="ltx_ref">Oberweger <span class="ltx_text ltx_font_italic">et al.</span>, 2018</a>]</cite> shows that Deep Networks can be
robust to partial occlusions when predicting a pose from a image containing the
target object when trained on examples with occlusions, but only to some extend.
To become more robust and more accurate under large occlusions,
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx63" title="" class="ltx_ref">Oberweger <span class="ltx_text ltx_font_italic">et al.</span>, 2018</a>, <a href="#bib.bibx41" title="" class="ltx_ref">Hu <span class="ltx_text ltx_font_italic">et al.</span>, 2019</a>, <a href="#bib.bibx68" title="" class="ltx_ref">Peng <span class="ltx_text ltx_font_italic">et al.</span>, 2019</a>]</cite> proposed to predict the 3D pose in the form of
the 2D reprojections of some 3D points as in BB8, but by combining multiple
predictions, where each prediction is performed from different local image
information. As shown in Figure <a href="#S7.F13" title="Figure 13 ‣ 7.7 Robustness to Partial Occlusions: [Oberweger et al., 2018], [Hu et al., 2019], PVNet ‣ 7 Modern Approaches to 3D Object Pose Estimation ‣ Recent Advances in 3D Object and Hand Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>, the key idea is that
local image information that is not disturbed by occlusions will result into
good predictions; local image information disturbed by occlusions will predict
erroneous reprojections, but these can be filtered out with a robust estimation.</p>
</div>
<div id="S7.SS7.p2" class="ltx_para">
<p id="S7.SS7.p2.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx63" title="" class="ltx_ref">Oberweger <span class="ltx_text ltx_font_italic">et al.</span>, 2018</a>]</cite> predicts the 2D reprojections in the form of heatmaps, to
handle the ambiguities of mapping between image locations and the reprojections,
as many image locations can look the same. However, this makes predictions
relatively slow. Instead, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx41" title="" class="ltx_ref">Hu <span class="ltx_text ltx_font_italic">et al.</span>, 2019</a>]</cite> predicts a single 2D displacement between
the image location and each 2D reprojection. These results in many possible
reprojections, some noisy, but the correct 3D pose can be retrieved using
RANSAC. PVNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx68" title="" class="ltx_ref">Peng <span class="ltx_text ltx_font_italic">et al.</span>, 2019</a>]</cite> chose to predict the directions towards the
reprojections, rather than a full 2D displacement, but relies on a similar
RANSAC procedure to estimate the final 3D pose. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx41" title="" class="ltx_ref">Hu <span class="ltx_text ltx_font_italic">et al.</span>, 2019</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx68" title="" class="ltx_ref">Peng <span class="ltx_text ltx_font_italic">et al.</span>, 2019</a>]</cite>
also predict the masks of the target objects, in order to consider the
predictions from the image locations that lie on the objects, as they are the
only informative ones.</p>
</div>
</section>
<section id="S7.SS8" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.8 </span>DPOD and Pix2Pose</h3>

<figure id="S7.F14" class="ltx_figure"><img src="/html/2006.05927/assets/images/pix2pose.png" id="S7.F14.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="134" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 14: </span> Pix2Pose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx66" title="" class="ltx_ref">Park <span class="ltx_text ltx_font_italic">et al.</span>, 2019b</a>]</cite> predicts the object
coordinates of the pixels lying on the object surface, and computes the 3D
pose from these correspondences. It relies on a GAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx21" title="" class="ltx_ref">Goodfellow <span class="ltx_text ltx_font_italic">et al.</span>, 2014</a>]</cite> to
perform this prediction robustly even under occlusion. Image from
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx66" title="" class="ltx_ref">Park <span class="ltx_text ltx_font_italic">et al.</span>, 2019b</a>]</cite>.</figcaption>
</figure>
<div id="S7.SS8.p1" class="ltx_para">
<p id="S7.SS8.p1.1" class="ltx_p">DPOD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx110" title="" class="ltx_ref">Zakharov <span class="ltx_text ltx_font_italic">et al.</span>, 2019</a>]</cite> and Pix2Pose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx66" title="" class="ltx_ref">Park <span class="ltx_text ltx_font_italic">et al.</span>, 2019b</a>]</cite> are also two methods that have
been presented at the same conference and present similarities. They learn to
predict for each pixel of an input image centered on a target object its 3D
coordinates in the object coordinate system, see Figure <a href="#S7.F14" title="Figure 14 ‣ 7.8 DPOD and Pix2Pose ‣ 7 Modern Approaches to 3D Object Pose Estimation ‣ Recent Advances in 3D Object and Hand Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>.
More exactly, DPOD predicts the pixels’ texture coordinates but this is
fundamentally the same thing as they both provide 2D-3D correspondences between
image locations and their 3D object coordinates. Such representation is often
called ’object coordinates’ and more recently ’location field’ and has already
been used for 3D pose estimation in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">Brachmann <span class="ltx_text ltx_font_italic">et al.</span>, 2014</a>, <a href="#bib.bibx10" title="" class="ltx_ref">Brachmann <span class="ltx_text ltx_font_italic">et al.</span>, 2016</a>]</cite> and before
that in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx92" title="" class="ltx_ref">Taylor <span class="ltx_text ltx_font_italic">et al.</span>, 2012</a>]</cite> for human pose estimation. From these 2D-3D
correspondences, it is then possible to estimate the object pose using RANSAC
and P<math id="S7.SS8.p1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S7.SS8.p1.1.m1.1a"><mi id="S7.SS8.p1.1.m1.1.1" xref="S7.SS8.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S7.SS8.p1.1.m1.1b"><ci id="S7.SS8.p1.1.m1.1.1.cmml" xref="S7.SS8.p1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS8.p1.1.m1.1c">n</annotation></semantics></math>P. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx66" title="" class="ltx_ref">Park <span class="ltx_text ltx_font_italic">et al.</span>, 2019b</a>]</cite> relies on a GAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx21" title="" class="ltx_ref">Goodfellow <span class="ltx_text ltx_font_italic">et al.</span>, 2014</a>]</cite> to learn
to perform this prediction robustly even under occlusion. Note that DPOD has
been demonstrated to run in real-time on a tablet.</p>
</div>
<div id="S7.SS8.p2" class="ltx_para">
<p id="S7.SS8.p2.5" class="ltx_p">Pix2Pose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx66" title="" class="ltx_ref">Park <span class="ltx_text ltx_font_italic">et al.</span>, 2019b</a>]</cite> also uses a loss function that deals with symmetrical
objects and can be written:</p>
<table id="S7.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S7.E5.m1.5" class="ltx_math_unparsed" alttext="\mathcal{L}=\min_{{\bf R}\in\text{Sym}}\frac{1}{|\mathcal{V}|}\sum_{{\bf M}\in\mathcal{V}}{\|\text{Tr}({\bf M};\hat{{\bf p}})-\text{Tr}({\bf M};{\bf R}.\bar{{\bf p}})\|_{2}}\&gt;," display="block"><semantics id="S7.E5.m1.5a"><mrow id="S7.E5.m1.5b"><mi class="ltx_font_mathcaligraphic" id="S7.E5.m1.5.6">ℒ</mi><mo id="S7.E5.m1.5.7">=</mo><munder id="S7.E5.m1.5.8"><mi id="S7.E5.m1.5.8.2">min</mi><mrow id="S7.E5.m1.5.8.3"><mi id="S7.E5.m1.5.8.3.2">𝐑</mi><mo id="S7.E5.m1.5.8.3.1">∈</mo><mtext id="S7.E5.m1.5.8.3.3">Sym</mtext></mrow></munder><mfrac id="S7.E5.m1.1.1"><mn id="S7.E5.m1.1.1.3">1</mn><mrow id="S7.E5.m1.1.1.1.3"><mo stretchy="false" id="S7.E5.m1.1.1.1.3.1">|</mo><mi class="ltx_font_mathcaligraphic" id="S7.E5.m1.1.1.1.1">𝒱</mi><mo stretchy="false" id="S7.E5.m1.1.1.1.3.2">|</mo></mrow></mfrac><munder id="S7.E5.m1.5.9"><mo movablelimits="false" rspace="0em" id="S7.E5.m1.5.9.2">∑</mo><mrow id="S7.E5.m1.5.9.3"><mi id="S7.E5.m1.5.9.3.2">𝐌</mi><mo id="S7.E5.m1.5.9.3.1">∈</mo><mi class="ltx_font_mathcaligraphic" id="S7.E5.m1.5.9.3.3">𝒱</mi></mrow></munder><mo lspace="0em" rspace="0.167em" id="S7.E5.m1.5.10">∥</mo><mtext id="S7.E5.m1.5.11">Tr</mtext><mrow id="S7.E5.m1.5.12"><mo stretchy="false" id="S7.E5.m1.5.12.1">(</mo><mi id="S7.E5.m1.2.2">𝐌</mi><mo id="S7.E5.m1.5.12.2">;</mo><mover accent="true" id="S7.E5.m1.3.3"><mi id="S7.E5.m1.3.3.2">𝐩</mi><mo id="S7.E5.m1.3.3.1">^</mo></mover><mo stretchy="false" id="S7.E5.m1.5.12.3">)</mo></mrow><mo id="S7.E5.m1.5.13">−</mo><mtext id="S7.E5.m1.5.14">Tr</mtext><mrow id="S7.E5.m1.5.15"><mo stretchy="false" id="S7.E5.m1.5.15.1">(</mo><mi id="S7.E5.m1.4.4">𝐌</mi><mo id="S7.E5.m1.5.15.2">;</mo><mi id="S7.E5.m1.5.5">𝐑</mi><mo lspace="0em" rspace="0.167em" id="S7.E5.m1.5.15.3">.</mo><mover accent="true" id="S7.E5.m1.5.15.4"><mi id="S7.E5.m1.5.15.4.2">𝐩</mi><mo id="S7.E5.m1.5.15.4.1">¯</mo></mover><mo stretchy="false" id="S7.E5.m1.5.15.5">)</mo></mrow><msub id="S7.E5.m1.5.16"><mo lspace="0em" rspace="0.167em" id="S7.E5.m1.5.16.2">∥</mo><mn id="S7.E5.m1.5.16.3">2</mn></msub><mo id="S7.E5.m1.5.17">,</mo></mrow><annotation encoding="application/x-tex" id="S7.E5.m1.5c">\mathcal{L}=\min_{{\bf R}\in\text{Sym}}\frac{1}{|\mathcal{V}|}\sum_{{\bf M}\in\mathcal{V}}{\|\text{Tr}({\bf M};\hat{{\bf p}})-\text{Tr}({\bf M};{\bf R}.\bar{{\bf p}})\|_{2}}\&gt;,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p id="S7.SS8.p2.4" class="ltx_p">where <math id="S7.SS8.p2.1.m1.1" class="ltx_Math" alttext="\hat{{\bf p}}" display="inline"><semantics id="S7.SS8.p2.1.m1.1a"><mover accent="true" id="S7.SS8.p2.1.m1.1.1" xref="S7.SS8.p2.1.m1.1.1.cmml"><mi id="S7.SS8.p2.1.m1.1.1.2" xref="S7.SS8.p2.1.m1.1.1.2.cmml">𝐩</mi><mo id="S7.SS8.p2.1.m1.1.1.1" xref="S7.SS8.p2.1.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S7.SS8.p2.1.m1.1b"><apply id="S7.SS8.p2.1.m1.1.1.cmml" xref="S7.SS8.p2.1.m1.1.1"><ci id="S7.SS8.p2.1.m1.1.1.1.cmml" xref="S7.SS8.p2.1.m1.1.1.1">^</ci><ci id="S7.SS8.p2.1.m1.1.1.2.cmml" xref="S7.SS8.p2.1.m1.1.1.2">𝐩</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS8.p2.1.m1.1c">\hat{{\bf p}}</annotation></semantics></math> and <math id="S7.SS8.p2.2.m2.1" class="ltx_Math" alttext="\bar{{\bf p}}" display="inline"><semantics id="S7.SS8.p2.2.m2.1a"><mover accent="true" id="S7.SS8.p2.2.m2.1.1" xref="S7.SS8.p2.2.m2.1.1.cmml"><mi id="S7.SS8.p2.2.m2.1.1.2" xref="S7.SS8.p2.2.m2.1.1.2.cmml">𝐩</mi><mo id="S7.SS8.p2.2.m2.1.1.1" xref="S7.SS8.p2.2.m2.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="S7.SS8.p2.2.m2.1b"><apply id="S7.SS8.p2.2.m2.1.1.cmml" xref="S7.SS8.p2.2.m2.1.1"><ci id="S7.SS8.p2.2.m2.1.1.1.cmml" xref="S7.SS8.p2.2.m2.1.1.1">¯</ci><ci id="S7.SS8.p2.2.m2.1.1.2.cmml" xref="S7.SS8.p2.2.m2.1.1.2">𝐩</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS8.p2.2.m2.1c">\bar{{\bf p}}</annotation></semantics></math> denote the predicted and ground truth
poses respectively, <span id="S7.SS8.p2.4.1" class="ltx_text ltx_markedasmath">Sym</span> is a set of rotations corresponding to the
object symmetries, and <math id="S7.SS8.p2.4.m4.2" class="ltx_Math" alttext="{\bf R}.{\bf p}" display="inline"><semantics id="S7.SS8.p2.4.m4.2a"><mrow id="S7.SS8.p2.4.m4.2.3.2" xref="S7.SS8.p2.4.m4.2.3.1.cmml"><mi id="S7.SS8.p2.4.m4.1.1" xref="S7.SS8.p2.4.m4.1.1.cmml">𝐑</mi><mo lspace="0em" rspace="0.167em" id="S7.SS8.p2.4.m4.2.3.2.1" xref="S7.SS8.p2.4.m4.2.3.1a.cmml">.</mo><mi id="S7.SS8.p2.4.m4.2.2" xref="S7.SS8.p2.4.m4.2.2.cmml">𝐩</mi></mrow><annotation-xml encoding="MathML-Content" id="S7.SS8.p2.4.m4.2b"><apply id="S7.SS8.p2.4.m4.2.3.1.cmml" xref="S7.SS8.p2.4.m4.2.3.2"><csymbol cd="ambiguous" id="S7.SS8.p2.4.m4.2.3.1a.cmml" xref="S7.SS8.p2.4.m4.2.3.2.1">formulae-sequence</csymbol><ci id="S7.SS8.p2.4.m4.1.1.cmml" xref="S7.SS8.p2.4.m4.1.1">𝐑</ci><ci id="S7.SS8.p2.4.m4.2.2.cmml" xref="S7.SS8.p2.4.m4.2.2">𝐩</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS8.p2.4.m4.2c">{\bf R}.{\bf p}</annotation></semantics></math> denotes the composition of such a rotation
and a 3D pose. This deals with symmetries in a much more satisfying way than the
ADD-S loss (Eq. (<a href="#S6.E3" title="In ADD, ADI, ADD-S, and the 6D Pose metrics. ‣ 6.4 Metrics ‣ 6 Datasets ‣ Recent Advances in 3D Object and Hand Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>)).</p>
</div>
</section>
<section id="S7.SS9" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.9 </span>Discussion</h3>

<div id="S7.SS9.p1" class="ltx_para">
<p id="S7.SS9.p1.1" class="ltx_p">As can be seen, the last recent years have seen rich developments in 3D object
pose estimation, and methods have became even more robust, more accurate, and
faster. Many methods rely on 2D segmentation to detect the objects, which seems
pretty robust, but assumes that only several instances of the same object do not
overlap in the image. Most methods also rely on a refinement step, which may
slow things down, but relax the need for having a single strong detection stage.</p>
</div>
<div id="S7.SS9.p2" class="ltx_para">
<p id="S7.SS9.p2.1" class="ltx_p">There are still several caveats though. First, it remains difficult to compare
methods based on Deep Learning, even though the use of public benchmarks helped
to promote fair comparisons. Quantitative results depend not only on the method
itself, but also on how much effort the authors put into training their methods
and augmenting training data. Also, the focus on the benchmarks may make the
method overfit to their data, and it is not clear how well current methods
generalize to the real world. Also, these methods rely on large numbers of
registered training images and/or on textured models of the objects, which can
be cumbersome to acquire.</p>
</div>
</section>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>3D Pose Estimation for Object Category</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">So far, we discussed methods that estimate the 3D pose of specific objects,
which are known in advance. As we already mentioned, this requires the
cumbersome step of capturing a training set for each object. One possible
direction to avoid this step is to consider a “category”-level approach, for
objects that belong to a clear category, such as ’car’ or ’chair’. The
annotation burden moves then to images of objects from the target categories,
but we can then estimate the 3D pose of new, unseen objects from these
categories.</p>
</div>
<div id="S8.p2" class="ltx_para">
<p id="S8.p2.1" class="ltx_p">Some early category-level methods only estimate 3 degrees-of-freedom (2 for the
image location and 1 for the rotation over the ground plane) of the object pose
using regression, classification or hybrid variants of the two. For example,
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx105" title="" class="ltx_ref">Xiang <span class="ltx_text ltx_font_italic">et al.</span>, 2016</a>]</cite> directly regresses azimuth, elevation and in-plane
rotation using a Convolutional Neural Network (CNN), while
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx100" title="" class="ltx_ref">Tulsiani <span class="ltx_text ltx_font_italic">et al.</span>, 2015</a>, <a href="#bib.bibx99" title="" class="ltx_ref">Tulsiani and
Malik, 2015</a>]</cite> perform viewpoint classification
by discretizing the range of each angle into a number of disjoint bins and
predicting the most likely bin using a CNN.</p>
</div>
<div id="S8.p3" class="ltx_para">
<p id="S8.p3.1" class="ltx_p">To estimate a full 3D pose, many methods rely on ’semantic keypoints’, which can
be detected in the images, and correspond to 3D points on the object. For
example, to estimate the 3D pose of a car, one may one to consider the corners
of the roof and the lights as semantic keypoints. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx69" title="" class="ltx_ref">Pepik <span class="ltx_text ltx_font_italic">et al.</span>, 2015</a>]</cite> recovers
the pose from keypoint predictions and CAD models using a P<math id="S8.p3.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S8.p3.1.m1.1a"><mi id="S8.p3.1.m1.1.1" xref="S8.p3.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S8.p3.1.m1.1b"><ci id="S8.p3.1.m1.1.1.cmml" xref="S8.p3.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S8.p3.1.m1.1c">n</annotation></semantics></math>P algorithm, and
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx67" title="" class="ltx_ref">Pavlakos <span class="ltx_text ltx_font_italic">et al.</span>, 2017</a>]</cite> predicts semantic keypoints and trains a deformable shape
model which takes keypoint uncertainties into account. Relying on such
keypoints, however, is even more depending in terms of annotations as keypoints
have to be careful chosen and manually located in many images, and as they do
not generalize across categories.</p>
</div>
<figure id="S8.F15" class="ltx_figure"><img src="/html/2006.05927/assets/images/grabner18.png" id="S8.F15.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="177" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 15: </span> Estimating the 3D pose of an unknown object from a
known category. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx24" title="" class="ltx_ref">Grabner <span class="ltx_text ltx_font_italic">et al.</span>, 2018</a>]</cite> predicts the 2D reprojections of the
corners of the 3D bounding box <em id="S8.F15.3.1" class="ltx_emph ltx_font_italic">and</em> the size of the bounding box. For
this information, it is possible to compute the pose of the object using a PnP
algorithm. Image from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx24" title="" class="ltx_ref">Grabner <span class="ltx_text ltx_font_italic">et al.</span>, 2018</a>]</cite>.</figcaption>
</figure>
<div id="S8.p4" class="ltx_para">
<p id="S8.p4.1" class="ltx_p">In fact, if one is not careful, the concept of 3D pose for object categories is
ill-defined, as different objects from the same category are likely to have
different sizes. To properly define this pose, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx24" title="" class="ltx_ref">Grabner <span class="ltx_text ltx_font_italic">et al.</span>, 2018</a>]</cite> considers that
the 3D pose of an object from a category is defined as the 3D pose of its 3D
bounding box. It then proposes a method illustrated in
Figure <a href="#S8.F15" title="Figure 15 ‣ 8 3D Pose Estimation for Object Category ‣ Recent Advances in 3D Object and Hand Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a> that extends BB8 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx73" title="" class="ltx_ref">Rad and Lepetit, 2017</a>]</cite>. BB8 estimates a 3D
pose by predicting the 2D reprojections of the corners of its 3D bounding
box. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx24" title="" class="ltx_ref">Grabner <span class="ltx_text ltx_font_italic">et al.</span>, 2018</a>]</cite> predicts similar 2D reprojections, plus the size of the 3D
bounding box in the form of 3 values (length, height, width). Using these 3
values, it is possible to compute the 3D coordinates of the corners in a
coordinate system related to the object. From these 3D coordinates, and from
the predicted 2D reprojections, it is possible to compute the 3D pose of the 3D
bounding box.</p>
</div>
<section id="S8.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">3D model retrieval. </h4>

<div id="S8.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S8.SS0.SSS0.Px1.p1.1" class="ltx_p">Since objects from the same category have various 3D models,
it may also be interesting to recover a 3D model for these objects, in addition
to their 3D poses. Different approaches are possible. One is to recover an
existing 3D model from a database that fits well the object. Large datasets of
light 3D models exist for many categories, which makes this approach
attractive <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">Chang <span class="ltx_text ltx_font_italic">et al.</span>, 2015</a>]</cite>. To make this approach efficient, it is best to rely
on metric learning, so that an embedding computed for the object from the input
image can be matched against the embeddings for the 3D
models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">Aubry and
Russell, 2015</a>, <a href="#bib.bibx43" title="" class="ltx_ref">Izadinia <span class="ltx_text ltx_font_italic">et al.</span>, 2017</a>, <a href="#bib.bibx24" title="" class="ltx_ref">Grabner <span class="ltx_text ltx_font_italic">et al.</span>, 2018</a>]</cite>. If the
similarity between these embeddings can be estimated based on their Euclidean
distance or their dot product, then it is possible to rely on efficient
techniques to perform this match.</p>
</div>
<figure id="S8.F16" class="ltx_figure">
<table id="S8.F16.2.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S8.F16.2.2.2" class="ltx_tr">
<td id="S8.F16.1.1.1.1" class="ltx_td ltx_align_center"><img src="/html/2006.05927/assets/images/grabner19a.png" id="S8.F16.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="269" height="91" alt="Refer to caption"></td>
<td id="S8.F16.2.2.2.2" class="ltx_td ltx_align_center"><img src="/html/2006.05927/assets/images/grabner19b.png" id="S8.F16.2.2.2.2.g1" class="ltx_graphics ltx_img_landscape" width="269" height="89" alt="Refer to caption"></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 16: </span> Recovering a 3D model for an unknown object
from a known category using an image. (a) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx25" title="" class="ltx_ref">Grabner <span class="ltx_text ltx_font_italic">et al.</span>, 2019</a>]</cite>
generates renderings of the object coordinates (called location fields in
the paper) from 3D models (top), and predicts similar object coordinates
from the input images (bottom). Images of objects and 3D models are matched
based on embeddings computed from the object coordinates. (b) An example of
recovered 3D poses and 3D models for two chairs. The 3D models capture the
general shapes of the real chairs, but do not fit perfectly.</figcaption>
</figure>
<div id="S8.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S8.SS0.SSS0.Px1.p2.1" class="ltx_p">The challenge is that the object images and the 3D models have very different
natures, while this approach needs to compute embeddings that are comparable
from these two sources. The solution is then to replace the 3D models by image
renderings; the embeddings for the image renderings can then be compared more
easily with the embeddings for the input images. Remains the domain gap between
the real input images and the synthetic image renderings. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx25" title="" class="ltx_ref">Grabner <span class="ltx_text ltx_font_italic">et al.</span>, 2019</a>]</cite>,
as shown in Figure <a href="#S8.F16" title="Figure 16 ‣ 3D model retrieval. ‣ 8 3D Pose Estimation for Object Category ‣ Recent Advances in 3D Object and Hand Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16</span></a>, this is solved by predicting the object
coordinates for the input image using a deep network, and by rendering the
object coordinates for the synthetic images, rather than a regular
rendering. This brings the representations for the input images and the 3D
models even closer. From these representations, it is then easier to compute
suitable embeddings.</p>
</div>
<div id="S8.SS0.SSS0.Px1.p3" class="ltx_para">
<p id="S8.SS0.SSS0.Px1.p3.1" class="ltx_p">Another approach is to predict the object 3D geometry directly from images.
This approach learns a mapping from the appearance of an object to its 3D
geometry. This is more appealing than recovering a 3D model from a database
that has no guarantee to fit perfectly the object, since this latter method can
potentially adapt better to the object’s
geometry <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx83" title="" class="ltx_ref">Shin <span class="ltx_text ltx_font_italic">et al.</span>, 2018</a>, <a href="#bib.bibx26" title="" class="ltx_ref">Groueix <span class="ltx_text ltx_font_italic">et al.</span>, 2018</a>, <a href="#bib.bibx65" title="" class="ltx_ref">Park <span class="ltx_text ltx_font_italic">et al.</span>, 2019a</a>]</cite>. This is however much more challenging,
and current methods are still often limited to clean images with blank
background, and sometimes even synthetic renderings, however this is a very
active area, and more progress can be expected in the near future.</p>
</div>
</section>
</section>
<section id="S9" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9 </span>3D Hand Pose Estimation from Depth Maps</h2>

<div id="S9.p1" class="ltx_para">
<p id="S9.p1.1" class="ltx_p">While related to 3D object pose estimation, hand pose estimation has its own
specificities. On one hand (no pun intended), it is more challenging, if only
because more degrees-of-freedom must be estimated. On the other hand, while
considering new 3D objects still requires acquiring new data and/or specific 3D
models, a single model can generalize to unseen hands, despite differences in
size, shape, and skin color, as these differences remain small, and considerably
smaller than differences between two arbitrary objects. In practice, this means
that a model does not need new data nor retraining to adapt to new users, which
is very convenient.</p>
</div>
<div id="S9.p2" class="ltx_para">
<p id="S9.p2.1" class="ltx_p">However, the motivation for 3D hand pose in AR applications is mostly for making
possible natural interaction with virtual objects. As we will briefly discuss
in Section <a href="#S10.SS6" title="10.6 Manipulating Virtual Objects ‣ 10 3D Hand Pose Estimation from an RGB Image ‣ Recent Advances in 3D Object and Hand Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10.6</span></a>, convincing interaction requires very high
accuracy, which current developments are aiming to achieve.</p>
</div>
<div id="S9.p3" class="ltx_para">
<p id="S9.p3.1" class="ltx_p">We will start with 3D hand pose estimation from a depth map. As for 3D object
pose estimation, the literature is extremly large, and we will focus here as
well on a few representative methods. This is the direction currently taken in
the industry, because it can provide a robust and accurate estimate. It is
currently being deployed on commercial solutions, such as HoloLens 2 and Oculus.
We will then turn to hand pose estimation from color images, which is more
challenging, but has also the potential to avoid the drawbacks of using depth
cameras. We will turn to pose estimation from color images in the next section.</p>
</div>
<section id="S9.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.1 </span>DeepPrior++</h3>

<div id="S9.SS1.p1" class="ltx_para">
<p id="S9.SS1.p1.1" class="ltx_p">Estimating the 3D pose of a hand from a depth map is in fact now relatively easy
and robust, when using a well engineered solution based on Deep Learning. For
example, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx63" title="" class="ltx_ref">Oberweger <span class="ltx_text ltx_font_italic">et al.</span>, 2018</a>]</cite> discusses the implementation of a method called
DeepPrior++, which is simple in principle and performs well.</p>
</div>
<div id="S9.SS1.p2" class="ltx_para">
<p id="S9.SS1.p2.1" class="ltx_p">DeepPrior++ first selects a bounding box on the hand, and then predicts the 3D
joint locations from the depth data within this bounding box. An accurate
localization of the bounding appears to improve the final accuracy, and the
method from the center of mass of the depth data within a threshold, and then
uses a refinement step that can be iterated. This refinement step is performed
by a regression network predicting, from a 3D bounding box centered on the
center of mass, the 3D location of one of the joints used as a referential.
Then, the 3D joint locations can be predicted using a second Network that is not
very different from the one presented in Figure <a href="#S5.F4" title="Figure 4 ‣ Deep Learning. ‣ 5 Machine Learning and Deep Learning ‣ Recent Advances in 3D Object and Hand Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, except that
it relies on ResNet block <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx33" title="" class="ltx_ref">He <span class="ltx_text ltx_font_italic">et al.</span>, 2016</a>]</cite> for more accurate results. The input to
this network is therefore the input depth map cropped to contain only the hand,
and its output is made of the 3D coordinates of all the hand joints.</p>
</div>
<div id="S9.SS1.p3" class="ltx_para">
<p id="S9.SS1.p3.1" class="ltx_p">Simple data augmentation appears to improve accuracy, even when starting from an
already large dataset. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx63" title="" class="ltx_ref">Oberweger <span class="ltx_text ltx_font_italic">et al.</span>, 2018</a>]</cite> applies random in-plane rotations,
scales, 3D offsets to the input data.</p>
</div>
</section>
<section id="S9.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.2 </span>V2V-PoseNet</h3>

<div id="S9.SS2.p1" class="ltx_para">
<p id="S9.SS2.p1.1" class="ltx_p">V2V-PoseNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx58" title="" class="ltx_ref">Moon <span class="ltx_text ltx_font_italic">et al.</span>, 2018</a>]</cite> is a method that has been identified by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx109" title="" class="ltx_ref">Yuan <span class="ltx_text ltx_font_italic">et al.</span>, 2017</a>]</cite>
as one of the best performing methods at the time, based on the results on a
public benchmark. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx58" title="" class="ltx_ref">Moon <span class="ltx_text ltx_font_italic">et al.</span>, 2018</a>]</cite> argues that using a cropped depth map as input
to the network makes training difficult, as the same hand pose
appears (slightly) differently in a depth map depending on where it reprojects.</p>
</div>
<figure id="S9.F17" class="ltx_figure"><img src="/html/2006.05927/assets/images/V2V.png" id="S9.F17.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="165" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 17: </span> Architecture of V2V-PoseNet. The network takes a
voxel-based representation of the depth map centered on the hand, and predicts
the 3D joint locations in the form of heatmaps. </figcaption>
</figure>
<div id="S9.SS2.p2" class="ltx_para">
<p id="S9.SS2.p2.1" class="ltx_p">To avoid distortion, V2V-PoseNet converts the depth map information in the hand
bounding box into a voxel-based representation (it uses a refinement step
similar to the one in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx63" title="" class="ltx_ref">Oberweger <span class="ltx_text ltx_font_italic">et al.</span>, 2018</a>]</cite>, so accuracy of the hand detection
seems important). The conversion is done by voxelizing the 3D bounding box for
the hand, and each voxel that contains at least one pixel from the depth map is
set to occupied. The voxel-based representation therefore corresponds to a
dicretization of the hand surface.</p>
</div>
<div id="S9.SS2.p3" class="ltx_para">
<p id="S9.SS2.p3.1" class="ltx_p">Then, a network is trained to take this voxel-based representation as input, and
predicts the 3D joint locations in the form of 3D heatmaps. A 3D heatmap is a
3D array containing confidence values. There is one 3D heatmap for each joint,
and the joint location is taken as the 3D location with the higher confidence.</p>
</div>
</section>
<section id="S9.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.3 </span>A2J</h3>

<div id="S9.SS3.p1" class="ltx_para">
<p id="S9.SS3.p1.1" class="ltx_p">The A2J method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx108" title="" class="ltx_ref">Xiong <span class="ltx_text ltx_font_italic">et al.</span>, 2019</a>]</cite> performs similarly or better than V2V-PoseNet, but
faster. The prediction of the 3D joint locations is performed via what are
called in the paper “anchor points”. Anchor points are 2D locations regularly
sampled over the bounding box of the hand, obtained as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx58" title="" class="ltx_ref">Moon <span class="ltx_text ltx_font_italic">et al.</span>, 2018</a>]</cite> and
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx63" title="" class="ltx_ref">Oberweger <span class="ltx_text ltx_font_italic">et al.</span>, 2018</a>]</cite>. As shown in Figure <a href="#S9.F18" title="Figure 18 ‣ 9.3 A2J ‣ 9 3D Hand Pose Estimation from Depth Maps ‣ Recent Advances in 3D Object and Hand Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">18</span></a>, a network is trained to
predict, for each anchor point, and for each joint, 1) a 2D displacement from
the anchor point to the joint, 2) the depth of the joint, and 3) a weight
reflecting the confidence for the prediction. The prediction of 2D
displacements makes this method close to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx41" title="" class="ltx_ref">Hu <span class="ltx_text ltx_font_italic">et al.</span>, 2019</a>]</cite> and PVNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx68" title="" class="ltx_ref">Peng <span class="ltx_text ltx_font_italic">et al.</span>, 2019</a>]</cite>,
which were developed for 3D object pose estimation under occlusions.</p>
</div>
<figure id="S9.F18" class="ltx_figure">
<table id="S9.F18.2.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S9.F18.2.2.2" class="ltx_tr">
<td id="S9.F18.1.1.1.1" class="ltx_td ltx_align_center"><img src="/html/2006.05927/assets/images/A2J-index.png" id="S9.F18.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="269" height="151" alt="Refer to caption"></td>
<td id="S9.F18.2.2.2.2" class="ltx_td ltx_align_center"><img src="/html/2006.05927/assets/images/A2J-index-mid.png" id="S9.F18.2.2.2.2.g1" class="ltx_graphics ltx_img_landscape" width="269" height="144" alt="Refer to caption"></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 18: </span> The A2J method predicts, for regularly sampled
“anchor points” (gray dots), and for each joint, a 2D displacement towards
the 2D joint location (green line segments), its depth, and the confidence
for these predictions. Red dots correspond to anchor points with high
confidences. The 3D joint locations can be estimated from these as weighted
sums (yellow squares). Images from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx108" title="" class="ltx_ref">Xiong <span class="ltx_text ltx_font_italic">et al.</span>, 2019</a>]</cite>.</figcaption>
</figure>
<div id="S9.SS3.p2" class="ltx_para">
<p id="S9.SS3.p2.1" class="ltx_p">From the output of the network, it is possible to estimate the 3D joint
locations as weighted sums of the 2D locations and depths, where the weights of
the sums are the ones predicted by the network. Special care is taken to make
sure the weights sum to 1. The paper argues that this is similar to predictions
by an “ensemble”, which are known to generalize well <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx12" title="" class="ltx_ref">Breiman, 1996</a>]</cite>. The
weights play a critical role here: Without them, the predicted locations would
be a linear combination of the network output, which could be done by a standard
layer without having to introduce anchor points. The product between weights
and 2D displacements and depths make the predicted joint locations a more
complex, non-linear transformation of the network output.</p>
</div>
</section>
<section id="S9.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.4 </span>Discussion</h3>

<div id="S9.SS4.p1" class="ltx_para">
<p id="S9.SS4.p1.1" class="ltx_p">As discussed in detail in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">Armagan <span class="ltx_text ltx_font_italic">et al.</span>, 2020</a>]</cite>, the coverage of the space of
possible hand poses is critical to achieve good performance. This should not be
surprising: Deep Networks essentially learn a mapping between the input (here
the depth map) and the output (here the hand pose) from the training set. They
can interpolate very well between samples, but poorly extrapolate: If the input
contains a pose too different from any sample in the training set, the output is
likely to be wrong. When using as input depth maps, which do not suffer from
light changes or cluttered background like color images, having a good training
set is the main important issue.</p>
</div>
<div id="S9.SS4.p2" class="ltx_para">
<p id="S9.SS4.p2.1" class="ltx_p">That was in fact already discussed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx63" title="" class="ltx_ref">Oberweger <span class="ltx_text ltx_font_italic">et al.</span>, 2018</a>]</cite>, and even before that
in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx89" title="" class="ltx_ref">Supancic <span class="ltx_text ltx_font_italic">et al.</span>, 2015</a>]</cite>. Combining the DeepPrior++ that relies on a simple mapping
with a simple domain adaptation method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx74" title="" class="ltx_ref">Rad <span class="ltx_text ltx_font_italic">et al.</span>, 2018</a>]</cite> in order to use more
synthetic data appears to perform very well, and still outperfoms more complex
methods on the NYU dataset, for example. While it is not entirely clear, the
direction taken by HoloLens 2 and Oculus is to train a simple model on a large
set of synthetic training data, and refine the pose estimate using an ICP
algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx82" title="" class="ltx_ref">Sharp <span class="ltx_text ltx_font_italic">et al.</span>, 2015</a>]</cite> aligning a 3D model of the hand to the depth data.</p>
</div>
</section>
</section>
<section id="S10" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">10 </span>3D Hand Pose Estimation from an RGB Image</h2>

<div id="S10.p1" class="ltx_para">
<p id="S10.p1.1" class="ltx_p">We now turn to modern approaches to 3D hand pose estimation from an RGB image,
which is significantly more challenging than from a depth map, but which is a
field that has been impressively fast progress over the past few years.</p>
</div>
<figure id="S10.F19" class="ltx_figure"><img src="/html/2006.05927/assets/images/zimmerman17.png" id="S10.F19.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="152" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 19: </span> Overview of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx112" title="" class="ltx_ref">Zimmermann and Brox, 2017</a>]</cite>. The hand is first
segmented in 2D to obtain a 2D bounding box. From the image information within
this bounding box, a set of heatmaps for the hand joints is predicted and lifted
to 3D. Image from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx112" title="" class="ltx_ref">Zimmermann and Brox, 2017</a>]</cite>.</figcaption>
</figure>
<section id="S10.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">10.1 </span><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx112" title="" class="ltx_ref">Zimmermann and Brox, 2017</a>]</cite>
</h3>

<div id="S10.SS1.p1" class="ltx_para">
<p id="S10.SS1.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx112" title="" class="ltx_ref">Zimmermann and Brox, 2017</a>]</cite> was one of the first methods predicting the 3D hand pose from
a single color image, without using depth information. An overview of the
method is shown in Figure <a href="#S10.F19" title="Figure 19 ‣ 10 3D Hand Pose Estimation from an RGB Image ‣ Recent Advances in 3D Object and Hand Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">19</span></a>. The hand is first segmented in
2D, as in recent approaches for 3D object pose estimation, to obtain a 2D
bounding box centered on it. From this window, a 2D heatmap is predicted for
each joint, using Convolutional Pose Machines <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx104" title="" class="ltx_ref">Wei <span class="ltx_text ltx_font_italic">et al.</span>, 2016</a>]</cite>. This provides
information on the 2D joint locations. To obtain the final 3D pose, the heatmaps
are lifted in 3D by using 2 networks: One network predicts the 3D joint
locations in a canonical system attached to the hand; the other network predicts
the 3D pose (3D rotation and translation) of the hand in the camera coordinate
system. From these 2 set of information, it is possible to compute the 3D joint
coordinates in the camera coordinate system. This is shown to work slightly
better than a single network directly predicting the 3D joint locations in the
camera coordinate system.</p>
</div>
<div id="S10.SS1.p2" class="ltx_para">
<p id="S10.SS1.p2.1" class="ltx_p">To train the segmentation, heatmap prediction, and 3D pose predictions,
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx112" title="" class="ltx_ref">Zimmermann and Brox, 2017</a>]</cite> created a dataset of synthetic images of hands, as the 3D
pose can be known exactly. To do so, they used freely available 3D models of
humans with corresponding animations from Mixamo, a company specialized in
character animation, and Blender to render the images over random
background. Lighting, viewpoint, and skin properties were also randomized. The
dataset is publicly available online.</p>
</div>
</section>
<section id="S10.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">10.2 </span><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx42" title="" class="ltx_ref">Iqbal <span class="ltx_text ltx_font_italic">et al.</span>, 2018</a>]</cite>
</h3>

<div id="S10.SS2.p1" class="ltx_para">
<p id="S10.SS2.p1.1" class="ltx_p">Like <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx112" title="" class="ltx_ref">Zimmermann and Brox, 2017</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx42" title="" class="ltx_ref">Iqbal <span class="ltx_text ltx_font_italic">et al.</span>, 2018</a>]</cite> relies on 2D heatmaps from a color image
to lift them to 3D, but with several differences that improve the pose accuracy.
They oppose two approaches to 3D pose prediction: The first approach relies on
heatmaps; it is generally accurate but keeping this accuracy with 3D heatmaps is
still intractable, as a finely sampled 3D volume can be very large. The second
approach is to predict the 3D joints in a “holistic” way, meaning that the
values of the 3D joint locations are directly predicted from the input image, as
was done in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx98" title="" class="ltx_ref">Toshev and Szegedy, 2014</a>, <a href="#bib.bibx87" title="" class="ltx_ref">Sun <span class="ltx_text ltx_font_italic">et al.</span>, 2017</a>]</cite> for human body pose prediction, and
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx62" title="" class="ltx_ref">Oberweger <span class="ltx_text ltx_font_italic">et al.</span>, 2017</a>]</cite> for hand pose prediction from depth data. Unfortunately,
“holistic” approaches tend to be less accurate.</p>
</div>
<figure id="S10.F20" class="ltx_figure"><img src="/html/2006.05927/assets/images/iqbal18.png" id="S10.F20.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="222" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 20: </span> Architecture of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx42" title="" class="ltx_ref">Iqbal <span class="ltx_text ltx_font_italic">et al.</span>, 2018</a>]</cite>. The method
predicts a heatmap and a depth map for each joint, and estimates the 3D pose
using geometric constraints. The model is trained end-to-end, which lets the
model find optimal heatmaps and depth maps. Image from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx42" title="" class="ltx_ref">Iqbal <span class="ltx_text ltx_font_italic">et al.</span>, 2018</a>]</cite>.</figcaption>
</figure>
<div id="S10.SS2.p2" class="ltx_para">
<p id="S10.SS2.p2.1" class="ltx_p">To keep the accuracy of heatmaps, and to predict the joint depths with a
tractable method, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx42" title="" class="ltx_ref">Iqbal <span class="ltx_text ltx_font_italic">et al.</span>, 2018</a>]</cite> also predicts, in addition to the 2D heatmaps,
a depth map for each joint, where the depth values are predicted depths for the
corresponding joint relative to a reference joint. This is shown in
Figure <a href="#S10.F20" title="Figure 20 ‣ 10.2 [Iqbal et al., 2018] ‣ 10 3D Hand Pose Estimation from an RGB Image ‣ Recent Advances in 3D Object and Hand Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">20</span></a>. The final 3D pose is computed using geometric
constraints from this information. Instead of learning to predict the heatmaps
in a supervised way, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx42" title="" class="ltx_ref">Iqbal <span class="ltx_text ltx_font_italic">et al.</span>, 2018</a>]</cite> learns to predict “latent” heatmaps. This
means that the deep model is pretrained to predict heatmaps and depth maps, but
the constraints on the predicted heatmaps and depth maps are removed from the
full loss, and the model is trained “end-to-end”, with a loss involving only
the final predicted 3D pose. This lets the optimization find heatmaps and depth
maps that are easier to predict, while being more useful to predict the correct
pose.</p>
</div>
</section>
<section id="S10.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">10.3 </span>GANerated Hands</h3>

<div id="S10.SS3.p1" class="ltx_para">
<p id="S10.SS3.p1.1" class="ltx_p">As already mentioned above, training data and its diversity is of crucial
importance in Deep Learning. For 3D hand pose estimation from color images,
this means that training data need to span the hand pose space, the hand shape
space, possible lighting, possible background, etc. Moreover, this data has to
be annotated in 3D, which is very challenging. Generating synthetic images is
thus attractive, but because of the “domain gap” between real and synthetic
images, this may impact the accuracy of a model trained on synthetic images and
applied to real images.</p>
</div>
<div id="S10.SS3.p2" class="ltx_para">
<p id="S10.SS3.p2.1" class="ltx_p">To bridge the domain gap between synthetic and real images, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx59" title="" class="ltx_ref">Mueller <span class="ltx_text ltx_font_italic">et al.</span>, 2018a</a>]</cite>
extends CycleGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx111" title="" class="ltx_ref">Zhu <span class="ltx_text ltx_font_italic">et al.</span>, 2017</a>]</cite>, which is itself based on Generative Adversarial
Networks (GANs). In original GANs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx21" title="" class="ltx_ref">Goodfellow <span class="ltx_text ltx_font_italic">et al.</span>, 2014</a>]</cite>, a network is trained to
generate images from random vectors. In order to ensure the generated images
are “realistic”, a second network is trained jointly with the first one to
distinguish the generated images from real ones. When this network fails and
cannot classify the images generated by the first network as generated, it means
these generated images are similar to real ones. CycleGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx111" title="" class="ltx_ref">Zhu <span class="ltx_text ltx_font_italic">et al.</span>, 2017</a>]</cite> builds on
this idea to change an image from a domain into an image with the <em id="S10.SS3.p2.1.1" class="ltx_emph ltx_font_italic">same
content</em> but similar to the images from another domain. An image generator is
thus train to take an image from some domain as input and to generate as output
another image. A second network ensures that the output looks similar to the
target domain. To make sure the content of the image is preserved, a second pair
of networks is jointly trained with the first one: This pair is trained jointly
with the first one, to re-generate the original input image from the image
generated by the first generator.</p>
</div>
<figure id="S10.F21" class="ltx_figure"><img src="/html/2006.05927/assets/images/gan_arch.png" id="S10.F21.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="359" height="147" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 21: </span> The architecture used in
GANerated <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx59" title="" class="ltx_ref">Mueller <span class="ltx_text ltx_font_italic">et al.</span>, 2018a</a>]</cite> predicts both 2D heatmaps and 3D joints
locations, and estimates the 3D hand pose with a (non-learned)
optimization. Image from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx59" title="" class="ltx_ref">Mueller <span class="ltx_text ltx_font_italic">et al.</span>, 2018a</a>]</cite>.</figcaption>
</figure>
<div id="S10.SS3.p3" class="ltx_para">
<p id="S10.SS3.p3.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx59" title="" class="ltx_ref">Mueller <span class="ltx_text ltx_font_italic">et al.</span>, 2018a</a>]</cite> extends CycleGAN by adding a constraint between the original
image (here a synthetic image) and the image returned by the first
generator. The synthetic image is a hand rendered over a uniform background, and
a segmentation network is trained jointly to segment the generated image, so
that the predicted segmentation matches the segmentation of the hand in the
synthetic image. This helps preserving, at least to some extent, the 3D pose of
the hand in the transformed image. Figure <a href="#S10.F21" title="Figure 21 ‣ 10.3 GANerated Hands ‣ 10 3D Hand Pose Estimation from an RGB Image ‣ Recent Advances in 3D Object and Hand Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">21</span></a> shows two
synthetic images transformed by CycleGAN, with and without this constraint.
Another advantage is that the plain background can be changed by a random
background to augment the dataset of transformed images.</p>
</div>
<div id="S10.SS3.p4" class="ltx_para">
<p id="S10.SS3.p4.1" class="ltx_p">Besides this data generation, the 3D hand pose prediction in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx59" title="" class="ltx_ref">Mueller <span class="ltx_text ltx_font_italic">et al.</span>, 2018a</a>]</cite> is
also interesting. A network is trained to predict <em id="S10.SS3.p4.1.1" class="ltx_emph ltx_font_italic">both</em> 2D heatmaps for
the 2D joint locations, and the 3D joint locations. Given the output of this
network over a video stream, an optimization is performed to fit a kinematic
model to these 2D and 3D predictions under joint angle constraints and temporal
smoothness constraints. The output of the network is therefore not used directly
as the 3D hand pose, but used as a observation in this final optimization.</p>
</div>
<figure id="S10.F22" class="ltx_figure"><img src="/html/2006.05927/assets/images/Ge19.png" id="S10.F22.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="209" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 22: </span> The architecture used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx20" title="" class="ltx_ref">Ge <span class="ltx_text ltx_font_italic">et al.</span>, 2019</a>]</cite> predicts
2D heatmaps for the joints and a 3D mesh for the hand. Image from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx20" title="" class="ltx_ref">Ge <span class="ltx_text ltx_font_italic">et al.</span>, 2019</a>]</cite>.</figcaption>
</figure>
</section>
<section id="S10.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">10.4 </span>3D Hand Shape and Pose Estimation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx20" title="" class="ltx_ref">Ge <span class="ltx_text ltx_font_italic">et al.</span>, 2019</a>, <a href="#bib.bibx8" title="" class="ltx_ref">Boukhayma <span class="ltx_text ltx_font_italic">et al.</span>, 2019</a>]</cite>
</h3>

<div id="S10.SS4.p1" class="ltx_para">
<p id="S10.SS4.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx20" title="" class="ltx_ref">Ge <span class="ltx_text ltx_font_italic">et al.</span>, 2019</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx8" title="" class="ltx_ref">Boukhayma <span class="ltx_text ltx_font_italic">et al.</span>, 2019</a>]</cite> were published at the same
conference (CVPR’19) and both propose a method for predicting the hand shape in
addition to the hand pose.</p>
</div>
<div id="S10.SS4.p2" class="ltx_para">
<p id="S10.SS4.p2.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx20" title="" class="ltx_ref">Ge <span class="ltx_text ltx_font_italic">et al.</span>, 2019</a>]</cite> first generates a training set of hands under various poses, where
each image is annotated with the pose and shape parameters, with special care to
make the rendered images as realistic as possible. Using this dataset, they
train a network (shown in Figure <a href="#S10.F22" title="Figure 22 ‣ 10.3 GANerated Hands ‣ 10 3D Hand Pose Estimation from an RGB Image ‣ Recent Advances in 3D Object and Hand Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">22</span></a>) to predict a “latent feature
vector”, which is fed to a Graph CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx15" title="" class="ltx_ref">Defferrard <span class="ltx_text ltx_font_italic">et al.</span>, 2016</a>]</cite>. The motivation for
using a Graph CNN is to predict a 3D mesh for the hand, which can be represented
as a graph. The loss function thus a loss that compares, for synthetic training
images, the mesh predicted by the GraphCNN and the ground truth mesh, and the
predicted 3D pose and the ground truth pose. For real training images, which do
not have ground truth mesh available, they use the pose loss term, but also a
term that compares the predicted 3D mesh with the depth map for the input RGB
image when available, and another term that compares the same predicted 3D mesh
with a mesh predicted from the ground truth 2D heatmaps.</p>
</div>
<figure id="S10.F23" class="ltx_figure"><img src="/html/2006.05927/assets/images/Bouk19.png" id="S10.F23.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="168" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 23: </span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx8" title="" class="ltx_ref">Boukhayma <span class="ltx_text ltx_font_italic">et al.</span>, 2019</a>]</cite> learns to predict
the pose and shape parameters of the hand MANO model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx78" title="" class="ltx_ref">Romero <span class="ltx_text ltx_font_italic">et al.</span>, 2017</a>]</cite>
without 3D annotations on the pose and shape, by minimizing on the
reprojection error of the joints. Image from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx8" title="" class="ltx_ref">Boukhayma <span class="ltx_text ltx_font_italic">et al.</span>, 2019</a>]</cite>.</figcaption>
</figure>
<div id="S10.SS4.p3" class="ltx_para">
<p id="S10.SS4.p3.1" class="ltx_p">The method of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx8" title="" class="ltx_ref">Boukhayma <span class="ltx_text ltx_font_italic">et al.</span>, 2019</a>]</cite> is simpler. It relies on the MANO
model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx78" title="" class="ltx_ref">Romero <span class="ltx_text ltx_font_italic">et al.</span>, 2017</a>]</cite>, which is a popular deformable and parameterized model of
the human hand, created from many captures of real hands. The MANO model
provides a differentiable function that generates a 3D model of a hand, given
pose parameters and shape parameters. It does not require a dataset annotated
with the shape parameters, but instead it can re-uses existing datasets,
annotated only with the 3D hand pose. To do so, they train a network (shown in
Figure <a href="#S10.F23" title="Figure 23 ‣ 10.4 3D Hand Shape and Pose Estimation [Ge et al., 2019, Boukhayma et al., 2019] ‣ 10 3D Hand Pose Estimation from an RGB Image ‣ Recent Advances in 3D Object and Hand Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">23</span></a>) to predict the hand shape and pose parameters, and
the loss function is the 2D distances between the reprojections of the 3D
joints, as computed from the predicted shape and pose parameters and the
annotated 2D joints.</p>
</div>
</section>
<section id="S10.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">10.5 </span>Implementation in MediaPipe</h3>

<div id="S10.SS5.p1" class="ltx_para">
<p id="S10.SS5.p1.1" class="ltx_p">We can also mention a Google Project <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx6" title="" class="ltx_ref">Bazarevsky and Zhang, 2019</a>]</cite> for real-time 3D hand
pose implementation on mobile devices. They first detect the palm, rather than
the full hand: This allows them to only consider squared bounding boxes, and to
avoid confusion between multiple hand detections. The 3D hand pose is directly
predicted as the 3D joint locations, using a model trained on real and synthetic
images.</p>
</div>
<div id="S10.SS5.p2" class="ltx_para">
<p id="S10.SS5.p2.1" class="ltx_p">Unfortunately, the work is not presented in a formal research publication, and
it is difficult to compare their results with the ones obtained by other
methods. However, the implementation is publicly available, and appears to work
well. This may show again that good engineering can make simple approaches
perform well.</p>
</div>
</section>
<section id="S10.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">10.6 </span>Manipulating Virtual Objects</h3>

<div id="S10.SS6.p1" class="ltx_para">
<p id="S10.SS6.p1.1" class="ltx_p">One of the main motivations to track the 3D pose of a hand for an AR system is
to make possible natural interaction with virtual objects. It is therefore
important to be able to exploit the estimated 3D hand pose to compute how the
virtual objects should move when they are in interaction with the user’s hand.</p>
</div>
<div id="S10.SS6.p2" class="ltx_para">
<p id="S10.SS6.p2.1" class="ltx_p">This is not a trivial problem. In the real world, our ability to interact with
objects is due to the presence of friction between the surfaces of the objects
and our hands, as friction is a force that resists motion. However, friction is
very challenging to simulate correctly. Also, nothing prevents the real hand to
penetrate the virtual objects, and make realism fail.</p>
</div>
<div id="S10.SS6.p3" class="ltx_para">
<p id="S10.SS6.p3.1" class="ltx_p">Some approaches rely on heuristics to perform a set of object interactions. It
may be reasonable to focus on object grasping for AR systems, as it is the most
interesting interaction. Some methods are data-driven and synthesize
prerecorded, real hand data to identify the most similar one during runtime from
a predefined
database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx49" title="" class="ltx_ref">Li <span class="ltx_text ltx_font_italic">et al.</span>, 2007</a>, <a href="#bib.bibx57" title="" class="ltx_ref">Miller <span class="ltx_text ltx_font_italic">et al.</span>, 2003</a>, <a href="#bib.bibx77" title="" class="ltx_ref">Rijpkema and
Girard, 1991</a>]</cite>.</p>
</div>
<div id="S10.SS6.p4" class="ltx_para">
<p id="S10.SS6.p4.1" class="ltx_p">A friction model from physics, the <span id="S10.SS6.p4.1.1" class="ltx_text ltx_font_italic">Coulomb-Contensou</span> model was used in
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx90" title="" class="ltx_ref">Talvas <span class="ltx_text ltx_font_italic">et al.</span>, 2015</a>]</cite> to reach more accurate results, but the computational
complexity becomes very high, and not necessarily compatible with real-time
constraints. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx39" title="" class="ltx_ref">Hoell <span class="ltx_text ltx_font_italic">et al.</span>, 2018</a>]</cite> relies on the simpler Coulomb model that appears to
be a good trade-off between realism and tractability. The force applied by the
user are taken proportional to how much their real hand penetrates the virtual
objects. Computing accurately the forces requires very accurate 3D hand poses,
otherwise they can become very unstable.</p>
</div>
<div id="S10.SS6.p5" class="ltx_para">
<p id="S10.SS6.p5.1" class="ltx_p">Very recently, videos demonstrating the real-time interaction system of Oculus
using depth-based hand tracking appeared. No technical details are available
yet, but the system seems to allow for realistic grasping-based interactions.</p>
</div>
</section>
</section>
<section id="S11" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">11 </span>3D Object+Hand Pose Estimation</h2>

<div id="S11.p1" class="ltx_para">
<p id="S11.p1.1" class="ltx_p">We finally turn to the problem of estimating a 3D pose for both a hand and an
object, when the hand directly manipulates the object. Even if the existing
methods are still preliminary, the potential application to AR is very
appealing, as this could offer tangible interfaces by manipulating objects, or
even the possibility to bring real objects into the virtual world.</p>
</div>
<div id="S11.p2" class="ltx_para">
<p id="S11.p2.1" class="ltx_p">This problem still remains very challenging, as as the close contact between the
hand and the object results in mutual occlusions that can be large. Dealing
with egocentric views, which are more relevant for AR applications, is often
even more challenging. Fortunately, there are also physical constraints between
the hand and the object, such as impossibility of penetration but also natural
grasping poses, that may help solving this problem.</p>
</div>
<div id="S11.p3" class="ltx_para">
<p id="S11.p3.1" class="ltx_p">Pioneered approaches joint hand+object pose
estimation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx64" title="" class="ltx_ref">Oikonomidis <span class="ltx_text ltx_font_italic">et al.</span>, 2011</a>, <a href="#bib.bibx103" title="" class="ltx_ref">Wang <span class="ltx_text ltx_font_italic">et al.</span>, 2011</a>, <a href="#bib.bibx4" title="" class="ltx_ref">Ballan <span class="ltx_text ltx_font_italic">et al.</span>, 2012</a>]</cite> typically relied
on frame-by-frame tracking, and in the case of
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx47" title="" class="ltx_ref">Kyriazis and
Argyros, 2013</a>, <a href="#bib.bibx101" title="" class="ltx_ref">Tzionas <span class="ltx_text ltx_font_italic">et al.</span>, 2016</a>]</cite>, also on a physics simulator to
exploit the constraints between hand and object. However, frame-to-frame
tracking, when used alone, may require careful initialization from the user, and
may drift over time. The ability to estimate the 3D poses from a single image
without prior from previous frames or from the user makes tracking more robust.
Given the difficulty of the task, this has been possible only with the use of
Deep Learning.</p>
</div>
<section id="S11.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">11.1 </span>ObMan and HOPS-Net</h3>

<div id="S11.SS1.p1" class="ltx_para">
<p id="S11.SS1.p1.1" class="ltx_p">ObMan <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx32" title="" class="ltx_ref">Hasson <span class="ltx_text ltx_font_italic">et al.</span>, 2019b</a>]</cite> and HOPS-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx46" title="" class="ltx_ref">Kokic <span class="ltx_text ltx_font_italic">et al.</span>, 2019</a>]</cite> almost
simultaneously proposed methods with some similarities for estimating the 3D
pose <em id="S11.SS1.p1.1.1" class="ltx_emph ltx_font_italic">and shape</em> of both a hand and of the object it manipulates from a
single RGB image. Creating realistic training data is the first challenge, and
they both created a dataset of synthetic images of hands and objects using the
<span id="S11.SS1.p1.1.2" class="ltx_text ltx_font_italic">GraspIt!</span> simulation environment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx56" title="" class="ltx_ref">Miller and Allen, 2004</a>]</cite> for generating
realistic grasps given 3D models for the hand and for the object.</p>
</div>
<div id="S11.SS1.p2" class="ltx_para">
<p id="S11.SS1.p2.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx32" title="" class="ltx_ref">Hasson <span class="ltx_text ltx_font_italic">et al.</span>, 2019b</a>]</cite> train their model on their synthetic dataset before fine-tuning
it on <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx19" title="" class="ltx_ref">Garcia-Hernando <span class="ltx_text ltx_font_italic">et al.</span>, 2018</a>]</cite>, which provide annotated real images. To make the
synthetic images more realistic, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx46" title="" class="ltx_ref">Kokic <span class="ltx_text ltx_font_italic">et al.</span>, 2019</a>]</cite> use “Augmented
CycleGAN” <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">Almahairi <span class="ltx_text ltx_font_italic">et al.</span>, 2018</a>]</cite>: The motivation to use Augmented CycleGAN rather
than CycleGAN is that the former can deal with many-to-many mappings, while the
latter considers only one-to-one mappings, and that a synthetic image can
correspond to multiple real images.</p>
</div>
<figure id="S11.F24" class="ltx_figure"><img src="/html/2006.05927/assets/images/hasson19.png" id="S11.F24.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="419" height="216" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 24: </span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx32" title="" class="ltx_ref">Hasson <span class="ltx_text ltx_font_italic">et al.</span>, 2019b</a>]</cite> learns to directly predict
pose and shape parameters for the hand, and shape for the object, using a term
that encourages the hand and the object to be in contact without
interpenetration. Image from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx32" title="" class="ltx_ref">Hasson <span class="ltx_text ltx_font_italic">et al.</span>, 2019b</a>]</cite>.</figcaption>
</figure>
<div id="S11.SS1.p3" class="ltx_para">
<p id="S11.SS1.p3.1" class="ltx_p">As shown in Figure <a href="#S11.F24" title="Figure 24 ‣ 11.1 ObMan and HOPS-Net ‣ 11 3D Object+Hand Pose Estimation ‣ Recent Advances in 3D Object and Hand Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">24</span></a>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx32" title="" class="ltx_ref">Hasson <span class="ltx_text ltx_font_italic">et al.</span>, 2019b</a>]</cite> does not go through an
explicit 2D detection of the object or the hand, by contrast with the other
methods we already discussed. Instead, the method directly predicts from the
input image the pose and shape parameters for the hand (using the MANO model),
and the shape for the object. To predict the object shape, they use
AtlasNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx26" title="" class="ltx_ref">Groueix <span class="ltx_text ltx_font_italic">et al.</span>, 2018</a>]</cite>, a method that can predict a set of 3D points from a
color image. This shape prediction does not require to know the object in
advance, and predicts the shape in the camera coordinates system, there is
therefore no notion of object pose. The loss function to train the architecture
includes a term that prevents intersection between the hand and the object, and
a term that penalizes cases in which the hand is close to the surface of the
object without being in contact.</p>
</div>
<figure id="S11.F25" class="ltx_figure"><img src="/html/2006.05927/assets/images/kokic19.png" id="S11.F25.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="124" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 25: </span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx46" title="" class="ltx_ref">Kokic <span class="ltx_text ltx_font_italic">et al.</span>, 2019</a>]</cite> first predicts the
hand pose and detects the object in 2D, then predicts the object’s pose for
its 2D bounding pose and the hand pose. It also finds a 3D model for the
object from its appearance, and uses it to refine the object pose so that the
object is in contact with the hand. Image from
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx46" title="" class="ltx_ref">Kokic <span class="ltx_text ltx_font_italic">et al.</span>, 2019</a>]</cite>.</figcaption>
</figure>
<div id="S11.SS1.p4" class="ltx_para">
<p id="S11.SS1.p4.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx46" title="" class="ltx_ref">Kokic <span class="ltx_text ltx_font_italic">et al.</span>, 2019</a>]</cite> focuses on objects from specific categories (bottle,
mug, knife, and bowl), and uses Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx34" title="" class="ltx_ref">He <span class="ltx_text ltx_font_italic">et al.</span>, 2017</a>]</cite> to detect and segment the
object: The object does not have to be known as long as it belongs to a known
category. The method also relies on <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx112" title="" class="ltx_ref">Zimmermann and Brox, 2017</a>]</cite> to predict the 3D hand
pose. A network is trained to predict, from the bounding box predicted by
Mask-RCNN and the hand pose (to provide additional constraints), the 3D object
pose and a description vector for the shape. The 3D pose representation depends
on the category to handle symmetries in the object shape, and the description
vector is matched against a database of 3D models to retrieve a 3D model with a
similar shape (see <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx25" title="" class="ltx_ref">Grabner <span class="ltx_text ltx_font_italic">et al.</span>, 2019</a>]</cite> discussed in Section <a href="#S8" title="8 3D Pose Estimation for Object Category ‣ Recent Advances in 3D Object and Hand Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>). A
final refinement step optimizes the object pose so that it is in contact with
the hand.</p>
</div>
<figure id="S11.F26" class="ltx_figure"><img src="/html/2006.05927/assets/images/H_O.png" id="S11.F26.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="156" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 26: </span> The H+O method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx94" title="" class="ltx_ref">Tekin <span class="ltx_text ltx_font_italic">et al.</span>, 2019a</a>]</cite> predicts the
hand and object poses both as a set of 3D locations, to get a unified
representation and simplify the loss function. These 3D locations are
predicted <span id="S11.F26.3.1" class="ltx_text ltx_font_italic">via</span> a 3D grid. Moreover, the predicted 3D locations are
provided to a recurrent neural network, to propagate this information over
time and recognize the action performed by the hand. Image from
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx94" title="" class="ltx_ref">Tekin <span class="ltx_text ltx_font_italic">et al.</span>, 2019a</a>]</cite>.</figcaption>
</figure>
</section>
<section id="S11.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">11.2 </span>H+O <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx94" title="" class="ltx_ref">Tekin <span class="ltx_text ltx_font_italic">et al.</span>, 2019a</a>]</cite>
</h3>

<div id="S11.SS2.p1" class="ltx_para">
<p id="S11.SS2.p1.1" class="ltx_p">H+O <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx94" title="" class="ltx_ref">Tekin <span class="ltx_text ltx_font_italic">et al.</span>, 2019a</a>]</cite> is a method that can predict the 3D poses of a hand and an
object from an image, but it also learns a temporal model to recognize the
performed actions (for example, pouring, opening, or cleaning) and to introduce
temporal constraints to improve the pose estimation. Moreover, the architecture
remains relatively simple, as shown in Figure <a href="#S11.F26" title="Figure 26 ‣ 11.1 ObMan and HOPS-Net ‣ 11 3D Object+Hand Pose Estimation ‣ Recent Advances in 3D Object and Hand Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">26</span></a>. It is trained on
the First-Person Hand Action dataset (see
Section <a href="#S6.SS3" title="6.3 Datasets for Object and Hand Pose Estimation ‣ 6 Datasets ‣ Recent Advances in 3D Object and Hand Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.3</span></a>).</p>
</div>
<div id="S11.SS2.p2" class="ltx_para">
<p id="S11.SS2.p2.1" class="ltx_p">The 3D pose of the object is predicted as the 3D locations of the corners of the
3D bounding box—from this information, it is possible to compute a 3D rotation
and translation, using <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx23" title="" class="ltx_ref">Gower, 1975</a>]</cite> for example. Since the 3D hand pose can
also be predicted as a set of 3D locations, this makes the 2 output of the
network consistent, and avoids a weight to tune between the loss term for the
hand and the loss term for the object. More exactly, the 3D locations are not
predicted directly. The 3D space is split into cells, and for each cell and
each 3D location, it is predicted if the 3D location is within this cell, plus
an offset between a reference corner of the cell and the 3D location—this
offset ensures that the 3D location can be predicted accurately, even with the
space discretization into cells. A confidence is also predicted for each 3D
location prediction. The 3D hand and object poses are computed by taking the 3D
points with the highest confidences.</p>
</div>
<div id="S11.SS2.p3" class="ltx_para">
<p id="S11.SS2.p3.1" class="ltx_p">To enforce both temporal constraints and recognize the actions, the method
relies on a Recurrent Neural Network, and more exactly on an Long Short-Term
Memory (LSTM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx37" title="" class="ltx_ref">Hochreiter and
Schmidhuber, 1997</a>]</cite>. Such network can propagate information from
the predicted 3D locations over time, and it is used here to predict the action
and the nature of the object, in addition to be a way to learn constraints
between the hand and the object.</p>
</div>
</section>
<section id="S11.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">11.3 </span>HOnnotate <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx28" title="" class="ltx_ref">Hampali <span class="ltx_text ltx_font_italic">et al.</span>, 2020</a>]</cite>
</h3>

<div id="S11.SS3.p1" class="ltx_para">
<p id="S11.SS3.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx28" title="" class="ltx_ref">Hampali <span class="ltx_text ltx_font_italic">et al.</span>, 2020</a>]</cite> proposed a method to automatically annotate real images of
hands grasping objects with their 3D poses (see
Figure <a href="#S6.F6" title="Figure 6 ‣ 6.3 Datasets for Object and Hand Pose Estimation ‣ 6 Datasets ‣ Recent Advances in 3D Object and Hand Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>(f)), which works with a single RGB-D camera,
but can exploit more cameras if available for better robustness and
accuracy. The main idea is to optimize jointly all the 3D poses of the hand and
the object over the sequence to exploit temporal consistency. The method is
automated, which means that it can be used easily to labeled new sequences.</p>
</div>
<div id="S11.SS3.p2" class="ltx_para">
<p id="S11.SS3.p2.1" class="ltx_p">The authors use the resulting dataset called HO-3D to learn to predict from a
single RGB image the 3D pose of a hand manipulating an object. This is done by
training a Deep Network to predict the 2D joint locations of the hand along with
the joint direction vectors and lift them to 3D by fitting a MANO model to these
predictions. This reaches good accuracy despite occlusions by the object, even
when the object was not seen during training.</p>
</div>
</section>
</section>
<section id="S12" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">12 </span>The Future of 3D Object and Hand Pose Estimation</h2>

<div id="S12.p1" class="ltx_para">
<p id="S12.p1.1" class="ltx_p">This chapter aimed at demonstrating and explaining the impressive development of
3D pose estimation in computer vision since the early pioneer works, and in the
context of potential applications to Augmented Reality. Methods are becoming
more robust and accurate, while benefiting on fast implementations on GPUs. We
focused on some popular works, but they are only entries to many other works we
could not present here, and that the reader can explore on their own.</p>
</div>
<div id="S12.p2" class="ltx_para">
<p id="S12.p2.1" class="ltx_p">However, as we pointed out at the end of Section <a href="#S7" title="7 Modern Approaches to 3D Object Pose Estimation ‣ Recent Advances in 3D Object and Hand Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, it is
still too early to draw conclusions on what is the best methodology for 3D pose
estimation. Quantitative results not only reflect the contributions of the
methods, and also depend on how much effort the authors put in their
implemetation. As a result, there are sometimes contractory conclusions between
papers. For example, is it important to first detect the object or the hand in
2D first, or not? Are 2D heatmaps important for accuracy, or not? What is the
best pose representation for a 3D pose: A 3D rotation and translation, or a set
of 3D points? Also, performance is not the only aspect here: For example, using
a set of 3D points for both the object and hand poses as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx95" title="" class="ltx_ref">Tekin <span class="ltx_text ltx_font_italic">et al.</span>, 2019b</a>]</cite>
relaxes for tuning the weights in the loss function, which is also interesting
in practice.</p>
</div>
<div id="S12.p3" class="ltx_para">
<p id="S12.p3.1" class="ltx_p">The focus on some public benchmarks may also bias the conclusions on the
performance of the methods in the real world: How well do they perform under
poor lighting? How well do they perform on different cameras without
retraining?</p>
</div>
<div id="S12.p4" class="ltx_para">
<p id="S12.p4.1" class="ltx_p">Another aspect that needs to be solved in the dependence on training sets, which
are complex to create, and to a training stage each time a new object should be
considered. The problem does not really occur for hands, as a large enough
training set for hands would ensure gene ralization to unseen hands. For
objects, variability in terms of shape and appearance is of course much more
important. Some methods have shown some generalization power within known
categories <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx25" title="" class="ltx_ref">Grabner <span class="ltx_text ltx_font_italic">et al.</span>, 2019</a>, <a href="#bib.bibx46" title="" class="ltx_ref">Kokic <span class="ltx_text ltx_font_italic">et al.</span>, 2019</a>]</cite>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx107" title="" class="ltx_ref">Xiang <span class="ltx_text ltx_font_italic">et al.</span>, 2018b</a>]</cite> has shown some
generalization to new objects, but only on simple synthetic images and for known
3D models. Being able to consider instantly (without retraining nor capturing a
3D model) any object in an AR system is a dream that will probably become
accessible in the next years.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx1.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Almahairi <span id="bib.bibx1.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2018<span id="bib.bibx1.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
A. Almahairi, S. Rajeswar, A. Sordoni, P. Bachman, and A. Courville.

</span>
<span class="ltx_bibblock">Augmented CycleGAN: Learning Many-To-Many Mappings from Unpaired
Data.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx1.6.1" class="ltx_text ltx_font_italic">arXiv Preprint</span>, 2018.

</span>
</li>
<li id="bib.bibx2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx2.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Armagan <span id="bib.bibx2.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2020<span id="bib.bibx2.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
A. Armagan, G. Garcia-Hernando, S. Baek, S. Hampali, M. Rad, Z. Zhang, S. Xie,
M. Chen, B. Zhang, F. Xiong, Y. Xiao, Z. Cao, J. Yuan, P. Ren, W. Huang,
H. Sun, M. Hrúz, J. Kanis, Z. Krn̂oul, Q. Wan, S. Li, L. Yang, D. Lee,
A. Yao, W. Zhou, S. Mei, Y. Liu, A. Spurr, U. Iqbal, P. Molchanov,
P. Weinzaepfel, R. Brégier, G. Rogez, V. Lepetit, and T.-K. Kim.

</span>
<span class="ltx_bibblock">Measuring Generalisation to Unseen Viewpoints, Articulations, Shapes
and Objects for 3D Hand Pose Estimation Under Hand-Object Interaction.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx2.6.1" class="ltx_text ltx_font_italic">arXiv Preprint</span>, 2020.

</span>
</li>
<li id="bib.bibx3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx3.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Aubry and
Russell, 2015<span id="bib.bibx3.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
M. Aubry and B. Russell.

</span>
<span class="ltx_bibblock">Understanding Deep Features with Computer-Generated Imagery.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx3.3.1" class="ltx_text ltx_font_italic">Conference on Computer Vision and Pattern Recognition</span>, pages
2875–2883, 2015.

</span>
</li>
<li id="bib.bibx4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx4.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Ballan <span id="bib.bibx4.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2012<span id="bib.bibx4.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
L. Ballan, A. Taneja, J. Gall, L. Van Gool, and M. Pollefeys.

</span>
<span class="ltx_bibblock">Motion Capture of Hands in Action Using Discriminative Salient
Points.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx4.6.1" class="ltx_text ltx_font_italic">European Conference on Computer Vision</span>, pages 640–653,
October 2012.

</span>
</li>
<li id="bib.bibx5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx5.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Bay <span id="bib.bibx5.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2008<span id="bib.bibx5.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
H. Bay, A. Ess, T. Tuytelaars, and L. Van Gool.

</span>
<span class="ltx_bibblock">SURF: Speeded Up Robust Features.

</span>
<span class="ltx_bibblock"><span id="bib.bibx5.6.1" class="ltx_text ltx_font_italic">Computer Vision and Image Understanding</span>, 10(3):346–359, 2008.

</span>
</li>
<li id="bib.bibx6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx6.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Bazarevsky and Zhang, 2019<span id="bib.bibx6.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
V. Bazarevsky and F. Zhang.

</span>
<span class="ltx_bibblock">Google Project on Hand Pose Prediction, 2019.

</span>
</li>
<li id="bib.bibx7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx7.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Billinghurst <span id="bib.bibx7.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2001<span id="bib.bibx7.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
M. Billinghurst, H. Kato, and I. Poupyrev.

</span>
<span class="ltx_bibblock">The Magicbook: A Transitional AR Interface.

</span>
<span class="ltx_bibblock"><span id="bib.bibx7.6.1" class="ltx_text ltx_font_italic">Computers &amp; Graphics</span>, 25:745–753, 10 2001.

</span>
</li>
<li id="bib.bibx8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx8.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Boukhayma <span id="bib.bibx8.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2019<span id="bib.bibx8.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
A. Boukhayma, R. de Bem, and P. H.S. Torr.

</span>
<span class="ltx_bibblock">3D Hand Shape and Pose from Images in the Wild.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx8.6.1" class="ltx_text ltx_font_italic">Conference on Computer Vision and Pattern Recognition</span>, 2019.

</span>
</li>
<li id="bib.bibx9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx9.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Brachmann <span id="bib.bibx9.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2014<span id="bib.bibx9.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
E. Brachmann, A. Krull, F. Michel, S. Gumhold, J. Shotton, and C. Rother.

</span>
<span class="ltx_bibblock">Learning 6D Object Pose Estimation Using 3D Object Coordinates.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx9.6.1" class="ltx_text ltx_font_italic">European Conference on Computer Vision</span>, 2014.

</span>
</li>
<li id="bib.bibx10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx10.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Brachmann <span id="bib.bibx10.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2016<span id="bib.bibx10.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
E. Brachmann, F. Michel, A. Krull, M. M. Yang, S. Gumhold, and C. Rother.

</span>
<span class="ltx_bibblock">Uncertainty-Driven 6D Pose Estimation of Objects and Scenes from a
Single RGB Image.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx10.6.1" class="ltx_text ltx_font_italic">Conference on Computer Vision and Pattern Recognition</span>, 2016.

</span>
</li>
<li id="bib.bibx11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx11.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Brégier <span id="bib.bibx11.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2018<span id="bib.bibx11.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
R. Brégier, F. Devernay, L. Leyrit, and J.L. Crowley.

</span>
<span class="ltx_bibblock">Defining the Pose of Any 3D Rigid Object and an Associated
Distance.

</span>
<span class="ltx_bibblock"><span id="bib.bibx11.6.1" class="ltx_text ltx_font_italic">International Journal of Computer Vision</span>, 2018.

</span>
</li>
<li id="bib.bibx12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx12.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Breiman, 1996<span id="bib.bibx12.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
L. Breiman.

</span>
<span class="ltx_bibblock">Bagging Predictors.

</span>
<span class="ltx_bibblock"><span id="bib.bibx12.3.1" class="ltx_text ltx_font_italic">Machine Learning</span>, 24(2):123–140, 1996.

</span>
</li>
<li id="bib.bibx13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx13.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Calli <span id="bib.bibx13.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2017<span id="bib.bibx13.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
B. Calli, A. Singh, J. Bruce, A. Walsman, K. Konolige, S. Srinivasa, P. Abbeel,
and A. M. Dollar.

</span>
<span class="ltx_bibblock">Yale-Cmu-Berkeley Dataset for Robotic Manipulation Research.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx13.6.1" class="ltx_text ltx_font_italic">International Journal of Robotics Research</span>, 2017.

</span>
</li>
<li id="bib.bibx14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx14.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Chang <span id="bib.bibx14.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2015<span id="bib.bibx14.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li,
S. Savarese, M. Savva, S. Song, H. Su, J. Xiao, L. Yi, and F. Yu.

</span>
<span class="ltx_bibblock">ShapeNet: An Information-Rich 3D Model Repository.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx14.6.1" class="ltx_text ltx_font_italic">arXiv Preprint</span>, 2015.

</span>
</li>
<li id="bib.bibx15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx15.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Defferrard <span id="bib.bibx15.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2016<span id="bib.bibx15.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
M. Defferrard, X. Bresson, and P. Vandergheynst.

</span>
<span class="ltx_bibblock">Convolutional Neural Networks on Graphs with Fast Localized Spectral
Filtering.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx15.6.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, pages
3844–3852, 2016.

</span>
</li>
<li id="bib.bibx16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx16.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Drummond and Cipolla, 2002<span id="bib.bibx16.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
T. Drummond and R. Cipolla.

</span>
<span class="ltx_bibblock">Real-Time Visual Tracking of Complex Structures.

</span>
<span class="ltx_bibblock"><span id="bib.bibx16.3.1" class="ltx_text ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>,
27(7):932–946, July 2002.

</span>
</li>
<li id="bib.bibx17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx17.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Eldan and Shamir, 2016<span id="bib.bibx17.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
R. Eldan and O. Shamir.

</span>
<span class="ltx_bibblock">The Power of Depth for Feedforward Neural Networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx17.3.1" class="ltx_text ltx_font_italic">Conference on Learning Theory</span>, 2016.

</span>
</li>
<li id="bib.bibx18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx18.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Gao <span id="bib.bibx18.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2003<span id="bib.bibx18.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
X. Gao, X. Hou, J. Tang, and H. Cheng.

</span>
<span class="ltx_bibblock">Complete Solution Classification for the Perspective-Three-Point
Problem.

</span>
<span class="ltx_bibblock"><span id="bib.bibx18.6.1" class="ltx_text ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>,
25(8):930–943, 2003.

</span>
</li>
<li id="bib.bibx19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx19.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Garcia-Hernando <span id="bib.bibx19.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2018<span id="bib.bibx19.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
G. Garcia-Hernando, S. Yuan, S. Baek, and T.-K. Kim.

</span>
<span class="ltx_bibblock">First-Person Hand Action Benchmark with RGB-D Videos and 3D Hand
Pose Annotations.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx19.6.1" class="ltx_text ltx_font_italic">Conference on Computer Vision and Pattern Recognition</span>, pages
409–419, 2018.

</span>
</li>
<li id="bib.bibx20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx20.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Ge <span id="bib.bibx20.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2019<span id="bib.bibx20.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
L. Ge, Z. Ren, Y. Li, Z. Xue, Y. Wang, J. Cai, and J. Yuan.

</span>
<span class="ltx_bibblock">3D Hand Shape and Pose Estimation from a Single RGB Image.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx20.6.1" class="ltx_text ltx_font_italic">Conference on Computer Vision and Pattern Recognition</span>, 2019.

</span>
</li>
<li id="bib.bibx21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx21.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Goodfellow <span id="bib.bibx21.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2014<span id="bib.bibx21.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
A. Courville, and A. Y. Bengio.

</span>
<span class="ltx_bibblock">Generative Adversarial Nets.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx21.6.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, 2014.

</span>
</li>
<li id="bib.bibx22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx22.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Goodfellow <span id="bib.bibx22.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2016<span id="bib.bibx22.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
I. Goodfellow, Y. Bengio, and A. Courville.

</span>
<span class="ltx_bibblock"><span id="bib.bibx22.6.1" class="ltx_text ltx_font_italic">Deep Learning</span>.

</span>
<span class="ltx_bibblock">MIT Press, 2016.

</span>
</li>
<li id="bib.bibx23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx23.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Gower, 1975<span id="bib.bibx23.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
J. C. Gower.

</span>
<span class="ltx_bibblock">Generalized Procrustes Analysis.

</span>
<span class="ltx_bibblock"><span id="bib.bibx23.3.1" class="ltx_text ltx_font_italic">Psychometrika</span>, 40(1):33–51, 1975.

</span>
</li>
<li id="bib.bibx24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx24.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Grabner <span id="bib.bibx24.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2018<span id="bib.bibx24.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
A. Grabner, P. M. Roth, and V. Lepetit.

</span>
<span class="ltx_bibblock">3D Pose Estimation and 3D Model Retrieval for Objects in the Wild.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx24.6.1" class="ltx_text ltx_font_italic">Conference on Computer Vision and Pattern Recognition</span>, 2018.

</span>
</li>
<li id="bib.bibx25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx25.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Grabner <span id="bib.bibx25.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2019<span id="bib.bibx25.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
A. Grabner, P. M. Roth, and V. Lepetit.

</span>
<span class="ltx_bibblock">Location Field Descriptors: Single Image 3D Model Retrieval in the
Wild.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx25.6.1" class="ltx_text ltx_font_italic">International Conference on 3D Vision</span>, 2019.

</span>
</li>
<li id="bib.bibx26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx26.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Groueix <span id="bib.bibx26.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2018<span id="bib.bibx26.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
T. Groueix, M. Fisher, V. Kim, B. Russell, and M. Aubry.

</span>
<span class="ltx_bibblock">Atlasnet: A Papier-Mâché Approach to Learning 3D Surface
Generation.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx26.6.1" class="ltx_text ltx_font_italic">Conference on Computer Vision and Pattern Recognition</span>, 2018.

</span>
</li>
<li id="bib.bibx27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx27.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Gustus <span id="bib.bibx27.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2012<span id="bib.bibx27.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
A. Gustus, G. Stillfried, J. Visser, H. Jörntell, and P. van der Smagt.

</span>
<span class="ltx_bibblock">Human Hand Modelling: Kinematics, Dynamics, Applications.

</span>
<span class="ltx_bibblock"><span id="bib.bibx27.6.1" class="ltx_text ltx_font_italic">Biological Cybernetics</span>, 106(11):741–755, 2012.

</span>
</li>
<li id="bib.bibx28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx28.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Hampali <span id="bib.bibx28.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2020<span id="bib.bibx28.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
S. Hampali, M. Rad, M. Oberweger, and A. V. Lepetit.

</span>
<span class="ltx_bibblock">HOnnotate: A Method for 3D Annotation of Hand and Object Poses.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx28.6.1" class="ltx_text ltx_font_italic">Conference on Computer Vision and Pattern Recognition</span>, 2020.

</span>
</li>
<li id="bib.bibx29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx29.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Harris and Stennett, 1990<span id="bib.bibx29.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
C. Harris and C. Stennett.

</span>
<span class="ltx_bibblock">RAPiD-A Video Rate Object Tracker.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx29.3.1" class="ltx_text ltx_font_italic">British Machine Vision Conference</span>, 1990.

</span>
</li>
<li id="bib.bibx30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx30.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Harris and Stephens, 1988<span id="bib.bibx30.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
C.G. Harris and M.J. Stephens.

</span>
<span class="ltx_bibblock">A Combined Corner and Edge Detector.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx30.3.1" class="ltx_text ltx_font_italic">Fourth Alvey Vision Conference</span>, 1988.

</span>
</li>
<li id="bib.bibx31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx31.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Hasson <span id="bib.bibx31.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2019a<span id="bib.bibx31.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Y. Hasson, G. Varol, D. Tzionas, I. Kalevatykh, M. J. Black, I. Laptev, and
C. Schmid.

</span>
<span class="ltx_bibblock">Learning Joint Reconstruction of Hands and Manipulated Objects.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx31.6.1" class="ltx_text ltx_font_italic">Conference on Computer Vision and Pattern Recognition</span>, pages
11807–11816, 2019.

</span>
</li>
<li id="bib.bibx32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx32.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Hasson <span id="bib.bibx32.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2019b<span id="bib.bibx32.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Y. Hasson, G. Varol, D. Tzionas, I. Kalevatykh, M. J. Black, I. Laptev, and
C. Schmid.

</span>
<span class="ltx_bibblock">Learning Joint Reconstruction of Hands and Manipulated Objects.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx32.6.1" class="ltx_text ltx_font_italic">Conference on Computer Vision and Pattern Recognition</span>, pages
11807–11816, 2019.

</span>
</li>
<li id="bib.bibx33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx33.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>He <span id="bib.bibx33.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2016<span id="bib.bibx33.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
K. He, X. Zhang, S. Ren, and J. Sun.

</span>
<span class="ltx_bibblock">Deep Residual Learning for Image Recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx33.6.1" class="ltx_text ltx_font_italic">Conference on Computer Vision and Pattern Recognition</span>, 2016.

</span>
</li>
<li id="bib.bibx34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx34.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>He <span id="bib.bibx34.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2017<span id="bib.bibx34.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
K. He, G. Gkioxari, P. Dollár, and R. Girshick.

</span>
<span class="ltx_bibblock">Mask R-CNN.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx34.6.1" class="ltx_text ltx_font_italic">International Conference on Computer Vision</span>, 2017.

</span>
</li>
<li id="bib.bibx35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx35.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Hinterstoisser <span id="bib.bibx35.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2012a<span id="bib.bibx35.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
S. Hinterstoisser, C. Cagniart, S. Ilic, P. F. Sturm, N. Navab, P. Fua, and
V. Lepetit.

</span>
<span class="ltx_bibblock">Gradient Response Maps for Real-Time Detection of Textureless
Objects.

</span>
<span class="ltx_bibblock"><span id="bib.bibx35.6.1" class="ltx_text ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>,
34, May 2012.

</span>
</li>
<li id="bib.bibx36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx36.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Hinterstoisser <span id="bib.bibx36.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2012b<span id="bib.bibx36.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
S. Hinterstoisser, V. Lepetit, S. Ilic, S. Holzer, G. R. Bradski, K. Konolige,
and N. Navab.

</span>
<span class="ltx_bibblock">Model Based Training, Detection and Pose Estimation of Texture-Less
3D Objects in Heavily Cluttered Scenes.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx36.6.1" class="ltx_text ltx_font_italic">Asian Conference on Computer Vision</span>, 2012.

</span>
</li>
<li id="bib.bibx37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx37.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Hochreiter and
Schmidhuber, 1997<span id="bib.bibx37.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
S. Hochreiter and J. Schmidhuber.

</span>
<span class="ltx_bibblock">Long Short-Term Memory.

</span>
<span class="ltx_bibblock"><span id="bib.bibx37.3.1" class="ltx_text ltx_font_italic">Neural Computation</span>, 9(8):1735–1780, 1997.

</span>
</li>
<li id="bib.bibx38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx38.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Hodan <span id="bib.bibx38.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2016<span id="bib.bibx38.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
T. Hodan, P. Haluza, S. Obdrzalek, J. Matas, M. Lourakis, and X. Zabulis.

</span>
<span class="ltx_bibblock">On Evaluation of 6D Object Pose Estimation.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx38.6.1" class="ltx_text ltx_font_italic">European Conference on Computer Vision</span>, 2016.

</span>
</li>
<li id="bib.bibx39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx39.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Hoell <span id="bib.bibx39.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2018<span id="bib.bibx39.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
M. Hoell, M. Oberweger, C. Arth, and A. V. Lepetit.

</span>
<span class="ltx_bibblock">Efficient Physics-Based Implementation for Realistic Hand-Object
Interaction in Virtual Reality.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx39.6.1" class="ltx_text ltx_font_italic">IEEE Conference on Virtual Reality</span>, 2018.

</span>
</li>
<li id="bib.bibx40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx40.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Hornik <span id="bib.bibx40.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 1989<span id="bib.bibx40.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
K. Hornik, M. Stinchcombe, and H. White.

</span>
<span class="ltx_bibblock">Multilayer Feedforward Networks Are Universal Approximators.

</span>
<span class="ltx_bibblock"><span id="bib.bibx40.6.1" class="ltx_text ltx_font_italic">Neural Networks</span>, 1989.

</span>
</li>
<li id="bib.bibx41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx41.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Hu <span id="bib.bibx41.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2019<span id="bib.bibx41.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Y. Hu, J. Hugonot, P. Fua, and M. Salzmann.

</span>
<span class="ltx_bibblock">Segmentation-Driven 6D Object Pose Estimation.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx41.6.1" class="ltx_text ltx_font_italic">Conference on Computer Vision and Pattern Recognition</span>, 2019.

</span>
</li>
<li id="bib.bibx42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx42.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Iqbal <span id="bib.bibx42.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2018<span id="bib.bibx42.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
U. Iqbal, P. Molchanov, T. Breuel, J. Gall, and J. Kautz.

</span>
<span class="ltx_bibblock">Hand Pose Estimation via Latent 2.5D Heatmap Regression.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx42.6.1" class="ltx_text ltx_font_italic">European Conference on Computer Vision</span>, 2018.

</span>
</li>
<li id="bib.bibx43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx43.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Izadinia <span id="bib.bibx43.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2017<span id="bib.bibx43.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
H. Izadinia, Q. Shan, and S. Seitz.

</span>
<span class="ltx_bibblock">IM2CAD.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx43.6.1" class="ltx_text ltx_font_italic">Conference on Computer Vision and Pattern Recognition</span>, 2017.

</span>
</li>
<li id="bib.bibx44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx44.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Kehl <span id="bib.bibx44.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2017<span id="bib.bibx44.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
W. Kehl, F. Manhardt, F. Tombari, S. Ilic, and N. Navab.

</span>
<span class="ltx_bibblock">SSD-6D: Making Rgb-Based 3D Detection and 6D Pose Estimation Great
Again.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx44.6.1" class="ltx_text ltx_font_italic">International Conference on Computer Vision</span>, 2017.

</span>
</li>
<li id="bib.bibx45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx45.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Kim <span id="bib.bibx45.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2010<span id="bib.bibx45.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
K. Kim, M. Grundmann, A. Shamir, I. Matthews, J. Hodgins, and I. Essa.

</span>
<span class="ltx_bibblock">Motion Fields to Predict Play Evolution in Dynamic Sport Scenes.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx45.6.1" class="ltx_text ltx_font_italic">Conference on Computer Vision and Pattern Recognition</span>, 2010.

</span>
</li>
<li id="bib.bibx46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx46.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Kokic <span id="bib.bibx46.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2019<span id="bib.bibx46.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
M. Kokic, D. Kragic, and J. Bohg.

</span>
<span class="ltx_bibblock">Learning to Estimate Pose and Shape of Hand-Held Objects from RGB
Images.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx46.6.1" class="ltx_text ltx_font_italic">International Conference on Intelligent Robots and Systems</span>,
pages 3980–3987, 11 2019.

</span>
</li>
<li id="bib.bibx47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx47.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Kyriazis and
Argyros, 2013<span id="bib.bibx47.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
N. Kyriazis and A. A. Argyros.

</span>
<span class="ltx_bibblock">Physically Plausible 3D Scene Tracking: The Single Actor
Hypothesis.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx47.3.1" class="ltx_text ltx_font_italic">Conference on Computer Vision and Pattern Recognition</span>, 2013.

</span>
</li>
<li id="bib.bibx48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx48.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Lepetit <span id="bib.bibx48.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2005<span id="bib.bibx48.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
V. Lepetit, P. Lagger, and P. Fua.

</span>
<span class="ltx_bibblock">Randomized Trees for Real-Time Keypoint Recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx48.6.1" class="ltx_text ltx_font_italic">Conference on Computer Vision and Pattern Recognition</span>, June
2005.

</span>
</li>
<li id="bib.bibx49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx49.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Li <span id="bib.bibx49.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2007<span id="bib.bibx49.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Y. Li, J. L. Fu, and N. S. Pollard.

</span>
<span class="ltx_bibblock">Data-Driven Grasp Synthesis Using Shape Matching and Task-Based
Pruning.

</span>
<span class="ltx_bibblock"><span id="bib.bibx49.6.1" class="ltx_text ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics</span>,
13(4):732–747, July 2007.

</span>
</li>
<li id="bib.bibx50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx50.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Li <span id="bib.bibx50.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2018<span id="bib.bibx50.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Y. Li, G. Wang, X. Ji, Y. Xiang, and D. Fox.

</span>
<span class="ltx_bibblock">DeepIM: Deep Iterative Matching for 6D Pose Estimation.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx50.6.1" class="ltx_text ltx_font_italic">European Conference on Computer Vision</span>, 2018.

</span>
</li>
<li id="bib.bibx51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx51.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Lin <span id="bib.bibx51.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2014<span id="bib.bibx51.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
T-.Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Dollár, and C.L. Zitnick.

</span>
<span class="ltx_bibblock">Microsoft COCO: Common Objects in Context.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx51.6.1" class="ltx_text ltx_font_italic">European Conference on Computer Vision</span>, pages 740–755,
2014.

</span>
</li>
<li id="bib.bibx52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx52.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Liu <span id="bib.bibx52.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2016<span id="bib.bibx52.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S.E. Reed, C.-Y. Fu, and A.C. Berg.

</span>
<span class="ltx_bibblock">SSD: Single Shot Multibox Detector.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx52.6.1" class="ltx_text ltx_font_italic">European Conference on Computer Vision</span>, 2016.

</span>
</li>
<li id="bib.bibx53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx53.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Lowe, 1991<span id="bib.bibx53.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
D. G. Lowe.

</span>
<span class="ltx_bibblock">Fitting Parameterized Three-Dimensional Models to Images.

</span>
<span class="ltx_bibblock"><span id="bib.bibx53.3.1" class="ltx_text ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>,
13(5):441–450, June 1991.

</span>
</li>
<li id="bib.bibx54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx54.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Lowe, 2001<span id="bib.bibx54.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
D.G. Lowe.

</span>
<span class="ltx_bibblock">Local Feature View Clustering for 3D Object Recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx54.3.1" class="ltx_text ltx_font_italic">Conference on Computer Vision and Pattern Recognition</span>, pages
682–688, 2001.

</span>
</li>
<li id="bib.bibx55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx55.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Lowe, 2004<span id="bib.bibx55.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
D.G. Lowe.

</span>
<span class="ltx_bibblock">Distinctive Image Features from Scale-Invariant Keypoints.

</span>
<span class="ltx_bibblock"><span id="bib.bibx55.3.1" class="ltx_text ltx_font_italic">International Journal of Computer Vision</span>, 20(2), 2004.

</span>
</li>
<li id="bib.bibx56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx56.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Miller and Allen, 2004<span id="bib.bibx56.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
A.T. Miller and P.K. Allen.

</span>
<span class="ltx_bibblock">Graspit! a Versatile Simulator for Robotic Grasping.

</span>
<span class="ltx_bibblock"><span id="bib.bibx56.3.1" class="ltx_text ltx_font_italic">IEEE Robotics &amp; Automation Magazine</span>, 2004.

</span>
</li>
<li id="bib.bibx57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx57.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Miller <span id="bib.bibx57.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2003<span id="bib.bibx57.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
A. T. Miller, S. Knoop, H. I. Christensen, and P. K. Allen.

</span>
<span class="ltx_bibblock">Automatic Grasp Planning Using Shape Primitives.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx57.6.1" class="ltx_text ltx_font_italic">Proc. ICRA</span>, pages 1824–1829, 2003.

</span>
</li>
<li id="bib.bibx58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx58.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Moon <span id="bib.bibx58.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2018<span id="bib.bibx58.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
G. Moon, J. Y. Chang, and K. M. Lee.

</span>
<span class="ltx_bibblock">V2v-Posenet: Voxel-To-Voxel Prediction Network for Accurate 3D Hand
and Human Pose Estimation from a Single Depth Map.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx58.6.1" class="ltx_text ltx_font_italic">Conference on Computer Vision and Pattern Recognition</span>, 2018.

</span>
</li>
<li id="bib.bibx59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx59.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Mueller <span id="bib.bibx59.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2018a<span id="bib.bibx59.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
F. Mueller, F. Bernard, O. Sotnychenko, D. Mehta, S. Sridhar, D. Casas, and
A. C. Theobalt.

</span>
<span class="ltx_bibblock">GANerated Hands for Real-Time 3D Hand Tracking from Monocular
RGB.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx59.6.1" class="ltx_text ltx_font_italic">Conference on Computer Vision and Pattern Recognition</span>, 2018.

</span>
</li>
<li id="bib.bibx60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx60.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Mueller <span id="bib.bibx60.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2018b<span id="bib.bibx60.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
F. Mueller, F. Bernard, O. Sotnychenko, D. Mehta, S. Sridhar, D. Casas, and
C. Theobalt.

</span>
<span class="ltx_bibblock">GANerated Hands for Real-Time 3D Hand Tracking from Monocular
RGB.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx60.6.1" class="ltx_text ltx_font_italic">Conference on Computer Vision and Pattern Recognition</span>, pages
49–59, 2018.

</span>
</li>
<li id="bib.bibx61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx61.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Oberweger <span id="bib.bibx61.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2015<span id="bib.bibx61.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
M. Oberweger, P. Wohlhart, and V. Lepetit.

</span>
<span class="ltx_bibblock">Training a Feedback Loop for Hand Pose Estimation.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx61.6.1" class="ltx_text ltx_font_italic">International Conference on Computer Vision</span>, 2015.

</span>
</li>
<li id="bib.bibx62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx62.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Oberweger <span id="bib.bibx62.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2017<span id="bib.bibx62.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
M. Oberweger, P. Wohlhart, and V. Lepeti.

</span>
<span class="ltx_bibblock">DeepPrior++: Improving Fast and Accurate 3D Hand Pose Estimation.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx62.6.1" class="ltx_text ltx_font_italic">International Conference on Computer Vision</span>, page 2, 2017.

</span>
</li>
<li id="bib.bibx63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx63.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Oberweger <span id="bib.bibx63.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2018<span id="bib.bibx63.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
M. Oberweger, M. Rad, and V. Lepetit.

</span>
<span class="ltx_bibblock">Making Deep Heatmaps Robust to Partial Occlusions for 3D Object Pose
Estimation.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx63.6.1" class="ltx_text ltx_font_italic">European Conference on Computer Vision</span>, 2018.

</span>
</li>
<li id="bib.bibx64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx64.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Oikonomidis <span id="bib.bibx64.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2011<span id="bib.bibx64.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
I. Oikonomidis, N. Kyriazis, and A. A. Argyros.

</span>
<span class="ltx_bibblock">Full DoF Tracking of a Hand Interacting with an Object by Modeling
Occlusions and Physical Constraints.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx64.6.1" class="ltx_text ltx_font_italic">International Conference on Computer Vision</span>, pages
2088–2095, 2011.

</span>
</li>
<li id="bib.bibx65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx65.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Park <span id="bib.bibx65.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2019a<span id="bib.bibx65.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
J. J. Park, P. Florence, J. Straub, R. Newcombe, and A. S. Lovegrove.

</span>
<span class="ltx_bibblock">Deepsdf: Learning Continuous Signed Distance Functions for Shape
Representation.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx65.6.1" class="ltx_text ltx_font_italic">Conference on Computer Vision and Pattern Recognition</span>, 2019.

</span>
</li>
<li id="bib.bibx66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx66.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Park <span id="bib.bibx66.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2019b<span id="bib.bibx66.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
K. Park, T. Patten, and M. Vincze.

</span>
<span class="ltx_bibblock">Pix2pose: Pixel-Wise Coordinate Regression of Objects for 6D Pose
Estimation.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx66.6.1" class="ltx_text ltx_font_italic">International Conference on Computer Vision</span>, 2019.

</span>
</li>
<li id="bib.bibx67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx67.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Pavlakos <span id="bib.bibx67.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2017<span id="bib.bibx67.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
G. Pavlakos, X. Zhou, A. Chan, K. Derpanis, and K. Daniilidis.

</span>
<span class="ltx_bibblock">6-DoF Object Pose from Semantic Keypoints.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx67.6.1" class="ltx_text ltx_font_italic">International Conference on Robotics and Automation</span>, pages
2011–2018, 2017.

</span>
</li>
<li id="bib.bibx68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx68.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Peng <span id="bib.bibx68.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2019<span id="bib.bibx68.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
S. Peng, Y. Liu, Q. Huang, H. Bao, and X. Zhou.

</span>
<span class="ltx_bibblock">Pvnet: Pixel-Wise Voting Network for 6DoF Pose Estimation.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx68.6.1" class="ltx_text ltx_font_italic">Conference on Computer Vision and Pattern Recognition</span>, 2019.

</span>
</li>
<li id="bib.bibx69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx69.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Pepik <span id="bib.bibx69.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2015<span id="bib.bibx69.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
B. Pepik, M. Stark, P. Gehler, T. Ritschel, and B. Schiele.

</span>
<span class="ltx_bibblock">3D Object Class Detection in the Wild.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx69.6.1" class="ltx_text ltx_font_italic">Conference on Computer Vision and Pattern Recognition</span>, 2015.

</span>
</li>
<li id="bib.bibx70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx70.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Pinkus, 1999<span id="bib.bibx70.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
A. Pinkus.

</span>
<span class="ltx_bibblock">Approximation Theory of the MLP Model in Neural Networks.

</span>
<span class="ltx_bibblock"><span id="bib.bibx70.3.1" class="ltx_text ltx_font_italic">Acta Numerica</span>, 1999.

</span>
</li>
<li id="bib.bibx71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx71.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Poirson <span id="bib.bibx71.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2016<span id="bib.bibx71.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
P. Poirson, P. Ammirato, C.-Y. Fu, W. Liu, J. Kosecka, and A. C. Berg.

</span>
<span class="ltx_bibblock">Fast Single Shot Detection and Pose Estimation.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx71.6.1" class="ltx_text ltx_font_italic">International Conference on 3D Vision</span>, 2016.

</span>
</li>
<li id="bib.bibx72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx72.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Pumarola <span id="bib.bibx72.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2018<span id="bib.bibx72.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
A. Pumarola, A. Agudo, L. Porzi, A. Sanfeliu, V. Lepetit, and F. Moreno-Noguer.

</span>
<span class="ltx_bibblock">Geometry-Aware Network for Non-Rigid Shape Prediction from a Single
View.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx72.6.1" class="ltx_text ltx_font_italic">Conference on Computer Vision and Pattern Recognition</span>, 2018.

</span>
</li>
<li id="bib.bibx73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx73.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Rad and Lepetit, 2017<span id="bib.bibx73.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
M. Rad and V. Lepetit.

</span>
<span class="ltx_bibblock">BB8: A Scalable, Accurate, Robust to Partial Occlusion Method for
Predicting the 3D Poses of Challenging Objects Without Using Depth.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx73.3.1" class="ltx_text ltx_font_italic">International Conference on Computer Vision</span>, 2017.

</span>
</li>
<li id="bib.bibx74" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx74.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Rad <span id="bib.bibx74.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2018<span id="bib.bibx74.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
M. Rad, M. Oberweger, and V. Lepetit.

</span>
<span class="ltx_bibblock">Feature Mapping for Learning Fast and Accurate 3D Pose Inference
from Synthetic Images.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx74.6.1" class="ltx_text ltx_font_italic">Conference on Computer Vision and Pattern Recognition</span>, 2018.

</span>
</li>
<li id="bib.bibx75" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx75.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Redmon and Farhadi, 2017<span id="bib.bibx75.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
J. Redmon and A. Farhadi.

</span>
<span class="ltx_bibblock">Yolo9000: Better, Faster, Stronger.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx75.3.1" class="ltx_text ltx_font_italic">arXiv Preprint</span>, 2017.

</span>
</li>
<li id="bib.bibx76" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx76.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Redmon <span id="bib.bibx76.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2016<span id="bib.bibx76.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
J. Redmon, S. Divvala, R. Girshick, and A. Farhadi.

</span>
<span class="ltx_bibblock">You Only Look Once: Unified, Real-Time Object Detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx76.6.1" class="ltx_text ltx_font_italic">Conference on Computer Vision and Pattern Recognition</span>, 2016.

</span>
</li>
<li id="bib.bibx77" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx77.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Rijpkema and
Girard, 1991<span id="bib.bibx77.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
H. Rijpkema and M. Girard.

</span>
<span class="ltx_bibblock">Computer Animation of Knowledge-Based Human Grasping.

</span>
<span class="ltx_bibblock"><span id="bib.bibx77.3.1" class="ltx_text ltx_font_italic">SIGGRAPH Computer Graphics</span>, 25(4):339–348, July 1991.

</span>
</li>
<li id="bib.bibx78" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx78.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Romero <span id="bib.bibx78.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2017<span id="bib.bibx78.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
J. Romero, D. Tzionas, and M. J. Black.

</span>
<span class="ltx_bibblock">Embodied Hands: Modeling and Capturing Hands and Bodies Together.

</span>
<span class="ltx_bibblock"><span id="bib.bibx78.6.1" class="ltx_text ltx_font_italic">ACM Transactions on Graphics (TOG)</span>, 2017.

</span>
</li>
<li id="bib.bibx79" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx79.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Rublee <span id="bib.bibx79.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2011<span id="bib.bibx79.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
E. Rublee, V. Rabaud, K. Konolidge, and G. Bradski.

</span>
<span class="ltx_bibblock">ORB: An Efficient Alternative to SIFT or SURF.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx79.6.1" class="ltx_text ltx_font_italic">International Conference on Computer Vision</span>, 2011.

</span>
</li>
<li id="bib.bibx80" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx80.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Russakovsky <span id="bib.bibx80.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2015<span id="bib.bibx80.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
O. Russakovsky, J. Deng, H. Su, J. Krause, S.Satheesh, S. Ma, Z. Huang,
A. Karpathy, A. Khosla, M. Bernstein, A.C. Berg, and L. Fei-Fei.

</span>
<span class="ltx_bibblock">ImageNet Large Scale Visual Recognition Challenge.

</span>
<span class="ltx_bibblock"><span id="bib.bibx80.6.1" class="ltx_text ltx_font_italic">International Journal of Computer Vision</span>, 115(3):211–252,
2015.

</span>
</li>
<li id="bib.bibx81" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx81.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Salzmann and Fua, 2010<span id="bib.bibx81.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
M. Salzmann and P. Fua.

</span>
<span class="ltx_bibblock"><span id="bib.bibx81.3.1" class="ltx_text ltx_font_italic">Deformable Surface 3D Reconstruction from Monocular Images</span>.

</span>
<span class="ltx_bibblock">Morgan-Claypool, 2010.

</span>
</li>
<li id="bib.bibx82" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx82.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Sharp <span id="bib.bibx82.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2015<span id="bib.bibx82.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
T. Sharp, C. Keskin, D. P. Robertson, J. Taylor, J. Shotton, D. Kim,
C. Rhemann, I. Leichter, A. Vinnikov, Y. Wei, D. Freedman, P. Kohli,
E. Krupka, A. W. Fitzgibbon, and A. S. Izadi.

</span>
<span class="ltx_bibblock">Accurate, Robust, and Flexible Real-Time Hand Tracking.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx82.6.1" class="ltx_text ltx_font_italic">ACM Conference on Human Factors in Computing Systems</span>, 2015.

</span>
</li>
<li id="bib.bibx83" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx83.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Shin <span id="bib.bibx83.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2018<span id="bib.bibx83.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
D. Shin, C. C. Fowlkes, and D. Hoiem.

</span>
<span class="ltx_bibblock">Pixels, Voxels, and Views: A Study of Shape Representations for
Single View 3D Object Shape Prediction.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx83.6.1" class="ltx_text ltx_font_italic">Conference on Computer Vision and Pattern Recognition</span>, 2018.

</span>
</li>
<li id="bib.bibx84" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx84.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Shotton <span id="bib.bibx84.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2013<span id="bib.bibx84.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
J. Shotton, R. Girshick, A. Fitzgibbon, T. Sharp, M. Cook, M. Finocchio,
R. Moore, P. Kohli, A. Criminisi, A. Kipman, and A. Blake.

</span>
<span class="ltx_bibblock">Efficient Human Pose Estimation from Single Depth Images.

</span>
<span class="ltx_bibblock"><span id="bib.bibx84.6.1" class="ltx_text ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>,
35(12):2821–2840, 2013.

</span>
</li>
<li id="bib.bibx85" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx85.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Simon <span id="bib.bibx85.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2017<span id="bib.bibx85.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
T. Simon, H. Joo, I. A. Matthews, and A. Y. Sheikh.

</span>
<span class="ltx_bibblock">Hand Keypoint Detection in Single Images Using Multiview
Bootstrapping.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx85.6.1" class="ltx_text ltx_font_italic">Conference on Computer Vision and Pattern Recognition</span>, 2017.

</span>
</li>
<li id="bib.bibx86" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx86.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Sun <span id="bib.bibx86.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2015<span id="bib.bibx86.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
X. Sun, Y. Wei, S. Liang, X. Tang, and J. Sun.

</span>
<span class="ltx_bibblock">Cascaded Hand Pose Regression.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx86.6.1" class="ltx_text ltx_font_italic">Conference on Computer Vision and Pattern Recognition</span>, 2015.

</span>
</li>
<li id="bib.bibx87" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx87.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Sun <span id="bib.bibx87.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2017<span id="bib.bibx87.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
X. Sun, J. Shang, S. Liang, and Y. Wei.

</span>
<span class="ltx_bibblock">Compositional Human Pose Regression.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx87.6.1" class="ltx_text ltx_font_italic">International Conference on Computer Vision</span>, 2017.

</span>
</li>
<li id="bib.bibx88" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx88.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Sundermeyer <span id="bib.bibx88.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2019<span id="bib.bibx88.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
M. Sundermeyer, Z.-C. Marton, M. Durner, and R. Triebel.

</span>
<span class="ltx_bibblock">Augmented Autoencoders: Implicit 3D Orientation Learning for 6D
Object Detection.

</span>
<span class="ltx_bibblock"><span id="bib.bibx88.6.1" class="ltx_text ltx_font_italic">International Journal of Computer Vision</span>, 2019.

</span>
</li>
<li id="bib.bibx89" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx89.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Supancic <span id="bib.bibx89.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2015<span id="bib.bibx89.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
J. S. Supancic, G. Rogez, Y. Yang, J. Shotton, and D. Ramanan.

</span>
<span class="ltx_bibblock">Depth-Based Hand Pose Estimation: Data, Methods, and Challenges.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx89.6.1" class="ltx_text ltx_font_italic">International Conference on Computer Vision</span>, 2015.

</span>
</li>
<li id="bib.bibx90" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx90.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Talvas <span id="bib.bibx90.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2015<span id="bib.bibx90.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
A. Talvas, M. Marchal, C. Duriez, and M. A. Otaduy.

</span>
<span class="ltx_bibblock">Aggregate Constraints for Virtual Manipulation with Soft Fingers.

</span>
<span class="ltx_bibblock"><span id="bib.bibx90.6.1" class="ltx_text ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics</span>,
21(4):452–461, April 2015.

</span>
</li>
<li id="bib.bibx91" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx91.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Tang <span id="bib.bibx91.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2014<span id="bib.bibx91.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
D. Tang, H. J. Chang, A. Tejani, , and T.-K. Kim.

</span>
<span class="ltx_bibblock">Latent Regression Forest: Structured Estimation of 3D Articulated
Hand Posture.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx91.6.1" class="ltx_text ltx_font_italic">Conference on Computer Vision and Pattern Recognition</span>, 2014.

</span>
</li>
<li id="bib.bibx92" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx92.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Taylor <span id="bib.bibx92.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2012<span id="bib.bibx92.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
J. Taylor, J. Shotton, T. Sharp, and A. Fitzgibbon.

</span>
<span class="ltx_bibblock">The Vitruvian Manifold: Inferring Dense Correspondences for One-Shot
Human Pose Estimation.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx92.6.1" class="ltx_text ltx_font_italic">Conference on Computer Vision and Pattern Recognition</span>, pages
103–110, 2012.

</span>
</li>
<li id="bib.bibx93" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx93.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Tekin <span id="bib.bibx93.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2018<span id="bib.bibx93.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
B. Tekin, S. N. Sinha, and P. Fua.

</span>
<span class="ltx_bibblock">Real-Time Seamless Single Shot 6D Object Pose Prediction.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx93.6.1" class="ltx_text ltx_font_italic">Conference on Computer Vision and Pattern Recognition</span>, 2018.

</span>
</li>
<li id="bib.bibx94" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx94.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Tekin <span id="bib.bibx94.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2019a<span id="bib.bibx94.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
B. Tekin, F. Bogo, and M. Pollefeys.

</span>
<span class="ltx_bibblock">H+o: Unified Egocentric Recognition of 3D Hand-Object Poses and
Interactions.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx94.6.1" class="ltx_text ltx_font_italic">Conference on Computer Vision and Pattern Recognition</span>, 2019.

</span>
</li>
<li id="bib.bibx95" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx95.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Tekin <span id="bib.bibx95.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2019b<span id="bib.bibx95.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
B. Tekin, F. Bogo, and M. Pollefeys.

</span>
<span class="ltx_bibblock">H+o: Unified Egocentric Recognition of 3D Hand-Object Poses and
Interactions.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx95.6.1" class="ltx_text ltx_font_italic">Conference on Computer Vision and Pattern Recognition</span>, 2019.

</span>
</li>
<li id="bib.bibx96" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx96.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Tolani <span id="bib.bibx96.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2000<span id="bib.bibx96.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
D. Tolani, A. Goswami, and N. I. Badler.

</span>
<span class="ltx_bibblock">Real-Time Inverse Kinematics Techniques for Anthropomorphic Limbs.

</span>
<span class="ltx_bibblock"><span id="bib.bibx96.6.1" class="ltx_text ltx_font_italic">Graphical Models</span>, 62(5):353–388, 2000.

</span>
</li>
<li id="bib.bibx97" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx97.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Tompson <span id="bib.bibx97.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2014<span id="bib.bibx97.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
J. Tompson, A. Jain, Y. LeCun, and C. Bregler.

</span>
<span class="ltx_bibblock">Joint Training of a Convolutional Network and a Graphical Model for
Human Pose Estimation.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx97.6.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, 2014.

</span>
</li>
<li id="bib.bibx98" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx98.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Toshev and Szegedy, 2014<span id="bib.bibx98.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
A. Toshev and C. Szegedy.

</span>
<span class="ltx_bibblock">DeepPose: Human Pose Estimation via Deep Neural Networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx98.3.1" class="ltx_text ltx_font_italic">Conference on Computer Vision and Pattern Recognition</span>, 2014.

</span>
</li>
<li id="bib.bibx99" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx99.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Tulsiani and
Malik, 2015<span id="bib.bibx99.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
S. Tulsiani and J. Malik.

</span>
<span class="ltx_bibblock">Viewpoints and Keypoints.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx99.3.1" class="ltx_text ltx_font_italic">Conference on Computer Vision and Pattern Recognition</span>, pages
1510–1519, 2015.

</span>
</li>
<li id="bib.bibx100" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx100.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Tulsiani <span id="bib.bibx100.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2015<span id="bib.bibx100.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
S. Tulsiani, J. Carreira, and J. Malik.

</span>
<span class="ltx_bibblock">Pose Induction for Novel Object Categories.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx100.6.1" class="ltx_text ltx_font_italic">International Conference on Computer Vision</span>, pages 64–72,
2015.

</span>
</li>
<li id="bib.bibx101" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx101.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Tzionas <span id="bib.bibx101.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2016<span id="bib.bibx101.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
D. Tzionas, L. Ballan, A. Srikantha, P. Aponte, M. Pollefeys, and J. Gall.

</span>
<span class="ltx_bibblock">Capturing Hands in Action Using Discriminative Salient Points and
Physics Simulation.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx101.6.1" class="ltx_text ltx_font_italic">International Journal of Computer Vision</span>, 2016.

</span>
</li>
<li id="bib.bibx102" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx102.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Vacchetti <span id="bib.bibx102.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2004<span id="bib.bibx102.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
L. Vacchetti, V. Lepetit, and P. Fua.

</span>
<span class="ltx_bibblock">Stable Real-Time 3D Tracking Using Online and Offline Information.

</span>
<span class="ltx_bibblock"><span id="bib.bibx102.6.1" class="ltx_text ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>,
26(10), October 2004.

</span>
</li>
<li id="bib.bibx103" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx103.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Wang <span id="bib.bibx103.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2011<span id="bib.bibx103.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
R. Wang, S. Paris, and J. Popović.

</span>
<span class="ltx_bibblock">6D Hands: Markerless Hand-Tracking for Computer Aided Design.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx103.6.1" class="ltx_text ltx_font_italic">ACM Symposium on User Interface Software and Technology</span>,
pages 549–558, 2011.

</span>
</li>
<li id="bib.bibx104" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx104.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Wei <span id="bib.bibx104.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2016<span id="bib.bibx104.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
S. E. Wei, V. Ramakrishna, T. Kanade, and Y. Sheikh.

</span>
<span class="ltx_bibblock">Convolutional Pose Machines.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx104.6.1" class="ltx_text ltx_font_italic">Conference on Computer Vision and Pattern Recognition</span>, 2016.

</span>
</li>
<li id="bib.bibx105" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx105.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Xiang <span id="bib.bibx105.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2016<span id="bib.bibx105.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Y. Xiang, W. Kim, W. Chen, J. Ji, C. Choy, H. Su, R. Mottaghi, L. Guibas, and
S. Savarese.

</span>
<span class="ltx_bibblock">ObjectNet3D: A Large Scale Database for 3D Object Recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx105.6.1" class="ltx_text ltx_font_italic">European Conference on Computer Vision</span>, pages 160–176,
2016.

</span>
</li>
<li id="bib.bibx106" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx106.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Xiang <span id="bib.bibx106.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2018a<span id="bib.bibx106.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Y. Xiang, T. Schmidt, V. Narayanan, and D. Fox.

</span>
<span class="ltx_bibblock">Posecnn: A Convolutional Neural Network for 6D Object Pose
Estimation in Cluttered Scenes.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx106.6.1" class="ltx_text ltx_font_italic">Robotics: Science and Systems Conference</span>, 2018.

</span>
</li>
<li id="bib.bibx107" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx107.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Xiang <span id="bib.bibx107.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2018b<span id="bib.bibx107.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Y. Xiang, T. Schmidt, V. Narayanan, and D. Fox.

</span>
<span class="ltx_bibblock">PoseCNN: A Convolutional Neural Network for 6D Object Pose
Estimation in Cluttered Scenes.

</span>
<span class="ltx_bibblock"><span id="bib.bibx107.6.1" class="ltx_text ltx_font_italic">Robotics: Science and Systems Conference</span>, 2018.

</span>
</li>
<li id="bib.bibx108" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx108.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Xiong <span id="bib.bibx108.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2019<span id="bib.bibx108.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
F. Xiong, B. Zhanga, Y. Xiao, Z. Cao, T. Yu, J. T. Zhou, and J. Yuan.

</span>
<span class="ltx_bibblock">A2J: Anchor-To-Joint Regression Network for 3D Articulated Pose
Estimation from a Single Depth Image.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx108.6.1" class="ltx_text ltx_font_italic">International Conference on Computer Vision</span>, 2019.

</span>
</li>
<li id="bib.bibx109" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx109.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Yuan <span id="bib.bibx109.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2017<span id="bib.bibx109.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
S. Yuan, Q. Ye, B. Stenger, S. Jain, and T.-K. Kim.

</span>
<span class="ltx_bibblock">BigHand2.2M Benchmark: Hand Pose Dataset and State of the Art
Analysis.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx109.6.1" class="ltx_text ltx_font_italic">Conference on Computer Vision and Pattern Recognition</span>, 2017.

</span>
</li>
<li id="bib.bibx110" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx110.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Zakharov <span id="bib.bibx110.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2019<span id="bib.bibx110.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
S. Zakharov, I. Shugurov, and S. Ilic.

</span>
<span class="ltx_bibblock">DPOD: 6D Pose Object Detector and Refiner.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx110.6.1" class="ltx_text ltx_font_italic">International Conference on Computer Vision</span>, 2019.

</span>
</li>
<li id="bib.bibx111" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx111.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Zhu <span id="bib.bibx111.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2017<span id="bib.bibx111.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros.

</span>
<span class="ltx_bibblock">Unpaired Image-To-Image Translation Using Cycle-Consistent
Adversarial Networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx111.6.1" class="ltx_text ltx_font_italic">International Conference on Computer Vision</span>, 2017.

</span>
</li>
<li id="bib.bibx112" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx112.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Zimmermann and Brox, 2017<span id="bib.bibx112.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
C. Zimmermann and T. Brox.

</span>
<span class="ltx_bibblock">Learning to Estimate 3D Hand Pose from Single RGB Images.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx112.3.1" class="ltx_text ltx_font_italic">International Conference on Computer Vision</span>, 2017.

</span>
</li>
<li id="bib.bibx113" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx113.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Zimmermann <span id="bib.bibx113.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2019<span id="bib.bibx113.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
C. Zimmermann, D. Ceylan, J. Yang, B. C. Russell, M. J. Argus, and T. Brox.

</span>
<span class="ltx_bibblock">FreiHAND: A Dataset for Markerless Capture of Hand Pose and Shape
from Single RGB Images.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx113.6.1" class="ltx_text ltx_font_italic">International Conference on Computer Vision</span>, pages 813–822,
2019.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2006.05926" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2006.05927" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2006.05927">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2006.05927" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2006.05928" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar  7 06:41:29 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
