<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2306.17645] Federated Object Detection for Quality Inspection in Shared Production This project was funded and done in collaboration with Huawei Technologies Düsseldorf GmbH, at the European Research Center in Munich.</title><meta property="og:description" content="Federated learning (FL) has emerged as a promising approach for training machine learning models on decentralized data without compromising data privacy.
In this paper, we propose a FL algorithm for object detection in…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Federated Object Detection for Quality Inspection in Shared Production This project was funded and done in collaboration with Huawei Technologies Düsseldorf GmbH, at the European Research Center in Munich.">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Federated Object Detection for Quality Inspection in Shared Production This project was funded and done in collaboration with Huawei Technologies Düsseldorf GmbH, at the European Research Center in Munich.">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2306.17645">

<!--Generated on Wed Feb 28 20:40:26 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Federated Object Detection (FedOD),  Federated Learning (FL),  YOLOv5,  non-IID Dataset,  Data Privacy
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Federated Object Detection for Quality Inspection in Shared Production
<br class="ltx_break"><span id="id1.id1" class="ltx_note ltx_role_thanks"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span>This project was funded and done in collaboration with Huawei Technologies Düsseldorf GmbH, at the European Research Center
in Munich.</span></span></span>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">1<sup id="id2.1.id1" class="ltx_sup">st</sup> Vinit Hegiste
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id3.2.id1" class="ltx_text ltx_font_italic">Chair of Machine Tools and Control Systems</span>
<br class="ltx_break"><span id="id4.3.id2" class="ltx_text ltx_font_italic">RPTU Kaiserslautern-Landau
<br class="ltx_break"></span>Kaiserslautern, Germany 
<br class="ltx_break">vinit.hegiste@rptu.de
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">2<sup id="id5.1.id1" class="ltx_sup">nd</sup> Tatjana Legler
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id6.2.id1" class="ltx_text ltx_font_italic">Chair of Machine Tools and Control Systems</span>
<br class="ltx_break"><span id="id7.3.id2" class="ltx_text ltx_font_italic">RPTU Kaiserslautern-Landau
<br class="ltx_break"></span>Kaiserslautern, Germany 
<br class="ltx_break">tatjana.legler@rptu.de
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">3<sup id="id8.1.id1" class="ltx_sup">rd</sup> Kirill Fridman
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id9.2.id1" class="ltx_text ltx_font_italic">Huawei Technologies Düsseldorf GmbH
<br class="ltx_break"></span>Düsseldorf, Germany 
<br class="ltx_break">kirill.fridman@huawei.com
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">4<sup id="id10.1.id1" class="ltx_sup">th</sup> Martin Ruskowski
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id11.2.id1" class="ltx_text ltx_font_italic">Innovative Factory Systems (IFS)</span>
<br class="ltx_break"><span id="id12.3.id2" class="ltx_text ltx_font_italic">German Research Center for Artificial Intelligence (DFKI)
<br class="ltx_break"></span>Kaiserslautern, Germany 
<br class="ltx_break">martin.ruskowski@dfki.de
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id13.id1" class="ltx_p">Federated learning (FL) has emerged as a promising approach for training machine learning models on decentralized data without compromising data privacy.
In this paper, we propose a FL algorithm for object detection in quality inspection tasks using YOLOv5 as the object detection algorithm and Federated Averaging (FedAvg) as the FL algorithm.
We apply this approach to a manufacturing use-case where multiple factories/clients contribute data for training a global object detection model while preserving data privacy on a non-IID dataset.
Our experiments demonstrate that our FL approach achieves better generalization performance on the overall clients’ test dataset and generates improved bounding boxes around the objects compared to models trained using local clients’ datasets.
This work showcases the potential of FL for quality inspection tasks in the manufacturing industry and provides valuable insights into the performance and feasibility of utilizing YOLOv5 and FedAvg for federated object detection.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Federated Object Detection (FedOD), Federated Learning (FL), YOLOv5, non-IID Dataset, Data Privacy

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Object detection (OD) is a pivotal deep learning task, sparked by breakthroughs like YOLO (You Only Look Once) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and SSD (Single Shot Detector) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, which unify object classification and localization into a single step, enhancing efficiency and speed. While Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, EfficientDet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> adopt two-stage detectors, they are surpassed by the speed and accuracy of single shot detectors like YOLO and SSD. In manufacturing, OD proves indispensable, facilitating solutions for quality inspection, error classification, object tracking, and more.
Despite common applications among companies such as quality inspection and fault detection, data sharing is hindered by privacy and competitive concerns. The persistent challenge for custom object detectors remains dataset annotation. Here, horizontal federated learning (FL) offers a solution. FL effectively addresses privacy in collaborative learning scenarios by enabling joint model training without centralized data sharing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Applying horizontal FL to OD enhances the global federated model by aggregating class-specific samples from diverse clients. This approach augments model robustness compared to local training.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In our previous work, we applied FL to image classification tasks in manufacturing for quality inspection, showing that this approach can achieve comparable performance to centralized learning while preserving data privacy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. In some cases, the global model could even generalize to the dataset’s feature space better than the model trained using centralized learning.
This paper extends our previous work by proposing a FL algorithm for object detection in quality inspection tasks, using YOLOv5 (You Look Only Once version 5) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> as the object detector algorithm and federated averaging (FedAvg) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> as the FL algorithm. OD is a critical task in quality inspection, where detecting and localizing defects in images is essential for ensuring product quality. By using a FL approach, we can train a global object detection model that incorporates data from multiple factories while maintaining data privacy. Our experiments demonstrate the feasibility and effectiveness of this approach for quality inspection tasks in manufacturing.
In addition, we would like to highlight how the quality inspection service based on FL can be integrated into the Production Level 4 Shared Production ecosystem based on Skill-Based Production. To support the requirements of Shared Production, we will offer the quality inspection service as Software-as-a-service on a marketplace. That means that the quality inspection service should have the ability of self-description. To achieve interoperability and provide the software to a marketplace, we use the Asset Administration Shell (AAS) to describe the software service in the form of submodels.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related work</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">For this paper, we used YOLOv5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> as this version of YOLO has been there for the last three years, while being updated and therefore stable as compared to the latest YOLOv7 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> and other YOLO versions.
The output from OD can also be used for various purposes like locating objects in a fixed environment, counting objects, object segmentation, detecting and classifying faults, etc. These utilities hold promise in industrial quality inspection, motivating collaborative training of a global model to detect diverse errors.
Federated object detection (FedOD) finds relevance here, allowing the global model to learn from local data features across clients.
There are very few papers published in the field of FedOD, with no detailed research in applying the algorithm for specific custom use cases in manufacturing setting.
Luo et al. try to tackle this problem by introducing the algorithm to real life dataset in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, but the application just focuses on images from surveillance cameras for general object detection on common objects from the streets, and does not tackle any specific use case or neither provides a depth analysis regarding the precision of the global model or any comparison with the local client models. It also uses YOLOv3, which is easily outperformed by the current state-of-the-art YOLOv5.
The rest of the papers using FL with YOLOv5 algorithm such as, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> introduces Active learning during FedOD to solve the unlabeled data problem. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> introduces FedVisionBC, a blockchain-based federated learning system for visual object detection, to solve the privacy challenges of FL.
FedCV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> focuses on creating a framework for automating the process of FedOD, but the repository is still a work in progress and difficult to use for custom use cases, as the paper demonstrates the model for basic public datasets.
Su et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, uses RetinaNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> with backbone ResNet50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> as their detection algorithm- <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> uses their custom algorithm which uses an ensemble step to tackle the challenges arising from cross-domain FedOD.
Our paper follows a similar algorithm of FedOD using FedAvg<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, and tries to explore the usability of Federated OD for quality inspection in shared production scenario with non-IID dataset using different use cases.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">The concept of the Asset Administration Shell (AAS) serves as the implementation of the digital twin in the context of Industry 4.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.
Acting as a digital representation of an asset or service, the AAS comprises various sub-models that encompass all relevant information and functionalities of the asset or service. This includes its features, characteristics, properties, and capabilities. The AAS facilitates communication through diverse channels and applications, serving as the crucial link between physical objects and the connected, digital, and distributed world.
Initially employed for creating digital representations of physical assets, the AAS can now also accommodate software modules. Its integration is a fundamental prerequisite for incorporating production modules into emerging production architectures.
Looking ahead to 2025, the flexible production network aims to operate seamlessly through the employment of the digital platform Gaia-X <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.
For assets such as machines and services to be integrated into the European data platform Gaia-X in the future, they must meet specific technical standards, including considerations related to security and skill descriptions. This relevant information is encapsulated within the AAS, ensuring compliance and enabling assets to participate effectively within the Gaia-X ecosystem.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Methodology</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The use-case of quality inspection USB sticks in manufacturing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> was extended via annotating the previous dataset in YOLO format <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, as required by the YOLOv5 algorithm.
To further extend the usability of this algorithm, we introduce a new use-case where two companies/clients produce cabins and windshields as mentioned in Figure <a href="#S3.F1" title="Figure 1 ‣ III Methodology ‣ Federated Object Detection for Quality Inspection in Shared Production This project was funded and done in collaboration with Huawei Technologies Düsseldorf GmbH, at the European Research Center in Munich." class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
The quality inspection use-case here is to detect whether the cabin is with or without a windshield.
The dataset and further details will be explained in subsection <a href="#S3.SS1" title="III-A Datasets ‣ III Methodology ‣ Federated Object Detection for Quality Inspection in Shared Production This project was funded and done in collaboration with Huawei Technologies Düsseldorf GmbH, at the European Research Center in Munich." class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span></span></a>.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2306.17645/assets/images/cabin_with_windshield.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="287" height="101" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Illustration of the Cabin with windshield (4 different types) and without windshield use-case.</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Datasets</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In this subsection, we will explain both USB and cabin quality inspection use-case.
Starting with USB quality inspection, we have three clients and each of them has three classes consisting of ’Okay’, ’Not_Okay’ and ’Hidden’ (similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>). The dataset is non-independent and identically distributed (non-IID), and the dataset distribution along with their respective classes can be seen in Figure <a href="#S3.F2" title="Figure 2 ‣ III-A Datasets ‣ III Methodology ‣ Federated Object Detection for Quality Inspection in Shared Production This project was funded and done in collaboration with Huawei Technologies Düsseldorf GmbH, at the European Research Center in Munich." class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
Client1 has the Huawei USB sticks, client2 has a blue brick style USB stick and client3 has a red brick style USB stick. Each client consists of three classes, where the ’Not_OKAY’ class of each client has a different error type.
As shown in Figure <a href="#S3.F3" title="Figure 3 ‣ III-A Datasets ‣ III Methodology ‣ Federated Object Detection for Quality Inspection in Shared Production This project was funded and done in collaboration with Huawei Technologies Düsseldorf GmbH, at the European Research Center in Munich." class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a><span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The object in the images are zoomed a little for clear view</span></span></span>, client1’s USB error is small sticker marks on the USB port which illustrates scratch, client2’s USB error is the USB port being damaged and client3’s error is a rusted USB port.
Similar to the use-case mentioned in paper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, where we showcased successful federated image classification for custom quality inspection, in this paper the purpose is to see that if federated OD algorithm could achieve the same results where the global federated model is able to learn all different types of USB errors and more importantly draw a perfect bounding box over the object. The detailed part regarding FedOD algorithm is mentioned in subsection <a href="#S3" title="III Methodology ‣ Federated Object Detection for Quality Inspection in Shared Production This project was funded and done in collaboration with Huawei Technologies Düsseldorf GmbH, at the European Research Center in Munich." class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2306.17645/assets/images/client1,2,3_d.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="287" height="109" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Training dataset distribution and label instances of client1 (Huawei on left), client2 (SF blue in middle) and client3 (SF red on right)</figcaption>
</figure>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2306.17645/assets/images/usb_annotated.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="287" height="217" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Small subset example of USB quality inspection dataset, client1 (Huawei on left), client2 (SF blue in middle) and client3 (SF red on right)</figcaption>
</figure>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">For the second use-case, there are two companies/clients which manufacture cabins with and without windshield. The main application is to create an object detector model to classify and detect the object correctly in a given video or image.
We refer to the companies as client1 and client2, where client1 only manufactures blue colored cabins and windshield of type A and B and client2 manufactures red colored cabins (a little different design than the blue colored cabins) and windshield of type C and D (refer to Figure <a href="#S3.F5" title="Figure 5 ‣ III-A Datasets ‣ III Methodology ‣ Federated Object Detection for Quality Inspection in Shared Production This project was funded and done in collaboration with Huawei Technologies Düsseldorf GmbH, at the European Research Center in Munich." class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>).
The classes instance for each clients’ training dataset can be seen in Figure <a href="#S3.F4" title="Figure 4 ‣ III-A Datasets ‣ III Methodology ‣ Federated Object Detection for Quality Inspection in Shared Production This project was funded and done in collaboration with Huawei Technologies Düsseldorf GmbH, at the European Research Center in Munich." class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Each client had a total of approximately 600 images, out of which 15% each is used for validation and testing.
The dataset was created with the cabin installed in a chassis (but in the annotation the bounding box was only drawn over the cabin) and with 3 different backgrounds. Various parameters such as different lighting conditions, shadows, blurry images, etc. were also introduced via creation of the custom dataset.
The federated approach of these use-cases is explained further in subsection <a href="#S3.SS2" title="III-B Implementation ‣ III Methodology ‣ Federated Object Detection for Quality Inspection in Shared Production This project was funded and done in collaboration with Huawei Technologies Düsseldorf GmbH, at the European Research Center in Munich." class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a></p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2306.17645/assets/images/client_blue_red.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="269" height="135" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Training dataset distribution and label instances of client1 (Blue cabin on left), client2 (Red cabin on right)</figcaption>
</figure>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2306.17645/assets/images/cabin_dataset.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="293" height="102" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Small subset of the cabin quality inspection dataset, client1 (Blue cabin on left), client2 (Red cabin on right)</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Implementation</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">As previously mentioned, there are two clients involved in the manufacturing of cabins and windshields with different designs and types. For their local quality inspection models, they utilize their respective local datasets (see Figure <a href="#S3.F5" title="Figure 5 ‣ III-A Datasets ‣ III Methodology ‣ Federated Object Detection for Quality Inspection in Shared Production This project was funded and done in collaboration with Huawei Technologies Düsseldorf GmbH, at the European Research Center in Munich." class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>) to train YOLOv5 models that can detect cabins with or without windshields in a given frame.
Both clients’ models achieve more than 95% accuracy on their own local test datasets. Client1’s model is evaluated on blue cabins without windshields and with windshields of type A and B, while client2’s model is tested on cabins without windshields and with windshields of type C and D.
In the future, both clients plan to manufacture the remaining two windshields, each with its own color type. This means that client 1 will produce cabins with windshield type C and D, while client 2 will manufacture cabins with windshield type A and B, in addition to their existing production.
The clients’ locally trained models, based on their old datasets, were tested with the new cabin-windshield combinations and can be referred in Figure <a href="#S3.F6" title="Figure 6 ‣ III-B Implementation ‣ III Methodology ‣ Federated Object Detection for Quality Inspection in Shared Production This project was funded and done in collaboration with Huawei Technologies Düsseldorf GmbH, at the European Research Center in Munich." class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. However, the results show that although the local models correctly classify the images as either ’Cabin_without_windshield’ or ’Cabin_with_windshield,’ the bounding boxes generated are imprecise and sometimes crop part of the windshield. In some cases, false positives are detected, and labels are assigned with lower confidence scores, as depicted from the bottom-left of Figure <a href="#S3.F6" title="Figure 6 ‣ III-B Implementation ‣ III Methodology ‣ Federated Object Detection for Quality Inspection in Shared Production This project was funded and done in collaboration with Huawei Technologies Düsseldorf GmbH, at the European Research Center in Munich." class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2306.17645/assets/images/indiviual_vs_fedod.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="269" height="188" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Output of models trained on local dataset, client1 (left upper) and client2 (left lower) and global federated YOLOv5 model (right column) on unseen windshield type dataset.</figcaption>
</figure>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">While the clients could potentially share their datasets to train a centralized YOLOv5 model for quality inspection, personal or competitive reasons prevent them from sharing their raw local image data. Consequently, both clients would need to create new additional data for the new combinations, annotate the dataset, and re-train the entire model to classify the new ’Cabin_with_windshield’ images.
However, the process of manually creating and annotating the dataset for each client is tedious. This is where FL plays a crucial role, enabling the development of a final global model that can accurately detect objects from both clients without the need to share their raw image data.</p>
</div>
<figure id="S3.F7" class="ltx_figure"><img src="/html/2306.17645/assets/images/FedOD_cabinclients.png" id="S3.F7.g1" class="ltx_graphics ltx_centering ltx_img_square" width="240" height="223" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Illustration of federated learning between cabin clients</figcaption>
</figure>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">The FL algorithm employed in our work is FedAvg [6].A neutral server is utilized to perform federated averaging on the model weights from all clients. Certain assumptions are made, such as including the active participation of all clients in each communication round and also their trustworthiness in sharing their model weights.
Before commencing the training process, all clients agree on a standardized label nomenclature, the YOLO model architecture, and other hyperparameters such as local epochs, optimizer, and batch size. They then initiate their respective local training procedures. Figure <a href="#S3.F7" title="Figure 7 ‣ III-B Implementation ‣ III Methodology ‣ Federated Object Detection for Quality Inspection in Shared Production This project was funded and done in collaboration with Huawei Technologies Düsseldorf GmbH, at the European Research Center in Munich." class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows a pictorial view of this federated learning process.
Once the local training is completed, the model weights achieved by each client are sent to the neutral server. After receiving the model weights from all clients, the server performs federated averaging and sends the updated global weights back to each client. This process is referred to as one communication round (CR).
Through multiple CRs, the global model gradually improves and demonstrates enhanced performance on both clients’ test datasets. In this particular use-case, each client runs the global model received from the server on its local test dataset, providing feedback on the accuracy of the global model and transmitting new local weights accordingly.
The average accuracy across all clients’ test datasets serves as the stopping parameter. Once the server receives the accuracy of the previous global model on all the clients’ local test datasets and calculates the average accuracy to be greater than 96% (as this was the accuracy of the normally trained YOLOv5 model), the previous global weights are transmitted to the clients as the final global federated model.
In this particular use-case, the global model was achieved after 10 CRs with 15 local epochs. The output of this model on a similar test dataset demonstrated very high accuracy, with highly precise bounding boxes around the objects. Figure <a href="#S3.F6" title="Figure 6 ‣ III-B Implementation ‣ III Methodology ‣ Federated Object Detection for Quality Inspection in Shared Production This project was funded and done in collaboration with Huawei Technologies Düsseldorf GmbH, at the European Research Center in Munich." class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> showcases the output of the global federated model on similar images. It can be observed that the confidence scores of the predictions are very high, and the bounding boxes accurately encompass the windshields. There are no false positive detections, and the model output exhibits robustness even for blurry images.
A similar setting was used for the USB quality inspection dataset (Figure <a href="#S3.F3" title="Figure 3 ‣ III-A Datasets ‣ III Methodology ‣ Federated Object Detection for Quality Inspection in Shared Production This project was funded and done in collaboration with Huawei Technologies Düsseldorf GmbH, at the European Research Center in Munich." class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>), where the final global model was able to correctly classify various errors and precisely draw bounding boxes around specific USB sticks. The global federated USB stick model was even able to detect a combination never seen in the dataset, such as client 1’s sticker error when applied to client 2’s USB sticks.
This aligns with the results presented in the paper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, which focused on a federated image classification setting.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Experimentation</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">In this paper, the focus is primarily on the cabin quality inspection use-case, and we conducted experiments based on this scenario. While a quick overview of the results for the USB federated OD model can be seen in Figure <a href="#S4.F9" title="Figure 9 ‣ IV Results and Discussion ‣ Federated Object Detection for Quality Inspection in Shared Production This project was funded and done in collaboration with Huawei Technologies Düsseldorf GmbH, at the European Research Center in Munich." class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>, we will specifically concentrate on evaluating the performance of different models in the cabin quality inspection domain. Throughout the experiments, we will refer to client1’s locally trained model as the ”Blue cabin model,” client2’s model as the ”Red cabin model,” and the global federated model as ”FedOD.”</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">The following experiments were conducted:</p>
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">Testing the Blue cabin model, Red cabin model, and FedOD model on the new cabin-windshield combinations as test dataset.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">Evaluating all three models for live object detection with various combinations of cabins and windshields.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">Testing all three models on images obtained from the quality inspection module of the manufacturing process.</p>
</div>
</li>
</ol>
<p id="S3.SS3.p2.2" class="ltx_p">In the first experiment, the test dataset consisted of blue cabins with windshields of type C and D, as well as red cabins with windshields of type A and B. These combinations were not present in the local training datasets of either client. The goal is to assess the performance of these models on these previously unseen combinations.
For the second experiment, all three models (Blue cabin, Red cabin, and FedOD) were validated simultaneously to detect objects in live frames with different combinations of cabins and windshields. This experiment aims to compare the outputs of the models under various scenarios, including frames with multiple cabins.
Lastly, in the third experiment, images from the quality inspection module situated at SmartFactory-Kaiserslautern (SF-KL) will be used as input for the models. It’s important to note that the background and lighting conditions of these images differ significantly from those in the training dataset, adding a new level of complexity to the evaluation process.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.4.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.5.2" class="ltx_text ltx_font_italic">Integration with the Industry 4.0 Shared Production architecture</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">In this subsection, we explore and give one possible solution how we can offer our quality inspection AI-software service to different companies over Gaia-X platform. Therefore, we orientated on the offering of production services in a Gaia-X data spaces published in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. In this case, a potential user of the service can download and use the service with own dataset in own production line without any significant efforts. To offer the quality inspection service in Gaia-X we have described the quality inspection service (its features, characteristics, properties, and capabilities) in a submodel of AAS. Currently, there is no standard submodel template available to describe AI-service capabilities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, so to create the submodel, we used Google model cards <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> as an example of how AI software services can be described in a common way.
With the help of Gaia-X connector, we can connect to the related data space and offer our AAS-based description of the software service. That means it can be found in the Gaia-X service catalog and the service provider can offer the software service via marketplace (Figure <a href="#S3.F8" title="Figure 8 ‣ III-D Integration with the Industry 4.0 Shared Production architecture ‣ III Methodology ‣ Federated Object Detection for Quality Inspection in Shared Production This project was funded and done in collaboration with Huawei Technologies Düsseldorf GmbH, at the European Research Center in Munich." class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>).</p>
</div>
<figure id="S3.F8" class="ltx_figure"><img src="/html/2306.17645/assets/images/AAS_GAIAX.png" id="S3.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="140" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Industry 4.0 data space for software services <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite></figcaption>
</figure>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">If the customer connects to the data space, he can browse available software services on the marketplace, select one, which matches his requirements, download it (as a Docker container) and use in his production line. All this operation is performed with the help of service generalized description available via AAS. The customer can download the service or, if the service is already running on the customer side, just update the model weights from the global federated model.
Because the quality inspection service based on FL algorithm, the customer has also an opportunity to contribute to the service by improving the model quality via an additional round of training on his local dataset. But the main challenge here is to make sure that all customers have the similar data classes and use case. Each FL model is trained for a specific use case, and precise description of possible use cases is necessary to offer a product on a marketplace. Otherwise, the model quality can be corrupted by ingesting the wrong local datasets. Although some precautions, such as attempting an automatic assignment of class labels from different clients, can be taken <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>; however, a trustworthy environment must be in place to ensure cooperation.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Results and Discussion</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we present the results of federated USB quality inspection and results of our experiments conducted for the cabin quality inspection use-case, which are described in detail in subsection <a href="#S3.SS3" title="III-C Experimentation ‣ III Methodology ‣ Federated Object Detection for Quality Inspection in Shared Production This project was funded and done in collaboration with Huawei Technologies Düsseldorf GmbH, at the European Research Center in Munich." class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-C</span></span></a>. We focus on three key experiments to compare the performance of three different models.
Figure <a href="#S4.F9" title="Figure 9 ‣ IV Results and Discussion ‣ Federated Object Detection for Quality Inspection in Shared Production This project was funded and done in collaboration with Huawei Technologies Düsseldorf GmbH, at the European Research Center in Munich." class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>, shows 4 windows with output from each model, namely the global federated model which was achieved by 15 local epochs for 5 CR, followed by client1, client2 and client3 models which were trained for 150 epochs based on their local dataset. We can see that the global FedOD model is not only able to predict all clients’ error types, but also detect error of client3 (rust) on client1’s USB stick.</p>
</div>
<figure id="S4.F9" class="ltx_figure"><img src="/html/2306.17645/assets/images/4model_fedod_usb.png" id="S4.F9.g1" class="ltx_graphics ltx_centering ltx_img_square" width="287" height="246" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Live comparison of federated global model with models trained on their local respective dataset</figcaption>
</figure>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">Moving on to the experiments, in the first experiment, we evaluated these models on a test dataset that included both blue and red cabins with different windshield combinations. It was observed that the Blue cabin model was unable to accurately classify or detect the red-colored cabins and windshields. Similarly, the Red cabin model struggled to detect the blue-colored cabins and windshields, producing incorrect bounding boxes for those images. In contrast, the FedOD model demonstrated superior performance by successfully detecting all different cabin and windshield combinations, generating highly precise bounding boxes for most test images. Detailed results for this experiment can be found in Table <a href="#S4.T1" title="TABLE I ‣ IV Results and Discussion ‣ Federated Object Detection for Quality Inspection in Shared Production This project was funded and done in collaboration with Huawei Technologies Düsseldorf GmbH, at the European Research Center in Munich." class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>.
Table <a href="#S4.T1" title="TABLE I ‣ IV Results and Discussion ‣ Federated Object Detection for Quality Inspection in Shared Production This project was funded and done in collaboration with Huawei Technologies Düsseldorf GmbH, at the European Research Center in Munich." class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> presents the average precision (AP) values for different IoU thresholds ranging from 0.50 to 0.95, as well as the mean average precision (mAP) at an IoU threshold of 0.5. The FedOD model achieved an AP[.50:.05:.95] of 0.93 and an mAP of 1.0, showcasing its robust performance across a wide range of IoU thresholds.
To further investigate the accuracy of the Blue cabin and Red cabin models, we conducted a second experiment where each model was tested specifically on its corresponding cabin and windshield color combination. Additionally, we tested the FedOD model on both combinations to enable a direct comparison. The results of this experiment are presented in Table <a href="#S4.T2" title="TABLE II ‣ IV Results and Discussion ‣ Federated Object Detection for Quality Inspection in Shared Production This project was funded and done in collaboration with Huawei Technologies Düsseldorf GmbH, at the European Research Center in Munich." class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>.
The mAP and AP[.50:.05:.95] values in Table <a href="#S4.T2" title="TABLE II ‣ IV Results and Discussion ‣ Federated Object Detection for Quality Inspection in Shared Production This project was funded and done in collaboration with Huawei Technologies Düsseldorf GmbH, at the European Research Center in Munich." class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> indicate that the FedOD model outperformed the local models in terms of precision. It consistently achieved higher mAP and AP scores, suggesting that the global FedOD model excels in predicting precise bounding boxes, even when confronted with unseen combination types.
These results highlight the effectiveness of the FedOD model in the cabin quality inspection use-case, demonstrating its ability to accurately detect different cabin and windshield combinations which were not even the part of its local training datasets. The superior performance of the FedOD model provides strong evidence for the advantages of FL in collaborative object detection scenarios.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>mAP metrics comparison of Blue cabin, Red cabin and FedOD model on an unseen Test dataset (AP= Average precision, APm = AP of medium-sized objects, APl= AP of large-sized objects, AR = Average Recall)</figcaption>
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<td id="S4.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Model</td>
<td id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Training Dataset</td>
<td id="S4.T1.1.1.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Test Dataset</td>
<td id="S4.T1.1.1.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">mAP</td>
<td id="S4.T1.1.1.1.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">AP@[.50:.05:.95]</td>
<td id="S4.T1.1.1.1.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">APm</td>
<td id="S4.T1.1.1.1.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">APl</td>
<td id="S4.T1.1.1.1.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">ARm</td>
<td id="S4.T1.1.1.1.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">ARl</td>
</tr>
<tr id="S4.T1.1.2.2" class="ltx_tr">
<td id="S4.T1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table id="S4.T1.1.2.2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.2.2.1.1.1" class="ltx_tr">
<td id="S4.T1.1.2.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Client1</td>
</tr>
<tr id="S4.T1.1.2.2.1.1.2" class="ltx_tr">
<td id="S4.T1.1.2.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">(Blue cabin)</td>
</tr>
</table>
</td>
<td id="S4.T1.1.2.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S4.T1.1.2.2.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.2.2.2.1.1" class="ltx_tr">
<td id="S4.T1.1.2.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Blue colored cabins without</td>
</tr>
<tr id="S4.T1.1.2.2.2.1.2" class="ltx_tr">
<td id="S4.T1.1.2.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">windshield and cabin with</td>
</tr>
<tr id="S4.T1.1.2.2.2.1.3" class="ltx_tr">
<td id="S4.T1.1.2.2.2.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">windshields of type A and B</td>
</tr>
</table>
</td>
<td id="S4.T1.1.2.2.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" rowspan="3"><span id="S4.T1.1.2.2.3.1" class="ltx_text">
<span id="S4.T1.1.2.2.3.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.1.2.2.3.1.1.1" class="ltx_tr">
<span id="S4.T1.1.2.2.3.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Blue colored cabins with windshield</span></span>
<span id="S4.T1.1.2.2.3.1.1.2" class="ltx_tr">
<span id="S4.T1.1.2.2.3.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">type C and D and Red colored cabins</span></span>
<span id="S4.T1.1.2.2.3.1.1.3" class="ltx_tr">
<span id="S4.T1.1.2.2.3.1.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">with windshield type A and B. (Does</span></span>
<span id="S4.T1.1.2.2.3.1.1.4" class="ltx_tr">
<span id="S4.T1.1.2.2.3.1.1.4.1" class="ltx_td ltx_nopad_r ltx_align_left">not belong to any training dataset)</span></span>
</span></span></td>
<td id="S4.T1.1.2.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.42</td>
<td id="S4.T1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.35</td>
<td id="S4.T1.1.2.2.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">__</td>
<td id="S4.T1.1.2.2.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.35</td>
<td id="S4.T1.1.2.2.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">__</td>
<td id="S4.T1.1.2.2.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.36</td>
</tr>
<tr id="S4.T1.1.3.3" class="ltx_tr">
<td id="S4.T1.1.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table id="S4.T1.1.3.3.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.3.3.1.1.1" class="ltx_tr">
<td id="S4.T1.1.3.3.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Client2</td>
</tr>
<tr id="S4.T1.1.3.3.1.1.2" class="ltx_tr">
<td id="S4.T1.1.3.3.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">(Red cabin)</td>
</tr>
</table>
</td>
<td id="S4.T1.1.3.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S4.T1.1.3.3.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.3.3.2.1.1" class="ltx_tr">
<td id="S4.T1.1.3.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Red colored cabins without</td>
</tr>
<tr id="S4.T1.1.3.3.2.1.2" class="ltx_tr">
<td id="S4.T1.1.3.3.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">windshield and cabin with</td>
</tr>
<tr id="S4.T1.1.3.3.2.1.3" class="ltx_tr">
<td id="S4.T1.1.3.3.2.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">windshields of type C and D</td>
</tr>
</table>
</td>
<td id="S4.T1.1.3.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.49</td>
<td id="S4.T1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.42</td>
<td id="S4.T1.1.3.3.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">__</td>
<td id="S4.T1.1.3.3.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.42</td>
<td id="S4.T1.1.3.3.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">__</td>
<td id="S4.T1.1.3.3.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.43</td>
</tr>
<tr id="S4.T1.1.4.4" class="ltx_tr">
<td id="S4.T1.1.4.4.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">
<table id="S4.T1.1.4.4.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.4.4.1.1.1" class="ltx_tr">
<td id="S4.T1.1.4.4.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T1.1.4.4.1.1.1.1.1" class="ltx_text ltx_font_bold">Global Federated</span></td>
</tr>
<tr id="S4.T1.1.4.4.1.1.2" class="ltx_tr">
<td id="S4.T1.1.4.4.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T1.1.4.4.1.1.2.1.1" class="ltx_text ltx_font_bold">model</span></td>
</tr>
</table>
</td>
<td id="S4.T1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">__</td>
<td id="S4.T1.1.4.4.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.1.4.4.3.1" class="ltx_text ltx_font_bold">1.0</span></td>
<td id="S4.T1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.1.4.4.4.1" class="ltx_text ltx_font_bold">0.93</span></td>
<td id="S4.T1.1.4.4.5" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">__</td>
<td id="S4.T1.1.4.4.6" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.1.4.4.6.1" class="ltx_text ltx_font_bold">0.93</span></td>
<td id="S4.T1.1.4.4.7" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">__</td>
<td id="S4.T1.1.4.4.8" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.1.4.4.8.1" class="ltx_text ltx_font_bold">0.96</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>The centralized trained YOLOv5 models (client1 and client2) trained on their local dataset and FedOD model are tested with Windshield combination not present in their training dataset</figcaption>
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Model</td>
<td id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Training Dataset</td>
<td id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Test Dataset</td>
<td id="S4.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">mAP</td>
<td id="S4.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">AP@[.50:.05:.95]</td>
<td id="S4.T2.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">APm</td>
<td id="S4.T2.1.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">APl</td>
<td id="S4.T2.1.1.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">ARm</td>
<td id="S4.T2.1.1.1.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">ARl</td>
</tr>
<tr id="S4.T2.1.2.2" class="ltx_tr">
<td id="S4.T2.1.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table id="S4.T2.1.2.2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.2.2.1.1.1" class="ltx_tr">
<td id="S4.T2.1.2.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Client1</td>
</tr>
<tr id="S4.T2.1.2.2.1.1.2" class="ltx_tr">
<td id="S4.T2.1.2.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">(Blue cabin)</td>
</tr>
</table>
</td>
<td id="S4.T2.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="S4.T2.1.2.2.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.2.2.2.1.1" class="ltx_tr">
<td id="S4.T2.1.2.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Blue cabins and</td>
</tr>
<tr id="S4.T2.1.2.2.2.1.2" class="ltx_tr">
<td id="S4.T2.1.2.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">windshield of</td>
</tr>
<tr id="S4.T2.1.2.2.2.1.3" class="ltx_tr">
<td id="S4.T2.1.2.2.2.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center">type A and B</td>
</tr>
</table>
</td>
<td id="S4.T2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="S4.T2.1.2.2.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.2.2.3.1.1" class="ltx_tr">
<td id="S4.T2.1.2.2.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Blue cabins with</td>
</tr>
<tr id="S4.T2.1.2.2.3.1.2" class="ltx_tr">
<td id="S4.T2.1.2.2.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">windshield type</td>
</tr>
<tr id="S4.T2.1.2.2.3.1.3" class="ltx_tr">
<td id="S4.T2.1.2.2.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center">C and D</td>
</tr>
</table>
</td>
<td id="S4.T2.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.83</td>
<td id="S4.T2.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.70</td>
<td id="S4.T2.1.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">__</td>
<td id="S4.T2.1.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.70</td>
<td id="S4.T2.1.2.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">__</td>
<td id="S4.T2.1.2.2.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.73</td>
</tr>
<tr id="S4.T2.1.3.3" class="ltx_tr">
<td id="S4.T2.1.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table id="S4.T2.1.3.3.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.3.3.1.1.1" class="ltx_tr">
<td id="S4.T2.1.3.3.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.1.3.3.1.1.1.1.1" class="ltx_text ltx_font_bold">Global federated</span></td>
</tr>
<tr id="S4.T2.1.3.3.1.1.2" class="ltx_tr">
<td id="S4.T2.1.3.3.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.1.3.3.1.1.2.1.1" class="ltx_text ltx_font_bold">model</span></td>
</tr>
</table>
</td>
<td id="S4.T2.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.3.3.2.1" class="ltx_text ltx_font_bold">___</span></td>
<td id="S4.T2.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="S4.T2.1.3.3.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.3.3.3.1.1" class="ltx_tr">
<td id="S4.T2.1.3.3.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Blue cabins with</td>
</tr>
<tr id="S4.T2.1.3.3.3.1.2" class="ltx_tr">
<td id="S4.T2.1.3.3.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">windshield type</td>
</tr>
<tr id="S4.T2.1.3.3.3.1.3" class="ltx_tr">
<td id="S4.T2.1.3.3.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center">C and D</td>
</tr>
</table>
</td>
<td id="S4.T2.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.3.3.4.1" class="ltx_text ltx_font_bold">1.0</span></td>
<td id="S4.T2.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.3.3.5.1" class="ltx_text ltx_font_bold">0.96</span></td>
<td id="S4.T2.1.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.3.3.6.1" class="ltx_text ltx_font_bold">__</span></td>
<td id="S4.T2.1.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.3.3.7.1" class="ltx_text ltx_font_bold">0.96</span></td>
<td id="S4.T2.1.3.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.3.3.8.1" class="ltx_text ltx_font_bold">__</span></td>
<td id="S4.T2.1.3.3.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.3.3.9.1" class="ltx_text ltx_font_bold">0.98</span></td>
</tr>
<tr id="S4.T2.1.4.4" class="ltx_tr">
<td id="S4.T2.1.4.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table id="S4.T2.1.4.4.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.4.4.1.1.1" class="ltx_tr">
<td id="S4.T2.1.4.4.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Client2</td>
</tr>
<tr id="S4.T2.1.4.4.1.1.2" class="ltx_tr">
<td id="S4.T2.1.4.4.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">(Red cabin)</td>
</tr>
</table>
</td>
<td id="S4.T2.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="S4.T2.1.4.4.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.4.4.2.1.1" class="ltx_tr">
<td id="S4.T2.1.4.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Red cabins and</td>
</tr>
<tr id="S4.T2.1.4.4.2.1.2" class="ltx_tr">
<td id="S4.T2.1.4.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">windshield of</td>
</tr>
<tr id="S4.T2.1.4.4.2.1.3" class="ltx_tr">
<td id="S4.T2.1.4.4.2.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center">type C and D</td>
</tr>
</table>
</td>
<td id="S4.T2.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="S4.T2.1.4.4.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.4.4.3.1.1" class="ltx_tr">
<td id="S4.T2.1.4.4.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Red cabins with</td>
</tr>
<tr id="S4.T2.1.4.4.3.1.2" class="ltx_tr">
<td id="S4.T2.1.4.4.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">windshield type</td>
</tr>
<tr id="S4.T2.1.4.4.3.1.3" class="ltx_tr">
<td id="S4.T2.1.4.4.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center">A and B</td>
</tr>
</table>
</td>
<td id="S4.T2.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.97</td>
<td id="S4.T2.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.83</td>
<td id="S4.T2.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">__</td>
<td id="S4.T2.1.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.83</td>
<td id="S4.T2.1.4.4.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">__</td>
<td id="S4.T2.1.4.4.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.86</td>
</tr>
<tr id="S4.T2.1.5.5" class="ltx_tr">
<td id="S4.T2.1.5.5.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">
<table id="S4.T2.1.5.5.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.5.5.1.1.1" class="ltx_tr">
<td id="S4.T2.1.5.5.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.1.5.5.1.1.1.1.1" class="ltx_text ltx_font_bold">Global federated</span></td>
</tr>
<tr id="S4.T2.1.5.5.1.1.2" class="ltx_tr">
<td id="S4.T2.1.5.5.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.1.5.5.1.1.2.1.1" class="ltx_text ltx_font_bold">model</span></td>
</tr>
</table>
</td>
<td id="S4.T2.1.5.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">___</td>
<td id="S4.T2.1.5.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">
<table id="S4.T2.1.5.5.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.5.5.3.1.1" class="ltx_tr">
<td id="S4.T2.1.5.5.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Red cabins with</td>
</tr>
<tr id="S4.T2.1.5.5.3.1.2" class="ltx_tr">
<td id="S4.T2.1.5.5.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">windshield type</td>
</tr>
<tr id="S4.T2.1.5.5.3.1.3" class="ltx_tr">
<td id="S4.T2.1.5.5.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center">A and B</td>
</tr>
</table>
</td>
<td id="S4.T2.1.5.5.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T2.1.5.5.4.1" class="ltx_text ltx_font_bold">1.0</span></td>
<td id="S4.T2.1.5.5.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T2.1.5.5.5.1" class="ltx_text ltx_font_bold">0.91</span></td>
<td id="S4.T2.1.5.5.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T2.1.5.5.6.1" class="ltx_text ltx_font_bold">__</span></td>
<td id="S4.T2.1.5.5.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T2.1.5.5.7.1" class="ltx_text ltx_font_bold">0.91</span></td>
<td id="S4.T2.1.5.5.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T2.1.5.5.8.1" class="ltx_text ltx_font_bold">__</span></td>
<td id="S4.T2.1.5.5.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T2.1.5.5.9.1" class="ltx_text ltx_font_bold">0.93</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">In the second experiment, we developed a custom code to simultaneously run all three models (Blue, Red and FedOD cabin model) in parallel on live video streams. This setup allowed us to directly compare the output of each model and observe any discernible differences.
Multiple cabin combinations were tested within a single frame, as depicted in Figure <a href="#S4.F10" title="Figure 10 ‣ IV Results and Discussion ‣ Federated Object Detection for Quality Inspection in Shared Production This project was funded and done in collaboration with Huawei Technologies Düsseldorf GmbH, at the European Research Center in Munich." class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> and Figure <a href="#S4.F11" title="Figure 11 ‣ IV Results and Discussion ‣ Federated Object Detection for Quality Inspection in Shared Production This project was funded and done in collaboration with Huawei Technologies Düsseldorf GmbH, at the European Research Center in Munich." class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>. Each figure consists of three windows. The top-left window displays the output of the global FedOD model.
The top-right window showcases the Blue cabin model, which was trained using the local dataset of client1, and the bottom-left window presents the output from the Red cabin model, trained on the local dataset of client2.
Figure <a href="#S4.F10" title="Figure 10 ‣ IV Results and Discussion ‣ Federated Object Detection for Quality Inspection in Shared Production This project was funded and done in collaboration with Huawei Technologies Düsseldorf GmbH, at the European Research Center in Munich." class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> and Figure <a href="#S4.F11" title="Figure 11 ‣ IV Results and Discussion ‣ Federated Object Detection for Quality Inspection in Shared Production This project was funded and done in collaboration with Huawei Technologies Düsseldorf GmbH, at the European Research Center in Munich." class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>, follow the same pattern of windows and the labels output are changed to 0 and 1 for representing ’Cabin_without_windshield’ and ’Cabin_with_windshield’ respectively, to provide a clear visual comparison of the models’ outputs across different bounding boxes.</p>
</div>
<figure id="S4.F10" class="ltx_figure"><img src="/html/2306.17645/assets/images/fed_cabin1.png" id="S4.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="287" height="229" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Comparison of federated global model vs. models trained with their local data using live object detection (0: Cabin_without_windshield, 1: Cabin_with_windshield)</figcaption>
</figure>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">In Figure <a href="#S4.F10" title="Figure 10 ‣ IV Results and Discussion ‣ Federated Object Detection for Quality Inspection in Shared Production This project was funded and done in collaboration with Huawei Technologies Düsseldorf GmbH, at the European Research Center in Munich." class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>, the frame contains 4 cabins with the combination available in the training dataset. The FedOD model demonstrates superior performance by accurately detecting all 4 cabins with high confidence for each client’s combination, while it is evident that the Blue cabin model fails to detect the red cabin and vice versa for the Red cabin model. Both models correctly identify only their own design type, which proves the low performance of these models in Table <a href="#S4.T1" title="TABLE I ‣ IV Results and Discussion ‣ Federated Object Detection for Quality Inspection in Shared Production This project was funded and done in collaboration with Huawei Technologies Düsseldorf GmbH, at the European Research Center in Munich." class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>.
Moving on to Figure <a href="#S4.F11" title="Figure 11 ‣ IV Results and Discussion ‣ Federated Object Detection for Quality Inspection in Shared Production This project was funded and done in collaboration with Huawei Technologies Düsseldorf GmbH, at the European Research Center in Munich." class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>, a red-designed cabin from client2 with windshield type A from client1 was tested. The results are remarkable: the Red cabin model successfully classifies the object with a high confidence score, but the bounding box drawn is imprecise and crops a portion of the windshield. On the other hand, the Blue cabin model fails to detect this specific object entirely.
Figure <a href="#S4.F11" title="Figure 11 ‣ IV Results and Discussion ‣ Federated Object Detection for Quality Inspection in Shared Production This project was funded and done in collaboration with Huawei Technologies Düsseldorf GmbH, at the European Research Center in Munich." class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> also presents a reverse scenario of the previous test case, where a blue cabin with windshield type C from client2 was tested. The Blue cabin model correctly classifies the object, but similar to the previous scenario, it struggles to generate an accurate bounding box. The output of the Red cabin model is intriguing: since windshield type C was part of its training dataset, it appears to classify the object as ’Cabin_with_windshield’, although with an incorrect bounding box. In contrast, the FedOD model not only accurately classifies both the objects, but also draws a precise bounding box over these unseen combination types.</p>
</div>
<figure id="S4.F11" class="ltx_figure"><img src="/html/2306.17645/assets/images/fed_cabin3.png" id="S4.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="287" height="230" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Comparison of federated global model vs. models trained with their local data using live object detection</figcaption>
</figure>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p">The third experiment involved testing the same models on images obtained from the demonstrator located at SF-KL in the quality inspection module, as described in subsection <a href="#S3.SS3" title="III-C Experimentation ‣ III Methodology ‣ Federated Object Detection for Quality Inspection in Shared Production This project was funded and done in collaboration with Huawei Technologies Düsseldorf GmbH, at the European Research Center in Munich." class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-C</span></span></a>. Figure <a href="#S4.F12" title="Figure 12 ‣ IV Results and Discussion ‣ Federated Object Detection for Quality Inspection in Shared Production This project was funded and done in collaboration with Huawei Technologies Düsseldorf GmbH, at the European Research Center in Munich." class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>, <a href="#S4.F13" title="Figure 13 ‣ IV Results and Discussion ‣ Federated Object Detection for Quality Inspection in Shared Production This project was funded and done in collaboration with Huawei Technologies Düsseldorf GmbH, at the European Research Center in Munich." class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>, and <a href="#S4.F14" title="Figure 14 ‣ IV Results and Discussion ‣ Federated Object Detection for Quality Inspection in Shared Production This project was funded and done in collaboration with Huawei Technologies Düsseldorf GmbH, at the European Research Center in Munich." class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a> showcase the results obtained from these tests.
It is worth noting that the images captured in this setting have significantly different background and lighting conditions compared to the images used in the training dataset. The consistent use of the same set of images in all three figures allows for a direct comparison of the output generated by each model.
In Figure <a href="#S4.F12" title="Figure 12 ‣ IV Results and Discussion ‣ Federated Object Detection for Quality Inspection in Shared Production This project was funded and done in collaboration with Huawei Technologies Düsseldorf GmbH, at the European Research Center in Munich." class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>, the output of the Blue cabin model reveals its inability to correctly classify instances of ’Cabin_without_windshield’. Additionally, the model predicts numerous false positives on the trailer objects and even in frames where no objects are present.
Similarly, Figure <a href="#S4.F13" title="Figure 13 ‣ IV Results and Discussion ‣ Federated Object Detection for Quality Inspection in Shared Production This project was funded and done in collaboration with Huawei Technologies Düsseldorf GmbH, at the European Research Center in Munich." class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a> illustrates the performance of the Red cabin model, which also struggles to accurately detect objects in the images from the quality inspection module. The model exhibits misclassifications and false positives, particularly on the trailers and frames without any objects.
The results obtained from our global FedOD model on the same test images are truly remarkable. The enhanced algorithm, which combines the power of FL and OD, has demonstrated substantial improvements in accuracy and precision compared to the previous individual client models. Figure <a href="#S4.F14" title="Figure 14 ‣ IV Results and Discussion ‣ Federated Object Detection for Quality Inspection in Shared Production This project was funded and done in collaboration with Huawei Technologies Düsseldorf GmbH, at the European Research Center in Munich." class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>, shows the output of FedOD model on the test images.
The figure reveals the FedOD model’s exceptional performance in predicting bounding boxes with remarkable precision and confidence scores.
Notably, the model exhibits no false positives when detecting different types of trailers or when confronted with frames containing just the background and no objects. These findings provide compelling evidence of the versatility and generalizability of our FedOD model, particularly in detecting identical objects in diverse and previously unseen environments, as well as various object combinations which are unseen by the training model.</p>
</div>
<figure id="S4.F12" class="ltx_figure"><img src="/html/2306.17645/assets/images/bluecabin_localmodel.png" id="S4.F12.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="287" height="123" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Output of model trained with only blue cabin dataset (Blue cabin model) on images from quality inspection of demonstrator</figcaption>
</figure>
<figure id="S4.F13" class="ltx_figure"><img src="/html/2306.17645/assets/images/redcabin_localmodel.png" id="S4.F13.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="287" height="125" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Output of model trained with only red cabin dataset (Red cabin model) on images from quality inspection of demonstrator</figcaption>
</figure>
<figure id="S4.F14" class="ltx_figure"><img src="/html/2306.17645/assets/images/fed_global_cabin.png" id="S4.F14.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="287" height="124" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>Output of global federated model (FedOD model) on images from quality inspection of demonstrator (0: Cabin_without_windshield, 1: Cabin_with_windshield)</figcaption>
</figure>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.1" class="ltx_p">In our future work we will focus on implementation of the standard AAS sub-model for AI-service capabilities description that should be released soon according to Industrial Digital Twin Association<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. The general use-case requires a vendor independent, standard method of describing software functions. Accurate description is crucial for a federated learning approach to enable each partner participation in global model creation. As mentioned earlier, for this research we have used Google AI model cards as a reference<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. In addition, we will also implement the updated version of Gaia-X data space connector to provide simultaneous access and download of the service from Gaia-X marketplace to multiple customers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we presented a comprehensive investigation into the effectiveness of a global FedOD model for quality inspection use case in a shared production environment.
By combining the power of FL and object detection, we achieved substantial improvements in accuracy and precision compared to the individual client models.
Our experimental results showcased the remarkable performance of the FedOD model across multiple scenarios.
The FedOD model outperformed the local models by accurately detecting all combinations and generating highly precise bounding boxes. The achieved average precision (AP) and mean average precision (mAP) scores further demonstrated the model’s robustness across multiple unseen test data combinations.
Moreover, the simultaneous evaluation of all three models on live video streams provided valuable insights. The FedOD model consistently demonstrated superior performance in detecting objects, even in the presence of different cabin and windshield combinations.
Furthermore, our experiments on images obtained from the demonstrator in the quality inspection module validated the versatility of the FedOD model. Despite significant variations in background and lighting conditions compared to the training dataset, the model showcased exceptional performance in correctly classifying objects and generating precise bounding boxes. This proves that Federated Object detection is efficient has a wider scope in future collaborative work between multiple partners in manufacturing and other scenarios as well.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once:
Unified, real-time object detection,” in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2016.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C. Berg,
“Ssd: Single shot multibox detector,” in <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Computer vision - ECCV
2016</em>, ser. Lecture Notes in Computer Science, B. Leibe, J. Matas, N. Sebe,
and M. Welling, Eds.   Cham: Springer,
2016, vol. 9905, pp. 21–37.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time
object detection with region proposal networks,” 2016.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
K. He, G. Gkioxari, P. Dollár, and R. Girshick, “Mask r-cnn,” in <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">2017
IEEE International Conference on Computer Vision (ICCV)</em>, 2017, pp.
2980–2988.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
M. Tan, R. Pang, and Q. V. Le, “Efficientdet: Scalable and efficient object
detection,” in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR)</em>, June 2020.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y. Arcas,
“Communication-efficient learning of deep networks from decentralized
data,” in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 20th International Conference on
Artificial Intelligence and Statistics</em>, ser. Proceedings of Machine Learning
Research, A. Singh and J. Zhu, Eds., vol. 54.   Fort Lauderdale, FL, USA: PMLR, 2017, pp. 1273–1282.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
V. Hegiste, T. Legler, and M. Ruskowski, “Application of federated machine
learning in manufacturing,” in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">2022 International Conference on
Industry 4.0 Technology (I4Tech)</em>.   IEEE, 2022, pp. 1–8.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
G. J. et al., “ultralytics/yolov5: v3.1 - Bug Fixes and Performance
Improvements,” Oct. 2020.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
C.-Y. Wang, A. Bochkovskiy, and H.-Y. M. Liao, “Yolov7: Trainable
bag-of-freebies sets new state-of-the-art for real-time object detectors,”
2022.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
J. Luo, X. Wu, Y. Luo, A. Huang, Y. Huang, Y. Liu, and Q. Yang, “Real-world
image datasets for federated learning,” 2021.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
J. R. van Bommel, “Active learning during federated learning for object
detection,” July 2021.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
J. Zhang, J. Zhou, J. Guo, and X. Sun, “Visual object detection for
privacy-preserving federated learning,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, vol. 11, pp.
33 324–33 335, 2023.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
C. He, A. Shah, Z. Tang, D. Fan, A. N. Sivashunmugam, K. Bhogaraju, M. Shimpi,
L. Shen, X. Chu, M. Soltanolkotabi, and S. Avestimehr, “Fedcv: A federated
learning framework for diverse computer vision tasks,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, vol.
abs/2111.11066, 2021.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
S. Su, B. Li, C. Zhang, M. Yang, and X. Xue, “Cross-domain federated object
detection,” 2022.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollár, “Focal loss for dense
object detection,” in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">2017 IEEE International Conference on Computer
Vision (ICCV)</em>, 2017, pp. 2999–3007.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</em>, 2016, pp. 770–778.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
T. A. Abdel-Aty, E. Negri, and S. Galparoli, “Asset administration shell in
manufacturing: Applications and relationship with digital twin,”
<em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">IFAC-PapersOnLine</em>, vol. 55, no. 10, pp. 2533–2538, 2022, 10th IFAC
Conference on Manufacturing Modelling, Management and Control MIM 2022.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
K. Alexopoulos, M. Weber, T. Trautner, M. Manns, N. Nikolakis, M. Weigold, and
B. Engel, “An industrial data-spaces framework for resilient manufacturing
value chains,” <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Procedia CIRP</em>, vol. 116, pp. 299–304, 2023, 30th CIRP
Life Cycle Engineering Conference.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
S. Jungbluth, A. Witton, J. Hermann, and M. Ruskowski, “Architecture for
shared production leveraging asset administration shell and gaia-x (in
press),” 2023.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
M. e. a. Simon, “Realisierung einer shared production,” 2023.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
M. Volkmann, A. Wagner, J. Hermann, and M. Ruskowski, “Asset administration
shells and gaia-x enabled shared production scenario (in press),” in
<em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Lecture Notes in Mechanical Engineering (LNME)</em>, 2023.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
“Idta registered aas submodel templates,” 2023. [Online]. Available:
https://industrialdigitaltwin.org/en/content-hub/submodels

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
“Google cloud model cards,” 2023. [Online]. Available:
https://modelcards.withgoogle.com/

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
T. Legler, V. Hegiste, and M. Ruskowski, “Mapping of newcomer clients in
federated learning based on activation strength,” 2023, 32nd International
Conference Flexible Automation and Intelligent Manufacturing, in press.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
“Eclipse dataspace components,” 2023. [Online]. Available:
https://projects.eclipse.org/projects/technology.edc

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2306.17644" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2306.17645" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2306.17645">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2306.17645" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2306.17646" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 20:40:26 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
