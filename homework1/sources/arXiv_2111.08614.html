<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2111.08614] IKEA Object State Dataset: A 6DoF object pose estimation dataset and benchmark for multi-state assembly objects</title><meta property="og:description" content="Utilizing 6DoF(Degrees of Freedom) pose information of an object and its components is critical for object state detection tasks. We present IKEA Object State Dataset, a new dataset that contains IKEA furniture 3D mode…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="IKEA Object State Dataset: A 6DoF object pose estimation dataset and benchmark for multi-state assembly objects">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="IKEA Object State Dataset: A 6DoF object pose estimation dataset and benchmark for multi-state assembly objects">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2111.08614">

<!--Generated on Tue Mar 19 16:24:17 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="object state detection,  6DoF object pose estimation dataset">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>TU Kaiserslautern, Kaiserslautern, Germany </span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany </span></span></span><span id="id3" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">institutetext: </span>IKEA Marketing &amp; Communication AB
<br class="ltx_break"><span id="id3.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">email: </span>{Yongzhi.Su, Mingxin.Liu, Jason.Rambach, Didier.Stricker}@dfki.de</span></span></span> 
<br class="ltx_break"><span id="id3.2" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">email: </span>{antonia.pehrson2, anton.berg}@inter.ikea.com</span></span></span></span></span></span>
<h1 class="ltx_title ltx_title_document">IKEA Object State Dataset: A 6DoF object pose estimation dataset and benchmark for multi-state assembly objects</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yongzhi Su
</span><span class="ltx_author_notes">1122</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mingxin Liu
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jason Rambach
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Antonia Pehrson
</span><span class="ltx_author_notes">33</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Anton Berg
</span><span class="ltx_author_notes">33</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Didier Stricker
</span><span class="ltx_author_notes">1122</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Utilizing 6DoF(Degrees of Freedom) pose information of an object and its components is critical for object state detection tasks. We present IKEA Object State Dataset, a new dataset that contains IKEA furniture 3D models, RGBD video of the assembly process, the 6DoF pose of furniture parts and their bounding box. The proposed dataset will be available at <a target="_blank" href="https://github.com/mxllmx/IKEAObjectStateDataset" title="" class="ltx_ref ltx_font_italic">https://github.com/mxllmx/IKEAObjectStateDataset</a>.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>object state detection, 6DoF object pose estimation dataset
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In recent years, AR (Augmented Reality) has been one of the most popular technologies in Industry 4.0 and intelligent manufacturing. The use of AR is beneficial for improving and accelerating product and process development. One application of AR is maintenance-assembly-repair. AR assisted assembly is proved significantly faster than manual assembly under paper-based or digital tablet instructions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. It can help operators to understand the presented information, to shorten the production time and to avoid human errors. Such assembly tasks need to deal with objects that consist of several removable and adjustable components, so object state detection, i.e. detecting the current assembly step or state of objects, is very important for related application development.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Existing object state detectors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> estimate the 6DoF pose of each object’s component, then determine the current assembly step. They rely on the deep learning method thus need large datasets with 6DoF pose information for training. Considering realistic conditions such as time and hardware, using large-scale synthetic data for training and small portion of real data for testing is a compromised method. However, there is still a reality gap between synthetic data and real-world data. Even though the rendering engine can generate highly photorealistic or generalized synthetic objects datasets, synthesizing human motions during assembly is complex and challenging. In a real assembly scene, the operator may interact with the objects in varied and unexpected ways to achieve the goal. The generated occlusion and possible interactions are difficult to be reflected in a synthetic dataset.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">There are assembly datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> that combine human action recognition and object information together. Nevertheless, their works focus more on human poses, which is helpful for better understanding and detecting patterns in task-oriented human activities but has limitations on the collection of object locations and orientations. Because only object bounding boxes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> or object segmentation annotations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> are included. Other famous objects 6DoF datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> provide non-deformable rigid objects’ position and orientation, which are integral and motionless, and not suitable for an assembly task.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Distinct from previous studies, our IKEA Object State Dataset focuses on the 6DoF pose of each object’s component in every state. We use IKEA furniture 3D models published by  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. For every kind of IKEA furniture presented in our dataset, its parts are almost textureless and of the same colour and shape. Therefore, the appearance information in frames could be misleading. Additionally, during the assembly process, the movement of an object part is sometimes slight. It could be just a little clockwise spin and hard to distinguish. The ambiguity is also caused by inter-parts occlusion, human motion occlusion, change of angle, etc. We give our best effort to shoot the video sequence without blind spots, so we set up 4 RGBD cameras with different viewpoints and records the process of IKEA furniture assembly synchronously. Our dataset contains 3D mesh models of the IKEA furniture objects, depth, RGB images and the 6DoF object poses for each frame.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>6D Object Pose Datasests</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Many researchers created 6DoF object pose datasets as standard benchmarks for various estimation tasks. They continue to provide innovative thinking and methods for determining the 3D models’ position and rotation information in the images. Below are the details of several well-known datasets:</p>
</div>
<div id="S2.SS1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS1.p2.1" class="ltx_p"><span id="S2.SS1.p2.1.1" class="ltx_text ltx_font_bold">LM-O (Linemod-Occluded) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite></span>: the dataset is improved based on the Linemod dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Instead of using a template-based technique, it uses a decision forest that jointly predicts both 3D object coordinates and objects instance probabilities. This dataset contains 10k images of 20 textured and textureless objects, captured under three different lighting conditions.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para ltx_noindent">
<p id="S2.SS1.p3.2" class="ltx_p"><span id="S2.SS1.p3.2.1" class="ltx_text ltx_font_bold">T-LESS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite></span>: this work places emphasis on the textureless rigid objects. The dataset contains 30 industry-relevant objects, which have no significant texture, discriminative colour or distinctive reflectance properties. Their shapes and/or sizes often bear similarities, and a few objects are served as compositions of other objects. Then the result images are divided into <math id="S2.SS1.p3.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S2.SS1.p3.1.m1.1a"><mo id="S2.SS1.p3.1.m1.1.1" xref="S2.SS1.p3.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.1.m1.1b"><csymbol cd="latexml" id="S2.SS1.p3.1.m1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.1.m1.1c">\sim</annotation></semantics></math>39K training with a black background and <math id="S2.SS1.p3.2.m2.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S2.SS1.p3.2.m2.1a"><mo id="S2.SS1.p3.2.m2.1.1" xref="S2.SS1.p3.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.2.m2.1b"><csymbol cd="latexml" id="S2.SS1.p3.2.m2.1.1.cmml" xref="S2.SS1.p3.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.2.m2.1c">\sim</annotation></semantics></math>10K test images in 20 different scenes.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para ltx_noindent">
<p id="S2.SS1.p4.1" class="ltx_p"><span id="S2.SS1.p4.1.1" class="ltx_text ltx_font_bold">YCB-Video <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite></span>: it contains 6D poses of 21 YCB objects in 92 videos with a total of 133,827 frames. The household objects own different symmetries. They are arranged in various poses and spatial configurations, and some of the frames also contain severe occlusion between objects.</p>
</div>
<div id="S2.SS1.p5" class="ltx_para ltx_noindent">
<p id="S2.SS1.p5.1" class="ltx_p"><span id="S2.SS1.p5.1.1" class="ltx_text ltx_font_bold">HomebrewedDB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite></span>: the dataset is consist of 34,830 images, 33 objects (3 types) over 13 scenes of various difficulty. The scenes’ complexity varies regarding the number and size of objects, occlusion and clutter levels. By having a camera pose in each image estimated from the markerboard and having object poses in the markerboard coordinate system, 6D object poses for each of the frames can easily be computed.</p>
</div>
<div id="S2.SS1.p6" class="ltx_para ltx_noindent">
<p id="S2.SS1.p6.1" class="ltx_p"><span id="S2.SS1.p6.1.1" class="ltx_text ltx_font_bold">Fraunhofer IPA Bin-Picking dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite></span>: both synthetic and real-world scenes are contained in this dataset, which is designed for bin-picking scenarios with robotic arm. The ten objects have both industrial elements like a gear shaft and household objects like a candlestick. For the real data part, the dataset has 520 fully annotated point clouds and corresponding depth images, via fitting point cloud representation and object’s CAD model with the Iterative Closest Point (ICP) algorithm. As well as about 206,000 synthetic scenes, the dataset is one of the enormous public datasets for object pose estimation in general.</p>
</div>
<div id="S2.SS1.p7" class="ltx_para">
<p id="S2.SS1.p7.1" class="ltx_p">The overview of these datasets is given in Table <a href="#S2.T1" title="Table 1 ‣ 2.1 6D Object Pose Datasests ‣ 2 Related Work ‣ IKEA Object State Dataset: A 6DoF object pose estimation dataset and benchmark for multi-state assembly objects" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. These works inspired the way of generating a 6DoF object pose dataset. Most of them fix the objects’ poses and track objects in the trajectory of cameras. In contrast, our dataset setup is fixing the cameras, and the objects are movable during the process.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Overview of existing 6DoF object pose estimation datasets.</figcaption>
<table id="S2.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S2.T1.1.1" class="ltx_tr">
<td id="S2.T1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.1.1.1" class="ltx_text">Dataset</span></td>
<td id="S2.T1.1.1.2" class="ltx_td ltx_align_center ltx_border_t">
<span id="S2.T1.1.1.2.1" class="ltx_text"></span> <span id="S2.T1.1.1.2.2" class="ltx_text">
<span id="S2.T1.1.1.2.2.1" class="ltx_tabular ltx_align_middle">
<span id="S2.T1.1.1.2.2.1.1" class="ltx_tr">
<span id="S2.T1.1.1.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Visual</span></span>
<span id="S2.T1.1.1.2.2.1.2" class="ltx_tr">
<span id="S2.T1.1.1.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Modality</span></span>
</span></span><span id="S2.T1.1.1.2.3" class="ltx_text"></span></td>
<td id="S2.T1.1.1.3" class="ltx_td ltx_align_center ltx_border_t">
<span id="S2.T1.1.1.3.1" class="ltx_text"></span> <span id="S2.T1.1.1.3.2" class="ltx_text">
<span id="S2.T1.1.1.3.2.1" class="ltx_tabular ltx_align_middle">
<span id="S2.T1.1.1.3.2.1.1" class="ltx_tr">
<span id="S2.T1.1.1.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Real/Synthetic</span></span>
<span id="S2.T1.1.1.3.2.1.2" class="ltx_tr">
<span id="S2.T1.1.1.3.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">images</span></span>
</span></span><span id="S2.T1.1.1.3.3" class="ltx_text"></span></td>
<td id="S2.T1.1.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.1.4.1" class="ltx_text">#objects</span></td>
<td id="S2.T1.1.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.1.5.1" class="ltx_text">resolution</span></td>
<td id="S2.T1.1.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.1.6.1" class="ltx_text">marker</span></td>
</tr>
<tr id="S2.T1.1.2" class="ltx_tr">
<td id="S2.T1.1.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Linemod-Occluded <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>
</td>
<td id="S2.T1.1.2.2" class="ltx_td ltx_align_center ltx_border_t">RGBD</td>
<td id="S2.T1.1.2.3" class="ltx_td ltx_align_center ltx_border_t">10k/-</td>
<td id="S2.T1.1.2.4" class="ltx_td ltx_align_center ltx_border_t">20</td>
<td id="S2.T1.1.2.5" class="ltx_td ltx_align_center ltx_border_t">640 x 480</td>
<td id="S2.T1.1.2.6" class="ltx_td ltx_align_center ltx_border_t">yes</td>
</tr>
<tr id="S2.T1.1.3" class="ltx_tr">
<td id="S2.T1.1.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">T-LESS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</td>
<td id="S2.T1.1.3.2" class="ltx_td ltx_align_center ltx_border_t">RGBD</td>
<td id="S2.T1.1.3.3" class="ltx_td ltx_align_center ltx_border_t">48k/-</td>
<td id="S2.T1.1.3.4" class="ltx_td ltx_align_center ltx_border_t">30</td>
<td id="S2.T1.1.3.5" class="ltx_td ltx_align_center ltx_border_t">1280 x 1024</td>
<td id="S2.T1.1.3.6" class="ltx_td ltx_align_center ltx_border_t">yes</td>
</tr>
<tr id="S2.T1.1.4" class="ltx_tr">
<td id="S2.T1.1.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">YCB-Video <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>
</td>
<td id="S2.T1.1.4.2" class="ltx_td ltx_align_center ltx_border_t">RGBD</td>
<td id="S2.T1.1.4.3" class="ltx_td ltx_align_center ltx_border_t">133827/-</td>
<td id="S2.T1.1.4.4" class="ltx_td ltx_align_center ltx_border_t">21</td>
<td id="S2.T1.1.4.5" class="ltx_td ltx_align_center ltx_border_t">640 x 480</td>
<td id="S2.T1.1.4.6" class="ltx_td ltx_align_center ltx_border_t">no</td>
</tr>
<tr id="S2.T1.1.5" class="ltx_tr">
<td id="S2.T1.1.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.5.1.1" class="ltx_text">HomebrewedDB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite></span></td>
<td id="S2.T1.1.5.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.5.2.1" class="ltx_text">RGBD</span></td>
<td id="S2.T1.1.5.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.5.3.1" class="ltx_text">34830/-</span></td>
<td id="S2.T1.1.5.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.5.4.1" class="ltx_text">30</span></td>
<td id="S2.T1.1.5.5" class="ltx_td ltx_align_center ltx_border_t">
<span id="S2.T1.1.5.5.1" class="ltx_text"></span> <span id="S2.T1.1.5.5.2" class="ltx_text">
<span id="S2.T1.1.5.5.2.1" class="ltx_tabular ltx_align_middle">
<span id="S2.T1.1.5.5.2.1.1" class="ltx_tr">
<span id="S2.T1.1.5.5.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">640 x 480 &amp;</span></span>
<span id="S2.T1.1.5.5.2.1.2" class="ltx_tr">
<span id="S2.T1.1.5.5.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">1920 x 1080</span></span>
</span></span><span id="S2.T1.1.5.5.3" class="ltx_text"></span></td>
<td id="S2.T1.1.5.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.5.6.1" class="ltx_text">yes</span></td>
</tr>
<tr id="S2.T1.1.6" class="ltx_tr">
<td id="S2.T1.1.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Fraunhofer IPA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>
</td>
<td id="S2.T1.1.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">Stereo</td>
<td id="S2.T1.1.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">520/206,000</td>
<td id="S2.T1.1.6.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">10</td>
<td id="S2.T1.1.6.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">1280×1024</td>
<td id="S2.T1.1.6.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">no</td>
</tr>
</table>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Assembly Datasets</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">There was IKEA furniture dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> comprising 800 images and 225 3D models with furniture pose. Another research direction about furniture datasets is furniture parsing. The approach  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> try to find the interaction element (IE) of the furniture item and compute the semantic segmentation by performing subset selection.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">In terms of furniture assembly datasets, Youngkyoon Jang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> use head-mounted cameras to shoot the process of assembling a camping tent outdoors. However, all 1,171,897 frames are annotated with subtask label, uncertainty label, error label and eye-tracking information, which targets the egocentric task. Chakraborty and Hebert <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> focus on occlusion encountered by object trackers. They try to detect small tools like screws, which are easily occluded, and their dataset is collected from the internet. Similar to our work, Yizhak Ben-Shabat et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> provide a 3,046,977 frames dataset also with IKEA furniture items. It consists of four different furniture types (side table, coffee table, TV bench, and drawer), 371 assemblies in three views. Each procedure frame is annotated with the human skeleton and the object segmentation that is preferable for human-object interactions research. Our dataset uses similar object models, as depicted in Fig. <a href="#S2.F1" title="Figure 1 ‣ 2.2 Assembly Datasets ‣ 2 Related Work ‣ IKEA Object State Dataset: A 6DoF object pose estimation dataset and benchmark for multi-state assembly objects" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, but put effort into object poses.</p>
</div>
<figure id="S2.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_many">
<div id="S2.F1.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:69.4pt;">
<img src="/html/2111.08614/assets/x1.png" id="S2.F1.1.g1" class="ltx_graphics ltx_img_square" width="69" height="69" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<div id="S2.F1.2" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:69.4pt;">
<img src="/html/2111.08614/assets/x2.png" id="S2.F1.2.g1" class="ltx_graphics ltx_img_square" width="69" height="69" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<div id="S2.F1.3" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:69.4pt;">
<img src="/html/2111.08614/assets/x3.png" id="S2.F1.3.g1" class="ltx_graphics ltx_img_square" width="69" height="69" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<div id="S2.F1.4" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:69.4pt;">
<img src="/html/2111.08614/assets/x4.png" id="S2.F1.4.g1" class="ltx_graphics ltx_img_square" width="69" height="69" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<div id="S2.F1.5" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:69.4pt;">
<img src="/html/2111.08614/assets/x5.png" id="S2.F1.5.g1" class="ltx_graphics ltx_img_square" width="69" height="69" alt="Refer to caption">
</div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>3D models of IKEA Object State Dataset. Objects from left to right are: a white side table, a white small cabinet, a white cabinet with 2 drawers, a black step stool and a black bar stool.</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>IKEA Object State Dataset</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Hardware Setup</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Four Microsoft Kinect Azure cameras are applied in the hardware setup. It is a cutting-edge time-of-flight RGBD camera with sufficient developer tools. We arrange the four cameras in a circle. They are placed in a position higher than the objects, with the lens facing down and aiming at the operating space. The operating space is a classic indoor scene where the operator will assemble the IKEA furniture. Three calibration boards are fixed on the floor to determine the relative pose of the cameras. The computer that we use to run the acquisition program has the following parameters: Intel Xeon(R) CPU E3-1245 v5@3.50GHz x 8, GeForce GTX 1050 Ti/PCle/SSE2 and 64-bit Ubuntu 18.04.6 LTS.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Data acquisition</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The factory calibration of four Kinect Azure cameras is good enough for our task. They will be divided into one master device and three subordinate devices. With the help of the sensor SDK, we develop an application to start, synchronize the cameras and capture the assembly scene. Recording device data will be stored in a Matroska(.mkv) file. The Matroska stores video tracks, including colour and depth tracks. Tools such as <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_italic">ffmpeg</span> or the <span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_italic">mkvinfo</span> command can view and extract information from recording files. Through them, we can obtain RGB images aligned with 16-bit depth images. Fig. <a href="#S3.F2" title="Figure 2 ‣ 3.2 Data acquisition ‣ 3 IKEA Object State Dataset ‣ IKEA Object State Dataset: A 6DoF object pose estimation dataset and benchmark for multi-state assembly objects" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows image samples.</p>
</div>
<figure id="S3.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_many">
<div id="S3.F2.2" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:69.4pt;">
<img src="/html/2111.08614/assets/x6.png" id="S3.F2.1.g1" class="ltx_graphics ltx_img_landscape" width="69" height="38" alt="Refer to caption"><img src="/html/2111.08614/assets/x7.png" id="S3.F2.2.g2" class="ltx_graphics ltx_centering ltx_img_square" width="69" height="69" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<div id="S3.F2.4" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:69.4pt;">
<img src="/html/2111.08614/assets/x8.png" id="S3.F2.3.g1" class="ltx_graphics ltx_img_landscape" width="50" height="28" alt="Refer to caption"><img src="/html/2111.08614/assets/x9.png" id="S3.F2.4.g2" class="ltx_graphics ltx_centering ltx_img_square" width="69" height="69" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<div id="S3.F2.6" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:69.4pt;">
<img src="/html/2111.08614/assets/x10.png" id="S3.F2.5.g1" class="ltx_graphics ltx_img_landscape" width="69" height="38" alt="Refer to caption"><img src="/html/2111.08614/assets/x11.png" id="S3.F2.6.g2" class="ltx_graphics ltx_centering ltx_img_square" width="69" height="69" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<div id="S3.F2.8" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:69.4pt;">
<img src="/html/2111.08614/assets/x12.png" id="S3.F2.7.g1" class="ltx_graphics ltx_img_landscape" width="69" height="38" alt="Refer to caption"><img src="/html/2111.08614/assets/x13.png" id="S3.F2.8.g2" class="ltx_graphics ltx_centering ltx_img_square" width="69" height="69" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<div id="S3.F2.10" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:69.4pt;">
<img src="/html/2111.08614/assets/x14.png" id="S3.F2.9.g1" class="ltx_graphics ltx_img_landscape" width="69" height="38" alt="Refer to caption"><img src="/html/2111.08614/assets/x15.png" id="S3.F2.10.g2" class="ltx_graphics ltx_centering ltx_img_square" width="69" height="69" alt="Refer to caption">
</div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Examples of images captured from one camera. The upper row are the RGB images, and we will classify the assembly action step manually; the bottom row are correspondingly normalized depth images. The shown depth images are normalized into (0,255) for display reasons.</figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Data annotation</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The fixed position of markers can help calculate the relationship between cameras, so we treat the master device’s coordinate as the world coordinate. Other cameras’ results will be transformed into the world coordinate.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">For each camera, their recorded images will be classified into different assembly states manually. For instance, the IKEA side table assembly has five states. With one more table leg being assembled, the image will be marked as the next state. After the classification, we provide the initial guess of the objects’ poses only in each state’s first frame to avoid manually annotating all the frames. Rest poses can be tracked through ICP algorithm.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">Every depth and corresponding colour frame from the four cameras with the same timestamp will be used to reconstruct the whole scene and obtain the point cloud. Then ICP algorithm will align the 3D model with the point cloud. Through the registration of multi-view data, we can receive the 6DoF object poses. Finally, all poses are refined in an optimization step.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion and future work</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">This paper introduces the IKEA Object State Dataset, a large-scale, multi-view dataset for IKEA furniture assembly. To the best of our knowledge, most previous work pays more attention to immovable objects or understanding human behaviours. Our dataset focus on providing fully annotated frames with 6DoF pose for all object components. When hardware is available, we will continue our work on data processing and benchmark experiments.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Badami, I., Tom, M., Mathias, M., Leibe, B.: 3d semantic segmentation of
modular furniture using rjmcmc. In: 2017 IEEE Winter Conference on
Applications of Computer Vision (WACV). pp. 64–72. IEEE (2017)

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Ben-Shabat, Y., Yu, X., Saleh, F., Campbell, D., Rodriguez-Opazo, C., Li, H.,
Gould, S.: The ikea asm dataset: Understanding people assembling furniture
through actions, objects and pose. In: Proceedings of the IEEE/CVF Winter
Conference on Applications of Computer Vision. pp. 847–859 (2021)

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Brachmann, E., Krull, A., Michel, F., Gumhold, S., Shotton, J., Rother, C.:
Learning 6d object pose estimation using 3d object coordinates. In: European
conference on computer vision. pp. 536–551. Springer (2014)

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Chakraborty, S., Hebert, M.: Learning to track object position through
occlusion. arXiv preprint arXiv:2106.10766 (2021)

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Funk, M., Kosch, T., Schmidt, A.: Interactive worker assistance: comparing the
effects of in-situ projection, head-mounted displays, tablet, and paper
instructions. In: Proceedings of the 2016 ACM International Joint Conference
on Pervasive and Ubiquitous Computing. pp. 934–939 (2016)

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Hinterstoisser, S., Lepetit, V., Ilic, S., Holzer, S., Bradski, G., Konolige,
K., Navab, N.: Model based training, detection and pose estimation of
texture-less 3d objects in heavily cluttered scenes. In: Asian conference on
computer vision. pp. 548–562. Springer (2012)

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Hodan, T., Haluza, P., Obdržálek, Š., Matas, J., Lourakis, M.,
Zabulis, X.: T-less: An rgb-d dataset for 6d pose estimation of texture-less
objects. In: 2017 IEEE Winter Conference on Applications of Computer Vision
(WACV). pp. 880–888. IEEE (2017)

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://github.com/IKEA/IKEA3DAssemblyDataset/" title="" class="ltx_ref">https://github.com/IKEA/IKEA3DAssemblyDataset/</a>

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Jang, Y., Sullivan, B., Ludwig, C., Gilchrist, I., Damen, D., Mayol-Cuevas, W.:
Epic-tent: An egocentric video dataset for camping tent assembly. In:
Proceedings of the IEEE/CVF International Conference on Computer Vision
Workshops. pp. 0–0 (2019)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Kaskman, R., Zakharov, S., Shugurov, I., Ilic, S.: Homebreweddb: Rgb-d dataset
for 6d pose estimation of 3d objects. In: Proceedings of the IEEE/CVF
International Conference on Computer Vision Workshops. pp. 0–0 (2019)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Kleeberger, K., Landgraf, C., Huber, M.F.: Large-scale 6d object pose
estimation dataset for industrial bin-picking. In: 2019 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS). pp.
2573–2578. IEEE (2019)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Lim, J.J., Pirsiavash, H., Torralba, A.: Parsing ikea objects: Fine pose
estimation. In: Proceedings of the IEEE International Conference on Computer
Vision. pp. 2992–2999 (2013)

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Liu, H., Su, Y., Rambach, J., Pagani, A., Stricker, D.: Tga: Two-level group
attention for assembly state detection. In: 2020 IEEE International Symposium
on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct). pp. 258–263. IEEE
(2020)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Su, Y., Rambach, J., Minaskan, N., Lesur, P., Pagani, A., Stricker, D.: Deep
multi-state object pose estimation for augmented reality assembly. In: 2019
IEEE International Symposium on Mixed and Augmented Reality Adjunct
(ISMAR-Adjunct). pp. 222–227. IEEE (2019)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Toyer, S., Cherian, A., Han, T., Gould, S.: Human pose forecasting via deep
markov models. In: 2017 International Conference on Digital Image Computing:
Techniques and Applications (DICTA). pp. 1–8. IEEE (2017)

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Xiang, Y., Schmidt, T., Narayanan, V., Fox, D.: Posecnn: A convolutional neural
network for 6d object pose estimation in cluttered scenes. arXiv preprint
arXiv:1711.00199 (2017)

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2111.08613" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2111.08614" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2111.08614">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2111.08614" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2111.08615" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar 19 16:24:17 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
