<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2310.01931] MarineDet: Towards Open-Marine Object Detection</title><meta property="og:description" content="Marine object detection has gained prominence in marine research, driven by the pressing need to unravel oceanic mysteries and enhance our understanding of invaluable marine ecosystems. There is a profound requirement …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="MarineDet: Towards Open-Marine Object Detection">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="MarineDet: Towards Open-Marine Object Detection">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2310.01931">

<!--Generated on Wed Feb 28 02:31:54 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">MarineDet: Towards Open-Marine Object Detection</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Haixin Liang<sup id="id10.2.id1" class="ltx_sup"><span id="id10.2.id1.1" class="ltx_text ltx_font_italic">1†</span></sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Ziqiang Zheng<sup id="id11.2.id1" class="ltx_sup"><span id="id11.2.id1.1" class="ltx_text ltx_font_italic">2†∗</span></sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Zeyu Ma<sup id="id12.2.id1" class="ltx_sup"><span id="id12.2.id1.1" class="ltx_text ltx_font_italic">3</span></sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Sai-Kit Yeung<sup id="id13.7.id1" class="ltx_sup"><span id="id13.7.id1.1" class="ltx_text ltx_font_italic">1,2</span></sup>
</span><span class="ltx_author_notes"><sup id="id14.8.id1" class="ltx_sup">1</sup>Haixin Liang and Sai-Kit Yeung are with the Division of Integrative Systems and Design, Hong Kong University of Science and Technology.
<sup id="id15.9.id1" class="ltx_sup">2</sup>Ziqiang Zheng and Sai-Kit Yeung are with the Department of Computer Science and Engineering, The Hong Kong University of Science and Technology.<sup id="id16.10.id1" class="ltx_sup">3</sup>Zeyu Ma is with the School of Computer Science and Engineering, University of Electronic Science and Technology of China<sup id="id17.11.id1" class="ltx_sup"><span id="id17.11.id1.1" class="ltx_text ltx_font_italic">†</span></sup>These two authors contributed to this work equally.<sup id="id18.12.id1" class="ltx_sup">∗</sup>Corresponding author: Ziqiang Zheng (zhengziqiang1@gmail.com)
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id19.id1" class="ltx_p">Marine object detection has gained prominence in marine research, driven by the pressing need to unravel oceanic mysteries and enhance our understanding of invaluable marine ecosystems. There is a profound requirement to efficiently and accurately identify and localize <span id="id19.id1.1" class="ltx_text ltx_font_italic">diverse</span> and <span id="id19.id1.2" class="ltx_text ltx_font_italic">unseen</span> marine entities within underwater imagery. The open-marine object detection (<span id="id19.id1.3" class="ltx_text ltx_font_bold">OMOD</span> for short) is required to detect diverse and unseen marine objects, performing categorization and localization simultaneously. To achieve OMOD, we present <span id="id19.id1.4" class="ltx_text ltx_font_bold">MarineDet</span>. We formulate a joint <span id="id19.id1.5" class="ltx_text ltx_font_italic">visual-text semantic space</span> through pre-training and then perform <span id="id19.id1.6" class="ltx_text ltx_font_italic">marine-specific training</span> to achieve in-air-to-marine knowledge transfer. Considering there is no specific dataset designed for OMOD, we construct a <span id="id19.id1.7" class="ltx_text ltx_font_bold">MarineDet dataset</span> consisting of 821 marine-relative object categories to promote and measure OMOD performance. The experimental results demonstrate the superior performance of MarineDet over existing generalist and specialist object detection algorithms. To the best of our knowledge, we are the first to present OMOD, which holds a more valuable and practical setting for marine ecosystem monitoring and management. Our research not only pushes the boundaries of marine understanding but also offers a standard pipeline for OMOD.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Marine ecosystems are vital components of the health of our planet, playing a critical role in regulating climate, supporting biodiversity, and providing essential resources and services to both marine and terrestrial life. Performing marine object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> is essential for comprehending the intricate dynamics of underwater ecosystems, enabling accurate species identification, behavior analysis, and ecosystem health assessment. Marine object detection could also support thorough biodiversity assessments <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, tracking and monitoring the movements of marine species, comprehending ecosystem dynamics, facilitating data-driven insights into the health and behavior of underwater environments, guiding conservation strategies, and bolstering the ability to protect fragile marine habitats.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2310.01931/assets/open_marine.jpg" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="544" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The demonstration of open-marine object detection. To achieve effective open-marine object detection, we first construct a large scale marine vocabulary databased to include a large range of marine object conceptions in 1), then we propose a large scale <span id="S1.F1.4.1" class="ltx_text ltx_font_bold">MarineDet dataset</span> for performing open-vocabulary object detection in 2). During the training procedure, the bounding box annotations and semantic embedding supervision from the <span id="S1.F1.5.2" class="ltx_text ltx_font_italic">seen</span> categories are provided for optimizing the open-vocabulary object detection models in 3). At the final inference stage represented in 4), the trained model could detect the <span id="S1.F1.6.3" class="ltx_text ltx_font_italic">unseen</span> categories based on the hierarchical relationships presented in the constructed marine knowledge database.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">However, the existing marine object detection algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> are mainly limited to close-set object detection. The trained models can only recognize very few fixed pre-defined object categories. It is heavily constrained by its specificity to pre-defined target classes, potentially overlooking or misclassifying novel or rare species, artifacts, or object categories that are not part of the established training dataset. This limitation restricts its adaptability to evolving marine environments and hinders its effectiveness in scenarios where the identification of unforeseen objects is paramount. To address these issues, <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">open-marine object detection</span> (<span id="S1.p2.1.2" class="ltx_text ltx_font_bold">OMOD</span> for short) as demonstrated in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ MarineDet: Towards Open-Marine Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents a unique and advantageous way for underwater analysis, offering a range of benefits that are pivotal in advancing our understanding of marine ecosystems. Unlike existing detection algorithms that focus on specific targets, OMOD allows for the identification of a wide spectrum of marine entities, including both <span id="S1.p2.1.3" class="ltx_text ltx_font_italic">seen</span> and <span id="S1.p2.1.4" class="ltx_text ltx_font_italic">unseen</span> marine object categories. Furthermore, OMOD contributes to biodiversity research by enabling the discovery of new and rare species, facilitating a more comprehensive assessment of ecosystem health and diversity.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Even though OMOD has overwhelming advantages, there are still some significant challenges. Performing OMOD poses challenges that span from identifying <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">unseen</span> or <span id="S1.p3.1.2" class="ltx_text ltx_font_italic">rare</span> species, to adapting to dynamic and diverse underwater environments with varying lighting and underwater conditions. In this paper, we take the first attempt to perform OMOD and present <span id="S1.p3.1.3" class="ltx_text ltx_font_bold">MarineDet</span>, a flexible, applicable, and versatile marine object detector for effectively detecting marine object categories. We formulate a joint <span id="S1.p3.1.4" class="ltx_text ltx_font_bold">visual-text semantic space</span> through contrastive pre-training based on large-scale general-purpose image-text pairs. This pre-training facilitates the alignment of the regional visual features within the images and corresponding textual descriptions. After pre-training, the model has a strong ability to recognize a wide range of objects and discriminate the foreground categories from the background. Then we perform an <span id="S1.p3.1.5" class="ltx_text ltx_font_bold">in-air-to-marine</span> knowledge transfer by transferring the learned pre-trained model to the marine domain. We demonstrate that such <span id="S1.p3.1.6" class="ltx_text ltx_font_bold">marine-specific training</span> could empower the detection model with a strong ability to recognize both <span id="S1.p3.1.7" class="ltx_text ltx_font_bold">seen</span> and <span id="S1.p3.1.8" class="ltx_text ltx_font_bold">unseen</span> marine object categories. To promote and evaluate the performance of OMOD, we also present our <span id="S1.p3.1.9" class="ltx_text ltx_font_bold">MarineDet dataset</span>, including 821 diverse marine-relative object categories with corresponding bounding box annotations. We have conducted comprehensive experiments based on different settings (<em id="S1.p3.1.10" class="ltx_emph ltx_font_italic">e.g.</em>, fully-supervised, open-vocabulary, and fine-grained), exploring the effectiveness and boundaries of our MarineDet. The main contributions of this paper are as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We are the first to perform open-marine object detection, empowering the detection model with the ability to detect a large range of seen and unseen marine objects.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We present our MarineDet dataset, the first dataset specially designed for OMOD. We scalably generate comprehensive attribute annotations based on ChatGPT-3.5/GPT-4.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Comprehensive experiments under various settings have been conducted to explore the effectiveness and boundaries of our MarineDet for OMOD.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Work</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">Marine Object Recognition</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Marine object recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> could unveil the mysteries of the oceans and harness technology to elevate marine research <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, conservation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, and industrial endeavors. Marine object detection provides a valuable tool for environmental monitoring and identifying shifts or disturbances in marine ecosystems. To enable marine object recognition, various datasets (<em id="S2.SS1.p1.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, MAS3K <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, WildFish <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, WildFish++ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, UDD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, SUIM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>) have been proposed for promoting the recognition performance of marine organisms. However, most of the existing datasets only focus on a limited number of marine objects or only provide image-level annotations for fine-grained image classification. In this work, we aim to perform OMOD (recognizing and localizing objects meanwhile), which could not only detect a wide spectrum of seen marine object categories but also some unseen objects.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">Open-vocabulary Object Detection</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Open-vocabulary detection (OVD) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> aims to generalize beyond the limited number of pre-fixed classes during the training phase. The goal is to detect novel classes defined by an unbounded (open) vocabulary at the inference stage. Unlike traditional closed-domain detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, which focuses on limited predefined classes, OVD navigates the intricacies of recognizing both <span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_italic">seen</span> and <span id="S2.SS2.p1.1.2" class="ltx_text ltx_font_italic">unseen</span> objects. This approach is pivotal for scenarios where novel objects or anomalies may arise, offering the potential to unlock new insights, enhance safety, and revolutionize fields such as autonomous driving <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, surveillance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, environmental monitoring <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> and underwater exploring. The dominant way of performing OVD is to adopt a pre-trained visual encoder from a trained cross-modality alignment model (<em id="S2.SS2.p1.1.3" class="ltx_emph ltx_font_italic">e.g.</em>, CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> and BLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>) which is optimized by millions of image-text pairs from public websites. OVR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> adopted the powerful pre-trained image encoder from CLIP and utilized it for detector initialization for better feature extraction on unseen object categories. RegionCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> proposed to perform the regional visual feature and the textual conception alignment to promote the generalization ability to unseen categories. However, there are relatively few or no research works that focus on open-marine object detection. In this work, we make the first attempt to perform OMOD.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2310.01931/assets/x1.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="269" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The framework overview of our MarineDet, where there are two main procedures: 1) <span id="S2.F2.3.1" class="ltx_text ltx_font_bold">pre-training</span> for joint visual-text semantic space construction and 2) <span id="S2.F2.4.2" class="ltx_text ltx_font_bold">marine-specific training</span>.</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Our Method</span>
</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">Overview</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.5" class="ltx_p"><span id="S3.SS1.p1.5.1" class="ltx_text ltx_font_bold">Problem Formulation</span>. We first provide preliminaries for open-marine object detection. We aim to develop algorithms that can accurately detect, localize, and categorize objects within images, even when faced with previously unseen object categories. Let <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="O" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">O</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">𝑂</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">O</annotation></semantics></math> represent the set of all possible object categories, where <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="|O|\rightarrow\infty" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mrow id="S3.SS1.p1.2.m2.1.2" xref="S3.SS1.p1.2.m2.1.2.cmml"><mrow id="S3.SS1.p1.2.m2.1.2.2.2" xref="S3.SS1.p1.2.m2.1.2.2.1.cmml"><mo stretchy="false" id="S3.SS1.p1.2.m2.1.2.2.2.1" xref="S3.SS1.p1.2.m2.1.2.2.1.1.cmml">|</mo><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">O</mi><mo stretchy="false" id="S3.SS1.p1.2.m2.1.2.2.2.2" xref="S3.SS1.p1.2.m2.1.2.2.1.1.cmml">|</mo></mrow><mo stretchy="false" id="S3.SS1.p1.2.m2.1.2.1" xref="S3.SS1.p1.2.m2.1.2.1.cmml">→</mo><mi mathvariant="normal" id="S3.SS1.p1.2.m2.1.2.3" xref="S3.SS1.p1.2.m2.1.2.3.cmml">∞</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.2.cmml" xref="S3.SS1.p1.2.m2.1.2"><ci id="S3.SS1.p1.2.m2.1.2.1.cmml" xref="S3.SS1.p1.2.m2.1.2.1">→</ci><apply id="S3.SS1.p1.2.m2.1.2.2.1.cmml" xref="S3.SS1.p1.2.m2.1.2.2.2"><abs id="S3.SS1.p1.2.m2.1.2.2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.2.2.2.1"></abs><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">𝑂</ci></apply><infinity id="S3.SS1.p1.2.m2.1.2.3.cmml" xref="S3.SS1.p1.2.m2.1.2.3"></infinity></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">|O|\rightarrow\infty</annotation></semantics></math>, indicating an open-ended vocabulary. We detect the presence of objects within the image by generating bounding boxes. We then assign category labels <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="C=\{c_{i}\}" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mrow id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">C</mi><mo id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">=</mo><mrow id="S3.SS1.p1.3.m3.1.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS1.p1.3.m3.1.1.1.1.2" xref="S3.SS1.p1.3.m3.1.1.1.2.cmml">{</mo><msub id="S3.SS1.p1.3.m3.1.1.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.1.1.1.2" xref="S3.SS1.p1.3.m3.1.1.1.1.1.2.cmml">c</mi><mi id="S3.SS1.p1.3.m3.1.1.1.1.1.3" xref="S3.SS1.p1.3.m3.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS1.p1.3.m3.1.1.1.1.3" xref="S3.SS1.p1.3.m3.1.1.1.2.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><eq id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2"></eq><ci id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">𝐶</ci><set id="S3.SS1.p1.3.m3.1.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1"><apply id="S3.SS1.p1.3.m3.1.1.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.1.2">𝑐</ci><ci id="S3.SS1.p1.3.m3.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.1.3">𝑖</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">C=\{c_{i}\}</annotation></semantics></math> to the detected objects based on the similarity between the semantic textual embedding of <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="c_{i}" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><msub id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml"><mi id="S3.SS1.p1.4.m4.1.1.2" xref="S3.SS1.p1.4.m4.1.1.2.cmml">c</mi><mi id="S3.SS1.p1.4.m4.1.1.3" xref="S3.SS1.p1.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><apply id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2">𝑐</ci><ci id="S3.SS1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">c_{i}</annotation></semantics></math> and the regional features identified by the bounding box, where <math id="S3.SS1.p1.5.m5.1" class="ltx_Math" alttext="c_{i}" display="inline"><semantics id="S3.SS1.p1.5.m5.1a"><msub id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml"><mi id="S3.SS1.p1.5.m5.1.1.2" xref="S3.SS1.p1.5.m5.1.1.2.cmml">c</mi><mi id="S3.SS1.p1.5.m5.1.1.3" xref="S3.SS1.p1.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><apply id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1.2">𝑐</ci><ci id="S3.SS1.p1.5.m5.1.1.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">c_{i}</annotation></semantics></math> corresponds to the category label. To perform OMOD, we present <span id="S3.SS1.p1.5.2" class="ltx_text ltx_font_bold">MarineDet</span>, as illustrated in Fig. <a href="#S2.F2" title="Figure 2 ‣ II-B Open-vocabulary Object Detection ‣ II Related Work ‣ MarineDet: Towards Open-Marine Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, where there are two main procedures: 1) <span id="S3.SS1.p1.5.3" class="ltx_text ltx_font_bold">pre-training</span> for joint visual-text semantic space construction and 2) <span id="S3.SS1.p1.5.4" class="ltx_text ltx_font_bold">marine-specific training</span>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Pre-training for Joint Visual-Text Semantic Space</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.7" class="ltx_p">The multi-modal foundation model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> has demonstrated its powerful ability to construct a joint visual-text semantic space, where we could perform various downstream tasks. Through such constructed joint visual-text semantic space, we could include wide object conceptions and align the visual features and the corresponding textual descriptions. In this work, we take the first attempt to demonstrate the significance and valuability of introducing a constructed joint visual-text semantic space for effective open-marine object detection. During the pre-training procedure, we aim to compute the similarity between the extracted visual features from the RPN module and the generated textual conception based on contrastive learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.6" class="ltx_Math" alttext="\begin{array}[]{l}\mathcal{L}_{CL}=-\frac{1}{N}\sum_{i=1}^{N}\log\frac{\exp(\mathcal{S}(v_{i},t_{l}^{{}^{\prime}})/\tau)}{\sum_{l=1}^{L}\exp(\mathcal{S}(v,t_{l})/\tau)},\end{array}" display="block"><semantics id="S3.E1.m1.6a"><mtable displaystyle="true" id="S3.E1.m1.6.6" xref="S3.E1.m1.6.6.cmml"><mtr id="S3.E1.m1.6.6a" xref="S3.E1.m1.6.6.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E1.m1.6.6b" xref="S3.E1.m1.6.6.cmml"><mrow id="S3.E1.m1.6.6.6.6.6.6" xref="S3.E1.m1.6.6.6.6.6.6.1.cmml"><mrow id="S3.E1.m1.6.6.6.6.6.6.1" xref="S3.E1.m1.6.6.6.6.6.6.1.cmml"><msub id="S3.E1.m1.6.6.6.6.6.6.1.2" xref="S3.E1.m1.6.6.6.6.6.6.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.6.6.6.6.6.6.1.2.2" xref="S3.E1.m1.6.6.6.6.6.6.1.2.2.cmml">ℒ</mi><mrow id="S3.E1.m1.6.6.6.6.6.6.1.2.3" xref="S3.E1.m1.6.6.6.6.6.6.1.2.3.cmml"><mi id="S3.E1.m1.6.6.6.6.6.6.1.2.3.2" xref="S3.E1.m1.6.6.6.6.6.6.1.2.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.6.6.6.6.6.6.1.2.3.1" xref="S3.E1.m1.6.6.6.6.6.6.1.2.3.1.cmml">​</mo><mi id="S3.E1.m1.6.6.6.6.6.6.1.2.3.3" xref="S3.E1.m1.6.6.6.6.6.6.1.2.3.3.cmml">L</mi></mrow></msub><mo id="S3.E1.m1.6.6.6.6.6.6.1.1" xref="S3.E1.m1.6.6.6.6.6.6.1.1.cmml">=</mo><mrow id="S3.E1.m1.6.6.6.6.6.6.1.3" xref="S3.E1.m1.6.6.6.6.6.6.1.3.cmml"><mo id="S3.E1.m1.6.6.6.6.6.6.1.3a" xref="S3.E1.m1.6.6.6.6.6.6.1.3.cmml">−</mo><mrow id="S3.E1.m1.6.6.6.6.6.6.1.3.2" xref="S3.E1.m1.6.6.6.6.6.6.1.3.2.cmml"><mstyle displaystyle="false" id="S3.E1.m1.6.6.6.6.6.6.1.3.2.2" xref="S3.E1.m1.6.6.6.6.6.6.1.3.2.2.cmml"><mfrac id="S3.E1.m1.6.6.6.6.6.6.1.3.2.2a" xref="S3.E1.m1.6.6.6.6.6.6.1.3.2.2.cmml"><mn id="S3.E1.m1.6.6.6.6.6.6.1.3.2.2.2" xref="S3.E1.m1.6.6.6.6.6.6.1.3.2.2.2.cmml">1</mn><mi id="S3.E1.m1.6.6.6.6.6.6.1.3.2.2.3" xref="S3.E1.m1.6.6.6.6.6.6.1.3.2.2.3.cmml">N</mi></mfrac></mstyle><mo lspace="0em" rspace="0em" id="S3.E1.m1.6.6.6.6.6.6.1.3.2.1" xref="S3.E1.m1.6.6.6.6.6.6.1.3.2.1.cmml">​</mo><mrow id="S3.E1.m1.6.6.6.6.6.6.1.3.2.3" xref="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.cmml"><mstyle displaystyle="false" id="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.1" xref="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.1.cmml"><msubsup id="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.1a" xref="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.1.cmml"><mo id="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.1.2.2" xref="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.1.2.2.cmml">∑</mo><mrow id="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.1.2.3" xref="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.1.2.3.cmml"><mi id="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.1.2.3.2" xref="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.1.2.3.2.cmml">i</mi><mo id="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.1.2.3.1" xref="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.1.2.3.1.cmml">=</mo><mn id="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.1.2.3.3" xref="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.1.2.3.3.cmml">1</mn></mrow><mi id="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.1.3" xref="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.1.3.cmml">N</mi></msubsup></mstyle><mrow id="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.2" xref="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.2.cmml"><mi id="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.2.1" xref="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.2.1.cmml">log</mi><mo lspace="0.167em" id="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.2a" xref="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.2.cmml">⁡</mo><mstyle displaystyle="false" id="S3.E1.m1.5.5.5.5.5.5" xref="S3.E1.m1.5.5.5.5.5.5.cmml"><mfrac id="S3.E1.m1.5.5.5.5.5.5a" xref="S3.E1.m1.5.5.5.5.5.5.cmml"><mrow id="S3.E1.m1.2.2.2.2.2.2.2.2" xref="S3.E1.m1.2.2.2.2.2.2.2.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">exp</mi><mo id="S3.E1.m1.2.2.2.2.2.2.2.2a" xref="S3.E1.m1.2.2.2.2.2.2.2.3.cmml">⁡</mo><mrow id="S3.E1.m1.2.2.2.2.2.2.2.2.1" xref="S3.E1.m1.2.2.2.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.2.2.2.2.2.2.1.2" xref="S3.E1.m1.2.2.2.2.2.2.2.3.cmml">(</mo><mrow id="S3.E1.m1.2.2.2.2.2.2.2.2.1.1" xref="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.cmml"><mrow id="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2" xref="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.4" xref="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.4.cmml">𝒮</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.3" xref="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.3.cmml">​</mo><mrow id="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.2.2" xref="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.2.3.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.2.2.3" xref="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.2.3.cmml">(</mo><msub id="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.1.1.1.1" xref="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.1.1.1.1.2" xref="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.1.1.1.1.2.cmml">v</mi><mi id="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.1.1.1.1.3" xref="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.2.2.4" xref="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.2.3.cmml">,</mo><msubsup id="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.2.2.2" xref="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.2.2.2.cmml"><mi id="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.2.2.2.2.2" xref="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.2.2.2.2.2.cmml">t</mi><mi id="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.2.2.2.2.3" xref="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.2.2.2.2.3.cmml">l</mi><msup id="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.2.2.2.3" xref="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.2.2.2.3.cmml"><mi id="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.2.2.2.3a" xref="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.2.2.2.3.cmml"></mi><mo id="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.2.2.2.3.1" xref="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.2.2.2.3.1.cmml">′</mo></msup></msubsup><mo stretchy="false" id="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.2.2.5" xref="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.3" xref="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.3.cmml">/</mo><mi id="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.4" xref="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.4.cmml">τ</mi></mrow><mo stretchy="false" id="S3.E1.m1.2.2.2.2.2.2.2.2.1.3" xref="S3.E1.m1.2.2.2.2.2.2.2.3.cmml">)</mo></mrow></mrow><mrow id="S3.E1.m1.5.5.5.5.5.5.5" xref="S3.E1.m1.5.5.5.5.5.5.5.cmml"><mstyle displaystyle="false" id="S3.E1.m1.5.5.5.5.5.5.5.4" xref="S3.E1.m1.5.5.5.5.5.5.5.4.cmml"><msubsup id="S3.E1.m1.5.5.5.5.5.5.5.4a" xref="S3.E1.m1.5.5.5.5.5.5.5.4.cmml"><mo id="S3.E1.m1.5.5.5.5.5.5.5.4.2.2" xref="S3.E1.m1.5.5.5.5.5.5.5.4.2.2.cmml">∑</mo><mrow id="S3.E1.m1.5.5.5.5.5.5.5.4.2.3" xref="S3.E1.m1.5.5.5.5.5.5.5.4.2.3.cmml"><mi id="S3.E1.m1.5.5.5.5.5.5.5.4.2.3.2" xref="S3.E1.m1.5.5.5.5.5.5.5.4.2.3.2.cmml">l</mi><mo id="S3.E1.m1.5.5.5.5.5.5.5.4.2.3.1" xref="S3.E1.m1.5.5.5.5.5.5.5.4.2.3.1.cmml">=</mo><mn id="S3.E1.m1.5.5.5.5.5.5.5.4.2.3.3" xref="S3.E1.m1.5.5.5.5.5.5.5.4.2.3.3.cmml">1</mn></mrow><mi id="S3.E1.m1.5.5.5.5.5.5.5.4.3" xref="S3.E1.m1.5.5.5.5.5.5.5.4.3.cmml">L</mi></msubsup></mstyle><mrow id="S3.E1.m1.5.5.5.5.5.5.5.3.1" xref="S3.E1.m1.5.5.5.5.5.5.5.3.2.cmml"><mi id="S3.E1.m1.4.4.4.4.4.4.4.2" xref="S3.E1.m1.4.4.4.4.4.4.4.2.cmml">exp</mi><mo id="S3.E1.m1.5.5.5.5.5.5.5.3.1a" xref="S3.E1.m1.5.5.5.5.5.5.5.3.2.cmml">⁡</mo><mrow id="S3.E1.m1.5.5.5.5.5.5.5.3.1.1" xref="S3.E1.m1.5.5.5.5.5.5.5.3.2.cmml"><mo stretchy="false" id="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.2" xref="S3.E1.m1.5.5.5.5.5.5.5.3.2.cmml">(</mo><mrow id="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1" xref="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.cmml"><mrow id="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.1" xref="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.1.3" xref="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.1.3.cmml">𝒮</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.1.2" xref="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.1.1.1" xref="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.1.1.1.2" xref="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.1.1.2.cmml">(</mo><mi id="S3.E1.m1.3.3.3.3.3.3.3.1" xref="S3.E1.m1.3.3.3.3.3.3.3.1.cmml">v</mi><mo id="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.1.1.1.3" xref="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.1.1.2.cmml">,</mo><msub id="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.1.1.1.1" xref="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.1.1.1.1.2" xref="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.1.1.1.1.2.cmml">t</mi><mi id="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.1.1.1.1.3" xref="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.1.1.1.1.3.cmml">l</mi></msub><mo stretchy="false" id="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.1.1.1.4" xref="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.2" xref="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.2.cmml">/</mo><mi id="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.3" xref="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.3.cmml">τ</mi></mrow><mo stretchy="false" id="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.3" xref="S3.E1.m1.5.5.5.5.5.5.5.3.2.cmml">)</mo></mrow></mrow></mrow></mfrac></mstyle></mrow></mrow></mrow></mrow></mrow><mo id="S3.E1.m1.6.6.6.6.6.6.2" xref="S3.E1.m1.6.6.6.6.6.6.1.cmml">,</mo></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S3.E1.m1.6b"><matrix id="S3.E1.m1.6.6.cmml" xref="S3.E1.m1.6.6"><matrixrow id="S3.E1.m1.6.6a.cmml" xref="S3.E1.m1.6.6"><apply id="S3.E1.m1.6.6.6.6.6.6.1.cmml" xref="S3.E1.m1.6.6.6.6.6.6"><eq id="S3.E1.m1.6.6.6.6.6.6.1.1.cmml" xref="S3.E1.m1.6.6.6.6.6.6.1.1"></eq><apply id="S3.E1.m1.6.6.6.6.6.6.1.2.cmml" xref="S3.E1.m1.6.6.6.6.6.6.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.6.6.6.6.1.2.1.cmml" xref="S3.E1.m1.6.6.6.6.6.6.1.2">subscript</csymbol><ci id="S3.E1.m1.6.6.6.6.6.6.1.2.2.cmml" xref="S3.E1.m1.6.6.6.6.6.6.1.2.2">ℒ</ci><apply id="S3.E1.m1.6.6.6.6.6.6.1.2.3.cmml" xref="S3.E1.m1.6.6.6.6.6.6.1.2.3"><times id="S3.E1.m1.6.6.6.6.6.6.1.2.3.1.cmml" xref="S3.E1.m1.6.6.6.6.6.6.1.2.3.1"></times><ci id="S3.E1.m1.6.6.6.6.6.6.1.2.3.2.cmml" xref="S3.E1.m1.6.6.6.6.6.6.1.2.3.2">𝐶</ci><ci id="S3.E1.m1.6.6.6.6.6.6.1.2.3.3.cmml" xref="S3.E1.m1.6.6.6.6.6.6.1.2.3.3">𝐿</ci></apply></apply><apply id="S3.E1.m1.6.6.6.6.6.6.1.3.cmml" xref="S3.E1.m1.6.6.6.6.6.6.1.3"><minus id="S3.E1.m1.6.6.6.6.6.6.1.3.1.cmml" xref="S3.E1.m1.6.6.6.6.6.6.1.3"></minus><apply id="S3.E1.m1.6.6.6.6.6.6.1.3.2.cmml" xref="S3.E1.m1.6.6.6.6.6.6.1.3.2"><times id="S3.E1.m1.6.6.6.6.6.6.1.3.2.1.cmml" xref="S3.E1.m1.6.6.6.6.6.6.1.3.2.1"></times><apply id="S3.E1.m1.6.6.6.6.6.6.1.3.2.2.cmml" xref="S3.E1.m1.6.6.6.6.6.6.1.3.2.2"><divide id="S3.E1.m1.6.6.6.6.6.6.1.3.2.2.1.cmml" xref="S3.E1.m1.6.6.6.6.6.6.1.3.2.2"></divide><cn type="integer" id="S3.E1.m1.6.6.6.6.6.6.1.3.2.2.2.cmml" xref="S3.E1.m1.6.6.6.6.6.6.1.3.2.2.2">1</cn><ci id="S3.E1.m1.6.6.6.6.6.6.1.3.2.2.3.cmml" xref="S3.E1.m1.6.6.6.6.6.6.1.3.2.2.3">𝑁</ci></apply><apply id="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.cmml" xref="S3.E1.m1.6.6.6.6.6.6.1.3.2.3"><apply id="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.1.cmml" xref="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.1.1.cmml" xref="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.1">superscript</csymbol><apply id="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.1.2.cmml" xref="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.1.2.1.cmml" xref="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.1">subscript</csymbol><sum id="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.1.2.2.cmml" xref="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.1.2.2"></sum><apply id="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.1.2.3.cmml" xref="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.1.2.3"><eq id="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.1.2.3.1.cmml" xref="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.1.2.3.1"></eq><ci id="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.1.2.3.2.cmml" xref="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.1.2.3.2">𝑖</ci><cn type="integer" id="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.1.2.3.3.cmml" xref="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.1.2.3.3">1</cn></apply></apply><ci id="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.1.3.cmml" xref="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.1.3">𝑁</ci></apply><apply id="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.2.cmml" xref="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.2"><log id="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.2.1.cmml" xref="S3.E1.m1.6.6.6.6.6.6.1.3.2.3.2.1"></log><apply id="S3.E1.m1.5.5.5.5.5.5.cmml" xref="S3.E1.m1.5.5.5.5.5.5"><divide id="S3.E1.m1.5.5.5.5.5.5.6.cmml" xref="S3.E1.m1.5.5.5.5.5.5"></divide><apply id="S3.E1.m1.2.2.2.2.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2.2"><exp id="S3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1"></exp><apply id="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2.2.1.1"><divide id="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.3.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.3"></divide><apply id="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2"><times id="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.3.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.3"></times><ci id="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.4.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.4">𝒮</ci><interval closure="open" id="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.2.3.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.2.2"><apply id="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.1.1.1.1.2">𝑣</ci><ci id="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.2.2.2.1.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.2.2.2">superscript</csymbol><apply id="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.2.2.2.2.1.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.2.2.2.2.2">𝑡</ci><ci id="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.2.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.2.2.2.2.3">𝑙</ci></apply><apply id="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.2.2.2.3"><ci id="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.2.2.2.3.1.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.2.2.2.2.3.1">′</ci></apply></apply></interval></apply><ci id="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.4.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2.2.1.1.4">𝜏</ci></apply></apply><apply id="S3.E1.m1.5.5.5.5.5.5.5.cmml" xref="S3.E1.m1.5.5.5.5.5.5.5"><apply id="S3.E1.m1.5.5.5.5.5.5.5.4.cmml" xref="S3.E1.m1.5.5.5.5.5.5.5.4"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.5.5.5.5.5.4.1.cmml" xref="S3.E1.m1.5.5.5.5.5.5.5.4">superscript</csymbol><apply id="S3.E1.m1.5.5.5.5.5.5.5.4.2.cmml" xref="S3.E1.m1.5.5.5.5.5.5.5.4"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.5.5.5.5.5.4.2.1.cmml" xref="S3.E1.m1.5.5.5.5.5.5.5.4">subscript</csymbol><sum id="S3.E1.m1.5.5.5.5.5.5.5.4.2.2.cmml" xref="S3.E1.m1.5.5.5.5.5.5.5.4.2.2"></sum><apply id="S3.E1.m1.5.5.5.5.5.5.5.4.2.3.cmml" xref="S3.E1.m1.5.5.5.5.5.5.5.4.2.3"><eq id="S3.E1.m1.5.5.5.5.5.5.5.4.2.3.1.cmml" xref="S3.E1.m1.5.5.5.5.5.5.5.4.2.3.1"></eq><ci id="S3.E1.m1.5.5.5.5.5.5.5.4.2.3.2.cmml" xref="S3.E1.m1.5.5.5.5.5.5.5.4.2.3.2">𝑙</ci><cn type="integer" id="S3.E1.m1.5.5.5.5.5.5.5.4.2.3.3.cmml" xref="S3.E1.m1.5.5.5.5.5.5.5.4.2.3.3">1</cn></apply></apply><ci id="S3.E1.m1.5.5.5.5.5.5.5.4.3.cmml" xref="S3.E1.m1.5.5.5.5.5.5.5.4.3">𝐿</ci></apply><apply id="S3.E1.m1.5.5.5.5.5.5.5.3.2.cmml" xref="S3.E1.m1.5.5.5.5.5.5.5.3.1"><exp id="S3.E1.m1.4.4.4.4.4.4.4.2.cmml" xref="S3.E1.m1.4.4.4.4.4.4.4.2"></exp><apply id="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.cmml" xref="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1"><divide id="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.2.cmml" xref="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.2"></divide><apply id="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.1.cmml" xref="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.1"><times id="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.1.2.cmml" xref="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.1.2"></times><ci id="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.1.3.cmml" xref="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.1.3">𝒮</ci><interval closure="open" id="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.1.1.2.cmml" xref="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.1.1.1"><ci id="S3.E1.m1.3.3.3.3.3.3.3.1.cmml" xref="S3.E1.m1.3.3.3.3.3.3.3.1">𝑣</ci><apply id="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.1.1.1.1.2">𝑡</ci><ci id="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.1.1.1.1.3">𝑙</ci></apply></interval></apply><ci id="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.3.cmml" xref="S3.E1.m1.5.5.5.5.5.5.5.3.1.1.1.3">𝜏</ci></apply></apply></apply></apply></apply></apply></apply></apply></apply></matrixrow></matrix></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.6c">\begin{array}[]{l}\mathcal{L}_{CL}=-\frac{1}{N}\sum_{i=1}^{N}\log\frac{\exp(\mathcal{S}(v_{i},t_{l}^{{}^{\prime}})/\tau)}{\sum_{l=1}^{L}\exp(\mathcal{S}(v,t_{l})/\tau)},\end{array}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p1.6" class="ltx_p">where <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">S</annotation></semantics></math> denotes similarity between extracted visual features <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="v_{i}" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><msub id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">v</mi><mi id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">𝑣</ci><ci id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">v_{i}</annotation></semantics></math> and the generated textual conception <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="t_{l}" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><msub id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml"><mi id="S3.SS2.p1.3.m3.1.1.2" xref="S3.SS2.p1.3.m3.1.1.2.cmml">t</mi><mi id="S3.SS2.p1.3.m3.1.1.3" xref="S3.SS2.p1.3.m3.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><apply id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2">𝑡</ci><ci id="S3.SS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">t_{l}</annotation></semantics></math> from embedding set <math id="S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="\{L\}" display="inline"><semantics id="S3.SS2.p1.4.m4.1a"><mrow id="S3.SS2.p1.4.m4.1.2.2" xref="S3.SS2.p1.4.m4.1.2.1.cmml"><mo stretchy="false" id="S3.SS2.p1.4.m4.1.2.2.1" xref="S3.SS2.p1.4.m4.1.2.1.cmml">{</mo><mi id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml">L</mi><mo stretchy="false" id="S3.SS2.p1.4.m4.1.2.2.2" xref="S3.SS2.p1.4.m4.1.2.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><set id="S3.SS2.p1.4.m4.1.2.1.cmml" xref="S3.SS2.p1.4.m4.1.2.2"><ci id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">𝐿</ci></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">\{L\}</annotation></semantics></math>, <math id="S3.SS2.p1.5.m5.1" class="ltx_Math" alttext="t_{l}^{{}^{\prime}}" display="inline"><semantics id="S3.SS2.p1.5.m5.1a"><msubsup id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml"><mi id="S3.SS2.p1.5.m5.1.1.2.2" xref="S3.SS2.p1.5.m5.1.1.2.2.cmml">t</mi><mi id="S3.SS2.p1.5.m5.1.1.2.3" xref="S3.SS2.p1.5.m5.1.1.2.3.cmml">l</mi><msup id="S3.SS2.p1.5.m5.1.1.3" xref="S3.SS2.p1.5.m5.1.1.3.cmml"><mi id="S3.SS2.p1.5.m5.1.1.3a" xref="S3.SS2.p1.5.m5.1.1.3.cmml"></mi><mo id="S3.SS2.p1.5.m5.1.1.3.1" xref="S3.SS2.p1.5.m5.1.1.3.1.cmml">′</mo></msup></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><apply id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m5.1.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1">superscript</csymbol><apply id="S3.SS2.p1.5.m5.1.1.2.cmml" xref="S3.SS2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m5.1.1.2.1.cmml" xref="S3.SS2.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.p1.5.m5.1.1.2.2.cmml" xref="S3.SS2.p1.5.m5.1.1.2.2">𝑡</ci><ci id="S3.SS2.p1.5.m5.1.1.2.3.cmml" xref="S3.SS2.p1.5.m5.1.1.2.3">𝑙</ci></apply><apply id="S3.SS2.p1.5.m5.1.1.3.cmml" xref="S3.SS2.p1.5.m5.1.1.3"><ci id="S3.SS2.p1.5.m5.1.1.3.1.cmml" xref="S3.SS2.p1.5.m5.1.1.3.1">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">t_{l}^{{}^{\prime}}</annotation></semantics></math> is the most similar textual conception to <math id="S3.SS2.p1.6.m6.1" class="ltx_Math" alttext="v_{i}" display="inline"><semantics id="S3.SS2.p1.6.m6.1a"><msub id="S3.SS2.p1.6.m6.1.1" xref="S3.SS2.p1.6.m6.1.1.cmml"><mi id="S3.SS2.p1.6.m6.1.1.2" xref="S3.SS2.p1.6.m6.1.1.2.cmml">v</mi><mi id="S3.SS2.p1.6.m6.1.1.3" xref="S3.SS2.p1.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m6.1b"><apply id="S3.SS2.p1.6.m6.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.6.m6.1.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.p1.6.m6.1.1.2.cmml" xref="S3.SS2.p1.6.m6.1.1.2">𝑣</ci><ci id="S3.SS2.p1.6.m6.1.1.3.cmml" xref="S3.SS2.p1.6.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m6.1c">v_{i}</annotation></semantics></math>. The model is optimized by large-scale image-text pairs. In this way, the model could learn a strong generalized foreground-background discrimination ability.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.2" class="ltx_p">Meanwhile, we also design image-level contrastive loss <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="\mathcal{L}_{CL\_image}" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><msub id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">ℒ</mi><mrow id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml"><mi id="S3.SS2.p2.1.m1.1.1.3.2" xref="S3.SS2.p2.1.m1.1.1.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.1.m1.1.1.3.1" xref="S3.SS2.p2.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p2.1.m1.1.1.3.3" xref="S3.SS2.p2.1.m1.1.1.3.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.1.m1.1.1.3.1a" xref="S3.SS2.p2.1.m1.1.1.3.1.cmml">​</mo><mi mathvariant="normal" id="S3.SS2.p2.1.m1.1.1.3.4" xref="S3.SS2.p2.1.m1.1.1.3.4.cmml">_</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.1.m1.1.1.3.1b" xref="S3.SS2.p2.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p2.1.m1.1.1.3.5" xref="S3.SS2.p2.1.m1.1.1.3.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.1.m1.1.1.3.1c" xref="S3.SS2.p2.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p2.1.m1.1.1.3.6" xref="S3.SS2.p2.1.m1.1.1.3.6.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.1.m1.1.1.3.1d" xref="S3.SS2.p2.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p2.1.m1.1.1.3.7" xref="S3.SS2.p2.1.m1.1.1.3.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.1.m1.1.1.3.1e" xref="S3.SS2.p2.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p2.1.m1.1.1.3.8" xref="S3.SS2.p2.1.m1.1.1.3.8.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.1.m1.1.1.3.1f" xref="S3.SS2.p2.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p2.1.m1.1.1.3.9" xref="S3.SS2.p2.1.m1.1.1.3.9.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">ℒ</ci><apply id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3"><times id="S3.SS2.p2.1.m1.1.1.3.1.cmml" xref="S3.SS2.p2.1.m1.1.1.3.1"></times><ci id="S3.SS2.p2.1.m1.1.1.3.2.cmml" xref="S3.SS2.p2.1.m1.1.1.3.2">𝐶</ci><ci id="S3.SS2.p2.1.m1.1.1.3.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3">𝐿</ci><ci id="S3.SS2.p2.1.m1.1.1.3.4.cmml" xref="S3.SS2.p2.1.m1.1.1.3.4">_</ci><ci id="S3.SS2.p2.1.m1.1.1.3.5.cmml" xref="S3.SS2.p2.1.m1.1.1.3.5">𝑖</ci><ci id="S3.SS2.p2.1.m1.1.1.3.6.cmml" xref="S3.SS2.p2.1.m1.1.1.3.6">𝑚</ci><ci id="S3.SS2.p2.1.m1.1.1.3.7.cmml" xref="S3.SS2.p2.1.m1.1.1.3.7">𝑎</ci><ci id="S3.SS2.p2.1.m1.1.1.3.8.cmml" xref="S3.SS2.p2.1.m1.1.1.3.8">𝑔</ci><ci id="S3.SS2.p2.1.m1.1.1.3.9.cmml" xref="S3.SS2.p2.1.m1.1.1.3.9">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">\mathcal{L}_{CL\_image}</annotation></semantics></math> for aligning the whole image and corresponding textual description. To optimize the overall pre-training, the overall loss function is described as <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="\mathcal{L}_{Pre-train}=\mathcal{L}_{CL}+\mathcal{L}_{CL\_image}" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mrow id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><msub id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.2.m2.1.1.2.2" xref="S3.SS2.p2.2.m2.1.1.2.2.cmml">ℒ</mi><mrow id="S3.SS2.p2.2.m2.1.1.2.3" xref="S3.SS2.p2.2.m2.1.1.2.3.cmml"><mrow id="S3.SS2.p2.2.m2.1.1.2.3.2" xref="S3.SS2.p2.2.m2.1.1.2.3.2.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2.3.2.2" xref="S3.SS2.p2.2.m2.1.1.2.3.2.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.2.m2.1.1.2.3.2.1" xref="S3.SS2.p2.2.m2.1.1.2.3.2.1.cmml">​</mo><mi id="S3.SS2.p2.2.m2.1.1.2.3.2.3" xref="S3.SS2.p2.2.m2.1.1.2.3.2.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.2.m2.1.1.2.3.2.1a" xref="S3.SS2.p2.2.m2.1.1.2.3.2.1.cmml">​</mo><mi id="S3.SS2.p2.2.m2.1.1.2.3.2.4" xref="S3.SS2.p2.2.m2.1.1.2.3.2.4.cmml">e</mi></mrow><mo id="S3.SS2.p2.2.m2.1.1.2.3.1" xref="S3.SS2.p2.2.m2.1.1.2.3.1.cmml">−</mo><mrow id="S3.SS2.p2.2.m2.1.1.2.3.3" xref="S3.SS2.p2.2.m2.1.1.2.3.3.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2.3.3.2" xref="S3.SS2.p2.2.m2.1.1.2.3.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.2.m2.1.1.2.3.3.1" xref="S3.SS2.p2.2.m2.1.1.2.3.3.1.cmml">​</mo><mi id="S3.SS2.p2.2.m2.1.1.2.3.3.3" xref="S3.SS2.p2.2.m2.1.1.2.3.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.2.m2.1.1.2.3.3.1a" xref="S3.SS2.p2.2.m2.1.1.2.3.3.1.cmml">​</mo><mi id="S3.SS2.p2.2.m2.1.1.2.3.3.4" xref="S3.SS2.p2.2.m2.1.1.2.3.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.2.m2.1.1.2.3.3.1b" xref="S3.SS2.p2.2.m2.1.1.2.3.3.1.cmml">​</mo><mi id="S3.SS2.p2.2.m2.1.1.2.3.3.5" xref="S3.SS2.p2.2.m2.1.1.2.3.3.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.2.m2.1.1.2.3.3.1c" xref="S3.SS2.p2.2.m2.1.1.2.3.3.1.cmml">​</mo><mi id="S3.SS2.p2.2.m2.1.1.2.3.3.6" xref="S3.SS2.p2.2.m2.1.1.2.3.3.6.cmml">n</mi></mrow></mrow></msub><mo id="S3.SS2.p2.2.m2.1.1.1" xref="S3.SS2.p2.2.m2.1.1.1.cmml">=</mo><mrow id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml"><msub id="S3.SS2.p2.2.m2.1.1.3.2" xref="S3.SS2.p2.2.m2.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.2.m2.1.1.3.2.2" xref="S3.SS2.p2.2.m2.1.1.3.2.2.cmml">ℒ</mi><mrow id="S3.SS2.p2.2.m2.1.1.3.2.3" xref="S3.SS2.p2.2.m2.1.1.3.2.3.cmml"><mi id="S3.SS2.p2.2.m2.1.1.3.2.3.2" xref="S3.SS2.p2.2.m2.1.1.3.2.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.2.m2.1.1.3.2.3.1" xref="S3.SS2.p2.2.m2.1.1.3.2.3.1.cmml">​</mo><mi id="S3.SS2.p2.2.m2.1.1.3.2.3.3" xref="S3.SS2.p2.2.m2.1.1.3.2.3.3.cmml">L</mi></mrow></msub><mo id="S3.SS2.p2.2.m2.1.1.3.1" xref="S3.SS2.p2.2.m2.1.1.3.1.cmml">+</mo><msub id="S3.SS2.p2.2.m2.1.1.3.3" xref="S3.SS2.p2.2.m2.1.1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.2.m2.1.1.3.3.2" xref="S3.SS2.p2.2.m2.1.1.3.3.2.cmml">ℒ</mi><mrow id="S3.SS2.p2.2.m2.1.1.3.3.3" xref="S3.SS2.p2.2.m2.1.1.3.3.3.cmml"><mi id="S3.SS2.p2.2.m2.1.1.3.3.3.2" xref="S3.SS2.p2.2.m2.1.1.3.3.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.2.m2.1.1.3.3.3.1" xref="S3.SS2.p2.2.m2.1.1.3.3.3.1.cmml">​</mo><mi id="S3.SS2.p2.2.m2.1.1.3.3.3.3" xref="S3.SS2.p2.2.m2.1.1.3.3.3.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.2.m2.1.1.3.3.3.1a" xref="S3.SS2.p2.2.m2.1.1.3.3.3.1.cmml">​</mo><mi mathvariant="normal" id="S3.SS2.p2.2.m2.1.1.3.3.3.4" xref="S3.SS2.p2.2.m2.1.1.3.3.3.4.cmml">_</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.2.m2.1.1.3.3.3.1b" xref="S3.SS2.p2.2.m2.1.1.3.3.3.1.cmml">​</mo><mi id="S3.SS2.p2.2.m2.1.1.3.3.3.5" xref="S3.SS2.p2.2.m2.1.1.3.3.3.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.2.m2.1.1.3.3.3.1c" xref="S3.SS2.p2.2.m2.1.1.3.3.3.1.cmml">​</mo><mi id="S3.SS2.p2.2.m2.1.1.3.3.3.6" xref="S3.SS2.p2.2.m2.1.1.3.3.3.6.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.2.m2.1.1.3.3.3.1d" xref="S3.SS2.p2.2.m2.1.1.3.3.3.1.cmml">​</mo><mi id="S3.SS2.p2.2.m2.1.1.3.3.3.7" xref="S3.SS2.p2.2.m2.1.1.3.3.3.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.2.m2.1.1.3.3.3.1e" xref="S3.SS2.p2.2.m2.1.1.3.3.3.1.cmml">​</mo><mi id="S3.SS2.p2.2.m2.1.1.3.3.3.8" xref="S3.SS2.p2.2.m2.1.1.3.3.3.8.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.2.m2.1.1.3.3.3.1f" xref="S3.SS2.p2.2.m2.1.1.3.3.3.1.cmml">​</mo><mi id="S3.SS2.p2.2.m2.1.1.3.3.3.9" xref="S3.SS2.p2.2.m2.1.1.3.3.3.9.cmml">e</mi></mrow></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><eq id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1"></eq><apply id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.2.1.cmml" xref="S3.SS2.p2.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS2.p2.2.m2.1.1.2.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2.2">ℒ</ci><apply id="S3.SS2.p2.2.m2.1.1.2.3.cmml" xref="S3.SS2.p2.2.m2.1.1.2.3"><minus id="S3.SS2.p2.2.m2.1.1.2.3.1.cmml" xref="S3.SS2.p2.2.m2.1.1.2.3.1"></minus><apply id="S3.SS2.p2.2.m2.1.1.2.3.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2.3.2"><times id="S3.SS2.p2.2.m2.1.1.2.3.2.1.cmml" xref="S3.SS2.p2.2.m2.1.1.2.3.2.1"></times><ci id="S3.SS2.p2.2.m2.1.1.2.3.2.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2.3.2.2">𝑃</ci><ci id="S3.SS2.p2.2.m2.1.1.2.3.2.3.cmml" xref="S3.SS2.p2.2.m2.1.1.2.3.2.3">𝑟</ci><ci id="S3.SS2.p2.2.m2.1.1.2.3.2.4.cmml" xref="S3.SS2.p2.2.m2.1.1.2.3.2.4">𝑒</ci></apply><apply id="S3.SS2.p2.2.m2.1.1.2.3.3.cmml" xref="S3.SS2.p2.2.m2.1.1.2.3.3"><times id="S3.SS2.p2.2.m2.1.1.2.3.3.1.cmml" xref="S3.SS2.p2.2.m2.1.1.2.3.3.1"></times><ci id="S3.SS2.p2.2.m2.1.1.2.3.3.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2.3.3.2">𝑡</ci><ci id="S3.SS2.p2.2.m2.1.1.2.3.3.3.cmml" xref="S3.SS2.p2.2.m2.1.1.2.3.3.3">𝑟</ci><ci id="S3.SS2.p2.2.m2.1.1.2.3.3.4.cmml" xref="S3.SS2.p2.2.m2.1.1.2.3.3.4">𝑎</ci><ci id="S3.SS2.p2.2.m2.1.1.2.3.3.5.cmml" xref="S3.SS2.p2.2.m2.1.1.2.3.3.5">𝑖</ci><ci id="S3.SS2.p2.2.m2.1.1.2.3.3.6.cmml" xref="S3.SS2.p2.2.m2.1.1.2.3.3.6">𝑛</ci></apply></apply></apply><apply id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3"><plus id="S3.SS2.p2.2.m2.1.1.3.1.cmml" xref="S3.SS2.p2.2.m2.1.1.3.1"></plus><apply id="S3.SS2.p2.2.m2.1.1.3.2.cmml" xref="S3.SS2.p2.2.m2.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.3.2.1.cmml" xref="S3.SS2.p2.2.m2.1.1.3.2">subscript</csymbol><ci id="S3.SS2.p2.2.m2.1.1.3.2.2.cmml" xref="S3.SS2.p2.2.m2.1.1.3.2.2">ℒ</ci><apply id="S3.SS2.p2.2.m2.1.1.3.2.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3.2.3"><times id="S3.SS2.p2.2.m2.1.1.3.2.3.1.cmml" xref="S3.SS2.p2.2.m2.1.1.3.2.3.1"></times><ci id="S3.SS2.p2.2.m2.1.1.3.2.3.2.cmml" xref="S3.SS2.p2.2.m2.1.1.3.2.3.2">𝐶</ci><ci id="S3.SS2.p2.2.m2.1.1.3.2.3.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3.2.3.3">𝐿</ci></apply></apply><apply id="S3.SS2.p2.2.m2.1.1.3.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.3.3.1.cmml" xref="S3.SS2.p2.2.m2.1.1.3.3">subscript</csymbol><ci id="S3.SS2.p2.2.m2.1.1.3.3.2.cmml" xref="S3.SS2.p2.2.m2.1.1.3.3.2">ℒ</ci><apply id="S3.SS2.p2.2.m2.1.1.3.3.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3.3.3"><times id="S3.SS2.p2.2.m2.1.1.3.3.3.1.cmml" xref="S3.SS2.p2.2.m2.1.1.3.3.3.1"></times><ci id="S3.SS2.p2.2.m2.1.1.3.3.3.2.cmml" xref="S3.SS2.p2.2.m2.1.1.3.3.3.2">𝐶</ci><ci id="S3.SS2.p2.2.m2.1.1.3.3.3.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3.3.3.3">𝐿</ci><ci id="S3.SS2.p2.2.m2.1.1.3.3.3.4.cmml" xref="S3.SS2.p2.2.m2.1.1.3.3.3.4">_</ci><ci id="S3.SS2.p2.2.m2.1.1.3.3.3.5.cmml" xref="S3.SS2.p2.2.m2.1.1.3.3.3.5">𝑖</ci><ci id="S3.SS2.p2.2.m2.1.1.3.3.3.6.cmml" xref="S3.SS2.p2.2.m2.1.1.3.3.3.6">𝑚</ci><ci id="S3.SS2.p2.2.m2.1.1.3.3.3.7.cmml" xref="S3.SS2.p2.2.m2.1.1.3.3.3.7">𝑎</ci><ci id="S3.SS2.p2.2.m2.1.1.3.3.3.8.cmml" xref="S3.SS2.p2.2.m2.1.1.3.3.3.8">𝑔</ci><ci id="S3.SS2.p2.2.m2.1.1.3.3.3.9.cmml" xref="S3.SS2.p2.2.m2.1.1.3.3.3.9">𝑒</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">\mathcal{L}_{Pre-train}=\mathcal{L}_{CL}+\mathcal{L}_{CL\_image}</annotation></semantics></math>. The pre-training is designed to learn feature representations of a large range of object categories, not just limited pre-defined object categories. Through this pre-training, we could introduce redundant knowledge of wide object categories and promote the ability to recognize the foreground object categories from the background.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.5.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.6.2" class="ltx_text ltx_font_italic">Marine-specific Training</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.2" class="ltx_p">After the pre-training procedure, the trained model
by redundant image-text pairs (containing various object categories) could contain generalized foreground-background discrimination ability. Different from existing close-set object detection algorithms, we replace the classifier with textual category prototypes, which could significantly improve generalization to novel categories under the OMOD setting. We use cosine similarity to calculate the matching probability between regional feature <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="v_{i}" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><msub id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mi id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">v</mi><mi id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2">𝑣</ci><ci id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">v_{i}</annotation></semantics></math> and textual conception <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="t_{l}^{{}^{\prime}}" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><msubsup id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml"><mi id="S3.SS3.p1.2.m2.1.1.2.2" xref="S3.SS3.p1.2.m2.1.1.2.2.cmml">t</mi><mi id="S3.SS3.p1.2.m2.1.1.2.3" xref="S3.SS3.p1.2.m2.1.1.2.3.cmml">l</mi><msup id="S3.SS3.p1.2.m2.1.1.3" xref="S3.SS3.p1.2.m2.1.1.3.cmml"><mi id="S3.SS3.p1.2.m2.1.1.3a" xref="S3.SS3.p1.2.m2.1.1.3.cmml"></mi><mo id="S3.SS3.p1.2.m2.1.1.3.1" xref="S3.SS3.p1.2.m2.1.1.3.1.cmml">′</mo></msup></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><apply id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">superscript</csymbol><apply id="S3.SS3.p1.2.m2.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.2.1.cmml" xref="S3.SS3.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p1.2.m2.1.1.2.2.cmml" xref="S3.SS3.p1.2.m2.1.1.2.2">𝑡</ci><ci id="S3.SS3.p1.2.m2.1.1.2.3.cmml" xref="S3.SS3.p1.2.m2.1.1.2.3">𝑙</ci></apply><apply id="S3.SS3.p1.2.m2.1.1.3.cmml" xref="S3.SS3.p1.2.m2.1.1.3"><ci id="S3.SS3.p1.2.m2.1.1.3.1.cmml" xref="S3.SS3.p1.2.m2.1.1.3.1">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">t_{l}^{{}^{\prime}}</annotation></semantics></math> in the training:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.6" class="ltx_Math" alttext="P(t_{l}^{{}^{\prime}}|v_{i})=\frac{\exp{Sim(t_{l}^{{}^{\prime}},v_{i})}}{\sum_{l\in\{O\}}\exp{Sim(t_{l},v_{i})}}," display="block"><semantics id="S3.E2.m1.6a"><mrow id="S3.E2.m1.6.6.1" xref="S3.E2.m1.6.6.1.1.cmml"><mrow id="S3.E2.m1.6.6.1.1" xref="S3.E2.m1.6.6.1.1.cmml"><mrow id="S3.E2.m1.6.6.1.1.1" xref="S3.E2.m1.6.6.1.1.1.cmml"><mi id="S3.E2.m1.6.6.1.1.1.3" xref="S3.E2.m1.6.6.1.1.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.6.6.1.1.1.2" xref="S3.E2.m1.6.6.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m1.6.6.1.1.1.1.1" xref="S3.E2.m1.6.6.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.6.6.1.1.1.1.1.2" xref="S3.E2.m1.6.6.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.6.6.1.1.1.1.1.1" xref="S3.E2.m1.6.6.1.1.1.1.1.1.cmml"><msubsup id="S3.E2.m1.6.6.1.1.1.1.1.1.2" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.6.6.1.1.1.1.1.1.2.2.2" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2.2.2.cmml">t</mi><mi id="S3.E2.m1.6.6.1.1.1.1.1.1.2.2.3" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2.2.3.cmml">l</mi><msup id="S3.E2.m1.6.6.1.1.1.1.1.1.2.3" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2.3.cmml"><mi id="S3.E2.m1.6.6.1.1.1.1.1.1.2.3a" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2.3.cmml"></mi><mo id="S3.E2.m1.6.6.1.1.1.1.1.1.2.3.1" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2.3.1.cmml">′</mo></msup></msubsup><mo fence="false" id="S3.E2.m1.6.6.1.1.1.1.1.1.1" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.cmml">|</mo><msub id="S3.E2.m1.6.6.1.1.1.1.1.1.3" xref="S3.E2.m1.6.6.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.6.6.1.1.1.1.1.1.3.2" xref="S3.E2.m1.6.6.1.1.1.1.1.1.3.2.cmml">v</mi><mi id="S3.E2.m1.6.6.1.1.1.1.1.1.3.3" xref="S3.E2.m1.6.6.1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo stretchy="false" id="S3.E2.m1.6.6.1.1.1.1.1.3" xref="S3.E2.m1.6.6.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.6.6.1.1.2" xref="S3.E2.m1.6.6.1.1.2.cmml">=</mo><mfrac id="S3.E2.m1.5.5" xref="S3.E2.m1.5.5.cmml"><mrow id="S3.E2.m1.2.2.2" xref="S3.E2.m1.2.2.2.cmml"><mrow id="S3.E2.m1.2.2.2.4" xref="S3.E2.m1.2.2.2.4.cmml"><mi id="S3.E2.m1.2.2.2.4.1" xref="S3.E2.m1.2.2.2.4.1.cmml">exp</mi><mo lspace="0.167em" id="S3.E2.m1.2.2.2.4a" xref="S3.E2.m1.2.2.2.4.cmml">⁡</mo><mrow id="S3.E2.m1.2.2.2.4.2" xref="S3.E2.m1.2.2.2.4.2.cmml"><mi id="S3.E2.m1.2.2.2.4.2.2" xref="S3.E2.m1.2.2.2.4.2.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.2.4.2.1" xref="S3.E2.m1.2.2.2.4.2.1.cmml">​</mo><mi id="S3.E2.m1.2.2.2.4.2.3" xref="S3.E2.m1.2.2.2.4.2.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.2.4.2.1a" xref="S3.E2.m1.2.2.2.4.2.1.cmml">​</mo><mi id="S3.E2.m1.2.2.2.4.2.4" xref="S3.E2.m1.2.2.2.4.2.4.cmml">m</mi></mrow></mrow><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.2.3" xref="S3.E2.m1.2.2.2.3.cmml">​</mo><mrow id="S3.E2.m1.2.2.2.2.2" xref="S3.E2.m1.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.E2.m1.2.2.2.2.2.3" xref="S3.E2.m1.2.2.2.2.3.cmml">(</mo><msubsup id="S3.E2.m1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.2.2" xref="S3.E2.m1.1.1.1.1.1.1.2.2.cmml">t</mi><mi id="S3.E2.m1.1.1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.1.1.2.3.cmml">l</mi><msup id="S3.E2.m1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.3a" xref="S3.E2.m1.1.1.1.1.1.1.3.cmml"></mi><mo id="S3.E2.m1.1.1.1.1.1.1.3.1" xref="S3.E2.m1.1.1.1.1.1.1.3.1.cmml">′</mo></msup></msubsup><mo id="S3.E2.m1.2.2.2.2.2.4" xref="S3.E2.m1.2.2.2.2.3.cmml">,</mo><msub id="S3.E2.m1.2.2.2.2.2.2" xref="S3.E2.m1.2.2.2.2.2.2.cmml"><mi id="S3.E2.m1.2.2.2.2.2.2.2" xref="S3.E2.m1.2.2.2.2.2.2.2.cmml">v</mi><mi id="S3.E2.m1.2.2.2.2.2.2.3" xref="S3.E2.m1.2.2.2.2.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="S3.E2.m1.2.2.2.2.2.5" xref="S3.E2.m1.2.2.2.2.3.cmml">)</mo></mrow></mrow><mrow id="S3.E2.m1.5.5.5" xref="S3.E2.m1.5.5.5.cmml"><msub id="S3.E2.m1.5.5.5.4" xref="S3.E2.m1.5.5.5.4.cmml"><mo id="S3.E2.m1.5.5.5.4.2" xref="S3.E2.m1.5.5.5.4.2.cmml">∑</mo><mrow id="S3.E2.m1.3.3.3.1.1" xref="S3.E2.m1.3.3.3.1.1.cmml"><mi id="S3.E2.m1.3.3.3.1.1.3" xref="S3.E2.m1.3.3.3.1.1.3.cmml">l</mi><mo id="S3.E2.m1.3.3.3.1.1.2" xref="S3.E2.m1.3.3.3.1.1.2.cmml">∈</mo><mrow id="S3.E2.m1.3.3.3.1.1.4.2" xref="S3.E2.m1.3.3.3.1.1.4.1.cmml"><mo stretchy="false" id="S3.E2.m1.3.3.3.1.1.4.2.1" xref="S3.E2.m1.3.3.3.1.1.4.1.cmml">{</mo><mi id="S3.E2.m1.3.3.3.1.1.1" xref="S3.E2.m1.3.3.3.1.1.1.cmml">O</mi><mo stretchy="false" id="S3.E2.m1.3.3.3.1.1.4.2.2" xref="S3.E2.m1.3.3.3.1.1.4.1.cmml">}</mo></mrow></mrow></msub><mrow id="S3.E2.m1.5.5.5.3" xref="S3.E2.m1.5.5.5.3.cmml"><mrow id="S3.E2.m1.5.5.5.3.4" xref="S3.E2.m1.5.5.5.3.4.cmml"><mi id="S3.E2.m1.5.5.5.3.4.1" xref="S3.E2.m1.5.5.5.3.4.1.cmml">exp</mi><mo lspace="0.167em" id="S3.E2.m1.5.5.5.3.4a" xref="S3.E2.m1.5.5.5.3.4.cmml">⁡</mo><mrow id="S3.E2.m1.5.5.5.3.4.2" xref="S3.E2.m1.5.5.5.3.4.2.cmml"><mi id="S3.E2.m1.5.5.5.3.4.2.2" xref="S3.E2.m1.5.5.5.3.4.2.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.5.3.4.2.1" xref="S3.E2.m1.5.5.5.3.4.2.1.cmml">​</mo><mi id="S3.E2.m1.5.5.5.3.4.2.3" xref="S3.E2.m1.5.5.5.3.4.2.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.5.3.4.2.1a" xref="S3.E2.m1.5.5.5.3.4.2.1.cmml">​</mo><mi id="S3.E2.m1.5.5.5.3.4.2.4" xref="S3.E2.m1.5.5.5.3.4.2.4.cmml">m</mi></mrow></mrow><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.5.3.3" xref="S3.E2.m1.5.5.5.3.3.cmml">​</mo><mrow id="S3.E2.m1.5.5.5.3.2.2" xref="S3.E2.m1.5.5.5.3.2.3.cmml"><mo stretchy="false" id="S3.E2.m1.5.5.5.3.2.2.3" xref="S3.E2.m1.5.5.5.3.2.3.cmml">(</mo><msub id="S3.E2.m1.4.4.4.2.1.1.1" xref="S3.E2.m1.4.4.4.2.1.1.1.cmml"><mi id="S3.E2.m1.4.4.4.2.1.1.1.2" xref="S3.E2.m1.4.4.4.2.1.1.1.2.cmml">t</mi><mi id="S3.E2.m1.4.4.4.2.1.1.1.3" xref="S3.E2.m1.4.4.4.2.1.1.1.3.cmml">l</mi></msub><mo id="S3.E2.m1.5.5.5.3.2.2.4" xref="S3.E2.m1.5.5.5.3.2.3.cmml">,</mo><msub id="S3.E2.m1.5.5.5.3.2.2.2" xref="S3.E2.m1.5.5.5.3.2.2.2.cmml"><mi id="S3.E2.m1.5.5.5.3.2.2.2.2" xref="S3.E2.m1.5.5.5.3.2.2.2.2.cmml">v</mi><mi id="S3.E2.m1.5.5.5.3.2.2.2.3" xref="S3.E2.m1.5.5.5.3.2.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="S3.E2.m1.5.5.5.3.2.2.5" xref="S3.E2.m1.5.5.5.3.2.3.cmml">)</mo></mrow></mrow></mrow></mfrac></mrow><mo id="S3.E2.m1.6.6.1.2" xref="S3.E2.m1.6.6.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.6b"><apply id="S3.E2.m1.6.6.1.1.cmml" xref="S3.E2.m1.6.6.1"><eq id="S3.E2.m1.6.6.1.1.2.cmml" xref="S3.E2.m1.6.6.1.1.2"></eq><apply id="S3.E2.m1.6.6.1.1.1.cmml" xref="S3.E2.m1.6.6.1.1.1"><times id="S3.E2.m1.6.6.1.1.1.2.cmml" xref="S3.E2.m1.6.6.1.1.1.2"></times><ci id="S3.E2.m1.6.6.1.1.1.3.cmml" xref="S3.E2.m1.6.6.1.1.1.3">𝑃</ci><apply id="S3.E2.m1.6.6.1.1.1.1.1.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.6.6.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1">conditional</csymbol><apply id="S3.E2.m1.6.6.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E2.m1.6.6.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2.2.2">𝑡</ci><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2.2.3">𝑙</ci></apply><apply id="S3.E2.m1.6.6.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2.3"><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2.3.1">′</ci></apply></apply><apply id="S3.E2.m1.6.6.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.3.2">𝑣</ci><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply></apply><apply id="S3.E2.m1.5.5.cmml" xref="S3.E2.m1.5.5"><divide id="S3.E2.m1.5.5.6.cmml" xref="S3.E2.m1.5.5"></divide><apply id="S3.E2.m1.2.2.2.cmml" xref="S3.E2.m1.2.2.2"><times id="S3.E2.m1.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.3"></times><apply id="S3.E2.m1.2.2.2.4.cmml" xref="S3.E2.m1.2.2.2.4"><exp id="S3.E2.m1.2.2.2.4.1.cmml" xref="S3.E2.m1.2.2.2.4.1"></exp><apply id="S3.E2.m1.2.2.2.4.2.cmml" xref="S3.E2.m1.2.2.2.4.2"><times id="S3.E2.m1.2.2.2.4.2.1.cmml" xref="S3.E2.m1.2.2.2.4.2.1"></times><ci id="S3.E2.m1.2.2.2.4.2.2.cmml" xref="S3.E2.m1.2.2.2.4.2.2">𝑆</ci><ci id="S3.E2.m1.2.2.2.4.2.3.cmml" xref="S3.E2.m1.2.2.2.4.2.3">𝑖</ci><ci id="S3.E2.m1.2.2.2.4.2.4.cmml" xref="S3.E2.m1.2.2.2.4.2.4">𝑚</ci></apply></apply><interval closure="open" id="S3.E2.m1.2.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.2.2"><apply id="S3.E2.m1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.2">𝑡</ci><ci id="S3.E2.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.3">𝑙</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3"><ci id="S3.E2.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3.1">′</ci></apply></apply><apply id="S3.E2.m1.2.2.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.2.2.2.2.1.cmml" xref="S3.E2.m1.2.2.2.2.2.2">subscript</csymbol><ci id="S3.E2.m1.2.2.2.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2.2.2.2">𝑣</ci><ci id="S3.E2.m1.2.2.2.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.2.2.2.3">𝑖</ci></apply></interval></apply><apply id="S3.E2.m1.5.5.5.cmml" xref="S3.E2.m1.5.5.5"><apply id="S3.E2.m1.5.5.5.4.cmml" xref="S3.E2.m1.5.5.5.4"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.5.4.1.cmml" xref="S3.E2.m1.5.5.5.4">subscript</csymbol><sum id="S3.E2.m1.5.5.5.4.2.cmml" xref="S3.E2.m1.5.5.5.4.2"></sum><apply id="S3.E2.m1.3.3.3.1.1.cmml" xref="S3.E2.m1.3.3.3.1.1"><in id="S3.E2.m1.3.3.3.1.1.2.cmml" xref="S3.E2.m1.3.3.3.1.1.2"></in><ci id="S3.E2.m1.3.3.3.1.1.3.cmml" xref="S3.E2.m1.3.3.3.1.1.3">𝑙</ci><set id="S3.E2.m1.3.3.3.1.1.4.1.cmml" xref="S3.E2.m1.3.3.3.1.1.4.2"><ci id="S3.E2.m1.3.3.3.1.1.1.cmml" xref="S3.E2.m1.3.3.3.1.1.1">𝑂</ci></set></apply></apply><apply id="S3.E2.m1.5.5.5.3.cmml" xref="S3.E2.m1.5.5.5.3"><times id="S3.E2.m1.5.5.5.3.3.cmml" xref="S3.E2.m1.5.5.5.3.3"></times><apply id="S3.E2.m1.5.5.5.3.4.cmml" xref="S3.E2.m1.5.5.5.3.4"><exp id="S3.E2.m1.5.5.5.3.4.1.cmml" xref="S3.E2.m1.5.5.5.3.4.1"></exp><apply id="S3.E2.m1.5.5.5.3.4.2.cmml" xref="S3.E2.m1.5.5.5.3.4.2"><times id="S3.E2.m1.5.5.5.3.4.2.1.cmml" xref="S3.E2.m1.5.5.5.3.4.2.1"></times><ci id="S3.E2.m1.5.5.5.3.4.2.2.cmml" xref="S3.E2.m1.5.5.5.3.4.2.2">𝑆</ci><ci id="S3.E2.m1.5.5.5.3.4.2.3.cmml" xref="S3.E2.m1.5.5.5.3.4.2.3">𝑖</ci><ci id="S3.E2.m1.5.5.5.3.4.2.4.cmml" xref="S3.E2.m1.5.5.5.3.4.2.4">𝑚</ci></apply></apply><interval closure="open" id="S3.E2.m1.5.5.5.3.2.3.cmml" xref="S3.E2.m1.5.5.5.3.2.2"><apply id="S3.E2.m1.4.4.4.2.1.1.1.cmml" xref="S3.E2.m1.4.4.4.2.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.4.2.1.1.1.1.cmml" xref="S3.E2.m1.4.4.4.2.1.1.1">subscript</csymbol><ci id="S3.E2.m1.4.4.4.2.1.1.1.2.cmml" xref="S3.E2.m1.4.4.4.2.1.1.1.2">𝑡</ci><ci id="S3.E2.m1.4.4.4.2.1.1.1.3.cmml" xref="S3.E2.m1.4.4.4.2.1.1.1.3">𝑙</ci></apply><apply id="S3.E2.m1.5.5.5.3.2.2.2.cmml" xref="S3.E2.m1.5.5.5.3.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.5.3.2.2.2.1.cmml" xref="S3.E2.m1.5.5.5.3.2.2.2">subscript</csymbol><ci id="S3.E2.m1.5.5.5.3.2.2.2.2.cmml" xref="S3.E2.m1.5.5.5.3.2.2.2.2">𝑣</ci><ci id="S3.E2.m1.5.5.5.3.2.2.2.3.cmml" xref="S3.E2.m1.5.5.5.3.2.2.2.3">𝑖</ci></apply></interval></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.6c">P(t_{l}^{{}^{\prime}}|v_{i})=\frac{\exp{Sim(t_{l}^{{}^{\prime}},v_{i})}}{\sum_{l\in\{O\}}\exp{Sim(t_{l},v_{i})}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p1.6" class="ltx_p">where <math id="S3.SS3.p1.3.m1.1" class="ltx_Math" alttext="Sim()" display="inline"><semantics id="S3.SS3.p1.3.m1.1a"><mrow id="S3.SS3.p1.3.m1.1.1" xref="S3.SS3.p1.3.m1.1.1.cmml"><mi id="S3.SS3.p1.3.m1.1.1.2" xref="S3.SS3.p1.3.m1.1.1.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.3.m1.1.1.1" xref="S3.SS3.p1.3.m1.1.1.1.cmml">​</mo><mi id="S3.SS3.p1.3.m1.1.1.3" xref="S3.SS3.p1.3.m1.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.3.m1.1.1.1a" xref="S3.SS3.p1.3.m1.1.1.1.cmml">​</mo><mi id="S3.SS3.p1.3.m1.1.1.4" xref="S3.SS3.p1.3.m1.1.1.4.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.3.m1.1.1.1b" xref="S3.SS3.p1.3.m1.1.1.1.cmml">​</mo><mrow id="S3.SS3.p1.3.m1.1.1.5.2" xref="S3.SS3.p1.3.m1.1.1.cmml"><mo stretchy="false" id="S3.SS3.p1.3.m1.1.1.5.2.1" xref="S3.SS3.p1.3.m1.1.1.5.1.cmml">(</mo><mo stretchy="false" id="S3.SS3.p1.3.m1.1.1.5.2.2" xref="S3.SS3.p1.3.m1.1.1.5.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m1.1b"><apply id="S3.SS3.p1.3.m1.1.1.cmml" xref="S3.SS3.p1.3.m1.1.1"><times id="S3.SS3.p1.3.m1.1.1.1.cmml" xref="S3.SS3.p1.3.m1.1.1.1"></times><ci id="S3.SS3.p1.3.m1.1.1.2.cmml" xref="S3.SS3.p1.3.m1.1.1.2">𝑆</ci><ci id="S3.SS3.p1.3.m1.1.1.3.cmml" xref="S3.SS3.p1.3.m1.1.1.3">𝑖</ci><ci id="S3.SS3.p1.3.m1.1.1.4.cmml" xref="S3.SS3.p1.3.m1.1.1.4">𝑚</ci><list id="S3.SS3.p1.3.m1.1.1.5.1.cmml" xref="S3.SS3.p1.3.m1.1.1.5.2.1"></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m1.1c">Sim()</annotation></semantics></math> is Cosine similarity function, and <math id="S3.SS3.p1.4.m2.1" class="ltx_Math" alttext="t_{l}^{{}^{\prime}}" display="inline"><semantics id="S3.SS3.p1.4.m2.1a"><msubsup id="S3.SS3.p1.4.m2.1.1" xref="S3.SS3.p1.4.m2.1.1.cmml"><mi id="S3.SS3.p1.4.m2.1.1.2.2" xref="S3.SS3.p1.4.m2.1.1.2.2.cmml">t</mi><mi id="S3.SS3.p1.4.m2.1.1.2.3" xref="S3.SS3.p1.4.m2.1.1.2.3.cmml">l</mi><msup id="S3.SS3.p1.4.m2.1.1.3" xref="S3.SS3.p1.4.m2.1.1.3.cmml"><mi id="S3.SS3.p1.4.m2.1.1.3a" xref="S3.SS3.p1.4.m2.1.1.3.cmml"></mi><mo id="S3.SS3.p1.4.m2.1.1.3.1" xref="S3.SS3.p1.4.m2.1.1.3.1.cmml">′</mo></msup></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m2.1b"><apply id="S3.SS3.p1.4.m2.1.1.cmml" xref="S3.SS3.p1.4.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.4.m2.1.1.1.cmml" xref="S3.SS3.p1.4.m2.1.1">superscript</csymbol><apply id="S3.SS3.p1.4.m2.1.1.2.cmml" xref="S3.SS3.p1.4.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.4.m2.1.1.2.1.cmml" xref="S3.SS3.p1.4.m2.1.1">subscript</csymbol><ci id="S3.SS3.p1.4.m2.1.1.2.2.cmml" xref="S3.SS3.p1.4.m2.1.1.2.2">𝑡</ci><ci id="S3.SS3.p1.4.m2.1.1.2.3.cmml" xref="S3.SS3.p1.4.m2.1.1.2.3">𝑙</ci></apply><apply id="S3.SS3.p1.4.m2.1.1.3.cmml" xref="S3.SS3.p1.4.m2.1.1.3"><ci id="S3.SS3.p1.4.m2.1.1.3.1.cmml" xref="S3.SS3.p1.4.m2.1.1.3.1">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m2.1c">t_{l}^{{}^{\prime}}</annotation></semantics></math> is specific category in <math id="S3.SS3.p1.5.m3.1" class="ltx_Math" alttext="O" display="inline"><semantics id="S3.SS3.p1.5.m3.1a"><mi id="S3.SS3.p1.5.m3.1.1" xref="S3.SS3.p1.5.m3.1.1.cmml">O</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.5.m3.1b"><ci id="S3.SS3.p1.5.m3.1.1.cmml" xref="S3.SS3.p1.5.m3.1.1">𝑂</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.5.m3.1c">O</annotation></semantics></math>. We assign the category label to the image regions within the generated bounding box based on the maximum matching probability. However, directly adopting the pre-trained model for detector initialization and feature extraction for OMOD faces two tricky challenges: 1) the huge conception distance between the in-air object categories presented in CLIP and marine object categories; and 2) the appearance shift from the in-air condition to the underwater/marine conditions. Since the CLIP model is mainly optimized by general-purpose image-text pairs, there are relatively few images containing marine object conceptions. Furthermore, the RPN module also shows a limited ability to extract meaningful foregrounds from marine images. We support these two claims in Sec. <a href="#S4.SS3" title="IV-C Comparisons with SOTAs ‣ IV Experiments ‣ MarineDet: Towards Open-Marine Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-C</span></span></a>. To address these issues, we propose a novel <span id="S3.SS3.p1.6.1" class="ltx_text ltx_font_bold">MarineDet dataset</span>, which is specially designed for OMOD. We fine-tune the whole detection network (initialized from the pre-trained model) and the RPN module to the marine domain. To obtain accurate and robust region features, we have also preserved the detection loss function <math id="S3.SS3.p1.6.m4.1" class="ltx_Math" alttext="\mathcal{L}_{Det}" display="inline"><semantics id="S3.SS3.p1.6.m4.1a"><msub id="S3.SS3.p1.6.m4.1.1" xref="S3.SS3.p1.6.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p1.6.m4.1.1.2" xref="S3.SS3.p1.6.m4.1.1.2.cmml">ℒ</mi><mrow id="S3.SS3.p1.6.m4.1.1.3" xref="S3.SS3.p1.6.m4.1.1.3.cmml"><mi id="S3.SS3.p1.6.m4.1.1.3.2" xref="S3.SS3.p1.6.m4.1.1.3.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.6.m4.1.1.3.1" xref="S3.SS3.p1.6.m4.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p1.6.m4.1.1.3.3" xref="S3.SS3.p1.6.m4.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.6.m4.1.1.3.1a" xref="S3.SS3.p1.6.m4.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p1.6.m4.1.1.3.4" xref="S3.SS3.p1.6.m4.1.1.3.4.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.6.m4.1b"><apply id="S3.SS3.p1.6.m4.1.1.cmml" xref="S3.SS3.p1.6.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.6.m4.1.1.1.cmml" xref="S3.SS3.p1.6.m4.1.1">subscript</csymbol><ci id="S3.SS3.p1.6.m4.1.1.2.cmml" xref="S3.SS3.p1.6.m4.1.1.2">ℒ</ci><apply id="S3.SS3.p1.6.m4.1.1.3.cmml" xref="S3.SS3.p1.6.m4.1.1.3"><times id="S3.SS3.p1.6.m4.1.1.3.1.cmml" xref="S3.SS3.p1.6.m4.1.1.3.1"></times><ci id="S3.SS3.p1.6.m4.1.1.3.2.cmml" xref="S3.SS3.p1.6.m4.1.1.3.2">𝐷</ci><ci id="S3.SS3.p1.6.m4.1.1.3.3.cmml" xref="S3.SS3.p1.6.m4.1.1.3.3">𝑒</ci><ci id="S3.SS3.p1.6.m4.1.1.3.4.cmml" xref="S3.SS3.p1.6.m4.1.1.3.4">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.6.m4.1c">\mathcal{L}_{Det}</annotation></semantics></math> from Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. Through an <span id="S3.SS3.p1.6.2" class="ltx_text ltx_font_bold">in-air-to-marine</span> knowledge transfer, our MarineDet could effectively perform domain-specific object detection. More importantly, we argue that such an in-air-to-marine fine-tuning could achieve better performance than the existing object detection algorithms initialized from ImageNet pre-trained weights. The detector could be better initialized with a strong foreground-background discrimination ability and shared feature extraction ability (<em id="S3.SS3.p1.6.3" class="ltx_emph ltx_font_italic">e.g.</em>, color, texture, shape, and <span id="S3.SS3.p1.6.4" class="ltx_text ltx_font_italic">etc</span>).</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Experiments</span>
</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">MarineDet Dataset</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Considering there is no comprehensive dataset designed for OMOD, in this work, we propose our <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">MarineDet dataset</span>, which contains 821 diverse object categories. We have also included non-organism categories (<em id="S4.SS1.p1.1.2" class="ltx_emph ltx_font_italic">e.g.</em>, underwater wrecks, surveying devices, sculptures, and <span id="S4.SS1.p1.1.3" class="ltx_text ltx_font_italic">etc</span>). To include a diverse and comprehensive marine-relative object category list, we ask ChatGPT-3.5/GPT-4 to generate the category candidates and we remove the duplicates. For each category, we manually download relative images from the Internet or crawl the required images from the Google image engine and Flickr website. During the annotation procedure, we manually label these images with dense bounding box annotations. For the images from each category, we <span id="S4.SS1.p1.1.4" class="ltx_text ltx_font_bold">only</span> label the objects that belong to the defined object category while ignoring other object categories. In other words, we only label one dominant object category for each image. We provide visualization of some images (with large illumination, visibility, and diversity variations) from our MarineDet dataset in Fig. <a href="#S4.F3" title="Figure 3 ‣ IV-A MarineDet Dataset ‣ IV Experiments ‣ MarineDet: Towards Open-Marine Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Since our images are crawled from public websites, there are inevitably some noisy images. We remove these noisy images and make sure every category contains at least 10 images. We provide the statistics of our MarineDet dataset and a direct comparison with the existing marine object recognition datasets in Table <a href="#S4.T1" title="TABLE I ‣ IV-A MarineDet Dataset ‣ IV Experiments ‣ MarineDet: Towards Open-Marine Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>. Our MarineDet dataset is the first dataset that contains a large range of marine object conceptions.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2310.01931/assets/challenge_imgs.jpg" id="S4.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="256" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>We illustrate some images from our MarineDet dataset. For images with more than one single object, we only visualize one object instance for better illustration.</figcaption>
</figure>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span></figcaption>
<figcaption class="ltx_caption">A direct comparison between our MarineDet dataset with existing marine object recognition datasets.</figcaption>
<div id="S4.T1.12" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:390.3pt;height:75pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-135.2pt,25.8pt) scale(0.590750983579283,0.590750983579283) ;">
<table id="S4.T1.12.12" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.12.12.13.1" class="ltx_tr">
<th id="S4.T1.12.12.13.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Datasets</th>
<th id="S4.T1.12.12.13.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Categories</th>
<th id="S4.T1.12.12.13.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Images</th>
<th id="S4.T1.12.12.13.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Annotation</th>
<th id="S4.T1.12.12.13.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">No-organism</th>
<th id="S4.T1.12.12.13.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Diversity</th>
<th id="S4.T1.12.12.13.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Camouflaged</th>
<th id="S4.T1.12.12.13.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Task</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.2.2.2" class="ltx_tr">
<td id="S4.T1.2.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">URPC2018</td>
<td id="S4.T1.2.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4</td>
<td id="S4.T1.2.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3,701</td>
<td id="S4.T1.2.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">BBOX</td>
<td id="S4.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T1.1.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T1.1.1.1.1.m1.1a"><mo id="S4.T1.1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.m1.1b"><times id="S4.T1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.m1.1c">\times</annotation></semantics></math></td>
<td id="S4.T1.2.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Medium</td>
<td id="S4.T1.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T1.2.2.2.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T1.2.2.2.2.m1.1a"><mo id="S4.T1.2.2.2.2.m1.1.1" xref="S4.T1.2.2.2.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.2.m1.1b"><times id="S4.T1.2.2.2.2.m1.1.1.cmml" xref="S4.T1.2.2.2.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.2.m1.1c">\times</annotation></semantics></math></td>
<td id="S4.T1.2.2.2.8" class="ltx_td ltx_align_center ltx_border_t">Underwater robotics contest</td>
</tr>
<tr id="S4.T1.4.4.4" class="ltx_tr">
<td id="S4.T1.4.4.4.3" class="ltx_td ltx_align_center ltx_border_r">DUO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>
</td>
<td id="S4.T1.4.4.4.4" class="ltx_td ltx_align_center ltx_border_r">4</td>
<td id="S4.T1.4.4.4.5" class="ltx_td ltx_align_center ltx_border_r">7,782</td>
<td id="S4.T1.4.4.4.6" class="ltx_td ltx_align_center ltx_border_r">Mask</td>
<td id="S4.T1.3.3.3.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T1.3.3.3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T1.3.3.3.1.m1.1a"><mo id="S4.T1.3.3.3.1.m1.1.1" xref="S4.T1.3.3.3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.1.m1.1b"><times id="S4.T1.3.3.3.1.m1.1.1.cmml" xref="S4.T1.3.3.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.1.m1.1c">\times</annotation></semantics></math></td>
<td id="S4.T1.4.4.4.7" class="ltx_td ltx_align_center ltx_border_r">Medium</td>
<td id="S4.T1.4.4.4.2" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T1.4.4.4.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T1.4.4.4.2.m1.1a"><mo id="S4.T1.4.4.4.2.m1.1.1" xref="S4.T1.4.4.4.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.4.2.m1.1b"><times id="S4.T1.4.4.4.2.m1.1.1.cmml" xref="S4.T1.4.4.4.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.4.2.m1.1c">\times</annotation></semantics></math></td>
<td id="S4.T1.4.4.4.8" class="ltx_td ltx_align_center">Underwater robot picking</td>
</tr>
<tr id="S4.T1.6.6.6" class="ltx_tr">
<td id="S4.T1.6.6.6.3" class="ltx_td ltx_align_center ltx_border_r">SUIM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>
</td>
<td id="S4.T1.6.6.6.4" class="ltx_td ltx_align_center ltx_border_r">8</td>
<td id="S4.T1.6.6.6.5" class="ltx_td ltx_align_center ltx_border_r">1,500</td>
<td id="S4.T1.6.6.6.6" class="ltx_td ltx_align_center ltx_border_r">Mask</td>
<td id="S4.T1.5.5.5.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T1.5.5.5.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S4.T1.5.5.5.1.m1.1a"><mi mathvariant="normal" id="S4.T1.5.5.5.1.m1.1.1" xref="S4.T1.5.5.5.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T1.5.5.5.1.m1.1b"><ci id="S4.T1.5.5.5.1.m1.1.1.cmml" xref="S4.T1.5.5.5.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.5.5.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S4.T1.6.6.6.7" class="ltx_td ltx_align_center ltx_border_r">Medium</td>
<td id="S4.T1.6.6.6.2" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T1.6.6.6.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T1.6.6.6.2.m1.1a"><mo id="S4.T1.6.6.6.2.m1.1.1" xref="S4.T1.6.6.6.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.6.6.6.2.m1.1b"><times id="S4.T1.6.6.6.2.m1.1.1.cmml" xref="S4.T1.6.6.6.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.6.6.2.m1.1c">\times</annotation></semantics></math></td>
<td id="S4.T1.6.6.6.8" class="ltx_td ltx_align_center">Underwater scene segmentation</td>
</tr>
<tr id="S4.T1.8.8.8" class="ltx_tr">
<td id="S4.T1.8.8.8.3" class="ltx_td ltx_align_center ltx_border_r">MAS3K <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>
</td>
<td id="S4.T1.8.8.8.4" class="ltx_td ltx_align_center ltx_border_r">37</td>
<td id="S4.T1.8.8.8.5" class="ltx_td ltx_align_center ltx_border_r">3,103</td>
<td id="S4.T1.8.8.8.6" class="ltx_td ltx_align_center ltx_border_r">Mask</td>
<td id="S4.T1.7.7.7.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T1.7.7.7.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T1.7.7.7.1.m1.1a"><mo id="S4.T1.7.7.7.1.m1.1.1" xref="S4.T1.7.7.7.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.7.7.7.1.m1.1b"><times id="S4.T1.7.7.7.1.m1.1.1.cmml" xref="S4.T1.7.7.7.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.7.7.1.m1.1c">\times</annotation></semantics></math></td>
<td id="S4.T1.8.8.8.7" class="ltx_td ltx_align_center ltx_border_r">High</td>
<td id="S4.T1.8.8.8.2" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T1.8.8.8.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S4.T1.8.8.8.2.m1.1a"><mi mathvariant="normal" id="S4.T1.8.8.8.2.m1.1.1" xref="S4.T1.8.8.8.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T1.8.8.8.2.m1.1b"><ci id="S4.T1.8.8.8.2.m1.1.1.cmml" xref="S4.T1.8.8.8.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.8.8.8.2.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S4.T1.8.8.8.8" class="ltx_td ltx_align_center">Marine animal segmentation</td>
</tr>
<tr id="S4.T1.10.10.10" class="ltx_tr">
<td id="S4.T1.10.10.10.3" class="ltx_td ltx_align_center ltx_border_r">Wildfish <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>
</td>
<td id="S4.T1.10.10.10.4" class="ltx_td ltx_align_center ltx_border_r">1,000</td>
<td id="S4.T1.10.10.10.5" class="ltx_td ltx_align_center ltx_border_r">54,459</td>
<td id="S4.T1.10.10.10.6" class="ltx_td ltx_align_center ltx_border_r">Category</td>
<td id="S4.T1.9.9.9.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T1.9.9.9.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T1.9.9.9.1.m1.1a"><mo id="S4.T1.9.9.9.1.m1.1.1" xref="S4.T1.9.9.9.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.9.9.9.1.m1.1b"><times id="S4.T1.9.9.9.1.m1.1.1.cmml" xref="S4.T1.9.9.9.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.9.9.9.1.m1.1c">\times</annotation></semantics></math></td>
<td id="S4.T1.10.10.10.7" class="ltx_td ltx_align_center ltx_border_r">High</td>
<td id="S4.T1.10.10.10.2" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T1.10.10.10.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T1.10.10.10.2.m1.1a"><mo id="S4.T1.10.10.10.2.m1.1.1" xref="S4.T1.10.10.10.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.10.10.10.2.m1.1b"><times id="S4.T1.10.10.10.2.m1.1.1.cmml" xref="S4.T1.10.10.10.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.10.10.10.2.m1.1c">\times</annotation></semantics></math></td>
<td id="S4.T1.10.10.10.8" class="ltx_td ltx_align_center">Fine-grained fish classification</td>
</tr>
<tr id="S4.T1.12.12.12" class="ltx_tr" style="background-color:#D6D6D6;">
<td id="S4.T1.12.12.12.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t"><span id="S4.T1.12.12.12.3.1" class="ltx_text" style="background-color:#D6D6D6;">MarineDet</span></td>
<td id="S4.T1.12.12.12.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t"><span id="S4.T1.12.12.12.4.1" class="ltx_text" style="background-color:#D6D6D6;">821</span></td>
<td id="S4.T1.12.12.12.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t"><span id="S4.T1.12.12.12.5.1" class="ltx_text" style="background-color:#D6D6D6;">22,679</span></td>
<td id="S4.T1.12.12.12.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t"><span id="S4.T1.12.12.12.6.1" class="ltx_text" style="background-color:#D6D6D6;">BBOX</span></td>
<td id="S4.T1.11.11.11.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t"><math id="S4.T1.11.11.11.1.m1.1" class="ltx_Math" style="background-color:#D6D6D6;" alttext="\checkmark" display="inline"><semantics id="S4.T1.11.11.11.1.m1.1a"><mi mathbackground="#D6D6D6" mathvariant="normal" id="S4.T1.11.11.11.1.m1.1.1" xref="S4.T1.11.11.11.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T1.11.11.11.1.m1.1b"><ci id="S4.T1.11.11.11.1.m1.1.1.cmml" xref="S4.T1.11.11.11.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.11.11.11.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S4.T1.12.12.12.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t"><span id="S4.T1.12.12.12.7.1" class="ltx_text" style="background-color:#D6D6D6;">High</span></td>
<td id="S4.T1.12.12.12.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t"><math id="S4.T1.12.12.12.2.m1.1" class="ltx_Math" style="background-color:#D6D6D6;" alttext="\checkmark" display="inline"><semantics id="S4.T1.12.12.12.2.m1.1a"><mi mathbackground="#D6D6D6" mathvariant="normal" id="S4.T1.12.12.12.2.m1.1.1" xref="S4.T1.12.12.12.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T1.12.12.12.2.m1.1b"><ci id="S4.T1.12.12.12.2.m1.1.1.cmml" xref="S4.T1.12.12.12.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.12.12.12.2.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S4.T1.12.12.12.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.12.12.12.8.1" class="ltx_text" style="background-color:#D6D6D6;">Open-marine object detection</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">Instruction-following attribute generation based on ChatGPT-3.5/GPT-4</span>. To save human power, we generate scalable attribute annotations for each object category based on ChatGPT-3.5/GPT-4 following designed instructions. We generate the hierarchical biological classification of the marine objects, including “Kingdom”, “Phylum”, “Class”, “Order”, “Family”, “Genus” and “Species” annotations. There are 31 object categories that cannot be defined by the ChatGPT-3.5/GPT-4. We choose “Class” as the clustering metric to obtain 26 Classes except 31 undefined object categories. Besides, in our MarineDet dataset, some object categories are given the scientific name (<em id="S4.SS1.p2.1.2" class="ltx_emph ltx_font_italic">e.g.</em>, “Argopecten irrdaians”) rather than the common name (“Atlantic bay scallop”). Considering this, we generated the common name of these 821 object categories. All generated attribute annotations will be used for data split and evaluation. We will release these annotations with the bounding box annotations for more detailed and fine-grained marine object detection. Since all the attribute annotations are generated by ChatGPT-3.5/GPT-4, there are inevitably mistakes and noises.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.5.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.6.2" class="ltx_text ltx_font_italic">Implementation Details and Experimental Setups</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.5" class="ltx_p"><span id="S4.SS2.p1.5.1" class="ltx_text ltx_font_bold">Implementation details</span>. There are mainly two procedures in our framework: <span id="S4.SS2.p1.5.2" class="ltx_text ltx_font_italic">Pre-training</span> and <span id="S4.SS2.p1.5.3" class="ltx_text ltx_font_italic">Training</span>. <span id="S4.SS2.p1.5.4" class="ltx_text ltx_font_bold">Pre-training</span>. In the pre-training procedure, we adopt a pre-trained and <span id="S4.SS2.p1.5.5" class="ltx_text ltx_font_bold">frozen</span> CLIP language encoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> as the language encoder and a ResNet-50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> network as the visual backbone. The COCO Caption dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> is used for contrastive learning. We adopt an SGD optimizer to optimize the model. The model is first trained with a learning rate of <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="lr=5e^{-3}" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mrow id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml"><mi id="S4.SS2.p1.1.m1.1.1.2.2" xref="S4.SS2.p1.1.m1.1.1.2.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.1.m1.1.1.2.1" xref="S4.SS2.p1.1.m1.1.1.2.1.cmml">​</mo><mi id="S4.SS2.p1.1.m1.1.1.2.3" xref="S4.SS2.p1.1.m1.1.1.2.3.cmml">r</mi></mrow><mo id="S4.SS2.p1.1.m1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.cmml">=</mo><mrow id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml"><mn id="S4.SS2.p1.1.m1.1.1.3.2" xref="S4.SS2.p1.1.m1.1.1.3.2.cmml">5</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p1.1.m1.1.1.3.1" xref="S4.SS2.p1.1.m1.1.1.3.1.cmml">​</mo><msup id="S4.SS2.p1.1.m1.1.1.3.3" xref="S4.SS2.p1.1.m1.1.1.3.3.cmml"><mi id="S4.SS2.p1.1.m1.1.1.3.3.2" xref="S4.SS2.p1.1.m1.1.1.3.3.2.cmml">e</mi><mrow id="S4.SS2.p1.1.m1.1.1.3.3.3" xref="S4.SS2.p1.1.m1.1.1.3.3.3.cmml"><mo id="S4.SS2.p1.1.m1.1.1.3.3.3a" xref="S4.SS2.p1.1.m1.1.1.3.3.3.cmml">−</mo><mn id="S4.SS2.p1.1.m1.1.1.3.3.3.2" xref="S4.SS2.p1.1.m1.1.1.3.3.3.2.cmml">3</mn></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><eq id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1"></eq><apply id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2"><times id="S4.SS2.p1.1.m1.1.1.2.1.cmml" xref="S4.SS2.p1.1.m1.1.1.2.1"></times><ci id="S4.SS2.p1.1.m1.1.1.2.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2.2">𝑙</ci><ci id="S4.SS2.p1.1.m1.1.1.2.3.cmml" xref="S4.SS2.p1.1.m1.1.1.2.3">𝑟</ci></apply><apply id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3"><times id="S4.SS2.p1.1.m1.1.1.3.1.cmml" xref="S4.SS2.p1.1.m1.1.1.3.1"></times><cn type="integer" id="S4.SS2.p1.1.m1.1.1.3.2.cmml" xref="S4.SS2.p1.1.m1.1.1.3.2">5</cn><apply id="S4.SS2.p1.1.m1.1.1.3.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3.3"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.1.1.3.3.1.cmml" xref="S4.SS2.p1.1.m1.1.1.3.3">superscript</csymbol><ci id="S4.SS2.p1.1.m1.1.1.3.3.2.cmml" xref="S4.SS2.p1.1.m1.1.1.3.3.2">𝑒</ci><apply id="S4.SS2.p1.1.m1.1.1.3.3.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3.3.3"><minus id="S4.SS2.p1.1.m1.1.1.3.3.3.1.cmml" xref="S4.SS2.p1.1.m1.1.1.3.3.3"></minus><cn type="integer" id="S4.SS2.p1.1.m1.1.1.3.3.3.2.cmml" xref="S4.SS2.p1.1.m1.1.1.3.3.3.2">3</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">lr=5e^{-3}</annotation></semantics></math> for 300K iterations, and then <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="lr=5e^{-4}" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mrow id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><mrow id="S4.SS2.p1.2.m2.1.1.2" xref="S4.SS2.p1.2.m2.1.1.2.cmml"><mi id="S4.SS2.p1.2.m2.1.1.2.2" xref="S4.SS2.p1.2.m2.1.1.2.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.2.m2.1.1.2.1" xref="S4.SS2.p1.2.m2.1.1.2.1.cmml">​</mo><mi id="S4.SS2.p1.2.m2.1.1.2.3" xref="S4.SS2.p1.2.m2.1.1.2.3.cmml">r</mi></mrow><mo id="S4.SS2.p1.2.m2.1.1.1" xref="S4.SS2.p1.2.m2.1.1.1.cmml">=</mo><mrow id="S4.SS2.p1.2.m2.1.1.3" xref="S4.SS2.p1.2.m2.1.1.3.cmml"><mn id="S4.SS2.p1.2.m2.1.1.3.2" xref="S4.SS2.p1.2.m2.1.1.3.2.cmml">5</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p1.2.m2.1.1.3.1" xref="S4.SS2.p1.2.m2.1.1.3.1.cmml">​</mo><msup id="S4.SS2.p1.2.m2.1.1.3.3" xref="S4.SS2.p1.2.m2.1.1.3.3.cmml"><mi id="S4.SS2.p1.2.m2.1.1.3.3.2" xref="S4.SS2.p1.2.m2.1.1.3.3.2.cmml">e</mi><mrow id="S4.SS2.p1.2.m2.1.1.3.3.3" xref="S4.SS2.p1.2.m2.1.1.3.3.3.cmml"><mo id="S4.SS2.p1.2.m2.1.1.3.3.3a" xref="S4.SS2.p1.2.m2.1.1.3.3.3.cmml">−</mo><mn id="S4.SS2.p1.2.m2.1.1.3.3.3.2" xref="S4.SS2.p1.2.m2.1.1.3.3.3.2.cmml">4</mn></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><eq id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1.1"></eq><apply id="S4.SS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2"><times id="S4.SS2.p1.2.m2.1.1.2.1.cmml" xref="S4.SS2.p1.2.m2.1.1.2.1"></times><ci id="S4.SS2.p1.2.m2.1.1.2.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2.2">𝑙</ci><ci id="S4.SS2.p1.2.m2.1.1.2.3.cmml" xref="S4.SS2.p1.2.m2.1.1.2.3">𝑟</ci></apply><apply id="S4.SS2.p1.2.m2.1.1.3.cmml" xref="S4.SS2.p1.2.m2.1.1.3"><times id="S4.SS2.p1.2.m2.1.1.3.1.cmml" xref="S4.SS2.p1.2.m2.1.1.3.1"></times><cn type="integer" id="S4.SS2.p1.2.m2.1.1.3.2.cmml" xref="S4.SS2.p1.2.m2.1.1.3.2">5</cn><apply id="S4.SS2.p1.2.m2.1.1.3.3.cmml" xref="S4.SS2.p1.2.m2.1.1.3.3"><csymbol cd="ambiguous" id="S4.SS2.p1.2.m2.1.1.3.3.1.cmml" xref="S4.SS2.p1.2.m2.1.1.3.3">superscript</csymbol><ci id="S4.SS2.p1.2.m2.1.1.3.3.2.cmml" xref="S4.SS2.p1.2.m2.1.1.3.3.2">𝑒</ci><apply id="S4.SS2.p1.2.m2.1.1.3.3.3.cmml" xref="S4.SS2.p1.2.m2.1.1.3.3.3"><minus id="S4.SS2.p1.2.m2.1.1.3.3.3.1.cmml" xref="S4.SS2.p1.2.m2.1.1.3.3.3"></minus><cn type="integer" id="S4.SS2.p1.2.m2.1.1.3.3.3.2.cmml" xref="S4.SS2.p1.2.m2.1.1.3.3.3.2">4</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">lr=5e^{-4}</annotation></semantics></math> for another 300K iterations. The whole training process is with a batch size of 16 on 4 Tesla A100 GPUs, which will take about 1 week. In the <span id="S4.SS2.p1.5.6" class="ltx_text ltx_font_bold">Training</span> procedure, we adopt the Faster RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> as our baseline model and initialize the network backbone with the pre-training model. We replace the classifier weight with textual category embedding. We start the training process by setting the learning rate as <math id="S4.SS2.p1.3.m3.1" class="ltx_Math" alttext="lr=5e^{-3}" display="inline"><semantics id="S4.SS2.p1.3.m3.1a"><mrow id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml"><mrow id="S4.SS2.p1.3.m3.1.1.2" xref="S4.SS2.p1.3.m3.1.1.2.cmml"><mi id="S4.SS2.p1.3.m3.1.1.2.2" xref="S4.SS2.p1.3.m3.1.1.2.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.3.m3.1.1.2.1" xref="S4.SS2.p1.3.m3.1.1.2.1.cmml">​</mo><mi id="S4.SS2.p1.3.m3.1.1.2.3" xref="S4.SS2.p1.3.m3.1.1.2.3.cmml">r</mi></mrow><mo id="S4.SS2.p1.3.m3.1.1.1" xref="S4.SS2.p1.3.m3.1.1.1.cmml">=</mo><mrow id="S4.SS2.p1.3.m3.1.1.3" xref="S4.SS2.p1.3.m3.1.1.3.cmml"><mn id="S4.SS2.p1.3.m3.1.1.3.2" xref="S4.SS2.p1.3.m3.1.1.3.2.cmml">5</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p1.3.m3.1.1.3.1" xref="S4.SS2.p1.3.m3.1.1.3.1.cmml">​</mo><msup id="S4.SS2.p1.3.m3.1.1.3.3" xref="S4.SS2.p1.3.m3.1.1.3.3.cmml"><mi id="S4.SS2.p1.3.m3.1.1.3.3.2" xref="S4.SS2.p1.3.m3.1.1.3.3.2.cmml">e</mi><mrow id="S4.SS2.p1.3.m3.1.1.3.3.3" xref="S4.SS2.p1.3.m3.1.1.3.3.3.cmml"><mo id="S4.SS2.p1.3.m3.1.1.3.3.3a" xref="S4.SS2.p1.3.m3.1.1.3.3.3.cmml">−</mo><mn id="S4.SS2.p1.3.m3.1.1.3.3.3.2" xref="S4.SS2.p1.3.m3.1.1.3.3.3.2.cmml">3</mn></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><apply id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1"><eq id="S4.SS2.p1.3.m3.1.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1.1"></eq><apply id="S4.SS2.p1.3.m3.1.1.2.cmml" xref="S4.SS2.p1.3.m3.1.1.2"><times id="S4.SS2.p1.3.m3.1.1.2.1.cmml" xref="S4.SS2.p1.3.m3.1.1.2.1"></times><ci id="S4.SS2.p1.3.m3.1.1.2.2.cmml" xref="S4.SS2.p1.3.m3.1.1.2.2">𝑙</ci><ci id="S4.SS2.p1.3.m3.1.1.2.3.cmml" xref="S4.SS2.p1.3.m3.1.1.2.3">𝑟</ci></apply><apply id="S4.SS2.p1.3.m3.1.1.3.cmml" xref="S4.SS2.p1.3.m3.1.1.3"><times id="S4.SS2.p1.3.m3.1.1.3.1.cmml" xref="S4.SS2.p1.3.m3.1.1.3.1"></times><cn type="integer" id="S4.SS2.p1.3.m3.1.1.3.2.cmml" xref="S4.SS2.p1.3.m3.1.1.3.2">5</cn><apply id="S4.SS2.p1.3.m3.1.1.3.3.cmml" xref="S4.SS2.p1.3.m3.1.1.3.3"><csymbol cd="ambiguous" id="S4.SS2.p1.3.m3.1.1.3.3.1.cmml" xref="S4.SS2.p1.3.m3.1.1.3.3">superscript</csymbol><ci id="S4.SS2.p1.3.m3.1.1.3.3.2.cmml" xref="S4.SS2.p1.3.m3.1.1.3.3.2">𝑒</ci><apply id="S4.SS2.p1.3.m3.1.1.3.3.3.cmml" xref="S4.SS2.p1.3.m3.1.1.3.3.3"><minus id="S4.SS2.p1.3.m3.1.1.3.3.3.1.cmml" xref="S4.SS2.p1.3.m3.1.1.3.3.3"></minus><cn type="integer" id="S4.SS2.p1.3.m3.1.1.3.3.3.2.cmml" xref="S4.SS2.p1.3.m3.1.1.3.3.3.2">3</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">lr=5e^{-3}</annotation></semantics></math> and then decreasing it to <math id="S4.SS2.p1.4.m4.1" class="ltx_Math" alttext="lr=5e^{-4}" display="inline"><semantics id="S4.SS2.p1.4.m4.1a"><mrow id="S4.SS2.p1.4.m4.1.1" xref="S4.SS2.p1.4.m4.1.1.cmml"><mrow id="S4.SS2.p1.4.m4.1.1.2" xref="S4.SS2.p1.4.m4.1.1.2.cmml"><mi id="S4.SS2.p1.4.m4.1.1.2.2" xref="S4.SS2.p1.4.m4.1.1.2.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.4.m4.1.1.2.1" xref="S4.SS2.p1.4.m4.1.1.2.1.cmml">​</mo><mi id="S4.SS2.p1.4.m4.1.1.2.3" xref="S4.SS2.p1.4.m4.1.1.2.3.cmml">r</mi></mrow><mo id="S4.SS2.p1.4.m4.1.1.1" xref="S4.SS2.p1.4.m4.1.1.1.cmml">=</mo><mrow id="S4.SS2.p1.4.m4.1.1.3" xref="S4.SS2.p1.4.m4.1.1.3.cmml"><mn id="S4.SS2.p1.4.m4.1.1.3.2" xref="S4.SS2.p1.4.m4.1.1.3.2.cmml">5</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p1.4.m4.1.1.3.1" xref="S4.SS2.p1.4.m4.1.1.3.1.cmml">​</mo><msup id="S4.SS2.p1.4.m4.1.1.3.3" xref="S4.SS2.p1.4.m4.1.1.3.3.cmml"><mi id="S4.SS2.p1.4.m4.1.1.3.3.2" xref="S4.SS2.p1.4.m4.1.1.3.3.2.cmml">e</mi><mrow id="S4.SS2.p1.4.m4.1.1.3.3.3" xref="S4.SS2.p1.4.m4.1.1.3.3.3.cmml"><mo id="S4.SS2.p1.4.m4.1.1.3.3.3a" xref="S4.SS2.p1.4.m4.1.1.3.3.3.cmml">−</mo><mn id="S4.SS2.p1.4.m4.1.1.3.3.3.2" xref="S4.SS2.p1.4.m4.1.1.3.3.3.2.cmml">4</mn></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.4.m4.1b"><apply id="S4.SS2.p1.4.m4.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1"><eq id="S4.SS2.p1.4.m4.1.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1.1"></eq><apply id="S4.SS2.p1.4.m4.1.1.2.cmml" xref="S4.SS2.p1.4.m4.1.1.2"><times id="S4.SS2.p1.4.m4.1.1.2.1.cmml" xref="S4.SS2.p1.4.m4.1.1.2.1"></times><ci id="S4.SS2.p1.4.m4.1.1.2.2.cmml" xref="S4.SS2.p1.4.m4.1.1.2.2">𝑙</ci><ci id="S4.SS2.p1.4.m4.1.1.2.3.cmml" xref="S4.SS2.p1.4.m4.1.1.2.3">𝑟</ci></apply><apply id="S4.SS2.p1.4.m4.1.1.3.cmml" xref="S4.SS2.p1.4.m4.1.1.3"><times id="S4.SS2.p1.4.m4.1.1.3.1.cmml" xref="S4.SS2.p1.4.m4.1.1.3.1"></times><cn type="integer" id="S4.SS2.p1.4.m4.1.1.3.2.cmml" xref="S4.SS2.p1.4.m4.1.1.3.2">5</cn><apply id="S4.SS2.p1.4.m4.1.1.3.3.cmml" xref="S4.SS2.p1.4.m4.1.1.3.3"><csymbol cd="ambiguous" id="S4.SS2.p1.4.m4.1.1.3.3.1.cmml" xref="S4.SS2.p1.4.m4.1.1.3.3">superscript</csymbol><ci id="S4.SS2.p1.4.m4.1.1.3.3.2.cmml" xref="S4.SS2.p1.4.m4.1.1.3.3.2">𝑒</ci><apply id="S4.SS2.p1.4.m4.1.1.3.3.3.cmml" xref="S4.SS2.p1.4.m4.1.1.3.3.3"><minus id="S4.SS2.p1.4.m4.1.1.3.3.3.1.cmml" xref="S4.SS2.p1.4.m4.1.1.3.3.3"></minus><cn type="integer" id="S4.SS2.p1.4.m4.1.1.3.3.3.2.cmml" xref="S4.SS2.p1.4.m4.1.1.3.3.3.2">4</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.4.m4.1c">lr=5e^{-4}</annotation></semantics></math> and <math id="S4.SS2.p1.5.m5.1" class="ltx_Math" alttext="lr=5e^{-5}" display="inline"><semantics id="S4.SS2.p1.5.m5.1a"><mrow id="S4.SS2.p1.5.m5.1.1" xref="S4.SS2.p1.5.m5.1.1.cmml"><mrow id="S4.SS2.p1.5.m5.1.1.2" xref="S4.SS2.p1.5.m5.1.1.2.cmml"><mi id="S4.SS2.p1.5.m5.1.1.2.2" xref="S4.SS2.p1.5.m5.1.1.2.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.5.m5.1.1.2.1" xref="S4.SS2.p1.5.m5.1.1.2.1.cmml">​</mo><mi id="S4.SS2.p1.5.m5.1.1.2.3" xref="S4.SS2.p1.5.m5.1.1.2.3.cmml">r</mi></mrow><mo id="S4.SS2.p1.5.m5.1.1.1" xref="S4.SS2.p1.5.m5.1.1.1.cmml">=</mo><mrow id="S4.SS2.p1.5.m5.1.1.3" xref="S4.SS2.p1.5.m5.1.1.3.cmml"><mn id="S4.SS2.p1.5.m5.1.1.3.2" xref="S4.SS2.p1.5.m5.1.1.3.2.cmml">5</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p1.5.m5.1.1.3.1" xref="S4.SS2.p1.5.m5.1.1.3.1.cmml">​</mo><msup id="S4.SS2.p1.5.m5.1.1.3.3" xref="S4.SS2.p1.5.m5.1.1.3.3.cmml"><mi id="S4.SS2.p1.5.m5.1.1.3.3.2" xref="S4.SS2.p1.5.m5.1.1.3.3.2.cmml">e</mi><mrow id="S4.SS2.p1.5.m5.1.1.3.3.3" xref="S4.SS2.p1.5.m5.1.1.3.3.3.cmml"><mo id="S4.SS2.p1.5.m5.1.1.3.3.3a" xref="S4.SS2.p1.5.m5.1.1.3.3.3.cmml">−</mo><mn id="S4.SS2.p1.5.m5.1.1.3.3.3.2" xref="S4.SS2.p1.5.m5.1.1.3.3.3.2.cmml">5</mn></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.5.m5.1b"><apply id="S4.SS2.p1.5.m5.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1"><eq id="S4.SS2.p1.5.m5.1.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1.1"></eq><apply id="S4.SS2.p1.5.m5.1.1.2.cmml" xref="S4.SS2.p1.5.m5.1.1.2"><times id="S4.SS2.p1.5.m5.1.1.2.1.cmml" xref="S4.SS2.p1.5.m5.1.1.2.1"></times><ci id="S4.SS2.p1.5.m5.1.1.2.2.cmml" xref="S4.SS2.p1.5.m5.1.1.2.2">𝑙</ci><ci id="S4.SS2.p1.5.m5.1.1.2.3.cmml" xref="S4.SS2.p1.5.m5.1.1.2.3">𝑟</ci></apply><apply id="S4.SS2.p1.5.m5.1.1.3.cmml" xref="S4.SS2.p1.5.m5.1.1.3"><times id="S4.SS2.p1.5.m5.1.1.3.1.cmml" xref="S4.SS2.p1.5.m5.1.1.3.1"></times><cn type="integer" id="S4.SS2.p1.5.m5.1.1.3.2.cmml" xref="S4.SS2.p1.5.m5.1.1.3.2">5</cn><apply id="S4.SS2.p1.5.m5.1.1.3.3.cmml" xref="S4.SS2.p1.5.m5.1.1.3.3"><csymbol cd="ambiguous" id="S4.SS2.p1.5.m5.1.1.3.3.1.cmml" xref="S4.SS2.p1.5.m5.1.1.3.3">superscript</csymbol><ci id="S4.SS2.p1.5.m5.1.1.3.3.2.cmml" xref="S4.SS2.p1.5.m5.1.1.3.3.2">𝑒</ci><apply id="S4.SS2.p1.5.m5.1.1.3.3.3.cmml" xref="S4.SS2.p1.5.m5.1.1.3.3.3"><minus id="S4.SS2.p1.5.m5.1.1.3.3.3.1.cmml" xref="S4.SS2.p1.5.m5.1.1.3.3.3"></minus><cn type="integer" id="S4.SS2.p1.5.m5.1.1.3.3.3.2.cmml" xref="S4.SS2.p1.5.m5.1.1.3.3.3.2">5</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.5.m5.1c">lr=5e^{-5}</annotation></semantics></math> when appropriate. We train 60k iterations with a batch size of 8 on 2 Tesla A100 GPUs which takes 16 hours.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.1" class="ltx_p"><span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_bold">Experimental setup</span>. We mainly perform experiments under two settings: <span id="S4.SS2.p2.1.2" class="ltx_text ltx_font_bold">fully supervised</span> and <span id="S4.SS2.p2.1.3" class="ltx_text ltx_font_bold">open-vocabulary</span>. Under the former fully supervised setting, all the 821 object categories are used for training. Under the latter open-vocabulary setting, some object categories are regarded as “<span id="S4.SS2.p2.1.4" class="ltx_text ltx_font_italic">seen</span>” categories and other object categories as “<span id="S4.SS2.p2.1.5" class="ltx_text ltx_font_italic">unseen</span>” categories. Under both settings, four-fifths of the images from each category are used for training and the rest of the images are used for evaluation. We construct <span id="S4.SS2.p2.1.6" class="ltx_text ltx_font_italic">seen</span>/<span id="S4.SS2.p2.1.7" class="ltx_text ltx_font_italic">unseen</span> split following three settings: 1) Intra-“<span id="S4.SS2.p2.1.8" class="ltx_text ltx_font_italic">Class</span>”: all the object categories (613 categories) from 20 “Classes” are regarded as <span id="S4.SS2.p2.1.9" class="ltx_text ltx_font_italic">seen</span> categories and other 177 categories from other 6 “Classes” as <span id="S4.SS2.p2.1.10" class="ltx_text ltx_font_italic">unseen</span> categories. 2) Inter-“<span id="S4.SS2.p2.1.11" class="ltx_text ltx_font_italic">Class</span>”: we choose one object category from every 4 object categories in each “Class” as the <span id="S4.SS2.p2.1.12" class="ltx_text ltx_font_italic">unseen</span> category and the other 3 object categories as <span id="S4.SS2.p2.1.13" class="ltx_text ltx_font_italic">seen</span> categories. We omit the “Class” that contains less than 4 object categories. In total, there are 572 object categories as <span id="S4.SS2.p2.1.14" class="ltx_text ltx_font_italic">seen</span> categories and other 183 categories as <span id="S4.SS2.p2.1.15" class="ltx_text ltx_font_italic">unseen</span> categories. 3) “<span id="S4.SS2.p2.1.16" class="ltx_text ltx_font_italic">Class</span>”-level: we adopt the same 20 “Classes” as <span id="S4.SS2.p2.1.17" class="ltx_text ltx_font_italic">seen</span> categories and the other 6 “Classes” as <span id="S4.SS2.p2.1.18" class="ltx_text ltx_font_italic">unseen</span> categories. Compared with Intra-“<span id="S4.SS2.p2.1.19" class="ltx_text ltx_font_italic">Class</span>” setting, the “<span id="S4.SS2.p2.1.20" class="ltx_text ltx_font_italic">Class</span>”-level setting is performing coarse object recognition while the Intra-“<span id="S4.SS2.p2.1.21" class="ltx_text ltx_font_italic">Class</span>” setting is performing fine-grained object recognition.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2310.01931/assets/x2.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="173" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The qualitative comparison between different algorithms. The left part of the dashed line represents the results of <span id="S4.F4.3.1" class="ltx_text ltx_font_italic">seen</span> “Classes” while the right part shows the results of <span id="S4.F4.4.2" class="ltx_text ltx_font_italic">unseen</span> “Classes”.</figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.5.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.6.2" class="ltx_text ltx_font_italic">Comparisons with SOTAs</span>
</h3>

<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span></figcaption>
<figcaption class="ltx_caption">We report the quantitative result comparison between different object detection algorithms. <math id="S4.T2.1.m1.1" class="ltx_Math" alttext="-" display="inline"><semantics id="S4.T2.1.m1.1a"><mo id="S4.T2.1.m1.1.1" xref="S4.T2.1.m1.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.m1.1b"><minus id="S4.T2.1.m1.1.1.cmml" xref="S4.T2.1.m1.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.m1.1c">-</annotation></semantics></math> indicates that the results cannot be computed.</figcaption>
<div id="S4.T2.27" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:665.6pt;height:113.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-195.4pt,33.3pt) scale(0.63,0.63) ;">
<table id="S4.T2.27.26" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.27.26.27.1" class="ltx_tr">
<td id="S4.T2.27.26.27.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" rowspan="2"><span id="S4.T2.27.26.27.1.1.1" class="ltx_text">Method</span></td>
<td id="S4.T2.27.26.27.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" rowspan="2"><span id="S4.T2.27.26.27.1.2.1" class="ltx_text">
<span id="S4.T2.27.26.27.1.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.27.26.27.1.2.1.1.1" class="ltx_tr">
<span id="S4.T2.27.26.27.1.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">MarineDet</span></span>
<span id="S4.T2.27.26.27.1.2.1.1.2" class="ltx_tr">
<span id="S4.T2.27.26.27.1.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Dataset</span></span>
</span></span></td>
<td id="S4.T2.27.26.27.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="6">Seen (20 “Classes”)</td>
<td id="S4.T2.27.26.27.1.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="7">Unseen (6 “Classes”)</td>
</tr>
<tr id="S4.T2.3.2.2" class="ltx_tr">
<td id="S4.T2.2.1.1.1" class="ltx_td ltx_align_center">mAP<sub id="S4.T2.2.1.1.1.1" class="ltx_sub"><span id="S4.T2.2.1.1.1.1.1" class="ltx_text ltx_font_italic">50</span></sub>
</td>
<td id="S4.T2.3.2.2.3" class="ltx_td ltx_align_center">Actinopterygii</td>
<td id="S4.T2.3.2.2.4" class="ltx_td ltx_align_center">Chondrichthyes</td>
<td id="S4.T2.3.2.2.5" class="ltx_td ltx_align_center">Gastropoda</td>
<td id="S4.T2.3.2.2.6" class="ltx_td ltx_align_center">Malacostraca</td>
<td id="S4.T2.3.2.2.7" class="ltx_td ltx_align_center ltx_border_r">Reptilia</td>
<td id="S4.T2.3.2.2.2" class="ltx_td ltx_align_center">mAP<sub id="S4.T2.3.2.2.2.1" class="ltx_sub"><span id="S4.T2.3.2.2.2.1.1" class="ltx_text ltx_font_italic">50</span></sub>
</td>
<td id="S4.T2.3.2.2.8" class="ltx_td ltx_align_center">Cephalopoda</td>
<td id="S4.T2.3.2.2.9" class="ltx_td ltx_align_center">Holothuroidea</td>
<td id="S4.T2.3.2.2.10" class="ltx_td ltx_align_center">Asteroidea</td>
<td id="S4.T2.3.2.2.11" class="ltx_td ltx_align_center">Demospongiae</td>
<td id="S4.T2.3.2.2.12" class="ltx_td ltx_align_center">Mammalia</td>
<td id="S4.T2.3.2.2.13" class="ltx_td ltx_align_center">Scyphozoa</td>
</tr>
<tr id="S4.T2.11.10.10" class="ltx_tr">
<td id="S4.T2.11.10.10.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">FasterRCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>
</td>
<td id="S4.T2.4.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="3"><span id="S4.T2.4.3.3.1.1" class="ltx_text"><math id="S4.T2.4.3.3.1.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S4.T2.4.3.3.1.1.m1.1a"><mi mathvariant="normal" id="S4.T2.4.3.3.1.1.m1.1.1" xref="S4.T2.4.3.3.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.4.3.3.1.1.m1.1b"><ci id="S4.T2.4.3.3.1.1.m1.1.1.cmml" xref="S4.T2.4.3.3.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.3.3.1.1.m1.1c">\checkmark</annotation></semantics></math></span></td>
<td id="S4.T2.11.10.10.10" class="ltx_td ltx_align_center ltx_border_t">41.5</td>
<td id="S4.T2.11.10.10.11" class="ltx_td ltx_align_center ltx_border_t">58.3</td>
<td id="S4.T2.11.10.10.12" class="ltx_td ltx_align_center ltx_border_t">47.3</td>
<td id="S4.T2.11.10.10.13" class="ltx_td ltx_align_center ltx_border_t">49.8</td>
<td id="S4.T2.11.10.10.14" class="ltx_td ltx_align_center ltx_border_t">50.0</td>
<td id="S4.T2.11.10.10.15" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">64.2</td>
<td id="S4.T2.5.4.4.2" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T2.5.4.4.2.m1.1" class="ltx_Math" alttext="-" display="inline"><semantics id="S4.T2.5.4.4.2.m1.1a"><mo id="S4.T2.5.4.4.2.m1.1.1" xref="S4.T2.5.4.4.2.m1.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S4.T2.5.4.4.2.m1.1b"><minus id="S4.T2.5.4.4.2.m1.1.1.cmml" xref="S4.T2.5.4.4.2.m1.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.4.4.2.m1.1c">-</annotation></semantics></math></td>
<td id="S4.T2.6.5.5.3" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T2.6.5.5.3.m1.1" class="ltx_Math" alttext="-" display="inline"><semantics id="S4.T2.6.5.5.3.m1.1a"><mo id="S4.T2.6.5.5.3.m1.1.1" xref="S4.T2.6.5.5.3.m1.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S4.T2.6.5.5.3.m1.1b"><minus id="S4.T2.6.5.5.3.m1.1.1.cmml" xref="S4.T2.6.5.5.3.m1.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.5.5.3.m1.1c">-</annotation></semantics></math></td>
<td id="S4.T2.7.6.6.4" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T2.7.6.6.4.m1.1" class="ltx_Math" alttext="-" display="inline"><semantics id="S4.T2.7.6.6.4.m1.1a"><mo id="S4.T2.7.6.6.4.m1.1.1" xref="S4.T2.7.6.6.4.m1.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S4.T2.7.6.6.4.m1.1b"><minus id="S4.T2.7.6.6.4.m1.1.1.cmml" xref="S4.T2.7.6.6.4.m1.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.6.6.4.m1.1c">-</annotation></semantics></math></td>
<td id="S4.T2.8.7.7.5" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T2.8.7.7.5.m1.1" class="ltx_Math" alttext="-" display="inline"><semantics id="S4.T2.8.7.7.5.m1.1a"><mo id="S4.T2.8.7.7.5.m1.1.1" xref="S4.T2.8.7.7.5.m1.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S4.T2.8.7.7.5.m1.1b"><minus id="S4.T2.8.7.7.5.m1.1.1.cmml" xref="S4.T2.8.7.7.5.m1.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.8.7.7.5.m1.1c">-</annotation></semantics></math></td>
<td id="S4.T2.9.8.8.6" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T2.9.8.8.6.m1.1" class="ltx_Math" alttext="-" display="inline"><semantics id="S4.T2.9.8.8.6.m1.1a"><mo id="S4.T2.9.8.8.6.m1.1.1" xref="S4.T2.9.8.8.6.m1.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S4.T2.9.8.8.6.m1.1b"><minus id="S4.T2.9.8.8.6.m1.1.1.cmml" xref="S4.T2.9.8.8.6.m1.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.9.8.8.6.m1.1c">-</annotation></semantics></math></td>
<td id="S4.T2.10.9.9.7" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T2.10.9.9.7.m1.1" class="ltx_Math" alttext="-" display="inline"><semantics id="S4.T2.10.9.9.7.m1.1a"><mo id="S4.T2.10.9.9.7.m1.1.1" xref="S4.T2.10.9.9.7.m1.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S4.T2.10.9.9.7.m1.1b"><minus id="S4.T2.10.9.9.7.m1.1.1.cmml" xref="S4.T2.10.9.9.7.m1.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.10.9.9.7.m1.1c">-</annotation></semantics></math></td>
<td id="S4.T2.11.10.10.8" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T2.11.10.10.8.m1.1" class="ltx_Math" alttext="-" display="inline"><semantics id="S4.T2.11.10.10.8.m1.1a"><mo id="S4.T2.11.10.10.8.m1.1.1" xref="S4.T2.11.10.10.8.m1.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S4.T2.11.10.10.8.m1.1b"><minus id="S4.T2.11.10.10.8.m1.1.1.cmml" xref="S4.T2.11.10.10.8.m1.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.11.10.10.8.m1.1c">-</annotation></semantics></math></td>
</tr>
<tr id="S4.T2.18.17.17" class="ltx_tr">
<td id="S4.T2.18.17.17.8" class="ltx_td ltx_align_center ltx_border_r">YOLOX <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>
</td>
<td id="S4.T2.18.17.17.9" class="ltx_td ltx_align_center">46.4</td>
<td id="S4.T2.18.17.17.10" class="ltx_td ltx_align_center">62.5</td>
<td id="S4.T2.18.17.17.11" class="ltx_td ltx_align_center">55.5</td>
<td id="S4.T2.18.17.17.12" class="ltx_td ltx_align_center">53.8</td>
<td id="S4.T2.18.17.17.13" class="ltx_td ltx_align_center">49.1</td>
<td id="S4.T2.18.17.17.14" class="ltx_td ltx_align_center ltx_border_r">66.4</td>
<td id="S4.T2.12.11.11.1" class="ltx_td ltx_align_center"><math id="S4.T2.12.11.11.1.m1.1" class="ltx_Math" alttext="-" display="inline"><semantics id="S4.T2.12.11.11.1.m1.1a"><mo id="S4.T2.12.11.11.1.m1.1.1" xref="S4.T2.12.11.11.1.m1.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S4.T2.12.11.11.1.m1.1b"><minus id="S4.T2.12.11.11.1.m1.1.1.cmml" xref="S4.T2.12.11.11.1.m1.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.12.11.11.1.m1.1c">-</annotation></semantics></math></td>
<td id="S4.T2.13.12.12.2" class="ltx_td ltx_align_center"><math id="S4.T2.13.12.12.2.m1.1" class="ltx_Math" alttext="-" display="inline"><semantics id="S4.T2.13.12.12.2.m1.1a"><mo id="S4.T2.13.12.12.2.m1.1.1" xref="S4.T2.13.12.12.2.m1.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S4.T2.13.12.12.2.m1.1b"><minus id="S4.T2.13.12.12.2.m1.1.1.cmml" xref="S4.T2.13.12.12.2.m1.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.13.12.12.2.m1.1c">-</annotation></semantics></math></td>
<td id="S4.T2.14.13.13.3" class="ltx_td ltx_align_center"><math id="S4.T2.14.13.13.3.m1.1" class="ltx_Math" alttext="-" display="inline"><semantics id="S4.T2.14.13.13.3.m1.1a"><mo id="S4.T2.14.13.13.3.m1.1.1" xref="S4.T2.14.13.13.3.m1.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S4.T2.14.13.13.3.m1.1b"><minus id="S4.T2.14.13.13.3.m1.1.1.cmml" xref="S4.T2.14.13.13.3.m1.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.14.13.13.3.m1.1c">-</annotation></semantics></math></td>
<td id="S4.T2.15.14.14.4" class="ltx_td ltx_align_center"><math id="S4.T2.15.14.14.4.m1.1" class="ltx_Math" alttext="-" display="inline"><semantics id="S4.T2.15.14.14.4.m1.1a"><mo id="S4.T2.15.14.14.4.m1.1.1" xref="S4.T2.15.14.14.4.m1.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S4.T2.15.14.14.4.m1.1b"><minus id="S4.T2.15.14.14.4.m1.1.1.cmml" xref="S4.T2.15.14.14.4.m1.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.15.14.14.4.m1.1c">-</annotation></semantics></math></td>
<td id="S4.T2.16.15.15.5" class="ltx_td ltx_align_center"><math id="S4.T2.16.15.15.5.m1.1" class="ltx_Math" alttext="-" display="inline"><semantics id="S4.T2.16.15.15.5.m1.1a"><mo id="S4.T2.16.15.15.5.m1.1.1" xref="S4.T2.16.15.15.5.m1.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S4.T2.16.15.15.5.m1.1b"><minus id="S4.T2.16.15.15.5.m1.1.1.cmml" xref="S4.T2.16.15.15.5.m1.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.16.15.15.5.m1.1c">-</annotation></semantics></math></td>
<td id="S4.T2.17.16.16.6" class="ltx_td ltx_align_center"><math id="S4.T2.17.16.16.6.m1.1" class="ltx_Math" alttext="-" display="inline"><semantics id="S4.T2.17.16.16.6.m1.1a"><mo id="S4.T2.17.16.16.6.m1.1.1" xref="S4.T2.17.16.16.6.m1.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S4.T2.17.16.16.6.m1.1b"><minus id="S4.T2.17.16.16.6.m1.1.1.cmml" xref="S4.T2.17.16.16.6.m1.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.17.16.16.6.m1.1c">-</annotation></semantics></math></td>
<td id="S4.T2.18.17.17.7" class="ltx_td ltx_align_center"><math id="S4.T2.18.17.17.7.m1.1" class="ltx_Math" alttext="-" display="inline"><semantics id="S4.T2.18.17.17.7.m1.1a"><mo id="S4.T2.18.17.17.7.m1.1.1" xref="S4.T2.18.17.17.7.m1.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S4.T2.18.17.17.7.m1.1b"><minus id="S4.T2.18.17.17.7.m1.1.1.cmml" xref="S4.T2.18.17.17.7.m1.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.18.17.17.7.m1.1c">-</annotation></semantics></math></td>
</tr>
<tr id="S4.T2.25.24.24" class="ltx_tr">
<td id="S4.T2.25.24.24.8" class="ltx_td ltx_align_center ltx_border_r">GridRCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>
</td>
<td id="S4.T2.25.24.24.9" class="ltx_td ltx_align_center">39.1</td>
<td id="S4.T2.25.24.24.10" class="ltx_td ltx_align_center">63.2</td>
<td id="S4.T2.25.24.24.11" class="ltx_td ltx_align_center">50.3</td>
<td id="S4.T2.25.24.24.12" class="ltx_td ltx_align_center">47.7</td>
<td id="S4.T2.25.24.24.13" class="ltx_td ltx_align_center">51.3</td>
<td id="S4.T2.25.24.24.14" class="ltx_td ltx_align_center ltx_border_r">67.0</td>
<td id="S4.T2.19.18.18.1" class="ltx_td ltx_align_center"><math id="S4.T2.19.18.18.1.m1.1" class="ltx_Math" alttext="-" display="inline"><semantics id="S4.T2.19.18.18.1.m1.1a"><mo id="S4.T2.19.18.18.1.m1.1.1" xref="S4.T2.19.18.18.1.m1.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S4.T2.19.18.18.1.m1.1b"><minus id="S4.T2.19.18.18.1.m1.1.1.cmml" xref="S4.T2.19.18.18.1.m1.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.19.18.18.1.m1.1c">-</annotation></semantics></math></td>
<td id="S4.T2.20.19.19.2" class="ltx_td ltx_align_center"><math id="S4.T2.20.19.19.2.m1.1" class="ltx_Math" alttext="-" display="inline"><semantics id="S4.T2.20.19.19.2.m1.1a"><mo id="S4.T2.20.19.19.2.m1.1.1" xref="S4.T2.20.19.19.2.m1.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S4.T2.20.19.19.2.m1.1b"><minus id="S4.T2.20.19.19.2.m1.1.1.cmml" xref="S4.T2.20.19.19.2.m1.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.20.19.19.2.m1.1c">-</annotation></semantics></math></td>
<td id="S4.T2.21.20.20.3" class="ltx_td ltx_align_center"><math id="S4.T2.21.20.20.3.m1.1" class="ltx_Math" alttext="-" display="inline"><semantics id="S4.T2.21.20.20.3.m1.1a"><mo id="S4.T2.21.20.20.3.m1.1.1" xref="S4.T2.21.20.20.3.m1.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S4.T2.21.20.20.3.m1.1b"><minus id="S4.T2.21.20.20.3.m1.1.1.cmml" xref="S4.T2.21.20.20.3.m1.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.21.20.20.3.m1.1c">-</annotation></semantics></math></td>
<td id="S4.T2.22.21.21.4" class="ltx_td ltx_align_center"><math id="S4.T2.22.21.21.4.m1.1" class="ltx_Math" alttext="-" display="inline"><semantics id="S4.T2.22.21.21.4.m1.1a"><mo id="S4.T2.22.21.21.4.m1.1.1" xref="S4.T2.22.21.21.4.m1.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S4.T2.22.21.21.4.m1.1b"><minus id="S4.T2.22.21.21.4.m1.1.1.cmml" xref="S4.T2.22.21.21.4.m1.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.22.21.21.4.m1.1c">-</annotation></semantics></math></td>
<td id="S4.T2.23.22.22.5" class="ltx_td ltx_align_center"><math id="S4.T2.23.22.22.5.m1.1" class="ltx_Math" alttext="-" display="inline"><semantics id="S4.T2.23.22.22.5.m1.1a"><mo id="S4.T2.23.22.22.5.m1.1.1" xref="S4.T2.23.22.22.5.m1.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S4.T2.23.22.22.5.m1.1b"><minus id="S4.T2.23.22.22.5.m1.1.1.cmml" xref="S4.T2.23.22.22.5.m1.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.23.22.22.5.m1.1c">-</annotation></semantics></math></td>
<td id="S4.T2.24.23.23.6" class="ltx_td ltx_align_center"><math id="S4.T2.24.23.23.6.m1.1" class="ltx_Math" alttext="-" display="inline"><semantics id="S4.T2.24.23.23.6.m1.1a"><mo id="S4.T2.24.23.23.6.m1.1.1" xref="S4.T2.24.23.23.6.m1.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S4.T2.24.23.23.6.m1.1b"><minus id="S4.T2.24.23.23.6.m1.1.1.cmml" xref="S4.T2.24.23.23.6.m1.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.24.23.23.6.m1.1c">-</annotation></semantics></math></td>
<td id="S4.T2.25.24.24.7" class="ltx_td ltx_align_center"><math id="S4.T2.25.24.24.7.m1.1" class="ltx_Math" alttext="-" display="inline"><semantics id="S4.T2.25.24.24.7.m1.1a"><mo id="S4.T2.25.24.24.7.m1.1.1" xref="S4.T2.25.24.24.7.m1.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S4.T2.25.24.24.7.m1.1b"><minus id="S4.T2.25.24.24.7.m1.1.1.cmml" xref="S4.T2.25.24.24.7.m1.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.25.24.24.7.m1.1c">-</annotation></semantics></math></td>
</tr>
<tr id="S4.T2.26.25.25" class="ltx_tr">
<td id="S4.T2.26.25.25.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">UniDetector <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>
</td>
<td id="S4.T2.26.25.25.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T2.26.25.25.1.1" class="ltx_text"><math id="S4.T2.26.25.25.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T2.26.25.25.1.1.m1.1a"><mo id="S4.T2.26.25.25.1.1.m1.1.1" xref="S4.T2.26.25.25.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.26.25.25.1.1.m1.1b"><times id="S4.T2.26.25.25.1.1.m1.1.1.cmml" xref="S4.T2.26.25.25.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.26.25.25.1.1.m1.1c">\times</annotation></semantics></math></span></td>
<td id="S4.T2.26.25.25.3" class="ltx_td ltx_align_center ltx_border_t">0.9</td>
<td id="S4.T2.26.25.25.4" class="ltx_td ltx_align_center ltx_border_t">5.6</td>
<td id="S4.T2.26.25.25.5" class="ltx_td ltx_align_center ltx_border_t">0.5</td>
<td id="S4.T2.26.25.25.6" class="ltx_td ltx_align_center ltx_border_t">0</td>
<td id="S4.T2.26.25.25.7" class="ltx_td ltx_align_center ltx_border_t">7.9</td>
<td id="S4.T2.26.25.25.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.1</td>
<td id="S4.T2.26.25.25.9" class="ltx_td ltx_align_center ltx_border_t">1.9</td>
<td id="S4.T2.26.25.25.10" class="ltx_td ltx_align_center ltx_border_t">7.9</td>
<td id="S4.T2.26.25.25.11" class="ltx_td ltx_align_center ltx_border_t">0</td>
<td id="S4.T2.26.25.25.12" class="ltx_td ltx_align_center ltx_border_t">0.1</td>
<td id="S4.T2.26.25.25.13" class="ltx_td ltx_align_center ltx_border_t">0</td>
<td id="S4.T2.26.25.25.14" class="ltx_td ltx_align_center ltx_border_t">3.2</td>
<td id="S4.T2.26.25.25.15" class="ltx_td ltx_align_center ltx_border_t">0.1</td>
</tr>
<tr id="S4.T2.27.26.28.2" class="ltx_tr">
<td id="S4.T2.27.26.28.2.1" class="ltx_td ltx_align_center ltx_border_r">GroundingDINO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>
</td>
<td id="S4.T2.27.26.28.2.2" class="ltx_td ltx_align_center">0.9</td>
<td id="S4.T2.27.26.28.2.3" class="ltx_td ltx_align_center">8.2</td>
<td id="S4.T2.27.26.28.2.4" class="ltx_td ltx_align_center">2.0</td>
<td id="S4.T2.27.26.28.2.5" class="ltx_td ltx_align_center">1.9</td>
<td id="S4.T2.27.26.28.2.6" class="ltx_td ltx_align_center">2.9</td>
<td id="S4.T2.27.26.28.2.7" class="ltx_td ltx_align_center ltx_border_r">0.4</td>
<td id="S4.T2.27.26.28.2.8" class="ltx_td ltx_align_center">9.4</td>
<td id="S4.T2.27.26.28.2.9" class="ltx_td ltx_align_center">27.6</td>
<td id="S4.T2.27.26.28.2.10" class="ltx_td ltx_align_center">3.1</td>
<td id="S4.T2.27.26.28.2.11" class="ltx_td ltx_align_center">1.7</td>
<td id="S4.T2.27.26.28.2.12" class="ltx_td ltx_align_center">1.1</td>
<td id="S4.T2.27.26.28.2.13" class="ltx_td ltx_align_center">19.5</td>
<td id="S4.T2.27.26.28.2.14" class="ltx_td ltx_align_center">3.6</td>
</tr>
<tr id="S4.T2.27.26.26" class="ltx_tr">
<td id="S4.T2.27.26.26.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">UniDetector <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>
</td>
<td id="S4.T2.27.26.26.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" rowspan="3"><span id="S4.T2.27.26.26.1.1" class="ltx_text"><math id="S4.T2.27.26.26.1.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S4.T2.27.26.26.1.1.m1.1a"><mi mathvariant="normal" id="S4.T2.27.26.26.1.1.m1.1.1" xref="S4.T2.27.26.26.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.27.26.26.1.1.m1.1b"><ci id="S4.T2.27.26.26.1.1.m1.1.1.cmml" xref="S4.T2.27.26.26.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.27.26.26.1.1.m1.1c">\checkmark</annotation></semantics></math></span></td>
<td id="S4.T2.27.26.26.3" class="ltx_td ltx_align_center ltx_border_t">40.9</td>
<td id="S4.T2.27.26.26.4" class="ltx_td ltx_align_center ltx_border_t">72.0</td>
<td id="S4.T2.27.26.26.5" class="ltx_td ltx_align_center ltx_border_t">59.1</td>
<td id="S4.T2.27.26.26.6" class="ltx_td ltx_align_center ltx_border_t">51.3</td>
<td id="S4.T2.27.26.26.7" class="ltx_td ltx_align_center ltx_border_t">67.8</td>
<td id="S4.T2.27.26.26.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">87.8</td>
<td id="S4.T2.27.26.26.9" class="ltx_td ltx_align_center ltx_border_t">11.6</td>
<td id="S4.T2.27.26.26.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.27.26.26.10.1" class="ltx_text ltx_font_bold">37.4</span></td>
<td id="S4.T2.27.26.26.11" class="ltx_td ltx_align_center ltx_border_t">4.6</td>
<td id="S4.T2.27.26.26.12" class="ltx_td ltx_align_center ltx_border_t">3.9</td>
<td id="S4.T2.27.26.26.13" class="ltx_td ltx_align_center ltx_border_t">0.9</td>
<td id="S4.T2.27.26.26.14" class="ltx_td ltx_align_center ltx_border_t">18.0</td>
<td id="S4.T2.27.26.26.15" class="ltx_td ltx_align_center ltx_border_t">4.6</td>
</tr>
<tr id="S4.T2.27.26.29.3" class="ltx_tr">
<td id="S4.T2.27.26.29.3.1" class="ltx_td ltx_align_center ltx_border_r">OVR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>
</td>
<td id="S4.T2.27.26.29.3.2" class="ltx_td ltx_align_center">42.7</td>
<td id="S4.T2.27.26.29.3.3" class="ltx_td ltx_align_center">76.4</td>
<td id="S4.T2.27.26.29.3.4" class="ltx_td ltx_align_center"><span id="S4.T2.27.26.29.3.4.1" class="ltx_text ltx_font_bold">62.5</span></td>
<td id="S4.T2.27.26.29.3.5" class="ltx_td ltx_align_center">55.6</td>
<td id="S4.T2.27.26.29.3.6" class="ltx_td ltx_align_center"><span id="S4.T2.27.26.29.3.6.1" class="ltx_text ltx_font_bold">73.9</span></td>
<td id="S4.T2.27.26.29.3.7" class="ltx_td ltx_align_center ltx_border_r">90.1</td>
<td id="S4.T2.27.26.29.3.8" class="ltx_td ltx_align_center">14.5</td>
<td id="S4.T2.27.26.29.3.9" class="ltx_td ltx_align_center">21.0</td>
<td id="S4.T2.27.26.29.3.10" class="ltx_td ltx_align_center">3.3</td>
<td id="S4.T2.27.26.29.3.11" class="ltx_td ltx_align_center"><span id="S4.T2.27.26.29.3.11.1" class="ltx_text ltx_font_bold">14.7</span></td>
<td id="S4.T2.27.26.29.3.12" class="ltx_td ltx_align_center">0.3</td>
<td id="S4.T2.27.26.29.3.13" class="ltx_td ltx_align_center">13.5</td>
<td id="S4.T2.27.26.29.3.14" class="ltx_td ltx_align_center">34.4</td>
</tr>
<tr id="S4.T2.27.26.30.4" class="ltx_tr" style="background-color:#D6D6D6;">
<td id="S4.T2.27.26.30.4.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T2.27.26.30.4.1.1" class="ltx_text" style="background-color:#D6D6D6;">Ours</span></td>
<td id="S4.T2.27.26.30.4.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.27.26.30.4.2.1" class="ltx_text ltx_font_bold" style="background-color:#D6D6D6;">48.3</span></td>
<td id="S4.T2.27.26.30.4.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.27.26.30.4.3.1" class="ltx_text ltx_font_bold" style="background-color:#D6D6D6;">80.2</span></td>
<td id="S4.T2.27.26.30.4.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.27.26.30.4.4.1" class="ltx_text" style="background-color:#D6D6D6;">61.4</span></td>
<td id="S4.T2.27.26.30.4.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.27.26.30.4.5.1" class="ltx_text ltx_font_bold" style="background-color:#D6D6D6;">59.2</span></td>
<td id="S4.T2.27.26.30.4.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.27.26.30.4.6.1" class="ltx_text" style="background-color:#D6D6D6;">73.8</span></td>
<td id="S4.T2.27.26.30.4.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T2.27.26.30.4.7.1" class="ltx_text ltx_font_bold" style="background-color:#D6D6D6;">90.1</span></td>
<td id="S4.T2.27.26.30.4.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.27.26.30.4.8.1" class="ltx_text ltx_font_bold" style="background-color:#D6D6D6;">17.7</span></td>
<td id="S4.T2.27.26.30.4.9" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.27.26.30.4.9.1" class="ltx_text" style="background-color:#D6D6D6;">15.8</span></td>
<td id="S4.T2.27.26.30.4.10" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.27.26.30.4.10.1" class="ltx_text ltx_font_bold" style="background-color:#D6D6D6;">4.8</span></td>
<td id="S4.T2.27.26.30.4.11" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.27.26.30.4.11.1" class="ltx_text" style="background-color:#D6D6D6;">3.8</span></td>
<td id="S4.T2.27.26.30.4.12" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.27.26.30.4.12.1" class="ltx_text ltx_font_bold" style="background-color:#D6D6D6;">2.0</span></td>
<td id="S4.T2.27.26.30.4.13" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.27.26.30.4.13.1" class="ltx_text ltx_font_bold" style="background-color:#D6D6D6;">26.4</span></td>
<td id="S4.T2.27.26.30.4.14" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.27.26.30.4.14.1" class="ltx_text ltx_font_bold" style="background-color:#D6D6D6;">53.5</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.2" class="ltx_p">We mainly include 3 close-set object detection algorithms (Faster-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, GridRCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> and YOLOX <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>) and 3 open-vocabulary/open-set object detection algorithms (OVR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, GroundingDINO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> and UniDetector <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>) for comparison. We perform experiments under “<span id="S4.SS3.p1.2.1" class="ltx_text ltx_font_italic">Class</span>”-level setting, where 20 Classes are regarded as <span id="S4.SS3.p1.2.2" class="ltx_text ltx_font_italic">seen</span> “Classes” and the other 6 Classes as <span id="S4.SS3.p1.2.3" class="ltx_text ltx_font_italic">unseen</span> “Classes”. For close-set object detection algorithms, we only report mAP<sub id="S4.SS3.p1.2.4" class="ltx_sub"><span id="S4.SS3.p1.2.4.1" class="ltx_text ltx_font_italic">50</span></sub> of 20 seen “Classes” and cannot compute the results of other 6 <span id="S4.SS3.p1.2.5" class="ltx_text ltx_font_italic">unseen</span> “Classes”. For the three open-vocabulary/open-set object detection algorithms, we compare them under two settings: without and with continuous training on our MarineDet dataset. Under the first setting, considering these object detection generalist models could detect a large range of object conceptions since these models are optimized by large-scale datasets (COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, LVIS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> and ODinW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>), we directly utilize their released pre-trained models for inference except OVR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> since the RPN model is not available. Under the second setting, we continuously optimize their pre-trained models on our MarineDet dataset. We do not optimize the GroundingDINO model since the training codes are not available. All the experiments are conducted following the same train/val data split and we report the quantitative results in Table <a href="#S4.T2" title="TABLE II ‣ IV-C Comparisons with SOTAs ‣ IV Experiments ‣ MarineDet: Towards Open-Marine Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>. Due to limited space, we only illustrate the AP<sub id="S4.SS3.p1.2.6" class="ltx_sub"><span id="S4.SS3.p1.2.6.1" class="ltx_text ltx_font_italic">50</span></sub> of 5 <span id="S4.SS3.p1.2.7" class="ltx_text ltx_font_italic">seen</span> “Classes” and provide detailed results in the appendix.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.2" class="ltx_p">We have noticed that the existing generalist object detection algorithms without continuous training only show very limited detection performance as reported in Table <a href="#S4.T2" title="TABLE II ‣ IV-C Comparisons with SOTAs ‣ IV Experiments ‣ MarineDet: Towards Open-Marine Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>. The possible reasons could come from 1) the huge conception distance between the in-air object categories and marine object categories; and 2) the appearance shift from the in-air images to underwater images. The pre-trained models only show a limited ability to extract foreground objects. Furthermore, as demonstrated, open-vocabulary/open-set object detection algorithms, when continuously trained on the MarineDet dataset, typically exhibit improved detection performance even on <span id="S4.SS3.p2.2.1" class="ltx_text ltx_font_italic">seen</span> categories compared to close-set object detection algorithms. We attribute such promoted performance to the optimization through large-scale datasets with redundant supervision during the pre-training phase. Our method could achieve the best object detection performance for both <span id="S4.SS3.p2.2.2" class="ltx_text ltx_font_italic">unseen</span> and <span id="S4.SS3.p2.2.3" class="ltx_text ltx_font_italic">seen</span> categories. We provide the qualitative result comparison of OVR, UniDetector, and MarineDet in Fig. <a href="#S4.F4" title="Figure 4 ‣ IV-B Implementation Details and Experimental Setups ‣ IV Experiments ‣ MarineDet: Towards Open-Marine Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. UniDetector misrecognized many objects to “Cephalopoda” and achieved 37.4 AP<sub id="S4.SS3.p2.2.4" class="ltx_sub"><span id="S4.SS3.p2.2.4.1" class="ltx_text ltx_font_italic">50</span></sub> on “Cephalopoda” with only low results on other unseen “Classes”. OVR achieved the highest result on unseen “Asteroidea”. Our method achieves the highest mAP<sub id="S4.SS3.p2.2.5" class="ltx_sub"><span id="S4.SS3.p2.2.5.1" class="ltx_text ltx_font_italic">50</span></sub> on 6 unseen “Classes” among all the methods.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE III: </span></figcaption>
<figcaption class="ltx_caption">We conduct open-vocabulary marine object detection experiments in a fine-grained manner.</figcaption>
<div id="S4.T3.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:384.3pt;height:61.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-33.9pt,5.4pt) scale(0.85,0.85) ;">
<table id="S4.T3.2.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.2.2.2" class="ltx_tr">
<th id="S4.T3.2.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">Method</th>
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">mAP<sub id="S4.T3.1.1.1.1.1" class="ltx_sub"><span id="S4.T3.1.1.1.1.1.1" class="ltx_text ltx_font_italic">50</span></sub> (613 <span id="S4.T3.1.1.1.1.2" class="ltx_text ltx_font_italic">seen</span> categories)</th>
<th id="S4.T3.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">mAP<sub id="S4.T3.2.2.2.2.1" class="ltx_sub"><span id="S4.T3.2.2.2.2.1.1" class="ltx_text ltx_font_italic">50</span></sub> (177 <span id="S4.T3.2.2.2.2.2" class="ltx_text ltx_font_italic">unseen</span> categories)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.2.2.3.1" class="ltx_tr">
<th id="S4.T3.2.2.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">UniDetector <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>
</th>
<td id="S4.T3.2.2.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">21.1</td>
<td id="S4.T3.2.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t">0.5</td>
</tr>
<tr id="S4.T3.2.2.4.2" class="ltx_tr">
<th id="S4.T3.2.2.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">OVR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>
</th>
<td id="S4.T3.2.2.4.2.2" class="ltx_td ltx_align_center ltx_border_r">23.6</td>
<td id="S4.T3.2.2.4.2.3" class="ltx_td ltx_align_center">0.9</td>
</tr>
<tr id="S4.T3.2.2.5.3" class="ltx_tr" style="background-color:#D6D6D6;">
<th id="S4.T3.2.2.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="S4.T3.2.2.5.3.1.1" class="ltx_text" style="background-color:#D6D6D6;">Ours</span></th>
<td id="S4.T3.2.2.5.3.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T3.2.2.5.3.2.1" class="ltx_text ltx_font_bold" style="background-color:#D6D6D6;">37.7</span></td>
<td id="S4.T3.2.2.5.3.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.2.2.5.3.3.1" class="ltx_text ltx_font_bold" style="background-color:#D6D6D6;">7.3</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">We then conduct experiments in a fine-grained manner. All the defined object categories by ChatGPT-3.5/GPT-4: (613 categories) from the 20 <span id="S4.SS3.p3.1.1" class="ltx_text ltx_font_italic">seen</span> “Classes” are regarded as <span id="S4.SS3.p3.1.2" class="ltx_text ltx_font_italic">seen</span> categories and the other 177 object categories from the 6 <span id="S4.SS3.p3.1.3" class="ltx_text ltx_font_italic">unseen</span> “Classes” as <span id="S4.SS3.p3.1.4" class="ltx_text ltx_font_italic">unseen</span> categories. Under this experimental setup, the model is required to perform fine-grained object recognition for the object categories with similar morphologies and appearances. We report the quantitative results of different methods in Table <a href="#S4.T3" title="TABLE III ‣ IV-C Comparisons with SOTAs ‣ IV Experiments ‣ MarineDet: Towards Open-Marine Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>. Please note all the methods have been trained on our MarineDet dataset. Compared with OVR and UniDetector, our method demonstrates a more robust and stronger ability to detect unseen object categories even under the fine-grained setting. It still remains a challenging task to perform accurate and robust OMOD under a fine-grained setting.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p">We have also performed experiments to demonstrate the ability of our MarineDet could effectively detect marine objects even compared with those specialist models that are designed for detecting pre-defined object categories. We compare Faster-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, YOLOX <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> and GridRCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> on URPC dataset. Following the default train/val split of <span id="footnotex1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://challenge.datacastle.cn/v3/cmptDetail.html?id=680" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:70%;">https://challenge.datacastle.cn/v3/cmptDetail.html?id=680</a></span></span></span>, we train Faster-RCNN, YOLOX, and GridRCNN and report the corresponding experimental results in Table <a href="#S4.T4" title="TABLE IV ‣ IV-C Comparisons with SOTAs ‣ IV Experiments ‣ MarineDet: Towards Open-Marine Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>. For our MarineDet, we first adopt our trained model under “<span id="S4.SS3.p4.1.1" class="ltx_text ltx_font_italic">Class</span>”-level setting for inference. Then we fine-tune the model under the open-vocabulary and fully-supervised settings. As reported in Table <a href="#S4.T4" title="TABLE IV ‣ IV-C Comparisons with SOTAs ‣ IV Experiments ‣ MarineDet: Towards Open-Marine Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>, the proposed MarineDet could achieve satisfactory detection performance for “Sea urchin” even without any fine-tuning on domain-specific data. After the fine-tuning, we achieved a large performance improvement compared with Faster-RCNN, YOLOX, and GridRCNN.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE IV: </span></figcaption>
<figcaption class="ltx_caption">We report the quantitative object detection results with specialist object detection algorithms. <math id="S4.T4.1.m1.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="S4.T4.1.m1.1a"><mo id="S4.T4.1.m1.1.1" xref="S4.T4.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S4.T4.1.m1.1b"><ci id="S4.T4.1.m1.1.1.cmml" xref="S4.T4.1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.m1.1c">\dagger</annotation></semantics></math> indicates it belongs to <span id="S4.T4.9.1" class="ltx_text ltx_font_italic">seen</span> category under our “<span id="S4.T4.10.2" class="ltx_text ltx_font_italic">Class</span>”-level setting.</figcaption>
<div id="S4.T4.8" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:318.1pt;height:90.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-61.8pt,17.6pt) scale(0.72,0.72) ;">
<table id="S4.T4.8.7" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.4.3.3" class="ltx_tr">
<th id="S4.T4.4.3.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">Method</th>
<th id="S4.T4.4.3.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">URPC</th>
<th id="S4.T4.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Sea urchin<sup id="S4.T4.2.1.1.1.1" class="ltx_sup"><span id="S4.T4.2.1.1.1.1.1" class="ltx_text ltx_font_italic">†</span></sup>
</th>
<th id="S4.T4.3.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Scollop<sup id="S4.T4.3.2.2.2.1" class="ltx_sup"><span id="S4.T4.3.2.2.2.1.1" class="ltx_text ltx_font_italic">†</span></sup>
</th>
<th id="S4.T4.4.3.3.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Starfish</th>
<th id="S4.T4.4.3.3.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Sea cucumber</th>
<th id="S4.T4.4.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">mAP<sub id="S4.T4.4.3.3.3.1" class="ltx_sub"><span id="S4.T4.4.3.3.3.1.1" class="ltx_text ltx_font_italic">50</span></sub>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.5.4.4" class="ltx_tr">
<th id="S4.T4.5.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">FasterRCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>
</th>
<th id="S4.T4.5.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="3"><span id="S4.T4.5.4.4.1.1" class="ltx_text"><math id="S4.T4.5.4.4.1.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S4.T4.5.4.4.1.1.m1.1a"><mi mathvariant="normal" id="S4.T4.5.4.4.1.1.m1.1.1" xref="S4.T4.5.4.4.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T4.5.4.4.1.1.m1.1b"><ci id="S4.T4.5.4.4.1.1.m1.1.1.cmml" xref="S4.T4.5.4.4.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.5.4.4.1.1.m1.1c">\checkmark</annotation></semantics></math></span></th>
<td id="S4.T4.5.4.4.3" class="ltx_td ltx_align_center ltx_border_t">48.0</td>
<td id="S4.T4.5.4.4.4" class="ltx_td ltx_align_center ltx_border_t">48.3</td>
<td id="S4.T4.5.4.4.5" class="ltx_td ltx_align_center ltx_border_t">52.4</td>
<td id="S4.T4.5.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">41.5</td>
<td id="S4.T4.5.4.4.7" class="ltx_td ltx_align_center ltx_border_t">47.5</td>
</tr>
<tr id="S4.T4.8.7.8.1" class="ltx_tr">
<th id="S4.T4.8.7.8.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">YOLOX <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>
</th>
<td id="S4.T4.8.7.8.1.2" class="ltx_td ltx_align_center">51.5</td>
<td id="S4.T4.8.7.8.1.3" class="ltx_td ltx_align_center">52.6</td>
<td id="S4.T4.8.7.8.1.4" class="ltx_td ltx_align_center">54.7</td>
<td id="S4.T4.8.7.8.1.5" class="ltx_td ltx_align_center ltx_border_r">45.8</td>
<td id="S4.T4.8.7.8.1.6" class="ltx_td ltx_align_center">51.1</td>
</tr>
<tr id="S4.T4.8.7.9.2" class="ltx_tr">
<th id="S4.T4.8.7.9.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">GridRCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>
</th>
<td id="S4.T4.8.7.9.2.2" class="ltx_td ltx_align_center">50.8</td>
<td id="S4.T4.8.7.9.2.3" class="ltx_td ltx_align_center">45.6</td>
<td id="S4.T4.8.7.9.2.4" class="ltx_td ltx_align_center">59.0</td>
<td id="S4.T4.8.7.9.2.5" class="ltx_td ltx_align_center ltx_border_r">52.5</td>
<td id="S4.T4.8.7.9.2.6" class="ltx_td ltx_align_center">52.0</td>
</tr>
<tr id="S4.T4.6.5.5" class="ltx_tr">
<th id="S4.T4.6.5.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Ours (inference)</th>
<th id="S4.T4.6.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><math id="S4.T4.6.5.5.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T4.6.5.5.1.m1.1a"><mo id="S4.T4.6.5.5.1.m1.1.1" xref="S4.T4.6.5.5.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T4.6.5.5.1.m1.1b"><times id="S4.T4.6.5.5.1.m1.1.1.cmml" xref="S4.T4.6.5.5.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.6.5.5.1.m1.1c">\times</annotation></semantics></math></th>
<td id="S4.T4.6.5.5.3" class="ltx_td ltx_align_center ltx_border_t">31.0</td>
<td id="S4.T4.6.5.5.4" class="ltx_td ltx_align_center ltx_border_t">0.5</td>
<td id="S4.T4.6.5.5.5" class="ltx_td ltx_align_center ltx_border_t">1.7</td>
<td id="S4.T4.6.5.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.1</td>
<td id="S4.T4.6.5.5.7" class="ltx_td ltx_align_center ltx_border_t">8.3</td>
</tr>
<tr id="S4.T4.7.6.6" class="ltx_tr">
<th id="S4.T4.7.6.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Ours (open-vocabulary)</th>
<th id="S4.T4.7.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><math id="S4.T4.7.6.6.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S4.T4.7.6.6.1.m1.1a"><mi mathvariant="normal" id="S4.T4.7.6.6.1.m1.1.1" xref="S4.T4.7.6.6.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T4.7.6.6.1.m1.1b"><ci id="S4.T4.7.6.6.1.m1.1.1.cmml" xref="S4.T4.7.6.6.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.7.6.6.1.m1.1c">\checkmark</annotation></semantics></math></th>
<td id="S4.T4.7.6.6.3" class="ltx_td ltx_align_center"><span id="S4.T4.7.6.6.3.1" class="ltx_text ltx_font_bold">88.6</span></td>
<td id="S4.T4.7.6.6.4" class="ltx_td ltx_align_center">76.5</td>
<td id="S4.T4.7.6.6.5" class="ltx_td ltx_align_center">2.3</td>
<td id="S4.T4.7.6.6.6" class="ltx_td ltx_align_center ltx_border_r">0.2</td>
<td id="S4.T4.7.6.6.7" class="ltx_td ltx_align_center">41.9</td>
</tr>
<tr id="S4.T4.8.7.7" class="ltx_tr" style="background-color:#D6D6D6;">
<th id="S4.T4.8.7.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="S4.T4.8.7.7.2.1" class="ltx_text" style="background-color:#D6D6D6;">Ours (fully-supervised)</span></th>
<th id="S4.T4.8.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r"><math id="S4.T4.8.7.7.1.m1.1" class="ltx_Math" style="background-color:#D6D6D6;" alttext="\checkmark" display="inline"><semantics id="S4.T4.8.7.7.1.m1.1a"><mi mathbackground="#D6D6D6" mathvariant="normal" id="S4.T4.8.7.7.1.m1.1.1" xref="S4.T4.8.7.7.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T4.8.7.7.1.m1.1b"><ci id="S4.T4.8.7.7.1.m1.1.1.cmml" xref="S4.T4.8.7.7.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.8.7.7.1.m1.1c">\checkmark</annotation></semantics></math></th>
<td id="S4.T4.8.7.7.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.8.7.7.3.1" class="ltx_text" style="background-color:#D6D6D6;">86.4</span></td>
<td id="S4.T4.8.7.7.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.8.7.7.4.1" class="ltx_text ltx_font_bold" style="background-color:#D6D6D6;">83.8</span></td>
<td id="S4.T4.8.7.7.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.8.7.7.5.1" class="ltx_text" style="background-color:#D6D6D6;">45.8</span></td>
<td id="S4.T4.8.7.7.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T4.8.7.7.6.1" class="ltx_text ltx_font_bold" style="background-color:#D6D6D6;">66.6</span></td>
<td id="S4.T4.8.7.7.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.8.7.7.7.1" class="ltx_text ltx_font_bold" style="background-color:#D6D6D6;">70.6</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE V: </span></figcaption>
<figcaption class="ltx_caption">We report the experimental results under different settings and also the detailed category split.</figcaption>
<div id="S4.T5.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:268.6pt;height:108pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<table id="S4.T5.2.2" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T5.1.1.1" class="ltx_tr">
<td id="S4.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" rowspan="2"><span id="S4.T5.1.1.1.2.1" class="ltx_text">Exp</span></td>
<td id="S4.T5.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" rowspan="2"><span id="S4.T5.1.1.1.3.1" class="ltx_text">Settings</span></td>
<td id="S4.T5.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2">Categories</td>
<td id="S4.T5.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">mAP<sub id="S4.T5.1.1.1.1.1" class="ltx_sub"><span id="S4.T5.1.1.1.1.1.1" class="ltx_text ltx_font_italic">50</span></sub>
</td>
</tr>
<tr id="S4.T5.2.2.3.1" class="ltx_tr">
<td id="S4.T5.2.2.3.1.1" class="ltx_td ltx_align_center">Seen</td>
<td id="S4.T5.2.2.3.1.2" class="ltx_td ltx_align_center ltx_border_r">Unseen</td>
<td id="S4.T5.2.2.3.1.3" class="ltx_td ltx_align_center">Seen</td>
<td id="S4.T5.2.2.3.1.4" class="ltx_td ltx_align_center">Unseen</td>
</tr>
<tr id="S4.T5.2.2.4.2" class="ltx_tr">
<td id="S4.T5.2.2.4.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Exp1</td>
<td id="S4.T5.2.2.4.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Intra-“<span id="S4.T5.2.2.4.2.2.1" class="ltx_text ltx_font_italic">Class</span>”</td>
<td id="S4.T5.2.2.4.2.3" class="ltx_td ltx_align_center ltx_border_t">613</td>
<td id="S4.T5.2.2.4.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">177</td>
<td id="S4.T5.2.2.4.2.5" class="ltx_td ltx_align_center ltx_border_t">37.7</td>
<td id="S4.T5.2.2.4.2.6" class="ltx_td ltx_align_center ltx_border_t">7.3</td>
</tr>
<tr id="S4.T5.2.2.5.3" class="ltx_tr">
<td id="S4.T5.2.2.5.3.1" class="ltx_td ltx_align_center ltx_border_r">Exp2</td>
<td id="S4.T5.2.2.5.3.2" class="ltx_td ltx_align_center ltx_border_r">Inter-“<span id="S4.T5.2.2.5.3.2.1" class="ltx_text ltx_font_italic">Class</span>”</td>
<td id="S4.T5.2.2.5.3.3" class="ltx_td ltx_align_center">572</td>
<td id="S4.T5.2.2.5.3.4" class="ltx_td ltx_align_center ltx_border_r">183</td>
<td id="S4.T5.2.2.5.3.5" class="ltx_td ltx_align_center">38.7</td>
<td id="S4.T5.2.2.5.3.6" class="ltx_td ltx_align_center">23.7</td>
</tr>
<tr id="S4.T5.2.2.6.4" class="ltx_tr">
<td id="S4.T5.2.2.6.4.1" class="ltx_td ltx_align_center ltx_border_r">Exp3</td>
<td id="S4.T5.2.2.6.4.2" class="ltx_td ltx_align_center ltx_border_r">“<span id="S4.T5.2.2.6.4.2.1" class="ltx_text ltx_font_italic">Class</span>”-level</td>
<td id="S4.T5.2.2.6.4.3" class="ltx_td ltx_align_center">20</td>
<td id="S4.T5.2.2.6.4.4" class="ltx_td ltx_align_center ltx_border_r">6</td>
<td id="S4.T5.2.2.6.4.5" class="ltx_td ltx_align_center">48.3</td>
<td id="S4.T5.2.2.6.4.6" class="ltx_td ltx_align_center">17.7</td>
</tr>
<tr id="S4.T5.2.2.2" class="ltx_tr">
<td id="S4.T5.2.2.2.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">Exp4</td>
<td id="S4.T5.2.2.2.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">Fully supervised</td>
<td id="S4.T5.2.2.2.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">821</td>
<td id="S4.T5.2.2.2.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">0</td>
<td id="S4.T5.2.2.2.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">35.9</td>
<td id="S4.T5.2.2.2.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><math id="S4.T5.2.2.2.1.m1.1" class="ltx_Math" alttext="-" display="inline"><semantics id="S4.T5.2.2.2.1.m1.1a"><mo id="S4.T5.2.2.2.1.m1.1.1" xref="S4.T5.2.2.2.1.m1.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S4.T5.2.2.2.1.m1.1b"><minus id="S4.T5.2.2.2.1.m1.1.1.cmml" xref="S4.T5.2.2.2.1.m1.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.2.2.2.1.m1.1c">-</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS4.5.1.1" class="ltx_text">IV-D</span> </span><span id="S4.SS4.6.2" class="ltx_text ltx_font_italic">Ablation Studies</span>
</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.3" class="ltx_p">We aim to explore the boundary of our MarineDet under different settings. The experimental results are reported in Table <a href="#S4.T5" title="TABLE V ‣ IV-C Comparisons with SOTAs ‣ IV Experiments ‣ MarineDet: Towards Open-Marine Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>. Under the fully supervised setting, the detection model is required to perform very fine-grained marine object recognition and thus only achieved 35.9 mAP<sub id="S4.SS4.p1.3.1" class="ltx_sub"><span id="S4.SS4.p1.3.1.1" class="ltx_text ltx_font_italic">50</span></sub> on 821 object categories. Comparing the experimental results of Exp1 and Exp2, we could achieve 23.7 mAP<sub id="S4.SS4.p1.3.2" class="ltx_sub"><span id="S4.SS4.p1.3.2.1" class="ltx_text ltx_font_italic">50</span></sub> on 183 <span id="S4.SS4.p1.3.3" class="ltx_text ltx_font_italic">unseen</span> marine object categories while only 7.3 mAP<sub id="S4.SS4.p1.3.4" class="ltx_sub"><span id="S4.SS4.p1.3.4.1" class="ltx_text ltx_font_italic">50</span></sub> on 177 <span id="S4.SS4.p1.3.5" class="ltx_text ltx_font_italic">unseen</span> marine object categories. We attribute this phenomenon to the reason that the model could better utilize the relationship between similar marine object categories that belong to the same “Class”. How to utilize the relationship between different marine species to promote open-marine object detection performance is a new and valuable research direction. Comparing the experimental results of Exp1 and Exp3, it still requires more effort to perform fine-grained marine object detection under the OMOD setting.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS5.5.1.1" class="ltx_text">IV-E</span> </span><span id="S4.SS5.6.2" class="ltx_text ltx_font_italic">Discussions and Limitations</span>
</h3>

<div id="S4.SS5.p1" class="ltx_para ltx_noindent">
<p id="S4.SS5.p1.1" class="ltx_p"><span id="S4.SS5.p1.1.1" class="ltx_text ltx_font_bold">Potential applications</span>. OMOD could enable comprehensive monitoring of marine species diversity and population dynamics, aiding in ecological research and conservation efforts. Detecting and tracking invasive species in marine environments, allowing for timely intervention and mitigation strategies to protect native ecosystems. The versatility of OMOD has the potential to revolutionize underwater research, from advancing ecological knowledge to informing conservation policies and sustainable management practices.</p>
</div>
<div id="S4.SS5.p2" class="ltx_para ltx_noindent">
<p id="S4.SS5.p2.1" class="ltx_p"><span id="S4.SS5.p2.1.1" class="ltx_text ltx_font_bold">Limitations</span>. Despite our state-of-the-art OMOD performance, our method is not without limitations. Since the language encoder is borrowed from CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>optimized by the general-purpose image-text pairs, the language encoder is not optimized for marine applications. The generated textual semantic embedding cannot always be effective enough for marine conditions, which will lead to the performance degradation of some categories. We could further design an optimized textual semantic space that can better model semantic information of marine data and improve the overall accuracy and robustness of MarineDet.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this work, we present MarineDet, the first work to perform OMOD. We have also proposed the MarineDet dataset with 821 marine-relative object categories, which could support open-marine and fine-grained marine object detection. We have included comprehensive experiments under various experimental settings and ablation studies also provide insight on OMOD. We believe our work and the proposed MarineDet dataset could boost the development of open-marine object detection. We pave the way for future open-marine object recognition research in both academic and industrial communities.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
D. P. Williams, “On adaptive underwater object detection,” in <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS)</span>,
pp. 4741–4748, IEEE, 2011.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
M. Fulton, J. Hong, M. J. Islam, and J. Sattar, “Robotic detection of marine
litter using deep visual detection models,” in <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">IEEE International
conference on robotics and automation (ICRA)</span>, pp. 5752–5758, IEEE, 2019.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
F. Zocco, T.-C. Lin, C.-I. Huang, H.-C. Wang, M. O. Khyam, and M. Van,
“Towards more efficient efficientdets and real-time marine debris
detection,” <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">IEEE Robotics and Automation Letters (RA-L)</span>, vol. 8,
no. 4, pp. 2134–2141, 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Y. Xia and J. Sattar, “Visual diver recognition for underwater human-robot
collaboration,” in <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">international conference on robotics and automation
(ICRA)</span>, pp. 6839–6845, IEEE, 2019.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
B. Bovcon, J. Muhovič, J. Perš, and M. Kristan, “The mastr1325
dataset for training deep usv obstacle detection models,” in <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS)</span>,
pp. 3431–3438, IEEE, 2019.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
H. Lyu, Z. Shao, T. Cheng, Y. Yin, and X. Gao, “Sea-surface object detection
based on electro-optical sensors: A review,” <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">IEEE Intelligent
Transportation Systems Magazine</span>, pp. 2–27, 2022.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
C. Kuenzer, M. Ottinger, M. Wegmann, H. Guo, C. Wang, J. Zhang, S. Dech, and
M. Wikelski, “Earth observation satellite sensors for biodiversity
monitoring: potentials and bottlenecks,” <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">International Journal of
Remote Sensing</span>, vol. 35, no. 18, pp. 6599–6647, 2014.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
B. Fan, W. Chen, Y. Cong, and J. Tian, “Dual refinement underwater object
detection network,” in <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">European Conference on Computer Vision (ECCV)</span>,
pp. 275–291, Springer, 2020.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
J. Yan, Z. Zhou, B. Su, and Z. Xuanyuan, “Underwater object detection
algorithm based on attention mechanism and cross-stage partial fast spatial
pyramidal pooling,” <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Frontiers in Marine Science</span>, p. 2299, 2022.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
M. Kapoor, S. Patra, B. N. Subudhi, V. Jakhetiya, and A. Bansal, “Underwater
moving object detection using an end-to-end encoder-decoder architecture and
graphsage with aggregator and refactoring,” in <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">IEEE/CVF Conference on
Computer Vision and Pattern Recognition Workshps (CVPRW)</span>, pp. 5635–5644,
2023.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
D. Miller, F. Dayoub, M. Milford, and N. Sünderhauf, “Evaluating merging
strategies for sampling-based uncertainty techniques in object detection,”
in <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">IEEE International Conference on Robotics and Automation (ICRA)</span>,
pp. 2348–2354, IEEE, 2019.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
B. Sadrfaridpour, Y. Aloimonos, M. Yu, Y. Tao, and D. Webster, “Detecting and
counting oysters,” in <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">IEEE International Conference on Robotics and
Automation (ICRA)</span>, pp. 2156–2162, IEEE, 2021.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
X. Lin, N. J. Sanket, N. Karapetyan, and Y. Aloimonos, “Oysternet: Enhanced
oyster detection using simulation,” in <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">IEEE International Conference on
Robotics and Automation (ICRA)</span>, pp. 5170–5176, IEEE, 2023.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
L. Li, B. Dong, E. Rigall, T. Zhou, J. Dong, and G. Chen, “Marine animal
segmentation,” <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Circuits and Systems for Video
Technology (TCSVT)</span>, vol. 32, no. 4, pp. 2303–2314, 2021.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
J. Hong, M. Fulton, and J. Sattar, “Trashcan: A semantically-segmented dataset
towards visual detection of marine debris,” <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:2007.08097</span>, 2020.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
L. Li, E. Rigall, J. Dong, and G. Chen, “Mas3k: An open dataset for marine
animal segmentation,” in <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">International Symposium on Benchmarking,
Measuring and Optimization</span>, pp. 194–212, Springer, 2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
P. Zhuang, Y. Wang, and Y. Qiao, “Wildfish: A large benchmark for fish
recognition in the wild,” in <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">ACM international conference on Multimedia
(ACM MM)</span>, pp. 1301–1309, 2018.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
P. Zhuang, Y. Wang, and Y. Qiao, “Wildfish++: A comprehensive fish benchmark
for multimedia research,” <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Multimedia (TMM)</span>,
vol. 23, pp. 3603–3617, 2020.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
C. Liu, Z. Wang, S. Wang, T. Tang, Y. Tao, C. Yang, H. Li, X. Liu, and X. Fan,
“A new dataset, poisson gan and aquanet for underwater object grabbing,”
<span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</span>,
vol. 32, no. 5, pp. 2831–2844, 2021.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
M. J. Islam, C. Edge, Y. Xiao, P. Luo, M. Mehtaz, C. Morse, S. S. Enan, and
J. Sattar, “Semantic segmentation of underwater imagery: Dataset and
benchmark,” in <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS)</span>, pp. 1769–1776, IEEE, 2020.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
A. Zareian, K. D. Rosa, D. H. Hu, and S.-F. Chang, “Open-vocabulary object
detection using captions,” in <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR)</span>, pp. 14393–14402, 2021.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
L. Yao, J. Han, X. Liang, D. Xu, W. Zhang, Z. Li, and H. Xu, “Detclipv2:
Scalable open-vocabulary object detection pre-training via word-region
alignment,” in <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</span>, pp. 23497–23506, 2023.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
D. Kim, A. Angelova, and W. Kuo, “Region-aware pretraining for open-vocabulary
object detection with vision transformers,” in <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR)</span>, pp. 11144–11154, 2023.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
O. Zohar, K.-C. Wang, and S. Yeung, “Prob: Probabilistic objectness for open
world object detection,” in <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</span>, pp. 11444–11453, 2023.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Z. Wang, Y. Li, X. Chen, S.-N. Lim, A. Torralba, H. Zhao, and S. Wang,
“Detecting everything in the open world: Towards universal object
detection,” in <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</span>, pp. 11433–11443, 2023.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Z. Ge, S. Liu, F. Wang, Z. Li, and J. Sun, “Yolox: Exceeding yolo series in
2021,” <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2107.08430</span>, 2021.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time
object detection with region proposal networks,” <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">Advances in neural
information processing systems (Neurips)</span>, vol. 28, 2015.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
X. Lu, B. Li, Y. Yue, Q. Li, and J. Yan, “Grid r-cnn,” in <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR)</span>,
pp. 7363–7372, 2019.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Z. Zheng, Y. Wu, X. Han, and J. Shi, “Forkgan: Seeing into the rainy
night,” in <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">European Conference on Computer Vision (ECCV)</span>,
pp. 155–170, 2020.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
G. Ferri, P. Stinco, G. De Magistris, A. Tesei, and K. D. LePage, “Cooperative
autonomy and data fusion for underwater surveillance with networked auvs,”
in <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">IEEE International Conference on Robotics and Automation (ICRA)</span>,
pp. 871–877, IEEE, 2020.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
N. Karapetyan, J. Moulton, J. S. Lewis, A. Q. Li, J. M. O’Kane, and
I. Rekleitis, “Multi-robot dubins coverage with autonomous surface
vehicles,” in <span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">IEEE International Conference on Robotics and Automation
(ICRA)</span>, pp. 2373–2379, IEEE, 2018.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry,
A. Askell, P. Mishkin, J. Clark, <span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Learning transferable visual
models from natural language supervision,” in <span id="bib.bib32.2.2" class="ltx_text ltx_font_italic">International Conference
on Machine Learning (ICML)</span>, pp. 8748–8763, PMLR, 2021.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
J. Li, D. Li, C. Xiong, and S. Hoi, “Blip: Bootstrapping language-image
pre-training for unified vision-language understanding and generation,” in
<span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning (ICML)</span>, pp. 12888–12900,
PMLR, 2022.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
J. Li, D. Li, S. Savarese, and S. Hoi, “Blip-2: Bootstrapping language-image
pre-training with frozen image encoders and large language models,” <span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning (ICML)</span>, 2023.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
C. Liu, H. Li, S. Wang, M. Zhu, D. Wang, X. Fan, and Z. Wang, “A dataset and
benchmark of underwater object detection for robot picking,” in <span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">IEEE
International Conference on Multimedia and Expo Workshops (ICMEW)</span>, pp. 1–6,
IEEE, 2021.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in <span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</span>, pp. 770–778, 2016.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Dollár, and C. L. Zitnick, “Microsoft coco: Common objects in
context,” in <span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">European Conference on Computer Vision (ECCV)</span>,
pp. 740–755, Springer, 2014.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
S. Liu, Z. Zeng, T. Ren, F. Li, H. Zhang, J. Yang, C. Li, J. Yang, H. Su,
J. Zhu, <span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Grounding dino: Marrying dino with grounded
pre-training for open-set object detection,” <span id="bib.bib38.2.2" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:2303.05499</span>, 2023.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
A. Gupta, P. Dollar, and R. Girshick, “Lvis: A dataset for large vocabulary
instance segmentation,” in <span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</span>, pp. 5356–5364, 2019.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
C. Li, H. Liu, L. Li, P. Zhang, J. Aneja, J. Yang, P. Jin, H. Hu, Z. Liu, Y. J.
Lee, <span id="bib.bib40.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Elevater: A benchmark and toolkit for evaluating
language-augmented visual models,” <span id="bib.bib40.2.2" class="ltx_text ltx_font_italic">Advances in Neural Information
Processing Systems (Neurips)</span>, vol. 35, pp. 9287–9301, 2022.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2310.01930" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2310.01931" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2310.01931">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2310.01931" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2310.01932" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 02:31:54 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
