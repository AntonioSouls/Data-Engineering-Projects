<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2309.03548] Trash to Treasure: Low-Light Object Detection via Decomposition-and-Aggregation</title><meta property="og:description" content="Object detection in low-light scenarios has attracted much attention in the past few years. A mainstream and representative scheme introduces enhancers as the pre-processing for regular detectors. However, because of tâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Trash to Treasure: Low-Light Object Detection via Decomposition-and-Aggregation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Trash to Treasure: Low-Light Object Detection via Decomposition-and-Aggregation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2309.03548">

<!--Generated on Wed Feb 28 07:50:57 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Trash to Treasure:
Low-Light Object Detection via Decomposition-and-Aggregation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Xiaohan Cui, Long Ma, Tengyu Ma, Jinyuan Liu, Xin Fan, Risheng Liu

</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Object detection in low-light scenarios has attracted much attention in the past few years. A mainstream and representative scheme introduces enhancers as the pre-processing for regular detectors. However, because of the disparity in task objectives between the enhancer and detector, this paradigm cannot shine at its best ability. In this work, we try to <span id="id1.id1.1" class="ltx_text ltx_font_italic">arouse the potential of enhancer + detector</span>. Different from existing works, we extend the illumination-based enhancers (our newly designed or existing) as a scene decomposition module, whose removed illumination is exploited as the auxiliary in the detector for extracting detection-friendly features. A semantic aggregation module is further established for integrating multi-scale scene-related semantic information in the context space. Actually, our built scheme successfully transforms the â€œ<span id="id1.id1.2" class="ltx_text ltx_font_italic">trash</span>â€ (i.e., the ignored illumination in the detector) into the â€œ<span id="id1.id1.3" class="ltx_text ltx_font_italic">treasure</span>â€ for the detector. Plenty of experiments are conducted to reveal our superiority against other state-of-the-art methods. The code will be public if it is accepted.</p>
</div>
<div id="p1" class="ltx_para">
<span id="p1.3" class="ltx_ERROR undefined">{strip}</span>
<table id="p1.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="p1.2.2" class="ltx_tr">
<td id="p1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><img src="/html/2309.03548/assets/x1.png" id="p1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="204" height="135" alt="[Uncaptioned image]"></td>
<td id="p1.2.2.2" class="ltx_td ltx_nopad_l ltx_align_center"><img src="/html/2309.03548/assets/x2.png" id="p1.2.2.2.g1" class="ltx_graphics ltx_img_landscape" width="317" height="135" alt="[Uncaptioned image]"></td>
</tr>
</tbody>
</table>
</div>
<figure id="S0.F1" class="ltx_figure">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Comparing algorithmic pipelines (<span id="S0.F1.3.1" class="ltx_text ltx_font_italic">left</span>) and visual results (<span id="S0.F1.4.2" class="ltx_text ltx_font_italic">right</span>). Different from the representative scheme (i.e., enhancer + detector) of low-light detection, we treat the illumination which is viewed as the trash as the treasure, to establish our decomposition-and-aggregation process. It needs to be emphasized that our defined scene decomposition can be instantiated as any illumination-based enhancers. The results in the top row of the right part are two methods based on the representative scheme (enhancer is defined as RUAS and SCI, and the detector is set as DSFD), and a recent method (HLA), respectively. The results in the bottom row are generated by our T2 with different settings of scene decomposition. We can observe that our T2 significantly upgrades the detection ability against the representative scheme. Additionally, our method brightens images (without any loss constraint for enhancement) to realize the enhancement exactly wanted for the detector. </figcaption>
</figure>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Introduction</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">Object detection is a representative, familiar vision task both in industrial and academic communities. Along with the development of deep learning techniques, object detection in the regular environment has achieved prominent achievements. However, in specific settings, such as salient object detectionÂ <cite class="ltx_cite ltx_citemacro_citep">(Piao etÂ al. <a href="#bib.bib30" title="" class="ltx_ref">2019</a>, <a href="#bib.bib31" title="" class="ltx_ref">2020</a>; Zhang etÂ al. <a href="#bib.bib45" title="" class="ltx_ref">2020</a>)</cite> and low-light object detection, there remains immense potential for research and optimization. Especially, images taken in low light conditions (e.g., nighttime) usually contain complex degradation to heavily limit the performance of regular detectorsÂ <cite class="ltx_cite ltx_citemacro_citep">(He etÂ al. <a href="#bib.bib5" title="" class="ltx_ref">2016</a>; Jin etÂ al. <a href="#bib.bib7" title="" class="ltx_ref">2021</a>; Tang etÂ al. <a href="#bib.bib32" title="" class="ltx_ref">2020</a>; Liu etÂ al. <a href="#bib.bib22" title="" class="ltx_ref">2021d</a>; Tang etÂ al. <a href="#bib.bib33" title="" class="ltx_ref">2022</a>; Liu etÂ al. <a href="#bib.bib14" title="" class="ltx_ref">2021a</a>, <a href="#bib.bib16" title="" class="ltx_ref">2020</a>)</cite>. This degradation can be due to factors such as noise, low contrast, and insufficient illumination, which can reduce the visibility of objects and make them hard to detect. In the following, we will review existing works and introduce our contributions.</p>
</div>
<section id="Sx1.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Related Works</h3>

<div id="Sx1.SSx1.p1" class="ltx_para">
<p id="Sx1.SSx1.p1.1" class="ltx_p">In the past few decades, various schemes have emerged for handling low-light object detection, which can be roughly divided into two categories. The mainstream one is to cascade enhancer and detector, the other is to specifically-design the low-light detector.</p>
</div>
<div id="Sx1.SSx1.p2" class="ltx_para">
<p id="Sx1.SSx1.p2.1" class="ltx_p">For the enhancer-introduced schemes, the well-known UG2+ Challenge competition<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><span class="ltx_ref ltx_nolink ltx_url ltx_ref_self">http://www.ug2challenge.org/</span></span></span></span> is a landmark event to drive research for low-light face detection. The cascade of enhancement and detection is the most common solution in the two consecutive championship schemes, e.g., the CAS-Newcastle team adopted this cascaded schemeÂ <cite class="ltx_cite ltx_citemacro_citep">(Yuan etÂ al. <a href="#bib.bib44" title="" class="ltx_ref">2019</a>)</cite>, which illustrates the significance of researching the cascaded pattern. In addition, the latest enhancement schemes (e.g., Â <cite class="ltx_cite ltx_citemacro_citep">(Guo etÂ al. <a href="#bib.bib4" title="" class="ltx_ref">2020</a>; Liu etÂ al. <a href="#bib.bib18" title="" class="ltx_ref">2022a</a>; Ma etÂ al. <a href="#bib.bib27" title="" class="ltx_ref">2022a</a>; Xue etÂ al. <a href="#bib.bib41" title="" class="ltx_ref">2022</a>; Ma etÂ al. <a href="#bib.bib26" title="" class="ltx_ref">2023</a>)</cite>), and the workÂ <cite class="ltx_cite ltx_citemacro_citep">(Lv, Li, and Lu <a href="#bib.bib25" title="" class="ltx_ref">2021</a>)</cite> are no longer satisfied with the improvement of visual quality and try to apply the well-designed enhancer to the detection task to verify the effectiveness of the algorithm. For example, the improvement of detection performance on the Dark Face<cite class="ltx_cite ltx_citemacro_citep">(Yang etÂ al. <a href="#bib.bib43" title="" class="ltx_ref">2020b</a>; Liu etÂ al. <a href="#bib.bib15" title="" class="ltx_ref">2021b</a>)</cite> dataset is a crucial experiment to demonstrate enhanced performance. However, these works often bring limited performance improvement, and it is even better to directly detect low light images. An intuitive explanation is that these enhancements are designed for human-friendly visual quality<cite class="ltx_cite ltx_citemacro_citep">(Lore, Akintayo, and Sarkar <a href="#bib.bib24" title="" class="ltx_ref">2017</a>; Wei etÂ al. <a href="#bib.bib38" title="" class="ltx_ref">2018</a>; Chen etÂ al. <a href="#bib.bib1" title="" class="ltx_ref">2018</a>; Li etÂ al. <a href="#bib.bib10" title="" class="ltx_ref">2018</a>)</cite>, and are difficult to apply to machine vision tasks focused on high-level semantic information, e.g., object detection. Certain methods produce black edges, retain dark noisy areas, or enhance contrast to improve the overall visual quality, but these enhancements can have a negative impact on the performance of object detection.</p>
</div>
<div id="Sx1.SSx1.p3" class="ltx_para">
<p id="Sx1.SSx1.p3.1" class="ltx_p">The other type of scheme makes an effort to start from the perspective of designing detectors in an end-to-end manner. Through a bidirectional low-level adaptation and multi-task high-level adaptation scheme, HLAÂ <cite class="ltx_cite ltx_citemacro_citep">(Wang, Yang, and Liu <a href="#bib.bib37" title="" class="ltx_ref">2021</a>; Wang etÂ al. <a href="#bib.bib36" title="" class="ltx_ref">2022</a>)</cite> proposed a joint high-low adaptation framework. This method converted a face detector trained under normal light into a face detector under low light. While domain adaptation methods can mitigate the issue of having a limited number of labeled datasets, their performance improvements were often modest. The method proposed in REGÂ <cite class="ltx_cite ltx_citemacro_citep">(Liang etÂ al. <a href="#bib.bib12" title="" class="ltx_ref">2021</a>)</cite> is to seamlessly couple a cycle exposure generation module and a multiple exposure detection module, which improved the detection effects by effectively suppressing uneven illumination and noise problems. Moreover, the workÂ <cite class="ltx_cite ltx_citemacro_citep">(Cui etÂ al. <a href="#bib.bib2" title="" class="ltx_ref">2021</a>)</cite> did not apply the enhancement directly to the low-light image, but used the traditional camera signal processing method to transform the normal-light image into a low-light image, and utilized a predictive transform decoder to predict the parameters involved in the illumination transformation to complete the self-supervised training. <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al. <a href="#bib.bib21" title="" class="ltx_ref">2022b</a>)</cite> proposed a method called IA-YOLO that improves object detection performance in adverse weather conditions. Moreover, clustering helps in extracting better featuresÂ <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al. <a href="#bib.bib17" title="" class="ltx_ref">2012</a>; Wu, Lin, and Zha <a href="#bib.bib39" title="" class="ltx_ref">2019</a>)</cite>. However, these specifically-designed methods lack the full exploration of scene information, resulting in limited performance improvement.</p>
</div>
</section>
<section id="Sx1.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Our Contributions</h3>

<div id="Sx1.SSx2.p1" class="ltx_para">
<p id="Sx1.SSx2.p1.1" class="ltx_p">To overcome the above drawbacks and arouse the potential of enhancer + detector, this work establishes a new detector with decomposition-and-aggregation by fully exploiting the illumination that is viewed as the trash in the previous schemes. As shown in the left part of FigureÂ <a href="#S0.F1" title="Figure 1 â€£ Trash to Treasure: Low-Light Object Detection via Decomposition-and-Aggregation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we compare the algorithmic pipeline with the representative cascaded scheme. Our proposed <span id="Sx1.SSx2.p1.1.1" class="ltx_text ltx_font_bold">T</span>rash to <span id="Sx1.SSx2.p1.1.2" class="ltx_text ltx_font_bold">T</span>reasure (<span id="Sx1.SSx2.p1.1.3" class="ltx_text ltx_font_bold">T2</span>) is acquired around the fact that â€œthe illumination is a treasure not trash for the detectorâ€, to make full use of the complete scene information. Benefiting from the flexibility of our designed scene decomposition, existing illumination-based enhancers can be plugged into T2. The right part in FigureÂ <a href="#S0.F1" title="Figure 1 â€£ Trash to Treasure: Low-Light Object Detection via Decomposition-and-Aggregation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> demonstrates the visual results among different state-of-the-art methods. It can be easily observed that T2 not only acquires the best detection accuracy but also significantly ameliorate the detection accuracy for existing methods. Our main contributions can be summarized as</p>
<ul id="Sx1.I1" class="ltx_itemize">
<li id="Sx1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="Sx1.I1.i1.p1" class="ltx_para">
<p id="Sx1.I1.i1.p1.1" class="ltx_p">By deeply analyzing the latent relationship between enhancer and detector, we conclude two key challenges for the cascaded pattern. The one challenge is how to realize detection-oriented enhancement, instead of pure human eye-friendly visual quality. The other is how to reduce information discrepancy between enhanced output and regular data as much as possible.</p>
</div>
</li>
<li id="Sx1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="Sx1.I1.i2.p1" class="ltx_para">
<p id="Sx1.I1.i2.p1.1" class="ltx_p">By rethinking illumination-based enhancers, we construct a scene decomposition module to acquire two scene-related components for characterizing the low-light scenarios. It supports learning detection-oriented enhancement without introducing additional training constraints related to visual quality.</p>
</div>
</li>
<li id="Sx1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="Sx1.I1.i3.p1" class="ltx_para">
<p id="Sx1.I1.i3.p1.1" class="ltx_p">To effectively utilize the decomposed components, we design a semantic aggregation module that is composed of a weight-sharing extractor and a multi-scale aggregator. It fully exploits the scene-related content in the feature space to reduce the information discrepancy between regular and low-light data.</p>
</div>
</li>
<li id="Sx1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="Sx1.I1.i4.p1" class="ltx_para">
<p id="Sx1.I1.i4.p1.1" class="ltx_p">Extensive experimental evaluations are performed to verify our superiority in detection accuracy against other state-of-the-art methods. A series of algorithmic analyses indicate that our T2 successfully realizes the intended target, i.e., detection-oriented enhancement.</p>
</div>
</li>
</ul>
</div>
<figure id="Sx1.F2" class="ltx_figure">
<table id="Sx1.F2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="Sx1.F2.1.1" class="ltx_tr">
<td id="Sx1.F2.1.1.1" class="ltx_td ltx_align_center"><img src="/html/2309.03548/assets/x3.png" id="Sx1.F2.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="530" height="148" alt="Refer to caption"></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The overall architecture of our proposed method. Our method mainly contains two parts, Scene Decomposition (SA) for generating illumination and reflectance from low-light observation based on Retinex theory, and Scene Aggregation (SA) for integrating multi-scale scene-related information to strengthen feature representation. </figcaption>
</figure>
<figure id="Sx1.F3" class="ltx_figure">
<table id="Sx1.F3.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="Sx1.F3.1.1" class="ltx_tr">
<td id="Sx1.F3.1.1.1" class="ltx_td ltx_align_center"><img src="/html/2309.03548/assets/x4.png" id="Sx1.F3.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="527" height="119" alt="Refer to caption"></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The computational flow of our proposed multi-scale feature aggregator.</figcaption>
</figure>
</section>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Proposed Method</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">In this section, we present our motivation by analyzing the latent relationship between enhancer and detector. We then construct a scene decomposition module to endow the perception-related ability to the Retinex-based enhancer. Finally, we introduce a semantic aggregation module to strengthen the feature representation. The overall architecture of our proposed method can be found in FigureÂ <a href="#Sx1.F2" title="Figure 2 â€£ Our Contributions â€£ Introduction â€£ Trash to Treasure: Low-Light Object Detection via Decomposition-and-Aggregation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<section id="Sx2.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Two Key Challenges of Enhancer + Detector</h3>

<div id="Sx2.SSx1.p1" class="ltx_para">
<p id="Sx2.SSx1.p1.1" class="ltx_p">Unlike regular object detection in normal-light environments, the main challenge for low-light object detection is that degraded observations with poor visibility heavily influence feature extraction, causing the accuracy to drop sharply. It is a direct and commonly-used manner to generate new visible data by improving visual quality. It is widely adopted in multiple champion solutions born in the UG2+ Challenge competition (landmark for low-light face detection). Among them, experimental explorations suggest that adopting a classical (e.g., MSRCRÂ <cite class="ltx_cite ltx_citemacro_citep">(Jobson, Rahman, and Woodell <a href="#bib.bib8" title="" class="ltx_ref">1997</a>)</cite>) one with poor visual quality is more effective than the latest advanced enhancer. Therefore, the preconceived conclusion â€œhigher visual quality is more beneficial for detectionâ€ cannot be satisfied.</p>
</div>
<div id="Sx2.SSx1.p2" class="ltx_para">
<p id="Sx2.SSx1.p2.1" class="ltx_p">Investigating its reason, on the one hand, these two tasks have different objectives, i.e., pixel-level visual-friendly to human eyes (enhancer) and semantic-level perception-friendly to machines (detector). In other words, the directly cascaded pattern aims at improving detection accuracy, and the enhancer is designed for catering to visual quality, rather than detection-desired high-quality data.
On the other hand, the enhancer indeed ameliorates visual quality but it inevitably destroys the inherent distribution that keeps the same situation as natural images, causing the enhanced data to keep a distinct information discrepancy with regularly captured data (maybe low-light, maybe normal-light), which heavily restricts the feature extraction. Combing the above analyses, we can conclude two key challenges for the paradigm of enhancer + detector, described as</p>
<ol id="Sx2.I2" class="ltx_enumerate">
<li id="Sx2.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="Sx2.I2.i1.p1" class="ltx_para">
<p id="Sx2.I2.i1.p1.1" class="ltx_p"><span id="Sx2.I2.i1.p1.1.1" class="ltx_text ltx_font_italic">What kind of data acquired from the enhancer is required for the detector in low-light scenes?</span></p>
</div>
</li>
<li id="Sx2.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="Sx2.I2.i2.p1" class="ltx_para">
<p id="Sx2.I2.i2.p1.1" class="ltx_p"><span id="Sx2.I2.i2.p1.1.1" class="ltx_text ltx_font_italic">How to narrow down the information discrepancy between enhanced results and regular data?</span></p>
</div>
</li>
</ol>
</div>
<div id="Sx2.SSx1.p3" class="ltx_para">
<p id="Sx2.SSx1.p3.1" class="ltx_p">To settle these two challenges, in the following, we build a scene decomposition module to enable the Retinex-based enhancer beyond visual quality to generate detection-desired data. Then we construct a semantic aggregation module to fully exploit decomposed scene-related features to reduce the information discrepancy.</p>
</div>
</section>
<section id="Sx2.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Scene Decomposition Module</h3>

<div id="Sx2.SSx2.p1" class="ltx_para">
<p id="Sx2.SSx2.p1.1" class="ltx_p">In this part, by rethinking enhancement for detection, we explain the necessity of utilizing the removed illumination. Substantially, we establish a scene decomposition module to acquire the decomposed components.</p>
</div>
<section id="Sx2.SSx2.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Rethinking Illumination-based Enhancers</h4>

<div id="Sx2.SSx2.SSSx1.p1" class="ltx_para">
<p id="Sx2.SSx2.SSSx1.p1.5" class="ltx_p">Existing low-light image enhancement techniquesÂ <cite class="ltx_cite ltx_citemacro_citep">(Ma etÂ al. <a href="#bib.bib27" title="" class="ltx_ref">2022a</a>; Liu etÂ al. <a href="#bib.bib19" title="" class="ltx_ref">2021c</a>)</cite> are mostly developed according to the Retinex theoryÂ <cite class="ltx_cite ltx_citemacro_citep">(Land and McCann <a href="#bib.bib9" title="" class="ltx_ref">1971</a>)</cite> (formulated as <math id="Sx2.SSx2.SSSx1.p1.1.m1.1" class="ltx_Math" alttext="\mathbf{L}=\mathbf{R}\otimes\mathbf{I}" display="inline"><semantics id="Sx2.SSx2.SSSx1.p1.1.m1.1a"><mrow id="Sx2.SSx2.SSSx1.p1.1.m1.1.1" xref="Sx2.SSx2.SSSx1.p1.1.m1.1.1.cmml"><mi id="Sx2.SSx2.SSSx1.p1.1.m1.1.1.2" xref="Sx2.SSx2.SSSx1.p1.1.m1.1.1.2.cmml">ğ‹</mi><mo id="Sx2.SSx2.SSSx1.p1.1.m1.1.1.1" xref="Sx2.SSx2.SSSx1.p1.1.m1.1.1.1.cmml">=</mo><mrow id="Sx2.SSx2.SSSx1.p1.1.m1.1.1.3" xref="Sx2.SSx2.SSSx1.p1.1.m1.1.1.3.cmml"><mi id="Sx2.SSx2.SSSx1.p1.1.m1.1.1.3.2" xref="Sx2.SSx2.SSSx1.p1.1.m1.1.1.3.2.cmml">ğ‘</mi><mo lspace="0.222em" rspace="0.222em" id="Sx2.SSx2.SSSx1.p1.1.m1.1.1.3.1" xref="Sx2.SSx2.SSSx1.p1.1.m1.1.1.3.1.cmml">âŠ—</mo><mi id="Sx2.SSx2.SSSx1.p1.1.m1.1.1.3.3" xref="Sx2.SSx2.SSSx1.p1.1.m1.1.1.3.3.cmml">ğˆ</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx2.SSx2.SSSx1.p1.1.m1.1b"><apply id="Sx2.SSx2.SSSx1.p1.1.m1.1.1.cmml" xref="Sx2.SSx2.SSSx1.p1.1.m1.1.1"><eq id="Sx2.SSx2.SSSx1.p1.1.m1.1.1.1.cmml" xref="Sx2.SSx2.SSSx1.p1.1.m1.1.1.1"></eq><ci id="Sx2.SSx2.SSSx1.p1.1.m1.1.1.2.cmml" xref="Sx2.SSx2.SSSx1.p1.1.m1.1.1.2">ğ‹</ci><apply id="Sx2.SSx2.SSSx1.p1.1.m1.1.1.3.cmml" xref="Sx2.SSx2.SSSx1.p1.1.m1.1.1.3"><csymbol cd="latexml" id="Sx2.SSx2.SSSx1.p1.1.m1.1.1.3.1.cmml" xref="Sx2.SSx2.SSSx1.p1.1.m1.1.1.3.1">tensor-product</csymbol><ci id="Sx2.SSx2.SSSx1.p1.1.m1.1.1.3.2.cmml" xref="Sx2.SSx2.SSSx1.p1.1.m1.1.1.3.2">ğ‘</ci><ci id="Sx2.SSx2.SSSx1.p1.1.m1.1.1.3.3.cmml" xref="Sx2.SSx2.SSSx1.p1.1.m1.1.1.3.3">ğˆ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx2.SSSx1.p1.1.m1.1c">\mathbf{L}=\mathbf{R}\otimes\mathbf{I}</annotation></semantics></math>, where <math id="Sx2.SSx2.SSSx1.p1.2.m2.1" class="ltx_Math" alttext="\otimes" display="inline"><semantics id="Sx2.SSx2.SSSx1.p1.2.m2.1a"><mo id="Sx2.SSx2.SSSx1.p1.2.m2.1.1" xref="Sx2.SSx2.SSSx1.p1.2.m2.1.1.cmml">âŠ—</mo><annotation-xml encoding="MathML-Content" id="Sx2.SSx2.SSSx1.p1.2.m2.1b"><csymbol cd="latexml" id="Sx2.SSx2.SSSx1.p1.2.m2.1.1.cmml" xref="Sx2.SSx2.SSSx1.p1.2.m2.1.1">tensor-product</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx2.SSSx1.p1.2.m2.1c">\otimes</annotation></semantics></math> denotes the element-wise multiplication), this principle describes that low-light observation <math id="Sx2.SSx2.SSSx1.p1.3.m3.1" class="ltx_Math" alttext="\mathbf{L}" display="inline"><semantics id="Sx2.SSx2.SSSx1.p1.3.m3.1a"><mi id="Sx2.SSx2.SSSx1.p1.3.m3.1.1" xref="Sx2.SSx2.SSSx1.p1.3.m3.1.1.cmml">ğ‹</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx2.SSSx1.p1.3.m3.1b"><ci id="Sx2.SSx2.SSSx1.p1.3.m3.1.1.cmml" xref="Sx2.SSx2.SSSx1.p1.3.m3.1.1">ğ‹</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx2.SSSx1.p1.3.m3.1c">\mathbf{L}</annotation></semantics></math> can be decomposed as the normal-light image <math id="Sx2.SSx2.SSSx1.p1.4.m4.1" class="ltx_Math" alttext="\mathbf{R}" display="inline"><semantics id="Sx2.SSx2.SSSx1.p1.4.m4.1a"><mi id="Sx2.SSx2.SSSx1.p1.4.m4.1.1" xref="Sx2.SSx2.SSSx1.p1.4.m4.1.1.cmml">ğ‘</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx2.SSSx1.p1.4.m4.1b"><ci id="Sx2.SSx2.SSSx1.p1.4.m4.1.1.cmml" xref="Sx2.SSx2.SSSx1.p1.4.m4.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx2.SSSx1.p1.4.m4.1c">\mathbf{R}</annotation></semantics></math> (also called reflectance) and the removed illumination <math id="Sx2.SSx2.SSSx1.p1.5.m5.1" class="ltx_Math" alttext="\mathbf{I}" display="inline"><semantics id="Sx2.SSx2.SSSx1.p1.5.m5.1a"><mi id="Sx2.SSx2.SSSx1.p1.5.m5.1.1" xref="Sx2.SSx2.SSSx1.p1.5.m5.1.1.cmml">ğˆ</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx2.SSSx1.p1.5.m5.1b"><ci id="Sx2.SSx2.SSSx1.p1.5.m5.1.1.cmml" xref="Sx2.SSx2.SSSx1.p1.5.m5.1.1">ğˆ</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx2.SSSx1.p1.5.m5.1c">\mathbf{I}</annotation></semantics></math>. We can find a basic fact from this model, i.e., the enhanced result needs to remove the illumination from the low-light observation. This means the enhanced result cannot keep the same information capacity as the original observation and the missing content is exactly the removed illumination based on the information conservation. However, most detectors are established in regular data without information reduction.</p>
</div>
<div id="Sx2.SSx2.SSSx1.p2" class="ltx_para">
<p id="Sx2.SSx2.SSSx1.p2.1" class="ltx_p">From this perspective, the illumination is abandoned for the enhancer (only needs to focus on the normal-light image), but it should be utilized for the detector to improve the information utilization to the maximum extent.</p>
</div>
</section>
<section id="Sx2.SSx2.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Decomposing Scene for Detection</h4>

<div id="Sx2.SSx2.SSSx2.p1" class="ltx_para">
<p id="Sx2.SSx2.SSSx2.p1.1" class="ltx_p">As described above, we know the removed illumination is essential for detection. That is to say, the two decomposed components from low-light observations are equally important for detection, rather than only focusing on the visual quality of the single component in the enhancer. Generally, the low-light observations reflect the scene information including objects and background. After performing Retinex theory, these two components still contain the scene information. Therefore, we call the module for generating two decomposed components as Scene Decomposition Module (SDM).</p>
</div>
<div id="Sx2.SSx2.SSSx2.p2" class="ltx_para">
<p id="Sx2.SSx2.SSSx2.p2.1" class="ltx_p">Here we provide a simple setting for SDM, which consists of three residual blocks and each residual block includes two Conv-BN-ReLU layers. The 1<math id="Sx2.SSx2.SSSx2.p2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="Sx2.SSx2.SSSx2.p2.1.m1.1a"><mo id="Sx2.SSx2.SSSx2.p2.1.m1.1.1" xref="Sx2.SSx2.SSSx2.p2.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="Sx2.SSx2.SSSx2.p2.1.m1.1b"><times id="Sx2.SSx2.SSSx2.p2.1.m1.1.1.cmml" xref="Sx2.SSx2.SSSx2.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx2.SSSx2.p2.1.m1.1c">\times</annotation></semantics></math>1 convolutional layer is used to adjust the number of channels of the feature map at the time of input and output.
Notably, we would like to emphasize that SDM can be initialized by existing Retinex-based enhancer<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Please refer to Sec.Â <a href="#Sx2.SSx5" title="Discussion â€£ Proposed Method â€£ Trash to Treasure: Low-Light Object Detection via Decomposition-and-Aggregation" class="ltx_ref"><span class="ltx_text ltx_ref_title">Discussion</span></a> for experimental supports.</span></span></span>.</p>
</div>
<figure id="Sx2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Quantitative results among different versions for illumination-based enhancers including RUAS and SCI.</figcaption>
<table id="Sx2.T1.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Sx2.T1.6.7.1" class="ltx_tr">
<th id="Sx2.T1.6.7.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding:1.5pt 17.1pt;">Â Â Â Â Â Â <span id="Sx2.T1.6.7.1.1.1" class="ltx_text ltx_font_bold">Method</span></th>
<th id="Sx2.T1.6.7.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding:1.5pt 17.1pt;">Â Â Â Â Â Â <span id="Sx2.T1.6.7.1.2.1" class="ltx_text ltx_font_bold">Description</span></th>
<th id="Sx2.T1.6.7.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding:1.5pt 17.1pt;">Â Â Â Â Â Â <span id="Sx2.T1.6.7.1.3.1" class="ltx_text ltx_font_bold">mAP(%)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Sx2.T1.6.8.1" class="ltx_tr">
<td id="Sx2.T1.6.8.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding:1.5pt 17.1pt;">Â Â Â Â Â Â RUAS</td>
<td id="Sx2.T1.6.8.1.2" class="ltx_td ltx_align_left ltx_border_t" style="padding:1.5pt 17.1pt;">Â Â Â Â Â Â Fine-tune detector</td>
<td id="Sx2.T1.6.8.1.3" class="ltx_td ltx_align_left ltx_border_t" style="padding:1.5pt 17.1pt;">Â Â Â Â Â Â 65.98</td>
</tr>
<tr id="Sx2.T1.1.1" class="ltx_tr">
<td id="Sx2.T1.1.1.1" class="ltx_td ltx_align_left" style="padding:1.5pt 17.1pt;">Â Â Â Â Â Â RUAS<sup id="Sx2.T1.1.1.1.1" class="ltx_sup"><span id="Sx2.T1.1.1.1.1.1" class="ltx_text ltx_font_italic">+</span></sup></td>
<td id="Sx2.T1.1.1.2" class="ltx_td ltx_align_left" style="padding:1.5pt 17.1pt;">Â Â Â Â Â Â Jointly training</td>
<td id="Sx2.T1.1.1.3" class="ltx_td ltx_align_left" style="padding:1.5pt 17.1pt;">Â Â Â Â Â Â 65.70</td>
</tr>
<tr id="Sx2.T1.3.3" class="ltx_tr">
<td id="Sx2.T1.2.2.1" class="ltx_td ltx_align_left" style="padding:1.5pt 17.1pt;">Â Â Â Â Â Â RUAS<sup id="Sx2.T1.2.2.1.1" class="ltx_sup"><span id="Sx2.T1.2.2.1.1.1" class="ltx_text ltx_font_italic">++</span></sup></td>
<td id="Sx2.T1.3.3.3" class="ltx_td ltx_align_left" style="padding:1.5pt 17.1pt;">Â Â Â Â Â Â T2 (SD = RUAS)</td>
<td id="Sx2.T1.3.3.2" class="ltx_td ltx_align_left" style="padding:1.5pt 17.1pt;">Â Â Â Â Â Â 67.36<math id="Sx2.T1.3.3.2.m1.1" class="ltx_Math" alttext="{}_{\color[rgb]{1,0,0}{\uparrow{\textbf{1.66}}}}" display="inline"><semantics id="Sx2.T1.3.3.2.m1.1a"><msub id="Sx2.T1.3.3.2.m1.1.1" xref="Sx2.T1.3.3.2.m1.1.1.cmml"><mi id="Sx2.T1.3.3.2.m1.1.1a" xref="Sx2.T1.3.3.2.m1.1.1.cmml"></mi><mrow id="Sx2.T1.3.3.2.m1.1.1.1" xref="Sx2.T1.3.3.2.m1.1.1.1.cmml"><mi id="Sx2.T1.3.3.2.m1.1.1.1.2" xref="Sx2.T1.3.3.2.m1.1.1.1.2.cmml"></mi><mo mathcolor="#FF0000" stretchy="false" id="Sx2.T1.3.3.2.m1.1.1.1.1" xref="Sx2.T1.3.3.2.m1.1.1.1.1.cmml">â†‘</mo><mtext class="ltx_mathvariant_bold" mathcolor="#FF0000" id="Sx2.T1.3.3.2.m1.1.1.1.3" xref="Sx2.T1.3.3.2.m1.1.1.1.3a.cmml">1.66</mtext></mrow></msub><annotation-xml encoding="MathML-Content" id="Sx2.T1.3.3.2.m1.1b"><apply id="Sx2.T1.3.3.2.m1.1.1.cmml" xref="Sx2.T1.3.3.2.m1.1.1"><apply id="Sx2.T1.3.3.2.m1.1.1.1.cmml" xref="Sx2.T1.3.3.2.m1.1.1.1"><ci id="Sx2.T1.3.3.2.m1.1.1.1.1.cmml" xref="Sx2.T1.3.3.2.m1.1.1.1.1">â†‘</ci><csymbol cd="latexml" id="Sx2.T1.3.3.2.m1.1.1.1.2.cmml" xref="Sx2.T1.3.3.2.m1.1.1.1.2">absent</csymbol><ci id="Sx2.T1.3.3.2.m1.1.1.1.3a.cmml" xref="Sx2.T1.3.3.2.m1.1.1.1.3"><mtext class="ltx_mathvariant_bold" mathsize="70%" id="Sx2.T1.3.3.2.m1.1.1.1.3.cmml" xref="Sx2.T1.3.3.2.m1.1.1.1.3">1.66</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.T1.3.3.2.m1.1c">{}_{\color[rgb]{1,0,0}{\uparrow{\textbf{1.66}}}}</annotation></semantics></math></td>
</tr>
<tr id="Sx2.T1.6.9.2" class="ltx_tr">
<td id="Sx2.T1.6.9.2.1" class="ltx_td ltx_align_left ltx_border_t" style="padding:1.5pt 17.1pt;">Â Â Â Â Â Â SCI</td>
<td id="Sx2.T1.6.9.2.2" class="ltx_td ltx_align_left ltx_border_t" style="padding:1.5pt 17.1pt;">Â Â Â Â Â Â Fine-tune detector</td>
<td id="Sx2.T1.6.9.2.3" class="ltx_td ltx_align_left ltx_border_t" style="padding:1.5pt 17.1pt;">Â Â Â Â Â Â 65.68</td>
</tr>
<tr id="Sx2.T1.4.4" class="ltx_tr">
<td id="Sx2.T1.4.4.1" class="ltx_td ltx_align_left" style="padding:1.5pt 17.1pt;">Â Â Â Â Â Â SCI<sup id="Sx2.T1.4.4.1.1" class="ltx_sup"><span id="Sx2.T1.4.4.1.1.1" class="ltx_text ltx_font_italic">+</span></sup></td>
<td id="Sx2.T1.4.4.2" class="ltx_td ltx_align_left" style="padding:1.5pt 17.1pt;">Â Â Â Â Â Â Jointly training</td>
<td id="Sx2.T1.4.4.3" class="ltx_td ltx_align_left" style="padding:1.5pt 17.1pt;">Â Â Â Â Â Â 65.42</td>
</tr>
<tr id="Sx2.T1.6.6" class="ltx_tr">
<td id="Sx2.T1.5.5.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding:1.5pt 17.1pt;">Â Â Â Â Â Â SCI<sup id="Sx2.T1.5.5.1.1" class="ltx_sup"><span id="Sx2.T1.5.5.1.1.1" class="ltx_text ltx_font_italic">++</span></sup></td>
<td id="Sx2.T1.6.6.3" class="ltx_td ltx_align_left ltx_border_bb" style="padding:1.5pt 17.1pt;">Â Â Â Â Â Â T2 (SD = SCI)</td>
<td id="Sx2.T1.6.6.2" class="ltx_td ltx_align_left ltx_border_bb" style="padding:1.5pt 17.1pt;">Â Â Â Â Â Â 68.33<math id="Sx2.T1.6.6.2.m1.1" class="ltx_Math" alttext="{}_{\color[rgb]{1,0,0}{\uparrow{\textbf{2.91}}}}" display="inline"><semantics id="Sx2.T1.6.6.2.m1.1a"><msub id="Sx2.T1.6.6.2.m1.1.1" xref="Sx2.T1.6.6.2.m1.1.1.cmml"><mi id="Sx2.T1.6.6.2.m1.1.1a" xref="Sx2.T1.6.6.2.m1.1.1.cmml"></mi><mrow id="Sx2.T1.6.6.2.m1.1.1.1" xref="Sx2.T1.6.6.2.m1.1.1.1.cmml"><mi id="Sx2.T1.6.6.2.m1.1.1.1.2" xref="Sx2.T1.6.6.2.m1.1.1.1.2.cmml"></mi><mo mathcolor="#FF0000" stretchy="false" id="Sx2.T1.6.6.2.m1.1.1.1.1" xref="Sx2.T1.6.6.2.m1.1.1.1.1.cmml">â†‘</mo><mtext class="ltx_mathvariant_bold" mathcolor="#FF0000" id="Sx2.T1.6.6.2.m1.1.1.1.3" xref="Sx2.T1.6.6.2.m1.1.1.1.3a.cmml">2.91</mtext></mrow></msub><annotation-xml encoding="MathML-Content" id="Sx2.T1.6.6.2.m1.1b"><apply id="Sx2.T1.6.6.2.m1.1.1.cmml" xref="Sx2.T1.6.6.2.m1.1.1"><apply id="Sx2.T1.6.6.2.m1.1.1.1.cmml" xref="Sx2.T1.6.6.2.m1.1.1.1"><ci id="Sx2.T1.6.6.2.m1.1.1.1.1.cmml" xref="Sx2.T1.6.6.2.m1.1.1.1.1">â†‘</ci><csymbol cd="latexml" id="Sx2.T1.6.6.2.m1.1.1.1.2.cmml" xref="Sx2.T1.6.6.2.m1.1.1.1.2">absent</csymbol><ci id="Sx2.T1.6.6.2.m1.1.1.1.3a.cmml" xref="Sx2.T1.6.6.2.m1.1.1.1.3"><mtext class="ltx_mathvariant_bold" mathsize="70%" id="Sx2.T1.6.6.2.m1.1.1.1.3.cmml" xref="Sx2.T1.6.6.2.m1.1.1.1.3">2.91</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.T1.6.6.2.m1.1c">{}_{\color[rgb]{1,0,0}{\uparrow{\textbf{2.91}}}}</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="Sx2.SSx3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Semantic Aggregation Module</h3>

<div id="Sx2.SSx3.p1" class="ltx_para">
<p id="Sx2.SSx3.p1.1" class="ltx_p">The previous section has generated two decomposed components by scene decomposition, the next issue is how to exploit them for the detector. Here we build a semantic aggregation module that consists of a weigh-sharing feature extractor and a multi-scale feature aggregator.</p>
</div>
<section id="Sx2.SSx3.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Weight-Sharing Feature Extractor</h4>

<div id="Sx2.SSx3.SSSx1.p1" class="ltx_para">
<p id="Sx2.SSx3.SSSx1.p1.1" class="ltx_p">Our defined scene decomposition module aims at generating two decomposed components to perform scene information. Although they are acquired based on the knowledge from low-light image enhancement, their output status should be related to the detection accuracy. Here we adopt an VGG-16 used in DFSDÂ <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al. <a href="#bib.bib11" title="" class="ltx_ref">2019</a>)</cite> and S3FDÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al. <a href="#bib.bib46" title="" class="ltx_ref">2017</a>)</cite> as its basic architecture. We use the weight-sharing backbone network to extract the features of the two decomposed components.</p>
</div>
</section>
<section id="Sx2.SSx3.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Multi-Scale Feature Aggregator</h4>

<div id="Sx2.SSx3.SSSx2.p1" class="ltx_para">
<p id="Sx2.SSx3.SSSx2.p1.15" class="ltx_p">We know that the feature pyramid networkÂ <cite class="ltx_cite ltx_citemacro_citep">(Lin etÂ al. <a href="#bib.bib13" title="" class="ltx_ref">2017</a>)</cite> is a commonly-used structure for the detector, which can improve the detection accuracy, especially the extremely small objects.
Through our above-built weight-sharing feature extractor, we can obtain two groups of multi-scale scene-related semantic features in the context space. To effectively integrate them, we define a multi-scale feature aggregator by introducing the Retinex knowledge into the feature pyramid network. This process can be formulated as</p>
<table id="Sx2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="Sx2.E1.m1.9" class="ltx_math_unparsed" alttext="\left\{\begin{aligned} \mathcal{F}_{8(2)}&amp;=\mathcal{F}^{\mathbf{I}}_{8(2)}\otimes\mathcal{F}^{\mathbf{R}}_{8(2)},\\
\mathcal{F}_{a(b)}&amp;=\mathcal{F}^{\mathbf{I}}_{a(b_{a})}\otimes\mathcal{F}^{\mathbf{R}}_{a(b_{a})}+(\mathcal{F}_{a+1(b_{a+1})})_{\uparrow},\\
\end{aligned}\right." display="block"><semantics id="Sx2.E1.m1.9a"><mrow id="Sx2.E1.m1.9b"><mo id="Sx2.E1.m1.9.10">{</mo><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt" id="Sx2.E1.m1.9.9"><mtr id="Sx2.E1.m1.9.9a"><mtd class="ltx_align_right" columnalign="right" id="Sx2.E1.m1.9.9b"><msub id="Sx2.E1.m1.1.1.1.1.1"><mi class="ltx_font_mathcaligraphic" id="Sx2.E1.m1.1.1.1.1.1.3">â„±</mi><mrow id="Sx2.E1.m1.1.1.1.1.1.1.1"><mn id="Sx2.E1.m1.1.1.1.1.1.1.1.3">8</mn><mo lspace="0em" rspace="0em" id="Sx2.E1.m1.1.1.1.1.1.1.1.2">â€‹</mo><mrow id="Sx2.E1.m1.1.1.1.1.1.1.1.4.2"><mo stretchy="false" id="Sx2.E1.m1.1.1.1.1.1.1.1.4.2.1">(</mo><mn id="Sx2.E1.m1.1.1.1.1.1.1.1.1">2</mn><mo stretchy="false" id="Sx2.E1.m1.1.1.1.1.1.1.1.4.2.2">)</mo></mrow></mrow></msub></mtd><mtd class="ltx_align_left" columnalign="left" id="Sx2.E1.m1.9.9c"><mrow id="Sx2.E1.m1.4.4.4.4.3.3"><mrow id="Sx2.E1.m1.4.4.4.4.3.3.1"><mi id="Sx2.E1.m1.4.4.4.4.3.3.1.2"></mi><mo id="Sx2.E1.m1.4.4.4.4.3.3.1.1">=</mo><mrow id="Sx2.E1.m1.4.4.4.4.3.3.1.3"><msubsup id="Sx2.E1.m1.4.4.4.4.3.3.1.3.2"><mi class="ltx_font_mathcaligraphic" id="Sx2.E1.m1.4.4.4.4.3.3.1.3.2.2.2">â„±</mi><mrow id="Sx2.E1.m1.2.2.2.2.1.1.1"><mn id="Sx2.E1.m1.2.2.2.2.1.1.1.3">8</mn><mo lspace="0em" rspace="0em" id="Sx2.E1.m1.2.2.2.2.1.1.1.2">â€‹</mo><mrow id="Sx2.E1.m1.2.2.2.2.1.1.1.4.2"><mo stretchy="false" id="Sx2.E1.m1.2.2.2.2.1.1.1.4.2.1">(</mo><mn id="Sx2.E1.m1.2.2.2.2.1.1.1.1">2</mn><mo stretchy="false" id="Sx2.E1.m1.2.2.2.2.1.1.1.4.2.2">)</mo></mrow></mrow><mi id="Sx2.E1.m1.4.4.4.4.3.3.1.3.2.2.3">ğˆ</mi></msubsup><mo lspace="0.222em" rspace="0.222em" id="Sx2.E1.m1.4.4.4.4.3.3.1.3.1">âŠ—</mo><msubsup id="Sx2.E1.m1.4.4.4.4.3.3.1.3.3"><mi class="ltx_font_mathcaligraphic" id="Sx2.E1.m1.4.4.4.4.3.3.1.3.3.2.2">â„±</mi><mrow id="Sx2.E1.m1.3.3.3.3.2.2.1"><mn id="Sx2.E1.m1.3.3.3.3.2.2.1.3">8</mn><mo lspace="0em" rspace="0em" id="Sx2.E1.m1.3.3.3.3.2.2.1.2">â€‹</mo><mrow id="Sx2.E1.m1.3.3.3.3.2.2.1.4.2"><mo stretchy="false" id="Sx2.E1.m1.3.3.3.3.2.2.1.4.2.1">(</mo><mn id="Sx2.E1.m1.3.3.3.3.2.2.1.1">2</mn><mo stretchy="false" id="Sx2.E1.m1.3.3.3.3.2.2.1.4.2.2">)</mo></mrow></mrow><mi id="Sx2.E1.m1.4.4.4.4.3.3.1.3.3.2.3">ğ‘</mi></msubsup></mrow></mrow><mo id="Sx2.E1.m1.4.4.4.4.3.3.2">,</mo></mrow></mtd></mtr><mtr id="Sx2.E1.m1.9.9d"><mtd class="ltx_align_right" columnalign="right" id="Sx2.E1.m1.9.9e"><msub id="Sx2.E1.m1.5.5.5.1.1"><mi class="ltx_font_mathcaligraphic" id="Sx2.E1.m1.5.5.5.1.1.3">â„±</mi><mrow id="Sx2.E1.m1.5.5.5.1.1.1.1"><mi id="Sx2.E1.m1.5.5.5.1.1.1.1.3">a</mi><mo lspace="0em" rspace="0em" id="Sx2.E1.m1.5.5.5.1.1.1.1.2">â€‹</mo><mrow id="Sx2.E1.m1.5.5.5.1.1.1.1.4.2"><mo stretchy="false" id="Sx2.E1.m1.5.5.5.1.1.1.1.4.2.1">(</mo><mi id="Sx2.E1.m1.5.5.5.1.1.1.1.1">b</mi><mo stretchy="false" id="Sx2.E1.m1.5.5.5.1.1.1.1.4.2.2">)</mo></mrow></mrow></msub></mtd><mtd class="ltx_align_left" columnalign="left" id="Sx2.E1.m1.9.9f"><mrow id="Sx2.E1.m1.9.9.9.5.4.4"><mrow id="Sx2.E1.m1.9.9.9.5.4.4.1"><mi id="Sx2.E1.m1.9.9.9.5.4.4.1.3"></mi><mo id="Sx2.E1.m1.9.9.9.5.4.4.1.2">=</mo><mrow id="Sx2.E1.m1.9.9.9.5.4.4.1.1"><mrow id="Sx2.E1.m1.9.9.9.5.4.4.1.1.3"><msubsup id="Sx2.E1.m1.9.9.9.5.4.4.1.1.3.2"><mi class="ltx_font_mathcaligraphic" id="Sx2.E1.m1.9.9.9.5.4.4.1.1.3.2.2.2">â„±</mi><mrow id="Sx2.E1.m1.6.6.6.2.1.1.1"><mi id="Sx2.E1.m1.6.6.6.2.1.1.1.3">a</mi><mo lspace="0em" rspace="0em" id="Sx2.E1.m1.6.6.6.2.1.1.1.2">â€‹</mo><mrow id="Sx2.E1.m1.6.6.6.2.1.1.1.1.1"><mo stretchy="false" id="Sx2.E1.m1.6.6.6.2.1.1.1.1.1.2">(</mo><msub id="Sx2.E1.m1.6.6.6.2.1.1.1.1.1.1"><mi id="Sx2.E1.m1.6.6.6.2.1.1.1.1.1.1.2">b</mi><mi id="Sx2.E1.m1.6.6.6.2.1.1.1.1.1.1.3">a</mi></msub><mo stretchy="false" id="Sx2.E1.m1.6.6.6.2.1.1.1.1.1.3">)</mo></mrow></mrow><mi id="Sx2.E1.m1.9.9.9.5.4.4.1.1.3.2.2.3">ğˆ</mi></msubsup><mo lspace="0.222em" rspace="0.222em" id="Sx2.E1.m1.9.9.9.5.4.4.1.1.3.1">âŠ—</mo><msubsup id="Sx2.E1.m1.9.9.9.5.4.4.1.1.3.3"><mi class="ltx_font_mathcaligraphic" id="Sx2.E1.m1.9.9.9.5.4.4.1.1.3.3.2.2">â„±</mi><mrow id="Sx2.E1.m1.7.7.7.3.2.2.1"><mi id="Sx2.E1.m1.7.7.7.3.2.2.1.3">a</mi><mo lspace="0em" rspace="0em" id="Sx2.E1.m1.7.7.7.3.2.2.1.2">â€‹</mo><mrow id="Sx2.E1.m1.7.7.7.3.2.2.1.1.1"><mo stretchy="false" id="Sx2.E1.m1.7.7.7.3.2.2.1.1.1.2">(</mo><msub id="Sx2.E1.m1.7.7.7.3.2.2.1.1.1.1"><mi id="Sx2.E1.m1.7.7.7.3.2.2.1.1.1.1.2">b</mi><mi id="Sx2.E1.m1.7.7.7.3.2.2.1.1.1.1.3">a</mi></msub><mo stretchy="false" id="Sx2.E1.m1.7.7.7.3.2.2.1.1.1.3">)</mo></mrow></mrow><mi id="Sx2.E1.m1.9.9.9.5.4.4.1.1.3.3.2.3">ğ‘</mi></msubsup></mrow><mo id="Sx2.E1.m1.9.9.9.5.4.4.1.1.2">+</mo><msub id="Sx2.E1.m1.9.9.9.5.4.4.1.1.1"><mrow id="Sx2.E1.m1.9.9.9.5.4.4.1.1.1.1.1"><mo stretchy="false" id="Sx2.E1.m1.9.9.9.5.4.4.1.1.1.1.1.2">(</mo><msub id="Sx2.E1.m1.9.9.9.5.4.4.1.1.1.1.1.1"><mi class="ltx_font_mathcaligraphic" id="Sx2.E1.m1.9.9.9.5.4.4.1.1.1.1.1.1.2">â„±</mi><mrow id="Sx2.E1.m1.8.8.8.4.3.3.1"><mi id="Sx2.E1.m1.8.8.8.4.3.3.1.3">a</mi><mo id="Sx2.E1.m1.8.8.8.4.3.3.1.2">+</mo><mrow id="Sx2.E1.m1.8.8.8.4.3.3.1.1"><mn id="Sx2.E1.m1.8.8.8.4.3.3.1.1.3">1</mn><mo lspace="0em" rspace="0em" id="Sx2.E1.m1.8.8.8.4.3.3.1.1.2">â€‹</mo><mrow id="Sx2.E1.m1.8.8.8.4.3.3.1.1.1.1"><mo stretchy="false" id="Sx2.E1.m1.8.8.8.4.3.3.1.1.1.1.2">(</mo><msub id="Sx2.E1.m1.8.8.8.4.3.3.1.1.1.1.1"><mi id="Sx2.E1.m1.8.8.8.4.3.3.1.1.1.1.1.2">b</mi><mrow id="Sx2.E1.m1.8.8.8.4.3.3.1.1.1.1.1.3"><mi id="Sx2.E1.m1.8.8.8.4.3.3.1.1.1.1.1.3.2">a</mi><mo id="Sx2.E1.m1.8.8.8.4.3.3.1.1.1.1.1.3.1">+</mo><mn id="Sx2.E1.m1.8.8.8.4.3.3.1.1.1.1.1.3.3">1</mn></mrow></msub><mo stretchy="false" id="Sx2.E1.m1.8.8.8.4.3.3.1.1.1.1.3">)</mo></mrow></mrow></mrow></msub><mo stretchy="false" id="Sx2.E1.m1.9.9.9.5.4.4.1.1.1.1.1.3">)</mo></mrow><mo stretchy="false" id="Sx2.E1.m1.9.9.9.5.4.4.1.1.1.3">â†‘</mo></msub></mrow></mrow><mo id="Sx2.E1.m1.9.9.9.5.4.4.2">,</mo></mrow></mtd></mtr></mtable></mrow><annotation encoding="application/x-tex" id="Sx2.E1.m1.9c">\left\{\begin{aligned} \mathcal{F}_{8(2)}&amp;=\mathcal{F}^{\mathbf{I}}_{8(2)}\otimes\mathcal{F}^{\mathbf{R}}_{8(2)},\\
\mathcal{F}_{a(b)}&amp;=\mathcal{F}^{\mathbf{I}}_{a(b_{a})}\otimes\mathcal{F}^{\mathbf{R}}_{a(b_{a})}+(\mathcal{F}_{a+1(b_{a+1})})_{\uparrow},\\
\end{aligned}\right.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="Sx2.SSx3.SSSx2.p1.14" class="ltx_p">where <math id="Sx2.SSx3.SSSx2.p1.1.m1.4" class="ltx_Math" alttext="3\leq a\leq 7,b\in\{2,3\}" display="inline"><semantics id="Sx2.SSx3.SSSx2.p1.1.m1.4a"><mrow id="Sx2.SSx3.SSSx2.p1.1.m1.4.4.2" xref="Sx2.SSx3.SSSx2.p1.1.m1.4.4.3.cmml"><mrow id="Sx2.SSx3.SSSx2.p1.1.m1.3.3.1.1" xref="Sx2.SSx3.SSSx2.p1.1.m1.3.3.1.1.cmml"><mn id="Sx2.SSx3.SSSx2.p1.1.m1.3.3.1.1.2" xref="Sx2.SSx3.SSSx2.p1.1.m1.3.3.1.1.2.cmml">3</mn><mo id="Sx2.SSx3.SSSx2.p1.1.m1.3.3.1.1.3" xref="Sx2.SSx3.SSSx2.p1.1.m1.3.3.1.1.3.cmml">â‰¤</mo><mi id="Sx2.SSx3.SSSx2.p1.1.m1.3.3.1.1.4" xref="Sx2.SSx3.SSSx2.p1.1.m1.3.3.1.1.4.cmml">a</mi><mo id="Sx2.SSx3.SSSx2.p1.1.m1.3.3.1.1.5" xref="Sx2.SSx3.SSSx2.p1.1.m1.3.3.1.1.5.cmml">â‰¤</mo><mn id="Sx2.SSx3.SSSx2.p1.1.m1.3.3.1.1.6" xref="Sx2.SSx3.SSSx2.p1.1.m1.3.3.1.1.6.cmml">7</mn></mrow><mo id="Sx2.SSx3.SSSx2.p1.1.m1.4.4.2.3" xref="Sx2.SSx3.SSSx2.p1.1.m1.4.4.3a.cmml">,</mo><mrow id="Sx2.SSx3.SSSx2.p1.1.m1.4.4.2.2" xref="Sx2.SSx3.SSSx2.p1.1.m1.4.4.2.2.cmml"><mi id="Sx2.SSx3.SSSx2.p1.1.m1.4.4.2.2.2" xref="Sx2.SSx3.SSSx2.p1.1.m1.4.4.2.2.2.cmml">b</mi><mo id="Sx2.SSx3.SSSx2.p1.1.m1.4.4.2.2.1" xref="Sx2.SSx3.SSSx2.p1.1.m1.4.4.2.2.1.cmml">âˆˆ</mo><mrow id="Sx2.SSx3.SSSx2.p1.1.m1.4.4.2.2.3.2" xref="Sx2.SSx3.SSSx2.p1.1.m1.4.4.2.2.3.1.cmml"><mo stretchy="false" id="Sx2.SSx3.SSSx2.p1.1.m1.4.4.2.2.3.2.1" xref="Sx2.SSx3.SSSx2.p1.1.m1.4.4.2.2.3.1.cmml">{</mo><mn id="Sx2.SSx3.SSSx2.p1.1.m1.1.1" xref="Sx2.SSx3.SSSx2.p1.1.m1.1.1.cmml">2</mn><mo id="Sx2.SSx3.SSSx2.p1.1.m1.4.4.2.2.3.2.2" xref="Sx2.SSx3.SSSx2.p1.1.m1.4.4.2.2.3.1.cmml">,</mo><mn id="Sx2.SSx3.SSSx2.p1.1.m1.2.2" xref="Sx2.SSx3.SSSx2.p1.1.m1.2.2.cmml">3</mn><mo stretchy="false" id="Sx2.SSx3.SSSx2.p1.1.m1.4.4.2.2.3.2.3" xref="Sx2.SSx3.SSSx2.p1.1.m1.4.4.2.2.3.1.cmml">}</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx2.SSx3.SSSx2.p1.1.m1.4b"><apply id="Sx2.SSx3.SSSx2.p1.1.m1.4.4.3.cmml" xref="Sx2.SSx3.SSSx2.p1.1.m1.4.4.2"><csymbol cd="ambiguous" id="Sx2.SSx3.SSSx2.p1.1.m1.4.4.3a.cmml" xref="Sx2.SSx3.SSSx2.p1.1.m1.4.4.2.3">formulae-sequence</csymbol><apply id="Sx2.SSx3.SSSx2.p1.1.m1.3.3.1.1.cmml" xref="Sx2.SSx3.SSSx2.p1.1.m1.3.3.1.1"><and id="Sx2.SSx3.SSSx2.p1.1.m1.3.3.1.1a.cmml" xref="Sx2.SSx3.SSSx2.p1.1.m1.3.3.1.1"></and><apply id="Sx2.SSx3.SSSx2.p1.1.m1.3.3.1.1b.cmml" xref="Sx2.SSx3.SSSx2.p1.1.m1.3.3.1.1"><leq id="Sx2.SSx3.SSSx2.p1.1.m1.3.3.1.1.3.cmml" xref="Sx2.SSx3.SSSx2.p1.1.m1.3.3.1.1.3"></leq><cn type="integer" id="Sx2.SSx3.SSSx2.p1.1.m1.3.3.1.1.2.cmml" xref="Sx2.SSx3.SSSx2.p1.1.m1.3.3.1.1.2">3</cn><ci id="Sx2.SSx3.SSSx2.p1.1.m1.3.3.1.1.4.cmml" xref="Sx2.SSx3.SSSx2.p1.1.m1.3.3.1.1.4">ğ‘</ci></apply><apply id="Sx2.SSx3.SSSx2.p1.1.m1.3.3.1.1c.cmml" xref="Sx2.SSx3.SSSx2.p1.1.m1.3.3.1.1"><leq id="Sx2.SSx3.SSSx2.p1.1.m1.3.3.1.1.5.cmml" xref="Sx2.SSx3.SSSx2.p1.1.m1.3.3.1.1.5"></leq><share href="#Sx2.SSx3.SSSx2.p1.1.m1.3.3.1.1.4.cmml" id="Sx2.SSx3.SSSx2.p1.1.m1.3.3.1.1d.cmml" xref="Sx2.SSx3.SSSx2.p1.1.m1.3.3.1.1"></share><cn type="integer" id="Sx2.SSx3.SSSx2.p1.1.m1.3.3.1.1.6.cmml" xref="Sx2.SSx3.SSSx2.p1.1.m1.3.3.1.1.6">7</cn></apply></apply><apply id="Sx2.SSx3.SSSx2.p1.1.m1.4.4.2.2.cmml" xref="Sx2.SSx3.SSSx2.p1.1.m1.4.4.2.2"><in id="Sx2.SSx3.SSSx2.p1.1.m1.4.4.2.2.1.cmml" xref="Sx2.SSx3.SSSx2.p1.1.m1.4.4.2.2.1"></in><ci id="Sx2.SSx3.SSSx2.p1.1.m1.4.4.2.2.2.cmml" xref="Sx2.SSx3.SSSx2.p1.1.m1.4.4.2.2.2">ğ‘</ci><set id="Sx2.SSx3.SSSx2.p1.1.m1.4.4.2.2.3.1.cmml" xref="Sx2.SSx3.SSSx2.p1.1.m1.4.4.2.2.3.2"><cn type="integer" id="Sx2.SSx3.SSSx2.p1.1.m1.1.1.cmml" xref="Sx2.SSx3.SSSx2.p1.1.m1.1.1">2</cn><cn type="integer" id="Sx2.SSx3.SSSx2.p1.1.m1.2.2.cmml" xref="Sx2.SSx3.SSSx2.p1.1.m1.2.2">3</cn></set></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx3.SSSx2.p1.1.m1.4c">3\leq a\leq 7,b\in\{2,3\}</annotation></semantics></math>. <math id="Sx2.SSx3.SSSx2.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{F}^{\mathbf{I}}" display="inline"><semantics id="Sx2.SSx3.SSSx2.p1.2.m2.1a"><msup id="Sx2.SSx3.SSSx2.p1.2.m2.1.1" xref="Sx2.SSx3.SSSx2.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx2.SSx3.SSSx2.p1.2.m2.1.1.2" xref="Sx2.SSx3.SSSx2.p1.2.m2.1.1.2.cmml">â„±</mi><mi id="Sx2.SSx3.SSSx2.p1.2.m2.1.1.3" xref="Sx2.SSx3.SSSx2.p1.2.m2.1.1.3.cmml">ğˆ</mi></msup><annotation-xml encoding="MathML-Content" id="Sx2.SSx3.SSSx2.p1.2.m2.1b"><apply id="Sx2.SSx3.SSSx2.p1.2.m2.1.1.cmml" xref="Sx2.SSx3.SSSx2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="Sx2.SSx3.SSSx2.p1.2.m2.1.1.1.cmml" xref="Sx2.SSx3.SSSx2.p1.2.m2.1.1">superscript</csymbol><ci id="Sx2.SSx3.SSSx2.p1.2.m2.1.1.2.cmml" xref="Sx2.SSx3.SSSx2.p1.2.m2.1.1.2">â„±</ci><ci id="Sx2.SSx3.SSSx2.p1.2.m2.1.1.3.cmml" xref="Sx2.SSx3.SSSx2.p1.2.m2.1.1.3">ğˆ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx3.SSSx2.p1.2.m2.1c">\mathcal{F}^{\mathbf{I}}</annotation></semantics></math> and <math id="Sx2.SSx3.SSSx2.p1.3.m3.1" class="ltx_Math" alttext="\mathcal{F}^{\mathbf{R}}" display="inline"><semantics id="Sx2.SSx3.SSSx2.p1.3.m3.1a"><msup id="Sx2.SSx3.SSSx2.p1.3.m3.1.1" xref="Sx2.SSx3.SSSx2.p1.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx2.SSx3.SSSx2.p1.3.m3.1.1.2" xref="Sx2.SSx3.SSSx2.p1.3.m3.1.1.2.cmml">â„±</mi><mi id="Sx2.SSx3.SSSx2.p1.3.m3.1.1.3" xref="Sx2.SSx3.SSSx2.p1.3.m3.1.1.3.cmml">ğ‘</mi></msup><annotation-xml encoding="MathML-Content" id="Sx2.SSx3.SSSx2.p1.3.m3.1b"><apply id="Sx2.SSx3.SSSx2.p1.3.m3.1.1.cmml" xref="Sx2.SSx3.SSSx2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="Sx2.SSx3.SSSx2.p1.3.m3.1.1.1.cmml" xref="Sx2.SSx3.SSSx2.p1.3.m3.1.1">superscript</csymbol><ci id="Sx2.SSx3.SSSx2.p1.3.m3.1.1.2.cmml" xref="Sx2.SSx3.SSSx2.p1.3.m3.1.1.2">â„±</ci><ci id="Sx2.SSx3.SSSx2.p1.3.m3.1.1.3.cmml" xref="Sx2.SSx3.SSSx2.p1.3.m3.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx3.SSSx2.p1.3.m3.1c">\mathcal{F}^{\mathbf{R}}</annotation></semantics></math> represent the generated features from the feature extractor according to illumination and reflectance, respectively. <math id="Sx2.SSx3.SSSx2.p1.4.m4.1" class="ltx_Math" alttext="\mathcal{F}_{a(b)}" display="inline"><semantics id="Sx2.SSx3.SSSx2.p1.4.m4.1a"><msub id="Sx2.SSx3.SSSx2.p1.4.m4.1.2" xref="Sx2.SSx3.SSSx2.p1.4.m4.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx2.SSx3.SSSx2.p1.4.m4.1.2.2" xref="Sx2.SSx3.SSSx2.p1.4.m4.1.2.2.cmml">â„±</mi><mrow id="Sx2.SSx3.SSSx2.p1.4.m4.1.1.1" xref="Sx2.SSx3.SSSx2.p1.4.m4.1.1.1.cmml"><mi id="Sx2.SSx3.SSSx2.p1.4.m4.1.1.1.3" xref="Sx2.SSx3.SSSx2.p1.4.m4.1.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="Sx2.SSx3.SSSx2.p1.4.m4.1.1.1.2" xref="Sx2.SSx3.SSSx2.p1.4.m4.1.1.1.2.cmml">â€‹</mo><mrow id="Sx2.SSx3.SSSx2.p1.4.m4.1.1.1.4.2" xref="Sx2.SSx3.SSSx2.p1.4.m4.1.1.1.cmml"><mo stretchy="false" id="Sx2.SSx3.SSSx2.p1.4.m4.1.1.1.4.2.1" xref="Sx2.SSx3.SSSx2.p1.4.m4.1.1.1.cmml">(</mo><mi id="Sx2.SSx3.SSSx2.p1.4.m4.1.1.1.1" xref="Sx2.SSx3.SSSx2.p1.4.m4.1.1.1.1.cmml">b</mi><mo stretchy="false" id="Sx2.SSx3.SSSx2.p1.4.m4.1.1.1.4.2.2" xref="Sx2.SSx3.SSSx2.p1.4.m4.1.1.1.cmml">)</mo></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="Sx2.SSx3.SSSx2.p1.4.m4.1b"><apply id="Sx2.SSx3.SSSx2.p1.4.m4.1.2.cmml" xref="Sx2.SSx3.SSSx2.p1.4.m4.1.2"><csymbol cd="ambiguous" id="Sx2.SSx3.SSSx2.p1.4.m4.1.2.1.cmml" xref="Sx2.SSx3.SSSx2.p1.4.m4.1.2">subscript</csymbol><ci id="Sx2.SSx3.SSSx2.p1.4.m4.1.2.2.cmml" xref="Sx2.SSx3.SSSx2.p1.4.m4.1.2.2">â„±</ci><apply id="Sx2.SSx3.SSSx2.p1.4.m4.1.1.1.cmml" xref="Sx2.SSx3.SSSx2.p1.4.m4.1.1.1"><times id="Sx2.SSx3.SSSx2.p1.4.m4.1.1.1.2.cmml" xref="Sx2.SSx3.SSSx2.p1.4.m4.1.1.1.2"></times><ci id="Sx2.SSx3.SSSx2.p1.4.m4.1.1.1.3.cmml" xref="Sx2.SSx3.SSSx2.p1.4.m4.1.1.1.3">ğ‘</ci><ci id="Sx2.SSx3.SSSx2.p1.4.m4.1.1.1.1.cmml" xref="Sx2.SSx3.SSSx2.p1.4.m4.1.1.1.1">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx3.SSSx2.p1.4.m4.1c">\mathcal{F}_{a(b)}</annotation></semantics></math> denotes the feature generated in <math id="Sx2.SSx3.SSSx2.p1.5.m5.1" class="ltx_Math" alttext="b" display="inline"><semantics id="Sx2.SSx3.SSSx2.p1.5.m5.1a"><mi id="Sx2.SSx3.SSSx2.p1.5.m5.1.1" xref="Sx2.SSx3.SSSx2.p1.5.m5.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx3.SSSx2.p1.5.m5.1b"><ci id="Sx2.SSx3.SSSx2.p1.5.m5.1.1.cmml" xref="Sx2.SSx3.SSSx2.p1.5.m5.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx3.SSSx2.p1.5.m5.1c">b</annotation></semantics></math>-th convolutional layer in the <math id="Sx2.SSx3.SSSx2.p1.6.m6.1" class="ltx_Math" alttext="a" display="inline"><semantics id="Sx2.SSx3.SSSx2.p1.6.m6.1a"><mi id="Sx2.SSx3.SSSx2.p1.6.m6.1.1" xref="Sx2.SSx3.SSSx2.p1.6.m6.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx3.SSSx2.p1.6.m6.1b"><ci id="Sx2.SSx3.SSSx2.p1.6.m6.1.1.cmml" xref="Sx2.SSx3.SSSx2.p1.6.m6.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx3.SSSx2.p1.6.m6.1c">a</annotation></semantics></math>-th convolution block. We are utilizing six layers of features: <math id="Sx2.SSx3.SSSx2.p1.7.m7.1" class="ltx_Math" alttext="\mathcal{F}_{3(3)}" display="inline"><semantics id="Sx2.SSx3.SSSx2.p1.7.m7.1a"><msub id="Sx2.SSx3.SSSx2.p1.7.m7.1.2" xref="Sx2.SSx3.SSSx2.p1.7.m7.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx2.SSx3.SSSx2.p1.7.m7.1.2.2" xref="Sx2.SSx3.SSSx2.p1.7.m7.1.2.2.cmml">â„±</mi><mrow id="Sx2.SSx3.SSSx2.p1.7.m7.1.1.1" xref="Sx2.SSx3.SSSx2.p1.7.m7.1.1.1.cmml"><mn id="Sx2.SSx3.SSSx2.p1.7.m7.1.1.1.3" xref="Sx2.SSx3.SSSx2.p1.7.m7.1.1.1.3.cmml">3</mn><mo lspace="0em" rspace="0em" id="Sx2.SSx3.SSSx2.p1.7.m7.1.1.1.2" xref="Sx2.SSx3.SSSx2.p1.7.m7.1.1.1.2.cmml">â€‹</mo><mrow id="Sx2.SSx3.SSSx2.p1.7.m7.1.1.1.4.2" xref="Sx2.SSx3.SSSx2.p1.7.m7.1.1.1.cmml"><mo stretchy="false" id="Sx2.SSx3.SSSx2.p1.7.m7.1.1.1.4.2.1" xref="Sx2.SSx3.SSSx2.p1.7.m7.1.1.1.cmml">(</mo><mn id="Sx2.SSx3.SSSx2.p1.7.m7.1.1.1.1" xref="Sx2.SSx3.SSSx2.p1.7.m7.1.1.1.1.cmml">3</mn><mo stretchy="false" id="Sx2.SSx3.SSSx2.p1.7.m7.1.1.1.4.2.2" xref="Sx2.SSx3.SSSx2.p1.7.m7.1.1.1.cmml">)</mo></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="Sx2.SSx3.SSSx2.p1.7.m7.1b"><apply id="Sx2.SSx3.SSSx2.p1.7.m7.1.2.cmml" xref="Sx2.SSx3.SSSx2.p1.7.m7.1.2"><csymbol cd="ambiguous" id="Sx2.SSx3.SSSx2.p1.7.m7.1.2.1.cmml" xref="Sx2.SSx3.SSSx2.p1.7.m7.1.2">subscript</csymbol><ci id="Sx2.SSx3.SSSx2.p1.7.m7.1.2.2.cmml" xref="Sx2.SSx3.SSSx2.p1.7.m7.1.2.2">â„±</ci><apply id="Sx2.SSx3.SSSx2.p1.7.m7.1.1.1.cmml" xref="Sx2.SSx3.SSSx2.p1.7.m7.1.1.1"><times id="Sx2.SSx3.SSSx2.p1.7.m7.1.1.1.2.cmml" xref="Sx2.SSx3.SSSx2.p1.7.m7.1.1.1.2"></times><cn type="integer" id="Sx2.SSx3.SSSx2.p1.7.m7.1.1.1.3.cmml" xref="Sx2.SSx3.SSSx2.p1.7.m7.1.1.1.3">3</cn><cn type="integer" id="Sx2.SSx3.SSSx2.p1.7.m7.1.1.1.1.cmml" xref="Sx2.SSx3.SSSx2.p1.7.m7.1.1.1.1">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx3.SSSx2.p1.7.m7.1c">\mathcal{F}_{3(3)}</annotation></semantics></math>, <math id="Sx2.SSx3.SSSx2.p1.8.m8.1" class="ltx_Math" alttext="\mathcal{F}_{4(3)}" display="inline"><semantics id="Sx2.SSx3.SSSx2.p1.8.m8.1a"><msub id="Sx2.SSx3.SSSx2.p1.8.m8.1.2" xref="Sx2.SSx3.SSSx2.p1.8.m8.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx2.SSx3.SSSx2.p1.8.m8.1.2.2" xref="Sx2.SSx3.SSSx2.p1.8.m8.1.2.2.cmml">â„±</mi><mrow id="Sx2.SSx3.SSSx2.p1.8.m8.1.1.1" xref="Sx2.SSx3.SSSx2.p1.8.m8.1.1.1.cmml"><mn id="Sx2.SSx3.SSSx2.p1.8.m8.1.1.1.3" xref="Sx2.SSx3.SSSx2.p1.8.m8.1.1.1.3.cmml">4</mn><mo lspace="0em" rspace="0em" id="Sx2.SSx3.SSSx2.p1.8.m8.1.1.1.2" xref="Sx2.SSx3.SSSx2.p1.8.m8.1.1.1.2.cmml">â€‹</mo><mrow id="Sx2.SSx3.SSSx2.p1.8.m8.1.1.1.4.2" xref="Sx2.SSx3.SSSx2.p1.8.m8.1.1.1.cmml"><mo stretchy="false" id="Sx2.SSx3.SSSx2.p1.8.m8.1.1.1.4.2.1" xref="Sx2.SSx3.SSSx2.p1.8.m8.1.1.1.cmml">(</mo><mn id="Sx2.SSx3.SSSx2.p1.8.m8.1.1.1.1" xref="Sx2.SSx3.SSSx2.p1.8.m8.1.1.1.1.cmml">3</mn><mo stretchy="false" id="Sx2.SSx3.SSSx2.p1.8.m8.1.1.1.4.2.2" xref="Sx2.SSx3.SSSx2.p1.8.m8.1.1.1.cmml">)</mo></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="Sx2.SSx3.SSSx2.p1.8.m8.1b"><apply id="Sx2.SSx3.SSSx2.p1.8.m8.1.2.cmml" xref="Sx2.SSx3.SSSx2.p1.8.m8.1.2"><csymbol cd="ambiguous" id="Sx2.SSx3.SSSx2.p1.8.m8.1.2.1.cmml" xref="Sx2.SSx3.SSSx2.p1.8.m8.1.2">subscript</csymbol><ci id="Sx2.SSx3.SSSx2.p1.8.m8.1.2.2.cmml" xref="Sx2.SSx3.SSSx2.p1.8.m8.1.2.2">â„±</ci><apply id="Sx2.SSx3.SSSx2.p1.8.m8.1.1.1.cmml" xref="Sx2.SSx3.SSSx2.p1.8.m8.1.1.1"><times id="Sx2.SSx3.SSSx2.p1.8.m8.1.1.1.2.cmml" xref="Sx2.SSx3.SSSx2.p1.8.m8.1.1.1.2"></times><cn type="integer" id="Sx2.SSx3.SSSx2.p1.8.m8.1.1.1.3.cmml" xref="Sx2.SSx3.SSSx2.p1.8.m8.1.1.1.3">4</cn><cn type="integer" id="Sx2.SSx3.SSSx2.p1.8.m8.1.1.1.1.cmml" xref="Sx2.SSx3.SSSx2.p1.8.m8.1.1.1.1">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx3.SSSx2.p1.8.m8.1c">\mathcal{F}_{4(3)}</annotation></semantics></math>, <math id="Sx2.SSx3.SSSx2.p1.9.m9.1" class="ltx_Math" alttext="\mathcal{F}_{5(3)}" display="inline"><semantics id="Sx2.SSx3.SSSx2.p1.9.m9.1a"><msub id="Sx2.SSx3.SSSx2.p1.9.m9.1.2" xref="Sx2.SSx3.SSSx2.p1.9.m9.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx2.SSx3.SSSx2.p1.9.m9.1.2.2" xref="Sx2.SSx3.SSSx2.p1.9.m9.1.2.2.cmml">â„±</mi><mrow id="Sx2.SSx3.SSSx2.p1.9.m9.1.1.1" xref="Sx2.SSx3.SSSx2.p1.9.m9.1.1.1.cmml"><mn id="Sx2.SSx3.SSSx2.p1.9.m9.1.1.1.3" xref="Sx2.SSx3.SSSx2.p1.9.m9.1.1.1.3.cmml">5</mn><mo lspace="0em" rspace="0em" id="Sx2.SSx3.SSSx2.p1.9.m9.1.1.1.2" xref="Sx2.SSx3.SSSx2.p1.9.m9.1.1.1.2.cmml">â€‹</mo><mrow id="Sx2.SSx3.SSSx2.p1.9.m9.1.1.1.4.2" xref="Sx2.SSx3.SSSx2.p1.9.m9.1.1.1.cmml"><mo stretchy="false" id="Sx2.SSx3.SSSx2.p1.9.m9.1.1.1.4.2.1" xref="Sx2.SSx3.SSSx2.p1.9.m9.1.1.1.cmml">(</mo><mn id="Sx2.SSx3.SSSx2.p1.9.m9.1.1.1.1" xref="Sx2.SSx3.SSSx2.p1.9.m9.1.1.1.1.cmml">3</mn><mo stretchy="false" id="Sx2.SSx3.SSSx2.p1.9.m9.1.1.1.4.2.2" xref="Sx2.SSx3.SSSx2.p1.9.m9.1.1.1.cmml">)</mo></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="Sx2.SSx3.SSSx2.p1.9.m9.1b"><apply id="Sx2.SSx3.SSSx2.p1.9.m9.1.2.cmml" xref="Sx2.SSx3.SSSx2.p1.9.m9.1.2"><csymbol cd="ambiguous" id="Sx2.SSx3.SSSx2.p1.9.m9.1.2.1.cmml" xref="Sx2.SSx3.SSSx2.p1.9.m9.1.2">subscript</csymbol><ci id="Sx2.SSx3.SSSx2.p1.9.m9.1.2.2.cmml" xref="Sx2.SSx3.SSSx2.p1.9.m9.1.2.2">â„±</ci><apply id="Sx2.SSx3.SSSx2.p1.9.m9.1.1.1.cmml" xref="Sx2.SSx3.SSSx2.p1.9.m9.1.1.1"><times id="Sx2.SSx3.SSSx2.p1.9.m9.1.1.1.2.cmml" xref="Sx2.SSx3.SSSx2.p1.9.m9.1.1.1.2"></times><cn type="integer" id="Sx2.SSx3.SSSx2.p1.9.m9.1.1.1.3.cmml" xref="Sx2.SSx3.SSSx2.p1.9.m9.1.1.1.3">5</cn><cn type="integer" id="Sx2.SSx3.SSSx2.p1.9.m9.1.1.1.1.cmml" xref="Sx2.SSx3.SSSx2.p1.9.m9.1.1.1.1">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx3.SSSx2.p1.9.m9.1c">\mathcal{F}_{5(3)}</annotation></semantics></math>, <math id="Sx2.SSx3.SSSx2.p1.10.m10.1" class="ltx_Math" alttext="\mathcal{F}_{6(2)}" display="inline"><semantics id="Sx2.SSx3.SSSx2.p1.10.m10.1a"><msub id="Sx2.SSx3.SSSx2.p1.10.m10.1.2" xref="Sx2.SSx3.SSSx2.p1.10.m10.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx2.SSx3.SSSx2.p1.10.m10.1.2.2" xref="Sx2.SSx3.SSSx2.p1.10.m10.1.2.2.cmml">â„±</mi><mrow id="Sx2.SSx3.SSSx2.p1.10.m10.1.1.1" xref="Sx2.SSx3.SSSx2.p1.10.m10.1.1.1.cmml"><mn id="Sx2.SSx3.SSSx2.p1.10.m10.1.1.1.3" xref="Sx2.SSx3.SSSx2.p1.10.m10.1.1.1.3.cmml">6</mn><mo lspace="0em" rspace="0em" id="Sx2.SSx3.SSSx2.p1.10.m10.1.1.1.2" xref="Sx2.SSx3.SSSx2.p1.10.m10.1.1.1.2.cmml">â€‹</mo><mrow id="Sx2.SSx3.SSSx2.p1.10.m10.1.1.1.4.2" xref="Sx2.SSx3.SSSx2.p1.10.m10.1.1.1.cmml"><mo stretchy="false" id="Sx2.SSx3.SSSx2.p1.10.m10.1.1.1.4.2.1" xref="Sx2.SSx3.SSSx2.p1.10.m10.1.1.1.cmml">(</mo><mn id="Sx2.SSx3.SSSx2.p1.10.m10.1.1.1.1" xref="Sx2.SSx3.SSSx2.p1.10.m10.1.1.1.1.cmml">2</mn><mo stretchy="false" id="Sx2.SSx3.SSSx2.p1.10.m10.1.1.1.4.2.2" xref="Sx2.SSx3.SSSx2.p1.10.m10.1.1.1.cmml">)</mo></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="Sx2.SSx3.SSSx2.p1.10.m10.1b"><apply id="Sx2.SSx3.SSSx2.p1.10.m10.1.2.cmml" xref="Sx2.SSx3.SSSx2.p1.10.m10.1.2"><csymbol cd="ambiguous" id="Sx2.SSx3.SSSx2.p1.10.m10.1.2.1.cmml" xref="Sx2.SSx3.SSSx2.p1.10.m10.1.2">subscript</csymbol><ci id="Sx2.SSx3.SSSx2.p1.10.m10.1.2.2.cmml" xref="Sx2.SSx3.SSSx2.p1.10.m10.1.2.2">â„±</ci><apply id="Sx2.SSx3.SSSx2.p1.10.m10.1.1.1.cmml" xref="Sx2.SSx3.SSSx2.p1.10.m10.1.1.1"><times id="Sx2.SSx3.SSSx2.p1.10.m10.1.1.1.2.cmml" xref="Sx2.SSx3.SSSx2.p1.10.m10.1.1.1.2"></times><cn type="integer" id="Sx2.SSx3.SSSx2.p1.10.m10.1.1.1.3.cmml" xref="Sx2.SSx3.SSSx2.p1.10.m10.1.1.1.3">6</cn><cn type="integer" id="Sx2.SSx3.SSSx2.p1.10.m10.1.1.1.1.cmml" xref="Sx2.SSx3.SSSx2.p1.10.m10.1.1.1.1">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx3.SSSx2.p1.10.m10.1c">\mathcal{F}_{6(2)}</annotation></semantics></math>, <math id="Sx2.SSx3.SSSx2.p1.11.m11.1" class="ltx_Math" alttext="\mathcal{F}_{7(2)}" display="inline"><semantics id="Sx2.SSx3.SSSx2.p1.11.m11.1a"><msub id="Sx2.SSx3.SSSx2.p1.11.m11.1.2" xref="Sx2.SSx3.SSSx2.p1.11.m11.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx2.SSx3.SSSx2.p1.11.m11.1.2.2" xref="Sx2.SSx3.SSSx2.p1.11.m11.1.2.2.cmml">â„±</mi><mrow id="Sx2.SSx3.SSSx2.p1.11.m11.1.1.1" xref="Sx2.SSx3.SSSx2.p1.11.m11.1.1.1.cmml"><mn id="Sx2.SSx3.SSSx2.p1.11.m11.1.1.1.3" xref="Sx2.SSx3.SSSx2.p1.11.m11.1.1.1.3.cmml">7</mn><mo lspace="0em" rspace="0em" id="Sx2.SSx3.SSSx2.p1.11.m11.1.1.1.2" xref="Sx2.SSx3.SSSx2.p1.11.m11.1.1.1.2.cmml">â€‹</mo><mrow id="Sx2.SSx3.SSSx2.p1.11.m11.1.1.1.4.2" xref="Sx2.SSx3.SSSx2.p1.11.m11.1.1.1.cmml"><mo stretchy="false" id="Sx2.SSx3.SSSx2.p1.11.m11.1.1.1.4.2.1" xref="Sx2.SSx3.SSSx2.p1.11.m11.1.1.1.cmml">(</mo><mn id="Sx2.SSx3.SSSx2.p1.11.m11.1.1.1.1" xref="Sx2.SSx3.SSSx2.p1.11.m11.1.1.1.1.cmml">2</mn><mo stretchy="false" id="Sx2.SSx3.SSSx2.p1.11.m11.1.1.1.4.2.2" xref="Sx2.SSx3.SSSx2.p1.11.m11.1.1.1.cmml">)</mo></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="Sx2.SSx3.SSSx2.p1.11.m11.1b"><apply id="Sx2.SSx3.SSSx2.p1.11.m11.1.2.cmml" xref="Sx2.SSx3.SSSx2.p1.11.m11.1.2"><csymbol cd="ambiguous" id="Sx2.SSx3.SSSx2.p1.11.m11.1.2.1.cmml" xref="Sx2.SSx3.SSSx2.p1.11.m11.1.2">subscript</csymbol><ci id="Sx2.SSx3.SSSx2.p1.11.m11.1.2.2.cmml" xref="Sx2.SSx3.SSSx2.p1.11.m11.1.2.2">â„±</ci><apply id="Sx2.SSx3.SSSx2.p1.11.m11.1.1.1.cmml" xref="Sx2.SSx3.SSSx2.p1.11.m11.1.1.1"><times id="Sx2.SSx3.SSSx2.p1.11.m11.1.1.1.2.cmml" xref="Sx2.SSx3.SSSx2.p1.11.m11.1.1.1.2"></times><cn type="integer" id="Sx2.SSx3.SSSx2.p1.11.m11.1.1.1.3.cmml" xref="Sx2.SSx3.SSSx2.p1.11.m11.1.1.1.3">7</cn><cn type="integer" id="Sx2.SSx3.SSSx2.p1.11.m11.1.1.1.1.cmml" xref="Sx2.SSx3.SSSx2.p1.11.m11.1.1.1.1">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx3.SSSx2.p1.11.m11.1c">\mathcal{F}_{7(2)}</annotation></semantics></math>, and <math id="Sx2.SSx3.SSSx2.p1.12.m12.1" class="ltx_Math" alttext="\mathcal{F}_{8(2)}" display="inline"><semantics id="Sx2.SSx3.SSSx2.p1.12.m12.1a"><msub id="Sx2.SSx3.SSSx2.p1.12.m12.1.2" xref="Sx2.SSx3.SSSx2.p1.12.m12.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx2.SSx3.SSSx2.p1.12.m12.1.2.2" xref="Sx2.SSx3.SSSx2.p1.12.m12.1.2.2.cmml">â„±</mi><mrow id="Sx2.SSx3.SSSx2.p1.12.m12.1.1.1" xref="Sx2.SSx3.SSSx2.p1.12.m12.1.1.1.cmml"><mn id="Sx2.SSx3.SSSx2.p1.12.m12.1.1.1.3" xref="Sx2.SSx3.SSSx2.p1.12.m12.1.1.1.3.cmml">8</mn><mo lspace="0em" rspace="0em" id="Sx2.SSx3.SSSx2.p1.12.m12.1.1.1.2" xref="Sx2.SSx3.SSSx2.p1.12.m12.1.1.1.2.cmml">â€‹</mo><mrow id="Sx2.SSx3.SSSx2.p1.12.m12.1.1.1.4.2" xref="Sx2.SSx3.SSSx2.p1.12.m12.1.1.1.cmml"><mo stretchy="false" id="Sx2.SSx3.SSSx2.p1.12.m12.1.1.1.4.2.1" xref="Sx2.SSx3.SSSx2.p1.12.m12.1.1.1.cmml">(</mo><mn id="Sx2.SSx3.SSSx2.p1.12.m12.1.1.1.1" xref="Sx2.SSx3.SSSx2.p1.12.m12.1.1.1.1.cmml">2</mn><mo stretchy="false" id="Sx2.SSx3.SSSx2.p1.12.m12.1.1.1.4.2.2" xref="Sx2.SSx3.SSSx2.p1.12.m12.1.1.1.cmml">)</mo></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="Sx2.SSx3.SSSx2.p1.12.m12.1b"><apply id="Sx2.SSx3.SSSx2.p1.12.m12.1.2.cmml" xref="Sx2.SSx3.SSSx2.p1.12.m12.1.2"><csymbol cd="ambiguous" id="Sx2.SSx3.SSSx2.p1.12.m12.1.2.1.cmml" xref="Sx2.SSx3.SSSx2.p1.12.m12.1.2">subscript</csymbol><ci id="Sx2.SSx3.SSSx2.p1.12.m12.1.2.2.cmml" xref="Sx2.SSx3.SSSx2.p1.12.m12.1.2.2">â„±</ci><apply id="Sx2.SSx3.SSSx2.p1.12.m12.1.1.1.cmml" xref="Sx2.SSx3.SSSx2.p1.12.m12.1.1.1"><times id="Sx2.SSx3.SSSx2.p1.12.m12.1.1.1.2.cmml" xref="Sx2.SSx3.SSSx2.p1.12.m12.1.1.1.2"></times><cn type="integer" id="Sx2.SSx3.SSSx2.p1.12.m12.1.1.1.3.cmml" xref="Sx2.SSx3.SSSx2.p1.12.m12.1.1.1.3">8</cn><cn type="integer" id="Sx2.SSx3.SSSx2.p1.12.m12.1.1.1.1.cmml" xref="Sx2.SSx3.SSSx2.p1.12.m12.1.1.1.1">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx3.SSSx2.p1.12.m12.1c">\mathcal{F}_{8(2)}</annotation></semantics></math>. <math id="Sx2.SSx3.SSSx2.p1.13.m13.1" class="ltx_Math" alttext="(\cdot)_{\uparrow}" display="inline"><semantics id="Sx2.SSx3.SSSx2.p1.13.m13.1a"><msub id="Sx2.SSx3.SSSx2.p1.13.m13.1.2" xref="Sx2.SSx3.SSSx2.p1.13.m13.1.2.cmml"><mrow id="Sx2.SSx3.SSSx2.p1.13.m13.1.2.2.2" xref="Sx2.SSx3.SSSx2.p1.13.m13.1.2.cmml"><mo stretchy="false" id="Sx2.SSx3.SSSx2.p1.13.m13.1.2.2.2.1" xref="Sx2.SSx3.SSSx2.p1.13.m13.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="Sx2.SSx3.SSSx2.p1.13.m13.1.1" xref="Sx2.SSx3.SSSx2.p1.13.m13.1.1.cmml">â‹…</mo><mo stretchy="false" id="Sx2.SSx3.SSSx2.p1.13.m13.1.2.2.2.2" xref="Sx2.SSx3.SSSx2.p1.13.m13.1.2.cmml">)</mo></mrow><mo stretchy="false" id="Sx2.SSx3.SSSx2.p1.13.m13.1.2.3" xref="Sx2.SSx3.SSSx2.p1.13.m13.1.2.3.cmml">â†‘</mo></msub><annotation-xml encoding="MathML-Content" id="Sx2.SSx3.SSSx2.p1.13.m13.1b"><apply id="Sx2.SSx3.SSSx2.p1.13.m13.1.2.cmml" xref="Sx2.SSx3.SSSx2.p1.13.m13.1.2"><csymbol cd="ambiguous" id="Sx2.SSx3.SSSx2.p1.13.m13.1.2.1.cmml" xref="Sx2.SSx3.SSSx2.p1.13.m13.1.2">subscript</csymbol><ci id="Sx2.SSx3.SSSx2.p1.13.m13.1.1.cmml" xref="Sx2.SSx3.SSSx2.p1.13.m13.1.1">â‹…</ci><ci id="Sx2.SSx3.SSSx2.p1.13.m13.1.2.3.cmml" xref="Sx2.SSx3.SSSx2.p1.13.m13.1.2.3">â†‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx3.SSSx2.p1.13.m13.1c">(\cdot)_{\uparrow}</annotation></semantics></math> represents the upsamling operation. The aggregated operation is defined as the element-wise multiplication <math id="Sx2.SSx3.SSSx2.p1.14.m14.1" class="ltx_Math" alttext="\otimes" display="inline"><semantics id="Sx2.SSx3.SSSx2.p1.14.m14.1a"><mo id="Sx2.SSx3.SSSx2.p1.14.m14.1.1" xref="Sx2.SSx3.SSSx2.p1.14.m14.1.1.cmml">âŠ—</mo><annotation-xml encoding="MathML-Content" id="Sx2.SSx3.SSSx2.p1.14.m14.1b"><csymbol cd="latexml" id="Sx2.SSx3.SSSx2.p1.14.m14.1.1.cmml" xref="Sx2.SSx3.SSSx2.p1.14.m14.1.1">tensor-product</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx3.SSSx2.p1.14.m14.1c">\otimes</annotation></semantics></math>. It is because multiplication exactly corresponds to the division used in the scene decomposition. In other words, this way reconstructs the original scene information that existed in the original low-light observation. The detailed computational process can be found in FigureÂ <a href="#Sx1.F3" title="Figure 3 â€£ Our Contributions â€£ Introduction â€£ Trash to Treasure: Low-Light Object Detection via Decomposition-and-Aggregation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="Sx2.F4" class="ltx_figure">
<table id="Sx2.F4.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="Sx2.F4.1.1" class="ltx_tr">
<td id="Sx2.F4.1.1.1" class="ltx_td ltx_align_center"><img src="/html/2309.03548/assets/x5.png" id="Sx2.F4.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="530" height="92" alt="Refer to caption"></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Comparing decomposed components among different methods on a low-light image. Except the input, the bottom left and top right for each image are illumination and reflectance, respectively. </figcaption>
</figure>
</section>
</section>
<section id="Sx2.SSx4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Training Loss</h3>

<div id="Sx2.SSx4.p1" class="ltx_para">
<p id="Sx2.SSx4.p1.2" class="ltx_p">In the training phase, we adopt a multi-task loss functionÂ <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al. <a href="#bib.bib20" title="" class="ltx_ref">2016</a>; Girshick <a href="#bib.bib3" title="" class="ltx_ref">2015</a>)</cite> for the overall network. The objective loss function consists of a weighted sum of location loss <math id="Sx2.SSx4.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{L}_{loc}" display="inline"><semantics id="Sx2.SSx4.p1.1.m1.1a"><msub id="Sx2.SSx4.p1.1.m1.1.1" xref="Sx2.SSx4.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx2.SSx4.p1.1.m1.1.1.2" xref="Sx2.SSx4.p1.1.m1.1.1.2.cmml">â„’</mi><mrow id="Sx2.SSx4.p1.1.m1.1.1.3" xref="Sx2.SSx4.p1.1.m1.1.1.3.cmml"><mi id="Sx2.SSx4.p1.1.m1.1.1.3.2" xref="Sx2.SSx4.p1.1.m1.1.1.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="Sx2.SSx4.p1.1.m1.1.1.3.1" xref="Sx2.SSx4.p1.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="Sx2.SSx4.p1.1.m1.1.1.3.3" xref="Sx2.SSx4.p1.1.m1.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="Sx2.SSx4.p1.1.m1.1.1.3.1a" xref="Sx2.SSx4.p1.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="Sx2.SSx4.p1.1.m1.1.1.3.4" xref="Sx2.SSx4.p1.1.m1.1.1.3.4.cmml">c</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="Sx2.SSx4.p1.1.m1.1b"><apply id="Sx2.SSx4.p1.1.m1.1.1.cmml" xref="Sx2.SSx4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="Sx2.SSx4.p1.1.m1.1.1.1.cmml" xref="Sx2.SSx4.p1.1.m1.1.1">subscript</csymbol><ci id="Sx2.SSx4.p1.1.m1.1.1.2.cmml" xref="Sx2.SSx4.p1.1.m1.1.1.2">â„’</ci><apply id="Sx2.SSx4.p1.1.m1.1.1.3.cmml" xref="Sx2.SSx4.p1.1.m1.1.1.3"><times id="Sx2.SSx4.p1.1.m1.1.1.3.1.cmml" xref="Sx2.SSx4.p1.1.m1.1.1.3.1"></times><ci id="Sx2.SSx4.p1.1.m1.1.1.3.2.cmml" xref="Sx2.SSx4.p1.1.m1.1.1.3.2">ğ‘™</ci><ci id="Sx2.SSx4.p1.1.m1.1.1.3.3.cmml" xref="Sx2.SSx4.p1.1.m1.1.1.3.3">ğ‘œ</ci><ci id="Sx2.SSx4.p1.1.m1.1.1.3.4.cmml" xref="Sx2.SSx4.p1.1.m1.1.1.3.4">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx4.p1.1.m1.1c">\mathcal{L}_{loc}</annotation></semantics></math> and a confidence loss <math id="Sx2.SSx4.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{L}_{conf}" display="inline"><semantics id="Sx2.SSx4.p1.2.m2.1a"><msub id="Sx2.SSx4.p1.2.m2.1.1" xref="Sx2.SSx4.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx2.SSx4.p1.2.m2.1.1.2" xref="Sx2.SSx4.p1.2.m2.1.1.2.cmml">â„’</mi><mrow id="Sx2.SSx4.p1.2.m2.1.1.3" xref="Sx2.SSx4.p1.2.m2.1.1.3.cmml"><mi id="Sx2.SSx4.p1.2.m2.1.1.3.2" xref="Sx2.SSx4.p1.2.m2.1.1.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="Sx2.SSx4.p1.2.m2.1.1.3.1" xref="Sx2.SSx4.p1.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="Sx2.SSx4.p1.2.m2.1.1.3.3" xref="Sx2.SSx4.p1.2.m2.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="Sx2.SSx4.p1.2.m2.1.1.3.1a" xref="Sx2.SSx4.p1.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="Sx2.SSx4.p1.2.m2.1.1.3.4" xref="Sx2.SSx4.p1.2.m2.1.1.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="Sx2.SSx4.p1.2.m2.1.1.3.1b" xref="Sx2.SSx4.p1.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="Sx2.SSx4.p1.2.m2.1.1.3.5" xref="Sx2.SSx4.p1.2.m2.1.1.3.5.cmml">f</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="Sx2.SSx4.p1.2.m2.1b"><apply id="Sx2.SSx4.p1.2.m2.1.1.cmml" xref="Sx2.SSx4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="Sx2.SSx4.p1.2.m2.1.1.1.cmml" xref="Sx2.SSx4.p1.2.m2.1.1">subscript</csymbol><ci id="Sx2.SSx4.p1.2.m2.1.1.2.cmml" xref="Sx2.SSx4.p1.2.m2.1.1.2">â„’</ci><apply id="Sx2.SSx4.p1.2.m2.1.1.3.cmml" xref="Sx2.SSx4.p1.2.m2.1.1.3"><times id="Sx2.SSx4.p1.2.m2.1.1.3.1.cmml" xref="Sx2.SSx4.p1.2.m2.1.1.3.1"></times><ci id="Sx2.SSx4.p1.2.m2.1.1.3.2.cmml" xref="Sx2.SSx4.p1.2.m2.1.1.3.2">ğ‘</ci><ci id="Sx2.SSx4.p1.2.m2.1.1.3.3.cmml" xref="Sx2.SSx4.p1.2.m2.1.1.3.3">ğ‘œ</ci><ci id="Sx2.SSx4.p1.2.m2.1.1.3.4.cmml" xref="Sx2.SSx4.p1.2.m2.1.1.3.4">ğ‘›</ci><ci id="Sx2.SSx4.p1.2.m2.1.1.3.5.cmml" xref="Sx2.SSx4.p1.2.m2.1.1.3.5">ğ‘“</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx4.p1.2.m2.1c">\mathcal{L}_{conf}</annotation></semantics></math>, formulated as</p>
<table id="Sx2.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="Sx2.E2.m1.10" class="ltx_Math" alttext="\mathcal{L}(x,c,l,g)=\frac{1}{N}(\mathcal{L}_{conf}(x,c)+\alpha\mathcal{L}_{loc}(x,l,g))," display="block"><semantics id="Sx2.E2.m1.10a"><mrow id="Sx2.E2.m1.10.10.1" xref="Sx2.E2.m1.10.10.1.1.cmml"><mrow id="Sx2.E2.m1.10.10.1.1" xref="Sx2.E2.m1.10.10.1.1.cmml"><mrow id="Sx2.E2.m1.10.10.1.1.3" xref="Sx2.E2.m1.10.10.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx2.E2.m1.10.10.1.1.3.2" xref="Sx2.E2.m1.10.10.1.1.3.2.cmml">â„’</mi><mo lspace="0em" rspace="0em" id="Sx2.E2.m1.10.10.1.1.3.1" xref="Sx2.E2.m1.10.10.1.1.3.1.cmml">â€‹</mo><mrow id="Sx2.E2.m1.10.10.1.1.3.3.2" xref="Sx2.E2.m1.10.10.1.1.3.3.1.cmml"><mo stretchy="false" id="Sx2.E2.m1.10.10.1.1.3.3.2.1" xref="Sx2.E2.m1.10.10.1.1.3.3.1.cmml">(</mo><mi id="Sx2.E2.m1.1.1" xref="Sx2.E2.m1.1.1.cmml">x</mi><mo id="Sx2.E2.m1.10.10.1.1.3.3.2.2" xref="Sx2.E2.m1.10.10.1.1.3.3.1.cmml">,</mo><mi id="Sx2.E2.m1.2.2" xref="Sx2.E2.m1.2.2.cmml">c</mi><mo id="Sx2.E2.m1.10.10.1.1.3.3.2.3" xref="Sx2.E2.m1.10.10.1.1.3.3.1.cmml">,</mo><mi id="Sx2.E2.m1.3.3" xref="Sx2.E2.m1.3.3.cmml">l</mi><mo id="Sx2.E2.m1.10.10.1.1.3.3.2.4" xref="Sx2.E2.m1.10.10.1.1.3.3.1.cmml">,</mo><mi id="Sx2.E2.m1.4.4" xref="Sx2.E2.m1.4.4.cmml">g</mi><mo stretchy="false" id="Sx2.E2.m1.10.10.1.1.3.3.2.5" xref="Sx2.E2.m1.10.10.1.1.3.3.1.cmml">)</mo></mrow></mrow><mo id="Sx2.E2.m1.10.10.1.1.2" xref="Sx2.E2.m1.10.10.1.1.2.cmml">=</mo><mrow id="Sx2.E2.m1.10.10.1.1.1" xref="Sx2.E2.m1.10.10.1.1.1.cmml"><mfrac id="Sx2.E2.m1.10.10.1.1.1.3" xref="Sx2.E2.m1.10.10.1.1.1.3.cmml"><mn id="Sx2.E2.m1.10.10.1.1.1.3.2" xref="Sx2.E2.m1.10.10.1.1.1.3.2.cmml">1</mn><mi id="Sx2.E2.m1.10.10.1.1.1.3.3" xref="Sx2.E2.m1.10.10.1.1.1.3.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="Sx2.E2.m1.10.10.1.1.1.2" xref="Sx2.E2.m1.10.10.1.1.1.2.cmml">â€‹</mo><mrow id="Sx2.E2.m1.10.10.1.1.1.1.1" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.cmml"><mo stretchy="false" id="Sx2.E2.m1.10.10.1.1.1.1.1.2" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.cmml">(</mo><mrow id="Sx2.E2.m1.10.10.1.1.1.1.1.1" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.cmml"><mrow id="Sx2.E2.m1.10.10.1.1.1.1.1.1.2" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.cmml"><msub id="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.2" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.2.2" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.2.2.cmml">â„’</mi><mrow id="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.2.3" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.2.3.cmml"><mi id="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.2.3.2" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.2.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.2.3.1" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.2.3.1.cmml">â€‹</mo><mi id="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.2.3.3" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.2.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.2.3.1a" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.2.3.1.cmml">â€‹</mo><mi id="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.2.3.4" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.2.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.2.3.1b" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.2.3.1.cmml">â€‹</mo><mi id="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.2.3.5" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.2.3.5.cmml">f</mi></mrow></msub><mo lspace="0em" rspace="0em" id="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.1" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.1.cmml">â€‹</mo><mrow id="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.3.2" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.3.1.cmml"><mo stretchy="false" id="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.3.2.1" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.3.1.cmml">(</mo><mi id="Sx2.E2.m1.5.5" xref="Sx2.E2.m1.5.5.cmml">x</mi><mo id="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.3.2.2" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.3.1.cmml">,</mo><mi id="Sx2.E2.m1.6.6" xref="Sx2.E2.m1.6.6.cmml">c</mi><mo stretchy="false" id="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.3.2.3" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.3.1.cmml">)</mo></mrow></mrow><mo id="Sx2.E2.m1.10.10.1.1.1.1.1.1.1" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.1.cmml">+</mo><mrow id="Sx2.E2.m1.10.10.1.1.1.1.1.1.3" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.cmml"><mi id="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.2" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.2.cmml">Î±</mi><mo lspace="0em" rspace="0em" id="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.1" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.1.cmml">â€‹</mo><msub id="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.3" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.3.2" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.3.2.cmml">â„’</mi><mrow id="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.3.3" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.3.3.cmml"><mi id="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.3.3.2" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.3.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.3.3.1" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.3.3.1.cmml">â€‹</mo><mi id="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.3.3.3" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.3.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.3.3.1a" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.3.3.1.cmml">â€‹</mo><mi id="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.3.3.4" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.3.3.4.cmml">c</mi></mrow></msub><mo lspace="0em" rspace="0em" id="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.1a" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.1.cmml">â€‹</mo><mrow id="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.4.2" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.4.1.cmml"><mo stretchy="false" id="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.4.2.1" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.4.1.cmml">(</mo><mi id="Sx2.E2.m1.7.7" xref="Sx2.E2.m1.7.7.cmml">x</mi><mo id="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.4.2.2" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.4.1.cmml">,</mo><mi id="Sx2.E2.m1.8.8" xref="Sx2.E2.m1.8.8.cmml">l</mi><mo id="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.4.2.3" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.4.1.cmml">,</mo><mi id="Sx2.E2.m1.9.9" xref="Sx2.E2.m1.9.9.cmml">g</mi><mo stretchy="false" id="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.4.2.4" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.4.1.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="Sx2.E2.m1.10.10.1.1.1.1.1.3" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="Sx2.E2.m1.10.10.1.2" xref="Sx2.E2.m1.10.10.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx2.E2.m1.10b"><apply id="Sx2.E2.m1.10.10.1.1.cmml" xref="Sx2.E2.m1.10.10.1"><eq id="Sx2.E2.m1.10.10.1.1.2.cmml" xref="Sx2.E2.m1.10.10.1.1.2"></eq><apply id="Sx2.E2.m1.10.10.1.1.3.cmml" xref="Sx2.E2.m1.10.10.1.1.3"><times id="Sx2.E2.m1.10.10.1.1.3.1.cmml" xref="Sx2.E2.m1.10.10.1.1.3.1"></times><ci id="Sx2.E2.m1.10.10.1.1.3.2.cmml" xref="Sx2.E2.m1.10.10.1.1.3.2">â„’</ci><vector id="Sx2.E2.m1.10.10.1.1.3.3.1.cmml" xref="Sx2.E2.m1.10.10.1.1.3.3.2"><ci id="Sx2.E2.m1.1.1.cmml" xref="Sx2.E2.m1.1.1">ğ‘¥</ci><ci id="Sx2.E2.m1.2.2.cmml" xref="Sx2.E2.m1.2.2">ğ‘</ci><ci id="Sx2.E2.m1.3.3.cmml" xref="Sx2.E2.m1.3.3">ğ‘™</ci><ci id="Sx2.E2.m1.4.4.cmml" xref="Sx2.E2.m1.4.4">ğ‘”</ci></vector></apply><apply id="Sx2.E2.m1.10.10.1.1.1.cmml" xref="Sx2.E2.m1.10.10.1.1.1"><times id="Sx2.E2.m1.10.10.1.1.1.2.cmml" xref="Sx2.E2.m1.10.10.1.1.1.2"></times><apply id="Sx2.E2.m1.10.10.1.1.1.3.cmml" xref="Sx2.E2.m1.10.10.1.1.1.3"><divide id="Sx2.E2.m1.10.10.1.1.1.3.1.cmml" xref="Sx2.E2.m1.10.10.1.1.1.3"></divide><cn type="integer" id="Sx2.E2.m1.10.10.1.1.1.3.2.cmml" xref="Sx2.E2.m1.10.10.1.1.1.3.2">1</cn><ci id="Sx2.E2.m1.10.10.1.1.1.3.3.cmml" xref="Sx2.E2.m1.10.10.1.1.1.3.3">ğ‘</ci></apply><apply id="Sx2.E2.m1.10.10.1.1.1.1.1.1.cmml" xref="Sx2.E2.m1.10.10.1.1.1.1.1"><plus id="Sx2.E2.m1.10.10.1.1.1.1.1.1.1.cmml" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.1"></plus><apply id="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.cmml" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.2"><times id="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.1.cmml" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.1"></times><apply id="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.2.cmml" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.2.1.cmml" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.2.2.cmml" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.2.2">â„’</ci><apply id="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.2.3.cmml" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.2.3"><times id="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.2.3.1.cmml" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.2.3.1"></times><ci id="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.2.3.2.cmml" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.2.3.2">ğ‘</ci><ci id="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.2.3.3.cmml" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.2.3.3">ğ‘œ</ci><ci id="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.2.3.4.cmml" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.2.3.4">ğ‘›</ci><ci id="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.2.3.5.cmml" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.2.3.5">ğ‘“</ci></apply></apply><interval closure="open" id="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.3.1.cmml" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.2.3.2"><ci id="Sx2.E2.m1.5.5.cmml" xref="Sx2.E2.m1.5.5">ğ‘¥</ci><ci id="Sx2.E2.m1.6.6.cmml" xref="Sx2.E2.m1.6.6">ğ‘</ci></interval></apply><apply id="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.cmml" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.3"><times id="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.1.cmml" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.1"></times><ci id="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.2.cmml" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.2">ğ›¼</ci><apply id="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.3.cmml" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.3.1.cmml" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.3">subscript</csymbol><ci id="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.3.2.cmml" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.3.2">â„’</ci><apply id="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.3.3.cmml" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.3.3"><times id="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.3.3.1.cmml" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.3.3.1"></times><ci id="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.3.3.2.cmml" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.3.3.2">ğ‘™</ci><ci id="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.3.3.3.cmml" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.3.3.3">ğ‘œ</ci><ci id="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.3.3.4.cmml" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.3.3.4">ğ‘</ci></apply></apply><vector id="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.4.1.cmml" xref="Sx2.E2.m1.10.10.1.1.1.1.1.1.3.4.2"><ci id="Sx2.E2.m1.7.7.cmml" xref="Sx2.E2.m1.7.7">ğ‘¥</ci><ci id="Sx2.E2.m1.8.8.cmml" xref="Sx2.E2.m1.8.8">ğ‘™</ci><ci id="Sx2.E2.m1.9.9.cmml" xref="Sx2.E2.m1.9.9">ğ‘”</ci></vector></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.E2.m1.10c">\mathcal{L}(x,c,l,g)=\frac{1}{N}(\mathcal{L}_{conf}(x,c)+\alpha\mathcal{L}_{loc}(x,l,g)),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="Sx2.SSx4.p1.12" class="ltx_p">where <math id="Sx2.SSx4.p1.3.m1.1" class="ltx_Math" alttext="\mathcal{L}_{conf}" display="inline"><semantics id="Sx2.SSx4.p1.3.m1.1a"><msub id="Sx2.SSx4.p1.3.m1.1.1" xref="Sx2.SSx4.p1.3.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx2.SSx4.p1.3.m1.1.1.2" xref="Sx2.SSx4.p1.3.m1.1.1.2.cmml">â„’</mi><mrow id="Sx2.SSx4.p1.3.m1.1.1.3" xref="Sx2.SSx4.p1.3.m1.1.1.3.cmml"><mi id="Sx2.SSx4.p1.3.m1.1.1.3.2" xref="Sx2.SSx4.p1.3.m1.1.1.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="Sx2.SSx4.p1.3.m1.1.1.3.1" xref="Sx2.SSx4.p1.3.m1.1.1.3.1.cmml">â€‹</mo><mi id="Sx2.SSx4.p1.3.m1.1.1.3.3" xref="Sx2.SSx4.p1.3.m1.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="Sx2.SSx4.p1.3.m1.1.1.3.1a" xref="Sx2.SSx4.p1.3.m1.1.1.3.1.cmml">â€‹</mo><mi id="Sx2.SSx4.p1.3.m1.1.1.3.4" xref="Sx2.SSx4.p1.3.m1.1.1.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="Sx2.SSx4.p1.3.m1.1.1.3.1b" xref="Sx2.SSx4.p1.3.m1.1.1.3.1.cmml">â€‹</mo><mi id="Sx2.SSx4.p1.3.m1.1.1.3.5" xref="Sx2.SSx4.p1.3.m1.1.1.3.5.cmml">f</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="Sx2.SSx4.p1.3.m1.1b"><apply id="Sx2.SSx4.p1.3.m1.1.1.cmml" xref="Sx2.SSx4.p1.3.m1.1.1"><csymbol cd="ambiguous" id="Sx2.SSx4.p1.3.m1.1.1.1.cmml" xref="Sx2.SSx4.p1.3.m1.1.1">subscript</csymbol><ci id="Sx2.SSx4.p1.3.m1.1.1.2.cmml" xref="Sx2.SSx4.p1.3.m1.1.1.2">â„’</ci><apply id="Sx2.SSx4.p1.3.m1.1.1.3.cmml" xref="Sx2.SSx4.p1.3.m1.1.1.3"><times id="Sx2.SSx4.p1.3.m1.1.1.3.1.cmml" xref="Sx2.SSx4.p1.3.m1.1.1.3.1"></times><ci id="Sx2.SSx4.p1.3.m1.1.1.3.2.cmml" xref="Sx2.SSx4.p1.3.m1.1.1.3.2">ğ‘</ci><ci id="Sx2.SSx4.p1.3.m1.1.1.3.3.cmml" xref="Sx2.SSx4.p1.3.m1.1.1.3.3">ğ‘œ</ci><ci id="Sx2.SSx4.p1.3.m1.1.1.3.4.cmml" xref="Sx2.SSx4.p1.3.m1.1.1.3.4">ğ‘›</ci><ci id="Sx2.SSx4.p1.3.m1.1.1.3.5.cmml" xref="Sx2.SSx4.p1.3.m1.1.1.3.5">ğ‘“</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx4.p1.3.m1.1c">\mathcal{L}_{conf}</annotation></semantics></math> is focal loss and <math id="Sx2.SSx4.p1.4.m2.1" class="ltx_Math" alttext="\mathcal{L}_{loc}" display="inline"><semantics id="Sx2.SSx4.p1.4.m2.1a"><msub id="Sx2.SSx4.p1.4.m2.1.1" xref="Sx2.SSx4.p1.4.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx2.SSx4.p1.4.m2.1.1.2" xref="Sx2.SSx4.p1.4.m2.1.1.2.cmml">â„’</mi><mrow id="Sx2.SSx4.p1.4.m2.1.1.3" xref="Sx2.SSx4.p1.4.m2.1.1.3.cmml"><mi id="Sx2.SSx4.p1.4.m2.1.1.3.2" xref="Sx2.SSx4.p1.4.m2.1.1.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="Sx2.SSx4.p1.4.m2.1.1.3.1" xref="Sx2.SSx4.p1.4.m2.1.1.3.1.cmml">â€‹</mo><mi id="Sx2.SSx4.p1.4.m2.1.1.3.3" xref="Sx2.SSx4.p1.4.m2.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="Sx2.SSx4.p1.4.m2.1.1.3.1a" xref="Sx2.SSx4.p1.4.m2.1.1.3.1.cmml">â€‹</mo><mi id="Sx2.SSx4.p1.4.m2.1.1.3.4" xref="Sx2.SSx4.p1.4.m2.1.1.3.4.cmml">c</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="Sx2.SSx4.p1.4.m2.1b"><apply id="Sx2.SSx4.p1.4.m2.1.1.cmml" xref="Sx2.SSx4.p1.4.m2.1.1"><csymbol cd="ambiguous" id="Sx2.SSx4.p1.4.m2.1.1.1.cmml" xref="Sx2.SSx4.p1.4.m2.1.1">subscript</csymbol><ci id="Sx2.SSx4.p1.4.m2.1.1.2.cmml" xref="Sx2.SSx4.p1.4.m2.1.1.2">â„’</ci><apply id="Sx2.SSx4.p1.4.m2.1.1.3.cmml" xref="Sx2.SSx4.p1.4.m2.1.1.3"><times id="Sx2.SSx4.p1.4.m2.1.1.3.1.cmml" xref="Sx2.SSx4.p1.4.m2.1.1.3.1"></times><ci id="Sx2.SSx4.p1.4.m2.1.1.3.2.cmml" xref="Sx2.SSx4.p1.4.m2.1.1.3.2">ğ‘™</ci><ci id="Sx2.SSx4.p1.4.m2.1.1.3.3.cmml" xref="Sx2.SSx4.p1.4.m2.1.1.3.3">ğ‘œ</ci><ci id="Sx2.SSx4.p1.4.m2.1.1.3.4.cmml" xref="Sx2.SSx4.p1.4.m2.1.1.3.4">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx4.p1.4.m2.1c">\mathcal{L}_{loc}</annotation></semantics></math> is smooth L1 loss, <math id="Sx2.SSx4.p1.5.m3.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="Sx2.SSx4.p1.5.m3.1a"><mi id="Sx2.SSx4.p1.5.m3.1.1" xref="Sx2.SSx4.p1.5.m3.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx4.p1.5.m3.1b"><ci id="Sx2.SSx4.p1.5.m3.1.1.cmml" xref="Sx2.SSx4.p1.5.m3.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx4.p1.5.m3.1c">\alpha</annotation></semantics></math> represents the trade-off parameter between the two losses. <math id="Sx2.SSx4.p1.6.m4.1" class="ltx_Math" alttext="N" display="inline"><semantics id="Sx2.SSx4.p1.6.m4.1a"><mi id="Sx2.SSx4.p1.6.m4.1.1" xref="Sx2.SSx4.p1.6.m4.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx4.p1.6.m4.1b"><ci id="Sx2.SSx4.p1.6.m4.1.1.cmml" xref="Sx2.SSx4.p1.6.m4.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx4.p1.6.m4.1c">N</annotation></semantics></math> is the number of the default boxes, <math id="Sx2.SSx4.p1.7.m5.2" class="ltx_Math" alttext="{x}_{ij}=\{1,0\}" display="inline"><semantics id="Sx2.SSx4.p1.7.m5.2a"><mrow id="Sx2.SSx4.p1.7.m5.2.3" xref="Sx2.SSx4.p1.7.m5.2.3.cmml"><msub id="Sx2.SSx4.p1.7.m5.2.3.2" xref="Sx2.SSx4.p1.7.m5.2.3.2.cmml"><mi id="Sx2.SSx4.p1.7.m5.2.3.2.2" xref="Sx2.SSx4.p1.7.m5.2.3.2.2.cmml">x</mi><mrow id="Sx2.SSx4.p1.7.m5.2.3.2.3" xref="Sx2.SSx4.p1.7.m5.2.3.2.3.cmml"><mi id="Sx2.SSx4.p1.7.m5.2.3.2.3.2" xref="Sx2.SSx4.p1.7.m5.2.3.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="Sx2.SSx4.p1.7.m5.2.3.2.3.1" xref="Sx2.SSx4.p1.7.m5.2.3.2.3.1.cmml">â€‹</mo><mi id="Sx2.SSx4.p1.7.m5.2.3.2.3.3" xref="Sx2.SSx4.p1.7.m5.2.3.2.3.3.cmml">j</mi></mrow></msub><mo id="Sx2.SSx4.p1.7.m5.2.3.1" xref="Sx2.SSx4.p1.7.m5.2.3.1.cmml">=</mo><mrow id="Sx2.SSx4.p1.7.m5.2.3.3.2" xref="Sx2.SSx4.p1.7.m5.2.3.3.1.cmml"><mo stretchy="false" id="Sx2.SSx4.p1.7.m5.2.3.3.2.1" xref="Sx2.SSx4.p1.7.m5.2.3.3.1.cmml">{</mo><mn id="Sx2.SSx4.p1.7.m5.1.1" xref="Sx2.SSx4.p1.7.m5.1.1.cmml">1</mn><mo id="Sx2.SSx4.p1.7.m5.2.3.3.2.2" xref="Sx2.SSx4.p1.7.m5.2.3.3.1.cmml">,</mo><mn id="Sx2.SSx4.p1.7.m5.2.2" xref="Sx2.SSx4.p1.7.m5.2.2.cmml">0</mn><mo stretchy="false" id="Sx2.SSx4.p1.7.m5.2.3.3.2.3" xref="Sx2.SSx4.p1.7.m5.2.3.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx2.SSx4.p1.7.m5.2b"><apply id="Sx2.SSx4.p1.7.m5.2.3.cmml" xref="Sx2.SSx4.p1.7.m5.2.3"><eq id="Sx2.SSx4.p1.7.m5.2.3.1.cmml" xref="Sx2.SSx4.p1.7.m5.2.3.1"></eq><apply id="Sx2.SSx4.p1.7.m5.2.3.2.cmml" xref="Sx2.SSx4.p1.7.m5.2.3.2"><csymbol cd="ambiguous" id="Sx2.SSx4.p1.7.m5.2.3.2.1.cmml" xref="Sx2.SSx4.p1.7.m5.2.3.2">subscript</csymbol><ci id="Sx2.SSx4.p1.7.m5.2.3.2.2.cmml" xref="Sx2.SSx4.p1.7.m5.2.3.2.2">ğ‘¥</ci><apply id="Sx2.SSx4.p1.7.m5.2.3.2.3.cmml" xref="Sx2.SSx4.p1.7.m5.2.3.2.3"><times id="Sx2.SSx4.p1.7.m5.2.3.2.3.1.cmml" xref="Sx2.SSx4.p1.7.m5.2.3.2.3.1"></times><ci id="Sx2.SSx4.p1.7.m5.2.3.2.3.2.cmml" xref="Sx2.SSx4.p1.7.m5.2.3.2.3.2">ğ‘–</ci><ci id="Sx2.SSx4.p1.7.m5.2.3.2.3.3.cmml" xref="Sx2.SSx4.p1.7.m5.2.3.2.3.3">ğ‘—</ci></apply></apply><set id="Sx2.SSx4.p1.7.m5.2.3.3.1.cmml" xref="Sx2.SSx4.p1.7.m5.2.3.3.2"><cn type="integer" id="Sx2.SSx4.p1.7.m5.1.1.cmml" xref="Sx2.SSx4.p1.7.m5.1.1">1</cn><cn type="integer" id="Sx2.SSx4.p1.7.m5.2.2.cmml" xref="Sx2.SSx4.p1.7.m5.2.2">0</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx4.p1.7.m5.2c">{x}_{ij}=\{1,0\}</annotation></semantics></math> represents an indicator that the <math id="Sx2.SSx4.p1.8.m6.1" class="ltx_Math" alttext="i" display="inline"><semantics id="Sx2.SSx4.p1.8.m6.1a"><mi id="Sx2.SSx4.p1.8.m6.1.1" xref="Sx2.SSx4.p1.8.m6.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx4.p1.8.m6.1b"><ci id="Sx2.SSx4.p1.8.m6.1.1.cmml" xref="Sx2.SSx4.p1.8.m6.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx4.p1.8.m6.1c">i</annotation></semantics></math>-th default box matches the <math id="Sx2.SSx4.p1.9.m7.1" class="ltx_Math" alttext="j" display="inline"><semantics id="Sx2.SSx4.p1.9.m7.1a"><mi id="Sx2.SSx4.p1.9.m7.1.1" xref="Sx2.SSx4.p1.9.m7.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx4.p1.9.m7.1b"><ci id="Sx2.SSx4.p1.9.m7.1.1.cmml" xref="Sx2.SSx4.p1.9.m7.1.1">ğ‘—</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx4.p1.9.m7.1c">j</annotation></semantics></math>-th ground truth box, <math id="Sx2.SSx4.p1.10.m8.1" class="ltx_Math" alttext="{c}" display="inline"><semantics id="Sx2.SSx4.p1.10.m8.1a"><mi id="Sx2.SSx4.p1.10.m8.1.1" xref="Sx2.SSx4.p1.10.m8.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx4.p1.10.m8.1b"><ci id="Sx2.SSx4.p1.10.m8.1.1.cmml" xref="Sx2.SSx4.p1.10.m8.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx4.p1.10.m8.1c">{c}</annotation></semantics></math> represents the predicted class confidence scores and <math id="Sx2.SSx4.p1.11.m9.2" class="ltx_Math" alttext="{l},{g}" display="inline"><semantics id="Sx2.SSx4.p1.11.m9.2a"><mrow id="Sx2.SSx4.p1.11.m9.2.3.2" xref="Sx2.SSx4.p1.11.m9.2.3.1.cmml"><mi id="Sx2.SSx4.p1.11.m9.1.1" xref="Sx2.SSx4.p1.11.m9.1.1.cmml">l</mi><mo id="Sx2.SSx4.p1.11.m9.2.3.2.1" xref="Sx2.SSx4.p1.11.m9.2.3.1.cmml">,</mo><mi id="Sx2.SSx4.p1.11.m9.2.2" xref="Sx2.SSx4.p1.11.m9.2.2.cmml">g</mi></mrow><annotation-xml encoding="MathML-Content" id="Sx2.SSx4.p1.11.m9.2b"><list id="Sx2.SSx4.p1.11.m9.2.3.1.cmml" xref="Sx2.SSx4.p1.11.m9.2.3.2"><ci id="Sx2.SSx4.p1.11.m9.1.1.cmml" xref="Sx2.SSx4.p1.11.m9.1.1">ğ‘™</ci><ci id="Sx2.SSx4.p1.11.m9.2.2.cmml" xref="Sx2.SSx4.p1.11.m9.2.2">ğ‘”</ci></list></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx4.p1.11.m9.2c">{l},{g}</annotation></semantics></math> denote the predicted box and the ground truth box, respectively. In this paper, we set <math id="Sx2.SSx4.p1.12.m10.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="Sx2.SSx4.p1.12.m10.1a"><mi id="Sx2.SSx4.p1.12.m10.1.1" xref="Sx2.SSx4.p1.12.m10.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx4.p1.12.m10.1b"><ci id="Sx2.SSx4.p1.12.m10.1.1.cmml" xref="Sx2.SSx4.p1.12.m10.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx4.p1.12.m10.1c">\alpha</annotation></semantics></math> to 1.0. We do not apply any loss constraints specifically to the scene decomposition module.</p>
</div>
</section>
<section id="Sx2.SSx5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Discussion</h3>

<div id="Sx2.SSx5.p1" class="ltx_para">
<p id="Sx2.SSx5.p1.1" class="ltx_p">In this part, we present detailed discussions from two aspects to deeply recognize our proposed method.</p>
</div>
<div id="Sx2.SSx5.p2" class="ltx_para">
<p id="Sx2.SSx5.p2.2" class="ltx_p"><span id="Sx2.SSx5.p2.2.1" class="ltx_text ltx_font_italic">(1) The illumination is a treasure not trash for the detector</span>.
Our proposed algorithm is established based on the fact that â€œthe illumination is a treasure not trash for the detectorâ€. Here we verify this fact from an experimental perspective.
In TableÂ <a href="#Sx2.T1" title="Table 1 â€£ Decomposing Scene for Detection â€£ Scene Decomposition Module â€£ Proposed Method â€£ Trash to Treasure: Low-Light Object Detection via Decomposition-and-Aggregation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we investigate different training patterns for enhancer + detector. Here, we consider two representative enhancers (RUAS and SCI) and the classical SSD detector with Feature Pyramid Networks.
The fine-tune detector and joint training methods cannot achieve good detection results.
Fortunately, after introducing the illumination for the detector (i.e., RUAS<sup id="Sx2.SSx5.p2.2.2" class="ltx_sup"><span id="Sx2.SSx5.p2.2.2.1" class="ltx_text ltx_font_italic">++</span></sup> and SCI<sup id="Sx2.SSx5.p2.2.3" class="ltx_sup"><span id="Sx2.SSx5.p2.2.3.1" class="ltx_text ltx_font_italic">++</span></sup>), the detection accuracy for these two enhancers all realize a significant boost (see the red-bold texts in the right bottom corner of the third and last rows in TableÂ <a href="#Sx2.T1" title="Table 1 â€£ Decomposing Scene for Detection â€£ Scene Decomposition Module â€£ Proposed Method â€£ Trash to Treasure: Low-Light Object Detection via Decomposition-and-Aggregation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). Deeply thinking, the performance improvement is benefited from the entire expression for scene information.
In a word, the experiments can fully verify the necessity of introducing illumination for the detector. Compared with existing manners, we can conclude that â€œillumination is a treasure not trash for the detectorâ€.</p>
</div>
<div id="Sx2.SSx5.p3" class="ltx_para">
<p id="Sx2.SSx5.p3.1" class="ltx_p"><span id="Sx2.SSx5.p3.1.1" class="ltx_text ltx_font_italic">(2) Our T2 realizes the detection-oriented enhancement</span>.
In our designed algorithm, the scene decomposition module is constructed based on physical knowledge (i.e., Retinex theory) for low-light image enhancement. Although we do not define the loss functions related to visual quality in the training phase, this module still implicitly possesses the tendentiousness for enhancing image quality. To verify it, we show the decomposed components among different methods (most methods come from TableÂ <a href="#Sx2.T1" title="Table 1 â€£ Decomposing Scene for Detection â€£ Scene Decomposition Module â€£ Proposed Method â€£ Trash to Treasure: Low-Light Object Detection via Decomposition-and-Aggregation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) on low-light scenarios, respectively.</p>
</div>
<div id="Sx2.SSx5.p4" class="ltx_para">
<p id="Sx2.SSx5.p4.4" class="ltx_p">Clearly, as seen in FigureÂ <a href="#Sx2.F4" title="Figure 4 â€£ Multi-Scale Feature Aggregator â€£ Semantic Aggregation Module â€£ Proposed Method â€£ Trash to Treasure: Low-Light Object Detection via Decomposition-and-Aggregation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, compared with the original enhanced outputs (i.e., RUAS and SCI), the other detection-oriented enhancers, even without visual quality-related constraints, all achieve enhancement effects. Notably, our fully designed T2 exhibits a certain image enhancement effect and is considered to possess friendly-detection features (as observed in the zoomed-in region). Moreover, RUAS<sup id="Sx2.SSx5.p4.4.1" class="ltx_sup"><span id="Sx2.SSx5.p4.4.1.1" class="ltx_text ltx_font_italic">++</span></sup> and SCI<sup id="Sx2.SSx5.p4.4.2" class="ltx_sup"><span id="Sx2.SSx5.p4.4.2.1" class="ltx_text ltx_font_italic">++</span></sup> perform better visual quality than RUAS<sup id="Sx2.SSx5.p4.4.3" class="ltx_sup">+</sup> and SCI<sup id="Sx2.SSx5.p4.4.4" class="ltx_sup">+</sup>, which also indicates that our T2 also realizes a mutual promotion for visual quality and detection accuracy (see TableÂ <a href="#Sx2.T1" title="Table 1 â€£ Decomposing Scene for Detection â€£ Scene Decomposition Module â€£ Proposed Method â€£ Trash to Treasure: Low-Light Object Detection via Decomposition-and-Aggregation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<figure id="Sx2.F5" class="ltx_figure">
<table id="Sx2.F5.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="Sx2.F5.2.2" class="ltx_tr">
<td id="Sx2.F5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><img src="/html/2309.03548/assets/x6.png" id="Sx2.F5.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="258" height="183" alt="Refer to caption"></td>
<td id="Sx2.F5.2.2.2" class="ltx_td ltx_nopad_l ltx_align_center"><img src="/html/2309.03548/assets/x7.png" id="Sx2.F5.2.2.2.g1" class="ltx_graphics ltx_img_landscape" width="258" height="183" alt="Refer to caption"></td>
</tr>
<tr id="Sx2.F5.2.3.1" class="ltx_tr">
<td id="Sx2.F5.2.3.1.1" class="ltx_td ltx_nopad_r ltx_align_center">(a) Pyramidbox</td>
<td id="Sx2.F5.2.3.1.2" class="ltx_td ltx_nopad_l ltx_align_center">(b) DSFD</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The PR curves of different state-of-the-art methods and our proposed approach on the DARK FACE dataset. <math id="Sx2.F5.5.m1.1" class="ltx_Math" alttext="(\cdot)^{\mathtt{P}}" display="inline"><semantics id="Sx2.F5.5.m1.1b"><msup id="Sx2.F5.5.m1.1.2" xref="Sx2.F5.5.m1.1.2.cmml"><mrow id="Sx2.F5.5.m1.1.2.2.2" xref="Sx2.F5.5.m1.1.2.cmml"><mo stretchy="false" id="Sx2.F5.5.m1.1.2.2.2.1" xref="Sx2.F5.5.m1.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="Sx2.F5.5.m1.1.1" xref="Sx2.F5.5.m1.1.1.cmml">â‹…</mo><mo stretchy="false" id="Sx2.F5.5.m1.1.2.2.2.2" xref="Sx2.F5.5.m1.1.2.cmml">)</mo></mrow><mi id="Sx2.F5.5.m1.1.2.3" xref="Sx2.F5.5.m1.1.2.3.cmml">ğ™¿</mi></msup><annotation-xml encoding="MathML-Content" id="Sx2.F5.5.m1.1c"><apply id="Sx2.F5.5.m1.1.2.cmml" xref="Sx2.F5.5.m1.1.2"><csymbol cd="ambiguous" id="Sx2.F5.5.m1.1.2.1.cmml" xref="Sx2.F5.5.m1.1.2">superscript</csymbol><ci id="Sx2.F5.5.m1.1.1.cmml" xref="Sx2.F5.5.m1.1.1">â‹…</ci><ci id="Sx2.F5.5.m1.1.2.3.cmml" xref="Sx2.F5.5.m1.1.2.3">ğ™¿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.F5.5.m1.1d">(\cdot)^{\mathtt{P}}</annotation></semantics></math> and <math id="Sx2.F5.6.m2.1" class="ltx_Math" alttext="(\cdot)^{\mathtt{F}}" display="inline"><semantics id="Sx2.F5.6.m2.1b"><msup id="Sx2.F5.6.m2.1.2" xref="Sx2.F5.6.m2.1.2.cmml"><mrow id="Sx2.F5.6.m2.1.2.2.2" xref="Sx2.F5.6.m2.1.2.cmml"><mo stretchy="false" id="Sx2.F5.6.m2.1.2.2.2.1" xref="Sx2.F5.6.m2.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="Sx2.F5.6.m2.1.1" xref="Sx2.F5.6.m2.1.1.cmml">â‹…</mo><mo stretchy="false" id="Sx2.F5.6.m2.1.2.2.2.2" xref="Sx2.F5.6.m2.1.2.cmml">)</mo></mrow><mi id="Sx2.F5.6.m2.1.2.3" xref="Sx2.F5.6.m2.1.2.3.cmml">ğ™µ</mi></msup><annotation-xml encoding="MathML-Content" id="Sx2.F5.6.m2.1c"><apply id="Sx2.F5.6.m2.1.2.cmml" xref="Sx2.F5.6.m2.1.2"><csymbol cd="ambiguous" id="Sx2.F5.6.m2.1.2.1.cmml" xref="Sx2.F5.6.m2.1.2">superscript</csymbol><ci id="Sx2.F5.6.m2.1.1.cmml" xref="Sx2.F5.6.m2.1.1">â‹…</ci><ci id="Sx2.F5.6.m2.1.2.3.cmml" xref="Sx2.F5.6.m2.1.2.3">ğ™µ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.F5.6.m2.1d">(\cdot)^{\mathtt{F}}</annotation></semantics></math> represent the version of using directly pre-trained detectors and fine-tune detectors, respectively.</figcaption>
</figure>
<figure id="Sx2.F6" class="ltx_figure">
<table id="Sx2.F6.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="Sx2.F6.1.1" class="ltx_tr">
<td id="Sx2.F6.1.1.1" class="ltx_td ltx_align_center"><img src="/html/2309.03548/assets/x8.png" id="Sx2.F6.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="530" height="181" alt="Refer to caption"></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>More visual results of object detection on the DARK FACE dataset. </figcaption>
</figure>
</section>
</section>
<section id="Sx3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Experimental Results</h2>

<figure id="Sx3.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Quantitative results of object detection on the ExDark dataset about the finetuned detector (SSD) on the enhanced results generated by all the compared methods. The best result is in red whereas the second best one is in blue.</figcaption>
<table id="Sx3.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Sx3.T2.1.1.1" class="ltx_tr">
<th id="Sx3.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding:1pt 4.0pt;">Method</th>
<th id="Sx3.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:1pt 4.0pt;">Bicycle</th>
<th id="Sx3.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:1pt 4.0pt;">Boat</th>
<th id="Sx3.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:1pt 4.0pt;">Bottle</th>
<th id="Sx3.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:1pt 4.0pt;">Bus</th>
<th id="Sx3.T2.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:1pt 4.0pt;">Car</th>
<th id="Sx3.T2.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:1pt 4.0pt;">Cat</th>
<th id="Sx3.T2.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:1pt 4.0pt;">Chair</th>
<th id="Sx3.T2.1.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:1pt 4.0pt;">Cup</th>
<th id="Sx3.T2.1.1.1.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:1pt 4.0pt;">Dog</th>
<th id="Sx3.T2.1.1.1.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:1pt 4.0pt;">Motorbike</th>
<th id="Sx3.T2.1.1.1.12" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:1pt 4.0pt;">People</th>
<th id="Sx3.T2.1.1.1.13" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding:1pt 4.0pt;">Table</th>
<th id="Sx3.T2.1.1.1.14" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:1pt 4.0pt;">mAP</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Sx3.T2.1.2.1" class="ltx_tr">
<th id="Sx3.T2.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:1pt 4.0pt;">Low-Light Input</th>
<td id="Sx3.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 4.0pt;">61.87</td>
<td id="Sx3.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 4.0pt;"><span id="Sx3.T2.1.2.1.3.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">54.42</span></td>
<td id="Sx3.T2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 4.0pt;">41.77</td>
<td id="Sx3.T2.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 4.0pt;">85.55</td>
<td id="Sx3.T2.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 4.0pt;"><span id="Sx3.T2.1.2.1.6.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">62.14</span></td>
<td id="Sx3.T2.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 4.0pt;">57.28</td>
<td id="Sx3.T2.1.2.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 4.0pt;">45.81</td>
<td id="Sx3.T2.1.2.1.9" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 4.0pt;">39.30</td>
<td id="Sx3.T2.1.2.1.10" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 4.0pt;">57.58</td>
<td id="Sx3.T2.1.2.1.11" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 4.0pt;">63.68</td>
<td id="Sx3.T2.1.2.1.12" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 4.0pt;">54.29</td>
<td id="Sx3.T2.1.2.1.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1pt 4.0pt;">51.88</td>
<td id="Sx3.T2.1.2.1.14" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 4.0pt;">56.30</td>
</tr>
<tr id="Sx3.T2.1.3.2" class="ltx_tr">
<th id="Sx3.T2.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:1pt 4.0pt;">DeepUPE</th>
<td id="Sx3.T2.1.3.2.2" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">63.99</td>
<td id="Sx3.T2.1.3.2.3" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">47.82</td>
<td id="Sx3.T2.1.3.2.4" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">40.34</td>
<td id="Sx3.T2.1.3.2.5" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">88.92</td>
<td id="Sx3.T2.1.3.2.6" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">62.12</td>
<td id="Sx3.T2.1.3.2.7" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;"><span id="Sx3.T2.1.3.2.7.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">58.52</span></td>
<td id="Sx3.T2.1.3.2.8" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">46.00</td>
<td id="Sx3.T2.1.3.2.9" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;"><span id="Sx3.T2.1.3.2.9.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">42.06</span></td>
<td id="Sx3.T2.1.3.2.10" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">59.07</td>
<td id="Sx3.T2.1.3.2.11" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;"><span id="Sx3.T2.1.3.2.11.1" class="ltx_text ltx_font_bold" style="color:#FF0000;">64.28</span></td>
<td id="Sx3.T2.1.3.2.12" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;"><span id="Sx3.T2.1.3.2.12.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">54.97</span></td>
<td id="Sx3.T2.1.3.2.13" class="ltx_td ltx_align_center ltx_border_r" style="padding:1pt 4.0pt;">51.86</td>
<td id="Sx3.T2.1.3.2.14" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">56.66</td>
</tr>
<tr id="Sx3.T2.1.4.3" class="ltx_tr">
<th id="Sx3.T2.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:1pt 4.0pt;">ZeroDCE</th>
<td id="Sx3.T2.1.4.3.2" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;"><span id="Sx3.T2.1.4.3.2.1" class="ltx_text ltx_font_bold" style="color:#FF0000;">68.04</span></td>
<td id="Sx3.T2.1.4.3.3" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">50.19</td>
<td id="Sx3.T2.1.4.3.4" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">41.61</td>
<td id="Sx3.T2.1.4.3.5" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;"><span id="Sx3.T2.1.4.3.5.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">88.94</span></td>
<td id="Sx3.T2.1.4.3.6" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">60.04</td>
<td id="Sx3.T2.1.4.3.7" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">54.76</td>
<td id="Sx3.T2.1.4.3.8" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;"><span id="Sx3.T2.1.4.3.8.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">48.62</span></td>
<td id="Sx3.T2.1.4.3.9" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">38.25</td>
<td id="Sx3.T2.1.4.3.10" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">58.68</td>
<td id="Sx3.T2.1.4.3.11" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">62.62</td>
<td id="Sx3.T2.1.4.3.12" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">54.79</td>
<td id="Sx3.T2.1.4.3.13" class="ltx_td ltx_align_center ltx_border_r" style="padding:1pt 4.0pt;"><span id="Sx3.T2.1.4.3.13.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">55.67</span></td>
<td id="Sx3.T2.1.4.3.14" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;"><span id="Sx3.T2.1.4.3.14.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">56.85</span></td>
</tr>
<tr id="Sx3.T2.1.5.4" class="ltx_tr">
<th id="Sx3.T2.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:1pt 4.0pt;">EnGAN</th>
<td id="Sx3.T2.1.5.4.2" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">60.24</td>
<td id="Sx3.T2.1.5.4.3" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">52.17</td>
<td id="Sx3.T2.1.5.4.4" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">38.31</td>
<td id="Sx3.T2.1.5.4.5" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;"><span id="Sx3.T2.1.5.4.5.1" class="ltx_text ltx_font_bold" style="color:#FF0000;">89.82</span></td>
<td id="Sx3.T2.1.5.4.6" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">60.60</td>
<td id="Sx3.T2.1.5.4.7" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">56.49</td>
<td id="Sx3.T2.1.5.4.8" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">47.45</td>
<td id="Sx3.T2.1.5.4.9" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">38.24</td>
<td id="Sx3.T2.1.5.4.10" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">57.75</td>
<td id="Sx3.T2.1.5.4.11" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">63.13</td>
<td id="Sx3.T2.1.5.4.12" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">52.59</td>
<td id="Sx3.T2.1.5.4.13" class="ltx_td ltx_align_center ltx_border_r" style="padding:1pt 4.0pt;">50.04</td>
<td id="Sx3.T2.1.5.4.14" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">55.57</td>
</tr>
<tr id="Sx3.T2.1.6.5" class="ltx_tr">
<th id="Sx3.T2.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:1pt 4.0pt;">KinD</th>
<td id="Sx3.T2.1.6.5.2" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">62.76</td>
<td id="Sx3.T2.1.6.5.3" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">54.07</td>
<td id="Sx3.T2.1.6.5.4" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">40.36</td>
<td id="Sx3.T2.1.6.5.5" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">84.62</td>
<td id="Sx3.T2.1.6.5.6" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">58.33</td>
<td id="Sx3.T2.1.6.5.7" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">53.07</td>
<td id="Sx3.T2.1.6.5.8" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">42.16</td>
<td id="Sx3.T2.1.6.5.9" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">35.86</td>
<td id="Sx3.T2.1.6.5.10" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;"><span id="Sx3.T2.1.6.5.10.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">59.90</span></td>
<td id="Sx3.T2.1.6.5.11" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">58.34</td>
<td id="Sx3.T2.1.6.5.12" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">53.75</td>
<td id="Sx3.T2.1.6.5.13" class="ltx_td ltx_align_center ltx_border_r" style="padding:1pt 4.0pt;">48.71</td>
<td id="Sx3.T2.1.6.5.14" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">54.33</td>
</tr>
<tr id="Sx3.T2.1.7.6" class="ltx_tr">
<th id="Sx3.T2.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:1pt 4.0pt;">DRBN</th>
<td id="Sx3.T2.1.7.6.2" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">63.54</td>
<td id="Sx3.T2.1.7.6.3" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">53.14</td>
<td id="Sx3.T2.1.7.6.4" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">36.35</td>
<td id="Sx3.T2.1.7.6.5" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">86.38</td>
<td id="Sx3.T2.1.7.6.6" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">58.83</td>
<td id="Sx3.T2.1.7.6.7" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">51.66</td>
<td id="Sx3.T2.1.7.6.8" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">39.85</td>
<td id="Sx3.T2.1.7.6.9" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">33.73</td>
<td id="Sx3.T2.1.7.6.10" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">55.50</td>
<td id="Sx3.T2.1.7.6.11" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">59.87</td>
<td id="Sx3.T2.1.7.6.12" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">52.12</td>
<td id="Sx3.T2.1.7.6.13" class="ltx_td ltx_align_center ltx_border_r" style="padding:1pt 4.0pt;">50.23</td>
<td id="Sx3.T2.1.7.6.14" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">53.44</td>
</tr>
<tr id="Sx3.T2.1.8.7" class="ltx_tr">
<th id="Sx3.T2.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:1pt 4.0pt;">RUAS</th>
<td id="Sx3.T2.1.8.7.2" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">63.49</td>
<td id="Sx3.T2.1.8.7.3" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">47.93</td>
<td id="Sx3.T2.1.8.7.4" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">40.61</td>
<td id="Sx3.T2.1.8.7.5" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">85.94</td>
<td id="Sx3.T2.1.8.7.6" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">60.19</td>
<td id="Sx3.T2.1.8.7.7" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">50.14</td>
<td id="Sx3.T2.1.8.7.8" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">44.46</td>
<td id="Sx3.T2.1.8.7.9" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">39.12</td>
<td id="Sx3.T2.1.8.7.10" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">54.37</td>
<td id="Sx3.T2.1.8.7.11" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">60.20</td>
<td id="Sx3.T2.1.8.7.12" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">53.52</td>
<td id="Sx3.T2.1.8.7.13" class="ltx_td ltx_align_center ltx_border_r" style="padding:1pt 4.0pt;">51.91</td>
<td id="Sx3.T2.1.8.7.14" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">54.32</td>
</tr>
<tr id="Sx3.T2.1.9.8" class="ltx_tr">
<th id="Sx3.T2.1.9.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:1pt 4.0pt;">SCI</th>
<td id="Sx3.T2.1.9.8.2" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;"><span id="Sx3.T2.1.9.8.2.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">65.41</span></td>
<td id="Sx3.T2.1.9.8.3" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">52.08</td>
<td id="Sx3.T2.1.9.8.4" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;"><span id="Sx3.T2.1.9.8.4.1" class="ltx_text ltx_font_bold" style="color:#FF0000;">42.31</span></td>
<td id="Sx3.T2.1.9.8.5" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">88.80</td>
<td id="Sx3.T2.1.9.8.6" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">60.44</td>
<td id="Sx3.T2.1.9.8.7" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">53.11</td>
<td id="Sx3.T2.1.9.8.8" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">47.21</td>
<td id="Sx3.T2.1.9.8.9" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">38.29</td>
<td id="Sx3.T2.1.9.8.10" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">56.57</td>
<td id="Sx3.T2.1.9.8.11" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">60.78</td>
<td id="Sx3.T2.1.9.8.12" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">54.80</td>
<td id="Sx3.T2.1.9.8.13" class="ltx_td ltx_align_center ltx_border_r" style="padding:1pt 4.0pt;">49.49</td>
<td id="Sx3.T2.1.9.8.14" class="ltx_td ltx_align_center" style="padding:1pt 4.0pt;">55.77</td>
</tr>
<tr id="Sx3.T2.1.10.9" class="ltx_tr">
<th id="Sx3.T2.1.10.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding:1pt 4.0pt;">Ours</th>
<td id="Sx3.T2.1.10.9.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1pt 4.0pt;">63.47</td>
<td id="Sx3.T2.1.10.9.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1pt 4.0pt;"><span id="Sx3.T2.1.10.9.3.1" class="ltx_text ltx_font_bold" style="color:#FF0000;">57.87</span></td>
<td id="Sx3.T2.1.10.9.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1pt 4.0pt;"><span id="Sx3.T2.1.10.9.4.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">41.92</span></td>
<td id="Sx3.T2.1.10.9.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1pt 4.0pt;">88.16</td>
<td id="Sx3.T2.1.10.9.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1pt 4.0pt;"><span id="Sx3.T2.1.10.9.6.1" class="ltx_text ltx_font_bold" style="color:#FF0000;">67.09</span></td>
<td id="Sx3.T2.1.10.9.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1pt 4.0pt;"><span id="Sx3.T2.1.10.9.7.1" class="ltx_text ltx_font_bold" style="color:#FF0000;">64.24</span></td>
<td id="Sx3.T2.1.10.9.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1pt 4.0pt;"><span id="Sx3.T2.1.10.9.8.1" class="ltx_text ltx_font_bold" style="color:#FF0000;">51.93</span></td>
<td id="Sx3.T2.1.10.9.9" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1pt 4.0pt;"><span id="Sx3.T2.1.10.9.9.1" class="ltx_text ltx_font_bold" style="color:#FF0000;">42.85</span></td>
<td id="Sx3.T2.1.10.9.10" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1pt 4.0pt;"><span id="Sx3.T2.1.10.9.10.1" class="ltx_text ltx_font_bold" style="color:#FF0000;">60.40</span></td>
<td id="Sx3.T2.1.10.9.11" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1pt 4.0pt;"><span id="Sx3.T2.1.10.9.11.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">63.83</span></td>
<td id="Sx3.T2.1.10.9.12" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1pt 4.0pt;"><span id="Sx3.T2.1.10.9.12.1" class="ltx_text ltx_font_bold" style="color:#FF0000;">58.69</span></td>
<td id="Sx3.T2.1.10.9.13" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding:1pt 4.0pt;"><span id="Sx3.T2.1.10.9.13.1" class="ltx_text ltx_font_bold" style="color:#FF0000;">56.48</span></td>
<td id="Sx3.T2.1.10.9.14" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1pt 4.0pt;"><span id="Sx3.T2.1.10.9.14.1" class="ltx_text ltx_font_bold" style="color:#FF0000;">59.74</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="Sx3.F7" class="ltx_figure">
<table id="Sx3.F7.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="Sx3.F7.1.1" class="ltx_tr">
<td id="Sx3.F7.1.1.1" class="ltx_td ltx_align_center"><img src="/html/2309.03548/assets/x9.png" id="Sx3.F7.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="530" height="163" alt="Refer to caption"></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>More visual results of object detection on the ExDark dataset.</figcaption>
</figure>
<section id="Sx3.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Implementation Details</h3>

<section id="Sx3.SSx1.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Datasets and Metrics</h4>

<div id="Sx3.SSx1.SSSx1.p1" class="ltx_para">
<p id="Sx3.SSx1.SSSx1.p1.3" class="ltx_p">We conducted our experiments using the DARK FACE dataset, which consists of 6000 low-light images captured in real-world environments. The images have a resolution of 1080<math id="Sx3.SSx1.SSSx1.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="Sx3.SSx1.SSSx1.p1.1.m1.1a"><mo id="Sx3.SSx1.SSSx1.p1.1.m1.1.1" xref="Sx3.SSx1.SSSx1.p1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.SSSx1.p1.1.m1.1b"><times id="Sx3.SSx1.SSSx1.p1.1.m1.1.1.cmml" xref="Sx3.SSx1.SSSx1.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.SSSx1.p1.1.m1.1c">\times</annotation></semantics></math>720 and contain a variable number of faces, typically ranging from 1 to 20. The labeled faces exhibit a wide range of scales, varying from 1<math id="Sx3.SSx1.SSSx1.p1.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="Sx3.SSx1.SSSx1.p1.2.m2.1a"><mo id="Sx3.SSx1.SSSx1.p1.2.m2.1.1" xref="Sx3.SSx1.SSSx1.p1.2.m2.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.SSSx1.p1.2.m2.1b"><times id="Sx3.SSx1.SSSx1.p1.2.m2.1.1.cmml" xref="Sx3.SSx1.SSSx1.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.SSSx1.p1.2.m2.1c">\times</annotation></semantics></math>2 to 335<math id="Sx3.SSx1.SSSx1.p1.3.m3.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="Sx3.SSx1.SSSx1.p1.3.m3.1a"><mo id="Sx3.SSx1.SSSx1.p1.3.m3.1.1" xref="Sx3.SSx1.SSSx1.p1.3.m3.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.SSSx1.p1.3.m3.1b"><times id="Sx3.SSx1.SSSx1.p1.3.m3.1.1.cmml" xref="Sx3.SSx1.SSSx1.p1.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.SSSx1.p1.3.m3.1c">\times</annotation></semantics></math>296. For our experiments, we randomly selected 1000 images for testing, while the remaining images were used for training. To evaluate the performance of our approach, we employed the mean Average Precision (mAP) as the evaluation metric.</p>
</div>
</section>
<section id="Sx3.SSx1.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Parameters Setting</h4>

<div id="Sx3.SSx1.SSSx2.p1" class="ltx_para">
<p id="Sx3.SSx1.SSSx2.p1.2" class="ltx_p">For model training, we employed SGD with a momentum of 0.9 and weight decay of 0.0005. The batch size was set to 4, and the initial learning rate was 0.0005. During the training stage, a face anchor was labeled as a positive anchor if it had an Intersection over Union (IoU) of over 0.3 with the ground truth. Furthermore, we maintained a ratio of 3:1 between negative and positive anchors. To ensure consistency, we resized the images to 640<math id="Sx3.SSx1.SSSx2.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="Sx3.SSx1.SSSx2.p1.1.m1.1a"><mo id="Sx3.SSx1.SSSx2.p1.1.m1.1.1" xref="Sx3.SSx1.SSSx2.p1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.SSSx2.p1.1.m1.1b"><times id="Sx3.SSx1.SSSx2.p1.1.m1.1.1.cmml" xref="Sx3.SSx1.SSSx2.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.SSSx2.p1.1.m1.1c">\times</annotation></semantics></math>640 for training and 1500<math id="Sx3.SSx1.SSSx2.p1.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="Sx3.SSx1.SSSx2.p1.2.m2.1a"><mo id="Sx3.SSx1.SSSx2.p1.2.m2.1.1" xref="Sx3.SSx1.SSSx2.p1.2.m2.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.SSSx2.p1.2.m2.1b"><times id="Sx3.SSx1.SSSx2.p1.2.m2.1.1.cmml" xref="Sx3.SSx1.SSSx2.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.SSSx2.p1.2.m2.1c">\times</annotation></semantics></math>1000 for testing. For effective bounding box selection, we applied non-maximum suppression using Jaccard overlap with a threshold of 0.3, retaining the top 750 high-confidence bounding boxes per image, as inspired by Neubeck and Van Goolâ€™s workÂ <cite class="ltx_cite ltx_citemacro_citep">(Neubeck and VanÂ Gool <a href="#bib.bib29" title="" class="ltx_ref">2006</a>)</cite>.</p>
</div>
</section>
<section id="Sx3.SSx1.SSSx3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Compared Methods</h4>

<div id="Sx3.SSx1.SSSx3.p1" class="ltx_para">
<p id="Sx3.SSx1.SSSx3.p1.1" class="ltx_p">In order to provide a comprehensive evaluation of our detection results, we employed both qualitative and quantitative analyses. Our detection framework is built upon two state-of-the-art face detectors, namely PyramidboxÂ <cite class="ltx_cite ltx_citemacro_citep">(Tang etÂ al. <a href="#bib.bib34" title="" class="ltx_ref">2018</a>)</cite> and DSFDÂ <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al. <a href="#bib.bib11" title="" class="ltx_ref">2019</a>)</cite>. Pyramidbox is a context-assisted single shot face detector that has shown impressive performance in various face detection benchmarks. DSFD is a dual shot face detector that leverages a dual-stage strategy to improve the detection performance, especially for small and hard faces.
To enhance the quality of the input images, we utilized a variety of image enhancement methods including DeepUPEÂ <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al. <a href="#bib.bib35" title="" class="ltx_ref">2019</a>)</cite>, DRBNÂ <cite class="ltx_cite ltx_citemacro_citep">(Yang etÂ al. <a href="#bib.bib42" title="" class="ltx_ref">2020a</a>)</cite>, EnGANÂ <cite class="ltx_cite ltx_citemacro_citep">(Jiang etÂ al. <a href="#bib.bib6" title="" class="ltx_ref">2021</a>)</cite>, FIDEÂ <cite class="ltx_cite ltx_citemacro_citep">(Xu etÂ al. <a href="#bib.bib40" title="" class="ltx_ref">2020</a>)</cite>, KinDÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhang, Zhang, and Guo <a href="#bib.bib47" title="" class="ltx_ref">2019</a>)</cite>, RUASÂ <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al. <a href="#bib.bib19" title="" class="ltx_ref">2021c</a>)</cite>, ZeroDCEÂ <cite class="ltx_cite ltx_citemacro_citep">(Guo etÂ al. <a href="#bib.bib4" title="" class="ltx_ref">2020</a>)</cite>, and SCIÂ <cite class="ltx_cite ltx_citemacro_citep">(Ma etÂ al. <a href="#bib.bib27" title="" class="ltx_ref">2022a</a>)</cite>. In addition to these image enhancement methods, we also compared our results with two dedicated low-light face detection solutions, HLAÂ <cite class="ltx_cite ltx_citemacro_citep">(Wang, Yang, and Liu <a href="#bib.bib37" title="" class="ltx_ref">2021</a>)</cite> and REGÂ <cite class="ltx_cite ltx_citemacro_citep">(Liang etÂ al. <a href="#bib.bib12" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
</section>
</section>
<section id="Sx3.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Evaluations on the DARK FACE dataset</h3>

<section id="Sx3.SSx2.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Quantitative comparisons</h4>

<div id="Sx3.SSx2.SSSx1.p1" class="ltx_para">
<p id="Sx3.SSx2.SSSx1.p1.1" class="ltx_p">In-depth analysis was conducted by plotting Precision-Recall (PR) curves for two detectors, as illustrated in FigureÂ <a href="#Sx2.F5" title="Figure 5 â€£ Discussion â€£ Proposed Method â€£ Trash to Treasure: Low-Light Object Detection via Decomposition-and-Aggregation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. This comparison revealed two noteworthy conclusions.</p>
</div>
<div id="Sx3.SSx2.SSSx1.p2" class="ltx_para">
<p id="Sx3.SSx2.SSSx1.p2.1" class="ltx_p">Firstly, our proposed method exhibited significant advancements when compared to both the combination of advanced enhancer and detector, as well as mainstream low-light face detectors. To be more specific, our method achieved a remarkable 14.8% higher mean Average Precision (mAP) than HLA and an impressive 8.2% higher mAP than the best hybrid scheme, which involved the finetuning version of SCI and DSFD. Importantly, our proposed method consistently outperformed different detectors across various numerical metrics.</p>
</div>
<div id="Sx3.SSx2.SSSx1.p3" class="ltx_para">
<p id="Sx3.SSx2.SSSx1.p3.1" class="ltx_p">Secondly, these findings lend support to our initial motivation that solely relying on visual aesthetics may not necessarily lead to improved detection performance. Although the jointly fine-tuned methods demonstrated some enhancement in performance, there remains substantial room for further improvement.</p>
</div>
</section>
<section id="Sx3.SSx2.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Qualitative comparisons</h4>

<div id="Sx3.SSx2.SSSx2.p1" class="ltx_para">
<p id="Sx3.SSx2.SSSx2.p1.1" class="ltx_p">We selected three representative scenarios to demonstrate our detection precision and visual effects straightforwardly, including extreme darkness, dense crowd, and low contrast. FigureÂ <a href="#Sx2.F6" title="Figure 6 â€£ Discussion â€£ Proposed Method â€£ Trash to Treasure: Low-Light Object Detection via Decomposition-and-Aggregation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> demonstrated four groups of visual and detection results comparisons on the DARK FACE dataset. We can observe that our methods obtain two obvious advantages compared with these competitors. Firstly, our method could effectively extract efficient semantic information from illumination for detection and realize high accuracy (<em id="Sx3.SSx2.SSSx2.p1.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> the people in the distance in each group of examples).
Secondly, benefiting from the guidance of follow-up detection, our method could decompose more suitable illumination to preserve the detection performance. In contrast, our method had less noise interference in detection.</p>
</div>
<div id="Sx3.SSx2.SSSx2.p2" class="ltx_para">
<p id="Sx3.SSx2.SSSx2.p2.1" class="ltx_p">Thus, by leveraging the decomposition and aggregation, the proposed method actually realized the representative feature extraction and utilization, which can simultaneously provide better scene understanding.</p>
</div>
</section>
</section>
<section id="Sx3.SSx3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Evaluations on the ExDARK dataset</h3>

<div id="Sx3.SSx3.p1" class="ltx_para">
<p id="Sx3.SSx3.p1.1" class="ltx_p">In order to fully verify the performance of detection, we presented more results on the ExDark<cite class="ltx_cite ltx_citemacro_citep">(Loh and Chan <a href="#bib.bib23" title="" class="ltx_ref">2019</a>)</cite> low-light object detection dataset. This dataset involves 12 categories. 737 images were randomly sampled for testing and the remaining 6626 low-light images were used for training and validation. For this dataset, we set the maximum epoch as 100, and the batch size as 32. We used Adam and the learning rate was initialized to 3e-5. We utilized the SSD detection model as the baseline in all methods for the ExDark dataset. We compared our method with the â€Enhancer+Detectorâ€ detection pattern, which considered low-light image enhancement as a pre-processing method<cite class="ltx_cite ltx_citemacro_citep">(Ma etÂ al. <a href="#bib.bib28" title="" class="ltx_ref">2022b</a>)</cite>.
TableÂ <a href="#Sx3.T2" title="Table 2 â€£ Experimental Results â€£ Trash to Treasure: Low-Light Object Detection via Decomposition-and-Aggregation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (score-threhold=0.5) reported the detection results on the specific class. It could be easily observed that our method was significantly superior to other methods. Furthermore, FigureÂ <a href="#Sx3.F7" title="Figure 7 â€£ Experimental Results â€£ Trash to Treasure: Low-Light Object Detection via Decomposition-and-Aggregation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> provided the visual comparison among different methods on the ExDark dataset, our method detected more correct objects and made fewer errors.</p>
</div>
<figure id="Sx3.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Ablation study among different settings.</figcaption>
<table id="Sx3.T3.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Sx3.T3.3.3" class="ltx_tr">
<th id="Sx3.T3.3.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:1.5pt 10.0pt;"><span id="Sx3.T3.3.3.4.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="Sx3.T3.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:1.5pt 10.0pt;"><math id="Sx3.T3.1.1.1.m1.1" class="ltx_Math" alttext="\mathbf{L}" display="inline"><semantics id="Sx3.T3.1.1.1.m1.1a"><mi id="Sx3.T3.1.1.1.m1.1.1" xref="Sx3.T3.1.1.1.m1.1.1.cmml">ğ‹</mi><annotation-xml encoding="MathML-Content" id="Sx3.T3.1.1.1.m1.1b"><ci id="Sx3.T3.1.1.1.m1.1.1.cmml" xref="Sx3.T3.1.1.1.m1.1.1">ğ‹</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.T3.1.1.1.m1.1c">\mathbf{L}</annotation></semantics></math></th>
<th id="Sx3.T3.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:1.5pt 10.0pt;"><math id="Sx3.T3.2.2.2.m1.1" class="ltx_Math" alttext="\mathbf{I}" display="inline"><semantics id="Sx3.T3.2.2.2.m1.1a"><mi id="Sx3.T3.2.2.2.m1.1.1" xref="Sx3.T3.2.2.2.m1.1.1.cmml">ğˆ</mi><annotation-xml encoding="MathML-Content" id="Sx3.T3.2.2.2.m1.1b"><ci id="Sx3.T3.2.2.2.m1.1.1.cmml" xref="Sx3.T3.2.2.2.m1.1.1">ğˆ</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.T3.2.2.2.m1.1c">\mathbf{I}</annotation></semantics></math></th>
<th id="Sx3.T3.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:1.5pt 10.0pt;"><math id="Sx3.T3.3.3.3.m1.1" class="ltx_Math" alttext="\mathbf{R}" display="inline"><semantics id="Sx3.T3.3.3.3.m1.1a"><mi id="Sx3.T3.3.3.3.m1.1.1" xref="Sx3.T3.3.3.3.m1.1.1.cmml">ğ‘</mi><annotation-xml encoding="MathML-Content" id="Sx3.T3.3.3.3.m1.1b"><ci id="Sx3.T3.3.3.3.m1.1.1.cmml" xref="Sx3.T3.3.3.3.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.T3.3.3.3.m1.1c">\mathbf{R}</annotation></semantics></math></th>
<th id="Sx3.T3.3.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:1.5pt 10.0pt;"><span id="Sx3.T3.3.3.5.1" class="ltx_text ltx_font_bold">FPN</span></th>
<th id="Sx3.T3.3.3.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:1.5pt 10.0pt;"><span id="Sx3.T3.3.3.6.1" class="ltx_text ltx_font_bold">mAP (%)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Sx3.T3.3.4.1" class="ltx_tr">
<td id="Sx3.T3.3.4.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.5pt 10.0pt;">A</td>
<td id="Sx3.T3.3.4.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.5pt 10.0pt;">âœ“</td>
<td id="Sx3.T3.3.4.1.3" class="ltx_td ltx_border_t" style="padding:1.5pt 10.0pt;"></td>
<td id="Sx3.T3.3.4.1.4" class="ltx_td ltx_border_t" style="padding:1.5pt 10.0pt;"></td>
<td id="Sx3.T3.3.4.1.5" class="ltx_td ltx_border_t" style="padding:1.5pt 10.0pt;"></td>
<td id="Sx3.T3.3.4.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.5pt 10.0pt;">61.13</td>
</tr>
<tr id="Sx3.T3.3.5.2" class="ltx_tr">
<td id="Sx3.T3.3.5.2.1" class="ltx_td ltx_align_center" style="padding:1.5pt 10.0pt;">B</td>
<td id="Sx3.T3.3.5.2.2" class="ltx_td ltx_align_center" style="padding:1.5pt 10.0pt;">âœ“</td>
<td id="Sx3.T3.3.5.2.3" class="ltx_td" style="padding:1.5pt 10.0pt;"></td>
<td id="Sx3.T3.3.5.2.4" class="ltx_td" style="padding:1.5pt 10.0pt;"></td>
<td id="Sx3.T3.3.5.2.5" class="ltx_td ltx_align_center" style="padding:1.5pt 10.0pt;">âœ“</td>
<td id="Sx3.T3.3.5.2.6" class="ltx_td ltx_align_center" style="padding:1.5pt 10.0pt;">66.42</td>
</tr>
<tr id="Sx3.T3.3.6.3" class="ltx_tr">
<td id="Sx3.T3.3.6.3.1" class="ltx_td ltx_align_center" style="padding:1.5pt 10.0pt;">C</td>
<td id="Sx3.T3.3.6.3.2" class="ltx_td" style="padding:1.5pt 10.0pt;"></td>
<td id="Sx3.T3.3.6.3.3" class="ltx_td ltx_align_center" style="padding:1.5pt 10.0pt;">âœ“</td>
<td id="Sx3.T3.3.6.3.4" class="ltx_td" style="padding:1.5pt 10.0pt;"></td>
<td id="Sx3.T3.3.6.3.5" class="ltx_td" style="padding:1.5pt 10.0pt;"></td>
<td id="Sx3.T3.3.6.3.6" class="ltx_td ltx_align_center" style="padding:1.5pt 10.0pt;">59.28</td>
</tr>
<tr id="Sx3.T3.3.7.4" class="ltx_tr">
<td id="Sx3.T3.3.7.4.1" class="ltx_td ltx_align_center" style="padding:1.5pt 10.0pt;">D</td>
<td id="Sx3.T3.3.7.4.2" class="ltx_td" style="padding:1.5pt 10.0pt;"></td>
<td id="Sx3.T3.3.7.4.3" class="ltx_td" style="padding:1.5pt 10.0pt;"></td>
<td id="Sx3.T3.3.7.4.4" class="ltx_td ltx_align_center" style="padding:1.5pt 10.0pt;">âœ“</td>
<td id="Sx3.T3.3.7.4.5" class="ltx_td" style="padding:1.5pt 10.0pt;"></td>
<td id="Sx3.T3.3.7.4.6" class="ltx_td ltx_align_center" style="padding:1.5pt 10.0pt;">66.94</td>
</tr>
<tr id="Sx3.T3.3.8.5" class="ltx_tr">
<td id="Sx3.T3.3.8.5.1" class="ltx_td ltx_align_center" style="padding:1.5pt 10.0pt;">E</td>
<td id="Sx3.T3.3.8.5.2" class="ltx_td" style="padding:1.5pt 10.0pt;"></td>
<td id="Sx3.T3.3.8.5.3" class="ltx_td" style="padding:1.5pt 10.0pt;"></td>
<td id="Sx3.T3.3.8.5.4" class="ltx_td ltx_align_center" style="padding:1.5pt 10.0pt;">âœ“</td>
<td id="Sx3.T3.3.8.5.5" class="ltx_td ltx_align_center" style="padding:1.5pt 10.0pt;">âœ“</td>
<td id="Sx3.T3.3.8.5.6" class="ltx_td ltx_align_center" style="padding:1.5pt 10.0pt;">67.68</td>
</tr>
<tr id="Sx3.T3.3.9.6" class="ltx_tr">
<td id="Sx3.T3.3.9.6.1" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1.5pt 10.0pt;">T2</td>
<td id="Sx3.T3.3.9.6.2" class="ltx_td ltx_border_bb" style="padding:1.5pt 10.0pt;"></td>
<td id="Sx3.T3.3.9.6.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1.5pt 10.0pt;">âœ“</td>
<td id="Sx3.T3.3.9.6.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1.5pt 10.0pt;">âœ“</td>
<td id="Sx3.T3.3.9.6.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1.5pt 10.0pt;">âœ“</td>
<td id="Sx3.T3.3.9.6.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1.5pt 10.0pt;"><span id="Sx3.T3.3.9.6.6.1" class="ltx_text ltx_font_bold">69.67</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="Sx3.SSx4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Algorithmic Analyses</h3>

<section id="Sx3.SSx4.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Ablation Study</h4>

<div id="Sx3.SSx4.SSSx1.p1" class="ltx_para">
<p id="Sx3.SSx4.SSSx1.p1.4" class="ltx_p">We firstly provided five variants (denoted as â€œA<math id="Sx3.SSx4.SSSx1.p1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="Sx3.SSx4.SSSx1.p1.1.m1.1a"><mo id="Sx3.SSx4.SSSx1.p1.1.m1.1.1" xref="Sx3.SSx4.SSSx1.p1.1.m1.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="Sx3.SSx4.SSSx1.p1.1.m1.1b"><csymbol cd="latexml" id="Sx3.SSx4.SSSx1.p1.1.m1.1.1.cmml" xref="Sx3.SSx4.SSSx1.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx4.SSSx1.p1.1.m1.1c">\sim</annotation></semantics></math>Eâ€) under different settings to demonstrate the proposed mechanisms. Concrete numerical results were reported in TableÂ <a href="#Sx3.T3" title="Table 3 â€£ Evaluations on the ExDARK dataset â€£ Experimental Results â€£ Trash to Treasure: Low-Light Object Detection via Decomposition-and-Aggregation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (when both illumination and reflectance were used as input to the feature extractor, the FPN represented our semantic aggregation FPN). We can summarize three vital observations from this table. Firstly, the illumination layer also maintains enough information capacity, which almost obtains a comparable result compared with the version using low-light inputs <math id="Sx3.SSx4.SSSx1.p1.2.m2.1" class="ltx_Math" alttext="\mathbf{L}" display="inline"><semantics id="Sx3.SSx4.SSSx1.p1.2.m2.1a"><mi id="Sx3.SSx4.SSSx1.p1.2.m2.1.1" xref="Sx3.SSx4.SSSx1.p1.2.m2.1.1.cmml">ğ‹</mi><annotation-xml encoding="MathML-Content" id="Sx3.SSx4.SSSx1.p1.2.m2.1b"><ci id="Sx3.SSx4.SSSx1.p1.2.m2.1.1.cmml" xref="Sx3.SSx4.SSSx1.p1.2.m2.1.1">ğ‹</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx4.SSSx1.p1.2.m2.1c">\mathbf{L}</annotation></semantics></math>. Furthermore, reflection alone cannot obtain the best performance. Thus, it demonstrates the reasonability to extract the information from illumination. Secondly,
FPN network plays an important role to improve the performance, which can significantly improve 8.6% and 0.74 higher mAP when the
inputs are low-light images <math id="Sx3.SSx4.SSSx1.p1.3.m3.1" class="ltx_Math" alttext="\mathbf{L}" display="inline"><semantics id="Sx3.SSx4.SSSx1.p1.3.m3.1a"><mi id="Sx3.SSx4.SSSx1.p1.3.m3.1.1" xref="Sx3.SSx4.SSSx1.p1.3.m3.1.1.cmml">ğ‹</mi><annotation-xml encoding="MathML-Content" id="Sx3.SSx4.SSSx1.p1.3.m3.1b"><ci id="Sx3.SSx4.SSSx1.p1.3.m3.1.1.cmml" xref="Sx3.SSx4.SSSx1.p1.3.m3.1.1">ğ‹</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx4.SSSx1.p1.3.m3.1c">\mathbf{L}</annotation></semantics></math> and reflection <math id="Sx3.SSx4.SSSx1.p1.4.m4.1" class="ltx_Math" alttext="\mathbf{R}" display="inline"><semantics id="Sx3.SSx4.SSSx1.p1.4.m4.1a"><mi id="Sx3.SSx4.SSSx1.p1.4.m4.1.1" xref="Sx3.SSx4.SSSx1.p1.4.m4.1.1.cmml">ğ‘</mi><annotation-xml encoding="MathML-Content" id="Sx3.SSx4.SSSx1.p1.4.m4.1b"><ci id="Sx3.SSx4.SSSx1.p1.4.m4.1.1.cmml" xref="Sx3.SSx4.SSSx1.p1.4.m4.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx4.SSSx1.p1.4.m4.1c">\mathbf{R}</annotation></semantics></math> respectively. Therefore, we use the semantic aggregation FPN in T2.
For the performance of using scene information including illumination and reflectance, accuracy is significantly improved.</p>
</div>
</section>
</section>
</section>
<section id="Sx4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Concluding Remarks</h2>

<div id="Sx4.p1" class="ltx_para">
<p id="Sx4.p1.1" class="ltx_p">In this work, we proposed a new low-light object detector consisting of the scene decomposition and the semantic aggregation modules. We first analyzed the latent relationship between enhancer and detector to definitely point out the two key challenges. Then we constructed a scene decomposition module to present scene characteristics. A semantic aggregation composed of a weight-sharing feature extractor and multi-scale feature aggregator is proposed to integrate the scene information in the feature space. Finally, extensive experiments are performed to reveal our superiority.</p>
</div>
<div id="Sx4.p2" class="ltx_para">
<p id="Sx4.p2.1" class="ltx_p"><span id="Sx4.p2.1.1" class="ltx_text ltx_font_italic">Broader Impacts</span>. How to effectively characterize scene information is an inherent appeal in designing our method. It is actually a general research focus for a variety of adverse conditions. Decomposing the scenes in the image space and aggregating the scenes in the feature space are crucial measures for our proposed T2. This way provides a new perspective to understand and handle the pattern of enhancer + detector, which will rekindle the enthusiasm to research the task of detection in adverse conditions.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2018)</span>
<span class="ltx_bibblock">
Chen, C.; Chen, Q.; Xu, J.; and Koltun, V. 2018.

</span>
<span class="ltx_bibblock">Learning to see in the dark.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, 3291â€“3300.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui etÂ al. (2021)</span>
<span class="ltx_bibblock">
Cui, Z.; Qi, G.-J.; Gu, L.; You, S.; Zhang, Z.; and Harada, T. 2021.

</span>
<span class="ltx_bibblock">Multitask aet with orthogonal tangent regularity for dark object
detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</em>, 2553â€“2562.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Girshick (2015)</span>
<span class="ltx_bibblock">
Girshick, R. 2015.

</span>
<span class="ltx_bibblock">Fast r-cnn.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</em>, 1440â€“1448.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo etÂ al. (2020)</span>
<span class="ltx_bibblock">
Guo, C.; Li, C.; Guo, J.; Loy, C.Â C.; Hou, J.; Kwong, S.; and Cong, R. 2020.

</span>
<span class="ltx_bibblock">Zero-reference deep curve estimation for low-light image enhancement.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, 1780â€“1789.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He etÂ al. (2016)</span>
<span class="ltx_bibblock">
He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, 770â€“778.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang etÂ al. (2021)</span>
<span class="ltx_bibblock">
Jiang, Y.; Gong, X.; Liu, D.; Cheng, Y.; Fang, C.; Shen, X.; Yang, J.; Zhou,
P.; and Wang, Z. 2021.

</span>
<span class="ltx_bibblock">Enlightengan: Deep light enhancement without paired supervision.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Image Processing</em>, 30: 2340â€“2349.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin etÂ al. (2021)</span>
<span class="ltx_bibblock">
Jin, D.; Ma, L.; Liu, R.; and Fan, X. 2021.

</span>
<span class="ltx_bibblock">Bridging the gap between low-light scenes: Bilevel learning for fast
adaptation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 29th ACM International Conference on
Multimedia</em>, 2401â€“2409.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jobson, Rahman, and Woodell (1997)</span>
<span class="ltx_bibblock">
Jobson, D.Â J.; Rahman, Z.-u.; and Woodell, G.Â A. 1997.

</span>
<span class="ltx_bibblock">A multiscale retinex for bridging the gap between color images and
the human observation of scenes.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Image processing</em>, 6(7): 965â€“976.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Land and McCann (1971)</span>
<span class="ltx_bibblock">
Land, E.Â H.; and McCann, J.Â J. 1971.

</span>
<span class="ltx_bibblock">Lightness and retinex theory.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Josa</em>, 61(1): 1â€“11.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2018)</span>
<span class="ltx_bibblock">
Li, C.; Guo, J.; Porikli, F.; and Pang, Y. 2018.

</span>
<span class="ltx_bibblock">LightenNet: A convolutional neural network for weakly illuminated
image enhancement.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Pattern recognition letters</em>, 104: 15â€“22.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2019)</span>
<span class="ltx_bibblock">
Li, J.; Wang, Y.; Wang, C.; Tai, Y.; Qian, J.; Yang, J.; Wang, C.; Li, J.; and
Huang, F. 2019.

</span>
<span class="ltx_bibblock">DSFD: dual shot face detector.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, 5060â€“5069.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang etÂ al. (2021)</span>
<span class="ltx_bibblock">
Liang, J.; Wang, J.; Quan, Y.; Chen, T.; Liu, J.; Ling, H.; and Xu, Y. 2021.

</span>
<span class="ltx_bibblock">Recurrent exposure generation for low-light face detection.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Multimedia</em>, 24: 1609â€“1621.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin etÂ al. (2017)</span>
<span class="ltx_bibblock">
Lin, T.-Y.; DollÃ¡r, P.; Girshick, R.; He, K.; Hariharan, B.; and Belongie,
S. 2017.

</span>
<span class="ltx_bibblock">Feature pyramid networks for object detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, 2117â€“2125.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2021a)</span>
<span class="ltx_bibblock">
Liu, J.; Fan, X.; Jiang, J.; Liu, R.; and Luo, Z. 2021a.

</span>
<span class="ltx_bibblock">Learning a deep multi-scale feature ensemble and an edge-attention
guidance for image fusion.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Circuits and Systems for Video
Technology</em>, 32(1): 105â€“119.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2021b)</span>
<span class="ltx_bibblock">
Liu, J.; Xu, D.; Yang, W.; Fan, M.; and Huang, H. 2021b.

</span>
<span class="ltx_bibblock">Benchmarking low-light image enhancement and beyond.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">International Journal of Computer Vision</em>, 129: 1153â€“1184.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2020)</span>
<span class="ltx_bibblock">
Liu, R.; Fan, X.; Zhu, M.; Hou, M.; and Luo, Z. 2020.

</span>
<span class="ltx_bibblock">Real-world underwater enhancement: Challenges, benchmarks, and
solutions under natural light.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on circuits and systems for video
technology</em>, 30(12): 4861â€“4875.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2012)</span>
<span class="ltx_bibblock">
Liu, R.; Lin, Z.; DeÂ la Torre, F.; and Su, Z. 2012.

</span>
<span class="ltx_bibblock">Fixed-rank representation for unsupervised visual learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">2012 ieee conference on computer vision and pattern
recognition</em>, 598â€“605. IEEE.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2022a)</span>
<span class="ltx_bibblock">
Liu, R.; Ma, L.; Ma, T.; Fan, X.; and Luo, Z. 2022a.

</span>
<span class="ltx_bibblock">Learning with nested scene modeling and cooperative architecture
search for low-light vision.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine
Intelligence</em>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2021c)</span>
<span class="ltx_bibblock">
Liu, R.; Ma, L.; Zhang, J.; Fan, X.; and Luo, Z. 2021c.

</span>
<span class="ltx_bibblock">Retinex-inspired unrolling with cooperative prior architecture search
for low-light image enhancement.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, 10561â€“10570.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2016)</span>
<span class="ltx_bibblock">
Liu, W.; Anguelov, D.; Erhan, D.; Szegedy, C.; Reed, S.; Fu, C.-Y.; and Berg,
A.Â C. 2016.

</span>
<span class="ltx_bibblock">Ssd: Single shot multibox detector.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">European conference on computer vision</em>, 21â€“37. Springer.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2022b)</span>
<span class="ltx_bibblock">
Liu, W.; Ren, G.; Yu, R.; Guo, S.; Zhu, J.; and Zhang, L. 2022b.

</span>
<span class="ltx_bibblock">Image-adaptive YOLO for object detection in adverse weather
conditions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</em>, volumeÂ 36, 1792â€“1800.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2021d)</span>
<span class="ltx_bibblock">
Liu, Z.; Lin, Y.; Cao, Y.; Hu, H.; Wei, Y.; Zhang, Z.; Lin, S.; and Guo, B.
2021d.

</span>
<span class="ltx_bibblock">Swin transformer: Hierarchical vision transformer using shifted
windows.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF international conference on
computer vision</em>, 10012â€“10022.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loh and Chan (2019)</span>
<span class="ltx_bibblock">
Loh, Y.Â P.; and Chan, C.Â S. 2019.

</span>
<span class="ltx_bibblock">Getting to know low-light images with the exclusively dark dataset.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Computer Vision and Image Understanding</em>, 178: 30â€“42.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lore, Akintayo, and Sarkar (2017)</span>
<span class="ltx_bibblock">
Lore, K.Â G.; Akintayo, A.; and Sarkar, S. 2017.

</span>
<span class="ltx_bibblock">LLNet: A deep autoencoder approach to natural low-light image
enhancement.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Pattern Recognition</em>, 61: 650â€“662.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lv, Li, and Lu (2021)</span>
<span class="ltx_bibblock">
Lv, F.; Li, Y.; and Lu, F. 2021.

</span>
<span class="ltx_bibblock">Attention guided low-light image enhancement with a large scale
low-light simulation dataset.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">International Journal of Computer Vision</em>, 129(7): 2175â€“2193.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma etÂ al. (2023)</span>
<span class="ltx_bibblock">
Ma, L.; Jin, D.; An, N.; Liu, J.; Fan, X.; and Liu, R. 2023.

</span>
<span class="ltx_bibblock">Bilevel Fast Scene Adaptation for Low-Light Image Enhancement.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.01343</em>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma etÂ al. (2022a)</span>
<span class="ltx_bibblock">
Ma, L.; Ma, T.; Liu, R.; Fan, X.; and Luo, Z. 2022a.

</span>
<span class="ltx_bibblock">Toward Fast, Flexible, and Robust Low-Light Image Enhancement.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, 5637â€“5646.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma etÂ al. (2022b)</span>
<span class="ltx_bibblock">
Ma, T.; Ma, L.; Fan, X.; Luo, Z.; and Liu, R. 2022b.

</span>
<span class="ltx_bibblock">PIA: Parallel Architecture with Illumination Allocator for Joint
Enhancement and Detection in Low-Light.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 30th ACM International Conference on
Multimedia</em>, 2070â€“2078.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Neubeck and VanÂ Gool (2006)</span>
<span class="ltx_bibblock">
Neubeck, A.; and VanÂ Gool, L. 2006.

</span>
<span class="ltx_bibblock">Efficient non-maximum suppression.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">18th International Conference on Pattern Recognition
(ICPRâ€™06)</em>, volumeÂ 3, 850â€“855. IEEE.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Piao etÂ al. (2019)</span>
<span class="ltx_bibblock">
Piao, Y.; Ji, W.; Li, J.; Zhang, M.; and Lu, H. 2019.

</span>
<span class="ltx_bibblock">Depth-induced multi-scale recurrent attention network for saliency
detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF international conference on
computer vision</em>, 7254â€“7263.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Piao etÂ al. (2020)</span>
<span class="ltx_bibblock">
Piao, Y.; Rong, Z.; Zhang, M.; Ren, W.; and Lu, H. 2020.

</span>
<span class="ltx_bibblock">A2dele: Adaptive and attentive depth distiller for efficient RGB-D
salient object detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition</em>, 9060â€“9069.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang etÂ al. (2020)</span>
<span class="ltx_bibblock">
Tang, H.; Li, Z.; Peng, Z.; and Tang, J. 2020.

</span>
<span class="ltx_bibblock">Blockmix: meta regularization and self-calibrated inference for
metric-based meta-learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 28th ACM international conference on
multimedia</em>, 610â€“618.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang etÂ al. (2022)</span>
<span class="ltx_bibblock">
Tang, H.; Yuan, C.; Li, Z.; and Tang, J. 2022.

</span>
<span class="ltx_bibblock">Learning attention-guided pyramidal features for few-shot
fine-grained recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Pattern Recognition</em>, 130: 108792.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang etÂ al. (2018)</span>
<span class="ltx_bibblock">
Tang, X.; Du, D.Â K.; He, Z.; and Liu, J. 2018.

</span>
<span class="ltx_bibblock">Pyramidbox: A context-assisted single shot face detector.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings of the European conference on computer vision
(ECCV)</em>, 797â€“813.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2019)</span>
<span class="ltx_bibblock">
Wang, R.; Zhang, Q.; Fu, C.-W.; Shen, X.; Zheng, W.-S.; and Jia, J. 2019.

</span>
<span class="ltx_bibblock">Underexposed photo enhancement using deep illumination estimation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, 6849â€“6857.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2022)</span>
<span class="ltx_bibblock">
Wang, W.; Wang, X.; Yang, W.; and Liu, J. 2022.

</span>
<span class="ltx_bibblock">Unsupervised face detection in the dark.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine
Intelligence</em>, 45(1): 1250â€“1266.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang, Yang, and Liu (2021)</span>
<span class="ltx_bibblock">
Wang, W.; Yang, W.; and Liu, J. 2021.

</span>
<span class="ltx_bibblock">Hla-face: Joint high-low adaptation for low light face detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, 16195â€“16204.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei etÂ al. (2018)</span>
<span class="ltx_bibblock">
Wei, C.; Wang, W.; Yang, W.; and Liu, J. 2018.

</span>
<span class="ltx_bibblock">Deep retinex decomposition for low-light enhancement.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1808.04560</em>.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu, Lin, and Zha (2019)</span>
<span class="ltx_bibblock">
Wu, J.; Lin, Z.; and Zha, H. 2019.

</span>
<span class="ltx_bibblock">Essential tensor learning for multi-view spectral clustering.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Image Processing</em>, 28(12): 5910â€“5922.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu etÂ al. (2020)</span>
<span class="ltx_bibblock">
Xu, K.; Yang, X.; Yin, B.; and Lau, R.Â W. 2020.

</span>
<span class="ltx_bibblock">Learning to restore low-light images via
decomposition-and-enhancement.

</span>
<span class="ltx_bibblock">In <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, 2281â€“2290.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue etÂ al. (2022)</span>
<span class="ltx_bibblock">
Xue, X.; He, J.; Ma, L.; Wang, Y.; Fan, X.; and Liu, R. 2022.

</span>
<span class="ltx_bibblock">Best of Both Worlds: See and Understand Clearly in the Dark.

</span>
<span class="ltx_bibblock">In <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 30th ACM International Conference on
Multimedia</em>, 2154â€“2162.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang etÂ al. (2020a)</span>
<span class="ltx_bibblock">
Yang, W.; Wang, S.; Fang, Y.; Wang, Y.; and Liu, J. 2020a.

</span>
<span class="ltx_bibblock">From fidelity to perceptual quality: A semi-supervised approach for
low-light image enhancement.

</span>
<span class="ltx_bibblock">In <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition</em>, 3063â€“3072.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang etÂ al. (2020b)</span>
<span class="ltx_bibblock">
Yang, W.; Yuan, Y.; Ren, W.; Liu, J.; Scheirer, W.Â J.; Wang, Z.; Zhang, T.;
Zhong, Q.; Xie, D.; Pu, S.; etÂ al. 2020b.

</span>
<span class="ltx_bibblock">Advancing image understanding in poor visibility environments: A
collective benchmark study.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Image Processing</em>, 29: 5737â€“5752.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan etÂ al. (2019)</span>
<span class="ltx_bibblock">
Yuan, Y.; Yang, W.; Ren, W.; Liu, J.; Scheirer, W.Â J.; and Wang, Z. 2019.

</span>
<span class="ltx_bibblock">UG2+Track 2: A Collective Benchmark Effort for Evaluating and
Advancing Image Understanding in Poor Visibility Environments.

</span>
<span class="ltx_bibblock"><em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1904.04474</em>.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2020)</span>
<span class="ltx_bibblock">
Zhang, M.; Ren, W.; Piao, Y.; Rong, Z.; and Lu, H. 2020.

</span>
<span class="ltx_bibblock">Select, supplement and focus for RGB-D saliency detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition</em>, 3472â€“3481.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2017)</span>
<span class="ltx_bibblock">
Zhang, S.; Zhu, X.; Lei, Z.; Shi, H.; Wang, X.; and Li, S.Â Z. 2017.

</span>
<span class="ltx_bibblock">S3fd: Single shot scale-invariant face detector.

</span>
<span class="ltx_bibblock">In <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</em>, 192â€“201.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang, Zhang, and Guo (2019)</span>
<span class="ltx_bibblock">
Zhang, Y.; Zhang, J.; and Guo, X. 2019.

</span>
<span class="ltx_bibblock">Kindling the darkness: A practical low-light image enhancer.

</span>
<span class="ltx_bibblock">In <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 27th ACM international conference on
multimedia</em>, 1632â€“1640.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2309.03547" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2309.03548" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2309.03548">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2309.03548" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2309.03549" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 07:50:57 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
