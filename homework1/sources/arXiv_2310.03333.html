<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2310.03333] Real-time Multi-modal Object Detection and Tracking on Edge for Regulatory Compliance Monitoring</title><meta property="og:description" content="Regulatory compliance auditing across diverse industrial domains requires heightened quality assurance and traceability. Present manual and intermittent approaches to such auditing yield significant challenges, potenti…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Real-time Multi-modal Object Detection and Tracking on Edge for Regulatory Compliance Monitoring">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Real-time Multi-modal Object Detection and Tracking on Edge for Regulatory Compliance Monitoring">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2310.03333">

<!--Generated on Wed Feb 28 02:27:57 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Real-time Multi-modal Object Detection and Tracking on Edge for Regulatory Compliance Monitoring</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jia Syuen Lim<sup id="id10.10.id1" class="ltx_sup"><span id="id10.10.id1.1" class="ltx_text ltx_font_italic">†</span></sup> Ziwei Wang<sup id="id11.11.id2" class="ltx_sup"><span id="id11.11.id2.1" class="ltx_text ltx_font_italic">†</span></sup> Jiajun Liu<sup id="id12.12.id3" class="ltx_sup"><span id="id12.12.id3.1" class="ltx_text ltx_font_italic">†</span></sup> Abdelwahed Khamis<sup id="id13.13.id4" class="ltx_sup"><span id="id13.13.id4.1" class="ltx_text ltx_font_italic">†</span></sup> 
<br class="ltx_break">Reza Arablouei<sup id="id14.14.id5" class="ltx_sup"><span id="id14.14.id5.1" class="ltx_text ltx_font_italic">†</span></sup> Robert Barlow<sup id="id15.15.id6" class="ltx_sup"><span id="id15.15.id6.1" class="ltx_text ltx_font_italic">‡</span></sup> Ryan McAllister<sup id="id16.16.id7" class="ltx_sup"><span id="id16.16.id7.1" class="ltx_text ltx_font_italic">‡</span></sup>
</span><span class="ltx_author_notes">Affiliations: <sup id="id17.17.id1" class="ltx_sup"><span id="id17.17.id1.1" class="ltx_text ltx_font_italic">†</span></sup>Data61, CSIRO, <sup id="id18.18.id2" class="ltx_sup"><span id="id18.18.id2.1" class="ltx_text ltx_font_italic">‡</span></sup>Agriculture &amp; Food, CSIRO</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id19.id1" class="ltx_p">Regulatory compliance auditing across diverse industrial domains requires heightened quality assurance and traceability. Present manual and intermittent approaches to such auditing yield significant challenges, potentially leading to oversights in the monitoring process. To address these issues, we introduce a real-time, multi-modal sensing system employing 3D time-of-flight and RGB cameras, coupled with unsupervised learning techniques on edge AI devices. This enables continuous object tracking thereby enhancing efficiency in record-keeping and minimizing manual interventions. While we validate the system in a knife sanitization context within agrifood facilities, emphasizing its prowess against occlusion and low-light issues with RGB cameras, its potential spans various industrial monitoring settings.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Regulatory compliance auditing of food processing facilities is a mandatory requirement in the agrifood export industries. However, the majority of these audits are currently conducted through manual inspections, which are intermittent and lack traceability. For instance, crucial hygiene regulations such as handwashing and knife sanitizing, which are essential for preventing food-borne illnesses and cross-contamination, are often monitored manually, leading to potential inconsistencies or oversight. Implementing automated regulatory compliance checkpoints, along with sensor-driven data collection and machine learning-based analytics, can offer a robust solution. It can not only reduce labor costs significantly but also enhance quality assurance by enabling continuous monitoring and systematic record-keeping.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">With the growing computational power of modern edge AI devices, the deployment of deep neural networks (DNNs) for real-time detection of visual objects has become more prevalent <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. However, most existing DNN-based algorithms in computer vision applications are limited to two-dimensional images and lack the ability to incorporate geometrical knowledge or fine details.
Their performance can degrade due to occlusion and low ambient light settings.
Recent years have witnessed remarkable advancements in compact yet high-performance 3D time-of-flight (ToF) technology.
The increased accessibility of ToF technology has facilitated the creation of accurate geometrical and spatial representations of physical objects in complex environments, including low-light conditions.
By integrating information from multiple sensing modes and leveraging their complementary properties, the drawbacks of perception and blind spots associated with individual modes can be effectively mitigated.
This integration can lead to improved accuracy and robustness in real-world applications, such as behavior recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2310.03333/assets/pics/system_workflow.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="550" height="235" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">The overall architecture of our proposed system.</span></figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this work, we introduce a near-real-time multi-modal sensing system for object tracking on edge devices across various industrial contexts. The system integrates a ToF camera with a 3D ToF sensor and an RGB camera. This depth information enhances discernment between static and dynamic entities. While our initial evaluation focuses on knife sanitization monitoring, the system’s broader aim is to address diverse industrial tracking needs. It’s designed to detect and track objects like knives, reducing the need for manual annotation. Our prototype employs an unsupervised multi-modal detector to identify object positions irrespective of positioning or color nuances. We assess its performance in a lab simulating industrial environments. System specifics are in section <a href="#S2" title="2 System Design ‣ Real-time Multi-modal Object Detection and Tracking on Edge for Regulatory Compliance Monitoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, with performance metrics in section <a href="#S3" title="3 Experiment and Results ‣ Real-time Multi-modal Object Detection and Tracking on Edge for Regulatory Compliance Monitoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>System Design</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The proposed near-real-time detection and tracking system, depicted in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Real-time Multi-modal Object Detection and Tracking on Edge for Regulatory Compliance Monitoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, comprises two main hardware components: an integrated ToF camera and an edge AI device.
We utilize the Robot Operating System 2 to manage multi-modal data streams from the ToF camera to the edge AI device.
This system setup allows us to preprocess data and implement our detection and tracking pipeline in near real-time.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Hardware Components</h3>

<section id="S2.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>Time-of-flight camera.</h4>

<div id="S2.SS1.SSS1.p1" class="ltx_para">
<p id="S2.SS1.SSS1.p1.1" class="ltx_p">We utilize an Intel® RealSense™ ToF camera, which is a compact and lightweight device equipped with high-resolution ToF sensing technology, which is ideal for edge deployment. We initialize both its RGB camera and ToF depth sensor to operate at a frame rate of 30Hz.</p>
</div>
</section>
<section id="S2.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span>Edge AI device.</h4>

<div id="S2.SS1.SSS2.p1" class="ltx_para">
<p id="S2.SS1.SSS2.p1.1" class="ltx_p">We utilize an NVIDIA® Jetson AGX Orin™ development kit to receive and process data captured by the ToF camera via USB-C connection.</p>
</div>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Software Pipeline</h3>

<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Robot Operating System 2 (ROS 2).</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.1" class="ltx_p">We make use of the open-source middleware ROS 2, which provides a range of services including device control, message-passing, and package management for robotics software.</p>
</div>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Preprocessing components.</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.1" class="ltx_p">A series of preprocessing steps are implemented prior to detection modules.</p>
</div>
<div id="S2.SS2.SSS2.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS2.p2.1" class="ltx_p"><span id="S2.SS2.SSS2.p2.1.1" class="ltx_text ltx_font_bold">Static environment prescan</span>: Initially, we conduct a comprehensive prescan of the static environment, ensuring that the knife sterilizer is clear of any objects, and save this scan as the static background model.</p>
</div>
<div id="S2.SS2.SSS2.p3" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS2.p3.1" class="ltx_p"><span id="S2.SS2.SSS2.p3.1.1" class="ltx_text ltx_font_bold">Background subtraction</span>: We apply a coarse-to-fine filtering process to subtract the static background from the point cloud. First, to minimize the computational load, we apply a coarse filter to exclude points that fit within the voxel grid of the background model. Next, we enhance the background subtraction process by employing a fine filter that utilizes a KDTree search to recursively isolate points within a specific distance from the background, assuming that distant points belong to foreground objects.</p>
</div>
<div id="S2.SS2.SSS2.p4" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS2.p4.1" class="ltx_p"><span id="S2.SS2.SSS2.p4.1.1" class="ltx_text ltx_font_bold">Accumulation of points</span>: We implement a first-in-first-out (FIFO) queue mechanism to accumulate point clouds from <math id="S2.SS2.SSS2.p4.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS2.SSS2.p4.1.m1.1a"><mi id="S2.SS2.SSS2.p4.1.m1.1.1" xref="S2.SS2.SSS2.p4.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p4.1.m1.1b"><ci id="S2.SS2.SSS2.p4.1.m1.1.1.cmml" xref="S2.SS2.SSS2.p4.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p4.1.m1.1c">k</annotation></semantics></math> successive frames. This approach allows for the generation of more comprehensive and denser point clouds.</p>
</div>
</section>
<section id="S2.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.3 </span>3D object detection.</h4>

<div id="S2.SS2.SSS3.p1" class="ltx_para">
<p id="S2.SS2.SSS3.p1.1" class="ltx_p">Our 3D detection pipeline can be divided into several components as in the following.</p>
</div>
<div id="S2.SS2.SSS3.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS3.p2.1" class="ltx_p"><span id="S2.SS2.SSS3.p2.1.1" class="ltx_text ltx_font_bold">Random sampling</span>: The uneven distribution of accumulated points can introduce noise, potentially compromising the performance of our algorithm.
To address this issue, we employ a random subsampling technique to effectively reduce the unevenness and noise in the point distribution, whilst enhancing the overall efficiency.</p>
</div>
<div id="S2.SS2.SSS3.p3" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS3.p3.1" class="ltx_p"><span id="S2.SS2.SSS3.p3.1.1" class="ltx_text ltx_font_bold">Gaussian kernel density estimation (KDE)</span>: Traditional clustering techniques often encounter difficulties when dealing with closely clumped point clouds, which is frequently observed in scenarios where knives are densely packed. To address this, we employ the KDE algorithm to predict the density map of the point clouds and employ percentile filtering to enhance the separability of the point clouds.</p>
</div>
<div id="S2.SS2.SSS3.p4" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS3.p4.1" class="ltx_p"><span id="S2.SS2.SSS3.p4.1.1" class="ltx_text ltx_font_bold">HDBSCAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite></span>: After processing the point cloud by the KDE algorithm, we use the hierarchical density-based spatial clustering of applications with noise (HDBSCAN) algorithm to determine the number of knives present.</p>
</div>
<div id="S2.SS2.SSS3.p5" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS3.p5.1" class="ltx_p"><span id="S2.SS2.SSS3.p5.1.1" class="ltx_text ltx_font_bold">Simple online and real-time tracking (SORT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite></span>: SORT is a real-time algorithm designed to track multiple objects in a live video stream.
We obtain the 3D cluster centroids from HDBSCAN and project them onto a 2D image plane. The projected centroids are fed to SORT for subsequent 2D object tracking.</p>
</div>
</section>
<section id="S2.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.4 </span>Hand detection.</h4>

<div id="S2.SS2.SSS4.p1" class="ltx_para">
<p id="S2.SS2.SSS4.p1.1" class="ltx_p">During the knife counting process, occasionally the knife handles may be occluded by worker’s hands when they come into the scene.
We use the MediaPipe Hand Landmarker tool to detect the presence of hands in live image streams. Once the presence of any hand is detected, an event notification is dispatched to the 3D detection node, instructing it to temporarily halt the knife counting process until no hand is detected.</p>
</div>
<figure id="S2.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S2.F2.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.03333/assets/pics/raw_points.png" id="S2.F2.1.g1" class="ltx_graphics ltx_img_square" width="598" height="561" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S2.F2.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.03333/assets/pics/kde_density.png" id="S2.F2.2.g1" class="ltx_graphics ltx_img_square" width="598" height="557" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S2.F2.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.03333/assets/pics/cluster_w_filter.png" id="S2.F2.3.g1" class="ltx_graphics ltx_img_square" width="598" height="560" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.5.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S2.F2.6.2" class="ltx_text" style="font-size:90%;">From left, point clouds of the objects of interest isolated from the background, the density maps produced by KDE, and clusters identified by HDBSCAN.</span></figcaption>
</figure>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiment and Results</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">To assess the performance of the proposed system, we conducted an experiment at a pilot plant within CSIRO, under the approval of the relevant ethics committee. During the experiment, a ToF camera was set up on a tripod to capture a top-down view of a knife sterilizer. We designed our experiment to simulate a range of real-world scenarios. In total, we established five different scenarios, each representing a different number of knives, from one to five. For each scenario, we created 50 unique instances, representing various knife positions within the sterilization bath.</p>
</div>
<div id="S3.p2" class="ltx_para ltx_noindent">
<p id="S3.p2.1" class="ltx_p"><span id="S3.p2.1.1" class="ltx_text ltx_font_bold">Density-based filtering:</span>
The range between the sensor and the object directly affects the density of the captured point clouds, which in turn provides valuable information to the KDE algorithm. We observe that density peaks at the tip of each knife handle as illustrated in Figure <a href="#S2.F2" title="Figure 2 ‣ 2.2.4 Hand detection. ‣ 2.2 Software Pipeline ‣ 2 System Design ‣ Real-time Multi-modal Object Detection and Tracking on Edge for Regulatory Compliance Monitoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Thus, we use percentile filtering to exclude outliers, primarily the body of the knife handles. This filtering step further enhances the separability of the point clouds, enhancing their suitability for unsupervised cluster analysis.</p>
</div>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p id="S3.p3.1" class="ltx_p"><span id="S3.p3.1.1" class="ltx_text ltx_font_bold">Detection performance:</span>
The final detection results, produced by the SORT algorithm, take the form of bounding boxes. Figure <a href="#S3.F3" title="Figure 3 ‣ 3 Experiment and Results ‣ Real-time Multi-modal Object Detection and Tracking on Edge for Regulatory Compliance Monitoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> presents a side-by-side view of a heatmap of the objects of interest alongside the corresponding detection results. To accurately quantify discrepancies between the system’s predictions and the ground truth, we employ the mean absolute error (MAE), root mean square error (RMSE), and normalized RMSE (NRMSE) performance measures. Corresponding results for varying numbers of objects are comprehensively presented in Table <a href="#S3.T1" title="Table 1 ‣ 3 Experiment and Results ‣ Real-time Multi-modal Object Detection and Tracking on Edge for Regulatory Compliance Monitoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.2.1.1" class="ltx_tr">
<th id="S3.T1.2.1.1.1" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T1.2.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.2.1.1.1.1.1" class="ltx_p"><span id="S3.T1.2.1.1.1.1.1.1" class="ltx_text" style="font-size:90%;">No. of objects</span></span>
</span>
</th>
<th id="S3.T1.2.1.1.2" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T1.2.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.2.1.1.2.1.1" class="ltx_p"><span id="S3.T1.2.1.1.2.1.1.1" class="ltx_text" style="font-size:90%;">MAE</span></span>
</span>
</th>
<th id="S3.T1.2.1.1.3" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T1.2.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.2.1.1.3.1.1" class="ltx_p"><span id="S3.T1.2.1.1.3.1.1.1" class="ltx_text" style="font-size:90%;">RMSE</span></span>
</span>
</th>
<th id="S3.T1.2.1.1.4" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T1.2.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.2.1.1.4.1.1" class="ltx_p"><span id="S3.T1.2.1.1.4.1.1.1" class="ltx_text" style="font-size:90%;">NRMSE</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.2.2.1" class="ltx_tr">
<td id="S3.T1.2.2.1.1" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T1.2.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.2.2.1.1.1.1" class="ltx_p"><span id="S3.T1.2.2.1.1.1.1.1" class="ltx_text" style="font-size:90%;">1</span></span>
</span>
</td>
<td id="S3.T1.2.2.1.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T1.2.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.2.2.1.2.1.1" class="ltx_p"><span id="S3.T1.2.2.1.2.1.1.1" class="ltx_text" style="font-size:90%;">0.168</span></span>
</span>
</td>
<td id="S3.T1.2.2.1.3" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T1.2.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.2.2.1.3.1.1" class="ltx_p"><span id="S3.T1.2.2.1.3.1.1.1" class="ltx_text" style="font-size:90%;">0.410</span></span>
</span>
</td>
<td id="S3.T1.2.2.1.4" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T1.2.2.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.2.2.1.4.1.1" class="ltx_p"><span id="S3.T1.2.2.1.4.1.1.1" class="ltx_text" style="font-size:90%;">0.410</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.2.3.2" class="ltx_tr">
<td id="S3.T1.2.3.2.1" class="ltx_td ltx_align_justify">
<span id="S3.T1.2.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.2.3.2.1.1.1" class="ltx_p"><span id="S3.T1.2.3.2.1.1.1.1" class="ltx_text" style="font-size:90%;">2</span></span>
</span>
</td>
<td id="S3.T1.2.3.2.2" class="ltx_td ltx_align_justify">
<span id="S3.T1.2.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.2.3.2.2.1.1" class="ltx_p"><span id="S3.T1.2.3.2.2.1.1.1" class="ltx_text" style="font-size:90%;">0.477</span></span>
</span>
</td>
<td id="S3.T1.2.3.2.3" class="ltx_td ltx_align_justify">
<span id="S3.T1.2.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.2.3.2.3.1.1" class="ltx_p"><span id="S3.T1.2.3.2.3.1.1.1" class="ltx_text" style="font-size:90%;">0.924</span></span>
</span>
</td>
<td id="S3.T1.2.3.2.4" class="ltx_td ltx_align_justify">
<span id="S3.T1.2.3.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.2.3.2.4.1.1" class="ltx_p"><span id="S3.T1.2.3.2.4.1.1.1" class="ltx_text" style="font-size:90%;">0.462</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.2.4.3" class="ltx_tr">
<td id="S3.T1.2.4.3.1" class="ltx_td ltx_align_justify">
<span id="S3.T1.2.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.2.4.3.1.1.1" class="ltx_p"><span id="S3.T1.2.4.3.1.1.1.1" class="ltx_text" style="font-size:90%;">3</span></span>
</span>
</td>
<td id="S3.T1.2.4.3.2" class="ltx_td ltx_align_justify">
<span id="S3.T1.2.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.2.4.3.2.1.1" class="ltx_p"><span id="S3.T1.2.4.3.2.1.1.1" class="ltx_text" style="font-size:90%;">0.655</span></span>
</span>
</td>
<td id="S3.T1.2.4.3.3" class="ltx_td ltx_align_justify">
<span id="S3.T1.2.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.2.4.3.3.1.1" class="ltx_p"><span id="S3.T1.2.4.3.3.1.1.1" class="ltx_text" style="font-size:90%;">1.263</span></span>
</span>
</td>
<td id="S3.T1.2.4.3.4" class="ltx_td ltx_align_justify">
<span id="S3.T1.2.4.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.2.4.3.4.1.1" class="ltx_p"><span id="S3.T1.2.4.3.4.1.1.1" class="ltx_text" style="font-size:90%;">0.421</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.2.5.4" class="ltx_tr">
<td id="S3.T1.2.5.4.1" class="ltx_td ltx_align_justify">
<span id="S3.T1.2.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.2.5.4.1.1.1" class="ltx_p"><span id="S3.T1.2.5.4.1.1.1.1" class="ltx_text" style="font-size:90%;">4</span></span>
</span>
</td>
<td id="S3.T1.2.5.4.2" class="ltx_td ltx_align_justify">
<span id="S3.T1.2.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.2.5.4.2.1.1" class="ltx_p"><span id="S3.T1.2.5.4.2.1.1.1" class="ltx_text" style="font-size:90%;">0.953</span></span>
</span>
</td>
<td id="S3.T1.2.5.4.3" class="ltx_td ltx_align_justify">
<span id="S3.T1.2.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.2.5.4.3.1.1" class="ltx_p"><span id="S3.T1.2.5.4.3.1.1.1" class="ltx_text" style="font-size:90%;">1.520</span></span>
</span>
</td>
<td id="S3.T1.2.5.4.4" class="ltx_td ltx_align_justify">
<span id="S3.T1.2.5.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.2.5.4.4.1.1" class="ltx_p"><span id="S3.T1.2.5.4.4.1.1.1" class="ltx_text" style="font-size:90%;">0.380</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.2.6.5" class="ltx_tr">
<td id="S3.T1.2.6.5.1" class="ltx_td ltx_align_justify ltx_border_bb">
<span id="S3.T1.2.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.2.6.5.1.1.1" class="ltx_p"><span id="S3.T1.2.6.5.1.1.1.1" class="ltx_text" style="font-size:90%;">5</span></span>
</span>
</td>
<td id="S3.T1.2.6.5.2" class="ltx_td ltx_align_justify ltx_border_bb">
<span id="S3.T1.2.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.2.6.5.2.1.1" class="ltx_p"><span id="S3.T1.2.6.5.2.1.1.1" class="ltx_text" style="font-size:90%;">1.114</span></span>
</span>
</td>
<td id="S3.T1.2.6.5.3" class="ltx_td ltx_align_justify ltx_border_bb">
<span id="S3.T1.2.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.2.6.5.3.1.1" class="ltx_p"><span id="S3.T1.2.6.5.3.1.1.1" class="ltx_text" style="font-size:90%;">1.876</span></span>
</span>
</td>
<td id="S3.T1.2.6.5.4" class="ltx_td ltx_align_justify ltx_border_bb">
<span id="S3.T1.2.6.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.2.6.5.4.1.1" class="ltx_p"><span id="S3.T1.2.6.5.4.1.1.1" class="ltx_text" style="font-size:90%;">0.375</span></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>Detection performance for different numbers of objects.</figcaption>
</figure>
<figure id="S3.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S3.F3.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:65.0pt;">
<img src="/html/2310.03333/assets/pics/2d_heatmap.png" id="S3.F3.1.g1" class="ltx_graphics ltx_img_square" width="598" height="598" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S3.F3.2" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:65.0pt;">
<img src="/html/2310.03333/assets/pics/bbox_reults.png" id="S3.F3.2.g1" class="ltx_graphics ltx_img_square" width="598" height="598" alt="Refer to caption">
</div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.4.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.5.2" class="ltx_text" style="font-size:90%;">Example 2D heatmap and detection results.</span></figcaption>
</figure>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">The evaluation results demonstrate the commendable performance of our system, even though there is a slight increase in the detection error as the number of objects increases, which can primarily be attributed to the inherent challenges associated with point cloud clustering. As the ToF camera scans more objects, the density of the point cloud for each individual object may decrease, posing challenges to the object detection process. However, despite the complexities involved, the system consistently performs well across different object counts, highlighting its reliability and effectiveness.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
<span id="bib.bib1.1.1" class="ltx_text ltx_font_smallcaps">Bewley, A., Ge, Z., Ott, L., Ramos, F., and Upcroft, B.</span>

</span>
<span class="ltx_bibblock">Simple online and realtime tracking.

</span>
<span class="ltx_bibblock">In <span id="bib.bib1.2.1" class="ltx_text ltx_font_italic">IEEE ICIP</span> (2016), pp. 3464–3468.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
<span id="bib.bib2.1.1" class="ltx_text ltx_font_smallcaps">McInnes, L., Healy, J., and Astels, S.</span>

</span>
<span class="ltx_bibblock">HDBSCAN: Hierarchical density based clustering.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text ltx_font_italic">JOSS</span> (2017).

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
<span id="bib.bib3.1.1" class="ltx_text ltx_font_smallcaps">Stäcker, L., Fei, J., Heidenreich, P., Bonarens, F., Rambach, J., Stricker, D., and Stiller, C.</span>

</span>
<span class="ltx_bibblock">Deployment of deep neural networks for object detection on edge ai devices with runtime optimization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib3.2.1" class="ltx_text ltx_font_italic">IEEE ICCVW</span> (2021), pp. 1015–1022.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
<span id="bib.bib4.1.1" class="ltx_text ltx_font_smallcaps">Wang, Z., Liu, J., Arablouei, R., Bishop-Hurley, G., Matthews, M., and Borges, P.</span>

</span>
<span class="ltx_bibblock">Multi-modal sensing for behaviour recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib4.2.1" class="ltx_text ltx_font_italic">MobiCom</span> (2022), p. 900–902.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2310.03332" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2310.03333" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2310.03333">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2310.03333" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2310.03334" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 02:27:57 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
