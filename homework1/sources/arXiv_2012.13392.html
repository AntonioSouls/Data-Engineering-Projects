<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2012.13392] Deep Learning-Based Human Pose Estimation: A Survey</title><meta property="og:description" content="Human pose estimation aims to locate the human body parts and build human body representation (e.g., body skeleton) from input data such as images and videos. It has drawn increasing attention during the past decade anâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Deep Learning-Based Human Pose Estimation: A Survey">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Deep Learning-Based Human Pose Estimation: A Survey">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2012.13392">

<!--Generated on Fri Mar  8 00:27:54 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Survey of human pose estimation,  2D and 3D pose estimation,  deep learning-based pose estimation,  pose estimation datasets,  pose estimation metrics">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Deep Learning-Based Human Pose Estimation: A Survey</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ce Zheng
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:cezheng@knights.ucf.edu">cezheng@knights.ucf.edu</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_affiliation_institution">University of Central Florida</span><span id="id2.2.id2" class="ltx_text ltx_affiliation_streetaddress">4328 Scorpius St, Suite 245</span><span id="id3.3.id3" class="ltx_text ltx_affiliation_city">Orlando</span><span id="id4.4.id4" class="ltx_text ltx_affiliation_state">Florida</span><span id="id5.5.id5" class="ltx_text ltx_affiliation_country">USA</span><span id="id6.6.id6" class="ltx_text ltx_affiliation_postcode">32816</span>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wenhan Wu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:wwu25@uncc.edu">wwu25@uncc.edu</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id7.1.id1" class="ltx_text ltx_affiliation_institution">University of North Carolina at Charlotte</span><span id="id8.2.id2" class="ltx_text ltx_affiliation_streetaddress">9201 University City Blvd</span><span id="id9.3.id3" class="ltx_text ltx_affiliation_city">Charlotte</span><span id="id10.4.id4" class="ltx_text ltx_affiliation_state">North Carolina</span><span id="id11.5.id5" class="ltx_text ltx_affiliation_country">USA</span><span id="id12.6.id6" class="ltx_text ltx_affiliation_postcode">28223</span>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chen Chen
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:chen.chen@crcv.ucf.edu">chen.chen@crcv.ucf.edu</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id13.1.id1" class="ltx_text ltx_affiliation_institution">University of Central Florida</span><span id="id14.2.id2" class="ltx_text ltx_affiliation_streetaddress">4328 Scorpius St, Suite 245</span><span id="id15.3.id3" class="ltx_text ltx_affiliation_city">Orlando</span><span id="id16.4.id4" class="ltx_text ltx_affiliation_state">Florida</span><span id="id17.5.id5" class="ltx_text ltx_affiliation_country">USA</span><span id="id18.6.id6" class="ltx_text ltx_affiliation_postcode">32816</span>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Taojiannan Yang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:taoyang1122@knights.ucf.edu">taoyang1122@knights.ucf.edu</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id19.1.id1" class="ltx_text ltx_affiliation_institution">University of Central Florida</span><span id="id20.2.id2" class="ltx_text ltx_affiliation_streetaddress">4328 Scorpius St, Suite 245</span><span id="id21.3.id3" class="ltx_text ltx_affiliation_city">Orlando</span><span id="id22.4.id4" class="ltx_text ltx_affiliation_state">Florida</span><span id="id23.5.id5" class="ltx_text ltx_affiliation_country">USA</span><span id="id24.6.id6" class="ltx_text ltx_affiliation_postcode">32816</span>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sijie Zhu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:sizhu@knights.ucf.edu">sizhu@knights.ucf.edu</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id25.1.id1" class="ltx_text ltx_affiliation_institution">University of Central Florida</span><span id="id26.2.id2" class="ltx_text ltx_affiliation_streetaddress">4328 Scorpius St, Suite 245</span><span id="id27.3.id3" class="ltx_text ltx_affiliation_city">Orlando</span><span id="id28.4.id4" class="ltx_text ltx_affiliation_state">Florida</span><span id="id29.5.id5" class="ltx_text ltx_affiliation_country">USA</span><span id="id30.6.id6" class="ltx_text ltx_affiliation_postcode">32816</span>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ju Shen
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:jshen1@udayton.edu">jshen1@udayton.edu</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id31.1.id1" class="ltx_text ltx_affiliation_institution">University of Dayton</span><span id="id32.2.id2" class="ltx_text ltx_affiliation_streetaddress">300 College Park</span><span id="id33.3.id3" class="ltx_text ltx_affiliation_city">Dayton</span><span id="id34.4.id4" class="ltx_text ltx_affiliation_state">Ohio</span><span id="id35.5.id5" class="ltx_text ltx_affiliation_country">USA</span><span id="id36.6.id6" class="ltx_text ltx_affiliation_postcode">45469</span>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Nasser Kehtarnavaz
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:kehtar@utdallas.edu">kehtar@utdallas.edu</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id37.1.id1" class="ltx_text ltx_affiliation_institution">University of Texas at Dallas</span><span id="id38.2.id2" class="ltx_text ltx_affiliation_streetaddress"> 800 W. Campbell Road</span><span id="id39.3.id3" class="ltx_text ltx_affiliation_city">Richardson</span><span id="id40.4.id4" class="ltx_text ltx_affiliation_state">Texas</span><span id="id41.5.id5" class="ltx_text ltx_affiliation_country">USA</span><span id="id42.6.id6" class="ltx_text ltx_affiliation_postcode">75080</span>
</span></span></span>
<span class="ltx_author_before">Â andÂ </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mubarak Shah
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:shah@crcv.ucf.edu">shah@crcv.ucf.edu</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id43.1.id1" class="ltx_text ltx_affiliation_institution">University of Central Florida</span><span id="id44.2.id2" class="ltx_text ltx_affiliation_streetaddress">4328 Scorpius St, Suite 245</span><span id="id45.3.id3" class="ltx_text ltx_affiliation_city">Orlando</span><span id="id46.4.id4" class="ltx_text ltx_affiliation_state">Florida</span><span id="id47.5.id5" class="ltx_text ltx_affiliation_country">USA</span><span id="id48.6.id6" class="ltx_text ltx_affiliation_postcode">32816</span>
</span></span></span>
</div>
<div class="ltx_dates">(2018)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id49.id1" class="ltx_p">Human pose estimation aims to locate the human body parts and build human body representation (e.g., body skeleton) from input data such as images and videos. It has drawn increasing attention during the past decade and has been utilized in a wide range of applications including human-computer interaction, motion analysis, augmented reality, and virtual reality. Although the recently developed deep learning-based solutions have achieved high performance in human pose estimation, there still remain challenges due to insufficient training data, depth ambiguities, and occlusion.
The goal of this survey paper is to provide a comprehensive review of recent deep learning-based solutions for both 2D and 3D pose estimation via a systematic analysis and comparison of these solutions based on their input data and inference procedures. More than 260 research papers since 2014 are covered in this survey. Furthermore, 2D and 3D human pose estimation datasets and evaluation metrics are included. Quantitative performance comparisons of the reviewed methods on popular datasets are summarized and discussed. Finally, the challenges involved, applications, and future research directions are concluded. A regularly updated project page is provided:
<a target="_blank" href="https://github.com/zczcwh/DL-HPE" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/zczcwh/DL-HPE</a></p>
</div>
<div class="ltx_keywords">Survey of human pose estimation, 2D and 3D pose estimation, deep learning-based pose estimation, pose estimation datasets, pose estimation metrics
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_copyright"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">copyright: </span>acmcopyright</span></span></span><span id="id2" class="ltx_note ltx_note_frontmatter ltx_role_journalyear"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">journalyear: </span>2018</span></span></span><span id="id3" class="ltx_note ltx_note_frontmatter ltx_role_doi"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">doi: </span>10.1145/1122445.1122456</span></span></span><span id="id4" class="ltx_note ltx_note_frontmatter ltx_role_journal"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">journal: </span>JACM</span></span></span><span id="id5" class="ltx_note ltx_note_frontmatter ltx_role_journalvolume"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">journalvolume: </span>37</span></span></span><span id="id6" class="ltx_note ltx_note_frontmatter ltx_role_journalnumber"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">journalnumber: </span>4</span></span></span><span id="id7" class="ltx_note ltx_note_frontmatter ltx_role_article"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">article: </span>111</span></span></span><span id="id8" class="ltx_note ltx_note_frontmatter ltx_role_publicationmonth"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">publicationmonth: </span>8</span></span></span><span id="id9" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">ccs: </span>Computing methodologiesÂ Computer vision</span></span></span><span id="id10" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">ccs: </span>General and referenceÂ Surveys and overviews</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Human pose estimation (HPE), which has been extensively studied in computer vision literature, involves estimating the configuration of human body parts from input data captured by sensors, in particular images and videos. HPE provides geometric and motion information about the human body which has been applied to a wide range of applications (e.g., human-computer interaction, motion analysis, augmented reality (AR), virtual reality (VR), healthcare, etc.). With the rapid development of deep learning solutions in recent years, such solutions have been shown to outperform classical computer vision methods in various tasks including image classification <cite class="ltx_cite ltx_citemacro_citep">(Krizhevsky
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib106" title="" class="ltx_ref">2012</a>)</cite>, semantic segmentation <cite class="ltx_cite ltx_citemacro_citep">(Long
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib143" title="" class="ltx_ref">2015</a>)</cite>, and object detection <cite class="ltx_cite ltx_citemacro_citep">(Ren
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib205" title="" class="ltx_ref">2015</a>)</cite>. Significant progress and remarkable performance have already been made by employing deep learning techniques in HPE tasks. However, challenges such as occlusion, insufficient training data, and depth ambiguity still pose difficulties to be overcome. 2D HPE from images and videos with 2D pose annotations is easily achievable and high performance has been reached for the human pose estimation of a single person using deep learning techniques. More recently, attention has been paid to highly occluded multi-person HPE in complex scenes. In contrast, for 3D HPE, obtaining accurate 3D pose annotations is much more difficult than its 2D counterpart. Motion capture systems can collect 3D pose annotation in controlled lab environments; however, they have limitations for in-the-wild environments. For 3D HPE from monocular RGB images and videos, the main challenge is depth ambiguities. In a multi-view setting, viewpoints association is the key issue that needs to be addressed. Some works have utilized sensors such as depth sensors, inertial measurement units (IMUs), and radio frequency devices, but these approaches are usually not cost-effective and require special-purpose hardware.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Given the rapid progress in HPE research, this article attempts to track recent advances and summarize their achievements in order to provide
a clear picture of current research on deep learning-based 2D and 3D HPE.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2012.13392/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="226" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Taxonomy of this survey.</figcaption>
</figure>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1. </span>Previous surveys and our contributions</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">There are several related surveys and reviews previously reported on HPE. Among them, <cite class="ltx_cite ltx_citemacro_citep">(Moeslund and
Granum, <a href="#bib.bib164" title="" class="ltx_ref">2001</a>; Moeslund
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib165" title="" class="ltx_ref">2006</a>; Poppe, <a href="#bib.bib195" title="" class="ltx_ref">2007</a>; Ji and Liu, <a href="#bib.bib86" title="" class="ltx_ref">2009</a>)</cite> focus on the general field of visual-based human motion capture including pose estimation, tracking, and action recognition. Therefore, pose estimation is only one of the topics covered in these surveys. The research works on 3D HPE before 2012 are reviewed in <cite class="ltx_cite ltx_citemacro_citep">(Holte
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib68" title="" class="ltx_ref">2012</a>)</cite>. The body parts parsing-based methods for single-view and multi-view HPE are reported in <cite class="ltx_cite ltx_citemacro_citep">(Liu
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib142" title="" class="ltx_ref">2015</a>)</cite>. These surveys published during 2001-2015 mainly focused on conventional methods without deep learning.
A survey on both traditional and deep learning-based methods related to HPE is presented in <cite class="ltx_cite ltx_citemacro_citep">(Gong etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib61" title="" class="ltx_ref">2016</a>)</cite>. However, only a handful of deep learning-based approaches are included. The survey in <cite class="ltx_cite ltx_citemacro_citep">(Sarafianos etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib215" title="" class="ltx_ref">2016</a>)</cite> covers 3D HPE methods with RGB inputs, while the survey in <cite class="ltx_cite ltx_citemacro_citep">(Munea etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib170" title="" class="ltx_ref">2020</a>)</cite> only reviews 2D HPE methods. Monocular HPE from the classical to recent deep learning-based methods (till 2019, less than 100 papers) is summarized in <cite class="ltx_cite ltx_citemacro_citep">(Chen
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2020f</a>)</cite>. However, it only covers 2D HPE and 3D single-view HPE from monocular cameras. 3D multi-view HPE from monocular cameras and 3D HPE from other sensors are ignored. Also, no extensive performance comparisons or in-depth analyses are given, and the discussion on existing challenges and future directions is relatively short.</p>
</div>
<div id="S1.SS1.p2" class="ltx_para">
<p id="S1.SS1.p2.1" class="ltx_p"><span id="S1.SS1.p2.1.1" class="ltx_text ltx_font_italic">This survey aims to address the shortcomings of the previous surveys in terms of providing a systematic review of the recent deep learning-based solutions to 2D and 3D HPE but also covering other aspects of HPE including the performance evaluation of (2D and 3D) HPE methods on popular datasets, their applications, and comprehensive discussion. The key points that distinguish this survey from the previous ones are as follows:</span></p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">A comprehensive review of recent deep learning-based 2D and 3D HPE methods (up to 2022 with more than 260 papers) is provided by categorizing them according to 2D or 3D scenarios, single-view or multi-view, from monocular images/videos or other sources, and learning paradigm.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Extensive performance evaluation of 2D and 3D HPE methods. We summarize and compare reported performances of promising methods on common datasets based on their categories. The comparison of results provides cues for the strengths and weaknesses of different methods, revealing the research trends and future directions of HPE.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">An overview of a wide range of HPE applications, such as surveillance, AR/VR, and healthcare.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">An thorough discussion of 2D and 3D HPE is presented in terms of key challenges in HPE pointing to potential future research toward improving performance.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2. </span>Paper organization</h3>

<div id="S1.SS2.p1" class="ltx_para">
<p id="S1.SS2.p1.1" class="ltx_p">HPE is divided into two main categories: 2D HPE (Â§Â <a href="#S2" title="2. 2D human pose estimation â€£ Deep Learning-Based Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) and 3D HPE (Â§Â <a href="#S3" title="3. 3D human pose estimation â€£ Deep Learning-Based Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). Fig. <a href="#S1.F1" title="Figure 1 â€£ 1. Introduction â€£ Deep Learning-Based Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the taxonomy of deep learning methods for HPE.
According to the number of people, 2D HPE methods are categorized into single-person and multi-person settings.
For single-person methods (Â§Â <a href="#S2.SS1" title="2.1. 2D single-person pose estimation â€£ 2. 2D human pose estimation â€£ Deep Learning-Based Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>), there are two categories: regression methods and
heatmap-based methods.
For multi-person methods (Â§Â <a href="#S2.SS2" title="2.2. 2D multi-person pose estimation â€£ 2. 2D human pose estimation â€£ Deep Learning-Based Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>), there are also two types of methods: top-down methods and
bottom-up methods.</p>
</div>
<div id="S1.SS2.p2" class="ltx_para">
<p id="S1.SS2.p2.1" class="ltx_p">3D HPE methods are classified according to the input source types: monocular RGB images and videos (Â§Â <a href="#S3.SS1" title="3.1. 3D HPE from monocular RGB images and videos â€£ 3. 3D human pose estimation â€£ Deep Learning-Based Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>), or other sensors (e.g., inertial measurement unit sensors, Â§Â <a href="#S3.SS2" title="3.2. 3D HPE from other sources â€£ 3. 3D human pose estimation â€£ Deep Learning-Based Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>). The majority of these methods use monocular RGB images and videos, and they are further divided into single-view single-person (Â§Â <a href="#S3.SS1.SSS1" title="3.1.1. Single-view single person 3D HPE â€£ 3.1. 3D HPE from monocular RGB images and videos â€£ 3. 3D human pose estimation â€£ Deep Learning-Based Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1.1</span></a>); single-view multi-person (Â§Â <a href="#S3.SS1.SSS2" title="3.1.2. Single-view multi-person 3D HPE â€£ 3.1. 3D HPE from monocular RGB images and videos â€£ 3. 3D human pose estimation â€£ Deep Learning-Based Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1.2</span></a>); and multi-view methods (Â§Â <a href="#S3.SS1.SSS3" title="3.1.3. Multi-view 3D HPE â€£ 3.1. 3D HPE from monocular RGB images and videos â€£ 3. 3D human pose estimation â€£ Deep Learning-Based Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1.3</span></a>). Multi-view settings are deployed mainly for multi-person pose estimation. Hence, single-person or multi-person is not specified in this category.</p>
</div>
<div id="S1.SS2.p3" class="ltx_para">
<p id="S1.SS2.p3.1" class="ltx_p">Next, depending on the 2D and 3D HPE pipelines, the datasets and evaluation metrics commonly used are summarized followed by a comparison of results of the promising methods (Â§Â <a href="#S4" title="4. Datasets and Evaluation Metrics â€£ Deep Learning-Based Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>). In addition, various applications of HPE such as AR/VR are mentioned (Â§Â <a href="#S5" title="5. Applications â€£ Deep Learning-Based Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>). Finally, the paper ends by an
thorough discussion of
some promising directions for future research
(Â§Â <a href="#S6" title="6. Conclusion and Future Directions â€£ Deep Learning-Based Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>).</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>2D human pose estimation</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">2D HPE methods estimate the 2D position or spatial location of human body keypoints from images or videos. Traditional 2D HPE methods adopt different hand-crafted feature extraction techniques for body parts, and these early works describe the human body as a stick figure to obtain global pose structures. Recently, deep learning-based approaches have achieved a major breakthrough in HPE by improving the results significantly. In the following, we review deep learning-based 2D HPE methods with respect to single-person and multi-person scenarios.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>2D single-person pose estimation</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">2D single-person pose estimation is used to localize human body joint positions when the input is a single-person image. If there are several people, the input image is cropped first so that there is only one person in each cropped patch (or sub-image). This process can be achieved automatically by an upper-body detector <cite class="ltx_cite ltx_citemacro_citep">(Micilotta
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib162" title="" class="ltx_ref">2006</a>)</cite> or a full-body detector <cite class="ltx_cite ltx_citemacro_citep">(Ren
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib205" title="" class="ltx_ref">2015</a>)</cite>.
In general, there are two categories for single-person pipelines that employ deep learning techniques: regression methods and heatmap-based methods. Regression methods apply an end-to-end framework to learn a mapping from the input image to the positions of body joints or parameters of human body models <cite class="ltx_cite ltx_citemacro_citep">(Toshev and
Szegedy, <a href="#bib.bib234" title="" class="ltx_ref">2014</a>)</cite>. The goal of heatmap-based methods is to predict approximate locations of body parts and joints <cite class="ltx_cite ltx_citemacro_citep">(Chen and Yuille, <a href="#bib.bib27" title="" class="ltx_ref">2014</a>)</cite> <cite class="ltx_cite ltx_citemacro_citep">(Newell
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib172" title="" class="ltx_ref">2016</a>)</cite>, which are supervised by heatmaps representation <cite class="ltx_cite ltx_citemacro_citep">(Tompson etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib232" title="" class="ltx_ref">2015</a>; Wei
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib255" title="" class="ltx_ref">2016</a>)</cite>. Heatmap-based frameworks are now widely used in 2D HPE tasks. The general frameworks of 2D single-person HPE methods are depicted in Fig. <a href="#S2.F2" title="Figure 2 â€£ 2.1. 2D single-person pose estimation â€£ 2. 2D human pose estimation â€£ Deep Learning-Based Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2012.13392/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="369" height="256" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span><span id="S2.F2.2.1" class="ltx_text" style="font-size:80%;">Single-person 2D HPE frameworks. (a) Regression methods directly learn a mapping (via a deep neural network) from the original image to the kinematic body model and produce joint coordinates. (b) Given the ground-truth 2D pose, the ground-truth heatmaps of each joint are generated by applying a Gaussian kernel to each jointâ€™s location. Then, heatmap-based methods utilize a model to predict the heatmap of each joint. </span></figcaption>
</figure>
<section id="S2.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1. </span>Regression methods</h4>

<div id="S2.SS1.SSS1.p1" class="ltx_para">
<p id="S2.SS1.SSS1.p1.1" class="ltx_p">There are many works based on the <span id="S2.SS1.SSS1.p1.1.1" class="ltx_text ltx_font_bold">regression framework</span> (e.g., <cite class="ltx_cite ltx_citemacro_citep">(Toshev and
Szegedy, <a href="#bib.bib234" title="" class="ltx_ref">2014</a>; Pfister etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib191" title="" class="ltx_ref">2014</a>; Carreira etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2016</a>; Sun
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib223" title="" class="ltx_ref">2017</a>; Luvizon
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib149" title="" class="ltx_ref">2019</a>; Nibali
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib173" title="" class="ltx_ref">2018</a>; Li etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib119" title="" class="ltx_ref">2014</a>; Fan
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib55" title="" class="ltx_ref">2015</a>; Luvizon
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib148" title="" class="ltx_ref">2018</a>; Zhang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib285" title="" class="ltx_ref">2019b</a>; Li
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib117" title="" class="ltx_ref">2021c</a>; Panteleris and
Argyros, <a href="#bib.bib179" title="" class="ltx_ref">2021</a>; Mao
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib154" title="" class="ltx_ref">2021</a>; Mao etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib155" title="" class="ltx_ref">2022</a>)</cite>) to predict joint coordinates from images as shown in Fig. <a href="#S2.F2" title="Figure 2 â€£ 2.1. 2D single-person pose estimation â€£ 2. 2D human pose estimation â€£ Deep Learning-Based Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (a). Using AlexNet <cite class="ltx_cite ltx_citemacro_citep">(Krizhevsky
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib106" title="" class="ltx_ref">2012</a>)</cite> as the backbone, Toshev and Szegedy <cite class="ltx_cite ltx_citemacro_citep">(Toshev and
Szegedy, <a href="#bib.bib234" title="" class="ltx_ref">2014</a>)</cite> proposed a cascaded deep neural network regressor named DeepPose to learn keypoints from images. Due to the impressive performance of DeepPose, the research paradigm of HPE began to shift from classic approaches to deep learning, in particular convolutional neural networks (CNNs).
Sun et al. <cite class="ltx_cite ltx_citemacro_citep">(Sun
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib223" title="" class="ltx_ref">2017</a>)</cite> introduced a structure-aware regression method called â€compositional pose regressionâ€ based on ResNet-50 <cite class="ltx_cite ltx_citemacro_citep">(He
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib67" title="" class="ltx_ref">2016</a>)</cite>. This method adopts a re-parameterized and bone-based representation that contains human body information and pose structure, instead of the traditional joint-based representation. Luvizon et al. <cite class="ltx_cite ltx_citemacro_citep">(Luvizon
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib149" title="" class="ltx_ref">2019</a>)</cite> proposed an end-to-end regression approach for HPE using soft-argmax function to convert feature maps into joint coordinates in a fully differentiable framework. Li et al. <cite class="ltx_cite ltx_citemacro_citep">(Li
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib117" title="" class="ltx_ref">2021c</a>)</cite> first designed a transformer-based cascade network for regressing human keypoints. The spatial correlation of joints and appearance is captured by self-attention mechanism. Different from the previous methods, Li et al. <cite class="ltx_cite ltx_citemacro_citep">(Li
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib113" title="" class="ltx_ref">2021a</a>)</cite> proposed a normalizing flow model named RLE (Log-likelihood Estimation) to capture the distribution of joint location, aiming for finding the optimized parameters by residual log-likelihood estimation.</p>
</div>
<div id="S2.SS1.SSS1.p2" class="ltx_para">
<p id="S2.SS1.SSS1.p2.1" class="ltx_p">A good feature that encodes rich pose information is critical for regression-based methods. One popular strategy to learn better feature representation is <span id="S2.SS1.SSS1.p2.1.1" class="ltx_text ltx_font_bold">multi-task learning</span> <cite class="ltx_cite ltx_citemacro_citep">(Ruder, <a href="#bib.bib210" title="" class="ltx_ref">2017</a>)</cite>. By sharing representations between related tasks (e.g., pose estimation and pose-based action recognition), the model can generalize better on the original task (pose estimation).
Following this direction, Li et al. <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib119" title="" class="ltx_ref">2014</a>)</cite> proposed a heterogeneous multi-task framework that consists of two tasks: predicting joint coordinates from full images by a regressor and detecting body parts from image patches using a sliding window. Fan et al. <cite class="ltx_cite ltx_citemacro_citep">(Fan
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib55" title="" class="ltx_ref">2015</a>)</cite> proposed a dual-source (i.e., image patches and full images) CNN for two tasks: joint detection which determines whether a patch contains a body joint, and joint localization which finds the exact location
of the joint in the patch. Each task corresponds to a loss function, and the combination of two tasks leads to improved results.</p>
</div>
</section>
<section id="S2.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2. </span>Heatmap-based methods</h4>

<div id="S2.SS1.SSS2.p1" class="ltx_para">
<p id="S2.SS1.SSS2.p1.5" class="ltx_p">Instead of estimating the 2D coordinates of human joints directly, heatmap-based methods for HPE aim to estimate the 2D heatmaps which are generated by adding 2D Gaussian kernels on each jointâ€™s location as shown in Fig. <a href="#S2.F2" title="Figure 2 â€£ 2.1. 2D single-person pose estimation â€£ 2. 2D human pose estimation â€£ Deep Learning-Based Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
Concretely, the goal is to estimate <math id="S2.SS1.SSS2.p1.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S2.SS1.SSS2.p1.1.m1.1a"><mi id="S2.SS1.SSS2.p1.1.m1.1.1" xref="S2.SS1.SSS2.p1.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p1.1.m1.1b"><ci id="S2.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S2.SS1.SSS2.p1.1.m1.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p1.1.m1.1c">K</annotation></semantics></math> heatmaps <math id="S2.SS1.SSS2.p1.2.m2.4" class="ltx_Math" alttext="\{H_{1},H_{2},...,H_{K}\}" display="inline"><semantics id="S2.SS1.SSS2.p1.2.m2.4a"><mrow id="S2.SS1.SSS2.p1.2.m2.4.4.3" xref="S2.SS1.SSS2.p1.2.m2.4.4.4.cmml"><mo stretchy="false" id="S2.SS1.SSS2.p1.2.m2.4.4.3.4" xref="S2.SS1.SSS2.p1.2.m2.4.4.4.cmml">{</mo><msub id="S2.SS1.SSS2.p1.2.m2.2.2.1.1" xref="S2.SS1.SSS2.p1.2.m2.2.2.1.1.cmml"><mi id="S2.SS1.SSS2.p1.2.m2.2.2.1.1.2" xref="S2.SS1.SSS2.p1.2.m2.2.2.1.1.2.cmml">H</mi><mn id="S2.SS1.SSS2.p1.2.m2.2.2.1.1.3" xref="S2.SS1.SSS2.p1.2.m2.2.2.1.1.3.cmml">1</mn></msub><mo id="S2.SS1.SSS2.p1.2.m2.4.4.3.5" xref="S2.SS1.SSS2.p1.2.m2.4.4.4.cmml">,</mo><msub id="S2.SS1.SSS2.p1.2.m2.3.3.2.2" xref="S2.SS1.SSS2.p1.2.m2.3.3.2.2.cmml"><mi id="S2.SS1.SSS2.p1.2.m2.3.3.2.2.2" xref="S2.SS1.SSS2.p1.2.m2.3.3.2.2.2.cmml">H</mi><mn id="S2.SS1.SSS2.p1.2.m2.3.3.2.2.3" xref="S2.SS1.SSS2.p1.2.m2.3.3.2.2.3.cmml">2</mn></msub><mo id="S2.SS1.SSS2.p1.2.m2.4.4.3.6" xref="S2.SS1.SSS2.p1.2.m2.4.4.4.cmml">,</mo><mi mathvariant="normal" id="S2.SS1.SSS2.p1.2.m2.1.1" xref="S2.SS1.SSS2.p1.2.m2.1.1.cmml">â€¦</mi><mo id="S2.SS1.SSS2.p1.2.m2.4.4.3.7" xref="S2.SS1.SSS2.p1.2.m2.4.4.4.cmml">,</mo><msub id="S2.SS1.SSS2.p1.2.m2.4.4.3.3" xref="S2.SS1.SSS2.p1.2.m2.4.4.3.3.cmml"><mi id="S2.SS1.SSS2.p1.2.m2.4.4.3.3.2" xref="S2.SS1.SSS2.p1.2.m2.4.4.3.3.2.cmml">H</mi><mi id="S2.SS1.SSS2.p1.2.m2.4.4.3.3.3" xref="S2.SS1.SSS2.p1.2.m2.4.4.3.3.3.cmml">K</mi></msub><mo stretchy="false" id="S2.SS1.SSS2.p1.2.m2.4.4.3.8" xref="S2.SS1.SSS2.p1.2.m2.4.4.4.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p1.2.m2.4b"><set id="S2.SS1.SSS2.p1.2.m2.4.4.4.cmml" xref="S2.SS1.SSS2.p1.2.m2.4.4.3"><apply id="S2.SS1.SSS2.p1.2.m2.2.2.1.1.cmml" xref="S2.SS1.SSS2.p1.2.m2.2.2.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p1.2.m2.2.2.1.1.1.cmml" xref="S2.SS1.SSS2.p1.2.m2.2.2.1.1">subscript</csymbol><ci id="S2.SS1.SSS2.p1.2.m2.2.2.1.1.2.cmml" xref="S2.SS1.SSS2.p1.2.m2.2.2.1.1.2">ğ»</ci><cn type="integer" id="S2.SS1.SSS2.p1.2.m2.2.2.1.1.3.cmml" xref="S2.SS1.SSS2.p1.2.m2.2.2.1.1.3">1</cn></apply><apply id="S2.SS1.SSS2.p1.2.m2.3.3.2.2.cmml" xref="S2.SS1.SSS2.p1.2.m2.3.3.2.2"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p1.2.m2.3.3.2.2.1.cmml" xref="S2.SS1.SSS2.p1.2.m2.3.3.2.2">subscript</csymbol><ci id="S2.SS1.SSS2.p1.2.m2.3.3.2.2.2.cmml" xref="S2.SS1.SSS2.p1.2.m2.3.3.2.2.2">ğ»</ci><cn type="integer" id="S2.SS1.SSS2.p1.2.m2.3.3.2.2.3.cmml" xref="S2.SS1.SSS2.p1.2.m2.3.3.2.2.3">2</cn></apply><ci id="S2.SS1.SSS2.p1.2.m2.1.1.cmml" xref="S2.SS1.SSS2.p1.2.m2.1.1">â€¦</ci><apply id="S2.SS1.SSS2.p1.2.m2.4.4.3.3.cmml" xref="S2.SS1.SSS2.p1.2.m2.4.4.3.3"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p1.2.m2.4.4.3.3.1.cmml" xref="S2.SS1.SSS2.p1.2.m2.4.4.3.3">subscript</csymbol><ci id="S2.SS1.SSS2.p1.2.m2.4.4.3.3.2.cmml" xref="S2.SS1.SSS2.p1.2.m2.4.4.3.3.2">ğ»</ci><ci id="S2.SS1.SSS2.p1.2.m2.4.4.3.3.3.cmml" xref="S2.SS1.SSS2.p1.2.m2.4.4.3.3.3">ğ¾</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p1.2.m2.4c">\{H_{1},H_{2},...,H_{K}\}</annotation></semantics></math> for a total of <math id="S2.SS1.SSS2.p1.3.m3.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S2.SS1.SSS2.p1.3.m3.1a"><mi id="S2.SS1.SSS2.p1.3.m3.1.1" xref="S2.SS1.SSS2.p1.3.m3.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p1.3.m3.1b"><ci id="S2.SS1.SSS2.p1.3.m3.1.1.cmml" xref="S2.SS1.SSS2.p1.3.m3.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p1.3.m3.1c">K</annotation></semantics></math> keypoints. The pixel value <math id="S2.SS1.SSS2.p1.4.m4.2" class="ltx_Math" alttext="H_{i}(x,y)" display="inline"><semantics id="S2.SS1.SSS2.p1.4.m4.2a"><mrow id="S2.SS1.SSS2.p1.4.m4.2.3" xref="S2.SS1.SSS2.p1.4.m4.2.3.cmml"><msub id="S2.SS1.SSS2.p1.4.m4.2.3.2" xref="S2.SS1.SSS2.p1.4.m4.2.3.2.cmml"><mi id="S2.SS1.SSS2.p1.4.m4.2.3.2.2" xref="S2.SS1.SSS2.p1.4.m4.2.3.2.2.cmml">H</mi><mi id="S2.SS1.SSS2.p1.4.m4.2.3.2.3" xref="S2.SS1.SSS2.p1.4.m4.2.3.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p1.4.m4.2.3.1" xref="S2.SS1.SSS2.p1.4.m4.2.3.1.cmml">â€‹</mo><mrow id="S2.SS1.SSS2.p1.4.m4.2.3.3.2" xref="S2.SS1.SSS2.p1.4.m4.2.3.3.1.cmml"><mo stretchy="false" id="S2.SS1.SSS2.p1.4.m4.2.3.3.2.1" xref="S2.SS1.SSS2.p1.4.m4.2.3.3.1.cmml">(</mo><mi id="S2.SS1.SSS2.p1.4.m4.1.1" xref="S2.SS1.SSS2.p1.4.m4.1.1.cmml">x</mi><mo id="S2.SS1.SSS2.p1.4.m4.2.3.3.2.2" xref="S2.SS1.SSS2.p1.4.m4.2.3.3.1.cmml">,</mo><mi id="S2.SS1.SSS2.p1.4.m4.2.2" xref="S2.SS1.SSS2.p1.4.m4.2.2.cmml">y</mi><mo stretchy="false" id="S2.SS1.SSS2.p1.4.m4.2.3.3.2.3" xref="S2.SS1.SSS2.p1.4.m4.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p1.4.m4.2b"><apply id="S2.SS1.SSS2.p1.4.m4.2.3.cmml" xref="S2.SS1.SSS2.p1.4.m4.2.3"><times id="S2.SS1.SSS2.p1.4.m4.2.3.1.cmml" xref="S2.SS1.SSS2.p1.4.m4.2.3.1"></times><apply id="S2.SS1.SSS2.p1.4.m4.2.3.2.cmml" xref="S2.SS1.SSS2.p1.4.m4.2.3.2"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p1.4.m4.2.3.2.1.cmml" xref="S2.SS1.SSS2.p1.4.m4.2.3.2">subscript</csymbol><ci id="S2.SS1.SSS2.p1.4.m4.2.3.2.2.cmml" xref="S2.SS1.SSS2.p1.4.m4.2.3.2.2">ğ»</ci><ci id="S2.SS1.SSS2.p1.4.m4.2.3.2.3.cmml" xref="S2.SS1.SSS2.p1.4.m4.2.3.2.3">ğ‘–</ci></apply><interval closure="open" id="S2.SS1.SSS2.p1.4.m4.2.3.3.1.cmml" xref="S2.SS1.SSS2.p1.4.m4.2.3.3.2"><ci id="S2.SS1.SSS2.p1.4.m4.1.1.cmml" xref="S2.SS1.SSS2.p1.4.m4.1.1">ğ‘¥</ci><ci id="S2.SS1.SSS2.p1.4.m4.2.2.cmml" xref="S2.SS1.SSS2.p1.4.m4.2.2">ğ‘¦</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p1.4.m4.2c">H_{i}(x,y)</annotation></semantics></math> in each keypoint heatmap indicates the probability that the keypoint lies in the position <math id="S2.SS1.SSS2.p1.5.m5.2" class="ltx_Math" alttext="(x,y)" display="inline"><semantics id="S2.SS1.SSS2.p1.5.m5.2a"><mrow id="S2.SS1.SSS2.p1.5.m5.2.3.2" xref="S2.SS1.SSS2.p1.5.m5.2.3.1.cmml"><mo stretchy="false" id="S2.SS1.SSS2.p1.5.m5.2.3.2.1" xref="S2.SS1.SSS2.p1.5.m5.2.3.1.cmml">(</mo><mi id="S2.SS1.SSS2.p1.5.m5.1.1" xref="S2.SS1.SSS2.p1.5.m5.1.1.cmml">x</mi><mo id="S2.SS1.SSS2.p1.5.m5.2.3.2.2" xref="S2.SS1.SSS2.p1.5.m5.2.3.1.cmml">,</mo><mi id="S2.SS1.SSS2.p1.5.m5.2.2" xref="S2.SS1.SSS2.p1.5.m5.2.2.cmml">y</mi><mo stretchy="false" id="S2.SS1.SSS2.p1.5.m5.2.3.2.3" xref="S2.SS1.SSS2.p1.5.m5.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p1.5.m5.2b"><interval closure="open" id="S2.SS1.SSS2.p1.5.m5.2.3.1.cmml" xref="S2.SS1.SSS2.p1.5.m5.2.3.2"><ci id="S2.SS1.SSS2.p1.5.m5.1.1.cmml" xref="S2.SS1.SSS2.p1.5.m5.1.1">ğ‘¥</ci><ci id="S2.SS1.SSS2.p1.5.m5.2.2.cmml" xref="S2.SS1.SSS2.p1.5.m5.2.2">ğ‘¦</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p1.5.m5.2c">(x,y)</annotation></semantics></math> (see Fig. <a href="#S2.F2" title="Figure 2 â€£ 2.1. 2D single-person pose estimation â€£ 2. 2D human pose estimation â€£ Deep Learning-Based Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (b)). The target (or ground-truth) heatmap is generated by a 2D Gaussian centered at the ground-truth joint location <cite class="ltx_cite ltx_citemacro_citep">(Tompson etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib232" title="" class="ltx_ref">2015</a>)</cite><cite class="ltx_cite ltx_citemacro_citep">(Tompson
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib233" title="" class="ltx_ref">2014</a>)</cite>. Thus pose estimation networks are trained by minimizing the discrepancy (e.g., the Mean Squared-Error (MSE)) between the predicted heatmaps and target heatmaps. Compared with joint coordinates, heatmaps preserve the spatial location information while it can make the training process smoother.</p>
</div>
<div id="S2.SS1.SSS2.p2" class="ltx_para">
<p id="S2.SS1.SSS2.p2.1" class="ltx_p">Therefore, there is a recent growing interest in leveraging heatmaps to represent the joint locations and developing <span id="S2.SS1.SSS2.p2.1.1" class="ltx_text ltx_font_bold">effective CNN architectures</span> for HPE, e.g., <cite class="ltx_cite ltx_citemacro_citep">(Tompson
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib233" title="" class="ltx_ref">2014</a>; Ramakrishna etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib201" title="" class="ltx_ref">2014</a>; Tompson etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib232" title="" class="ltx_ref">2015</a>; Lifshitz
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib130" title="" class="ltx_ref">2016</a>; Bulat and
Tzimiropoulos, <a href="#bib.bib12" title="" class="ltx_ref">2016</a>; Newell
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib172" title="" class="ltx_ref">2016</a>; Wei
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib255" title="" class="ltx_ref">2016</a>; Gkioxari
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2016</a>; Belagiannis and
Zisserman, <a href="#bib.bib8" title="" class="ltx_ref">2017</a>; Yang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib270" title="" class="ltx_ref">2017</a>; Luo etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib146" title="" class="ltx_ref">2018</a>; Debnath etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib47" title="" class="ltx_ref">2018</a>; Xiao etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib260" title="" class="ltx_ref">2018</a>; Zhang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib286" title="" class="ltx_ref">2019a</a>; Artacho and
Savakis, <a href="#bib.bib7" title="" class="ltx_ref">2020</a>; Li etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib125" title="" class="ltx_ref">2022b</a>)</cite>.
As one of the fundamental works, Wei et al. <cite class="ltx_cite ltx_citemacro_citep">(Wei
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib255" title="" class="ltx_ref">2016</a>)</cite> introduced a convolutional networks-based sequential framework named Convolutional Pose Machines (CPM) to predict the locations of keypoints with multi-stage processing (the convolutional networks in each stage utilize the 2D belief maps generated from previous stages and produce the increasingly refined predictions of body part locations). At the same time, Newell et al. <cite class="ltx_cite ltx_citemacro_citep">(Newell
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib172" title="" class="ltx_ref">2016</a>)</cite> proposed an encoder-decoder network named â€stacked hourglassâ€ to repeat bottom-up and top-down processing with intermediate supervision. In this work, the encoder squeezes features through the bottleneck and then the decoder expands them for the substage.
The stacked hourglass (SHG) network consists of consecutive steps of pooling and upsampling layers to capture information at every scale. Since then, complex variations of the SHG architecture were developed for HPE. Following <cite class="ltx_cite ltx_citemacro_citep">(Newell
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib172" title="" class="ltx_ref">2016</a>)</cite>, Chu et al. <cite class="ltx_cite ltx_citemacro_citep">(Chu
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib42" title="" class="ltx_ref">2017</a>)</cite> designed novel Hourglass Residual Units (HRUs), which extend the residual units with a side branch of filters with larger receptive fields, to capture features from various scales. Yang et al. <cite class="ltx_cite ltx_citemacro_citep">(Yang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib270" title="" class="ltx_ref">2017</a>)</cite> designed a multi-branch Pyramid Residual Module (PRM) to replace the residual unit in SHG, leading to enhanced invariance in scales of deep CNNs. Sun et al. <cite class="ltx_cite ltx_citemacro_citep">(Sun
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib222" title="" class="ltx_ref">2019</a>)</cite> presented a novel High-Resolution Net (HRNet) to learn reliable high-resolution representations by connecting multi-resolution subnetworks in parallel and conducting repeated multi-scale fusions, which results in more accurate keypoint heatmap prediction. Inspired by HRNet, Yu et al. <cite class="ltx_cite ltx_citemacro_citep">(Yu
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib275" title="" class="ltx_ref">2021</a>)</cite> introduced a light-weighted HRNet named Lite-HRNet, which designed conditional channel weighting blocks to exchange information between channels and resolutions. Recently, due to the superior performance, the HRNet <cite class="ltx_cite ltx_citemacro_citep">(Sun
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib222" title="" class="ltx_ref">2019</a>)</cite> and its variations <cite class="ltx_cite ltx_citemacro_citep">(Cheng etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2020</a>; Yuan etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib278" title="" class="ltx_ref">2021</a>; Yu
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib275" title="" class="ltx_ref">2021</a>)</cite> have been widely adopted in HPE and other pose-related tasks.</p>
</div>
<div id="S2.SS1.SSS2.p3" class="ltx_para">
<p id="S2.SS1.SSS2.p3.1" class="ltx_p">With the emergence of <span id="S2.SS1.SSS2.p3.1.1" class="ltx_text ltx_font_bold">Generative Adversarial Networks (GANs)</span> <cite class="ltx_cite ltx_citemacro_citep">(Goodfellow etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib62" title="" class="ltx_ref">2014</a>)</cite>, they are explored in HPE to generate biologically plausible pose configurations and to discriminate the predictions with high confidence from those with low confidence, which could infer the poses of the occluded body parts. Inspired by hourglass architecture which efficiently refines joints, Chen et al. <cite class="ltx_cite ltx_citemacro_citep">(Chen
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2017</a>)</cite> constructed a structure-aware conditional adversarial networkâ€“Adversarial PoseNetâ€“which contains an hourglass network-based pose generator and two discriminators to discriminate reasonable body poses from unreasonable ones. Chou et al. <cite class="ltx_cite ltx_citemacro_citep">(Chou
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2018</a>)</cite> built an adversarial learning-based network with two stacked hourglass networks sharing the same structure as the discriminator and generator, respectively. The generator estimates the location of each joint, and the discriminator distinguishes between the ground-truth heatmaps and predicted ones. Unlike GANs-based methods that take the HPE network as the generator and utilize the discriminator to provide supervision, Peng et al. <cite class="ltx_cite ltx_citemacro_citep">(Peng
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib189" title="" class="ltx_ref">2018</a>)</cite> developed an adversarial data augmentation network to optimize data augmentation and network training by treating the HPE network as a discriminator and using augmentation network as a generator to perform adversarial augmentations.</p>
</div>
<div id="S2.SS1.SSS2.p4" class="ltx_para">
<p id="S2.SS1.SSS2.p4.1" class="ltx_p">Besides these efforts in designing effective networks for HPE, <span id="S2.SS1.SSS2.p4.1.1" class="ltx_text ltx_font_bold">body structure information</span> is also investigated to provide more and better supervision information for building HPE networks. Yang et al. <cite class="ltx_cite ltx_citemacro_citep">(Yang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib271" title="" class="ltx_ref">2016</a>)</cite> designed an end-to-end CNN framework for HPE, which can find hard negatives by incorporating the spatial and appearance consistency among human body parts. A structured feature-level learning framework was proposed in <cite class="ltx_cite ltx_citemacro_citep">(Chu
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib41" title="" class="ltx_ref">2016</a>)</cite> for reasoning the correlations among human body joints in HPE, which captures richer information of human body joints and improves the learning results.
Ke et al. <cite class="ltx_cite ltx_citemacro_citep">(Ke
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib97" title="" class="ltx_ref">2018</a>)</cite> designed a multi-scale structure-aware neural network, which combines multi-scale supervision, multi-scale feature combination, structure-aware loss information scheme, and a keypoint masking training method to improve HPE models in complex scenarios. Tang et al. <cite class="ltx_cite ltx_citemacro_citep">(Tang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib225" title="" class="ltx_ref">2018</a>)</cite> built an hourglass-based supervision network, termed as Deeply Learned Compositional Model, to describe the complex and realistic relationships among body parts and learn the compositional pattern information (the orientation, scale, and shape information of each body part) in human bodies. Different from the previous approaches which consider all body parts, Tang and Wu <cite class="ltx_cite ltx_citemacro_citep">(Tang and Wu, <a href="#bib.bib224" title="" class="ltx_ref">2019</a>)</cite> revealed that not all parts are related to each other, therefore introducing a Part-based Branches Network to learn representations specific to each part group rather than a shared representation for all parts.</p>
</div>
<div id="S2.SS1.SSS2.p5" class="ltx_para">
<p id="S2.SS1.SSS2.p5.1" class="ltx_p"><span id="S2.SS1.SSS2.p5.1.1" class="ltx_text ltx_font_bold">Human poses in video sequences</span> are (3D) spatio-temporal signals. Therefore, modeling spatio-temporal information is important for HPE from videos. Jain et al. <cite class="ltx_cite ltx_citemacro_citep">(Jain
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib82" title="" class="ltx_ref">2014</a>)</cite> designed a two-branch CNN framework to incorporate both color and motion features within frame pairs to build an expressive temporal-spatial model in HPE.
Pfister et al. <cite class="ltx_cite ltx_citemacro_citep">(Pfister
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib190" title="" class="ltx_ref">2015</a>)</cite> proposed a CNN that can utilize temporal context information from multiple frames by using optical flow to align predicted heatmaps from neighboring frames. Different from the previous video-based methods which are computationally intensive, Luo et al. <cite class="ltx_cite ltx_citemacro_citep">(Luo etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib146" title="" class="ltx_ref">2018</a>)</cite> introduced a recurrent structure with Long Short-Term Memory to capture temporal geometric consistency and dependency from different frames. This method results in faster training time for the HPE network for videos. Zhang et al. <cite class="ltx_cite ltx_citemacro_citep">(Zhang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib294" title="" class="ltx_ref">2020e</a>)</cite> introduced a keyframe proposal network for capturing spatial and temporal information from frames, and a human pose interpolation module for efficient video-based HPE.</p>
</div>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span><span id="S2.SS2.1.1" class="ltx_text">2D multi-person pose estimation</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Compared to single-person HPE, multi-person HPE is more difficult and challenging because it needs to figure out the number of people and their positions, and how to group keypoints for different people. In order to solve these problems, multi-person HPE methods can be classified into top-down and bottom-up methods. Top-down methods employ off-the-shelf person detectors to obtain a set of boxes (each corresponding to one person) from the input images and then apply single-person pose estimators to each person box to generate multi-person poses. Different from top-down methods, bottom-up methods locate all the body joints in one image first and then group them into individual subjects. In the top-down pipeline, the number of people in the input image will directly affect the computing time. The computing speed for bottom-up methods is usually faster than top-down methods since they do not need to detect the pose for each person separately. Fig. <a href="#S2.F3" title="Figure 3 â€£ 2.2. 2D multi-person pose estimation â€£ 2. 2D human pose estimation â€£ Deep Learning-Based Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the general frameworks for 2D multi-person HPE methods.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2012.13392/assets/x3.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="220" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span><span id="S2.F3.2.1" class="ltx_text" style="font-size:80%;">Illustration of the multi-person 2D HPE frameworks. (a) Top-down approaches have two sub-tasks: (1) human detection and (2) pose estimation in the region of a single human; (b) Bottom-up approaches also have two sub-tasks: (1) detect all keypoints candidates of body parts and (2) associate body parts in different human bodies and assemble them into individual pose representations.</span></figcaption>
</figure>
<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1. </span>Top-down pipeline</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.1" class="ltx_p">In the top-down pipeline as shown in Fig. <a href="#S2.F3" title="Figure 3 â€£ 2.2. 2D multi-person pose estimation â€£ 2. 2D human pose estimation â€£ Deep Learning-Based Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (a), there are two important parts: a human body detector to obtain person bounding boxes and a single-person pose estimator to predict the locations of keypoints within these bounding boxes. A line of works focus on <span id="S2.SS2.SSS1.p1.1.1" class="ltx_text ltx_font_bold">designing and improving the modules</span> in HPE networks, e.g., <cite class="ltx_cite ltx_citemacro_citep">(Papandreou etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib181" title="" class="ltx_ref">2017</a>; Huang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib73" title="" class="ltx_ref">2017</a>; Xiao etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib260" title="" class="ltx_ref">2018</a>; Sun
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib222" title="" class="ltx_ref">2019</a>; Li etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib122" title="" class="ltx_ref">2019a</a>; Moon
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib167" title="" class="ltx_ref">2019b</a>; Wang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib247" title="" class="ltx_ref">2020b</a>; Huang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib72" title="" class="ltx_ref">2020d</a>; Cai etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2020</a>; Zhang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib284" title="" class="ltx_ref">2020h</a>; Liu
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib135" title="" class="ltx_ref">2021b</a>)</cite>. To answer the question â€how good could a simple method beâ€ in building an HPE network, Xiao et al. <cite class="ltx_cite ltx_citemacro_citep">(Xiao etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib260" title="" class="ltx_ref">2018</a>)</cite> added a few deconvolutional layers in the ResNet (backbone network) to build a simple yet effective structure to produce heatmaps for high-resolution representations.
To improve the accuracy of keypoint localization, Wang et al. <cite class="ltx_cite ltx_citemacro_citep">(Wang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib247" title="" class="ltx_ref">2020b</a>)</cite> introduced a two-stage graph-based and model-agnostic framework, called Graph-PCNN. It consists of a localization subnet to obtain rough keypoint locations and a graph pose refinement module to get refined keypoints localization representations.
Cai et al. <cite class="ltx_cite ltx_citemacro_citep">(Cai etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2020</a>)</cite> introduced a multi-stage network with a Residual Steps Network (RSN) module to learn delicate local representations by efficient intra-level feature fusion strategies, and a Pose Refine Machine (PRM) module to find a trade-off between local and global representations in the features.</p>
</div>
<div id="S2.SS2.SSS1.p2" class="ltx_para">
<p id="S2.SS2.SSS1.p2.1" class="ltx_p"><span id="S2.SS2.SSS1.p2.1.1" class="ltx_text ltx_font_bold">Estimating poses under occlusion and truncation scenes</span> often occurs in multi-person settings since the overlapping of limbs is inevitable.
Human detectors may fail in the first step of top-down pipeline due to occlusion. Thus, robustness to occlusion or truncation is an important aspect of the multi-person HPE approaches. Towards this goal, Iqbal and Gall <cite class="ltx_cite ltx_citemacro_citep">(Iqbal and Gall, <a href="#bib.bib79" title="" class="ltx_ref">2016</a>)</cite> built a convolutional pose machine-based pose estimator to estimate the joint candidates. Then they used integer linear programming to solve the joint-to-person association problem and obtain human body poses even in presence of severe occlusions. Fang et al. <cite class="ltx_cite ltx_citemacro_citep">(Fang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib56" title="" class="ltx_ref">2017</a>)</cite> designed a regional multi-person pose estimation (RMPE) approach to improve the performance of HPE in complex scenes. The RMPE framework has three parts: Symmetric Spatial Transformer Network (to detect single person region within an inaccurate bounding box), Parametric Pose Non-Maximum-Suppression (to solve the redundant detection problem), and Pose-Guided Proposals
Generator (to augment training data). Papandreou et al. <cite class="ltx_cite ltx_citemacro_citep">(Papandreou etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib181" title="" class="ltx_ref">2017</a>)</cite> proposed a two-stage architecture with a Faster R-CNN person detector to create bounding boxes for candidate human bodies and a keypoint estimator to predict the locations of keypoints by using a form of heatmap-offset aggregation. The method works well in occluded and cluttered scenes. To alleviate the occlusion problem in HPE, Chen et al. <cite class="ltx_cite ltx_citemacro_citep">(Chen
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2018b</a>)</cite> presented a Cascade Pyramid Network (CPN) which includes two parts: GlobalNet (a feature pyramid network to predict the invisible keypoints) and RefineNet (a network to integrate all levels of features from the GlobalNet with a keypoint mining loss). Their results reveal that CPN has a good performance in predicting occluded keypoints.
Su et al. <cite class="ltx_cite ltx_citemacro_citep">(Su
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib220" title="" class="ltx_ref">2019</a>)</cite> designed two modules, the Channel Shuffle Module and the Spatial &amp; Channel-wise Attention Residual Bottleneck, to achieve channel-wise and spatial information enhancement for better multi-person HPE under occluded scenes.
Qiu et al. <cite class="ltx_cite ltx_citemacro_citep">(Qiu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib200" title="" class="ltx_ref">2020</a>)</cite> developed an Occluded Pose Estimation and Correction module and an occluded pose dataset to solve the occlusion problem in crowd pose estimation.
Umer et al. <cite class="ltx_cite ltx_citemacro_citep">(Umer
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib238" title="" class="ltx_ref">2020</a>)</cite> proposed a keypoint correspondence framework to recover missed poses using temporal information of the previous frame in occluded scenes. The network is trained using self-supervision to improve the pose estimation results on sparsely annotated video datasets.</p>
</div>
<div id="S2.SS2.SSS1.p3" class="ltx_para">
<p id="S2.SS2.SSS1.p3.1" class="ltx_p">Recently, <span id="S2.SS2.SSS1.p3.1.1" class="ltx_text ltx_font_bold">transformer-based methods</span> have attracted more attention <cite class="ltx_cite ltx_citemacro_citep">(Li
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib123" title="" class="ltx_ref">2021b</a>, <a href="#bib.bib126" title="" class="ltx_ref">e</a>; Yang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib269" title="" class="ltx_ref">2021</a>; Yuan etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib278" title="" class="ltx_ref">2021</a>; Shi
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib217" title="" class="ltx_ref">2022</a>; Ma etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib151" title="" class="ltx_ref">2022</a>)</cite> since the attention modules in transformer can obtain long-range dependencies and global evidence of the predicted keypoints, which are more powerful than CNNs. The early exploration <cite class="ltx_cite ltx_citemacro_citep">(Yang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib269" title="" class="ltx_ref">2021</a>)</cite> proposed a transformer-based model for 2D HPE named TransPose, which utilizes the attention layers to predict the heatmaps of the keypoints and learn the fine-grained evidence for HPE in occlusion scenarios. Following <cite class="ltx_cite ltx_citemacro_citep">(Yang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib269" title="" class="ltx_ref">2021</a>)</cite>, Li et al. <cite class="ltx_cite ltx_citemacro_citep">(Li
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib126" title="" class="ltx_ref">2021e</a>)</cite> built a pure transformer-based model named TokenPose to capture the constraint cues and visual appearance relationships by using token representation. In contrast to the methods based on the vision transformer which learn the representations in low resolution, Yuan et al. <cite class="ltx_cite ltx_citemacro_citep">(Yuan etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib278" title="" class="ltx_ref">2021</a>)</cite> presented a high-resolution transformer named HRFormer by exchanging the blocks in HRNet <cite class="ltx_cite ltx_citemacro_citep">(Cheng etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2020</a>)</cite> with transformer modules, which improves the memory and computing efficiency.
Ma et al. <cite class="ltx_cite ltx_citemacro_citep">(Ma etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib151" title="" class="ltx_ref">2022</a>)</cite> applied the token-Pruned Pose Transformer (PPT) for locating the areas of the human body, which enables the model to estimate the multi-view pose efficiently. Different from the traditional two-step structures in HPE, Shi et al. <cite class="ltx_cite ltx_citemacro_citep">(Shi
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib217" title="" class="ltx_ref">2022</a>)</cite> proposed a fully end-to-end framework based on the attention mechanism, which directly estimates the instance-aware body poses.</p>
</div>
<div id="S2.SS2.SSS1.p4" class="ltx_para">
<p id="S2.SS2.SSS1.p4.1" class="ltx_p">Besides the image-based works introduced above, <span id="S2.SS2.SSS1.p4.1.1" class="ltx_text ltx_font_bold">multi-frame pose estimation in videos</span> is also popular in multi-person 2D HPE <cite class="ltx_cite ltx_citemacro_citep">(Guo
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib64" title="" class="ltx_ref">2018</a>; Bertasius etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2019</a>; Liu
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib140" title="" class="ltx_ref">2021a</a>; Xu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib265" title="" class="ltx_ref">2021</a>; Liu
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib141" title="" class="ltx_ref">2022</a>)</cite>, which leverages the temporal information in video sequences to facilitate the pose estimation. In order to reduce the cost of labeling frames in the video, Bertasius et al.<cite class="ltx_cite ltx_citemacro_citep">(Bertasius etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2019</a>)</cite> proposed a network named PoseWarper, which improves the label propagation between frames and benefits the training with the sparse annotations. To alleviate the motion blur and pose occlusions among video frames, Liu et al. <cite class="ltx_cite ltx_citemacro_citep">(Liu
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib140" title="" class="ltx_ref">2021a</a>)</cite> designed a network named DCpose for multi-frame HPE, which contains three modules (Pose Temporal Merger, Pose Residual Fusion, and Pose Correction Network) to exploit the temporal information between frames for keypoint detection. Nevertheless, these methods failed to fully utilize the information from neighboring frames. To solve this issue, Liu et al. <cite class="ltx_cite ltx_citemacro_citep">(Liu
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib141" title="" class="ltx_ref">2022</a>)</cite> introduced a hierarchical alignment framework for alleviating the aggregation of unaligned contexts between two frames.</p>
</div>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2. </span>Bottom-up pipeline</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.1" class="ltx_p">The bottom-up pipeline (e.g., <cite class="ltx_cite ltx_citemacro_citep">(Insafutdinov etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib76" title="" class="ltx_ref">2017</a>; Cao
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2017</a>; Newell
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib171" title="" class="ltx_ref">2017</a>; Fieraru etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2018</a>; Tian
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib229" title="" class="ltx_ref">2019</a>; Kreiss
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib105" title="" class="ltx_ref">2019</a>; Nie
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib176" title="" class="ltx_ref">2019</a>; Jin etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib89" title="" class="ltx_ref">2020a</a>; Cheng etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2020</a>; Wang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib253" title="" class="ltx_ref">2022a</a>, <a href="#bib.bib245" title="" class="ltx_ref">c</a>)</cite>) has two main steps including body joint detection (i.e., extracting local features and predicting body joint candidates) and joint candidates assembling for individual bodies (i.e., grouping joint candidates to build pose representations with part association strategies) as shown in Fig. <a href="#S2.F3" title="Figure 3 â€£ 2.2. 2D multi-person pose estimation â€£ 2. 2D human pose estimation â€£ Deep Learning-Based Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (b).</p>
</div>
<div id="S2.SS2.SSS2.p2" class="ltx_para">
<p id="S2.SS2.SSS2.p2.1" class="ltx_p">Pishchulin et al. <cite class="ltx_cite ltx_citemacro_citep">(Pishchulin etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib193" title="" class="ltx_ref">2016</a>)</cite> proposed a Fast R-CNN-based body part detector named DeepCut, which is one of the earliest <span id="S2.SS2.SSS2.p2.1.1" class="ltx_text ltx_font_bold">two-stage</span> bottom-up approaches. It first detects all the body part candidates, then labels each part and assembles these parts using integer linear programming (ILP) to a final pose. However, DeepCut is computationally expensive. To this end, Insafutdinov et al.<cite class="ltx_cite ltx_citemacro_citep">(Insafutdinov etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib77" title="" class="ltx_ref">2016</a>)</cite> introduced DeeperCut to improve DeepCut by applying a stronger body part detector with a better incremental optimization strategy and image-conditioned pairwise terms to group body parts, leading to improved performance as well as a faster speed. Later, Cao et al. <cite class="ltx_cite ltx_citemacro_citep">(Cao
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2017</a>)</cite> built a detector named OpenPose, which uses Convolutional Pose Machines <cite class="ltx_cite ltx_citemacro_citep">(Wei
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib255" title="" class="ltx_ref">2016</a>)</cite> to predict keypoint coordinates via heatmaps and Part Affinity Fields (PAFs, a set of 2D vector fields with vector maps that encode the position and orientation of limbs) to associate the keypoints to each person. OpenPose largely accelerates the speed of bottom-up multi-person HPE. Based on the OpenPose framework, Zhu et al. <cite class="ltx_cite ltx_citemacro_citep">(Zhu
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib316" title="" class="ltx_ref">2017</a>)</cite> improved the OpenPose structure by adding redundant edges to increase the connections between joints in PAFs and obtained better performance than the baseline approach. Although OpenPose-based methods have achieved impressive results on high-resolution images, they have poor performance with low-resolution images and occlusions. To address this problem, Kreiss et al. <cite class="ltx_cite ltx_citemacro_citep">(Kreiss
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib105" title="" class="ltx_ref">2019</a>)</cite> proposed a bottom-up method called PifPaf that uses a Part Intensity Field to predict the locations of body parts and a Part Association Field to represent the joints association. This method outperformed previous OpenPose-based approaches on low-resolution and occluded scenes.
Motivated by OpenPose <cite class="ltx_cite ltx_citemacro_citep">(Cao
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2017</a>)</cite> and stacked hourglass structure <cite class="ltx_cite ltx_citemacro_citep">(Newell
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib172" title="" class="ltx_ref">2016</a>)</cite>, Newell et al. <cite class="ltx_cite ltx_citemacro_citep">(Newell
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib171" title="" class="ltx_ref">2017</a>)</cite> introduced a <span id="S2.SS2.SSS2.p2.1.2" class="ltx_text ltx_font_bold">single-stage</span> deep network to simultaneously obtain pose detection and group assignments. Following <cite class="ltx_cite ltx_citemacro_citep">(Newell
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib171" title="" class="ltx_ref">2017</a>)</cite>, Jin et al. <cite class="ltx_cite ltx_citemacro_citep">(Jin etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib89" title="" class="ltx_ref">2020a</a>)</cite> proposed a new differentiable Hierarchical Graph Grouping method to learn the human part grouping. Based on <cite class="ltx_cite ltx_citemacro_citep">(Newell
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib171" title="" class="ltx_ref">2017</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citep">(Sun
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib222" title="" class="ltx_ref">2019</a>)</cite>, Cheng et al. <cite class="ltx_cite ltx_citemacro_citep">(Cheng etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2020</a>)</cite> proposed an extension of HRNet, named Higher Resolution Network, which deconvolves the high-resolution heatmaps generated by HRNet to solve the scale variation challenge in bottom-up multi-person HPE.</p>
</div>
<div id="S2.SS2.SSS2.p3" class="ltx_para">
<p id="S2.SS2.SSS2.p3.1" class="ltx_p"><span id="S2.SS2.SSS2.p3.1.1" class="ltx_text ltx_font_bold">Multi-task structures</span> are also employed in bottom-up HPE methods. Papandreou et al. <cite class="ltx_cite ltx_citemacro_citep">(Papandreou etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib180" title="" class="ltx_ref">2018</a>)</cite> introduced PersonLab to combine the pose estimation module and the person segmentation module for keypoints detection and association. PersonLab consists of short-range offsets (for refining heatmaps), mid-range offsets (for predicting the keypoints), and long-range offsets (for grouping keypoints into instances). Kocabas et al. <cite class="ltx_cite ltx_citemacro_citep">(Kocabas
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib100" title="" class="ltx_ref">2018</a>)</cite> presented a multi-task learning model with a pose residual net, named MultiPoseNet, which can perform keypoint prediction, human detection, and semantic segmentation tasks altogether. However, these methods are struggling in dealing with the variance of human scales, to address this problem, Luo et al. <cite class="ltx_cite ltx_citemacro_citep">(Luo
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib147" title="" class="ltx_ref">2021</a>)</cite> introduced a method named SAHR (scale-adaptive heatmap regression) to optimize the joint standard deviation adaptively, which improved the tolerance of various human scales and labeling ambiguities in an efficient way.</p>
</div>
</section>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>2D HPE Summary</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">In summary, the performance of 2D HPE has been significantly improved with the blooming of deep learning techniques. In recent years, deeper and more powerful networks have enhanced the performance of 2D single-person HPE methods such as DeepPose <cite class="ltx_cite ltx_citemacro_citep">(Toshev and
Szegedy, <a href="#bib.bib234" title="" class="ltx_ref">2014</a>)</cite> and Stacked Hourglass Network <cite class="ltx_cite ltx_citemacro_citep">(Newell
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib172" title="" class="ltx_ref">2016</a>)</cite>, as well as in 2D multi-person HPE like AlphaPose <cite class="ltx_cite ltx_citemacro_citep">(Fang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib56" title="" class="ltx_ref">2017</a>)</cite> and OpenPose <cite class="ltx_cite ltx_citemacro_citep">(Cao
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">Although promising performance has been achieved, there are several challenges in 2D HPE that need to be further addressed in future research. First is the reliable detection of individuals under significant occlusion <cite class="ltx_cite ltx_citemacro_citep">(Chen
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2018b</a>)</cite>, e.g., in crowd scenarios. Person detectors in top-down 2D HPE methods may fail to identify the boundaries of largely overlapped human bodies. Similarly, the difficulty of keypoint association is more pronounced for bottom-up approaches in occluded scenes.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">The second challenge is computation efficiency. Although some methods like OpenPose <cite class="ltx_cite ltx_citemacro_citep">(Cao
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2017</a>)</cite> can achieve near real-time processing on special hardware with moderate computing power (e.g., 22 FPS with an Nvidia GTX 1080 Ti GPU), it is still difficult to implement the networks on resource-constrained devices. Real-world applications
(e.g., gaming, AR, and VR)
require more efficient HPE methods on commercial devices which can bring better interactive experiences for users.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p id="S2.SS3.p4.1" class="ltx_p">Another challenge lies in the limited data for rare poses. Although the size of current datasets for 2D HPE is large enough (e.g., COCO dataset <cite class="ltx_cite ltx_citemacro_citep">(Lin etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib133" title="" class="ltx_ref">2014</a>)</cite>) for normal pose estimation (e.g., standing, walking, running), these datasets have limited training data for unusual poses, e.g., falling. The data imbalance may cause model bias, resulting in poor performance on those poses. It would be useful to develop effective data generation or augmentation techniques to generate extra pose data for training more robust models.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>3D human pose estimation</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">3D HPE, which aims to predict the locations of body joints in 3D space, has attracted much interest in recent years since it can provide extensive 3D structure information related to the human body.
It can be applied to various applications (e.g., 3D movie and animation industries, virtual reality, and sports analysis). Although significant improvements have been achieved in 2D HPE, 3D HPE still remains a challenging task.
Most existing works tackle 3D HPE from monocular images or videos, which is an ill-posed and inverse problem due to projection of 3D to 2D where one dimension is lost.
When multiple views are available or other sensors such as IMU and LiDAR are deployed, 3D HPE can be a well-posed problem employing information fusion techniques. Another limitation is that deep learning models are data-hungry and sensitive to the data collection environment. Unlike 2D HPE datasets where accurate 2D pose annotation can be easily obtained, collecting accurate 3D pose annotation is time-consuming and manual labeling is not practical. Also, datasets are usually collected from indoor environments with selected daily actions. Recent works <cite class="ltx_cite ltx_citemacro_citep">(Zhou
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib309" title="" class="ltx_ref">2017</a>; Yang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib272" title="" class="ltx_ref">2018</a>; Wandt and
Rosenhahn, <a href="#bib.bib243" title="" class="ltx_ref">2019</a>)</cite> revealed the poor generalization of models trained with biased datasets by cross-dataset inference. In this section, we first focus on 3D HPE from monocular RGB images and videos and then cover 3D HPE based on other sensors.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>3D HPE from monocular RGB images and videos</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The monocular camera is the most widely used sensor for HPE in both 2D and 3D scenarios. Recent progress in deep learning-based 2D HPE from monocular images and videos has enabled researchers to extend their works to 3D HPE.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">The reconstruction of 3D human poses from a single view of monocular images and videos is a nontrivial task that suffers from self-occlusions and other object occlusions, depth ambiguities, and insufficient training data. It is a severely ill-posed problem because different 3D human poses can be projected to a similar 2D pose projection.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">The problem of occlusion can be alleviated by estimating 3D human pose from multi-view cameras. In a multi-view setting, the viewpoints association needs to be addressed. Thus deep learning-based 3D HPE methods are divided into three categories: single-view single-person 3D HPE, single-view multi-person 3D HPE, and multi-view 3D HPE.</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1. </span>Single-view single person 3D HPE</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">Single-person 3D HPE approaches can be classified into skeleton-only and human mesh recovery (HMR) categories based on whether to reconstruct 3D human skeleton or to recover 3D human mesh by employing a human body model.</p>
</div>
<div id="S3.SS1.SSS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.SSS1.p2.1" class="ltx_p"><span id="S3.SS1.SSS1.p2.1.1" class="ltx_text ltx_font_bold">A. Skeleton-only.</span> The skeleton-only methods estimate 3D human joints as the final output. They do not employ human body models to reconstruct 3D human mesh representation. These methods can be further divided into direct estimation approaches and 2D to 3D lifting approaches.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2012.13392/assets/x4.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="256" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span><span id="S3.F4.2.1" class="ltx_text" style="font-size:80%;">Single-person 3D HPE frameworks. (a) Direct estimation approaches directly estimate the 3D human pose from 2D images. (b) 2D to 3D lifting approaches leverage the predicted 2D human pose (intermediate representation) for 3D pose estimation. (c) Human mesh recovery methods incorporate parametric body models to recover a high-quality 3D human mesh. The 3D pose and shape parameters inferred by the 3D pose and shape network are fed into the model regressor to reconstruct 3D human mesh. Part of the figure is from <cite class="ltx_cite ltx_citemacro_citep">(Arnab
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2019</a>)</cite>.</span></figcaption>
</figure>
<div id="S3.SS1.SSS1.p3" class="ltx_para">
<p id="S3.SS1.SSS1.p3.1" class="ltx_p"><span id="S3.SS1.SSS1.p3.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Direct estimation<span id="S3.SS1.SSS1.p3.1.1.1" class="ltx_text ltx_font_upright">:</span></span> As shown in Fig. <a href="#S3.F4" title="Figure 4 â€£ 3.1.1. Single-view single person 3D HPE â€£ 3.1. 3D HPE from monocular RGB images and videos â€£ 3. 3D human pose estimation â€£ Deep Learning-Based Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>(a), direct estimation methods infer the 3D human pose from 2D images without intermediately estimating 2D pose representation, e.g., <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib120" title="" class="ltx_ref">2015</a>; Tekin etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib226" title="" class="ltx_ref">2016a</a>; Sun
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib223" title="" class="ltx_ref">2017</a>; Pavlakos etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib185" title="" class="ltx_ref">2017a</a>; Pavlakos
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib184" title="" class="ltx_ref">2018a</a>)</cite>. Li and Chan <cite class="ltx_cite ltx_citemacro_citep">(Li and Chan, <a href="#bib.bib118" title="" class="ltx_ref">2014</a>)</cite> employed a shallow network to train the body part detector with sliding windows and the pose coordinate regression synchronously.
Sun et al. <cite class="ltx_cite ltx_citemacro_citep">(Sun
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib223" title="" class="ltx_ref">2017</a>)</cite> proposed a structure-aware regression approach. Instead of using a joint-based representation, they adopted a bone-based representation with more stability. A compositional loss was defined by exploiting the 3D bone structure with bone-based representation that encodes long-range interactions between the bones.
Pavlakos et al. <cite class="ltx_cite ltx_citemacro_citep">(Pavlakos etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib185" title="" class="ltx_ref">2017a</a>; Pavlakos
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib184" title="" class="ltx_ref">2018a</a>)</cite> introduced a volumetric representation to convert the highly non-linear 3D coordinate regression problem to a manageable form in a discretized space. The voxel likelihoods for each joint in the volume were predicted by a convolutional network. Ordinal depth relations of human joints were used to alleviate the need for accurate 3D ground truth poses.</p>
</div>
<div id="S3.SS1.SSS1.p4" class="ltx_para">
<p id="S3.SS1.SSS1.p4.1" class="ltx_p"><span id="S3.SS1.SSS1.p4.1.1" class="ltx_text ltx_font_bold ltx_font_italic">2D to 3D lifting<span id="S3.SS1.SSS1.p4.1.1.1" class="ltx_text ltx_font_upright">:</span></span> Motivated by the recent success of 2D HPE, 2D to 3D lifting approaches that infer 3D human pose from the intermediately estimated 2D human pose have become a popular 3D HPE solution as illustrated in Fig. <a href="#S3.F4" title="Figure 4 â€£ 3.1.1. Single-view single person 3D HPE â€£ 3.1. 3D HPE from monocular RGB images and videos â€£ 3. 3D human pose estimation â€£ Deep Learning-Based Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> (b). In the first stage, off-the-shelf 2D HPE models are employed to estimate 2D pose. Then 2D to 3D lifting is used to obtain 3D pose in the second stage, e.g., <cite class="ltx_cite ltx_citemacro_citep">(Chen and Ramanan, <a href="#bib.bib19" title="" class="ltx_ref">2017</a>; Martinez etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib157" title="" class="ltx_ref">2017</a>; Tekin etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib227" title="" class="ltx_ref">2017</a>; Zhou etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib308" title="" class="ltx_ref">2019</a>; Moreno-Noguer, <a href="#bib.bib169" title="" class="ltx_ref">2017</a>; Li and Lee, <a href="#bib.bib112" title="" class="ltx_ref">2019</a>)</cite>. Benefiting from the excellent performance of state-of-the-art 2D pose detectors, 2D to 3D lifting approaches generally outperform direct estimation approaches.
Martinez et al. <cite class="ltx_cite ltx_citemacro_citep">(Martinez etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib157" title="" class="ltx_ref">2017</a>)</cite> proposed a fully connected residual network to regress 3D joint locations based on the 2D joint locations. Despite achieving state-of-the-art results at that time, the method could fail due to reconstruction ambiguity of over-reliance on the 2D pose detector. Tekin et al. <cite class="ltx_cite ltx_citemacro_citep">(Tekin etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib227" title="" class="ltx_ref">2017</a>)</cite> and Zhou et al. <cite class="ltx_cite ltx_citemacro_citep">(Zhou etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib308" title="" class="ltx_ref">2019</a>)</cite> adopted 2D heatmaps instead of 2D pose as intermediate representations for estimating 3D pose.
Wang et al. <cite class="ltx_cite ltx_citemacro_citep">(Wang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib251" title="" class="ltx_ref">2018</a>)</cite> developed a pairwise ranking CNN to predict the depth ranking of pairwise human joints. Then, a coarse-to-fine pose estimator was used to regress the 3D pose from 2D joints and the depth ranking matrix. Jahangiri and Yuille <cite class="ltx_cite ltx_citemacro_citep">(Jahangiri and
Yuille, <a href="#bib.bib81" title="" class="ltx_ref">2017</a>)</cite>, Sharma et al. <cite class="ltx_cite ltx_citemacro_citep">(Sharma etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib216" title="" class="ltx_ref">2019</a>)</cite>, and Li and Lee <cite class="ltx_cite ltx_citemacro_citep">(Li and Lee, <a href="#bib.bib112" title="" class="ltx_ref">2019</a>)</cite> first generated multiple diverse 3D pose hypotheses then applied ranking networks to select the best 3D pose.</p>
</div>
<div id="S3.SS1.SSS1.p5" class="ltx_para">
<p id="S3.SS1.SSS1.p5.1" class="ltx_p">Given that a human pose can be represented as a graph where the joints are the nodes and the bones are the edges, <span id="S3.SS1.SSS1.p5.1.1" class="ltx_text ltx_font_bold">Graph Convolutional Networks (GCNs)</span> have been applied to the 2D-to-3D pose lifting problem by showing promising performance <cite class="ltx_cite ltx_citemacro_citep">(Ci
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib43" title="" class="ltx_ref">2019</a>; Zhao
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib297" title="" class="ltx_ref">2019b</a>; Choi
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2020</a>; Liu
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib138" title="" class="ltx_ref">2020a</a>; Zeng
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib282" title="" class="ltx_ref">2020</a>)</cite>.
Ci et al. <cite class="ltx_cite ltx_citemacro_citep">(Ci
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib43" title="" class="ltx_ref">2019</a>)</cite> proposed a Locally Connected Network (LCN), which leverages both a fully connected network and GCN to encode the relationship between local joint neighborhoods. LCN can overcome the limitations of GCN that the weight-sharing scheme harms the pose estimation modelâ€™s representation ability, and the structure matrix lacks the flexibility to support customized node dependence. Zhao et al. <cite class="ltx_cite ltx_citemacro_citep">(Zhao
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib297" title="" class="ltx_ref">2019b</a>)</cite> also tackled the limitation of the shared weight matrix of convolution filters for all the nodes in GCN. A Semantic-GCN was proposed to investigate the semantic information and relationship, which is not explicitly represented in the graph. The semantic graph convolution (SemGConv) operation is used to learn channel-wise weights for edges. Both local and global relationships among nodes are captured since SemGConv and non-local layers are interleaved. Zhou et al. <cite class="ltx_cite ltx_citemacro_citep">(Zou and Tang, <a href="#bib.bib317" title="" class="ltx_ref">2021</a>)</cite> further introduced a novel modulated GCN network which consists of weight modulation and affinity modulation. The weight modulation exploits different modulation vectors for different nodes that disentangles the feature transformations. The affinity modulation explores additional joint correlations beyond the defined human skeleton.</p>
</div>
<div id="S3.SS1.SSS1.p6" class="ltx_para">
<p id="S3.SS1.SSS1.p6.1" class="ltx_p"><span id="S3.SS1.SSS1.p6.1.1" class="ltx_text ltx_font_bold">The kinematic model</span> is an articulated body representation by connected bones and joints with kinematic constraints, which has gained increasing attention in 3D HPE in recent years. Many methods leverage prior knowledge based on the kinematic model such as skeletal joint connectivity information, joint rotation properties, and fixed bone-length ratios for plausible pose estimation, e.g., <cite class="ltx_cite ltx_citemacro_citep">(Zhou
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib310" title="" class="ltx_ref">2016a</a>; Mehta etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib161" title="" class="ltx_ref">2017</a>; Nie
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib174" title="" class="ltx_ref">2017</a>; Wang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib246" title="" class="ltx_ref">2019a</a>; Kundu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib109" title="" class="ltx_ref">2020c</a>; Xu
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib264" title="" class="ltx_ref">2020c</a>; Nie etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib175" title="" class="ltx_ref">2020</a>; Georgakis etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib59" title="" class="ltx_ref">2020</a>)</cite>. Zhou et al. <cite class="ltx_cite ltx_citemacro_citep">(Zhou
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib310" title="" class="ltx_ref">2016a</a>)</cite> embedded a kinematic model into a network as kinematic layers to enforce the orientation and rotation constraints. Nie et al. <cite class="ltx_cite ltx_citemacro_citep">(Nie
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib174" title="" class="ltx_ref">2017</a>)</cite> and Lee et al. <cite class="ltx_cite ltx_citemacro_citep">(Lee etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib111" title="" class="ltx_ref">2018</a>)</cite> employed a skeleton-LSTM network to leverage joint relations and connectivity. Observing that human body parts have a distinct degree of freedom (DOF) based on the kinematic structure, Wang et al. <cite class="ltx_cite ltx_citemacro_citep">(Wang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib246" title="" class="ltx_ref">2019a</a>)</cite> and Nie et al. <cite class="ltx_cite ltx_citemacro_citep">(Nie etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib175" title="" class="ltx_ref">2020</a>)</cite> proposed bidirectional networks to model the kinematic and geometric dependencies of the human skeleton. Kundu et al. <cite class="ltx_cite ltx_citemacro_citep">(Kundu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib109" title="" class="ltx_ref">2020c</a>)</cite> <cite class="ltx_cite ltx_citemacro_citep">(Kundu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib108" title="" class="ltx_ref">2020b</a>)</cite> designed a kinematic structure preservation approach by inferring local-kinematic parameters with energy-based loss and explored 2D part segments based on the parent-relative local limb kinematic model. Xu et al. <cite class="ltx_cite ltx_citemacro_citep">(Xu
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib264" title="" class="ltx_ref">2020c</a>)</cite> demonstrated that noise in the 2D joint is one of the key obstacles for accurate 3D pose estimation. Hence a 2D pose correction module was employed to refine unreliable 2D joints based on the kinematic structure. Zanfir et al. <cite class="ltx_cite ltx_citemacro_citep">(Zanfir etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib279" title="" class="ltx_ref">2020</a>)</cite> introduced a kinematic latent normalizing flow representation (a sequence of invertible transformations applied to the original distribution) with differentiable semantic body part alignment loss functions.</p>
</div>
<div id="S3.SS1.SSS1.p7" class="ltx_para">
<p id="S3.SS1.SSS1.p7.1" class="ltx_p">3D HPE datasets are usually collected from controlled environments with selected common motions. It is difficult to obtain accurate 3D pose annotations for in-the-wild data. Thus 3D HPE for <span id="S3.SS1.SSS1.p7.1.1" class="ltx_text ltx_font_bold">in-the-wild data with unusual poses and occlusions</span> is still a challenge.
To this end, a group of 2D to 3D lifting methods estimate the 3D human pose from in-the-wild images without 3D pose annotations such as <cite class="ltx_cite ltx_citemacro_citep">(Zhou
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib309" title="" class="ltx_ref">2017</a>; Habibie etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib65" title="" class="ltx_ref">2019</a>; Chen etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2019</a>; Yang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib272" title="" class="ltx_ref">2018</a>; Wandt and
Rosenhahn, <a href="#bib.bib243" title="" class="ltx_ref">2019</a>)</cite>. Zhou et al. <cite class="ltx_cite ltx_citemacro_citep">(Zhou
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib309" title="" class="ltx_ref">2017</a>)</cite> proposed a weakly supervised transfer learning method that uses 2D annotations of in-the-wild images as weak labels. A 3D pose estimation module was connected with intermediate layers of the 2D pose estimation module. For in-the-wild images, 2D pose estimation module performed a supervised 2D heatmap regression and a 3D bone length constraint-induced loss was applied in the weakly supervised 3D pose estimation module. Habibie et al. <cite class="ltx_cite ltx_citemacro_citep">(Habibie etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib65" title="" class="ltx_ref">2019</a>)</cite> tailored a projection loss to refine the 3D human pose without 3D annotation. A 3D-2D projection module was designed to estimate the 2D body joint locations with the predicted 3D pose from the earlier network layer. The projection loss was used to update the 3D human pose without requiring 3D annotations. Inspired by <cite class="ltx_cite ltx_citemacro_citep">(Drover etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib51" title="" class="ltx_ref">2018</a>)</cite>, Chen et al. <cite class="ltx_cite ltx_citemacro_citep">(Chen etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2019</a>)</cite> proposed an unsupervised lifting network based on the closure and invariance lifting properties with a geometric self-consistency loss for the lift-reproject-lift process. Closure means for a lifted 3D skeleton, after random rotation and re-projection, the resulting 2D skeleton will lie within the distribution of valid 2D poses. Invariance means when changing the viewpoint of 2D projection from a 3D skeleton, the re-lifted 3D skeleton should be the same.</p>
</div>
<div id="S3.SS1.SSS1.p8" class="ltx_para">
<p id="S3.SS1.SSS1.p8.1" class="ltx_p">Instead of estimating 3D human pose from monocular images, <span id="S3.SS1.SSS1.p8.1.1" class="ltx_text ltx_font_bold">videos can provide temporal information</span> to improve accuracy and robustness of 3D HPE, e.g., <cite class="ltx_cite ltx_citemacro_citep">(Zhou
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib312" title="" class="ltx_ref">2016b</a>; Zhou etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib313" title="" class="ltx_ref">2018</a>; Dabral etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib45" title="" class="ltx_ref">2018</a>; Pavllo etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib188" title="" class="ltx_ref">2019</a>; Cheng etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2019</a>; Cai etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">2019</a>; Wang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib249" title="" class="ltx_ref">2020d</a>; Tekin
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib228" title="" class="ltx_ref">2016b</a>)</cite>.
Hossain and Little <cite class="ltx_cite ltx_citemacro_citep">(Rayat
ImtiazÂ Hossain and Little, <a href="#bib.bib202" title="" class="ltx_ref">2018</a>)</cite> proposed a recurrent neural network using a Long Short-Term Memory (LSTM) unit with shortcut connections to exploit temporal information from sequences of human pose. Their method exploits the past events in a sequence-to-sequence network to predict temporally consistent 3D pose. Noticing that the complementary property between spatial constraints and temporal correlations is usually ignored by prior work, Dabral et al. <cite class="ltx_cite ltx_citemacro_citep">(Dabral etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib45" title="" class="ltx_ref">2018</a>)</cite>, Cai et al. <cite class="ltx_cite ltx_citemacro_citep">(Cai etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">2019</a>)</cite>, and Li et al. <cite class="ltx_cite ltx_citemacro_citep">(Li
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib128" title="" class="ltx_ref">2019</a>)</cite> exploited the spatial-temporal relationships and constraints (e.g., bone-length constraint and left-right symmetry constraint) to improve 3D HPE performance from sequential frames. Pavllo et al. <cite class="ltx_cite ltx_citemacro_citep">(Pavllo etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib188" title="" class="ltx_ref">2019</a>)</cite> proposed a temporal convolution network to estimate 3D pose over 2D keypoints from consecutive 2D sequences. However, their method is based on the assumption that prediction errors are temporally non-continuous and independent, which may not hold in the presence of occlusions <cite class="ltx_cite ltx_citemacro_citep">(Cheng etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2019</a>)</cite>. Based on <cite class="ltx_cite ltx_citemacro_citep">(Pavllo etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib188" title="" class="ltx_ref">2019</a>)</cite>, Chen et al. <cite class="ltx_cite ltx_citemacro_citep">(Chen
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2021</a>)</cite> added a bone direction module and bone length module to ensure human anatomy temporal consistency across video frames, while Liu et al. <cite class="ltx_cite ltx_citemacro_citep">(Liu
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib139" title="" class="ltx_ref">2020c</a>)</cite> utilized the attention mechanism to recognize significant frames and model long-range dependencies in large temporal receptive fields. Zeng et al. <cite class="ltx_cite ltx_citemacro_citep">(Zeng
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib282" title="" class="ltx_ref">2020</a>)</cite> employed the split-and-recombine strategy to address the rare and unseen pose problem. The human body is first split into local regions for processing through separate temporal convolutional network branches, then the low-dimensional global context obtained from each branch is combined for maintaining global coherence.</p>
</div>
<div id="S3.SS1.SSS1.p9" class="ltx_para">
<p id="S3.SS1.SSS1.p9.1" class="ltx_p"><span id="S3.SS1.SSS1.p9.1.1" class="ltx_text ltx_font_bold">Transformer architectures</span> have become the model of choice in natural language processing due to the self-attention mechanism, and now are developing rapidly in the field of computer vision. Recent works have demonstrated the powerful global representation ability of transformer attention mechanism in various vision tasks <cite class="ltx_cite ltx_citemacro_citep">(Khan etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib98" title="" class="ltx_ref">2021</a>)</cite>. Zheng et al. <cite class="ltx_cite ltx_citemacro_citep">(Zheng etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib305" title="" class="ltx_ref">2021</a>)</cite> presented the first purely transformer-based approach, named PoseFormer, for 3D HPE without convolutional architectures involved. The spatial transformer module encodes local relationships between human body joints, and the temporal transformer module captures the global dependencies across frames in the entire sequence.
Li et al. <cite class="ltx_cite ltx_citemacro_citep">(Li
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib121" title="" class="ltx_ref">2022a</a>)</cite> further designed a multi-hypothesis transformer to exploit spatial-temporal representations of multiple pose hypotheses.
Zhao et al. <cite class="ltx_cite ltx_citemacro_citep">(Zhao
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib300" title="" class="ltx_ref">2023</a>)</cite> further proposed PoseFormerV2, which exploits a compact representation of lengthy skeleton sequences in the
frequency domain to efficiently scale up the receptive field and boost robustness to noisy 2D joint detection.</p>
</div>
<div id="S3.SS1.SSS1.p10" class="ltx_para ltx_noindent">
<p id="S3.SS1.SSS1.p10.1" class="ltx_p"><span id="S3.SS1.SSS1.p10.1.1" class="ltx_text ltx_font_bold">B. Human Mesh Recovery (HMR).</span> HMR methods incorporate parametric body models such as SMPL <cite class="ltx_cite ltx_citemacro_citep">(Loper etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib144" title="" class="ltx_ref">2015</a>)</cite> to recovery human mesh as shown in Fig. <a href="#S3.F4" title="Figure 4 â€£ 3.1.1. Single-view single person 3D HPE â€£ 3.1. 3D HPE from monocular RGB images and videos â€£ 3. 3D human pose estimation â€£ Deep Learning-Based Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>(c). The SMPL (Skinned Multi-Person Linear) model <cite class="ltx_cite ltx_citemacro_citep">(Loper etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib144" title="" class="ltx_ref">2015</a>)</cite> is a widely used model in 3D HPE, which can be modeled with natural pose-dependent deformations exhibiting soft-tissue dynamics. To learn how people deform with poses, there are 1786 high-resolution 3D scans of different subjects of poses with template mesh in SMPL to optimize the blend weights <cite class="ltx_cite ltx_citemacro_citep">(Kavan, <a href="#bib.bib96" title="" class="ltx_ref">2014</a>)</cite>, pose-dependent blend shapes, the mean template shape, and the regressor from vertices to joint locations. The 3D pose can be obtained by using the model-defined joint regression matrix <cite class="ltx_cite ltx_citemacro_citep">(Kolotouros etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib102" title="" class="ltx_ref">2019</a>)</cite>.
There are also other popular volumetric models such as DYNA <cite class="ltx_cite ltx_citemacro_citep">(Pons-Moll etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib194" title="" class="ltx_ref">2015</a>)</cite>, Stitched Puppet model <cite class="ltx_cite ltx_citemacro_citep">(Zuffi and Black, <a href="#bib.bib318" title="" class="ltx_ref">2015</a>)</cite>, Frankenstein &amp; Adam <cite class="ltx_cite ltx_citemacro_citep">(Joo
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib94" title="" class="ltx_ref">2018</a>)</cite>, and GHUM &amp; GHUML(ite) <cite class="ltx_cite ltx_citemacro_citep">(Xu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib263" title="" class="ltx_ref">2020a</a>)</cite>.</p>
</div>
<div id="S3.SS1.SSS1.p11" class="ltx_para">
<p id="S3.SS1.SSS1.p11.1" class="ltx_p"><span id="S3.SS1.SSS1.p11.1.1" class="ltx_text ltx_font_bold">Volumetric models are used to recover high-quality human mesh</span>, providing extra shape information of the human body. As one of the most popular volumetric models, the SMPL model <cite class="ltx_cite ltx_citemacro_citep">(Loper etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib144" title="" class="ltx_ref">2015</a>)</cite> has been widely used in 3D HPE, e.g.,
<cite class="ltx_cite ltx_citemacro_citep">(Bogo etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2016</a>; Kolotouros etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib102" title="" class="ltx_ref">2019</a>; Zhu
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib314" title="" class="ltx_ref">2019</a>; Arnab
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2019</a>; Moon and Lee, <a href="#bib.bib168" title="" class="ltx_ref">2020</a>; Zhang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib290" title="" class="ltx_ref">2020c</a>; Li
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib116" title="" class="ltx_ref">2021d</a>; Zhang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib288" title="" class="ltx_ref">2021</a>)</cite>, because it is compatible with existing rendering engines. Tan et al. <cite class="ltx_cite ltx_citemacro_citep">(VinceÂ Tan and
Cipolla, <a href="#bib.bib240" title="" class="ltx_ref">2017</a>)</cite>, Tung et al. <cite class="ltx_cite ltx_citemacro_citep">(Tung
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib237" title="" class="ltx_ref">2017</a>)</cite>, Pavlakos et al. <cite class="ltx_cite ltx_citemacro_citep">(Pavlakos
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib187" title="" class="ltx_ref">2018b</a>)</cite>, and Omran et al. <cite class="ltx_cite ltx_citemacro_citep">(Omran etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib177" title="" class="ltx_ref">2018</a>)</cite> regressed SMPL parameters to reconstruct 3D human mesh. Instead of predicting SMPL parameters, Kolotouros et al. <cite class="ltx_cite ltx_citemacro_citep">(Kolotouros etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib103" title="" class="ltx_ref">2019</a>)</cite> regressed the locations of the SMPL mesh vertices using a Graph-CNN architecture.
Kocabas et al. <cite class="ltx_cite ltx_citemacro_citep">(Kocabas
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib99" title="" class="ltx_ref">2020</a>)</cite> included the large-scale motion capture dataset AMASS <cite class="ltx_cite ltx_citemacro_citep">(Mahmood etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib153" title="" class="ltx_ref">2019</a>)</cite> for adversarial training of their SMPL-based method named VIBE (Video Inference for Body Pose and Shape Estimation). VIBE leveraged AMASS to discriminate between real human motions and those predicted by the pose regression module. Since low-resolution visual content is more common in real-world scenarios than high-resolution visual content, existing well-trained models may fail when the resolution is degraded. Xu et al. <cite class="ltx_cite ltx_citemacro_citep">(Xu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib267" title="" class="ltx_ref">2020b</a>)</cite> introduced the contrastive learning scheme into a self-supervised resolution-aware SMPL-based network. The self-supervised contrastive learning scheme uses a self-supervision loss and a contrastive feature loss to enforce feature and scale consistency. Choi et al. <cite class="ltx_cite ltx_citemacro_citep">(Choi
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib38" title="" class="ltx_ref">2021</a>)</cite> presented a temporally consistent mesh recovery system (named TCMR) to smooth 3D human motion output using a bi-directional gated recurrent unit. Kolotouros et al. <cite class="ltx_cite ltx_citemacro_citep">(Kolotouros etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib104" title="" class="ltx_ref">2021</a>)</cite> proposed a probabilistic model using conditional normalizing flow for 3D human mesh recovery from 2D evidence. Zheng et al. <cite class="ltx_cite ltx_citemacro_citep">(Zheng
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib303" title="" class="ltx_ref">2022</a>)</cite> designed a lightweight transformer-based method that can reconstruct human mesh from 2D human pose with a significant computation and memory cost reduction, while the performance is competitive with Pose2Mesh <cite class="ltx_cite ltx_citemacro_citep">(Choi
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S3.SS1.SSS1.p12" class="ltx_para">
<p id="S3.SS1.SSS1.p12.1" class="ltx_p">There are a few recent attempts to utilize <span id="S3.SS1.SSS1.p12.1.1" class="ltx_text ltx_font_bold">transformer</span> in HMR <cite class="ltx_cite ltx_citemacro_citep">(Lin
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib131" title="" class="ltx_ref">2021a</a>; Zheng
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib303" title="" class="ltx_ref">2022</a>, <a href="#bib.bib302" title="" class="ltx_ref">2023a</a>, <a href="#bib.bib304" title="" class="ltx_ref">2023b</a>)</cite>. Lin et al. proposed METRO <cite class="ltx_cite ltx_citemacro_citep">(Lin
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib131" title="" class="ltx_ref">2021a</a>)</cite> and MeshGraphormer <cite class="ltx_cite ltx_citemacro_citep">(Lin
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib132" title="" class="ltx_ref">2021b</a>)</cite> that combine CNNs with transformer networks to regress SMPL mesh vertices from a single image. However, they pursued higher accuracy while sacrificing computation and memory.
FeatER <cite class="ltx_cite ltx_citemacro_citep">(Zheng
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib304" title="" class="ltx_ref">2023b</a>)</cite> and POTTER <cite class="ltx_cite ltx_citemacro_citep">(Zheng
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib302" title="" class="ltx_ref">2023a</a>)</cite> reduced the computational and memory cost by proposing lightweight transformer architectures, while both of them outperform METRO by only requiring less than 10%of total parameters and 15% MACs.</p>
</div>
<div id="S3.SS1.SSS1.p13" class="ltx_para">
<p id="S3.SS1.SSS1.p13.1" class="ltx_p">There are several <span id="S3.SS1.SSS1.p13.1.1" class="ltx_text ltx_font_bold">extended SMPL-based models</span> to address the limitations of the SMPL model such as high computational complexity, and lack of hands and facial landmarks. SMPLify <cite class="ltx_cite ltx_citemacro_citep">(Lassner etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib110" title="" class="ltx_ref">2017</a>)</cite> is an optimization method that fits the SMPL model to the detected 2D joints and minimizes the re-projection error. Pavlakos et al. <cite class="ltx_cite ltx_citemacro_citep">(Pavlakos etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib183" title="" class="ltx_ref">2019</a>)</cite> introduced a new model, named SMPL-X, that can also predict fully articulated hands and facial landmarks. Following the SMPLify method, they also proposed SMPLify-X, which is an improved version learned from AMASS dataset <cite class="ltx_cite ltx_citemacro_citep">(Mahmood etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib153" title="" class="ltx_ref">2019</a>)</cite>. Hassan et al. <cite class="ltx_cite ltx_citemacro_citep">(Hassan
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib66" title="" class="ltx_ref">2019</a>)</cite> further extended SMPLify-X to PROX â€“ a method enforcing Proximal Relationships with Object eXclusion by adding 3D environmental constraints. Kolotouros et al. <cite class="ltx_cite ltx_citemacro_citep">(Kolotouros etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib102" title="" class="ltx_ref">2019</a>)</cite> integrated the regression-based and optimization-based SMPL parameter estimation methods to a new one named SPIN (SMPL oPtimization IN the loop) while employing SMPLify in the training loop. The estimated 2D pose served as the initialization in an iterative optimization routine for producing a more accurate 3D pose and shape.</p>
</div>
<div id="S3.SS1.SSS1.p14" class="ltx_para">
<p id="S3.SS1.SSS1.p14.1" class="ltx_p">Instead of using the SMPL-based model, other models have also been used for recovering 3D human pose or mesh, e.g.,<cite class="ltx_cite ltx_citemacro_citep">(Qammaz and
Argyros, <a href="#bib.bib196" title="" class="ltx_ref">2019</a>; Saito
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib213" title="" class="ltx_ref">2020</a>; Wang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib244" title="" class="ltx_ref">2020a</a>)</cite>. Chen et al. <cite class="ltx_cite ltx_citemacro_citep">(Cheng
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2019</a>)</cite> introduced a Cylinder Man Model to generate occlusion labels for 3D data and performed data augmentation. A pose regularization term was introduced to penalize wrong estimated occlusion labels. Xiang et al. <cite class="ltx_cite ltx_citemacro_citep">(Xiang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib259" title="" class="ltx_ref">2019</a>)</cite> utilized the Adam model <cite class="ltx_cite ltx_citemacro_citep">(Joo
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib94" title="" class="ltx_ref">2018</a>)</cite> to reconstruct the 3D motions. A 3D human representation, named 3D Part Orientation Fields (POFs), was introduced to encode the 3D orientation of human body parts in the 2D space.
Wang et al. <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib244" title="" class="ltx_ref">2020a</a>)</cite> presented a new Bone-level Skinned Model of human mesh, which decouples bone modeling and identity-specific variations by setting bone lengths and joint angles. Fisch and Clark <cite class="ltx_cite ltx_citemacro_citep">(Fisch and Clark, <a href="#bib.bib58" title="" class="ltx_ref">2020</a>)</cite> introduced an orientation keypoints model which can compute full 3-axis joint rotations including yaw, pitch, and roll for 6D HPE. 
<br class="ltx_break"></p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2. </span>Single-view multi-person 3D HPE</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">For 3D multi-person HPE from monocular RGB images or videos, similar categories as 2D multi-person HPE are noted here: top-down approaches and bottom-up approaches as shown in Fig. <a href="#S3.F5" title="Figure 5 â€£ 3.1.2. Single-view multi-person 3D HPE â€£ 3.1. 3D HPE from monocular RGB images and videos â€£ 3. 3D human pose estimation â€£ Deep Learning-Based Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> (a) and Fig. <a href="#S3.F5" title="Figure 5 â€£ 3.1.2. Single-view multi-person 3D HPE â€£ 3.1. 3D HPE from monocular RGB images and videos â€£ 3. 3D human pose estimation â€£ Deep Learning-Based Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> (b), respectively. The comparison between 2D top-down and bottom-up approaches in Section <a href="#S2.SS2" title="2.2. 2D multi-person pose estimation â€£ 2. 2D human pose estimation â€£ Deep Learning-Based Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a> is applicable to the 3D case.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2012.13392/assets/x5.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="187" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span><span id="S3.F5.2.1" class="ltx_text" style="font-size:80%;">Illustration of the multi-person 3D HPE frameworks. (a) Top-Down methods first detect single-person regions by human detection network. For each single-person region, individual 3D poses can be estimated by 3D pose network. Then all 3D poses are aligned to the world coordinate. (b) Bottom-Up methods first estimate all body joints and depth maps, then associate body parts to each person according to the root depth and part relative depth. Part of the figure is from <cite class="ltx_cite ltx_citemacro_citep">(Zhen etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib301" title="" class="ltx_ref">2020</a>)</cite>.</span></figcaption>
</figure>
<div id="S3.SS1.SSS2.p2" class="ltx_para">
<p id="S3.SS1.SSS2.p2.1" class="ltx_p"><span id="S3.SS1.SSS2.p2.1.1" class="ltx_text ltx_font_bold">Top-down approaches.</span>
Top-down approaches of 3D multi-person HPE first perform human detection to detect each individual person. Then for each detected person, the absolute root (center joint of the human) coordinate and 3D root-relative pose are estimated by 3D pose networks. Based on the absolute root coordinate of each person and their root-relative pose, all poses are aligned to the world coordinate. Rogez et al. <cite class="ltx_cite ltx_citemacro_citep">(Rogez
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib208" title="" class="ltx_ref">2017</a>)</cite> localized candidate regions of each person to generate potential poses, and used a regressor to jointly refine the pose proposals. This localization-classification-regression method, named LCR-Net, performed well on the controlled environment datasets but could not generalize well to in-the-wild images. To address this issue, Rogez et al. <cite class="ltx_cite ltx_citemacro_citep">(Rogez
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib209" title="" class="ltx_ref">2019</a>)</cite> proposed LCR-Net++ by using synthetic data augmentation for the training data to improve performance. Zanfir et al. <cite class="ltx_cite ltx_citemacro_citep">(Zanfir etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib280" title="" class="ltx_ref">2018</a>)</cite> added semantic segmentation to the 3D multi-person HPE module with scene constraints. Additionally, the 3D temporal assignment problem was tackled by the Hungarian matching method for video-based multi-person 3D HPE. Moon et al. <cite class="ltx_cite ltx_citemacro_citep">(Moon
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib166" title="" class="ltx_ref">2019a</a>)</cite> introduced a camera distance-aware approach under top-down pipeline. The RootNet estimated the camera-centered coordinates of human bodyâ€™s roots.
Then the root-relative 3D pose of each cropped human was estimated by the proposed PoseNet. Benzine et al. <cite class="ltx_cite ltx_citemacro_citep">(Benzine etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite> proposed a single-shot approach named PandaNet (Pose estimAtioN and Detection Anchor-based Network). A low-resolution anchor-based representation was introduced to avoid the occlusion problem. A pose-aware anchor selection module was developed to address the overlapping problem by removing ambiguous anchors. An automatic weighting of losses associated with different scales was used to handle the imbalance issue of different sizes of people. Li et al. <cite class="ltx_cite ltx_citemacro_citep">(Li
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib114" title="" class="ltx_ref">2020b</a>)</cite> tackled the lack of global information in the top-down approaches. They adopted a Hierarchical Multi-person Ordinal Relations method to leverage body-level semantic and global consistency for encoding the interaction information hierarchically.</p>
</div>
<div id="S3.SS1.SSS2.p3" class="ltx_para">
<p id="S3.SS1.SSS2.p3.1" class="ltx_p"><span id="S3.SS1.SSS2.p3.1.1" class="ltx_text ltx_font_bold">Bottom-up approaches.</span> In contrast to top-down approaches, bottom-up approaches first produce all body joint locations and depth maps, then associate body parts to each person according to the root depth and part relative depth. A key challenge of bottom-up approaches is how to group human body joints belonging to each person. Zanfir et al. <cite class="ltx_cite ltx_citemacro_citep">(Zanfir etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib281" title="" class="ltx_ref">2018</a>)</cite> formulated the person grouping problem as a binary integer programming (BIP) problem. A limb scoring module was used to estimate candidate kinematic connections of detected joints and a skeleton grouping module assembled limbs into skeletons by solving the BIP problem. Nie et al. <cite class="ltx_cite ltx_citemacro_citep">(Nie
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib176" title="" class="ltx_ref">2019</a>)</cite> proposed a Single-stage multi-person Pose Machine (SPM) to define the unique identity root joint for each person. The body joints were aligned to each root joint by using dense displacement maps. However, this method is limited in that only paired 2D images and 3D pose annotations can be used for supervised learning. Without paired 2D images and 3D pose annotations, Kundu et al. <cite class="ltx_cite ltx_citemacro_citep">(Kundu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib107" title="" class="ltx_ref">2020a</a>)</cite> proposed a frozen network to exploit the shared latent space between two diverse modalities under a practical deployment paradigm such that the learning could be cast as a cross-model alignment problem. Fabbri et al. <cite class="ltx_cite ltx_citemacro_citep">(Fabbri etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2020</a>)</cite> developed a distance-based heuristic for linking joints in a multi-person setting. Specifically, starting from detected
heads (i.e., the joint with the highest confidence), the remaining joints are connected by selecting the closest ones in terms of 3D Euclidean distance. Chen et al. <cite class="ltx_cite ltx_citemacro_citep">(Cheng
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2021b</a>)</cite> integrated top-down and bottom-up approaches in their method. A top-down network first estimates joint heatmaps inside each bounding box, then a bottom-up network incorporates estimated joint heatmaps to handle the scale variation.</p>
</div>
<div id="S3.SS1.SSS2.p4" class="ltx_para">
<p id="S3.SS1.SSS2.p4.1" class="ltx_p">Another challenge of bottom-up approaches is occlusion. To cope with this challenge, Metha et al. <cite class="ltx_cite ltx_citemacro_citep">(Mehta etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib160" title="" class="ltx_ref">2018</a>)</cite> developed an Occlusion-Robust Pose-Maps (ORPM) approach to incorporate redundancy into the location-maps formulation, which facilitates person association in the heatmaps, especially for occluded scenes. Zhen et al. <cite class="ltx_cite ltx_citemacro_citep">(Zhen etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib301" title="" class="ltx_ref">2020</a>)</cite> leveraged a depth-aware part association algorithm to assign joints to individuals by reasoning about inter-person occlusion and bone-length constraints. Mehta et al. <cite class="ltx_cite ltx_citemacro_citep">(Mehta etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib159" title="" class="ltx_ref">2020</a>)</cite> quickly inferred intermediate 3D pose of visible body joints regardless of the accuracy. Then the completed 3D pose is reconstructed by inferring occluded joints using learned pose priors and global context. The final 3D pose was refined by applying temporal coherence and fitting the kinematic skeletal model.</p>
</div>
<div id="S3.SS1.SSS2.p5" class="ltx_para">
<p id="S3.SS1.SSS2.p5.1" class="ltx_p"><span id="S3.SS1.SSS2.p5.1.1" class="ltx_text ltx_font_bold">Comparison of top-down and bottom-up approaches.</span>
Top-down approaches usually achieve promising results by relying on state-of-the-art person detection methods and single-person HPE methods. But the computational complexity and the inference time may become excessive with the increase in the number of humans, especially in crowded scenes. Moreover, since top-down approaches first detect the bounding box for each person, global information in the scene may get neglected. The estimated depth of cropped region may be inconsistent with the actual depth ordering and the predicted human bodies may be placed in overlapping positions. On the contrary, the bottom-up approaches enjoy linear computation and time complexity. However, if the goal is to recover 3D body mesh, it is not straightforward for the bottom-up approaches to reconstruct human body meshes. For top-down approaches, after detecting each individual person, human body mesh of each person can be easily recovered by incorporating the 3D single-person human mesh recovery method. While for the bottom-up approaches, an additional model regressor module is needed to reconstruct human body meshes based on the final 3D poses.
</p>
</div>
</section>
<section id="S3.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.3. </span>Multi-view 3D HPE</h4>

<div id="S3.SS1.SSS3.p1" class="ltx_para">
<p id="S3.SS1.SSS3.p1.1" class="ltx_p">Partial occlusion is a challenging problem for 3D HPE in the single-view setting. The natural solution to overcome this problem is to estimate 3D human pose from multiple views, since the occluded part in one view may become visible in other views. In order to reconstruct the 3D pose from multiple views, the association of corresponding locations between different cameras needs to be resolved. We do not specify single-person or multi-person in this category since the multi-view setting is deployed mainly for multi-person pose estimation.</p>
</div>
<div id="S3.SS1.SSS3.p2" class="ltx_para">
<p id="S3.SS1.SSS3.p2.1" class="ltx_p">A group of methods <cite class="ltx_cite ltx_citemacro_citep">(Qiu
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib199" title="" class="ltx_ref">2019</a>; Liang and Lin, <a href="#bib.bib129" title="" class="ltx_ref">2019</a>; Dong
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib49" title="" class="ltx_ref">2019</a>; Tu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib236" title="" class="ltx_ref">2020</a>; Dong
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib50" title="" class="ltx_ref">2021</a>)</cite> used body models to tackle the association problem by optimizing model parameters to match the model projection with the 2D pose. The widely used 3D pictorial structure model <cite class="ltx_cite ltx_citemacro_citep">(Burenius etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2013</a>)</cite> is such a model. However, these methods usually need large memory and expensive computational cost, especially for multi-person 3D HPE under multi-view settings. Rhodin et al. <cite class="ltx_cite ltx_citemacro_citep">(Rhodin etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib207" title="" class="ltx_ref">2018b</a>)</cite> employed a multi-view consistency constraint in the network, however it requires a large amount of 3D ground-truth training data. To overcome this limitation, Rhodin et al. <cite class="ltx_cite ltx_citemacro_citep">(Rhodin
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib206" title="" class="ltx_ref">2018a</a>)</cite> further proposed an encoder-decoder framework to learn the geometry-aware 3D latent representation from multi-view images and background segmentation without 3D annotations.
Chen et al. <cite class="ltx_cite ltx_citemacro_citep">(Chen
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2020b</a>)</cite>, Mitra et al. <cite class="ltx_cite ltx_citemacro_citep">(Mitra
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib163" title="" class="ltx_ref">2020</a>)</cite>, Zhang et al.<cite class="ltx_cite ltx_citemacro_citep">(Zhang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib293" title="" class="ltx_ref">2020a</a>)</cite>, and Huang et al. <cite class="ltx_cite ltx_citemacro_citep">(Huang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib70" title="" class="ltx_ref">2020b</a>)</cite>
proposed multi-view matching frameworks to reconstruct 3D human pose across all viewpoints with consistency constraints. Pavlakos et al. <cite class="ltx_cite ltx_citemacro_citep">(Pavlakos etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib186" title="" class="ltx_ref">2017b</a>)</cite> and Zhang et al. <cite class="ltx_cite ltx_citemacro_citep">(Zhang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib296" title="" class="ltx_ref">2020g</a>)</cite> aggregated the 2D keypoint heatmaps of multi-view images into a 3D pictorial structure model based on all the calibrated camera parameters. However, when multi-view camera environments change, the model needs to be retrained.
Qiu et al. <cite class="ltx_cite ltx_citemacro_citep">(Qiu
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib199" title="" class="ltx_ref">2019</a>)</cite>, and Kocabas et al. <cite class="ltx_cite ltx_citemacro_citep">(Kocabas
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib101" title="" class="ltx_ref">2019</a>)</cite> employed epipolar geometry to match paired multi-view poses for 3D pose reconstruction and generalized their methods to new multi-view camera environments. It should be noted that matching each pair of views separately without the cycle consistency constraint may lead to incorrect 3D pose reconstructions <cite class="ltx_cite ltx_citemacro_citep">(Dong
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib49" title="" class="ltx_ref">2019</a>)</cite>. Tu et al. <cite class="ltx_cite ltx_citemacro_citep">(Tu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib236" title="" class="ltx_ref">2020</a>)</cite> aggregated all the features in each camera view in the 3D voxel space to avoid incorrect estimation in each camera view. A cuboid proposal network and a pose regression network were designed to localize all people and to estimate the 3D pose, respectively. When given sufficient viewpoints (more than ten), it is not practical to use all viewpoints for 3D pose estimation. Pirinen et al. <cite class="ltx_cite ltx_citemacro_citep">(Pirinen etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib192" title="" class="ltx_ref">2019</a>)</cite> proposed a self-supervised reinforcement learning approach to select a small set of viewpoints to reconstruct the 3D pose via triangulation. Wang et al. <cite class="ltx_cite ltx_citemacro_citep">(Wang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib252" title="" class="ltx_ref">2021</a>)</cite> introduced a transformer-based model that directly regresses 3D poses from multi-view images without relying on any intermediate task. The proposed Multi-view Pose transformer (MvP) was designed to represent query embedding of multi-person joints. The multi-view information was fused by a novel geometrically guided attention mechanism.</p>
</div>
<div id="S3.SS1.SSS3.p3" class="ltx_para">
<p id="S3.SS1.SSS3.p3.1" class="ltx_p">Besides accuracy, <span id="S3.SS1.SSS3.p3.1.1" class="ltx_text ltx_font_bold">lightweight architecture, fast inference time, and efficient adaptation</span> to new camera settings also need to be taken into consideration in multi-view HPE. In contrast to <cite class="ltx_cite ltx_citemacro_citep">(Dong
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib49" title="" class="ltx_ref">2019</a>)</cite> which matched all view inputs together, Chen et al. <cite class="ltx_cite ltx_citemacro_citep">(Chen
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2020a</a>)</cite> applied an iterative processing strategy to match 2D poses of each view with the 3D pose while the 3D pose was updated iteratively. Compared to previous methods whose running time may explode with the increase in the number of cameras, their method has linear time complexity. Remelli et al. <cite class="ltx_cite ltx_citemacro_citep">(Remelli
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib204" title="" class="ltx_ref">2020</a>)</cite> encoded images of each view into a unified latent representation so that the feature maps were disentangled from camera viewpoints. As a lightweight canonical fusion, these 2D representations are lifted to the 3D pose using a GPU-based Direct Linear Transform to accelerate processing. In order to improve the generalization ability of the multi-view fusion scheme, Xie et al. <cite class="ltx_cite ltx_citemacro_citep">(Xie
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib261" title="" class="ltx_ref">2020</a>)</cite> proposed a pre-trained multi-view fusion model (MetaFuse), which can be efficiently adapted to new camera settings with few labeled data. They deployed the model-agnostic meta-learning framework to learn the optimal initialization of the generic fusion model for adaptation. To reduce the computational cost of the VoxelPose <cite class="ltx_cite ltx_citemacro_citep">(Tu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib236" title="" class="ltx_ref">2020</a>)</cite>, Ye et al. presented Faster VoxelPose <cite class="ltx_cite ltx_citemacro_citep">(Ye
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib274" title="" class="ltx_ref">2022</a>)</cite> which re-projects the feature volume to three two-dimensional coordinate planes for estimating X, Y, Z coordinates from them separately. The fps of Faster VoxelPose is 31.1, which is almost 10 <math id="S3.SS1.SSS3.p3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS1.SSS3.p3.1.m1.1a"><mo id="S3.SS1.SSS3.p3.1.m1.1.1" xref="S3.SS1.SSS3.p3.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p3.1.m1.1b"><times id="S3.SS1.SSS3.p3.1.m1.1.1.cmml" xref="S3.SS1.SSS3.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p3.1.m1.1c">\times</annotation></semantics></math> speed up compared to VoxelPose.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>3D HPE from other sources</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Although a monocular RGB camera is the most common device used for 3D HPE, other sensors (e.g., depth sensor, IMUs, and radio frequency device) are also used for this purpose.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold">Depth and point cloud sensors</span>: Depth sensors have gained more attention recently for 3D computer vision tasks due to their low-cost and increased utilization. As one of the key challenges in 3D HPE, the depth ambiguity problem can be alleviated by using depth sensors. Yu et al. <cite class="ltx_cite ltx_citemacro_citep">(Yu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib276" title="" class="ltx_ref">2019</a>)</cite>, Xiong et al. <cite class="ltx_cite ltx_citemacro_citep">(Xiong etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib262" title="" class="ltx_ref">2019</a>)</cite>, Kadkhodamohammadi et al. <cite class="ltx_cite ltx_citemacro_citep">(Kadkhodamohammadi etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib95" title="" class="ltx_ref">2017</a>)</cite>, and Zhi et al. <cite class="ltx_cite ltx_citemacro_citep">(Zhi etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib306" title="" class="ltx_ref">2020</a>)</cite> utilized depth images to estimate 3D human pose.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">Compared with depth images, point clouds can provide more information. The state-of-the-art point cloud feature extraction techniques, PointNet <cite class="ltx_cite ltx_citemacro_citep">(Qi
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib197" title="" class="ltx_ref">2017a</a>)</cite> and PointNet++ <cite class="ltx_cite ltx_citemacro_citep">(Qi
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib198" title="" class="ltx_ref">2017b</a>)</cite>, have demonstrated excellent performance for classification and segmentation tasks. Jiang et al. <cite class="ltx_cite ltx_citemacro_citep">(Jiang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib87" title="" class="ltx_ref">2019</a>)</cite> and Wang et al. <cite class="ltx_cite ltx_citemacro_citep">(Wang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib250" title="" class="ltx_ref">2020c</a>)</cite> combined PointNet++ with the 3D human body model to recover 3D human mesh.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p"><span id="S3.SS2.p4.1.1" class="ltx_text ltx_font_bold">IMUs with monocular images</span>: Wearable Inertial Measurement Units (IMUs) can track the orientation and acceleration of human body parts by recording motions without object occlusions and clothes obstructions. Marcard et al.<cite class="ltx_cite ltx_citemacro_citep">(VonÂ Marcard etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib242" title="" class="ltx_ref">2017</a>; von Marcard etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib241" title="" class="ltx_ref">2018</a>)</cite>, Huang et al. <cite class="ltx_cite ltx_citemacro_citep">(Huang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib75" title="" class="ltx_ref">2018</a>)</cite>, Zhang et al. <cite class="ltx_cite ltx_citemacro_citep">(Zhang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib295" title="" class="ltx_ref">2020f</a>)</cite>, and Huang et al. <cite class="ltx_cite ltx_citemacro_citep">(Huang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib71" title="" class="ltx_ref">2020c</a>)</cite> proposed IMU-based HPE methods to reconstruct 3D human pose. However, drifting may occur over time when using IMUs.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p"><span id="S3.SS2.p5.1.1" class="ltx_text ltx_font_bold">Radio frequency device</span>: Radio Frequency (RF) based sensing technology has also been used for 3D HPE, e.g., <cite class="ltx_cite ltx_citemacro_citep">(Zhao etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib299" title="" class="ltx_ref">2018</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citep">(Zhao etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib298" title="" class="ltx_ref">2019a</a>)</cite>. The ability to traverse walls and bounce off human bodies in the WiFi range without carrying wireless transmitters is the major advantage of deploying an RF-based sensing system. Also, privacy can be preserved due to non-visual data. However, RF signals have a relatively low spatial resolution compared to visual camera images, and RF systems have been shown to generate coarse 3D pose estimation.</p>
</div>
<div id="S3.SS2.p6" class="ltx_para">
<p id="S3.SS2.p6.1" class="ltx_p"><span id="S3.SS2.p6.1.1" class="ltx_text ltx_font_bold">Other sensors/sources:</span> Besides using the aforementioned sensors, Isogawa et al. <cite class="ltx_cite ltx_citemacro_citep">(Isogawa
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib80" title="" class="ltx_ref">2020</a>)</cite> estimated 3D human pose from the 3D spatio-temporal histogram of photons captured by a non-line-of-sight (NLOS) imaging system. Some methods <cite class="ltx_cite ltx_citemacro_citep">(Tome
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib231" title="" class="ltx_ref">2019</a>; Tome etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib230" title="" class="ltx_ref">2020</a>; Xu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib266" title="" class="ltx_ref">2019</a>)</cite> tackled the egocentric 3D pose estimation via a fish-eye camera. Saini et al. <cite class="ltx_cite ltx_citemacro_citep">(Saini etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib211" title="" class="ltx_ref">2019</a>)</cite> estimated human motion using images captured by multiple Autonomous Micro Aerial Vehicles (MAVs). Clever et al. <cite class="ltx_cite ltx_citemacro_citep">(Clever etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2020</a>)</cite> focused on the HPE of the rest position in bed from pressure images which were collected by a pressure sensing mat.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>3D HPE Summary</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">3D HPE has made significant advancements in recent years.
Since a large number of 3D HPE methods apply the 2D to 3D lifting strategy,
the performance of 3D HPE has been improved considerably due to the progress made in 2D HPE. Some 2D HPE methods such as OpenPose <cite class="ltx_cite ltx_citemacro_citep">(Cao
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2017</a>)</cite>, AlphaPose <cite class="ltx_cite ltx_citemacro_citep">(Fang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib56" title="" class="ltx_ref">2017</a>)</cite>, and HRNet <cite class="ltx_cite ltx_citemacro_citep">(Sun
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib222" title="" class="ltx_ref">2019</a>)</cite> have been extensively used as 2D pose detectors in 3D HPE methods. Besides the 3D pose, some methods also recover 3D human mesh from images or videos, e.g., <cite class="ltx_cite ltx_citemacro_citep">(Kolotouros etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib102" title="" class="ltx_ref">2019</a>; Kocabas
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib99" title="" class="ltx_ref">2020</a>; Zeng etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib283" title="" class="ltx_ref">2020</a>; Zhou
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib307" title="" class="ltx_ref">2020</a>)</cite>. However, despite the progress made so far, there are still several challenges.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">One challenge is model generalization. High-quality 3D ground truth pose annotations
depend on motion capture systems that cannot be easily deployed in a random environment.
Therefore, the existing datasets are mainly captured in constrained scenes.
The state-of-the-art methods can achieve promising results on these datasets, but their performance degrades when applied to in-the-wild data.
It is possible to leverage gaming engines to generate synthetic datasets with diverse poses and complex scenes, e.g., SURREAL dataset <cite class="ltx_cite ltx_citemacro_citep">(Varol etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib239" title="" class="ltx_ref">2017</a>)</cite> and GTA-IM dataset <cite class="ltx_cite ltx_citemacro_citep">(Cao
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2020</a>)</cite>. However, learning from synthetic data may not achieve the
desired performance due to a gap between synthetic and real data distributions.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">Like 2D HPE, robustness to occlusion and computational efficiency are two key challenges for 3D HPE as well.
The performance of current 3D HPE methods drops considerably in crowded scenarios due to severe mutual occlusion and possibly low-resolution content of each person. 3D HPE is more computation-demanding than 2D HPE. For example, 2D to 3D lifting approaches rely on 2D poses as intermediate representations for inferring 3D poses. It is critical to develop computationally efficient 2D HPE pipelines while maintaining high accuracy for pose estimation.
</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Datasets and Evaluation Metrics</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Datasets are very much needed in conducting HPE. They are also necessary to provide a fair comparison among different algorithms.
In this section, we present the most widely used datasets and evaluation metrics for evaluating 2D and 3D deep learning-based HPE methods.
The results achieved by existing methods on these popular datasets are summarized.
</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Datasets for 2D HPE</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Although there are several datasets used for 2D HPE tasks before 2014, only a few recent works use these datasets because they have several shortcomings such as a lack of diverse object movements and limited data. Since deep learning-based approaches are fueled by a large amount of training data, this section mainly discusses the recent and large-scale 2D human pose datasets.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p"><a target="_blank" href="http://human-pose.mpi-inf.mpg.de/#" title="" class="ltx_ref ltx_href ltx_font_bold">Max Planck Institute for Informatics (MPII) Human Pose Dataset</a> <cite class="ltx_cite ltx_citemacro_citep">(Andriluka etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2014</a>)</cite> is a popular dataset for evaluation of articulated HPE
which includes around 25,000 images containing over 40,000 individuals with annotated body joints.
Rich annotations including body part occlusions, 3D torso, and head orientations are labeled by workers on Amazon Mechanical Turk. Images in MPII are suitable for 2D single-person or multi-person HPE.
</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p"><a target="_blank" href="https://cocodataset.org/#home" title="" class="ltx_ref ltx_href ltx_font_bold">Microsoft Common Objects in Context (COCO) Dataset</a> <cite class="ltx_cite ltx_citemacro_citep">(Lin etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib133" title="" class="ltx_ref">2014</a>)</cite> is the most widely used large-scale dataset. It has more than 330,000 images and 200,000 labeled subjects with keypoints, and each individual person is labeled with 17 joints.
In addition, Jin et al. <cite class="ltx_cite ltx_citemacro_citep">(Jin etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib90" title="" class="ltx_ref">2020b</a>)</cite> proposed COCO-WholeBody Dataset with whole-body annotations for HPE.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p"><a target="_blank" href="https://posetrack.net/" title="" class="ltx_ref ltx_href ltx_font_bold">PoseTrack Dataset</a> <cite class="ltx_cite ltx_citemacro_citep">(Andriluka etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2018a</a>)</cite> is a large-scale dataset for HPE and articulated tracking in videos, including body part occlusion and truncation in crowded environments. There are two versions for PoseTrack dataset: PoseTrack2017 <cite class="ltx_cite ltx_citemacro_citep">(Andriluka etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2018a</a>)</cite> contains 514 video sequences with 16,219 pose annotations, which are split into 250 (training), 50 (validation), and 214 (testing) sequences. PoseTrack2018 <cite class="ltx_cite ltx_citemacro_citep">(Andriluka etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2018b</a>)</cite> has 1,138 video sequences with 153,615 pose annotations, which are divided into 593 for training, 170 for validation, and 375 for testing. Each person in PoseTrack is labeled with 15 joints and an additional label for keypoint visibility.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p">We refer the readers to the original references for details about other datasets including <span id="S4.SS1.p5.1.1" class="ltx_text ltx_font_bold">LSP</span> <cite class="ltx_cite ltx_citemacro_citep">(Johnson and
Everingham, <a href="#bib.bib91" title="" class="ltx_ref">2010</a>)</cite>, <span id="S4.SS1.p5.1.2" class="ltx_text ltx_font_bold">FLIC</span> <cite class="ltx_cite ltx_citemacro_citep">(Sapp and Taskar, <a href="#bib.bib214" title="" class="ltx_ref">2013</a>)</cite>, <span id="S4.SS1.p5.1.3" class="ltx_text ltx_font_bold">AIC-HKD</span> <cite class="ltx_cite ltx_citemacro_citep">(Wu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib258" title="" class="ltx_ref">2017</a>)</cite>, <span id="S4.SS1.p5.1.4" class="ltx_text ltx_font_bold">CrowdPose</span> <cite class="ltx_cite ltx_citemacro_citep">(Li
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib115" title="" class="ltx_ref">2019b</a>)</cite>, <span id="S4.SS1.p5.1.5" class="ltx_text ltx_font_bold">Penn Action</span> <cite class="ltx_cite ltx_citemacro_citep">(Zhang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib292" title="" class="ltx_ref">2013</a>)</cite>, <span id="S4.SS1.p5.1.6" class="ltx_text ltx_font_bold">J-HMDB</span> <cite class="ltx_cite ltx_citemacro_citep">(Jhuang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib84" title="" class="ltx_ref">2013</a>)</cite>, and <span id="S4.SS1.p5.1.7" class="ltx_text ltx_font_bold">HiEve</span> <cite class="ltx_cite ltx_citemacro_citep">(Lin etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib134" title="" class="ltx_ref">2020</a>)</cite>. A summary of these datasets is presented in Table <a href="#S4.T1" title="Table 1 â€£ 4.1. Datasets for 2D HPE â€£ 4. Datasets and Evaluation Metrics â€£ Deep Learning-Based Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span><span id="S4.T1.2.1" class="ltx_text" style="font-size:80%;">Datasets for 2D HPE. </span></figcaption>
<div id="S4.T1.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:179.2pt;vertical-align:-0.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-263.6pt,108.6pt) scale(0.451321334919956,0.451321334919956) ;">
<table id="S4.T1.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.3.1.1.1" class="ltx_tr">
<th id="S4.T1.3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;" colspan="8">Â Â Â Â Â Â Â Â <span id="S4.T1.3.1.1.1.1.1" class="ltx_text ltx_font_bold">Image-based datasets</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.3.1.2.1" class="ltx_tr">
<th id="S4.T1.3.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;" rowspan="2">Â Â Â Â Â Â Â Â <span id="S4.T1.3.1.2.1.1.1" class="ltx_text">Name</span></th>
<th id="S4.T1.3.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;" rowspan="2">Â Â Â Â Â Â Â Â <span id="S4.T1.3.1.2.1.2.1" class="ltx_text">Year</span></th>
<th id="S4.T1.3.1.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;" rowspan="2">Â Â Â Â Â Â Â Â <span id="S4.T1.3.1.2.1.3.1" class="ltx_text">
<span id="S4.T1.3.1.2.1.3.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.3.1.2.1.3.1.1.1" class="ltx_tr">
<span id="S4.T1.3.1.2.1.3.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â Single-Person</span></span>
<span id="S4.T1.3.1.2.1.3.1.1.2" class="ltx_tr">
<span id="S4.T1.3.1.2.1.3.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â /Multi-Person</span></span>
</span></span></th>
<th id="S4.T1.3.1.2.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;" rowspan="2">Â Â Â Â Â Â Â Â <span id="S4.T1.3.1.2.1.4.1" class="ltx_text">Joints</span></th>
<td id="S4.T1.3.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;" colspan="3">Â Â Â Â Â Â Â Â Number of images</td>
<td id="S4.T1.3.1.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;" rowspan="2">Â Â Â Â Â Â Â Â <span id="S4.T1.3.1.2.1.6.1" class="ltx_text">
<span id="S4.T1.3.1.2.1.6.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.3.1.2.1.6.1.1.1" class="ltx_tr">
<span id="S4.T1.3.1.2.1.6.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â Evaluation</span></span>
<span id="S4.T1.3.1.2.1.6.1.1.2" class="ltx_tr">
<span id="S4.T1.3.1.2.1.6.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â protocol</span></span>
</span></span></td>
</tr>
<tr id="S4.T1.3.1.3.2" class="ltx_tr">
<td id="S4.T1.3.1.3.2.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â Train</td>
<td id="S4.T1.3.1.3.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â Val</td>
<td id="S4.T1.3.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â Test</td>
</tr>
<tr id="S4.T1.3.1.4.3" class="ltx_tr">
<th id="S4.T1.3.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â LSP <cite class="ltx_cite ltx_citemacro_citep">(Johnson and
Everingham, <a href="#bib.bib91" title="" class="ltx_ref">2010</a>)</cite></th>
<th id="S4.T1.3.1.4.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 2010</th>
<th id="S4.T1.3.1.4.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â Single</th>
<th id="S4.T1.3.1.4.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 14</th>
<td id="S4.T1.3.1.4.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 1k</td>
<td id="S4.T1.3.1.4.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T1.3.1.4.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 1k</td>
<td id="S4.T1.3.1.4.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â PCP</td>
</tr>
<tr id="S4.T1.3.1.5.4" class="ltx_tr">
<th id="S4.T1.3.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â LSP-extended <cite class="ltx_cite ltx_citemacro_citep">(Johnson and
Everingham, <a href="#bib.bib92" title="" class="ltx_ref">2011</a>)</cite></th>
<th id="S4.T1.3.1.5.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 2011</th>
<th id="S4.T1.3.1.5.4.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â Single</th>
<th id="S4.T1.3.1.5.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 14</th>
<td id="S4.T1.3.1.5.4.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 10k</td>
<td id="S4.T1.3.1.5.4.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T1.3.1.5.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T1.3.1.5.4.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â PCP</td>
</tr>
<tr id="S4.T1.3.1.6.5" class="ltx_tr">
<th id="S4.T1.3.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â FLIC <cite class="ltx_cite ltx_citemacro_citep">(Sapp and Taskar, <a href="#bib.bib214" title="" class="ltx_ref">2013</a>)</cite></th>
<th id="S4.T1.3.1.6.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 2013</th>
<th id="S4.T1.3.1.6.5.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â Single</th>
<th id="S4.T1.3.1.6.5.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 10</th>
<td id="S4.T1.3.1.6.5.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 5k</td>
<td id="S4.T1.3.1.6.5.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T1.3.1.6.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 1k</td>
<td id="S4.T1.3.1.6.5.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â PCP&amp;PCK</td>
</tr>
<tr id="S4.T1.3.1.7.6" class="ltx_tr">
<th id="S4.T1.3.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â FLIC-full <cite class="ltx_cite ltx_citemacro_citep">(Sapp and Taskar, <a href="#bib.bib214" title="" class="ltx_ref">2013</a>)</cite></th>
<th id="S4.T1.3.1.7.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 2013</th>
<th id="S4.T1.3.1.7.6.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â Single</th>
<th id="S4.T1.3.1.7.6.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 10</th>
<td id="S4.T1.3.1.7.6.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 20k</td>
<td id="S4.T1.3.1.7.6.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T1.3.1.7.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T1.3.1.7.6.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â PCP&amp;PCK</td>
</tr>
<tr id="S4.T1.3.1.8.7" class="ltx_tr">
<th id="S4.T1.3.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â FLIC-plus <cite class="ltx_cite ltx_citemacro_citep">(Tompson
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib233" title="" class="ltx_ref">2014</a>)</cite></th>
<th id="S4.T1.3.1.8.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 2014</th>
<th id="S4.T1.3.1.8.7.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â Single</th>
<th id="S4.T1.3.1.8.7.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 10</th>
<td id="S4.T1.3.1.8.7.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 17k</td>
<td id="S4.T1.3.1.8.7.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T1.3.1.8.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T1.3.1.8.7.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â PCP&amp;PCK</td>
</tr>
<tr id="S4.T1.3.1.9.8" class="ltx_tr">
<th id="S4.T1.3.1.9.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;" rowspan="2">Â Â Â Â Â Â Â Â <span id="S4.T1.3.1.9.8.1.1" class="ltx_text">MPII <cite class="ltx_cite ltx_citemacro_citep">(Andriluka etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2014</a>)</cite></span></th>
<th id="S4.T1.3.1.9.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;" rowspan="2">Â Â Â Â Â Â Â Â <span id="S4.T1.3.1.9.8.2.1" class="ltx_text">2014</span></th>
<th id="S4.T1.3.1.9.8.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â Single</th>
<th id="S4.T1.3.1.9.8.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 16</th>
<td id="S4.T1.3.1.9.8.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 29k</td>
<td id="S4.T1.3.1.9.8.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T1.3.1.9.8.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 12k</td>
<td id="S4.T1.3.1.9.8.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â PCPm/PCKh</td>
</tr>
<tr id="S4.T1.3.1.10.9" class="ltx_tr">
<th id="S4.T1.3.1.10.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â Multiple</th>
<th id="S4.T1.3.1.10.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 16</th>
<td id="S4.T1.3.1.10.9.3" class="ltx_td ltx_align_center" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 3.8k</td>
<td id="S4.T1.3.1.10.9.4" class="ltx_td ltx_align_center" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T1.3.1.10.9.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 1.7k</td>
<td id="S4.T1.3.1.10.9.6" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â mAp</td>
</tr>
<tr id="S4.T1.3.1.11.10" class="ltx_tr">
<th id="S4.T1.3.1.11.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â COCO2016 <cite class="ltx_cite ltx_citemacro_citep">(Lin etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib133" title="" class="ltx_ref">2014</a>)</cite></th>
<th id="S4.T1.3.1.11.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 2016</th>
<th id="S4.T1.3.1.11.10.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â Multiple</th>
<th id="S4.T1.3.1.11.10.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 17</th>
<td id="S4.T1.3.1.11.10.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 45k</td>
<td id="S4.T1.3.1.11.10.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 22k</td>
<td id="S4.T1.3.1.11.10.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 80k</td>
<td id="S4.T1.3.1.11.10.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â AP</td>
</tr>
<tr id="S4.T1.3.1.12.11" class="ltx_tr">
<th id="S4.T1.3.1.12.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â COCO2017 <cite class="ltx_cite ltx_citemacro_citep">(Lin etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib133" title="" class="ltx_ref">2014</a>)</cite></th>
<th id="S4.T1.3.1.12.11.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 2017</th>
<th id="S4.T1.3.1.12.11.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â Multiple</th>
<th id="S4.T1.3.1.12.11.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 17</th>
<td id="S4.T1.3.1.12.11.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 64k</td>
<td id="S4.T1.3.1.12.11.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 2.7k</td>
<td id="S4.T1.3.1.12.11.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 40k</td>
<td id="S4.T1.3.1.12.11.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â AP</td>
</tr>
<tr id="S4.T1.3.1.13.12" class="ltx_tr">
<th id="S4.T1.3.1.13.12.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â AIC-HKD <cite class="ltx_cite ltx_citemacro_citep">(Wu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib258" title="" class="ltx_ref">2017</a>)</cite></th>
<th id="S4.T1.3.1.13.12.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 2017</th>
<th id="S4.T1.3.1.13.12.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â Multiple</th>
<th id="S4.T1.3.1.13.12.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 14</th>
<td id="S4.T1.3.1.13.12.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 210k</td>
<td id="S4.T1.3.1.13.12.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 30k</td>
<td id="S4.T1.3.1.13.12.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 60k</td>
<td id="S4.T1.3.1.13.12.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â AP</td>
</tr>
<tr id="S4.T1.3.1.14.13" class="ltx_tr">
<th id="S4.T1.3.1.14.13.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â CrowdPose <cite class="ltx_cite ltx_citemacro_citep">(Li
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib115" title="" class="ltx_ref">2019b</a>)</cite></th>
<th id="S4.T1.3.1.14.13.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 2019</th>
<th id="S4.T1.3.1.14.13.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â Multiple</th>
<th id="S4.T1.3.1.14.13.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 14</th>
<td id="S4.T1.3.1.14.13.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 10k</td>
<td id="S4.T1.3.1.14.13.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 2k</td>
<td id="S4.T1.3.1.14.13.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 8k</td>
<td id="S4.T1.3.1.14.13.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â mAP</td>
</tr>
<tr id="S4.T1.3.1.15.14" class="ltx_tr">
<th id="S4.T1.3.1.15.14.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;" colspan="8">Â Â Â Â Â Â Â Â <span id="S4.T1.3.1.15.14.1.1" class="ltx_text ltx_font_bold">Video-based datasets</span></th>
</tr>
<tr id="S4.T1.3.1.16.15" class="ltx_tr">
<th id="S4.T1.3.1.16.15.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;" rowspan="2">Â Â Â Â Â Â Â Â <span id="S4.T1.3.1.16.15.1.1" class="ltx_text">Name</span></th>
<th id="S4.T1.3.1.16.15.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;" rowspan="2">Â Â Â Â Â Â Â Â <span id="S4.T1.3.1.16.15.2.1" class="ltx_text">Year</span></th>
<th id="S4.T1.3.1.16.15.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;" rowspan="2">Â Â Â Â Â Â Â Â <span id="S4.T1.3.1.16.15.3.1" class="ltx_text">
<span id="S4.T1.3.1.16.15.3.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.3.1.16.15.3.1.1.1" class="ltx_tr">
<span id="S4.T1.3.1.16.15.3.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â Single-Person</span></span>
<span id="S4.T1.3.1.16.15.3.1.1.2" class="ltx_tr">
<span id="S4.T1.3.1.16.15.3.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â /Multi-Person</span></span>
</span></span></th>
<th id="S4.T1.3.1.16.15.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;" rowspan="2">Â Â Â Â Â Â Â Â <span id="S4.T1.3.1.16.15.4.1" class="ltx_text">Joints</span></th>
<td id="S4.T1.3.1.16.15.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;" colspan="3">Â Â Â Â Â Â Â Â Number of videos</td>
<td id="S4.T1.3.1.16.15.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;" rowspan="2">Â Â Â Â Â Â Â Â <span id="S4.T1.3.1.16.15.6.1" class="ltx_text">
<span id="S4.T1.3.1.16.15.6.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.3.1.16.15.6.1.1.1" class="ltx_tr">
<span id="S4.T1.3.1.16.15.6.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â Evaluation</span></span>
<span id="S4.T1.3.1.16.15.6.1.1.2" class="ltx_tr">
<span id="S4.T1.3.1.16.15.6.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â protocol</span></span>
</span></span></td>
</tr>
<tr id="S4.T1.3.1.17.16" class="ltx_tr">
<td id="S4.T1.3.1.17.16.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â Train</td>
<td id="S4.T1.3.1.17.16.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â Val</td>
<td id="S4.T1.3.1.17.16.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â Test</td>
</tr>
<tr id="S4.T1.3.1.18.17" class="ltx_tr">
<th id="S4.T1.3.1.18.17.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â Penn Action <cite class="ltx_cite ltx_citemacro_citep">(Zhang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib292" title="" class="ltx_ref">2013</a>)</cite></th>
<th id="S4.T1.3.1.18.17.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 2013</th>
<th id="S4.T1.3.1.18.17.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â Single</th>
<th id="S4.T1.3.1.18.17.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 13</th>
<td id="S4.T1.3.1.18.17.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 1k</td>
<td id="S4.T1.3.1.18.17.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T1.3.1.18.17.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 1k</td>
<td id="S4.T1.3.1.18.17.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
</tr>
<tr id="S4.T1.3.1.19.18" class="ltx_tr">
<th id="S4.T1.3.1.19.18.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â J-HMDB <cite class="ltx_cite ltx_citemacro_citep">(Jhuang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib84" title="" class="ltx_ref">2013</a>)</cite></th>
<th id="S4.T1.3.1.19.18.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 2013</th>
<th id="S4.T1.3.1.19.18.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â Single</th>
<th id="S4.T1.3.1.19.18.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 15</th>
<td id="S4.T1.3.1.19.18.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 0.6k</td>
<td id="S4.T1.3.1.19.18.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T1.3.1.19.18.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 0.3k</td>
<td id="S4.T1.3.1.19.18.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
</tr>
<tr id="S4.T1.3.1.20.19" class="ltx_tr">
<th id="S4.T1.3.1.20.19.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â PoseTrack2017 <cite class="ltx_cite ltx_citemacro_citep">(Andriluka etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2018a</a>)</cite></th>
<th id="S4.T1.3.1.20.19.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 2017</th>
<th id="S4.T1.3.1.20.19.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â Multiple</th>
<th id="S4.T1.3.1.20.19.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 15</th>
<td id="S4.T1.3.1.20.19.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 250</td>
<td id="S4.T1.3.1.20.19.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 50</td>
<td id="S4.T1.3.1.20.19.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 214</td>
<td id="S4.T1.3.1.20.19.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â mAP</td>
</tr>
<tr id="S4.T1.3.1.21.20" class="ltx_tr">
<th id="S4.T1.3.1.21.20.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â PoseTrack2018 <cite class="ltx_cite ltx_citemacro_citep">(Andriluka etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2018b</a>)</cite></th>
<th id="S4.T1.3.1.21.20.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 2018</th>
<th id="S4.T1.3.1.21.20.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â Multiple</th>
<th id="S4.T1.3.1.21.20.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 15</th>
<td id="S4.T1.3.1.21.20.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 593</td>
<td id="S4.T1.3.1.21.20.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 170</td>
<td id="S4.T1.3.1.21.20.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 375</td>
<td id="S4.T1.3.1.21.20.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â mAP</td>
</tr>
<tr id="S4.T1.3.1.22.21" class="ltx_tr">
<th id="S4.T1.3.1.22.21.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â HiEve <cite class="ltx_cite ltx_citemacro_citep">(Lin etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib134" title="" class="ltx_ref">2020</a>)</cite></th>
<th id="S4.T1.3.1.22.21.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 2020</th>
<th id="S4.T1.3.1.22.21.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â Multiple</th>
<th id="S4.T1.3.1.22.21.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 14</th>
<td id="S4.T1.3.1.22.21.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 19</td>
<td id="S4.T1.3.1.22.21.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T1.3.1.22.21.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 13</td>
<td id="S4.T1.3.1.22.21.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â mAP</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Evaluation Metrics for 2D HPE</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">It is difficult to precisely evaluate the performance of HPE because there are many features and requirements that need to be considered (e.g., upper/full human body, single/multiple pose estimation, the size of human body). As a result, many evaluation metrics have been used for 2D HPE. Here we summarize the commonly used ones.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p"><span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_bold">Percentage of Correct Parts (PCP)</span> <cite class="ltx_cite ltx_citemacro_citep">(Eichner etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">2012</a>)</cite> is a measure commonly used in early works on 2D HPE, which evaluates stick predictions to report the localization accuracy for limbs. The localization of limbs is determined when the distance between the predicted joint and ground truth joint is less than a fraction of the limb length (between 0.1 to 0.5). In some works, the PCP measure is also referred to as PCP@0.5, where the threshold is 0.5. This measure is used for single-person HPE evaluation. However, PCP has not been widely implemented in the latest works because it penalizes the limbs with short lengths which are hard to detect. The performance of a model is considered better when it has a higher PCP measure. In order to address the drawbacks of PCP, Percentage of Detected Joints (PDJ) is introduced, where a predicted joint is considered as detected if the distance between predicted joints and true joints is within a certain fraction of the torso diameter<cite class="ltx_cite ltx_citemacro_citep">(Toshev and
Szegedy, <a href="#bib.bib234" title="" class="ltx_ref">2014</a>)</cite>.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p"><span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_bold">Percentage of Correct Keypoints (PCK)</span> <cite class="ltx_cite ltx_citemacro_citep">(Yang and Ramanan, <a href="#bib.bib273" title="" class="ltx_ref">2012</a>)</cite> is also used to measure the accuracy of localization of different keypoints within a given threshold. The threshold is set to 50 percent of the head segment length of each test image and it is denoted as PCKh@0.5. PCK is referred to as PCK@0.2 when the distance between detected joints and true joints is less than 0.2 times the torso diameter. The higher the PCK value, the better model performance is regarded.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p"><span id="S4.SS2.p4.1.1" class="ltx_text ltx_font_bold">Average Precision (AP) and Average Recall (AR)</span>. AP measure is an index to measure the accuracy of keypoints detection according to precision (the ratio of true positive results to the total positive results) and recall (the ratio of true positive results to the total number of ground truth positives). AP computes the average precision value for recall over 0 to 1. AP has several similar variants. For example, Average Precision of Keypoints (APK) is introduced in <cite class="ltx_cite ltx_citemacro_citep">(Yang and Ramanan, <a href="#bib.bib273" title="" class="ltx_ref">2012</a>)</cite>. Mean Average Precision (mAP), which is the mean of average precision over all classes, is a widely used metric on the MPII and PoseTrack datasets. Average Recall (AR) is another metric used in the COCO keypoint evaluation <cite class="ltx_cite ltx_citemacro_citep">(Lin etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib133" title="" class="ltx_ref">2014</a>)</cite>. Object Keypoint Similarity (OKS) plays a similar role as the Intersection over Union (IoU) in object detection and is used for AP or AR. This measure is computed from the scale of the subject and the distance between predicted points and ground truth points. The COCO evaluation usually uses mAP across 10 OKS thresholds as the evaluation metric.</p>
</div>
<figure id="S4.T2" class="ltx_table">

<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2. </span><span id="S4.T2.2.1" class="ltx_text" style="font-size:80%;">Comparison of different methods on the MPII dataset for 2D single-person HPE using PCKh@0.5 measure (i.e., the threshold is equal to 50 percent of the head segment length of each test image). <span id="S4.T2.2.1.1" class="ltx_text ltx_font_bold">H</span>: Heatmap; <span id="S4.T2.2.1.2" class="ltx_text ltx_font_bold">R</span>: Regression. (<span id="S4.T2.2.1.3" class="ltx_text" style="color:#FF0000;">Red</span>: best; <span id="S4.T2.2.1.4" class="ltx_text" style="color:#0000FF;">Blue</span>: second best)
</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S4.T2.3" class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:130.5pt;vertical-align:-0.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-383.1pt,114.9pt) scale(0.361404015912464,0.361404015912464) ;">
<table id="S4.T2.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.3.1.1.1" class="ltx_tr">
<th id="S4.T2.3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;" colspan="13">Â Â Â Â Â Â Â Â Max Planck Institute for Informatics (MPII)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.3.1.2.1" class="ltx_tr">
<th id="S4.T2.3.1.2.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;"></th>
<th id="S4.T2.3.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T2.3.1.2.1.2.1" class="ltx_text">Year</span></th>
<th id="S4.T2.3.1.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T2.3.1.2.1.3.1" class="ltx_text">Method</span></th>
<td id="S4.T2.3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T2.3.1.2.1.4.1" class="ltx_text">Head</span></td>
<td id="S4.T2.3.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T2.3.1.2.1.5.1" class="ltx_text">Shoulder</span></td>
<td id="S4.T2.3.1.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T2.3.1.2.1.6.1" class="ltx_text">Elbow</span></td>
<td id="S4.T2.3.1.2.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T2.3.1.2.1.7.1" class="ltx_text">Wrist</span></td>
<td id="S4.T2.3.1.2.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T2.3.1.2.1.8.1" class="ltx_text">Hip</span></td>
<td id="S4.T2.3.1.2.1.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T2.3.1.2.1.9.1" class="ltx_text">Knee</span></td>
<td id="S4.T2.3.1.2.1.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T2.3.1.2.1.10.1" class="ltx_text">Ankle</span></td>
<td id="S4.T2.3.1.2.1.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T2.3.1.2.1.11.1" class="ltx_text">Mean</span></td>
<td id="S4.T2.3.1.2.1.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T2.3.1.2.1.12.1" class="ltx_text">Params(M)</span></td>
<td id="S4.T2.3.1.2.1.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T2.3.1.2.1.13.1" class="ltx_text">FLOPs(G)</span></td>
</tr>
<tr id="S4.T2.3.1.3.2" class="ltx_tr">
<th id="S4.T2.3.1.3.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;"></th>
<th id="S4.T2.3.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 2017</th>
<th id="S4.T2.3.1.3.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Chu
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib42" title="" class="ltx_ref">2017</a>)</cite></th>
<td id="S4.T2.3.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T2.3.1.3.2.4.1" class="ltx_text" style="color:#0000FF;">98.5</span>
</td>
<td id="S4.T2.3.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 96.3</td>
<td id="S4.T2.3.1.3.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 91.9</td>
<td id="S4.T2.3.1.3.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 88.1</td>
<td id="S4.T2.3.1.3.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 90.6</td>
<td id="S4.T2.3.1.3.2.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 88.0</td>
<td id="S4.T2.3.1.3.2.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 85.0</td>
<td id="S4.T2.3.1.3.2.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 91.5</td>
<td id="S4.T2.3.1.3.2.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T2.3.1.3.2.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
</tr>
<tr id="S4.T2.3.1.4.3" class="ltx_tr">
<th id="S4.T2.3.1.4.3.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding:-0.5pt 25.0pt;"></th>
<th id="S4.T2.3.1.4.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 2017</th>
<th id="S4.T2.3.1.4.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Yang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib270" title="" class="ltx_ref">2017</a>)</cite></th>
<td id="S4.T2.3.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T2.3.1.4.3.4.1" class="ltx_text" style="color:#0000FF;">98.5</span>
</td>
<td id="S4.T2.3.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 96.7</td>
<td id="S4.T2.3.1.4.3.6" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 92.5</td>
<td id="S4.T2.3.1.4.3.7" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 88.7</td>
<td id="S4.T2.3.1.4.3.8" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 91.1</td>
<td id="S4.T2.3.1.4.3.9" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 88.6</td>
<td id="S4.T2.3.1.4.3.10" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 86.0</td>
<td id="S4.T2.3.1.4.3.11" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 92.0</td>
<td id="S4.T2.3.1.4.3.12" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 7.3</td>
<td id="S4.T2.3.1.4.3.13" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 14.7</td>
</tr>
<tr id="S4.T2.3.1.5.4" class="ltx_tr">
<th id="S4.T2.3.1.5.4.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding:-0.5pt 25.0pt;"></th>
<th id="S4.T2.3.1.5.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 2018</th>
<th id="S4.T2.3.1.5.4.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Ke
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib97" title="" class="ltx_ref">2018</a>)</cite></th>
<td id="S4.T2.3.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T2.3.1.5.4.4.1" class="ltx_text" style="color:#0000FF;">98.5</span>
</td>
<td id="S4.T2.3.1.5.4.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 96.8</td>
<td id="S4.T2.3.1.5.4.6" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 92.7</td>
<td id="S4.T2.3.1.5.4.7" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 88.4</td>
<td id="S4.T2.3.1.5.4.8" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 90.6</td>
<td id="S4.T2.3.1.5.4.9" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 89.3</td>
<td id="S4.T2.3.1.5.4.10" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 86.3</td>
<td id="S4.T2.3.1.5.4.11" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 92.1</td>
<td id="S4.T2.3.1.5.4.12" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T2.3.1.5.4.13" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
</tr>
<tr id="S4.T2.3.1.6.5" class="ltx_tr">
<th id="S4.T2.3.1.6.5.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding:-0.5pt 25.0pt;"></th>
<th id="S4.T2.3.1.6.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 2018</th>
<th id="S4.T2.3.1.6.5.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Tang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib225" title="" class="ltx_ref">2018</a>)</cite></th>
<td id="S4.T2.3.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 98.4</td>
<td id="S4.T2.3.1.6.5.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 96.9</td>
<td id="S4.T2.3.1.6.5.6" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 92.6</td>
<td id="S4.T2.3.1.6.5.7" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 88.7</td>
<td id="S4.T2.3.1.6.5.8" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T2.3.1.6.5.8.1" class="ltx_text" style="color:#0000FF;">91.8</span>
</td>
<td id="S4.T2.3.1.6.5.9" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 89.4</td>
<td id="S4.T2.3.1.6.5.10" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 86.2</td>
<td id="S4.T2.3.1.6.5.11" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 92.3</td>
<td id="S4.T2.3.1.6.5.12" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 15.5</td>
<td id="S4.T2.3.1.6.5.13" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 33.6</td>
</tr>
<tr id="S4.T2.3.1.7.6" class="ltx_tr">
<th id="S4.T2.3.1.7.6.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding:-0.5pt 25.0pt;"></th>
<th id="S4.T2.3.1.7.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 2018</th>
<th id="S4.T2.3.1.7.6.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Xiao etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib260" title="" class="ltx_ref">2018</a>)</cite></th>
<td id="S4.T2.3.1.7.6.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T2.3.1.7.6.4.1" class="ltx_text" style="color:#0000FF;">98.5</span>
</td>
<td id="S4.T2.3.1.7.6.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 96.6</td>
<td id="S4.T2.3.1.7.6.6" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 91.9</td>
<td id="S4.T2.3.1.7.6.7" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 87.6</td>
<td id="S4.T2.3.1.7.6.8" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 91.1</td>
<td id="S4.T2.3.1.7.6.9" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 88.1</td>
<td id="S4.T2.3.1.7.6.10" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 84.1</td>
<td id="S4.T2.3.1.7.6.11" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 91.5</td>
<td id="S4.T2.3.1.7.6.12" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T2.3.1.7.6.13" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
</tr>
<tr id="S4.T2.3.1.8.7" class="ltx_tr">
<th id="S4.T2.3.1.8.7.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding:-0.5pt 25.0pt;"></th>
<th id="S4.T2.3.1.8.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 2019</th>
<th id="S4.T2.3.1.8.7.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Sun
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib222" title="" class="ltx_ref">2019</a>)</cite></th>
<td id="S4.T2.3.1.8.7.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T2.3.1.8.7.4.1" class="ltx_text" style="color:#FF0000;">98.6</span>
</td>
<td id="S4.T2.3.1.8.7.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 96.9</td>
<td id="S4.T2.3.1.8.7.6" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 92.8</td>
<td id="S4.T2.3.1.8.7.7" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 89.0</td>
<td id="S4.T2.3.1.8.7.8" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 91.5</td>
<td id="S4.T2.3.1.8.7.9" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 89.0</td>
<td id="S4.T2.3.1.8.7.10" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 85.7</td>
<td id="S4.T2.3.1.8.7.11" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 92.3</td>
<td id="S4.T2.3.1.8.7.12" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 28.5</td>
<td id="S4.T2.3.1.8.7.13" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 9.5</td>
</tr>
<tr id="S4.T2.3.1.9.8" class="ltx_tr">
<th id="S4.T2.3.1.9.8.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding:-0.5pt 25.0pt;"></th>
<th id="S4.T2.3.1.9.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 2019</th>
<th id="S4.T2.3.1.9.8.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib286" title="" class="ltx_ref">2019a</a>)</cite></th>
<td id="S4.T2.3.1.9.8.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T2.3.1.9.8.4.1" class="ltx_text" style="color:#FF0000;">98.6</span>
</td>
<td id="S4.T2.3.1.9.8.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 97.0</td>
<td id="S4.T2.3.1.9.8.6" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 92.8</td>
<td id="S4.T2.3.1.9.8.7" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 88.8</td>
<td id="S4.T2.3.1.9.8.8" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 91.7</td>
<td id="S4.T2.3.1.9.8.9" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 89.8</td>
<td id="S4.T2.3.1.9.8.10" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T2.3.1.9.8.10.1" class="ltx_text" style="color:#0000FF;">86.6</span>
</td>
<td id="S4.T2.3.1.9.8.11" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 92.5</td>
<td id="S4.T2.3.1.9.8.12" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 23.9</td>
<td id="S4.T2.3.1.9.8.13" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 41.4</td>
</tr>
<tr id="S4.T2.3.1.10.9" class="ltx_tr">
<th id="S4.T2.3.1.10.9.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding:-0.5pt 25.0pt;"></th>
<th id="S4.T2.3.1.10.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 2019</th>
<th id="S4.T2.3.1.10.9.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib122" title="" class="ltx_ref">2019a</a>)</cite></th>
<td id="S4.T2.3.1.10.9.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 98.4</td>
<td id="S4.T2.3.1.10.9.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T2.3.1.10.9.5.1" class="ltx_text" style="color:#0000FF;">97.1</span>
</td>
<td id="S4.T2.3.1.10.9.6" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T2.3.1.10.9.6.1" class="ltx_text" style="color:#0000FF;">93.2</span>
</td>
<td id="S4.T2.3.1.10.9.7" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T2.3.1.10.9.7.1" class="ltx_text" style="color:#0000FF;">89.2</span>
</td>
<td id="S4.T2.3.1.10.9.8" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T2.3.1.10.9.8.1" class="ltx_text" style="color:#FF0000;">92.0</span>
</td>
<td id="S4.T2.3.1.10.9.9" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T2.3.1.10.9.9.1" class="ltx_text" style="color:#0000FF;">90.1</span>
</td>
<td id="S4.T2.3.1.10.9.10" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 85.5</td>
<td id="S4.T2.3.1.10.9.11" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 92.6</td>
<td id="S4.T2.3.1.10.9.12" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T2.3.1.10.9.13" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
</tr>
<tr id="S4.T2.3.1.11.10" class="ltx_tr">
<th id="S4.T2.3.1.11.10.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding:-0.5pt 25.0pt;"></th>
<th id="S4.T2.3.1.11.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 2020</th>
<th id="S4.T2.3.1.11.10.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Artacho and
Savakis, <a href="#bib.bib7" title="" class="ltx_ref">2020</a>)</cite></th>
<td id="S4.T2.3.1.11.10.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T2.3.1.11.10.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T2.3.1.11.10.6" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T2.3.1.11.10.7" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T2.3.1.11.10.8" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T2.3.1.11.10.9" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T2.3.1.11.10.10" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T2.3.1.11.10.11" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T2.3.1.11.10.11.1" class="ltx_text" style="color:#0000FF;">92.7</span>
</td>
<td id="S4.T2.3.1.11.10.12" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T2.3.1.11.10.13" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
</tr>
<tr id="S4.T2.3.1.12.11" class="ltx_tr">
<th id="S4.T2.3.1.12.11.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding:-0.5pt 25.0pt;"></th>
<th id="S4.T2.3.1.12.11.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 2020</th>
<th id="S4.T2.3.1.12.11.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Cai etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2020</a>)</cite></th>
<td id="S4.T2.3.1.12.11.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T2.3.1.12.11.4.1" class="ltx_text" style="color:#0000FF;">98.5</span>
</td>
<td id="S4.T2.3.1.12.11.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T2.3.1.12.11.5.1" class="ltx_text" style="color:#FF0000;">97.3</span>
</td>
<td id="S4.T2.3.1.12.11.6" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T2.3.1.12.11.6.1" class="ltx_text" style="color:#FF0000;">93.9</span>
</td>
<td id="S4.T2.3.1.12.11.7" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T2.3.1.12.11.7.1" class="ltx_text" style="color:#FF0000;">89.9</span>
</td>
<td id="S4.T2.3.1.12.11.8" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T2.3.1.12.11.8.1" class="ltx_text" style="color:#FF0000;">92.0</span>
</td>
<td id="S4.T2.3.1.12.11.9" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T2.3.1.12.11.9.1" class="ltx_text" style="color:#FF0000;">90.6</span>
</td>
<td id="S4.T2.3.1.12.11.10" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T2.3.1.12.11.10.1" class="ltx_text" style="color:#FF0000;">86.8</span>
</td>
<td id="S4.T2.3.1.12.11.11" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T2.3.1.12.11.11.1" class="ltx_text" style="color:#FF0000;">93.0</span>
</td>
<td id="S4.T2.3.1.12.11.12" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T2.3.1.12.11.13" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
</tr>
<tr id="S4.T2.3.1.13.12" class="ltx_tr">
<th id="S4.T2.3.1.13.12.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding:-0.5pt 25.0pt;"></th>
<th id="S4.T2.3.1.13.12.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 2021</th>
<th id="S4.T2.3.1.13.12.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Li
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib126" title="" class="ltx_ref">2021e</a>)</cite></th>
<td id="S4.T2.3.1.13.12.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 97.1</td>
<td id="S4.T2.3.1.13.12.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 95.9</td>
<td id="S4.T2.3.1.13.12.6" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 90.4</td>
<td id="S4.T2.3.1.13.12.7" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 86.0</td>
<td id="S4.T2.3.1.13.12.8" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 89.3</td>
<td id="S4.T2.3.1.13.12.9" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 87.1</td>
<td id="S4.T2.3.1.13.12.10" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 82.5</td>
<td id="S4.T2.3.1.13.12.11" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 90.2</td>
<td id="S4.T2.3.1.13.12.12" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 28.1</td>
<td id="S4.T2.3.1.13.12.13" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
</tr>
<tr id="S4.T2.3.1.14.13" class="ltx_tr">
<th id="S4.T2.3.1.14.13.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T2.3.1.14.13.1.1" class="ltx_text">H</span></th>
<th id="S4.T2.3.1.14.13.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 2022</th>
<th id="S4.T2.3.1.14.13.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib125" title="" class="ltx_ref">2022b</a>)</cite></th>
<td id="S4.T2.3.1.14.13.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 97.2</td>
<td id="S4.T2.3.1.14.13.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 96.0</td>
<td id="S4.T2.3.1.14.13.6" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 90.4</td>
<td id="S4.T2.3.1.14.13.7" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 85.6</td>
<td id="S4.T2.3.1.14.13.8" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 89.5</td>
<td id="S4.T2.3.1.14.13.9" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 85.8</td>
<td id="S4.T2.3.1.14.13.10" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 81.8</td>
<td id="S4.T2.3.1.14.13.11" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 90.0</td>
<td id="S4.T2.3.1.14.13.12" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T2.3.1.14.13.13" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
</tr>
<tr id="S4.T2.3.1.15.14" class="ltx_tr">
<th id="S4.T2.3.1.15.14.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;"></th>
<th id="S4.T2.3.1.15.14.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 2017</th>
<th id="S4.T2.3.1.15.14.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Sun
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib223" title="" class="ltx_ref">2017</a>)</cite></th>
<td id="S4.T2.3.1.15.14.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 97.5</td>
<td id="S4.T2.3.1.15.14.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 94.3</td>
<td id="S4.T2.3.1.15.14.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 87.0</td>
<td id="S4.T2.3.1.15.14.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 81.2</td>
<td id="S4.T2.3.1.15.14.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 86.5</td>
<td id="S4.T2.3.1.15.14.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 78.5</td>
<td id="S4.T2.3.1.15.14.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 75.4</td>
<td id="S4.T2.3.1.15.14.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 86.4</td>
<td id="S4.T2.3.1.15.14.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T2.3.1.15.14.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
</tr>
<tr id="S4.T2.3.1.16.15" class="ltx_tr">
<th id="S4.T2.3.1.16.15.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding:-0.5pt 25.0pt;"></th>
<th id="S4.T2.3.1.16.15.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 2019</th>
<th id="S4.T2.3.1.16.15.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Zhang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib285" title="" class="ltx_ref">2019b</a>)</cite></th>
<td id="S4.T2.3.1.16.15.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 98.3</td>
<td id="S4.T2.3.1.16.15.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 96.4</td>
<td id="S4.T2.3.1.16.15.6" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 91.5</td>
<td id="S4.T2.3.1.16.15.7" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 87.4</td>
<td id="S4.T2.3.1.16.15.8" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 90.0</td>
<td id="S4.T2.3.1.16.15.9" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 87.1</td>
<td id="S4.T2.3.1.16.15.10" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 83.7</td>
<td id="S4.T2.3.1.16.15.11" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 91.1</td>
<td id="S4.T2.3.1.16.15.12" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 3.0</td>
<td id="S4.T2.3.1.16.15.13" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 9.0</td>
</tr>
<tr id="S4.T2.3.1.17.16" class="ltx_tr">
<th id="S4.T2.3.1.17.16.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding:-0.5pt 25.0pt;"></th>
<th id="S4.T2.3.1.17.16.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 2019</th>
<th id="S4.T2.3.1.17.16.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Luvizon
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib149" title="" class="ltx_ref">2019</a>)</cite></th>
<td id="S4.T2.3.1.17.16.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 98.1</td>
<td id="S4.T2.3.1.17.16.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 96.6</td>
<td id="S4.T2.3.1.17.16.6" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 92.0</td>
<td id="S4.T2.3.1.17.16.7" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 87.5</td>
<td id="S4.T2.3.1.17.16.8" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 90.6</td>
<td id="S4.T2.3.1.17.16.9" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 88</td>
<td id="S4.T2.3.1.17.16.10" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 82.7</td>
<td id="S4.T2.3.1.17.16.11" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 91.2</td>
<td id="S4.T2.3.1.17.16.12" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T2.3.1.17.16.13" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
</tr>
<tr id="S4.T2.3.1.18.17" class="ltx_tr">
<th id="S4.T2.3.1.18.17.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding:-0.5pt 25.0pt;"></th>
<th id="S4.T2.3.1.18.17.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 2021</th>
<th id="S4.T2.3.1.18.17.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Li
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib117" title="" class="ltx_ref">2021c</a>)</cite></th>
<td id="S4.T2.3.1.18.17.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 97.3</td>
<td id="S4.T2.3.1.18.17.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 96.0</td>
<td id="S4.T2.3.1.18.17.6" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 90.6</td>
<td id="S4.T2.3.1.18.17.7" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 84.5</td>
<td id="S4.T2.3.1.18.17.8" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 89.7</td>
<td id="S4.T2.3.1.18.17.9" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 85.5</td>
<td id="S4.T2.3.1.18.17.10" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 79.0</td>
<td id="S4.T2.3.1.18.17.11" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 89.5</td>
<td id="S4.T2.3.1.18.17.12" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T2.3.1.18.17.13" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
</tr>
<tr id="S4.T2.3.1.19.18" class="ltx_tr">
<th id="S4.T2.3.1.19.18.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding:-0.5pt 25.0pt;"></th>
<th id="S4.T2.3.1.19.18.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 2021</th>
<th id="S4.T2.3.1.19.18.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Mao
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib154" title="" class="ltx_ref">2021</a>)</cite></th>
<td id="S4.T2.3.1.19.18.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 98.0</td>
<td id="S4.T2.3.1.19.18.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 95.9</td>
<td id="S4.T2.3.1.19.18.6" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 91.0</td>
<td id="S4.T2.3.1.19.18.7" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 86.0</td>
<td id="S4.T2.3.1.19.18.8" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 89.8</td>
<td id="S4.T2.3.1.19.18.9" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 86.6</td>
<td id="S4.T2.3.1.19.18.10" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 82.6</td>
<td id="S4.T2.3.1.19.18.11" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 90.4</td>
<td id="S4.T2.3.1.19.18.12" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T2.3.1.19.18.13" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
</tr>
<tr id="S4.T2.3.1.20.19" class="ltx_tr">
<th id="S4.T2.3.1.20.19.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T2.3.1.20.19.1.1" class="ltx_text">R</span></th>
<th id="S4.T2.3.1.20.19.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 2022</th>
<th id="S4.T2.3.1.20.19.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Mao etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib155" title="" class="ltx_ref">2022</a>)</cite></th>
<td id="S4.T2.3.1.20.19.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T2.3.1.20.19.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T2.3.1.20.19.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T2.3.1.20.19.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T2.3.1.20.19.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T2.3.1.20.19.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T2.3.1.20.19.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T2.3.1.20.19.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â 90.5</td>
<td id="S4.T2.3.1.20.19.12" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T2.3.1.20.19.13" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding:-0.5pt 25.0pt;">Â Â Â Â Â Â Â Â -</td>
</tr>
</tbody>
</table>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<ul id="S4.I1" class="ltx_itemize ltx_centering ltx_figure_panel">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.p1.1.1" class="ltx_text" style="font-size:50%;">Note: </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.I1.i1.p1.1.2.1" class="ltx_text" style="font-size:50%;">(</span>Insafutdinov etÂ al<span class="ltx_text">.</span><span id="S4.I1.i1.p1.1.3.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib77" title="" class="ltx_ref">2016</a><span id="S4.I1.i1.p1.1.4.3" class="ltx_text" style="font-size:50%;">)</span></cite><span id="S4.I1.i1.p1.1.5" class="ltx_text" style="font-size:50%;">, </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.I1.i1.p1.1.6.1" class="ltx_text" style="font-size:50%;">(</span>Sun
etÂ al<span class="ltx_text">.</span><span id="S4.I1.i1.p1.1.7.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib222" title="" class="ltx_ref">2019</a><span id="S4.I1.i1.p1.1.8.3" class="ltx_text" style="font-size:50%;">)</span></cite><span id="S4.I1.i1.p1.1.9" class="ltx_text" style="font-size:50%;">, </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.I1.i1.p1.1.10.1" class="ltx_text" style="font-size:50%;">(</span>Li etÂ al<span class="ltx_text">.</span><span id="S4.I1.i1.p1.1.11.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib122" title="" class="ltx_ref">2019a</a><span id="S4.I1.i1.p1.1.12.3" class="ltx_text" style="font-size:50%;">)</span></cite><span id="S4.I1.i1.p1.1.13" class="ltx_text" style="font-size:50%;">, </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.I1.i1.p1.1.14.1" class="ltx_text" style="font-size:50%;">(</span>Cai etÂ al<span class="ltx_text">.</span><span id="S4.I1.i1.p1.1.15.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib15" title="" class="ltx_ref">2020</a><span id="S4.I1.i1.p1.1.16.3" class="ltx_text" style="font-size:50%;">)</span></cite><span id="S4.I1.i1.p1.1.17" class="ltx_text" style="font-size:50%;">, </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.I1.i1.p1.1.18.1" class="ltx_text" style="font-size:50%;">(</span>Rogez
etÂ al<span class="ltx_text">.</span><span id="S4.I1.i1.p1.1.19.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib208" title="" class="ltx_ref">2017</a><span id="S4.I1.i1.p1.1.20.3" class="ltx_text" style="font-size:50%;">)</span></cite><span id="S4.I1.i1.p1.1.21" class="ltx_text" style="font-size:50%;">,</span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.I1.i1.p1.1.22.1" class="ltx_text" style="font-size:50%;">(</span>Li
etÂ al<span class="ltx_text">.</span><span id="S4.I1.i1.p1.1.23.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib126" title="" class="ltx_ref">2021e</a><span id="S4.I1.i1.p1.1.24.3" class="ltx_text" style="font-size:50%;">)</span></cite><span id="S4.I1.i1.p1.1.25" class="ltx_text" style="font-size:50%;"> are 2D multi-person HPE methods, which are also applied to the single-person case here.</span></p>
</div>
</li>
</ul>
</div>
</div>
</figure>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3. </span><span id="S4.T3.20.1" class="ltx_text" style="font-size:80%;">Comparison of different 2D multi-person HPE methods on the test-dev set of the COCO dataset using AP measure (AP.5: AP at OKS = 0.50; AP.75: AP at OKS = 0.75; AP(M) is used for medium objects; AP(L) is used for large objects). <span id="S4.T3.20.1.1" class="ltx_text ltx_font_bold">Extra</span>: extra data is used for training. <span id="S4.T3.20.1.2" class="ltx_text ltx_font_bold">T</span>: Top-down; <span id="S4.T3.20.1.3" class="ltx_text ltx_font_bold">B</span>: Bottom-up
</span></figcaption>
<div id="S4.T3.18" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:123.4pt;vertical-align:-0.3pt;"><span class="ltx_transformed_inner" style="transform:translate(-417.3pt,118.5pt) scale(0.341937522184255,0.341937522184255) ;">
<table id="S4.T3.18.18" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.18.18.19.1" class="ltx_tr">
<th id="S4.T3.18.18.19.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;" colspan="13">Â Â Â Â Â Â Â Â Microsoft Common Objects in Context (COCO)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.18.18.20.1" class="ltx_tr">
<th id="S4.T3.18.18.20.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;"></th>
<th id="S4.T3.18.18.20.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T3.18.18.20.1.2.1" class="ltx_text">Year</span></th>
<th id="S4.T3.18.18.20.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T3.18.18.20.1.3.1" class="ltx_text">Method</span></th>
<td id="S4.T3.18.18.20.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T3.18.18.20.1.4.1" class="ltx_text ltx_font_bold">Extra</span></td>
<td id="S4.T3.18.18.20.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T3.18.18.20.1.5.1" class="ltx_text">Backbone</span></td>
<td id="S4.T3.18.18.20.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T3.18.18.20.1.6.1" class="ltx_text">Input size</span></td>
<td id="S4.T3.18.18.20.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T3.18.18.20.1.7.1" class="ltx_text">AP</span></td>
<td id="S4.T3.18.18.20.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T3.18.18.20.1.8.1" class="ltx_text">AP.5</span></td>
<td id="S4.T3.18.18.20.1.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T3.18.18.20.1.9.1" class="ltx_text">AP.75</span></td>
<td id="S4.T3.18.18.20.1.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T3.18.18.20.1.10.1" class="ltx_text">AP(M)</span></td>
<td id="S4.T3.18.18.20.1.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T3.18.18.20.1.11.1" class="ltx_text">AP(L)</span></td>
<td id="S4.T3.18.18.20.1.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T3.18.18.20.1.12.1" class="ltx_text">Params(M)</span></td>
<td id="S4.T3.18.18.20.1.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T3.18.18.20.1.13.1" class="ltx_text">FLOPs(G)</span></td>
</tr>
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.2" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;"></th>
<th id="S4.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 2019</th>
<th id="S4.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Sun
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib222" title="" class="ltx_ref">2019</a>)</cite></th>
<td id="S4.T3.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â no</td>
<td id="S4.T3.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â HRNet-W32</td>
<td id="S4.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 384<math id="S4.T3.1.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T3.1.1.1.1.m1.1a"><mo id="S4.T3.1.1.1.1.m1.1.1" xref="S4.T3.1.1.1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.m1.1b"><times id="S4.T3.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.m1.1c">\times</annotation></semantics></math>288</td>
<td id="S4.T3.1.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 74.9</td>
<td id="S4.T3.1.1.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 92.5</td>
<td id="S4.T3.1.1.1.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 82.8</td>
<td id="S4.T3.1.1.1.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 71.3</td>
<td id="S4.T3.1.1.1.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 80.9</td>
<td id="S4.T3.1.1.1.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 28.5</td>
<td id="S4.T3.1.1.1.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 16.0</td>
</tr>
<tr id="S4.T3.2.2.2" class="ltx_tr">
<th id="S4.T3.2.2.2.2" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;"></th>
<th id="S4.T3.2.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 2019</th>
<th id="S4.T3.2.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Sun
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib222" title="" class="ltx_ref">2019</a>)</cite></th>
<td id="S4.T3.2.2.2.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â no</td>
<td id="S4.T3.2.2.2.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â HRNet-W48</td>
<td id="S4.T3.2.2.2.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 384<math id="S4.T3.2.2.2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T3.2.2.2.1.m1.1a"><mo id="S4.T3.2.2.2.1.m1.1.1" xref="S4.T3.2.2.2.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.1.m1.1b"><times id="S4.T3.2.2.2.1.m1.1.1.cmml" xref="S4.T3.2.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.1.m1.1c">\times</annotation></semantics></math>288</td>
<td id="S4.T3.2.2.2.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 75.5</td>
<td id="S4.T3.2.2.2.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 92.5</td>
<td id="S4.T3.2.2.2.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 83.3</td>
<td id="S4.T3.2.2.2.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 71.9</td>
<td id="S4.T3.2.2.2.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 81.5</td>
<td id="S4.T3.2.2.2.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 63.6</td>
<td id="S4.T3.2.2.2.13" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 32.9</td>
</tr>
<tr id="S4.T3.3.3.3" class="ltx_tr">
<th id="S4.T3.3.3.3.2" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;"></th>
<th id="S4.T3.3.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 2019</th>
<th id="S4.T3.3.3.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Sun
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib222" title="" class="ltx_ref">2019</a>)</cite></th>
<td id="S4.T3.3.3.3.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â yes</td>
<td id="S4.T3.3.3.3.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â HRNet-W48</td>
<td id="S4.T3.3.3.3.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 384<math id="S4.T3.3.3.3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T3.3.3.3.1.m1.1a"><mo id="S4.T3.3.3.3.1.m1.1.1" xref="S4.T3.3.3.3.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.1.m1.1b"><times id="S4.T3.3.3.3.1.m1.1.1.cmml" xref="S4.T3.3.3.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.1.m1.1c">\times</annotation></semantics></math>288</td>
<td id="S4.T3.3.3.3.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 77.0</td>
<td id="S4.T3.3.3.3.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 92.7</td>
<td id="S4.T3.3.3.3.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 84.5</td>
<td id="S4.T3.3.3.3.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 73.4</td>
<td id="S4.T3.3.3.3.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 83.1</td>
<td id="S4.T3.3.3.3.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 63.6</td>
<td id="S4.T3.3.3.3.13" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 32.9</td>
</tr>
<tr id="S4.T3.4.4.4" class="ltx_tr">
<th id="S4.T3.4.4.4.2" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;"></th>
<th id="S4.T3.4.4.4.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 2019</th>
<th id="S4.T3.4.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib122" title="" class="ltx_ref">2019a</a>)</cite></th>
<td id="S4.T3.4.4.4.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â yes</td>
<td id="S4.T3.4.4.4.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 4xResNet-50</td>
<td id="S4.T3.4.4.4.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 384<math id="S4.T3.4.4.4.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T3.4.4.4.1.m1.1a"><mo id="S4.T3.4.4.4.1.m1.1.1" xref="S4.T3.4.4.4.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T3.4.4.4.1.m1.1b"><times id="S4.T3.4.4.4.1.m1.1.1.cmml" xref="S4.T3.4.4.4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.4.4.1.m1.1c">\times</annotation></semantics></math>288</td>
<td id="S4.T3.4.4.4.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 77.1</td>
<td id="S4.T3.4.4.4.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T3.4.4.4.8.1" class="ltx_text" style="color:#0000FF;">93.8</span>
</td>
<td id="S4.T3.4.4.4.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 84.6</td>
<td id="S4.T3.4.4.4.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 73.4</td>
<td id="S4.T3.4.4.4.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 82.3</td>
<td id="S4.T3.4.4.4.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T3.4.4.4.13" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â -</td>
</tr>
<tr id="S4.T3.5.5.5" class="ltx_tr">
<th id="S4.T3.5.5.5.2" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;"></th>
<th id="S4.T3.5.5.5.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 2020</th>
<th id="S4.T3.5.5.5.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Zhang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib284" title="" class="ltx_ref">2020h</a>)</cite></th>
<td id="S4.T3.5.5.5.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â no</td>
<td id="S4.T3.5.5.5.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â HRNet-W48</td>
<td id="S4.T3.5.5.5.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 384<math id="S4.T3.5.5.5.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T3.5.5.5.1.m1.1a"><mo id="S4.T3.5.5.5.1.m1.1.1" xref="S4.T3.5.5.5.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T3.5.5.5.1.m1.1b"><times id="S4.T3.5.5.5.1.m1.1.1.cmml" xref="S4.T3.5.5.5.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.5.5.1.m1.1c">\times</annotation></semantics></math>288</td>
<td id="S4.T3.5.5.5.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 76.2</td>
<td id="S4.T3.5.5.5.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 92.5</td>
<td id="S4.T3.5.5.5.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 83.6</td>
<td id="S4.T3.5.5.5.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 72.5</td>
<td id="S4.T3.5.5.5.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 82.4</td>
<td id="S4.T3.5.5.5.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 63.6</td>
<td id="S4.T3.5.5.5.13" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 32.9</td>
</tr>
<tr id="S4.T3.6.6.6" class="ltx_tr">
<th id="S4.T3.6.6.6.2" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;"></th>
<th id="S4.T3.6.6.6.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 2020</th>
<th id="S4.T3.6.6.6.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Zhang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib284" title="" class="ltx_ref">2020h</a>)</cite></th>
<td id="S4.T3.6.6.6.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â yes</td>
<td id="S4.T3.6.6.6.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â HRNet-W48</td>
<td id="S4.T3.6.6.6.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 384<math id="S4.T3.6.6.6.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T3.6.6.6.1.m1.1a"><mo id="S4.T3.6.6.6.1.m1.1.1" xref="S4.T3.6.6.6.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T3.6.6.6.1.m1.1b"><times id="S4.T3.6.6.6.1.m1.1.1.cmml" xref="S4.T3.6.6.6.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.6.6.6.1.m1.1c">\times</annotation></semantics></math>288</td>
<td id="S4.T3.6.6.6.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 77.4</td>
<td id="S4.T3.6.6.6.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 92.6</td>
<td id="S4.T3.6.6.6.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 84.6</td>
<td id="S4.T3.6.6.6.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 73.6</td>
<td id="S4.T3.6.6.6.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T3.6.6.6.11.1" class="ltx_text" style="color:#0000FF;">83.7</span>
</td>
<td id="S4.T3.6.6.6.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 63.6</td>
<td id="S4.T3.6.6.6.13" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 32.9</td>
</tr>
<tr id="S4.T3.7.7.7" class="ltx_tr">
<th id="S4.T3.7.7.7.2" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;"></th>
<th id="S4.T3.7.7.7.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 2020</th>
<th id="S4.T3.7.7.7.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Cai etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2020</a>)</cite></th>
<td id="S4.T3.7.7.7.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â no</td>
<td id="S4.T3.7.7.7.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 4xRSN-50</td>
<td id="S4.T3.7.7.7.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 384<math id="S4.T3.7.7.7.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T3.7.7.7.1.m1.1a"><mo id="S4.T3.7.7.7.1.m1.1.1" xref="S4.T3.7.7.7.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T3.7.7.7.1.m1.1b"><times id="S4.T3.7.7.7.1.m1.1.1.cmml" xref="S4.T3.7.7.7.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.7.7.7.1.m1.1c">\times</annotation></semantics></math>288</td>
<td id="S4.T3.7.7.7.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T3.7.7.7.7.1" class="ltx_text" style="color:#0000FF;">78.6</span>
</td>
<td id="S4.T3.7.7.7.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T3.7.7.7.8.1" class="ltx_text" style="color:#FF0000;">94.3</span>
</td>
<td id="S4.T3.7.7.7.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T3.7.7.7.9.1" class="ltx_text" style="color:#FF0000;">86.6</span>
</td>
<td id="S4.T3.7.7.7.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T3.7.7.7.10.1" class="ltx_text" style="color:#0000FF;">75.5</span>
</td>
<td id="S4.T3.7.7.7.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 83.3</td>
<td id="S4.T3.7.7.7.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 111.8</td>
<td id="S4.T3.7.7.7.13" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 65.9</td>
</tr>
<tr id="S4.T3.8.8.8" class="ltx_tr">
<th id="S4.T3.8.8.8.2" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;"></th>
<th id="S4.T3.8.8.8.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 2021</th>
<th id="S4.T3.8.8.8.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Yang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib269" title="" class="ltx_ref">2021</a>)</cite></th>
<td id="S4.T3.8.8.8.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â no</td>
<td id="S4.T3.8.8.8.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â HRNet-W48</td>
<td id="S4.T3.8.8.8.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 256<math id="S4.T3.8.8.8.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T3.8.8.8.1.m1.1a"><mo id="S4.T3.8.8.8.1.m1.1.1" xref="S4.T3.8.8.8.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T3.8.8.8.1.m1.1b"><times id="S4.T3.8.8.8.1.m1.1.1.cmml" xref="S4.T3.8.8.8.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.8.8.8.1.m1.1c">\times</annotation></semantics></math>192</td>
<td id="S4.T3.8.8.8.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 75.0</td>
<td id="S4.T3.8.8.8.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 92.2</td>
<td id="S4.T3.8.8.8.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 82.3</td>
<td id="S4.T3.8.8.8.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 71.3</td>
<td id="S4.T3.8.8.8.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 81.1</td>
<td id="S4.T3.8.8.8.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 17.5</td>
<td id="S4.T3.8.8.8.13" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 21.8</td>
</tr>
<tr id="S4.T3.9.9.9" class="ltx_tr">
<th id="S4.T3.9.9.9.2" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;"></th>
<th id="S4.T3.9.9.9.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 2021</th>
<th id="S4.T3.9.9.9.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Li
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib126" title="" class="ltx_ref">2021e</a>)</cite></th>
<td id="S4.T3.9.9.9.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â no</td>
<td id="S4.T3.9.9.9.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â HRNet-W48</td>
<td id="S4.T3.9.9.9.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 384<math id="S4.T3.9.9.9.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T3.9.9.9.1.m1.1a"><mo id="S4.T3.9.9.9.1.m1.1.1" xref="S4.T3.9.9.9.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T3.9.9.9.1.m1.1b"><times id="S4.T3.9.9.9.1.m1.1.1.cmml" xref="S4.T3.9.9.9.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.9.9.9.1.m1.1c">\times</annotation></semantics></math>288</td>
<td id="S4.T3.9.9.9.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 75.9</td>
<td id="S4.T3.9.9.9.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 92.3</td>
<td id="S4.T3.9.9.9.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 83.4</td>
<td id="S4.T3.9.9.9.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 72.2</td>
<td id="S4.T3.9.9.9.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 82.1</td>
<td id="S4.T3.9.9.9.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 29.8</td>
<td id="S4.T3.9.9.9.13" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 22.1</td>
</tr>
<tr id="S4.T3.10.10.10" class="ltx_tr">
<th id="S4.T3.10.10.10.2" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;"></th>
<th id="S4.T3.10.10.10.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 2021</th>
<th id="S4.T3.10.10.10.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Liu
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib135" title="" class="ltx_ref">2021b</a>)</cite></th>
<td id="S4.T3.10.10.10.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â no</td>
<td id="S4.T3.10.10.10.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â HRNet-W48</td>
<td id="S4.T3.10.10.10.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 384<math id="S4.T3.10.10.10.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T3.10.10.10.1.m1.1a"><mo id="S4.T3.10.10.10.1.m1.1.1" xref="S4.T3.10.10.10.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T3.10.10.10.1.m1.1b"><times id="S4.T3.10.10.10.1.m1.1.1.cmml" xref="S4.T3.10.10.10.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.10.10.10.1.m1.1c">\times</annotation></semantics></math>288</td>
<td id="S4.T3.10.10.10.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T3.10.10.10.7.1" class="ltx_text" style="color:#FF0000;">79.5</span>
</td>
<td id="S4.T3.10.10.10.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 93.6</td>
<td id="S4.T3.10.10.10.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T3.10.10.10.9.1" class="ltx_text" style="color:#0000FF;">85.9</span>
</td>
<td id="S4.T3.10.10.10.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T3.10.10.10.10.1" class="ltx_text" style="color:#FF0000;">76.3</span>
</td>
<td id="S4.T3.10.10.10.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T3.10.10.10.11.1" class="ltx_text" style="color:#FF0000;">84.3</span>
</td>
<td id="S4.T3.10.10.10.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 35.4</td>
<td id="S4.T3.10.10.10.13" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 70.1</td>
</tr>
<tr id="S4.T3.11.11.11" class="ltx_tr">
<th id="S4.T3.11.11.11.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T3.11.11.11.2.1" class="ltx_text">T</span></th>
<th id="S4.T3.11.11.11.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 2022</th>
<th id="S4.T3.11.11.11.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Ma etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib151" title="" class="ltx_ref">2022</a>)</cite></th>
<td id="S4.T3.11.11.11.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â no</td>
<td id="S4.T3.11.11.11.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â Transformer</td>
<td id="S4.T3.11.11.11.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 256<math id="S4.T3.11.11.11.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T3.11.11.11.1.m1.1a"><mo id="S4.T3.11.11.11.1.m1.1.1" xref="S4.T3.11.11.11.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T3.11.11.11.1.m1.1b"><times id="S4.T3.11.11.11.1.m1.1.1.cmml" xref="S4.T3.11.11.11.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.11.11.11.1.m1.1c">\times</annotation></semantics></math>192</td>
<td id="S4.T3.11.11.11.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 75.2</td>
<td id="S4.T3.11.11.11.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 89.8</td>
<td id="S4.T3.11.11.11.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 81.7</td>
<td id="S4.T3.11.11.11.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 71.7</td>
<td id="S4.T3.11.11.11.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 82.1</td>
<td id="S4.T3.11.11.11.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 20.8</td>
<td id="S4.T3.11.11.11.13" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 8.7</td>
</tr>
<tr id="S4.T3.12.12.12" class="ltx_tr">
<th id="S4.T3.12.12.12.2" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;"></th>
<th id="S4.T3.12.12.12.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 2017</th>
<th id="S4.T3.12.12.12.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Newell
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib171" title="" class="ltx_ref">2017</a>)</cite></th>
<td id="S4.T3.12.12.12.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â no</td>
<td id="S4.T3.12.12.12.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â Hourglass</td>
<td id="S4.T3.12.12.12.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 512<math id="S4.T3.12.12.12.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T3.12.12.12.1.m1.1a"><mo id="S4.T3.12.12.12.1.m1.1.1" xref="S4.T3.12.12.12.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T3.12.12.12.1.m1.1b"><times id="S4.T3.12.12.12.1.m1.1.1.cmml" xref="S4.T3.12.12.12.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.12.12.12.1.m1.1c">\times</annotation></semantics></math>512</td>
<td id="S4.T3.12.12.12.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 65.5</td>
<td id="S4.T3.12.12.12.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 86.8</td>
<td id="S4.T3.12.12.12.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 72.3</td>
<td id="S4.T3.12.12.12.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 60.6</td>
<td id="S4.T3.12.12.12.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 72.6</td>
<td id="S4.T3.12.12.12.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T3.12.12.12.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â -</td>
</tr>
<tr id="S4.T3.13.13.13" class="ltx_tr">
<th id="S4.T3.13.13.13.2" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;"></th>
<th id="S4.T3.13.13.13.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 2018</th>
<th id="S4.T3.13.13.13.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Papandreou etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib180" title="" class="ltx_ref">2018</a>)</cite></th>
<td id="S4.T3.13.13.13.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â no</td>
<td id="S4.T3.13.13.13.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â ResNet-152</td>
<td id="S4.T3.13.13.13.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 1401<math id="S4.T3.13.13.13.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T3.13.13.13.1.m1.1a"><mo id="S4.T3.13.13.13.1.m1.1.1" xref="S4.T3.13.13.13.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T3.13.13.13.1.m1.1b"><times id="S4.T3.13.13.13.1.m1.1.1.cmml" xref="S4.T3.13.13.13.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.13.13.13.1.m1.1c">\times</annotation></semantics></math>1401</td>
<td id="S4.T3.13.13.13.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 68.7</td>
<td id="S4.T3.13.13.13.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 89.0</td>
<td id="S4.T3.13.13.13.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 75.4</td>
<td id="S4.T3.13.13.13.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 64.1</td>
<td id="S4.T3.13.13.13.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 75.5</td>
<td id="S4.T3.13.13.13.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 68.7</td>
<td id="S4.T3.13.13.13.13" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 405.5</td>
</tr>
<tr id="S4.T3.14.14.14" class="ltx_tr">
<th id="S4.T3.14.14.14.2" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;"></th>
<th id="S4.T3.14.14.14.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 2019</th>
<th id="S4.T3.14.14.14.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Tian
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib229" title="" class="ltx_ref">2019</a>)</cite></th>
<td id="S4.T3.14.14.14.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â no</td>
<td id="S4.T3.14.14.14.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â ResNet-101</td>
<td id="S4.T3.14.14.14.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 800<math id="S4.T3.14.14.14.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T3.14.14.14.1.m1.1a"><mo id="S4.T3.14.14.14.1.m1.1.1" xref="S4.T3.14.14.14.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T3.14.14.14.1.m1.1b"><times id="S4.T3.14.14.14.1.m1.1.1.cmml" xref="S4.T3.14.14.14.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.14.14.14.1.m1.1c">\times</annotation></semantics></math>800</td>
<td id="S4.T3.14.14.14.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 64.8</td>
<td id="S4.T3.14.14.14.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 87.8</td>
<td id="S4.T3.14.14.14.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 71.1</td>
<td id="S4.T3.14.14.14.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 60.4</td>
<td id="S4.T3.14.14.14.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 71.5</td>
<td id="S4.T3.14.14.14.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T3.14.14.14.13" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â -</td>
</tr>
<tr id="S4.T3.15.15.15" class="ltx_tr">
<th id="S4.T3.15.15.15.2" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;"></th>
<th id="S4.T3.15.15.15.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 2020</th>
<th id="S4.T3.15.15.15.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Jin etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib89" title="" class="ltx_ref">2020a</a>)</cite></th>
<td id="S4.T3.15.15.15.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â no</td>
<td id="S4.T3.15.15.15.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â Hourglass</td>
<td id="S4.T3.15.15.15.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 512<math id="S4.T3.15.15.15.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T3.15.15.15.1.m1.1a"><mo id="S4.T3.15.15.15.1.m1.1.1" xref="S4.T3.15.15.15.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T3.15.15.15.1.m1.1b"><times id="S4.T3.15.15.15.1.m1.1.1.cmml" xref="S4.T3.15.15.15.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.15.15.15.1.m1.1c">\times</annotation></semantics></math>512</td>
<td id="S4.T3.15.15.15.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 67.6</td>
<td id="S4.T3.15.15.15.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 85.1</td>
<td id="S4.T3.15.15.15.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 73.7</td>
<td id="S4.T3.15.15.15.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 62.7</td>
<td id="S4.T3.15.15.15.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 74.6</td>
<td id="S4.T3.15.15.15.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T3.15.15.15.13" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â -</td>
</tr>
<tr id="S4.T3.16.16.16" class="ltx_tr">
<th id="S4.T3.16.16.16.2" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;"></th>
<th id="S4.T3.16.16.16.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 2020</th>
<th id="S4.T3.16.16.16.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Cheng etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2020</a>)</cite></th>
<td id="S4.T3.16.16.16.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â no</td>
<td id="S4.T3.16.16.16.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â HRNet-W48</td>
<td id="S4.T3.16.16.16.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 640<math id="S4.T3.16.16.16.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T3.16.16.16.1.m1.1a"><mo id="S4.T3.16.16.16.1.m1.1.1" xref="S4.T3.16.16.16.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T3.16.16.16.1.m1.1b"><times id="S4.T3.16.16.16.1.m1.1.1.cmml" xref="S4.T3.16.16.16.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.16.16.16.1.m1.1c">\times</annotation></semantics></math>640</td>
<td id="S4.T3.16.16.16.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 70.5</td>
<td id="S4.T3.16.16.16.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 89.3</td>
<td id="S4.T3.16.16.16.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 77.2</td>
<td id="S4.T3.16.16.16.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 66.6</td>
<td id="S4.T3.16.16.16.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 75.8</td>
<td id="S4.T3.16.16.16.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 63.8</td>
<td id="S4.T3.16.16.16.13" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 154.3</td>
</tr>
<tr id="S4.T3.17.17.17" class="ltx_tr">
<th id="S4.T3.17.17.17.2" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;"></th>
<th id="S4.T3.17.17.17.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 2021</th>
<th id="S4.T3.17.17.17.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Luo
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib147" title="" class="ltx_ref">2021</a>)</cite></th>
<td id="S4.T3.17.17.17.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â no</td>
<td id="S4.T3.17.17.17.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â HRNet-W48</td>
<td id="S4.T3.17.17.17.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 640<math id="S4.T3.17.17.17.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T3.17.17.17.1.m1.1a"><mo id="S4.T3.17.17.17.1.m1.1.1" xref="S4.T3.17.17.17.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T3.17.17.17.1.m1.1b"><times id="S4.T3.17.17.17.1.m1.1.1.cmml" xref="S4.T3.17.17.17.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.17.17.17.1.m1.1c">\times</annotation></semantics></math>640</td>
<td id="S4.T3.17.17.17.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 72.0</td>
<td id="S4.T3.17.17.17.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 90.7</td>
<td id="S4.T3.17.17.17.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 78.8</td>
<td id="S4.T3.17.17.17.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 67.8</td>
<td id="S4.T3.17.17.17.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 77.7</td>
<td id="S4.T3.17.17.17.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 63.8</td>
<td id="S4.T3.17.17.17.13" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 154.6</td>
</tr>
<tr id="S4.T3.18.18.18" class="ltx_tr">
<th id="S4.T3.18.18.18.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T3.18.18.18.2.1" class="ltx_text">B</span></th>
<th id="S4.T3.18.18.18.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 2022</th>
<th id="S4.T3.18.18.18.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Wang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib245" title="" class="ltx_ref">2022c</a>)</cite></th>
<td id="S4.T3.18.18.18.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â no</td>
<td id="S4.T3.18.18.18.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â HRNet-W32</td>
<td id="S4.T3.18.18.18.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 640<math id="S4.T3.18.18.18.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T3.18.18.18.1.m1.1a"><mo id="S4.T3.18.18.18.1.m1.1.1" xref="S4.T3.18.18.18.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T3.18.18.18.1.m1.1b"><times id="S4.T3.18.18.18.1.m1.1.1.cmml" xref="S4.T3.18.18.18.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.18.18.18.1.m1.1c">\times</annotation></semantics></math>640</td>
<td id="S4.T3.18.18.18.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 72.8</td>
<td id="S4.T3.18.18.18.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 91.2</td>
<td id="S4.T3.18.18.18.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 79.9</td>
<td id="S4.T3.18.18.18.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 68.3</td>
<td id="S4.T3.18.18.18.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 79.3</td>
<td id="S4.T3.18.18.18.12" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T3.18.18.18.13" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â -</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4. </span><span id="S4.T4.2.1" class="ltx_text" style="font-size:80%;">Comparison of different 2D video-based HPE methods on the PoseTrack2017 test set and PoseTrack2018 test set</span></figcaption>
<div id="S4.T4.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:89.8pt;vertical-align:-0.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-307.1pt,63.3pt) scale(0.413813980089786,0.413813980089786) ;">
<table id="S4.T4.3.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T4.3.1.1.1" class="ltx_tr">
<td id="S4.T4.3.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;" colspan="11">Â Â Â Â Â Â Â Â PoseTrack2017</td>
</tr>
<tr id="S4.T4.3.1.2.2" class="ltx_tr">
<td id="S4.T4.3.1.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â Year</td>
<td id="S4.T4.3.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â Method</td>
<td id="S4.T4.3.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â Backbone</td>
<td id="S4.T4.3.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â Head</td>
<td id="S4.T4.3.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â Shoulder</td>
<td id="S4.T4.3.1.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â Elbow</td>
<td id="S4.T4.3.1.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â Wrist</td>
<td id="S4.T4.3.1.2.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â Hip</td>
<td id="S4.T4.3.1.2.2.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â Knee</td>
<td id="S4.T4.3.1.2.2.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â Ankle</td>
<td id="S4.T4.3.1.2.2.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â Total</td>
</tr>
<tr id="S4.T4.3.1.3.3" class="ltx_tr">
<td id="S4.T4.3.1.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 2018</td>
<td id="S4.T4.3.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Doering
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib48" title="" class="ltx_ref">2018</a>)</cite></td>
<td id="S4.T4.3.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â VGG</td>
<td id="S4.T4.3.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T4.3.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T4.3.1.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T4.3.1.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 53.1</td>
<td id="S4.T4.3.1.3.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T4.3.1.3.3.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T4.3.1.3.3.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 50.4</td>
<td id="S4.T4.3.1.3.3.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 63.4</td>
</tr>
<tr id="S4.T4.3.1.4.4" class="ltx_tr">
<td id="S4.T4.3.1.4.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 2019</td>
<td id="S4.T4.3.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Bertasius etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2019</a>)</cite></td>
<td id="S4.T4.3.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â HRNet-W48</td>
<td id="S4.T4.3.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 79.5</td>
<td id="S4.T4.3.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 84.3</td>
<td id="S4.T4.3.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 80.1</td>
<td id="S4.T4.3.1.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 75.8</td>
<td id="S4.T4.3.1.4.4.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 77.6</td>
<td id="S4.T4.3.1.4.4.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 76.8</td>
<td id="S4.T4.3.1.4.4.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 70.8</td>
<td id="S4.T4.3.1.4.4.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 77.9</td>
</tr>
<tr id="S4.T4.3.1.5.5" class="ltx_tr">
<td id="S4.T4.3.1.5.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 2020</td>
<td id="S4.T4.3.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Snower
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib219" title="" class="ltx_ref">2020</a>)</cite></td>
<td id="S4.T4.3.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â Transformer</td>
<td id="S4.T4.3.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T4.3.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T4.3.1.5.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T4.3.1.5.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 71.9</td>
<td id="S4.T4.3.1.5.5.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T4.3.1.5.5.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T4.3.1.5.5.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 65.0</td>
<td id="S4.T4.3.1.5.5.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 74.0</td>
</tr>
<tr id="S4.T4.3.1.6.6" class="ltx_tr">
<td id="S4.T4.3.1.6.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 2021</td>
<td id="S4.T4.3.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Liu
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib140" title="" class="ltx_ref">2021a</a>)</cite></td>
<td id="S4.T4.3.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â HRNet-W48</td>
<td id="S4.T4.3.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T4.3.1.6.6.4.1" class="ltx_text" style="color:#0000FF;">84.3</span>
</td>
<td id="S4.T4.3.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T4.3.1.6.6.5.1" class="ltx_text" style="color:#0000FF;">84.9</span>
</td>
<td id="S4.T4.3.1.6.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T4.3.1.6.6.6.1" class="ltx_text" style="color:#0000FF;">80.5</span>
</td>
<td id="S4.T4.3.1.6.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T4.3.1.6.6.7.1" class="ltx_text" style="color:#0000FF;">76.1</span>
</td>
<td id="S4.T4.3.1.6.6.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T4.3.1.6.6.8.1" class="ltx_text" style="color:#0000FF;">77.9</span>
</td>
<td id="S4.T4.3.1.6.6.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T4.3.1.6.6.9.1" class="ltx_text" style="color:#0000FF;">77.1</span>
</td>
<td id="S4.T4.3.1.6.6.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T4.3.1.6.6.10.1" class="ltx_text" style="color:#0000FF;">71.2</span>
</td>
<td id="S4.T4.3.1.6.6.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T4.3.1.6.6.11.1" class="ltx_text" style="color:#0000FF;">79.2</span>
</td>
</tr>
<tr id="S4.T4.3.1.7.7" class="ltx_tr">
<td id="S4.T4.3.1.7.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 2022</td>
<td id="S4.T4.3.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Liu
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib141" title="" class="ltx_ref">2022</a>)</cite></td>
<td id="S4.T4.3.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â HRNet-W48</td>
<td id="S4.T4.3.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T4.3.1.7.7.4.1" class="ltx_text" style="color:#FF0000;">86.1</span>
</td>
<td id="S4.T4.3.1.7.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T4.3.1.7.7.5.1" class="ltx_text" style="color:#FF0000;">86.1</span>
</td>
<td id="S4.T4.3.1.7.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T4.3.1.7.7.6.1" class="ltx_text" style="color:#FF0000;">81.8</span>
</td>
<td id="S4.T4.3.1.7.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T4.3.1.7.7.7.1" class="ltx_text" style="color:#FF0000;">77.4</span>
</td>
<td id="S4.T4.3.1.7.7.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T4.3.1.7.7.8.1" class="ltx_text" style="color:#FF0000;">79.5</span>
</td>
<td id="S4.T4.3.1.7.7.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T4.3.1.7.7.9.1" class="ltx_text" style="color:#FF0000;">79.1</span>
</td>
<td id="S4.T4.3.1.7.7.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T4.3.1.7.7.10.1" class="ltx_text" style="color:#FF0000;">73.6</span>
</td>
<td id="S4.T4.3.1.7.7.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T4.3.1.7.7.11.1" class="ltx_text" style="color:#FF0000;">80.9</span>
</td>
</tr>
<tr id="S4.T4.3.1.8.8" class="ltx_tr">
<td id="S4.T4.3.1.8.8.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;" colspan="11">Â Â Â Â Â Â Â Â PoseTrack2018</td>
</tr>
<tr id="S4.T4.3.1.9.9" class="ltx_tr">
<td id="S4.T4.3.1.9.9.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 2018</td>
<td id="S4.T4.3.1.9.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Guo
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib64" title="" class="ltx_ref">2018</a>)</cite></td>
<td id="S4.T4.3.1.9.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â ResNet-152</td>
<td id="S4.T4.3.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T4.3.1.9.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T4.3.1.9.9.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T4.3.1.9.9.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 74.5</td>
<td id="S4.T4.3.1.9.9.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T4.3.1.9.9.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â -</td>
<td id="S4.T4.3.1.9.9.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 69.0</td>
<td id="S4.T4.3.1.9.9.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 76.4</td>
</tr>
<tr id="S4.T4.3.1.10.10" class="ltx_tr">
<td id="S4.T4.3.1.10.10.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 2019</td>
<td id="S4.T4.3.1.10.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Bertasius etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2019</a>)</cite></td>
<td id="S4.T4.3.1.10.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â HRNet-W48</td>
<td id="S4.T4.3.1.10.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 78.9</td>
<td id="S4.T4.3.1.10.10.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T4.3.1.10.10.5.1" class="ltx_text" style="color:#0000FF;">84.4</span>
</td>
<td id="S4.T4.3.1.10.10.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T4.3.1.10.10.6.1" class="ltx_text" style="color:#0000FF;">80.9</span>
</td>
<td id="S4.T4.3.1.10.10.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 76.8</td>
<td id="S4.T4.3.1.10.10.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 75.6</td>
<td id="S4.T4.3.1.10.10.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 77.5</td>
<td id="S4.T4.3.1.10.10.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 71.8</td>
<td id="S4.T4.3.1.10.10.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 78.0</td>
</tr>
<tr id="S4.T4.3.1.11.11" class="ltx_tr">
<td id="S4.T4.3.1.11.11.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 2021</td>
<td id="S4.T4.3.1.11.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Liu
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib140" title="" class="ltx_ref">2021a</a>)</cite></td>
<td id="S4.T4.3.1.11.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â HRNet-W48</td>
<td id="S4.T4.3.1.11.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T4.3.1.11.11.4.1" class="ltx_text" style="color:#0000FF;">82.8</span>
</td>
<td id="S4.T4.3.1.11.11.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 84.0</td>
<td id="S4.T4.3.1.11.11.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 80.8</td>
<td id="S4.T4.3.1.11.11.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T4.3.1.11.11.7.1" class="ltx_text" style="color:#0000FF;">77.2</span>
</td>
<td id="S4.T4.3.1.11.11.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T4.3.1.11.11.8.1" class="ltx_text" style="color:#0000FF;">76.1</span>
</td>
<td id="S4.T4.3.1.11.11.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T4.3.1.11.11.9.1" class="ltx_text" style="color:#0000FF;">77.6</span>
</td>
<td id="S4.T4.3.1.11.11.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T4.3.1.11.11.10.1" class="ltx_text" style="color:#0000FF;">72.3</span>
</td>
<td id="S4.T4.3.1.11.11.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T4.3.1.11.11.11.1" class="ltx_text" style="color:#0000FF;">79.0</span>
</td>
</tr>
<tr id="S4.T4.3.1.12.12" class="ltx_tr">
<td id="S4.T4.3.1.12.12.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â 2022</td>
<td id="S4.T4.3.1.12.12.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <cite class="ltx_cite ltx_citemacro_citep">(Liu
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib141" title="" class="ltx_ref">2022</a>)</cite></td>
<td id="S4.T4.3.1.12.12.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â HRNet-W48</td>
<td id="S4.T4.3.1.12.12.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T4.3.1.12.12.4.1" class="ltx_text" style="color:#FF0000;">83.6</span>
</td>
<td id="S4.T4.3.1.12.12.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T4.3.1.12.12.5.1" class="ltx_text" style="color:#FF0000;">84.5</span>
</td>
<td id="S4.T4.3.1.12.12.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T4.3.1.12.12.6.1" class="ltx_text" style="color:#FF0000;">81.4</span>
</td>
<td id="S4.T4.3.1.12.12.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T4.3.1.12.12.7.1" class="ltx_text" style="color:#FF0000;">77.9</span>
</td>
<td id="S4.T4.3.1.12.12.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T4.3.1.12.12.8.1" class="ltx_text" style="color:#FF0000;">76.8</span>
</td>
<td id="S4.T4.3.1.12.12.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T4.3.1.12.12.9.1" class="ltx_text" style="color:#FF0000;">78.3</span>
</td>
<td id="S4.T4.3.1.12.12.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T4.3.1.12.12.10.1" class="ltx_text" style="color:#FF0000;">72.9</span>
</td>
<td id="S4.T4.3.1.12.12.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:25.0pt;padding-right:25.0pt;">Â Â Â Â Â Â Â Â <span id="S4.T4.3.1.12.12.11.1" class="ltx_text" style="color:#FF0000;">79.6</span>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:50%;"><span class="ltx_tag ltx_tag_table">Table 5. </span><span id="S4.T5.3.1" class="ltx_text" style="font-size:160%;">Datasets for 3D HPE.</span></figcaption>
<div id="S4.T5.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:437.3pt;height:168.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-61.7pt,23.8pt) scale(0.78,0.78) ;">
<table id="S4.T5.4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T5.4.1.1.1" class="ltx_tr">
<th id="S4.T5.4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T5.4.1.1.1.1.1" class="ltx_text" style="font-size:50%;">Dataset</span></th>
<th id="S4.T5.4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T5.4.1.1.1.2.1" class="ltx_text" style="font-size:50%;">Year</span></th>
<td id="S4.T5.4.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.1.1.3.1" class="ltx_text" style="font-size:50%;">Capture system</span></td>
<td id="S4.T5.4.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.1.1.4.1" class="ltx_text" style="font-size:50%;">Environment</span></td>
<td id="S4.T5.4.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.1.1.5.1" class="ltx_text" style="font-size:50%;">Size</span></td>
<td id="S4.T5.4.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.1.1.6.1" class="ltx_text" style="font-size:50%;">Single person</span></td>
<td id="S4.T5.4.1.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.1.1.7.1" class="ltx_text" style="font-size:50%;">Multi-person</span></td>
<td id="S4.T5.4.1.1.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.1.1.8.1" class="ltx_text" style="font-size:50%;">Single view</span></td>
<td id="S4.T5.4.1.1.1.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.1.1.9.1" class="ltx_text" style="font-size:50%;">Multi-view</span></td>
</tr>
<tr id="S4.T5.4.1.2.2" class="ltx_tr">
<th id="S4.T5.4.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T5.4.1.2.2.1.1" class="ltx_text" style="font-size:50%;">HumanEva </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T5.4.1.2.2.1.2.1" class="ltx_text" style="font-size:50%;">(</span>Sigal
etÂ al<span class="ltx_text">.</span><span id="S4.T5.4.1.2.2.1.3.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib218" title="" class="ltx_ref">2010</a><span id="S4.T5.4.1.2.2.1.4.3" class="ltx_text" style="font-size:50%;">)</span></cite>
</th>
<th id="S4.T5.4.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T5.4.1.2.2.2.1" class="ltx_text" style="font-size:50%;">2010</span></th>
<td id="S4.T5.4.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.2.2.3.1" class="ltx_text" style="font-size:50%;">Marker-based MoCap</span></td>
<td id="S4.T5.4.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.2.2.4.1" class="ltx_text" style="font-size:50%;">Indoor</span></td>
<td id="S4.T5.4.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.2.2.5.1" class="ltx_text" style="font-size:50%;">6 subject, 7 actions, 40k frames</span></td>
<td id="S4.T5.4.1.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.2.2.6.1" class="ltx_text" style="font-size:50%;">Yes</span></td>
<td id="S4.T5.4.1.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.2.2.7.1" class="ltx_text" style="font-size:50%;">No</span></td>
<td id="S4.T5.4.1.2.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.2.2.8.1" class="ltx_text" style="font-size:50%;">Yes</span></td>
<td id="S4.T5.4.1.2.2.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.2.2.9.1" class="ltx_text" style="font-size:50%;">Yes</span></td>
</tr>
<tr id="S4.T5.4.1.3.3" class="ltx_tr">
<th id="S4.T5.4.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T5.4.1.3.3.1.1" class="ltx_text" style="font-size:50%;">Human3.6M </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T5.4.1.3.3.1.2.1" class="ltx_text" style="font-size:50%;">(</span>Ionescu etÂ al<span class="ltx_text">.</span><span id="S4.T5.4.1.3.3.1.3.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib78" title="" class="ltx_ref">2014</a><span id="S4.T5.4.1.3.3.1.4.3" class="ltx_text" style="font-size:50%;">)</span></cite>
</th>
<th id="S4.T5.4.1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T5.4.1.3.3.2.1" class="ltx_text" style="font-size:50%;">2014</span></th>
<td id="S4.T5.4.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.3.3.3.1" class="ltx_text" style="font-size:50%;">Marker-based MoCap</span></td>
<td id="S4.T5.4.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.3.3.4.1" class="ltx_text" style="font-size:50%;">Indoor</span></td>
<td id="S4.T5.4.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.3.3.5.1" class="ltx_text" style="font-size:50%;">11 subjects, 17 actions, 3.6M frames</span></td>
<td id="S4.T5.4.1.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.3.3.6.1" class="ltx_text" style="font-size:50%;">Yes</span></td>
<td id="S4.T5.4.1.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.3.3.7.1" class="ltx_text" style="font-size:50%;">No</span></td>
<td id="S4.T5.4.1.3.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.3.3.8.1" class="ltx_text" style="font-size:50%;">Yes</span></td>
<td id="S4.T5.4.1.3.3.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.3.3.9.1" class="ltx_text" style="font-size:50%;">Yes</span></td>
</tr>
<tr id="S4.T5.4.1.4.4" class="ltx_tr">
<th id="S4.T5.4.1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T5.4.1.4.4.1.1" class="ltx_text" style="font-size:50%;">CMU Panoptic </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T5.4.1.4.4.1.2.1" class="ltx_text" style="font-size:50%;">(</span>Joo
etÂ al<span class="ltx_text">.</span><span id="S4.T5.4.1.4.4.1.3.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib93" title="" class="ltx_ref">2017</a><span id="S4.T5.4.1.4.4.1.4.3" class="ltx_text" style="font-size:50%;">)</span></cite>
</th>
<th id="S4.T5.4.1.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T5.4.1.4.4.2.1" class="ltx_text" style="font-size:50%;">2016</span></th>
<td id="S4.T5.4.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.4.4.3.1" class="ltx_text" style="font-size:50%;">Marker-less MoCap</span></td>
<td id="S4.T5.4.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.4.4.4.1" class="ltx_text" style="font-size:50%;">Indoor</span></td>
<td id="S4.T5.4.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.4.4.5.1" class="ltx_text" style="font-size:50%;">8 subjects, 1.5M frames</span></td>
<td id="S4.T5.4.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.4.4.6.1" class="ltx_text" style="font-size:50%;">Yes</span></td>
<td id="S4.T5.4.1.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.4.4.7.1" class="ltx_text" style="font-size:50%;">Yes</span></td>
<td id="S4.T5.4.1.4.4.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.4.4.8.1" class="ltx_text" style="font-size:50%;">Yes</span></td>
<td id="S4.T5.4.1.4.4.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.4.4.9.1" class="ltx_text" style="font-size:50%;">Yes</span></td>
</tr>
<tr id="S4.T5.4.1.5.5" class="ltx_tr">
<th id="S4.T5.4.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T5.4.1.5.5.1.1" class="ltx_text" style="font-size:50%;">MPI-INF-3DHP </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T5.4.1.5.5.1.2.1" class="ltx_text" style="font-size:50%;">(</span>Mehta etÂ al<span class="ltx_text">.</span><span id="S4.T5.4.1.5.5.1.3.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib158" title="" class="ltx_ref">2017</a><span id="S4.T5.4.1.5.5.1.4.3" class="ltx_text" style="font-size:50%;">)</span></cite>
</th>
<th id="S4.T5.4.1.5.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T5.4.1.5.5.2.1" class="ltx_text" style="font-size:50%;">2017</span></th>
<td id="S4.T5.4.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.5.5.3.1" class="ltx_text" style="font-size:50%;">Marker-less MoCap</span></td>
<td id="S4.T5.4.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.5.5.4.1" class="ltx_text" style="font-size:50%;">Indoor and outdoor</span></td>
<td id="S4.T5.4.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.5.5.5.1" class="ltx_text" style="font-size:50%;">8 subjects, 8 actions, 1.3M frames</span></td>
<td id="S4.T5.4.1.5.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.5.5.6.1" class="ltx_text" style="font-size:50%;">Yes</span></td>
<td id="S4.T5.4.1.5.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.5.5.7.1" class="ltx_text" style="font-size:50%;">No</span></td>
<td id="S4.T5.4.1.5.5.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.5.5.8.1" class="ltx_text" style="font-size:50%;">Yes</span></td>
<td id="S4.T5.4.1.5.5.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.5.5.9.1" class="ltx_text" style="font-size:50%;">Yes</span></td>
</tr>
<tr id="S4.T5.4.1.6.6" class="ltx_tr">
<th id="S4.T5.4.1.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T5.4.1.6.6.1.1" class="ltx_text" style="font-size:50%;">TotalCapture </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T5.4.1.6.6.1.2.1" class="ltx_text" style="font-size:50%;">(</span>Trumble etÂ al<span class="ltx_text">.</span><span id="S4.T5.4.1.6.6.1.3.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib235" title="" class="ltx_ref">2017</a><span id="S4.T5.4.1.6.6.1.4.3" class="ltx_text" style="font-size:50%;">)</span></cite>
</th>
<th id="S4.T5.4.1.6.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T5.4.1.6.6.2.1" class="ltx_text" style="font-size:50%;">2017</span></th>
<td id="S4.T5.4.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.6.6.3.1" class="ltx_text" style="font-size:50%;">Marker-based MoCap with IMUs</span></td>
<td id="S4.T5.4.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.6.6.4.1" class="ltx_text" style="font-size:50%;">Indoor</span></td>
<td id="S4.T5.4.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.6.6.5.1" class="ltx_text" style="font-size:50%;">5 subjects, 5 actions, 1.9M frames</span></td>
<td id="S4.T5.4.1.6.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.6.6.6.1" class="ltx_text" style="font-size:50%;">Yes</span></td>
<td id="S4.T5.4.1.6.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.6.6.7.1" class="ltx_text" style="font-size:50%;">No</span></td>
<td id="S4.T5.4.1.6.6.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.6.6.8.1" class="ltx_text" style="font-size:50%;">Yes</span></td>
<td id="S4.T5.4.1.6.6.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.6.6.9.1" class="ltx_text" style="font-size:50%;">Yes</span></td>
</tr>
<tr id="S4.T5.4.1.7.7" class="ltx_tr">
<th id="S4.T5.4.1.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T5.4.1.7.7.1.1" class="ltx_text" style="font-size:50%;">3DPW </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T5.4.1.7.7.1.2.1" class="ltx_text" style="font-size:50%;">(</span>von Marcard etÂ al<span class="ltx_text">.</span><span id="S4.T5.4.1.7.7.1.3.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib241" title="" class="ltx_ref">2018</a><span id="S4.T5.4.1.7.7.1.4.3" class="ltx_text" style="font-size:50%;">)</span></cite>
</th>
<th id="S4.T5.4.1.7.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T5.4.1.7.7.2.1" class="ltx_text" style="font-size:50%;">2018</span></th>
<td id="S4.T5.4.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.7.7.3.1" class="ltx_text" style="font-size:50%;">Hand-held cameras with IMUs</span></td>
<td id="S4.T5.4.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.7.7.4.1" class="ltx_text" style="font-size:50%;">Indoor and outdoor</span></td>
<td id="S4.T5.4.1.7.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.7.7.5.1" class="ltx_text" style="font-size:50%;">7 subjects, 51k frames</span></td>
<td id="S4.T5.4.1.7.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.7.7.6.1" class="ltx_text" style="font-size:50%;">Yes</span></td>
<td id="S4.T5.4.1.7.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.7.7.7.1" class="ltx_text" style="font-size:50%;">Yes</span></td>
<td id="S4.T5.4.1.7.7.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.7.7.8.1" class="ltx_text" style="font-size:50%;">Yes</span></td>
<td id="S4.T5.4.1.7.7.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.7.7.9.1" class="ltx_text" style="font-size:50%;">No</span></td>
</tr>
<tr id="S4.T5.4.1.8.8" class="ltx_tr">
<th id="S4.T5.4.1.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T5.4.1.8.8.1.1" class="ltx_text" style="font-size:50%;">MuPoTS-3D </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T5.4.1.8.8.1.2.1" class="ltx_text" style="font-size:50%;">(</span>Mehta etÂ al<span class="ltx_text">.</span><span id="S4.T5.4.1.8.8.1.3.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib160" title="" class="ltx_ref">2018</a><span id="S4.T5.4.1.8.8.1.4.3" class="ltx_text" style="font-size:50%;">)</span></cite>
</th>
<th id="S4.T5.4.1.8.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T5.4.1.8.8.2.1" class="ltx_text" style="font-size:50%;">2018</span></th>
<td id="S4.T5.4.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.8.8.3.1" class="ltx_text" style="font-size:50%;">Marker-less MoCap</span></td>
<td id="S4.T5.4.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.8.8.4.1" class="ltx_text" style="font-size:50%;">Indoor and outdoor</span></td>
<td id="S4.T5.4.1.8.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.8.8.5.1" class="ltx_text" style="font-size:50%;">8 subjects, 8k frames</span></td>
<td id="S4.T5.4.1.8.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.8.8.6.1" class="ltx_text" style="font-size:50%;">Yes</span></td>
<td id="S4.T5.4.1.8.8.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.8.8.7.1" class="ltx_text" style="font-size:50%;">Yes</span></td>
<td id="S4.T5.4.1.8.8.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.8.8.8.1" class="ltx_text" style="font-size:50%;">Yes</span></td>
<td id="S4.T5.4.1.8.8.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.8.8.9.1" class="ltx_text" style="font-size:50%;">Yes</span></td>
</tr>
<tr id="S4.T5.4.1.9.9" class="ltx_tr">
<th id="S4.T5.4.1.9.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T5.4.1.9.9.1.1" class="ltx_text" style="font-size:50%;">AMASS </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T5.4.1.9.9.1.2.1" class="ltx_text" style="font-size:50%;">(</span>Mahmood etÂ al<span class="ltx_text">.</span><span id="S4.T5.4.1.9.9.1.3.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib153" title="" class="ltx_ref">2019</a><span id="S4.T5.4.1.9.9.1.4.3" class="ltx_text" style="font-size:50%;">)</span></cite>
</th>
<th id="S4.T5.4.1.9.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T5.4.1.9.9.2.1" class="ltx_text" style="font-size:50%;">2019</span></th>
<td id="S4.T5.4.1.9.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.9.9.3.1" class="ltx_text" style="font-size:50%;">Marker-based MoCap</span></td>
<td id="S4.T5.4.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.9.9.4.1" class="ltx_text" style="font-size:50%;">Indoor and outdoor</span></td>
<td id="S4.T5.4.1.9.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.9.9.5.1" class="ltx_text" style="font-size:50%;">300 subjects, 9M frames</span></td>
<td id="S4.T5.4.1.9.9.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.9.9.6.1" class="ltx_text" style="font-size:50%;">Yes</span></td>
<td id="S4.T5.4.1.9.9.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.9.9.7.1" class="ltx_text" style="font-size:50%;">No</span></td>
<td id="S4.T5.4.1.9.9.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.9.9.8.1" class="ltx_text" style="font-size:50%;">Yes</span></td>
<td id="S4.T5.4.1.9.9.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.9.9.9.1" class="ltx_text" style="font-size:50%;">Yes</span></td>
</tr>
<tr id="S4.T5.4.1.10.10" class="ltx_tr">
<th id="S4.T5.4.1.10.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T5.4.1.10.10.1.1" class="ltx_text" style="font-size:50%;">NBA2K </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T5.4.1.10.10.1.2.1" class="ltx_text" style="font-size:50%;">(</span>Zhu etÂ al<span class="ltx_text">.</span><span id="S4.T5.4.1.10.10.1.3.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib315" title="" class="ltx_ref">2020</a><span id="S4.T5.4.1.10.10.1.4.3" class="ltx_text" style="font-size:50%;">)</span></cite>
</th>
<th id="S4.T5.4.1.10.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T5.4.1.10.10.2.1" class="ltx_text" style="font-size:50%;">2020</span></th>
<td id="S4.T5.4.1.10.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.10.10.3.1" class="ltx_text" style="font-size:50%;">NBA2K19 game engine</span></td>
<td id="S4.T5.4.1.10.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.10.10.4.1" class="ltx_text" style="font-size:50%;">Indoor</span></td>
<td id="S4.T5.4.1.10.10.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.10.10.5.1" class="ltx_text" style="font-size:50%;">27 subjects, 27k poses</span></td>
<td id="S4.T5.4.1.10.10.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.10.10.6.1" class="ltx_text" style="font-size:50%;">Yes</span></td>
<td id="S4.T5.4.1.10.10.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.10.10.7.1" class="ltx_text" style="font-size:50%;">No</span></td>
<td id="S4.T5.4.1.10.10.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.10.10.8.1" class="ltx_text" style="font-size:50%;">Yes</span></td>
<td id="S4.T5.4.1.10.10.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.10.10.9.1" class="ltx_text" style="font-size:50%;">No</span></td>
</tr>
<tr id="S4.T5.4.1.11.11" class="ltx_tr">
<th id="S4.T5.4.1.11.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T5.4.1.11.11.1.1" class="ltx_text" style="font-size:50%;">GTA-IM </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T5.4.1.11.11.1.2.1" class="ltx_text" style="font-size:50%;">(</span>Cao
etÂ al<span class="ltx_text">.</span><span id="S4.T5.4.1.11.11.1.3.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib16" title="" class="ltx_ref">2020</a><span id="S4.T5.4.1.11.11.1.4.3" class="ltx_text" style="font-size:50%;">)</span></cite>
</th>
<th id="S4.T5.4.1.11.11.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T5.4.1.11.11.2.1" class="ltx_text" style="font-size:50%;">2020</span></th>
<td id="S4.T5.4.1.11.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.11.11.3.1" class="ltx_text" style="font-size:50%;">GTA game engine</span></td>
<td id="S4.T5.4.1.11.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.11.11.4.1" class="ltx_text" style="font-size:50%;">Indoor</span></td>
<td id="S4.T5.4.1.11.11.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.11.11.5.1" class="ltx_text" style="font-size:50%;">1M frames</span></td>
<td id="S4.T5.4.1.11.11.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.11.11.6.1" class="ltx_text" style="font-size:50%;">Yes</span></td>
<td id="S4.T5.4.1.11.11.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.11.11.7.1" class="ltx_text" style="font-size:50%;">No</span></td>
<td id="S4.T5.4.1.11.11.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.11.11.8.1" class="ltx_text" style="font-size:50%;">Yes</span></td>
<td id="S4.T5.4.1.11.11.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.4.1.11.11.9.1" class="ltx_text" style="font-size:50%;">No</span></td>
</tr>
<tr id="S4.T5.4.1.12.12" class="ltx_tr">
<th id="S4.T5.4.1.12.12.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T5.4.1.12.12.1.1" class="ltx_text" style="font-size:50%;">Occlusion-Person </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T5.4.1.12.12.1.2.1" class="ltx_text" style="font-size:50%;">(</span>Zhang
etÂ al<span class="ltx_text">.</span><span id="S4.T5.4.1.12.12.1.3.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib296" title="" class="ltx_ref">2020g</a><span id="S4.T5.4.1.12.12.1.4.3" class="ltx_text" style="font-size:50%;">)</span></cite>
</th>
<th id="S4.T5.4.1.12.12.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T5.4.1.12.12.2.1" class="ltx_text" style="font-size:50%;">2020</span></th>
<td id="S4.T5.4.1.12.12.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T5.4.1.12.12.3.1" class="ltx_text" style="font-size:50%;">Unreal Engine 4 game engine</span></td>
<td id="S4.T5.4.1.12.12.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T5.4.1.12.12.4.1" class="ltx_text" style="font-size:50%;">Indoor</span></td>
<td id="S4.T5.4.1.12.12.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T5.4.1.12.12.5.1" class="ltx_text" style="font-size:50%;">73k frames</span></td>
<td id="S4.T5.4.1.12.12.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T5.4.1.12.12.6.1" class="ltx_text" style="font-size:50%;">Yes</span></td>
<td id="S4.T5.4.1.12.12.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T5.4.1.12.12.7.1" class="ltx_text" style="font-size:50%;">No</span></td>
<td id="S4.T5.4.1.12.12.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T5.4.1.12.12.8.1" class="ltx_text" style="font-size:50%;">Yes</span></td>
<td id="S4.T5.4.1.12.12.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T5.4.1.12.12.9.1" class="ltx_text" style="font-size:50%;">Yes</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Performance Comparison of 2D HPE Methods</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p"><span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_bold">Single-person 2D HPE:</span> Table <a href="#S4.T2" title="Table 2 â€£ 4.2. Evaluation Metrics for 2D HPE â€£ 4. Datasets and Evaluation Metrics â€£ Deep Learning-Based Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the comparison results for different 2D single-person HPE methods on the MPII dataset using PCKh@0.5 measure. Although both heatmap-based and regression-based methods have impressive results, they have their own limitations in 2D single-person HPE. Regression methods can learn a nonlinear mapping from input images to keypoint coordinates with an end-to-end framework, which offers a fast learning paradigm and a sub-pixel level prediction accuracy. However, they usually give sub-optimal solutions <cite class="ltx_cite ltx_citemacro_citep">(Luvizon
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib149" title="" class="ltx_ref">2019</a>)</cite> due to the highly nonlinear problem.
Heatmap-based methods outperform regression-based approaches and are more widely used in 2D HPE<cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib122" title="" class="ltx_ref">2019a</a>)</cite><cite class="ltx_cite ltx_citemacro_citep">(Cai etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2020</a>)</cite><cite class="ltx_cite ltx_citemacro_citep">(Luvizon
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib149" title="" class="ltx_ref">2019</a>)</cite> since (1) the probabilistic prediction of each pixel in a heatmap can improve the accuracy of locating the keypoints, and (2) heatmaps provide richer supervision information by preserving the spatial location information. However, the precision of the predicted keypoints is dependent on the resolution of heatmaps. The computational cost and memory footprint are significantly increased when using high-resolution heatmaps<cite class="ltx_cite ltx_citemacro_citep">(Sun
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib222" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_bold">Multi-person 2D HPE:</span>
Table <a href="#S4.T3" title="Table 3 â€£ 4.2. Evaluation Metrics for 2D HPE â€£ 4. Datasets and Evaluation Metrics â€£ Deep Learning-Based Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> presents the experimental results of different 2D HPE methods on the test-dev set of the COCO dataset, together with a summary of the experiment settings (extra data, backbones in models, input image size) and AP scores for each approach. The comparison experiments highlight the significant results of both top-down and bottom-up methods in multi-person HPE. Presumably, the top-down pipeline yields better results because it first detects each individual from the image using detection methods, then predicts the locations of keypoints using single-person HPE approaches. In this case, the keypoint estimation for each detected person is made easier, as the background is largely removed. But on the other hand, bottom-up methods are generally faster than top-down methods, because they directly detect all the keypoints and group them into individual poses using keypoint association strategies such as affinity linking <cite class="ltx_cite ltx_citemacro_citep">(Cao
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2017</a>)</cite>, associative embedding <cite class="ltx_cite ltx_citemacro_citep">(Newell
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib171" title="" class="ltx_ref">2017</a>)</cite>, and pixel-wise keypoint regression <cite class="ltx_cite ltx_citemacro_citep">(Zhou
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib311" title="" class="ltx_ref">2019</a>)</cite>. Besides the image-based methods listed above, Table <a href="#S4.T4" title="Table 4 â€£ 4.2. Evaluation Metrics for 2D HPE â€£ 4. Datasets and Evaluation Metrics â€£ Deep Learning-Based Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> also illustrates the comparisons of the recent video-based works on PoseTrack2017 and PoseTrack2018 datasets. The detailed results of the test sets are summarized.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4. </span>Datasets for 3D HPE</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">In contrast to numerous 2D human pose datasets with high-quality annotation, acquiring accurate 3D annotation for 3D HPE datasets is a challenging task that requires motion capture systems such as MoCap and wearable IMUs.
Due to the page limit, we only review several widely used large-scale 3D pose datasets for deep learning-based 3D HPE in the following.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p"><a target="_blank" href="http://vision.imar.ro/human3.6m/" title="" class="ltx_ref ltx_href ltx_font_bold">Human3.6M</a> <cite class="ltx_cite ltx_citemacro_citep">(Ionescu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib78" title="" class="ltx_ref">2014</a>)</cite> is the most widely used indoor dataset for 3D HPE from monocular images and videos. There are 11 professional actors performing 17 activities
from 4 different views in an indoor laboratory environment. This dataset contains 3.6 million 3D human poses with 3D ground truth annotation captured by accurate marker-based MoCap systems. Protocol #1 uses images of subjects S1, S5, S6, and S7 for training, and images of subjects S9 and S11 for testing.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p"><a target="_blank" href="http://gvv.mpi-inf.mpg.de/projects/SingleShotMultiPerson/" title="" class="ltx_ref ltx_href ltx_font_bold">MuPoTS-3D</a> <cite class="ltx_cite ltx_citemacro_citep">(Mehta etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib160" title="" class="ltx_ref">2018</a>)</cite> is a multi-person 3D test set and its ground-truth 3D poses were captured by a multi-view marker-less MoCap system containing 20 real-world scenes (5 indoor and 15 outdoor). There are challenging samples with occlusions, drastic illumination changes, and lens flares in some of the outdoor footage. More than 8,000 frames were collected in the 20 sequences by 8 subjects.</p>
</div>
<div id="S4.SS4.p4" class="ltx_para">
<p id="S4.SS4.p4.1" class="ltx_p">Readers are referred to the original references for details about other datasets including <span id="S4.SS4.p4.1.1" class="ltx_text ltx_font_bold">MPI-INF-3DHP<cite class="ltx_cite ltx_citemacro_citep"><span id="S4.SS4.p4.1.1.1.1" class="ltx_text ltx_font_medium">(</span>Mehta etÂ al<span class="ltx_text">.</span><span id="S4.SS4.p4.1.1.2.2.1.1" class="ltx_text ltx_font_medium">, </span><a href="#bib.bib158" title="" class="ltx_ref">2017</a><span id="S4.SS4.p4.1.1.3.3" class="ltx_text ltx_font_medium">)</span></cite></span>, <span id="S4.SS4.p4.1.2" class="ltx_text ltx_font_bold">HumanEva</span> <cite class="ltx_cite ltx_citemacro_citep">(Sigal
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib218" title="" class="ltx_ref">2010</a>)</cite>, <span id="S4.SS4.p4.1.3" class="ltx_text ltx_font_bold">CMU Panoptic Dataset</span> <cite class="ltx_cite ltx_citemacro_citep">(Joo
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib93" title="" class="ltx_ref">2017</a>)</cite>, <span id="S4.SS4.p4.1.4" class="ltx_text ltx_font_bold">TotalCapture</span> <cite class="ltx_cite ltx_citemacro_citep">(Trumble etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib235" title="" class="ltx_ref">2017</a>)</cite>,<span id="S4.SS4.p4.1.5" class="ltx_text ltx_font_bold">MuCo-3DHP Dataset</span> <cite class="ltx_cite ltx_citemacro_citep">(Mehta etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib160" title="" class="ltx_ref">2018</a>)</cite>, <span id="S4.SS4.p4.1.6" class="ltx_text ltx_font_bold">3DPW</span> <cite class="ltx_cite ltx_citemacro_citep">(von Marcard etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib241" title="" class="ltx_ref">2018</a>)</cite>, <span id="S4.SS4.p4.1.7" class="ltx_text ltx_font_bold">AMASS</span> <cite class="ltx_cite ltx_citemacro_citep">(Mahmood etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib153" title="" class="ltx_ref">2019</a>)</cite>, <span id="S4.SS4.p4.1.8" class="ltx_text ltx_font_bold">NBA2K</span> <cite class="ltx_cite ltx_citemacro_citep">(Zhu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib315" title="" class="ltx_ref">2020</a>)</cite>, <span id="S4.SS4.p4.1.9" class="ltx_text ltx_font_bold">GTA-IM</span> <cite class="ltx_cite ltx_citemacro_citep">(Cao
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2020</a>)</cite>, and <span id="S4.SS4.p4.1.10" class="ltx_text ltx_font_bold">Occlusion-Person</span> <cite class="ltx_cite ltx_citemacro_citep">(Zhang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib296" title="" class="ltx_ref">2020g</a>)</cite>. A summary of these datasets is shown in Table <a href="#S4.T5" title="Table 5 â€£ 4.2. Evaluation Metrics for 2D HPE â€£ 4. Datasets and Evaluation Metrics â€£ Deep Learning-Based Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure id="S4.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:50%;"><span class="ltx_tag ltx_tag_table">Table 6. </span><span id="S4.T6.3.1" class="ltx_text" style="font-size:160%;">Comparison of different 3D single-view single-person HPE approaches on the Human3.6M dataset (Protocol 1). In skeleton-only approaches, â€œDirectâ€ indicates the methods directly estimating 3D pose without 2D pose representation. â€œLiftingâ€ denotes the methods lifting the 2D pose representation to the 3D space. The reported total parameters (Params) and FLOPs for 2D-3D lifting methods are computed without including the params and FLOPs of the external 2D pose detector. </span></figcaption>
<div id="S4.T6.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:306.7pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-153.0pt,108.0pt) scale(0.586325160634932,0.586325160634932) ;">
<table id="S4.T6.4.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T6.4.1.1.1" class="ltx_tr">
<td id="S4.T6.4.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="7"><span id="S4.T6.4.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:50%;">Skeleton-only methods</span></td>
<td id="S4.T6.4.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="16"><span id="S4.T6.4.1.1.1.2.1" class="ltx_text" style="font-size:50%;">MPJPE</span></td>
<td id="S4.T6.4.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.1.1.3.1" class="ltx_text" style="font-size:50%;">PA-MPJPE</span></td>
</tr>
<tr id="S4.T6.4.1.2.2" class="ltx_tr">
<td id="S4.T6.4.1.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.2.2.1.1" class="ltx_text" style="font-size:50%;">Approaches</span></td>
<td id="S4.T6.4.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.2.2.2.1" class="ltx_text" style="font-size:50%;">Year</span></td>
<td id="S4.T6.4.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.2.2.3.1" class="ltx_text" style="font-size:50%;">Methods</span></td>
<td id="S4.T6.4.1.2.2.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.2.2.4.1" class="ltx_text" style="font-size:50%;">2D pose detector</span></td>
<td id="S4.T6.4.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.2.2.5.1" class="ltx_text" style="font-size:50%;">Input</span></td>
<td id="S4.T6.4.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.2.2.6.1" class="ltx_text" style="font-size:50%;">Params(M)</span></td>
<td id="S4.T6.4.1.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.2.2.7.1" class="ltx_text" style="font-size:50%;">FLOPs(G)</span></td>
<td id="S4.T6.4.1.2.2.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.2.2.8.1" class="ltx_text" style="font-size:50%;">Dir.</span></td>
<td id="S4.T6.4.1.2.2.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.2.2.9.1" class="ltx_text" style="font-size:50%;">Disc.</span></td>
<td id="S4.T6.4.1.2.2.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.2.2.10.1" class="ltx_text" style="font-size:50%;">Eat.</span></td>
<td id="S4.T6.4.1.2.2.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.2.2.11.1" class="ltx_text" style="font-size:50%;">Greet</span></td>
<td id="S4.T6.4.1.2.2.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.2.2.12.1" class="ltx_text" style="font-size:50%;">Phone</span></td>
<td id="S4.T6.4.1.2.2.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.2.2.13.1" class="ltx_text" style="font-size:50%;">Photo</span></td>
<td id="S4.T6.4.1.2.2.14" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.2.2.14.1" class="ltx_text" style="font-size:50%;">Pose</span></td>
<td id="S4.T6.4.1.2.2.15" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.2.2.15.1" class="ltx_text" style="font-size:50%;">Purch.</span></td>
<td id="S4.T6.4.1.2.2.16" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.2.2.16.1" class="ltx_text" style="font-size:50%;">Sit</span></td>
<td id="S4.T6.4.1.2.2.17" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.2.2.17.1" class="ltx_text" style="font-size:50%;">SitD.</span></td>
<td id="S4.T6.4.1.2.2.18" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.2.2.18.1" class="ltx_text" style="font-size:50%;">Somke</span></td>
<td id="S4.T6.4.1.2.2.19" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.2.2.19.1" class="ltx_text" style="font-size:50%;">Wait</span></td>
<td id="S4.T6.4.1.2.2.20" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.2.2.20.1" class="ltx_text" style="font-size:50%;">WalkD.</span></td>
<td id="S4.T6.4.1.2.2.21" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.2.2.21.1" class="ltx_text" style="font-size:50%;">Walk</span></td>
<td id="S4.T6.4.1.2.2.22" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.2.2.22.1" class="ltx_text" style="font-size:50%;">WalkT.</span></td>
<td id="S4.T6.4.1.2.2.23" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.2.2.23.1" class="ltx_text" style="font-size:50%;">Average</span></td>
<td id="S4.T6.4.1.2.2.24" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.2.2.24.1" class="ltx_text" style="font-size:50%;">Average</span></td>
</tr>
<tr id="S4.T6.4.1.3.3" class="ltx_tr">
<td id="S4.T6.4.1.3.3.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S4.T6.4.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.3.3.2.1" class="ltx_text" style="font-size:50%;">2017</span></td>
<td id="S4.T6.4.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T6.4.1.3.3.3.1.1" class="ltx_text" style="font-size:50%;">(</span>Pavlakos etÂ al<span class="ltx_text">.</span><span id="S4.T6.4.1.3.3.3.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib185" title="" class="ltx_ref">2017a</a><span id="S4.T6.4.1.3.3.3.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></td>
<td id="S4.T6.4.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.3.3.4.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.3.3.5.1" class="ltx_text" style="font-size:50%;">image</span></td>
<td id="S4.T6.4.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.3.3.6.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.3.3.7.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.3.3.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.3.3.8.1" class="ltx_text" style="font-size:50%;">67.4</span></td>
<td id="S4.T6.4.1.3.3.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.3.3.9.1" class="ltx_text" style="font-size:50%;">71.9</span></td>
<td id="S4.T6.4.1.3.3.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.3.3.10.1" class="ltx_text" style="font-size:50%;">66.7</span></td>
<td id="S4.T6.4.1.3.3.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.3.3.11.1" class="ltx_text" style="font-size:50%;">69.1</span></td>
<td id="S4.T6.4.1.3.3.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.3.3.12.1" class="ltx_text" style="font-size:50%;">72.0</span></td>
<td id="S4.T6.4.1.3.3.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.3.3.13.1" class="ltx_text" style="font-size:50%;">77.0</span></td>
<td id="S4.T6.4.1.3.3.14" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.3.3.14.1" class="ltx_text" style="font-size:50%;">65.0</span></td>
<td id="S4.T6.4.1.3.3.15" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.3.3.15.1" class="ltx_text" style="font-size:50%;">68.3</span></td>
<td id="S4.T6.4.1.3.3.16" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.3.3.16.1" class="ltx_text" style="font-size:50%;">83.7</span></td>
<td id="S4.T6.4.1.3.3.17" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.3.3.17.1" class="ltx_text" style="font-size:50%;">96.5</span></td>
<td id="S4.T6.4.1.3.3.18" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.3.3.18.1" class="ltx_text" style="font-size:50%;">71.7</span></td>
<td id="S4.T6.4.1.3.3.19" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.3.3.19.1" class="ltx_text" style="font-size:50%;">65.8</span></td>
<td id="S4.T6.4.1.3.3.20" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.3.3.20.1" class="ltx_text" style="font-size:50%;">74.9</span></td>
<td id="S4.T6.4.1.3.3.21" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.3.3.21.1" class="ltx_text" style="font-size:50%;">59.1</span></td>
<td id="S4.T6.4.1.3.3.22" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.3.3.22.1" class="ltx_text" style="font-size:50%;">63.2</span></td>
<td id="S4.T6.4.1.3.3.23" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.3.3.23.1" class="ltx_text" style="font-size:50%;">71.9</span></td>
<td id="S4.T6.4.1.3.3.24" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.3.3.24.1" class="ltx_text" style="font-size:50%;">51.9</span></td>
</tr>
<tr id="S4.T6.4.1.4.4" class="ltx_tr">
<td id="S4.T6.4.1.4.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.4.4.1.1" class="ltx_text" style="font-size:50%;">Direct</span></td>
<td id="S4.T6.4.1.4.4.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.4.4.2.1" class="ltx_text" style="font-size:50%;">2018</span></td>
<td id="S4.T6.4.1.4.4.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T6.4.1.4.4.3.1.1" class="ltx_text" style="font-size:50%;">(</span>Pavlakos
etÂ al<span class="ltx_text">.</span><span id="S4.T6.4.1.4.4.3.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib184" title="" class="ltx_ref">2018a</a><span id="S4.T6.4.1.4.4.3.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></td>
<td id="S4.T6.4.1.4.4.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.4.4.4.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.4.4.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.4.4.5.1" class="ltx_text" style="font-size:50%;">image</span></td>
<td id="S4.T6.4.1.4.4.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.4.4.6.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.4.4.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.4.4.7.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.4.4.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.4.4.8.1" class="ltx_text" style="font-size:50%;">48.5</span></td>
<td id="S4.T6.4.1.4.4.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.4.4.9.1" class="ltx_text" style="font-size:50%;">54.4</span></td>
<td id="S4.T6.4.1.4.4.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.4.4.10.1" class="ltx_text" style="font-size:50%;">54.4</span></td>
<td id="S4.T6.4.1.4.4.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.4.4.11.1" class="ltx_text" style="font-size:50%;">52.0</span></td>
<td id="S4.T6.4.1.4.4.12" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.4.4.12.1" class="ltx_text" style="font-size:50%;">59.4</span></td>
<td id="S4.T6.4.1.4.4.13" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.4.4.13.1" class="ltx_text" style="font-size:50%;">65.3</span></td>
<td id="S4.T6.4.1.4.4.14" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.4.4.14.1" class="ltx_text" style="font-size:50%;">49.9</span></td>
<td id="S4.T6.4.1.4.4.15" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.4.4.15.1" class="ltx_text" style="font-size:50%;">52.9</span></td>
<td id="S4.T6.4.1.4.4.16" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.4.4.16.1" class="ltx_text" style="font-size:50%;">65.8</span></td>
<td id="S4.T6.4.1.4.4.17" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.4.4.17.1" class="ltx_text" style="font-size:50%;">71.1</span></td>
<td id="S4.T6.4.1.4.4.18" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.4.4.18.1" class="ltx_text" style="font-size:50%;">56.6</span></td>
<td id="S4.T6.4.1.4.4.19" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.4.4.19.1" class="ltx_text" style="font-size:50%;">52.9</span></td>
<td id="S4.T6.4.1.4.4.20" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.4.4.20.1" class="ltx_text" style="font-size:50%;">60.9</span></td>
<td id="S4.T6.4.1.4.4.21" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.4.4.21.1" class="ltx_text" style="font-size:50%;">44.7</span></td>
<td id="S4.T6.4.1.4.4.22" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.4.4.22.1" class="ltx_text" style="font-size:50%;">47.8</span></td>
<td id="S4.T6.4.1.4.4.23" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.4.4.23.1" class="ltx_text" style="font-size:50%;">56.2</span></td>
<td id="S4.T6.4.1.4.4.24" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.4.4.24.1" class="ltx_text" style="font-size:50%;">41.8</span></td>
</tr>
<tr id="S4.T6.4.1.5.5" class="ltx_tr">
<td id="S4.T6.4.1.5.5.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S4.T6.4.1.5.5.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.5.5.2.1" class="ltx_text" style="font-size:50%;">2019</span></td>
<td id="S4.T6.4.1.5.5.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T6.4.1.5.5.3.1.1" class="ltx_text" style="font-size:50%;">(</span>Li and Lee<span id="S4.T6.4.1.5.5.3.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib112" title="" class="ltx_ref">2019</a><span id="S4.T6.4.1.5.5.3.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></td>
<td id="S4.T6.4.1.5.5.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.5.5.4.1" class="ltx_text" style="font-size:50%;">Hourglass</span></td>
<td id="S4.T6.4.1.5.5.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.5.5.5.1" class="ltx_text" style="font-size:50%;">image</span></td>
<td id="S4.T6.4.1.5.5.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.5.5.6.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.5.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.5.5.7.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.5.5.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.5.5.8.1" class="ltx_text" style="font-size:50%;">43.8</span></td>
<td id="S4.T6.4.1.5.5.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.5.5.9.1" class="ltx_text" style="font-size:50%;">48.6</span></td>
<td id="S4.T6.4.1.5.5.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.5.5.10.1" class="ltx_text" style="font-size:50%;">49.1</span></td>
<td id="S4.T6.4.1.5.5.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.5.5.11.1" class="ltx_text" style="font-size:50%;">49.8</span></td>
<td id="S4.T6.4.1.5.5.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.5.5.12.1" class="ltx_text" style="font-size:50%;">57.6</span></td>
<td id="S4.T6.4.1.5.5.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.5.5.13.1" class="ltx_text" style="font-size:50%;">61.5</span></td>
<td id="S4.T6.4.1.5.5.14" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.5.5.14.1" class="ltx_text" style="font-size:50%;">45.9</span></td>
<td id="S4.T6.4.1.5.5.15" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.5.5.15.1" class="ltx_text" style="font-size:50%;">48.3</span></td>
<td id="S4.T6.4.1.5.5.16" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.5.5.16.1" class="ltx_text" style="font-size:50%;">62.0</span></td>
<td id="S4.T6.4.1.5.5.17" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.5.5.17.1" class="ltx_text" style="font-size:50%;">73.4</span></td>
<td id="S4.T6.4.1.5.5.18" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.5.5.18.1" class="ltx_text" style="font-size:50%;">54.8</span></td>
<td id="S4.T6.4.1.5.5.19" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.5.5.19.1" class="ltx_text" style="font-size:50%;">50.6</span></td>
<td id="S4.T6.4.1.5.5.20" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.5.5.20.1" class="ltx_text" style="font-size:50%;">56.0</span></td>
<td id="S4.T6.4.1.5.5.21" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.5.5.21.1" class="ltx_text" style="font-size:50%;">43.4</span></td>
<td id="S4.T6.4.1.5.5.22" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.5.5.22.1" class="ltx_text" style="font-size:50%;">45.5</span></td>
<td id="S4.T6.4.1.5.5.23" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.5.5.23.1" class="ltx_text" style="font-size:50%;">52.7</span></td>
<td id="S4.T6.4.1.5.5.24" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.5.5.24.1" class="ltx_text" style="font-size:50%;">42.6</span></td>
</tr>
<tr id="S4.T6.4.1.6.6" class="ltx_tr">
<td id="S4.T6.4.1.6.6.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S4.T6.4.1.6.6.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.6.6.2.1" class="ltx_text" style="font-size:50%;">2019</span></td>
<td id="S4.T6.4.1.6.6.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T6.4.1.6.6.3.1.1" class="ltx_text" style="font-size:50%;">(</span>Zhao
etÂ al<span class="ltx_text">.</span><span id="S4.T6.4.1.6.6.3.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib297" title="" class="ltx_ref">2019b</a><span id="S4.T6.4.1.6.6.3.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></td>
<td id="S4.T6.4.1.6.6.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.6.6.4.1" class="ltx_text" style="font-size:50%;">own design</span></td>
<td id="S4.T6.4.1.6.6.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.6.6.5.1" class="ltx_text" style="font-size:50%;">image</span></td>
<td id="S4.T6.4.1.6.6.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.6.6.6.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.6.6.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.6.6.7.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.6.6.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.6.6.8.1" class="ltx_text" style="font-size:50%;">47.3</span></td>
<td id="S4.T6.4.1.6.6.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.6.6.9.1" class="ltx_text" style="font-size:50%;">60.7</span></td>
<td id="S4.T6.4.1.6.6.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.6.6.10.1" class="ltx_text" style="font-size:50%;">51.4</span></td>
<td id="S4.T6.4.1.6.6.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.6.6.11.1" class="ltx_text" style="font-size:50%;">60.5</span></td>
<td id="S4.T6.4.1.6.6.12" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.6.6.12.1" class="ltx_text" style="font-size:50%;">61.1</span></td>
<td id="S4.T6.4.1.6.6.13" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.6.6.13.1" class="ltx_text" style="font-size:50%;color:#0000FF;">49.9</span></td>
<td id="S4.T6.4.1.6.6.14" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.6.6.14.1" class="ltx_text" style="font-size:50%;">47.3</span></td>
<td id="S4.T6.4.1.6.6.15" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.6.6.15.1" class="ltx_text" style="font-size:50%;">68.1</span></td>
<td id="S4.T6.4.1.6.6.16" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.6.6.16.1" class="ltx_text" style="font-size:50%;">86.2</span></td>
<td id="S4.T6.4.1.6.6.17" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.6.6.17.1" class="ltx_text" style="font-size:50%;color:#FF0000;">55.0</span></td>
<td id="S4.T6.4.1.6.6.18" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.6.6.18.1" class="ltx_text" style="font-size:50%;">67.8</span></td>
<td id="S4.T6.4.1.6.6.19" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.6.6.19.1" class="ltx_text" style="font-size:50%;">61.0</span></td>
<td id="S4.T6.4.1.6.6.20" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.6.6.20.1" class="ltx_text" style="font-size:50%;">42.1</span></td>
<td id="S4.T6.4.1.6.6.21" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.6.6.21.1" class="ltx_text" style="font-size:50%;">60.6</span></td>
<td id="S4.T6.4.1.6.6.22" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.6.6.22.1" class="ltx_text" style="font-size:50%;">45.3</span></td>
<td id="S4.T6.4.1.6.6.23" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.6.6.23.1" class="ltx_text" style="font-size:50%;">57.6</span></td>
<td id="S4.T6.4.1.6.6.24" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.6.6.24.1" class="ltx_text" style="font-size:50%;">-</span></td>
</tr>
<tr id="S4.T6.4.1.7.7" class="ltx_tr">
<td id="S4.T6.4.1.7.7.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S4.T6.4.1.7.7.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.7.7.2.1" class="ltx_text" style="font-size:50%;">2021</span></td>
<td id="S4.T6.4.1.7.7.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T6.4.1.7.7.3.1.1" class="ltx_text" style="font-size:50%;">(</span>Zou and Tang<span id="S4.T6.4.1.7.7.3.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib317" title="" class="ltx_ref">2021</a><span id="S4.T6.4.1.7.7.3.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></td>
<td id="S4.T6.4.1.7.7.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.7.7.4.1" class="ltx_text" style="font-size:50%;">CPN</span></td>
<td id="S4.T6.4.1.7.7.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.7.7.5.1" class="ltx_text" style="font-size:50%;">image</span></td>
<td id="S4.T6.4.1.7.7.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.7.7.6.1" class="ltx_text" style="font-size:50%;">0.3</span></td>
<td id="S4.T6.4.1.7.7.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.7.7.7.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.7.7.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.7.7.8.1" class="ltx_text" style="font-size:50%;">45.4</span></td>
<td id="S4.T6.4.1.7.7.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.7.7.9.1" class="ltx_text" style="font-size:50%;">49.2</span></td>
<td id="S4.T6.4.1.7.7.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.7.7.10.1" class="ltx_text" style="font-size:50%;">45.7</span></td>
<td id="S4.T6.4.1.7.7.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.7.7.11.1" class="ltx_text" style="font-size:50%;">49.4</span></td>
<td id="S4.T6.4.1.7.7.12" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.7.7.12.1" class="ltx_text" style="font-size:50%;">50.4</span></td>
<td id="S4.T6.4.1.7.7.13" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.7.7.13.1" class="ltx_text" style="font-size:50%;">58.2</span></td>
<td id="S4.T6.4.1.7.7.14" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.7.7.14.1" class="ltx_text" style="font-size:50%;">47.9</span></td>
<td id="S4.T6.4.1.7.7.15" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.7.7.15.1" class="ltx_text" style="font-size:50%;">46.0</span></td>
<td id="S4.T6.4.1.7.7.16" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.7.7.16.1" class="ltx_text" style="font-size:50%;">57.5</span></td>
<td id="S4.T6.4.1.7.7.17" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.7.7.17.1" class="ltx_text" style="font-size:50%;">63.0</span></td>
<td id="S4.T6.4.1.7.7.18" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.7.7.18.1" class="ltx_text" style="font-size:50%;">49.7</span></td>
<td id="S4.T6.4.1.7.7.19" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.7.7.19.1" class="ltx_text" style="font-size:50%;">46.6</span></td>
<td id="S4.T6.4.1.7.7.20" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.7.7.20.1" class="ltx_text" style="font-size:50%;">52.2</span></td>
<td id="S4.T6.4.1.7.7.21" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.7.7.21.1" class="ltx_text" style="font-size:50%;">38.9</span></td>
<td id="S4.T6.4.1.7.7.22" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.7.7.22.1" class="ltx_text" style="font-size:50%;">40.8</span></td>
<td id="S4.T6.4.1.7.7.23" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.7.7.23.1" class="ltx_text" style="font-size:50%;">49.4</span></td>
<td id="S4.T6.4.1.7.7.24" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.7.7.24.1" class="ltx_text" style="font-size:50%;">39.1</span></td>
</tr>
<tr id="S4.T6.4.1.8.8" class="ltx_tr">
<td id="S4.T6.4.1.8.8.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S4.T6.4.1.8.8.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.8.8.2.1" class="ltx_text" style="font-size:50%;">2019</span></td>
<td id="S4.T6.4.1.8.8.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T6.4.1.8.8.3.1.1" class="ltx_text" style="font-size:50%;">(</span>Pavllo etÂ al<span class="ltx_text">.</span><span id="S4.T6.4.1.8.8.3.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib188" title="" class="ltx_ref">2019</a><span id="S4.T6.4.1.8.8.3.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></td>
<td id="S4.T6.4.1.8.8.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.8.8.4.1" class="ltx_text" style="font-size:50%;">CPN</span></td>
<td id="S4.T6.4.1.8.8.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.8.8.5.1" class="ltx_text" style="font-size:50%;">Video</span></td>
<td id="S4.T6.4.1.8.8.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.8.8.6.1" class="ltx_text" style="font-size:50%;">17</span></td>
<td id="S4.T6.4.1.8.8.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.8.8.7.1" class="ltx_text" style="font-size:50%;">0.03</span></td>
<td id="S4.T6.4.1.8.8.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.8.8.8.1" class="ltx_text" style="font-size:50%;">45.2</span></td>
<td id="S4.T6.4.1.8.8.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.8.8.9.1" class="ltx_text" style="font-size:50%;">46.7</span></td>
<td id="S4.T6.4.1.8.8.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.8.8.10.1" class="ltx_text" style="font-size:50%;">43.3</span></td>
<td id="S4.T6.4.1.8.8.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.8.8.11.1" class="ltx_text" style="font-size:50%;">45.6</span></td>
<td id="S4.T6.4.1.8.8.12" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.8.8.12.1" class="ltx_text" style="font-size:50%;">48.1</span></td>
<td id="S4.T6.4.1.8.8.13" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.8.8.13.1" class="ltx_text" style="font-size:50%;">55.1</span></td>
<td id="S4.T6.4.1.8.8.14" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.8.8.14.1" class="ltx_text" style="font-size:50%;">44.6</span></td>
<td id="S4.T6.4.1.8.8.15" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.8.8.15.1" class="ltx_text" style="font-size:50%;">44.3</span></td>
<td id="S4.T6.4.1.8.8.16" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.8.8.16.1" class="ltx_text" style="font-size:50%;">57.3</span></td>
<td id="S4.T6.4.1.8.8.17" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.8.8.17.1" class="ltx_text" style="font-size:50%;">65.8</span></td>
<td id="S4.T6.4.1.8.8.18" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.8.8.18.1" class="ltx_text" style="font-size:50%;">47.1</span></td>
<td id="S4.T6.4.1.8.8.19" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.8.8.19.1" class="ltx_text" style="font-size:50%;">44.0</span></td>
<td id="S4.T6.4.1.8.8.20" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.8.8.20.1" class="ltx_text" style="font-size:50%;">49.0</span></td>
<td id="S4.T6.4.1.8.8.21" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.8.8.21.1" class="ltx_text" style="font-size:50%;">32.8</span></td>
<td id="S4.T6.4.1.8.8.22" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.8.8.22.1" class="ltx_text" style="font-size:50%;">33.9</span></td>
<td id="S4.T6.4.1.8.8.23" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.8.8.23.1" class="ltx_text" style="font-size:50%;">46.8</span></td>
<td id="S4.T6.4.1.8.8.24" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.8.8.24.1" class="ltx_text" style="font-size:50%;">36.5</span></td>
</tr>
<tr id="S4.T6.4.1.9.9" class="ltx_tr">
<td id="S4.T6.4.1.9.9.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S4.T6.4.1.9.9.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.9.9.2.1" class="ltx_text" style="font-size:50%;">2019</span></td>
<td id="S4.T6.4.1.9.9.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T6.4.1.9.9.3.1.1" class="ltx_text" style="font-size:50%;">(</span>Cai etÂ al<span class="ltx_text">.</span><span id="S4.T6.4.1.9.9.3.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib14" title="" class="ltx_ref">2019</a><span id="S4.T6.4.1.9.9.3.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></td>
<td id="S4.T6.4.1.9.9.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.9.9.4.1" class="ltx_text" style="font-size:50%;">CPN</span></td>
<td id="S4.T6.4.1.9.9.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.9.9.5.1" class="ltx_text" style="font-size:50%;">Video</span></td>
<td id="S4.T6.4.1.9.9.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.9.9.6.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.9.9.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.9.9.7.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.9.9.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.9.9.8.1" class="ltx_text" style="font-size:50%;">44.6</span></td>
<td id="S4.T6.4.1.9.9.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.9.9.9.1" class="ltx_text" style="font-size:50%;">47.4</span></td>
<td id="S4.T6.4.1.9.9.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.9.9.10.1" class="ltx_text" style="font-size:50%;">45.6</span></td>
<td id="S4.T6.4.1.9.9.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.9.9.11.1" class="ltx_text" style="font-size:50%;">48.8</span></td>
<td id="S4.T6.4.1.9.9.12" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.9.9.12.1" class="ltx_text" style="font-size:50%;">50.8</span></td>
<td id="S4.T6.4.1.9.9.13" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.9.9.13.1" class="ltx_text" style="font-size:50%;">59.0</span></td>
<td id="S4.T6.4.1.9.9.14" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.9.9.14.1" class="ltx_text" style="font-size:50%;">47.2</span></td>
<td id="S4.T6.4.1.9.9.15" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.9.9.15.1" class="ltx_text" style="font-size:50%;">43.9</span></td>
<td id="S4.T6.4.1.9.9.16" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.9.9.16.1" class="ltx_text" style="font-size:50%;">57.9</span></td>
<td id="S4.T6.4.1.9.9.17" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.9.9.17.1" class="ltx_text" style="font-size:50%;">61.9</span></td>
<td id="S4.T6.4.1.9.9.18" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.9.9.18.1" class="ltx_text" style="font-size:50%;">49.7</span></td>
<td id="S4.T6.4.1.9.9.19" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.9.9.19.1" class="ltx_text" style="font-size:50%;">46.6</span></td>
<td id="S4.T6.4.1.9.9.20" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.9.9.20.1" class="ltx_text" style="font-size:50%;">51.3</span></td>
<td id="S4.T6.4.1.9.9.21" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.9.9.21.1" class="ltx_text" style="font-size:50%;">37.1</span></td>
<td id="S4.T6.4.1.9.9.22" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.9.9.22.1" class="ltx_text" style="font-size:50%;">39.4</span></td>
<td id="S4.T6.4.1.9.9.23" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.9.9.23.1" class="ltx_text" style="font-size:50%;">48.8</span></td>
<td id="S4.T6.4.1.9.9.24" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.9.9.24.1" class="ltx_text" style="font-size:50%;">39.0</span></td>
</tr>
<tr id="S4.T6.4.1.10.10" class="ltx_tr">
<td id="S4.T6.4.1.10.10.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S4.T6.4.1.10.10.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.10.10.2.1" class="ltx_text" style="font-size:50%;">2020</span></td>
<td id="S4.T6.4.1.10.10.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T6.4.1.10.10.3.1.1" class="ltx_text" style="font-size:50%;">(</span>Liu
etÂ al<span class="ltx_text">.</span><span id="S4.T6.4.1.10.10.3.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib139" title="" class="ltx_ref">2020c</a><span id="S4.T6.4.1.10.10.3.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></td>
<td id="S4.T6.4.1.10.10.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.10.10.4.1" class="ltx_text" style="font-size:50%;">CPN</span></td>
<td id="S4.T6.4.1.10.10.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.10.10.5.1" class="ltx_text" style="font-size:50%;">Video</span></td>
<td id="S4.T6.4.1.10.10.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.10.10.6.1" class="ltx_text" style="font-size:50%;">11</span></td>
<td id="S4.T6.4.1.10.10.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.10.10.7.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.10.10.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.10.10.8.1" class="ltx_text" style="font-size:50%;">41.8</span></td>
<td id="S4.T6.4.1.10.10.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.10.10.9.1" class="ltx_text" style="font-size:50%;">44.8</span></td>
<td id="S4.T6.4.1.10.10.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.10.10.10.1" class="ltx_text" style="font-size:50%;">41.1</span></td>
<td id="S4.T6.4.1.10.10.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.10.10.11.1" class="ltx_text" style="font-size:50%;">44.9</span></td>
<td id="S4.T6.4.1.10.10.12" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.10.10.12.1" class="ltx_text" style="font-size:50%;">47.4</span></td>
<td id="S4.T6.4.1.10.10.13" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.10.10.13.1" class="ltx_text" style="font-size:50%;">54.1</span></td>
<td id="S4.T6.4.1.10.10.14" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.10.10.14.1" class="ltx_text" style="font-size:50%;">43.4</span></td>
<td id="S4.T6.4.1.10.10.15" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.10.10.15.1" class="ltx_text" style="font-size:50%;">42.2</span></td>
<td id="S4.T6.4.1.10.10.16" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.10.10.16.1" class="ltx_text" style="font-size:50%;">56.2</span></td>
<td id="S4.T6.4.1.10.10.17" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.10.10.17.1" class="ltx_text" style="font-size:50%;">63.6</span></td>
<td id="S4.T6.4.1.10.10.18" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.10.10.18.1" class="ltx_text" style="font-size:50%;">45.3</span></td>
<td id="S4.T6.4.1.10.10.19" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.10.10.19.1" class="ltx_text" style="font-size:50%;">43.5</span></td>
<td id="S4.T6.4.1.10.10.20" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.10.10.20.1" class="ltx_text" style="font-size:50%;">45.3</span></td>
<td id="S4.T6.4.1.10.10.21" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.10.10.21.1" class="ltx_text" style="font-size:50%;color:#0000FF;">31.3</span></td>
<td id="S4.T6.4.1.10.10.22" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.10.10.22.1" class="ltx_text" style="font-size:50%;">32.2</span></td>
<td id="S4.T6.4.1.10.10.23" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.10.10.23.1" class="ltx_text" style="font-size:50%;">45.1</span></td>
<td id="S4.T6.4.1.10.10.24" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.10.10.24.1" class="ltx_text" style="font-size:50%;">35.6</span></td>
</tr>
<tr id="S4.T6.4.1.11.11" class="ltx_tr">
<td id="S4.T6.4.1.11.11.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S4.T6.4.1.11.11.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.11.11.2.1" class="ltx_text" style="font-size:50%;">2020</span></td>
<td id="S4.T6.4.1.11.11.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T6.4.1.11.11.3.1.1" class="ltx_text" style="font-size:50%;">(</span>Zeng
etÂ al<span class="ltx_text">.</span><span id="S4.T6.4.1.11.11.3.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib282" title="" class="ltx_ref">2020</a><span id="S4.T6.4.1.11.11.3.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></td>
<td id="S4.T6.4.1.11.11.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.11.11.4.1" class="ltx_text" style="font-size:50%;">CPN</span></td>
<td id="S4.T6.4.1.11.11.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.11.11.5.1" class="ltx_text" style="font-size:50%;">Video</span></td>
<td id="S4.T6.4.1.11.11.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.11.11.6.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.11.11.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.11.11.7.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.11.11.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.11.11.8.1" class="ltx_text" style="font-size:50%;">46.6</span></td>
<td id="S4.T6.4.1.11.11.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.11.11.9.1" class="ltx_text" style="font-size:50%;">47.1</span></td>
<td id="S4.T6.4.1.11.11.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.11.11.10.1" class="ltx_text" style="font-size:50%;">43.9</span></td>
<td id="S4.T6.4.1.11.11.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.11.11.11.1" class="ltx_text" style="font-size:50%;">41.6</span></td>
<td id="S4.T6.4.1.11.11.12" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.11.11.12.1" class="ltx_text" style="font-size:50%;">45.8</span></td>
<td id="S4.T6.4.1.11.11.13" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.11.11.13.1" class="ltx_text" style="font-size:50%;color:#FF0000;">49.6</span></td>
<td id="S4.T6.4.1.11.11.14" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.11.11.14.1" class="ltx_text" style="font-size:50%;">46.5</span></td>
<td id="S4.T6.4.1.11.11.15" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.11.11.15.1" class="ltx_text" style="font-size:50%;color:#0000FF;">40.0</span></td>
<td id="S4.T6.4.1.11.11.16" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.11.11.16.1" class="ltx_text" style="font-size:50%;">53.4</span></td>
<td id="S4.T6.4.1.11.11.17" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.11.11.17.1" class="ltx_text" style="font-size:50%;">61.1</span></td>
<td id="S4.T6.4.1.11.11.18" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.11.11.18.1" class="ltx_text" style="font-size:50%;">46.1</span></td>
<td id="S4.T6.4.1.11.11.19" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.11.11.19.1" class="ltx_text" style="font-size:50%;">42.6</span></td>
<td id="S4.T6.4.1.11.11.20" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.11.11.20.1" class="ltx_text" style="font-size:50%;">43.1</span></td>
<td id="S4.T6.4.1.11.11.21" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.11.11.21.1" class="ltx_text" style="font-size:50%;">31.5</span></td>
<td id="S4.T6.4.1.11.11.22" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.11.11.22.1" class="ltx_text" style="font-size:50%;">32.6</span></td>
<td id="S4.T6.4.1.11.11.23" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.11.11.23.1" class="ltx_text" style="font-size:50%;">44.8</span></td>
<td id="S4.T6.4.1.11.11.24" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.11.11.24.1" class="ltx_text" style="font-size:50%;">-</span></td>
</tr>
<tr id="S4.T6.4.1.12.12" class="ltx_tr">
<td id="S4.T6.4.1.12.12.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S4.T6.4.1.12.12.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.12.12.2.1" class="ltx_text" style="font-size:50%;">2020</span></td>
<td id="S4.T6.4.1.12.12.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T6.4.1.12.12.3.1.1" class="ltx_text" style="font-size:50%;">(</span>Wang
etÂ al<span class="ltx_text">.</span><span id="S4.T6.4.1.12.12.3.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib249" title="" class="ltx_ref">2020d</a><span id="S4.T6.4.1.12.12.3.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></td>
<td id="S4.T6.4.1.12.12.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.12.12.4.1" class="ltx_text" style="font-size:50%;">HRNet</span></td>
<td id="S4.T6.4.1.12.12.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.12.12.5.1" class="ltx_text" style="font-size:50%;">Video</span></td>
<td id="S4.T6.4.1.12.12.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.12.12.6.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.12.12.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.12.12.7.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.12.12.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.12.12.8.1" class="ltx_text" style="font-size:50%;color:#0000FF;">38.2</span></td>
<td id="S4.T6.4.1.12.12.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.12.12.9.1" class="ltx_text" style="font-size:50%;color:#0000FF;">41.0</span></td>
<td id="S4.T6.4.1.12.12.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.12.12.10.1" class="ltx_text" style="font-size:50%;">45.9</span></td>
<td id="S4.T6.4.1.12.12.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.12.12.11.1" class="ltx_text" style="font-size:50%;color:#FF0000;">39.7</span></td>
<td id="S4.T6.4.1.12.12.12" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.12.12.12.1" class="ltx_text" style="font-size:50%;color:#FF0000;">41.4</span></td>
<td id="S4.T6.4.1.12.12.13" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.12.12.13.1" class="ltx_text" style="font-size:50%;">51.4</span></td>
<td id="S4.T6.4.1.12.12.14" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.12.12.14.1" class="ltx_text" style="font-size:50%;">41.6</span></td>
<td id="S4.T6.4.1.12.12.15" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.12.12.15.1" class="ltx_text" style="font-size:50%;">41.4</span></td>
<td id="S4.T6.4.1.12.12.16" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.12.12.16.1" class="ltx_text" style="font-size:50%;color:#0000FF;">52.0</span></td>
<td id="S4.T6.4.1.12.12.17" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.12.12.17.1" class="ltx_text" style="font-size:50%;color:#0000FF;">57.4</span></td>
<td id="S4.T6.4.1.12.12.18" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.12.12.18.1" class="ltx_text" style="font-size:50%;color:#FF0000;">41.8</span></td>
<td id="S4.T6.4.1.12.12.19" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.12.12.19.1" class="ltx_text" style="font-size:50%;">44.4</span></td>
<td id="S4.T6.4.1.12.12.20" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.12.12.20.1" class="ltx_text" style="font-size:50%;color:#0000FF;">41.6</span></td>
<td id="S4.T6.4.1.12.12.21" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.12.12.21.1" class="ltx_text" style="font-size:50%;">33.1</span></td>
<td id="S4.T6.4.1.12.12.22" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.12.12.22.1" class="ltx_text" style="font-size:50%;color:#0000FF;">30.0</span></td>
<td id="S4.T6.4.1.12.12.23" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.12.12.23.1" class="ltx_text" style="font-size:50%;color:#0000FF;">42.6</span></td>
<td id="S4.T6.4.1.12.12.24" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.12.12.24.1" class="ltx_text" style="font-size:50%;color:#0000FF;">32.7</span></td>
</tr>
<tr id="S4.T6.4.1.13.13" class="ltx_tr">
<td id="S4.T6.4.1.13.13.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S4.T6.4.1.13.13.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.13.13.2.1" class="ltx_text" style="font-size:50%;">2020</span></td>
<td id="S4.T6.4.1.13.13.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T6.4.1.13.13.3.1.1" class="ltx_text" style="font-size:50%;">(</span>Chen
etÂ al<span class="ltx_text">.</span><span id="S4.T6.4.1.13.13.3.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib24" title="" class="ltx_ref">2021</a><span id="S4.T6.4.1.13.13.3.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></td>
<td id="S4.T6.4.1.13.13.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.13.13.4.1" class="ltx_text" style="font-size:50%;">CPN</span></td>
<td id="S4.T6.4.1.13.13.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.13.13.5.1" class="ltx_text" style="font-size:50%;">Video</span></td>
<td id="S4.T6.4.1.13.13.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.13.13.6.1" class="ltx_text" style="font-size:50%;">59</span></td>
<td id="S4.T6.4.1.13.13.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.13.13.7.1" class="ltx_text" style="font-size:50%;">0.1</span></td>
<td id="S4.T6.4.1.13.13.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.13.13.8.1" class="ltx_text" style="font-size:50%;">41.4</span></td>
<td id="S4.T6.4.1.13.13.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.13.13.9.1" class="ltx_text" style="font-size:50%;">43.5</span></td>
<td id="S4.T6.4.1.13.13.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.13.13.10.1" class="ltx_text" style="font-size:50%;">40.1</span></td>
<td id="S4.T6.4.1.13.13.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.13.13.11.1" class="ltx_text" style="font-size:50%;">42.9</span></td>
<td id="S4.T6.4.1.13.13.12" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.13.13.12.1" class="ltx_text" style="font-size:50%;">46.6</span></td>
<td id="S4.T6.4.1.13.13.13" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.13.13.13.1" class="ltx_text" style="font-size:50%;">51.9</span></td>
<td id="S4.T6.4.1.13.13.14" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.13.13.14.1" class="ltx_text" style="font-size:50%;">41.7</span></td>
<td id="S4.T6.4.1.13.13.15" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.13.13.15.1" class="ltx_text" style="font-size:50%;">42.3</span></td>
<td id="S4.T6.4.1.13.13.16" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.13.13.16.1" class="ltx_text" style="font-size:50%;">53.9</span></td>
<td id="S4.T6.4.1.13.13.17" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.13.13.17.1" class="ltx_text" style="font-size:50%;">60.2</span></td>
<td id="S4.T6.4.1.13.13.18" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.13.13.18.1" class="ltx_text" style="font-size:50%;">45.4</span></td>
<td id="S4.T6.4.1.13.13.19" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.13.13.19.1" class="ltx_text" style="font-size:50%;">41.7</span></td>
<td id="S4.T6.4.1.13.13.20" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.13.13.20.1" class="ltx_text" style="font-size:50%;">46.0</span></td>
<td id="S4.T6.4.1.13.13.21" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.13.13.21.1" class="ltx_text" style="font-size:50%;">31.5</span></td>
<td id="S4.T6.4.1.13.13.22" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.13.13.22.1" class="ltx_text" style="font-size:50%;">32.7</span></td>
<td id="S4.T6.4.1.13.13.23" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.13.13.23.1" class="ltx_text" style="font-size:50%;">44.1</span></td>
<td id="S4.T6.4.1.13.13.24" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.13.13.24.1" class="ltx_text" style="font-size:50%;">35.0</span></td>
</tr>
<tr id="S4.T6.4.1.14.14" class="ltx_tr">
<td id="S4.T6.4.1.14.14.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S4.T6.4.1.14.14.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.14.14.2.1" class="ltx_text" style="font-size:50%;">2021</span></td>
<td id="S4.T6.4.1.14.14.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T6.4.1.14.14.3.1.1" class="ltx_text" style="font-size:50%;">(</span>Zheng etÂ al<span class="ltx_text">.</span><span id="S4.T6.4.1.14.14.3.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib305" title="" class="ltx_ref">2021</a><span id="S4.T6.4.1.14.14.3.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></td>
<td id="S4.T6.4.1.14.14.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.14.14.4.1" class="ltx_text" style="font-size:50%;">CPN</span></td>
<td id="S4.T6.4.1.14.14.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.14.14.5.1" class="ltx_text" style="font-size:50%;">Video</span></td>
<td id="S4.T6.4.1.14.14.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.14.14.6.1" class="ltx_text" style="font-size:50%;">10</span></td>
<td id="S4.T6.4.1.14.14.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.14.14.7.1" class="ltx_text" style="font-size:50%;">1.4</span></td>
<td id="S4.T6.4.1.14.14.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.14.14.8.1" class="ltx_text" style="font-size:50%;">41.5</span></td>
<td id="S4.T6.4.1.14.14.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.14.14.9.1" class="ltx_text" style="font-size:50%;">44.8</span></td>
<td id="S4.T6.4.1.14.14.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.14.14.10.1" class="ltx_text" style="font-size:50%;color:#0000FF;">39.8</span></td>
<td id="S4.T6.4.1.14.14.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.14.14.11.1" class="ltx_text" style="font-size:50%;">42.5</span></td>
<td id="S4.T6.4.1.14.14.12" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.14.14.12.1" class="ltx_text" style="font-size:50%;">46.5</span></td>
<td id="S4.T6.4.1.14.14.13" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.14.14.13.1" class="ltx_text" style="font-size:50%;">51.6</span></td>
<td id="S4.T6.4.1.14.14.14" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.14.14.14.1" class="ltx_text" style="font-size:50%;">42.1</span></td>
<td id="S4.T6.4.1.14.14.15" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.14.14.15.1" class="ltx_text" style="font-size:50%;">42.0</span></td>
<td id="S4.T6.4.1.14.14.16" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.14.14.16.1" class="ltx_text" style="font-size:50%;">53.3</span></td>
<td id="S4.T6.4.1.14.14.17" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.14.14.17.1" class="ltx_text" style="font-size:50%;">60.7</span></td>
<td id="S4.T6.4.1.14.14.18" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.14.14.18.1" class="ltx_text" style="font-size:50%;">45.5</span></td>
<td id="S4.T6.4.1.14.14.19" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.14.14.19.1" class="ltx_text" style="font-size:50%;">43.3</span></td>
<td id="S4.T6.4.1.14.14.20" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.14.14.20.1" class="ltx_text" style="font-size:50%;">46.1</span></td>
<td id="S4.T6.4.1.14.14.21" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.14.14.21.1" class="ltx_text" style="font-size:50%;">31.8</span></td>
<td id="S4.T6.4.1.14.14.22" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.14.14.22.1" class="ltx_text" style="font-size:50%;">32.2</span></td>
<td id="S4.T6.4.1.14.14.23" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.14.14.23.1" class="ltx_text" style="font-size:50%;">44.3</span></td>
<td id="S4.T6.4.1.14.14.24" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.14.14.24.1" class="ltx_text" style="font-size:50%;">34.6</span></td>
</tr>
<tr id="S4.T6.4.1.15.15" class="ltx_tr">
<td id="S4.T6.4.1.15.15.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S4.T6.4.1.15.15.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.15.15.2.1" class="ltx_text" style="font-size:50%;">2022</span></td>
<td id="S4.T6.4.1.15.15.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T6.4.1.15.15.3.1.1" class="ltx_text" style="font-size:50%;">(</span>Li
etÂ al<span class="ltx_text">.</span><span id="S4.T6.4.1.15.15.3.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib121" title="" class="ltx_ref">2022a</a><span id="S4.T6.4.1.15.15.3.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></td>
<td id="S4.T6.4.1.15.15.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.15.15.4.1" class="ltx_text" style="font-size:50%;">CPN</span></td>
<td id="S4.T6.4.1.15.15.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.15.15.5.1" class="ltx_text" style="font-size:50%;">Video</span></td>
<td id="S4.T6.4.1.15.15.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.15.15.6.1" class="ltx_text" style="font-size:50%;">32</span></td>
<td id="S4.T6.4.1.15.15.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.15.15.7.1" class="ltx_text" style="font-size:50%;">7.0</span></td>
<td id="S4.T6.4.1.15.15.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.15.15.8.1" class="ltx_text" style="font-size:50%;">39.2</span></td>
<td id="S4.T6.4.1.15.15.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.15.15.9.1" class="ltx_text" style="font-size:50%;">43.1</span></td>
<td id="S4.T6.4.1.15.15.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.15.15.10.1" class="ltx_text" style="font-size:50%;">40.1</span></td>
<td id="S4.T6.4.1.15.15.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.15.15.11.1" class="ltx_text" style="font-size:50%;color:#0000FF;">40.9</span></td>
<td id="S4.T6.4.1.15.15.12" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.15.15.12.1" class="ltx_text" style="font-size:50%;">44.9</span></td>
<td id="S4.T6.4.1.15.15.13" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.15.15.13.1" class="ltx_text" style="font-size:50%;">51.2</span></td>
<td id="S4.T6.4.1.15.15.14" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.15.15.14.1" class="ltx_text" style="font-size:50%;color:#0000FF;">40.6</span></td>
<td id="S4.T6.4.1.15.15.15" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.15.15.15.1" class="ltx_text" style="font-size:50%;">41.3</span></td>
<td id="S4.T6.4.1.15.15.16" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.15.15.16.1" class="ltx_text" style="font-size:50%;">53.5</span></td>
<td id="S4.T6.4.1.15.15.17" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.15.15.17.1" class="ltx_text" style="font-size:50%;">60.3</span></td>
<td id="S4.T6.4.1.15.15.18" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.15.15.18.1" class="ltx_text" style="font-size:50%;">43.7</span></td>
<td id="S4.T6.4.1.15.15.19" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.15.15.19.1" class="ltx_text" style="font-size:50%;color:#0000FF;">41.1</span></td>
<td id="S4.T6.4.1.15.15.20" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.15.15.20.1" class="ltx_text" style="font-size:50%;">43.8</span></td>
<td id="S4.T6.4.1.15.15.21" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.15.15.21.1" class="ltx_text" style="font-size:50%;color:#FF0000;">29.8</span></td>
<td id="S4.T6.4.1.15.15.22" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.15.15.22.1" class="ltx_text" style="font-size:50%;">30.6</span></td>
<td id="S4.T6.4.1.15.15.23" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.15.15.23.1" class="ltx_text" style="font-size:50%;">43.0</span></td>
<td id="S4.T6.4.1.15.15.24" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.15.15.24.1" class="ltx_text" style="font-size:50%;">-</span></td>
</tr>
<tr id="S4.T6.4.1.16.16" class="ltx_tr">
<td id="S4.T6.4.1.16.16.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.16.16.1.1" class="ltx_text" style="font-size:50%;">Lifting</span></td>
<td id="S4.T6.4.1.16.16.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.16.16.2.1" class="ltx_text" style="font-size:50%;">2022</span></td>
<td id="S4.T6.4.1.16.16.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T6.4.1.16.16.3.1.1" class="ltx_text" style="font-size:50%;">(</span>Zhang
etÂ al<span class="ltx_text">.</span><span id="S4.T6.4.1.16.16.3.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib289" title="" class="ltx_ref">2022</a><span id="S4.T6.4.1.16.16.3.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></td>
<td id="S4.T6.4.1.16.16.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.16.16.4.1" class="ltx_text" style="font-size:50%;">CPN</span></td>
<td id="S4.T6.4.1.16.16.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.16.16.5.1" class="ltx_text" style="font-size:50%;">Video</span></td>
<td id="S4.T6.4.1.16.16.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.16.16.6.1" class="ltx_text" style="font-size:50%;">41</span></td>
<td id="S4.T6.4.1.16.16.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.16.16.7.1" class="ltx_text" style="font-size:50%;">0.6</span></td>
<td id="S4.T6.4.1.16.16.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.16.16.8.1" class="ltx_text" style="font-size:50%;color:#FF0000;">37.6</span></td>
<td id="S4.T6.4.1.16.16.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.16.16.9.1" class="ltx_text" style="font-size:50%;color:#FF0000;">40.9</span></td>
<td id="S4.T6.4.1.16.16.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.16.16.10.1" class="ltx_text" style="font-size:50%;color:#FF0000;">37.3</span></td>
<td id="S4.T6.4.1.16.16.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.16.16.11.1" class="ltx_text" style="font-size:50%;color:#FF0000;">39.7</span></td>
<td id="S4.T6.4.1.16.16.12" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.16.16.12.1" class="ltx_text" style="font-size:50%;color:#0000FF;">42.3</span></td>
<td id="S4.T6.4.1.16.16.13" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.16.16.13.1" class="ltx_text" style="font-size:50%;color:#0000FF;">49.9</span></td>
<td id="S4.T6.4.1.16.16.14" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.16.16.14.1" class="ltx_text" style="font-size:50%;color:#FF0000;">40.1</span></td>
<td id="S4.T6.4.1.16.16.15" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.16.16.15.1" class="ltx_text" style="font-size:50%;color:#FF0000;">39.8</span></td>
<td id="S4.T6.4.1.16.16.16" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.16.16.16.1" class="ltx_text" style="font-size:50%;color:#FF0000;">51.7</span></td>
<td id="S4.T6.4.1.16.16.17" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.16.16.17.1" class="ltx_text" style="font-size:50%;color:#FF0000;">55.0</span></td>
<td id="S4.T6.4.1.16.16.18" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.16.16.18.1" class="ltx_text" style="font-size:50%;color:#0000FF;">42.1</span></td>
<td id="S4.T6.4.1.16.16.19" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.16.16.19.1" class="ltx_text" style="font-size:50%;color:#FF0000;">39.8</span></td>
<td id="S4.T6.4.1.16.16.20" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.16.16.20.1" class="ltx_text" style="font-size:50%;color:#FF0000;">41.0</span></td>
<td id="S4.T6.4.1.16.16.21" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.16.16.21.1" class="ltx_text" style="font-size:50%;color:#FF0000;">27.9</span></td>
<td id="S4.T6.4.1.16.16.22" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.16.16.22.1" class="ltx_text" style="font-size:50%;color:#FF0000;">27.9</span></td>
<td id="S4.T6.4.1.16.16.23" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.16.16.23.1" class="ltx_text" style="font-size:50%;color:#FF0000;">40.9</span></td>
<td id="S4.T6.4.1.16.16.24" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.16.16.24.1" class="ltx_text" style="font-size:50%;color:#FF0000;">32.6</span></td>
</tr>
<tr id="S4.T6.4.1.17.17" class="ltx_tr">
<td id="S4.T6.4.1.17.17.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="7"><span id="S4.T6.4.1.17.17.1.1" class="ltx_text ltx_font_bold" style="font-size:50%;">Human mesh recovery methods</span></td>
<td id="S4.T6.4.1.17.17.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="16"><span id="S4.T6.4.1.17.17.2.1" class="ltx_text" style="font-size:50%;">MPJPE</span></td>
<td id="S4.T6.4.1.17.17.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.17.17.3.1" class="ltx_text" style="font-size:50%;">PA-MPJPE</span></td>
</tr>
<tr id="S4.T6.4.1.18.18" class="ltx_tr">
<td id="S4.T6.4.1.18.18.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.18.18.1.1" class="ltx_text" style="font-size:50%;">output</span></td>
<td id="S4.T6.4.1.18.18.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.18.18.2.1" class="ltx_text" style="font-size:50%;">Year</span></td>
<td id="S4.T6.4.1.18.18.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.18.18.3.1" class="ltx_text" style="font-size:50%;">Methods</span></td>
<td id="S4.T6.4.1.18.18.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.18.18.4.1" class="ltx_text" style="font-size:50%;">Model</span></td>
<td id="S4.T6.4.1.18.18.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.18.18.5.1" class="ltx_text" style="font-size:50%;">Input</span></td>
<td id="S4.T6.4.1.18.18.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.18.18.6.1" class="ltx_text" style="font-size:50%;">Params(M)</span></td>
<td id="S4.T6.4.1.18.18.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.18.18.7.1" class="ltx_text" style="font-size:50%;">FLOPs(G)</span></td>
<td id="S4.T6.4.1.18.18.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.18.18.8.1" class="ltx_text" style="font-size:50%;">Dir.</span></td>
<td id="S4.T6.4.1.18.18.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.18.18.9.1" class="ltx_text" style="font-size:50%;">Disc.</span></td>
<td id="S4.T6.4.1.18.18.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.18.18.10.1" class="ltx_text" style="font-size:50%;">Eat.</span></td>
<td id="S4.T6.4.1.18.18.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.18.18.11.1" class="ltx_text" style="font-size:50%;">Greet</span></td>
<td id="S4.T6.4.1.18.18.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.18.18.12.1" class="ltx_text" style="font-size:50%;">Phone</span></td>
<td id="S4.T6.4.1.18.18.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.18.18.13.1" class="ltx_text" style="font-size:50%;">Photo</span></td>
<td id="S4.T6.4.1.18.18.14" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.18.18.14.1" class="ltx_text" style="font-size:50%;">Pose</span></td>
<td id="S4.T6.4.1.18.18.15" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.18.18.15.1" class="ltx_text" style="font-size:50%;">Purch.</span></td>
<td id="S4.T6.4.1.18.18.16" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.18.18.16.1" class="ltx_text" style="font-size:50%;">Sit</span></td>
<td id="S4.T6.4.1.18.18.17" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.18.18.17.1" class="ltx_text" style="font-size:50%;">SitD.</span></td>
<td id="S4.T6.4.1.18.18.18" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.18.18.18.1" class="ltx_text" style="font-size:50%;">Somke</span></td>
<td id="S4.T6.4.1.18.18.19" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.18.18.19.1" class="ltx_text" style="font-size:50%;">Wait</span></td>
<td id="S4.T6.4.1.18.18.20" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.18.18.20.1" class="ltx_text" style="font-size:50%;">WalkD.</span></td>
<td id="S4.T6.4.1.18.18.21" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.18.18.21.1" class="ltx_text" style="font-size:50%;">Walk</span></td>
<td id="S4.T6.4.1.18.18.22" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.18.18.22.1" class="ltx_text" style="font-size:50%;">WalkT.</span></td>
<td id="S4.T6.4.1.18.18.23" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.18.18.23.1" class="ltx_text" style="font-size:50%;">Average</span></td>
<td id="S4.T6.4.1.18.18.24" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.18.18.24.1" class="ltx_text" style="font-size:50%;">Average</span></td>
</tr>
<tr id="S4.T6.4.1.19.19" class="ltx_tr">
<td id="S4.T6.4.1.19.19.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S4.T6.4.1.19.19.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.19.19.2.1" class="ltx_text" style="font-size:50%;">2019</span></td>
<td id="S4.T6.4.1.19.19.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T6.4.1.19.19.3.1.1" class="ltx_text" style="font-size:50%;">(</span>Kolotouros etÂ al<span class="ltx_text">.</span><span id="S4.T6.4.1.19.19.3.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib103" title="" class="ltx_ref">2019</a><span id="S4.T6.4.1.19.19.3.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></td>
<td id="S4.T6.4.1.19.19.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.19.19.4.1" class="ltx_text" style="font-size:50%;">SMPL</span></td>
<td id="S4.T6.4.1.19.19.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.19.19.5.1" class="ltx_text" style="font-size:50%;">Image</span></td>
<td id="S4.T6.4.1.19.19.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.19.19.6.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.19.19.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.19.19.7.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.19.19.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.19.19.8.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.19.19.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.19.19.9.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.19.19.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.19.19.10.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.19.19.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.19.19.11.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.19.19.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.19.19.12.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.19.19.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.19.19.13.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.19.19.14" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.19.19.14.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.19.19.15" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.19.19.15.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.19.19.16" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.19.19.16.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.19.19.17" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.19.19.17.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.19.19.18" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.19.19.18.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.19.19.19" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.19.19.19.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.19.19.20" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.19.19.20.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.19.19.21" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.19.19.21.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.19.19.22" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.19.19.22.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.19.19.23" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.19.19.23.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.19.19.24" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.19.19.24.1" class="ltx_text" style="font-size:50%;">50.1</span></td>
</tr>
<tr id="S4.T6.4.1.20.20" class="ltx_tr">
<td id="S4.T6.4.1.20.20.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S4.T6.4.1.20.20.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.20.20.2.1" class="ltx_text" style="font-size:50%;">2019</span></td>
<td id="S4.T6.4.1.20.20.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T6.4.1.20.20.3.1.1" class="ltx_text" style="font-size:50%;">(</span>Kolotouros etÂ al<span class="ltx_text">.</span><span id="S4.T6.4.1.20.20.3.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib102" title="" class="ltx_ref">2019</a><span id="S4.T6.4.1.20.20.3.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></td>
<td id="S4.T6.4.1.20.20.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.20.20.4.1" class="ltx_text" style="font-size:50%;">SMPL</span></td>
<td id="S4.T6.4.1.20.20.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.20.20.5.1" class="ltx_text" style="font-size:50%;">Image</span></td>
<td id="S4.T6.4.1.20.20.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.20.20.6.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.20.20.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.20.20.7.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.20.20.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.20.20.8.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.20.20.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.20.20.9.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.20.20.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.20.20.10.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.20.20.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.20.20.11.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.20.20.12" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.20.20.12.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.20.20.13" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.20.20.13.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.20.20.14" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.20.20.14.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.20.20.15" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.20.20.15.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.20.20.16" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.20.20.16.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.20.20.17" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.20.20.17.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.20.20.18" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.20.20.18.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.20.20.19" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.20.20.19.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.20.20.20" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.20.20.20.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.20.20.21" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.20.20.21.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.20.20.22" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.20.20.22.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.20.20.23" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.20.20.23.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.20.20.24" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.20.20.24.1" class="ltx_text" style="font-size:50%;">41.1</span></td>
</tr>
<tr id="S4.T6.4.1.21.21" class="ltx_tr">
<td id="S4.T6.4.1.21.21.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S4.T6.4.1.21.21.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.21.21.2.1" class="ltx_text" style="font-size:50%;">2019</span></td>
<td id="S4.T6.4.1.21.21.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T6.4.1.21.21.3.1.1" class="ltx_text" style="font-size:50%;">(</span>Xiang
etÂ al<span class="ltx_text">.</span><span id="S4.T6.4.1.21.21.3.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib259" title="" class="ltx_ref">2019</a><span id="S4.T6.4.1.21.21.3.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></td>
<td id="S4.T6.4.1.21.21.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.21.21.4.1" class="ltx_text" style="font-size:50%;">Adam</span></td>
<td id="S4.T6.4.1.21.21.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.21.21.5.1" class="ltx_text" style="font-size:50%;">Image</span></td>
<td id="S4.T6.4.1.21.21.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.21.21.6.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.21.21.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.21.21.7.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.21.21.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.21.21.8.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.21.21.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.21.21.9.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.21.21.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.21.21.10.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.21.21.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.21.21.11.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.21.21.12" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.21.21.12.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.21.21.13" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.21.21.13.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.21.21.14" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.21.21.14.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.21.21.15" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.21.21.15.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.21.21.16" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.21.21.16.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.21.21.17" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.21.21.17.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.21.21.18" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.21.21.18.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.21.21.19" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.21.21.19.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.21.21.20" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.21.21.20.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.21.21.21" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.21.21.21.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.21.21.22" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.21.21.22.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.21.21.23" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.21.21.23.1" class="ltx_text" style="font-size:50%;">58.3</span></td>
<td id="S4.T6.4.1.21.21.24" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.21.21.24.1" class="ltx_text" style="font-size:50%;">-</span></td>
</tr>
<tr id="S4.T6.4.1.22.22" class="ltx_tr">
<td id="S4.T6.4.1.22.22.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S4.T6.4.1.22.22.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.22.22.2.1" class="ltx_text" style="font-size:50%;">2020</span></td>
<td id="S4.T6.4.1.22.22.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T6.4.1.22.22.3.1.1" class="ltx_text" style="font-size:50%;">(</span>Choi
etÂ al<span class="ltx_text">.</span><span id="S4.T6.4.1.22.22.3.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib39" title="" class="ltx_ref">2020</a><span id="S4.T6.4.1.22.22.3.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></td>
<td id="S4.T6.4.1.22.22.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.22.22.4.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.22.22.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.22.22.5.1" class="ltx_text" style="font-size:50%;">Image</span></td>
<td id="S4.T6.4.1.22.22.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.22.22.6.1" class="ltx_text" style="font-size:50%;">140</span></td>
<td id="S4.T6.4.1.22.22.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.22.22.7.1" class="ltx_text" style="font-size:50%;">11</span></td>
<td id="S4.T6.4.1.22.22.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.22.22.8.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.22.22.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.22.22.9.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.22.22.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.22.22.10.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.22.22.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.22.22.11.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.22.22.12" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.22.22.12.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.22.22.13" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.22.22.13.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.22.22.14" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.22.22.14.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.22.22.15" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.22.22.15.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.22.22.16" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.22.22.16.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.22.22.17" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.22.22.17.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.22.22.18" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.22.22.18.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.22.22.19" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.22.22.19.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.22.22.20" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.22.22.20.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.22.22.21" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.22.22.21.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.22.22.22" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.22.22.22.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.22.22.23" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.22.22.23.1" class="ltx_text" style="font-size:50%;">64.9</span></td>
<td id="S4.T6.4.1.22.22.24" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.22.22.24.1" class="ltx_text" style="font-size:50%;">47.0</span></td>
</tr>
<tr id="S4.T6.4.1.23.23" class="ltx_tr">
<td id="S4.T6.4.1.23.23.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S4.T6.4.1.23.23.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.23.23.2.1" class="ltx_text" style="font-size:50%;">2021</span></td>
<td id="S4.T6.4.1.23.23.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T6.4.1.23.23.3.1.1" class="ltx_text" style="font-size:50%;">(</span>Zhang etÂ al<span class="ltx_text">.</span><span id="S4.T6.4.1.23.23.3.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib288" title="" class="ltx_ref">2021</a><span id="S4.T6.4.1.23.23.3.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></td>
<td id="S4.T6.4.1.23.23.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.23.23.4.1" class="ltx_text" style="font-size:50%;">SMPL</span></td>
<td id="S4.T6.4.1.23.23.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.23.23.5.1" class="ltx_text" style="font-size:50%;">Image</span></td>
<td id="S4.T6.4.1.23.23.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.23.23.6.1" class="ltx_text" style="font-size:50%;">45.2</span></td>
<td id="S4.T6.4.1.23.23.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.23.23.7.1" class="ltx_text" style="font-size:50%;">11</span></td>
<td id="S4.T6.4.1.23.23.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.23.23.8.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.23.23.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.23.23.9.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.23.23.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.23.23.10.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.23.23.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.23.23.11.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.23.23.12" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.23.23.12.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.23.23.13" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.23.23.13.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.23.23.14" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.23.23.14.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.23.23.15" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.23.23.15.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.23.23.16" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.23.23.16.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.23.23.17" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.23.23.17.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.23.23.18" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.23.23.18.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.23.23.19" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.23.23.19.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.23.23.20" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.23.23.20.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.23.23.21" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.23.23.21.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.23.23.22" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.23.23.22.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.23.23.23" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.23.23.23.1" class="ltx_text" style="font-size:50%;">57.7</span></td>
<td id="S4.T6.4.1.23.23.24" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.23.23.24.1" class="ltx_text" style="font-size:50%;">40.5</span></td>
</tr>
<tr id="S4.T6.4.1.24.24" class="ltx_tr">
<td id="S4.T6.4.1.24.24.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S4.T6.4.1.24.24.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.24.24.2.1" class="ltx_text" style="font-size:50%;">2021</span></td>
<td id="S4.T6.4.1.24.24.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T6.4.1.24.24.3.1.1" class="ltx_text" style="font-size:50%;">(</span>Lin
etÂ al<span class="ltx_text">.</span><span id="S4.T6.4.1.24.24.3.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib131" title="" class="ltx_ref">2021a</a><span id="S4.T6.4.1.24.24.3.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></td>
<td id="S4.T6.4.1.24.24.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.24.24.4.1" class="ltx_text" style="font-size:50%;">SMPL</span></td>
<td id="S4.T6.4.1.24.24.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.24.24.5.1" class="ltx_text" style="font-size:50%;">Image</span></td>
<td id="S4.T6.4.1.24.24.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.24.24.6.1" class="ltx_text" style="font-size:50%;">230</span></td>
<td id="S4.T6.4.1.24.24.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.24.24.7.1" class="ltx_text" style="font-size:50%;">57</span></td>
<td id="S4.T6.4.1.24.24.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.24.24.8.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.24.24.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.24.24.9.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.24.24.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.24.24.10.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.24.24.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.24.24.11.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.24.24.12" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.24.24.12.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.24.24.13" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.24.24.13.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.24.24.14" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.24.24.14.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.24.24.15" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.24.24.15.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.24.24.16" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.24.24.16.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.24.24.17" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.24.24.17.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.24.24.18" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.24.24.18.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.24.24.19" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.24.24.19.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.24.24.20" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.24.24.20.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.24.24.21" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.24.24.21.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.24.24.22" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.24.24.22.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.24.24.23" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.24.24.23.1" class="ltx_text" style="font-size:50%;">54.0</span></td>
<td id="S4.T6.4.1.24.24.24" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.24.24.24.1" class="ltx_text" style="font-size:50%;">36.7</span></td>
</tr>
<tr id="S4.T6.4.1.25.25" class="ltx_tr">
<td id="S4.T6.4.1.25.25.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S4.T6.4.1.25.25.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.25.25.2.1" class="ltx_text" style="font-size:50%;">2021</span></td>
<td id="S4.T6.4.1.25.25.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T6.4.1.25.25.3.1.1" class="ltx_text" style="font-size:50%;">(</span>Lin
etÂ al<span class="ltx_text">.</span><span id="S4.T6.4.1.25.25.3.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib132" title="" class="ltx_ref">2021b</a><span id="S4.T6.4.1.25.25.3.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></td>
<td id="S4.T6.4.1.25.25.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.25.25.4.1" class="ltx_text" style="font-size:50%;">SMPL</span></td>
<td id="S4.T6.4.1.25.25.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.25.25.5.1" class="ltx_text" style="font-size:50%;">Image</span></td>
<td id="S4.T6.4.1.25.25.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.25.25.6.1" class="ltx_text" style="font-size:50%;">230</span></td>
<td id="S4.T6.4.1.25.25.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.25.25.7.1" class="ltx_text" style="font-size:50%;">57</span></td>
<td id="S4.T6.4.1.25.25.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.25.25.8.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.25.25.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.25.25.9.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.25.25.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.25.25.10.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.25.25.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.25.25.11.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.25.25.12" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.25.25.12.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.25.25.13" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.25.25.13.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.25.25.14" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.25.25.14.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.25.25.15" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.25.25.15.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.25.25.16" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.25.25.16.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.25.25.17" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.25.25.17.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.25.25.18" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.25.25.18.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.25.25.19" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.25.25.19.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.25.25.20" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.25.25.20.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.25.25.21" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.25.25.21.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.25.25.22" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.25.25.22.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.25.25.23" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.25.25.23.1" class="ltx_text" style="font-size:50%;color:#FF0000;">51.2</span></td>
<td id="S4.T6.4.1.25.25.24" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.25.25.24.1" class="ltx_text" style="font-size:50%;color:#0000FF;">34.5</span></td>
</tr>
<tr id="S4.T6.4.1.26.26" class="ltx_tr">
<td id="S4.T6.4.1.26.26.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S4.T6.4.1.26.26.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.26.26.2.1" class="ltx_text" style="font-size:50%;">2022</span></td>
<td id="S4.T6.4.1.26.26.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T6.4.1.26.26.3.1.1" class="ltx_text" style="font-size:50%;">(</span>Cho
etÂ al<span class="ltx_text">.</span><span id="S4.T6.4.1.26.26.3.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib37" title="" class="ltx_ref">2022</a><span id="S4.T6.4.1.26.26.3.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></td>
<td id="S4.T6.4.1.26.26.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.26.26.4.1" class="ltx_text" style="font-size:50%;">SMPL</span></td>
<td id="S4.T6.4.1.26.26.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.26.26.5.1" class="ltx_text" style="font-size:50%;">Image</span></td>
<td id="S4.T6.4.1.26.26.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.26.26.6.1" class="ltx_text" style="font-size:50%;">49</span></td>
<td id="S4.T6.4.1.26.26.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.26.26.7.1" class="ltx_text" style="font-size:50%;">16</span></td>
<td id="S4.T6.4.1.26.26.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.26.26.8.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.26.26.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.26.26.9.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.26.26.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.26.26.10.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.26.26.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.26.26.11.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.26.26.12" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.26.26.12.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.26.26.13" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.26.26.13.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.26.26.14" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.26.26.14.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.26.26.15" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.26.26.15.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.26.26.16" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.26.26.16.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.26.26.17" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.26.26.17.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.26.26.18" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.26.26.18.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.26.26.19" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.26.26.19.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.26.26.20" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.26.26.20.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.26.26.21" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.26.26.21.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.26.26.22" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.26.26.22.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.26.26.23" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.26.26.23.1" class="ltx_text" style="font-size:50%;color:#0000FF;">52.2</span></td>
<td id="S4.T6.4.1.26.26.24" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.26.26.24.1" class="ltx_text" style="font-size:50%;color:#FF0000;">33.7</span></td>
</tr>
<tr id="S4.T6.4.1.27.27" class="ltx_tr">
<td id="S4.T6.4.1.27.27.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S4.T6.4.1.27.27.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.27.27.2.1" class="ltx_text" style="font-size:50%;">2019</span></td>
<td id="S4.T6.4.1.27.27.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T6.4.1.27.27.3.1.1" class="ltx_text" style="font-size:50%;">(</span>Arnab
etÂ al<span class="ltx_text">.</span><span id="S4.T6.4.1.27.27.3.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib6" title="" class="ltx_ref">2019</a><span id="S4.T6.4.1.27.27.3.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></td>
<td id="S4.T6.4.1.27.27.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.27.27.4.1" class="ltx_text" style="font-size:50%;">SMPL</span></td>
<td id="S4.T6.4.1.27.27.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.27.27.5.1" class="ltx_text" style="font-size:50%;">Video</span></td>
<td id="S4.T6.4.1.27.27.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.27.27.6.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.27.27.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.27.27.7.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.27.27.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.27.27.8.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.27.27.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.27.27.9.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.27.27.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.27.27.10.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.27.27.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.27.27.11.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.27.27.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.27.27.12.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.27.27.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.27.27.13.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.27.27.14" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.27.27.14.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.27.27.15" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.27.27.15.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.27.27.16" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.27.27.16.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.27.27.17" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.27.27.17.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.27.27.18" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.27.27.18.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.27.27.19" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.27.27.19.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.27.27.20" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.27.27.20.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.27.27.21" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.27.27.21.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.27.27.22" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.27.27.22.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.27.27.23" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.27.27.23.1" class="ltx_text" style="font-size:50%;">77.8</span></td>
<td id="S4.T6.4.1.27.27.24" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.27.27.24.1" class="ltx_text" style="font-size:50%;">54.3</span></td>
</tr>
<tr id="S4.T6.4.1.28.28" class="ltx_tr">
<td id="S4.T6.4.1.28.28.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S4.T6.4.1.28.28.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.28.28.2.1" class="ltx_text" style="font-size:50%;">2020</span></td>
<td id="S4.T6.4.1.28.28.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T6.4.1.28.28.3.1.1" class="ltx_text" style="font-size:50%;">(</span>Kocabas
etÂ al<span class="ltx_text">.</span><span id="S4.T6.4.1.28.28.3.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib99" title="" class="ltx_ref">2020</a><span id="S4.T6.4.1.28.28.3.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></td>
<td id="S4.T6.4.1.28.28.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.28.28.4.1" class="ltx_text" style="font-size:50%;">SMPL</span></td>
<td id="S4.T6.4.1.28.28.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.28.28.5.1" class="ltx_text" style="font-size:50%;">Video</span></td>
<td id="S4.T6.4.1.28.28.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.28.28.6.1" class="ltx_text" style="font-size:50%;">59</span></td>
<td id="S4.T6.4.1.28.28.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.28.28.7.1" class="ltx_text" style="font-size:50%;">10</span></td>
<td id="S4.T6.4.1.28.28.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.28.28.8.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.28.28.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.28.28.9.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.28.28.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.28.28.10.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.28.28.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.28.28.11.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.28.28.12" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.28.28.12.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.28.28.13" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.28.28.13.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.28.28.14" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.28.28.14.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.28.28.15" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.28.28.15.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.28.28.16" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.28.28.16.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.28.28.17" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.28.28.17.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.28.28.18" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.28.28.18.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.28.28.19" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.28.28.19.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.28.28.20" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.28.28.20.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.28.28.21" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.28.28.21.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.28.28.22" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.28.28.22.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.28.28.23" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.28.28.23.1" class="ltx_text" style="font-size:50%;">65.6</span></td>
<td id="S4.T6.4.1.28.28.24" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.28.28.24.1" class="ltx_text" style="font-size:50%;">41.4</span></td>
</tr>
<tr id="S4.T6.4.1.29.29" class="ltx_tr">
<td id="S4.T6.4.1.29.29.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.29.29.1.1" class="ltx_text" style="font-size:50%;">Mesh</span></td>
<td id="S4.T6.4.1.29.29.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.29.29.2.1" class="ltx_text" style="font-size:50%;">2021</span></td>
<td id="S4.T6.4.1.29.29.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:5.0pt;padding-right:5.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T6.4.1.29.29.3.1.1" class="ltx_text" style="font-size:50%;">(</span>Choi
etÂ al<span class="ltx_text">.</span><span id="S4.T6.4.1.29.29.3.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib38" title="" class="ltx_ref">2021</a><span id="S4.T6.4.1.29.29.3.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></td>
<td id="S4.T6.4.1.29.29.4" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.29.29.4.1" class="ltx_text" style="font-size:50%;">SMPL</span></td>
<td id="S4.T6.4.1.29.29.5" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.29.29.5.1" class="ltx_text" style="font-size:50%;">Video</span></td>
<td id="S4.T6.4.1.29.29.6" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.29.29.6.1" class="ltx_text" style="font-size:50%;">123</span></td>
<td id="S4.T6.4.1.29.29.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.29.29.7.1" class="ltx_text" style="font-size:50%;">10</span></td>
<td id="S4.T6.4.1.29.29.8" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.29.29.8.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.29.29.9" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.29.29.9.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.29.29.10" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.29.29.10.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.29.29.11" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.29.29.11.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.29.29.12" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.29.29.12.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.29.29.13" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.29.29.13.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.29.29.14" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.29.29.14.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.29.29.15" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.29.29.15.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.29.29.16" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.29.29.16.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.29.29.17" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.29.29.17.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.29.29.18" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.29.29.18.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.29.29.19" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.29.29.19.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.29.29.20" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.29.29.20.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.29.29.21" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.29.29.21.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.29.29.22" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.29.29.22.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T6.4.1.29.29.23" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.29.29.23.1" class="ltx_text" style="font-size:50%;">62.3</span></td>
<td id="S4.T6.4.1.29.29.24" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T6.4.1.29.29.24.1" class="ltx_text" style="font-size:50%;">41.1</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5. </span>Evaluation Metrics for 3D HPE</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.5" class="ltx_p"><span id="S4.SS5.p1.5.1" class="ltx_text ltx_font_bold">MPJPE</span> (Mean Per Joint Position Error) is the most widely used metric to evaluate the performance of 3D HPE. MPJPE is computed by using the Euclidean distance between the estimated 3D joints and the ground truth positions
as follows:</p>
<table id="S6.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S4.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.E1.m1.1" class="ltx_Math" alttext="\displaystyle\small{MPJPE}=\frac{1}{N}\sum_{i=1}^{N}\|J_{i}-J_{i}^{*}\|_{2}," display="inline"><semantics id="S4.E1.m1.1a"><mrow id="S4.E1.m1.1.1.1" xref="S4.E1.m1.1.1.1.1.cmml"><mrow id="S4.E1.m1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.cmml"><mrow id="S4.E1.m1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.3.cmml"><mi mathsize="90%" id="S4.E1.m1.1.1.1.1.3.2" xref="S4.E1.m1.1.1.1.1.3.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1.1.3.1" xref="S4.E1.m1.1.1.1.1.3.1.cmml">â€‹</mo><mi mathsize="90%" id="S4.E1.m1.1.1.1.1.3.3" xref="S4.E1.m1.1.1.1.1.3.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1.1.3.1a" xref="S4.E1.m1.1.1.1.1.3.1.cmml">â€‹</mo><mi mathsize="90%" id="S4.E1.m1.1.1.1.1.3.4" xref="S4.E1.m1.1.1.1.1.3.4.cmml">J</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1.1.3.1b" xref="S4.E1.m1.1.1.1.1.3.1.cmml">â€‹</mo><mi mathsize="90%" id="S4.E1.m1.1.1.1.1.3.5" xref="S4.E1.m1.1.1.1.1.3.5.cmml">P</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1.1.3.1c" xref="S4.E1.m1.1.1.1.1.3.1.cmml">â€‹</mo><mi mathsize="90%" id="S4.E1.m1.1.1.1.1.3.6" xref="S4.E1.m1.1.1.1.1.3.6.cmml">E</mi></mrow><mo mathsize="90%" id="S4.E1.m1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.2.cmml">=</mo><mrow id="S4.E1.m1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.cmml"><mstyle displaystyle="true" id="S4.E1.m1.1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.1.3.cmml"><mfrac id="S4.E1.m1.1.1.1.1.1.3a" xref="S4.E1.m1.1.1.1.1.1.3.cmml"><mn mathsize="90%" id="S4.E1.m1.1.1.1.1.1.3.2" xref="S4.E1.m1.1.1.1.1.1.3.2.cmml">1</mn><mi mathsize="90%" id="S4.E1.m1.1.1.1.1.1.3.3" xref="S4.E1.m1.1.1.1.1.1.3.3.cmml">N</mi></mfrac></mstyle><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S4.E1.m1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.cmml"><mstyle displaystyle="true" id="S4.E1.m1.1.1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.1.1.2.cmml"><munderover id="S4.E1.m1.1.1.1.1.1.1.2a" xref="S4.E1.m1.1.1.1.1.1.1.2.cmml"><mo maxsize="90%" minsize="90%" movablelimits="false" stretchy="true" id="S4.E1.m1.1.1.1.1.1.1.2.2.2" xref="S4.E1.m1.1.1.1.1.1.1.2.2.2.cmml">âˆ‘</mo><mrow id="S4.E1.m1.1.1.1.1.1.1.2.2.3" xref="S4.E1.m1.1.1.1.1.1.1.2.2.3.cmml"><mi mathsize="90%" id="S4.E1.m1.1.1.1.1.1.1.2.2.3.2" xref="S4.E1.m1.1.1.1.1.1.1.2.2.3.2.cmml">i</mi><mo mathsize="90%" id="S4.E1.m1.1.1.1.1.1.1.2.2.3.1" xref="S4.E1.m1.1.1.1.1.1.1.2.2.3.1.cmml">=</mo><mn mathsize="90%" id="S4.E1.m1.1.1.1.1.1.1.2.2.3.3" xref="S4.E1.m1.1.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi mathsize="90%" id="S4.E1.m1.1.1.1.1.1.1.2.3" xref="S4.E1.m1.1.1.1.1.1.1.2.3.cmml">N</mi></munderover></mstyle><msub id="S4.E1.m1.1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.1.cmml"><mrow id="S4.E1.m1.1.1.1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.1.1.2.cmml"><mo maxsize="90%" minsize="90%" id="S4.E1.m1.1.1.1.1.1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.1.1.1.1.2.1.cmml">â€–</mo><mrow id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi mathsize="90%" id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">J</mi><mi mathsize="90%" id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo mathsize="90%" id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><msubsup id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi mathsize="90%" id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.3.2.2" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.3.2.2.cmml">J</mi><mi mathsize="90%" id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.3.2.3" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.3.2.3.cmml">i</mi><mo mathsize="90%" id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">âˆ—</mo></msubsup></mrow><mo maxsize="90%" minsize="90%" id="S4.E1.m1.1.1.1.1.1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.1.1.1.1.2.1.cmml">â€–</mo></mrow><mn mathsize="90%" id="S4.E1.m1.1.1.1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.1.1.1.3.cmml">2</mn></msub></mrow></mrow></mrow><mo mathsize="90%" id="S4.E1.m1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.1b"><apply id="S4.E1.m1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1"><eq id="S4.E1.m1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.2"></eq><apply id="S4.E1.m1.1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.1.3"><times id="S4.E1.m1.1.1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.1.1.3.1"></times><ci id="S4.E1.m1.1.1.1.1.3.2.cmml" xref="S4.E1.m1.1.1.1.1.3.2">ğ‘€</ci><ci id="S4.E1.m1.1.1.1.1.3.3.cmml" xref="S4.E1.m1.1.1.1.1.3.3">ğ‘ƒ</ci><ci id="S4.E1.m1.1.1.1.1.3.4.cmml" xref="S4.E1.m1.1.1.1.1.3.4">ğ½</ci><ci id="S4.E1.m1.1.1.1.1.3.5.cmml" xref="S4.E1.m1.1.1.1.1.3.5">ğ‘ƒ</ci><ci id="S4.E1.m1.1.1.1.1.3.6.cmml" xref="S4.E1.m1.1.1.1.1.3.6">ğ¸</ci></apply><apply id="S4.E1.m1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1"><times id="S4.E1.m1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.1.2"></times><apply id="S4.E1.m1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.1.1.3"><divide id="S4.E1.m1.1.1.1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.1.1.1.3"></divide><cn type="integer" id="S4.E1.m1.1.1.1.1.1.3.2.cmml" xref="S4.E1.m1.1.1.1.1.1.3.2">1</cn><ci id="S4.E1.m1.1.1.1.1.1.3.3.cmml" xref="S4.E1.m1.1.1.1.1.1.3.3">ğ‘</ci></apply><apply id="S4.E1.m1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1"><apply id="S4.E1.m1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S4.E1.m1.1.1.1.1.1.1.2.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.1.1.2.2.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2">subscript</csymbol><sum id="S4.E1.m1.1.1.1.1.1.1.2.2.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2.2.2"></sum><apply id="S4.E1.m1.1.1.1.1.1.1.2.2.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2.2.3"><eq id="S4.E1.m1.1.1.1.1.1.1.2.2.3.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2.2.3.1"></eq><ci id="S4.E1.m1.1.1.1.1.1.1.2.2.3.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2.2.3.2">ğ‘–</ci><cn type="integer" id="S4.E1.m1.1.1.1.1.1.1.2.2.3.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S4.E1.m1.1.1.1.1.1.1.2.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2.3">ğ‘</ci></apply><apply id="S4.E1.m1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S4.E1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S4.E1.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1"><minus id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1"></minus><apply id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.2.2">ğ½</ci><ci id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.2.3">ğ‘–</ci></apply><apply id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.3">superscript</csymbol><apply id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.3.2.2">ğ½</ci><ci id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.3.2.3">ğ‘–</ci></apply><times id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.3.3"></times></apply></apply></apply><cn type="integer" id="S4.E1.m1.1.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.1c">\displaystyle\small{MPJPE}=\frac{1}{N}\sum_{i=1}^{N}\|J_{i}-J_{i}^{*}\|_{2},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S4.SS5.p1.4" class="ltx_p">where <math id="S4.SS5.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.SS5.p1.1.m1.1a"><mi id="S4.SS5.p1.1.m1.1.1" xref="S4.SS5.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.1.m1.1b"><ci id="S4.SS5.p1.1.m1.1.1.cmml" xref="S4.SS5.p1.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.1.m1.1c">N</annotation></semantics></math> is the number of joints, <math id="S4.SS5.p1.2.m2.1" class="ltx_Math" alttext="J_{i}" display="inline"><semantics id="S4.SS5.p1.2.m2.1a"><msub id="S4.SS5.p1.2.m2.1.1" xref="S4.SS5.p1.2.m2.1.1.cmml"><mi id="S4.SS5.p1.2.m2.1.1.2" xref="S4.SS5.p1.2.m2.1.1.2.cmml">J</mi><mi id="S4.SS5.p1.2.m2.1.1.3" xref="S4.SS5.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.2.m2.1b"><apply id="S4.SS5.p1.2.m2.1.1.cmml" xref="S4.SS5.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS5.p1.2.m2.1.1.1.cmml" xref="S4.SS5.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS5.p1.2.m2.1.1.2.cmml" xref="S4.SS5.p1.2.m2.1.1.2">ğ½</ci><ci id="S4.SS5.p1.2.m2.1.1.3.cmml" xref="S4.SS5.p1.2.m2.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.2.m2.1c">J_{i}</annotation></semantics></math> and <math id="S4.SS5.p1.3.m3.1" class="ltx_Math" alttext="J^{*}_{i}" display="inline"><semantics id="S4.SS5.p1.3.m3.1a"><msubsup id="S4.SS5.p1.3.m3.1.1" xref="S4.SS5.p1.3.m3.1.1.cmml"><mi id="S4.SS5.p1.3.m3.1.1.2.2" xref="S4.SS5.p1.3.m3.1.1.2.2.cmml">J</mi><mi id="S4.SS5.p1.3.m3.1.1.3" xref="S4.SS5.p1.3.m3.1.1.3.cmml">i</mi><mo id="S4.SS5.p1.3.m3.1.1.2.3" xref="S4.SS5.p1.3.m3.1.1.2.3.cmml">âˆ—</mo></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.3.m3.1b"><apply id="S4.SS5.p1.3.m3.1.1.cmml" xref="S4.SS5.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS5.p1.3.m3.1.1.1.cmml" xref="S4.SS5.p1.3.m3.1.1">subscript</csymbol><apply id="S4.SS5.p1.3.m3.1.1.2.cmml" xref="S4.SS5.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS5.p1.3.m3.1.1.2.1.cmml" xref="S4.SS5.p1.3.m3.1.1">superscript</csymbol><ci id="S4.SS5.p1.3.m3.1.1.2.2.cmml" xref="S4.SS5.p1.3.m3.1.1.2.2">ğ½</ci><times id="S4.SS5.p1.3.m3.1.1.2.3.cmml" xref="S4.SS5.p1.3.m3.1.1.2.3"></times></apply><ci id="S4.SS5.p1.3.m3.1.1.3.cmml" xref="S4.SS5.p1.3.m3.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.3.m3.1c">J^{*}_{i}</annotation></semantics></math> are the ground truth position and the estimated position of the <math id="S4.SS5.p1.4.m4.1" class="ltx_Math" alttext="i_{th}" display="inline"><semantics id="S4.SS5.p1.4.m4.1a"><msub id="S4.SS5.p1.4.m4.1.1" xref="S4.SS5.p1.4.m4.1.1.cmml"><mi id="S4.SS5.p1.4.m4.1.1.2" xref="S4.SS5.p1.4.m4.1.1.2.cmml">i</mi><mrow id="S4.SS5.p1.4.m4.1.1.3" xref="S4.SS5.p1.4.m4.1.1.3.cmml"><mi id="S4.SS5.p1.4.m4.1.1.3.2" xref="S4.SS5.p1.4.m4.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p1.4.m4.1.1.3.1" xref="S4.SS5.p1.4.m4.1.1.3.1.cmml">â€‹</mo><mi id="S4.SS5.p1.4.m4.1.1.3.3" xref="S4.SS5.p1.4.m4.1.1.3.3.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.4.m4.1b"><apply id="S4.SS5.p1.4.m4.1.1.cmml" xref="S4.SS5.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS5.p1.4.m4.1.1.1.cmml" xref="S4.SS5.p1.4.m4.1.1">subscript</csymbol><ci id="S4.SS5.p1.4.m4.1.1.2.cmml" xref="S4.SS5.p1.4.m4.1.1.2">ğ‘–</ci><apply id="S4.SS5.p1.4.m4.1.1.3.cmml" xref="S4.SS5.p1.4.m4.1.1.3"><times id="S4.SS5.p1.4.m4.1.1.3.1.cmml" xref="S4.SS5.p1.4.m4.1.1.3.1"></times><ci id="S4.SS5.p1.4.m4.1.1.3.2.cmml" xref="S4.SS5.p1.4.m4.1.1.3.2">ğ‘¡</ci><ci id="S4.SS5.p1.4.m4.1.1.3.3.cmml" xref="S4.SS5.p1.4.m4.1.1.3.3">â„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.4.m4.1c">i_{th}</annotation></semantics></math> joint.</p>
</div>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.1" class="ltx_p"><span id="S4.SS5.p2.1.1" class="ltx_text ltx_font_bold">PA-MPJPE</span>, also called Reconstruction Error, is the MPJPE after rigid alignment by post-processing between the estimated pose and the ground truth pose.</p>
</div>
<div id="S4.SS5.p3" class="ltx_para">
<p id="S4.SS5.p3.1" class="ltx_p"><span id="S4.SS5.p3.1.1" class="ltx_text ltx_font_bold">NMPJPE</span> is defined as the MPJPE after normalizing the predicted positions in scale to the reference <cite class="ltx_cite ltx_citemacro_citep">(Rhodin etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib207" title="" class="ltx_ref">2018b</a>)</cite>.</p>
</div>
<div id="S4.SS5.p4" class="ltx_para">
<p id="S4.SS5.p4.4" class="ltx_p"><span id="S4.SS5.p4.4.1" class="ltx_text ltx_font_bold">MPVE</span> (Mean Per Vertex Error) <cite class="ltx_cite ltx_citemacro_citep">(Pavlakos
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib187" title="" class="ltx_ref">2018b</a>)</cite> measures
the Euclidean distances between the ground truth vertices and the predicted vertices:</p>
<table id="S6.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S4.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(2)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.E2.m1.1" class="ltx_Math" alttext="\displaystyle\small{MPVE}=\frac{1}{N}\sum_{i=1}^{N}\|V_{i}-V_{i}^{*}\|_{2}," display="inline"><semantics id="S4.E2.m1.1a"><mrow id="S4.E2.m1.1.1.1" xref="S4.E2.m1.1.1.1.1.cmml"><mrow id="S4.E2.m1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.cmml"><mrow id="S4.E2.m1.1.1.1.1.3" xref="S4.E2.m1.1.1.1.1.3.cmml"><mi mathsize="90%" id="S4.E2.m1.1.1.1.1.3.2" xref="S4.E2.m1.1.1.1.1.3.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.1.1.3.1" xref="S4.E2.m1.1.1.1.1.3.1.cmml">â€‹</mo><mi mathsize="90%" id="S4.E2.m1.1.1.1.1.3.3" xref="S4.E2.m1.1.1.1.1.3.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.1.1.3.1a" xref="S4.E2.m1.1.1.1.1.3.1.cmml">â€‹</mo><mi mathsize="90%" id="S4.E2.m1.1.1.1.1.3.4" xref="S4.E2.m1.1.1.1.1.3.4.cmml">V</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.1.1.3.1b" xref="S4.E2.m1.1.1.1.1.3.1.cmml">â€‹</mo><mi mathsize="90%" id="S4.E2.m1.1.1.1.1.3.5" xref="S4.E2.m1.1.1.1.1.3.5.cmml">E</mi></mrow><mo mathsize="90%" id="S4.E2.m1.1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.2.cmml">=</mo><mrow id="S4.E2.m1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.1.cmml"><mstyle displaystyle="true" id="S4.E2.m1.1.1.1.1.1.3" xref="S4.E2.m1.1.1.1.1.1.3.cmml"><mfrac id="S4.E2.m1.1.1.1.1.1.3a" xref="S4.E2.m1.1.1.1.1.1.3.cmml"><mn mathsize="90%" id="S4.E2.m1.1.1.1.1.1.3.2" xref="S4.E2.m1.1.1.1.1.1.3.2.cmml">1</mn><mi mathsize="90%" id="S4.E2.m1.1.1.1.1.1.3.3" xref="S4.E2.m1.1.1.1.1.1.3.3.cmml">N</mi></mfrac></mstyle><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S4.E2.m1.1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.1.1.cmml"><mstyle displaystyle="true" id="S4.E2.m1.1.1.1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.1.1.2.cmml"><munderover id="S4.E2.m1.1.1.1.1.1.1.2a" xref="S4.E2.m1.1.1.1.1.1.1.2.cmml"><mo maxsize="90%" minsize="90%" movablelimits="false" stretchy="true" id="S4.E2.m1.1.1.1.1.1.1.2.2.2" xref="S4.E2.m1.1.1.1.1.1.1.2.2.2.cmml">âˆ‘</mo><mrow id="S4.E2.m1.1.1.1.1.1.1.2.2.3" xref="S4.E2.m1.1.1.1.1.1.1.2.2.3.cmml"><mi mathsize="90%" id="S4.E2.m1.1.1.1.1.1.1.2.2.3.2" xref="S4.E2.m1.1.1.1.1.1.1.2.2.3.2.cmml">i</mi><mo mathsize="90%" id="S4.E2.m1.1.1.1.1.1.1.2.2.3.1" xref="S4.E2.m1.1.1.1.1.1.1.2.2.3.1.cmml">=</mo><mn mathsize="90%" id="S4.E2.m1.1.1.1.1.1.1.2.2.3.3" xref="S4.E2.m1.1.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi mathsize="90%" id="S4.E2.m1.1.1.1.1.1.1.2.3" xref="S4.E2.m1.1.1.1.1.1.1.2.3.cmml">N</mi></munderover></mstyle><msub id="S4.E2.m1.1.1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.1.1.1.cmml"><mrow id="S4.E2.m1.1.1.1.1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.1.1.1.1.2.cmml"><mo maxsize="90%" minsize="90%" id="S4.E2.m1.1.1.1.1.1.1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.1.1.1.1.2.1.cmml">â€–</mo><mrow id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi mathsize="90%" id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">V</mi><mi mathsize="90%" id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo mathsize="90%" id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><msubsup id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi mathsize="90%" id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.3.2.2" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.3.2.2.cmml">V</mi><mi mathsize="90%" id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.3.2.3" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.3.2.3.cmml">i</mi><mo mathsize="90%" id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">âˆ—</mo></msubsup></mrow><mo maxsize="90%" minsize="90%" id="S4.E2.m1.1.1.1.1.1.1.1.1.1.3" xref="S4.E2.m1.1.1.1.1.1.1.1.1.2.1.cmml">â€–</mo></mrow><mn mathsize="90%" id="S4.E2.m1.1.1.1.1.1.1.1.3" xref="S4.E2.m1.1.1.1.1.1.1.1.3.cmml">2</mn></msub></mrow></mrow></mrow><mo mathsize="90%" id="S4.E2.m1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.1b"><apply id="S4.E2.m1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1"><eq id="S4.E2.m1.1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.2"></eq><apply id="S4.E2.m1.1.1.1.1.3.cmml" xref="S4.E2.m1.1.1.1.1.3"><times id="S4.E2.m1.1.1.1.1.3.1.cmml" xref="S4.E2.m1.1.1.1.1.3.1"></times><ci id="S4.E2.m1.1.1.1.1.3.2.cmml" xref="S4.E2.m1.1.1.1.1.3.2">ğ‘€</ci><ci id="S4.E2.m1.1.1.1.1.3.3.cmml" xref="S4.E2.m1.1.1.1.1.3.3">ğ‘ƒ</ci><ci id="S4.E2.m1.1.1.1.1.3.4.cmml" xref="S4.E2.m1.1.1.1.1.3.4">ğ‘‰</ci><ci id="S4.E2.m1.1.1.1.1.3.5.cmml" xref="S4.E2.m1.1.1.1.1.3.5">ğ¸</ci></apply><apply id="S4.E2.m1.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.1"><times id="S4.E2.m1.1.1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.1.2"></times><apply id="S4.E2.m1.1.1.1.1.1.3.cmml" xref="S4.E2.m1.1.1.1.1.1.3"><divide id="S4.E2.m1.1.1.1.1.1.3.1.cmml" xref="S4.E2.m1.1.1.1.1.1.3"></divide><cn type="integer" id="S4.E2.m1.1.1.1.1.1.3.2.cmml" xref="S4.E2.m1.1.1.1.1.1.3.2">1</cn><ci id="S4.E2.m1.1.1.1.1.1.3.3.cmml" xref="S4.E2.m1.1.1.1.1.1.3.3">ğ‘</ci></apply><apply id="S4.E2.m1.1.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1"><apply id="S4.E2.m1.1.1.1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S4.E2.m1.1.1.1.1.1.1.2.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.1.1.2.2.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.2">subscript</csymbol><sum id="S4.E2.m1.1.1.1.1.1.1.2.2.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.2.2.2"></sum><apply id="S4.E2.m1.1.1.1.1.1.1.2.2.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.2.2.3"><eq id="S4.E2.m1.1.1.1.1.1.1.2.2.3.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.2.2.3.1"></eq><ci id="S4.E2.m1.1.1.1.1.1.1.2.2.3.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.2.2.3.2">ğ‘–</ci><cn type="integer" id="S4.E2.m1.1.1.1.1.1.1.2.2.3.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S4.E2.m1.1.1.1.1.1.1.2.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.2.3">ğ‘</ci></apply><apply id="S4.E2.m1.1.1.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S4.E2.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S4.E2.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1"><minus id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1"></minus><apply id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2">ğ‘‰</ci><ci id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.2.3">ğ‘–</ci></apply><apply id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.3">superscript</csymbol><apply id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.3.2.2">ğ‘‰</ci><ci id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.3.2.3">ğ‘–</ci></apply><times id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3"></times></apply></apply></apply><cn type="integer" id="S4.E2.m1.1.1.1.1.1.1.1.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.1c">\displaystyle\small{MPVE}=\frac{1}{N}\sum_{i=1}^{N}\|V_{i}-V_{i}^{*}\|_{2},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S4.SS5.p4.3" class="ltx_p">where <math id="S4.SS5.p4.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.SS5.p4.1.m1.1a"><mi id="S4.SS5.p4.1.m1.1.1" xref="S4.SS5.p4.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.p4.1.m1.1b"><ci id="S4.SS5.p4.1.m1.1.1.cmml" xref="S4.SS5.p4.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p4.1.m1.1c">N</annotation></semantics></math> is the number of vertices, <math id="S4.SS5.p4.2.m2.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S4.SS5.p4.2.m2.1a"><mi id="S4.SS5.p4.2.m2.1.1" xref="S4.SS5.p4.2.m2.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.p4.2.m2.1b"><ci id="S4.SS5.p4.2.m2.1.1.cmml" xref="S4.SS5.p4.2.m2.1.1">ğ‘‰</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p4.2.m2.1c">V</annotation></semantics></math> is the ground truth vertices, and <math id="S4.SS5.p4.3.m3.1" class="ltx_Math" alttext="V^{*}" display="inline"><semantics id="S4.SS5.p4.3.m3.1a"><msup id="S4.SS5.p4.3.m3.1.1" xref="S4.SS5.p4.3.m3.1.1.cmml"><mi id="S4.SS5.p4.3.m3.1.1.2" xref="S4.SS5.p4.3.m3.1.1.2.cmml">V</mi><mo id="S4.SS5.p4.3.m3.1.1.3" xref="S4.SS5.p4.3.m3.1.1.3.cmml">âˆ—</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS5.p4.3.m3.1b"><apply id="S4.SS5.p4.3.m3.1.1.cmml" xref="S4.SS5.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS5.p4.3.m3.1.1.1.cmml" xref="S4.SS5.p4.3.m3.1.1">superscript</csymbol><ci id="S4.SS5.p4.3.m3.1.1.2.cmml" xref="S4.SS5.p4.3.m3.1.1.2">ğ‘‰</ci><times id="S4.SS5.p4.3.m3.1.1.3.cmml" xref="S4.SS5.p4.3.m3.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p4.3.m3.1c">V^{*}</annotation></semantics></math> is the estimated vertices.</p>
</div>
<div id="S4.SS5.p5" class="ltx_para">
<p id="S4.SS5.p5.1" class="ltx_p"><span id="S4.SS5.p5.1.1" class="ltx_text ltx_font_bold">3DPCK</span> is a 3D extended version of the Percentage of Correct Keypoints (PCK) metric used in 2D HPE evaluation. An estimated joint is considered as correct if the distance between the estimation and the ground truth is within a certain threshold. Generally, the threshold is set to 150<math id="S4.SS5.p5.1.m1.1" class="ltx_Math" alttext="mm" display="inline"><semantics id="S4.SS5.p5.1.m1.1a"><mrow id="S4.SS5.p5.1.m1.1.1" xref="S4.SS5.p5.1.m1.1.1.cmml"><mi id="S4.SS5.p5.1.m1.1.1.2" xref="S4.SS5.p5.1.m1.1.1.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p5.1.m1.1.1.1" xref="S4.SS5.p5.1.m1.1.1.1.cmml">â€‹</mo><mi id="S4.SS5.p5.1.m1.1.1.3" xref="S4.SS5.p5.1.m1.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p5.1.m1.1b"><apply id="S4.SS5.p5.1.m1.1.1.cmml" xref="S4.SS5.p5.1.m1.1.1"><times id="S4.SS5.p5.1.m1.1.1.1.cmml" xref="S4.SS5.p5.1.m1.1.1.1"></times><ci id="S4.SS5.p5.1.m1.1.1.2.cmml" xref="S4.SS5.p5.1.m1.1.1.2">ğ‘š</ci><ci id="S4.SS5.p5.1.m1.1.1.3.cmml" xref="S4.SS5.p5.1.m1.1.1.3">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p5.1.m1.1c">mm</annotation></semantics></math>.</p>
</div>
<figure id="S4.T8" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.T8.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:195.1pt;">
<figcaption class="ltx_caption ltx_centering" style="font-size:50%;"><span class="ltx_tag ltx_tag_figure">Table 7. </span><span id="S4.T8.1.4.1" class="ltx_text" style="font-size:160%;">Comparison of different 3D single-view multi-person HPE approaches on the MuPoTS-3D dataset. The reported fps is taken from <cite class="ltx_cite ltx_citemacro_citep">(Wang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib254" title="" class="ltx_ref">2022b</a>)</cite>.</span></figcaption>
<table id="S4.T8.1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T8.1.1.2.1" class="ltx_tr">
<th id="S4.T8.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding:-0.25pt 3.0pt;" colspan="6"><span id="S4.T8.1.1.2.1.1.1" class="ltx_text" style="font-size:50%;">MuPoTS-3D</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T8.1.1.1" class="ltx_tr">
<th id="S4.T8.1.1.1.2" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding:-0.25pt 3.0pt;"></th>
<th id="S4.T8.1.1.1.3" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.25pt 3.0pt;"></th>
<th id="S4.T8.1.1.1.4" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.25pt 3.0pt;"></th>
<td id="S4.T8.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.25pt 3.0pt;" colspan="2">
<span id="S4.T8.1.1.1.1.1" class="ltx_text" style="font-size:50%;">3DPCK </span><math id="S4.T8.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T8.1.1.1.1.m1.1a"><mo mathsize="50%" stretchy="false" id="S4.T8.1.1.1.1.m1.1.1" xref="S4.T8.1.1.1.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T8.1.1.1.1.m1.1b"><ci id="S4.T8.1.1.1.1.m1.1.1.cmml" xref="S4.T8.1.1.1.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T8.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.25pt 3.0pt;" rowspan="2"><span id="S4.T8.1.1.1.5.1" class="ltx_text" style="font-size:50%;">fps</span></td>
</tr>
<tr id="S4.T8.1.1.3.1" class="ltx_tr">
<th id="S4.T8.1.1.3.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding:-0.25pt 3.0pt;"></th>
<th id="S4.T8.1.1.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.3.1.2.1" class="ltx_text" style="font-size:50%;">Year</span></th>
<th id="S4.T8.1.1.3.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.3.1.3.1" class="ltx_text" style="font-size:50%;">Method</span></th>
<td id="S4.T8.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.3.1.4.1" class="ltx_text" style="font-size:50%;">All people</span></td>
<td id="S4.T8.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.3.1.5.1" class="ltx_text" style="font-size:50%;">Matched people</span></td>
</tr>
<tr id="S4.T8.1.1.4.2" class="ltx_tr">
<th id="S4.T8.1.1.4.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding:-0.25pt 3.0pt;"></th>
<th id="S4.T8.1.1.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.4.2.2.1" class="ltx_text" style="font-size:50%;">2019</span></th>
<th id="S4.T8.1.1.4.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.25pt 3.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T8.1.1.4.2.3.1.1" class="ltx_text" style="font-size:50%;">(</span>Moon
etÂ al<span class="ltx_text">.</span><span id="S4.T8.1.1.4.2.3.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib166" title="" class="ltx_ref">2019a</a><span id="S4.T8.1.1.4.2.3.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></th>
<td id="S4.T8.1.1.4.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.4.2.4.1" class="ltx_text" style="font-size:50%;">81.8</span></td>
<td id="S4.T8.1.1.4.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.4.2.5.1" class="ltx_text" style="font-size:50%;color:#FF0000;">82.5</span></td>
<td id="S4.T8.1.1.4.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.4.2.6.1" class="ltx_text" style="font-size:50%;">9.3</span></td>
</tr>
<tr id="S4.T8.1.1.5.3" class="ltx_tr">
<th id="S4.T8.1.1.5.3.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding:-0.25pt 3.0pt;"></th>
<th id="S4.T8.1.1.5.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.5.3.2.1" class="ltx_text" style="font-size:50%;">2020</span></th>
<th id="S4.T8.1.1.5.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.25pt 3.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T8.1.1.5.3.3.1.1" class="ltx_text" style="font-size:50%;">(</span>Jiang etÂ al<span class="ltx_text">.</span><span id="S4.T8.1.1.5.3.3.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib88" title="" class="ltx_ref">2020</a><span id="S4.T8.1.1.5.3.3.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></th>
<td id="S4.T8.1.1.5.3.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.5.3.4.1" class="ltx_text" style="font-size:50%;">69.1</span></td>
<td id="S4.T8.1.1.5.3.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.5.3.5.1" class="ltx_text" style="font-size:50%;">72.2</span></td>
<td id="S4.T8.1.1.5.3.6" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.5.3.6.1" class="ltx_text" style="font-size:50%;">-</span></td>
</tr>
<tr id="S4.T8.1.1.6.4" class="ltx_tr">
<th id="S4.T8.1.1.6.4.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding:-0.25pt 3.0pt;"></th>
<th id="S4.T8.1.1.6.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.6.4.2.1" class="ltx_text" style="font-size:50%;">2020</span></th>
<th id="S4.T8.1.1.6.4.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.25pt 3.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T8.1.1.6.4.3.1.1" class="ltx_text" style="font-size:50%;">(</span>Li
etÂ al<span class="ltx_text">.</span><span id="S4.T8.1.1.6.4.3.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib114" title="" class="ltx_ref">2020b</a><span id="S4.T8.1.1.6.4.3.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></th>
<td id="S4.T8.1.1.6.4.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.6.4.4.1" class="ltx_text" style="font-size:50%;">82.0</span></td>
<td id="S4.T8.1.1.6.4.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.6.4.5.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T8.1.1.6.4.6" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.6.4.6.1" class="ltx_text" style="font-size:50%;">-</span></td>
</tr>
<tr id="S4.T8.1.1.7.5" class="ltx_tr">
<th id="S4.T8.1.1.7.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.7.5.1.1" class="ltx_text" style="font-size:50%;">Top down</span></th>
<th id="S4.T8.1.1.7.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.7.5.2.1" class="ltx_text" style="font-size:50%;">2021</span></th>
<th id="S4.T8.1.1.7.5.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.25pt 3.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T8.1.1.7.5.3.1.1" class="ltx_text" style="font-size:50%;">(</span>Cheng
etÂ al<span class="ltx_text">.</span><span id="S4.T8.1.1.7.5.3.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib33" title="" class="ltx_ref">2021a</a><span id="S4.T8.1.1.7.5.3.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></th>
<td id="S4.T8.1.1.7.5.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.7.5.4.1" class="ltx_text" style="font-size:50%;color:#0000FF;">87.5</span></td>
<td id="S4.T8.1.1.7.5.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.7.5.5.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T8.1.1.7.5.6" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.7.5.6.1" class="ltx_text" style="font-size:50%;">-</span></td>
</tr>
<tr id="S4.T8.1.1.8.6" class="ltx_tr">
<th id="S4.T8.1.1.8.6.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding:-0.25pt 3.0pt;"></th>
<th id="S4.T8.1.1.8.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.8.6.2.1" class="ltx_text" style="font-size:50%;">2018</span></th>
<th id="S4.T8.1.1.8.6.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.25pt 3.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T8.1.1.8.6.3.1.1" class="ltx_text" style="font-size:50%;">(</span>Mehta etÂ al<span class="ltx_text">.</span><span id="S4.T8.1.1.8.6.3.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib160" title="" class="ltx_ref">2018</a><span id="S4.T8.1.1.8.6.3.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></th>
<td id="S4.T8.1.1.8.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.8.6.4.1" class="ltx_text" style="font-size:50%;">65.0</span></td>
<td id="S4.T8.1.1.8.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.8.6.5.1" class="ltx_text" style="font-size:50%;">69.8</span></td>
<td id="S4.T8.1.1.8.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.8.6.6.1" class="ltx_text" style="font-size:50%;">-</span></td>
</tr>
<tr id="S4.T8.1.1.9.7" class="ltx_tr">
<th id="S4.T8.1.1.9.7.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding:-0.25pt 3.0pt;"></th>
<th id="S4.T8.1.1.9.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.9.7.2.1" class="ltx_text" style="font-size:50%;">2019</span></th>
<th id="S4.T8.1.1.9.7.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.25pt 3.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T8.1.1.9.7.3.1.1" class="ltx_text" style="font-size:50%;">(</span>Mehta etÂ al<span class="ltx_text">.</span><span id="S4.T8.1.1.9.7.3.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib159" title="" class="ltx_ref">2020</a><span id="S4.T8.1.1.9.7.3.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></th>
<td id="S4.T8.1.1.9.7.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.9.7.4.1" class="ltx_text" style="font-size:50%;">70.4</span></td>
<td id="S4.T8.1.1.9.7.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.9.7.5.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T8.1.1.9.7.6" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.9.7.6.1" class="ltx_text" style="font-size:50%;">-</span></td>
</tr>
<tr id="S4.T8.1.1.10.8" class="ltx_tr">
<th id="S4.T8.1.1.10.8.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding:-0.25pt 3.0pt;"></th>
<th id="S4.T8.1.1.10.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.10.8.2.1" class="ltx_text" style="font-size:50%;">2020</span></th>
<th id="S4.T8.1.1.10.8.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.25pt 3.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T8.1.1.10.8.3.1.1" class="ltx_text" style="font-size:50%;">(</span>Benzine etÂ al<span class="ltx_text">.</span><span id="S4.T8.1.1.10.8.3.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib9" title="" class="ltx_ref">2020</a><span id="S4.T8.1.1.10.8.3.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></th>
<td id="S4.T8.1.1.10.8.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.10.8.4.1" class="ltx_text" style="font-size:50%;">72.0</span></td>
<td id="S4.T8.1.1.10.8.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.10.8.5.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T8.1.1.10.8.6" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.10.8.6.1" class="ltx_text" style="font-size:50%;">-</span></td>
</tr>
<tr id="S4.T8.1.1.11.9" class="ltx_tr">
<th id="S4.T8.1.1.11.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.11.9.1.1" class="ltx_text" style="font-size:50%;">Bottom up</span></th>
<th id="S4.T8.1.1.11.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.11.9.2.1" class="ltx_text" style="font-size:50%;">2020</span></th>
<th id="S4.T8.1.1.11.9.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:-0.25pt 3.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T8.1.1.11.9.3.1.1" class="ltx_text" style="font-size:50%;">(</span>Zhen etÂ al<span class="ltx_text">.</span><span id="S4.T8.1.1.11.9.3.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib301" title="" class="ltx_ref">2020</a><span id="S4.T8.1.1.11.9.3.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></th>
<td id="S4.T8.1.1.11.9.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.11.9.4.1" class="ltx_text" style="font-size:50%;">73.5</span></td>
<td id="S4.T8.1.1.11.9.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.11.9.5.1" class="ltx_text" style="font-size:50%;color:#0000FF;">80.5</span></td>
<td id="S4.T8.1.1.11.9.6" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.11.9.6.1" class="ltx_text" style="font-size:50%;">9.3</span></td>
</tr>
<tr id="S4.T8.1.1.12.10" class="ltx_tr">
<th id="S4.T8.1.1.12.10.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding:-0.25pt 3.0pt;"></th>
<th id="S4.T8.1.1.12.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.12.10.2.1" class="ltx_text" style="font-size:50%;">2021</span></th>
<th id="S4.T8.1.1.12.10.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.25pt 3.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T8.1.1.12.10.3.1.1" class="ltx_text" style="font-size:50%;">(</span>Cheng
etÂ al<span class="ltx_text">.</span><span id="S4.T8.1.1.12.10.3.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib34" title="" class="ltx_ref">2021b</a><span id="S4.T8.1.1.12.10.3.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></th>
<td id="S4.T8.1.1.12.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.12.10.4.1" class="ltx_text" style="font-size:50%;color:#FF0000;">89.6</span></td>
<td id="S4.T8.1.1.12.10.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.12.10.5.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T8.1.1.12.10.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.12.10.6.1" class="ltx_text" style="font-size:50%;">-</span></td>
</tr>
<tr id="S4.T8.1.1.13.11" class="ltx_tr">
<th id="S4.T8.1.1.13.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.13.11.1.1" class="ltx_text" style="font-size:50%;">Integrated</span></th>
<th id="S4.T8.1.1.13.11.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.13.11.2.1" class="ltx_text" style="font-size:50%;">2022</span></th>
<th id="S4.T8.1.1.13.11.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding:-0.25pt 3.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T8.1.1.13.11.3.1.1" class="ltx_text" style="font-size:50%;">(</span>Wang
etÂ al<span class="ltx_text">.</span><span id="S4.T8.1.1.13.11.3.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib254" title="" class="ltx_ref">2022b</a><span id="S4.T8.1.1.13.11.3.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></th>
<td id="S4.T8.1.1.13.11.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.13.11.4.1" class="ltx_text" style="font-size:50%;">82.7</span></td>
<td id="S4.T8.1.1.13.11.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.13.11.5.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T8.1.1.13.11.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding:-0.25pt 3.0pt;"><span id="S4.T8.1.1.13.11.6.1" class="ltx_text" style="font-size:50%;">13.3</span></td>
</tr>
</tbody>
</table>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.T8.4" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:195.1pt;">
<figcaption class="ltx_caption ltx_centering" style="font-size:50%;"><span class="ltx_tag ltx_tag_figure">Table 8. </span><span id="S4.T8.4.6.1" class="ltx_text" style="font-size:160%;">Comparison of different 3D multi-view HPE approaches on the Human3.6M dataset. </span></figcaption>
<table id="S4.T8.4.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T8.4.3.4.1" class="ltx_tr">
<td id="S4.T8.4.3.4.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding:-0.25pt 1.0pt;" colspan="7"><span id="S4.T8.4.3.4.1.1.1" class="ltx_text" style="font-size:50%;">Human3.6M</span></td>
</tr>
<tr id="S4.T8.4.3.5.2" class="ltx_tr">
<td id="S4.T8.4.3.5.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_l ltx_border_r ltx_border_t" style="padding:-0.25pt 1.0pt;"></td>
<td id="S4.T8.4.3.5.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_r ltx_border_t" style="padding:-0.25pt 1.0pt;"></td>
<td id="S4.T8.4.3.5.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_r ltx_border_t" style="padding:-0.25pt 1.0pt;"></td>
<td id="S4.T8.4.3.5.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.25pt 1.0pt;" colspan="2"><span id="S4.T8.4.3.5.2.4.1" class="ltx_text" style="font-size:50%;">Protocol 1</span></td>
<td id="S4.T8.4.3.5.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.5.2.5.1" class="ltx_text" style="font-size:50%;">Protocol 1</span></td>
<td id="S4.T8.4.3.5.2.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.25pt 1.0pt;" rowspan="2"><span id="S4.T8.4.3.5.2.6.1" class="ltx_text" style="font-size:50%;">FLOPs(G)</span></td>
</tr>
<tr id="S4.T8.4.3.3" class="ltx_tr">
<td id="S4.T8.4.3.3.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_l ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.3.4.1" class="ltx_text" style="font-size:50%;">Year</span></td>
<td id="S4.T8.4.3.3.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.3.5.1" class="ltx_text" style="font-size:50%;">Method</span></td>
<td id="S4.T8.4.3.3.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.3.6.1" class="ltx_text" style="font-size:50%;">
<span id="S4.T8.4.3.3.6.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T8.4.3.3.6.1.1.1" class="ltx_tr">
<span id="S4.T8.4.3.3.6.1.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:-0.25pt 1.0pt;">Use extra</span></span>
<span id="S4.T8.4.3.3.6.1.1.2" class="ltx_tr">
<span id="S4.T8.4.3.3.6.1.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:-0.25pt 1.0pt;">3D data</span></span>
</span></span></td>
<td id="S4.T8.2.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.2.1.1.1.1" class="ltx_text" style="font-size:50%;">MPJPE <math id="S4.T8.2.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T8.2.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T8.2.1.1.1.1.m1.1.1" xref="S4.T8.2.1.1.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T8.2.1.1.1.1.m1.1b"><ci id="S4.T8.2.1.1.1.1.m1.1.1.cmml" xref="S4.T8.2.1.1.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.2.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math></span></td>
<td id="S4.T8.3.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.3.2.2.2.1" class="ltx_text" style="font-size:50%;">
<span id="S4.T8.3.2.2.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T8.3.2.2.2.1.1.2" class="ltx_tr">
<span id="S4.T8.3.2.2.2.1.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:-0.25pt 1.0pt;">Normalized</span></span>
<span id="S4.T8.3.2.2.2.1.1.1" class="ltx_tr">
<span id="S4.T8.3.2.2.2.1.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:-0.25pt 1.0pt;">MPJPE <math id="S4.T8.3.2.2.2.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T8.3.2.2.2.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T8.3.2.2.2.1.1.1.1.m1.1.1" xref="S4.T8.3.2.2.2.1.1.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T8.3.2.2.2.1.1.1.1.m1.1b"><ci id="S4.T8.3.2.2.2.1.1.1.1.m1.1.1.cmml" xref="S4.T8.3.2.2.2.1.1.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.3.2.2.2.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math></span></span>
</span></span></td>
<td id="S4.T8.4.3.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.3.3.1" class="ltx_text" style="font-size:50%;">PA-MPJPE <math id="S4.T8.4.3.3.3.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T8.4.3.3.3.1.m1.1a"><mo stretchy="false" id="S4.T8.4.3.3.3.1.m1.1.1" xref="S4.T8.4.3.3.3.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T8.4.3.3.3.1.m1.1b"><ci id="S4.T8.4.3.3.3.1.m1.1.1.cmml" xref="S4.T8.4.3.3.3.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.4.3.3.3.1.m1.1c">\downarrow</annotation></semantics></math></span></td>
</tr>
<tr id="S4.T8.4.3.6.3" class="ltx_tr">
<td id="S4.T8.4.3.6.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.6.3.1.1" class="ltx_text" style="font-size:50%;">2019</span></td>
<td id="S4.T8.4.3.6.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.25pt 1.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T8.4.3.6.3.2.1.1" class="ltx_text" style="font-size:50%;">(</span>Liang and Lin<span id="S4.T8.4.3.6.3.2.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib129" title="" class="ltx_ref">2019</a><span id="S4.T8.4.3.6.3.2.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></td>
<td id="S4.T8.4.3.6.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.6.3.3.1" class="ltx_text" style="font-size:50%;">Yes</span></td>
<td id="S4.T8.4.3.6.3.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.6.3.4.1" class="ltx_text" style="font-size:50%;">79.9</span></td>
<td id="S4.T8.4.3.6.3.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.6.3.5.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T8.4.3.6.3.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.6.3.6.1" class="ltx_text" style="font-size:50%;color:#FF0000;">45.1</span></td>
<td id="S4.T8.4.3.6.3.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.6.3.7.1" class="ltx_text" style="font-size:50%;">-</span></td>
</tr>
<tr id="S4.T8.4.3.7.4" class="ltx_tr">
<td id="S4.T8.4.3.7.4.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_l ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.7.4.1.1" class="ltx_text" style="font-size:50%;">2019</span></td>
<td id="S4.T8.4.3.7.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T8.4.3.7.4.2.1.1" class="ltx_text" style="font-size:50%;">(</span>Kocabas
etÂ al<span class="ltx_text">.</span><span id="S4.T8.4.3.7.4.2.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib101" title="" class="ltx_ref">2019</a><span id="S4.T8.4.3.7.4.2.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></td>
<td id="S4.T8.4.3.7.4.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.7.4.3.1" class="ltx_text" style="font-size:50%;">No</span></td>
<td id="S4.T8.4.3.7.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.7.4.4.1" class="ltx_text" style="font-size:50%;">60.6</span></td>
<td id="S4.T8.4.3.7.4.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.7.4.5.1" class="ltx_text" style="font-size:50%;color:#FF0000;">60.0</span></td>
<td id="S4.T8.4.3.7.4.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.7.4.6.1" class="ltx_text" style="font-size:50%;color:#0000FF;">47.5</span></td>
<td id="S4.T8.4.3.7.4.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.7.4.7.1" class="ltx_text" style="font-size:50%;">-</span></td>
</tr>
<tr id="S4.T8.4.3.8.5" class="ltx_tr">
<td id="S4.T8.4.3.8.5.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_l ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.8.5.1.1" class="ltx_text" style="font-size:50%;">2019</span></td>
<td id="S4.T8.4.3.8.5.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T8.4.3.8.5.2.1.1" class="ltx_text" style="font-size:50%;">(</span>Qiu
etÂ al<span class="ltx_text">.</span><span id="S4.T8.4.3.8.5.2.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib199" title="" class="ltx_ref">2019</a><span id="S4.T8.4.3.8.5.2.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></td>
<td id="S4.T8.4.3.8.5.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.8.5.3.1" class="ltx_text" style="font-size:50%;">No</span></td>
<td id="S4.T8.4.3.8.5.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.8.5.4.1" class="ltx_text" style="font-size:50%;">31.2</span></td>
<td id="S4.T8.4.3.8.5.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.8.5.5.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T8.4.3.8.5.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.8.5.6.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T8.4.3.8.5.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.8.5.7.1" class="ltx_text" style="font-size:50%;">55</span></td>
</tr>
<tr id="S4.T8.4.3.9.6" class="ltx_tr">
<td id="S4.T8.4.3.9.6.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_l ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.9.6.1.1" class="ltx_text" style="font-size:50%;">2019</span></td>
<td id="S4.T8.4.3.9.6.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T8.4.3.9.6.2.1.1" class="ltx_text" style="font-size:50%;">(</span>Qiu
etÂ al<span class="ltx_text">.</span><span id="S4.T8.4.3.9.6.2.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib199" title="" class="ltx_ref">2019</a><span id="S4.T8.4.3.9.6.2.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></td>
<td id="S4.T8.4.3.9.6.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.9.6.3.1" class="ltx_text" style="font-size:50%;">Yes</span></td>
<td id="S4.T8.4.3.9.6.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.9.6.4.1" class="ltx_text" style="font-size:50%;">26.2</span></td>
<td id="S4.T8.4.3.9.6.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.9.6.5.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T8.4.3.9.6.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.9.6.6.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T8.4.3.9.6.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.9.6.7.1" class="ltx_text" style="font-size:50%;">55</span></td>
</tr>
<tr id="S4.T8.4.3.10.7" class="ltx_tr">
<td id="S4.T8.4.3.10.7.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_l ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.10.7.1.1" class="ltx_text" style="font-size:50%;">2020</span></td>
<td id="S4.T8.4.3.10.7.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T8.4.3.10.7.2.1.1" class="ltx_text" style="font-size:50%;">(</span>Remelli
etÂ al<span class="ltx_text">.</span><span id="S4.T8.4.3.10.7.2.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib204" title="" class="ltx_ref">2020</a><span id="S4.T8.4.3.10.7.2.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></td>
<td id="S4.T8.4.3.10.7.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.10.7.3.1" class="ltx_text" style="font-size:50%;">No</span></td>
<td id="S4.T8.4.3.10.7.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.10.7.4.1" class="ltx_text" style="font-size:50%;">30.2</span></td>
<td id="S4.T8.4.3.10.7.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.10.7.5.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T8.4.3.10.7.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.10.7.6.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T8.4.3.10.7.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.10.7.7.1" class="ltx_text" style="font-size:50%;">-</span></td>
</tr>
<tr id="S4.T8.4.3.11.8" class="ltx_tr">
<td id="S4.T8.4.3.11.8.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_l ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.11.8.1.1" class="ltx_text" style="font-size:50%;">2020</span></td>
<td id="S4.T8.4.3.11.8.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T8.4.3.11.8.2.1.1" class="ltx_text" style="font-size:50%;">(</span>Xie
etÂ al<span class="ltx_text">.</span><span id="S4.T8.4.3.11.8.2.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib261" title="" class="ltx_ref">2020</a><span id="S4.T8.4.3.11.8.2.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></td>
<td id="S4.T8.4.3.11.8.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.11.8.3.1" class="ltx_text" style="font-size:50%;">No</span></td>
<td id="S4.T8.4.3.11.8.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.11.8.4.1" class="ltx_text" style="font-size:50%;">29.3</span></td>
<td id="S4.T8.4.3.11.8.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.11.8.5.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T8.4.3.11.8.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.11.8.6.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T8.4.3.11.8.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.11.8.7.1" class="ltx_text" style="font-size:50%;">-</span></td>
</tr>
<tr id="S4.T8.4.3.12.9" class="ltx_tr">
<td id="S4.T8.4.3.12.9.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_l ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.12.9.1.1" class="ltx_text" style="font-size:50%;">2020</span></td>
<td id="S4.T8.4.3.12.9.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T8.4.3.12.9.2.1.1" class="ltx_text" style="font-size:50%;">(</span>Zhang
etÂ al<span class="ltx_text">.</span><span id="S4.T8.4.3.12.9.2.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib296" title="" class="ltx_ref">2020g</a><span id="S4.T8.4.3.12.9.2.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></td>
<td id="S4.T8.4.3.12.9.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.12.9.3.1" class="ltx_text" style="font-size:50%;">Yes</span></td>
<td id="S4.T8.4.3.12.9.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.12.9.4.1" class="ltx_text" style="font-size:50%;color:#0000FF;">19.5</span></td>
<td id="S4.T8.4.3.12.9.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.12.9.5.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T8.4.3.12.9.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.12.9.6.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T8.4.3.12.9.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.12.9.7.1" class="ltx_text" style="font-size:50%;">-</span></td>
</tr>
<tr id="S4.T8.4.3.13.10" class="ltx_tr">
<td id="S4.T8.4.3.13.10.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_l ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.13.10.1.1" class="ltx_text" style="font-size:50%;">2021</span></td>
<td id="S4.T8.4.3.13.10.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T8.4.3.13.10.2.1.1" class="ltx_text" style="font-size:50%;">(</span>Ma etÂ al<span class="ltx_text">.</span><span id="S4.T8.4.3.13.10.2.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib150" title="" class="ltx_ref">2021</a><span id="S4.T8.4.3.13.10.2.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></td>
<td id="S4.T8.4.3.13.10.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.13.10.3.1" class="ltx_text" style="font-size:50%;">No</span></td>
<td id="S4.T8.4.3.13.10.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.13.10.4.1" class="ltx_text" style="font-size:50%;">25.8</span></td>
<td id="S4.T8.4.3.13.10.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.13.10.5.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T8.4.3.13.10.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.13.10.6.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T8.4.3.13.10.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.13.10.7.1" class="ltx_text" style="font-size:50%;">50</span></td>
</tr>
<tr id="S4.T8.4.3.14.11" class="ltx_tr">
<td id="S4.T8.4.3.14.11.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_l ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.14.11.1.1" class="ltx_text" style="font-size:50%;">2021</span></td>
<td id="S4.T8.4.3.14.11.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T8.4.3.14.11.2.1.1" class="ltx_text" style="font-size:50%;">(</span>Reddy etÂ al<span class="ltx_text">.</span><span id="S4.T8.4.3.14.11.2.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib203" title="" class="ltx_ref">2021</a><span id="S4.T8.4.3.14.11.2.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></td>
<td id="S4.T8.4.3.14.11.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.14.11.3.1" class="ltx_text" style="font-size:50%;">Yes</span></td>
<td id="S4.T8.4.3.14.11.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.14.11.4.1" class="ltx_text" style="font-size:50%;color:#FF0000;">18.7</span></td>
<td id="S4.T8.4.3.14.11.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.14.11.5.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T8.4.3.14.11.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.14.11.6.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T8.4.3.14.11.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.14.11.7.1" class="ltx_text" style="font-size:50%;">-</span></td>
</tr>
<tr id="S4.T8.4.3.15.12" class="ltx_tr">
<td id="S4.T8.4.3.15.12.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_l ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.15.12.1.1" class="ltx_text" style="font-size:50%;">2022</span></td>
<td id="S4.T8.4.3.15.12.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r" style="padding:-0.25pt 1.0pt;"><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T8.4.3.15.12.2.1.1" class="ltx_text" style="font-size:50%;">(</span>Ma etÂ al<span class="ltx_text">.</span><span id="S4.T8.4.3.15.12.2.2.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib151" title="" class="ltx_ref">2022</a><span id="S4.T8.4.3.15.12.2.3.3" class="ltx_text" style="font-size:50%;">)</span></cite></td>
<td id="S4.T8.4.3.15.12.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.15.12.3.1" class="ltx_text" style="font-size:50%;">Yes</span></td>
<td id="S4.T8.4.3.15.12.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.15.12.4.1" class="ltx_text" style="font-size:50%;">24.4</span></td>
<td id="S4.T8.4.3.15.12.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.15.12.5.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T8.4.3.15.12.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.15.12.6.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T8.4.3.15.12.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r" style="padding:-0.25pt 1.0pt;"><span id="S4.T8.4.3.15.12.7.1" class="ltx_text" style="font-size:50%;">15</span></td>
</tr>
</tbody>
</table>
</figure>
</div>
</div>
</figure>
<div id="S4.SS5.p6" class="ltx_para">
<p id="S4.SS5.p6.1" class="ltx_p"><span id="S4.SS5.p6.1.1" class="ltx_text ltx_font_bold">Summary.</span> As pointed out by Ji et al. <cite class="ltx_cite ltx_citemacro_citep">(JI
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib85" title="" class="ltx_ref">2020</a>)</cite>, low MPJPE does not always indicate an accurate pose estimation as it depends on the predicted scale of human shape and skeleton. Although 3DPCK is more robust to incorrect joints, it cannot evaluate the precision of correct joints.
Existing metrics are designed to evaluate the precision of an estimated pose in a single frame. However, the temporal consistency and smoothness of the reconstructed human pose cannot be examined over continuous frames by existing evaluation metrics. Designing frame-level evaluation metrics that can evaluate 3D HPE performance with temporal consistency and smoothness remains an open problem.</p>
</div>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6. </span>Performance Comparison of 3D HPE Methods</h3>

<div id="S4.SS6.p1" class="ltx_para">
<p id="S4.SS6.p1.1" class="ltx_p"><span id="S4.SS6.p1.1.1" class="ltx_text ltx_font_bold">Single-view single-person</span>: In Table <a href="#S4.T6" title="Table 6 â€£ 4.4. Datasets for 3D HPE â€£ 4. Datasets and Evaluation Metrics â€£ Deep Learning-Based Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, it is seen that most 3D single-view single-person HPE methods estimate 3D human pose with remarkable precision on the Human3.6M dataset. However, despite the fact that the Human3.6M dataset has a large size of training and testing data, it only contains 11 actors performing 17 activities in lab environments. When estimating 3D pose on in-the-wild data with more complex scenarios, the performance of these methods degrades quickly. Estimating 3D pose from videos can achieve better performance than from a single image because the temporal consistency is preserved.</p>
</div>
<div id="S4.SS6.p2" class="ltx_para">
<p id="S4.SS6.p2.1" class="ltx_p">For skeleton-only methods, 2D-to-3D lifting approaches generally outperform direct estimation approaches due to the excellent performance of state-of-the-art 2D pose detectors.
Beyond estimated 3D coordinates of joints, a group of methods utilized volumetric models such as SMPL to recover human mesh. These methods still reported MPJPE of joints since the datasets do not provide the ground truth mesh vertices. However, the reported MPJPE is higher than those methods that only estimate 3D joints. One of the reasons is that these methods regressed both pose parameters and shape parameters, then fed in the volumetric model for mesh reconstruction, only evaluating MPJPE of joints can not show their strength.</p>
</div>
<div id="S4.SS6.p3" class="ltx_para">
<p id="S4.SS6.p3.1" class="ltx_p"><span id="S4.SS6.p3.1.1" class="ltx_text ltx_font_bold">Single-view multi-person</span>: Estimating 3D poses in the multi-person setting is a harder task than in single-person due to more severe occlusion. As shown in Tables <a href="#S4.T8" title="Table 8 â€£ 4.5. Evaluation Metrics for 3D HPE â€£ 4. Datasets and Evaluation Metrics â€£ Deep Learning-Based Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, good progress has been made in the single-view multi-person HPE methods in recent years. The Top-Down methods perform better than Bottom-Up methods due to the state-of-the-art person detection methods and single-person HPE methods. On the other hand, Bottom-Up methods are more computationally and time efficient.</p>
</div>
<div id="S4.SS6.p4" class="ltx_para">
<p id="S4.SS6.p4.1" class="ltx_p"><span id="S4.SS6.p4.1.1" class="ltx_text ltx_font_bold">Multi-view</span>: By comparing the results from Table <a href="#S4.T6" title="Table 6 â€£ 4.4. Datasets for 3D HPE â€£ 4. Datasets and Evaluation Metrics â€£ Deep Learning-Based Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> and Table <a href="#S4.T8" title="Table 8 â€£ 4.5. Evaluation Metrics for 3D HPE â€£ 4. Datasets and Evaluation Metrics â€£ Deep Learning-Based Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, it is evident that the performance (e.g., MPJPE under Protocol 1) of multi-view 3D HPE methods has improved compared to single-view 3D HPE methods using the same dataset and evaluation metric. Occlusion and depth ambiguity can be alleviated through the multi-view setting.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Applications</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this section, we review related works of exploring HPE for a few popular applications.
</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text ltx_font_bold">Action recognition, prediction, detection, and tracking</span>: Pose information has been utilized as cues for various applications such as action recognition, prediction, detection, and tracking. Angelini et al. <cite class="ltx_cite ltx_citemacro_citep">(Angelini
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2018</a>)</cite> proposed a real-time action recognition method using a pose-based algorithm. Yan et al. <cite class="ltx_cite ltx_citemacro_citep">(Yan
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib268" title="" class="ltx_ref">2018</a>)</cite> leveraged the dynamic skeleton modality of pose for action recognition. Markovitz et al. <cite class="ltx_cite ltx_citemacro_citep">(Markovitz etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib156" title="" class="ltx_ref">2020</a>)</cite> studied human pose graphs for anomaly detection of human actions in videos. Cao et al. <cite class="ltx_cite ltx_citemacro_citep">(Cao
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2020</a>)</cite> used the predicted 3D pose for long-term human motion prediction. Sun et al. <cite class="ltx_cite ltx_citemacro_citep">(Sun
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib221" title="" class="ltx_ref">2020</a>)</cite> proposed a view-invariant probabilistic pose embedding for video alignment. Hua et al. <cite class="ltx_cite ltx_citemacro_citep">(Hua
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib69" title="" class="ltx_ref">2023</a>)</cite> proposed an attention-based contrastive learning framework that integrates local similarity and global features for skeleton-based action
representations.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Pose-based video surveillance enjoys the advantage of preserving privacy by monitoring through pose and human mesh representation instead of human sensitive identities. Das et al. <cite class="ltx_cite ltx_citemacro_citep">(Das etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib46" title="" class="ltx_ref">2020</a>)</cite> embedded video with poses to identify activities of daily living for monitoring human behavior.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p"><span id="S5.p4.1.1" class="ltx_text ltx_font_bold">Action correction and online coaching</span>:
Some activities such as dancing, sporting, and professional training require precise human body control guidance. Normally personal trainers are responsible for pose correction and action guidance in a face-to-face manner. With the help of 3D HPE and action detection, AI personal trainers can make coaching more convenient by simply setting up cameras without a personal trainer present. Wang et al. <cite class="ltx_cite ltx_citemacro_citep">(Wang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib248" title="" class="ltx_ref">2019b</a>)</cite> designed an AI coaching system with a pose estimation module for personalized athletic training assistance.</p>
</div>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p"><span id="S5.p5.1.1" class="ltx_text ltx_font_bold">Clothes parsing</span>:
The e-commerce trends have brought about a noticeable impact on various aspects including clothes purchases. Clothing products in pictures can no longer satisfy customersâ€™ demands, and customers hope to see reliable appearances as they wear their selected clothes. Clothes parsing <cite class="ltx_cite ltx_citemacro_citep">(Yu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib277" title="" class="ltx_ref">2019</a>)</cite> <cite class="ltx_cite ltx_citemacro_citep">(Saito etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib212" title="" class="ltx_ref">2019</a>)</cite> and pose transfer <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib124" title="" class="ltx_ref">2019</a>)</cite> make it possible by inferring the 3D appearance of a person wearing specific clothes. HPE can provide plausible human body regions for cloth parsing. Moreover, the recommendation system can be upgraded by evaluating appropriateness based on the inferred reliable 3D appearance of customers with selected items. Patel et al. <cite class="ltx_cite ltx_citemacro_citep">(Patel
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib182" title="" class="ltx_ref">2020</a>)</cite> achieved clothing prediction from 3D pose, shape, and garment style.</p>
</div>
<div id="S5.p6" class="ltx_para">
<p id="S5.p6.1" class="ltx_p"><span id="S5.p6.1.1" class="ltx_text ltx_font_bold">Animation, movie, and gaming:</span>
Motion capture is the key component to present characters with complex movements and realistic physical interactions in industries of animation, movies, and gaming. Equipments are usually expensive and complicated to set up. HPE can provide realistic pose information while alleviating the demand for professional high-cost equipment <cite class="ltx_cite ltx_citemacro_citep">(Willett etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib257" title="" class="ltx_ref">2020</a>)</cite><cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib137" title="" class="ltx_ref">2020b</a>)</cite>.</p>
</div>
<div id="S5.p7" class="ltx_para">
<p id="S5.p7.1" class="ltx_p"><span id="S5.p7.1.1" class="ltx_text ltx_font_bold">AR and VR</span>:
Augmented Reality (AR) technology aims to enhance the interactive experience of digital objects in the real-world environment. The objective of Virtual Reality (VR) technology is to provide an immersive experience for the users. AR and VR devices use human pose information as input to achieve their goals of different applications. A cartoon character can be generated in real-world scenes to replace a real person. Weng et al. <cite class="ltx_cite ltx_citemacro_citep">(Weng etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib256" title="" class="ltx_ref">2019</a>)</cite> created 3D character animation from a single photo with the help of 3D pose estimation and human mesh recovery. Zhang et al. <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib287" title="" class="ltx_ref">2020d</a>)</cite> presented a pose-based system that converts broadcast tennis match videos into interactive and controllable video sprites.</p>
</div>
<div id="S5.p8" class="ltx_para">
<p id="S5.p8.1" class="ltx_p"><span id="S5.p8.1.1" class="ltx_text ltx_font_bold">Healthcare</span>:
HPE provides quantitative human motion information that physicians can use to diagnose some complex diseases, create rehabilitation training, and operate physical therapy. Lu et al. <cite class="ltx_cite ltx_citemacro_citep">(Lu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib145" title="" class="ltx_ref">2020</a>)</cite> designed a pose-based estimation system for assessing Parkinsonâ€™s disease motor severity. Gu et al. <cite class="ltx_cite ltx_citemacro_citep">(Gu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib63" title="" class="ltx_ref">2019</a>)</cite> developed a pose-based physical therapy system that patients can be evaluated and advised at their homes. Furthermore, such a system can be established to detect abnormal actions and predict subsequent actions ahead of time. Alerts are sent immediately if the system determines that danger may occur. Chen et al. <cite class="ltx_cite ltx_citemacro_citep">(Chen
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib26" title="" class="ltx_ref">2020d</a>)</cite> used HPE algorithms for fall detection monitoring in order to provide immediate assistance. Also, HPE methods can provide reliable posture labels of patients in hospital environments to augment research on neural correlates to natural behaviors <cite class="ltx_cite ltx_citemacro_citep">(Chen
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2018a</a>)</cite>.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Conclusion and Future Directions</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this survey, we have presented a systematic overview of recent deep learning-based 2D and 3D HPE methods. A comprehensive taxonomy and performance comparison of these methods have been covered.
Despite great success, there are still many
challenges as discussed in Sections <a href="#S2.SS3" title="2.3. 2D HPE Summary â€£ 2. 2D human pose estimation â€£ Deep Learning-Based Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a> and <a href="#S3.SS3" title="3.3. 3D HPE Summary â€£ 3. 3D human pose estimation â€£ Deep Learning-Based Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>. We further point out a few promising future directions to promote advances in HPE research.</p>
</div>
<div id="S6.p2" class="ltx_para">
<ul id="S6.I1" class="ltx_itemize">
<li id="S6.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S6.I1.i1.p1" class="ltx_para">
<p id="S6.I1.i1.p1.1" class="ltx_p">Domain adaptation for HPE. For some applications such as estimating human pose from infant images <cite class="ltx_cite ltx_citemacro_citep">(Huang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib74" title="" class="ltx_ref">2020a</a>)</cite> or artwork collections<cite class="ltx_cite ltx_citemacro_citep">(Madhu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib152" title="" class="ltx_ref">2020</a>)</cite>, there are not enough training data with ground truth annotations. Moreover, data for these applications exhibit different distributions from that of the standard pose datasets. HPE methods trained on existing standard datasets may not generalize well across different domains. The recent trend to alleviate the domain gap is utilizing GAN-based learning approaches. Nonetheless, how to effectively transfer the human pose knowledge to bridge domain gaps remains unaddressed.</p>
</div>
</li>
<li id="S6.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S6.I1.i2.p1" class="ltx_para">
<p id="S6.I1.i2.p1.1" class="ltx_p">Human body models such as SMPL, SMPL-X, GHUM &amp; GHUML, and Adam are used to model human mesh representation.
However, these models have a huge number of parameters. How to reduce the number of parameters while preserving the reconstructed mesh quality is an intriguing problem. Also, different people have various deformations of body shape. A more effective human body model may utilize other information such as BMI <cite class="ltx_cite ltx_citemacro_citep">(Osman
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib178" title="" class="ltx_ref">2020</a>)</cite> and silhouette <cite class="ltx_cite ltx_citemacro_citep">(Li
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib127" title="" class="ltx_ref">2020a</a>)</cite> for better generalization.</p>
</div>
</li>
<li id="S6.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S6.I1.i3.p1" class="ltx_para">
<p id="S6.I1.i3.p1.1" class="ltx_p">Most existing methods ignore human interaction with 3D scenes. There are strong human-scene relationship constraints that can be explored such as a human subject cannot be simultaneously present in the locations of other objects in the scene. The physical constraints with semantic cues can provide reliable and realistic 3D HPE.</p>
</div>
</li>
<li id="S6.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S6.I1.i4.p1" class="ltx_para">
<p id="S6.I1.i4.p1.1" class="ltx_p">3D HPE is employed in visual tracking and analysis. Existing 3D HPE from videos are not smooth and continuous. One reason is that the evaluation metrics such as MPJPE cannot evaluate the smoothness and the degree of realisticness. Appropriate frame-level evaluation metrics focusing on temporal consistency and motion smoothness should be developed.</p>
</div>
</li>
<li id="S6.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S6.I1.i5.p1" class="ltx_para">
<p id="S6.I1.i5.p1.1" class="ltx_p">Existing well-trained networks pay less attention to resolution mismatch. The training data of HPE networks are usually high-resolution images or videos, which may lead to inaccurate estimation when predicting human pose from low-resolution input. The contrastive learning scheme <cite class="ltx_cite ltx_citemacro_citep">(Chen
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2020e</a>)</cite> (e.g., the original image and its low-resolution version as a positive pair) might be helpful for building resolution-aware HPE networks.</p>
</div>
</li>
<li id="S6.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S6.I1.i6.p1" class="ltx_para">
<p id="S6.I1.i6.p1.1" class="ltx_p">Deep neural networks in vision tasks are vulnerable to adversarial attacks. Imperceptible noise can significantly affect the performance of HPE. There are few works <cite class="ltx_cite ltx_citemacro_citep">(Liu
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib136" title="" class="ltx_ref">2019</a>)</cite> <cite class="ltx_cite ltx_citemacro_citep">(Jain
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib83" title="" class="ltx_ref">2019</a>)</cite> that consider adversarial attacks for HPE. The study of defense against adversarial attacks can improve the robustness of HPE networks and facilitate real-world pose-based applications.</p>
</div>
</li>
<li id="S6.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S6.I1.i7.p1" class="ltx_para">
<p id="S6.I1.i7.p1.1" class="ltx_p">Human body parts may have different movement patterns and shapes due to the heterogeneity of the human body. A single shared network architecture may not be optimal for estimating all body parts with varying degrees of freedom. Neural Architecture Search (NAS) <cite class="ltx_cite ltx_citemacro_citep">(Elsken
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib53" title="" class="ltx_ref">2019</a>)</cite> can search the optimal architecture for estimating each body part <cite class="ltx_cite ltx_citemacro_citep">(Chen
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2020c</a>)</cite>. Also, NAS can be used for discovering efficient HPE network architectures to reduce the computational cost <cite class="ltx_cite ltx_citemacro_citep">(Zhang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib291" title="" class="ltx_ref">2020b</a>)</cite>. It is also worth exploring multi-objective NAS in HPE when multiple objectives (e.g, latency, accuracy, and energy consumption) have to be met.</p>
</div>
</li>
</ul>
</div>
<div id="S6.p3" class="ltx_para ltx_noindent">
<p id="S6.p3.1" class="ltx_p"><span id="S6.p3.1.1" class="ltx_text ltx_font_bold">HPE workshops and challenges:</span> Finally, we list the HPE workshops and challenges (2017-2022) on our project page (<a target="_blank" href="https://github.com/zczcwh/DL-HPE" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/zczcwh/DL-HPE</a>) to facilitate research in this field.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Andriluka etÂ al<span id="bib.bib2.2.2.1" class="ltx_text">.</span> (2018a)</span>
<span class="ltx_bibblock">
M. Andriluka, U. Iqbal,
E. Ensafutdinov, L. Pishchulin,
A. Milan, J. Gall, and
Schiele B. 2018a.

</span>
<span class="ltx_bibblock">PoseTrack: A Benchmark for Human Pose
Estimation and Tracking. In <em id="bib.bib2.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Andriluka etÂ al<span id="bib.bib3.2.2.1" class="ltx_text">.</span> (2018b)</span>
<span class="ltx_bibblock">
Mykhaylo Andriluka, Umar
Iqbal, Eldar Insafutdinov, Leonid
Pishchulin, Anton Milan, Juergen Gall,
and Bernt Schiele. 2018b.

</span>
<span class="ltx_bibblock">Posetrack: A benchmark for human pose estimation
and tracking. In <em id="bib.bib3.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Andriluka etÂ al<span id="bib.bib4.2.2.1" class="ltx_text">.</span> (2014)</span>
<span class="ltx_bibblock">
Mykhaylo Andriluka, Leonid
Pishchulin, Peter Gehler, and Bernt
Schiele. 2014.

</span>
<span class="ltx_bibblock">2d human pose estimation: New benchmark and state
of the art analysis. In <em id="bib.bib4.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Angelini
etÂ al<span id="bib.bib5.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Federico Angelini, Zeyu
Fu, Yang Long, Ling Shao, and
SyedÂ Mohsen Naqvi. 2018.

</span>
<span class="ltx_bibblock">Actionxpose: A novel 2d multi-view pose-based
algorithm for real-time human action recognition. In
<em id="bib.bib5.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1810.12126</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arnab
etÂ al<span id="bib.bib6.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Anurag Arnab, Carl
Doersch, and Andrew Zisserman.
2019.

</span>
<span class="ltx_bibblock">Exploiting Temporal Context for 3D Human Pose
Estimation in the Wild. In <em id="bib.bib6.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Artacho and
Savakis (2020)</span>
<span class="ltx_bibblock">
Bruno Artacho and
Andreas Savakis. 2020.

</span>
<span class="ltx_bibblock">UniPose: Unified Human Pose Estimation in Single
Images and Videos. In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Belagiannis and
Zisserman (2017)</span>
<span class="ltx_bibblock">
Vasileios Belagiannis and
Andrew Zisserman. 2017.

</span>
<span class="ltx_bibblock">Recurrent human pose estimation. In
<em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">FG</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Benzine etÂ al<span id="bib.bib9.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Abdallah Benzine, Florian
Chabot, Bertrand Luvison, QuocÂ Cuong
Pham, and Catherine Achard.
2020.

</span>
<span class="ltx_bibblock">PandaNet: Anchor-Based Single-Shot Multi-Person 3D
Pose Estimation. In <em id="bib.bib9.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bertasius etÂ al<span id="bib.bib10.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Gedas Bertasius, Christoph
Feichtenhofer, Du Tran, Jianbo Shi,
and Lorenzo Torresani. 2019.

</span>
<span class="ltx_bibblock">Learning temporal pose estimation from
sparsely-labeled videos. In <em id="bib.bib10.3.1" class="ltx_emph ltx_font_italic">NeurIPS</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bogo etÂ al<span id="bib.bib11.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Federica Bogo, Angjoo
Kanazawa, Christoph Lassner, Peter
Gehler, Javier Romero, and MichaelÂ J.
Black. 2016.

</span>
<span class="ltx_bibblock">Keep it SMPL: Automatic Estimation of 3D Human
Pose and Shape from a Single Image. In <em id="bib.bib11.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bulat and
Tzimiropoulos (2016)</span>
<span class="ltx_bibblock">
Adrian Bulat and
Georgios Tzimiropoulos. 2016.

</span>
<span class="ltx_bibblock">Human pose estimation via convolutional part
heatmap regression. In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Burenius etÂ al<span id="bib.bib13.2.2.1" class="ltx_text">.</span> (2013)</span>
<span class="ltx_bibblock">
M. Burenius, J.
Sullivan, and S. Carlsson.
2013.

</span>
<span class="ltx_bibblock">3D Pictorial Structures for Multiple View
Articulated Pose Estimation. In <em id="bib.bib13.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai etÂ al<span id="bib.bib14.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Y. Cai, L. Ge,
J. Liu, J. Cai, T.
Cham, J. Yuan, and N.Â M.
Thalmann. 2019.

</span>
<span class="ltx_bibblock">Exploiting Spatial-Temporal Relationships for 3D
Pose Estimation via Graph Convolutional Networks. In
<em id="bib.bib14.3.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai etÂ al<span id="bib.bib15.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Yuanhao Cai, Zhicheng
Wang, Zhengxiong Luo, Binyi Yin,
Angang Du, Haoqian Wang,
Xinyu Zhou, Erjin Zhou,
Xiangyu Zhang, and Jian Sun.
2020.

</span>
<span class="ltx_bibblock">Learning Delicate Local Representations for
Multi-Person Pose Estimation. In <em id="bib.bib15.3.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2003.04030</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao
etÂ al<span id="bib.bib16.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Zhe Cao, Hang Gao,
Karttikeya Mangalam, Qizhi Cai,
Minh Vo, and Jitendra Malik.
2020.

</span>
<span class="ltx_bibblock">Long-term human motion prediction with scene
context.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao
etÂ al<span id="bib.bib17.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Zhe Cao, Tomas Simon,
Shih-En Wei, and Yaser Sheikh.
2017.

</span>
<span class="ltx_bibblock">Realtime multi-person 2d pose estimation using part
affinity fields. In <em id="bib.bib17.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carreira etÂ al<span id="bib.bib18.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Joao Carreira, Pulkit
Agrawal, Katerina Fragkiadaki, and
Jitendra Malik. 2016.

</span>
<span class="ltx_bibblock">Human pose estimation with iterative error
feedback. In <em id="bib.bib18.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen and Ramanan (2017)</span>
<span class="ltx_bibblock">
Ching-Hang Chen and Deva
Ramanan. 2017.

</span>
<span class="ltx_bibblock">3D Human Pose Estimation = 2D Pose Estimation +
Matching. In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al<span id="bib.bib20.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Ching-Hang Chen, Ambrish
Tyagi, Amit Agrawal, Dylan Drover,
Rohith MV, Stefan Stojanov, and
JamesÂ M. Rehg. 2019.

</span>
<span class="ltx_bibblock">Unsupervised 3D Pose Estimation With Geometric
Self-Supervision. In <em id="bib.bib20.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen
etÂ al<span id="bib.bib21.2.2.1" class="ltx_text">.</span> (2020b)</span>
<span class="ltx_bibblock">
He Chen, Pengfei Guo,
Pengfei Li, GimÂ Hee Lee, and
Gregory Chirikjian. 2020b.

</span>
<span class="ltx_bibblock">Multi-person 3D Pose Estimation in Crowded Scenes
Based on Multi-View Geometry. In <em id="bib.bib21.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen
etÂ al<span id="bib.bib22.3.3.1" class="ltx_text">.</span> (2018a)</span>
<span class="ltx_bibblock">
Kenny Chen, Paolo
Gabriel, Abdulwahab Alasfour, Chenghao
Gong, WernerÂ K Doyle, Orrin Devinsky,
Daniel Friedman, Patricia Dugan,
Lucia Melloni, Thomas Thesen,
etÂ al<span id="bib.bib22.4.1" class="ltx_text">.</span> 2018a.

</span>
<span class="ltx_bibblock">Patient-specific pose estimation in clinical
environments. In <em id="bib.bib22.5.1" class="ltx_emph ltx_font_italic">JTEHM</em>,
Vol.Â 6. 1â€“11.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen
etÂ al<span id="bib.bib23.2.2.1" class="ltx_text">.</span> (2020a)</span>
<span class="ltx_bibblock">
Long Chen, Haizhou Ai,
Rui Chen, Zijie Zhuang, and
Shuang Liu. 2020a.

</span>
<span class="ltx_bibblock">Cross-View Tracking for Multi-Human 3D Pose
Estimation at Over 100 FPS. In <em id="bib.bib23.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen
etÂ al<span id="bib.bib24.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Tianlang Chen, Chen Fang,
Xiaohui Shen, Yiheng Zhu,
Zhili Chen, and Jiebo Luo.
2021.

</span>
<span class="ltx_bibblock">Anatomy-aware 3D Human Pose Estimation with
Bone-based Pose Decomposition. In <em id="bib.bib24.3.1" class="ltx_emph ltx_font_italic">TCSVT</em>,
Vol.Â 32. 198â€“209.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen
etÂ al<span id="bib.bib25.2.2.1" class="ltx_text">.</span> (2020e)</span>
<span class="ltx_bibblock">
Ting Chen, Simon
Kornblith, Mohammad Norouzi, and
Geoffrey Hinton. 2020e.

</span>
<span class="ltx_bibblock">A simple framework for contrastive learning of
visual representations. In <em id="bib.bib25.3.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2002.05709</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen
etÂ al<span id="bib.bib26.2.2.1" class="ltx_text">.</span> (2020d)</span>
<span class="ltx_bibblock">
Weiming Chen, Zijie
Jiang, Hailin Guo, and Xiaoyang Ni.
2020d.

</span>
<span class="ltx_bibblock">Fall Detection Based on Key Points of
Human-Skeleton Using OpenPose. In <em id="bib.bib26.3.1" class="ltx_emph ltx_font_italic">Symmetry</em>,
Vol.Â 12. 744.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen and Yuille (2014)</span>
<span class="ltx_bibblock">
Xianjie Chen and AlanÂ L
Yuille. 2014.

</span>
<span class="ltx_bibblock">Articulated pose estimation by a graphical model
with image dependent pairwise relations. In
<em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen
etÂ al<span id="bib.bib28.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Yu Chen, Chunhua Shen,
Xiu-Shen Wei, Lingqiao Liu, and
Jian Yang. 2017.

</span>
<span class="ltx_bibblock">Adversarial posenet: A structure-aware
convolutional network for human pose estimation. In
<em id="bib.bib28.3.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen
etÂ al<span id="bib.bib29.2.2.1" class="ltx_text">.</span> (2020f)</span>
<span class="ltx_bibblock">
Yucheng Chen, Yingli
Tian, and Mingyi He. 2020f.

</span>
<span class="ltx_bibblock">Monocular human pose estimation: A survey of deep
learning-based methods. In <em id="bib.bib29.3.1" class="ltx_emph ltx_font_italic">CVIU</em>,
Vol.Â 192. 102897.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen
etÂ al<span id="bib.bib30.2.2.1" class="ltx_text">.</span> (2018b)</span>
<span class="ltx_bibblock">
Yilun Chen, Zhicheng
Wang, Yuxiang Peng, Zhiqiang Zhang,
Gang Yu, and Jian Sun.
2018b.

</span>
<span class="ltx_bibblock">Cascaded pyramid network for multi-person pose
estimation. In <em id="bib.bib30.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen
etÂ al<span id="bib.bib31.2.2.1" class="ltx_text">.</span> (2020c)</span>
<span class="ltx_bibblock">
Zerui Chen, Yan Huang,
Hongyuan Yu, Bin Xue, Ke
Han, Yiru Guo, and Liang Wang.
2020c.

</span>
<span class="ltx_bibblock">Towards Part-aware Monocular 3D Human Pose
Estimation: An Architecture Search Approach. In
<em id="bib.bib31.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng etÂ al<span id="bib.bib32.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Bowen Cheng, Bin Xiao,
Jingdong Wang, Honghui Shi,
ThomasÂ S Huang, and Lei Zhang.
2020.

</span>
<span class="ltx_bibblock">HigherHRNet: Scale-Aware Representation Learning
for Bottom-Up Human Pose Estimation. In <em id="bib.bib32.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng
etÂ al<span id="bib.bib33.2.2.1" class="ltx_text">.</span> (2021a)</span>
<span class="ltx_bibblock">
Yu Cheng, Bo Wang,
Bo Yang, and RobbyÂ T. Tan.
2021a.

</span>
<span class="ltx_bibblock">Graph and Temporal Convolutional Networks for 3D
Multi-person Pose Estimation in Monocular Videos. In
<em id="bib.bib33.3.1" class="ltx_emph ltx_font_italic">AAAI</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng
etÂ al<span id="bib.bib34.2.2.1" class="ltx_text">.</span> (2021b)</span>
<span class="ltx_bibblock">
Yu Cheng, Bo Wang,
Bo Yang, and RobbyÂ T. Tan.
2021b.

</span>
<span class="ltx_bibblock">Monocular 3D Multi-Person Pose Estimation by
Integrating Top-Down and Bottom-Up Networks. In
<em id="bib.bib34.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng etÂ al<span id="bib.bib35.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Y. Cheng, B. Yang,
B. Wang, Y. Wending, and
R. Tan. 2019.

</span>
<span class="ltx_bibblock">Occlusion-Aware Networks for 3D Human Pose
Estimation in Video. In <em id="bib.bib35.3.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng
etÂ al<span id="bib.bib36.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Yu Cheng, Bo Yang,
Bo Wang, Wending Yan, and
RobbyÂ T. Tan. 2019.

</span>
<span class="ltx_bibblock">Occlusion-Aware Networks for 3D Human Pose
Estimation in Video. In <em id="bib.bib36.3.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cho
etÂ al<span id="bib.bib37.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Junhyeong Cho, Kim
Youwang, and Tae-Hyun Oh.
2022.

</span>
<span class="ltx_bibblock">Cross-Attention of Disentangled Modalities for 3D
Human Mesh Recovery with Transformers. In <em id="bib.bib37.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choi
etÂ al<span id="bib.bib38.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Hongsuk Choi, Gyeongsik
Moon, JuÂ Yong Chang, and KyoungÂ Mu
Lee. 2021.

</span>
<span class="ltx_bibblock">Beyond Static Features for Temporally Consistent 3D
Human Pose and Shape From a Video. In <em id="bib.bib38.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choi
etÂ al<span id="bib.bib39.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Hongsuk Choi, Gyeongsik
Moon, and KyoungÂ Mu Lee.
2020.

</span>
<span class="ltx_bibblock">Pose2Mesh: Graph Convolutional Network for 3D Human
Pose and Mesh Recovery from a 2D Human Pose. In
<em id="bib.bib39.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chou
etÂ al<span id="bib.bib40.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Chia-Jung Chou, Jui-Ting
Chien, and Hwann-Tzong Chen.
2018.

</span>
<span class="ltx_bibblock">Self adversarial training for human pose
estimation. In <em id="bib.bib40.3.1" class="ltx_emph ltx_font_italic">APSIPA ASC</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chu
etÂ al<span id="bib.bib41.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Xiao Chu, Wanli Ouyang,
Hongsheng Li, and Xiaogang Wang.
2016.

</span>
<span class="ltx_bibblock">Structured feature learning for pose estimation.
In <em id="bib.bib41.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chu
etÂ al<span id="bib.bib42.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Xiao Chu, Wei Yang,
Wanli Ouyang, Cheng Ma,
AlanÂ L Yuille, and Xiaogang Wang.
2017.

</span>
<span class="ltx_bibblock">Multi-context attention for human pose estimation.
In <em id="bib.bib42.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ci
etÂ al<span id="bib.bib43.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
H. Ci, C. Wang,
X. Ma, and Y. Wang.
2019.

</span>
<span class="ltx_bibblock">Optimizing Network Structure for 3D Human Pose
Estimation. In <em id="bib.bib43.3.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clever etÂ al<span id="bib.bib44.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
HenryÂ M. Clever, Zackory
Erickson, Ariel Kapusta, Greg Turk,
Karen Liu, and CharlesÂ C. Kemp.
2020.

</span>
<span class="ltx_bibblock">Bodies at Rest: 3D Human Pose and Shape Estimation
From a Pressure Image Using Synthetic Data. In
<em id="bib.bib44.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dabral etÂ al<span id="bib.bib45.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Rishabh Dabral, Anurag
Mundhada, Uday Kusupati, Safeer Afaque,
Abhishek Sharma, and Arjun Jain.
2018.

</span>
<span class="ltx_bibblock">Learning 3D Human Pose from Structure and Motion.
In <em id="bib.bib45.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Das etÂ al<span id="bib.bib46.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Srijan Das, Saurav
Sharma, Rui Dai, FranÃ§ois
BrÃ©mond, and Monique Thonnat.
2020.

</span>
<span class="ltx_bibblock">VPN: Learning Video-Pose Embedding for Activities
of Daily Living. In <em id="bib.bib46.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Debnath etÂ al<span id="bib.bib47.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Bappaditya Debnath, Mary
Oâ€™Brien, Motonori Yamaguchi, and
Ardhendu Behera. 2018.

</span>
<span class="ltx_bibblock">Adapting MobileNets for mobile based upper body
pose estimation. In <em id="bib.bib47.3.1" class="ltx_emph ltx_font_italic">AVSS</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Doering
etÂ al<span id="bib.bib48.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Andreas Doering, Umar
Iqbal, and Juergen Gall.
2018.

</span>
<span class="ltx_bibblock">Joint flow: Temporal flow fields for multi person
tracking. In <em id="bib.bib48.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1805.04596</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong
etÂ al<span id="bib.bib49.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Junting Dong, Wen Jiang,
Qixing Huang, Hujun Bao, and
Xiaowei Zhou. 2019.

</span>
<span class="ltx_bibblock">Fast and Robust Multi-Person 3D Pose Estimation
From Multiple Views. In <em id="bib.bib49.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong
etÂ al<span id="bib.bib50.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Zijian Dong, Jie Song,
Xu Chen, Chen Guo, and
Otmar Hilliges. 2021.

</span>
<span class="ltx_bibblock">Shape-aware Multi-Person Pose Estimation from
Multi-view Images. In <em id="bib.bib50.3.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Drover etÂ al<span id="bib.bib51.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Dylan Drover, Ching-Hang
Chen, Amit Agrawal, Ambrish Tyagi, and
Cong PhuocÂ Huynh. 2018.

</span>
<span class="ltx_bibblock">Can 3d pose be learned from 2d projections alone?.
In <em id="bib.bib51.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eichner etÂ al<span id="bib.bib52.2.2.1" class="ltx_text">.</span> (2012)</span>
<span class="ltx_bibblock">
Marcin Eichner, Manuel
Marin-Jimenez, Andrew Zisserman, and
Vittorio Ferrari. 2012.

</span>
<span class="ltx_bibblock">2d articulated human pose estimation and retrieval
in (almost) unconstrained still images. In <em id="bib.bib52.3.1" class="ltx_emph ltx_font_italic">IJCV</em>,
Vol.Â 99. 190â€“214.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elsken
etÂ al<span id="bib.bib53.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Thomas Elsken, JanÂ Hendrik
Metzen, and Frank Hutter.
2019.

</span>
<span class="ltx_bibblock">Neural Architecture Search: A Survey. In
<em id="bib.bib53.3.1" class="ltx_emph ltx_font_italic">JMLR</em>, Vol.Â 20.
1997â€“2017.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fabbri etÂ al<span id="bib.bib54.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Matteo Fabbri, Fabio
Lanzi, Simone Calderara, Stefano
Alletto, and Rita Cucchiara.
2020.

</span>
<span class="ltx_bibblock">Compressed Volumetric Heatmaps for Multi-Person 3D
Pose Estimation. In <em id="bib.bib54.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan
etÂ al<span id="bib.bib55.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Xiaochuan Fan, Kang
Zheng, Yuewei Lin, and Song Wang.
2015.

</span>
<span class="ltx_bibblock">Combining local appearance and holistic view:
Dual-source deep neural networks for human pose estimation. In
<em id="bib.bib55.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang
etÂ al<span id="bib.bib56.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Hao-Shu Fang, Shuqin Xie,
Yu-Wing Tai, and Cewu Lu.
2017.

</span>
<span class="ltx_bibblock">Rmpe: Regional multi-person pose estimation. In
<em id="bib.bib56.3.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fieraru etÂ al<span id="bib.bib57.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Mihai Fieraru, Anna
Khoreva, Leonid Pishchulin, and Bernt
Schiele. 2018.

</span>
<span class="ltx_bibblock">Learning to refine human pose estimation. In
<em id="bib.bib57.3.1" class="ltx_emph ltx_font_italic">CVPR Workshops</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fisch and Clark (2020)</span>
<span class="ltx_bibblock">
Martin Fisch and Ronald
Clark. 2020.

</span>
<span class="ltx_bibblock">Orientation Keypoints for 6D Human Pose
Estimation. In <em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2009.04930</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Georgakis etÂ al<span id="bib.bib59.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Georgios Georgakis, Ren
Li, Srikrishna Karanam, Terrence Chen,
Jana Kosecka, and Ziyan Wu.
2020.

</span>
<span class="ltx_bibblock">Hierarchical Kinematic Human Mesh Recovery. In
<em id="bib.bib59.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gkioxari
etÂ al<span id="bib.bib60.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Georgia Gkioxari,
Alexander Toshev, and Navdeep Jaitly.
2016.

</span>
<span class="ltx_bibblock">Chained predictions using convolutional neural
networks. In <em id="bib.bib60.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gong etÂ al<span id="bib.bib61.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Wenjuan Gong, Xuena
Zhang, Jordi GonzÃ lez, Andrews
Sobral, Thierry Bouwmans, Changhe Tu,
and El-hadi Zahzah. 2016.

</span>
<span class="ltx_bibblock">Human pose estimation from monocular images: A
comprehensive survey. In <em id="bib.bib61.3.1" class="ltx_emph ltx_font_italic">Sensors</em>,
Vol.Â 16. 1966.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goodfellow etÂ al<span id="bib.bib62.2.2.1" class="ltx_text">.</span> (2014)</span>
<span class="ltx_bibblock">
Ian Goodfellow, Jean
Pouget-Abadie, Mehdi Mirza, Bing Xu,
David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio.
2014.

</span>
<span class="ltx_bibblock">Generative adversarial nets. In
<em id="bib.bib62.3.1" class="ltx_emph ltx_font_italic">NeurIPS</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu etÂ al<span id="bib.bib63.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Yiwen Gu, Shreya Pandit,
Elham Saraee, Timothy Nordahl,
Terry Ellis, and Margrit Betke.
2019.

</span>
<span class="ltx_bibblock">Home-based physical therapy with an interactive
computer vision system. In <em id="bib.bib63.3.1" class="ltx_emph ltx_font_italic">ICCV Workshops</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo
etÂ al<span id="bib.bib64.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Hengkai Guo, Tang Tang,
Guozhong Luo, Riwei Chen,
Yongchen Lu, and Linfu Wen.
2018.

</span>
<span class="ltx_bibblock">Multi-domain pose network for multi-person pose
estimation and tracking. In <em id="bib.bib64.3.1" class="ltx_emph ltx_font_italic">ECCV Workshops</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Habibie etÂ al<span id="bib.bib65.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
I. Habibie, W. Xu,
D. Mehta, G. Pons-Moll, and
C. Theobalt. 2019.

</span>
<span class="ltx_bibblock">In the Wild Human Pose Estimation Using Explicit 2D
Features and Intermediate 3D Representations. In
<em id="bib.bib65.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hassan
etÂ al<span id="bib.bib66.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Mohamed Hassan, Vasileios
Choutas, Dimitrios Tzionas, and
MichaelÂ J. Black. 2019.

</span>
<span class="ltx_bibblock">Resolving 3D Human Pose Ambiguities with 3D
Scene Constraints. In <em id="bib.bib66.3.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He
etÂ al<span id="bib.bib67.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu
Zhang, Shaoqing Ren, and Jian Sun.
2016.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition. In
<em id="bib.bib67.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Holte
etÂ al<span id="bib.bib68.2.2.1" class="ltx_text">.</span> (2012)</span>
<span class="ltx_bibblock">
MichaelÂ B Holte, Cuong
Tran, MohanÂ M Trivedi, and ThomasÂ B
Moeslund. 2012.

</span>
<span class="ltx_bibblock">Human pose estimation and activity recognition from
multi-view videos: Comparative explorations of recent developments. In
<em id="bib.bib68.3.1" class="ltx_emph ltx_font_italic">IEEE J Sel Top Signal Process</em>,
Vol.Â 6. 538â€“552.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hua
etÂ al<span id="bib.bib69.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Yilei Hua, Wenhan Wu,
Ce Zheng, Aidong Lu,
Mengyuan Liu, Chen Chen, and
Shiqian Wu. 2023.

</span>
<span class="ltx_bibblock">Part Aware Contrastive Learning for Self-Supervised
Action Recognition. In <em id="bib.bib69.3.1" class="ltx_emph ltx_font_italic">IJCAI</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang etÂ al<span id="bib.bib70.2.2.1" class="ltx_text">.</span> (2020b)</span>
<span class="ltx_bibblock">
Congzhentao Huang, Shuai
Jiang, Yang Li, Ziyue Zhang,
Jason Traish, Chen Deng,
Sam Ferguson, and Richard YiÂ Da Xu.
2020b.

</span>
<span class="ltx_bibblock">End-to-end Dynamic Matching Network for Multi-view
Multi-person 3d Pose Estimation. In <em id="bib.bib70.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang
etÂ al<span id="bib.bib71.2.2.1" class="ltx_text">.</span> (2020c)</span>
<span class="ltx_bibblock">
Fuyang Huang, Ailing
Zeng, Minhao Liu, Qiuxia Lai, and
Qiang Xu. 2020c.

</span>
<span class="ltx_bibblock">DeepFuse: An IMU-Aware Network for Real-Time 3D
Human Pose Estimation from Multi-View Image. In
<em id="bib.bib71.3.1" class="ltx_emph ltx_font_italic">WACV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang
etÂ al<span id="bib.bib72.2.2.1" class="ltx_text">.</span> (2020d)</span>
<span class="ltx_bibblock">
Junjie Huang, Zheng Zhu,
Feng Guo, and Guan Huang.
2020d.

</span>
<span class="ltx_bibblock">The Devil is in the Details: Delving into Unbiased
Data Processing for Human Pose Estimation. In
<em id="bib.bib72.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang
etÂ al<span id="bib.bib73.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Shaoli Huang, Mingming
Gong, and Dacheng Tao. 2017.

</span>
<span class="ltx_bibblock">A coarse-fine network for keypoint localization.
In <em id="bib.bib73.3.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang etÂ al<span id="bib.bib74.2.2.1" class="ltx_text">.</span> (2020a)</span>
<span class="ltx_bibblock">
Xiaofei Huang, Nihang Fu,
Shuangjun Liu, Kathan Vyas,
Amirreza Farnoosh, and Sarah
Ostadabbas. 2020a.

</span>
<span class="ltx_bibblock">Invariant Representation Learning for Infant Pose
Estimation with Small Data. In <em id="bib.bib74.3.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2010.06100</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang etÂ al<span id="bib.bib75.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Yinghao Huang, Manuel
Kaufmann, Emre Aksan, MichaelÂ J. Black,
Otmar Hilliges, and Gerard Pons-Moll.
2018.

</span>
<span class="ltx_bibblock">Deep Inertial Poser: Learning to Reconstruct Human
Pose from Sparse Inertial Measurements in Real Time. In
<em id="bib.bib75.3.1" class="ltx_emph ltx_font_italic">ACM TOG</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Insafutdinov etÂ al<span id="bib.bib76.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Eldar Insafutdinov,
Mykhaylo Andriluka, Leonid Pishchulin,
Siyu Tang, Evgeny Levinkov,
Bjoern Andres, and Bernt Schiele.
2017.

</span>
<span class="ltx_bibblock">Arttrack: Articulated multi-person tracking in the
wild. In <em id="bib.bib76.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Insafutdinov etÂ al<span id="bib.bib77.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Eldar Insafutdinov, Leonid
Pishchulin, Bjoern Andres, Mykhaylo
Andriluka, and Bernt Schiele.
2016.

</span>
<span class="ltx_bibblock">Deepercut: A deeper, stronger, and faster
multi-person pose estimation model. In <em id="bib.bib77.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ionescu etÂ al<span id="bib.bib78.2.2.1" class="ltx_text">.</span> (2014)</span>
<span class="ltx_bibblock">
C. Ionescu, D.
Papava, V. Olaru, and C.
Sminchisescu. 2014.

</span>
<span class="ltx_bibblock">Human3.6M: Large Scale Datasets and Predictive
Methods for 3D Human Sensing in Natural Environments. In
<em id="bib.bib78.3.1" class="ltx_emph ltx_font_italic">IEEE TPAMI</em>, Vol.Â 36.
1325â€“1339.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Iqbal and Gall (2016)</span>
<span class="ltx_bibblock">
Umar Iqbal and Juergen
Gall. 2016.

</span>
<span class="ltx_bibblock">Multi-person pose estimation with local
joint-to-person associations. In <em id="bib.bib79.1.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Isogawa
etÂ al<span id="bib.bib80.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Mariko Isogawa, Ye Yuan,
Matthew Oâ€™Toole, and KrisÂ M. Kitani.
2020.

</span>
<span class="ltx_bibblock">Optical Non-Line-of-Sight Physics-Based 3D Human
Pose Estimation. In <em id="bib.bib80.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jahangiri and
Yuille (2017)</span>
<span class="ltx_bibblock">
Ehsan Jahangiri and
AlanÂ L Yuille. 2017.

</span>
<span class="ltx_bibblock">Generating multiple diverse hypotheses for human 3d
pose consistent with 2d joint detections. In <em id="bib.bib81.1.1" class="ltx_emph ltx_font_italic">ICCV
Workshops</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jain
etÂ al<span id="bib.bib82.2.2.1" class="ltx_text">.</span> (2014)</span>
<span class="ltx_bibblock">
Arjun Jain, Jonathan
Tompson, Yann LeCun, and Christoph
Bregler. 2014.

</span>
<span class="ltx_bibblock">Modeep: A deep learning framework using motion
features for human pose estimation. In <em id="bib.bib82.3.1" class="ltx_emph ltx_font_italic">ACCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jain
etÂ al<span id="bib.bib83.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Naman Jain, Sahil Shah,
Abhishek Kumar, and Arjun Jain.
2019.

</span>
<span class="ltx_bibblock">On the Robustness of Human Pose Estimation. In
<em id="bib.bib83.3.1" class="ltx_emph ltx_font_italic">CVPR Workshops</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jhuang
etÂ al<span id="bib.bib84.2.2.1" class="ltx_text">.</span> (2013)</span>
<span class="ltx_bibblock">
H. Jhuang, J. Gall,
S. Zuffi, C. Schmid, and
M.Â J. Black. 2013.

</span>
<span class="ltx_bibblock">Towards understanding action recognition. In
<em id="bib.bib84.3.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">JI
etÂ al<span id="bib.bib85.3.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Xiaopeng JI, Qi FANG,
Junting DONG, Qing SHUAI,
Wen JIANG, and Xiaowei ZHOU.
2020.

</span>
<span class="ltx_bibblock">A survey on monocular 3D human pose estimation. In
<em id="bib.bib85.1.1" class="ltx_emph ltx_font_italic">Virtual Reality <math id="bib.bib85.1.1.m1.1" class="ltx_Math" alttext="\&amp;" display="inline"><semantics id="bib.bib85.1.1.m1.1a"><mo id="bib.bib85.1.1.m1.1.1" xref="bib.bib85.1.1.m1.1.1.cmml">&amp;</mo><annotation-xml encoding="MathML-Content" id="bib.bib85.1.1.m1.1b"><and id="bib.bib85.1.1.m1.1.1.cmml" xref="bib.bib85.1.1.m1.1.1"></and></annotation-xml><annotation encoding="application/x-tex" id="bib.bib85.1.1.m1.1c">\&amp;</annotation></semantics></math> Intelligent Hardware</em>,
Vol.Â 2. 471â€“500.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji and Liu (2009)</span>
<span class="ltx_bibblock">
Xiaofei Ji and Honghai
Liu. 2009.

</span>
<span class="ltx_bibblock">Advances in view-invariant human motion analysis: a
review. In <em id="bib.bib86.1.1" class="ltx_emph ltx_font_italic">IEEE TSMC</em>, Vol.Â 40.
13â€“24.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang
etÂ al<span id="bib.bib87.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Haiyong Jiang, Jianfei
Cai, and Jianmin Zheng.
2019.

</span>
<span class="ltx_bibblock">Skeleton-Aware 3D Human Shape Reconstruction From
Point Clouds. In <em id="bib.bib87.3.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang etÂ al<span id="bib.bib88.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Wen Jiang, Nikos
Kolotouros, Georgios Pavlakos, Xiaowei
Zhou, and Kostas Daniilidis.
2020.

</span>
<span class="ltx_bibblock">Coherent Reconstruction of Multiple Humans From a
Single Image. In <em id="bib.bib88.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin etÂ al<span id="bib.bib89.2.2.1" class="ltx_text">.</span> (2020a)</span>
<span class="ltx_bibblock">
Sheng Jin, Wentao Liu,
Enze Xie, Wenhai Wang,
Chen Qian, Wanli Ouyang, and
Ping Luo. 2020a.

</span>
<span class="ltx_bibblock">Differentiable Hierarchical Graph Grouping for
Multi-Person Pose Estimation. In <em id="bib.bib89.3.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2007.11864</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin etÂ al<span id="bib.bib90.2.2.1" class="ltx_text">.</span> (2020b)</span>
<span class="ltx_bibblock">
Sheng Jin, Lumin Xu,
Jin Xu, Can Wang, Wentao
Liu, Chen Qian, Wanli Ouyang, and
Ping Luo. 2020b.

</span>
<span class="ltx_bibblock">Whole-Body Human Pose Estimation in the Wild. In
<em id="bib.bib90.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2007.11858</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson and
Everingham (2010)</span>
<span class="ltx_bibblock">
Sam Johnson and Mark
Everingham. 2010.

</span>
<span class="ltx_bibblock">Clustered Pose and Nonlinear Appearance Models for
Human Pose Estimation. In <em id="bib.bib91.1.1" class="ltx_emph ltx_font_italic">BMVC</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson and
Everingham (2011)</span>
<span class="ltx_bibblock">
Sam Johnson and Mark
Everingham. 2011.

</span>
<span class="ltx_bibblock">Learning effective human pose estimation from
inaccurate annotation. In <em id="bib.bib92.1.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joo
etÂ al<span id="bib.bib93.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Hanbyul Joo, Tomas Simon,
Xulong Li, Hao Liu, Lei
Tan, Lin Gui, Sean Banerjee,
TimothyÂ Scott Godisart, Bart Nabbe,
Iain Matthews, Takeo Kanade,
Shohei Nobuhara, and Yaser Sheikh.
2017.

</span>
<span class="ltx_bibblock">Panoptic Studio: A Massively Multiview System for
Social Interaction Capture. In <em id="bib.bib93.3.1" class="ltx_emph ltx_font_italic">IEEE TPAMI</em>.
3334â€“3342.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joo
etÂ al<span id="bib.bib94.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
H. Joo, T. Simon,
and Y. Sheikh. 2018.

</span>
<span class="ltx_bibblock">Total Capture: A 3D Deformation Model for Tracking
Faces, Hands, and Bodies. In <em id="bib.bib94.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kadkhodamohammadi etÂ al<span id="bib.bib95.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
A. Kadkhodamohammadi, A.
Gangi, M. de Mathelin, and N.
Padoy. 2017.

</span>
<span class="ltx_bibblock">A Multi-view RGB-D Approach for Human Pose
Estimation in Operating Rooms. In <em id="bib.bib95.3.1" class="ltx_emph ltx_font_italic">WACV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kavan (2014)</span>
<span class="ltx_bibblock">
Ladislav Kavan.
2014.

</span>
<span class="ltx_bibblock">Part I: direct skinning methods and deformation
primitives. In <em id="bib.bib96.1.1" class="ltx_emph ltx_font_italic">ACM SIGGRAPH</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ke
etÂ al<span id="bib.bib97.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Lipeng Ke, Ming-Ching
Chang, Honggang Qi, and Siwei Lyu.
2018.

</span>
<span class="ltx_bibblock">Multi-scale structure-aware network for human pose
estimation. In <em id="bib.bib97.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khan etÂ al<span id="bib.bib98.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Salman Khan, Muzammal
Naseer, Munawar Hayat, SyedÂ Waqas Zamir,
FahadÂ Shahbaz Khan, and Mubarak Shah.
2021.

</span>
<span class="ltx_bibblock">Transformers in vision: A survey. In
<em id="bib.bib98.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2101.01169</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kocabas
etÂ al<span id="bib.bib99.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Muhammed Kocabas, Nikos
Athanasiou, and MichaelÂ J Black.
2020.

</span>
<span class="ltx_bibblock">VIBE: Video inference for human body pose and shape
estimation. In <em id="bib.bib99.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kocabas
etÂ al<span id="bib.bib100.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Muhammed Kocabas, Salih
Karagoz, and Emre Akbas.
2018.

</span>
<span class="ltx_bibblock">Multiposenet: Fast multi-person pose estimation
using pose residual network. In <em id="bib.bib100.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kocabas
etÂ al<span id="bib.bib101.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Muhammed Kocabas, Salih
Karagoz, and Emre Akbas.
2019.

</span>
<span class="ltx_bibblock">Self-Supervised Learning of 3D Human Pose Using
Multi-View Geometry. In <em id="bib.bib101.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kolotouros etÂ al<span id="bib.bib102.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
N. Kolotouros, G.
Pavlakos, M. Black, and K.
Daniilidis. 2019.

</span>
<span class="ltx_bibblock">Learning to Reconstruct 3D Human Pose and Shape via
Model-Fitting in the Loop. In <em id="bib.bib102.3.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kolotouros etÂ al<span id="bib.bib103.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Nikos Kolotouros, Georgios
Pavlakos, and Kostas Daniilidis.
2019.

</span>
<span class="ltx_bibblock">Convolutional Mesh Regression for Single-Image
Human Shape Reconstruction. In <em id="bib.bib103.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kolotouros etÂ al<span id="bib.bib104.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Nikos Kolotouros, Georgios
Pavlakos, Dinesh Jayaraman, and Kostas
Daniilidis. 2021.

</span>
<span class="ltx_bibblock">Probabilistic Modeling for Human Mesh Recovery. In
<em id="bib.bib104.3.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kreiss
etÂ al<span id="bib.bib105.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Sven Kreiss, Lorenzo
Bertoni, and Alexandre Alahi.
2019.

</span>
<span class="ltx_bibblock">Pifpaf: Composite fields for human pose
estimation. In <em id="bib.bib105.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krizhevsky
etÂ al<span id="bib.bib106.2.2.1" class="ltx_text">.</span> (2012)</span>
<span class="ltx_bibblock">
Alex Krizhevsky, Ilya
Sutskever, and GeoffreyÂ E Hinton.
2012.

</span>
<span class="ltx_bibblock">Imagenet classification with deep convolutional
neural networks. In <em id="bib.bib106.3.1" class="ltx_emph ltx_font_italic">NeurIPS</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kundu etÂ al<span id="bib.bib107.2.2.1" class="ltx_text">.</span> (2020a)</span>
<span class="ltx_bibblock">
JogendraÂ Nath Kundu,
Ambareesh Revanur, GovindÂ Vitthal
Waghmare, RahulÂ Mysore Venkatesh, and
R.Â Venkatesh Babu. 2020a.

</span>
<span class="ltx_bibblock">Unsupervised Cross-Modal Alignment for Multi-Person
3D Pose Estimation. In <em id="bib.bib107.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kundu etÂ al<span id="bib.bib108.2.2.1" class="ltx_text">.</span> (2020b)</span>
<span class="ltx_bibblock">
JogendraÂ Nath Kundu,
Siddharth Seth, Varun Jampani,
Mugalodi Rakesh, R.Â Venkatesh Babu, and
Anirban Chakraborty. 2020b.

</span>
<span class="ltx_bibblock">Self-Supervised 3D Human Pose Estimation via Part
Guided Novel Image Synthesis. In <em id="bib.bib108.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kundu etÂ al<span id="bib.bib109.2.2.1" class="ltx_text">.</span> (2020c)</span>
<span class="ltx_bibblock">
JogendraÂ Nath Kundu,
Siddharth Seth, MV Rahul,
Mugalodi Rakesh, VenkateshÂ Babu
Radhakrishnan, and Anirban Chakraborty.
2020c.

</span>
<span class="ltx_bibblock">Kinematic-Structure-Preserved Representation for
Unsupervised 3D Human Pose Estimation. In <em id="bib.bib109.3.1" class="ltx_emph ltx_font_italic">AAAI</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lassner etÂ al<span id="bib.bib110.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Christoph Lassner, Javier
Romero, Martin Kiefel, Federica Bogo,
MichaelÂ J. Black, and PeterÂ V. Gehler.
2017.

</span>
<span class="ltx_bibblock">Unite the People: Closing the Loop Between 3D and
2D Human Representations. In <em id="bib.bib110.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee etÂ al<span id="bib.bib111.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Kyoungoh Lee, Inwoong
Lee, and Sanghoon Lee. 2018.

</span>
<span class="ltx_bibblock">Propagating LSTM: 3D Pose Estimation based on Joint
Interdependency. In <em id="bib.bib111.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Lee (2019)</span>
<span class="ltx_bibblock">
Chen Li and GimÂ Hee
Lee. 2019.

</span>
<span class="ltx_bibblock">Generating Multiple Hypotheses for 3D Human Pose
Estimation With Mixture Density Network. In
<em id="bib.bib112.1.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li
etÂ al<span id="bib.bib113.2.2.1" class="ltx_text">.</span> (2021a)</span>
<span class="ltx_bibblock">
Jiefeng Li, Siyuan Bian,
Ailing Zeng, Can Wang,
Bo Pang, Wentao Liu, and
Cewu Lu. 2021a.

</span>
<span class="ltx_bibblock">Human pose regression with residual log-likelihood
estimation. In <em id="bib.bib113.3.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li
etÂ al<span id="bib.bib114.2.2.1" class="ltx_text">.</span> (2020b)</span>
<span class="ltx_bibblock">
Jiefeng Li, Can Wang,
Wentao Liu, Chen Qian, and
Cewu Lu. 2020b.

</span>
<span class="ltx_bibblock">HMOR: Hierarchical Multi-Person Ordinal Relations
for Monocular Multi-Person 3D Pose Estimation. In
<em id="bib.bib114.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li
etÂ al<span id="bib.bib115.2.2.1" class="ltx_text">.</span> (2019b)</span>
<span class="ltx_bibblock">
Jiefeng Li, Can Wang,
Hao Zhu, Yihuan Mao,
Hao-Shu Fang, and Cewu Lu.
2019b.

</span>
<span class="ltx_bibblock">Crowdpose: Efficient crowded scenes pose estimation
and a new benchmark. In <em id="bib.bib115.3.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li
etÂ al<span id="bib.bib116.2.2.1" class="ltx_text">.</span> (2021d)</span>
<span class="ltx_bibblock">
Jiefeng Li, Chao Xu,
Zhicun Chen, Siyuan Bian,
Lixin Yang, and Cewu Lu.
2021d.

</span>
<span class="ltx_bibblock">HybrIK: A Hybrid Analytical-Neural Inverse
Kinematics Solution for 3D Human Pose and Shape Estimation. In
<em id="bib.bib116.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li
etÂ al<span id="bib.bib117.2.2.1" class="ltx_text">.</span> (2021c)</span>
<span class="ltx_bibblock">
Ke Li, Shijie Wang,
Xiang Zhang, Yifan Xu,
Weijian Xu, and Zhuowen Tu.
2021c.

</span>
<span class="ltx_bibblock">Pose Recognition with Cascade Transformers. In
<em id="bib.bib117.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Chan (2014)</span>
<span class="ltx_bibblock">
Sijin Li and AntoniÂ B.
Chan. 2014.

</span>
<span class="ltx_bibblock">3D Human Pose Estimation from Monocular Images with
Deep Convolutional Neural Network. In <em id="bib.bib118.1.1" class="ltx_emph ltx_font_italic">ACCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al<span id="bib.bib119.2.2.1" class="ltx_text">.</span> (2014)</span>
<span class="ltx_bibblock">
Sijin Li, Zhi-Qiang Liu,
and AntoniÂ B Chan. 2014.

</span>
<span class="ltx_bibblock">Heterogeneous multi-task learning for human pose
estimation with deep convolutional neural network. In
<em id="bib.bib119.3.1" class="ltx_emph ltx_font_italic">CVPR Workshops</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al<span id="bib.bib120.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Sijin Li, Weichen Zhang,
and AntoniÂ B. Chan. 2015.

</span>
<span class="ltx_bibblock">Maximum-Margin Structured Learning With Deep
Networks for 3D Human Pose Estimation. In <em id="bib.bib120.3.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li
etÂ al<span id="bib.bib121.2.2.1" class="ltx_text">.</span> (2022a)</span>
<span class="ltx_bibblock">
Wenhao Li, Hong Liu,
Hao Tang, Pichao Wang, and
Luc VanÂ Gool. 2022a.

</span>
<span class="ltx_bibblock">MHFormer: Multi-Hypothesis Transformer for 3D Human
Pose Estimation. In <em id="bib.bib121.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al<span id="bib.bib122.2.2.1" class="ltx_text">.</span> (2019a)</span>
<span class="ltx_bibblock">
Wenbo Li, Zhicheng Wang,
Binyi Yin, Qixiang Peng,
Yuming Du, Tianzi Xiao,
Gang Yu, Hongtao Lu,
Yichen Wei, and Jian Sun.
2019a.

</span>
<span class="ltx_bibblock">Rethinking on multi-stage networks for human pose
estimation. In <em id="bib.bib122.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1901.00148</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li
etÂ al<span id="bib.bib123.2.2.1" class="ltx_text">.</span> (2021b)</span>
<span class="ltx_bibblock">
Yizhuo Li, Miao Hao,
Zonglin Di, NiteshÂ Bharadwaj Gundavarapu,
and Xiaolong Wang. 2021b.

</span>
<span class="ltx_bibblock">Test-time personalization with a transformer for
human pose estimation. In <em id="bib.bib123.3.1" class="ltx_emph ltx_font_italic">NeurIPS</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al<span id="bib.bib124.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Yining Li, Chen Huang,
and ChenÂ Change Loy. 2019.

</span>
<span class="ltx_bibblock">Dense Intrinsic Appearance Flow for Human Pose
Transfer. In <em id="bib.bib124.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al<span id="bib.bib125.2.2.1" class="ltx_text">.</span> (2022b)</span>
<span class="ltx_bibblock">
Yanjie Li, Sen Yang,
Peidong Liu, Shoukui Zhang,
Yunxiao Wang, Zhicheng Wang,
Wankou Yang, and Shu-Tao Xia.
2022b.

</span>
<span class="ltx_bibblock">SimCC: A Simple Coordinate Classification
Perspective for Human Pose Estimation. In <em id="bib.bib125.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib126" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li
etÂ al<span id="bib.bib126.2.2.1" class="ltx_text">.</span> (2021e)</span>
<span class="ltx_bibblock">
Yanjie Li, Shoukui Zhang,
Zhicheng Wang, Sen Yang,
Wankou Yang, Shu-Tao Xia, and
Erjin Zhou. 2021e.

</span>
<span class="ltx_bibblock">TokenPose: Learning Keypoint Tokens for Human Pose
Estimation. In <em id="bib.bib126.3.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib127" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li
etÂ al<span id="bib.bib127.2.2.1" class="ltx_text">.</span> (2020a)</span>
<span class="ltx_bibblock">
Zhongguo Li, Anders
Heyden, and Magnus Oskarsson.
2020a.

</span>
<span class="ltx_bibblock">A novel joint points and silhouette-based method to
estimate 3D human pose and shape. In <em id="bib.bib127.3.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:2012.06109</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib128" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li
etÂ al<span id="bib.bib128.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Z. Li, X. Wang,
F. Wang, and P. Jiang.
2019.

</span>
<span class="ltx_bibblock">On Boosting Single-Frame 3D Human Pose Estimation
via Monocular Videos. In <em id="bib.bib128.3.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib129" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang and Lin (2019)</span>
<span class="ltx_bibblock">
Junbang Liang and
MingÂ C. Lin. 2019.

</span>
<span class="ltx_bibblock">Shape-Aware Human Pose and Shape Reconstruction
Using Multi-View Images. In <em id="bib.bib129.1.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib130" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lifshitz
etÂ al<span id="bib.bib130.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Ita Lifshitz, Ethan
Fetaya, and Shimon Ullman.
2016.

</span>
<span class="ltx_bibblock">Human pose estimation using deep consensus voting.
In <em id="bib.bib130.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib131" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin
etÂ al<span id="bib.bib131.2.2.1" class="ltx_text">.</span> (2021a)</span>
<span class="ltx_bibblock">
Kevin Lin, Lijuan Wang,
and Zicheng Liu. 2021a.

</span>
<span class="ltx_bibblock">End-to-end human pose and mesh reconstruction with
transformers. In <em id="bib.bib131.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib132" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin
etÂ al<span id="bib.bib132.2.2.1" class="ltx_text">.</span> (2021b)</span>
<span class="ltx_bibblock">
Kevin Lin, Lijuan Wang,
and Zicheng Liu. 2021b.

</span>
<span class="ltx_bibblock">Mesh Graphormer. In
<em id="bib.bib132.3.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib133" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin etÂ al<span id="bib.bib133.2.2.1" class="ltx_text">.</span> (2014)</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael
Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan,
Piotr DollÃ¡r, and CÂ Lawrence
Zitnick. 2014.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context. In
<em id="bib.bib133.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib134" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin etÂ al<span id="bib.bib134.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Weiyao Lin, Huabin Liu,
Shizhan Liu, Yuxi Li,
Rui Qian, Tao Wang, Ning
Xu, Hongkai Xiong, Guo-Jun Qi, and
Nicu Sebe. 2020.

</span>
<span class="ltx_bibblock">Human in events: A large-scale benchmark for
human-centric video analysis in complex events. In
<em id="bib.bib134.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2005.04490</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib135" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu
etÂ al<span id="bib.bib135.2.2.1" class="ltx_text">.</span> (2021b)</span>
<span class="ltx_bibblock">
Huajun Liu, Fuqiang Liu,
Xinyi Fan, and Dong Huang.
2021b.

</span>
<span class="ltx_bibblock">Polarized self-attention: towards high-quality
pixel-wise regression. In <em id="bib.bib135.3.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2107.00782</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib136" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu
etÂ al<span id="bib.bib136.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Jian Liu, Naveed Akhtar,
and Ajmal Mian. 2019.

</span>
<span class="ltx_bibblock">Adversarial Attack on Skeleton-based Human Action
Recognition. In <em id="bib.bib136.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1909.06500</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib137" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al<span id="bib.bib137.2.2.1" class="ltx_text">.</span> (2020b)</span>
<span class="ltx_bibblock">
Jingyuan Liu, Hongbo Fu,
and Chiew-Lan Tai. 2020b.

</span>
<span class="ltx_bibblock">PoseTween: Pose-driven Tween Animation. In
<em id="bib.bib137.3.1" class="ltx_emph ltx_font_italic">ACM UIST</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib138" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu
etÂ al<span id="bib.bib138.2.2.1" class="ltx_text">.</span> (2020a)</span>
<span class="ltx_bibblock">
Kenkun Liu, Rongqi Ding,
Zhiming Zou, Le Wang, and
Wei Tang. 2020a.

</span>
<span class="ltx_bibblock">A Comprehensive Study of Weight Sharing in Graph
Networks for 3D Human Pose Estimation. In <em id="bib.bib138.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib139" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu
etÂ al<span id="bib.bib139.2.2.1" class="ltx_text">.</span> (2020c)</span>
<span class="ltx_bibblock">
Ruixu Liu, Ju Shen,
He Wang, Chen Chen,
Sen-ching Cheung, and Vijayan Asari.
2020c.

</span>
<span class="ltx_bibblock">Attention Mechanism Exploits Temporal Contexts:
Real-Time 3D Human Pose Reconstruction. In <em id="bib.bib139.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib140" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu
etÂ al<span id="bib.bib140.2.2.1" class="ltx_text">.</span> (2021a)</span>
<span class="ltx_bibblock">
Zhenguang Liu, Haoming
Chen, Runyang Feng, Shuang Wu,
Shouling Ji, Bailin Yang, and
Xun Wang. 2021a.

</span>
<span class="ltx_bibblock">Deep dual consecutive network for human pose
estimation. In <em id="bib.bib140.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib141" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu
etÂ al<span id="bib.bib141.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Zhenguang Liu, Runyang
Feng, Haoming Chen, Shuang Wu,
Yixing Gao, Yunjun Gao, and
Xiang Wang. 2022.

</span>
<span class="ltx_bibblock">Temporal Feature Alignment and Mutual Information
Maximization for Video-Based Human Pose Estimation. In
<em id="bib.bib141.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib142" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu
etÂ al<span id="bib.bib142.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Zhao Liu, Jianke Zhu,
Jiajun Bu, and Chun Chen.
2015.

</span>
<span class="ltx_bibblock">A survey of human pose estimation: the body parts
parsing based methods. In <em id="bib.bib142.3.1" class="ltx_emph ltx_font_italic">JVCIR</em>,
Vol.Â 32. 10â€“19.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib143" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Long
etÂ al<span id="bib.bib143.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Jonathan Long, Evan
Shelhamer, and Trevor Darrell.
2015.

</span>
<span class="ltx_bibblock">Fully Convolutional Networks for Semantic
Segmentation. In <em id="bib.bib143.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib144" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loper etÂ al<span id="bib.bib144.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Matthew Loper, Naureen
Mahmood, Javier Romero, Gerard
Pons-Moll, and MichaelÂ J. Black.
2015.

</span>
<span class="ltx_bibblock">SMPL: A Skinned Multi-Person Linear Model. In
<em id="bib.bib144.3.1" class="ltx_emph ltx_font_italic">ACM TOG</em>, Vol.Â 34.
1â€“16.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib145" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu etÂ al<span id="bib.bib145.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Mandy Lu, Kathleen
Poston, Adolf Pfefferbaum, EdithÂ V
Sullivan, Li Fei-Fei, KilianÂ M Pohl,
JuanÂ Carlos Niebles, and Ehsan Adeli.
2020.

</span>
<span class="ltx_bibblock">Vision-based Estimation of MDS-UPDRS Gait Scores
for Assessing Parkinsonâ€™s Disease Motor Severity. In
<em id="bib.bib145.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2007.08920</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib146" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo etÂ al<span id="bib.bib146.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Yue Luo, Jimmy Ren,
Zhouxia Wang, Wenxiu Sun,
Jinshan Pan, Jianbo Liu,
Jiahao Pang, and Liang Lin.
2018.

</span>
<span class="ltx_bibblock">Lstm pose machines. In
<em id="bib.bib146.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib147" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo
etÂ al<span id="bib.bib147.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Zhengxiong Luo, Zhicheng
Wang, Yan Huang, Liang Wang,
Tieniu Tan, and Erjin Zhou.
2021.

</span>
<span class="ltx_bibblock">Rethinking the heatmap regression for bottom-up
human pose estimation. In <em id="bib.bib147.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib148" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luvizon
etÂ al<span id="bib.bib148.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
DiogoÂ C Luvizon, David
Picard, and Hedi Tabia.
2018.

</span>
<span class="ltx_bibblock">2d/3d pose estimation and action recognition using
multitask deep learning. In <em id="bib.bib148.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib149" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luvizon
etÂ al<span id="bib.bib149.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
DiogoÂ C Luvizon, Hedi
Tabia, and David Picard.
2019.

</span>
<span class="ltx_bibblock">Human pose regression by combining indirect part
detection and contextual information. In <em id="bib.bib149.3.1" class="ltx_emph ltx_font_italic">Computers
&amp; Graphics</em>, Vol.Â 85. 15â€“22.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib150" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma etÂ al<span id="bib.bib150.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Haoyu Ma, Liangjian Chen,
Deying Kong, Zhe Wang,
Xingwei Liu, Hao Tang,
Xiangyi Yan, Yusheng Xie,
Shih-Yao Lin, and Xiaohui Xie.
2021.

</span>
<span class="ltx_bibblock">Transfusion: Cross-view fusion with transformer for
3d human pose estimation. In <em id="bib.bib150.3.1" class="ltx_emph ltx_font_italic">BMVC</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib151" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma etÂ al<span id="bib.bib151.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Haoyu Ma, Zhe Wang,
Yifei Chen, Deying Kong,
Liangjian Chen, Xingwei Liu,
Xiangyi Yan, Hao Tang, and
Xiaohui Xie. 2022.

</span>
<span class="ltx_bibblock">PPT: token-Pruned Pose Transformer for monocular
and multi-view human pose estimation. In <em id="bib.bib151.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib152" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Madhu etÂ al<span id="bib.bib152.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Prathmesh Madhu, Angel
Villar-Corrales, Ronak Kosti, Torsten
Bendschus, Corinna Reinhardt, Peter
Bell, Andreas Maier, and Vincent
Christlein. 2020.

</span>
<span class="ltx_bibblock">Enhancing Human Pose Estimation in Ancient Vase
Paintings via Perceptually-grounded Style Transfer Learning. In
<em id="bib.bib152.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2012.05616</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib153" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mahmood etÂ al<span id="bib.bib153.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Naureen Mahmood, Nima
Ghorbani, NikolausÂ F. Troje, Gerard
Pons-Moll, and MichaelÂ J. Black.
2019.

</span>
<span class="ltx_bibblock">AMASS: Archive of Motion Capture as Surface
Shapes. In <em id="bib.bib153.3.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib154" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mao
etÂ al<span id="bib.bib154.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Weian Mao, Yongtao Ge,
Chunhua Shen, Zhi Tian,
Xinlong Wang, and Zhibin Wang.
2021.

</span>
<span class="ltx_bibblock">Tfpose: Direct human pose estimation with
transformers. In <em id="bib.bib154.3.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2103.15320</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib155" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mao etÂ al<span id="bib.bib155.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Weian Mao, Yongtao Ge,
Chunhua Shen, Zhi Tian,
Xinlong Wang, Zhibin Wang, and
Anton vanÂ den Hengel. 2022.

</span>
<span class="ltx_bibblock">Poseur: Direct Human Pose Regression with
Transformers. In <em id="bib.bib155.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib156" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Markovitz etÂ al<span id="bib.bib156.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Amir Markovitz, Gilad
Sharir, Itamar Friedman, Lihi
Zelnik-Manor, and Shai Avidan.
2020.

</span>
<span class="ltx_bibblock">Graph Embedded Pose Clustering for Anomaly
Detection. In <em id="bib.bib156.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib157" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Martinez etÂ al<span id="bib.bib157.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Julieta Martinez, Rayat
Hossain, Javier Romero, and JamesÂ J.
Little. 2017.

</span>
<span class="ltx_bibblock">A simple yet effective baseline for 3d human pose
estimation. In <em id="bib.bib157.3.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib158" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mehta etÂ al<span id="bib.bib158.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
D. Mehta, H. Rhodin,
D. Casas, P. Fua, O.
Sotnychenko, W. Xu, and C.
Theobalt. 2017.

</span>
<span class="ltx_bibblock">Monocular 3D Human Pose Estimation in the Wild
Using Improved CNN Supervision. In <em id="bib.bib158.3.1" class="ltx_emph ltx_font_italic">3DV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib159" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mehta etÂ al<span id="bib.bib159.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Dushyant Mehta, Oleksandr
Sotnychenko, Franziska Mueller, Weipeng
Xu, Mohamed Elgharib, Pascal Fua,
Hans-Peter Seidel, Helge Rhodin,
Gerard Pons-Moll, and Christian
Theobalt. 2020.

</span>
<span class="ltx_bibblock">XNect: Real-time Multi-Person 3D Motion Capture
with a Single RGB Camera. In <em id="bib.bib159.3.1" class="ltx_emph ltx_font_italic">ACM TOG</em>,
Vol.Â 39. 82â€“1.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib160" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mehta etÂ al<span id="bib.bib160.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Dushyant Mehta, Oleksandr
Sotnychenko, Franziska Mueller, Weipeng
Xu, Srinath Sridhar, Gerard Pons-Moll,
and Christian Theobalt. 2018.

</span>
<span class="ltx_bibblock">Single-Shot Multi-Person 3D Pose Estimation From
Monocular RGB. In <em id="bib.bib160.3.1" class="ltx_emph ltx_font_italic">3DV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib161" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mehta etÂ al<span id="bib.bib161.3.3.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Dushyant Mehta, Srinath
Sridhar, Oleksandr Sotnychenko, Helge
Rhodin, Mohammad Shafiei, Hans-Peter
Seidel, etÂ al<span id="bib.bib161.4.1" class="ltx_text">.</span> 2017.

</span>
<span class="ltx_bibblock">Vnect: Real-time 3d human pose estimation with a
single rgb camera. In <em id="bib.bib161.5.1" class="ltx_emph ltx_font_italic">ACM TOG</em>,
Vol.Â 36. 1â€“14.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib162" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Micilotta
etÂ al<span id="bib.bib162.2.2.1" class="ltx_text">.</span> (2006)</span>
<span class="ltx_bibblock">
AntonioÂ S Micilotta,
Eng-Jon Ong, and Richard Bowden.
2006.

</span>
<span class="ltx_bibblock">Real-time upper body detection and 3D pose
estimation in monoscopic images. In <em id="bib.bib162.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib163" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mitra
etÂ al<span id="bib.bib163.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Rahul Mitra, NiteshÂ B.
Gundavarapu, Abhishek Sharma, and Arjun
Jain. 2020.

</span>
<span class="ltx_bibblock">Multiview-Consistent Semi-Supervised Learning for
3D Human Pose Estimation. In <em id="bib.bib163.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib164" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moeslund and
Granum (2001)</span>
<span class="ltx_bibblock">
ThomasÂ B Moeslund and
Erik Granum. 2001.

</span>
<span class="ltx_bibblock">A survey of computer vision-based human motion
capture. In <em id="bib.bib164.1.1" class="ltx_emph ltx_font_italic">CVIU</em>, Vol.Â 81.
231â€“268.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib165" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moeslund
etÂ al<span id="bib.bib165.2.2.1" class="ltx_text">.</span> (2006)</span>
<span class="ltx_bibblock">
ThomasÂ B Moeslund, Adrian
Hilton, and Volker KrÃ¼ger.
2006.

</span>
<span class="ltx_bibblock">A survey of advances in vision-based human motion
capture and analysis. In <em id="bib.bib165.3.1" class="ltx_emph ltx_font_italic">CVIU</em>,
Vol.Â 104. 90â€“126.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib166" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moon
etÂ al<span id="bib.bib166.2.2.1" class="ltx_text">.</span> (2019a)</span>
<span class="ltx_bibblock">
Gyeongsik Moon, Juyong
Chang, and KyoungÂ Mu Lee.
2019a.

</span>
<span class="ltx_bibblock">Camera Distance-aware Top-down Approach for 3D
Multi-person Pose Estimation from a Single RGB Image. In
<em id="bib.bib166.3.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib167" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moon
etÂ al<span id="bib.bib167.2.2.1" class="ltx_text">.</span> (2019b)</span>
<span class="ltx_bibblock">
Gyeongsik Moon, JuÂ Yong
Chang, and KyoungÂ Mu Lee.
2019b.

</span>
<span class="ltx_bibblock">Posefix: Model-agnostic general human pose
refinement network. In <em id="bib.bib167.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib168" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moon and Lee (2020)</span>
<span class="ltx_bibblock">
Gyeongsik Moon and
KyoungÂ Mu Lee. 2020.

</span>
<span class="ltx_bibblock">I2L-MeshNet: Image-to-Lixel Prediction Network for
Accurate 3D Human Pose and Mesh Estimation from a Single RGB Image. In
<em id="bib.bib168.1.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib169" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moreno-Noguer (2017)</span>
<span class="ltx_bibblock">
Francesc Moreno-Noguer.
2017.

</span>
<span class="ltx_bibblock">3d human pose estimation from a single image via
distance matrix regression. In <em id="bib.bib169.1.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib170" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Munea etÂ al<span id="bib.bib170.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
TewodrosÂ Legesse Munea,
YalewÂ Zelalem Jembre, HalefomÂ Tekle
Weldegebriel, Longbiao Chen, Chenxi
Huang, and Chenhui Yang.
2020.

</span>
<span class="ltx_bibblock">The Progress of Human Pose Estimation: A Survey and
Taxonomy of Models Applied in 2D Human Pose Estimation. In
<em id="bib.bib170.3.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, Vol.Â 8.
133330â€“133348.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib171" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Newell
etÂ al<span id="bib.bib171.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Alejandro Newell, Zhiao
Huang, and Jia Deng. 2017.

</span>
<span class="ltx_bibblock">Associative embedding: End-to-end learning for
joint detection and grouping. In <em id="bib.bib171.3.1" class="ltx_emph ltx_font_italic">NeurIPS</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib172" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Newell
etÂ al<span id="bib.bib172.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Alejandro Newell, Kaiyu
Yang, and Jia Deng. 2016.

</span>
<span class="ltx_bibblock">Stacked hourglass networks for human pose
estimation. In <em id="bib.bib172.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib173" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nibali
etÂ al<span id="bib.bib173.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Aiden Nibali, Zhen He,
Stuart Morgan, and Luke Prendergast.
2018.

</span>
<span class="ltx_bibblock">Numerical coordinate regression with convolutional
neural networks. In <em id="bib.bib173.3.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1801.07372</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib174" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nie
etÂ al<span id="bib.bib174.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
B.Â X. Nie, P. Wei,
and S. Zhu. 2017.

</span>
<span class="ltx_bibblock">Monocular 3D Human Pose Estimation by Predicting
Depth on Joints. In <em id="bib.bib174.3.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib175" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nie etÂ al<span id="bib.bib175.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Qiang Nie, Ziwei Liu,
and Yunhui Liu. 2020.

</span>
<span class="ltx_bibblock">Unsupervised Human 3D Pose Representation with
Viewpoint and Pose Disentanglement. In <em id="bib.bib175.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib176" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nie
etÂ al<span id="bib.bib176.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Xuecheng Nie, Jiashi
Feng, Jianfeng Zhang, and Shuicheng
Yan. 2019.

</span>
<span class="ltx_bibblock">Single-Stage Multi-Person Pose Machines. In
<em id="bib.bib176.3.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib177" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Omran etÂ al<span id="bib.bib177.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Mohamed Omran, Christoph
Lassner, Gerard Pons-Moll, PeterÂ V.
Gehler, and Bernt Schiele.
2018.

</span>
<span class="ltx_bibblock">Neural Body Fitting: Unifying Deep Learning and
Model-Based Human Pose and Shape Estimation. In
<em id="bib.bib177.3.1" class="ltx_emph ltx_font_italic">3DV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib178" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Osman
etÂ al<span id="bib.bib178.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Ahmed AÂ A Osman, Timo
Bolkart, and MichaelÂ J. Black.
2020.

</span>
<span class="ltx_bibblock">STAR: A Spare Trained Articulated Human Body
Regressor. In <em id="bib.bib178.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib179" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Panteleris and
Argyros (2021)</span>
<span class="ltx_bibblock">
Paschalis Panteleris and
Antonis Argyros. 2021.

</span>
<span class="ltx_bibblock">PE-former: Pose Estimation Transformer. In
<em id="bib.bib179.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2112.04981</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib180" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papandreou etÂ al<span id="bib.bib180.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
George Papandreou, Tyler
Zhu, Liang-Chieh Chen, Spyros Gidaris,
Jonathan Tompson, and Kevin Murphy.
2018.

</span>
<span class="ltx_bibblock">Personlab: Person pose estimation and instance
segmentation with a bottom-up, part-based, geometric embedding model. In
<em id="bib.bib180.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib181" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papandreou etÂ al<span id="bib.bib181.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
George Papandreou, Tyler
Zhu, Nori Kanazawa, Alexander Toshev,
Jonathan Tompson, Chris Bregler, and
Kevin Murphy. 2017.

</span>
<span class="ltx_bibblock">Towards Accurate Multi-Person Pose Estimation in
the Wild. In <em id="bib.bib181.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib182" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patel
etÂ al<span id="bib.bib182.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Chaitanya Patel,
Zhouyingcheng Liao, and Gerard
Pons-Moll. 2020.

</span>
<span class="ltx_bibblock">TailorNet: Predicting Clothing in 3D as a Function
of Human Pose, Shape and Garment Style. In <em id="bib.bib182.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib183" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pavlakos etÂ al<span id="bib.bib183.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Georgios Pavlakos,
Vasileios Choutas, Nima Ghorbani,
Timo Bolkart, Ahmed A.Â A. Osman,
Dimitrios Tzionas, and MichaelÂ J.
Black. 2019.

</span>
<span class="ltx_bibblock">Expressive Body Capture: 3D Hands, Face, and Body
from a Single Image. In <em id="bib.bib183.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib184" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pavlakos
etÂ al<span id="bib.bib184.2.2.1" class="ltx_text">.</span> (2018a)</span>
<span class="ltx_bibblock">
Georgios Pavlakos, Xiaowei
Zhou, and Kostas Daniilidis.
2018a.

</span>
<span class="ltx_bibblock">Ordinal Depth Supervision for 3D Human Pose
Estimation. In <em id="bib.bib184.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib185" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pavlakos etÂ al<span id="bib.bib185.2.2.1" class="ltx_text">.</span> (2017a)</span>
<span class="ltx_bibblock">
Georgios Pavlakos, Xiaowei
Zhou, KonstantinosÂ G Derpanis, and
Kostas Daniilidis. 2017a.

</span>
<span class="ltx_bibblock">Coarse-to-Fine Volumetric Prediction for
Single-Image 3D Human Pose. In <em id="bib.bib185.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib186" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pavlakos etÂ al<span id="bib.bib186.2.2.1" class="ltx_text">.</span> (2017b)</span>
<span class="ltx_bibblock">
Georgios Pavlakos, Xiaowei
Zhou, KonstantinosÂ G. Derpanis, and
Kostas Daniilidis. 2017b.

</span>
<span class="ltx_bibblock">Harvesting Multiple Views for Marker-Less 3D Human
Pose Annotations. In <em id="bib.bib186.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib187" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pavlakos
etÂ al<span id="bib.bib187.2.2.1" class="ltx_text">.</span> (2018b)</span>
<span class="ltx_bibblock">
Georgios Pavlakos, Luyang
Zhu, Xiaowei Zhou, and Kostas
Daniilidis. 2018b.

</span>
<span class="ltx_bibblock">Learning to Estimate 3D Human Pose and Shape from
a Single Color Image. In <em id="bib.bib187.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib188" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pavllo etÂ al<span id="bib.bib188.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Dario Pavllo, Christoph
Feichtenhofer, David Grangier, and
Michael Auli. 2019.

</span>
<span class="ltx_bibblock">3D human pose estimation in video with temporal
convolutions and semi-supervised training. In
<em id="bib.bib188.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib189" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng
etÂ al<span id="bib.bib189.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Xi Peng, Zhiqiang Tang,
Fei Yang, RogerioÂ S Feris, and
Dimitris Metaxas. 2018.

</span>
<span class="ltx_bibblock">Jointly optimize data augmentation and network
training: Adversarial data augmentation in human pose estimation. In
<em id="bib.bib189.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib190" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pfister
etÂ al<span id="bib.bib190.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Tomas Pfister, James
Charles, and Andrew Zisserman.
2015.

</span>
<span class="ltx_bibblock">Flowing convnets for human pose estimation in
videos. In <em id="bib.bib190.3.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib191" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pfister etÂ al<span id="bib.bib191.2.2.1" class="ltx_text">.</span> (2014)</span>
<span class="ltx_bibblock">
Tomas Pfister, Karen
Simonyan, James Charles, and Andrew
Zisserman. 2014.

</span>
<span class="ltx_bibblock">Deep convolutional neural networks for efficient
pose estimation in gesture videos. In <em id="bib.bib191.3.1" class="ltx_emph ltx_font_italic">ACCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib192" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pirinen etÂ al<span id="bib.bib192.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Aleksis Pirinen, Erik
GÃ¤rtner, and Cristian Sminchisescu.
2019.

</span>
<span class="ltx_bibblock">Domes to Drones: Self-Supervised Active
Triangulation for 3D Human Pose Reconstruction.

</span>
<span class="ltx_bibblock">In <em id="bib.bib192.3.1" class="ltx_emph ltx_font_italic">NeurIPS</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib193" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pishchulin etÂ al<span id="bib.bib193.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Leonid Pishchulin, Eldar
Insafutdinov, Siyu Tang, Bjoern Andres,
Mykhaylo Andriluka, PeterÂ V Gehler, and
Bernt Schiele. 2016.

</span>
<span class="ltx_bibblock">Deepcut: Joint subset partition and labeling for
multi person pose estimation. In <em id="bib.bib193.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib194" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pons-Moll etÂ al<span id="bib.bib194.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Gerard Pons-Moll, Javier
Romero, Naureen Mahmood, and MichaelÂ J.
Black. 2015.

</span>
<span class="ltx_bibblock">Dyna: A Model of Dynamic Human Shape in Motion. In
<em id="bib.bib194.3.1" class="ltx_emph ltx_font_italic">ACM TOG</em>, Vol.Â 34.
1â€“14.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib195" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Poppe (2007)</span>
<span class="ltx_bibblock">
Ronald Poppe.
2007.

</span>
<span class="ltx_bibblock">Vision-based human motion analysis: An overview.
In <em id="bib.bib195.1.1" class="ltx_emph ltx_font_italic">CVIU</em>, Vol.Â 108.
4â€“18.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib196" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qammaz and
Argyros (2019)</span>
<span class="ltx_bibblock">
Ammar Qammaz and
AntonisÂ A Argyros. 2019.

</span>
<span class="ltx_bibblock">MocapNET: Ensemble of SNN Encoders for 3D Human
Pose Estimation in RGB Images. In <em id="bib.bib196.1.1" class="ltx_emph ltx_font_italic">BMVC</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib197" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qi
etÂ al<span id="bib.bib197.2.2.1" class="ltx_text">.</span> (2017a)</span>
<span class="ltx_bibblock">
CharlesÂ R Qi, Hao Su,
Kaichun Mo, and LeonidasÂ J Guibas.
2017a.

</span>
<span class="ltx_bibblock">Pointnet: Deep learning on point sets for 3d
classification and segmentation. In <em id="bib.bib197.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib198" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qi
etÂ al<span id="bib.bib198.2.2.1" class="ltx_text">.</span> (2017b)</span>
<span class="ltx_bibblock">
CharlesÂ Ruizhongtai Qi, Li
Yi, Hao Su, and LeonidasÂ J Guibas.
2017b.

</span>
<span class="ltx_bibblock">Pointnet++: Deep hierarchical feature learning on
point sets in a metric space. In <em id="bib.bib198.3.1" class="ltx_emph ltx_font_italic">NeurIPS</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib199" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qiu
etÂ al<span id="bib.bib199.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Haibo Qiu, Chunyu Wang,
Jingdong Wang, Naiyan Wang, and
Wenjun Zeng. 2019.

</span>
<span class="ltx_bibblock">Cross View Fusion for 3D Human Pose Estimation. In
<em id="bib.bib199.3.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib200" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qiu etÂ al<span id="bib.bib200.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Lingteng Qiu, Xuanye
Zhang, Yanran Li, Guanbin Li,
Xiaojun Wu, Zixiang Xiong,
Xiaoguang Han, and Shuguang Cui.
2020.

</span>
<span class="ltx_bibblock">Peeking into occluded joints: A novel framework for
crowd pose estimation. In <em id="bib.bib200.3.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2003.10506</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib201" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramakrishna etÂ al<span id="bib.bib201.2.2.1" class="ltx_text">.</span> (2014)</span>
<span class="ltx_bibblock">
Varun Ramakrishna, Daniel
Munoz, Martial Hebert, JamesÂ Andrew
Bagnell, and Yaser Sheikh.
2014.

</span>
<span class="ltx_bibblock">Pose machines: Articulated pose estimation via
inference machines. In <em id="bib.bib201.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib202" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rayat
ImtiazÂ Hossain and Little (2018)</span>
<span class="ltx_bibblock">
Mir Rayat ImtiazÂ Hossain and
JamesÂ J. Little. 2018.

</span>
<span class="ltx_bibblock">Exploiting temporal information for 3D human pose
estimation. In <em id="bib.bib202.1.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib203" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reddy etÂ al<span id="bib.bib203.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
NÂ Dinesh Reddy, Laurent
Guigues, Leonid Pishchulin, Jayan
Eledath, and SrinivasaÂ G Narasimhan.
2021.

</span>
<span class="ltx_bibblock">TesseTrack: End-to-End Learnable Multi-Person
Articulated 3D Pose Tracking. In <em id="bib.bib203.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib204" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Remelli
etÂ al<span id="bib.bib204.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Edoardo Remelli, Shangchen
Han, Sina Honari, Pascal Fua, and
Robert Wang. 2020.

</span>
<span class="ltx_bibblock">Lightweight Multi-View 3D Pose Estimation Through
Camera-Disentangled Representation. In <em id="bib.bib204.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib205" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren
etÂ al<span id="bib.bib205.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Shaoqing Ren, Kaiming He,
Ross Girshick, and Jian Sun.
2015.

</span>
<span class="ltx_bibblock">Faster r-cnn: Towards real-time object detection
with region proposal networks. In <em id="bib.bib205.3.1" class="ltx_emph ltx_font_italic">NeurIPS</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib206" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rhodin
etÂ al<span id="bib.bib206.2.2.1" class="ltx_text">.</span> (2018a)</span>
<span class="ltx_bibblock">
Helge Rhodin, Mathieu
Salzmann, and Pascal Fua.
2018a.

</span>
<span class="ltx_bibblock">Unsupervised Geometry-Aware Representation for 3D
Human Pose Estimation. In <em id="bib.bib206.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib207" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rhodin etÂ al<span id="bib.bib207.2.2.1" class="ltx_text">.</span> (2018b)</span>
<span class="ltx_bibblock">
Helge Rhodin, JÃ¶rg
SpÃ¶rri, Isinsu Katircioglu, Victor
Constantin, FrÃ©dÃ©ric Meyer, Erich
MÃ¼ller, Mathieu Salzmann, and Pascal
Fua. 2018b.

</span>
<span class="ltx_bibblock">Learning Monocular 3D Human Pose Estimation From
Multi-View Images. In <em id="bib.bib207.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib208" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rogez
etÂ al<span id="bib.bib208.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
G. Rogez, P.
Weinzaepfel, and C. Schmid.
2017.

</span>
<span class="ltx_bibblock">LCR-Net: Localization-Classification-Regression for
Human Pose. In <em id="bib.bib208.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib209" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rogez
etÂ al<span id="bib.bib209.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
GrÃ©gory Rogez, Philippe
Weinzaepfel, and Cordelia Schmid.
2019.

</span>
<span class="ltx_bibblock">LCR-Net++: Multi-person 2D and 3D Pose Detection in
Natural Images. In <em id="bib.bib209.3.1" class="ltx_emph ltx_font_italic">IEEE TPAMI</em>,
Vol.Â 42. 1146â€“1161.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib210" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ruder (2017)</span>
<span class="ltx_bibblock">
Sebastian Ruder.
2017.

</span>
<span class="ltx_bibblock">An overview of multi-task learning in deep neural
networks. In <em id="bib.bib210.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1706.05098</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib211" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saini etÂ al<span id="bib.bib211.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Nitin Saini, Eric Price,
Rahul Tallamraju, Raffi Enficiaud,
Roman Ludwig, Igor MartinoviÄ‡,
Aamir Ahmad, and Michael Black.
2019.

</span>
<span class="ltx_bibblock">Markerless Outdoor Human Motion Capture Using
Multiple Autonomous Micro Aerial Vehicles. In
<em id="bib.bib211.3.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib212" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saito etÂ al<span id="bib.bib212.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Shunsuke Saito, Zeng
Huang, Ryota Natsume, Shigeo Morishima,
Angjoo Kanazawa, and Hao Li.
2019.

</span>
<span class="ltx_bibblock">PIFu: Pixel-Aligned Implicit Function for
High-Resolution Clothed Human Digitization. In
<em id="bib.bib212.3.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib213" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saito
etÂ al<span id="bib.bib213.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Shunsuke Saito, Tomas
Simon, Jason Saragih, and Hanbyul
Joo. 2020.

</span>
<span class="ltx_bibblock">PIFuHD: Multi-Level Pixel-Aligned Implicit Function
for High-Resolution 3D Human Digitization. In
<em id="bib.bib213.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib214" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sapp and Taskar (2013)</span>
<span class="ltx_bibblock">
Ben Sapp and Ben
Taskar. 2013.

</span>
<span class="ltx_bibblock">Modec: Multimodal decomposable models for human
pose estimation. In <em id="bib.bib214.1.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib215" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sarafianos etÂ al<span id="bib.bib215.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Nikolaos Sarafianos,
Bogdan Boteanu, Bogdan Ionescu, and
IoannisÂ A Kakadiaris. 2016.

</span>
<span class="ltx_bibblock">3d human pose estimation: A review of the
literature and analysis of covariates. In <em id="bib.bib215.3.1" class="ltx_emph ltx_font_italic">CVIU</em>,
Vol.Â 152. 1â€“20.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib216" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharma etÂ al<span id="bib.bib216.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Saurabh Sharma, PavanÂ Teja
Varigonda, Prashast Bindal, Abhishek
Sharma, and Arjun Jain.
2019.

</span>
<span class="ltx_bibblock">Monocular 3D Human Pose Estimation by Generation
and Ordinal Ranking. In <em id="bib.bib216.3.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib217" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi
etÂ al<span id="bib.bib217.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Dahu Shi, Xing Wei,
Liangqi Li, Ye Ren, and
Wenming Tan. 2022.

</span>
<span class="ltx_bibblock">End-to-End Multi-Person Pose Estimation With
Transformers. In <em id="bib.bib217.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib218" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sigal
etÂ al<span id="bib.bib218.2.2.1" class="ltx_text">.</span> (2010)</span>
<span class="ltx_bibblock">
L. Sigal, A. Balan, and
M.Â J. Black. 2010.

</span>
<span class="ltx_bibblock">HumanEva: Synchronized video and motion capture
dataset and baseline algorithm for evaluation of articulated human motion.
In <em id="bib.bib218.3.1" class="ltx_emph ltx_font_italic">IJCV</em>, Vol.Â 87.
4.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib219" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Snower
etÂ al<span id="bib.bib219.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Michael Snower, Asim
Kadav, Farley Lai, and HansÂ Peter
Graf. 2020.

</span>
<span class="ltx_bibblock">15 keypoints is all you need. In
<em id="bib.bib219.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib220" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su
etÂ al<span id="bib.bib220.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Kai Su, Dongdong Yu,
Zhenqi Xu, Xin Geng, and
Changhu Wang. 2019.

</span>
<span class="ltx_bibblock">Multi-person pose estimation with enhanced
channel-wise and spatial information. In <em id="bib.bib220.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib221" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun
etÂ al<span id="bib.bib221.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
JenniferÂ J. Sun, Jiaping
Zhao, Liang-Chieh Chen, Florian Schroff,
Hartwig Adam, and Ting Liu.
2020.

</span>
<span class="ltx_bibblock">View-Invariant Probabilistic Embedding for Human
Pose. In <em id="bib.bib221.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib222" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun
etÂ al<span id="bib.bib222.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Ke Sun, Bin Xiao,
Dong Liu, and Jingdong Wang.
2019.

</span>
<span class="ltx_bibblock">Deep high-resolution representation learning for
human pose estimation. In <em id="bib.bib222.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib223" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun
etÂ al<span id="bib.bib223.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Xiao Sun, Jiaxiang Shang,
Shuang Liang, and Yichen Wei.
2017.

</span>
<span class="ltx_bibblock">Compositional human pose regression. In
<em id="bib.bib223.3.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib224" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang and Wu (2019)</span>
<span class="ltx_bibblock">
Wei Tang and Ying Wu.
2019.

</span>
<span class="ltx_bibblock">Does Learning Specific Features for Related Parts
Help Human Pose Estimation?. In <em id="bib.bib224.1.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib225" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang etÂ al<span id="bib.bib225.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Wei Tang, Pei Yu, and
Ying Wu. 2018.

</span>
<span class="ltx_bibblock">Deeply learned compositional models for human pose
estimation. In <em id="bib.bib225.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib226" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tekin etÂ al<span id="bib.bib226.2.2.1" class="ltx_text">.</span> (2016a)</span>
<span class="ltx_bibblock">
Bugra Tekin, Isinsu
Katircioglu, Mathieu Salzmann, Vincent
Lepetit, and Pascal Fua.
2016a.

</span>
<span class="ltx_bibblock">Structured Prediction of 3D Human Pose with Deep
Neural Networks. In <em id="bib.bib226.3.1" class="ltx_emph ltx_font_italic">BMVC</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib227" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tekin etÂ al<span id="bib.bib227.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Bugra Tekin, Pablo
MÃ¡rquez-Neila, Mathieu Salzmann, and
Pascal Fua. 2017.

</span>
<span class="ltx_bibblock">Learning to fuse 2d and 3d image cues for monocular
body pose estimation. In <em id="bib.bib227.3.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib228" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tekin
etÂ al<span id="bib.bib228.2.2.1" class="ltx_text">.</span> (2016b)</span>
<span class="ltx_bibblock">
Bugra Tekin, Artem
Rozantsev, Vincent Lepetit, and Pascal
Fua. 2016b.

</span>
<span class="ltx_bibblock">Direct Prediction of 3D Body Poses From Motion
Compensated Sequences. In <em id="bib.bib228.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib229" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tian
etÂ al<span id="bib.bib229.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Zhi Tian, Hao Chen, and
Chunhua Shen. 2019.

</span>
<span class="ltx_bibblock">DirectPose: Direct End-to-End Multi-Person Pose
Estimation. In <em id="bib.bib229.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1911.07451</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib230" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tome etÂ al<span id="bib.bib230.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Denis Tome, Thiemo
Alldieck, Patrick Peluse, Gerard
Pons-Moll, Lourdes Agapito, Hernan
Badino, and Fernando DeÂ la Torre.
2020.

</span>
<span class="ltx_bibblock">SelfPose: 3D Egocentric Pose Estimation from a
Headset Mounted Camera. In <em id="bib.bib230.3.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2011.01519</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib231" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tome
etÂ al<span id="bib.bib231.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Denis Tome, Patrick
Peluse, Lourdes Agapito, and Hernan
Badino. 2019.

</span>
<span class="ltx_bibblock">xR-EgoPose: Egocentric 3D Human Pose from an HMD
Camera. In <em id="bib.bib231.3.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib232" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tompson etÂ al<span id="bib.bib232.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Jonathan Tompson, Ross
Goroshin, Arjun Jain, Yann LeCun, and
Christoph Bregler. 2015.

</span>
<span class="ltx_bibblock">Efficient object localization using convolutional
networks. In <em id="bib.bib232.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib233" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tompson
etÂ al<span id="bib.bib233.2.2.1" class="ltx_text">.</span> (2014)</span>
<span class="ltx_bibblock">
JonathanÂ J Tompson, Arjun
Jain, Yann LeCun, and Christoph
Bregler. 2014.

</span>
<span class="ltx_bibblock">Joint training of a convolutional network and a
graphical model for human pose estimation. In
<em id="bib.bib233.3.1" class="ltx_emph ltx_font_italic">NeurIPS</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib234" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Toshev and
Szegedy (2014)</span>
<span class="ltx_bibblock">
Alexander Toshev and
Christian Szegedy. 2014.

</span>
<span class="ltx_bibblock">Deeppose: Human pose estimation via deep neural
networks. In <em id="bib.bib234.1.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib235" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trumble etÂ al<span id="bib.bib235.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Matt Trumble, Andrew
Gilbert, Charles Malleson, Adrian
Hilton, and John Collomosse.
2017.

</span>
<span class="ltx_bibblock">Total Capture: 3D Human Pose Estimation Fusing
Video and Inertial Sensors. In <em id="bib.bib235.3.1" class="ltx_emph ltx_font_italic">BMVC</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib236" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tu etÂ al<span id="bib.bib236.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Hanyue Tu, Chunyu Wang,
and Wenjun Zeng. 2020.

</span>
<span class="ltx_bibblock">VoxelPose: Towards Multi-Camera 3D Human Pose
Estimation in Wild Environment. In <em id="bib.bib236.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib237" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tung
etÂ al<span id="bib.bib237.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Hsiao-YuÂ Fish Tung,
Hsiao-Wei Tung, Ersin Yumer, and
Katerina Fragkiadaki. 2017.

</span>
<span class="ltx_bibblock">Self-Supervised Learning of Motion Capture. In
<em id="bib.bib237.3.1" class="ltx_emph ltx_font_italic">NeurIPS</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib238" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Umer
etÂ al<span id="bib.bib238.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Rafi Umer, Andreas
Doering, Bastian Leibe, and Juergen
Gall. 2020.

</span>
<span class="ltx_bibblock">Self-supervised Keypoint Correspondences for
Multi-Person Pose Estimation and Tracking in Videos. In
<em id="bib.bib238.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2004.12652</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib239" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Varol etÂ al<span id="bib.bib239.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Gul Varol, Javier Romero,
Xavier Martin, Naureen Mahmood,
MichaelÂ J Black, Ivan Laptev, and
Cordelia Schmid. 2017.

</span>
<span class="ltx_bibblock">Learning from synthetic humans. In
<em id="bib.bib239.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib240" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">VinceÂ Tan and
Cipolla (2017)</span>
<span class="ltx_bibblock">
IgnasÂ Budvytis VinceÂ Tan and
Roberto Cipolla. 2017.

</span>
<span class="ltx_bibblock">Indirect deep structured learning for 3D human body
shape and pose prediction. In <em id="bib.bib240.1.1" class="ltx_emph ltx_font_italic">BMVC</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib241" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">von Marcard etÂ al<span id="bib.bib241.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Timo von Marcard, Roberto
Henschel, MichaelÂ J. Black, Bodo
Rosenhahn, and Gerard Pons-Moll.
2018.

</span>
<span class="ltx_bibblock">Recovering Accurate 3D Human Pose in The Wild Using
IMUs and a Moving Camera. In <em id="bib.bib241.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib242" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">VonÂ Marcard etÂ al<span id="bib.bib242.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Timo VonÂ Marcard, Bodo
Rosenhahn, MichaelÂ J Black, and Gerard
Pons-Moll. 2017.

</span>
<span class="ltx_bibblock">Sparse inertial poser: Automatic 3d human pose
estimation from sparse imus. In <em id="bib.bib242.3.1" class="ltx_emph ltx_font_italic">Computer Graphics
Forum</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib243" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wandt and
Rosenhahn (2019)</span>
<span class="ltx_bibblock">
Bastian Wandt and Bodo
Rosenhahn. 2019.

</span>
<span class="ltx_bibblock">RepNet: Weakly Supervised Training of an
Adversarial Reprojection Network for 3D Human Pose Estimation. In
<em id="bib.bib243.1.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib244" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al<span id="bib.bib244.2.2.1" class="ltx_text">.</span> (2020a)</span>
<span class="ltx_bibblock">
Haoyang Wang, RizaÂ Alp
Guler, Iasonas Kokkinos, George
Papandreou, and Stefanos Zafeiriou.
2020a.

</span>
<span class="ltx_bibblock">BLSM: A Bone-Level Skinned Model of the Human
Mesh. In <em id="bib.bib244.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib245" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang
etÂ al<span id="bib.bib245.2.2.1" class="ltx_text">.</span> (2022c)</span>
<span class="ltx_bibblock">
Haixin Wang, Lu Zhou,
Yingying Chen, Ming Tang, and
Jinqiao Wang. 2022c.

</span>
<span class="ltx_bibblock">Regularizing Vector Embedding in Bottom-Up Human
Pose Estimation. In <em id="bib.bib245.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib246" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang
etÂ al<span id="bib.bib246.2.2.1" class="ltx_text">.</span> (2019a)</span>
<span class="ltx_bibblock">
Jue Wang, Shaoli Huang,
Xinchao Wang, and Dacheng Tao.
2019a.

</span>
<span class="ltx_bibblock">Not All Parts Are Created Equal: 3D Pose Estimation
by Modeling Bi-Directional Dependencies of Body Parts. In
<em id="bib.bib246.3.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib247" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang
etÂ al<span id="bib.bib247.2.2.1" class="ltx_text">.</span> (2020b)</span>
<span class="ltx_bibblock">
Jian Wang, Xiang Long,
Yuan Gao, Errui Ding, and
Shilei Wen. 2020b.

</span>
<span class="ltx_bibblock">Graph-PCNN: Two Stage Human Pose Estimation with
Graph Pose Refinement. In <em id="bib.bib247.3.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2007.10599</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib248" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang
etÂ al<span id="bib.bib248.2.2.1" class="ltx_text">.</span> (2019b)</span>
<span class="ltx_bibblock">
Jianbo Wang, Kai Qiu,
Houwen Peng, Jianlong Fu, and
Jianke Zhu. 2019b.

</span>
<span class="ltx_bibblock">AI Coach: Deep Human Pose Estimation and Analysis
for Personalized Athletic Training Assistance. In
<em id="bib.bib248.3.1" class="ltx_emph ltx_font_italic">ACM MM</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib249" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang
etÂ al<span id="bib.bib249.2.2.1" class="ltx_text">.</span> (2020d)</span>
<span class="ltx_bibblock">
Jingbo Wang, Sijie Yan,
Yuanjun Xiong, and Dahua Lin.
2020d.

</span>
<span class="ltx_bibblock">Motion Guided 3D Pose Estimation from Videos. In
<em id="bib.bib249.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib250" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang
etÂ al<span id="bib.bib250.2.2.1" class="ltx_text">.</span> (2020c)</span>
<span class="ltx_bibblock">
Kangkan Wang, Jin Xie,
Guofeng Zhang, Lei Liu, and
Jian Yang. 2020c.

</span>
<span class="ltx_bibblock">Sequential 3D Human Pose and Shape Estimation From
Point Clouds. In <em id="bib.bib250.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib251" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang
etÂ al<span id="bib.bib251.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Min Wang, Xipeng Chen,
Wentao Liu, Chen Qian,
Liang Lin, and Lizhuang Ma.
2018.

</span>
<span class="ltx_bibblock">DRPose3D: Depth Ranking in 3D Human Pose
Estimation. In <em id="bib.bib251.3.1" class="ltx_emph ltx_font_italic">IJCAI</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib252" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang
etÂ al<span id="bib.bib252.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Tao Wang, Jianfeng Zhang,
Yujun Cai, Shuicheng Yan, and
Jiashi Feng. 2021.

</span>
<span class="ltx_bibblock">Direct Multi-view Multi-person 3D Human Pose
Estimation. In <em id="bib.bib252.3.1" class="ltx_emph ltx_font_italic">NeurIPS</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib253" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang
etÂ al<span id="bib.bib253.2.2.1" class="ltx_text">.</span> (2022a)</span>
<span class="ltx_bibblock">
Yihan Wang, Muyang Li,
Han Cai, Wei-Ming Chen, and
Song Han. 2022a.

</span>
<span class="ltx_bibblock">Lite pose: Efficient architecture design for 2d
human pose estimation. In <em id="bib.bib253.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib254" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang
etÂ al<span id="bib.bib254.2.2.1" class="ltx_text">.</span> (2022b)</span>
<span class="ltx_bibblock">
Zitian Wang, Xuecheng
Nie, Xiaochao Qu, Yunpeng Chen, and
Si Liu. 2022b.

</span>
<span class="ltx_bibblock">Distribution-Aware Single-Stage Models for
Multi-Person 3D Pose Estimation. In <em id="bib.bib254.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib255" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei
etÂ al<span id="bib.bib255.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Shih-En Wei, Varun
Ramakrishna, Takeo Kanade, and Yaser
Sheikh. 2016.

</span>
<span class="ltx_bibblock">Convolutional pose machines. In
<em id="bib.bib255.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib256" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weng etÂ al<span id="bib.bib256.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
C. Weng, B. Curless,
and I. Kemelmacher-Shlizerman.
2019.

</span>
<span class="ltx_bibblock">Photo Wake-Up: 3D Character Animation From a Single
Photo. In <em id="bib.bib256.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib257" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Willett etÂ al<span id="bib.bib257.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
NoraÂ S Willett,
HijungÂ Valentina Shin, Zeyu Jin,
Wilmot Li, and Adam Finkelstein.
2020.

</span>
<span class="ltx_bibblock">Pose2Pose: pose selection and transfer for 2D
character animation. In <em id="bib.bib257.3.1" class="ltx_emph ltx_font_italic">IUI</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib258" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu etÂ al<span id="bib.bib258.3.3.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Jiahong Wu, He Zheng,
Bo Zhao, Yixin Li,
Baoming Yan, Rui Liang,
Wenjia Wang, Shipei Zhou,
etÂ al<span id="bib.bib258.4.1" class="ltx_text">.</span> 2017.

</span>
<span class="ltx_bibblock">Ai challenger: A large-scale dataset for going
deeper in image understanding. In <em id="bib.bib258.5.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1711.06475</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib259" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiang
etÂ al<span id="bib.bib259.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Donglai Xiang, Hanbyul
Joo, and Yaser Sheikh. 2019.

</span>
<span class="ltx_bibblock">Monocular total capture: Posing face, body, and
hands in the wild. In <em id="bib.bib259.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib260" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao etÂ al<span id="bib.bib260.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Bin Xiao, Haiping Wu,
and Yichen Wei. 2018.

</span>
<span class="ltx_bibblock">Simple Baselines for Human Pose Estimation and
Tracking. In <em id="bib.bib260.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib261" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie
etÂ al<span id="bib.bib261.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Rongchang Xie, Chunyu
Wang, and Yizhou Wang. 2020.

</span>
<span class="ltx_bibblock">MetaFuse: A Pre-trained Fusion Model for Human Pose
Estimation. In <em id="bib.bib261.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib262" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiong etÂ al<span id="bib.bib262.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Fu Xiong, Boshen Zhang,
Yang Xiao, Zhiguo Cao,
Taidong Yu, Joey ZhouÂ Tianyi, and
Junsong Yuan. 2019.

</span>
<span class="ltx_bibblock">A2J: Anchor-to-Joint Regression Network for 3D
Articulated Pose Estimation from a Single Depth Image. In
<em id="bib.bib262.3.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib263" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu etÂ al<span id="bib.bib263.3.2.1" class="ltx_text">.</span> (2020a)</span>
<span class="ltx_bibblock">
Hongyi Xu, EduardÂ Gabriel
Bazavan, Andrei Zanfir, WilliamÂ T.
Freeman, Rahul Sukthankar, and Cristian
Sminchisescu. 2020a.

</span>
<span class="ltx_bibblock">GHUM <math id="bib.bib263.1.m1.1" class="ltx_Math" alttext="\&amp;" display="inline"><semantics id="bib.bib263.1.m1.1a"><mo id="bib.bib263.1.m1.1.1" xref="bib.bib263.1.m1.1.1.cmml">&amp;</mo><annotation-xml encoding="MathML-Content" id="bib.bib263.1.m1.1b"><and id="bib.bib263.1.m1.1.1.cmml" xref="bib.bib263.1.m1.1.1"></and></annotation-xml><annotation encoding="application/x-tex" id="bib.bib263.1.m1.1c">\&amp;</annotation></semantics></math> GHUML: Generative 3D Human Shape and
Articulated Pose Models. In <em id="bib.bib263.4.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib264" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu
etÂ al<span id="bib.bib264.2.2.1" class="ltx_text">.</span> (2020c)</span>
<span class="ltx_bibblock">
Jingwei Xu, Zhenbo Yu,
Bingbing Ni, Jiancheng Yang,
Xiaokang Yang, and Wenjun Zhang.
2020c.

</span>
<span class="ltx_bibblock">Deep Kinematics Analysis for Monocular 3D Human
Pose Estimation. In <em id="bib.bib264.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib265" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu etÂ al<span id="bib.bib265.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Lumin Xu, Yingda Guan,
Sheng Jin, Wentao Liu,
Chen Qian, Ping Luo,
Wanli Ouyang, and Xiaogang Wang.
2021.

</span>
<span class="ltx_bibblock">Vipnas: Efficient video pose estimation via neural
architecture search. In <em id="bib.bib265.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib266" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu etÂ al<span id="bib.bib266.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Weipeng Xu, Avishek
Chatterjee, Michael Zollhoefer, Helge
Rhodin, Pascal Fua, Hans-Peter Seidel,
and Christian Theobalt. 2019.

</span>
<span class="ltx_bibblock">Mo 2 cap 2: Real-time mobile 3d motion capture with
a cap-mounted fisheye camera. In <em id="bib.bib266.3.1" class="ltx_emph ltx_font_italic">IEEE TVCG Proc.
VR</em>, Vol.Â 25. 2093â€“2101.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib267" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu etÂ al<span id="bib.bib267.2.2.1" class="ltx_text">.</span> (2020b)</span>
<span class="ltx_bibblock">
Xiangyu Xu, Hao Chen,
Francesc Moreno-Noguer, LaszloÂ A. Jeni,
and FernandoÂ De la Torre.
2020b.

</span>
<span class="ltx_bibblock">3D Human Shape and Pose from a Single
Low-Resolution Image with Self-Supervised Learning. In
<em id="bib.bib267.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib268" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan
etÂ al<span id="bib.bib268.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Sijie Yan, Yuanjun Xiong,
and Dahua Lin. 2018.

</span>
<span class="ltx_bibblock">Spatial temporal graph convolutional networks for
skeleton-based action recognition. In <em id="bib.bib268.3.1" class="ltx_emph ltx_font_italic">AAAI</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib269" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang
etÂ al<span id="bib.bib269.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Sen Yang, Zhibin Quan,
Mu Nie, and Wankou Yang.
2021.

</span>
<span class="ltx_bibblock">Transpose: Keypoint localization via transformer.
In <em id="bib.bib269.3.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib270" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang
etÂ al<span id="bib.bib270.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Wei Yang, Shuang Li,
Wanli Ouyang, Hongsheng Li, and
Xiaogang Wang. 2017.

</span>
<span class="ltx_bibblock">Learning feature pyramids for human pose
estimation. In <em id="bib.bib270.3.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib271" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang
etÂ al<span id="bib.bib271.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Wei Yang, Wanli Ouyang,
Hongsheng Li, and Xiaogang Wang.
2016.

</span>
<span class="ltx_bibblock">End-to-end learning of deformable mixture of parts
and deep convolutional neural networks for human pose estimation. In
<em id="bib.bib271.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib272" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang
etÂ al<span id="bib.bib272.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Wei Yang, Wanli Ouyang,
Xiaolong Wang, Jimmy Ren,
Hongsheng Li, and Xiaogang Wang.
2018.

</span>
<span class="ltx_bibblock">3D Human Pose Estimation in the Wild by Adversarial
Learning. In <em id="bib.bib272.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib273" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang and Ramanan (2012)</span>
<span class="ltx_bibblock">
Yi Yang and Deva
Ramanan. 2012.

</span>
<span class="ltx_bibblock">Articulated human detection with flexible mixtures
of parts. In <em id="bib.bib273.1.1" class="ltx_emph ltx_font_italic">IEEE TPAMI</em>,
Vol.Â 35. 2878â€“2890.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib274" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye
etÂ al<span id="bib.bib274.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Hang Ye, Wentao Zhu,
Chunyu Wang, Rujie Wu, and
Yizhou Wang. 2022.

</span>
<span class="ltx_bibblock">Faster VoxelPose: Real-time 3D Human Pose
Estimation by Orthographic Projection. In <em id="bib.bib274.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib275" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu
etÂ al<span id="bib.bib275.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Changqian Yu, Bin Xiao,
Changxin Gao, Lu Yuan,
Lei Zhang, Nong Sang, and
Jingdong Wang. 2021.

</span>
<span class="ltx_bibblock">Lite-hrnet: A lightweight high-resolution network.
In <em id="bib.bib275.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib276" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu etÂ al<span id="bib.bib276.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
T. Yu, J. Zhao,
Z. Zheng, K. Guo, Q.
Dai, H. Li, G. Pons-Moll, and
Y. Liu. 2019.

</span>
<span class="ltx_bibblock">DoubleFusion: Real-time Capture of Human
Performances with Inner Body Shapes from a Single Depth Sensor. In
<em id="bib.bib276.3.1" class="ltx_emph ltx_font_italic">IEEE TPAMI</em>. 7287â€“7296.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib277" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu etÂ al<span id="bib.bib277.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Tao Yu, Zerong Zheng,
Yuan Zhong, Jianhui Zhao,
Qionghai Dai, Gerard Pons-Moll, and
Yebin Liu. 2019.

</span>
<span class="ltx_bibblock">Simulcap: Single-view human performance capture
with cloth simulation. In <em id="bib.bib277.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib278" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan etÂ al<span id="bib.bib278.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Yuhui Yuan, Rao Fu,
Lang Huang, Weihong Lin,
Chao Zhang, Xilin Chen, and
Jingdong Wang. 2021.

</span>
<span class="ltx_bibblock">Hrformer: High-resolution vision transformer for
dense predict. In <em id="bib.bib278.3.1" class="ltx_emph ltx_font_italic">NeurIPS</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib279" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zanfir etÂ al<span id="bib.bib279.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Andrei Zanfir,
EduardÂ Gabriel Bazavan, Hongyi Xu,
Bill Freeman, Rahul Sukthankar, and
Cristian Sminchisescu. 2020.

</span>
<span class="ltx_bibblock">Weakly Supervised 3D Human Pose and Shape
Reconstruction with Normalizing Flows. In <em id="bib.bib279.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib280" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zanfir etÂ al<span id="bib.bib280.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
A. Zanfir, E.
Marinoiu, and C. Sminchisescu.
2018.

</span>
<span class="ltx_bibblock">Monocular 3D Pose and Shape Estimation of Multiple
People in Natural Scenes: The Importance of Multiple Scene Constraints. In
<em id="bib.bib280.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib281" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zanfir etÂ al<span id="bib.bib281.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Andrei Zanfir, Elisabeta
Marinoiu, Mihai Zanfir, Alin-Ionut Popa,
and Cristian Sminchisescu.
2018.

</span>
<span class="ltx_bibblock">Deep Network for the Integrated 3D Sensing of
Multiple People in Natural Images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib281.3.1" class="ltx_emph ltx_font_italic">NeurIPS</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib282" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng
etÂ al<span id="bib.bib282.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Ailing Zeng, Xiao Sun,
Fuyang Huang, Minhao Liu,
Qiang Xu, and Stephen Lin.
2020.

</span>
<span class="ltx_bibblock">SRNet: Improving Generalization in 3D Human Pose
Estimation with a Split-and-Recombine Approach. In
<em id="bib.bib282.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib283" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng etÂ al<span id="bib.bib283.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
W. Zeng, W. Ouyang,
P. Luo, W. Liu, and
X. Wang. 2020.

</span>
<span class="ltx_bibblock">3D Human Mesh Regression With Dense
Correspondence. In <em id="bib.bib283.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib284" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang
etÂ al<span id="bib.bib284.2.2.1" class="ltx_text">.</span> (2020h)</span>
<span class="ltx_bibblock">
Feng Zhang, Xiatian Zhu,
Hanbin Dai, Mao Ye, and
Ce Zhu. 2020h.

</span>
<span class="ltx_bibblock">Distribution-aware coordinate representation for
human pose estimation. In <em id="bib.bib284.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib285" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang
etÂ al<span id="bib.bib285.2.2.1" class="ltx_text">.</span> (2019b)</span>
<span class="ltx_bibblock">
Feng Zhang, Xiatian Zhu,
and Mao Ye. 2019b.

</span>
<span class="ltx_bibblock">Fast human pose estimation. In
<em id="bib.bib285.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib286" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al<span id="bib.bib286.2.2.1" class="ltx_text">.</span> (2019a)</span>
<span class="ltx_bibblock">
Hong Zhang, Hao Ouyang,
Shu Liu, Xiaojuan Qi,
Xiaoyong Shen, Ruigang Yang, and
Jiaya Jia. 2019a.

</span>
<span class="ltx_bibblock">Human pose estimation with spatial contextual
information. In <em id="bib.bib286.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1901.01760</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib287" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al<span id="bib.bib287.2.2.1" class="ltx_text">.</span> (2020d)</span>
<span class="ltx_bibblock">
Haotian Zhang, Cristobal
Sciutto, Maneesh Agrawala, and Kayvon
Fatahalian. 2020d.

</span>
<span class="ltx_bibblock">Vid2Player: Controllable Video Sprites that Behave
and Appear like Professional Tennis Players. In
<em id="bib.bib287.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2008.04524</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib288" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al<span id="bib.bib288.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Hongwen Zhang, Yating
Tian, Xinchi Zhou, Wanli Ouyang,
Yebin Liu, Limin Wang, and
Zhenan Sun. 2021.

</span>
<span class="ltx_bibblock">PyMAF: 3D Human Pose and Shape Regression with
Pyramidal Mesh Alignment Feedback Loop. In <em id="bib.bib288.3.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib289" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang
etÂ al<span id="bib.bib289.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Jinlu Zhang, Zhigang Tu,
Jianyu Yang, Yujin Chen, and
Junsong Yuan. 2022.

</span>
<span class="ltx_bibblock">MixSTE: Seq2seq Mixed Spatio-Temporal Encoder for
3D Human Pose Estimation in Video. In <em id="bib.bib289.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib290" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang
etÂ al<span id="bib.bib290.2.2.1" class="ltx_text">.</span> (2020c)</span>
<span class="ltx_bibblock">
Tianshu Zhang, Buzhen
Huang, and Yangang Wang.
2020c.

</span>
<span class="ltx_bibblock">Object-Occluded Human Shape and Pose Estimation
From a Single Color Image. In <em id="bib.bib290.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib291" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang
etÂ al<span id="bib.bib291.2.2.1" class="ltx_text">.</span> (2020b)</span>
<span class="ltx_bibblock">
Wenqiang Zhang, Jiemin
Fang, Xinggang Wang, and Wenyu Liu.
2020b.

</span>
<span class="ltx_bibblock">EfficientPose: Efficient Human Pose Estimation with
Neural Architecture Search. In <em id="bib.bib291.3.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2012.07086</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib292" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang
etÂ al<span id="bib.bib292.2.2.1" class="ltx_text">.</span> (2013)</span>
<span class="ltx_bibblock">
Weiyu Zhang, Menglong
Zhu, and KonstantinosÂ G Derpanis.
2013.

</span>
<span class="ltx_bibblock">From actemes to action: A strongly-supervised
representation for detailed action understanding. In
<em id="bib.bib292.3.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib293" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang
etÂ al<span id="bib.bib293.2.2.1" class="ltx_text">.</span> (2020a)</span>
<span class="ltx_bibblock">
Yuxiang Zhang, Liang An,
Tao Yu, xiu Li, Kun Li,
and Yebin Liu. 2020a.

</span>
<span class="ltx_bibblock">4D Association Graph for Realtime Multi-person
Motion Capture Using Multiple Video Cameras. In
<em id="bib.bib293.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib294" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang
etÂ al<span id="bib.bib294.2.2.1" class="ltx_text">.</span> (2020e)</span>
<span class="ltx_bibblock">
Yuexi Zhang, Yin Wang,
Octavia Camps, and Mario Sznaier.
2020e.

</span>
<span class="ltx_bibblock">Key Frame Proposal Network for Efficient Pose
Estimation in Videos. In <em id="bib.bib294.3.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2007.15217</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib295" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang
etÂ al<span id="bib.bib295.2.2.1" class="ltx_text">.</span> (2020f)</span>
<span class="ltx_bibblock">
Zhe Zhang, Chunyu Wang,
Wenhu Qin, and Wenjun Zeng.
2020f.

</span>
<span class="ltx_bibblock">Fusing Wearable IMUs With Multi-View Images for
Human Pose Estimation: A Geometric Approach. In
<em id="bib.bib295.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib296" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang
etÂ al<span id="bib.bib296.2.2.1" class="ltx_text">.</span> (2020g)</span>
<span class="ltx_bibblock">
Zhe Zhang, Chunyu Wang,
Weichao Qiu, Wenhu Qin, and
Wenjun Zeng. 2020g.

</span>
<span class="ltx_bibblock">AdaFuse: Adaptive Multiview Fusion for Accurate
Human Pose Estimation in the Wild. In <em id="bib.bib296.3.1" class="ltx_emph ltx_font_italic">IJCV</em>,
Vol.Â 129. 703â€“718.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib297" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao
etÂ al<span id="bib.bib297.2.2.1" class="ltx_text">.</span> (2019b)</span>
<span class="ltx_bibblock">
Long Zhao, Xi Peng,
Yu Tian, Mubbasir Kapadia, and
DimitrisÂ N. Metaxas. 2019b.

</span>
<span class="ltx_bibblock">Semantic Graph Convolutional Networks for 3D Human
Pose Regression. In <em id="bib.bib297.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib298" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao etÂ al<span id="bib.bib298.2.2.1" class="ltx_text">.</span> (2019a)</span>
<span class="ltx_bibblock">
Mingmin Zhao, Yingcheng
Liu, Aniruddh Raghu, Tianhong Li,
Hang Zhao, Antonio Torralba, and
Dina Katabi. 2019a.

</span>
<span class="ltx_bibblock">Through-Wall Human Mesh Recovery Using Radio
Signals. In <em id="bib.bib298.3.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib299" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao etÂ al<span id="bib.bib299.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Mingmin Zhao, Yonglong
Tian, Hang Zhao, MohammadÂ Abu Alsheikh,
Tianhong Li, Rumen Hristov,
Zachary Kabelac, Dina Katabi, and
Antonio Torralba. 2018.

</span>
<span class="ltx_bibblock">RF-based 3D skeletons. In
<em id="bib.bib299.3.1" class="ltx_emph ltx_font_italic">SIGCOMM</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib300" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao
etÂ al<span id="bib.bib300.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Qitao Zhao, Ce Zheng,
Mengyuan Liu, Pichao Wang, and
Chen Chen. 2023.

</span>
<span class="ltx_bibblock">PoseFormerV2: Exploring Frequency Domain for
Efficient and Robust 3D Human Pose Estimation. In
<em id="bib.bib300.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib301" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhen etÂ al<span id="bib.bib301.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Jianan Zhen, Qi Fang,
Jiaming Sun, Wentao Liu,
Wei Jiang, Hujun Bao, and
Xiaowei Zhou. 2020.

</span>
<span class="ltx_bibblock">SMAP: Single-Shot Multi-Person Absolute 3D Pose
Estimation. In <em id="bib.bib301.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib302" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng
etÂ al<span id="bib.bib302.2.2.1" class="ltx_text">.</span> (2023a)</span>
<span class="ltx_bibblock">
Ce Zheng, Xianpeng Liu,
Guo-Jun Qi, and Chen Chen.
2023a.

</span>
<span class="ltx_bibblock">POTTER: Pooling Attention Transformer for Efficient
Human Mesh Recovery. In <em id="bib.bib302.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib303" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng
etÂ al<span id="bib.bib303.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Ce Zheng, Matias
Mendieta, Pu Wang, Aidong Lu, and
Chen Chen. 2022.

</span>
<span class="ltx_bibblock">A Lightweight Graph Transformer Network for Human
Mesh Reconstruction from 2D Human Pose. In <em id="bib.bib303.3.1" class="ltx_emph ltx_font_italic">ACM
Multimedia</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib304" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng
etÂ al<span id="bib.bib304.2.2.1" class="ltx_text">.</span> (2023b)</span>
<span class="ltx_bibblock">
Ce Zheng, Matias
Mendieta, Taojiannan Yang, Guo-Jun Qi,
and Chen Chen. 2023b.

</span>
<span class="ltx_bibblock">FeatER: An Efficient Network for Human
Reconstruction via Feature Map-Based TransformER. In
<em id="bib.bib304.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib305" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng etÂ al<span id="bib.bib305.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Ce Zheng, Sijie Zhu,
Matias Mendieta, Taojiannan Yang,
Chen Chen, and Zhengming Ding.
2021.

</span>
<span class="ltx_bibblock">3D Human Pose Estimation with Spatial and Temporal
Transformers. In <em id="bib.bib305.3.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib306" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhi etÂ al<span id="bib.bib306.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Tiancheng Zhi, Christoph
Lassner, Tony Tung, Carsten Stoll,
SrinivasaÂ G. Narasimhan, and Minh Vo.
2020.

</span>
<span class="ltx_bibblock">TexMesh: Reconstructing Detailed Human Texture and
Geometry from RGB-D Video. In <em id="bib.bib306.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib307" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou
etÂ al<span id="bib.bib307.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Keyang Zhou, BharatÂ Lal
Bhatnagar, and Gerard Pons-Moll.
2020.

</span>
<span class="ltx_bibblock">Unsupervised Shape and Pose Disentanglement for 3D
Meshes. In <em id="bib.bib307.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2007.11341</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib308" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou etÂ al<span id="bib.bib308.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
K. Zhou, X. Han,
N. Jiang, K. Jia, and
J. Lu. 2019.

</span>
<span class="ltx_bibblock">HEMlets Pose: Learning Part-Centric Heatmap
Triplets for Accurate 3D Human Pose Estimation. In
<em id="bib.bib308.3.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib309" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou
etÂ al<span id="bib.bib309.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Xingyi Zhou, Qixing
Huang, Xiao Sun, Xiangyang Xue, and
Yichen Wei. 2017.

</span>
<span class="ltx_bibblock">Towards 3D Human Pose Estimation in the Wild: A
Weakly-Supervised Approach. In <em id="bib.bib309.3.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib310" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou
etÂ al<span id="bib.bib310.2.2.1" class="ltx_text">.</span> (2016a)</span>
<span class="ltx_bibblock">
Xingyi Zhou, Xiao Sun,
Wei Zhang, Shuang Liang, and
Yichen Wei. 2016a.

</span>
<span class="ltx_bibblock">Deep kinematic pose regression. In
<em id="bib.bib310.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib311" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou
etÂ al<span id="bib.bib311.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Xingyi Zhou, Dequan Wang,
and Philipp KrÃ¤henbÃ¼hl.
2019.

</span>
<span class="ltx_bibblock">Objects as points. In
<em id="bib.bib311.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1904.07850</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib312" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou
etÂ al<span id="bib.bib312.2.2.1" class="ltx_text">.</span> (2016b)</span>
<span class="ltx_bibblock">
Xiaowei Zhou, Menglong
Zhu, Kosta Derpanis, and Kostas
Daniilidis. 2016b.

</span>
<span class="ltx_bibblock">Sparseness Meets Deepness: 3D Human Pose Estimation
from Monocular Video. In <em id="bib.bib312.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib313" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou etÂ al<span id="bib.bib313.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Xiaowei Zhou, Menglong
Zhu, Georgios Pavlakos, Spyridon
Leonardos, KonstantinosÂ G Derpanis, and
Kostas Daniilidis. 2018.

</span>
<span class="ltx_bibblock">Monocap: Monocular human motion capture using a cnn
coupled with a geometric prior. In <em id="bib.bib313.3.1" class="ltx_emph ltx_font_italic">IEEE TPAMI</em>,
Vol.Â 41. 901â€“914.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib314" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu
etÂ al<span id="bib.bib314.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Hao Zhu, Xinxin Zuo,
Sen Wang, Xun Cao, and
Ruigang Yang. 2019.

</span>
<span class="ltx_bibblock">Detailed human shape estimation from a single image
by hierarchical mesh deformation. In <em id="bib.bib314.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib315" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu etÂ al<span id="bib.bib315.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Luyang Zhu, Konstantinos
Rematas, Brian Curless, Steve Seitz,
and Ira Kemelmacher-Shlizerman.
2020.

</span>
<span class="ltx_bibblock">Reconstructing NBA players. In
<em id="bib.bib315.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib316" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu
etÂ al<span id="bib.bib316.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Xiangyu Zhu, Yingying
Jiang, and Zhenbo Luo. 2017.

</span>
<span class="ltx_bibblock">Multi-person pose estimation for posetrack with
enhanced part affinity fields. In <em id="bib.bib316.3.1" class="ltx_emph ltx_font_italic">ICCV PoseTrack
Workshop</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib317" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zou and Tang (2021)</span>
<span class="ltx_bibblock">
Zhiming Zou and Wei
Tang. 2021.

</span>
<span class="ltx_bibblock">Modulated Graph Convolutional Network for 3D Human
Pose Estimation. In <em id="bib.bib317.1.1" class="ltx_emph ltx_font_italic">ICCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib318" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zuffi and Black (2015)</span>
<span class="ltx_bibblock">
Silvia Zuffi and
MichaelÂ J. Black. 2015.

</span>
<span class="ltx_bibblock">The Stitched Puppet: A Graphical Model of 3D
Human Shape and Pose. In <em id="bib.bib318.1.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2012.13391" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2012.13392" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2012.13392">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2012.13392" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2012.13393" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  8 00:27:54 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
