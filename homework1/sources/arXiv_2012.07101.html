<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2012.07101] Learning Heatmap-Style Jigsaw Puzzles Provides Good Pretraining for 2D Human Pose Estimation</title><meta property="og:description" content="The target of 2D human pose estimation is to locate the keypoints of body parts from input 2D images. State-of-the-art methods for pose estimation usually construct pixel-wise heatmaps from keypoints as labels for lear…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Learning Heatmap-Style Jigsaw Puzzles Provides Good Pretraining for 2D Human Pose Estimation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Learning Heatmap-Style Jigsaw Puzzles Provides Good Pretraining for 2D Human Pose Estimation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2012.07101">

<!--Generated on Sat Mar  9 04:51:03 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Learning Heatmap-Style Jigsaw Puzzles Provides Good
<br class="ltx_break">Pretraining for 2D Human Pose Estimation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kun Zhang<sup id="id19.2.id1" class="ltx_sup"><span id="id19.2.id1.1" class="ltx_text ltx_font_italic">1,2</span></sup><span id="footnotex1" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Rui Wu<sup id="id20.2.id1" class="ltx_sup"><span id="id20.2.id1.1" class="ltx_text ltx_font_italic">3</span></sup>
</span><span class="ltx_author_notes">Equal Contributions.</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Ping Yao<sup id="id21.2.id1" class="ltx_sup"><span id="id21.2.id1.1" class="ltx_text ltx_font_italic">1,2</span></sup>
</span><span class="ltx_author_notes">Corresponding Author.</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Kai Deng<sup id="id22.2.id1" class="ltx_sup"><span id="id22.2.id1.1" class="ltx_text ltx_font_italic">1,2</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Ding Li<sup id="id23.2.id1" class="ltx_sup"><span id="id23.2.id1.1" class="ltx_text ltx_font_italic">2,4</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Renbiao Liu<sup id="id24.2.id1" class="ltx_sup"><span id="id24.2.id1.1" class="ltx_text ltx_font_italic">5</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break">Chuanguang Yang<sup id="id25.2.id1" class="ltx_sup"><span id="id25.2.id1.1" class="ltx_text ltx_font_italic">1,2</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Ge Chen<sup id="id26.2.id1" class="ltx_sup"><span id="id26.2.id1.1" class="ltx_text ltx_font_italic">1,2</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Min Du<sup id="id27.2.id1" class="ltx_sup"><span id="id27.2.id1.1" class="ltx_text ltx_font_italic">3</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> and Tianyao Zheng<sup id="id28.3.id1" class="ltx_sup"><span id="id28.3.id1.1" class="ltx_text ltx_font_italic">1,2</span></sup>
<br class="ltx_break"><sup id="id29.4.id2" class="ltx_sup">1</sup>Institute of Computing Technology
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Chinese Academy of Sciences
<br class="ltx_break"><sup id="id30.4.id1" class="ltx_sup">2</sup>University of Chinese Academy of Sciences
<sup id="id31.5.id2" class="ltx_sup">3</sup>Horizon Robotics
<br class="ltx_break"><sup id="id32.6.id3" class="ltx_sup">4</sup>Institute of Automation
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Chinese Academy of Sciences
<sup id="id33.2.id1" class="ltx_sup">5</sup>Harbin Institute of Technology
<br class="ltx_break"><span id="id34.3.id2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">zhangkun@ieee.org {yaoping,dengkai19s,yangchuanguang,chenge18s,mailtozty}@ict.ac.cn
<br class="ltx_break">{rui.wu,min.du}@horizon.ai liding2016@ia.ac.cn richardodliu@gmail.com</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id35.id1" class="ltx_p">The target of 2D human pose estimation is to locate the keypoints of body parts from input 2D images. State-of-the-art methods for pose estimation usually construct pixel-wise heatmaps from keypoints as labels for learning convolution neural networks, which are usually initialized randomly or using classification models on ImageNet as their backbones. We note that 2D pose estimation task is highly dependent on the contextual relationship between image patches, thus we introduce a self-supervised method for pretraining 2D pose estimation networks. Specifically, we propose Heatmap-Style Jigsaw Puzzles (HSJP) problem as our pretext-task, whose target is to learn the location of each patch from an image composed of shuffled patches. During our pretraining process, we only use images of person instances in MS-COCO, rather than introducing extra and much larger ImageNet dataset. A heatmap-style label for patch location is designed and our learning process is in a non-contrastive way. The weights learned by HSJP pretext task are utilised as backbones of 2D human pose estimator, which are then finetuned on MS-COCO human keypoints dataset. With two popular and strong 2D human pose estimators, HRNet and SimpleBaseline, we evaluate mAP score on both MS-COCO validation and test-dev datasets. Our experiments show that downstream pose estimators with our self-supervised pretraining obtain much better performance than those trained from scratch, and are comparable to those using ImageNet classification models as their initial backbones.
<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup>Preprint Under Review.</span></span></span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">2D human pose estimation refers to the task of locating important human body parts (e.g. elbow, head, ankle, wrist, shoulder) from input images, which serves as a basic task for varieties of computer vision applications, such as person re-id <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, video analysis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>, action recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, and human-computer interaction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In recent years, the progress of 2D pose estimation has been greatly benefited by CNNs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>. There are three mainstreams for this task: (1) top-down methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> firstly separate all person instances with person detector, and then localise joints for each person instance with single-person pose estimation model; (2) bottom-up approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> firstly utilise joint estimator to localise all person joints, and then assign them to different person instances; (3) single-stage methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> predict person instances and human keypoints simultaneously. These three types of methods are suitable for different circumstances, but state-of-the-art performances usually come from heatmap-based top-down approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, such as HRNet-based Networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>, Pyramid-based Networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, and Hourglass Networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. These methods utilise CNNs to predict pixel-wise 2D heatmap for accurate human keypoint localisation. Our paper is focused on the pretraining of single-person pose estimator in the top-down framework.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S1.F1.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2012.07101/assets/x1.png" id="S1.F1.sf1.g1" class="ltx_graphics ltx_img_square" width="132" height="112" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.sf1.2.1.1" class="ltx_text" style="font-size:80%;">(a)</span> </span><span id="S1.F1.sf1.3.2" class="ltx_text" style="font-size:80%;">(a) Initial Image Patches</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S1.F1.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2012.07101/assets/x2.png" id="S1.F1.sf2.g1" class="ltx_graphics ltx_img_square" width="132" height="111" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.sf2.2.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span><span id="S1.F1.sf2.3.2" class="ltx_text" style="font-size:80%;">(b) Shuffled Image Patches</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S1.F1.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2012.07101/assets/heatmapforfig.jpg" id="S1.F1.sf3.g1" class="ltx_graphics ltx_img_square" width="172" height="146" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.sf3.2.1.1" class="ltx_text" style="font-size:80%;">(c)</span> </span><span id="S1.F1.sf3.3.2" class="ltx_text" style="font-size:80%;">(c)Target Heatmaps</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Description of our proposed Heatmap-Style Jigsaw Puzzles task. (a) Splitting the original image into <math id="S1.F1.4.m1.1" class="ltx_Math" alttext="N\times N" display="inline"><semantics id="S1.F1.4.m1.1b"><mrow id="S1.F1.4.m1.1.1" xref="S1.F1.4.m1.1.1.cmml"><mi id="S1.F1.4.m1.1.1.2" xref="S1.F1.4.m1.1.1.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S1.F1.4.m1.1.1.1" xref="S1.F1.4.m1.1.1.1.cmml">×</mo><mi id="S1.F1.4.m1.1.1.3" xref="S1.F1.4.m1.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.F1.4.m1.1c"><apply id="S1.F1.4.m1.1.1.cmml" xref="S1.F1.4.m1.1.1"><times id="S1.F1.4.m1.1.1.1.cmml" xref="S1.F1.4.m1.1.1.1"></times><ci id="S1.F1.4.m1.1.1.2.cmml" xref="S1.F1.4.m1.1.1.2">𝑁</ci><ci id="S1.F1.4.m1.1.1.3.cmml" xref="S1.F1.4.m1.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.4.m1.1d">N\times N</annotation></semantics></math> patches, <math id="S1.F1.5.m2.1" class="ltx_Math" alttext="N=3" display="inline"><semantics id="S1.F1.5.m2.1b"><mrow id="S1.F1.5.m2.1.1" xref="S1.F1.5.m2.1.1.cmml"><mi id="S1.F1.5.m2.1.1.2" xref="S1.F1.5.m2.1.1.2.cmml">N</mi><mo id="S1.F1.5.m2.1.1.1" xref="S1.F1.5.m2.1.1.1.cmml">=</mo><mn id="S1.F1.5.m2.1.1.3" xref="S1.F1.5.m2.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.F1.5.m2.1c"><apply id="S1.F1.5.m2.1.1.cmml" xref="S1.F1.5.m2.1.1"><eq id="S1.F1.5.m2.1.1.1.cmml" xref="S1.F1.5.m2.1.1.1"></eq><ci id="S1.F1.5.m2.1.1.2.cmml" xref="S1.F1.5.m2.1.1.2">𝑁</ci><cn type="integer" id="S1.F1.5.m2.1.1.3.cmml" xref="S1.F1.5.m2.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.5.m2.1d">N=3</annotation></semantics></math> in this figure for clear demonstration. (b) Randomly shuffling the split <math id="S1.F1.6.m3.1" class="ltx_Math" alttext="N\times N" display="inline"><semantics id="S1.F1.6.m3.1b"><mrow id="S1.F1.6.m3.1.1" xref="S1.F1.6.m3.1.1.cmml"><mi id="S1.F1.6.m3.1.1.2" xref="S1.F1.6.m3.1.1.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S1.F1.6.m3.1.1.1" xref="S1.F1.6.m3.1.1.1.cmml">×</mo><mi id="S1.F1.6.m3.1.1.3" xref="S1.F1.6.m3.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.F1.6.m3.1c"><apply id="S1.F1.6.m3.1.1.cmml" xref="S1.F1.6.m3.1.1"><times id="S1.F1.6.m3.1.1.1.cmml" xref="S1.F1.6.m3.1.1.1"></times><ci id="S1.F1.6.m3.1.1.2.cmml" xref="S1.F1.6.m3.1.1.2">𝑁</ci><ci id="S1.F1.6.m3.1.1.3.cmml" xref="S1.F1.6.m3.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.6.m3.1d">N\times N</annotation></semantics></math> patches to build jigsaw puzzles problem. (c) Ground truth heatmaps for channels corresponding to their image patches.</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">While self-supervised learning has recently achieved significant progress in varieties of computer vision tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>, supervised methods are still dominant for the pretraining of 2D pose estimation. Instead of finetuning from ImageNet classification weights <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, we design Heatmap-Style Jigsaw Puzzles (HSJP) problem as an efficient self-supervised learning pretext task for 2D human pose estimation. The description of our HSJP task is demonstrated in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Learning Heatmap-Style Jigsaw Puzzles Provides Good Pretraining for 2D Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. During the preprocessing of our HSJP task, we split the image of each person instance into <math id="S1.p3.1.m1.1" class="ltx_Math" alttext="N\times N" display="inline"><semantics id="S1.p3.1.m1.1a"><mrow id="S1.p3.1.m1.1.1" xref="S1.p3.1.m1.1.1.cmml"><mi id="S1.p3.1.m1.1.1.2" xref="S1.p3.1.m1.1.1.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S1.p3.1.m1.1.1.1" xref="S1.p3.1.m1.1.1.1.cmml">×</mo><mi id="S1.p3.1.m1.1.1.3" xref="S1.p3.1.m1.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b"><apply id="S1.p3.1.m1.1.1.cmml" xref="S1.p3.1.m1.1.1"><times id="S1.p3.1.m1.1.1.1.cmml" xref="S1.p3.1.m1.1.1.1"></times><ci id="S1.p3.1.m1.1.1.2.cmml" xref="S1.p3.1.m1.1.1.2">𝑁</ci><ci id="S1.p3.1.m1.1.1.3.cmml" xref="S1.p3.1.m1.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.1.m1.1c">N\times N</annotation></semantics></math> patches and shuffle them, which are illustrated in Figure <a href="#S1.F1.sf1" title="In Figure 1 ‣ 1 Introduction ‣ Learning Heatmap-Style Jigsaw Puzzles Provides Good Pretraining for 2D Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(a)</span></a> and <a href="#S1.F1.sf2" title="In Figure 1 ‣ 1 Introduction ‣ Learning Heatmap-Style Jigsaw Puzzles Provides Good Pretraining for 2D Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(b)</span></a>. As we have mentioned that the target of 2D pose estimation is to localise positions of human keypoints from input images, and in order to make our pretext task as similar to its downstream task as possible, the goal of HSJP task is to estimate the shuffled place of each original image patch with a heatmap-based approach. As Figure <a href="#S1.F1.sf3" title="In Figure 1 ‣ 1 Introduction ‣ Learning Heatmap-Style Jigsaw Puzzles Provides Good Pretraining for 2D Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(c)</span></a> shows, the ground truth of our heatmap for the channel corresponding to each original image patch is a 2D Gaussain distribution located in the shuffled centre. For example, the top-left patch in Figure <a href="#S1.F1.sf1" title="In Figure 1 ‣ 1 Introduction ‣ Learning Heatmap-Style Jigsaw Puzzles Provides Good Pretraining for 2D Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(a)</span></a> is located in the bottom-right of Figure <a href="#S1.F1.sf2" title="In Figure 1 ‣ 1 Introduction ‣ Learning Heatmap-Style Jigsaw Puzzles Provides Good Pretraining for 2D Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(b)</span></a>; therefore, the peak of Gaussian Distribution in the first channel is located in its bottom-right. In this way, the target of our HSJP pretext task is quite similar to those of many state-of-the-art 2D pose estimation methods: all of them predict target locations by heatmap-based means.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.3" class="ltx_p">During the pretraining process of our HSJP task, the network learns to predict Gaussian peaks as centres of those shuffled patches for correspondences to solve the jigsaw puzzles problem: the image patch on the Gaussian peak of the <math id="S1.p4.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S1.p4.1.m1.1a"><mi id="S1.p4.1.m1.1.1" xref="S1.p4.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S1.p4.1.m1.1b"><ci id="S1.p4.1.m1.1.1.cmml" xref="S1.p4.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.1.m1.1c">i</annotation></semantics></math>th channel in Figure <a href="#S1.F1.sf3" title="In Figure 1 ‣ 1 Introduction ‣ Learning Heatmap-Style Jigsaw Puzzles Provides Good Pretraining for 2D Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(c)</span></a> should be put back to the <math id="S1.p4.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S1.p4.2.m2.1a"><mi id="S1.p4.2.m2.1.1" xref="S1.p4.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S1.p4.2.m2.1b"><ci id="S1.p4.2.m2.1.1.cmml" xref="S1.p4.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.2.m2.1c">i</annotation></semantics></math>th patch in Figure <a href="#S1.F1.sf1" title="In Figure 1 ‣ 1 Introduction ‣ Learning Heatmap-Style Jigsaw Puzzles Provides Good Pretraining for 2D Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(a)</span></a>, <math id="S1.p4.3.m3.2" class="ltx_Math" alttext="i\in[0,N^{2})" display="inline"><semantics id="S1.p4.3.m3.2a"><mrow id="S1.p4.3.m3.2.2" xref="S1.p4.3.m3.2.2.cmml"><mi id="S1.p4.3.m3.2.2.3" xref="S1.p4.3.m3.2.2.3.cmml">i</mi><mo id="S1.p4.3.m3.2.2.2" xref="S1.p4.3.m3.2.2.2.cmml">∈</mo><mrow id="S1.p4.3.m3.2.2.1.1" xref="S1.p4.3.m3.2.2.1.2.cmml"><mo stretchy="false" id="S1.p4.3.m3.2.2.1.1.2" xref="S1.p4.3.m3.2.2.1.2.cmml">[</mo><mn id="S1.p4.3.m3.1.1" xref="S1.p4.3.m3.1.1.cmml">0</mn><mo id="S1.p4.3.m3.2.2.1.1.3" xref="S1.p4.3.m3.2.2.1.2.cmml">,</mo><msup id="S1.p4.3.m3.2.2.1.1.1" xref="S1.p4.3.m3.2.2.1.1.1.cmml"><mi id="S1.p4.3.m3.2.2.1.1.1.2" xref="S1.p4.3.m3.2.2.1.1.1.2.cmml">N</mi><mn id="S1.p4.3.m3.2.2.1.1.1.3" xref="S1.p4.3.m3.2.2.1.1.1.3.cmml">2</mn></msup><mo stretchy="false" id="S1.p4.3.m3.2.2.1.1.4" xref="S1.p4.3.m3.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.p4.3.m3.2b"><apply id="S1.p4.3.m3.2.2.cmml" xref="S1.p4.3.m3.2.2"><in id="S1.p4.3.m3.2.2.2.cmml" xref="S1.p4.3.m3.2.2.2"></in><ci id="S1.p4.3.m3.2.2.3.cmml" xref="S1.p4.3.m3.2.2.3">𝑖</ci><interval closure="closed-open" id="S1.p4.3.m3.2.2.1.2.cmml" xref="S1.p4.3.m3.2.2.1.1"><cn type="integer" id="S1.p4.3.m3.1.1.cmml" xref="S1.p4.3.m3.1.1">0</cn><apply id="S1.p4.3.m3.2.2.1.1.1.cmml" xref="S1.p4.3.m3.2.2.1.1.1"><csymbol cd="ambiguous" id="S1.p4.3.m3.2.2.1.1.1.1.cmml" xref="S1.p4.3.m3.2.2.1.1.1">superscript</csymbol><ci id="S1.p4.3.m3.2.2.1.1.1.2.cmml" xref="S1.p4.3.m3.2.2.1.1.1.2">𝑁</ci><cn type="integer" id="S1.p4.3.m3.2.2.1.1.1.3.cmml" xref="S1.p4.3.m3.2.2.1.1.1.3">2</cn></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.3.m3.2c">i\in[0,N^{2})</annotation></semantics></math>. These correspondences learnt in HSJP task are also helpful to the downstream task of 2D human pose estimation, for the case that knowing relative positions of image patches is also beneficial to the localisation of human keypoints. For instance, if we know whether each shuffled patch should be the top-left or top-right corner in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Learning Heatmap-Style Jigsaw Puzzles Provides Good Pretraining for 2D Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, it’s also helpful to locate shoulders of given person instance, as person shoulders are frequently located in neighbourhoods of top-left or top-right corners in original images before data augmentation.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Instead of supervised pretraining on labelled ImageNet dataset, our network is only pretrained with self-supervised HSJP pretext task on all person instances of MS-COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> dataset, and finetuned on their human keypoint labels. Although we make no use of extra labels for pretraining, our keypoint performance is comparable to that of ImageNet pretraining on MS-COCO validation and test-dev datasets.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Heatmap-based human pose estimation</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Since <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> firstly proposed to utilise a heatmap-based method for single-person pose estimation, recent state-of-the-art performances have mostly been achieved by relative means. For example, Hourglass-based Networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> apply symmetric network structures with skip connections to generate pixel-wise feature maps for accurate keypoint prediction; Pyramid-based Networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> consolidate feature maps across ResNet stages, and some improvements were achieved by repeating pyramid architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>; HRNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> keeps high-resolution representations and exchanges information across multi-resolution subnetworks, and some recent works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> focus on improving the preprocessing and postprocessing details of their codebase.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Self-supervised pretraining for computer vision</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Self-supervised pretraining methods aim at designing pretext tasks without using handcrafted annotations so as to benefit their downstream targets. The main idea of self-supervised representation learning is to utilise annotations that are freely available from original datasets. It also saves labour costs of labelling datasets for machine learning. The past few years have witnessed the springing up of self-supervised means for computer vision field. A variety of pretext tasks have been proposed, such as spatial position prediction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, transformation prediction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>, image inpainting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, image colourisation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, and learning latent embeddings <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>. Recently, some papers also discussed the robustness and usefulness of self-supervised pretraining approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Applications on various computer vision tasks have also been discovered by self-supervised learning researchers in recent years. For examples, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> predicts boxes constructed from dataset to benefit cardiac MR image analysis, and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> utilises self-supervised attention for weakly supervised semantic segmentation. However, supervised pretraining methods on large datasets such as ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> are still dominant for 2D human pose estimation. Starting from the status quo, we design to solve Heatmap-Style Jigsaw Puzzles (HSJP) as pretext task for pretraining heatmap-based 2D pose estimation networks, and we have achieved competitive performance on MS-COCO benchmark.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Methods for solving jigsaw puzzles</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">The target of jigsaw puzzles problem is to put shuffled image patches back to their original places, which serves as an effective pretext for learning visuospatial dependencies in real world <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. Some previous works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> also made use of jigsaw puzzles to learn vector-based representations for computer vision tasks such as image classification or object detection, but never had we encountered any paper that solve jigsaw puzzles as the pretext task for 2D human pose estimation methods: they are usually non-pretrained <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, or pretrained on ImageNet dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">A large number of papers focus on providing accurate methods for solving jigsaw puzzles. Early works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> consider using information from shape or appearance to solve this problem. Recently, there are two mainstreams for reassembling image patches: greedy methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> and global methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>. We don’t concentrate on proposing any accurate method for solving jigsaw puzzles in this paper, but aim at learning suitable representations for pretraining heatmap-based 2D pose estimation networks.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Heatmap-Style Jigsaw Puzzles</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">As is mentioned in the introduction, the target of 2D pose estimation is to localise positions of human keypoints from 2D input images. In order to make our self-supervised pretraining target as relative to downstream task as possible, we propose Heatmap-Style Jigsaw Puzzles (HSJP) as pretext task for self-supervised pretraining, whose goal is to estimate the shuffled locations of each original image patch. The pretext and downstream tasks both predict certain part of a person instance (patch or keypoint) with a heatmap-based approach. In this section, we will elaborate details of how to pretrain a 2D human pose estimator with HSJP task.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Target description for HSJP pretext task</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.6" class="ltx_p">The target of our proposed HSJP pretext is to estimate the shuffled place of each original image patch with a heatmap-based approach, whose inputs include all person instances cropped from COCO dataset. To make our self-supervised pretraining method comparable to ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> pretraining, the resolution of input is <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="224\times 224" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mn id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">224</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.1.m1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><times id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">224</cn><cn type="integer" id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">224\times 224</annotation></semantics></math>, which is the same as most popular classification methods on ImageNet dataset. We separate input image <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">I</annotation></semantics></math> into <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="N\times N" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mrow id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.3.m3.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.cmml">×</mo><mi id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><times id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1"></times><ci id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2">𝑁</ci><ci id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">N\times N</annotation></semantics></math> patches of <math id="S3.SS1.p1.4.m4.2" class="ltx_Math" alttext="\lceil\frac{224}{N}\rceil\times\lceil\frac{224}{N}\rceil" display="inline"><semantics id="S3.SS1.p1.4.m4.2a"><mrow id="S3.SS1.p1.4.m4.2.3" xref="S3.SS1.p1.4.m4.2.3.cmml"><mrow id="S3.SS1.p1.4.m4.2.3.2.2" xref="S3.SS1.p1.4.m4.2.3.2.1.cmml"><mo stretchy="false" id="S3.SS1.p1.4.m4.2.3.2.2.1" xref="S3.SS1.p1.4.m4.2.3.2.1.1.cmml">⌈</mo><mfrac id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml"><mn id="S3.SS1.p1.4.m4.1.1.2" xref="S3.SS1.p1.4.m4.1.1.2.cmml">224</mn><mi id="S3.SS1.p1.4.m4.1.1.3" xref="S3.SS1.p1.4.m4.1.1.3.cmml">N</mi></mfrac><mo rspace="0.055em" stretchy="false" id="S3.SS1.p1.4.m4.2.3.2.2.2" xref="S3.SS1.p1.4.m4.2.3.2.1.1.cmml">⌉</mo></mrow><mo rspace="0.222em" id="S3.SS1.p1.4.m4.2.3.1" xref="S3.SS1.p1.4.m4.2.3.1.cmml">×</mo><mrow id="S3.SS1.p1.4.m4.2.3.3.2" xref="S3.SS1.p1.4.m4.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS1.p1.4.m4.2.3.3.2.1" xref="S3.SS1.p1.4.m4.2.3.3.1.1.cmml">⌈</mo><mfrac id="S3.SS1.p1.4.m4.2.2" xref="S3.SS1.p1.4.m4.2.2.cmml"><mn id="S3.SS1.p1.4.m4.2.2.2" xref="S3.SS1.p1.4.m4.2.2.2.cmml">224</mn><mi id="S3.SS1.p1.4.m4.2.2.3" xref="S3.SS1.p1.4.m4.2.2.3.cmml">N</mi></mfrac><mo stretchy="false" id="S3.SS1.p1.4.m4.2.3.3.2.2" xref="S3.SS1.p1.4.m4.2.3.3.1.1.cmml">⌉</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.2b"><apply id="S3.SS1.p1.4.m4.2.3.cmml" xref="S3.SS1.p1.4.m4.2.3"><times id="S3.SS1.p1.4.m4.2.3.1.cmml" xref="S3.SS1.p1.4.m4.2.3.1"></times><apply id="S3.SS1.p1.4.m4.2.3.2.1.cmml" xref="S3.SS1.p1.4.m4.2.3.2.2"><ceiling id="S3.SS1.p1.4.m4.2.3.2.1.1.cmml" xref="S3.SS1.p1.4.m4.2.3.2.2.1"></ceiling><apply id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1"><divide id="S3.SS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1"></divide><cn type="integer" id="S3.SS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2">224</cn><ci id="S3.SS1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3">𝑁</ci></apply></apply><apply id="S3.SS1.p1.4.m4.2.3.3.1.cmml" xref="S3.SS1.p1.4.m4.2.3.3.2"><ceiling id="S3.SS1.p1.4.m4.2.3.3.1.1.cmml" xref="S3.SS1.p1.4.m4.2.3.3.2.1"></ceiling><apply id="S3.SS1.p1.4.m4.2.2.cmml" xref="S3.SS1.p1.4.m4.2.2"><divide id="S3.SS1.p1.4.m4.2.2.1.cmml" xref="S3.SS1.p1.4.m4.2.2"></divide><cn type="integer" id="S3.SS1.p1.4.m4.2.2.2.cmml" xref="S3.SS1.p1.4.m4.2.2.2">224</cn><ci id="S3.SS1.p1.4.m4.2.2.3.cmml" xref="S3.SS1.p1.4.m4.2.2.3">𝑁</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.2c">\lceil\frac{224}{N}\rceil\times\lceil\frac{224}{N}\rceil</annotation></semantics></math> during data preprocessing, as is illustrated in Figure <a href="#S1.F1.sf1" title="In Figure 1 ‣ 1 Introduction ‣ Learning Heatmap-Style Jigsaw Puzzles Provides Good Pretraining for 2D Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(a)</span></a>. We denote the index of each split image patch as <math id="S3.SS1.p1.5.m5.2" class="ltx_Math" alttext="i\in[0,N^{2})" display="inline"><semantics id="S3.SS1.p1.5.m5.2a"><mrow id="S3.SS1.p1.5.m5.2.2" xref="S3.SS1.p1.5.m5.2.2.cmml"><mi id="S3.SS1.p1.5.m5.2.2.3" xref="S3.SS1.p1.5.m5.2.2.3.cmml">i</mi><mo id="S3.SS1.p1.5.m5.2.2.2" xref="S3.SS1.p1.5.m5.2.2.2.cmml">∈</mo><mrow id="S3.SS1.p1.5.m5.2.2.1.1" xref="S3.SS1.p1.5.m5.2.2.1.2.cmml"><mo stretchy="false" id="S3.SS1.p1.5.m5.2.2.1.1.2" xref="S3.SS1.p1.5.m5.2.2.1.2.cmml">[</mo><mn id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml">0</mn><mo id="S3.SS1.p1.5.m5.2.2.1.1.3" xref="S3.SS1.p1.5.m5.2.2.1.2.cmml">,</mo><msup id="S3.SS1.p1.5.m5.2.2.1.1.1" xref="S3.SS1.p1.5.m5.2.2.1.1.1.cmml"><mi id="S3.SS1.p1.5.m5.2.2.1.1.1.2" xref="S3.SS1.p1.5.m5.2.2.1.1.1.2.cmml">N</mi><mn id="S3.SS1.p1.5.m5.2.2.1.1.1.3" xref="S3.SS1.p1.5.m5.2.2.1.1.1.3.cmml">2</mn></msup><mo stretchy="false" id="S3.SS1.p1.5.m5.2.2.1.1.4" xref="S3.SS1.p1.5.m5.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.2b"><apply id="S3.SS1.p1.5.m5.2.2.cmml" xref="S3.SS1.p1.5.m5.2.2"><in id="S3.SS1.p1.5.m5.2.2.2.cmml" xref="S3.SS1.p1.5.m5.2.2.2"></in><ci id="S3.SS1.p1.5.m5.2.2.3.cmml" xref="S3.SS1.p1.5.m5.2.2.3">𝑖</ci><interval closure="closed-open" id="S3.SS1.p1.5.m5.2.2.1.2.cmml" xref="S3.SS1.p1.5.m5.2.2.1.1"><cn type="integer" id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">0</cn><apply id="S3.SS1.p1.5.m5.2.2.1.1.1.cmml" xref="S3.SS1.p1.5.m5.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.2.2.1.1.1.1.cmml" xref="S3.SS1.p1.5.m5.2.2.1.1.1">superscript</csymbol><ci id="S3.SS1.p1.5.m5.2.2.1.1.1.2.cmml" xref="S3.SS1.p1.5.m5.2.2.1.1.1.2">𝑁</ci><cn type="integer" id="S3.SS1.p1.5.m5.2.2.1.1.1.3.cmml" xref="S3.SS1.p1.5.m5.2.2.1.1.1.3">2</cn></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.2c">i\in[0,N^{2})</annotation></semantics></math>, which is also corresponding to the channel of output heatmap. The size of each image patch is rounded up for the case where <math id="S3.SS1.p1.6.m6.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS1.p1.6.m6.1a"><mi id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><ci id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">N</annotation></semantics></math> cannot divide 224.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.6" class="ltx_p">We then apply Knuth-Durstenfeld algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> to randomly shuffle the split image patches, as is illustrated in Figure <a href="#S1.F1.sf2" title="In Figure 1 ‣ 1 Introduction ‣ Learning Heatmap-Style Jigsaw Puzzles Provides Good Pretraining for 2D Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(b)</span></a>. The centre of each shuffled image patch is denoted as <math id="S3.SS1.p2.1.m1.2" class="ltx_Math" alttext="\bm{p}_{i}=(x_{i},y_{i})" display="inline"><semantics id="S3.SS1.p2.1.m1.2a"><mrow id="S3.SS1.p2.1.m1.2.2" xref="S3.SS1.p2.1.m1.2.2.cmml"><msub id="S3.SS1.p2.1.m1.2.2.4" xref="S3.SS1.p2.1.m1.2.2.4.cmml"><mi id="S3.SS1.p2.1.m1.2.2.4.2" xref="S3.SS1.p2.1.m1.2.2.4.2.cmml">𝒑</mi><mi id="S3.SS1.p2.1.m1.2.2.4.3" xref="S3.SS1.p2.1.m1.2.2.4.3.cmml">i</mi></msub><mo id="S3.SS1.p2.1.m1.2.2.3" xref="S3.SS1.p2.1.m1.2.2.3.cmml">=</mo><mrow id="S3.SS1.p2.1.m1.2.2.2.2" xref="S3.SS1.p2.1.m1.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS1.p2.1.m1.2.2.2.2.3" xref="S3.SS1.p2.1.m1.2.2.2.3.cmml">(</mo><msub id="S3.SS1.p2.1.m1.1.1.1.1.1" xref="S3.SS1.p2.1.m1.1.1.1.1.1.cmml"><mi id="S3.SS1.p2.1.m1.1.1.1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.SS1.p2.1.m1.1.1.1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS1.p2.1.m1.2.2.2.2.4" xref="S3.SS1.p2.1.m1.2.2.2.3.cmml">,</mo><msub id="S3.SS1.p2.1.m1.2.2.2.2.2" xref="S3.SS1.p2.1.m1.2.2.2.2.2.cmml"><mi id="S3.SS1.p2.1.m1.2.2.2.2.2.2" xref="S3.SS1.p2.1.m1.2.2.2.2.2.2.cmml">y</mi><mi id="S3.SS1.p2.1.m1.2.2.2.2.2.3" xref="S3.SS1.p2.1.m1.2.2.2.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS1.p2.1.m1.2.2.2.2.5" xref="S3.SS1.p2.1.m1.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.2b"><apply id="S3.SS1.p2.1.m1.2.2.cmml" xref="S3.SS1.p2.1.m1.2.2"><eq id="S3.SS1.p2.1.m1.2.2.3.cmml" xref="S3.SS1.p2.1.m1.2.2.3"></eq><apply id="S3.SS1.p2.1.m1.2.2.4.cmml" xref="S3.SS1.p2.1.m1.2.2.4"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.2.2.4.1.cmml" xref="S3.SS1.p2.1.m1.2.2.4">subscript</csymbol><ci id="S3.SS1.p2.1.m1.2.2.4.2.cmml" xref="S3.SS1.p2.1.m1.2.2.4.2">𝒑</ci><ci id="S3.SS1.p2.1.m1.2.2.4.3.cmml" xref="S3.SS1.p2.1.m1.2.2.4.3">𝑖</ci></apply><interval closure="open" id="S3.SS1.p2.1.m1.2.2.2.3.cmml" xref="S3.SS1.p2.1.m1.2.2.2.2"><apply id="S3.SS1.p2.1.m1.1.1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p2.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.1.1.1.2">𝑥</ci><ci id="S3.SS1.p2.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.SS1.p2.1.m1.2.2.2.2.2.cmml" xref="S3.SS1.p2.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.2.2.2.2.2.1.cmml" xref="S3.SS1.p2.1.m1.2.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p2.1.m1.2.2.2.2.2.2.cmml" xref="S3.SS1.p2.1.m1.2.2.2.2.2.2">𝑦</ci><ci id="S3.SS1.p2.1.m1.2.2.2.2.2.3.cmml" xref="S3.SS1.p2.1.m1.2.2.2.2.2.3">𝑖</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.2c">\bm{p}_{i}=(x_{i},y_{i})</annotation></semantics></math>. In order to make the target <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="\bm{G}" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">𝑮</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">𝑮</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">\bm{G}</annotation></semantics></math> of pretext task as similar to 2D human pose estimation as possible, the ground truth heatmap for each output channel <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><mi id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">i</annotation></semantics></math> is a 2-dimensional Gaussian Distribution located on each patch centre <math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="\bm{p}_{i}" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><msub id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml"><mi id="S3.SS1.p2.4.m4.1.1.2" xref="S3.SS1.p2.4.m4.1.1.2.cmml">𝒑</mi><mi id="S3.SS1.p2.4.m4.1.1.3" xref="S3.SS1.p2.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><apply id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2">𝒑</ci><ci id="S3.SS1.p2.4.m4.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">\bm{p}_{i}</annotation></semantics></math>. For each output channel <math id="S3.SS1.p2.5.m5.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS1.p2.5.m5.1a"><mi id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><ci id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">i</annotation></semantics></math>, the heatmap response at location <math id="S3.SS1.p2.6.m6.2" class="ltx_Math" alttext="\bm{p}^{\prime}=(x^{\prime},y^{\prime})" display="inline"><semantics id="S3.SS1.p2.6.m6.2a"><mrow id="S3.SS1.p2.6.m6.2.2" xref="S3.SS1.p2.6.m6.2.2.cmml"><msup id="S3.SS1.p2.6.m6.2.2.4" xref="S3.SS1.p2.6.m6.2.2.4.cmml"><mi id="S3.SS1.p2.6.m6.2.2.4.2" xref="S3.SS1.p2.6.m6.2.2.4.2.cmml">𝒑</mi><mo id="S3.SS1.p2.6.m6.2.2.4.3" xref="S3.SS1.p2.6.m6.2.2.4.3.cmml">′</mo></msup><mo id="S3.SS1.p2.6.m6.2.2.3" xref="S3.SS1.p2.6.m6.2.2.3.cmml">=</mo><mrow id="S3.SS1.p2.6.m6.2.2.2.2" xref="S3.SS1.p2.6.m6.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS1.p2.6.m6.2.2.2.2.3" xref="S3.SS1.p2.6.m6.2.2.2.3.cmml">(</mo><msup id="S3.SS1.p2.6.m6.1.1.1.1.1" xref="S3.SS1.p2.6.m6.1.1.1.1.1.cmml"><mi id="S3.SS1.p2.6.m6.1.1.1.1.1.2" xref="S3.SS1.p2.6.m6.1.1.1.1.1.2.cmml">x</mi><mo id="S3.SS1.p2.6.m6.1.1.1.1.1.3" xref="S3.SS1.p2.6.m6.1.1.1.1.1.3.cmml">′</mo></msup><mo id="S3.SS1.p2.6.m6.2.2.2.2.4" xref="S3.SS1.p2.6.m6.2.2.2.3.cmml">,</mo><msup id="S3.SS1.p2.6.m6.2.2.2.2.2" xref="S3.SS1.p2.6.m6.2.2.2.2.2.cmml"><mi id="S3.SS1.p2.6.m6.2.2.2.2.2.2" xref="S3.SS1.p2.6.m6.2.2.2.2.2.2.cmml">y</mi><mo id="S3.SS1.p2.6.m6.2.2.2.2.2.3" xref="S3.SS1.p2.6.m6.2.2.2.2.2.3.cmml">′</mo></msup><mo stretchy="false" id="S3.SS1.p2.6.m6.2.2.2.2.5" xref="S3.SS1.p2.6.m6.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m6.2b"><apply id="S3.SS1.p2.6.m6.2.2.cmml" xref="S3.SS1.p2.6.m6.2.2"><eq id="S3.SS1.p2.6.m6.2.2.3.cmml" xref="S3.SS1.p2.6.m6.2.2.3"></eq><apply id="S3.SS1.p2.6.m6.2.2.4.cmml" xref="S3.SS1.p2.6.m6.2.2.4"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.2.2.4.1.cmml" xref="S3.SS1.p2.6.m6.2.2.4">superscript</csymbol><ci id="S3.SS1.p2.6.m6.2.2.4.2.cmml" xref="S3.SS1.p2.6.m6.2.2.4.2">𝒑</ci><ci id="S3.SS1.p2.6.m6.2.2.4.3.cmml" xref="S3.SS1.p2.6.m6.2.2.4.3">′</ci></apply><interval closure="open" id="S3.SS1.p2.6.m6.2.2.2.3.cmml" xref="S3.SS1.p2.6.m6.2.2.2.2"><apply id="S3.SS1.p2.6.m6.1.1.1.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1.1.1.1">superscript</csymbol><ci id="S3.SS1.p2.6.m6.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.6.m6.1.1.1.1.1.2">𝑥</ci><ci id="S3.SS1.p2.6.m6.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.6.m6.1.1.1.1.1.3">′</ci></apply><apply id="S3.SS1.p2.6.m6.2.2.2.2.2.cmml" xref="S3.SS1.p2.6.m6.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.2.2.2.2.2.1.cmml" xref="S3.SS1.p2.6.m6.2.2.2.2.2">superscript</csymbol><ci id="S3.SS1.p2.6.m6.2.2.2.2.2.2.cmml" xref="S3.SS1.p2.6.m6.2.2.2.2.2.2">𝑦</ci><ci id="S3.SS1.p2.6.m6.2.2.2.2.2.3.cmml" xref="S3.SS1.p2.6.m6.2.2.2.2.2.3">′</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.2c">\bm{p}^{\prime}=(x^{\prime},y^{\prime})</annotation></semantics></math> is computed as:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.4" class="ltx_Math" alttext="\bm{G}_{x^{\prime}y^{\prime}}^{i}=\exp(-\frac{(x^{\prime}-x_{i})^{2}+(y^{\prime}-y_{i})^{2}}{2\sigma^{2}})," display="block"><semantics id="S3.E1.m1.4a"><mrow id="S3.E1.m1.4.4.1" xref="S3.E1.m1.4.4.1.1.cmml"><mrow id="S3.E1.m1.4.4.1.1" xref="S3.E1.m1.4.4.1.1.cmml"><msubsup id="S3.E1.m1.4.4.1.1.3" xref="S3.E1.m1.4.4.1.1.3.cmml"><mi id="S3.E1.m1.4.4.1.1.3.2.2" xref="S3.E1.m1.4.4.1.1.3.2.2.cmml">𝑮</mi><mrow id="S3.E1.m1.4.4.1.1.3.2.3" xref="S3.E1.m1.4.4.1.1.3.2.3.cmml"><msup id="S3.E1.m1.4.4.1.1.3.2.3.2" xref="S3.E1.m1.4.4.1.1.3.2.3.2.cmml"><mi id="S3.E1.m1.4.4.1.1.3.2.3.2.2" xref="S3.E1.m1.4.4.1.1.3.2.3.2.2.cmml">x</mi><mo id="S3.E1.m1.4.4.1.1.3.2.3.2.3" xref="S3.E1.m1.4.4.1.1.3.2.3.2.3.cmml">′</mo></msup><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.1.1.3.2.3.1" xref="S3.E1.m1.4.4.1.1.3.2.3.1.cmml">​</mo><msup id="S3.E1.m1.4.4.1.1.3.2.3.3" xref="S3.E1.m1.4.4.1.1.3.2.3.3.cmml"><mi id="S3.E1.m1.4.4.1.1.3.2.3.3.2" xref="S3.E1.m1.4.4.1.1.3.2.3.3.2.cmml">y</mi><mo id="S3.E1.m1.4.4.1.1.3.2.3.3.3" xref="S3.E1.m1.4.4.1.1.3.2.3.3.3.cmml">′</mo></msup></mrow><mi id="S3.E1.m1.4.4.1.1.3.3" xref="S3.E1.m1.4.4.1.1.3.3.cmml">i</mi></msubsup><mo id="S3.E1.m1.4.4.1.1.2" xref="S3.E1.m1.4.4.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.4.4.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.2.cmml"><mi id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">exp</mi><mo id="S3.E1.m1.4.4.1.1.1.1a" xref="S3.E1.m1.4.4.1.1.1.2.cmml">⁡</mo><mrow id="S3.E1.m1.4.4.1.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.2.cmml"><mo stretchy="false" id="S3.E1.m1.4.4.1.1.1.1.1.2" xref="S3.E1.m1.4.4.1.1.1.2.cmml">(</mo><mrow id="S3.E1.m1.4.4.1.1.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.4.4.1.1.1.1.1.1a" xref="S3.E1.m1.4.4.1.1.1.1.1.1.cmml">−</mo><mfrac id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml"><mrow id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.2.cmml"><msup id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml"><msup id="S3.E1.m1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.cmml">x</mi><mo id="S3.E1.m1.1.1.1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.1.1.1.2.3.cmml">′</mo></msup><mo id="S3.E1.m1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">−</mo><msub id="S3.E1.m1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.3.2.cmml">x</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mn id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml">2</mn></msup><mo id="S3.E1.m1.2.2.2.3" xref="S3.E1.m1.2.2.2.3.cmml">+</mo><msup id="S3.E1.m1.2.2.2.2" xref="S3.E1.m1.2.2.2.2.cmml"><mrow id="S3.E1.m1.2.2.2.2.1.1" xref="S3.E1.m1.2.2.2.2.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.2.2.1.1.2" xref="S3.E1.m1.2.2.2.2.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.2.2.2.2.1.1.1" xref="S3.E1.m1.2.2.2.2.1.1.1.cmml"><msup id="S3.E1.m1.2.2.2.2.1.1.1.2" xref="S3.E1.m1.2.2.2.2.1.1.1.2.cmml"><mi id="S3.E1.m1.2.2.2.2.1.1.1.2.2" xref="S3.E1.m1.2.2.2.2.1.1.1.2.2.cmml">y</mi><mo id="S3.E1.m1.2.2.2.2.1.1.1.2.3" xref="S3.E1.m1.2.2.2.2.1.1.1.2.3.cmml">′</mo></msup><mo id="S3.E1.m1.2.2.2.2.1.1.1.1" xref="S3.E1.m1.2.2.2.2.1.1.1.1.cmml">−</mo><msub id="S3.E1.m1.2.2.2.2.1.1.1.3" xref="S3.E1.m1.2.2.2.2.1.1.1.3.cmml"><mi id="S3.E1.m1.2.2.2.2.1.1.1.3.2" xref="S3.E1.m1.2.2.2.2.1.1.1.3.2.cmml">y</mi><mi id="S3.E1.m1.2.2.2.2.1.1.1.3.3" xref="S3.E1.m1.2.2.2.2.1.1.1.3.3.cmml">i</mi></msub></mrow><mo stretchy="false" id="S3.E1.m1.2.2.2.2.1.1.3" xref="S3.E1.m1.2.2.2.2.1.1.1.cmml">)</mo></mrow><mn id="S3.E1.m1.2.2.2.2.3" xref="S3.E1.m1.2.2.2.2.3.cmml">2</mn></msup></mrow><mrow id="S3.E1.m1.2.2.4" xref="S3.E1.m1.2.2.4.cmml"><mn id="S3.E1.m1.2.2.4.2" xref="S3.E1.m1.2.2.4.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.4.1" xref="S3.E1.m1.2.2.4.1.cmml">​</mo><msup id="S3.E1.m1.2.2.4.3" xref="S3.E1.m1.2.2.4.3.cmml"><mi id="S3.E1.m1.2.2.4.3.2" xref="S3.E1.m1.2.2.4.3.2.cmml">σ</mi><mn id="S3.E1.m1.2.2.4.3.3" xref="S3.E1.m1.2.2.4.3.3.cmml">2</mn></msup></mrow></mfrac></mrow><mo stretchy="false" id="S3.E1.m1.4.4.1.1.1.1.1.3" xref="S3.E1.m1.4.4.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.4.4.1.2" xref="S3.E1.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.4b"><apply id="S3.E1.m1.4.4.1.1.cmml" xref="S3.E1.m1.4.4.1"><eq id="S3.E1.m1.4.4.1.1.2.cmml" xref="S3.E1.m1.4.4.1.1.2"></eq><apply id="S3.E1.m1.4.4.1.1.3.cmml" xref="S3.E1.m1.4.4.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.1.3.1.cmml" xref="S3.E1.m1.4.4.1.1.3">superscript</csymbol><apply id="S3.E1.m1.4.4.1.1.3.2.cmml" xref="S3.E1.m1.4.4.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.1.3.2.1.cmml" xref="S3.E1.m1.4.4.1.1.3">subscript</csymbol><ci id="S3.E1.m1.4.4.1.1.3.2.2.cmml" xref="S3.E1.m1.4.4.1.1.3.2.2">𝑮</ci><apply id="S3.E1.m1.4.4.1.1.3.2.3.cmml" xref="S3.E1.m1.4.4.1.1.3.2.3"><times id="S3.E1.m1.4.4.1.1.3.2.3.1.cmml" xref="S3.E1.m1.4.4.1.1.3.2.3.1"></times><apply id="S3.E1.m1.4.4.1.1.3.2.3.2.cmml" xref="S3.E1.m1.4.4.1.1.3.2.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.1.3.2.3.2.1.cmml" xref="S3.E1.m1.4.4.1.1.3.2.3.2">superscript</csymbol><ci id="S3.E1.m1.4.4.1.1.3.2.3.2.2.cmml" xref="S3.E1.m1.4.4.1.1.3.2.3.2.2">𝑥</ci><ci id="S3.E1.m1.4.4.1.1.3.2.3.2.3.cmml" xref="S3.E1.m1.4.4.1.1.3.2.3.2.3">′</ci></apply><apply id="S3.E1.m1.4.4.1.1.3.2.3.3.cmml" xref="S3.E1.m1.4.4.1.1.3.2.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.1.3.2.3.3.1.cmml" xref="S3.E1.m1.4.4.1.1.3.2.3.3">superscript</csymbol><ci id="S3.E1.m1.4.4.1.1.3.2.3.3.2.cmml" xref="S3.E1.m1.4.4.1.1.3.2.3.3.2">𝑦</ci><ci id="S3.E1.m1.4.4.1.1.3.2.3.3.3.cmml" xref="S3.E1.m1.4.4.1.1.3.2.3.3.3">′</ci></apply></apply></apply><ci id="S3.E1.m1.4.4.1.1.3.3.cmml" xref="S3.E1.m1.4.4.1.1.3.3">𝑖</ci></apply><apply id="S3.E1.m1.4.4.1.1.1.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1"><exp id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3"></exp><apply id="S3.E1.m1.4.4.1.1.1.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1"><minus id="S3.E1.m1.4.4.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1"></minus><apply id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2"><divide id="S3.E1.m1.2.2.3.cmml" xref="S3.E1.m1.2.2"></divide><apply id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.2"><plus id="S3.E1.m1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.3"></plus><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1">superscript</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1"><minus id="S3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1"></minus><apply id="S3.E1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2">superscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2">𝑥</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.3">′</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3.2">𝑥</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply><cn type="integer" id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3">2</cn></apply><apply id="S3.E1.m1.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2">superscript</csymbol><apply id="S3.E1.m1.2.2.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1"><minus id="S3.E1.m1.2.2.2.2.1.1.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.1"></minus><apply id="S3.E1.m1.2.2.2.2.1.1.1.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.1.1.1.2.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.2">superscript</csymbol><ci id="S3.E1.m1.2.2.2.2.1.1.1.2.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.2.2">𝑦</ci><ci id="S3.E1.m1.2.2.2.2.1.1.1.2.3.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.2.3">′</ci></apply><apply id="S3.E1.m1.2.2.2.2.1.1.1.3.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.1.1.1.3.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.2.2.2.2.1.1.1.3.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.3.2">𝑦</ci><ci id="S3.E1.m1.2.2.2.2.1.1.1.3.3.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.3.3">𝑖</ci></apply></apply><cn type="integer" id="S3.E1.m1.2.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.2.3">2</cn></apply></apply><apply id="S3.E1.m1.2.2.4.cmml" xref="S3.E1.m1.2.2.4"><times id="S3.E1.m1.2.2.4.1.cmml" xref="S3.E1.m1.2.2.4.1"></times><cn type="integer" id="S3.E1.m1.2.2.4.2.cmml" xref="S3.E1.m1.2.2.4.2">2</cn><apply id="S3.E1.m1.2.2.4.3.cmml" xref="S3.E1.m1.2.2.4.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.4.3.1.cmml" xref="S3.E1.m1.2.2.4.3">superscript</csymbol><ci id="S3.E1.m1.2.2.4.3.2.cmml" xref="S3.E1.m1.2.2.4.3.2">𝜎</ci><cn type="integer" id="S3.E1.m1.2.2.4.3.3.cmml" xref="S3.E1.m1.2.2.4.3.3">2</cn></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.4c">\bm{G}_{x^{\prime}y^{\prime}}^{i}=\exp(-\frac{(x^{\prime}-x_{i})^{2}+(y^{\prime}-y_{i})^{2}}{2\sigma^{2}}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p2.10" class="ltx_p">where <math id="S3.SS1.p2.7.m1.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S3.SS1.p2.7.m1.1a"><mi id="S3.SS1.p2.7.m1.1.1" xref="S3.SS1.p2.7.m1.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m1.1b"><ci id="S3.SS1.p2.7.m1.1.1.cmml" xref="S3.SS1.p2.7.m1.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m1.1c">\sigma</annotation></semantics></math> represents the standard deviation of 2D Gaussian Distribution, and <math id="S3.SS1.p2.8.m2.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S3.SS1.p2.8.m2.1a"><mi id="S3.SS1.p2.8.m2.1.1" xref="S3.SS1.p2.8.m2.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m2.1b"><ci id="S3.SS1.p2.8.m2.1.1.cmml" xref="S3.SS1.p2.8.m2.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m2.1c">\sigma</annotation></semantics></math> should be less than <math id="S3.SS1.p2.9.m3.1" class="ltx_Math" alttext="\frac{224}{6N}" display="inline"><semantics id="S3.SS1.p2.9.m3.1a"><mfrac id="S3.SS1.p2.9.m3.1.1" xref="S3.SS1.p2.9.m3.1.1.cmml"><mn id="S3.SS1.p2.9.m3.1.1.2" xref="S3.SS1.p2.9.m3.1.1.2.cmml">224</mn><mrow id="S3.SS1.p2.9.m3.1.1.3" xref="S3.SS1.p2.9.m3.1.1.3.cmml"><mn id="S3.SS1.p2.9.m3.1.1.3.2" xref="S3.SS1.p2.9.m3.1.1.3.2.cmml">6</mn><mo lspace="0em" rspace="0em" id="S3.SS1.p2.9.m3.1.1.3.1" xref="S3.SS1.p2.9.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS1.p2.9.m3.1.1.3.3" xref="S3.SS1.p2.9.m3.1.1.3.3.cmml">N</mi></mrow></mfrac><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.9.m3.1b"><apply id="S3.SS1.p2.9.m3.1.1.cmml" xref="S3.SS1.p2.9.m3.1.1"><divide id="S3.SS1.p2.9.m3.1.1.1.cmml" xref="S3.SS1.p2.9.m3.1.1"></divide><cn type="integer" id="S3.SS1.p2.9.m3.1.1.2.cmml" xref="S3.SS1.p2.9.m3.1.1.2">224</cn><apply id="S3.SS1.p2.9.m3.1.1.3.cmml" xref="S3.SS1.p2.9.m3.1.1.3"><times id="S3.SS1.p2.9.m3.1.1.3.1.cmml" xref="S3.SS1.p2.9.m3.1.1.3.1"></times><cn type="integer" id="S3.SS1.p2.9.m3.1.1.3.2.cmml" xref="S3.SS1.p2.9.m3.1.1.3.2">6</cn><ci id="S3.SS1.p2.9.m3.1.1.3.3.cmml" xref="S3.SS1.p2.9.m3.1.1.3.3">𝑁</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.9.m3.1c">\frac{224}{6N}</annotation></semantics></math> according to <math id="S3.SS1.p2.10.m4.1" class="ltx_Math" alttext="3\sigma" display="inline"><semantics id="S3.SS1.p2.10.m4.1a"><mrow id="S3.SS1.p2.10.m4.1.1" xref="S3.SS1.p2.10.m4.1.1.cmml"><mn id="S3.SS1.p2.10.m4.1.1.2" xref="S3.SS1.p2.10.m4.1.1.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S3.SS1.p2.10.m4.1.1.1" xref="S3.SS1.p2.10.m4.1.1.1.cmml">​</mo><mi id="S3.SS1.p2.10.m4.1.1.3" xref="S3.SS1.p2.10.m4.1.1.3.cmml">σ</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.10.m4.1b"><apply id="S3.SS1.p2.10.m4.1.1.cmml" xref="S3.SS1.p2.10.m4.1.1"><times id="S3.SS1.p2.10.m4.1.1.1.cmml" xref="S3.SS1.p2.10.m4.1.1.1"></times><cn type="integer" id="S3.SS1.p2.10.m4.1.1.2.cmml" xref="S3.SS1.p2.10.m4.1.1.2">3</cn><ci id="S3.SS1.p2.10.m4.1.1.3.cmml" xref="S3.SS1.p2.10.m4.1.1.3">𝜎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.10.m4.1c">3\sigma</annotation></semantics></math> rule to avoid mislocation.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.2" class="ltx_p">The network learns to predict 2D Gaussian heatmaps to obtain positions of shuffled image patches during the pretraining. We regress target heatmap by the same network as downstream task, except that the number of output channel is equal to <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="N\times N" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><mrow id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml"><mi id="S3.SS1.p3.1.m1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p3.1.m1.1.1.1" xref="S3.SS1.p3.1.m1.1.1.1.cmml">×</mo><mi id="S3.SS1.p3.1.m1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1"><times id="S3.SS1.p3.1.m1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1.1"></times><ci id="S3.SS1.p3.1.m1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.2">𝑁</ci><ci id="S3.SS1.p3.1.m1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">N\times N</annotation></semantics></math>: the number of image patches rather than the number of human keypoints. Our best <math id="S3.SS1.p3.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><mi id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><ci id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">N</annotation></semantics></math> for downstream performance is 6, which will be tested by ablation studies in Section <a href="#S5.SS2" title="5.2 How many pieces of image patches? ‣ 5 Ablation Studies ‣ Learning Heatmap-Style Jigsaw Puzzles Provides Good Pretraining for 2D Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Learning rich representations</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">There is obviously a challenging problem for our method proposed in Section <a href="#S3.SS1" title="3.1 Target description for HSJP pretext task ‣ 3 Heatmap-Style Jigsaw Puzzles ‣ Learning Heatmap-Style Jigsaw Puzzles Provides Good Pretraining for 2D Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>: the centres of target 2D Gaussian Distributions are always located on centres of image grids, such as <math id="S3.SS2.p1.1.m1.9" class="ltx_Math" alttext="(\frac{(2m+1)\cdot 224}{2N},\frac{(2n+1)\cdot 224}{2N}),m\in[0,N),n\in[0,N)" display="inline"><semantics id="S3.SS2.p1.1.m1.9a"><mrow id="S3.SS2.p1.1.m1.9.9.2" xref="S3.SS2.p1.1.m1.9.9.3.cmml"><mrow id="S3.SS2.p1.1.m1.8.8.1.1" xref="S3.SS2.p1.1.m1.8.8.1.1.cmml"><mrow id="S3.SS2.p1.1.m1.8.8.1.1.1.1" xref="S3.SS2.p1.1.m1.8.8.1.1.1.2.cmml"><mrow id="S3.SS2.p1.1.m1.8.8.1.1.1.1.1.2" xref="S3.SS2.p1.1.m1.8.8.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p1.1.m1.8.8.1.1.1.1.1.2.1" xref="S3.SS2.p1.1.m1.8.8.1.1.1.1.1.1.cmml">(</mo><mfrac id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mrow id="S3.SS2.p1.1.m1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.cmml"><mrow id="S3.SS2.p1.1.m1.1.1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p1.1.m1.1.1.1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p1.1.m1.1.1.1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.1.1.1.cmml"><mrow id="S3.SS2.p1.1.m1.1.1.1.1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.1.1.1.1.2.cmml"><mn id="S3.SS2.p1.1.m1.1.1.1.1.1.1.2.2" xref="S3.SS2.p1.1.m1.1.1.1.1.1.1.2.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.SS2.p1.1.m1.1.1.1.1.1.1.2.1" xref="S3.SS2.p1.1.m1.1.1.1.1.1.1.2.1.cmml">​</mo><mi id="S3.SS2.p1.1.m1.1.1.1.1.1.1.2.3" xref="S3.SS2.p1.1.m1.1.1.1.1.1.1.2.3.cmml">m</mi></mrow><mo id="S3.SS2.p1.1.m1.1.1.1.1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.1.1.1.1.cmml">+</mo><mn id="S3.SS2.p1.1.m1.1.1.1.1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.1.1.1.1.3.cmml">1</mn></mrow><mo rspace="0.055em" stretchy="false" id="S3.SS2.p1.1.m1.1.1.1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.1.1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S3.SS2.p1.1.m1.1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.1.2.cmml">⋅</mo><mn id="S3.SS2.p1.1.m1.1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.1.3.cmml">224</mn></mrow><mrow id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml"><mn id="S3.SS2.p1.1.m1.1.1.3.2" xref="S3.SS2.p1.1.m1.1.1.3.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.SS2.p1.1.m1.1.1.3.1" xref="S3.SS2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p1.1.m1.1.1.3.3" xref="S3.SS2.p1.1.m1.1.1.3.3.cmml">N</mi></mrow></mfrac><mo id="S3.SS2.p1.1.m1.8.8.1.1.1.1.1.2.2" xref="S3.SS2.p1.1.m1.8.8.1.1.1.1.1.1.cmml">,</mo><mfrac id="S3.SS2.p1.1.m1.2.2" xref="S3.SS2.p1.1.m1.2.2.cmml"><mrow id="S3.SS2.p1.1.m1.2.2.1" xref="S3.SS2.p1.1.m1.2.2.1.cmml"><mrow id="S3.SS2.p1.1.m1.2.2.1.1.1" xref="S3.SS2.p1.1.m1.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p1.1.m1.2.2.1.1.1.2" xref="S3.SS2.p1.1.m1.2.2.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p1.1.m1.2.2.1.1.1.1" xref="S3.SS2.p1.1.m1.2.2.1.1.1.1.cmml"><mrow id="S3.SS2.p1.1.m1.2.2.1.1.1.1.2" xref="S3.SS2.p1.1.m1.2.2.1.1.1.1.2.cmml"><mn id="S3.SS2.p1.1.m1.2.2.1.1.1.1.2.2" xref="S3.SS2.p1.1.m1.2.2.1.1.1.1.2.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.SS2.p1.1.m1.2.2.1.1.1.1.2.1" xref="S3.SS2.p1.1.m1.2.2.1.1.1.1.2.1.cmml">​</mo><mi id="S3.SS2.p1.1.m1.2.2.1.1.1.1.2.3" xref="S3.SS2.p1.1.m1.2.2.1.1.1.1.2.3.cmml">n</mi></mrow><mo id="S3.SS2.p1.1.m1.2.2.1.1.1.1.1" xref="S3.SS2.p1.1.m1.2.2.1.1.1.1.1.cmml">+</mo><mn id="S3.SS2.p1.1.m1.2.2.1.1.1.1.3" xref="S3.SS2.p1.1.m1.2.2.1.1.1.1.3.cmml">1</mn></mrow><mo rspace="0.055em" stretchy="false" id="S3.SS2.p1.1.m1.2.2.1.1.1.3" xref="S3.SS2.p1.1.m1.2.2.1.1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S3.SS2.p1.1.m1.2.2.1.2" xref="S3.SS2.p1.1.m1.2.2.1.2.cmml">⋅</mo><mn id="S3.SS2.p1.1.m1.2.2.1.3" xref="S3.SS2.p1.1.m1.2.2.1.3.cmml">224</mn></mrow><mrow id="S3.SS2.p1.1.m1.2.2.3" xref="S3.SS2.p1.1.m1.2.2.3.cmml"><mn id="S3.SS2.p1.1.m1.2.2.3.2" xref="S3.SS2.p1.1.m1.2.2.3.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.SS2.p1.1.m1.2.2.3.1" xref="S3.SS2.p1.1.m1.2.2.3.1.cmml">​</mo><mi id="S3.SS2.p1.1.m1.2.2.3.3" xref="S3.SS2.p1.1.m1.2.2.3.3.cmml">N</mi></mrow></mfrac><mo stretchy="false" id="S3.SS2.p1.1.m1.8.8.1.1.1.1.1.2.3" xref="S3.SS2.p1.1.m1.8.8.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.SS2.p1.1.m1.8.8.1.1.1.1.2" xref="S3.SS2.p1.1.m1.8.8.1.1.1.2.cmml">,</mo><mi id="S3.SS2.p1.1.m1.5.5" xref="S3.SS2.p1.1.m1.5.5.cmml">m</mi></mrow><mo id="S3.SS2.p1.1.m1.8.8.1.1.2" xref="S3.SS2.p1.1.m1.8.8.1.1.2.cmml">∈</mo><mrow id="S3.SS2.p1.1.m1.8.8.1.1.3.2" xref="S3.SS2.p1.1.m1.8.8.1.1.3.1.cmml"><mo stretchy="false" id="S3.SS2.p1.1.m1.8.8.1.1.3.2.1" xref="S3.SS2.p1.1.m1.8.8.1.1.3.1.cmml">[</mo><mn id="S3.SS2.p1.1.m1.3.3" xref="S3.SS2.p1.1.m1.3.3.cmml">0</mn><mo id="S3.SS2.p1.1.m1.8.8.1.1.3.2.2" xref="S3.SS2.p1.1.m1.8.8.1.1.3.1.cmml">,</mo><mi id="S3.SS2.p1.1.m1.4.4" xref="S3.SS2.p1.1.m1.4.4.cmml">N</mi><mo stretchy="false" id="S3.SS2.p1.1.m1.8.8.1.1.3.2.3" xref="S3.SS2.p1.1.m1.8.8.1.1.3.1.cmml">)</mo></mrow></mrow><mo id="S3.SS2.p1.1.m1.9.9.2.3" xref="S3.SS2.p1.1.m1.9.9.3a.cmml">,</mo><mrow id="S3.SS2.p1.1.m1.9.9.2.2" xref="S3.SS2.p1.1.m1.9.9.2.2.cmml"><mi id="S3.SS2.p1.1.m1.9.9.2.2.2" xref="S3.SS2.p1.1.m1.9.9.2.2.2.cmml">n</mi><mo id="S3.SS2.p1.1.m1.9.9.2.2.1" xref="S3.SS2.p1.1.m1.9.9.2.2.1.cmml">∈</mo><mrow id="S3.SS2.p1.1.m1.9.9.2.2.3.2" xref="S3.SS2.p1.1.m1.9.9.2.2.3.1.cmml"><mo stretchy="false" id="S3.SS2.p1.1.m1.9.9.2.2.3.2.1" xref="S3.SS2.p1.1.m1.9.9.2.2.3.1.cmml">[</mo><mn id="S3.SS2.p1.1.m1.6.6" xref="S3.SS2.p1.1.m1.6.6.cmml">0</mn><mo id="S3.SS2.p1.1.m1.9.9.2.2.3.2.2" xref="S3.SS2.p1.1.m1.9.9.2.2.3.1.cmml">,</mo><mi id="S3.SS2.p1.1.m1.7.7" xref="S3.SS2.p1.1.m1.7.7.cmml">N</mi><mo stretchy="false" id="S3.SS2.p1.1.m1.9.9.2.2.3.2.3" xref="S3.SS2.p1.1.m1.9.9.2.2.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.9b"><apply id="S3.SS2.p1.1.m1.9.9.3.cmml" xref="S3.SS2.p1.1.m1.9.9.2"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.9.9.3a.cmml" xref="S3.SS2.p1.1.m1.9.9.2.3">formulae-sequence</csymbol><apply id="S3.SS2.p1.1.m1.8.8.1.1.cmml" xref="S3.SS2.p1.1.m1.8.8.1.1"><in id="S3.SS2.p1.1.m1.8.8.1.1.2.cmml" xref="S3.SS2.p1.1.m1.8.8.1.1.2"></in><list id="S3.SS2.p1.1.m1.8.8.1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.8.8.1.1.1.1"><interval closure="open" id="S3.SS2.p1.1.m1.8.8.1.1.1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.8.8.1.1.1.1.1.2"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><divide id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1"></divide><apply id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1"><ci id="S3.SS2.p1.1.m1.1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.1.2">⋅</ci><apply id="S3.SS2.p1.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1.1.1"><plus id="S3.SS2.p1.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1.1.1.1.1"></plus><apply id="S3.SS2.p1.1.m1.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.1.1.1.1.2"><times id="S3.SS2.p1.1.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1.1.1.1.2.1"></times><cn type="integer" id="S3.SS2.p1.1.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.SS2.p1.1.m1.1.1.1.1.1.1.2.2">2</cn><ci id="S3.SS2.p1.1.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.SS2.p1.1.m1.1.1.1.1.1.1.2.3">𝑚</ci></apply><cn type="integer" id="S3.SS2.p1.1.m1.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.1.1.1.1.3">1</cn></apply><cn type="integer" id="S3.SS2.p1.1.m1.1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.1.3">224</cn></apply><apply id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3"><times id="S3.SS2.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.p1.1.m1.1.1.3.1"></times><cn type="integer" id="S3.SS2.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.p1.1.m1.1.1.3.2">2</cn><ci id="S3.SS2.p1.1.m1.1.1.3.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3.3">𝑁</ci></apply></apply><apply id="S3.SS2.p1.1.m1.2.2.cmml" xref="S3.SS2.p1.1.m1.2.2"><divide id="S3.SS2.p1.1.m1.2.2.2.cmml" xref="S3.SS2.p1.1.m1.2.2"></divide><apply id="S3.SS2.p1.1.m1.2.2.1.cmml" xref="S3.SS2.p1.1.m1.2.2.1"><ci id="S3.SS2.p1.1.m1.2.2.1.2.cmml" xref="S3.SS2.p1.1.m1.2.2.1.2">⋅</ci><apply id="S3.SS2.p1.1.m1.2.2.1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.2.2.1.1.1"><plus id="S3.SS2.p1.1.m1.2.2.1.1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.2.2.1.1.1.1.1"></plus><apply id="S3.SS2.p1.1.m1.2.2.1.1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.2.2.1.1.1.1.2"><times id="S3.SS2.p1.1.m1.2.2.1.1.1.1.2.1.cmml" xref="S3.SS2.p1.1.m1.2.2.1.1.1.1.2.1"></times><cn type="integer" id="S3.SS2.p1.1.m1.2.2.1.1.1.1.2.2.cmml" xref="S3.SS2.p1.1.m1.2.2.1.1.1.1.2.2">2</cn><ci id="S3.SS2.p1.1.m1.2.2.1.1.1.1.2.3.cmml" xref="S3.SS2.p1.1.m1.2.2.1.1.1.1.2.3">𝑛</ci></apply><cn type="integer" id="S3.SS2.p1.1.m1.2.2.1.1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.2.2.1.1.1.1.3">1</cn></apply><cn type="integer" id="S3.SS2.p1.1.m1.2.2.1.3.cmml" xref="S3.SS2.p1.1.m1.2.2.1.3">224</cn></apply><apply id="S3.SS2.p1.1.m1.2.2.3.cmml" xref="S3.SS2.p1.1.m1.2.2.3"><times id="S3.SS2.p1.1.m1.2.2.3.1.cmml" xref="S3.SS2.p1.1.m1.2.2.3.1"></times><cn type="integer" id="S3.SS2.p1.1.m1.2.2.3.2.cmml" xref="S3.SS2.p1.1.m1.2.2.3.2">2</cn><ci id="S3.SS2.p1.1.m1.2.2.3.3.cmml" xref="S3.SS2.p1.1.m1.2.2.3.3">𝑁</ci></apply></apply></interval><ci id="S3.SS2.p1.1.m1.5.5.cmml" xref="S3.SS2.p1.1.m1.5.5">𝑚</ci></list><interval closure="closed-open" id="S3.SS2.p1.1.m1.8.8.1.1.3.1.cmml" xref="S3.SS2.p1.1.m1.8.8.1.1.3.2"><cn type="integer" id="S3.SS2.p1.1.m1.3.3.cmml" xref="S3.SS2.p1.1.m1.3.3">0</cn><ci id="S3.SS2.p1.1.m1.4.4.cmml" xref="S3.SS2.p1.1.m1.4.4">𝑁</ci></interval></apply><apply id="S3.SS2.p1.1.m1.9.9.2.2.cmml" xref="S3.SS2.p1.1.m1.9.9.2.2"><in id="S3.SS2.p1.1.m1.9.9.2.2.1.cmml" xref="S3.SS2.p1.1.m1.9.9.2.2.1"></in><ci id="S3.SS2.p1.1.m1.9.9.2.2.2.cmml" xref="S3.SS2.p1.1.m1.9.9.2.2.2">𝑛</ci><interval closure="closed-open" id="S3.SS2.p1.1.m1.9.9.2.2.3.1.cmml" xref="S3.SS2.p1.1.m1.9.9.2.2.3.2"><cn type="integer" id="S3.SS2.p1.1.m1.6.6.cmml" xref="S3.SS2.p1.1.m1.6.6">0</cn><ci id="S3.SS2.p1.1.m1.7.7.cmml" xref="S3.SS2.p1.1.m1.7.7">𝑁</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.9c">(\frac{(2m+1)\cdot 224}{2N},\frac{(2n+1)\cdot 224}{2N}),m\in[0,N),n\in[0,N)</annotation></semantics></math>, so the network is easy to fix their responses on centres of grids (as is shown in the first heatmap of Figure <a href="#S3.F2" title="Figure 2 ‣ 3.2 Learning rich representations ‣ 3 Heatmap-Style Jigsaw Puzzles ‣ Learning Heatmap-Style Jigsaw Puzzles Provides Good Pretraining for 2D Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) rather than having responses anywhere. In this case, it’s hard for the network to learn rich representations for downstream tasks, because the learnt responses are highly related to output positions (whether they’re grid centres), instead of the contents of image patches.</p>
</div>
<figure id="S3.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S3.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2012.07101/assets/sfq.jpg" id="S3.F2.sf1.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf1.2.1.1" class="ltx_text" style="font-size:80%;">(a)</span> </span><span id="S3.F2.sf1.3.2" class="ltx_text" style="font-size:80%;">Image Patches</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S3.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2012.07101/assets/translated.jpg" id="S3.F2.sf2.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf2.2.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span><span id="S3.F2.sf2.3.2" class="ltx_text" style="font-size:80%;">Translated</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S3.F2.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2012.07101/assets/rotated.jpg" id="S3.F2.sf3.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf3.2.1.1" class="ltx_text" style="font-size:80%;">(c)</span> </span><span id="S3.F2.sf3.3.2" class="ltx_text" style="font-size:80%;">Rotated</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S3.F2.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2012.07101/assets/rescaled.jpg" id="S3.F2.sf4.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf4.2.1.1" class="ltx_text" style="font-size:80%;">(d)</span> </span><span id="S3.F2.sf4.3.2" class="ltx_text" style="font-size:80%;">Rescaled</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S3.F2.sf5" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2012.07101/assets/sumheatmap.jpg" id="S3.F2.sf5.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf5.2.1.1" class="ltx_text" style="font-size:80%;">(e)</span> </span><span id="S3.F2.sf5.3.2" class="ltx_text" style="font-size:80%;">Target Heatmap</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S3.F2.sf6" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2012.07101/assets/translatedtarget.jpg" id="S3.F2.sf6.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf6.2.1.1" class="ltx_text" style="font-size:80%;">(f)</span> </span><span id="S3.F2.sf6.3.2" class="ltx_text" style="font-size:80%;">Translated</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S3.F2.sf7" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2012.07101/assets/rotatedtarget.jpg" id="S3.F2.sf7.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf7.2.1.1" class="ltx_text" style="font-size:80%;">(g)</span> </span><span id="S3.F2.sf7.3.2" class="ltx_text" style="font-size:80%;">Rotated</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S3.F2.sf8" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2012.07101/assets/rescaledtarget.jpg" id="S3.F2.sf8.g1" class="ltx_graphics ltx_img_square" width="63" height="63" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf8.2.1.1" class="ltx_text" style="font-size:80%;">(h)</span> </span><span id="S3.F2.sf8.3.2" class="ltx_text" style="font-size:80%;">Rescaled</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Spatial data augmentations and possible heatmaps for all patches. In the first heatmap, without spatial data augmentations, the Gaussian Distributions are always on the centres of image grids, such as <math id="S3.F2.2.m1.2" class="ltx_Math" alttext="(\frac{(2m+1)\cdot 224}{2N},\frac{(2n+1)\cdot 224}{2N})" display="inline"><semantics id="S3.F2.2.m1.2b"><mrow id="S3.F2.2.m1.2.3.2" xref="S3.F2.2.m1.2.3.1.cmml"><mo stretchy="false" id="S3.F2.2.m1.2.3.2.1" xref="S3.F2.2.m1.2.3.1.cmml">(</mo><mfrac id="S3.F2.2.m1.1.1" xref="S3.F2.2.m1.1.1.cmml"><mrow id="S3.F2.2.m1.1.1.1" xref="S3.F2.2.m1.1.1.1.cmml"><mrow id="S3.F2.2.m1.1.1.1.1.1" xref="S3.F2.2.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.F2.2.m1.1.1.1.1.1.2" xref="S3.F2.2.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.F2.2.m1.1.1.1.1.1.1" xref="S3.F2.2.m1.1.1.1.1.1.1.cmml"><mrow id="S3.F2.2.m1.1.1.1.1.1.1.2" xref="S3.F2.2.m1.1.1.1.1.1.1.2.cmml"><mn id="S3.F2.2.m1.1.1.1.1.1.1.2.2" xref="S3.F2.2.m1.1.1.1.1.1.1.2.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.F2.2.m1.1.1.1.1.1.1.2.1" xref="S3.F2.2.m1.1.1.1.1.1.1.2.1.cmml">​</mo><mi id="S3.F2.2.m1.1.1.1.1.1.1.2.3" xref="S3.F2.2.m1.1.1.1.1.1.1.2.3.cmml">m</mi></mrow><mo id="S3.F2.2.m1.1.1.1.1.1.1.1" xref="S3.F2.2.m1.1.1.1.1.1.1.1.cmml">+</mo><mn id="S3.F2.2.m1.1.1.1.1.1.1.3" xref="S3.F2.2.m1.1.1.1.1.1.1.3.cmml">1</mn></mrow><mo rspace="0.055em" stretchy="false" id="S3.F2.2.m1.1.1.1.1.1.3" xref="S3.F2.2.m1.1.1.1.1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S3.F2.2.m1.1.1.1.2" xref="S3.F2.2.m1.1.1.1.2.cmml">⋅</mo><mn id="S3.F2.2.m1.1.1.1.3" xref="S3.F2.2.m1.1.1.1.3.cmml">224</mn></mrow><mrow id="S3.F2.2.m1.1.1.3" xref="S3.F2.2.m1.1.1.3.cmml"><mn id="S3.F2.2.m1.1.1.3.2" xref="S3.F2.2.m1.1.1.3.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.F2.2.m1.1.1.3.1" xref="S3.F2.2.m1.1.1.3.1.cmml">​</mo><mi id="S3.F2.2.m1.1.1.3.3" xref="S3.F2.2.m1.1.1.3.3.cmml">N</mi></mrow></mfrac><mo id="S3.F2.2.m1.2.3.2.2" xref="S3.F2.2.m1.2.3.1.cmml">,</mo><mfrac id="S3.F2.2.m1.2.2" xref="S3.F2.2.m1.2.2.cmml"><mrow id="S3.F2.2.m1.2.2.1" xref="S3.F2.2.m1.2.2.1.cmml"><mrow id="S3.F2.2.m1.2.2.1.1.1" xref="S3.F2.2.m1.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S3.F2.2.m1.2.2.1.1.1.2" xref="S3.F2.2.m1.2.2.1.1.1.1.cmml">(</mo><mrow id="S3.F2.2.m1.2.2.1.1.1.1" xref="S3.F2.2.m1.2.2.1.1.1.1.cmml"><mrow id="S3.F2.2.m1.2.2.1.1.1.1.2" xref="S3.F2.2.m1.2.2.1.1.1.1.2.cmml"><mn id="S3.F2.2.m1.2.2.1.1.1.1.2.2" xref="S3.F2.2.m1.2.2.1.1.1.1.2.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.F2.2.m1.2.2.1.1.1.1.2.1" xref="S3.F2.2.m1.2.2.1.1.1.1.2.1.cmml">​</mo><mi id="S3.F2.2.m1.2.2.1.1.1.1.2.3" xref="S3.F2.2.m1.2.2.1.1.1.1.2.3.cmml">n</mi></mrow><mo id="S3.F2.2.m1.2.2.1.1.1.1.1" xref="S3.F2.2.m1.2.2.1.1.1.1.1.cmml">+</mo><mn id="S3.F2.2.m1.2.2.1.1.1.1.3" xref="S3.F2.2.m1.2.2.1.1.1.1.3.cmml">1</mn></mrow><mo rspace="0.055em" stretchy="false" id="S3.F2.2.m1.2.2.1.1.1.3" xref="S3.F2.2.m1.2.2.1.1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S3.F2.2.m1.2.2.1.2" xref="S3.F2.2.m1.2.2.1.2.cmml">⋅</mo><mn id="S3.F2.2.m1.2.2.1.3" xref="S3.F2.2.m1.2.2.1.3.cmml">224</mn></mrow><mrow id="S3.F2.2.m1.2.2.3" xref="S3.F2.2.m1.2.2.3.cmml"><mn id="S3.F2.2.m1.2.2.3.2" xref="S3.F2.2.m1.2.2.3.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.F2.2.m1.2.2.3.1" xref="S3.F2.2.m1.2.2.3.1.cmml">​</mo><mi id="S3.F2.2.m1.2.2.3.3" xref="S3.F2.2.m1.2.2.3.3.cmml">N</mi></mrow></mfrac><mo stretchy="false" id="S3.F2.2.m1.2.3.2.3" xref="S3.F2.2.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.F2.2.m1.2c"><interval closure="open" id="S3.F2.2.m1.2.3.1.cmml" xref="S3.F2.2.m1.2.3.2"><apply id="S3.F2.2.m1.1.1.cmml" xref="S3.F2.2.m1.1.1"><divide id="S3.F2.2.m1.1.1.2.cmml" xref="S3.F2.2.m1.1.1"></divide><apply id="S3.F2.2.m1.1.1.1.cmml" xref="S3.F2.2.m1.1.1.1"><ci id="S3.F2.2.m1.1.1.1.2.cmml" xref="S3.F2.2.m1.1.1.1.2">⋅</ci><apply id="S3.F2.2.m1.1.1.1.1.1.1.cmml" xref="S3.F2.2.m1.1.1.1.1.1"><plus id="S3.F2.2.m1.1.1.1.1.1.1.1.cmml" xref="S3.F2.2.m1.1.1.1.1.1.1.1"></plus><apply id="S3.F2.2.m1.1.1.1.1.1.1.2.cmml" xref="S3.F2.2.m1.1.1.1.1.1.1.2"><times id="S3.F2.2.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.F2.2.m1.1.1.1.1.1.1.2.1"></times><cn type="integer" id="S3.F2.2.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.F2.2.m1.1.1.1.1.1.1.2.2">2</cn><ci id="S3.F2.2.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.F2.2.m1.1.1.1.1.1.1.2.3">𝑚</ci></apply><cn type="integer" id="S3.F2.2.m1.1.1.1.1.1.1.3.cmml" xref="S3.F2.2.m1.1.1.1.1.1.1.3">1</cn></apply><cn type="integer" id="S3.F2.2.m1.1.1.1.3.cmml" xref="S3.F2.2.m1.1.1.1.3">224</cn></apply><apply id="S3.F2.2.m1.1.1.3.cmml" xref="S3.F2.2.m1.1.1.3"><times id="S3.F2.2.m1.1.1.3.1.cmml" xref="S3.F2.2.m1.1.1.3.1"></times><cn type="integer" id="S3.F2.2.m1.1.1.3.2.cmml" xref="S3.F2.2.m1.1.1.3.2">2</cn><ci id="S3.F2.2.m1.1.1.3.3.cmml" xref="S3.F2.2.m1.1.1.3.3">𝑁</ci></apply></apply><apply id="S3.F2.2.m1.2.2.cmml" xref="S3.F2.2.m1.2.2"><divide id="S3.F2.2.m1.2.2.2.cmml" xref="S3.F2.2.m1.2.2"></divide><apply id="S3.F2.2.m1.2.2.1.cmml" xref="S3.F2.2.m1.2.2.1"><ci id="S3.F2.2.m1.2.2.1.2.cmml" xref="S3.F2.2.m1.2.2.1.2">⋅</ci><apply id="S3.F2.2.m1.2.2.1.1.1.1.cmml" xref="S3.F2.2.m1.2.2.1.1.1"><plus id="S3.F2.2.m1.2.2.1.1.1.1.1.cmml" xref="S3.F2.2.m1.2.2.1.1.1.1.1"></plus><apply id="S3.F2.2.m1.2.2.1.1.1.1.2.cmml" xref="S3.F2.2.m1.2.2.1.1.1.1.2"><times id="S3.F2.2.m1.2.2.1.1.1.1.2.1.cmml" xref="S3.F2.2.m1.2.2.1.1.1.1.2.1"></times><cn type="integer" id="S3.F2.2.m1.2.2.1.1.1.1.2.2.cmml" xref="S3.F2.2.m1.2.2.1.1.1.1.2.2">2</cn><ci id="S3.F2.2.m1.2.2.1.1.1.1.2.3.cmml" xref="S3.F2.2.m1.2.2.1.1.1.1.2.3">𝑛</ci></apply><cn type="integer" id="S3.F2.2.m1.2.2.1.1.1.1.3.cmml" xref="S3.F2.2.m1.2.2.1.1.1.1.3">1</cn></apply><cn type="integer" id="S3.F2.2.m1.2.2.1.3.cmml" xref="S3.F2.2.m1.2.2.1.3">224</cn></apply><apply id="S3.F2.2.m1.2.2.3.cmml" xref="S3.F2.2.m1.2.2.3"><times id="S3.F2.2.m1.2.2.3.1.cmml" xref="S3.F2.2.m1.2.2.3.1"></times><cn type="integer" id="S3.F2.2.m1.2.2.3.2.cmml" xref="S3.F2.2.m1.2.2.3.2">2</cn><ci id="S3.F2.2.m1.2.2.3.3.cmml" xref="S3.F2.2.m1.2.2.3.3">𝑁</ci></apply></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.2.m1.2d">(\frac{(2m+1)\cdot 224}{2N},\frac{(2n+1)\cdot 224}{2N})</annotation></semantics></math>. In the following three heatmaps, after rotating, rescaling, and translating augmentations in the training stage, the learnt responses are able to appear almost everywhere.</figcaption>
</figure>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.6" class="ltx_p">To tackle with this challenge, we introduce scaling <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="([-35\%,35\%])" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1.1.1"><mo stretchy="false" id="S3.SS2.p2.1.m1.1.1.1.2">(</mo><mrow id="S3.SS2.p2.1.m1.1.1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.1.1.3.cmml"><mo stretchy="false" id="S3.SS2.p2.1.m1.1.1.1.1.2.3" xref="S3.SS2.p2.1.m1.1.1.1.1.3.cmml">[</mo><mrow id="S3.SS2.p2.1.m1.1.1.1.1.1.1" xref="S3.SS2.p2.1.m1.1.1.1.1.1.1.cmml"><mo id="S3.SS2.p2.1.m1.1.1.1.1.1.1a" xref="S3.SS2.p2.1.m1.1.1.1.1.1.1.cmml">−</mo><mrow id="S3.SS2.p2.1.m1.1.1.1.1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.1.1.1.1.2.cmml"><mn id="S3.SS2.p2.1.m1.1.1.1.1.1.1.2.2" xref="S3.SS2.p2.1.m1.1.1.1.1.1.1.2.2.cmml">35</mn><mo id="S3.SS2.p2.1.m1.1.1.1.1.1.1.2.1" xref="S3.SS2.p2.1.m1.1.1.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo id="S3.SS2.p2.1.m1.1.1.1.1.2.4" xref="S3.SS2.p2.1.m1.1.1.1.1.3.cmml">,</mo><mrow id="S3.SS2.p2.1.m1.1.1.1.1.2.2" xref="S3.SS2.p2.1.m1.1.1.1.1.2.2.cmml"><mn id="S3.SS2.p2.1.m1.1.1.1.1.2.2.2" xref="S3.SS2.p2.1.m1.1.1.1.1.2.2.2.cmml">35</mn><mo id="S3.SS2.p2.1.m1.1.1.1.1.2.2.1" xref="S3.SS2.p2.1.m1.1.1.1.1.2.2.1.cmml">%</mo></mrow><mo stretchy="false" id="S3.SS2.p2.1.m1.1.1.1.1.2.5" xref="S3.SS2.p2.1.m1.1.1.1.1.3.cmml">]</mo></mrow><mo stretchy="false" id="S3.SS2.p2.1.m1.1.1.1.3">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><interval closure="closed" id="S3.SS2.p2.1.m1.1.1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.1.1.2"><apply id="S3.SS2.p2.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1.1.1.1"><minus id="S3.SS2.p2.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1.1.1.1"></minus><apply id="S3.SS2.p2.1.m1.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S3.SS2.p2.1.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1.1.1.1.2.1">percent</csymbol><cn type="integer" id="S3.SS2.p2.1.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.SS2.p2.1.m1.1.1.1.1.1.1.2.2">35</cn></apply></apply><apply id="S3.SS2.p2.1.m1.1.1.1.1.2.2.cmml" xref="S3.SS2.p2.1.m1.1.1.1.1.2.2"><csymbol cd="latexml" id="S3.SS2.p2.1.m1.1.1.1.1.2.2.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1.1.2.2.1">percent</csymbol><cn type="integer" id="S3.SS2.p2.1.m1.1.1.1.1.2.2.2.cmml" xref="S3.SS2.p2.1.m1.1.1.1.1.2.2.2">35</cn></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">([-35\%,35\%])</annotation></semantics></math>, translating <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="([-10\%,10\%])" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mrow id="S3.SS2.p2.2.m2.1.1.1"><mo stretchy="false" id="S3.SS2.p2.2.m2.1.1.1.2">(</mo><mrow id="S3.SS2.p2.2.m2.1.1.1.1.2" xref="S3.SS2.p2.2.m2.1.1.1.1.3.cmml"><mo stretchy="false" id="S3.SS2.p2.2.m2.1.1.1.1.2.3" xref="S3.SS2.p2.2.m2.1.1.1.1.3.cmml">[</mo><mrow id="S3.SS2.p2.2.m2.1.1.1.1.1.1" xref="S3.SS2.p2.2.m2.1.1.1.1.1.1.cmml"><mo id="S3.SS2.p2.2.m2.1.1.1.1.1.1a" xref="S3.SS2.p2.2.m2.1.1.1.1.1.1.cmml">−</mo><mrow id="S3.SS2.p2.2.m2.1.1.1.1.1.1.2" xref="S3.SS2.p2.2.m2.1.1.1.1.1.1.2.cmml"><mn id="S3.SS2.p2.2.m2.1.1.1.1.1.1.2.2" xref="S3.SS2.p2.2.m2.1.1.1.1.1.1.2.2.cmml">10</mn><mo id="S3.SS2.p2.2.m2.1.1.1.1.1.1.2.1" xref="S3.SS2.p2.2.m2.1.1.1.1.1.1.2.1.cmml">%</mo></mrow></mrow><mo id="S3.SS2.p2.2.m2.1.1.1.1.2.4" xref="S3.SS2.p2.2.m2.1.1.1.1.3.cmml">,</mo><mrow id="S3.SS2.p2.2.m2.1.1.1.1.2.2" xref="S3.SS2.p2.2.m2.1.1.1.1.2.2.cmml"><mn id="S3.SS2.p2.2.m2.1.1.1.1.2.2.2" xref="S3.SS2.p2.2.m2.1.1.1.1.2.2.2.cmml">10</mn><mo id="S3.SS2.p2.2.m2.1.1.1.1.2.2.1" xref="S3.SS2.p2.2.m2.1.1.1.1.2.2.1.cmml">%</mo></mrow><mo stretchy="false" id="S3.SS2.p2.2.m2.1.1.1.1.2.5" xref="S3.SS2.p2.2.m2.1.1.1.1.3.cmml">]</mo></mrow><mo stretchy="false" id="S3.SS2.p2.2.m2.1.1.1.3">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><interval closure="closed" id="S3.SS2.p2.2.m2.1.1.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.2"><apply id="S3.SS2.p2.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.1.1"><minus id="S3.SS2.p2.2.m2.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.1.1"></minus><apply id="S3.SS2.p2.2.m2.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S3.SS2.p2.2.m2.1.1.1.1.1.1.2.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.1.1.2.1">percent</csymbol><cn type="integer" id="S3.SS2.p2.2.m2.1.1.1.1.1.1.2.2.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.1.1.2.2">10</cn></apply></apply><apply id="S3.SS2.p2.2.m2.1.1.1.1.2.2.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.2.2"><csymbol cd="latexml" id="S3.SS2.p2.2.m2.1.1.1.1.2.2.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.2.2.1">percent</csymbol><cn type="integer" id="S3.SS2.p2.2.m2.1.1.1.1.2.2.2.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.2.2.2">10</cn></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">([-10\%,10\%])</annotation></semantics></math>, and rotating <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="([-45^{\circ},45^{\circ}])" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><mrow id="S3.SS2.p2.3.m3.1.1.1"><mo stretchy="false" id="S3.SS2.p2.3.m3.1.1.1.2">(</mo><mrow id="S3.SS2.p2.3.m3.1.1.1.1.2" xref="S3.SS2.p2.3.m3.1.1.1.1.3.cmml"><mo stretchy="false" id="S3.SS2.p2.3.m3.1.1.1.1.2.3" xref="S3.SS2.p2.3.m3.1.1.1.1.3.cmml">[</mo><mrow id="S3.SS2.p2.3.m3.1.1.1.1.1.1" xref="S3.SS2.p2.3.m3.1.1.1.1.1.1.cmml"><mo id="S3.SS2.p2.3.m3.1.1.1.1.1.1a" xref="S3.SS2.p2.3.m3.1.1.1.1.1.1.cmml">−</mo><msup id="S3.SS2.p2.3.m3.1.1.1.1.1.1.2" xref="S3.SS2.p2.3.m3.1.1.1.1.1.1.2.cmml"><mn id="S3.SS2.p2.3.m3.1.1.1.1.1.1.2.2" xref="S3.SS2.p2.3.m3.1.1.1.1.1.1.2.2.cmml">45</mn><mo id="S3.SS2.p2.3.m3.1.1.1.1.1.1.2.3" xref="S3.SS2.p2.3.m3.1.1.1.1.1.1.2.3.cmml">∘</mo></msup></mrow><mo id="S3.SS2.p2.3.m3.1.1.1.1.2.4" xref="S3.SS2.p2.3.m3.1.1.1.1.3.cmml">,</mo><msup id="S3.SS2.p2.3.m3.1.1.1.1.2.2" xref="S3.SS2.p2.3.m3.1.1.1.1.2.2.cmml"><mn id="S3.SS2.p2.3.m3.1.1.1.1.2.2.2" xref="S3.SS2.p2.3.m3.1.1.1.1.2.2.2.cmml">45</mn><mo id="S3.SS2.p2.3.m3.1.1.1.1.2.2.3" xref="S3.SS2.p2.3.m3.1.1.1.1.2.2.3.cmml">∘</mo></msup><mo stretchy="false" id="S3.SS2.p2.3.m3.1.1.1.1.2.5" xref="S3.SS2.p2.3.m3.1.1.1.1.3.cmml">]</mo></mrow><mo stretchy="false" id="S3.SS2.p2.3.m3.1.1.1.3">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><interval closure="closed" id="S3.SS2.p2.3.m3.1.1.1.1.3.cmml" xref="S3.SS2.p2.3.m3.1.1.1.1.2"><apply id="S3.SS2.p2.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1.1.1.1.1"><minus id="S3.SS2.p2.3.m3.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1.1.1.1.1"></minus><apply id="S3.SS2.p2.3.m3.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.1.1.1.1.1.1.2.1.cmml" xref="S3.SS2.p2.3.m3.1.1.1.1.1.1.2">superscript</csymbol><cn type="integer" id="S3.SS2.p2.3.m3.1.1.1.1.1.1.2.2.cmml" xref="S3.SS2.p2.3.m3.1.1.1.1.1.1.2.2">45</cn><compose id="S3.SS2.p2.3.m3.1.1.1.1.1.1.2.3.cmml" xref="S3.SS2.p2.3.m3.1.1.1.1.1.1.2.3"></compose></apply></apply><apply id="S3.SS2.p2.3.m3.1.1.1.1.2.2.cmml" xref="S3.SS2.p2.3.m3.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.1.1.1.1.2.2.1.cmml" xref="S3.SS2.p2.3.m3.1.1.1.1.2.2">superscript</csymbol><cn type="integer" id="S3.SS2.p2.3.m3.1.1.1.1.2.2.2.cmml" xref="S3.SS2.p2.3.m3.1.1.1.1.2.2.2">45</cn><compose id="S3.SS2.p2.3.m3.1.1.1.1.2.2.3.cmml" xref="S3.SS2.p2.3.m3.1.1.1.1.2.2.3"></compose></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">([-45^{\circ},45^{\circ}])</annotation></semantics></math> augmentations randomly to the spatial concatenated shuffled images during the training process, as is demonstrated in the last three columns of Figure <a href="#S3.F2" title="Figure 2 ‣ 3.2 Learning rich representations ‣ 3 Heatmap-Style Jigsaw Puzzles ‣ Learning Heatmap-Style Jigsaw Puzzles Provides Good Pretraining for 2D Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The target positions of Gaussian Distributions corresponding to shuffled patch centres are also changed according to spatial augmentations. By this means, the union of responses learnt by certain neural network is possible to cover the entire 2-dimensional space of <math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="224\times 224" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><mrow id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml"><mn id="S3.SS2.p2.4.m4.1.1.2" xref="S3.SS2.p2.4.m4.1.1.2.cmml">224</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.4.m4.1.1.1" xref="S3.SS2.p2.4.m4.1.1.1.cmml">×</mo><mn id="S3.SS2.p2.4.m4.1.1.3" xref="S3.SS2.p2.4.m4.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><apply id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1"><times id="S3.SS2.p2.4.m4.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1.1"></times><cn type="integer" id="S3.SS2.p2.4.m4.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1.2">224</cn><cn type="integer" id="S3.SS2.p2.4.m4.1.1.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">224\times 224</annotation></semantics></math> (except that <math id="S3.SS2.p2.5.m5.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS2.p2.5.m5.1a"><mi id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><ci id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">N</annotation></semantics></math> is a small number like 1, 2), rather than obtaining responses only on centres of <math id="S3.SS2.p2.6.m6.1" class="ltx_Math" alttext="N\times N" display="inline"><semantics id="S3.SS2.p2.6.m6.1a"><mrow id="S3.SS2.p2.6.m6.1.1" xref="S3.SS2.p2.6.m6.1.1.cmml"><mi id="S3.SS2.p2.6.m6.1.1.2" xref="S3.SS2.p2.6.m6.1.1.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.6.m6.1.1.1" xref="S3.SS2.p2.6.m6.1.1.1.cmml">×</mo><mi id="S3.SS2.p2.6.m6.1.1.3" xref="S3.SS2.p2.6.m6.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m6.1b"><apply id="S3.SS2.p2.6.m6.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1"><times id="S3.SS2.p2.6.m6.1.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1.1"></times><ci id="S3.SS2.p2.6.m6.1.1.2.cmml" xref="S3.SS2.p2.6.m6.1.1.2">𝑁</ci><ci id="S3.SS2.p2.6.m6.1.1.3.cmml" xref="S3.SS2.p2.6.m6.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m6.1c">N\times N</annotation></semantics></math> grids like the first column in Figure <a href="#S3.F2" title="Figure 2 ‣ 3.2 Learning rich representations ‣ 3 Heatmap-Style Jigsaw Puzzles ‣ Learning Heatmap-Style Jigsaw Puzzles Provides Good Pretraining for 2D Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. We don’t care whether these spatial data augmentations are able to improve the performance of solving jigsaw puzzles, but they’re beneficial to learn more spatial-comprehensive features for downstream tasks.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">Apart from spatial augmentations, we follow common colour augmentations in recent papers of self-supervised learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> to improve the diversity of training dataset. Our colour augmentations for training include colour adjustments (hue, contrast, brightness, saturation), colour distortion, optional grayscale conversion, gaussian blur, and solarisation. By the means of colour and spatial augmentations, our HSJP task is able to provide the network with diverse semantics and comprehensive representations.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Loss function for HSJP pretext task</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.3" class="ltx_p">As the form of our pretext task (HSJP) resemble its downstream task of 2D pose estimation - both of them predict some part of a person instance in a heatmap-based way, our loss function for HSJP pretext is similar to recent 2D pose estimation methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>. The output heatmaps from pose estimation networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> are usually <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="1/4" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mrow id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mn id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">1</mn><mo id="S3.SS3.p1.1.m1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.cmml">/</mo><mn id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><divide id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1"></divide><cn type="integer" id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2">1</cn><cn type="integer" id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">1/4</annotation></semantics></math> the size of input images (<math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="224\times 224" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mrow id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml"><mn id="S3.SS3.p1.2.m2.1.1.2" xref="S3.SS3.p1.2.m2.1.1.2.cmml">224</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p1.2.m2.1.1.1" xref="S3.SS3.p1.2.m2.1.1.1.cmml">×</mo><mn id="S3.SS3.p1.2.m2.1.1.3" xref="S3.SS3.p1.2.m2.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><apply id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1"><times id="S3.SS3.p1.2.m2.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1"></times><cn type="integer" id="S3.SS3.p1.2.m2.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.2">224</cn><cn type="integer" id="S3.SS3.p1.2.m2.1.1.3.cmml" xref="S3.SS3.p1.2.m2.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">224\times 224</annotation></semantics></math>), and we follow the same settings: the output heatmaps for HSJP pretext task are also downsampled to <math id="S3.SS3.p1.3.m3.1" class="ltx_Math" alttext="56\times 56" display="inline"><semantics id="S3.SS3.p1.3.m3.1a"><mrow id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml"><mn id="S3.SS3.p1.3.m3.1.1.2" xref="S3.SS3.p1.3.m3.1.1.2.cmml">56</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p1.3.m3.1.1.1" xref="S3.SS3.p1.3.m3.1.1.1.cmml">×</mo><mn id="S3.SS3.p1.3.m3.1.1.3" xref="S3.SS3.p1.3.m3.1.1.3.cmml">56</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><apply id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1"><times id="S3.SS3.p1.3.m3.1.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1.1"></times><cn type="integer" id="S3.SS3.p1.3.m3.1.1.2.cmml" xref="S3.SS3.p1.3.m3.1.1.2">56</cn><cn type="integer" id="S3.SS3.p1.3.m3.1.1.3.cmml" xref="S3.SS3.p1.3.m3.1.1.3">56</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">56\times 56</annotation></semantics></math>.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.6" class="ltx_p">We utilise the same <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="\rm MSE" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mi id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">MSE</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><ci id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">MSE</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">\rm MSE</annotation></semantics></math> loss function as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> to compare the predicted heatmap <math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="\bm{H}" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><mi id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml">𝑯</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><ci id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">𝑯</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">\bm{H}</annotation></semantics></math> with the groundtruth heatmap <math id="S3.SS3.p2.3.m3.1" class="ltx_Math" alttext="\bm{G}" display="inline"><semantics id="S3.SS3.p2.3.m3.1a"><mi id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml">𝑮</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><ci id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1">𝑮</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">\bm{G}</annotation></semantics></math> during the training process. If the centre of <math id="S3.SS3.p2.4.m4.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS3.p2.4.m4.1a"><mi id="S3.SS3.p2.4.m4.1.1" xref="S3.SS3.p2.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.1b"><ci id="S3.SS3.p2.4.m4.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.1c">i</annotation></semantics></math>th patch is outside the image after spatial augmentation, the channel corresponding to <math id="S3.SS3.p2.5.m5.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS3.p2.5.m5.1a"><mi id="S3.SS3.p2.5.m5.1.1" xref="S3.SS3.p2.5.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.5.m5.1b"><ci id="S3.SS3.p2.5.m5.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m5.1c">i</annotation></semantics></math>th patch is not considered into loss function during the training process. The <math id="S3.SS3.p2.6.m6.1" class="ltx_Math" alttext="\rm MSE" display="inline"><semantics id="S3.SS3.p2.6.m6.1a"><mi id="S3.SS3.p2.6.m6.1.1" xref="S3.SS3.p2.6.m6.1.1.cmml">MSE</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.6.m6.1b"><ci id="S3.SS3.p2.6.m6.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1">MSE</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.6.m6.1c">\rm MSE</annotation></semantics></math> loss is calculated as follows:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.3" class="ltx_Math" alttext="{\rm MSE}(\bm{G},\bm{H})=\frac{1}{N^{2}\cdot 56^{2}}\sum_{i=1}^{N^{2}}\sum_{x=1}^{56}\sum_{y=1}^{56}\bm{M}_{i}\cdot(\bm{G}_{xy}^{i}-\bm{H}_{xy}^{i})^{2}," display="block"><semantics id="S3.E2.m1.3a"><mrow id="S3.E2.m1.3.3.1" xref="S3.E2.m1.3.3.1.1.cmml"><mrow id="S3.E2.m1.3.3.1.1" xref="S3.E2.m1.3.3.1.1.cmml"><mrow id="S3.E2.m1.3.3.1.1.3" xref="S3.E2.m1.3.3.1.1.3.cmml"><mi id="S3.E2.m1.3.3.1.1.3.2" xref="S3.E2.m1.3.3.1.1.3.2.cmml">MSE</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.1.1.3.1" xref="S3.E2.m1.3.3.1.1.3.1.cmml">​</mo><mrow id="S3.E2.m1.3.3.1.1.3.3.2" xref="S3.E2.m1.3.3.1.1.3.3.1.cmml"><mo stretchy="false" id="S3.E2.m1.3.3.1.1.3.3.2.1" xref="S3.E2.m1.3.3.1.1.3.3.1.cmml">(</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">𝑮</mi><mo id="S3.E2.m1.3.3.1.1.3.3.2.2" xref="S3.E2.m1.3.3.1.1.3.3.1.cmml">,</mo><mi id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml">𝑯</mi><mo stretchy="false" id="S3.E2.m1.3.3.1.1.3.3.2.3" xref="S3.E2.m1.3.3.1.1.3.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.3.3.1.1.2" xref="S3.E2.m1.3.3.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.3.3.1.1.1" xref="S3.E2.m1.3.3.1.1.1.cmml"><mfrac id="S3.E2.m1.3.3.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.3.cmml"><mn id="S3.E2.m1.3.3.1.1.1.3.2" xref="S3.E2.m1.3.3.1.1.1.3.2.cmml">1</mn><mrow id="S3.E2.m1.3.3.1.1.1.3.3" xref="S3.E2.m1.3.3.1.1.1.3.3.cmml"><msup id="S3.E2.m1.3.3.1.1.1.3.3.2" xref="S3.E2.m1.3.3.1.1.1.3.3.2.cmml"><mi id="S3.E2.m1.3.3.1.1.1.3.3.2.2" xref="S3.E2.m1.3.3.1.1.1.3.3.2.2.cmml">N</mi><mn id="S3.E2.m1.3.3.1.1.1.3.3.2.3" xref="S3.E2.m1.3.3.1.1.1.3.3.2.3.cmml">2</mn></msup><mo lspace="0.222em" rspace="0.222em" id="S3.E2.m1.3.3.1.1.1.3.3.1" xref="S3.E2.m1.3.3.1.1.1.3.3.1.cmml">⋅</mo><msup id="S3.E2.m1.3.3.1.1.1.3.3.3" xref="S3.E2.m1.3.3.1.1.1.3.3.3.cmml"><mn id="S3.E2.m1.3.3.1.1.1.3.3.3.2" xref="S3.E2.m1.3.3.1.1.1.3.3.3.2.cmml">56</mn><mn id="S3.E2.m1.3.3.1.1.1.3.3.3.3" xref="S3.E2.m1.3.3.1.1.1.3.3.3.3.cmml">2</mn></msup></mrow></mfrac><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m1.3.3.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.cmml"><munderover id="S3.E2.m1.3.3.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S3.E2.m1.3.3.1.1.1.1.2.2.2" xref="S3.E2.m1.3.3.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E2.m1.3.3.1.1.1.1.2.2.3" xref="S3.E2.m1.3.3.1.1.1.1.2.2.3.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.2.2.3.2" xref="S3.E2.m1.3.3.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S3.E2.m1.3.3.1.1.1.1.2.2.3.1" xref="S3.E2.m1.3.3.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E2.m1.3.3.1.1.1.1.2.2.3.3" xref="S3.E2.m1.3.3.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><msup id="S3.E2.m1.3.3.1.1.1.1.2.3" xref="S3.E2.m1.3.3.1.1.1.1.2.3.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.2.3.2" xref="S3.E2.m1.3.3.1.1.1.1.2.3.2.cmml">N</mi><mn id="S3.E2.m1.3.3.1.1.1.1.2.3.3" xref="S3.E2.m1.3.3.1.1.1.1.2.3.3.cmml">2</mn></msup></munderover><mrow id="S3.E2.m1.3.3.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.cmml"><munderover id="S3.E2.m1.3.3.1.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S3.E2.m1.3.3.1.1.1.1.1.2.2.2" xref="S3.E2.m1.3.3.1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E2.m1.3.3.1.1.1.1.1.2.2.3" xref="S3.E2.m1.3.3.1.1.1.1.1.2.2.3.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.2.2.3.2" xref="S3.E2.m1.3.3.1.1.1.1.1.2.2.3.2.cmml">x</mi><mo id="S3.E2.m1.3.3.1.1.1.1.1.2.2.3.1" xref="S3.E2.m1.3.3.1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E2.m1.3.3.1.1.1.1.1.2.2.3.3" xref="S3.E2.m1.3.3.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mn id="S3.E2.m1.3.3.1.1.1.1.1.2.3" xref="S3.E2.m1.3.3.1.1.1.1.1.2.3.cmml">56</mn></munderover><mrow id="S3.E2.m1.3.3.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.cmml"><munderover id="S3.E2.m1.3.3.1.1.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.cmml"><mo movablelimits="false" id="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.3.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.3.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.3.2.cmml">y</mi><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.3.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.3.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mn id="S3.E2.m1.3.3.1.1.1.1.1.1.2.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.3.cmml">56</mn></munderover><mrow id="S3.E2.m1.3.3.1.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.cmml"><msub id="S3.E2.m1.3.3.1.1.1.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.3.2.cmml">𝑴</mi><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.cmml">⋅</mo><msup id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml"><msubsup id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml">𝑮</mi><mrow id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.3.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.3.2.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.3.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.3.1.cmml">​</mo><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.3.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.3.3.cmml">y</mi></mrow><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msubsup><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml">−</mo><msubsup id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.2.cmml">𝑯</mi><mrow id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.3.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.3.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.3.2.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.3.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.3.1.cmml">​</mo><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.3.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.3.3.cmml">y</mi></mrow><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></msubsup></mrow><mo stretchy="false" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mn id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.3.cmml">2</mn></msup></mrow></mrow></mrow></mrow></mrow></mrow><mo id="S3.E2.m1.3.3.1.2" xref="S3.E2.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.3b"><apply id="S3.E2.m1.3.3.1.1.cmml" xref="S3.E2.m1.3.3.1"><eq id="S3.E2.m1.3.3.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.2"></eq><apply id="S3.E2.m1.3.3.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.3"><times id="S3.E2.m1.3.3.1.1.3.1.cmml" xref="S3.E2.m1.3.3.1.1.3.1"></times><ci id="S3.E2.m1.3.3.1.1.3.2.cmml" xref="S3.E2.m1.3.3.1.1.3.2">MSE</ci><interval closure="open" id="S3.E2.m1.3.3.1.1.3.3.1.cmml" xref="S3.E2.m1.3.3.1.1.3.3.2"><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">𝑮</ci><ci id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2">𝑯</ci></interval></apply><apply id="S3.E2.m1.3.3.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1"><times id="S3.E2.m1.3.3.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.2"></times><apply id="S3.E2.m1.3.3.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.1.3"><divide id="S3.E2.m1.3.3.1.1.1.3.1.cmml" xref="S3.E2.m1.3.3.1.1.1.3"></divide><cn type="integer" id="S3.E2.m1.3.3.1.1.1.3.2.cmml" xref="S3.E2.m1.3.3.1.1.1.3.2">1</cn><apply id="S3.E2.m1.3.3.1.1.1.3.3.cmml" xref="S3.E2.m1.3.3.1.1.1.3.3"><ci id="S3.E2.m1.3.3.1.1.1.3.3.1.cmml" xref="S3.E2.m1.3.3.1.1.1.3.3.1">⋅</ci><apply id="S3.E2.m1.3.3.1.1.1.3.3.2.cmml" xref="S3.E2.m1.3.3.1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.3.3.2.1.cmml" xref="S3.E2.m1.3.3.1.1.1.3.3.2">superscript</csymbol><ci id="S3.E2.m1.3.3.1.1.1.3.3.2.2.cmml" xref="S3.E2.m1.3.3.1.1.1.3.3.2.2">𝑁</ci><cn type="integer" id="S3.E2.m1.3.3.1.1.1.3.3.2.3.cmml" xref="S3.E2.m1.3.3.1.1.1.3.3.2.3">2</cn></apply><apply id="S3.E2.m1.3.3.1.1.1.3.3.3.cmml" xref="S3.E2.m1.3.3.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.3.3.3.1.cmml" xref="S3.E2.m1.3.3.1.1.1.3.3.3">superscript</csymbol><cn type="integer" id="S3.E2.m1.3.3.1.1.1.3.3.3.2.cmml" xref="S3.E2.m1.3.3.1.1.1.3.3.3.2">56</cn><cn type="integer" id="S3.E2.m1.3.3.1.1.1.3.3.3.3.cmml" xref="S3.E2.m1.3.3.1.1.1.3.3.3.3">2</cn></apply></apply></apply><apply id="S3.E2.m1.3.3.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1"><apply id="S3.E2.m1.3.3.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.2.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.2">superscript</csymbol><apply id="S3.E2.m1.3.3.1.1.1.1.2.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.2.2.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.2">subscript</csymbol><sum id="S3.E2.m1.3.3.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.2.2.2"></sum><apply id="S3.E2.m1.3.3.1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.2.2.3"><eq id="S3.E2.m1.3.3.1.1.1.1.2.2.3.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.2.2.3.1"></eq><ci id="S3.E2.m1.3.3.1.1.1.1.2.2.3.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.2.2.3.2">𝑖</ci><cn type="integer" id="S3.E2.m1.3.3.1.1.1.1.2.2.3.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.2.2.3.3">1</cn></apply></apply><apply id="S3.E2.m1.3.3.1.1.1.1.2.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.2.3.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.2.3">superscript</csymbol><ci id="S3.E2.m1.3.3.1.1.1.1.2.3.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.2.3.2">𝑁</ci><cn type="integer" id="S3.E2.m1.3.3.1.1.1.1.2.3.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.2.3.3">2</cn></apply></apply><apply id="S3.E2.m1.3.3.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1"><apply id="S3.E2.m1.3.3.1.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E2.m1.3.3.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.2.2.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.2">subscript</csymbol><sum id="S3.E2.m1.3.3.1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.2.2.2"></sum><apply id="S3.E2.m1.3.3.1.1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.2.2.3"><eq id="S3.E2.m1.3.3.1.1.1.1.1.2.2.3.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.2.2.3.1"></eq><ci id="S3.E2.m1.3.3.1.1.1.1.1.2.2.3.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.2.2.3.2">𝑥</ci><cn type="integer" id="S3.E2.m1.3.3.1.1.1.1.1.2.2.3.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.2.2.3.3">1</cn></apply></apply><cn type="integer" id="S3.E2.m1.3.3.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.2.3">56</cn></apply><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1"><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2">subscript</csymbol><sum id="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2"></sum><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.3"><eq id="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.3.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.3.1"></eq><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.3.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.3.2">𝑦</ci><cn type="integer" id="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.3.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.3.3">1</cn></apply></apply><cn type="integer" id="S3.E2.m1.3.3.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.3">56</cn></apply><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1"><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2">⋅</ci><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.3.2">𝑴</ci><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.3.3">𝑖</ci></apply><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1"><minus id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1"></minus><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.2">𝑮</ci><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.3"><times id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.3.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.3.1"></times><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.3.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.3.2">𝑥</ci><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.3.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.3.3">𝑦</ci></apply></apply><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3">superscript</csymbol><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.2">𝑯</ci><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.3"><times id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.3.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.3.1"></times><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.3.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.3.2">𝑥</ci><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.3.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.3.3">𝑦</ci></apply></apply><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply><cn type="integer" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.3">2</cn></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.3c">{\rm MSE}(\bm{G},\bm{H})=\frac{1}{N^{2}\cdot 56^{2}}\sum_{i=1}^{N^{2}}\sum_{x=1}^{56}\sum_{y=1}^{56}\bm{M}_{i}\cdot(\bm{G}_{xy}^{i}-\bm{H}_{xy}^{i})^{2},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p2.12" class="ltx_p">where <math id="S3.SS3.p2.7.m1.1" class="ltx_Math" alttext="\bm{G}_{xy}^{i}" display="inline"><semantics id="S3.SS3.p2.7.m1.1a"><msubsup id="S3.SS3.p2.7.m1.1.1" xref="S3.SS3.p2.7.m1.1.1.cmml"><mi id="S3.SS3.p2.7.m1.1.1.2.2" xref="S3.SS3.p2.7.m1.1.1.2.2.cmml">𝑮</mi><mrow id="S3.SS3.p2.7.m1.1.1.2.3" xref="S3.SS3.p2.7.m1.1.1.2.3.cmml"><mi id="S3.SS3.p2.7.m1.1.1.2.3.2" xref="S3.SS3.p2.7.m1.1.1.2.3.2.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.7.m1.1.1.2.3.1" xref="S3.SS3.p2.7.m1.1.1.2.3.1.cmml">​</mo><mi id="S3.SS3.p2.7.m1.1.1.2.3.3" xref="S3.SS3.p2.7.m1.1.1.2.3.3.cmml">y</mi></mrow><mi id="S3.SS3.p2.7.m1.1.1.3" xref="S3.SS3.p2.7.m1.1.1.3.cmml">i</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.7.m1.1b"><apply id="S3.SS3.p2.7.m1.1.1.cmml" xref="S3.SS3.p2.7.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.7.m1.1.1.1.cmml" xref="S3.SS3.p2.7.m1.1.1">superscript</csymbol><apply id="S3.SS3.p2.7.m1.1.1.2.cmml" xref="S3.SS3.p2.7.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.7.m1.1.1.2.1.cmml" xref="S3.SS3.p2.7.m1.1.1">subscript</csymbol><ci id="S3.SS3.p2.7.m1.1.1.2.2.cmml" xref="S3.SS3.p2.7.m1.1.1.2.2">𝑮</ci><apply id="S3.SS3.p2.7.m1.1.1.2.3.cmml" xref="S3.SS3.p2.7.m1.1.1.2.3"><times id="S3.SS3.p2.7.m1.1.1.2.3.1.cmml" xref="S3.SS3.p2.7.m1.1.1.2.3.1"></times><ci id="S3.SS3.p2.7.m1.1.1.2.3.2.cmml" xref="S3.SS3.p2.7.m1.1.1.2.3.2">𝑥</ci><ci id="S3.SS3.p2.7.m1.1.1.2.3.3.cmml" xref="S3.SS3.p2.7.m1.1.1.2.3.3">𝑦</ci></apply></apply><ci id="S3.SS3.p2.7.m1.1.1.3.cmml" xref="S3.SS3.p2.7.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.7.m1.1c">\bm{G}_{xy}^{i}</annotation></semantics></math> is the groundtruth heatmap response at location <math id="S3.SS3.p2.8.m2.2" class="ltx_Math" alttext="(x,y)" display="inline"><semantics id="S3.SS3.p2.8.m2.2a"><mrow id="S3.SS3.p2.8.m2.2.3.2" xref="S3.SS3.p2.8.m2.2.3.1.cmml"><mo stretchy="false" id="S3.SS3.p2.8.m2.2.3.2.1" xref="S3.SS3.p2.8.m2.2.3.1.cmml">(</mo><mi id="S3.SS3.p2.8.m2.1.1" xref="S3.SS3.p2.8.m2.1.1.cmml">x</mi><mo id="S3.SS3.p2.8.m2.2.3.2.2" xref="S3.SS3.p2.8.m2.2.3.1.cmml">,</mo><mi id="S3.SS3.p2.8.m2.2.2" xref="S3.SS3.p2.8.m2.2.2.cmml">y</mi><mo stretchy="false" id="S3.SS3.p2.8.m2.2.3.2.3" xref="S3.SS3.p2.8.m2.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.8.m2.2b"><interval closure="open" id="S3.SS3.p2.8.m2.2.3.1.cmml" xref="S3.SS3.p2.8.m2.2.3.2"><ci id="S3.SS3.p2.8.m2.1.1.cmml" xref="S3.SS3.p2.8.m2.1.1">𝑥</ci><ci id="S3.SS3.p2.8.m2.2.2.cmml" xref="S3.SS3.p2.8.m2.2.2">𝑦</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.8.m2.2c">(x,y)</annotation></semantics></math> of channel <math id="S3.SS3.p2.9.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS3.p2.9.m3.1a"><mi id="S3.SS3.p2.9.m3.1.1" xref="S3.SS3.p2.9.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.9.m3.1b"><ci id="S3.SS3.p2.9.m3.1.1.cmml" xref="S3.SS3.p2.9.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.9.m3.1c">i</annotation></semantics></math>, and <math id="S3.SS3.p2.10.m4.1" class="ltx_Math" alttext="\bm{H}_{xy}^{i}" display="inline"><semantics id="S3.SS3.p2.10.m4.1a"><msubsup id="S3.SS3.p2.10.m4.1.1" xref="S3.SS3.p2.10.m4.1.1.cmml"><mi id="S3.SS3.p2.10.m4.1.1.2.2" xref="S3.SS3.p2.10.m4.1.1.2.2.cmml">𝑯</mi><mrow id="S3.SS3.p2.10.m4.1.1.2.3" xref="S3.SS3.p2.10.m4.1.1.2.3.cmml"><mi id="S3.SS3.p2.10.m4.1.1.2.3.2" xref="S3.SS3.p2.10.m4.1.1.2.3.2.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.10.m4.1.1.2.3.1" xref="S3.SS3.p2.10.m4.1.1.2.3.1.cmml">​</mo><mi id="S3.SS3.p2.10.m4.1.1.2.3.3" xref="S3.SS3.p2.10.m4.1.1.2.3.3.cmml">y</mi></mrow><mi id="S3.SS3.p2.10.m4.1.1.3" xref="S3.SS3.p2.10.m4.1.1.3.cmml">i</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.10.m4.1b"><apply id="S3.SS3.p2.10.m4.1.1.cmml" xref="S3.SS3.p2.10.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.10.m4.1.1.1.cmml" xref="S3.SS3.p2.10.m4.1.1">superscript</csymbol><apply id="S3.SS3.p2.10.m4.1.1.2.cmml" xref="S3.SS3.p2.10.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.10.m4.1.1.2.1.cmml" xref="S3.SS3.p2.10.m4.1.1">subscript</csymbol><ci id="S3.SS3.p2.10.m4.1.1.2.2.cmml" xref="S3.SS3.p2.10.m4.1.1.2.2">𝑯</ci><apply id="S3.SS3.p2.10.m4.1.1.2.3.cmml" xref="S3.SS3.p2.10.m4.1.1.2.3"><times id="S3.SS3.p2.10.m4.1.1.2.3.1.cmml" xref="S3.SS3.p2.10.m4.1.1.2.3.1"></times><ci id="S3.SS3.p2.10.m4.1.1.2.3.2.cmml" xref="S3.SS3.p2.10.m4.1.1.2.3.2">𝑥</ci><ci id="S3.SS3.p2.10.m4.1.1.2.3.3.cmml" xref="S3.SS3.p2.10.m4.1.1.2.3.3">𝑦</ci></apply></apply><ci id="S3.SS3.p2.10.m4.1.1.3.cmml" xref="S3.SS3.p2.10.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.10.m4.1c">\bm{H}_{xy}^{i}</annotation></semantics></math> is the predicted response. <math id="S3.SS3.p2.11.m5.1" class="ltx_Math" alttext="\bm{M}" display="inline"><semantics id="S3.SS3.p2.11.m5.1a"><mi id="S3.SS3.p2.11.m5.1.1" xref="S3.SS3.p2.11.m5.1.1.cmml">𝑴</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.11.m5.1b"><ci id="S3.SS3.p2.11.m5.1.1.cmml" xref="S3.SS3.p2.11.m5.1.1">𝑴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.11.m5.1c">\bm{M}</annotation></semantics></math> is a one-hot mask representing whether the centre of <math id="S3.SS3.p2.12.m6.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS3.p2.12.m6.1a"><mi id="S3.SS3.p2.12.m6.1.1" xref="S3.SS3.p2.12.m6.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.12.m6.1b"><ci id="S3.SS3.p2.12.m6.1.1.cmml" xref="S3.SS3.p2.12.m6.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.12.m6.1c">i</annotation></semantics></math>th patch is inside the input image.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Optimiser for HSJP pretraining</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">Similar to common 2D pose estimation methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>, we employ the same Adam optimiser <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> to train the pretext task. Our models for HSJP are trained for 240 epochs on all person instances of COCO train2017 dataset with 4 Nvidia Quadro RTX 6000 GPUs, and our batch size is 256. The base learning rate is 1e-3, and it is dropped to 1e-4 and 1e-5 at the 190th and 220th epoch.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparing the performances of networks pretrained with HSJP, ImageNet, and training from scratch on COCO val2017 and test-dev2017 sets. We also report mAP of some other state-of-the-art networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> in this table. (The performance without ImageNet pretraining is 0.734 in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, but we only reach 0.729 after several attempts.)</figcaption>
<table id="S3.T1.15.15" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.15.15.16.1" class="ltx_tr">
<td id="S3.T1.15.15.16.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Approach</td>
<td id="S3.T1.15.15.16.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Backbone</td>
<td id="S3.T1.15.15.16.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Pretraining</td>
<td id="S3.T1.15.15.16.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Dataset</td>
<td id="S3.T1.15.15.16.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Input Size</td>
<td id="S3.T1.15.15.16.1.6" class="ltx_td ltx_align_center ltx_border_t">AP</td>
</tr>
<tr id="S3.T1.2.2.2" class="ltx_tr">
<td id="S3.T1.2.2.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Stacked Hourglass <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>
</td>
<td id="S3.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8<math id="S3.T1.1.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T1.1.1.1.1.m1.1a"><mo id="S3.T1.1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.1.m1.1b"><times id="S3.T1.1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.1.m1.1c">\times</annotation></semantics></math>Stacked Hourglass</td>
<td id="S3.T1.2.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Scratch</td>
<td id="S3.T1.2.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">val2017</td>
<td id="S3.T1.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T1.2.2.2.2.m1.1" class="ltx_Math" alttext="256\times 192" display="inline"><semantics id="S3.T1.2.2.2.2.m1.1a"><mrow id="S3.T1.2.2.2.2.m1.1.1" xref="S3.T1.2.2.2.2.m1.1.1.cmml"><mn id="S3.T1.2.2.2.2.m1.1.1.2" xref="S3.T1.2.2.2.2.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S3.T1.2.2.2.2.m1.1.1.1" xref="S3.T1.2.2.2.2.m1.1.1.1.cmml">×</mo><mn id="S3.T1.2.2.2.2.m1.1.1.3" xref="S3.T1.2.2.2.2.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.2.m1.1b"><apply id="S3.T1.2.2.2.2.m1.1.1.cmml" xref="S3.T1.2.2.2.2.m1.1.1"><times id="S3.T1.2.2.2.2.m1.1.1.1.cmml" xref="S3.T1.2.2.2.2.m1.1.1.1"></times><cn type="integer" id="S3.T1.2.2.2.2.m1.1.1.2.cmml" xref="S3.T1.2.2.2.2.m1.1.1.2">256</cn><cn type="integer" id="S3.T1.2.2.2.2.m1.1.1.3.cmml" xref="S3.T1.2.2.2.2.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.2.2.m1.1c">256\times 192</annotation></semantics></math></td>
<td id="S3.T1.2.2.2.6" class="ltx_td ltx_align_center ltx_border_t">66.9</td>
</tr>
<tr id="S3.T1.3.3.3" class="ltx_tr">
<td id="S3.T1.3.3.3.2" class="ltx_td ltx_align_left ltx_border_r">CPN + OHKM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>
</td>
<td id="S3.T1.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r">ResNet50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>
</td>
<td id="S3.T1.3.3.3.4" class="ltx_td ltx_align_center ltx_border_r">ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>
</td>
<td id="S3.T1.3.3.3.5" class="ltx_td ltx_align_center ltx_border_r">val2017</td>
<td id="S3.T1.3.3.3.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S3.T1.3.3.3.1.m1.1" class="ltx_Math" alttext="256\times 192" display="inline"><semantics id="S3.T1.3.3.3.1.m1.1a"><mrow id="S3.T1.3.3.3.1.m1.1.1" xref="S3.T1.3.3.3.1.m1.1.1.cmml"><mn id="S3.T1.3.3.3.1.m1.1.1.2" xref="S3.T1.3.3.3.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S3.T1.3.3.3.1.m1.1.1.1" xref="S3.T1.3.3.3.1.m1.1.1.1.cmml">×</mo><mn id="S3.T1.3.3.3.1.m1.1.1.3" xref="S3.T1.3.3.3.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.3.1.m1.1b"><apply id="S3.T1.3.3.3.1.m1.1.1.cmml" xref="S3.T1.3.3.3.1.m1.1.1"><times id="S3.T1.3.3.3.1.m1.1.1.1.cmml" xref="S3.T1.3.3.3.1.m1.1.1.1"></times><cn type="integer" id="S3.T1.3.3.3.1.m1.1.1.2.cmml" xref="S3.T1.3.3.3.1.m1.1.1.2">256</cn><cn type="integer" id="S3.T1.3.3.3.1.m1.1.1.3.cmml" xref="S3.T1.3.3.3.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.3.1.m1.1c">256\times 192</annotation></semantics></math></td>
<td id="S3.T1.3.3.3.6" class="ltx_td ltx_align_center">69.4</td>
</tr>
<tr id="S3.T1.4.4.4" class="ltx_tr">
<td id="S3.T1.4.4.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">SimpleBaseline <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</td>
<td id="S3.T1.4.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">ResNet50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>
</td>
<td id="S3.T1.4.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Scratch</td>
<td id="S3.T1.4.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">val2017</td>
<td id="S3.T1.4.4.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T1.4.4.4.1.m1.1" class="ltx_Math" alttext="256\times 192" display="inline"><semantics id="S3.T1.4.4.4.1.m1.1a"><mrow id="S3.T1.4.4.4.1.m1.1.1" xref="S3.T1.4.4.4.1.m1.1.1.cmml"><mn id="S3.T1.4.4.4.1.m1.1.1.2" xref="S3.T1.4.4.4.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S3.T1.4.4.4.1.m1.1.1.1" xref="S3.T1.4.4.4.1.m1.1.1.1.cmml">×</mo><mn id="S3.T1.4.4.4.1.m1.1.1.3" xref="S3.T1.4.4.4.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.4.4.4.1.m1.1b"><apply id="S3.T1.4.4.4.1.m1.1.1.cmml" xref="S3.T1.4.4.4.1.m1.1.1"><times id="S3.T1.4.4.4.1.m1.1.1.1.cmml" xref="S3.T1.4.4.4.1.m1.1.1.1"></times><cn type="integer" id="S3.T1.4.4.4.1.m1.1.1.2.cmml" xref="S3.T1.4.4.4.1.m1.1.1.2">256</cn><cn type="integer" id="S3.T1.4.4.4.1.m1.1.1.3.cmml" xref="S3.T1.4.4.4.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.4.4.1.m1.1c">256\times 192</annotation></semantics></math></td>
<td id="S3.T1.4.4.4.6" class="ltx_td ltx_align_center ltx_border_t">69.1</td>
</tr>
<tr id="S3.T1.5.5.5" class="ltx_tr">
<td id="S3.T1.5.5.5.2" class="ltx_td ltx_align_left ltx_border_r">SimpleBaseline <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</td>
<td id="S3.T1.5.5.5.3" class="ltx_td ltx_align_center ltx_border_r">ResNet50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>
</td>
<td id="S3.T1.5.5.5.4" class="ltx_td ltx_align_center ltx_border_r">ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>
</td>
<td id="S3.T1.5.5.5.5" class="ltx_td ltx_align_center ltx_border_r">val2017</td>
<td id="S3.T1.5.5.5.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S3.T1.5.5.5.1.m1.1" class="ltx_Math" alttext="256\times 192" display="inline"><semantics id="S3.T1.5.5.5.1.m1.1a"><mrow id="S3.T1.5.5.5.1.m1.1.1" xref="S3.T1.5.5.5.1.m1.1.1.cmml"><mn id="S3.T1.5.5.5.1.m1.1.1.2" xref="S3.T1.5.5.5.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S3.T1.5.5.5.1.m1.1.1.1" xref="S3.T1.5.5.5.1.m1.1.1.1.cmml">×</mo><mn id="S3.T1.5.5.5.1.m1.1.1.3" xref="S3.T1.5.5.5.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.5.5.5.1.m1.1b"><apply id="S3.T1.5.5.5.1.m1.1.1.cmml" xref="S3.T1.5.5.5.1.m1.1.1"><times id="S3.T1.5.5.5.1.m1.1.1.1.cmml" xref="S3.T1.5.5.5.1.m1.1.1.1"></times><cn type="integer" id="S3.T1.5.5.5.1.m1.1.1.2.cmml" xref="S3.T1.5.5.5.1.m1.1.1.2">256</cn><cn type="integer" id="S3.T1.5.5.5.1.m1.1.1.3.cmml" xref="S3.T1.5.5.5.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.5.5.1.m1.1c">256\times 192</annotation></semantics></math></td>
<td id="S3.T1.5.5.5.6" class="ltx_td ltx_align_center">70.4</td>
</tr>
<tr id="S3.T1.6.6.6" class="ltx_tr">
<td id="S3.T1.6.6.6.2" class="ltx_td ltx_align_left ltx_border_r">SimpleBaseline <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</td>
<td id="S3.T1.6.6.6.3" class="ltx_td ltx_align_center ltx_border_r">ResNet50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>
</td>
<td id="S3.T1.6.6.6.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T1.6.6.6.4.1" class="ltx_text ltx_font_bold">HSJP, <span id="S3.T1.6.6.6.4.1.1" class="ltx_text ltx_font_italic">N</span>=6 (Ours)</span></td>
<td id="S3.T1.6.6.6.5" class="ltx_td ltx_align_center ltx_border_r">val2017</td>
<td id="S3.T1.6.6.6.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S3.T1.6.6.6.1.m1.1" class="ltx_Math" alttext="256\times 192" display="inline"><semantics id="S3.T1.6.6.6.1.m1.1a"><mrow id="S3.T1.6.6.6.1.m1.1.1" xref="S3.T1.6.6.6.1.m1.1.1.cmml"><mn id="S3.T1.6.6.6.1.m1.1.1.2" xref="S3.T1.6.6.6.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S3.T1.6.6.6.1.m1.1.1.1" xref="S3.T1.6.6.6.1.m1.1.1.1.cmml">×</mo><mn id="S3.T1.6.6.6.1.m1.1.1.3" xref="S3.T1.6.6.6.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.6.6.6.1.m1.1b"><apply id="S3.T1.6.6.6.1.m1.1.1.cmml" xref="S3.T1.6.6.6.1.m1.1.1"><times id="S3.T1.6.6.6.1.m1.1.1.1.cmml" xref="S3.T1.6.6.6.1.m1.1.1.1"></times><cn type="integer" id="S3.T1.6.6.6.1.m1.1.1.2.cmml" xref="S3.T1.6.6.6.1.m1.1.1.2">256</cn><cn type="integer" id="S3.T1.6.6.6.1.m1.1.1.3.cmml" xref="S3.T1.6.6.6.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.6.6.1.m1.1c">256\times 192</annotation></semantics></math></td>
<td id="S3.T1.6.6.6.6" class="ltx_td ltx_align_center"><span id="S3.T1.6.6.6.6.1" class="ltx_text ltx_font_bold">70.8</span></td>
</tr>
<tr id="S3.T1.7.7.7" class="ltx_tr">
<td id="S3.T1.7.7.7.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">HRNet W32 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>
</td>
<td id="S3.T1.7.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">HRNet W32</td>
<td id="S3.T1.7.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Scratch</td>
<td id="S3.T1.7.7.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">val2017</td>
<td id="S3.T1.7.7.7.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T1.7.7.7.1.m1.1" class="ltx_Math" alttext="256\times 192" display="inline"><semantics id="S3.T1.7.7.7.1.m1.1a"><mrow id="S3.T1.7.7.7.1.m1.1.1" xref="S3.T1.7.7.7.1.m1.1.1.cmml"><mn id="S3.T1.7.7.7.1.m1.1.1.2" xref="S3.T1.7.7.7.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S3.T1.7.7.7.1.m1.1.1.1" xref="S3.T1.7.7.7.1.m1.1.1.1.cmml">×</mo><mn id="S3.T1.7.7.7.1.m1.1.1.3" xref="S3.T1.7.7.7.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.7.7.7.1.m1.1b"><apply id="S3.T1.7.7.7.1.m1.1.1.cmml" xref="S3.T1.7.7.7.1.m1.1.1"><times id="S3.T1.7.7.7.1.m1.1.1.1.cmml" xref="S3.T1.7.7.7.1.m1.1.1.1"></times><cn type="integer" id="S3.T1.7.7.7.1.m1.1.1.2.cmml" xref="S3.T1.7.7.7.1.m1.1.1.2">256</cn><cn type="integer" id="S3.T1.7.7.7.1.m1.1.1.3.cmml" xref="S3.T1.7.7.7.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.7.7.7.1.m1.1c">256\times 192</annotation></semantics></math></td>
<td id="S3.T1.7.7.7.6" class="ltx_td ltx_align_center ltx_border_t">72.9</td>
</tr>
<tr id="S3.T1.8.8.8" class="ltx_tr">
<td id="S3.T1.8.8.8.2" class="ltx_td ltx_align_left ltx_border_r">HRNet W32 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>
</td>
<td id="S3.T1.8.8.8.3" class="ltx_td ltx_align_center ltx_border_r">HRNet W32</td>
<td id="S3.T1.8.8.8.4" class="ltx_td ltx_align_center ltx_border_r">ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>
</td>
<td id="S3.T1.8.8.8.5" class="ltx_td ltx_align_center ltx_border_r">val2017</td>
<td id="S3.T1.8.8.8.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S3.T1.8.8.8.1.m1.1" class="ltx_Math" alttext="256\times 192" display="inline"><semantics id="S3.T1.8.8.8.1.m1.1a"><mrow id="S3.T1.8.8.8.1.m1.1.1" xref="S3.T1.8.8.8.1.m1.1.1.cmml"><mn id="S3.T1.8.8.8.1.m1.1.1.2" xref="S3.T1.8.8.8.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S3.T1.8.8.8.1.m1.1.1.1" xref="S3.T1.8.8.8.1.m1.1.1.1.cmml">×</mo><mn id="S3.T1.8.8.8.1.m1.1.1.3" xref="S3.T1.8.8.8.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.8.8.8.1.m1.1b"><apply id="S3.T1.8.8.8.1.m1.1.1.cmml" xref="S3.T1.8.8.8.1.m1.1.1"><times id="S3.T1.8.8.8.1.m1.1.1.1.cmml" xref="S3.T1.8.8.8.1.m1.1.1.1"></times><cn type="integer" id="S3.T1.8.8.8.1.m1.1.1.2.cmml" xref="S3.T1.8.8.8.1.m1.1.1.2">256</cn><cn type="integer" id="S3.T1.8.8.8.1.m1.1.1.3.cmml" xref="S3.T1.8.8.8.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.8.8.8.1.m1.1c">256\times 192</annotation></semantics></math></td>
<td id="S3.T1.8.8.8.6" class="ltx_td ltx_align_center">74.4</td>
</tr>
<tr id="S3.T1.9.9.9" class="ltx_tr">
<td id="S3.T1.9.9.9.2" class="ltx_td ltx_align_left ltx_border_r">HRNet W32 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>
</td>
<td id="S3.T1.9.9.9.3" class="ltx_td ltx_align_center ltx_border_r">HRNet W32</td>
<td id="S3.T1.9.9.9.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T1.9.9.9.4.1" class="ltx_text ltx_font_bold">HSJP, <span id="S3.T1.9.9.9.4.1.1" class="ltx_text ltx_font_italic">N</span>=6 (Ours)</span></td>
<td id="S3.T1.9.9.9.5" class="ltx_td ltx_align_center ltx_border_r">val2017</td>
<td id="S3.T1.9.9.9.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S3.T1.9.9.9.1.m1.1" class="ltx_Math" alttext="256\times 192" display="inline"><semantics id="S3.T1.9.9.9.1.m1.1a"><mrow id="S3.T1.9.9.9.1.m1.1.1" xref="S3.T1.9.9.9.1.m1.1.1.cmml"><mn id="S3.T1.9.9.9.1.m1.1.1.2" xref="S3.T1.9.9.9.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S3.T1.9.9.9.1.m1.1.1.1" xref="S3.T1.9.9.9.1.m1.1.1.1.cmml">×</mo><mn id="S3.T1.9.9.9.1.m1.1.1.3" xref="S3.T1.9.9.9.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.9.9.9.1.m1.1b"><apply id="S3.T1.9.9.9.1.m1.1.1.cmml" xref="S3.T1.9.9.9.1.m1.1.1"><times id="S3.T1.9.9.9.1.m1.1.1.1.cmml" xref="S3.T1.9.9.9.1.m1.1.1.1"></times><cn type="integer" id="S3.T1.9.9.9.1.m1.1.1.2.cmml" xref="S3.T1.9.9.9.1.m1.1.1.2">256</cn><cn type="integer" id="S3.T1.9.9.9.1.m1.1.1.3.cmml" xref="S3.T1.9.9.9.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.9.9.9.1.m1.1c">256\times 192</annotation></semantics></math></td>
<td id="S3.T1.9.9.9.6" class="ltx_td ltx_align_center"><span id="S3.T1.9.9.9.6.1" class="ltx_text ltx_font_bold">74.5</span></td>
</tr>
<tr id="S3.T1.10.10.10" class="ltx_tr">
<td id="S3.T1.10.10.10.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">SimpleBaseline <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</td>
<td id="S3.T1.10.10.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">ResNet50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>
</td>
<td id="S3.T1.10.10.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Scratch</td>
<td id="S3.T1.10.10.10.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">test-dev2017</td>
<td id="S3.T1.10.10.10.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T1.10.10.10.1.m1.1" class="ltx_Math" alttext="256\times 192" display="inline"><semantics id="S3.T1.10.10.10.1.m1.1a"><mrow id="S3.T1.10.10.10.1.m1.1.1" xref="S3.T1.10.10.10.1.m1.1.1.cmml"><mn id="S3.T1.10.10.10.1.m1.1.1.2" xref="S3.T1.10.10.10.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S3.T1.10.10.10.1.m1.1.1.1" xref="S3.T1.10.10.10.1.m1.1.1.1.cmml">×</mo><mn id="S3.T1.10.10.10.1.m1.1.1.3" xref="S3.T1.10.10.10.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.10.10.10.1.m1.1b"><apply id="S3.T1.10.10.10.1.m1.1.1.cmml" xref="S3.T1.10.10.10.1.m1.1.1"><times id="S3.T1.10.10.10.1.m1.1.1.1.cmml" xref="S3.T1.10.10.10.1.m1.1.1.1"></times><cn type="integer" id="S3.T1.10.10.10.1.m1.1.1.2.cmml" xref="S3.T1.10.10.10.1.m1.1.1.2">256</cn><cn type="integer" id="S3.T1.10.10.10.1.m1.1.1.3.cmml" xref="S3.T1.10.10.10.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.10.10.10.1.m1.1c">256\times 192</annotation></semantics></math></td>
<td id="S3.T1.10.10.10.6" class="ltx_td ltx_align_center ltx_border_t">68.3</td>
</tr>
<tr id="S3.T1.11.11.11" class="ltx_tr">
<td id="S3.T1.11.11.11.2" class="ltx_td ltx_align_left ltx_border_r">SimpleBaseline <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</td>
<td id="S3.T1.11.11.11.3" class="ltx_td ltx_align_center ltx_border_r">ResNet50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>
</td>
<td id="S3.T1.11.11.11.4" class="ltx_td ltx_align_center ltx_border_r">ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>
</td>
<td id="S3.T1.11.11.11.5" class="ltx_td ltx_align_center ltx_border_r">test-dev2017</td>
<td id="S3.T1.11.11.11.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S3.T1.11.11.11.1.m1.1" class="ltx_Math" alttext="256\times 192" display="inline"><semantics id="S3.T1.11.11.11.1.m1.1a"><mrow id="S3.T1.11.11.11.1.m1.1.1" xref="S3.T1.11.11.11.1.m1.1.1.cmml"><mn id="S3.T1.11.11.11.1.m1.1.1.2" xref="S3.T1.11.11.11.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S3.T1.11.11.11.1.m1.1.1.1" xref="S3.T1.11.11.11.1.m1.1.1.1.cmml">×</mo><mn id="S3.T1.11.11.11.1.m1.1.1.3" xref="S3.T1.11.11.11.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.11.11.11.1.m1.1b"><apply id="S3.T1.11.11.11.1.m1.1.1.cmml" xref="S3.T1.11.11.11.1.m1.1.1"><times id="S3.T1.11.11.11.1.m1.1.1.1.cmml" xref="S3.T1.11.11.11.1.m1.1.1.1"></times><cn type="integer" id="S3.T1.11.11.11.1.m1.1.1.2.cmml" xref="S3.T1.11.11.11.1.m1.1.1.2">256</cn><cn type="integer" id="S3.T1.11.11.11.1.m1.1.1.3.cmml" xref="S3.T1.11.11.11.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.11.11.11.1.m1.1c">256\times 192</annotation></semantics></math></td>
<td id="S3.T1.11.11.11.6" class="ltx_td ltx_align_center">69.5</td>
</tr>
<tr id="S3.T1.12.12.12" class="ltx_tr">
<td id="S3.T1.12.12.12.2" class="ltx_td ltx_align_left ltx_border_r">SimpleBaseline <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</td>
<td id="S3.T1.12.12.12.3" class="ltx_td ltx_align_center ltx_border_r">ResNet50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>
</td>
<td id="S3.T1.12.12.12.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T1.12.12.12.4.1" class="ltx_text ltx_font_bold">HSJP, <span id="S3.T1.12.12.12.4.1.1" class="ltx_text ltx_font_italic">N</span>=6 (Ours)</span></td>
<td id="S3.T1.12.12.12.5" class="ltx_td ltx_align_center ltx_border_r">test-dev2017</td>
<td id="S3.T1.12.12.12.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S3.T1.12.12.12.1.m1.1" class="ltx_Math" alttext="256\times 192" display="inline"><semantics id="S3.T1.12.12.12.1.m1.1a"><mrow id="S3.T1.12.12.12.1.m1.1.1" xref="S3.T1.12.12.12.1.m1.1.1.cmml"><mn id="S3.T1.12.12.12.1.m1.1.1.2" xref="S3.T1.12.12.12.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S3.T1.12.12.12.1.m1.1.1.1" xref="S3.T1.12.12.12.1.m1.1.1.1.cmml">×</mo><mn id="S3.T1.12.12.12.1.m1.1.1.3" xref="S3.T1.12.12.12.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.12.12.12.1.m1.1b"><apply id="S3.T1.12.12.12.1.m1.1.1.cmml" xref="S3.T1.12.12.12.1.m1.1.1"><times id="S3.T1.12.12.12.1.m1.1.1.1.cmml" xref="S3.T1.12.12.12.1.m1.1.1.1"></times><cn type="integer" id="S3.T1.12.12.12.1.m1.1.1.2.cmml" xref="S3.T1.12.12.12.1.m1.1.1.2">256</cn><cn type="integer" id="S3.T1.12.12.12.1.m1.1.1.3.cmml" xref="S3.T1.12.12.12.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.12.12.12.1.m1.1c">256\times 192</annotation></semantics></math></td>
<td id="S3.T1.12.12.12.6" class="ltx_td ltx_align_center"><span id="S3.T1.12.12.12.6.1" class="ltx_text ltx_font_bold">69.8</span></td>
</tr>
<tr id="S3.T1.13.13.13" class="ltx_tr">
<td id="S3.T1.13.13.13.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">HRNet W32 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>
</td>
<td id="S3.T1.13.13.13.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">HRNet W32</td>
<td id="S3.T1.13.13.13.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Scratch</td>
<td id="S3.T1.13.13.13.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">test-dev2017</td>
<td id="S3.T1.13.13.13.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T1.13.13.13.1.m1.1" class="ltx_Math" alttext="256\times 192" display="inline"><semantics id="S3.T1.13.13.13.1.m1.1a"><mrow id="S3.T1.13.13.13.1.m1.1.1" xref="S3.T1.13.13.13.1.m1.1.1.cmml"><mn id="S3.T1.13.13.13.1.m1.1.1.2" xref="S3.T1.13.13.13.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S3.T1.13.13.13.1.m1.1.1.1" xref="S3.T1.13.13.13.1.m1.1.1.1.cmml">×</mo><mn id="S3.T1.13.13.13.1.m1.1.1.3" xref="S3.T1.13.13.13.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.13.13.13.1.m1.1b"><apply id="S3.T1.13.13.13.1.m1.1.1.cmml" xref="S3.T1.13.13.13.1.m1.1.1"><times id="S3.T1.13.13.13.1.m1.1.1.1.cmml" xref="S3.T1.13.13.13.1.m1.1.1.1"></times><cn type="integer" id="S3.T1.13.13.13.1.m1.1.1.2.cmml" xref="S3.T1.13.13.13.1.m1.1.1.2">256</cn><cn type="integer" id="S3.T1.13.13.13.1.m1.1.1.3.cmml" xref="S3.T1.13.13.13.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.13.13.13.1.m1.1c">256\times 192</annotation></semantics></math></td>
<td id="S3.T1.13.13.13.6" class="ltx_td ltx_align_center ltx_border_t">72.1</td>
</tr>
<tr id="S3.T1.14.14.14" class="ltx_tr">
<td id="S3.T1.14.14.14.2" class="ltx_td ltx_align_left ltx_border_r">HRNet W32 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>
</td>
<td id="S3.T1.14.14.14.3" class="ltx_td ltx_align_center ltx_border_r">HRNet W32</td>
<td id="S3.T1.14.14.14.4" class="ltx_td ltx_align_center ltx_border_r">ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>
</td>
<td id="S3.T1.14.14.14.5" class="ltx_td ltx_align_center ltx_border_r">test-dev2017</td>
<td id="S3.T1.14.14.14.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S3.T1.14.14.14.1.m1.1" class="ltx_Math" alttext="256\times 192" display="inline"><semantics id="S3.T1.14.14.14.1.m1.1a"><mrow id="S3.T1.14.14.14.1.m1.1.1" xref="S3.T1.14.14.14.1.m1.1.1.cmml"><mn id="S3.T1.14.14.14.1.m1.1.1.2" xref="S3.T1.14.14.14.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S3.T1.14.14.14.1.m1.1.1.1" xref="S3.T1.14.14.14.1.m1.1.1.1.cmml">×</mo><mn id="S3.T1.14.14.14.1.m1.1.1.3" xref="S3.T1.14.14.14.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.14.14.14.1.m1.1b"><apply id="S3.T1.14.14.14.1.m1.1.1.cmml" xref="S3.T1.14.14.14.1.m1.1.1"><times id="S3.T1.14.14.14.1.m1.1.1.1.cmml" xref="S3.T1.14.14.14.1.m1.1.1.1"></times><cn type="integer" id="S3.T1.14.14.14.1.m1.1.1.2.cmml" xref="S3.T1.14.14.14.1.m1.1.1.2">256</cn><cn type="integer" id="S3.T1.14.14.14.1.m1.1.1.3.cmml" xref="S3.T1.14.14.14.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.14.14.14.1.m1.1c">256\times 192</annotation></semantics></math></td>
<td id="S3.T1.14.14.14.6" class="ltx_td ltx_align_center">73.5</td>
</tr>
<tr id="S3.T1.15.15.15" class="ltx_tr">
<td id="S3.T1.15.15.15.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">HRNet W32 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>
</td>
<td id="S3.T1.15.15.15.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">HRNet W32</td>
<td id="S3.T1.15.15.15.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S3.T1.15.15.15.4.1" class="ltx_text ltx_font_bold">HSJP, <span id="S3.T1.15.15.15.4.1.1" class="ltx_text ltx_font_italic">N</span>=6 (Ours)</span></td>
<td id="S3.T1.15.15.15.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">test-dev2017</td>
<td id="S3.T1.15.15.15.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><math id="S3.T1.15.15.15.1.m1.1" class="ltx_Math" alttext="256\times 192" display="inline"><semantics id="S3.T1.15.15.15.1.m1.1a"><mrow id="S3.T1.15.15.15.1.m1.1.1" xref="S3.T1.15.15.15.1.m1.1.1.cmml"><mn id="S3.T1.15.15.15.1.m1.1.1.2" xref="S3.T1.15.15.15.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S3.T1.15.15.15.1.m1.1.1.1" xref="S3.T1.15.15.15.1.m1.1.1.1.cmml">×</mo><mn id="S3.T1.15.15.15.1.m1.1.1.3" xref="S3.T1.15.15.15.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.15.15.15.1.m1.1b"><apply id="S3.T1.15.15.15.1.m1.1.1.cmml" xref="S3.T1.15.15.15.1.m1.1.1"><times id="S3.T1.15.15.15.1.m1.1.1.1.cmml" xref="S3.T1.15.15.15.1.m1.1.1.1"></times><cn type="integer" id="S3.T1.15.15.15.1.m1.1.1.2.cmml" xref="S3.T1.15.15.15.1.m1.1.1.2">256</cn><cn type="integer" id="S3.T1.15.15.15.1.m1.1.1.3.cmml" xref="S3.T1.15.15.15.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.15.15.15.1.m1.1c">256\times 192</annotation></semantics></math></td>
<td id="S3.T1.15.15.15.6" class="ltx_td ltx_align_center ltx_border_b"><span id="S3.T1.15.15.15.6.1" class="ltx_text ltx_font_bold">73.6</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S3.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F3.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2012.07101/assets/x3.png" id="S3.F3.sf1.g1" class="ltx_graphics ltx_img_landscape" width="147" height="101" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.sf1.2.1.1" class="ltx_text" style="font-size:80%;">(a)</span> </span><span id="S3.F3.sf1.3.2" class="ltx_text" style="font-size:80%;">(a) Finetuning on HRNet from Scratch.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F3.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/2012.07101/assets/x4.png" id="S3.F3.sf2.g1" class="ltx_graphics ltx_img_landscape" width="147" height="101" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.sf2.2.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span><span id="S3.F3.sf2.3.2" class="ltx_text" style="font-size:80%;">(b) Finetuning on HRNet from ImageNet.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F3.sf3" class="ltx_figure ltx_figure_panel"><img src="/html/2012.07101/assets/x5.png" id="S3.F3.sf3.g1" class="ltx_graphics ltx_img_landscape" width="147" height="101" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.sf3.2.1.1" class="ltx_text" style="font-size:80%;">(c)</span> </span><span id="S3.F3.sf3.3.2" class="ltx_text" style="font-size:80%;">(c) Finetuning on HRNet from HSJP.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Detailed performance analysis of downstream task, based on HRNet W32 pretrained with different approaches. Both ImageNet and HSJP pretraining perform much better than training from scratch in their downstream task, for all OKS thresholds. HSJP share similar performance with ImageNet pretraining at low OKS thresholds (ImageNet slightly better), while HSJP pretrained HRNet performs much better for 2D human pose estimation at high thresholds (OKS <math id="S3.F3.2.m1.1" class="ltx_Math" alttext="\geq" display="inline"><semantics id="S3.F3.2.m1.1b"><mo id="S3.F3.2.m1.1.1" xref="S3.F3.2.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="S3.F3.2.m1.1c"><geq id="S3.F3.2.m1.1.1.cmml" xref="S3.F3.2.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.2.m1.1d">\geq</annotation></semantics></math> 0.85).</figcaption>
</figure>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Performance of solving jigsaw puzzles</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.3" class="ltx_p">To estimate the representation power of CNN after pretraining on HSJP pretext, we evaluate the average precision of jigsaw puzzles on all person instances in COCO val2017 dataset. We split and shuffle image patches as Section <a href="#S3.SS1" title="3.1 Target description for HSJP pretext task ‣ 3 Heatmap-Style Jigsaw Puzzles ‣ Learning Heatmap-Style Jigsaw Puzzles Provides Good Pretraining for 2D Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a> does, but we don’t use any data augmentation when testing the performance of HSJP, because the randomness of test-time augmentation makes the performance unstable. For the <math id="S3.SS5.p1.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS5.p1.1.m1.1a"><mi id="S3.SS5.p1.1.m1.1.1" xref="S3.SS5.p1.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.1.m1.1b"><ci id="S3.SS5.p1.1.m1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.1.m1.1c">i</annotation></semantics></math>th patch, if the peak of predicted Gaussian Distribution <math id="S3.SS5.p1.2.m2.1" class="ltx_Math" alttext="\hat{\bm{p}}_{i}" display="inline"><semantics id="S3.SS5.p1.2.m2.1a"><msub id="S3.SS5.p1.2.m2.1.1" xref="S3.SS5.p1.2.m2.1.1.cmml"><mover accent="true" id="S3.SS5.p1.2.m2.1.1.2" xref="S3.SS5.p1.2.m2.1.1.2.cmml"><mi id="S3.SS5.p1.2.m2.1.1.2.2" xref="S3.SS5.p1.2.m2.1.1.2.2.cmml">𝒑</mi><mo id="S3.SS5.p1.2.m2.1.1.2.1" xref="S3.SS5.p1.2.m2.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS5.p1.2.m2.1.1.3" xref="S3.SS5.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.2.m2.1b"><apply id="S3.SS5.p1.2.m2.1.1.cmml" xref="S3.SS5.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS5.p1.2.m2.1.1.1.cmml" xref="S3.SS5.p1.2.m2.1.1">subscript</csymbol><apply id="S3.SS5.p1.2.m2.1.1.2.cmml" xref="S3.SS5.p1.2.m2.1.1.2"><ci id="S3.SS5.p1.2.m2.1.1.2.1.cmml" xref="S3.SS5.p1.2.m2.1.1.2.1">^</ci><ci id="S3.SS5.p1.2.m2.1.1.2.2.cmml" xref="S3.SS5.p1.2.m2.1.1.2.2">𝒑</ci></apply><ci id="S3.SS5.p1.2.m2.1.1.3.cmml" xref="S3.SS5.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.2.m2.1c">\hat{\bm{p}}_{i}</annotation></semantics></math> lies in the neighbourhood of its corresponding ground-truth patch centre <math id="S3.SS5.p1.3.m3.1" class="ltx_Math" alttext="\bm{p}_{i}" display="inline"><semantics id="S3.SS5.p1.3.m3.1a"><msub id="S3.SS5.p1.3.m3.1.1" xref="S3.SS5.p1.3.m3.1.1.cmml"><mi id="S3.SS5.p1.3.m3.1.1.2" xref="S3.SS5.p1.3.m3.1.1.2.cmml">𝒑</mi><mi id="S3.SS5.p1.3.m3.1.1.3" xref="S3.SS5.p1.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.3.m3.1b"><apply id="S3.SS5.p1.3.m3.1.1.cmml" xref="S3.SS5.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS5.p1.3.m3.1.1.1.cmml" xref="S3.SS5.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS5.p1.3.m3.1.1.2.cmml" xref="S3.SS5.p1.3.m3.1.1.2">𝒑</ci><ci id="S3.SS5.p1.3.m3.1.1.3.cmml" xref="S3.SS5.p1.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.3.m3.1c">\bm{p}_{i}</annotation></semantics></math>, i.e.,</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.1" class="ltx_Math" alttext="||\hat{\bm{p}}_{i}-\bm{p}_{i}||_{2}&lt;\epsilon," display="block"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><msub id="S3.E3.m1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E3.m1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.cmml"><mover accent="true" id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.cmml">𝒑</mi><mo id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.1.cmml">^</mo></mover><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.2.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S3.E3.m1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml">−</mo><msub id="S3.E3.m1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.3.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.cmml">𝒑</mi><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.3.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S3.E3.m1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.3.cmml">2</mn></msub><mo id="S3.E3.m1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.2.cmml">&lt;</mo><mi id="S3.E3.m1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.3.cmml">ϵ</mi></mrow><mo id="S3.E3.m1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"><lt id="S3.E3.m1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.2"></lt><apply id="S3.E3.m1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1">subscript</csymbol><apply id="S3.E3.m1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E3.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1"><minus id="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1"></minus><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2">subscript</csymbol><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2"><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.1">^</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2">𝒑</ci></apply><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.2">𝒑</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply></apply><cn type="integer" id="S3.E3.m1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.3">2</cn></apply><ci id="S3.E3.m1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.3">italic-ϵ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">||\hat{\bm{p}}_{i}-\bm{p}_{i}||_{2}&lt;\epsilon,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS5.p1.6" class="ltx_p">we consider that the <math id="S3.SS5.p1.4.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS5.p1.4.m1.1a"><mi id="S3.SS5.p1.4.m1.1.1" xref="S3.SS5.p1.4.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.4.m1.1b"><ci id="S3.SS5.p1.4.m1.1.1.cmml" xref="S3.SS5.p1.4.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.4.m1.1c">i</annotation></semantics></math>th image patch is an accurately located one for solving jigsaw puzzles (<math id="S3.SS5.p1.5.m2.1" class="ltx_math_unparsed" alttext="||\cdot||_{2}" display="inline"><semantics id="S3.SS5.p1.5.m2.1a"><mrow id="S3.SS5.p1.5.m2.1b"><mo fence="false" rspace="0.167em" stretchy="false" id="S3.SS5.p1.5.m2.1.1">|</mo><mo fence="false" stretchy="false" id="S3.SS5.p1.5.m2.1.2">|</mo><mo lspace="0em" rspace="0em" id="S3.SS5.p1.5.m2.1.3">⋅</mo><mo fence="false" rspace="0.167em" stretchy="false" id="S3.SS5.p1.5.m2.1.4">|</mo><msub id="S3.SS5.p1.5.m2.1.5"><mo fence="false" stretchy="false" id="S3.SS5.p1.5.m2.1.5.2">|</mo><mn id="S3.SS5.p1.5.m2.1.5.3">2</mn></msub></mrow><annotation encoding="application/x-tex" id="S3.SS5.p1.5.m2.1c">||\cdot||_{2}</annotation></semantics></math> represents the L2 norm). If all of the <math id="S3.SS5.p1.6.m3.1" class="ltx_Math" alttext="N\times N" display="inline"><semantics id="S3.SS5.p1.6.m3.1a"><mrow id="S3.SS5.p1.6.m3.1.1" xref="S3.SS5.p1.6.m3.1.1.cmml"><mi id="S3.SS5.p1.6.m3.1.1.2" xref="S3.SS5.p1.6.m3.1.1.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS5.p1.6.m3.1.1.1" xref="S3.SS5.p1.6.m3.1.1.1.cmml">×</mo><mi id="S3.SS5.p1.6.m3.1.1.3" xref="S3.SS5.p1.6.m3.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.6.m3.1b"><apply id="S3.SS5.p1.6.m3.1.1.cmml" xref="S3.SS5.p1.6.m3.1.1"><times id="S3.SS5.p1.6.m3.1.1.1.cmml" xref="S3.SS5.p1.6.m3.1.1.1"></times><ci id="S3.SS5.p1.6.m3.1.1.2.cmml" xref="S3.SS5.p1.6.m3.1.1.2">𝑁</ci><ci id="S3.SS5.p1.6.m3.1.1.3.cmml" xref="S3.SS5.p1.6.m3.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.6.m3.1c">N\times N</annotation></semantics></math> patches in a shuffled image are correctly located, i.e.,</p>
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.4" class="ltx_Math" alttext="||\hat{\bm{p}}_{i}-\bm{p}_{i}||_{2}&lt;\epsilon,\forall i\in[0,N^{2})" display="block"><semantics id="S3.E4.m1.4a"><mrow id="S3.E4.m1.4.4.2" xref="S3.E4.m1.4.4.3.cmml"><mrow id="S3.E4.m1.3.3.1.1" xref="S3.E4.m1.3.3.1.1.cmml"><msub id="S3.E4.m1.3.3.1.1.1" xref="S3.E4.m1.3.3.1.1.1.cmml"><mrow id="S3.E4.m1.3.3.1.1.1.1.1" xref="S3.E4.m1.3.3.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E4.m1.3.3.1.1.1.1.1.2" xref="S3.E4.m1.3.3.1.1.1.1.2.1.cmml">‖</mo><mrow id="S3.E4.m1.3.3.1.1.1.1.1.1" xref="S3.E4.m1.3.3.1.1.1.1.1.1.cmml"><msub id="S3.E4.m1.3.3.1.1.1.1.1.1.2" xref="S3.E4.m1.3.3.1.1.1.1.1.1.2.cmml"><mover accent="true" id="S3.E4.m1.3.3.1.1.1.1.1.1.2.2" xref="S3.E4.m1.3.3.1.1.1.1.1.1.2.2.cmml"><mi id="S3.E4.m1.3.3.1.1.1.1.1.1.2.2.2" xref="S3.E4.m1.3.3.1.1.1.1.1.1.2.2.2.cmml">𝒑</mi><mo id="S3.E4.m1.3.3.1.1.1.1.1.1.2.2.1" xref="S3.E4.m1.3.3.1.1.1.1.1.1.2.2.1.cmml">^</mo></mover><mi id="S3.E4.m1.3.3.1.1.1.1.1.1.2.3" xref="S3.E4.m1.3.3.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S3.E4.m1.3.3.1.1.1.1.1.1.1" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.cmml">−</mo><msub id="S3.E4.m1.3.3.1.1.1.1.1.1.3" xref="S3.E4.m1.3.3.1.1.1.1.1.1.3.cmml"><mi id="S3.E4.m1.3.3.1.1.1.1.1.1.3.2" xref="S3.E4.m1.3.3.1.1.1.1.1.1.3.2.cmml">𝒑</mi><mi id="S3.E4.m1.3.3.1.1.1.1.1.1.3.3" xref="S3.E4.m1.3.3.1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo stretchy="false" id="S3.E4.m1.3.3.1.1.1.1.1.3" xref="S3.E4.m1.3.3.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S3.E4.m1.3.3.1.1.1.3" xref="S3.E4.m1.3.3.1.1.1.3.cmml">2</mn></msub><mo id="S3.E4.m1.3.3.1.1.2" xref="S3.E4.m1.3.3.1.1.2.cmml">&lt;</mo><mi id="S3.E4.m1.2.2" xref="S3.E4.m1.2.2.cmml">ϵ</mi></mrow><mo id="S3.E4.m1.4.4.2.3" xref="S3.E4.m1.4.4.3a.cmml">,</mo><mrow id="S3.E4.m1.4.4.2.2" xref="S3.E4.m1.4.4.2.2.cmml"><mrow id="S3.E4.m1.4.4.2.2.3" xref="S3.E4.m1.4.4.2.2.3.cmml"><mo rspace="0.167em" id="S3.E4.m1.4.4.2.2.3.1" xref="S3.E4.m1.4.4.2.2.3.1.cmml">∀</mo><mi id="S3.E4.m1.4.4.2.2.3.2" xref="S3.E4.m1.4.4.2.2.3.2.cmml">i</mi></mrow><mo id="S3.E4.m1.4.4.2.2.2" xref="S3.E4.m1.4.4.2.2.2.cmml">∈</mo><mrow id="S3.E4.m1.4.4.2.2.1.1" xref="S3.E4.m1.4.4.2.2.1.2.cmml"><mo stretchy="false" id="S3.E4.m1.4.4.2.2.1.1.2" xref="S3.E4.m1.4.4.2.2.1.2.cmml">[</mo><mn id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml">0</mn><mo id="S3.E4.m1.4.4.2.2.1.1.3" xref="S3.E4.m1.4.4.2.2.1.2.cmml">,</mo><msup id="S3.E4.m1.4.4.2.2.1.1.1" xref="S3.E4.m1.4.4.2.2.1.1.1.cmml"><mi id="S3.E4.m1.4.4.2.2.1.1.1.2" xref="S3.E4.m1.4.4.2.2.1.1.1.2.cmml">N</mi><mn id="S3.E4.m1.4.4.2.2.1.1.1.3" xref="S3.E4.m1.4.4.2.2.1.1.1.3.cmml">2</mn></msup><mo stretchy="false" id="S3.E4.m1.4.4.2.2.1.1.4" xref="S3.E4.m1.4.4.2.2.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.4b"><apply id="S3.E4.m1.4.4.3.cmml" xref="S3.E4.m1.4.4.2"><csymbol cd="ambiguous" id="S3.E4.m1.4.4.3a.cmml" xref="S3.E4.m1.4.4.2.3">formulae-sequence</csymbol><apply id="S3.E4.m1.3.3.1.1.cmml" xref="S3.E4.m1.3.3.1.1"><lt id="S3.E4.m1.3.3.1.1.2.cmml" xref="S3.E4.m1.3.3.1.1.2"></lt><apply id="S3.E4.m1.3.3.1.1.1.cmml" xref="S3.E4.m1.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.1.1.1.2.cmml" xref="S3.E4.m1.3.3.1.1.1">subscript</csymbol><apply id="S3.E4.m1.3.3.1.1.1.1.2.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1"><csymbol cd="latexml" id="S3.E4.m1.3.3.1.1.1.1.2.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.2">norm</csymbol><apply id="S3.E4.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1"><minus id="S3.E4.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1"></minus><apply id="S3.E4.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.1.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.2">subscript</csymbol><apply id="S3.E4.m1.3.3.1.1.1.1.1.1.2.2.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.2.2"><ci id="S3.E4.m1.3.3.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.2.2.1">^</ci><ci id="S3.E4.m1.3.3.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.2.2.2">𝒑</ci></apply><ci id="S3.E4.m1.3.3.1.1.1.1.1.1.2.3.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S3.E4.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.1.1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E4.m1.3.3.1.1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.3.2">𝒑</ci><ci id="S3.E4.m1.3.3.1.1.1.1.1.1.3.3.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply></apply><cn type="integer" id="S3.E4.m1.3.3.1.1.1.3.cmml" xref="S3.E4.m1.3.3.1.1.1.3">2</cn></apply><ci id="S3.E4.m1.2.2.cmml" xref="S3.E4.m1.2.2">italic-ϵ</ci></apply><apply id="S3.E4.m1.4.4.2.2.cmml" xref="S3.E4.m1.4.4.2.2"><in id="S3.E4.m1.4.4.2.2.2.cmml" xref="S3.E4.m1.4.4.2.2.2"></in><apply id="S3.E4.m1.4.4.2.2.3.cmml" xref="S3.E4.m1.4.4.2.2.3"><csymbol cd="latexml" id="S3.E4.m1.4.4.2.2.3.1.cmml" xref="S3.E4.m1.4.4.2.2.3.1">for-all</csymbol><ci id="S3.E4.m1.4.4.2.2.3.2.cmml" xref="S3.E4.m1.4.4.2.2.3.2">𝑖</ci></apply><interval closure="closed-open" id="S3.E4.m1.4.4.2.2.1.2.cmml" xref="S3.E4.m1.4.4.2.2.1.1"><cn type="integer" id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1">0</cn><apply id="S3.E4.m1.4.4.2.2.1.1.1.cmml" xref="S3.E4.m1.4.4.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.4.4.2.2.1.1.1.1.cmml" xref="S3.E4.m1.4.4.2.2.1.1.1">superscript</csymbol><ci id="S3.E4.m1.4.4.2.2.1.1.1.2.cmml" xref="S3.E4.m1.4.4.2.2.1.1.1.2">𝑁</ci><cn type="integer" id="S3.E4.m1.4.4.2.2.1.1.1.3.cmml" xref="S3.E4.m1.4.4.2.2.1.1.1.3">2</cn></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.4c">||\hat{\bm{p}}_{i}-\bm{p}_{i}||_{2}&lt;\epsilon,\forall i\in[0,N^{2})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S3.SS5.p1.7" class="ltx_p">the jigsaw puzzles problem for that image is considered as successfully solved.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.4" class="ltx_p">We report the average precision of HSJP for all person instances in COCO val2017 dataset, i.e.</p>
<table id="S3.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E5.m1.1" class="ltx_Math" alttext="P=\frac{T}{T+F}" display="block"><semantics id="S3.E5.m1.1a"><mrow id="S3.E5.m1.1.1" xref="S3.E5.m1.1.1.cmml"><mi id="S3.E5.m1.1.1.2" xref="S3.E5.m1.1.1.2.cmml">P</mi><mo id="S3.E5.m1.1.1.1" xref="S3.E5.m1.1.1.1.cmml">=</mo><mfrac id="S3.E5.m1.1.1.3" xref="S3.E5.m1.1.1.3.cmml"><mi id="S3.E5.m1.1.1.3.2" xref="S3.E5.m1.1.1.3.2.cmml">T</mi><mrow id="S3.E5.m1.1.1.3.3" xref="S3.E5.m1.1.1.3.3.cmml"><mi id="S3.E5.m1.1.1.3.3.2" xref="S3.E5.m1.1.1.3.3.2.cmml">T</mi><mo id="S3.E5.m1.1.1.3.3.1" xref="S3.E5.m1.1.1.3.3.1.cmml">+</mo><mi id="S3.E5.m1.1.1.3.3.3" xref="S3.E5.m1.1.1.3.3.3.cmml">F</mi></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.1b"><apply id="S3.E5.m1.1.1.cmml" xref="S3.E5.m1.1.1"><eq id="S3.E5.m1.1.1.1.cmml" xref="S3.E5.m1.1.1.1"></eq><ci id="S3.E5.m1.1.1.2.cmml" xref="S3.E5.m1.1.1.2">𝑃</ci><apply id="S3.E5.m1.1.1.3.cmml" xref="S3.E5.m1.1.1.3"><divide id="S3.E5.m1.1.1.3.1.cmml" xref="S3.E5.m1.1.1.3"></divide><ci id="S3.E5.m1.1.1.3.2.cmml" xref="S3.E5.m1.1.1.3.2">𝑇</ci><apply id="S3.E5.m1.1.1.3.3.cmml" xref="S3.E5.m1.1.1.3.3"><plus id="S3.E5.m1.1.1.3.3.1.cmml" xref="S3.E5.m1.1.1.3.3.1"></plus><ci id="S3.E5.m1.1.1.3.3.2.cmml" xref="S3.E5.m1.1.1.3.3.2">𝑇</ci><ci id="S3.E5.m1.1.1.3.3.3.cmml" xref="S3.E5.m1.1.1.3.3.3">𝐹</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.1c">P=\frac{T}{T+F}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p id="S3.SS5.p2.3" class="ltx_p">where <math id="S3.SS5.p2.1.m1.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS5.p2.1.m1.1a"><mi id="S3.SS5.p2.1.m1.1.1" xref="S3.SS5.p2.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.1.m1.1b"><ci id="S3.SS5.p2.1.m1.1.1.cmml" xref="S3.SS5.p2.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.1.m1.1c">T</annotation></semantics></math> represent the number of correctly solved jigsaw puzzles across all person instances in COCO val2017, and <math id="S3.SS5.p2.2.m2.1" class="ltx_Math" alttext="F" display="inline"><semantics id="S3.SS5.p2.2.m2.1a"><mi id="S3.SS5.p2.2.m2.1.1" xref="S3.SS5.p2.2.m2.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.2.m2.1b"><ci id="S3.SS5.p2.2.m2.1.1.cmml" xref="S3.SS5.p2.2.m2.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.2.m2.1c">F</annotation></semantics></math> is the number of unsuccessfully solved ones. During our pretraining, the indicator <math id="S3.SS5.p2.3.m3.1" class="ltx_Math" alttext="P" display="inline"><semantics id="S3.SS5.p2.3.m3.1a"><mi id="S3.SS5.p2.3.m3.1.1" xref="S3.SS5.p2.3.m3.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.3.m3.1b"><ci id="S3.SS5.p2.3.m3.1.1.cmml" xref="S3.SS5.p2.3.m3.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.3.m3.1c">P</annotation></semantics></math> is used to select a powerful network for solving HSJP problem, whose weights are then utilised for initialising downstream 2D pose estimators.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">This section is mainly composed of ablation studies and the downstream performances of 2D human pose estimation on MS-COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> val2017 and test-dev2017 datasets.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Dataset and metric for downstream task</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The MS-COCO human keypoint dataset includes more than 200k images and 250k person instances, and 150k of them are publicly available for training. We utilise OKS-based Mean Average Precision <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> (mAP score) under 10 different thresholds as evaluation metric on MS-COCO dataset, where OKS calculates the similarities between predicted keypoints and ground truth positions.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Finetuning on 2D human keypoint labels</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.3" class="ltx_p">We pretrain networks with HSJP on all person instances in MS-COCO train2017 dataset. After pretraining, we finetune our learnt weights on human keypoint labels with two popular 2D pose estimators, HRNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> and SimpleBaseline <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>. For fair comparison, we follow their training settings. The data augmentations include random rotation <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="([-45^{\circ},45^{\circ}])" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1.1"><mo stretchy="false" id="S4.SS2.p1.1.m1.1.1.1.2">(</mo><mrow id="S4.SS2.p1.1.m1.1.1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.1.1.3.cmml"><mo stretchy="false" id="S4.SS2.p1.1.m1.1.1.1.1.2.3" xref="S4.SS2.p1.1.m1.1.1.1.1.3.cmml">[</mo><mrow id="S4.SS2.p1.1.m1.1.1.1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.1.1.1.cmml"><mo id="S4.SS2.p1.1.m1.1.1.1.1.1.1a" xref="S4.SS2.p1.1.m1.1.1.1.1.1.1.cmml">−</mo><msup id="S4.SS2.p1.1.m1.1.1.1.1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.1.1.1.1.2.cmml"><mn id="S4.SS2.p1.1.m1.1.1.1.1.1.1.2.2" xref="S4.SS2.p1.1.m1.1.1.1.1.1.1.2.2.cmml">45</mn><mo id="S4.SS2.p1.1.m1.1.1.1.1.1.1.2.3" xref="S4.SS2.p1.1.m1.1.1.1.1.1.1.2.3.cmml">∘</mo></msup></mrow><mo id="S4.SS2.p1.1.m1.1.1.1.1.2.4" xref="S4.SS2.p1.1.m1.1.1.1.1.3.cmml">,</mo><msup id="S4.SS2.p1.1.m1.1.1.1.1.2.2" xref="S4.SS2.p1.1.m1.1.1.1.1.2.2.cmml"><mn id="S4.SS2.p1.1.m1.1.1.1.1.2.2.2" xref="S4.SS2.p1.1.m1.1.1.1.1.2.2.2.cmml">45</mn><mo id="S4.SS2.p1.1.m1.1.1.1.1.2.2.3" xref="S4.SS2.p1.1.m1.1.1.1.1.2.2.3.cmml">∘</mo></msup><mo stretchy="false" id="S4.SS2.p1.1.m1.1.1.1.1.2.5" xref="S4.SS2.p1.1.m1.1.1.1.1.3.cmml">]</mo></mrow><mo stretchy="false" id="S4.SS2.p1.1.m1.1.1.1.3">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><interval closure="closed" id="S4.SS2.p1.1.m1.1.1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.1.1.2"><apply id="S4.SS2.p1.1.m1.1.1.1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1.1.1.1"><minus id="S4.SS2.p1.1.m1.1.1.1.1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1.1.1.1"></minus><apply id="S4.SS2.p1.1.m1.1.1.1.1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1.1.1.1.2">superscript</csymbol><cn type="integer" id="S4.SS2.p1.1.m1.1.1.1.1.1.1.2.2.cmml" xref="S4.SS2.p1.1.m1.1.1.1.1.1.1.2.2">45</cn><compose id="S4.SS2.p1.1.m1.1.1.1.1.1.1.2.3.cmml" xref="S4.SS2.p1.1.m1.1.1.1.1.1.1.2.3"></compose></apply></apply><apply id="S4.SS2.p1.1.m1.1.1.1.1.2.2.cmml" xref="S4.SS2.p1.1.m1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.1.1.1.1.2.2.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1.1.2.2">superscript</csymbol><cn type="integer" id="S4.SS2.p1.1.m1.1.1.1.1.2.2.2.cmml" xref="S4.SS2.p1.1.m1.1.1.1.1.2.2.2">45</cn><compose id="S4.SS2.p1.1.m1.1.1.1.1.2.2.3.cmml" xref="S4.SS2.p1.1.m1.1.1.1.1.2.2.3"></compose></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">([-45^{\circ},45^{\circ}])</annotation></semantics></math>, rescaling <math id="S4.SS2.p1.2.m2.2" class="ltx_Math" alttext="([-35,35])" display="inline"><semantics id="S4.SS2.p1.2.m2.2a"><mrow id="S4.SS2.p1.2.m2.2.2.1"><mo stretchy="false" id="S4.SS2.p1.2.m2.2.2.1.2">(</mo><mrow id="S4.SS2.p1.2.m2.2.2.1.1.1" xref="S4.SS2.p1.2.m2.2.2.1.1.2.cmml"><mo stretchy="false" id="S4.SS2.p1.2.m2.2.2.1.1.1.2" xref="S4.SS2.p1.2.m2.2.2.1.1.2.cmml">[</mo><mrow id="S4.SS2.p1.2.m2.2.2.1.1.1.1" xref="S4.SS2.p1.2.m2.2.2.1.1.1.1.cmml"><mo id="S4.SS2.p1.2.m2.2.2.1.1.1.1a" xref="S4.SS2.p1.2.m2.2.2.1.1.1.1.cmml">−</mo><mn id="S4.SS2.p1.2.m2.2.2.1.1.1.1.2" xref="S4.SS2.p1.2.m2.2.2.1.1.1.1.2.cmml">35</mn></mrow><mo id="S4.SS2.p1.2.m2.2.2.1.1.1.3" xref="S4.SS2.p1.2.m2.2.2.1.1.2.cmml">,</mo><mn id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml">35</mn><mo stretchy="false" id="S4.SS2.p1.2.m2.2.2.1.1.1.4" xref="S4.SS2.p1.2.m2.2.2.1.1.2.cmml">]</mo></mrow><mo stretchy="false" id="S4.SS2.p1.2.m2.2.2.1.3">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.2b"><interval closure="closed" id="S4.SS2.p1.2.m2.2.2.1.1.2.cmml" xref="S4.SS2.p1.2.m2.2.2.1.1.1"><apply id="S4.SS2.p1.2.m2.2.2.1.1.1.1.cmml" xref="S4.SS2.p1.2.m2.2.2.1.1.1.1"><minus id="S4.SS2.p1.2.m2.2.2.1.1.1.1.1.cmml" xref="S4.SS2.p1.2.m2.2.2.1.1.1.1"></minus><cn type="integer" id="S4.SS2.p1.2.m2.2.2.1.1.1.1.2.cmml" xref="S4.SS2.p1.2.m2.2.2.1.1.1.1.2">35</cn></apply><cn type="integer" id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">35</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.2c">([-35,35])</annotation></semantics></math>, and flipping. We also maintain the same number of training epochs as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> under the same Adam optimiser, and the input size of our downstream task is fixed on <math id="S4.SS2.p1.3.m3.1" class="ltx_Math" alttext="256\times 192" display="inline"><semantics id="S4.SS2.p1.3.m3.1a"><mrow id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml"><mn id="S4.SS2.p1.3.m3.1.1.2" xref="S4.SS2.p1.3.m3.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p1.3.m3.1.1.1" xref="S4.SS2.p1.3.m3.1.1.1.cmml">×</mo><mn id="S4.SS2.p1.3.m3.1.1.3" xref="S4.SS2.p1.3.m3.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><apply id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1"><times id="S4.SS2.p1.3.m3.1.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1.1"></times><cn type="integer" id="S4.SS2.p1.3.m3.1.1.2.cmml" xref="S4.SS2.p1.3.m3.1.1.2">256</cn><cn type="integer" id="S4.SS2.p1.3.m3.1.1.3.cmml" xref="S4.SS2.p1.3.m3.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">256\times 192</annotation></semantics></math>. The only difference in our experiment is whether we initialise the downstream network with weights pretrained on labelled ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> dataset or on our self-supervised HSJP pretext task. For fair comparison, we also follow the same two-stage paradigm in SimpleBaseline <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, and HRNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>: we firstly detect all person instances with Faster-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> and then estimate keypoints of detected human bodies.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">The performances of different pretraining methods and networks are reported in Table <a href="#S3.T1" title="Table 1 ‣ 3.4 Optimiser for HSJP pretraining ‣ 3 Heatmap-Style Jigsaw Puzzles ‣ Learning Heatmap-Style Jigsaw Puzzles Provides Good Pretraining for 2D Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, which shows that the downstream 2D human pose estimators with our HSJP pretraining achieve much better performance than training from scratch, and the results are comparable to ImageNet pretraining (even a little better than ImageNet for 0.x%).</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Detailed analysis of downstream performance</h3>

<figure id="S4.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F4.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2012.07101/assets/x6.png" id="S4.F4.sf1.g1" class="ltx_graphics ltx_img_square" width="67" height="72" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.sf1.2.1.1" class="ltx_text" style="font-size:80%;">(a)</span> </span><span id="S4.F4.sf1.3.2" class="ltx_text" style="font-size:80%;">From Scratch</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F4.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/2012.07101/assets/x7.png" id="S4.F4.sf2.g1" class="ltx_graphics ltx_img_square" width="67" height="72" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.sf2.2.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span><span id="S4.F4.sf2.3.2" class="ltx_text" style="font-size:80%;">From ImageNet</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F4.sf3" class="ltx_figure ltx_figure_panel"><img src="/html/2012.07101/assets/x8.png" id="S4.F4.sf3.g1" class="ltx_graphics ltx_img_square" width="67" height="72" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.sf3.2.1.1" class="ltx_text" style="font-size:80%;">(c)</span> </span><span id="S4.F4.sf3.3.2" class="ltx_text" style="font-size:80%;">From HSJP</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Comparing errors of 2D pose estimation, via HRNet W32 with different pretraining methods. Our HSJP pretraining method enjoys the best downstream performance.</figcaption>
</figure>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2012.07101/assets/x9.png" id="S4.F5.g1" class="ltx_graphics ltx_img_landscape" width="221" height="128" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Analysing the transferability of HRNet W32 pretrained with HSJP pretext task. We freeze different layers of HRNet W32 during finetuning and report their mAP for 2D pose, to explore which layers are general or task-specific.</figcaption>
</figure>
<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.3" class="ltx_p">We also analyse detailed downstream performances with different OKS thresholds (from 0.5 to 0.95) on HRNet W32, as Figure <a href="#S3.F3" title="Figure 3 ‣ 3.4 Optimiser for HSJP pretraining ‣ 3 Heatmap-Style Jigsaw Puzzles ‣ Learning Heatmap-Style Jigsaw Puzzles Provides Good Pretraining for 2D Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows. It’s easy to conclude that pretrained HRNet W32 (with ImageNet or HSJP) achieve much better precision at all OKS thresholds than training from scratch. Comparing the performance of ImageNet with HSJP pretraining in Figure <a href="#S3.F3.sf2" title="In Figure 3 ‣ 3.4 Optimiser for HSJP pretraining ‣ 3 Heatmap-Style Jigsaw Puzzles ‣ Learning Heatmap-Style Jigsaw Puzzles Provides Good Pretraining for 2D Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(b)</span></a> and <a href="#S3.F3.sf3" title="In Figure 3 ‣ 3.4 Optimiser for HSJP pretraining ‣ 3 Heatmap-Style Jigsaw Puzzles ‣ Learning Heatmap-Style Jigsaw Puzzles Provides Good Pretraining for 2D Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(c)</span></a>, we can see that ImageNet pretrained HRNet performs slightly better (like 0.1%) at low OKS thresholds (for OKS <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="\leq" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mo id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">≤</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><leq id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">\leq</annotation></semantics></math> 0.8), while HSJP pretrained HRNet W32 performs much better (<math id="S4.SS3.p1.2.m2.1" class="ltx_Math" alttext="\geq" display="inline"><semantics id="S4.SS3.p1.2.m2.1a"><mo id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><geq id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">\geq</annotation></semantics></math> 1%) at high thresholds (for OKS <math id="S4.SS3.p1.3.m3.1" class="ltx_Math" alttext="\textgreater" display="inline"><semantics id="S4.SS3.p1.3.m3.1a"><mo id="S4.SS3.p1.3.m3.1.1" xref="S4.SS3.p1.3.m3.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.3.m3.1b"><gt id="S4.SS3.p1.3.m3.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.3.m3.1c">\textgreater</annotation></semantics></math> 0.8). Although ImageNet and HSJP share similar mAP on 2D human pose estimation, ImageNet pretrained HRNet W32 achieve this mAP score by slightly accumulating improved low-threshold performance, while HSJP pretrained HRNet W32 predicts more accurately at high OKS thresholds for 2D human pose estimation.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">Also, we use the method in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> to analyse the error of HRNet W32 under different pretraining methods, as Figure <a href="#S4.F4" title="Figure 4 ‣ 4.3 Detailed analysis of downstream performance ‣ 4 Experiments ‣ Learning Heatmap-Style Jigsaw Puzzles Provides Good Pretraining for 2D Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> demonstrates, and we can draw a similar conclusion. Both ImageNet and HSJP pretrained HRNet perform much better than training from scratch. HSJP pretrained HRNet W32 enjoys the highest ratio of good prediction, and the lowest ratio for inverting/swapping errors. ImageNet and our HSJP pretraining perform similarly for jitter and missing errors.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">Taking the finetuning experiments into consideration, we conclude that HSJP pretrained 2D human pose estimators are able to achieve much better performance than those trained from scratch, and their mAP score on the downstream task are also comparable to ImageNet pretrained networks. However, we don’t use any supervised data in our HSJP pretraining, which saves the labour costs of labelling extra datasets (such as ImageNet) for pretraining.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Transferability analysis of different layers</h3>

<figure id="S4.F6" class="ltx_figure"><img src="/html/2012.07101/assets/x10.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="216" height="168" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Semi-supervised training with 10% to 90% of MS-COCO human keypoint labels.</figcaption>
</figure>
<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">To explore which layers are general or task-specific, we perform similar experiment as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>: we freeze some layers of HSJP pretrained HRNet W32 during finetuning, to test the transferability of learnt feature maps. Our analyses on 4 layers of HRNet W32 are reported in Figure <a href="#S4.F5" title="Figure 5 ‣ 4.3 Detailed analysis of downstream performance ‣ 4 Experiments ‣ Learning Heatmap-Style Jigsaw Puzzles Provides Good Pretraining for 2D Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> as follows.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">(1) When freezing the top layers (0 to 2) of HSJP pretrained HRNet W32 during finetuning, the mAP for downstream 2D pose estimator does not decrease so much, so the top layers learnt by HSJP pretext are general for extracting image features.
(2) When freezing layers 0 to 3 of HSJP pretrained HRNet in the downstream task, the mAP score for 2D pose estimation decrease a lot, but the network does not collapse. Therefore, some features in layer 3 are general for feature extraction, and some features need to be task-specific for the downstream task. (3) When we freeze all weights in layers 0 to 4 of HRNet W32 learnt by HSJP pretext task, just leaving the last feature fusion layer for final heatmap regression, the downstream performance collapses to mAP of 0. Hence we know that the layers next to the output of network are specific for downstream task.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Semi-supervised training on keypoint labels</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">The HSJP pretext task is able to build a semi-supervised scheme for 2D human pose estimation: after pretraining backbone network with HSJP pretext task, we fine-tune the HSJP pre-trained network on a randomly selected subset (such as 10%, 20%) of the MS-COCO data set. We do not use any human keypoint labels during pretraining, and we only use partial of 2D human keypoint labels in MS-COCO dataset during finetuning. In this way, our method is semi-supervised. In our experiments for semi-supervised 2D pose estimation, we finetune the HSJP pretrained HRNet W32 on several subsets (from 10% to 90%) of MS-COCO human keypoint dataset, and we finally demonstrate the performances (mAP for 2D human pose estimation) of our semi-supervised learning in Figure <a href="#S4.F6" title="Figure 6 ‣ 4.4 Transferability analysis of different layers ‣ 4 Experiments ‣ Learning Heatmap-Style Jigsaw Puzzles Provides Good Pretraining for 2D Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Ablation Studies</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We finally discuss several hyperparameter settings in this section. As we follow the previous settings of 2D human pose estimation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> in the downstream task, our main concern is about hyperparameters in HSJP pretext task. The ablation studies mainly include three parts: (1) learning rate and number of epochs; (2) how many patches (<math id="S5.p1.1.m1.1" class="ltx_Math" alttext="N\times N" display="inline"><semantics id="S5.p1.1.m1.1a"><mrow id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml"><mi id="S5.p1.1.m1.1.1.2" xref="S5.p1.1.m1.1.1.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S5.p1.1.m1.1.1.1" xref="S5.p1.1.m1.1.1.1.cmml">×</mo><mi id="S5.p1.1.m1.1.1.3" xref="S5.p1.1.m1.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><apply id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1"><times id="S5.p1.1.m1.1.1.1.cmml" xref="S5.p1.1.m1.1.1.1"></times><ci id="S5.p1.1.m1.1.1.2.cmml" xref="S5.p1.1.m1.1.1.2">𝑁</ci><ci id="S5.p1.1.m1.1.1.3.cmml" xref="S5.p1.1.m1.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">N\times N</annotation></semantics></math>) should we divide the original input image into; (3) whether using initial (unshuffled) image in HSJP pretext task.</p>
</div>
<figure id="S5.T2" class="ltx_table">

<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Ablation studies on epoch numbers and learning rates for training HSJP pretext task. The experiments are performed with HRNet W32. The best performance with least cost comes from learning rate of 1e-3 and 240 epochs.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.T3.st1" class="ltx_table ltx_figure_panel ltx_align_center">
<table id="S5.T3.st1.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.st1.2.1.1" class="ltx_tr">
<th id="S5.T3.st1.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Learning Rates</th>
<th id="S5.T3.st1.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">HSJP Precision</th>
<th id="S5.T3.st1.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">mAP for 2D Pose</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.st1.2.2.1" class="ltx_tr">
<td id="S5.T3.st1.2.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3e-4</td>
<td id="S5.T3.st1.2.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.355</td>
<td id="S5.T3.st1.2.2.1.3" class="ltx_td ltx_align_center ltx_border_t">0.742</td>
</tr>
<tr id="S5.T3.st1.2.3.2" class="ltx_tr">
<td id="S5.T3.st1.2.3.2.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T3.st1.2.3.2.1.1" class="ltx_text ltx_font_bold">1e-3</span></td>
<td id="S5.T3.st1.2.3.2.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T3.st1.2.3.2.2.1" class="ltx_text ltx_font_bold">0.368</span></td>
<td id="S5.T3.st1.2.3.2.3" class="ltx_td ltx_align_center"><span id="S5.T3.st1.2.3.2.3.1" class="ltx_text ltx_font_bold">0.745</span></td>
</tr>
<tr id="S5.T3.st1.2.4.3" class="ltx_tr">
<td id="S5.T3.st1.2.4.3.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">2e-3</td>
<td id="S5.T3.st1.2.4.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.0(Collapse)</td>
<td id="S5.T3.st1.2.4.3.3" class="ltx_td ltx_align_center ltx_border_b">Not Available</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S5.T3.st1.3.1.1" class="ltx_text" style="font-size:80%;">(a)</span> </span><span id="S5.T3.st1.4.2" class="ltx_text" style="font-size:80%;">Performances of different learning rates.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.T3.st2" class="ltx_table ltx_figure_panel ltx_align_center">
<table id="S5.T3.st2.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.st2.2.1.1" class="ltx_tr">
<th id="S5.T3.st2.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">Epoch Numbers</th>
<th id="S5.T3.st2.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">HSJP Precision</th>
<th id="S5.T3.st2.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">mAP for 2D Pose</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.st2.2.2.1" class="ltx_tr">
<th id="S5.T3.st2.2.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">120</th>
<td id="S5.T3.st2.2.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.297</td>
<td id="S5.T3.st2.2.2.1.3" class="ltx_td ltx_align_center ltx_border_t">0.739</td>
</tr>
<tr id="S5.T3.st2.2.3.2" class="ltx_tr">
<th id="S5.T3.st2.2.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S5.T3.st2.2.3.2.1.1" class="ltx_text ltx_font_bold">240</span></th>
<td id="S5.T3.st2.2.3.2.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T3.st2.2.3.2.2.1" class="ltx_text ltx_font_bold">0.368</span></td>
<td id="S5.T3.st2.2.3.2.3" class="ltx_td ltx_align_center"><span id="S5.T3.st2.2.3.2.3.1" class="ltx_text ltx_font_bold">0.745</span></td>
</tr>
<tr id="S5.T3.st2.2.4.3" class="ltx_tr">
<th id="S5.T3.st2.2.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">480</th>
<td id="S5.T3.st2.2.4.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.407</td>
<td id="S5.T3.st2.2.4.3.3" class="ltx_td ltx_align_center ltx_border_b">0.745</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S5.T3.st2.3.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span><span id="S5.T3.st2.4.2" class="ltx_text" style="font-size:80%;">Performances of different epoch numbers.</span></figcaption>
</figure>
</div>
</div>
</figure>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>How many epochs under which learning rate?</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Our ablation study in this subsection is to explore the best epoch number and learning rate for training HSJP pretext task. We try 3 different learning rates (3e-4, 1e-3, and 2e-3) as well as 3 different epoch numbers (120, 240, and 480) for our HSJP pretext task on HRNet W32, and their performances are reported in Table <a href="#S5.T2" title="Table 2 ‣ 5 Ablation Studies ‣ Learning Heatmap-Style Jigsaw Puzzles Provides Good Pretraining for 2D Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. When training HSJP pretext task for 120 epochs, the network cannot learn rich enough representations, and the downstream task cannot reach its best performance. When training 480 epochs, the precision on HSJP pretext is higher than that of 240 epochs, but it does not help to increase the mAP of downstream task: 2D human pose estimation. We finally select 240 as the number of epochs for training our HSJP pretext task, as it’s not only of high performance but also time-saving (Training for 480 epochs is time-consuming, but useless for downstream task). Table <a href="#S5.T3.st1" title="In Table 2 ‣ 5 Ablation Studies ‣ Learning Heatmap-Style Jigsaw Puzzles Provides Good Pretraining for 2D Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(a)</span></a> illustrate that the pre-task can converge quickly at learning rate of 1e-3, and the network does not collapse. Thus we select 1e-3 as our learning rate.</p>
</div>
<figure id="S5.T3" class="ltx_table">

<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Ablation studies on splitting number <math id="S5.T3.3.3.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S5.T3.3.3.m1.1b"><mi id="S5.T3.3.3.m1.1.1" xref="S5.T3.3.3.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.T3.3.3.m1.1c"><ci id="S5.T3.3.3.m1.1.1.cmml" xref="S5.T3.3.3.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.3.m1.1d">N</annotation></semantics></math> and whether using unshuffled image. We can conclude that training without original image, and <math id="S5.T3.4.4.m2.1" class="ltx_Math" alttext="N=6" display="inline"><semantics id="S5.T3.4.4.m2.1b"><mrow id="S5.T3.4.4.m2.1.1" xref="S5.T3.4.4.m2.1.1.cmml"><mi id="S5.T3.4.4.m2.1.1.2" xref="S5.T3.4.4.m2.1.1.2.cmml">N</mi><mo id="S5.T3.4.4.m2.1.1.1" xref="S5.T3.4.4.m2.1.1.1.cmml">=</mo><mn id="S5.T3.4.4.m2.1.1.3" xref="S5.T3.4.4.m2.1.1.3.cmml">6</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T3.4.4.m2.1c"><apply id="S5.T3.4.4.m2.1.1.cmml" xref="S5.T3.4.4.m2.1.1"><eq id="S5.T3.4.4.m2.1.1.1.cmml" xref="S5.T3.4.4.m2.1.1.1"></eq><ci id="S5.T3.4.4.m2.1.1.2.cmml" xref="S5.T3.4.4.m2.1.1.2">𝑁</ci><cn type="integer" id="S5.T3.4.4.m2.1.1.3.cmml" xref="S5.T3.4.4.m2.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.4.4.m2.1d">N=6</annotation></semantics></math> provides the best performance for downstream 2D pose estimation. The setting ”From Scratch (450)” in our table trains HRNet for 450 epochs to balance the HSJP pretraining epochs (210+240) for fair comparison. </figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.T4.st1" class="ltx_table ltx_figure_panel ltx_align_center">
<table id="S5.T4.st1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T4.st1.1.1" class="ltx_tr">
<th id="S5.T4.st1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">
<math id="S5.T4.st1.1.1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S5.T4.st1.1.1.1.m1.1a"><mi id="S5.T4.st1.1.1.1.m1.1.1" xref="S5.T4.st1.1.1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.T4.st1.1.1.1.m1.1b"><ci id="S5.T4.st1.1.1.1.m1.1.1.cmml" xref="S5.T4.st1.1.1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.st1.1.1.1.m1.1c">N</annotation></semantics></math> for Splitting</th>
<td id="S5.T4.st1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">HSJP Precision</td>
<td id="S5.T4.st1.1.1.3" class="ltx_td ltx_align_center ltx_border_t">mAP for Pose</td>
</tr>
<tr id="S5.T4.st1.1.2.1" class="ltx_tr">
<th id="S5.T4.st1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">2</th>
<td id="S5.T4.st1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.993</td>
<td id="S5.T4.st1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">0.739</td>
</tr>
<tr id="S5.T4.st1.1.3.2" class="ltx_tr">
<th id="S5.T4.st1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">3</th>
<td id="S5.T4.st1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">0.954</td>
<td id="S5.T4.st1.1.3.2.3" class="ltx_td ltx_align_center">0.739</td>
</tr>
<tr id="S5.T4.st1.1.4.3" class="ltx_tr">
<th id="S5.T4.st1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">4</th>
<td id="S5.T4.st1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">0.766</td>
<td id="S5.T4.st1.1.4.3.3" class="ltx_td ltx_align_center">0.742</td>
</tr>
<tr id="S5.T4.st1.1.5.4" class="ltx_tr">
<th id="S5.T4.st1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">5</th>
<td id="S5.T4.st1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r">0.533</td>
<td id="S5.T4.st1.1.5.4.3" class="ltx_td ltx_align_center">0.743</td>
</tr>
<tr id="S5.T4.st1.1.6.5" class="ltx_tr">
<th id="S5.T4.st1.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S5.T4.st1.1.6.5.1.1" class="ltx_text ltx_font_bold">6</span></th>
<td id="S5.T4.st1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T4.st1.1.6.5.2.1" class="ltx_text ltx_font_bold">0.368</span></td>
<td id="S5.T4.st1.1.6.5.3" class="ltx_td ltx_align_center"><span id="S5.T4.st1.1.6.5.3.1" class="ltx_text ltx_font_bold">0.745</span></td>
</tr>
<tr id="S5.T4.st1.1.7.6" class="ltx_tr">
<th id="S5.T4.st1.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">7</th>
<td id="S5.T4.st1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_r">0.243</td>
<td id="S5.T4.st1.1.7.6.3" class="ltx_td ltx_align_center">0.743</td>
</tr>
<tr id="S5.T4.st1.1.8.7" class="ltx_tr">
<th id="S5.T4.st1.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">8</th>
<td id="S5.T4.st1.1.8.7.2" class="ltx_td ltx_align_center ltx_border_r">0.019</td>
<td id="S5.T4.st1.1.8.7.3" class="ltx_td ltx_align_center">0.735</td>
</tr>
<tr id="S5.T4.st1.1.9.8" class="ltx_tr">
<th id="S5.T4.st1.1.9.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">From Scratch (210)</th>
<td id="S5.T4.st1.1.9.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Not Available</td>
<td id="S5.T4.st1.1.9.8.3" class="ltx_td ltx_align_center ltx_border_t">0.729</td>
</tr>
<tr id="S5.T4.st1.1.10.9" class="ltx_tr">
<th id="S5.T4.st1.1.10.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">From Scratch (450)</th>
<td id="S5.T4.st1.1.10.9.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">Not Available</td>
<td id="S5.T4.st1.1.10.9.3" class="ltx_td ltx_align_center ltx_border_b">0.734</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S5.T4.st1.3.1.1" class="ltx_text" style="font-size:80%;">(a)</span> </span><span id="S5.T4.st1.4.2" class="ltx_text" style="font-size:80%;">HRNet W32 Performances pretrained without unshuffled image</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.T4.st2" class="ltx_table ltx_figure_panel ltx_align_center">
<table id="S5.T4.st2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T4.st2.1.1" class="ltx_tr">
<th id="S5.T4.st2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">
<math id="S5.T4.st2.1.1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S5.T4.st2.1.1.1.m1.1a"><mi id="S5.T4.st2.1.1.1.m1.1.1" xref="S5.T4.st2.1.1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.T4.st2.1.1.1.m1.1b"><ci id="S5.T4.st2.1.1.1.m1.1.1.cmml" xref="S5.T4.st2.1.1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.st2.1.1.1.m1.1c">N</annotation></semantics></math> for Splitting</th>
<td id="S5.T4.st2.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">HSJP Precision</td>
<td id="S5.T4.st2.1.1.3" class="ltx_td ltx_align_center ltx_border_t">mAP for Pose</td>
</tr>
<tr id="S5.T4.st2.1.2.1" class="ltx_tr">
<th id="S5.T4.st2.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">2</th>
<td id="S5.T4.st2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.995</td>
<td id="S5.T4.st2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">0.734</td>
</tr>
<tr id="S5.T4.st2.1.3.2" class="ltx_tr">
<th id="S5.T4.st2.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">3</th>
<td id="S5.T4.st2.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">0.989</td>
<td id="S5.T4.st2.1.3.2.3" class="ltx_td ltx_align_center">0.734</td>
</tr>
<tr id="S5.T4.st2.1.4.3" class="ltx_tr">
<th id="S5.T4.st2.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">4</th>
<td id="S5.T4.st2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">0.972</td>
<td id="S5.T4.st2.1.4.3.3" class="ltx_td ltx_align_center">0.735</td>
</tr>
<tr id="S5.T4.st2.1.5.4" class="ltx_tr">
<th id="S5.T4.st2.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">5</th>
<td id="S5.T4.st2.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r">0.954</td>
<td id="S5.T4.st2.1.5.4.3" class="ltx_td ltx_align_center">0.735</td>
</tr>
<tr id="S5.T4.st2.1.6.5" class="ltx_tr">
<th id="S5.T4.st2.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">6</th>
<td id="S5.T4.st2.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r">0.936</td>
<td id="S5.T4.st2.1.6.5.3" class="ltx_td ltx_align_center">0.734</td>
</tr>
<tr id="S5.T4.st2.1.7.6" class="ltx_tr">
<th id="S5.T4.st2.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">7</th>
<td id="S5.T4.st2.1.7.6.2" class="ltx_td ltx_align_center ltx_border_r">0.892</td>
<td id="S5.T4.st2.1.7.6.3" class="ltx_td ltx_align_center">0.735</td>
</tr>
<tr id="S5.T4.st2.1.8.7" class="ltx_tr">
<th id="S5.T4.st2.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">8</th>
<td id="S5.T4.st2.1.8.7.2" class="ltx_td ltx_align_center ltx_border_r">0.777</td>
<td id="S5.T4.st2.1.8.7.3" class="ltx_td ltx_align_center">0.735</td>
</tr>
<tr id="S5.T4.st2.1.9.8" class="ltx_tr">
<th id="S5.T4.st2.1.9.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">From Scratch (210)</th>
<td id="S5.T4.st2.1.9.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Not Available</td>
<td id="S5.T4.st2.1.9.8.3" class="ltx_td ltx_align_center ltx_border_t">0.729</td>
</tr>
<tr id="S5.T4.st2.1.10.9" class="ltx_tr">
<th id="S5.T4.st2.1.10.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">From Scratch (450)</th>
<td id="S5.T4.st2.1.10.9.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">Not Available</td>
<td id="S5.T4.st2.1.10.9.3" class="ltx_td ltx_align_center ltx_border_b">0.734</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S5.T4.st2.3.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span><span id="S5.T4.st2.4.2" class="ltx_text" style="font-size:80%;">HRNet W32 Performances pretrained with original unshuffled image</span></figcaption>
</figure>
</div>
</div>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>How many pieces of image patches?</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.4" class="ltx_p">We need to decide how many patches (the number <math id="S5.SS2.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S5.SS2.p1.1.m1.1a"><mi id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><ci id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">N</annotation></semantics></math>) should we divide the original image into during HSJP pretraining. If the number of image patches is too small (like <math id="S5.SS2.p1.2.m2.2" class="ltx_Math" alttext="N=1,2" display="inline"><semantics id="S5.SS2.p1.2.m2.2a"><mrow id="S5.SS2.p1.2.m2.2.3" xref="S5.SS2.p1.2.m2.2.3.cmml"><mi id="S5.SS2.p1.2.m2.2.3.2" xref="S5.SS2.p1.2.m2.2.3.2.cmml">N</mi><mo id="S5.SS2.p1.2.m2.2.3.1" xref="S5.SS2.p1.2.m2.2.3.1.cmml">=</mo><mrow id="S5.SS2.p1.2.m2.2.3.3.2" xref="S5.SS2.p1.2.m2.2.3.3.1.cmml"><mn id="S5.SS2.p1.2.m2.1.1" xref="S5.SS2.p1.2.m2.1.1.cmml">1</mn><mo id="S5.SS2.p1.2.m2.2.3.3.2.1" xref="S5.SS2.p1.2.m2.2.3.3.1.cmml">,</mo><mn id="S5.SS2.p1.2.m2.2.2" xref="S5.SS2.p1.2.m2.2.2.cmml">2</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.2.m2.2b"><apply id="S5.SS2.p1.2.m2.2.3.cmml" xref="S5.SS2.p1.2.m2.2.3"><eq id="S5.SS2.p1.2.m2.2.3.1.cmml" xref="S5.SS2.p1.2.m2.2.3.1"></eq><ci id="S5.SS2.p1.2.m2.2.3.2.cmml" xref="S5.SS2.p1.2.m2.2.3.2">𝑁</ci><list id="S5.SS2.p1.2.m2.2.3.3.1.cmml" xref="S5.SS2.p1.2.m2.2.3.3.2"><cn type="integer" id="S5.SS2.p1.2.m2.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1">1</cn><cn type="integer" id="S5.SS2.p1.2.m2.2.2.cmml" xref="S5.SS2.p1.2.m2.2.2">2</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.2.m2.2c">N=1,2</annotation></semantics></math>), the network will be easy to converge, so it will be hard for the network to learn feature maps with rich representations, as the jigsaw puzzling task is too simple. However, if the number of image patches is too large (such as <math id="S5.SS2.p1.3.m3.1" class="ltx_Math" alttext="N&gt;8" display="inline"><semantics id="S5.SS2.p1.3.m3.1a"><mrow id="S5.SS2.p1.3.m3.1.1" xref="S5.SS2.p1.3.m3.1.1.cmml"><mi id="S5.SS2.p1.3.m3.1.1.2" xref="S5.SS2.p1.3.m3.1.1.2.cmml">N</mi><mo id="S5.SS2.p1.3.m3.1.1.1" xref="S5.SS2.p1.3.m3.1.1.1.cmml">&gt;</mo><mn id="S5.SS2.p1.3.m3.1.1.3" xref="S5.SS2.p1.3.m3.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.3.m3.1b"><apply id="S5.SS2.p1.3.m3.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1"><gt id="S5.SS2.p1.3.m3.1.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1.1"></gt><ci id="S5.SS2.p1.3.m3.1.1.2.cmml" xref="S5.SS2.p1.3.m3.1.1.2">𝑁</ci><cn type="integer" id="S5.SS2.p1.3.m3.1.1.3.cmml" xref="S5.SS2.p1.3.m3.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.3.m3.1c">N&gt;8</annotation></semantics></math>), the jigsaw puzzling task is too complicated, and as our resolution of input image is limited to <math id="S5.SS2.p1.4.m4.1" class="ltx_Math" alttext="224\times 224" display="inline"><semantics id="S5.SS2.p1.4.m4.1a"><mrow id="S5.SS2.p1.4.m4.1.1" xref="S5.SS2.p1.4.m4.1.1.cmml"><mn id="S5.SS2.p1.4.m4.1.1.2" xref="S5.SS2.p1.4.m4.1.1.2.cmml">224</mn><mo lspace="0.222em" rspace="0.222em" id="S5.SS2.p1.4.m4.1.1.1" xref="S5.SS2.p1.4.m4.1.1.1.cmml">×</mo><mn id="S5.SS2.p1.4.m4.1.1.3" xref="S5.SS2.p1.4.m4.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.4.m4.1b"><apply id="S5.SS2.p1.4.m4.1.1.cmml" xref="S5.SS2.p1.4.m4.1.1"><times id="S5.SS2.p1.4.m4.1.1.1.cmml" xref="S5.SS2.p1.4.m4.1.1.1"></times><cn type="integer" id="S5.SS2.p1.4.m4.1.1.2.cmml" xref="S5.SS2.p1.4.m4.1.1.2">224</cn><cn type="integer" id="S5.SS2.p1.4.m4.1.1.3.cmml" xref="S5.SS2.p1.4.m4.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.4.m4.1c">224\times 224</annotation></semantics></math>, the pretraining procedure may not be easy to converge.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.7" class="ltx_p">For this concern, we test different settings of <math id="S5.SS2.p2.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S5.SS2.p2.1.m1.1a"><mi id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><ci id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">N</annotation></semantics></math> for our HSJP pretext task with HRNet W32 in this subsection, and finetune networks pretrained with different <math id="S5.SS2.p2.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S5.SS2.p2.2.m2.1a"><mi id="S5.SS2.p2.2.m2.1.1" xref="S5.SS2.p2.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.2.m2.1b"><ci id="S5.SS2.p2.2.m2.1.1.cmml" xref="S5.SS2.p2.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.2.m2.1c">N</annotation></semantics></math> on the downstream 2D pose estimator. The HSJP and downstream mAP performances on MS-COCO val2017 dataset are reported with the metrics in Section <a href="#S3.SS5" title="3.5 Performance of solving jigsaw puzzles ‣ 3 Heatmap-Style Jigsaw Puzzles ‣ Learning Heatmap-Style Jigsaw Puzzles Provides Good Pretraining for 2D Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.5</span></a> and <a href="#S4.SS2" title="4.2 Finetuning on 2D human keypoint labels ‣ 4 Experiments ‣ Learning Heatmap-Style Jigsaw Puzzles Provides Good Pretraining for 2D Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>. As Table <a href="#S5.T4.st1" title="In Table 3 ‣ 5.1 How many epochs under which learning rate? ‣ 5 Ablation Studies ‣ Learning Heatmap-Style Jigsaw Puzzles Provides Good Pretraining for 2D Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(a)</span></a> shows, with the increment of <math id="S5.SS2.p2.3.m3.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S5.SS2.p2.3.m3.1a"><mi id="S5.SS2.p2.3.m3.1.1" xref="S5.SS2.p2.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.3.m3.1b"><ci id="S5.SS2.p2.3.m3.1.1.cmml" xref="S5.SS2.p2.3.m3.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.3.m3.1c">N</annotation></semantics></math>, the mAP score for downstream 2D human pose estimator firstly increases, and then decreases. However, the HSJP precision decreases with the increment of <math id="S5.SS2.p2.4.m4.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S5.SS2.p2.4.m4.1a"><mi id="S5.SS2.p2.4.m4.1.1" xref="S5.SS2.p2.4.m4.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.4.m4.1b"><ci id="S5.SS2.p2.4.m4.1.1.cmml" xref="S5.SS2.p2.4.m4.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.4.m4.1c">N</annotation></semantics></math>, because the complexity of jigsaw puzzling problem also increases. When <math id="S5.SS2.p2.5.m5.1" class="ltx_Math" alttext="N=6" display="inline"><semantics id="S5.SS2.p2.5.m5.1a"><mrow id="S5.SS2.p2.5.m5.1.1" xref="S5.SS2.p2.5.m5.1.1.cmml"><mi id="S5.SS2.p2.5.m5.1.1.2" xref="S5.SS2.p2.5.m5.1.1.2.cmml">N</mi><mo id="S5.SS2.p2.5.m5.1.1.1" xref="S5.SS2.p2.5.m5.1.1.1.cmml">=</mo><mn id="S5.SS2.p2.5.m5.1.1.3" xref="S5.SS2.p2.5.m5.1.1.3.cmml">6</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.5.m5.1b"><apply id="S5.SS2.p2.5.m5.1.1.cmml" xref="S5.SS2.p2.5.m5.1.1"><eq id="S5.SS2.p2.5.m5.1.1.1.cmml" xref="S5.SS2.p2.5.m5.1.1.1"></eq><ci id="S5.SS2.p2.5.m5.1.1.2.cmml" xref="S5.SS2.p2.5.m5.1.1.2">𝑁</ci><cn type="integer" id="S5.SS2.p2.5.m5.1.1.3.cmml" xref="S5.SS2.p2.5.m5.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.5.m5.1c">N=6</annotation></semantics></math>, the convergence and the representation capability of pretext task are able to reach a balance: the pretraining process on HSJP pretext task does not collapse, and the network also learns high-quality representations for downstream 2D pose estimator (although it’s of low HSJP precision, our purpose is not to solve jigsaw puzzles more accurately, but to learn representations for downstream task). Therefore, we split our original input image of <math id="S5.SS2.p2.6.m6.1" class="ltx_Math" alttext="224\times 224" display="inline"><semantics id="S5.SS2.p2.6.m6.1a"><mrow id="S5.SS2.p2.6.m6.1.1" xref="S5.SS2.p2.6.m6.1.1.cmml"><mn id="S5.SS2.p2.6.m6.1.1.2" xref="S5.SS2.p2.6.m6.1.1.2.cmml">224</mn><mo lspace="0.222em" rspace="0.222em" id="S5.SS2.p2.6.m6.1.1.1" xref="S5.SS2.p2.6.m6.1.1.1.cmml">×</mo><mn id="S5.SS2.p2.6.m6.1.1.3" xref="S5.SS2.p2.6.m6.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.6.m6.1b"><apply id="S5.SS2.p2.6.m6.1.1.cmml" xref="S5.SS2.p2.6.m6.1.1"><times id="S5.SS2.p2.6.m6.1.1.1.cmml" xref="S5.SS2.p2.6.m6.1.1.1"></times><cn type="integer" id="S5.SS2.p2.6.m6.1.1.2.cmml" xref="S5.SS2.p2.6.m6.1.1.2">224</cn><cn type="integer" id="S5.SS2.p2.6.m6.1.1.3.cmml" xref="S5.SS2.p2.6.m6.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.6.m6.1c">224\times 224</annotation></semantics></math> into <math id="S5.SS2.p2.7.m7.1" class="ltx_Math" alttext="6\times 6" display="inline"><semantics id="S5.SS2.p2.7.m7.1a"><mrow id="S5.SS2.p2.7.m7.1.1" xref="S5.SS2.p2.7.m7.1.1.cmml"><mn id="S5.SS2.p2.7.m7.1.1.2" xref="S5.SS2.p2.7.m7.1.1.2.cmml">6</mn><mo lspace="0.222em" rspace="0.222em" id="S5.SS2.p2.7.m7.1.1.1" xref="S5.SS2.p2.7.m7.1.1.1.cmml">×</mo><mn id="S5.SS2.p2.7.m7.1.1.3" xref="S5.SS2.p2.7.m7.1.1.3.cmml">6</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.7.m7.1b"><apply id="S5.SS2.p2.7.m7.1.1.cmml" xref="S5.SS2.p2.7.m7.1.1"><times id="S5.SS2.p2.7.m7.1.1.1.cmml" xref="S5.SS2.p2.7.m7.1.1.1"></times><cn type="integer" id="S5.SS2.p2.7.m7.1.1.2.cmml" xref="S5.SS2.p2.7.m7.1.1.2">6</cn><cn type="integer" id="S5.SS2.p2.7.m7.1.1.3.cmml" xref="S5.SS2.p2.7.m7.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.7.m7.1c">6\times 6</annotation></semantics></math> image patches.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Whether using unshuffled image in pretext?</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">Similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, we consider concatenating original unshuffled person image to the input of HSJP pretext, as is shown in Figure <a href="#S5.F7" title="Figure 7 ‣ 5.3 Whether using unshuffled image in pretext? ‣ 5 Ablation Studies ‣ Learning Heatmap-Style Jigsaw Puzzles Provides Good Pretraining for 2D Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, for two reasons: (1) Taking unshuffled image as input makes the pretraining process similar to the downstream task of 2D human pose estimation, which also takes original person instance as input; (2) Concatenating original image helps to improve the precision of solving HSJP, because it provides a comparison for localising the initial positions of shuffled image patches.</p>
</div>
<figure id="S5.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F7.sf1" class="ltx_figure ltx_figure_panel">
<p id="S5.F7.sf1.1" class="ltx_p"><span id="S5.F7.sf1.1.1" class="ltx_text" style="position:relative; bottom:0.4pt;"><img src="/html/2012.07101/assets/x11.png" id="S5.F7.sf1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="103" height="46" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F7.sf1.3.1.1" class="ltx_text" style="font-size:80%;">(a)</span> </span><span id="S5.F7.sf1.4.2" class="ltx_text" style="font-size:80%;">(a) Using Shuffled Patches Only.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F7.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/2012.07101/assets/x12.png" id="S5.F7.sf2.g1" class="ltx_graphics ltx_img_square" width="103" height="87" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F7.sf2.2.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span><span id="S5.F7.sf2.3.2" class="ltx_text" style="font-size:80%;">(b) Adding Original Person Image.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Whether using original person image in HSJP.</figcaption>
</figure>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">As Figure <a href="#S5.T3" title="Table 3 ‣ 5.1 How many epochs under which learning rate? ‣ 5 Ablation Studies ‣ Learning Heatmap-Style Jigsaw Puzzles Provides Good Pretraining for 2D Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> demonstrates, we compare our performance of pretext (HSJP) and downstream (2D pose estimation) tasks under different settings: whether adding unshuffled image into input. Our experiments are also performed on HRNet W32. Unfortunately, although concatenating unshuffled image benefits the HSJP precision for pretext, it does not improve the performance of pose estimation downstream task: the mAP score is even worse than almost all weights pretrained without taking unshuffled images as inputs, and the performance is not better than training directly from scratch for 450 epochs. (In our scheme, the pretext HSJP task is trained for 240 epoch, the downstream task is trained for 210 epochs, so we try to train for 240+210=450 epochs from scratch for fair comparison.) Therefore, we do not concatenate the original image into input during HSJP pretraining, even if it seems to be reasonable.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this paper, we propose self-supervised Heatmap-Style Jigsaw Puzzles (HSJP) problem as pretext task, for the pretraining of 2D human pose estimators. The target of HSJP is to learn the location of shuffled patches with a heatmap-based approach. We only use images of person instances in MS-COCO dataset during our HSJP pretraining, rather than introducing extra and much larger dataset, such as ImageNet. The weights learned by HSJP pretext task are utilised as backbones of 2D human pose estimator, which are then finetuned on MS-COCO human keypoints labels. Our downstream mAP scores are evaluated on two popular and strong 2D human pose estimators, HRNet and SimpleBaseline, and experimental results show that downstream pose estimators with our self-supervised pretraining obtain much better performance than those trained from scratch, and are comparable to those using ImageNet classification models as their initial backbones. However, we don’t use any supervised data during our pretraining with HSJP pretext task, which saves the labour costs of labelling extra datasets (such as ImageNet) for pretraining.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">For future works, our pretraining scheme with HSJP pretext task has potential to be used on other heatmap-based or pixel-labelling tasks, such as face keypoint detection, crowd counting, and semantic segmentation.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Wenjia Bai, Chen Chen, Giacomo Tarroni, Jinming Duan, Florian Guitton,
Steffen E. Petersen, Yike Guo, Paul M. Matthews, and Daniel Rueckert.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Self-supervised learning for cardiac MR image segmentation by
anatomical position prediction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">MICCAI</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Dov Bridger, Dov Danon, and Ayellet Tal.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Solving jigsaw puzzles with eroded boundaries.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Yuanhao Cai, Zhicheng Wang, Zhengxiong Luo, Binyi Yin, Angang Du, Haoqian Wang,
Xinyu Zhou, Erjin Zhou, Xiangyu Zhang, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Learning delicate local representations for multi-person pose
estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib3.4.2" class="ltx_text" style="font-size:90%;">, abs/2003.04030, 2020.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Realtime multi-person 2d pose estimation using part affinity fields.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Maria Fabio Carlucci, Antonio D’Innocente, Silvia Bucci, Barbara Caputo, and
Tatiana Tommasi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Domain generalization by solving jigsaw puzzles.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey E.
Hinton.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Big self-supervised models are strong semi-supervised learners.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib6.4.2" class="ltx_text" style="font-size:90%;">, abs/2006.10029, 2020.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Tianlong Chen, Sijia Liu, Shiyu Chang, Yu Cheng, Lisa Amini, and Zhangyang
Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Adversarial robustness: From self-supervised pre-training to
fine-tuning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Yilun Chen, Zhicheng Wang, Yuxiang Peng, Zhiqiang Zhang, Gang Yu, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Cascaded pyramid network for multi-person pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Taeg Sang Cho, Shai Avidan, and William T. Freeman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">A probabilistic image jigsaw puzzle solver.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, 2010.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Imagenet: A large-scale hierarchical image database.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, 2009.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Carl Doersch, Abhinav Gupta, and Alexei A. Efros.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Unsupervised visual representation learning by context prediction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, December 2015.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Alexey Dosovitskiy, Jost Tobias Springenberg, Martin A. Riedmiller, and Thomas
Brox.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Discriminative unsupervised feature learning with convolutional
neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Richard Durstenfeld.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Algorithm 235: Random permutation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Commun. ACM</span><span id="bib.bib13.4.2" class="ltx_text" style="font-size:90%;">, 7:420, 1964.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
H. Freeman and L. Garder.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Apictorial jigsaw puzzles: The computer solution of a problem in
pattern recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Electronic Computers</span><span id="bib.bib14.4.2" class="ltx_text" style="font-size:90%;">, EC-13(2):118–127,
1964.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Andrew C. Gallagher.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Jigsaw puzzles with pieces of unknown orientation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, 2012.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec,
Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Ávila
Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray
Kavukcuoglu, Rémi Munos, and Michal Valko.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Bootstrap your own latent: A new approach to self-supervised
learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib16.4.2" class="ltx_text" style="font-size:90%;">, abs/2006.07733, 2020.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Momentum contrast for unsupervised visual representation learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Junjie Huang, Zheng Zhu, Feng Guo, and Guan Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">The devil is in the details: Delving into unbiased data processing
for human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Umar Iqbal, Anton Milan, and Juergen Gall.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Posetrack: Joint multi-person pose estimation and tracking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Lipeng Ke, Ming-Ching Chang, Honggang Qi, and Siwei Lyu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Multi-scale structure-aware network for human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Diederik P. Kingma and Jimmy Ba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Adam: A method for stochastic optimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
E. Donald Knuth.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">The art of computer programming, volume 2 (3rd ed.): seminumerical
algorithms.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">1997.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
David A. Kosiba, Pierre M. Devaux, Sanjay Balasubramanian, Tarak Gandhi, and
Rangachar Kasturi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">An automatic jigsaw puzzle solver.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICPR</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, 1994.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Sven Kreiss, Lorenzo Bertoni, and Alexandre Alahi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Pifpaf: Composite fields for human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Gustav Larsson, Michael Maire, and Gregory Shakhnarovich.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Learning representations for automatic colorization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Wenbo Li, Zhicheng Wang, Binyi Yin, Qixiang Peng, Yuming Du, Tianzi Xiao, Gang
Yu, Hongtao Lu, Yichen Wei, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Rethinking on multi-stage networks for human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib27.4.2" class="ltx_text" style="font-size:90%;">, abs/1901.00148, 2019.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona,
Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Microsoft COCO: common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Alejandro Newell and Jia Deng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">How useful is self-supervised pretraining for visual tasks?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Alejandro Newell, Kaiyu Yang, and Jia Deng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Stacked hourglass networks for human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Xuecheng Nie, Jiashi Feng, Jianfeng Zhang, and Shuicheng Yan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Single-stage multi-person pose machines.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Mehdi Noroozi and Paolo Favaro.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Unsupervised learning of visual representations by solving jigsaw
puzzles.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A.
Efros.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Context encoders: Feature learning by inpainting.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Marie-Morgane Paumard, David Picard, and Hedi Tabia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Image reassembly combining deep learning and shortest path problem.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Faster R-CNN: towards real-time object detection with region
proposal networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Trans. Pattern Anal. Mach. Intell.</span><span id="bib.bib35.4.2" class="ltx_text" style="font-size:90%;">, 39(6):1137–1149,
2017.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
T. E. John Richardson and Tomaso Vecchi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">A jigsaw-puzzle imagery task for assessing active visuospatial
processes in old and young people.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Behavior research methods, instruments, and computers</span><span id="bib.bib36.4.2" class="ltx_text" style="font-size:90%;">, pages
69–82, 2002.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Matteo Ruggero Ronchi and Pietro Perona.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Benchmarking and error diagnosis in multi-instance pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib37.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib37.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Kilho Son, James Hays, and David B. Cooper.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Solving square jigsaw puzzles with loop constraints.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib38.5.3" class="ltx_text" style="font-size:90%;">, Lecture Notes in Computer Science, 2014.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Kilho Son, Daniel Moreno, James Hays, and David B. Cooper.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">Solving small-piece jigsaw puzzles by growing consensus.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib39.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib39.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Chi Su, Jianing Li, Shiliang Zhang, Junliang Xing, Wen Gao, and Qi Tian.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Pose-driven deep convolutional model for person re-identification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib40.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib40.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Kai Su, Dongdong Yu, Zhenqi Xu, Xin Geng, and Changhu Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">Multi-person pose estimation with enhanced channel-wise and spatial
information.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib41.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib41.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Deep high-resolution representation learning for human pose
estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib42.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib42.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Zhi Tian, Hao Chen, and Chunhua Shen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Directpose: Direct end-to-end multi-person pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib43.4.2" class="ltx_text" style="font-size:90%;">, abs/1911.07451, 2019.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Jonathan Tompson, Arjun Jain, Yann LeCun, and Christoph Bregler.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Joint training of a convolutional network and a graphical model for
human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib44.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NIPS</span><span id="bib.bib44.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Alexander Toshev and Christian Szegedy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">Deeppose: Human pose estimation via deep neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib45.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib45.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Carl Vondrick, Abhinav Shrivastava, Alireza Fathi, Sergio Guadarrama, and Kevin
Murphy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">Tracking emerges by colorizing videos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib46.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib46.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Xiaolong Wang and Abhinav Gupta.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">Unsupervised learning of visual representations using videos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib47.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib47.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
Yude Wang, Jie Zhang, Meina Kan, Shiguang Shan, and Xilin Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">Self-supervised equivariant attention mechanism for weakly supervised
semantic segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib48.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib48.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
Bin Xiao, Haiping Wu, and Yichen Wei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">Simple baselines for human pose estimation and tracking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib49.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib49.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
Sijie Yan, Yuanjun Xiong, and Dahua Lin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">Spatial temporal graph convolutional networks for skeleton-based
action recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib50.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">AAAI</span><span id="bib.bib50.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text" style="font-size:90%;">
Chuanguang Yang, Zhulin An, Hui Zhu, Xiaolong Hu, Kun Zhang, Kaiqiang Xu, Chao
Li, and Yongjun Xu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.2.1" class="ltx_text" style="font-size:90%;">Gated convolutional networks with hybrid connectivity for image
classification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib51.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">AAAI</span><span id="bib.bib51.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text" style="font-size:90%;">
Mang Ye, Xu Zhang, Pong C. Yuen, and Shih-Fu Chang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.2.1" class="ltx_text" style="font-size:90%;">Unsupervised embedding learning via invariant and spreading instance
feature.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib52.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib52.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text" style="font-size:90%;">
Dong Zhang and Mubarak Shah.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.2.1" class="ltx_text" style="font-size:90%;">Human pose estimation in videos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib53.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib53.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text" style="font-size:90%;">
Feng Zhang, Xiatian Zhu, Hanbin Dai, Mao Ye, and Ce Zhu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.2.1" class="ltx_text" style="font-size:90%;">Distribution-aware coordinate representation for human pose
estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib54.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib54.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text" style="font-size:90%;">
Richard Zhang, Phillip Isola, and A. Alexei Efros.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.2.1" class="ltx_text" style="font-size:90%;">Colorful image colorization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib55.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib55.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
</ul>
</section>
<figure id="id16" class="ltx_figure"><img src="/html/2012.07101/assets/x13.png" id="id16.g1" class="ltx_graphics ltx_img_square" width="386" height="470" alt="[Uncaptioned image]">
</figure>
<figure id="id17" class="ltx_figure"><img src="/html/2012.07101/assets/x14.png" id="id17.g1" class="ltx_graphics ltx_img_portrait" width="387" height="488" alt="[Uncaptioned image]">
</figure>
<figure id="id18" class="ltx_figure"><img src="/html/2012.07101/assets/x15.png" id="id18.g1" class="ltx_graphics ltx_img_square" width="385" height="322" alt="[Uncaptioned image]">
</figure>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2012.07100" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2012.07101" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2012.07101">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2012.07101" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2012.07102" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  9 04:51:03 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
