<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2304.02186] Training Strategies for Vision Transformers for Object Detection</title><meta property="og:description" content="Vision-based Transformer have shown huge application in the perception module of autonomous driving in terms of predicting accurate 3D bounding boxes, owing to their strong capability in modeling long-range dependencie…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Training Strategies for Vision Transformers for Object Detection">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Training Strategies for Vision Transformers for Object Detection">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2304.02186">

<!--Generated on Thu Feb 29 16:21:53 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Training Strategies for Vision Transformers for Object Detection</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Apoorv Singh
<br class="ltx_break">Motional
<br class="ltx_break">USA
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">apoorv.singh@motional.com</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Vision-based Transformer have shown huge application in the perception module of autonomous driving in terms of predicting accurate 3D bounding boxes, owing to their strong capability in modeling long-range dependencies between the visual features. However Transformers, initially designed for language models, have mostly focused on the performance accuracy, and not so much on the inference-time budget. For a safety critical system like autonomous driving, real-time inference at the on-board compute is an absolute necessity. This keeps our object detection algorithm under a very tight run-time budget. In this paper, we evaluated a variety of strategies to optimize on the inference-time of vision transformers based object detection methods keeping a close-watch on any performance variations. Our chosen metric for these strategies is accuracy-runtime joint optimization. Moreover, for actual inference-time analysis we profile our strategies with float32 and float16 precision with TensorRT module. This is the most common format used by the industry for deployment of their Machine Learning networks on the edge devices. We showed that our strategies are able to improve inference-time by 63% at the cost of performance drop of mere 3% for our problem-statement defined in <a href="#S3" title="3 Evaluation Criteria ‣ Training Strategies for Vision Transformers for Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3</span></a>. These strategies brings down Vision Transformers detectors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> inference-time even less than traditional single-image based CNN detectors like FCOS<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. We recommend practitioners use these techniques to deploy Transformers based hefty multi-view networks on a budge-constrained robotic platform.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure"><img src="/html/2304.02186/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="230" height="150" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">A standard multi-view vision based detector <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> with CNN based backbone and FPN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> and transformers based detection head in gray-box. Inputs: Multi-view images, camera transformation matrices. Outputs: 3D bounding-boxes.</span></figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In the last decade, Convolution Neural Networks (CNNs) was driven by the model architectural updates <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> in the field of computer vision. Moreover there have been a plethora of techniques proposed to improve training strategies of these CNN models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Recently, Vision Transformers, first introduced by ViT<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, and iteratively reformulated by DETR-based approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> has emerged as the better alternative architecture for object detection with images. However, the literature and leader-boards of the Transformers’ object-detection community tends to focus most on the architectural impact of these hefty models. When these methods are to be used on an actual robotic-platform, runtime-accuracy joint-optimization is what that matters the most, owing to the fact that any edge-device would have a limited compute-budget. Moreover these ML algorithms have to be operated at high frequency as autonomous cars move fairly fast and need to update their road and dynamic agents’ understanding at at-least 10hz (10 times a second). Any top-performing method from a detection leader-board <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, would most likely be based out of ensemble approach, which is purely impractical on a compute-budget constrained on-board device. Performance of any machine learning model is dependent on three things: 1) Architecture; 2) Training Strategies; 3) Inference-time budget. This work, unlike others, focuses primarily on the later two: <em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">Training strategies</em> and <em id="S1.p1.1.2" class="ltx_emph ltx_font_italic">Inference-time budget</em>. We conclude that with the right network down-scaling strategies we can make Vision Transformers-based detectors practically capable to be fitted onto a deployment platform that operates within the inference-time budget and at high frequency. Transformers, which was initially inspired from language models, had less focus on inference-time bottlenecks early on as most of the processing of language models occurs on cloud the servers for example with ChatGPT. Autonomous car, an extremely safety critical system, has to perform its computation on-board device as we can not live-stream video data from cameras and laser sensors to a cloud servers; have it processed there; and sequentially download the processed predictions back to the device; in addition of doing all that within our real-time operation constraint.</p>
</div>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Motivation</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">Performance improvements not only come from the novel architectures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> but also from the modern scaling strategies of the Vision Transformers network. We noted that this is specially the case for inference-time improvements, as more researchers tend to focus on accuracy compared to inference-time optimizations if at all. Our work is to carefully analyze inference-time optimization strategies that may eventually lead to a Vision-based transformers models practically deploy-able on an autonomous vehicle platforms.</p>
</div>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2 </span>Contribution</h3>

<div id="S1.SS2.p1" class="ltx_para">
<p id="S1.SS2.p1.1" class="ltx_p">Our contributions can be summarized as below:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We identify modifications in Vision Transformer’s architecture of the current State-of-the-art (SoTA) object detection algorithms that lead to better runtime-accuracy optimization. We also list findings on model-precision and training schedule involved in this analysis. As a secondary check of the model we perform MACs (#operations) analysis to add to the third dimension over inference-time and performance trade-off.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We design post-processing and pre-processing strategies that lead to runtime-performance improvements for Vision Transformers models.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We design an effective evaluation strategy for an on-car detectors that is able to cater all our accuracy needs and inference-time constraints. We extend inference-time analysis one step further and evaluate the inference-time numbers on a <em id="S1.I1.i3.p1.1.1" class="ltx_emph ltx_font_italic">float16</em> and <em id="S1.I1.i3.p1.1.2" class="ltx_emph ltx_font_italic">float32</em> model precision in <em id="S1.I1.i3.p1.1.3" class="ltx_emph ltx_font_italic">TensorRT</em> model format.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">We also discuss further extension ideas for this work that researchers may focus on to further optimize on runtime-accuracy optimization for these models. We link those ideas with the representative work too, bridging a relation from other fields.</p>
</div>
<div id="S1.I1.i4.p2" class="ltx_para">
<p id="S1.I1.i4.p2.1" class="ltx_p">Based on our empirical results we claim that carefully adjusting input-image resolution and Transformer-decoder layer embedding space and other parameters can bring down network inference-time down by 63% with just minimal drop in performance. We evaluate all the results on our in-house dataset which in terms of representation can be compared to any public dataset like nuScenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> or Waymo Open Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> but much larger with more diverse classes and scenarios, or in other words much more difficult one too. By adopting proposed strategies, we were able to get inference-time for Vision Transformers detector models close to or in-fact even better than a lot of traditional CNN-based detectors, keeping the detection performance at par with the superior Vision Transformers models. To the author’s knowledge this is the first paper which does runtime-accuracy scaling strategies for Vision Transformers models. Moreover we do inference-time analysis on <em id="S1.I1.i4.p2.1.1" class="ltx_emph ltx_font_italic">TensorRT</em> model format along with MACs analysis.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.3 </span>Previous Work</h3>

<div id="S1.SS3.p1" class="ltx_para">
<p id="S1.SS3.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> paper highlights interesting strategies on model-scaling for CNN models with respect to speed-accuracy Pareto curve. However this analysis lacked runtime analysis on the <em id="S1.SS3.p1.1.1" class="ltx_emph ltx_font_italic">TensorRT</em> model. Moreover this paper focused more on CNN based backbone and not the Transformer based <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> module which tend to be SoTA detectors in the current literature. 
<br class="ltx_break">
<br class="ltx_break"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> focuses on the ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> based backbone. They only evaluated their models on a single-image based classification task, ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, which is a fairly simple task compared to multi-view 3D object detection tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. Hence a lot of learnings from here might not be valid for on-car deployment of these Transformers based networks. 
<br class="ltx_break">
<br class="ltx_break"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> focuses on classification performance with <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> approach, which handled the problem end to end with transformers by dividing patches into 16x16 grids <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. This has proven to be a slower and sub-optimal approach for the much more complex problem of multi-view object detection problems. Modern approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> have proved to better for this task as per the detection-leaderboard . In addition, this approach focuses on accuracy-training time pareto curve, but our focus is on accuracy-runtime or in other words accuracy-inference time of the network, which is a bigger problem statement in the autonomous vehicle industry. 
<br class="ltx_break">
<br class="ltx_break">While <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> focuses on Transformers but for the application of neural machine translation, which is a significantly different problem compared to the multi-view 3D object detection problem that we are trying to solve with Vision Transformers. 
<br class="ltx_break">
<br class="ltx_break"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> analysis is probably the most related to our studies however, it is yet different in multiple aspects. Firstly, this scaling studies again focuses on the single-image based classification task of ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> and does not have any mobile robot application. This paper also focuses on ViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> based approach, which is an end-to-end transformers based network and not DETR based approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> that have recently proven better track record in doing 3D object detection for complex datasets of autonomous driving with multi-view cameras. 
<br class="ltx_break">
<br class="ltx_break">In contrast to the other work, we focus on down-scaling of model with the close-watch on model’s performance degradation focused on the Vision Transformers models that are based on CNNs for extracting image features and transformers for detection head to predict boxes in the Bird’s Eye View space. Other transformers work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> primarily focuses on the architectural impact of their approach, however, we focus on training strategies that lead to joint optimization of performance and run-time thereby making these networks deploy-able on an autonomous vehicle under run-time and compute constraint.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Object Detection Methods</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section we will cover different strategies used in object detection starting off with <em id="S2.p1.1.1" class="ltx_emph ltx_font_italic">Single-image Based Detection</em> and then focusing majorly on <em id="S2.p1.1.2" class="ltx_emph ltx_font_italic">Multi-image Based Detection</em>.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Single-image Based Detection</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Single-image based object detection can be divided into <em id="S2.SS1.p1.1.1" class="ltx_emph ltx_font_italic">two-stage</em>, <em id="S2.SS1.p1.1.2" class="ltx_emph ltx_font_italic">single-stage</em> and <em id="S2.SS1.p1.1.3" class="ltx_emph ltx_font_italic">set-based</em> detectors in terms of chronological invention of these detectors. Two-stage detectors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> are a class of detectors that are divided into two stages. First stage is to predict arbitrary number of object proposals, and then in second stage they predict boxes by classifying and localizing those object proposals. However, these models suffers through high inference-time bottleneck because of the two-stage nature with redundant computations. Then came the single-stage detectors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> which brings down the inference-time of two-stage detectors. These models either uses heuristics based anchor boxes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> or center-based heatmap on features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> to make predictions. However these approaches still relies on limited receptive field associated with the CNNs and fail to develop long-range relations between the image features. This becomes especially the concern for multi-image detection problem in autonomous driving where 6-8 cameras are used to capture the entire <math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="360^{\circ}" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><msup id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml"><mn id="S2.SS1.p1.1.m1.1.1.2" xref="S2.SS1.p1.1.m1.1.1.2.cmml">360</mn><mo id="S2.SS1.p1.1.m1.1.1.3" xref="S2.SS1.p1.1.m1.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><apply id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.1.m1.1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">superscript</csymbol><cn type="integer" id="S2.SS1.p1.1.m1.1.1.2.cmml" xref="S2.SS1.p1.1.m1.1.1.2">360</cn><compose id="S2.SS1.p1.1.m1.1.1.3.cmml" xref="S2.SS1.p1.1.m1.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">360^{\circ}</annotation></semantics></math> surrounding scene. Next, researchers became interested in bringing transformer-based architecture leanings from language models to the computer vision task. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> reformulated the entire CNN-based problem to transformers based problem. This architecture has transformers based encoder a.k.a backbone as well as transformers based detection head. It suffered through inference-time constraints as attention module was applied to even very low-level pixels information and hence shooting up the computation requirement. However, some strategies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> have been explored on accuracy-runtime pareto curve, but it only focused on relatively easier image classification problem. Subsequently most recent detection work has been focused on CNN-based backbone and transformers based detection head in DETR-based approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>. These approaches were claimed to be much superior in making sparse predictions in the scene by leveraging self-reasoning of the object-proposals in self-attention layer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. These object-proposals are later refined as a set of predictions of boxes. This approach has also extended its way to the multi-view object detection problem as discussed in the next section.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Multi-image Based Detection</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">For a lot of robotics applications including autonomous driving, perception module needs to make prediction for the entire <math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="360^{\circ}" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><msup id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml"><mn id="S2.SS2.p1.1.m1.1.1.2" xref="S2.SS2.p1.1.m1.1.1.2.cmml">360</mn><mo id="S2.SS2.p1.1.m1.1.1.3" xref="S2.SS2.p1.1.m1.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><apply id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.1.m1.1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">superscript</csymbol><cn type="integer" id="S2.SS2.p1.1.m1.1.1.2.cmml" xref="S2.SS2.p1.1.m1.1.1.2">360</cn><compose id="S2.SS2.p1.1.m1.1.1.3.cmml" xref="S2.SS2.p1.1.m1.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">360^{\circ}</annotation></semantics></math> scene <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, and that too in the Bird’s eye view (BEV) so that it can be easily consumed by the other autonomy’s software module like path-prediction and path-planning. Until the last few years, multi-view based detection problem was handled using per-view Machine learning based methods and then all the detections were consolidated using heuristic methods of data association using IoU (Intersection over Union). This post-processing step merges duplicate detections coming from the adjacent camera-views. Recently a lot of work have been explored in learning end-to-end differentiable network for multi-view detection. These approaches can be broadly classified as per <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> by (1) <em id="S2.SS2.p1.1.1" class="ltx_emph ltx_font_italic">Geometry based view transformers</em> and (2) <em id="S2.SS2.p1.1.2" class="ltx_emph ltx_font_italic">Cross-attention based vision-transformer</em>. Geometry based models are typically based out of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> approach, which as an intermediate step creates BEV pseudo point-cloud by discretizing depth for each pixel and then runs a LiDAR based detector <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> on the pseudo point-cloud frustum from all the cameras. Due to the inherent two-stage process these models tend to have huge inference-time, despite being entirely based out of CNNs. Cross-attention based vision-transformer based approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> have shown to be SoTA in the leader-boards <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. These approaches typically have CNN based backbone and transformers based head which feeds in queries and CNN features to predict 3D bounding boxes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> have focused their work on sparse-queries that are either learned from the training-dataset or constructed using input-data, whereas <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> focuses on dense queries in the BEV map. For the scope of this paper we focus our methods on sparse-query based approaches which are much more optimal in terms of inference-time. Swin-transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> has explored in converting CNN based backbone to transformers based one using hierarchical Transformer whose representation is computed with shifted windows.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2304.02186/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="230" height="179" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S2.F2.3.2" class="ltx_text" style="font-size:90%;">Input-Output of the Problem Statement. Surround-view 8 camera images (top); LiDAR Point Cloud overlayed over an HD Map (bottom). Key: Green points: LiDAR point cloud; Pink box: Autonomous vehicle; Black-white map: Pre-computed HD map.</span></figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Evaluation Criteria</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section we discuss different fronts used for comparative analysis for different models viz., <em id="S3.p1.1.1" class="ltx_emph ltx_font_italic">Accuracy</em>, <em id="S3.p1.1.2" class="ltx_emph ltx_font_italic">Inference-time a.k.a run-time</em> and few less used ones: <em id="S3.p1.1.3" class="ltx_emph ltx_font_italic">Multiply–accumulate operation (MACs)</em> and Number of Parameters.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Model Accuracy</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">3D object detectors use multiple criteria to measure performance of the detectors viz., precision and recall. However, mean Average Precision (mAP) is the most common evaluation metric. Intersection over Union (IoU) is the ratio of the area of overlap and area of the union between the predicted box and ground-truth box. An IoU threshold value (generally 0.5) is used to judge if a prediction box matches with any particular ground-truth box. If IoU is greater than the threshold, then that prediction is treated as a True Positive (TP) else it is a False Positive (FP). A ground-truth object which fails to detect with any prediction box, is treated as a False Negative (FN). Precision is the fraction of relevant instances among the retrieved instances; while recall is the fraction of relevant instances that were retrieved.</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="Precision=TP/(TP+FP)" display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><mrow id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.1" xref="S3.E1.m1.1.1.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.3.3" xref="S3.E1.m1.1.1.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.1a" xref="S3.E1.m1.1.1.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.3.4" xref="S3.E1.m1.1.1.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.1b" xref="S3.E1.m1.1.1.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.3.5" xref="S3.E1.m1.1.1.3.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.1c" xref="S3.E1.m1.1.1.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.3.6" xref="S3.E1.m1.1.1.3.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.1d" xref="S3.E1.m1.1.1.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.3.7" xref="S3.E1.m1.1.1.3.7.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.1e" xref="S3.E1.m1.1.1.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.3.8" xref="S3.E1.m1.1.1.3.8.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.1f" xref="S3.E1.m1.1.1.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.3.9" xref="S3.E1.m1.1.1.3.9.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.1g" xref="S3.E1.m1.1.1.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.3.10" xref="S3.E1.m1.1.1.3.10.cmml">n</mi></mrow><mo id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.3" xref="S3.E1.m1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.3.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.3.1" xref="S3.E1.m1.1.1.1.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.3.3.cmml">P</mi></mrow><mo id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.2.cmml">/</mo><mrow id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.1.1.2.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.1.2.1" xref="S3.E1.m1.1.1.1.1.1.1.2.1.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.1.1.2.3.cmml">P</mi></mrow><mo id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">+</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.1.3.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.1.3.1" xref="S3.E1.m1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.1.3.3.cmml">P</mi></mrow></mrow><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><eq id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2"></eq><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><times id="S3.E1.m1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.3.1"></times><ci id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2">𝑃</ci><ci id="S3.E1.m1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.3.3">𝑟</ci><ci id="S3.E1.m1.1.1.3.4.cmml" xref="S3.E1.m1.1.1.3.4">𝑒</ci><ci id="S3.E1.m1.1.1.3.5.cmml" xref="S3.E1.m1.1.1.3.5">𝑐</ci><ci id="S3.E1.m1.1.1.3.6.cmml" xref="S3.E1.m1.1.1.3.6">𝑖</ci><ci id="S3.E1.m1.1.1.3.7.cmml" xref="S3.E1.m1.1.1.3.7">𝑠</ci><ci id="S3.E1.m1.1.1.3.8.cmml" xref="S3.E1.m1.1.1.3.8">𝑖</ci><ci id="S3.E1.m1.1.1.3.9.cmml" xref="S3.E1.m1.1.1.3.9">𝑜</ci><ci id="S3.E1.m1.1.1.3.10.cmml" xref="S3.E1.m1.1.1.3.10">𝑛</ci></apply><apply id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><divide id="S3.E1.m1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.2"></divide><apply id="S3.E1.m1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.3"><times id="S3.E1.m1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.3.1"></times><ci id="S3.E1.m1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.3.2">𝑇</ci><ci id="S3.E1.m1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.3.3">𝑃</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"><plus id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1"></plus><apply id="S3.E1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2"><times id="S3.E1.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.1"></times><ci id="S3.E1.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.2">𝑇</ci><ci id="S3.E1.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.3">𝑃</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3"><times id="S3.E1.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.1"></times><ci id="S3.E1.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.2">𝐹</ci><ci id="S3.E1.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.3">𝑃</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">Precision=TP/(TP+FP)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.1" class="ltx_Math" alttext="Recall=TP/(TP+FN)" display="block"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><mrow id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.3.2" xref="S3.E2.m1.1.1.3.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.1" xref="S3.E2.m1.1.1.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.3.3" xref="S3.E2.m1.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.1a" xref="S3.E2.m1.1.1.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.3.4" xref="S3.E2.m1.1.1.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.1b" xref="S3.E2.m1.1.1.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.3.5" xref="S3.E2.m1.1.1.3.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.1c" xref="S3.E2.m1.1.1.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.3.6" xref="S3.E2.m1.1.1.3.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.1d" xref="S3.E2.m1.1.1.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.3.7" xref="S3.E2.m1.1.1.3.7.cmml">l</mi></mrow><mo id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.3" xref="S3.E2.m1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.3.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.3.1" xref="S3.E2.m1.1.1.1.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.3.3.cmml">P</mi></mrow><mo id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.2.cmml">/</mo><mrow id="S3.E2.m1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.2.2" xref="S3.E2.m1.1.1.1.1.1.1.2.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.2.1" xref="S3.E2.m1.1.1.1.1.1.1.2.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.1.1.2.3.cmml">P</mi></mrow><mo id="S3.E2.m1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml">+</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.1.1.3.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.3.1" xref="S3.E2.m1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.1.1.3.3.cmml">N</mi></mrow></mrow><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><eq id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2"></eq><apply id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3"><times id="S3.E2.m1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.3.1"></times><ci id="S3.E2.m1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.3.2">𝑅</ci><ci id="S3.E2.m1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.3.3">𝑒</ci><ci id="S3.E2.m1.1.1.3.4.cmml" xref="S3.E2.m1.1.1.3.4">𝑐</ci><ci id="S3.E2.m1.1.1.3.5.cmml" xref="S3.E2.m1.1.1.3.5">𝑎</ci><ci id="S3.E2.m1.1.1.3.6.cmml" xref="S3.E2.m1.1.1.3.6">𝑙</ci><ci id="S3.E2.m1.1.1.3.7.cmml" xref="S3.E2.m1.1.1.3.7">𝑙</ci></apply><apply id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><divide id="S3.E2.m1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.2"></divide><apply id="S3.E2.m1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.3"><times id="S3.E2.m1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.3.1"></times><ci id="S3.E2.m1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.3.2">𝑇</ci><ci id="S3.E2.m1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.3.3">𝑃</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1"><plus id="S3.E2.m1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1"></plus><apply id="S3.E2.m1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2"><times id="S3.E2.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.1"></times><ci id="S3.E2.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.2">𝑇</ci><ci id="S3.E2.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.3">𝑃</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3"><times id="S3.E2.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3.1"></times><ci id="S3.E2.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3.2">𝐹</ci><ci id="S3.E2.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3.3">𝑁</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">Recall=TP/(TP+FN)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p1.2" class="ltx_p">Based on the above equations, average precision is computed separately for each class. To compare performance between different detectors (mAP) is used. It is a weighted mean based on the number of ground-truths per class. Alternatively F1 score is the second most common detection metric, which is defined as weighted average of the precision and recall. Higher <em id="S3.SS1.p1.2.1" class="ltx_emph ltx_font_italic">AP</em> detectors gives better performance when the model is deployed at varied confidence threshold, however higher <em id="S3.SS1.p1.2.2" class="ltx_emph ltx_font_italic">max-F1</em> score detector is used when the model is to be deployed at a known fixed optimal-confidence threshold score.</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.1" class="ltx_Math" alttext="F1=2*Precision*Recall/(Precision+Recall)" display="block"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml"><mrow id="S3.E3.m1.1.1.3" xref="S3.E3.m1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.3.2" xref="S3.E3.m1.1.1.3.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.3.1" xref="S3.E3.m1.1.1.3.1.cmml">​</mo><mn id="S3.E3.m1.1.1.3.3" xref="S3.E3.m1.1.1.3.3.cmml">1</mn></mrow><mo id="S3.E3.m1.1.1.2" xref="S3.E3.m1.1.1.2.cmml">=</mo><mrow id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.3" xref="S3.E3.m1.1.1.1.3.cmml"><mrow id="S3.E3.m1.1.1.1.3.2" xref="S3.E3.m1.1.1.1.3.2.cmml"><mrow id="S3.E3.m1.1.1.1.3.2.2" xref="S3.E3.m1.1.1.1.3.2.2.cmml"><mrow id="S3.E3.m1.1.1.1.3.2.2.2" xref="S3.E3.m1.1.1.1.3.2.2.2.cmml"><mn id="S3.E3.m1.1.1.1.3.2.2.2.2" xref="S3.E3.m1.1.1.1.3.2.2.2.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S3.E3.m1.1.1.1.3.2.2.2.1" xref="S3.E3.m1.1.1.1.3.2.2.2.1.cmml">∗</mo><mi id="S3.E3.m1.1.1.1.3.2.2.2.3" xref="S3.E3.m1.1.1.1.3.2.2.2.3.cmml">P</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.3.2.2.1" xref="S3.E3.m1.1.1.1.3.2.2.1.cmml">​</mo><mi id="S3.E3.m1.1.1.1.3.2.2.3" xref="S3.E3.m1.1.1.1.3.2.2.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.3.2.2.1a" xref="S3.E3.m1.1.1.1.3.2.2.1.cmml">​</mo><mi id="S3.E3.m1.1.1.1.3.2.2.4" xref="S3.E3.m1.1.1.1.3.2.2.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.3.2.2.1b" xref="S3.E3.m1.1.1.1.3.2.2.1.cmml">​</mo><mi id="S3.E3.m1.1.1.1.3.2.2.5" xref="S3.E3.m1.1.1.1.3.2.2.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.3.2.2.1c" xref="S3.E3.m1.1.1.1.3.2.2.1.cmml">​</mo><mi id="S3.E3.m1.1.1.1.3.2.2.6" xref="S3.E3.m1.1.1.1.3.2.2.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.3.2.2.1d" xref="S3.E3.m1.1.1.1.3.2.2.1.cmml">​</mo><mi id="S3.E3.m1.1.1.1.3.2.2.7" xref="S3.E3.m1.1.1.1.3.2.2.7.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.3.2.2.1e" xref="S3.E3.m1.1.1.1.3.2.2.1.cmml">​</mo><mi id="S3.E3.m1.1.1.1.3.2.2.8" xref="S3.E3.m1.1.1.1.3.2.2.8.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.3.2.2.1f" xref="S3.E3.m1.1.1.1.3.2.2.1.cmml">​</mo><mi id="S3.E3.m1.1.1.1.3.2.2.9" xref="S3.E3.m1.1.1.1.3.2.2.9.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.3.2.2.1g" xref="S3.E3.m1.1.1.1.3.2.2.1.cmml">​</mo><mi id="S3.E3.m1.1.1.1.3.2.2.10" xref="S3.E3.m1.1.1.1.3.2.2.10.cmml">n</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S3.E3.m1.1.1.1.3.2.1" xref="S3.E3.m1.1.1.1.3.2.1.cmml">∗</mo><mi id="S3.E3.m1.1.1.1.3.2.3" xref="S3.E3.m1.1.1.1.3.2.3.cmml">R</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.3.1" xref="S3.E3.m1.1.1.1.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.1.3.3" xref="S3.E3.m1.1.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.3.1a" xref="S3.E3.m1.1.1.1.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.1.3.4" xref="S3.E3.m1.1.1.1.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.3.1b" xref="S3.E3.m1.1.1.1.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.1.3.5" xref="S3.E3.m1.1.1.1.3.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.3.1c" xref="S3.E3.m1.1.1.1.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.1.3.6" xref="S3.E3.m1.1.1.1.3.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.3.1d" xref="S3.E3.m1.1.1.1.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.1.3.7" xref="S3.E3.m1.1.1.1.3.7.cmml">l</mi></mrow><mo id="S3.E3.m1.1.1.1.2" xref="S3.E3.m1.1.1.1.2.cmml">/</mo><mrow id="S3.E3.m1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.2.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.2.2" xref="S3.E3.m1.1.1.1.1.1.1.2.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.2.1" xref="S3.E3.m1.1.1.1.1.1.1.2.1.cmml">​</mo><mi id="S3.E3.m1.1.1.1.1.1.1.2.3" xref="S3.E3.m1.1.1.1.1.1.1.2.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.2.1a" xref="S3.E3.m1.1.1.1.1.1.1.2.1.cmml">​</mo><mi id="S3.E3.m1.1.1.1.1.1.1.2.4" xref="S3.E3.m1.1.1.1.1.1.1.2.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.2.1b" xref="S3.E3.m1.1.1.1.1.1.1.2.1.cmml">​</mo><mi id="S3.E3.m1.1.1.1.1.1.1.2.5" xref="S3.E3.m1.1.1.1.1.1.1.2.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.2.1c" xref="S3.E3.m1.1.1.1.1.1.1.2.1.cmml">​</mo><mi id="S3.E3.m1.1.1.1.1.1.1.2.6" xref="S3.E3.m1.1.1.1.1.1.1.2.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.2.1d" xref="S3.E3.m1.1.1.1.1.1.1.2.1.cmml">​</mo><mi id="S3.E3.m1.1.1.1.1.1.1.2.7" xref="S3.E3.m1.1.1.1.1.1.1.2.7.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.2.1e" xref="S3.E3.m1.1.1.1.1.1.1.2.1.cmml">​</mo><mi id="S3.E3.m1.1.1.1.1.1.1.2.8" xref="S3.E3.m1.1.1.1.1.1.1.2.8.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.2.1f" xref="S3.E3.m1.1.1.1.1.1.1.2.1.cmml">​</mo><mi id="S3.E3.m1.1.1.1.1.1.1.2.9" xref="S3.E3.m1.1.1.1.1.1.1.2.9.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.2.1g" xref="S3.E3.m1.1.1.1.1.1.1.2.1.cmml">​</mo><mi id="S3.E3.m1.1.1.1.1.1.1.2.10" xref="S3.E3.m1.1.1.1.1.1.1.2.10.cmml">n</mi></mrow><mo id="S3.E3.m1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.cmml">+</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.3.2" xref="S3.E3.m1.1.1.1.1.1.1.3.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.3.1" xref="S3.E3.m1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.1.1.1.1.3.3" xref="S3.E3.m1.1.1.1.1.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.3.1a" xref="S3.E3.m1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.1.1.1.1.3.4" xref="S3.E3.m1.1.1.1.1.1.1.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.3.1b" xref="S3.E3.m1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.1.1.1.1.3.5" xref="S3.E3.m1.1.1.1.1.1.1.3.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.3.1c" xref="S3.E3.m1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.1.1.1.1.3.6" xref="S3.E3.m1.1.1.1.1.1.1.3.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.3.1d" xref="S3.E3.m1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.1.1.1.1.3.7" xref="S3.E3.m1.1.1.1.1.1.1.3.7.cmml">l</mi></mrow></mrow><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1"><eq id="S3.E3.m1.1.1.2.cmml" xref="S3.E3.m1.1.1.2"></eq><apply id="S3.E3.m1.1.1.3.cmml" xref="S3.E3.m1.1.1.3"><times id="S3.E3.m1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.3.1"></times><ci id="S3.E3.m1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.3.2">𝐹</ci><cn type="integer" id="S3.E3.m1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.3.3">1</cn></apply><apply id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"><divide id="S3.E3.m1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.2"></divide><apply id="S3.E3.m1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.3"><times id="S3.E3.m1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.3.1"></times><apply id="S3.E3.m1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.3.2"><times id="S3.E3.m1.1.1.1.3.2.1.cmml" xref="S3.E3.m1.1.1.1.3.2.1"></times><apply id="S3.E3.m1.1.1.1.3.2.2.cmml" xref="S3.E3.m1.1.1.1.3.2.2"><times id="S3.E3.m1.1.1.1.3.2.2.1.cmml" xref="S3.E3.m1.1.1.1.3.2.2.1"></times><apply id="S3.E3.m1.1.1.1.3.2.2.2.cmml" xref="S3.E3.m1.1.1.1.3.2.2.2"><times id="S3.E3.m1.1.1.1.3.2.2.2.1.cmml" xref="S3.E3.m1.1.1.1.3.2.2.2.1"></times><cn type="integer" id="S3.E3.m1.1.1.1.3.2.2.2.2.cmml" xref="S3.E3.m1.1.1.1.3.2.2.2.2">2</cn><ci id="S3.E3.m1.1.1.1.3.2.2.2.3.cmml" xref="S3.E3.m1.1.1.1.3.2.2.2.3">𝑃</ci></apply><ci id="S3.E3.m1.1.1.1.3.2.2.3.cmml" xref="S3.E3.m1.1.1.1.3.2.2.3">𝑟</ci><ci id="S3.E3.m1.1.1.1.3.2.2.4.cmml" xref="S3.E3.m1.1.1.1.3.2.2.4">𝑒</ci><ci id="S3.E3.m1.1.1.1.3.2.2.5.cmml" xref="S3.E3.m1.1.1.1.3.2.2.5">𝑐</ci><ci id="S3.E3.m1.1.1.1.3.2.2.6.cmml" xref="S3.E3.m1.1.1.1.3.2.2.6">𝑖</ci><ci id="S3.E3.m1.1.1.1.3.2.2.7.cmml" xref="S3.E3.m1.1.1.1.3.2.2.7">𝑠</ci><ci id="S3.E3.m1.1.1.1.3.2.2.8.cmml" xref="S3.E3.m1.1.1.1.3.2.2.8">𝑖</ci><ci id="S3.E3.m1.1.1.1.3.2.2.9.cmml" xref="S3.E3.m1.1.1.1.3.2.2.9">𝑜</ci><ci id="S3.E3.m1.1.1.1.3.2.2.10.cmml" xref="S3.E3.m1.1.1.1.3.2.2.10">𝑛</ci></apply><ci id="S3.E3.m1.1.1.1.3.2.3.cmml" xref="S3.E3.m1.1.1.1.3.2.3">𝑅</ci></apply><ci id="S3.E3.m1.1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.1.3.3">𝑒</ci><ci id="S3.E3.m1.1.1.1.3.4.cmml" xref="S3.E3.m1.1.1.1.3.4">𝑐</ci><ci id="S3.E3.m1.1.1.1.3.5.cmml" xref="S3.E3.m1.1.1.1.3.5">𝑎</ci><ci id="S3.E3.m1.1.1.1.3.6.cmml" xref="S3.E3.m1.1.1.1.3.6">𝑙</ci><ci id="S3.E3.m1.1.1.1.3.7.cmml" xref="S3.E3.m1.1.1.1.3.7">𝑙</ci></apply><apply id="S3.E3.m1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1"><plus id="S3.E3.m1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1"></plus><apply id="S3.E3.m1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2"><times id="S3.E3.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.1"></times><ci id="S3.E3.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.2">𝑃</ci><ci id="S3.E3.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.3">𝑟</ci><ci id="S3.E3.m1.1.1.1.1.1.1.2.4.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.4">𝑒</ci><ci id="S3.E3.m1.1.1.1.1.1.1.2.5.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.5">𝑐</ci><ci id="S3.E3.m1.1.1.1.1.1.1.2.6.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.6">𝑖</ci><ci id="S3.E3.m1.1.1.1.1.1.1.2.7.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.7">𝑠</ci><ci id="S3.E3.m1.1.1.1.1.1.1.2.8.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.8">𝑖</ci><ci id="S3.E3.m1.1.1.1.1.1.1.2.9.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.9">𝑜</ci><ci id="S3.E3.m1.1.1.1.1.1.1.2.10.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.10">𝑛</ci></apply><apply id="S3.E3.m1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3"><times id="S3.E3.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.1"></times><ci id="S3.E3.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.2">𝑅</ci><ci id="S3.E3.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.3">𝑒</ci><ci id="S3.E3.m1.1.1.1.1.1.1.3.4.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.4">𝑐</ci><ci id="S3.E3.m1.1.1.1.1.1.1.3.5.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.5">𝑎</ci><ci id="S3.E3.m1.1.1.1.1.1.1.3.6.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.6">𝑙</ci><ci id="S3.E3.m1.1.1.1.1.1.1.3.7.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.7">𝑙</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">F1=2*Precision*Recall/(Precision+Recall)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p1.3" class="ltx_p">For all our evaluations we restrict our Ground-truths and predictions up-to <em id="S3.SS1.p1.3.1" class="ltx_emph ltx_font_italic">60meters</em> in the Bird’s eye view space from the ego-vehicle for comparison at common-grounds.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Inference-time</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In-terms of practical application, another equally important metric for detectors is the inference-time, defined as run-time during deployment of the model. We compare our models on <em id="S3.SS2.p1.1.1" class="ltx_emph ltx_font_italic">PyTorch</em> model at standard precision <em id="S3.SS2.p1.1.2" class="ltx_emph ltx_font_italic">float32</em> and also at optimized precision of <em id="S3.SS2.p1.1.3" class="ltx_emph ltx_font_italic">float16</em> in <a href="#S4.SS4" title="4.4 Model Formats and Precision ‣ 4 Methodology and Experiments ‣ Training Strategies for Vision Transformers for Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.4</span></a>. In most of the robotic applications deployment platforms is <em id="S3.SS2.p1.1.4" class="ltx_emph ltx_font_italic">Nvidia GPU</em>, for which <em id="S3.SS2.p1.1.5" class="ltx_emph ltx_font_italic">TensorRT</em> is used as the deployment library. <em id="S3.SS2.p1.1.6" class="ltx_emph ltx_font_italic">TensorRT</em> runtimes and <em id="S3.SS2.p1.1.7" class="ltx_emph ltx_font_italic">PyTorch</em> module inference-time aren not always correlated. So it is not a fair comparison of inference-time of the model until we do this analysis with the actual <em id="S3.SS2.p1.1.8" class="ltx_emph ltx_font_italic">TensorRT</em> format. In addition to <em id="S3.SS2.p1.1.9" class="ltx_emph ltx_font_italic">TensorRT float32</em> model precision we show run-time comparison in <em id="S3.SS2.p1.1.10" class="ltx_emph ltx_font_italic">TensorRT float16</em> format a.k.a <em id="S3.SS2.p1.1.11" class="ltx_emph ltx_font_italic">half-precision</em> as well.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>MACs and Parameters</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">MACs (Multiply–accumulate operation) and number of parameters in any ML model are other less used metrics to compare two models size along with the inference-time. While inference-time of the network is dependent on the hardware but MACs and number of parameters are hardware-agnostic parameters. MACs defined number of computations happen during inference of the model. These computations typically include number of <em id="S3.SS3.p1.1.1" class="ltx_emph ltx_font_italic">multiply</em> and <em id="S3.SS3.p1.1.2" class="ltx_emph ltx_font_italic">add</em> operations on the float numbers. These metrics are generally only used when deployment hardware is not known while development of the ML network. MACs and parameters might not always correlate with inference-time, which is more predictive measure of latency of the network.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Methodology and Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section we will present series of network down-scaling methods. We train our model on our in-house autonomous driving dataset with 3D BEV boxes as annotations and evaluated as per <em id="S4.p1.1.1" class="ltx_emph ltx_font_italic">mAP</em> and <em id="S4.p1.1.2" class="ltx_emph ltx_font_italic">max-F1</em> score mentioned in <a href="#S3" title="3 Evaluation Criteria ‣ Training Strategies for Vision Transformers for Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3</span></a> and inference time calculated on a <em id="S4.p1.1.3" class="ltx_emph ltx_font_italic">T4 Nvidia GPU</em> in different model formats. All the models are trained and evaluated on the same GPU as well. One sample of our dataset is shown in <a href="#S2.F2" title="In 2.2 Multi-image Based Detection ‣ 2 Object Detection Methods ‣ Training Strategies for Vision Transformers for Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>. Our training strategies can be categorized as: <em id="S4.p1.1.4" class="ltx_emph ltx_font_italic">Pre-training Strategies</em>, <em id="S4.p1.1.5" class="ltx_emph ltx_font_italic">Training Strategies</em>, <em id="S4.p1.1.6" class="ltx_emph ltx_font_italic">Post-training Strategies</em>, <em id="S4.p1.1.7" class="ltx_emph ltx_font_italic">Model Formats and Precision</em>. For profiling measures: inference-time is measure in milliseconds, MACs are measured as number of operations and Paramters are measured as number of parameters.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Pre-training Strategies</h3>

<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Input Resolution</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.7" class="ltx_p">To understand the over-head of backbone and detection head as shown in <a href="#S1.F1" title="In 1 Introduction ‣ Training Strategies for Vision Transformers for Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>, we ran profiling with inference-time, parameters count and mac operations on it. Results from <a href="#S4.T1" title="In 4.1.1 Input Resolution ‣ 4.1 Pre-training Strategies ‣ 4 Methodology and Experiments ‣ Training Strategies for Vision Transformers for Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a> shows that most of the inference-time overhead lies in the backbone. We deduced that image being a 2D matrix; total inference-time can be quadratically reduced if we can reduce the input resolution keeping a close watch on accuracy performance change. We trained and evaluated our model by aggressively reducing the input resolution as shown in <a href="#S4.T2" title="In 4.1.1 Input Resolution ‣ 4.1 Pre-training Strategies ‣ 4 Methodology and Experiments ‣ Training Strategies for Vision Transformers for Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">2</span></a>. We noticed that by reducing image-resolution smaller objects like pedestrians is affected more adversely compared to larger object vehicles. We concluded that we can safely reduce down the resolution to <math id="S4.SS1.SSS1.p1.1.m1.2" class="ltx_Math" alttext="(598,394)" display="inline"><semantics id="S4.SS1.SSS1.p1.1.m1.2a"><mrow id="S4.SS1.SSS1.p1.1.m1.2.3.2" xref="S4.SS1.SSS1.p1.1.m1.2.3.1.cmml"><mo stretchy="false" id="S4.SS1.SSS1.p1.1.m1.2.3.2.1" xref="S4.SS1.SSS1.p1.1.m1.2.3.1.cmml">(</mo><mn id="S4.SS1.SSS1.p1.1.m1.1.1" xref="S4.SS1.SSS1.p1.1.m1.1.1.cmml">598</mn><mo id="S4.SS1.SSS1.p1.1.m1.2.3.2.2" xref="S4.SS1.SSS1.p1.1.m1.2.3.1.cmml">,</mo><mn id="S4.SS1.SSS1.p1.1.m1.2.2" xref="S4.SS1.SSS1.p1.1.m1.2.2.cmml">394</mn><mo stretchy="false" id="S4.SS1.SSS1.p1.1.m1.2.3.2.3" xref="S4.SS1.SSS1.p1.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.1.m1.2b"><interval closure="open" id="S4.SS1.SSS1.p1.1.m1.2.3.1.cmml" xref="S4.SS1.SSS1.p1.1.m1.2.3.2"><cn type="integer" id="S4.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1">598</cn><cn type="integer" id="S4.SS1.SSS1.p1.1.m1.2.2.cmml" xref="S4.SS1.SSS1.p1.1.m1.2.2">394</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.1.m1.2c">(598,394)</annotation></semantics></math> i.e. <math id="S4.SS1.SSS1.p1.2.m2.1" class="ltx_Math" alttext="3/5^{th}" display="inline"><semantics id="S4.SS1.SSS1.p1.2.m2.1a"><mrow id="S4.SS1.SSS1.p1.2.m2.1.1" xref="S4.SS1.SSS1.p1.2.m2.1.1.cmml"><mn id="S4.SS1.SSS1.p1.2.m2.1.1.2" xref="S4.SS1.SSS1.p1.2.m2.1.1.2.cmml">3</mn><mo id="S4.SS1.SSS1.p1.2.m2.1.1.1" xref="S4.SS1.SSS1.p1.2.m2.1.1.1.cmml">/</mo><msup id="S4.SS1.SSS1.p1.2.m2.1.1.3" xref="S4.SS1.SSS1.p1.2.m2.1.1.3.cmml"><mn id="S4.SS1.SSS1.p1.2.m2.1.1.3.2" xref="S4.SS1.SSS1.p1.2.m2.1.1.3.2.cmml">5</mn><mrow id="S4.SS1.SSS1.p1.2.m2.1.1.3.3" xref="S4.SS1.SSS1.p1.2.m2.1.1.3.3.cmml"><mi id="S4.SS1.SSS1.p1.2.m2.1.1.3.3.2" xref="S4.SS1.SSS1.p1.2.m2.1.1.3.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p1.2.m2.1.1.3.3.1" xref="S4.SS1.SSS1.p1.2.m2.1.1.3.3.1.cmml">​</mo><mi id="S4.SS1.SSS1.p1.2.m2.1.1.3.3.3" xref="S4.SS1.SSS1.p1.2.m2.1.1.3.3.3.cmml">h</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.2.m2.1b"><apply id="S4.SS1.SSS1.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS1.p1.2.m2.1.1"><divide id="S4.SS1.SSS1.p1.2.m2.1.1.1.cmml" xref="S4.SS1.SSS1.p1.2.m2.1.1.1"></divide><cn type="integer" id="S4.SS1.SSS1.p1.2.m2.1.1.2.cmml" xref="S4.SS1.SSS1.p1.2.m2.1.1.2">3</cn><apply id="S4.SS1.SSS1.p1.2.m2.1.1.3.cmml" xref="S4.SS1.SSS1.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p1.2.m2.1.1.3.1.cmml" xref="S4.SS1.SSS1.p1.2.m2.1.1.3">superscript</csymbol><cn type="integer" id="S4.SS1.SSS1.p1.2.m2.1.1.3.2.cmml" xref="S4.SS1.SSS1.p1.2.m2.1.1.3.2">5</cn><apply id="S4.SS1.SSS1.p1.2.m2.1.1.3.3.cmml" xref="S4.SS1.SSS1.p1.2.m2.1.1.3.3"><times id="S4.SS1.SSS1.p1.2.m2.1.1.3.3.1.cmml" xref="S4.SS1.SSS1.p1.2.m2.1.1.3.3.1"></times><ci id="S4.SS1.SSS1.p1.2.m2.1.1.3.3.2.cmml" xref="S4.SS1.SSS1.p1.2.m2.1.1.3.3.2">𝑡</ci><ci id="S4.SS1.SSS1.p1.2.m2.1.1.3.3.3.cmml" xref="S4.SS1.SSS1.p1.2.m2.1.1.3.3.3">ℎ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.2.m2.1c">3/5^{th}</annotation></semantics></math> of the original (<math id="S4.SS1.SSS1.p1.3.m3.2" class="ltx_math_unparsed" alttext="996,656)" display="inline"><semantics id="S4.SS1.SSS1.p1.3.m3.2a"><mrow id="S4.SS1.SSS1.p1.3.m3.2b"><mn id="S4.SS1.SSS1.p1.3.m3.1.1">996</mn><mo id="S4.SS1.SSS1.p1.3.m3.2.3">,</mo><mn id="S4.SS1.SSS1.p1.3.m3.2.2">656</mn><mo stretchy="false" id="S4.SS1.SSS1.p1.3.m3.2.4">)</mo></mrow><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.3.m3.2c">996,656)</annotation></semantics></math> with <math id="S4.SS1.SSS1.p1.4.m4.1" class="ltx_Math" alttext="52.5\%" display="inline"><semantics id="S4.SS1.SSS1.p1.4.m4.1a"><mrow id="S4.SS1.SSS1.p1.4.m4.1.1" xref="S4.SS1.SSS1.p1.4.m4.1.1.cmml"><mn id="S4.SS1.SSS1.p1.4.m4.1.1.2" xref="S4.SS1.SSS1.p1.4.m4.1.1.2.cmml">52.5</mn><mo id="S4.SS1.SSS1.p1.4.m4.1.1.1" xref="S4.SS1.SSS1.p1.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.4.m4.1b"><apply id="S4.SS1.SSS1.p1.4.m4.1.1.cmml" xref="S4.SS1.SSS1.p1.4.m4.1.1"><csymbol cd="latexml" id="S4.SS1.SSS1.p1.4.m4.1.1.1.cmml" xref="S4.SS1.SSS1.p1.4.m4.1.1.1">percent</csymbol><cn type="float" id="S4.SS1.SSS1.p1.4.m4.1.1.2.cmml" xref="S4.SS1.SSS1.p1.4.m4.1.1.2">52.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.4.m4.1c">52.5\%</annotation></semantics></math> inference-time improvement at the cost of mere <math id="S4.SS1.SSS1.p1.5.m5.1" class="ltx_Math" alttext="1.4\%" display="inline"><semantics id="S4.SS1.SSS1.p1.5.m5.1a"><mrow id="S4.SS1.SSS1.p1.5.m5.1.1" xref="S4.SS1.SSS1.p1.5.m5.1.1.cmml"><mn id="S4.SS1.SSS1.p1.5.m5.1.1.2" xref="S4.SS1.SSS1.p1.5.m5.1.1.2.cmml">1.4</mn><mo id="S4.SS1.SSS1.p1.5.m5.1.1.1" xref="S4.SS1.SSS1.p1.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.5.m5.1b"><apply id="S4.SS1.SSS1.p1.5.m5.1.1.cmml" xref="S4.SS1.SSS1.p1.5.m5.1.1"><csymbol cd="latexml" id="S4.SS1.SSS1.p1.5.m5.1.1.1.cmml" xref="S4.SS1.SSS1.p1.5.m5.1.1.1">percent</csymbol><cn type="float" id="S4.SS1.SSS1.p1.5.m5.1.1.2.cmml" xref="S4.SS1.SSS1.p1.5.m5.1.1.2">1.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.5.m5.1c">1.4\%</annotation></semantics></math> drop in the pedestrian AP (Average precision) performance and the <math id="S4.SS1.SSS1.p1.6.m6.1" class="ltx_Math" alttext="0.5\%" display="inline"><semantics id="S4.SS1.SSS1.p1.6.m6.1a"><mrow id="S4.SS1.SSS1.p1.6.m6.1.1" xref="S4.SS1.SSS1.p1.6.m6.1.1.cmml"><mn id="S4.SS1.SSS1.p1.6.m6.1.1.2" xref="S4.SS1.SSS1.p1.6.m6.1.1.2.cmml">0.5</mn><mo id="S4.SS1.SSS1.p1.6.m6.1.1.1" xref="S4.SS1.SSS1.p1.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.6.m6.1b"><apply id="S4.SS1.SSS1.p1.6.m6.1.1.cmml" xref="S4.SS1.SSS1.p1.6.m6.1.1"><csymbol cd="latexml" id="S4.SS1.SSS1.p1.6.m6.1.1.1.cmml" xref="S4.SS1.SSS1.p1.6.m6.1.1.1">percent</csymbol><cn type="float" id="S4.SS1.SSS1.p1.6.m6.1.1.2.cmml" xref="S4.SS1.SSS1.p1.6.m6.1.1.2">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.6.m6.1c">0.5\%</annotation></semantics></math> drop in the vehicle AP performance. Also note that the number of parameters don’t change with input-resolution as it is based out of fully-convolutional based backbone<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> and an FPN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. For further experiments we will focus more on the inference-time metric rather than MACs and Parameters, as inference-time is a more representative metric for the given hardware. For further experiments we will change our baseline to <math id="S4.SS1.SSS1.p1.7.m7.2" class="ltx_Math" alttext="(598,394)" display="inline"><semantics id="S4.SS1.SSS1.p1.7.m7.2a"><mrow id="S4.SS1.SSS1.p1.7.m7.2.3.2" xref="S4.SS1.SSS1.p1.7.m7.2.3.1.cmml"><mo stretchy="false" id="S4.SS1.SSS1.p1.7.m7.2.3.2.1" xref="S4.SS1.SSS1.p1.7.m7.2.3.1.cmml">(</mo><mn id="S4.SS1.SSS1.p1.7.m7.1.1" xref="S4.SS1.SSS1.p1.7.m7.1.1.cmml">598</mn><mo id="S4.SS1.SSS1.p1.7.m7.2.3.2.2" xref="S4.SS1.SSS1.p1.7.m7.2.3.1.cmml">,</mo><mn id="S4.SS1.SSS1.p1.7.m7.2.2" xref="S4.SS1.SSS1.p1.7.m7.2.2.cmml">394</mn><mo stretchy="false" id="S4.SS1.SSS1.p1.7.m7.2.3.2.3" xref="S4.SS1.SSS1.p1.7.m7.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.7.m7.2b"><interval closure="open" id="S4.SS1.SSS1.p1.7.m7.2.3.1.cmml" xref="S4.SS1.SSS1.p1.7.m7.2.3.2"><cn type="integer" id="S4.SS1.SSS1.p1.7.m7.1.1.cmml" xref="S4.SS1.SSS1.p1.7.m7.1.1">598</cn><cn type="integer" id="S4.SS1.SSS1.p1.7.m7.2.2.cmml" xref="S4.SS1.SSS1.p1.7.m7.2.2">394</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.7.m7.2c">(598,394)</annotation></semantics></math> input-resolution to show accumulative effect of our strategies.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.2.1.1" class="ltx_tr">
<th id="S4.T1.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Module</th>
<th id="S4.T1.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Inference-time</th>
<th id="S4.T1.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">MACs</th>
<th id="S4.T1.2.1.1.4" class="ltx_td ltx_nopad_r ltx_align_right ltx_th ltx_th_column ltx_border_tt">Parameters</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.2.2.1" class="ltx_tr">
<th id="S4.T1.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Entire Network</th>
<td id="S4.T1.2.2.1.2" class="ltx_td ltx_align_center ltx_border_t">447</td>
<td id="S4.T1.2.2.1.3" class="ltx_td ltx_align_center ltx_border_t">832B</td>
<td id="S4.T1.2.2.1.4" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t">37M</td>
</tr>
<tr id="S4.T1.2.3.2" class="ltx_tr">
<th id="S4.T1.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Backbone + FPN</th>
<td id="S4.T1.2.3.2.2" class="ltx_td ltx_align_center">380</td>
<td id="S4.T1.2.3.2.3" class="ltx_td ltx_align_center">829B</td>
<td id="S4.T1.2.3.2.4" class="ltx_td ltx_nopad_r ltx_align_right">34M</td>
</tr>
<tr id="S4.T1.2.4.3" class="ltx_tr">
<th id="S4.T1.2.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Detection Head</th>
<td id="S4.T1.2.4.3.2" class="ltx_td ltx_align_center ltx_border_bb">67</td>
<td id="S4.T1.2.4.3.3" class="ltx_td ltx_align_center ltx_border_bb">2B</td>
<td id="S4.T1.2.4.3.4" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb">3M</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.3.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.4.2" class="ltx_text" style="font-size:90%;">Profiling different stages of the network in terms of inference-time, MACs and number of parameters.</span></figcaption>
</figure>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.2.1.1" class="ltx_tr">
<th id="S4.T2.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Input-resolution</th>
<th id="S4.T2.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Vehicle (AP)</th>
<th id="S4.T2.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Pedestrian (AP)</th>
<th id="S4.T2.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Inference-time (ms)</th>
<th id="S4.T2.2.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">MACs</th>
<th id="S4.T2.2.1.1.6" class="ltx_td ltx_nopad_r ltx_align_right ltx_th ltx_th_column ltx_border_tt">Parameters</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.2.2.1" class="ltx_tr">
<td id="S4.T2.2.2.1.1" class="ltx_td ltx_align_left ltx_border_t">996, 656</td>
<td id="S4.T2.2.2.1.2" class="ltx_td ltx_align_center ltx_border_t">64.9</td>
<td id="S4.T2.2.2.1.3" class="ltx_td ltx_align_center ltx_border_t">71.8</td>
<td id="S4.T2.2.2.1.4" class="ltx_td ltx_align_center ltx_border_t">447</td>
<td id="S4.T2.2.2.1.5" class="ltx_td ltx_align_center ltx_border_t">832B</td>
<td id="S4.T2.2.2.1.6" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t">37M</td>
</tr>
<tr id="S4.T2.2.3.2" class="ltx_tr">
<td id="S4.T2.2.3.2.1" class="ltx_td ltx_align_left">664, 437</td>
<td id="S4.T2.2.3.2.2" class="ltx_td ltx_align_center">64.5</td>
<td id="S4.T2.2.3.2.3" class="ltx_td ltx_align_center">71.5</td>
<td id="S4.T2.2.3.2.4" class="ltx_td ltx_align_center">242</td>
<td id="S4.T2.2.3.2.5" class="ltx_td ltx_align_center">372B</td>
<td id="S4.T2.2.3.2.6" class="ltx_td ltx_nopad_r ltx_align_right">37M</td>
</tr>
<tr id="S4.T2.2.4.3" class="ltx_tr">
<td id="S4.T2.2.4.3.1" class="ltx_td ltx_align_left"><span id="S4.T2.2.4.3.1.1" class="ltx_text ltx_font_bold">598, 394</span></td>
<td id="S4.T2.2.4.3.2" class="ltx_td ltx_align_center">64.0</td>
<td id="S4.T2.2.4.3.3" class="ltx_td ltx_align_center">71.4</td>
<td id="S4.T2.2.4.3.4" class="ltx_td ltx_align_center">212</td>
<td id="S4.T2.2.4.3.5" class="ltx_td ltx_align_center">305B</td>
<td id="S4.T2.2.4.3.6" class="ltx_td ltx_nopad_r ltx_align_right">37M</td>
</tr>
<tr id="S4.T2.2.5.4" class="ltx_tr">
<td id="S4.T2.2.5.4.1" class="ltx_td ltx_align_left ltx_border_bb">498, 328</td>
<td id="S4.T2.2.5.4.2" class="ltx_td ltx_align_center ltx_border_bb">60.4</td>
<td id="S4.T2.2.5.4.3" class="ltx_td ltx_align_center ltx_border_bb">70.6</td>
<td id="S4.T2.2.5.4.4" class="ltx_td ltx_align_center ltx_border_bb">166</td>
<td id="S4.T2.2.5.4.5" class="ltx_td ltx_align_center ltx_border_bb">212B</td>
<td id="S4.T2.2.5.4.6" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb">37M</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.3.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.4.2" class="ltx_text" style="font-size:90%;">Effects of reducing input-resolution on accuracy and inference-time. Input-resolution is represented as pixels in width and height dimension respectively.</span></figcaption>
</figure>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Image Pre-croppers</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.p1.2" class="ltx_p">In object detection, image Pre-cropping refers to cropping of the image from certain edges of the camera which represents least informative pixels to squeeze out model inference-time from it by saving onto those pixels’ computations. To select optimal pre-cropping strategy for our current network we used two methods, firstly we visually analyzed images as shown in <a href="#S2.F2" title="In 2.2 Multi-image Based Detection ‣ 2 Object Detection Methods ‣ Training Strategies for Vision Transformers for Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a> and tried to find the least informative part of the image in-terms of our problem statement i.e. detect all the safety critical agents on the road. We noticed that generally top few-tens of pixels in the image are the least important as they mostly include top of a tree/ top of a building/ top of a pillar etc. as shown in <a href="#S2.F2" title="In 2.2 Multi-image Based Detection ‣ 2 Object Detection Methods ‣ Training Strategies for Vision Transformers for Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>. Our second methods was quantitative analysis in which we gathered all the boxes that lies in those cropped top few-tens pixels to confirm that we are not pruning information of any of our objects of interest with those cropped pixels. We found that less than <math id="S4.SS1.SSS2.p1.1.m1.1" class="ltx_Math" alttext="0.25\%" display="inline"><semantics id="S4.SS1.SSS2.p1.1.m1.1a"><mrow id="S4.SS1.SSS2.p1.1.m1.1.1" xref="S4.SS1.SSS2.p1.1.m1.1.1.cmml"><mn id="S4.SS1.SSS2.p1.1.m1.1.1.2" xref="S4.SS1.SSS2.p1.1.m1.1.1.2.cmml">0.25</mn><mo id="S4.SS1.SSS2.p1.1.m1.1.1.1" xref="S4.SS1.SSS2.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.1.m1.1b"><apply id="S4.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS1.SSS2.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.SS1.SSS2.p1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1.2">0.25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.1.m1.1c">0.25\%</annotation></semantics></math> of our ground-truth in training dataset is actually in the top-50 pixels for input image-resolution of <math id="S4.SS1.SSS2.p1.2.m2.2" class="ltx_Math" alttext="(598,394)" display="inline"><semantics id="S4.SS1.SSS2.p1.2.m2.2a"><mrow id="S4.SS1.SSS2.p1.2.m2.2.3.2" xref="S4.SS1.SSS2.p1.2.m2.2.3.1.cmml"><mo stretchy="false" id="S4.SS1.SSS2.p1.2.m2.2.3.2.1" xref="S4.SS1.SSS2.p1.2.m2.2.3.1.cmml">(</mo><mn id="S4.SS1.SSS2.p1.2.m2.1.1" xref="S4.SS1.SSS2.p1.2.m2.1.1.cmml">598</mn><mo id="S4.SS1.SSS2.p1.2.m2.2.3.2.2" xref="S4.SS1.SSS2.p1.2.m2.2.3.1.cmml">,</mo><mn id="S4.SS1.SSS2.p1.2.m2.2.2" xref="S4.SS1.SSS2.p1.2.m2.2.2.cmml">394</mn><mo stretchy="false" id="S4.SS1.SSS2.p1.2.m2.2.3.2.3" xref="S4.SS1.SSS2.p1.2.m2.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.2.m2.2b"><interval closure="open" id="S4.SS1.SSS2.p1.2.m2.2.3.1.cmml" xref="S4.SS1.SSS2.p1.2.m2.2.3.2"><cn type="integer" id="S4.SS1.SSS2.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1">598</cn><cn type="integer" id="S4.SS1.SSS2.p1.2.m2.2.2.cmml" xref="S4.SS1.SSS2.p1.2.m2.2.2">394</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.2.m2.2c">(598,394)</annotation></semantics></math>. Our empirical findings are shown in <a href="#S4.T3" title="In 4.1.2 Image Pre-croppers ‣ 4.1 Pre-training Strategies ‣ 4 Methodology and Experiments ‣ Training Strategies for Vision Transformers for Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">3</span></a>. We noticed that removing pixels from the top has no/ positive effect on pedestrians and only minor effect on vehicles, as some of the features of vehicles might get missed specially for a very close-range vehicles which occupy the entire image. We concluded that removing 50 pixels from top is an optimal strategy for inference-time and accuracy joint-optimization with 10% improvement in inference-time at the cost of only 2% accuracy drop in vehicles and no drop in pedestrian.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.2.1.1" class="ltx_tr">
<th id="S4.T3.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Input-resolution</th>
<th id="S4.T3.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Cropped pixels (Top)</th>
<th id="S4.T3.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Pedestrian (AP)</th>
<th id="S4.T3.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Vehicle (AP)</th>
<th id="S4.T3.2.1.1.5" class="ltx_td ltx_nopad_r ltx_align_right ltx_th ltx_th_column ltx_border_tt">Inference-time</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.2.2.1" class="ltx_tr">
<td id="S4.T3.2.2.1.1" class="ltx_td ltx_align_left ltx_border_t">598, 394</td>
<td id="S4.T3.2.2.1.2" class="ltx_td ltx_align_center ltx_border_t">0</td>
<td id="S4.T3.2.2.1.3" class="ltx_td ltx_align_center ltx_border_t">64.0</td>
<td id="S4.T3.2.2.1.4" class="ltx_td ltx_align_center ltx_border_t">71.4</td>
<td id="S4.T3.2.2.1.5" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t">212</td>
</tr>
<tr id="S4.T3.2.3.2" class="ltx_tr">
<td id="S4.T3.2.3.2.1" class="ltx_td ltx_align_left">598, 394</td>
<td id="S4.T3.2.3.2.2" class="ltx_td ltx_align_center">25</td>
<td id="S4.T3.2.3.2.3" class="ltx_td ltx_align_center">64.8</td>
<td id="S4.T3.2.3.2.4" class="ltx_td ltx_align_center">70.3</td>
<td id="S4.T3.2.3.2.5" class="ltx_td ltx_nopad_r ltx_align_right">200</td>
</tr>
<tr id="S4.T3.2.4.3" class="ltx_tr">
<td id="S4.T3.2.4.3.1" class="ltx_td ltx_align_left">598, 394</td>
<td id="S4.T3.2.4.3.2" class="ltx_td ltx_align_center">50</td>
<td id="S4.T3.2.4.3.3" class="ltx_td ltx_align_center">64.8</td>
<td id="S4.T3.2.4.3.4" class="ltx_td ltx_align_center">69.6</td>
<td id="S4.T3.2.4.3.5" class="ltx_td ltx_nopad_r ltx_align_right">191</td>
</tr>
<tr id="S4.T3.2.5.4" class="ltx_tr">
<td id="S4.T3.2.5.4.1" class="ltx_td ltx_align_left ltx_border_bb">598, 394</td>
<td id="S4.T3.2.5.4.2" class="ltx_td ltx_align_center ltx_border_bb">100</td>
<td id="S4.T3.2.5.4.3" class="ltx_td ltx_align_center ltx_border_bb">62.8</td>
<td id="S4.T3.2.5.4.4" class="ltx_td ltx_align_center ltx_border_bb">65.4</td>
<td id="S4.T3.2.5.4.5" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb">175</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.3.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S4.T3.4.2" class="ltx_text" style="font-size:90%;">Empirical experiment with Pre-croppers.</span></figcaption>
</figure>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Training Strategies</h3>

<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Training Schedule</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">For our baseline experiments we used one-cycle learning rate (L.R.) scheduler <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> with maximum learning rate as <math id="S4.SS2.SSS1.p1.1.m1.1" class="ltx_Math" alttext="5e^{-5}" display="inline"><semantics id="S4.SS2.SSS1.p1.1.m1.1a"><mrow id="S4.SS2.SSS1.p1.1.m1.1.1" xref="S4.SS2.SSS1.p1.1.m1.1.1.cmml"><mn id="S4.SS2.SSS1.p1.1.m1.1.1.2" xref="S4.SS2.SSS1.p1.1.m1.1.1.2.cmml">5</mn><mo lspace="0em" rspace="0em" id="S4.SS2.SSS1.p1.1.m1.1.1.1" xref="S4.SS2.SSS1.p1.1.m1.1.1.1.cmml">​</mo><msup id="S4.SS2.SSS1.p1.1.m1.1.1.3" xref="S4.SS2.SSS1.p1.1.m1.1.1.3.cmml"><mi id="S4.SS2.SSS1.p1.1.m1.1.1.3.2" xref="S4.SS2.SSS1.p1.1.m1.1.1.3.2.cmml">e</mi><mrow id="S4.SS2.SSS1.p1.1.m1.1.1.3.3" xref="S4.SS2.SSS1.p1.1.m1.1.1.3.3.cmml"><mo id="S4.SS2.SSS1.p1.1.m1.1.1.3.3a" xref="S4.SS2.SSS1.p1.1.m1.1.1.3.3.cmml">−</mo><mn id="S4.SS2.SSS1.p1.1.m1.1.1.3.3.2" xref="S4.SS2.SSS1.p1.1.m1.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.1.m1.1b"><apply id="S4.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1"><times id="S4.SS2.SSS1.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS2.SSS1.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1.2">5</cn><apply id="S4.SS2.SSS1.p1.1.m1.1.1.3.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p1.1.m1.1.1.3.1.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1.3">superscript</csymbol><ci id="S4.SS2.SSS1.p1.1.m1.1.1.3.2.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1.3.2">𝑒</ci><apply id="S4.SS2.SSS1.p1.1.m1.1.1.3.3.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1.3.3"><minus id="S4.SS2.SSS1.p1.1.m1.1.1.3.3.1.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1.3.3"></minus><cn type="integer" id="S4.SS2.SSS1.p1.1.m1.1.1.3.3.2.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.1.m1.1c">5e^{-5}</annotation></semantics></math>. We experimented with three values associated with one-cycle learning rate viz., Start L.R; Max L.R. and End L.R. Empirical results are shown in <a href="#S4.T4" title="In 4.2.1 Training Schedule ‣ 4.2 Training Strategies ‣ 4 Methodology and Experiments ‣ Training Strategies for Vision Transformers for Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">4</span></a>. We concluded that there’s a value in using one-cycle learning rate with its limits by using higher learning rate in the middle of training and relatively much smaller learning rate at the beginning and end of the training. We also observed if we try to increase the ending learning rate, we suffer through training-divergence.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.12" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.12.13.1" class="ltx_tr">
<th id="S4.T4.12.13.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.12.13.1.1.1" class="ltx_text" style="font-size:90%;">L.R. Star</span></th>
<th id="S4.T4.12.13.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.12.13.1.2.1" class="ltx_text" style="font-size:90%;">L.R. Max.</span></th>
<th id="S4.T4.12.13.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.12.13.1.3.1" class="ltx_text" style="font-size:90%;">L.R. End</span></th>
<th id="S4.T4.12.13.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.12.13.1.4.1" class="ltx_text" style="font-size:90%;">Pedestrian</span></th>
<th id="S4.T4.12.13.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.12.13.1.5.1" class="ltx_text" style="font-size:90%;">Vehicle</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.3.3" class="ltx_tr">
<td id="S4.T4.1.1.1" class="ltx_td ltx_align_left ltx_border_t"><math id="S4.T4.1.1.1.m1.1" class="ltx_Math" alttext="5e^{-6}" display="inline"><semantics id="S4.T4.1.1.1.m1.1a"><mrow id="S4.T4.1.1.1.m1.1.1" xref="S4.T4.1.1.1.m1.1.1.cmml"><mn id="S4.T4.1.1.1.m1.1.1.2" xref="S4.T4.1.1.1.m1.1.1.2.cmml">5</mn><mo lspace="0em" rspace="0em" id="S4.T4.1.1.1.m1.1.1.1" xref="S4.T4.1.1.1.m1.1.1.1.cmml">​</mo><msup id="S4.T4.1.1.1.m1.1.1.3" xref="S4.T4.1.1.1.m1.1.1.3.cmml"><mi id="S4.T4.1.1.1.m1.1.1.3.2" xref="S4.T4.1.1.1.m1.1.1.3.2.cmml">e</mi><mrow id="S4.T4.1.1.1.m1.1.1.3.3" xref="S4.T4.1.1.1.m1.1.1.3.3.cmml"><mo id="S4.T4.1.1.1.m1.1.1.3.3a" xref="S4.T4.1.1.1.m1.1.1.3.3.cmml">−</mo><mn id="S4.T4.1.1.1.m1.1.1.3.3.2" xref="S4.T4.1.1.1.m1.1.1.3.3.2.cmml">6</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.m1.1b"><apply id="S4.T4.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.m1.1.1"><times id="S4.T4.1.1.1.m1.1.1.1.cmml" xref="S4.T4.1.1.1.m1.1.1.1"></times><cn type="integer" id="S4.T4.1.1.1.m1.1.1.2.cmml" xref="S4.T4.1.1.1.m1.1.1.2">5</cn><apply id="S4.T4.1.1.1.m1.1.1.3.cmml" xref="S4.T4.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T4.1.1.1.m1.1.1.3.1.cmml" xref="S4.T4.1.1.1.m1.1.1.3">superscript</csymbol><ci id="S4.T4.1.1.1.m1.1.1.3.2.cmml" xref="S4.T4.1.1.1.m1.1.1.3.2">𝑒</ci><apply id="S4.T4.1.1.1.m1.1.1.3.3.cmml" xref="S4.T4.1.1.1.m1.1.1.3.3"><minus id="S4.T4.1.1.1.m1.1.1.3.3.1.cmml" xref="S4.T4.1.1.1.m1.1.1.3.3"></minus><cn type="integer" id="S4.T4.1.1.1.m1.1.1.3.3.2.cmml" xref="S4.T4.1.1.1.m1.1.1.3.3.2">6</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.m1.1c">5e^{-6}</annotation></semantics></math></td>
<td id="S4.T4.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T4.2.2.2.m1.1" class="ltx_Math" alttext="5e^{-5}" display="inline"><semantics id="S4.T4.2.2.2.m1.1a"><mrow id="S4.T4.2.2.2.m1.1.1" xref="S4.T4.2.2.2.m1.1.1.cmml"><mn id="S4.T4.2.2.2.m1.1.1.2" xref="S4.T4.2.2.2.m1.1.1.2.cmml">5</mn><mo lspace="0em" rspace="0em" id="S4.T4.2.2.2.m1.1.1.1" xref="S4.T4.2.2.2.m1.1.1.1.cmml">​</mo><msup id="S4.T4.2.2.2.m1.1.1.3" xref="S4.T4.2.2.2.m1.1.1.3.cmml"><mi id="S4.T4.2.2.2.m1.1.1.3.2" xref="S4.T4.2.2.2.m1.1.1.3.2.cmml">e</mi><mrow id="S4.T4.2.2.2.m1.1.1.3.3" xref="S4.T4.2.2.2.m1.1.1.3.3.cmml"><mo id="S4.T4.2.2.2.m1.1.1.3.3a" xref="S4.T4.2.2.2.m1.1.1.3.3.cmml">−</mo><mn id="S4.T4.2.2.2.m1.1.1.3.3.2" xref="S4.T4.2.2.2.m1.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.m1.1b"><apply id="S4.T4.2.2.2.m1.1.1.cmml" xref="S4.T4.2.2.2.m1.1.1"><times id="S4.T4.2.2.2.m1.1.1.1.cmml" xref="S4.T4.2.2.2.m1.1.1.1"></times><cn type="integer" id="S4.T4.2.2.2.m1.1.1.2.cmml" xref="S4.T4.2.2.2.m1.1.1.2">5</cn><apply id="S4.T4.2.2.2.m1.1.1.3.cmml" xref="S4.T4.2.2.2.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T4.2.2.2.m1.1.1.3.1.cmml" xref="S4.T4.2.2.2.m1.1.1.3">superscript</csymbol><ci id="S4.T4.2.2.2.m1.1.1.3.2.cmml" xref="S4.T4.2.2.2.m1.1.1.3.2">𝑒</ci><apply id="S4.T4.2.2.2.m1.1.1.3.3.cmml" xref="S4.T4.2.2.2.m1.1.1.3.3"><minus id="S4.T4.2.2.2.m1.1.1.3.3.1.cmml" xref="S4.T4.2.2.2.m1.1.1.3.3"></minus><cn type="integer" id="S4.T4.2.2.2.m1.1.1.3.3.2.cmml" xref="S4.T4.2.2.2.m1.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.m1.1c">5e^{-5}</annotation></semantics></math></td>
<td id="S4.T4.3.3.3" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T4.3.3.3.m1.1" class="ltx_Math" alttext="5e^{-6}" display="inline"><semantics id="S4.T4.3.3.3.m1.1a"><mrow id="S4.T4.3.3.3.m1.1.1" xref="S4.T4.3.3.3.m1.1.1.cmml"><mn id="S4.T4.3.3.3.m1.1.1.2" xref="S4.T4.3.3.3.m1.1.1.2.cmml">5</mn><mo lspace="0em" rspace="0em" id="S4.T4.3.3.3.m1.1.1.1" xref="S4.T4.3.3.3.m1.1.1.1.cmml">​</mo><msup id="S4.T4.3.3.3.m1.1.1.3" xref="S4.T4.3.3.3.m1.1.1.3.cmml"><mi id="S4.T4.3.3.3.m1.1.1.3.2" xref="S4.T4.3.3.3.m1.1.1.3.2.cmml">e</mi><mrow id="S4.T4.3.3.3.m1.1.1.3.3" xref="S4.T4.3.3.3.m1.1.1.3.3.cmml"><mo id="S4.T4.3.3.3.m1.1.1.3.3a" xref="S4.T4.3.3.3.m1.1.1.3.3.cmml">−</mo><mn id="S4.T4.3.3.3.m1.1.1.3.3.2" xref="S4.T4.3.3.3.m1.1.1.3.3.2.cmml">6</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.3.3.3.m1.1b"><apply id="S4.T4.3.3.3.m1.1.1.cmml" xref="S4.T4.3.3.3.m1.1.1"><times id="S4.T4.3.3.3.m1.1.1.1.cmml" xref="S4.T4.3.3.3.m1.1.1.1"></times><cn type="integer" id="S4.T4.3.3.3.m1.1.1.2.cmml" xref="S4.T4.3.3.3.m1.1.1.2">5</cn><apply id="S4.T4.3.3.3.m1.1.1.3.cmml" xref="S4.T4.3.3.3.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T4.3.3.3.m1.1.1.3.1.cmml" xref="S4.T4.3.3.3.m1.1.1.3">superscript</csymbol><ci id="S4.T4.3.3.3.m1.1.1.3.2.cmml" xref="S4.T4.3.3.3.m1.1.1.3.2">𝑒</ci><apply id="S4.T4.3.3.3.m1.1.1.3.3.cmml" xref="S4.T4.3.3.3.m1.1.1.3.3"><minus id="S4.T4.3.3.3.m1.1.1.3.3.1.cmml" xref="S4.T4.3.3.3.m1.1.1.3.3"></minus><cn type="integer" id="S4.T4.3.3.3.m1.1.1.3.3.2.cmml" xref="S4.T4.3.3.3.m1.1.1.3.3.2">6</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.3.3.m1.1c">5e^{-6}</annotation></semantics></math></td>
<td id="S4.T4.3.3.4" class="ltx_td ltx_align_center ltx_border_t">60.7</td>
<td id="S4.T4.3.3.5" class="ltx_td ltx_align_center ltx_border_t">67.0</td>
</tr>
<tr id="S4.T4.6.6" class="ltx_tr">
<td id="S4.T4.4.4.1" class="ltx_td ltx_align_left"><math id="S4.T4.4.4.1.m1.1" class="ltx_Math" alttext="1e^{-5}" display="inline"><semantics id="S4.T4.4.4.1.m1.1a"><mrow id="S4.T4.4.4.1.m1.1.1" xref="S4.T4.4.4.1.m1.1.1.cmml"><mn id="S4.T4.4.4.1.m1.1.1.2" xref="S4.T4.4.4.1.m1.1.1.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S4.T4.4.4.1.m1.1.1.1" xref="S4.T4.4.4.1.m1.1.1.1.cmml">​</mo><msup id="S4.T4.4.4.1.m1.1.1.3" xref="S4.T4.4.4.1.m1.1.1.3.cmml"><mi id="S4.T4.4.4.1.m1.1.1.3.2" xref="S4.T4.4.4.1.m1.1.1.3.2.cmml">e</mi><mrow id="S4.T4.4.4.1.m1.1.1.3.3" xref="S4.T4.4.4.1.m1.1.1.3.3.cmml"><mo id="S4.T4.4.4.1.m1.1.1.3.3a" xref="S4.T4.4.4.1.m1.1.1.3.3.cmml">−</mo><mn id="S4.T4.4.4.1.m1.1.1.3.3.2" xref="S4.T4.4.4.1.m1.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.4.4.1.m1.1b"><apply id="S4.T4.4.4.1.m1.1.1.cmml" xref="S4.T4.4.4.1.m1.1.1"><times id="S4.T4.4.4.1.m1.1.1.1.cmml" xref="S4.T4.4.4.1.m1.1.1.1"></times><cn type="integer" id="S4.T4.4.4.1.m1.1.1.2.cmml" xref="S4.T4.4.4.1.m1.1.1.2">1</cn><apply id="S4.T4.4.4.1.m1.1.1.3.cmml" xref="S4.T4.4.4.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T4.4.4.1.m1.1.1.3.1.cmml" xref="S4.T4.4.4.1.m1.1.1.3">superscript</csymbol><ci id="S4.T4.4.4.1.m1.1.1.3.2.cmml" xref="S4.T4.4.4.1.m1.1.1.3.2">𝑒</ci><apply id="S4.T4.4.4.1.m1.1.1.3.3.cmml" xref="S4.T4.4.4.1.m1.1.1.3.3"><minus id="S4.T4.4.4.1.m1.1.1.3.3.1.cmml" xref="S4.T4.4.4.1.m1.1.1.3.3"></minus><cn type="integer" id="S4.T4.4.4.1.m1.1.1.3.3.2.cmml" xref="S4.T4.4.4.1.m1.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.4.4.1.m1.1c">1e^{-5}</annotation></semantics></math></td>
<td id="S4.T4.5.5.2" class="ltx_td ltx_align_center"><math id="S4.T4.5.5.2.m1.1" class="ltx_Math" alttext="1e^{-4}" display="inline"><semantics id="S4.T4.5.5.2.m1.1a"><mrow id="S4.T4.5.5.2.m1.1.1" xref="S4.T4.5.5.2.m1.1.1.cmml"><mn id="S4.T4.5.5.2.m1.1.1.2" xref="S4.T4.5.5.2.m1.1.1.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S4.T4.5.5.2.m1.1.1.1" xref="S4.T4.5.5.2.m1.1.1.1.cmml">​</mo><msup id="S4.T4.5.5.2.m1.1.1.3" xref="S4.T4.5.5.2.m1.1.1.3.cmml"><mi id="S4.T4.5.5.2.m1.1.1.3.2" xref="S4.T4.5.5.2.m1.1.1.3.2.cmml">e</mi><mrow id="S4.T4.5.5.2.m1.1.1.3.3" xref="S4.T4.5.5.2.m1.1.1.3.3.cmml"><mo id="S4.T4.5.5.2.m1.1.1.3.3a" xref="S4.T4.5.5.2.m1.1.1.3.3.cmml">−</mo><mn id="S4.T4.5.5.2.m1.1.1.3.3.2" xref="S4.T4.5.5.2.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.5.5.2.m1.1b"><apply id="S4.T4.5.5.2.m1.1.1.cmml" xref="S4.T4.5.5.2.m1.1.1"><times id="S4.T4.5.5.2.m1.1.1.1.cmml" xref="S4.T4.5.5.2.m1.1.1.1"></times><cn type="integer" id="S4.T4.5.5.2.m1.1.1.2.cmml" xref="S4.T4.5.5.2.m1.1.1.2">1</cn><apply id="S4.T4.5.5.2.m1.1.1.3.cmml" xref="S4.T4.5.5.2.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T4.5.5.2.m1.1.1.3.1.cmml" xref="S4.T4.5.5.2.m1.1.1.3">superscript</csymbol><ci id="S4.T4.5.5.2.m1.1.1.3.2.cmml" xref="S4.T4.5.5.2.m1.1.1.3.2">𝑒</ci><apply id="S4.T4.5.5.2.m1.1.1.3.3.cmml" xref="S4.T4.5.5.2.m1.1.1.3.3"><minus id="S4.T4.5.5.2.m1.1.1.3.3.1.cmml" xref="S4.T4.5.5.2.m1.1.1.3.3"></minus><cn type="integer" id="S4.T4.5.5.2.m1.1.1.3.3.2.cmml" xref="S4.T4.5.5.2.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.5.5.2.m1.1c">1e^{-4}</annotation></semantics></math></td>
<td id="S4.T4.6.6.3" class="ltx_td ltx_align_center"><math id="S4.T4.6.6.3.m1.1" class="ltx_Math" alttext="1e^{-5}" display="inline"><semantics id="S4.T4.6.6.3.m1.1a"><mrow id="S4.T4.6.6.3.m1.1.1" xref="S4.T4.6.6.3.m1.1.1.cmml"><mn id="S4.T4.6.6.3.m1.1.1.2" xref="S4.T4.6.6.3.m1.1.1.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S4.T4.6.6.3.m1.1.1.1" xref="S4.T4.6.6.3.m1.1.1.1.cmml">​</mo><msup id="S4.T4.6.6.3.m1.1.1.3" xref="S4.T4.6.6.3.m1.1.1.3.cmml"><mi id="S4.T4.6.6.3.m1.1.1.3.2" xref="S4.T4.6.6.3.m1.1.1.3.2.cmml">e</mi><mrow id="S4.T4.6.6.3.m1.1.1.3.3" xref="S4.T4.6.6.3.m1.1.1.3.3.cmml"><mo id="S4.T4.6.6.3.m1.1.1.3.3a" xref="S4.T4.6.6.3.m1.1.1.3.3.cmml">−</mo><mn id="S4.T4.6.6.3.m1.1.1.3.3.2" xref="S4.T4.6.6.3.m1.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.6.6.3.m1.1b"><apply id="S4.T4.6.6.3.m1.1.1.cmml" xref="S4.T4.6.6.3.m1.1.1"><times id="S4.T4.6.6.3.m1.1.1.1.cmml" xref="S4.T4.6.6.3.m1.1.1.1"></times><cn type="integer" id="S4.T4.6.6.3.m1.1.1.2.cmml" xref="S4.T4.6.6.3.m1.1.1.2">1</cn><apply id="S4.T4.6.6.3.m1.1.1.3.cmml" xref="S4.T4.6.6.3.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T4.6.6.3.m1.1.1.3.1.cmml" xref="S4.T4.6.6.3.m1.1.1.3">superscript</csymbol><ci id="S4.T4.6.6.3.m1.1.1.3.2.cmml" xref="S4.T4.6.6.3.m1.1.1.3.2">𝑒</ci><apply id="S4.T4.6.6.3.m1.1.1.3.3.cmml" xref="S4.T4.6.6.3.m1.1.1.3.3"><minus id="S4.T4.6.6.3.m1.1.1.3.3.1.cmml" xref="S4.T4.6.6.3.m1.1.1.3.3"></minus><cn type="integer" id="S4.T4.6.6.3.m1.1.1.3.3.2.cmml" xref="S4.T4.6.6.3.m1.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.6.6.3.m1.1c">1e^{-5}</annotation></semantics></math></td>
<td id="S4.T4.6.6.4" class="ltx_td ltx_align_center">63.1</td>
<td id="S4.T4.6.6.5" class="ltx_td ltx_align_center">72.4</td>
</tr>
<tr id="S4.T4.9.9" class="ltx_tr">
<td id="S4.T4.7.7.1" class="ltx_td ltx_align_left"><math id="S4.T4.7.7.1.m1.1" class="ltx_Math" alttext="1e^{-6}" display="inline"><semantics id="S4.T4.7.7.1.m1.1a"><mrow id="S4.T4.7.7.1.m1.1.1" xref="S4.T4.7.7.1.m1.1.1.cmml"><mn id="S4.T4.7.7.1.m1.1.1.2" xref="S4.T4.7.7.1.m1.1.1.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S4.T4.7.7.1.m1.1.1.1" xref="S4.T4.7.7.1.m1.1.1.1.cmml">​</mo><msup id="S4.T4.7.7.1.m1.1.1.3" xref="S4.T4.7.7.1.m1.1.1.3.cmml"><mi id="S4.T4.7.7.1.m1.1.1.3.2" xref="S4.T4.7.7.1.m1.1.1.3.2.cmml">e</mi><mrow id="S4.T4.7.7.1.m1.1.1.3.3" xref="S4.T4.7.7.1.m1.1.1.3.3.cmml"><mo id="S4.T4.7.7.1.m1.1.1.3.3a" xref="S4.T4.7.7.1.m1.1.1.3.3.cmml">−</mo><mn id="S4.T4.7.7.1.m1.1.1.3.3.2" xref="S4.T4.7.7.1.m1.1.1.3.3.2.cmml">6</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.7.7.1.m1.1b"><apply id="S4.T4.7.7.1.m1.1.1.cmml" xref="S4.T4.7.7.1.m1.1.1"><times id="S4.T4.7.7.1.m1.1.1.1.cmml" xref="S4.T4.7.7.1.m1.1.1.1"></times><cn type="integer" id="S4.T4.7.7.1.m1.1.1.2.cmml" xref="S4.T4.7.7.1.m1.1.1.2">1</cn><apply id="S4.T4.7.7.1.m1.1.1.3.cmml" xref="S4.T4.7.7.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T4.7.7.1.m1.1.1.3.1.cmml" xref="S4.T4.7.7.1.m1.1.1.3">superscript</csymbol><ci id="S4.T4.7.7.1.m1.1.1.3.2.cmml" xref="S4.T4.7.7.1.m1.1.1.3.2">𝑒</ci><apply id="S4.T4.7.7.1.m1.1.1.3.3.cmml" xref="S4.T4.7.7.1.m1.1.1.3.3"><minus id="S4.T4.7.7.1.m1.1.1.3.3.1.cmml" xref="S4.T4.7.7.1.m1.1.1.3.3"></minus><cn type="integer" id="S4.T4.7.7.1.m1.1.1.3.3.2.cmml" xref="S4.T4.7.7.1.m1.1.1.3.3.2">6</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.7.7.1.m1.1c">1e^{-6}</annotation></semantics></math></td>
<td id="S4.T4.8.8.2" class="ltx_td ltx_align_center"><math id="S4.T4.8.8.2.m1.1" class="ltx_Math" alttext="1e^{-4}" display="inline"><semantics id="S4.T4.8.8.2.m1.1a"><mrow id="S4.T4.8.8.2.m1.1.1" xref="S4.T4.8.8.2.m1.1.1.cmml"><mn id="S4.T4.8.8.2.m1.1.1.2" xref="S4.T4.8.8.2.m1.1.1.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S4.T4.8.8.2.m1.1.1.1" xref="S4.T4.8.8.2.m1.1.1.1.cmml">​</mo><msup id="S4.T4.8.8.2.m1.1.1.3" xref="S4.T4.8.8.2.m1.1.1.3.cmml"><mi id="S4.T4.8.8.2.m1.1.1.3.2" xref="S4.T4.8.8.2.m1.1.1.3.2.cmml">e</mi><mrow id="S4.T4.8.8.2.m1.1.1.3.3" xref="S4.T4.8.8.2.m1.1.1.3.3.cmml"><mo id="S4.T4.8.8.2.m1.1.1.3.3a" xref="S4.T4.8.8.2.m1.1.1.3.3.cmml">−</mo><mn id="S4.T4.8.8.2.m1.1.1.3.3.2" xref="S4.T4.8.8.2.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.8.8.2.m1.1b"><apply id="S4.T4.8.8.2.m1.1.1.cmml" xref="S4.T4.8.8.2.m1.1.1"><times id="S4.T4.8.8.2.m1.1.1.1.cmml" xref="S4.T4.8.8.2.m1.1.1.1"></times><cn type="integer" id="S4.T4.8.8.2.m1.1.1.2.cmml" xref="S4.T4.8.8.2.m1.1.1.2">1</cn><apply id="S4.T4.8.8.2.m1.1.1.3.cmml" xref="S4.T4.8.8.2.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T4.8.8.2.m1.1.1.3.1.cmml" xref="S4.T4.8.8.2.m1.1.1.3">superscript</csymbol><ci id="S4.T4.8.8.2.m1.1.1.3.2.cmml" xref="S4.T4.8.8.2.m1.1.1.3.2">𝑒</ci><apply id="S4.T4.8.8.2.m1.1.1.3.3.cmml" xref="S4.T4.8.8.2.m1.1.1.3.3"><minus id="S4.T4.8.8.2.m1.1.1.3.3.1.cmml" xref="S4.T4.8.8.2.m1.1.1.3.3"></minus><cn type="integer" id="S4.T4.8.8.2.m1.1.1.3.3.2.cmml" xref="S4.T4.8.8.2.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.8.8.2.m1.1c">1e^{-4}</annotation></semantics></math></td>
<td id="S4.T4.9.9.3" class="ltx_td ltx_align_center"><math id="S4.T4.9.9.3.m1.1" class="ltx_Math" alttext="1e^{-5}" display="inline"><semantics id="S4.T4.9.9.3.m1.1a"><mrow id="S4.T4.9.9.3.m1.1.1" xref="S4.T4.9.9.3.m1.1.1.cmml"><mn id="S4.T4.9.9.3.m1.1.1.2" xref="S4.T4.9.9.3.m1.1.1.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S4.T4.9.9.3.m1.1.1.1" xref="S4.T4.9.9.3.m1.1.1.1.cmml">​</mo><msup id="S4.T4.9.9.3.m1.1.1.3" xref="S4.T4.9.9.3.m1.1.1.3.cmml"><mi id="S4.T4.9.9.3.m1.1.1.3.2" xref="S4.T4.9.9.3.m1.1.1.3.2.cmml">e</mi><mrow id="S4.T4.9.9.3.m1.1.1.3.3" xref="S4.T4.9.9.3.m1.1.1.3.3.cmml"><mo id="S4.T4.9.9.3.m1.1.1.3.3a" xref="S4.T4.9.9.3.m1.1.1.3.3.cmml">−</mo><mn id="S4.T4.9.9.3.m1.1.1.3.3.2" xref="S4.T4.9.9.3.m1.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.9.9.3.m1.1b"><apply id="S4.T4.9.9.3.m1.1.1.cmml" xref="S4.T4.9.9.3.m1.1.1"><times id="S4.T4.9.9.3.m1.1.1.1.cmml" xref="S4.T4.9.9.3.m1.1.1.1"></times><cn type="integer" id="S4.T4.9.9.3.m1.1.1.2.cmml" xref="S4.T4.9.9.3.m1.1.1.2">1</cn><apply id="S4.T4.9.9.3.m1.1.1.3.cmml" xref="S4.T4.9.9.3.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T4.9.9.3.m1.1.1.3.1.cmml" xref="S4.T4.9.9.3.m1.1.1.3">superscript</csymbol><ci id="S4.T4.9.9.3.m1.1.1.3.2.cmml" xref="S4.T4.9.9.3.m1.1.1.3.2">𝑒</ci><apply id="S4.T4.9.9.3.m1.1.1.3.3.cmml" xref="S4.T4.9.9.3.m1.1.1.3.3"><minus id="S4.T4.9.9.3.m1.1.1.3.3.1.cmml" xref="S4.T4.9.9.3.m1.1.1.3.3"></minus><cn type="integer" id="S4.T4.9.9.3.m1.1.1.3.3.2.cmml" xref="S4.T4.9.9.3.m1.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.9.9.3.m1.1c">1e^{-5}</annotation></semantics></math></td>
<td id="S4.T4.9.9.4" class="ltx_td ltx_align_center">63.4</td>
<td id="S4.T4.9.9.5" class="ltx_td ltx_align_center">70.8</td>
</tr>
<tr id="S4.T4.12.12" class="ltx_tr">
<td id="S4.T4.10.10.1" class="ltx_td ltx_align_left ltx_border_bb"><math id="S4.T4.10.10.1.m1.1" class="ltx_Math" alttext="5e^{-6}" display="inline"><semantics id="S4.T4.10.10.1.m1.1a"><mrow id="S4.T4.10.10.1.m1.1.1" xref="S4.T4.10.10.1.m1.1.1.cmml"><mn id="S4.T4.10.10.1.m1.1.1.2" xref="S4.T4.10.10.1.m1.1.1.2.cmml">5</mn><mo lspace="0em" rspace="0em" id="S4.T4.10.10.1.m1.1.1.1" xref="S4.T4.10.10.1.m1.1.1.1.cmml">​</mo><msup id="S4.T4.10.10.1.m1.1.1.3" xref="S4.T4.10.10.1.m1.1.1.3.cmml"><mi id="S4.T4.10.10.1.m1.1.1.3.2" xref="S4.T4.10.10.1.m1.1.1.3.2.cmml">e</mi><mrow id="S4.T4.10.10.1.m1.1.1.3.3" xref="S4.T4.10.10.1.m1.1.1.3.3.cmml"><mo id="S4.T4.10.10.1.m1.1.1.3.3a" xref="S4.T4.10.10.1.m1.1.1.3.3.cmml">−</mo><mn id="S4.T4.10.10.1.m1.1.1.3.3.2" xref="S4.T4.10.10.1.m1.1.1.3.3.2.cmml">6</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.10.10.1.m1.1b"><apply id="S4.T4.10.10.1.m1.1.1.cmml" xref="S4.T4.10.10.1.m1.1.1"><times id="S4.T4.10.10.1.m1.1.1.1.cmml" xref="S4.T4.10.10.1.m1.1.1.1"></times><cn type="integer" id="S4.T4.10.10.1.m1.1.1.2.cmml" xref="S4.T4.10.10.1.m1.1.1.2">5</cn><apply id="S4.T4.10.10.1.m1.1.1.3.cmml" xref="S4.T4.10.10.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T4.10.10.1.m1.1.1.3.1.cmml" xref="S4.T4.10.10.1.m1.1.1.3">superscript</csymbol><ci id="S4.T4.10.10.1.m1.1.1.3.2.cmml" xref="S4.T4.10.10.1.m1.1.1.3.2">𝑒</ci><apply id="S4.T4.10.10.1.m1.1.1.3.3.cmml" xref="S4.T4.10.10.1.m1.1.1.3.3"><minus id="S4.T4.10.10.1.m1.1.1.3.3.1.cmml" xref="S4.T4.10.10.1.m1.1.1.3.3"></minus><cn type="integer" id="S4.T4.10.10.1.m1.1.1.3.3.2.cmml" xref="S4.T4.10.10.1.m1.1.1.3.3.2">6</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.10.10.1.m1.1c">5e^{-6}</annotation></semantics></math></td>
<td id="S4.T4.11.11.2" class="ltx_td ltx_align_center ltx_border_bb"><math id="S4.T4.11.11.2.m1.1" class="ltx_Math" alttext="5e^{-4}" display="inline"><semantics id="S4.T4.11.11.2.m1.1a"><mrow id="S4.T4.11.11.2.m1.1.1" xref="S4.T4.11.11.2.m1.1.1.cmml"><mn id="S4.T4.11.11.2.m1.1.1.2" xref="S4.T4.11.11.2.m1.1.1.2.cmml">5</mn><mo lspace="0em" rspace="0em" id="S4.T4.11.11.2.m1.1.1.1" xref="S4.T4.11.11.2.m1.1.1.1.cmml">​</mo><msup id="S4.T4.11.11.2.m1.1.1.3" xref="S4.T4.11.11.2.m1.1.1.3.cmml"><mi id="S4.T4.11.11.2.m1.1.1.3.2" xref="S4.T4.11.11.2.m1.1.1.3.2.cmml">e</mi><mrow id="S4.T4.11.11.2.m1.1.1.3.3" xref="S4.T4.11.11.2.m1.1.1.3.3.cmml"><mo id="S4.T4.11.11.2.m1.1.1.3.3a" xref="S4.T4.11.11.2.m1.1.1.3.3.cmml">−</mo><mn id="S4.T4.11.11.2.m1.1.1.3.3.2" xref="S4.T4.11.11.2.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.11.11.2.m1.1b"><apply id="S4.T4.11.11.2.m1.1.1.cmml" xref="S4.T4.11.11.2.m1.1.1"><times id="S4.T4.11.11.2.m1.1.1.1.cmml" xref="S4.T4.11.11.2.m1.1.1.1"></times><cn type="integer" id="S4.T4.11.11.2.m1.1.1.2.cmml" xref="S4.T4.11.11.2.m1.1.1.2">5</cn><apply id="S4.T4.11.11.2.m1.1.1.3.cmml" xref="S4.T4.11.11.2.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T4.11.11.2.m1.1.1.3.1.cmml" xref="S4.T4.11.11.2.m1.1.1.3">superscript</csymbol><ci id="S4.T4.11.11.2.m1.1.1.3.2.cmml" xref="S4.T4.11.11.2.m1.1.1.3.2">𝑒</ci><apply id="S4.T4.11.11.2.m1.1.1.3.3.cmml" xref="S4.T4.11.11.2.m1.1.1.3.3"><minus id="S4.T4.11.11.2.m1.1.1.3.3.1.cmml" xref="S4.T4.11.11.2.m1.1.1.3.3"></minus><cn type="integer" id="S4.T4.11.11.2.m1.1.1.3.3.2.cmml" xref="S4.T4.11.11.2.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.11.11.2.m1.1c">5e^{-4}</annotation></semantics></math></td>
<td id="S4.T4.12.12.3" class="ltx_td ltx_align_center ltx_border_bb"><math id="S4.T4.12.12.3.m1.1" class="ltx_Math" alttext="5e^{-6}" display="inline"><semantics id="S4.T4.12.12.3.m1.1a"><mrow id="S4.T4.12.12.3.m1.1.1" xref="S4.T4.12.12.3.m1.1.1.cmml"><mn id="S4.T4.12.12.3.m1.1.1.2" xref="S4.T4.12.12.3.m1.1.1.2.cmml">5</mn><mo lspace="0em" rspace="0em" id="S4.T4.12.12.3.m1.1.1.1" xref="S4.T4.12.12.3.m1.1.1.1.cmml">​</mo><msup id="S4.T4.12.12.3.m1.1.1.3" xref="S4.T4.12.12.3.m1.1.1.3.cmml"><mi id="S4.T4.12.12.3.m1.1.1.3.2" xref="S4.T4.12.12.3.m1.1.1.3.2.cmml">e</mi><mrow id="S4.T4.12.12.3.m1.1.1.3.3" xref="S4.T4.12.12.3.m1.1.1.3.3.cmml"><mo id="S4.T4.12.12.3.m1.1.1.3.3a" xref="S4.T4.12.12.3.m1.1.1.3.3.cmml">−</mo><mn id="S4.T4.12.12.3.m1.1.1.3.3.2" xref="S4.T4.12.12.3.m1.1.1.3.3.2.cmml">6</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.12.12.3.m1.1b"><apply id="S4.T4.12.12.3.m1.1.1.cmml" xref="S4.T4.12.12.3.m1.1.1"><times id="S4.T4.12.12.3.m1.1.1.1.cmml" xref="S4.T4.12.12.3.m1.1.1.1"></times><cn type="integer" id="S4.T4.12.12.3.m1.1.1.2.cmml" xref="S4.T4.12.12.3.m1.1.1.2">5</cn><apply id="S4.T4.12.12.3.m1.1.1.3.cmml" xref="S4.T4.12.12.3.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T4.12.12.3.m1.1.1.3.1.cmml" xref="S4.T4.12.12.3.m1.1.1.3">superscript</csymbol><ci id="S4.T4.12.12.3.m1.1.1.3.2.cmml" xref="S4.T4.12.12.3.m1.1.1.3.2">𝑒</ci><apply id="S4.T4.12.12.3.m1.1.1.3.3.cmml" xref="S4.T4.12.12.3.m1.1.1.3.3"><minus id="S4.T4.12.12.3.m1.1.1.3.3.1.cmml" xref="S4.T4.12.12.3.m1.1.1.3.3"></minus><cn type="integer" id="S4.T4.12.12.3.m1.1.1.3.3.2.cmml" xref="S4.T4.12.12.3.m1.1.1.3.3.2">6</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.12.12.3.m1.1c">5e^{-6}</annotation></semantics></math></td>
<td id="S4.T4.12.12.4" class="ltx_td ltx_align_center ltx_border_bb">64.0</td>
<td id="S4.T4.12.12.5" class="ltx_td ltx_align_center ltx_border_bb">72.9</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.14.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S4.T4.15.2" class="ltx_text" style="font-size:90%;">Empirical experiment with the one-cycle learning rate scheduler. Pedestrian and vehicle performance is measured in AP (Average Precision).</span></figcaption>
</figure>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Number of Decoders</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">Transformers decoders are sequential transformer layers that iteratively refines object-proposals so that they eventually converge to predictions that match the ground-truth as defined in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. In this section, we try to build a relation on how many number of sequential decoder layers are optimal. Similar analysis has also been done in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, but we wanted to confirm the findings based on our own dataset and other training conditions. With our empirical experiments as shown in <a href="#S4.T5" title="In 4.2.2 Number of Decoders ‣ 4.2 Training Strategies ‣ 4 Methodology and Experiments ‣ Training Strategies for Vision Transformers for Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">5</span></a>, we concluded that our baseline number of decoder layers i.e. <span id="S4.SS2.SSS2.p1.1.1" class="ltx_text ltx_font_bold">6</span>, is indeed the most optimal number of decoders for the problem statement.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<table id="S4.T5.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.2.1.1" class="ltx_tr">
<th id="S4.T5.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">#Decoders</th>
<th id="S4.T5.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Pedestrian</th>
<th id="S4.T5.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Vehicle</th>
<th id="S4.T5.2.1.1.4" class="ltx_td ltx_nopad_r ltx_align_right ltx_th ltx_th_column ltx_border_tt">Inference-time</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.2.2.1" class="ltx_tr">
<td id="S4.T5.2.2.1.1" class="ltx_td ltx_align_left ltx_border_t">1</td>
<td id="S4.T5.2.2.1.2" class="ltx_td ltx_align_center ltx_border_t">40.6</td>
<td id="S4.T5.2.2.1.3" class="ltx_td ltx_align_center ltx_border_t">49.7</td>
<td id="S4.T5.2.2.1.4" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t">134</td>
</tr>
<tr id="S4.T5.2.3.2" class="ltx_tr">
<td id="S4.T5.2.3.2.1" class="ltx_td ltx_align_left">2</td>
<td id="S4.T5.2.3.2.2" class="ltx_td ltx_align_center">57.9</td>
<td id="S4.T5.2.3.2.3" class="ltx_td ltx_align_center">64.3</td>
<td id="S4.T5.2.3.2.4" class="ltx_td ltx_nopad_r ltx_align_right">144</td>
</tr>
<tr id="S4.T5.2.4.3" class="ltx_tr">
<td id="S4.T5.2.4.3.1" class="ltx_td ltx_align_left">3</td>
<td id="S4.T5.2.4.3.2" class="ltx_td ltx_align_center">60.1</td>
<td id="S4.T5.2.4.3.3" class="ltx_td ltx_align_center">66.1</td>
<td id="S4.T5.2.4.3.4" class="ltx_td ltx_nopad_r ltx_align_right">155</td>
</tr>
<tr id="S4.T5.2.5.4" class="ltx_tr">
<td id="S4.T5.2.5.4.1" class="ltx_td ltx_align_left">5</td>
<td id="S4.T5.2.5.4.2" class="ltx_td ltx_align_center">61.9</td>
<td id="S4.T5.2.5.4.3" class="ltx_td ltx_align_center">67.4</td>
<td id="S4.T5.2.5.4.4" class="ltx_td ltx_nopad_r ltx_align_right">179</td>
</tr>
<tr id="S4.T5.2.6.5" class="ltx_tr">
<td id="S4.T5.2.6.5.1" class="ltx_td ltx_align_left">6</td>
<td id="S4.T5.2.6.5.2" class="ltx_td ltx_align_center">64.8</td>
<td id="S4.T5.2.6.5.3" class="ltx_td ltx_align_center">69.6</td>
<td id="S4.T5.2.6.5.4" class="ltx_td ltx_nopad_r ltx_align_right">191</td>
</tr>
<tr id="S4.T5.2.7.6" class="ltx_tr">
<td id="S4.T5.2.7.6.1" class="ltx_td ltx_align_left ltx_border_bb">7</td>
<td id="S4.T5.2.7.6.2" class="ltx_td ltx_align_center ltx_border_bb">65.0</td>
<td id="S4.T5.2.7.6.3" class="ltx_td ltx_align_center ltx_border_bb">70.1</td>
<td id="S4.T5.2.7.6.4" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb">202</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T5.3.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="S4.T5.4.2" class="ltx_text" style="font-size:90%;">Empirical experiment with number of transformers decoders. Pedestrian and vehicle performance is measured in AP (Average Precision).</span></figcaption>
</figure>
</section>
<section id="S4.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3 </span>Embedding Dimensions</h4>

<div id="S4.SS2.SSS3.p1" class="ltx_para">
<p id="S4.SS2.SSS3.p1.5" class="ltx_p">In this section we try to formulate the relevance of the long embedding dimension of the queries. Queries are latent representation of the object-proposals which are refined using transformers-decoders for making final predictions. Visualizations of the query in image-space can be referred in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. Baseline taken from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> has embedding dimension of <math id="S4.SS2.SSS3.p1.1.m1.1" class="ltx_Math" alttext="256" display="inline"><semantics id="S4.SS2.SSS3.p1.1.m1.1a"><mn id="S4.SS2.SSS3.p1.1.m1.1.1" xref="S4.SS2.SSS3.p1.1.m1.1.1.cmml">256</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p1.1.m1.1b"><cn type="integer" id="S4.SS2.SSS3.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS3.p1.1.m1.1.1">256</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p1.1.m1.1c">256</annotation></semantics></math>, but based on our results shown in <a href="#S4.T6" title="In 4.2.3 Embedding Dimensions ‣ 4.2 Training Strategies ‣ 4 Methodology and Experiments ‣ Training Strategies for Vision Transformers for Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">6</span></a>, <math id="S4.SS2.SSS3.p1.2.m2.1" class="ltx_Math" alttext="128" display="inline"><semantics id="S4.SS2.SSS3.p1.2.m2.1a"><mn id="S4.SS2.SSS3.p1.2.m2.1.1" xref="S4.SS2.SSS3.p1.2.m2.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p1.2.m2.1b"><cn type="integer" id="S4.SS2.SSS3.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS3.p1.2.m2.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p1.2.m2.1c">128</annotation></semantics></math> dimension gave <math id="S4.SS2.SSS3.p1.3.m3.1" class="ltx_Math" alttext="9.5\%" display="inline"><semantics id="S4.SS2.SSS3.p1.3.m3.1a"><mrow id="S4.SS2.SSS3.p1.3.m3.1.1" xref="S4.SS2.SSS3.p1.3.m3.1.1.cmml"><mn id="S4.SS2.SSS3.p1.3.m3.1.1.2" xref="S4.SS2.SSS3.p1.3.m3.1.1.2.cmml">9.5</mn><mo id="S4.SS2.SSS3.p1.3.m3.1.1.1" xref="S4.SS2.SSS3.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p1.3.m3.1b"><apply id="S4.SS2.SSS3.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS3.p1.3.m3.1.1"><csymbol cd="latexml" id="S4.SS2.SSS3.p1.3.m3.1.1.1.cmml" xref="S4.SS2.SSS3.p1.3.m3.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.SSS3.p1.3.m3.1.1.2.cmml" xref="S4.SS2.SSS3.p1.3.m3.1.1.2">9.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p1.3.m3.1c">9.5\%</annotation></semantics></math> better inference-time with no performance drop at all. It is also worth noting that due to internal optimization of the GPU, embedding dimension has to be of the power of 2 for inference-time optimization. This can be empirically seen in the row with <math id="S4.SS2.SSS3.p1.4.m4.1" class="ltx_Math" alttext="96" display="inline"><semantics id="S4.SS2.SSS3.p1.4.m4.1a"><mn id="S4.SS2.SSS3.p1.4.m4.1.1" xref="S4.SS2.SSS3.p1.4.m4.1.1.cmml">96</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p1.4.m4.1b"><cn type="integer" id="S4.SS2.SSS3.p1.4.m4.1.1.cmml" xref="S4.SS2.SSS3.p1.4.m4.1.1">96</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p1.4.m4.1c">96</annotation></semantics></math> embedding dimension where inference-time is close to the one with <math id="S4.SS2.SSS3.p1.5.m5.1" class="ltx_Math" alttext="128" display="inline"><semantics id="S4.SS2.SSS3.p1.5.m5.1a"><mn id="S4.SS2.SSS3.p1.5.m5.1.1" xref="S4.SS2.SSS3.p1.5.m5.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p1.5.m5.1b"><cn type="integer" id="S4.SS2.SSS3.p1.5.m5.1.1.cmml" xref="S4.SS2.SSS3.p1.5.m5.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p1.5.m5.1c">128</annotation></semantics></math> dimension-size despite having less computations.</p>
</div>
<figure id="S4.T6" class="ltx_table">
<table id="S4.T6.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T6.2.1.1" class="ltx_tr">
<th id="S4.T6.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S4.T6.2.1.1.1.1" class="ltx_text" style="font-size:90%;">Embedding Dimension</span></th>
<th id="S4.T6.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T6.2.1.1.2.1" class="ltx_text" style="font-size:90%;">Pedestrian</span></th>
<th id="S4.T6.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T6.2.1.1.3.1" class="ltx_text" style="font-size:90%;">Vehicle</span></th>
<th id="S4.T6.2.1.1.4" class="ltx_td ltx_nopad_r ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="S4.T6.2.1.1.4.1" class="ltx_text" style="font-size:90%;">Inference-time</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T6.2.2.1" class="ltx_tr">
<td id="S4.T6.2.2.1.1" class="ltx_td ltx_align_left ltx_border_t">256</td>
<td id="S4.T6.2.2.1.2" class="ltx_td ltx_align_center ltx_border_t">64.8</td>
<td id="S4.T6.2.2.1.3" class="ltx_td ltx_align_center ltx_border_t">69.6</td>
<td id="S4.T6.2.2.1.4" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t">211</td>
</tr>
<tr id="S4.T6.2.3.2" class="ltx_tr">
<td id="S4.T6.2.3.2.1" class="ltx_td ltx_align_left">128</td>
<td id="S4.T6.2.3.2.2" class="ltx_td ltx_align_center">64.7</td>
<td id="S4.T6.2.3.2.3" class="ltx_td ltx_align_center">69.9</td>
<td id="S4.T6.2.3.2.4" class="ltx_td ltx_nopad_r ltx_align_right">191</td>
</tr>
<tr id="S4.T6.2.4.3" class="ltx_tr">
<td id="S4.T6.2.4.3.1" class="ltx_td ltx_align_left">96</td>
<td id="S4.T6.2.4.3.2" class="ltx_td ltx_align_center">64.2</td>
<td id="S4.T6.2.4.3.3" class="ltx_td ltx_align_center">68.4</td>
<td id="S4.T6.2.4.3.4" class="ltx_td ltx_nopad_r ltx_align_right">190</td>
</tr>
<tr id="S4.T6.2.5.4" class="ltx_tr">
<td id="S4.T6.2.5.4.1" class="ltx_td ltx_align_left ltx_border_bb">64</td>
<td id="S4.T6.2.5.4.2" class="ltx_td ltx_align_center ltx_border_bb">61.4</td>
<td id="S4.T6.2.5.4.3" class="ltx_td ltx_align_center ltx_border_bb">66.7</td>
<td id="S4.T6.2.5.4.4" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb">183</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T6.3.1.1" class="ltx_text" style="font-size:90%;">Table 6</span>: </span><span id="S4.T6.4.2" class="ltx_text" style="font-size:90%;">Empirical experiment with the varied embedding dimensions of the decoder. Note: FPN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> and transformer queries dimensions are changed together in these experiments. Pedestrian and vehicle performance is measured in AP (Average Precision).</span></figcaption>
</figure>
</section>
<section id="S4.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.4 </span>Number of Queries</h4>

<div id="S4.SS2.SSS4.p1" class="ltx_para">
<p id="S4.SS2.SSS4.p1.6" class="ltx_p">In this section we experimented with the number of object-proposals a.k.a queries in transformers head. This parameter is specially very sensitive to Vision transformer head because we natively don’t perform Non-maximum suppression on the detection output, owing to the fact of using set-based loss. Set-based loss theoretically forces to make only single prediction per ground-truth object, in-comparison to dense-prediction methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> which makes multiple prediction per ground-truth. Theoretical way to get the optimal number for queries is by analyzing the training dataset and finding average number of ground-truths present per training sample. Then we may add <math id="S4.SS2.SSS4.p1.1.m1.1" class="ltx_Math" alttext="10-20\%" display="inline"><semantics id="S4.SS2.SSS4.p1.1.m1.1a"><mrow id="S4.SS2.SSS4.p1.1.m1.1.1" xref="S4.SS2.SSS4.p1.1.m1.1.1.cmml"><mn id="S4.SS2.SSS4.p1.1.m1.1.1.2" xref="S4.SS2.SSS4.p1.1.m1.1.1.2.cmml">10</mn><mo id="S4.SS2.SSS4.p1.1.m1.1.1.1" xref="S4.SS2.SSS4.p1.1.m1.1.1.1.cmml">−</mo><mrow id="S4.SS2.SSS4.p1.1.m1.1.1.3" xref="S4.SS2.SSS4.p1.1.m1.1.1.3.cmml"><mn id="S4.SS2.SSS4.p1.1.m1.1.1.3.2" xref="S4.SS2.SSS4.p1.1.m1.1.1.3.2.cmml">20</mn><mo id="S4.SS2.SSS4.p1.1.m1.1.1.3.1" xref="S4.SS2.SSS4.p1.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p1.1.m1.1b"><apply id="S4.SS2.SSS4.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS4.p1.1.m1.1.1"><minus id="S4.SS2.SSS4.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS4.p1.1.m1.1.1.1"></minus><cn type="integer" id="S4.SS2.SSS4.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS4.p1.1.m1.1.1.2">10</cn><apply id="S4.SS2.SSS4.p1.1.m1.1.1.3.cmml" xref="S4.SS2.SSS4.p1.1.m1.1.1.3"><csymbol cd="latexml" id="S4.SS2.SSS4.p1.1.m1.1.1.3.1.cmml" xref="S4.SS2.SSS4.p1.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="S4.SS2.SSS4.p1.1.m1.1.1.3.2.cmml" xref="S4.SS2.SSS4.p1.1.m1.1.1.3.2">20</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p1.1.m1.1c">10-20\%</annotation></semantics></math> of that number as a buffer for negative class mining. In addition to these analysis we empirically show results in <a href="#S4.T7" title="In 4.2.4 Number of Queries ‣ 4.2 Training Strategies ‣ 4 Methodology and Experiments ‣ Training Strategies for Vision Transformers for Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">7</span></a>. On an average our training dataset has  300 agents per <math id="S4.SS2.SSS4.p1.2.m2.1" class="ltx_Math" alttext="360^{\circ}" display="inline"><semantics id="S4.SS2.SSS4.p1.2.m2.1a"><msup id="S4.SS2.SSS4.p1.2.m2.1.1" xref="S4.SS2.SSS4.p1.2.m2.1.1.cmml"><mn id="S4.SS2.SSS4.p1.2.m2.1.1.2" xref="S4.SS2.SSS4.p1.2.m2.1.1.2.cmml">360</mn><mo id="S4.SS2.SSS4.p1.2.m2.1.1.3" xref="S4.SS2.SSS4.p1.2.m2.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p1.2.m2.1b"><apply id="S4.SS2.SSS4.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS4.p1.2.m2.1.1.1.cmml" xref="S4.SS2.SSS4.p1.2.m2.1.1">superscript</csymbol><cn type="integer" id="S4.SS2.SSS4.p1.2.m2.1.1.2.cmml" xref="S4.SS2.SSS4.p1.2.m2.1.1.2">360</cn><compose id="S4.SS2.SSS4.p1.2.m2.1.1.3.cmml" xref="S4.SS2.SSS4.p1.2.m2.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p1.2.m2.1c">360^{\circ}</annotation></semantics></math> view. We concluded that using <math id="S4.SS2.SSS4.p1.3.m3.1" class="ltx_Math" alttext="400" display="inline"><semantics id="S4.SS2.SSS4.p1.3.m3.1a"><mn id="S4.SS2.SSS4.p1.3.m3.1.1" xref="S4.SS2.SSS4.p1.3.m3.1.1.cmml">400</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p1.3.m3.1b"><cn type="integer" id="S4.SS2.SSS4.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS4.p1.3.m3.1.1">400</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p1.3.m3.1c">400</annotation></semantics></math> queries are optimal for this problem-statement, with a <math id="S4.SS2.SSS4.p1.4.m4.1" class="ltx_Math" alttext="5\%" display="inline"><semantics id="S4.SS2.SSS4.p1.4.m4.1a"><mrow id="S4.SS2.SSS4.p1.4.m4.1.1" xref="S4.SS2.SSS4.p1.4.m4.1.1.cmml"><mn id="S4.SS2.SSS4.p1.4.m4.1.1.2" xref="S4.SS2.SSS4.p1.4.m4.1.1.2.cmml">5</mn><mo id="S4.SS2.SSS4.p1.4.m4.1.1.1" xref="S4.SS2.SSS4.p1.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p1.4.m4.1b"><apply id="S4.SS2.SSS4.p1.4.m4.1.1.cmml" xref="S4.SS2.SSS4.p1.4.m4.1.1"><csymbol cd="latexml" id="S4.SS2.SSS4.p1.4.m4.1.1.1.cmml" xref="S4.SS2.SSS4.p1.4.m4.1.1.1">percent</csymbol><cn type="integer" id="S4.SS2.SSS4.p1.4.m4.1.1.2.cmml" xref="S4.SS2.SSS4.p1.4.m4.1.1.2">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p1.4.m4.1c">5\%</annotation></semantics></math> inference-time improvement at the cost of no performance. Moreover we actually noticed that precision has increased with <math id="S4.SS2.SSS4.p1.5.m5.1" class="ltx_Math" alttext="400" display="inline"><semantics id="S4.SS2.SSS4.p1.5.m5.1a"><mn id="S4.SS2.SSS4.p1.5.m5.1.1" xref="S4.SS2.SSS4.p1.5.m5.1.1.cmml">400</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p1.5.m5.1b"><cn type="integer" id="S4.SS2.SSS4.p1.5.m5.1.1.cmml" xref="S4.SS2.SSS4.p1.5.m5.1.1">400</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p1.5.m5.1c">400</annotation></semantics></math> query model when compared to <math id="S4.SS2.SSS4.p1.6.m6.1" class="ltx_Math" alttext="900" display="inline"><semantics id="S4.SS2.SSS4.p1.6.m6.1a"><mn id="S4.SS2.SSS4.p1.6.m6.1.1" xref="S4.SS2.SSS4.p1.6.m6.1.1.cmml">900</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p1.6.m6.1b"><cn type="integer" id="S4.SS2.SSS4.p1.6.m6.1.1.cmml" xref="S4.SS2.SSS4.p1.6.m6.1.1">900</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p1.6.m6.1c">900</annotation></semantics></math> queries because of more relevant predictions.</p>
</div>
<figure id="S4.T7" class="ltx_table">
<table id="S4.T7.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T7.2.1.1" class="ltx_tr">
<th id="S4.T7.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">#Queries</th>
<th id="S4.T7.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Pedestrian</th>
<th id="S4.T7.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Vehicle</th>
<th id="S4.T7.2.1.1.4" class="ltx_td ltx_nopad_r ltx_align_right ltx_th ltx_th_column ltx_border_tt">Inference-time</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T7.2.2.1" class="ltx_tr">
<td id="S4.T7.2.2.1.1" class="ltx_td ltx_align_left ltx_border_t">900</td>
<td id="S4.T7.2.2.1.2" class="ltx_td ltx_align_center ltx_border_t">64.8</td>
<td id="S4.T7.2.2.1.3" class="ltx_td ltx_align_center ltx_border_t">69.6</td>
<td id="S4.T7.2.2.1.4" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t">191</td>
</tr>
<tr id="S4.T7.2.3.2" class="ltx_tr">
<td id="S4.T7.2.3.2.1" class="ltx_td ltx_align_left">700</td>
<td id="S4.T7.2.3.2.2" class="ltx_td ltx_align_center">64.6</td>
<td id="S4.T7.2.3.2.3" class="ltx_td ltx_align_center">69.5</td>
<td id="S4.T7.2.3.2.4" class="ltx_td ltx_nopad_r ltx_align_right">187</td>
</tr>
<tr id="S4.T7.2.4.3" class="ltx_tr">
<td id="S4.T7.2.4.3.1" class="ltx_td ltx_align_left">500</td>
<td id="S4.T7.2.4.3.2" class="ltx_td ltx_align_center">65.0</td>
<td id="S4.T7.2.4.3.3" class="ltx_td ltx_align_center">69.9</td>
<td id="S4.T7.2.4.3.4" class="ltx_td ltx_nopad_r ltx_align_right">183</td>
</tr>
<tr id="S4.T7.2.5.4" class="ltx_tr">
<td id="S4.T7.2.5.4.1" class="ltx_td ltx_align_left">400</td>
<td id="S4.T7.2.5.4.2" class="ltx_td ltx_align_center">64.8</td>
<td id="S4.T7.2.5.4.3" class="ltx_td ltx_align_center">70.0</td>
<td id="S4.T7.2.5.4.4" class="ltx_td ltx_nopad_r ltx_align_right">181</td>
</tr>
<tr id="S4.T7.2.6.5" class="ltx_tr">
<td id="S4.T7.2.6.5.1" class="ltx_td ltx_align_left ltx_border_bb">300</td>
<td id="S4.T7.2.6.5.2" class="ltx_td ltx_align_center ltx_border_bb">61.5</td>
<td id="S4.T7.2.6.5.3" class="ltx_td ltx_align_center ltx_border_bb">66.7</td>
<td id="S4.T7.2.6.5.4" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb">181</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T7.3.1.1" class="ltx_text" style="font-size:90%;">Table 7</span>: </span><span id="S4.T7.4.2" class="ltx_text" style="font-size:90%;">Empirical experiment with the number of queries. Pedestrian and vehicle performance is measured in AP (Average Precision).</span></figcaption>
</figure>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Post-training Strategies</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">In this study we go through different post-processing techniques we can use to maximize our performance once the training has completed. These post-processing strategies’ inference-time overhead can be treated as negligent compared to the model inference-time.</p>
</div>
<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Pick Top-k boxes</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.1" class="ltx_p">In baseline paper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> we noted that the authors have made use of top-k boxes to filter predictions based on confidence values. We experimented by removing this top-k logic with a parameter sweep, but we concluded that having top-k boxes as 90% of the boxes a.k.a number of queries seemed to be the optimal number with vision transformer detectors.</p>
</div>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Non-Maximum Suppression (NMS)</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.1" class="ltx_p">Seminal paper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, claimed against using NMS approach with transformers based network. Their rationale behind this claim was that self-attention layer and set-based loss during training of the network are enough to not let the network make duplicate detections for an object. However, even after tuning down number of queries to just <math id="S4.SS3.SSS2.p1.1.m1.1" class="ltx_Math" alttext="400" display="inline"><semantics id="S4.SS3.SSS2.p1.1.m1.1a"><mn id="S4.SS3.SSS2.p1.1.m1.1.1" xref="S4.SS3.SSS2.p1.1.m1.1.1.cmml">400</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p1.1.m1.1b"><cn type="integer" id="S4.SS3.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS3.SSS2.p1.1.m1.1.1">400</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p1.1.m1.1c">400</annotation></semantics></math>, we noticed that there are some overlap bounding boxes for a single object. This is a clear application of the NMS method. We went ahead and implemented a simple version of class-agnostic NMS to see performance difference with the baseline in <a href="#S4.T8" title="In 4.3.2 Non-Maximum Suppression (NMS) ‣ 4.3 Post-training Strategies ‣ 4 Methodology and Experiments ‣ Training Strategies for Vision Transformers for Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">8</span></a>. We concluded that NMS has clear gain in vehicle class, as these boxes are generally bigger and transformers may try to associate a ground-truth with multiple queries. However, performance improvement wasn’t very noticeable in case of smaller sized class i.e. pedestrians.</p>
</div>
<figure id="S4.T8" class="ltx_table">
<table id="S4.T8.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T8.4.5.1" class="ltx_tr">
<th id="S4.T8.4.5.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">NMS</th>
<th id="S4.T8.4.5.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">IoU-threshold</th>
<th id="S4.T8.4.5.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Pedestrian</th>
<th id="S4.T8.4.5.1.4" class="ltx_td ltx_nopad_r ltx_align_right ltx_th ltx_th_column ltx_border_tt">Vehicle</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T8.1.1" class="ltx_tr">
<td id="S4.T8.1.1.1" class="ltx_td ltx_align_left ltx_border_t"><math id="S4.T8.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T8.1.1.1.m1.1a"><mo id="S4.T8.1.1.1.m1.1.1" xref="S4.T8.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T8.1.1.1.m1.1b"><times id="S4.T8.1.1.1.m1.1.1.cmml" xref="S4.T8.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.1.1.1.m1.1c">\times</annotation></semantics></math></td>
<td id="S4.T8.1.1.2" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T8.1.1.3" class="ltx_td ltx_align_center ltx_border_t">64.0</td>
<td id="S4.T8.1.1.4" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t">72.9</td>
</tr>
<tr id="S4.T8.2.2" class="ltx_tr">
<td id="S4.T8.2.2.1" class="ltx_td ltx_align_left"><math id="S4.T8.2.2.1.m1.1" class="ltx_Math" alttext="\surd" display="inline"><semantics id="S4.T8.2.2.1.m1.1a"><mo id="S4.T8.2.2.1.m1.1.1" xref="S4.T8.2.2.1.m1.1.1.cmml">√</mo><annotation-xml encoding="MathML-Content" id="S4.T8.2.2.1.m1.1b"><csymbol cd="latexml" id="S4.T8.2.2.1.m1.1.1.cmml" xref="S4.T8.2.2.1.m1.1.1">square-root</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.2.2.1.m1.1c">\surd</annotation></semantics></math></td>
<td id="S4.T8.2.2.2" class="ltx_td ltx_align_center">0.5</td>
<td id="S4.T8.2.2.3" class="ltx_td ltx_align_center">63.1</td>
<td id="S4.T8.2.2.4" class="ltx_td ltx_nopad_r ltx_align_right">72.5</td>
</tr>
<tr id="S4.T8.3.3" class="ltx_tr">
<td id="S4.T8.3.3.1" class="ltx_td ltx_align_left"><math id="S4.T8.3.3.1.m1.1" class="ltx_Math" alttext="\surd" display="inline"><semantics id="S4.T8.3.3.1.m1.1a"><mo id="S4.T8.3.3.1.m1.1.1" xref="S4.T8.3.3.1.m1.1.1.cmml">√</mo><annotation-xml encoding="MathML-Content" id="S4.T8.3.3.1.m1.1b"><csymbol cd="latexml" id="S4.T8.3.3.1.m1.1.1.cmml" xref="S4.T8.3.3.1.m1.1.1">square-root</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.3.3.1.m1.1c">\surd</annotation></semantics></math></td>
<td id="S4.T8.3.3.2" class="ltx_td ltx_align_center">0.3</td>
<td id="S4.T8.3.3.3" class="ltx_td ltx_align_center">63.5</td>
<td id="S4.T8.3.3.4" class="ltx_td ltx_nopad_r ltx_align_right">72.6</td>
</tr>
<tr id="S4.T8.4.4" class="ltx_tr">
<td id="S4.T8.4.4.1" class="ltx_td ltx_align_left ltx_border_bb"><math id="S4.T8.4.4.1.m1.1" class="ltx_Math" alttext="\surd" display="inline"><semantics id="S4.T8.4.4.1.m1.1a"><mo id="S4.T8.4.4.1.m1.1.1" xref="S4.T8.4.4.1.m1.1.1.cmml">√</mo><annotation-xml encoding="MathML-Content" id="S4.T8.4.4.1.m1.1b"><csymbol cd="latexml" id="S4.T8.4.4.1.m1.1.1.cmml" xref="S4.T8.4.4.1.m1.1.1">square-root</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.4.4.1.m1.1c">\surd</annotation></semantics></math></td>
<td id="S4.T8.4.4.2" class="ltx_td ltx_align_center ltx_border_bb">0.2</td>
<td id="S4.T8.4.4.3" class="ltx_td ltx_align_center ltx_border_bb">64.2</td>
<td id="S4.T8.4.4.4" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb">73.6</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T8.6.1.1" class="ltx_text" style="font-size:90%;">Table 8</span>: </span><span id="S4.T8.7.2" class="ltx_text" style="font-size:90%;">Empirical experiment with NMS post-processing. Pedestrian and vehicle performance is measured in AP (Average Precision).</span></figcaption>
</figure>
</section>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Model Formats and Precision</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">In this section we will highlight inference-time improvements we can get with the hardware accelerators after reducing the precision of the models. We will cover two formats: <em id="S4.SS4.p1.1.1" class="ltx_emph ltx_font_italic">PyTorch</em> and <em id="S4.SS4.p1.1.2" class="ltx_emph ltx_font_italic">TensorRT</em> with two different model precision: <em id="S4.SS4.p1.1.3" class="ltx_emph ltx_font_italic">float32</em> and <em id="S4.SS4.p1.1.4" class="ltx_emph ltx_font_italic">float16</em> in <a href="#S4.T9" title="In 4.4 Model Formats and Precision ‣ 4 Methodology and Experiments ‣ Training Strategies for Vision Transformers for Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">9</span></a>. For <em id="S4.SS4.p1.1.5" class="ltx_emph ltx_font_italic">PyTorch</em> model format we noted there’s approximate <math id="S4.SS4.p1.1.m1.1" class="ltx_Math" alttext="50\%" display="inline"><semantics id="S4.SS4.p1.1.m1.1a"><mrow id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml"><mn id="S4.SS4.p1.1.m1.1.1.2" xref="S4.SS4.p1.1.m1.1.1.2.cmml">50</mn><mo id="S4.SS4.p1.1.m1.1.1.1" xref="S4.SS4.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><apply id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS4.p1.1.m1.1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S4.SS4.p1.1.m1.1.1.2.cmml" xref="S4.SS4.p1.1.m1.1.1.2">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">50\%</annotation></semantics></math> inference-time boost just using half-precision a.k.a <em id="S4.SS4.p1.1.6" class="ltx_emph ltx_font_italic">float16</em> model precision. In addition to that <em id="S4.SS4.p1.1.7" class="ltx_emph ltx_font_italic">TensorRT</em> has further speedups by merging layers together and hence reducing number of operations. <em id="S4.SS4.p1.1.8" class="ltx_emph ltx_font_italic">int8</em> model precision wasn’t studied in this analysis. It may provide further boost in the inference-time of these models.</p>
</div>
<figure id="S4.T9" class="ltx_table">
<table id="S4.T9.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T9.2.1.1" class="ltx_tr">
<td id="S4.T9.2.1.1.1" class="ltx_td ltx_border_tt"></td>
<th id="S4.T9.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">PyTorch</th>
<th id="S4.T9.2.1.1.3" class="ltx_td ltx_nopad_r ltx_align_right ltx_th ltx_th_column ltx_border_tt">TensorRT</th>
</tr>
<tr id="S4.T9.2.2.2" class="ltx_tr">
<td id="S4.T9.2.2.2.1" class="ltx_td ltx_align_left ltx_border_t">float32</td>
<td id="S4.T9.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t">187</td>
<td id="S4.T9.2.2.2.3" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t">148</td>
</tr>
<tr id="S4.T9.2.3.3" class="ltx_tr">
<td id="S4.T9.2.3.3.1" class="ltx_td ltx_align_left ltx_border_bb">float16</td>
<td id="S4.T9.2.3.3.2" class="ltx_td ltx_align_center ltx_border_bb">98</td>
<td id="S4.T9.2.3.3.3" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb">62</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T9.3.1.1" class="ltx_text" style="font-size:90%;">Table 9</span>: </span><span id="S4.T9.4.2" class="ltx_text" style="font-size:90%;">Model precision and model-format inference-time comparison. All numbers are measured on the same T4 Nvidia GPU.</span></figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussions</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Societal Impact</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">We ran a series of scaling experiments to derive these training strategies on multiple-multi GPU machines for multiple days. Eventually we were able to prove our hypothesis on down-scaling strategies for hefty vision transformers models. We are hopeful that the cost and environmental effect associated with these experiments would be much lesser compared to the long term benefits these learning would have for future researchers and deployment engineers.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Further Extensions</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Here we will cover the aspects which were not in the scope of the paper but authors believe would be worthwhile to explore for future Vision Transformers strategies. Whole lot of work has happened over CNN-based Neural Architecture Search (NAS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> but we haven’t seen any of such work associated with Vision Transformer models yet. It could be a great direction to explore and effectively convenient too as transformers tend to have less hyper-parameters compared to the CNN networks to search for. In addition we can develop model pruning strategies for Vision Transformers models, looking at inference-time improvements with CNN based Vision models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, we might see some low hanging fruits here. Lastly we think for papers with dense queries <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, researchers could develop on smart ways to automatically block some of the non-practical queries based on the prior HD-map or physics modeling etc. to get gain back the low-latency model.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.2" class="ltx_p">In this work, we analyze runtime-accuracy performance numbers on different network related and pre-processing and post-processing related strategies on Vision Transformers model. We took an extra step of also comparing model runtimes on <em id="S6.p1.2.1" class="ltx_emph ltx_font_italic">TensorRT</em> modules in <em id="S6.p1.2.2" class="ltx_emph ltx_font_italic">float32</em> and <em id="S6.p1.2.3" class="ltx_emph ltx_font_italic">float16</em> precision which is more of a standard practice in the autonomous industry. In addition we also did MACs and #parameters analysis with these techniques to understand differences in the model as per the operations number. We concluded that depending on the problem statement, bigger and deeper models do not always lead to better results; despite wasting resources and environmental impact. With all our strategies, we are able to improve inference-time by <math id="S6.p1.1.m1.1" class="ltx_Math" alttext="63\%" display="inline"><semantics id="S6.p1.1.m1.1a"><mrow id="S6.p1.1.m1.1.1" xref="S6.p1.1.m1.1.1.cmml"><mn id="S6.p1.1.m1.1.1.2" xref="S6.p1.1.m1.1.1.2.cmml">63</mn><mo id="S6.p1.1.m1.1.1.1" xref="S6.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.p1.1.m1.1b"><apply id="S6.p1.1.m1.1.1.cmml" xref="S6.p1.1.m1.1.1"><csymbol cd="latexml" id="S6.p1.1.m1.1.1.1.cmml" xref="S6.p1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S6.p1.1.m1.1.1.2.cmml" xref="S6.p1.1.m1.1.1.2">63</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.1.m1.1c">63\%</annotation></semantics></math> at the cost of performance drop of mere <math id="S6.p1.2.m2.1" class="ltx_Math" alttext="3\%" display="inline"><semantics id="S6.p1.2.m2.1a"><mrow id="S6.p1.2.m2.1.1" xref="S6.p1.2.m2.1.1.cmml"><mn id="S6.p1.2.m2.1.1.2" xref="S6.p1.2.m2.1.1.2.cmml">3</mn><mo id="S6.p1.2.m2.1.1.1" xref="S6.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.p1.2.m2.1b"><apply id="S6.p1.2.m2.1.1.cmml" xref="S6.p1.2.m2.1.1"><csymbol cd="latexml" id="S6.p1.2.m2.1.1.1.cmml" xref="S6.p1.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S6.p1.2.m2.1.1.2.cmml" xref="S6.p1.2.m2.1.1.2">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.2.m2.1c">3\%</annotation></semantics></math> for our problem-statement. We hope researchers and engineers community would benefit from these deployment-time analysis of Vision transformers models on a robotic platforms and can unblock some of the concerns we are facing for the path to deployment for these models. Also we hope to get some environmental positive impact towards scaling down energy-hungry compute resources for beefier object detection algorithms.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Irwan Bello, William Fedus, Xianzhi Du, Ekin Dogus Cubuk, Aravind Srinivas,
Tsung-Yi Lin, Jonathon Shlens, and Barret Zoph.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Revisiting resnets: Improved training and scaling strategies.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib1.4.2" class="ltx_text" style="font-size:90%;">,
34:22614–22627, 2021.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong,
Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">nuscenes: A multimodal dataset for autonomous driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, pages 11621–11631, 2020.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander
Kirillov, and Sergey Zagoruyko.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">End-to-end object detection with transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2020: 16th European Conference,
Glasgow, UK, August 23–28, 2020, Proceedings, Part I 16</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, pages 213–229.
Springer, 2020.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Imagenet: A large-scale hierarchical image database.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2009 IEEE Conference on Computer Vision and Pattern
Recognition</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, pages 248–255, 2009.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">An image is worth 16x16 words: Transformers for image recognition at
scale.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2010.11929</span><span id="bib.bib5.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Xianzhi Du, Barret Zoph, Wei-Chih Hung, and Tsung-Yi Lin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Simple training strategies and model scaling for object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2107.00057</span><span id="bib.bib6.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Kaiwen Duan, Song Bai, Lingxi Xie, Honggang Qi, Qingming Huang, and Qi Tian.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Centernet: Keypoint triplets for object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF international conference on
computer vision</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, pages 6569–6578, 2019.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Rigging the lottery: Making all tickets winners.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, pages
2943–2952. PMLR, 2020.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Jonathan Frankle and Michael Carbin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">The lottery ticket hypothesis: Finding sparse, trainable neural
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1803.03635</span><span id="bib.bib9.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Mask r-cnn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE international conference on computer
vision</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, pages 2961–2969, 2017.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, pages 770–778, 2016.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang,
Tobias Weyand, Marco Andreetto, and Hartwig Adam.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Mobilenets: Efficient convolutional neural networks for mobile vision
applications.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1704.04861</span><span id="bib.bib12.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Jie Hu, Li Shen, and Gang Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Squeeze-and-excitation networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, pages 7132–7141, 2018.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Saghar Irandoust, Thibaut Durand, Yunduz Rakhmangulova, Wenjie Zi, and Hossein
Hajimirsadeghi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Training a vision transformer from scratch in less than 24 hours with
1 gpu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2211.05187</span><span id="bib.bib14.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Yu Qiao,
and Jifeng Dai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Bevformer: Learning bird’s-eye-view representation from
multi-camera images via spatiotemporal transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2022: 17th European Conference, Tel
Aviv, Israel, October 23–27, 2022, Proceedings, Part IX</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, pages 1–18.
Springer, 2022.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan,
and Serge Belongie.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Feature pyramid networks for object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, pages 2117–2125, 2017.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed,
Cheng-Yang Fu, and Alexander C Berg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Ssd: Single shot multibox detector.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2016: 14th European Conference,
Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part I 14</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">,
pages 21–37. Springer, 2016.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Yingfei Liu, Tiancai Wang, Xiangyu Zhang, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Petr: Position embedding transformation for multi-view 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2022: 17th European Conference, Tel
Aviv, Israel, October 23–27, 2022, Proceedings, Part XXVII</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, pages 531–548.
Springer, 2022.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Yingfei Liu, Junjie Yan, Fan Jia, Shuailin Li, Qi Gao, Tiancai Wang, Xiangyu
Zhang, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Petrv2: A unified framework for 3d perception from multi-camera
images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2206.01256</span><span id="bib.bib19.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
Baining Guo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Swin transformer: Hierarchical vision transformer using shifted
windows.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF international conference on
computer vision</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, pages 10012–10022, 2021.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Pruning convolutional neural networks for resource efficient
inference.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1611.06440</span><span id="bib.bib21.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Jongwoo Park, Apoorv Singh, and Varun Bankiti.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">3m3d: Multi-view, multi-path, multi-representation for 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2302.08231</span><span id="bib.bib22.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Jonah Philion and Sanja Fidler.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Lift, splat, shoot: Encoding images from arbitrary camera rigs by
implicitly unprojecting to 3d.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2020: 16th European Conference,
Glasgow, UK, August 23–28, 2020, Proceedings, Part XIV 16</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, pages 194–210.
Springer, 2020.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Martin Popel and Ondřej Bojar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Training tips for the transformer model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1804.00247</span><span id="bib.bib24.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">You only look once: Unified, real-time object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, pages 779–788, 2016.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Faster r-cnn: Towards real-time object detection with region proposal
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib26.4.2" class="ltx_text" style="font-size:90%;">, 28, 2015.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Apoorv Singh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Transformer-based sensor fusion for autonomous driving: A survey.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2302.11481</span><span id="bib.bib27.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Apoorv Singh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Vision-radar fusion for robotics bev detections: A survey.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2302.06643</span><span id="bib.bib28.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Apoorv Singh and Varun Bankiti.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Surround-view vision-based 3d detection for autonomous driving: A
survey.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2302.06650</span><span id="bib.bib29.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Leslie N Smith.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">A disciplined approach to neural network hyper-parameters: Part
1–learning rate, batch size, momentum, and weight decay.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1803.09820</span><span id="bib.bib30.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai
Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Scalability in perception for autonomous driving: Waymo open dataset.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, pages 2446–2454, 2020.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Mingxing Tan and Quoc Le.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Efficientnet: Rethinking model scaling for convolutional neural
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International conference on machine learning</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">, pages
6105–6114. PMLR, 2019.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Zhi Tian, Chunhua Shen, Hao Chen, and Tong He.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Fcos: Fully convolutional one-stage object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF international conference on
computer vision</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:90%;">, pages 9627–9636, 2019.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Attention is all you need.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib34.4.2" class="ltx_text" style="font-size:90%;">, 30, 2017.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Tai Wang, Xinge Zhu, Jiangmiao Pang, and Dahua Lin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Fcos3d: Fully convolutional one-stage monocular 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:90%;">, pages 913–922, 2021.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Yue Wang, Vitor Campagnolo Guizilini, Tianyuan Zhang, Yilun Wang, Hang Zhao,
and Justin Solomon.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Detr3d: 3d object detection from multi-view images via 3d-to-2d
queries.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Conference on Robot Learning</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:90%;">, pages 180–191. PMLR, 2022.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Zi Wang, Chengcheng Li, and Xiangyang Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Convolutional neural network pruning with structural redundancy
reduction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib37.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib37.5.3" class="ltx_text" style="font-size:90%;">, pages 14913–14922, 2021.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Sergi Adipraja Widjaja, Venice Erin Baylon Liong, Zhuang Jie Chong, and Apoorv
Singh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Machine learning-based framework for drivable surface annotation,
Jan. 19 2023.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text" style="font-size:90%;">US Patent App. 17/836,974.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">Scaling vision transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib39.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib39.5.3" class="ltx_text" style="font-size:90%;">, pages 12104–12113, 2022.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Deformable detr: Deformable transformers for end-to-end object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2010.04159</span><span id="bib.bib40.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2304.02184" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2304.02186" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2304.02186">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2304.02186" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2304.02187" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 16:21:53 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
