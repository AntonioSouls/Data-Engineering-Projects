<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2204.07370] 2D Human Pose Estimation: A Survey</title><meta property="og:description" content="Human pose estimation aims at localizing human anatomical keypoints or body parts in the input data (e.g., images, videos, or signals).
It forms a crucial component in enabling machines to have an insightful understand…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="2D Human Pose Estimation: A Survey">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="2D Human Pose Estimation: A Survey">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2204.07370">

<!--Generated on Mon Mar 11 12:10:17 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on Received: date / Accepted: date.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Human pose estimationpose estimation survey deep learning convolutional neural network">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p">∎

</p>
</div>
<span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>✉ Fengcheng Zhou
<br class="ltx_break">
      zfc@zjsu.edu.cn
</span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span><sup id="id2.1" class="ltx_sup">1</sup>    Zhejiang Gongshang University, Hangzhou, China. 
<br class="ltx_break"><sup id="id2.2" class="ltx_sup">2</sup>    Zhejiang Lab, Hangzhou, China.
</span></span></span>
<h1 class="ltx_title ltx_title_document">2D Human Pose Estimation: A Survey</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Haoming Chen <sup id="id2.1.id1" class="ltx_sup">1‡</sup>
</span><span class="ltx_author_notes">‡ The first two authors have equal contribution.</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Runyang Feng <sup id="id3.1.id1" class="ltx_sup">1‡</sup>
</span><span class="ltx_author_notes">* Corresponding Author.</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sifan Wu <sup id="id4.1.id1" class="ltx_sup">1</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hao Xu <sup id="id5.1.id1" class="ltx_sup">2</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Fengcheng Zhou <sup id="id6.1.id1" class="ltx_sup">1*</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhenguang Liu <sup id="id7.1.id1" class="ltx_sup">1</sup>
</span></span>
</div>
<div class="ltx_dates">(Received: date / Accepted: date)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.1" class="ltx_p">Human pose estimation aims at localizing human anatomical keypoints or body parts in the input data (<em id="id1.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, images, videos, or signals).
It forms a crucial component in enabling machines to have an insightful understanding of the behaviors of humans, and has become a salient problem in computer vision and related fields.
Deep learning techniques allow learning feature representations directly from the data, significantly pushing the performance boundary of human pose estimation.
In this paper, we reap the recent achievements of 2D human pose estimation methods and present a comprehensive survey.
Briefly, existing approaches put their efforts in three directions, namely <em id="id1.1.2" class="ltx_emph ltx_font_italic">network architecture design</em>, <em id="id1.1.3" class="ltx_emph ltx_font_italic">network training refinement</em>, and <em id="id1.1.4" class="ltx_emph ltx_font_italic">post processing</em>.
Network architecture design looks at the architecture of human pose estimation models, extracting more robust features for keypoint recognition and localization.
Network training refinement tap into the training of neural networks and aims to improve the representational ability of models.
Post processing further incorporates model-agnostic polishing strategies to improve the performance of keypoint detection.
More than <math id="id1.1.m1.1" class="ltx_Math" alttext="200" display="inline"><semantics id="id1.1.m1.1a"><mn id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">200</mn><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><cn type="integer" id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1">200</cn></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">200</annotation></semantics></math> research contributions are involved in this survey, covering methodological frameworks, common benchmark datasets, evaluation metrics, and performance comparisons.
We seek to provide researchers with a more comprehensive and systematic review on human pose estimation, allowing them to acquire a grand panorama and better identify future directions.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Human pose estimationpose estimation survey deep learning convolutional neural network
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">As a compelling and fundamental problem in computer vision, human pose estimation (HPE) has attracted intense attention in recent years.
As shown in Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ 2D Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the goal of 2D HPE is to: 1) recognize different person instances within the multimedia data (RGB images, videos, RF signals, or radar) recorded by sensors, and 2) to localize a set of pre-defined human anatomical keypoints for each person.
<span id="S1.p1.1.1" class="ltx_text">As the cornerstone of human-centric visual understanding, 2D HPE provides the groundwork for tackling multitudinous higher-order computer vision tasks such as 3D human pose estimation <cite class="ltx_cite ltx_citemacro_cite">Martinez et al. (<a href="#bib.bib111" title="" class="ltx_ref">2017</a>); Chen and Ramanan (<a href="#bib.bib16" title="" class="ltx_ref">2017</a>); Mehta et al. (<a href="#bib.bib112" title="" class="ltx_ref">2017</a>); Zeng et al. (<a href="#bib.bib190" title="" class="ltx_ref">2021</a>); Zou et al. (<a href="#bib.bib206" title="" class="ltx_ref">2021</a>); Garau et al. (<a href="#bib.bib39" title="" class="ltx_ref">2021</a>); Wehrbein et al. (<a href="#bib.bib176" title="" class="ltx_ref">2021</a>); Gao et al. (<a href="#bib.bib38" title="" class="ltx_ref">2016</a>, <a href="#bib.bib37" title="" class="ltx_ref">2015</a>)</cite>, human action recognition <cite class="ltx_cite ltx_citemacro_cite">Wang and Schmid (<a href="#bib.bib167" title="" class="ltx_ref">2013</a>); Baccouche et al. (<a href="#bib.bib4" title="" class="ltx_ref">2011</a>); Ji et al. (<a href="#bib.bib66" title="" class="ltx_ref">2012</a>)</cite>, human parsing <cite class="ltx_cite ltx_citemacro_cite">Ruan et al. (<a href="#bib.bib140" title="" class="ltx_ref">2019</a>); Gong et al. (<a href="#bib.bib44" title="" class="ltx_ref">2018</a>, <a href="#bib.bib43" title="" class="ltx_ref">2017</a>)</cite>, pose tracking <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib172" title="" class="ltx_ref">2020b</a>); Girdhar et al. (<a href="#bib.bib41" title="" class="ltx_ref">2018</a>); Xiao et al. (<a href="#bib.bib181" title="" class="ltx_ref">2018</a>)</cite>, motion prediction <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib98" title="" class="ltx_ref">2019</a>, <a href="#bib.bib102" title="" class="ltx_ref">2021d</a>, <a href="#bib.bib100" title="" class="ltx_ref">2021b</a>)</cite>, human motion retargeting <cite class="ltx_cite ltx_citemacro_cite">Chan et al. (<a href="#bib.bib13" title="" class="ltx_ref">2019</a>); Kappel et al. (<a href="#bib.bib74" title="" class="ltx_ref">2021</a>); Naksuk et al. (<a href="#bib.bib120" title="" class="ltx_ref">2005</a>)</cite>, and vision-and-language conversion <cite class="ltx_cite ltx_citemacro_cite">Guo et al. (<a href="#bib.bib51" title="" class="ltx_ref">2021b</a>); Datta et al. (<a href="#bib.bib22" title="" class="ltx_ref">2019</a>); Mogadala et al. (<a href="#bib.bib117" title="" class="ltx_ref">2021</a>); Guo et al. (<a href="#bib.bib50" title="" class="ltx_ref">2021a</a>, <a href="#bib.bib49" title="" class="ltx_ref">2019b</a>)</cite>.<span id="S1.p1.1.1.1" class="ltx_text">
HPE supports a wide spectrum of applications including human behaviors understanding, motion capture, violence detection, crowd riot scene identification, human-computer interaction, and autonomous driving.</span></span></p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Earlier methods <cite class="ltx_cite ltx_citemacro_cite">Wang and Mori (<a href="#bib.bib174" title="" class="ltx_ref">2008</a>); Wang and Li (<a href="#bib.bib164" title="" class="ltx_ref">2013</a>); Zhang et al. (<a href="#bib.bib196" title="" class="ltx_ref">2009</a>); Sapp et al. (<a href="#bib.bib143" title="" class="ltx_ref">2010</a>)</cite> adopt the probabilistic graphical model to represent relations between joints.
Unfortunately, these methods rely heavily on hand-crafted features which limit their generalization and performance.
More recently, the deep learning techniques <cite class="ltx_cite ltx_citemacro_cite">LeCun et al. (<a href="#bib.bib81" title="" class="ltx_ref">1998</a>); Schmidtke et al. (<a href="#bib.bib146" title="" class="ltx_ref">2021</a>); Liu et al. (<a href="#bib.bib103" title="" class="ltx_ref">2022</a>); Shang et al. (<a href="#bib.bib147" title="" class="ltx_ref">2019</a>); Li et al. (<a href="#bib.bib90" title="" class="ltx_ref">2021c</a>)</cite> enable learning feature representations automatically from data, which has significantly contributed to the advancement of human pose estimation.
These deep learning-based approaches <cite class="ltx_cite ltx_citemacro_cite">Toshev and Szegedy (<a href="#bib.bib161" title="" class="ltx_ref">2014</a>); Liu et al. (<a href="#bib.bib99" title="" class="ltx_ref">2021a</a>); Xiao et al. (<a href="#bib.bib181" title="" class="ltx_ref">2018</a>); Sun et al. (<a href="#bib.bib153" title="" class="ltx_ref">2019</a>); Cao et al. (<a href="#bib.bib9" title="" class="ltx_ref">2017a</a>); Liu et al. (<a href="#bib.bib101" title="" class="ltx_ref">2021c</a>); Li et al. (<a href="#bib.bib86" title="" class="ltx_ref">2021a</a>, <a href="#bib.bib91" title="" class="ltx_ref">d</a>)</cite>, commonly building upon the success of convolutional neural networks, have achieved outstanding performance on this task.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2204.07370/assets/Figures/Fig1-examples.jpg" id="S1.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="271" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An illustration of 2D human pose estimation on multimedia data, including images a), videos b), and RF signals c), d). Note that RGB images c) are presented for visual reference of RF signals-based HPE, and d) shows the skeleton extracted from the RF signals <em id="S1.F1.3.1" class="ltx_emph ltx_font_italic">alone</em>. The pictures in c) and d) are cited from <cite class="ltx_cite ltx_citemacro_cite">Zhao et al. (<a href="#bib.bib199" title="" class="ltx_ref">2018</a>)</cite>.</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Given the rapid development, this paper seeks to track recent progress and summarize their accomplishments to deliver a clearer panorama for 2D human pose estimation.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Several excellent surveys related to human pose estimation have been published, as presented in the Table <a href="#S1.T1" title="Table 1 ‣ 1 Introduction ‣ 2D Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, involving studies in areas of human motion capture and analysis <cite class="ltx_cite ltx_citemacro_cite">Moeslund and Granum (<a href="#bib.bib114" title="" class="ltx_ref">2001</a>); Moeslund et al. (<a href="#bib.bib115" title="" class="ltx_ref">2006</a>); Poppe (<a href="#bib.bib134" title="" class="ltx_ref">2007</a>); Ji and Liu (<a href="#bib.bib67" title="" class="ltx_ref">2009</a>)</cite>, activity recognition and 2D/3D HPE <cite class="ltx_cite ltx_citemacro_cite">Zheng et al. (<a href="#bib.bib200" title="" class="ltx_ref">2020</a>); Chen et al. (<a href="#bib.bib18" title="" class="ltx_ref">2020</a>); Liu et al. (<a href="#bib.bib97" title="" class="ltx_ref">2015</a>)</cite>, <em id="S1.p4.1.1" class="ltx_emph ltx_font_italic">etc</em>.
However, few surveys are dedicated to 2D human pose estimation.
On the other hand, most of existing surveys cast existing approaches into <em id="S1.p4.1.2" class="ltx_emph ltx_font_italic">single-person</em> and <em id="S1.p4.1.3" class="ltx_emph ltx_font_italic">multi-person</em> pose estimation methods.
The <em id="S1.p4.1.4" class="ltx_emph ltx_font_italic">single-person pose estimators</em> typically focus on the model architectures for keypoint detection, and can perform well in the multi-person pose estimation scenarios by predicting pose for each individual person within his/her bounding box.
Therefore, a pose estimation model can accommodate both <em id="S1.p4.1.5" class="ltx_emph ltx_font_italic">single-person</em> and <em id="S1.p4.1.6" class="ltx_emph ltx_font_italic">multi-person</em> scenes, and the division as above might be unnecessary.
Moreover, while human pose estimation on images or videos has been widely concerned, to the best of our knowledge there is still no work that summarizes signal-based human pose estimation, <em id="S1.p4.1.7" class="ltx_emph ltx_font_italic">e.g.</em>, RF signals and radar signals.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In this paper, we roughly cast human pose estimation methods into three categories, each containing several subcategories on a finer level.
(1) <em id="S1.p5.1.1" class="ltx_emph ltx_font_italic">Network architecture design</em> approaches attempt to devise vigorous models that capture robust representations across different scenes to effectively detect keypoints.
Methods in this category concentrate on extracting and processing human body features within a person bounding box <cite class="ltx_cite ltx_citemacro_cite">Xiao et al. (<a href="#bib.bib181" title="" class="ltx_ref">2018</a>); Fang et al. (<a href="#bib.bib34" title="" class="ltx_ref">2017</a>)</cite> or over the entire image <cite class="ltx_cite ltx_citemacro_cite">Cao et al. (<a href="#bib.bib9" title="" class="ltx_ref">2017a</a>); Cheng et al. (<a href="#bib.bib19" title="" class="ltx_ref">2020</a>)</cite>.
(2) <em id="S1.p5.1.2" class="ltx_emph ltx_font_italic">Network training refinement</em> approaches aim at optimizing neural network training, trying to improve the model ability without changing the network structure. Towards this aim, they engage in data augmentation techniques <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib171" title="" class="ltx_ref">2021</a>); Bin et al. (<a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite>, model training strategies <cite class="ltx_cite ltx_citemacro_cite">Xia et al. (<a href="#bib.bib180" title="" class="ltx_ref">2017</a>); Nie et al. (<a href="#bib.bib124" title="" class="ltx_ref">2018b</a>)</cite>, loss function constraints <cite class="ltx_cite ltx_citemacro_cite">Zhou et al. (<a href="#bib.bib203" title="" class="ltx_ref">2020b</a>); Chen et al. (<a href="#bib.bib17" title="" class="ltx_ref">2018</a>)</cite>, and domain adaption methods <cite class="ltx_cite ltx_citemacro_cite">Hidalgo et al. (<a href="#bib.bib55" title="" class="ltx_ref">2019</a>); Xu et al. (<a href="#bib.bib183" title="" class="ltx_ref">2020</a>)</cite>.
(3) <em id="S1.p5.1.3" class="ltx_emph ltx_font_italic">Post processing</em> methods focus on pose polishment upon the coarse pose estimates to improve the performance.
The methods within this category usually behave as a model-agnostic plugin. Representative techniques for pose polishement include quantization error minimization <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib193" title="" class="ltx_ref">2020a</a>); Huang et al. (<a href="#bib.bib58" title="" class="ltx_ref">2020a</a>)</cite> and pose resampling <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib99" title="" class="ltx_ref">2021a</a>); Wang et al. (<a href="#bib.bib172" title="" class="ltx_ref">2020b</a>)</cite>.
<em id="S1.p5.1.4" class="ltx_emph ltx_font_italic">Furthermore</em>, we also discuss the rarely involved topic of reconstructing 2D human poses from signals such as RF signals <cite class="ltx_cite ltx_citemacro_cite">Zhao et al. (<a href="#bib.bib199" title="" class="ltx_ref">2018</a>); Wang et al. (<a href="#bib.bib165" title="" class="ltx_ref">2019a</a>)</cite> and radar signals <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib83" title="" class="ltx_ref">2020a</a>)</cite>, hoping to fill the knowledge gap.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Summary of previous surveys and reviews related to the human pose estimation.</figcaption>
<div id="S1.T1.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:224.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-141.0pt,73.0pt) scale(0.606020552995685,0.606020552995685) ;">
<table id="S1.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S1.T1.1.1.1.1" class="ltx_tr">
<th id="S1.T1.1.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.1.1.1.1.1" class="ltx_p" style="width:241.8pt;">Survey Title</span>
</span>
</th>
<th id="S1.T1.1.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.1.1.2.1.1" class="ltx_p" style="width:28.5pt;">Year</span>
</span>
</th>
<th id="S1.T1.1.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.1.1.3.1.1" class="ltx_p" style="width:28.5pt;">Venue</span>
</span>
</th>
<th id="S1.T1.1.1.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.1.1.4.1.1" class="ltx_p" style="width:284.5pt;">Content</span>
</span>
</th>
<th id="S1.T1.1.1.1.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.1.1.5.1.1" class="ltx_p" style="width:28.5pt;"><span id="S1.T1.1.1.1.1.5.1.1.1" class="ltx_text">Single-Person</span></span>
</span>
</th>
<th id="S1.T1.1.1.1.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.1.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.1.1.6.1.1" class="ltx_p" style="width:28.5pt;"><span id="S1.T1.1.1.1.1.6.1.1.1" class="ltx_text">Multi-Person</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S1.T1.1.1.2.1" class="ltx_tr">
<td id="S1.T1.1.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.2.1.1.1.1" class="ltx_p" style="width:241.8pt;">A Survey of Computer Vision-Based Human Motion Capture <cite class="ltx_cite ltx_citemacro_cite">Moeslund and Granum (<a href="#bib.bib114" title="" class="ltx_ref">2001</a>)</cite></span>
</span>
</td>
<td id="S1.T1.1.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.2.1.2.1.1" class="ltx_p" style="width:28.5pt;">2001</span>
</span>
</td>
<td id="S1.T1.1.1.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.2.1.3.1.1" class="ltx_p" style="width:28.5pt;">CVIU</span>
</span>
</td>
<td id="S1.T1.1.1.2.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.2.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.2.1.4.1.1" class="ltx_p" style="width:284.5pt;">A survey of different functionalities in motion capture system, including initialization, tracking, pose estimation, and recognition.</span>
</span>
</td>
<td id="S1.T1.1.1.2.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.2.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.2.1.5.1.1" class="ltx_p" style="width:28.5pt;"><span id="S1.T1.1.1.2.1.5.1.1.1" class="ltx_text">✓</span></span>
</span>
</td>
<td id="S1.T1.1.1.2.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.2.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.2.1.6.1.1" class="ltx_p" style="width:28.5pt;"><span id="S1.T1.1.1.2.1.6.1.1.1" class="ltx_text">✓</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.1.3.2" class="ltx_tr">
<td id="S1.T1.1.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.3.2.1.1.1" class="ltx_p" style="width:241.8pt;">A survey of advances in vision-based human motion capture and analysis <cite class="ltx_cite ltx_citemacro_cite">Moeslund et al. (<a href="#bib.bib115" title="" class="ltx_ref">2006</a>)</cite></span>
</span>
</td>
<td id="S1.T1.1.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.3.2.2.1.1" class="ltx_p" style="width:28.5pt;">2006</span>
</span>
</td>
<td id="S1.T1.1.1.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.3.2.3.1.1" class="ltx_p" style="width:28.5pt;">CVIU</span>
</span>
</td>
<td id="S1.T1.1.1.3.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.3.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.3.2.4.1.1" class="ltx_p" style="width:284.5pt;">A survey of advances in human motion capture and analysis from 2000 to 2006.</span>
</span>
</td>
<td id="S1.T1.1.1.3.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.3.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.3.2.5.1.1" class="ltx_p" style="width:28.5pt;"><span id="S1.T1.1.1.3.2.5.1.1.1" class="ltx_text">✓</span></span>
</span>
</td>
<td id="S1.T1.1.1.3.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.3.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.3.2.6.1.1" class="ltx_p" style="width:28.5pt;"><span id="S1.T1.1.1.3.2.6.1.1.1" class="ltx_text">✓</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.1.4.3" class="ltx_tr">
<td id="S1.T1.1.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.4.3.1.1.1" class="ltx_p" style="width:241.8pt;">Vision-based human motion analysis: An overview <cite class="ltx_cite ltx_citemacro_cite">Poppe (<a href="#bib.bib134" title="" class="ltx_ref">2007</a>)</cite></span>
</span>
</td>
<td id="S1.T1.1.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.4.3.2.1.1" class="ltx_p" style="width:28.5pt;">2007</span>
</span>
</td>
<td id="S1.T1.1.1.4.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.4.3.3.1.1" class="ltx_p" style="width:28.5pt;">CVIU</span>
</span>
</td>
<td id="S1.T1.1.1.4.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.4.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.4.3.4.1.1" class="ltx_p" style="width:284.5pt;">An overview of markerless vision-based human motion analysis.</span>
</span>
</td>
<td id="S1.T1.1.1.4.3.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.4.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.4.3.5.1.1" class="ltx_p" style="width:28.5pt;"><span id="S1.T1.1.1.4.3.5.1.1.1" class="ltx_text">✓</span></span>
</span>
</td>
<td id="S1.T1.1.1.4.3.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.4.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.4.3.6.1.1" class="ltx_p" style="width:28.5pt;"><span id="S1.T1.1.1.4.3.6.1.1.1" class="ltx_text">✓</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.1.5.4" class="ltx_tr">
<td id="S1.T1.1.1.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.5.4.1.1.1" class="ltx_p" style="width:241.8pt;">Advances in view-invariant human motion analysis: A review <cite class="ltx_cite ltx_citemacro_cite">Ji and Liu (<a href="#bib.bib67" title="" class="ltx_ref">2009</a>)</cite></span>
</span>
</td>
<td id="S1.T1.1.1.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.5.4.2.1.1" class="ltx_p" style="width:28.5pt;">2010</span>
</span>
</td>
<td id="S1.T1.1.1.5.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.5.4.3.1.1" class="ltx_p" style="width:28.5pt;">TSMCS</span>
</span>
</td>
<td id="S1.T1.1.1.5.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.5.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.5.4.4.1.1" class="ltx_p" style="width:284.5pt;">A review of major issues in human motion analysis system, including human detection, view-invariant pose representation and estimation, and human behavior understanding.</span>
</span>
</td>
<td id="S1.T1.1.1.5.4.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.5.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.5.4.5.1.1" class="ltx_p" style="width:28.5pt;"><span id="S1.T1.1.1.5.4.5.1.1.1" class="ltx_text">✓</span></span>
</span>
</td>
<td id="S1.T1.1.1.5.4.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.5.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.5.4.6.1.1" class="ltx_p" style="width:28.5pt;"><span id="S1.T1.1.1.5.4.6.1.1.1" class="ltx_text">✓</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.1.6.5" class="ltx_tr">
<td id="S1.T1.1.1.6.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.6.5.1.1.1" class="ltx_p" style="width:241.8pt;">Visual analysis of humans <cite class="ltx_cite ltx_citemacro_cite">Moeslund et al. (<a href="#bib.bib116" title="" class="ltx_ref">2011</a>)</cite></span>
</span>
</td>
<td id="S1.T1.1.1.6.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.6.5.2.1.1" class="ltx_p" style="width:28.5pt;">2011</span>
</span>
</td>
<td id="S1.T1.1.1.6.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.6.5.3.1.1" class="ltx_p" style="width:28.5pt;">Book</span>
</span>
</td>
<td id="S1.T1.1.1.6.5.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.6.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.6.5.4.1.1" class="ltx_p" style="width:284.5pt;">A comprehensive overview of human analysis such as pose estimation and applications.</span>
</span>
</td>
<td id="S1.T1.1.1.6.5.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.6.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.6.5.5.1.1" class="ltx_p" style="width:28.5pt;"><span id="S1.T1.1.1.6.5.5.1.1.1" class="ltx_text">✓</span></span>
</span>
</td>
<td id="S1.T1.1.1.6.5.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.6.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.6.5.6.1.1" class="ltx_p" style="width:28.5pt;"><span id="S1.T1.1.1.6.5.6.1.1.1" class="ltx_text">✓</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.1.7.6" class="ltx_tr">
<td id="S1.T1.1.1.7.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.7.6.1.1.1" class="ltx_p" style="width:241.8pt;">Human pose estimation and activity recognition from multi-view videos: Comparative explorations of recent developments <cite class="ltx_cite ltx_citemacro_cite">Holte et al. (<a href="#bib.bib57" title="" class="ltx_ref">2012</a>)</cite></span>
</span>
</td>
<td id="S1.T1.1.1.7.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.7.6.2.1.1" class="ltx_p" style="width:28.5pt;">2012</span>
</span>
</td>
<td id="S1.T1.1.1.7.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.7.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.7.6.3.1.1" class="ltx_p" style="width:28.5pt;">JSTSP</span>
</span>
</td>
<td id="S1.T1.1.1.7.6.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.7.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.7.6.4.1.1" class="ltx_p" style="width:284.5pt;">A review of multi-view based 3D human pose estimation and activity recognition.</span>
</span>
</td>
<td id="S1.T1.1.1.7.6.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.7.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.7.6.5.1.1" class="ltx_p" style="width:28.5pt;"><span id="S1.T1.1.1.7.6.5.1.1.1" class="ltx_text">✓</span></span>
</span>
</td>
<td id="S1.T1.1.1.7.6.6" class="ltx_td ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
</tr>
<tr id="S1.T1.1.1.8.7" class="ltx_tr">
<td id="S1.T1.1.1.8.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.8.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.8.7.1.1.1" class="ltx_p" style="width:241.8pt;">A survey of human pose estimation: the body parts parsing based methods <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib97" title="" class="ltx_ref">2015</a>)</cite></span>
</span>
</td>
<td id="S1.T1.1.1.8.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.8.7.2.1.1" class="ltx_p" style="width:28.5pt;">2015</span>
</span>
</td>
<td id="S1.T1.1.1.8.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.8.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.8.7.3.1.1" class="ltx_p" style="width:28.5pt;">JVCIR</span>
</span>
</td>
<td id="S1.T1.1.1.8.7.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.8.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.8.7.4.1.1" class="ltx_p" style="width:284.5pt;">A survey of human parsing based 2D/3D human pose estimation.</span>
</span>
</td>
<td id="S1.T1.1.1.8.7.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.8.7.5.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.8.7.5.1.1" class="ltx_p" style="width:28.5pt;"><span id="S1.T1.1.1.8.7.5.1.1.1" class="ltx_text">✓</span></span>
</span>
</td>
<td id="S1.T1.1.1.8.7.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.8.7.6.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.8.7.6.1.1" class="ltx_p" style="width:28.5pt;"><span id="S1.T1.1.1.8.7.6.1.1.1" class="ltx_text">✓</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.1.9.8" class="ltx_tr">
<td id="S1.T1.1.1.9.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.9.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.9.8.1.1.1" class="ltx_p" style="width:241.8pt;">Human pose estimation from monocular images: A comprehensive survey <cite class="ltx_cite ltx_citemacro_cite">Gong et al. (<a href="#bib.bib45" title="" class="ltx_ref">2016</a>)</cite></span>
</span>
</td>
<td id="S1.T1.1.1.9.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.9.8.2.1.1" class="ltx_p" style="width:28.5pt;">2016</span>
</span>
</td>
<td id="S1.T1.1.1.9.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.9.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.9.8.3.1.1" class="ltx_p" style="width:28.5pt;">Sensors</span>
</span>
</td>
<td id="S1.T1.1.1.9.8.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.9.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.9.8.4.1.1" class="ltx_p" style="width:284.5pt;">A survey of conventional and deep learning methods for human pose estimation.</span>
</span>
</td>
<td id="S1.T1.1.1.9.8.5" class="ltx_td ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S1.T1.1.1.9.8.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.9.8.6.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.9.8.6.1.1" class="ltx_p" style="width:28.5pt;"><span id="S1.T1.1.1.9.8.6.1.1.1" class="ltx_text">✓</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.1.10.9" class="ltx_tr">
<td id="S1.T1.1.1.10.9.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.10.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.10.9.1.1.1" class="ltx_p" style="width:241.8pt;">3d human pose estimation: A review of the literature and analysis of covariates <cite class="ltx_cite ltx_citemacro_cite">Sarafianos et al. (<a href="#bib.bib145" title="" class="ltx_ref">2016</a>)</cite></span>
</span>
</td>
<td id="S1.T1.1.1.10.9.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.10.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.10.9.2.1.1" class="ltx_p" style="width:28.5pt;">2016</span>
</span>
</td>
<td id="S1.T1.1.1.10.9.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.10.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.10.9.3.1.1" class="ltx_p" style="width:28.5pt;">CVIU</span>
</span>
</td>
<td id="S1.T1.1.1.10.9.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.10.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.10.9.4.1.1" class="ltx_p" style="width:284.5pt;">A review of the advances in 3D human pose estimation from RGB images or image sequences.</span>
</span>
</td>
<td id="S1.T1.1.1.10.9.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.10.9.5.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.10.9.5.1.1" class="ltx_p" style="width:28.5pt;"><span id="S1.T1.1.1.10.9.5.1.1.1" class="ltx_text">✓</span></span>
</span>
</td>
<td id="S1.T1.1.1.10.9.6" class="ltx_td ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
</tr>
<tr id="S1.T1.1.1.11.10" class="ltx_tr">
<td id="S1.T1.1.1.11.10.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.11.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.11.10.1.1.1" class="ltx_p" style="width:241.8pt;">Monocular human pose estimation: a survey of deep learning-based methods <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib18" title="" class="ltx_ref">2020</a>)</cite></span>
</span>
</td>
<td id="S1.T1.1.1.11.10.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.11.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.11.10.2.1.1" class="ltx_p" style="width:28.5pt;">2020</span>
</span>
</td>
<td id="S1.T1.1.1.11.10.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.11.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.11.10.3.1.1" class="ltx_p" style="width:28.5pt;">CVIU</span>
</span>
</td>
<td id="S1.T1.1.1.11.10.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.11.10.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.11.10.4.1.1" class="ltx_p" style="width:284.5pt;">A survey of monocular based 2D/3D human pose estimation employing deep learning methods.</span>
</span>
</td>
<td id="S1.T1.1.1.11.10.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.11.10.5.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.11.10.5.1.1" class="ltx_p" style="width:28.5pt;"><span id="S1.T1.1.1.11.10.5.1.1.1" class="ltx_text">✓</span></span>
</span>
</td>
<td id="S1.T1.1.1.11.10.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.11.10.6.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.11.10.6.1.1" class="ltx_p" style="width:28.5pt;"><span id="S1.T1.1.1.11.10.6.1.1.1" class="ltx_text">✓</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.1.12.11" class="ltx_tr">
<td id="S1.T1.1.1.12.11.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.12.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.12.11.1.1.1" class="ltx_p" style="width:241.8pt;">The progress of human pose estimation: a survey and taxonomy of models applied in 2D human pose estimation <cite class="ltx_cite ltx_citemacro_cite">Munea et al. (<a href="#bib.bib119" title="" class="ltx_ref">2020</a>)</cite></span>
</span>
</td>
<td id="S1.T1.1.1.12.11.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.12.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.12.11.2.1.1" class="ltx_p" style="width:28.5pt;">2020</span>
</span>
</td>
<td id="S1.T1.1.1.12.11.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.12.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.12.11.3.1.1" class="ltx_p" style="width:28.5pt;">IEEE Access</span>
</span>
</td>
<td id="S1.T1.1.1.12.11.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.12.11.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.12.11.4.1.1" class="ltx_p" style="width:284.5pt;">A survey of researches on 2D human pose estimation.</span>
</span>
</td>
<td id="S1.T1.1.1.12.11.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.12.11.5.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.12.11.5.1.1" class="ltx_p" style="width:28.5pt;"><span id="S1.T1.1.1.12.11.5.1.1.1" class="ltx_text">✓</span></span>
</span>
</td>
<td id="S1.T1.1.1.12.11.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.12.11.6.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.12.11.6.1.1" class="ltx_p" style="width:28.5pt;"><span id="S1.T1.1.1.12.11.6.1.1.1" class="ltx_text">✓</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.1.13.12" class="ltx_tr">
<td id="S1.T1.1.1.13.12.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.13.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.13.12.1.1.1" class="ltx_p" style="width:241.8pt;">Deep learning-based human pose estimation: A survey <cite class="ltx_cite ltx_citemacro_cite">Zheng et al. (<a href="#bib.bib200" title="" class="ltx_ref">2020</a>)</cite></span>
</span>
</td>
<td id="S1.T1.1.1.13.12.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.13.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.13.12.2.1.1" class="ltx_p" style="width:28.5pt;">2020</span>
</span>
</td>
<td id="S1.T1.1.1.13.12.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.13.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.13.12.3.1.1" class="ltx_p" style="width:28.5pt;">arXiv</span>
</span>
</td>
<td id="S1.T1.1.1.13.12.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.13.12.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.13.12.4.1.1" class="ltx_p" style="width:284.5pt;">A survey of deep learning-based 2D/3D human pose estimation.</span>
</span>
</td>
<td id="S1.T1.1.1.13.12.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.13.12.5.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.13.12.5.1.1" class="ltx_p" style="width:28.5pt;"><span id="S1.T1.1.1.13.12.5.1.1.1" class="ltx_text">✓</span></span>
</span>
</td>
<td id="S1.T1.1.1.13.12.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<span id="S1.T1.1.1.13.12.6.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.13.12.6.1.1" class="ltx_p" style="width:28.5pt;"><span id="S1.T1.1.1.13.12.6.1.1.1" class="ltx_text">✓</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Scope</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">Our scope is limited to 2D human pose estimation with deep learning, we do not consider the conventional non-deep-lear-ning methods.
Topics such as the applications of 2D HPE <cite class="ltx_cite ltx_citemacro_cite">Zheng et al. (<a href="#bib.bib200" title="" class="ltx_ref">2020</a>)</cite> and the representations of human body models <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib18" title="" class="ltx_ref">2020</a>)</cite> that have been adequately covered by other reviews will not be detailed here either.
Nevertheless, there are still a breathtaking number of papers on 2D HPE, hence it is necessary to establish a selection criterion, in such a way that we restrict our attention to the top journal and conference papers since <math id="S1.SS1.p1.1.m1.1" class="ltx_Math" alttext="2014" display="inline"><semantics id="S1.SS1.p1.1.m1.1a"><mn id="S1.SS1.p1.1.m1.1.1" xref="S1.SS1.p1.1.m1.1.1.cmml">2014</mn><annotation-xml encoding="MathML-Content" id="S1.SS1.p1.1.m1.1b"><cn type="integer" id="S1.SS1.p1.1.m1.1.1.cmml" xref="S1.SS1.p1.1.m1.1.1">2014</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.SS1.p1.1.m1.1c">2014</annotation></semantics></math>.
In light of these constraints, we sincerely apologize to those authors whose works are not incorporated into this paper.</p>
</div>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2 </span>Outline</h3>

<div id="S1.SS2.p1" class="ltx_para">
<p id="S1.SS2.p1.1" class="ltx_p">The rest of this paper is organized as follows.
In Section <a href="#S2" title="2 Problem Statement ‣ 2D Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> we provide problem formulations for 2D human pose estimation, and briefly discuss the technological challenges of 2D HPE.
Then, we present works on network architecture design in Section <a href="#S3" title="3 Network Architecture Design Methods ‣ 2D Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, introduce network training refinement methods in Section <a href="#S4" title="4 Network Training Refinement ‣ 2D Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, and review post processing approaches in Section <a href="#S5" title="5 Post Processing Approaches ‣ 2D Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
Subsequently, we summarize the common benchmark datasets, evaluation metrics, and performance comparisons in Section <a href="#S6" title="6 Datasets and Evaluation ‣ 2D Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.
We further provide discussions in Section <a href="#S7" title="7 Discussion ‣ 2D Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, including open questions, signal-based 2D HPE, and future research directions.
Finally, we conclude the paper in Section <a href="#S8" title="8 Conclusion ‣ 2D Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Problem Statement</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we first define the problem of 2D HPE on the image and video data, followed by the discussion of technological challenges in this task.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>The Problem</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.6" class="ltx_p">Formally, the human pose estimation problem can be formualted as follows.
Given an image or a video as input, the goal is to detect the <span id="S2.SS1.p1.6.7" class="ltx_text ltx_font_bold">poses</span> of all persons in the input data.
<span id="S2.SS1.p1.6.6" class="ltx_text">Technically, presented with an observed image <math id="S2.SS1.p1.1.1.m1.1" class="ltx_Math" alttext="{I}" display="inline"><semantics id="S2.SS1.p1.1.1.m1.1a"><mi id="S2.SS1.p1.1.1.m1.1.1" xref="S2.SS1.p1.1.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.1.m1.1b"><ci id="S2.SS1.p1.1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.1.m1.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.1.m1.1c">{I}</annotation></semantics></math>, we aim to detect the pose  of each person <math id="S2.SS1.p1.3.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S2.SS1.p1.3.3.m3.1a"><mi id="S2.SS1.p1.3.3.m3.1.1" xref="S2.SS1.p1.3.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.3.m3.1b"><ci id="S2.SS1.p1.3.3.m3.1.1.cmml" xref="S2.SS1.p1.3.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.3.m3.1c">i</annotation></semantics></math> in the image <math id="S2.SS1.p1.4.4.m4.1" class="ltx_Math" alttext="\mathbf{P}=\{\mathbf{P}_{i}\}_{i=1}^{n}" display="inline"><semantics id="S2.SS1.p1.4.4.m4.1a"><mrow id="S2.SS1.p1.4.4.m4.1.1" xref="S2.SS1.p1.4.4.m4.1.1.cmml"><mi id="S2.SS1.p1.4.4.m4.1.1.3" xref="S2.SS1.p1.4.4.m4.1.1.3.cmml">𝐏</mi><mo id="S2.SS1.p1.4.4.m4.1.1.2" xref="S2.SS1.p1.4.4.m4.1.1.2.cmml">=</mo><msubsup id="S2.SS1.p1.4.4.m4.1.1.1" xref="S2.SS1.p1.4.4.m4.1.1.1.cmml"><mrow id="S2.SS1.p1.4.4.m4.1.1.1.1.1.1" xref="S2.SS1.p1.4.4.m4.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.SS1.p1.4.4.m4.1.1.1.1.1.1.2" xref="S2.SS1.p1.4.4.m4.1.1.1.1.1.2.cmml">{</mo><msub id="S2.SS1.p1.4.4.m4.1.1.1.1.1.1.1" xref="S2.SS1.p1.4.4.m4.1.1.1.1.1.1.1.cmml"><mi id="S2.SS1.p1.4.4.m4.1.1.1.1.1.1.1.2" xref="S2.SS1.p1.4.4.m4.1.1.1.1.1.1.1.2.cmml">𝐏</mi><mi id="S2.SS1.p1.4.4.m4.1.1.1.1.1.1.1.3" xref="S2.SS1.p1.4.4.m4.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S2.SS1.p1.4.4.m4.1.1.1.1.1.1.3" xref="S2.SS1.p1.4.4.m4.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S2.SS1.p1.4.4.m4.1.1.1.1.3" xref="S2.SS1.p1.4.4.m4.1.1.1.1.3.cmml"><mi id="S2.SS1.p1.4.4.m4.1.1.1.1.3.2" xref="S2.SS1.p1.4.4.m4.1.1.1.1.3.2.cmml">i</mi><mo id="S2.SS1.p1.4.4.m4.1.1.1.1.3.1" xref="S2.SS1.p1.4.4.m4.1.1.1.1.3.1.cmml">=</mo><mn id="S2.SS1.p1.4.4.m4.1.1.1.1.3.3" xref="S2.SS1.p1.4.4.m4.1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S2.SS1.p1.4.4.m4.1.1.1.3" xref="S2.SS1.p1.4.4.m4.1.1.1.3.cmml">n</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.4.m4.1b"><apply id="S2.SS1.p1.4.4.m4.1.1.cmml" xref="S2.SS1.p1.4.4.m4.1.1"><eq id="S2.SS1.p1.4.4.m4.1.1.2.cmml" xref="S2.SS1.p1.4.4.m4.1.1.2"></eq><ci id="S2.SS1.p1.4.4.m4.1.1.3.cmml" xref="S2.SS1.p1.4.4.m4.1.1.3">𝐏</ci><apply id="S2.SS1.p1.4.4.m4.1.1.1.cmml" xref="S2.SS1.p1.4.4.m4.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.4.4.m4.1.1.1.2.cmml" xref="S2.SS1.p1.4.4.m4.1.1.1">superscript</csymbol><apply id="S2.SS1.p1.4.4.m4.1.1.1.1.cmml" xref="S2.SS1.p1.4.4.m4.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.4.4.m4.1.1.1.1.2.cmml" xref="S2.SS1.p1.4.4.m4.1.1.1">subscript</csymbol><set id="S2.SS1.p1.4.4.m4.1.1.1.1.1.2.cmml" xref="S2.SS1.p1.4.4.m4.1.1.1.1.1.1"><apply id="S2.SS1.p1.4.4.m4.1.1.1.1.1.1.1.cmml" xref="S2.SS1.p1.4.4.m4.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.4.4.m4.1.1.1.1.1.1.1.1.cmml" xref="S2.SS1.p1.4.4.m4.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.SS1.p1.4.4.m4.1.1.1.1.1.1.1.2.cmml" xref="S2.SS1.p1.4.4.m4.1.1.1.1.1.1.1.2">𝐏</ci><ci id="S2.SS1.p1.4.4.m4.1.1.1.1.1.1.1.3.cmml" xref="S2.SS1.p1.4.4.m4.1.1.1.1.1.1.1.3">𝑖</ci></apply></set><apply id="S2.SS1.p1.4.4.m4.1.1.1.1.3.cmml" xref="S2.SS1.p1.4.4.m4.1.1.1.1.3"><eq id="S2.SS1.p1.4.4.m4.1.1.1.1.3.1.cmml" xref="S2.SS1.p1.4.4.m4.1.1.1.1.3.1"></eq><ci id="S2.SS1.p1.4.4.m4.1.1.1.1.3.2.cmml" xref="S2.SS1.p1.4.4.m4.1.1.1.1.3.2">𝑖</ci><cn type="integer" id="S2.SS1.p1.4.4.m4.1.1.1.1.3.3.cmml" xref="S2.SS1.p1.4.4.m4.1.1.1.1.3.3">1</cn></apply></apply><ci id="S2.SS1.p1.4.4.m4.1.1.1.3.cmml" xref="S2.SS1.p1.4.4.m4.1.1.1.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.4.m4.1c">\mathbf{P}=\{\mathbf{P}_{i}\}_{i=1}^{n}</annotation></semantics></math>, where <math id="S2.SS1.p1.5.5.m5.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.SS1.p1.5.5.m5.1a"><mi id="S2.SS1.p1.5.5.m5.1.1" xref="S2.SS1.p1.5.5.m5.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.5.5.m5.1b"><ci id="S2.SS1.p1.5.5.m5.1.1.cmml" xref="S2.SS1.p1.5.5.m5.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.5.5.m5.1c">n</annotation></semantics></math> denotes the number of persons in <math id="S2.SS1.p1.6.6.m6.1" class="ltx_Math" alttext="{I}" display="inline"><semantics id="S2.SS1.p1.6.6.m6.1a"><mi id="S2.SS1.p1.6.6.m6.1.1" xref="S2.SS1.p1.6.6.m6.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.6.6.m6.1b"><ci id="S2.SS1.p1.6.6.m6.1.1.cmml" xref="S2.SS1.p1.6.6.m6.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.6.6.m6.1c">{I}</annotation></semantics></math>.<span id="S2.SS1.p1.6.6.1" class="ltx_text"></span></span></p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p"><span id="S2.SS1.p2.1.1" class="ltx_text">To describe human poses, skeleton-based model <cite class="ltx_cite ltx_citemacro_cite">Felzenszwalb and Huttenlocher (<a href="#bib.bib35" title="" class="ltx_ref">2005</a>)</cite>, contour-based model <cite class="ltx_cite ltx_citemacro_cite">Ju et al. (<a href="#bib.bib73" title="" class="ltx_ref">1996</a>)</cite>, and volume-based model <cite class="ltx_cite ltx_citemacro_cite">Sidenbladh et al. (<a href="#bib.bib148" title="" class="ltx_ref">2000</a>)</cite> have been proposed in previous works.<span id="S2.SS1.p2.1.1.1" class="ltx_text">
In particular, the contour-based representation contains rough body contour and limb width information while the volume-based representation describes 3D human shapes.
The skeleton-based model, which characterizes the human body as a set of pre-defined joints, has been widely employed in 2D HPE.</span></span></p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Technical Challenges</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Ideally, an algorithm that is both highly accurate and efficient is desired to solve the problem of 2D HPE.
High accuracy detection ensures a precise human body information to facilitate downstream tasks such as 3D HPE and action recognition, while high efficiency allows real-time computing in different devices such as desktops and mobile phones.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Challenges in <em id="S2.SS2.p2.1.1" class="ltx_emph ltx_font_italic">accurate</em> pose detection come from several aspects.
(1) Nuisance phenomena such as under/over-exposure and human-objects entanglement frequently occur in real-world scenes, which may easily lead to detection failure.
(2) Due to the highly flexible human kinematic chains, pose occlusions even self-occlusions in many scenarios are inevitable, which will further confuse keypoint detectors using visual features.
(3) Motion blur and video defocus do frequently happen in videos, which deteriorates the accuracy of pose detection.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">When the pose estimation algorithms are applied to practical applications, besides accurate estimation, the running speed (<em id="S2.SS2.p3.1.1" class="ltx_emph ltx_font_italic">efficiency</em>) is also important.
However, high accuracy and high efficiency are often in conflict to each other since the high accuracy models tend to be deeper, requiring increased resources for computation and storage.
For example, HRNet-W48 <cite class="ltx_cite ltx_citemacro_cite">Sun et al. (<a href="#bib.bib153" title="" class="ltx_ref">2019</a>)</cite> has achieved state-of-the-art results on multiple benchmarks, which however has difficulties in achieving real-time pose estimation even with the help of powerful NVIDIA GTX-1080TI GPUs. Consequently, light-weight models with comparable precision are much coveted for mobile or wearable devices.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Network Architecture Design Methods</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">A key advantage of modern deep learning methods is the ability to learn feature representations automatically from data.
However, feature quality is closely related to the network architecture, therefore the topic of network design deserves to be investigated deeply.
Correspondingly, <em id="S3.p1.1.1" class="ltx_emph ltx_font_italic">network architecture design</em> methods aim at extracting powerful features by investigating various network designs to address human pose estimation.
In this section, we set out to introduce these approaches in detail with a focus on their network architectures.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2204.07370/assets/Figures/Fig2-top-down1.jpg" id="S3.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="231" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>A classical pipeline of top-down framework for human pose estimation. (a): Original image in the dataset. The goal is to detect the poses of all persons in the input image. An off-the-shelf object detector is employed to perform person detection and gives the human proposals. (b): The regions of human proposals are cropped from the original image to form the single person images. (c) Each cropped image is subjected to single person pose estimation (SPPE) to obtain the estimated pose, which is illustrated in (d). (e): All estimated poses are projected to the original image and yield the final results.</figcaption>
</figure>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">On a high level, these approaches typically fall into two general frameworks, namely <span id="S3.p2.1.1" class="ltx_text ltx_font_bold">top-down</span> framework <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib99" title="" class="ltx_ref">2021a</a>); Wei et al. (<a href="#bib.bib178" title="" class="ltx_ref">2016</a>); Fang et al. (<a href="#bib.bib34" title="" class="ltx_ref">2017</a>); Newell et al. (<a href="#bib.bib122" title="" class="ltx_ref">2016b</a>); Bertasius et al. (<a href="#bib.bib5" title="" class="ltx_ref">2019</a>); Sun et al. (<a href="#bib.bib153" title="" class="ltx_ref">2019</a>)</cite> and <span id="S3.p2.1.2" class="ltx_text ltx_font_bold">bottom-up</span> framework <cite class="ltx_cite ltx_citemacro_cite">Cao et al. (<a href="#bib.bib9" title="" class="ltx_ref">2017a</a>); Kreiss et al. (<a href="#bib.bib78" title="" class="ltx_ref">2019</a>); Geng et al. (<a href="#bib.bib40" title="" class="ltx_ref">2021</a>); Luo et al. (<a href="#bib.bib106" title="" class="ltx_ref">2021</a>); Wei et al. (<a href="#bib.bib177" title="" class="ltx_ref">2020</a>); Jin et al. (<a href="#bib.bib70" title="" class="ltx_ref">2020</a>)</cite>.
The top-down paradigm employs a two-step procedure that first detects human bounding boxes and then performs single person pose estimation for each bounding box, which is exemplified in Fig. <a href="#S3.F2" title="Figure 2 ‣ 3 Network Architecture Design Methods ‣ 2D Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
The bottom-up paradigm adopts the part-based procedure that first locates identity-free keypoints and then groups them into different person instances.
We may further divide different methods in these two paradigms into fine-grained sub-categories, where the top-down approaches are categorized into <em id="S3.p2.1.3" class="ltx_emph ltx_font_italic">regression-based</em> <cite class="ltx_cite ltx_citemacro_cite">Toshev and Szegedy (<a href="#bib.bib161" title="" class="ltx_ref">2014</a>); Carreira et al. (<a href="#bib.bib12" title="" class="ltx_ref">2016</a>)</cite>, <em id="S3.p2.1.4" class="ltx_emph ltx_font_italic">heatmap-based</em> <cite class="ltx_cite ltx_citemacro_cite">Sun et al. (<a href="#bib.bib153" title="" class="ltx_ref">2019</a>); Xiao et al. (<a href="#bib.bib181" title="" class="ltx_ref">2018</a>)</cite>, <em id="S3.p2.1.5" class="ltx_emph ltx_font_italic">video-based</em> <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib99" title="" class="ltx_ref">2021a</a>); Luo et al. (<a href="#bib.bib104" title="" class="ltx_ref">2018a</a>)</cite>, and <em id="S3.p2.1.6" class="ltx_emph ltx_font_italic">model compressing-based</em> <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib194" title="" class="ltx_ref">2019</a>); Yu et al. (<a href="#bib.bib187" title="" class="ltx_ref">2021</a>)</cite> methods, and the bottom-up approaches are classified into <em id="S3.p2.1.7" class="ltx_emph ltx_font_italic">one stage</em> <cite class="ltx_cite ltx_citemacro_cite">Nie et al. (<a href="#bib.bib127" title="" class="ltx_ref">2020</a>); Geng et al. (<a href="#bib.bib40" title="" class="ltx_ref">2021</a>)</cite> and <em id="S3.p2.1.8" class="ltx_emph ltx_font_italic">two-stage</em> methods <cite class="ltx_cite ltx_citemacro_cite">Cao et al. (<a href="#bib.bib9" title="" class="ltx_ref">2017a</a>); Kreiss et al. (<a href="#bib.bib78" title="" class="ltx_ref">2019</a>)</cite>.
In what follows, we introduce these categories in detail.
</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Top-Down Framework</h3>

<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Regression-Based Methods</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">Earlier works <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib198" title="" class="ltx_ref">2020b</a>); Wang et al. (<a href="#bib.bib170" title="" class="ltx_ref">2020a</a>); Qiu et al. (<a href="#bib.bib135" title="" class="ltx_ref">2020</a>); Zhang et al. (<a href="#bib.bib192" title="" class="ltx_ref">2018a</a>); Sun et al. (<a href="#bib.bib155" title="" class="ltx_ref">2018</a>); Fieraru et al. (<a href="#bib.bib36" title="" class="ltx_ref">2018</a>); Sun et al. (<a href="#bib.bib154" title="" class="ltx_ref">2017</a>); Carreira et al. (<a href="#bib.bib12" title="" class="ltx_ref">2016</a>); Fan et al. (<a href="#bib.bib33" title="" class="ltx_ref">2015</a>); Toshev and Szegedy (<a href="#bib.bib161" title="" class="ltx_ref">2014</a>); Li et al. (<a href="#bib.bib89" title="" class="ltx_ref">2014</a>)</cite> attempt to learn a mapping from input image to the pre-defined kinematic joints via an end-to-end network, and directly regress the keypoint coordinates, which we refer to as the <em id="S3.SS1.SSS1.p1.1.1" class="ltx_emph ltx_font_italic">regression-based</em> approaches.</p>
</div>
<div id="S3.SS1.SSS1.p2" class="ltx_para">
<p id="S3.SS1.SSS1.p2.1" class="ltx_p">For instance, <span id="S3.SS1.SSS1.p2.1.1" class="ltx_text">DeepPose <cite class="ltx_cite ltx_citemacro_cite">Toshev and Szegedy (<a href="#bib.bib161" title="" class="ltx_ref">2014</a>)</cite> sets the precedent of human pose estimation with deep learning technique. It <cite class="ltx_cite ltx_citemacro_cite">Toshev and Szegedy (<a href="#bib.bib161" title="" class="ltx_ref">2014</a>)</cite> first employs an iterative architecture to extract image features with the cascaded convolutional neural networks (AlexNet <cite class="ltx_cite ltx_citemacro_cite">Krizhevsky et al. (<a href="#bib.bib79" title="" class="ltx_ref">2012</a>)</cite>), and subsequently regresses the joint coordinates with fully connected layers.
Inspired by the remarkable performance of deep learning works such as DeepPose, researchers gradually turned from conventional methods to the deep learning ones.<span id="S3.SS1.SSS1.p2.1.1.1" class="ltx_text">
Building upon the GoogleNet <cite class="ltx_cite ltx_citemacro_cite">Szegedy et al. (<a href="#bib.bib156" title="" class="ltx_ref">2015</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Carreira et al. (<a href="#bib.bib12" title="" class="ltx_ref">2016</a>)</cite> proposes a self-correcting model, which progressively changes the initial joint coordinates estimations instead of directly predicting joint positions.
<cite class="ltx_cite ltx_citemacro_cite">Sun et al. (<a href="#bib.bib154" title="" class="ltx_ref">2017</a>)</cite> presents a structure-aware regression approach that utilizes a novel re-parameterized pose representation of bones. This method is constructed on the ResNet50 <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib52" title="" class="ltx_ref">2016</a>)</cite>, and is able to capture more structural human body information such as joint connections, which enriches the pure joint-based pose descriptions.</span></span></p>
</div>
<div id="S3.SS1.SSS1.p3" class="ltx_para">
<p id="S3.SS1.SSS1.p3.1" class="ltx_p"><span id="S3.SS1.SSS1.p3.1.1" class="ltx_text">Graph convolutional network (GCN) <cite class="ltx_cite ltx_citemacro_cite">Kipf and Welling (<a href="#bib.bib76" title="" class="ltx_ref">2016</a>)</cite> has recently been widely explored, which employs nodes and edges to represent entities and their correlations. Upon convolutions on the graph, the feature of a node is enhanced by incorporating features from the neighboring nodes. Compared to traditional methods, GCN provides another competitive and novel model to characterize the human body.<span id="S3.SS1.SSS1.p3.1.1.1" class="ltx_text">
<cite class="ltx_cite ltx_citemacro_cite">Qiu et al. (<a href="#bib.bib135" title="" class="ltx_ref">2020</a>)</cite> casts the human body as a graph structure where the nodes represent joints and the edges represent bones, and proposes to estimate invisible joints using an Image-Guided Progressive GCN module.</span></span></p>
</div>
<div id="S3.SS1.SSS1.p4" class="ltx_para">
<p id="S3.SS1.SSS1.p4.1" class="ltx_p"><span id="S3.SS1.SSS1.p4.1.1" class="ltx_text">Attention mechanism has greatly advanced the representation learning, and the Transformer <cite class="ltx_cite ltx_citemacro_cite">Carion et al. (<a href="#bib.bib11" title="" class="ltx_ref">2020</a>); Jaderberg et al. (<a href="#bib.bib64" title="" class="ltx_ref">2015</a>); Vaswani et al. (<a href="#bib.bib163" title="" class="ltx_ref">2017</a>); Zhu et al. (<a href="#bib.bib205" title="" class="ltx_ref">2020</a>)</cite> built upon self-attention has established new state-of-the-arts on multiple visual understanding tasks such as object detection, image classification, and semantic segmentation.<span id="S3.SS1.SSS1.p4.1.1.1" class="ltx_text">
<cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib87" title="" class="ltx_ref">2021b</a>)</cite> presents a cascaded Transformers performing end-to-end regression of human and keypoint detection, which first detects the bounding boxes for all persons and then separately regresses all joint coordinates for each person.</span></span></p>
</div>
<div id="S3.SS1.SSS1.p5" class="ltx_para">
<p id="S3.SS1.SSS1.p5.1" class="ltx_p"><span id="S3.SS1.SSS1.p5.1.1" class="ltx_text">The regression-based methods are highly efficient and show promising potential in real-time applications. Unfortunately, such approaches directly output a single 2D coordinates for each joint, failing to consider the area of the body part.<span id="S3.SS1.SSS1.p5.1.1.1" class="ltx_text">
To tackle this issue, heatmap-based approaches are introduced, which localize the keypoints by probabilistic heatmaps instead of determined coordinates.</span></span></p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Heatmap-Based Methods</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.6" class="ltx_p">In order to overcome the shortcomings of direct coordinate regression, heatmap-based joint representations have been widely adopted <cite class="ltx_cite ltx_citemacro_cite">Pfister et al. (<a href="#bib.bib131" title="" class="ltx_ref">2015</a>)</cite>, which leads to an easier optimization and a more robust generalization.
Specifically, the heatmap <math id="S3.SS1.SSS2.p1.1.m1.1" class="ltx_Math" alttext="H_{i}" display="inline"><semantics id="S3.SS1.SSS2.p1.1.m1.1a"><msub id="S3.SS1.SSS2.p1.1.m1.1.1" xref="S3.SS1.SSS2.p1.1.m1.1.1.cmml"><mi id="S3.SS1.SSS2.p1.1.m1.1.1.2" xref="S3.SS1.SSS2.p1.1.m1.1.1.2.cmml">H</mi><mi id="S3.SS1.SSS2.p1.1.m1.1.1.3" xref="S3.SS1.SSS2.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.1.m1.1b"><apply id="S3.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1.2">𝐻</ci><ci id="S3.SS1.SSS2.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.1.m1.1c">H_{i}</annotation></semantics></math> is generated via a 2D Gaussian centered at each joint location <math id="S3.SS1.SSS2.p1.2.m2.2" class="ltx_Math" alttext="(x_{i},y_{i})" display="inline"><semantics id="S3.SS1.SSS2.p1.2.m2.2a"><mrow id="S3.SS1.SSS2.p1.2.m2.2.2.2" xref="S3.SS1.SSS2.p1.2.m2.2.2.3.cmml"><mo stretchy="false" id="S3.SS1.SSS2.p1.2.m2.2.2.2.3" xref="S3.SS1.SSS2.p1.2.m2.2.2.3.cmml">(</mo><msub id="S3.SS1.SSS2.p1.2.m2.1.1.1.1" xref="S3.SS1.SSS2.p1.2.m2.1.1.1.1.cmml"><mi id="S3.SS1.SSS2.p1.2.m2.1.1.1.1.2" xref="S3.SS1.SSS2.p1.2.m2.1.1.1.1.2.cmml">x</mi><mi id="S3.SS1.SSS2.p1.2.m2.1.1.1.1.3" xref="S3.SS1.SSS2.p1.2.m2.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS1.SSS2.p1.2.m2.2.2.2.4" xref="S3.SS1.SSS2.p1.2.m2.2.2.3.cmml">,</mo><msub id="S3.SS1.SSS2.p1.2.m2.2.2.2.2" xref="S3.SS1.SSS2.p1.2.m2.2.2.2.2.cmml"><mi id="S3.SS1.SSS2.p1.2.m2.2.2.2.2.2" xref="S3.SS1.SSS2.p1.2.m2.2.2.2.2.2.cmml">y</mi><mi id="S3.SS1.SSS2.p1.2.m2.2.2.2.2.3" xref="S3.SS1.SSS2.p1.2.m2.2.2.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS1.SSS2.p1.2.m2.2.2.2.5" xref="S3.SS1.SSS2.p1.2.m2.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.2.m2.2b"><interval closure="open" id="S3.SS1.SSS2.p1.2.m2.2.2.3.cmml" xref="S3.SS1.SSS2.p1.2.m2.2.2.2"><apply id="S3.SS1.SSS2.p1.2.m2.1.1.1.1.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.2.m2.1.1.1.1.1.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p1.2.m2.1.1.1.1.2.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1.1.1.2">𝑥</ci><ci id="S3.SS1.SSS2.p1.2.m2.1.1.1.1.3.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1.1.1.3">𝑖</ci></apply><apply id="S3.SS1.SSS2.p1.2.m2.2.2.2.2.cmml" xref="S3.SS1.SSS2.p1.2.m2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.2.m2.2.2.2.2.1.cmml" xref="S3.SS1.SSS2.p1.2.m2.2.2.2.2">subscript</csymbol><ci id="S3.SS1.SSS2.p1.2.m2.2.2.2.2.2.cmml" xref="S3.SS1.SSS2.p1.2.m2.2.2.2.2.2">𝑦</ci><ci id="S3.SS1.SSS2.p1.2.m2.2.2.2.2.3.cmml" xref="S3.SS1.SSS2.p1.2.m2.2.2.2.2.3">𝑖</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.2.m2.2c">(x_{i},y_{i})</annotation></semantics></math>, encoding the probability of the location being the <math id="S3.SS1.SSS2.p1.3.m3.1" class="ltx_Math" alttext="i^{th}" display="inline"><semantics id="S3.SS1.SSS2.p1.3.m3.1a"><msup id="S3.SS1.SSS2.p1.3.m3.1.1" xref="S3.SS1.SSS2.p1.3.m3.1.1.cmml"><mi id="S3.SS1.SSS2.p1.3.m3.1.1.2" xref="S3.SS1.SSS2.p1.3.m3.1.1.2.cmml">i</mi><mrow id="S3.SS1.SSS2.p1.3.m3.1.1.3" xref="S3.SS1.SSS2.p1.3.m3.1.1.3.cmml"><mi id="S3.SS1.SSS2.p1.3.m3.1.1.3.2" xref="S3.SS1.SSS2.p1.3.m3.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS2.p1.3.m3.1.1.3.1" xref="S3.SS1.SSS2.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS1.SSS2.p1.3.m3.1.1.3.3" xref="S3.SS1.SSS2.p1.3.m3.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.3.m3.1b"><apply id="S3.SS1.SSS2.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.3.m3.1.1.1.cmml" xref="S3.SS1.SSS2.p1.3.m3.1.1">superscript</csymbol><ci id="S3.SS1.SSS2.p1.3.m3.1.1.2.cmml" xref="S3.SS1.SSS2.p1.3.m3.1.1.2">𝑖</ci><apply id="S3.SS1.SSS2.p1.3.m3.1.1.3.cmml" xref="S3.SS1.SSS2.p1.3.m3.1.1.3"><times id="S3.SS1.SSS2.p1.3.m3.1.1.3.1.cmml" xref="S3.SS1.SSS2.p1.3.m3.1.1.3.1"></times><ci id="S3.SS1.SSS2.p1.3.m3.1.1.3.2.cmml" xref="S3.SS1.SSS2.p1.3.m3.1.1.3.2">𝑡</ci><ci id="S3.SS1.SSS2.p1.3.m3.1.1.3.3.cmml" xref="S3.SS1.SSS2.p1.3.m3.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.3.m3.1c">i^{th}</annotation></semantics></math> joint.
During training, the goal is to predict <math id="S3.SS1.SSS2.p1.4.m4.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS1.SSS2.p1.4.m4.1a"><mi id="S3.SS1.SSS2.p1.4.m4.1.1" xref="S3.SS1.SSS2.p1.4.m4.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.4.m4.1b"><ci id="S3.SS1.SSS2.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS2.p1.4.m4.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.4.m4.1c">N</annotation></semantics></math> heatmaps <math id="S3.SS1.SSS2.p1.5.m5.1" class="ltx_math_unparsed" alttext="\{H_{1},H_{2},..,H_{N}\}" display="inline"><semantics id="S3.SS1.SSS2.p1.5.m5.1a"><mrow id="S3.SS1.SSS2.p1.5.m5.1b"><mo stretchy="false" id="S3.SS1.SSS2.p1.5.m5.1.1">{</mo><msub id="S3.SS1.SSS2.p1.5.m5.1.2"><mi id="S3.SS1.SSS2.p1.5.m5.1.2.2">H</mi><mn id="S3.SS1.SSS2.p1.5.m5.1.2.3">1</mn></msub><mo id="S3.SS1.SSS2.p1.5.m5.1.3">,</mo><msub id="S3.SS1.SSS2.p1.5.m5.1.4"><mi id="S3.SS1.SSS2.p1.5.m5.1.4.2">H</mi><mn id="S3.SS1.SSS2.p1.5.m5.1.4.3">2</mn></msub><mo id="S3.SS1.SSS2.p1.5.m5.1.5">,</mo><mo lspace="0em" rspace="0.0835em" id="S3.SS1.SSS2.p1.5.m5.1.6">.</mo><mo lspace="0.0835em" rspace="0.167em" id="S3.SS1.SSS2.p1.5.m5.1.7">.</mo><mo id="S3.SS1.SSS2.p1.5.m5.1.8">,</mo><msub id="S3.SS1.SSS2.p1.5.m5.1.9"><mi id="S3.SS1.SSS2.p1.5.m5.1.9.2">H</mi><mi id="S3.SS1.SSS2.p1.5.m5.1.9.3">N</mi></msub><mo stretchy="false" id="S3.SS1.SSS2.p1.5.m5.1.10">}</mo></mrow><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.5.m5.1c">\{H_{1},H_{2},..,H_{N}\}</annotation></semantics></math> for a total of <math id="S3.SS1.SSS2.p1.6.m6.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS1.SSS2.p1.6.m6.1a"><mi id="S3.SS1.SSS2.p1.6.m6.1.1" xref="S3.SS1.SSS2.p1.6.m6.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.6.m6.1b"><ci id="S3.SS1.SSS2.p1.6.m6.1.1.cmml" xref="S3.SS1.SSS2.p1.6.m6.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.6.m6.1c">N</annotation></semantics></math> joints.
Representative <em id="S3.SS1.SSS2.p1.6.1" class="ltx_emph ltx_font_italic">heatmap-based</em> approaches include:</p>
</div>
<div id="S3.SS1.SSS2.p2" class="ltx_para">
<p id="S3.SS1.SSS2.p2.1" class="ltx_p"><span id="S3.SS1.SSS2.p2.1.1" class="ltx_text ltx_font_bold">Iterative Architecture<span id="S3.SS1.SSS2.p2.1.1.1" class="ltx_text ltx_font_medium"> <span id="S3.SS1.SSS2.p2.1.1.1.1" class="ltx_text">
Conventionally, the iterative architecture <cite class="ltx_cite ltx_citemacro_cite">Toshev and Szegedy (<a href="#bib.bib161" title="" class="ltx_ref">2014</a>); Ramakrishna et al. (<a href="#bib.bib137" title="" class="ltx_ref">2014</a>); Wei et al. (<a href="#bib.bib178" title="" class="ltx_ref">2016</a>); Luo et al. (<a href="#bib.bib104" title="" class="ltx_ref">2018a</a>); Carreira et al. (<a href="#bib.bib12" title="" class="ltx_ref">2016</a>)</cite> is designed to produce and refine the keypoint heatmaps.
<cite class="ltx_cite ltx_citemacro_cite">Ramakrishna et al. (<a href="#bib.bib137" title="" class="ltx_ref">2014</a>)</cite> presents an inference machine model which gradually infers the locations of joints in multiple stages.
<cite class="ltx_cite ltx_citemacro_cite">Wei et al. (<a href="#bib.bib178" title="" class="ltx_ref">2016</a>)</cite> further extends the architecture of <cite class="ltx_cite ltx_citemacro_cite">Ramakrishna et al. (<a href="#bib.bib137" title="" class="ltx_ref">2014</a>)</cite> and builds a sequential prediction framework, which employs sequential convolutions to implicitly model long-range spatial dependencies between human body parts.
This approach harvests increasingly refined estimates for joint locations by operating on the results of previous stage, as shown in Fig. <a href="#S3.F3" title="Figure 3 ‣ 3.1.2 Heatmap-Based Methods ‣ 3.1 Top-Down Framework ‣ 3 Network Architecture Design Methods ‣ 2D Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
<cite class="ltx_cite ltx_citemacro_cite">Wei et al. (<a href="#bib.bib178" title="" class="ltx_ref">2016</a>)</cite> additionally proposes intermediate supervision to alleviate the inherent problem of <em id="S3.SS1.SSS2.p2.1.1.1.1.1" class="ltx_emph ltx_font_italic">vanishing gradients</em> in the iterative architectures.</span></span></span></p>
</div>
<div id="S3.SS1.SSS2.p3" class="ltx_para">
<p id="S3.SS1.SSS2.p3.1" class="ltx_p">Although the intermediate supervision strategy relieves the <em id="S3.SS1.SSS2.p3.1.1" class="ltx_emph ltx_font_italic">vanishing gradients</em> of multi-stage models, each stage still fails to build a deep sub-network to extract effective semantic features, which greatly limits their fitting capabilities.
This issue has been tackled with the emergence of residual network (ResNet) <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib52" title="" class="ltx_ref">2016</a>)</cite>, which introduces a shortcut and allows the errors at deeper layers to be back-propagated.
Benefiting from such a way, numerous large models <cite class="ltx_cite ltx_citemacro_cite">Newell et al. (<a href="#bib.bib122" title="" class="ltx_ref">2016b</a>); Chu et al. (<a href="#bib.bib20" title="" class="ltx_ref">2017</a>); Yang et al. (<a href="#bib.bib184" title="" class="ltx_ref">2017</a>); Liu et al. (<a href="#bib.bib96" title="" class="ltx_ref">2018</a>); Tang et al. (<a href="#bib.bib158" title="" class="ltx_ref">2018</a>); Ke et al. (<a href="#bib.bib75" title="" class="ltx_ref">2018</a>); Chen et al. (<a href="#bib.bib17" title="" class="ltx_ref">2018</a>); Xiao et al. (<a href="#bib.bib181" title="" class="ltx_ref">2018</a>); Sun et al. (<a href="#bib.bib153" title="" class="ltx_ref">2019</a>); Su et al. (<a href="#bib.bib152" title="" class="ltx_ref">2019</a>); Cai et al. (<a href="#bib.bib8" title="" class="ltx_ref">2020</a>); Jiang et al. (<a href="#bib.bib68" title="" class="ltx_ref">2020</a>)</cite> have been devised, which greatly boost the process of 2D HPE.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2204.07370/assets/Figures/Fig3-iterative.png" id="S3.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="232" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Illustration of the networks based on iterative architecture. The top portion (a) of the figure depicts the structure of Convolutional Pose Machine <cite class="ltx_cite ltx_citemacro_cite">Wei et al. (<a href="#bib.bib178" title="" class="ltx_ref">2016</a>)</cite> while the bottom part (b) shows the network of LSTM Pose Machines <cite class="ltx_cite ltx_citemacro_cite">Luo et al. (<a href="#bib.bib104" title="" class="ltx_ref">2018a</a>)</cite>. In <cite class="ltx_cite ltx_citemacro_cite">Wei et al. (<a href="#bib.bib178" title="" class="ltx_ref">2016</a>)</cite>, the prediction of each stage and image features are concatenated for the subsequent stage. <cite class="ltx_cite ltx_citemacro_cite">Luo et al. (<a href="#bib.bib104" title="" class="ltx_ref">2018a</a>)</cite> extends <cite class="ltx_cite ltx_citemacro_cite">Wei et al. (<a href="#bib.bib178" title="" class="ltx_ref">2016</a>)</cite> with LSTM. The heatmaps predicted at the previous stage, frame features, and a center map are concatenated to fed into the subsequent stage. Note that different stages in <cite class="ltx_cite ltx_citemacro_cite">Wei et al. (<a href="#bib.bib178" title="" class="ltx_ref">2016</a>)</cite> aim at optimizing pose estimation of the same image, while different stages in <cite class="ltx_cite ltx_citemacro_cite">Luo et al. (<a href="#bib.bib104" title="" class="ltx_ref">2018a</a>)</cite> process various video frames. </figcaption>
</figure>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2204.07370/assets/Figures/Fig4-net2.png" id="S3.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="476" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Illustration of the classical human pose detector that relys on the <em id="S3.F4.10.1" class="ltx_emph ltx_font_italic">high-to-low</em> and <em id="S3.F4.11.2" class="ltx_emph ltx_font_italic">low-to-high</em> framework. (a) Stacked hourglass network <cite class="ltx_cite ltx_citemacro_cite">Newell et al. (<a href="#bib.bib122" title="" class="ltx_ref">2016b</a>)</cite>. (b) Cascaded pyramid networks <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib17" title="" class="ltx_ref">2018</a>)</cite>. (c) SimpleBaseline <cite class="ltx_cite ltx_citemacro_cite">Xiao et al. (<a href="#bib.bib181" title="" class="ltx_ref">2018</a>)</cite>. (d) HRNet <cite class="ltx_cite ltx_citemacro_cite">Sun et al. (<a href="#bib.bib153" title="" class="ltx_ref">2019</a>)</cite>. Legend: <em id="S3.F4.12.3" class="ltx_emph ltx_font_italic">reg. conv.</em> = regular convolution layer, <em id="S3.F4.13.4" class="ltx_emph ltx_font_italic">strided conv.</em> = strided convolution layer for learnable downsampling, <em id="S3.F4.14.5" class="ltx_emph ltx_font_italic">trans. conv.</em> = transposed convolution layer for learnable upsampling, <em id="S3.F4.15.6" class="ltx_emph ltx_font_italic">ele. sum</em> = element-wise summation. For the architecture of (a) stacked hourglass, the high-to-low and low-to-high network architectures are symmetric. In (b) and (c), the high-to-low process is performed by a large visual backbone network (ResNet) which is <span id="S3.F4.16.7" class="ltx_text ltx_font_bold">heavy</span>, while the low-to-high process is implemented by some transposed convolutions or directly upsampling, which is <span id="S3.F4.17.8" class="ltx_text ltx_font_bold">light</span>. In (c), the skip-connection (dashed lines) aims to fuse the features with same spatial size in the high-to-low and low-to-high process. In HRNet (d), the high resolution representation is maintained in the entire propagation, and repeated multi-scale fusions are performed, each resolution features receive rich information from all resolutions.
</figcaption>
</figure>
<div id="S3.SS1.SSS2.p4" class="ltx_para">
<p id="S3.SS1.SSS2.p4.1" class="ltx_p"><span id="S3.SS1.SSS2.p4.1.1" class="ltx_text ltx_font_bold">Symmetric Architecture<span id="S3.SS1.SSS2.p4.1.1.1" class="ltx_text ltx_font_medium"> <span id="S3.SS1.SSS2.p4.1.1.1.1" class="ltx_text">
The deep models generally employ a <em id="S3.SS1.SSS2.p4.1.1.1.1.1" class="ltx_emph ltx_font_italic">high-to-low</em> (downsampling) and <em id="S3.SS1.SSS2.p4.1.1.1.1.2" class="ltx_emph ltx_font_italic">low-to-high</em> (upsampling) framework, where <em id="S3.SS1.SSS2.p4.1.1.1.1.3" class="ltx_emph ltx_font_italic">high</em> and <em id="S3.SS1.SSS2.p4.1.1.1.1.4" class="ltx_emph ltx_font_italic">low</em> denote the resolution of feature representations.
<cite class="ltx_cite ltx_citemacro_cite">Newell et al. (<a href="#bib.bib122" title="" class="ltx_ref">2016b</a>)</cite> proposes a novel stacked hourglass architecture based on the successive steps of pooling and upsampling, which incorporates features across all scales to capture the various spatial relationships between joints. The stacked hourglass architecture is depicted in Fig. <a href="#S3.F4" title="Figure 4 ‣ 3.1.2 Heatmap-Based Methods ‣ 3.1 Top-Down Framework ‣ 3 Network Architecture Design Methods ‣ 2D Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> a). Several variations <cite class="ltx_cite ltx_citemacro_cite">Chu et al. (<a href="#bib.bib20" title="" class="ltx_ref">2017</a>); Yang et al. (<a href="#bib.bib184" title="" class="ltx_ref">2017</a>); Ke et al. (<a href="#bib.bib75" title="" class="ltx_ref">2018</a>); Cai et al. (<a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite> that built upon the success of this stacked hourglass architecture are subsequently developed. Specifically, <cite class="ltx_cite ltx_citemacro_cite">Chu et al. (<a href="#bib.bib20" title="" class="ltx_ref">2017</a>)</cite> extends <cite class="ltx_cite ltx_citemacro_cite">Newell et al. (<a href="#bib.bib122" title="" class="ltx_ref">2016b</a>)</cite> to Hourglass Residual Units with a side branch including filters with larger receptive field, which greatly increases the receptive fields of the network and automatically learns features across different scales.
<cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a href="#bib.bib184" title="" class="ltx_ref">2017</a>)</cite> further replaces the residual blocks in the stacked hourglass <cite class="ltx_cite ltx_citemacro_cite">Newell et al. (<a href="#bib.bib122" title="" class="ltx_ref">2016b</a>)</cite> with the Pyramid Residual Modules which enhances the scale invariance of networks.
<cite class="ltx_cite ltx_citemacro_cite">Ke et al. (<a href="#bib.bib75" title="" class="ltx_ref">2018</a>)</cite> proposes a multi-scale supervision that combines the keypoint heatmaps across all scales, which leads to acquiring abundant contextual features and improves the performance of stacked hourglass network.
<cite class="ltx_cite ltx_citemacro_cite">Cai et al. (<a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite> designs a stacked hourglass-like network, <em id="S3.SS1.SSS2.p4.1.1.1.1.5" class="ltx_emph ltx_font_italic">i.e.</em>, Residual Steps Network which aggregates features with the same spatial size to produce the delicate localized descriptions.
<cite class="ltx_cite ltx_citemacro_cite">Tang and Wu (<a href="#bib.bib157" title="" class="ltx_ref">2019</a>)</cite> employs the hourglass network <cite class="ltx_cite ltx_citemacro_cite">Newell et al. (<a href="#bib.bib122" title="" class="ltx_ref">2016b</a>)</cite> as backbone, and proposes a part-based branching network to learn the representations specific to different part groups.
These hourglass-based models retain <em id="S3.SS1.SSS2.p4.1.1.1.1.6" class="ltx_emph ltx_font_italic">symmetric</em> architecture between high-to-low and low-to-high convolutions.</span></span></span></p>
</div>
<div id="S3.SS1.SSS2.p5" class="ltx_para">
<p id="S3.SS1.SSS2.p5.1" class="ltx_p"><span id="S3.SS1.SSS2.p5.1.1" class="ltx_text ltx_font_bold">Asymmetric Architecture<span id="S3.SS1.SSS2.p5.1.1.1" class="ltx_text ltx_font_medium"> <span id="S3.SS1.SSS2.p5.1.1.1.1" class="ltx_text">
Another line of work exploits an <em id="S3.SS1.SSS2.p5.1.1.1.1.1" class="ltx_emph ltx_font_italic">asymmetric</em> architecture <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib17" title="" class="ltx_ref">2018</a>); Xiao et al. (<a href="#bib.bib181" title="" class="ltx_ref">2018</a>); Insafutdinov et al. (<a href="#bib.bib62" title="" class="ltx_ref">2016</a>)</cite>, where the high-to-low process is <em id="S3.SS1.SSS2.p5.1.1.1.1.2" class="ltx_emph ltx_font_italic">heavy</em> and the low-to-high process is <em id="S3.SS1.SSS2.p5.1.1.1.1.3" class="ltx_emph ltx_font_italic">light</em>.
<cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib17" title="" class="ltx_ref">2018</a>)</cite> proposes a Cascaded Pyramid Network (Fig. <a href="#S3.F4" title="Figure 4 ‣ 3.1.2 Heatmap-Based Methods ‣ 3.1 Top-Down Framework ‣ 3 Network Architecture Design Methods ‣ 2D Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> c) that detects the simple keypoints with a GlobalNet, and handles the difficult keypoints with a RefineNet.
Specifically, the RefineNet consists of several regular convolutions, integrating all levels of feature representations from the GlobalNet.
<cite class="ltx_cite ltx_citemacro_cite">Xiao et al. (<a href="#bib.bib181" title="" class="ltx_ref">2018</a>)</cite> extends the ResNet <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib52" title="" class="ltx_ref">2016</a>)</cite> by adding a few deconvolutional layers instead of feature map interpolation, which is depicted in Fig. <a href="#S3.F4" title="Figure 4 ‣ 3.1.2 Heatmap-Based Methods ‣ 3.1 Top-Down Framework ‣ 3 Network Architecture Design Methods ‣ 2D Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> b).
These methods employ a sub-network of classical classification networks (VGGNet <cite class="ltx_cite ltx_citemacro_cite">Simonyan and Zisserman (<a href="#bib.bib149" title="" class="ltx_ref">2014</a>)</cite> and ResNet <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib52" title="" class="ltx_ref">2016</a>)</cite>) for <em id="S3.SS1.SSS2.p5.1.1.1.1.4" class="ltx_emph ltx_font_italic">high-to-low</em> convolution and adopt simple networks for <em id="S3.SS1.SSS2.p5.1.1.1.1.5" class="ltx_emph ltx_font_italic">low-to-high</em> convolution.
Undoubtedly, such asymmetric network architectures suffer from imbalances in feature encoding and decoding, which potentially affects model performance.</span></span></span></p>
</div>
<div id="S3.SS1.SSS2.p6" class="ltx_para">
<p id="S3.SS1.SSS2.p6.1" class="ltx_p"><span id="S3.SS1.SSS2.p6.1.1" class="ltx_text ltx_font_bold">High Resolution Architecture<span id="S3.SS1.SSS2.p6.1.1.1" class="ltx_text ltx_font_medium"> <span id="S3.SS1.SSS2.p6.1.1.1.1" class="ltx_text">
<span id="S3.SS1.SSS2.p6.1.1.1.1.1" class="ltx_text">Unlike previous models, <cite class="ltx_cite ltx_citemacro_cite">Sun et al. (<a href="#bib.bib153" title="" class="ltx_ref">2019</a>)</cite> proposes a representative network, HRNet<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Link of HRNet Project: <a target="_blank" href="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/leoxiaobin/deep-high-resolution-net.pytorch</a></span></span></span><span id="S3.SS1.SSS2.p6.1.1.1.1.1.1" class="ltx_text"> (Fig. <a href="#S3.F4" title="Figure 4 ‣ 3.1.2 Heatmap-Based Methods ‣ 3.1 Top-Down Framework ‣ 3 Network Architecture Design Methods ‣ 2D Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> d), which is able to maintain high resolution representations through the whole process, achieving state-of-the-art results on multiple vision tasks.
This work demonstrates the superiority of high-resolution representations for human pose estimation and inspires a wide spectrum of later researches <cite class="ltx_cite ltx_citemacro_cite">Jiang et al. (<a href="#bib.bib68" title="" class="ltx_ref">2020</a>); Wang et al. (<a href="#bib.bib172" title="" class="ltx_ref">2020b</a>); Liu et al. (<a href="#bib.bib99" title="" class="ltx_ref">2021a</a>)</cite>.<span id="S3.SS1.SSS2.p6.1.1.1.1.1.1.1" class="ltx_text">
<cite class="ltx_cite ltx_citemacro_cite">Jiang et al. (<a href="#bib.bib68" title="" class="ltx_ref">2020</a>)</cite> takes HRNet as the backbone network, and further incorporates the gating mechanism as well as feature attention module to select and fuse discriminative and attention-aware features.</span></span></span></span></span></span></p>
</div>
<div id="S3.SS1.SSS2.p7" class="ltx_para">
<p id="S3.SS1.SSS2.p7.1" class="ltx_p"><span id="S3.SS1.SSS2.p7.1.1" class="ltx_text ltx_font_bold">Composed Human Proposal Detection<span id="S3.SS1.SSS2.p7.1.1.1" class="ltx_text ltx_font_medium"> <span id="S3.SS1.SSS2.p7.1.1.1.1" class="ltx_text">
<span id="S3.SS1.SSS2.p7.1.1.1.1.1" class="ltx_text">The above models concentrate on pose estimation on a given human proposal which is cropped from the entire image, and simply employ off-the-shelf human proposal detectors for proposal identification. Existing work <cite class="ltx_cite ltx_citemacro_cite">Fang et al. (<a href="#bib.bib34" title="" class="ltx_ref">2017</a>); Li et al. (<a href="#bib.bib84" title="" class="ltx_ref">2019</a>)</cite> has demonstrated that the quality of human proposals (<em id="S3.SS1.SSS2.p7.1.1.1.1.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, human position and redundant detection) significantly affects the results of pose estimators.<span id="S3.SS1.SSS2.p7.1.1.1.1.1.2" class="ltx_text"> Therefore, a group of researches direct their efforts in refining human proposals. For instances,
<cite class="ltx_cite ltx_citemacro_cite">Papandreou et al. (<a href="#bib.bib128" title="" class="ltx_ref">2017</a>)</cite> presents a multi-person pose estimation method, which employs the Faster-RCNN <cite class="ltx_cite ltx_citemacro_cite">Ren et al. (<a href="#bib.bib138" title="" class="ltx_ref">2015</a>)</cite> as person detector and the ResNet-101 <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib52" title="" class="ltx_ref">2016</a>)</cite> as pose detector, and additionally proposes a novel keypoint NonMaximum-Suppression (NMS) strategy to address the problem of pose redundancy.
<cite class="ltx_cite ltx_citemacro_cite">Fang et al. (<a href="#bib.bib34" title="" class="ltx_ref">2017</a>)</cite> utilizes the SSD-512 <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib95" title="" class="ltx_ref">2016</a>)</cite> as human detector and the stacked hourglass <cite class="ltx_cite ltx_citemacro_cite">Newell et al. (<a href="#bib.bib122" title="" class="ltx_ref">2016b</a>)</cite> as single person pose detector, and further proposes a symmetric spatial transformer network to extract a high-quality single person region from an inaccurate bounding box to facilitate human pose estimation.
<cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib84" title="" class="ltx_ref">2019</a>)</cite> notices that single person bounding boxes in crowded scenes tend to contain multiple people, which deteriorates the performance of the pose detector.
To tackle this problem, <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib84" title="" class="ltx_ref">2019</a>)</cite> leverages a joint-candidate pose detector to predict the heatmaps with multiple peaks, and uses a graph network to perform global joints association.</span></span></span></span></span></p>
</div>
<div id="S3.SS1.SSS2.p8" class="ltx_para">
<p id="S3.SS1.SSS2.p8.1" class="ltx_p">In contrast, another group of researches propose to perform proposal detection and pose detection jointly. <cite class="ltx_cite ltx_citemacro_cite">Varamesh and Tuytelaars (<a href="#bib.bib162" title="" class="ltx_ref">2020</a>)</cite> develops a mixture model which simultaneously infers the human bounding boxes and keypoint locations in a dense regression fashion.
<cite class="ltx_cite ltx_citemacro_cite">Wei et al. (<a href="#bib.bib177" title="" class="ltx_ref">2020</a>)</cite> introduces a template offset model which first gives a good initialization for the human bounding boxes and poses, and then regresses the offsets between initialization and corresponding labels.
<cite class="ltx_cite ltx_citemacro_cite">Kocabas et al. (<a href="#bib.bib77" title="" class="ltx_ref">2018</a>)</cite> presents a MultiPoseNet which first detects the keypoints and human proposals separately, and then employs a Pose Residual Network to assign the detected keypoints to different bounding boxes. Specifically, the Pose Residual Network is implemented by a residual multilayer perceptron.
<cite class="ltx_cite ltx_citemacro_cite">Mao et al. (<a href="#bib.bib109" title="" class="ltx_ref">2021</a>)</cite> designs a pose estimation framework, which incorporates dynamic instance-aware convolutions and eliminates the process of bounding boxes cropping and keypoint grouping.</p>
</div>
<div id="S3.SS1.SSS2.p9" class="ltx_para">
<p id="S3.SS1.SSS2.p9.1" class="ltx_p"><span id="S3.SS1.SSS2.p9.1.1" class="ltx_text">Overall, heatmap-based methods are more popular than the regression-based paradigms due to their higher accuracy. However, the heatmap computation process brings new open problems, including expensive computational overhead and inevitable quantization error. <span id="S3.SS1.SSS2.p9.1.1.1" class="ltx_text"></span></span></p>
</div>
</section>
<section id="S3.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.3 </span>Video-Based Methods</h4>

<div id="S3.SS1.SSS3.p1" class="ltx_para">
<p id="S3.SS1.SSS3.p1.1" class="ltx_p">Human pose estimation on videos has also been a hot research topic.
The video, by nature, brings more challenges such as <em id="S3.SS1.SSS3.p1.1.1" class="ltx_emph ltx_font_italic">camera shift, rapid object movement</em>, and <em id="S3.SS1.SSS3.p1.1.2" class="ltx_emph ltx_font_italic">defocus</em>, which result in frame quality deterioration frequently.
On the other hand, different from still images, there exist abundant temporal clues across video frames (<em id="S3.SS1.SSS3.p1.1.3" class="ltx_emph ltx_font_italic">e.g.</em>, <em id="S3.SS1.SSS3.p1.1.4" class="ltx_emph ltx_font_italic">temporal dependency</em> and <em id="S3.SS1.SSS3.p1.1.5" class="ltx_emph ltx_font_italic">geometric consistency</em>), which provide valuable information for pose estimation.</p>
</div>
<div id="S3.SS1.SSS3.p2" class="ltx_para">
<p id="S3.SS1.SSS3.p2.1" class="ltx_p">We observe that most existing methods are trained on static images.
Directly applying the image-based models to videos (image sequence) might lead to unsatisfactory results since they fail to consider the temporal consistency across video frames.
To conquer this dilemma, a large number of approaches have explored utilizing the additional temporal information to achieve higher pose detection accuracy. According to how the temporal information is exploited, we broadly divide these approaches into <em id="S3.SS1.SSS3.p2.1.1" class="ltx_emph ltx_font_italic">optical flow-based</em> <cite class="ltx_cite ltx_citemacro_cite">Zhang and Shah (<a href="#bib.bib191" title="" class="ltx_ref">2015</a>); Pfister et al. (<a href="#bib.bib131" title="" class="ltx_ref">2015</a>); Song et al. (<a href="#bib.bib151" title="" class="ltx_ref">2017</a>); Zhang et al. (<a href="#bib.bib192" title="" class="ltx_ref">2018a</a>); Chang et al. (<a href="#bib.bib14" title="" class="ltx_ref">2020</a>)</cite>, <em id="S3.SS1.SSS3.p2.1.2" class="ltx_emph ltx_font_italic">RNN-based</em> (Recurrent Neural Networks) <cite class="ltx_cite ltx_citemacro_cite">Gkioxari et al. (<a href="#bib.bib42" title="" class="ltx_ref">2016</a>); Luo et al. (<a href="#bib.bib104" title="" class="ltx_ref">2018a</a>); Artacho and Savakis (<a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite>, <em id="S3.SS1.SSS3.p2.1.3" class="ltx_emph ltx_font_italic">pose tracking-based</em> <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a href="#bib.bib188" title="" class="ltx_ref">2018</a>); Girdhar et al. (<a href="#bib.bib41" title="" class="ltx_ref">2018</a>); Wang et al. (<a href="#bib.bib169" title="" class="ltx_ref">2019d</a>); Zhou et al. (<a href="#bib.bib201" title="" class="ltx_ref">2020a</a>); Wang et al. (<a href="#bib.bib172" title="" class="ltx_ref">2020b</a>); Yang et al. (<a href="#bib.bib186" title="" class="ltx_ref">2021</a>)</cite>, and <em id="S3.SS1.SSS3.p2.1.4" class="ltx_emph ltx_font_italic">key frame-based</em> <cite class="ltx_cite ltx_citemacro_cite">Charles et al. (<a href="#bib.bib15" title="" class="ltx_ref">2016</a>); Bertasius et al. (<a href="#bib.bib5" title="" class="ltx_ref">2019</a>); Nie et al. (<a href="#bib.bib126" title="" class="ltx_ref">2019b</a>); Zhang et al. (<a href="#bib.bib198" title="" class="ltx_ref">2020b</a>); Liu et al. (<a href="#bib.bib99" title="" class="ltx_ref">2021a</a>)</cite> paradigms.
Below, we elaborate these methods in detail.</p>
</div>
<div id="S3.SS1.SSS3.p3" class="ltx_para">
<p id="S3.SS1.SSS3.p3.1" class="ltx_p"><span id="S3.SS1.SSS3.p3.1.1" class="ltx_text ltx_font_bold">Optical Flow<span id="S3.SS1.SSS3.p3.1.1.1" class="ltx_text ltx_font_medium"> <span id="S3.SS1.SSS3.p3.1.1.1.1" class="ltx_text">
<em id="S3.SS1.SSS3.p3.1.1.1.1.1" class="ltx_emph ltx_font_italic">Optical flow</em><span id="S3.SS1.SSS3.p3.1.1.1.1.2" class="ltx_text"> models the apparent motion of individual pixels on the frame, attracting widespread attention <cite class="ltx_cite ltx_citemacro_cite">Dosovitskiy et al. (<a href="#bib.bib26" title="" class="ltx_ref">2015</a>); Ilg et al. (<a href="#bib.bib61" title="" class="ltx_ref">2017</a>)</cite>.<span id="S3.SS1.SSS3.p3.1.1.1.1.2.1" class="ltx_text">
The optical flow across frames usually reveals the motions of the human subjects, which are obviously useful for pose estimation.
<cite class="ltx_cite ltx_citemacro_cite">Pfister et al. (<a href="#bib.bib131" title="" class="ltx_ref">2015</a>)</cite> combines convolutional networks and optical flow into a uniform framework, which employs the flow field to align the features temporally across multiple frames, and utilizes the aligned features to improve the pose detection in individual frames.
<cite class="ltx_cite ltx_citemacro_cite">Song et al. (<a href="#bib.bib151" title="" class="ltx_ref">2017</a>)</cite> presents a Thin-Slicing Network which computes the dense optical flow between every two frames to propagate the initial estimation of joint position through time, and uses a flow-based warping mechanism to align the joint heatmaps for subsequent spatiotemporal inference.
<span id="S3.SS1.SSS3.p3.1.1.1.1.2.1.1" class="ltx_text">
<cite class="ltx_cite ltx_citemacro_cite">Chang et al. (<a href="#bib.bib14" title="" class="ltx_ref">2020</a>)</cite> focuses on human pose estimation in crowded scenes, which incorporates forward pose propagation and backward pose propagation to refine the pose of the current frame.
However, although the optical flow in these methods does contain useful features such as human motion information, the undesired background changes are also involved.
The <em id="S3.SS1.SSS3.p3.1.1.1.1.2.1.1.1" class="ltx_emph ltx_font_italic">noisy</em> motion representation greatly hinders them from obtaining expected performance.<span id="S3.SS1.SSS3.p3.1.1.1.1.2.1.1.2" class="ltx_text">
<cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib192" title="" class="ltx_ref">2018a</a>)</cite> proposes a novel deep motion representation, <em id="S3.SS1.SSS3.p3.1.1.1.1.2.1.1.2.1" class="ltx_emph ltx_font_italic">namely PoseFlow</em>, which is able to reveal human motion in videos while inhibiting some nuisance noises such as background and motion blur. The distilled robust flow representation can also be generalized to human action recognition tasks.</span></span></span></span></span></span></span></p>
</div>
<div id="S3.SS1.SSS3.p4" class="ltx_para">
<p id="S3.SS1.SSS3.p4.1" class="ltx_p"><span id="S3.SS1.SSS3.p4.1.1" class="ltx_text">The optical flow based representation can model the motion cues at the pixel level, which is favorable for capturing useful temporal information. However, the optical flow is only able to extract impure features and is quite sensitive to noises. <span id="S3.SS1.SSS3.p4.1.1.1" class="ltx_text"></span></span></p>
</div>
<div id="S3.SS1.SSS3.p5" class="ltx_para">
<p id="S3.SS1.SSS3.p5.1" class="ltx_p"><span id="S3.SS1.SSS3.p5.1.1" class="ltx_text ltx_font_bold">Recurrent Neural Network<span id="S3.SS1.SSS3.p5.1.1.1" class="ltx_text ltx_font_medium"> <span id="S3.SS1.SSS3.p5.1.1.1.1" class="ltx_text">
<span id="S3.SS1.SSS3.p5.1.1.1.1.1" class="ltx_text">Besides optical flow, <em id="S3.SS1.SSS3.p5.1.1.1.1.1.1" class="ltx_emph ltx_font_italic">Recurrent Neural Network (RNN)</em> also provides a way to model temporal contexts across frames.
RNN shows a promising performance in sequential prediction task, due to the nature that each output is jointly determined by the current input and the historical predictions.
Therefore, a group of approaches attempt to capture temporal contexts between video frames by RNN for improving pose estimation.<span id="S3.SS1.SSS3.p5.1.1.1.1.1.2" class="ltx_text">
<cite class="ltx_cite ltx_citemacro_cite">Gkioxari et al. (<a href="#bib.bib42" title="" class="ltx_ref">2016</a>)</cite> presents a sequence-to-sequence model, which employs the chained convolutional networks to process input images, and combines historical hidden status and current images to predict current keypoint heatmaps.
<cite class="ltx_cite ltx_citemacro_cite">Luo et al. (<a href="#bib.bib104" title="" class="ltx_ref">2018a</a>)</cite> extends the convolutional pose machine <cite class="ltx_cite ltx_citemacro_cite">Wei et al. (<a href="#bib.bib178" title="" class="ltx_ref">2016</a>)</cite> by using convolutional LSTM, which is able to model both spatial and temporal contexts for pose prediction.</span></span></span></span></span></p>
</div>
<div id="S3.SS1.SSS3.p6" class="ltx_para">
<p id="S3.SS1.SSS3.p6.1" class="ltx_p"><span id="S3.SS1.SSS3.p6.1.1" class="ltx_text">To our knowledge, existing RNN-based methods can effectively estimate human poses from the <em id="S3.SS1.SSS3.p6.1.1.1" class="ltx_emph ltx_font_italic">single-person</em> image sequence, yet they have not been applied to multi-person videos until now. We conjecture that RNN has difficulties in directly employing temporal information from multi-person videos, where extracting the temporal contexts of each person will be affected by the others.
<span id="S3.SS1.SSS3.p6.1.1.2" class="ltx_text"></span></span></p>
</div>
<div id="S3.SS1.SSS3.p7" class="ltx_para">
<p id="S3.SS1.SSS3.p7.1" class="ltx_p"><span id="S3.SS1.SSS3.p7.1.1" class="ltx_text ltx_font_bold">Pose Tracking<span id="S3.SS1.SSS3.p7.1.1.1" class="ltx_text ltx_font_medium"> <span id="S3.SS1.SSS3.p7.1.1.1.1" class="ltx_text">
<span id="S3.SS1.SSS3.p7.1.1.1.1.1" class="ltx_text">To alleviate the issue of RNN, some methods that built upon the <em id="S3.SS1.SSS3.p7.1.1.1.1.1.1" class="ltx_emph ltx_font_italic">pose tracking</em> have been proposed, which establish a tracklet for each person in video frames to filter the interference of irrelevant information.<span id="S3.SS1.SSS3.p7.1.1.1.1.1.2" class="ltx_text">
<cite class="ltx_cite ltx_citemacro_cite">Girdhar et al. (<a href="#bib.bib41" title="" class="ltx_ref">2018</a>)</cite> proposes a 3D Mask R-CNN (extension of Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib53" title="" class="ltx_ref">2017a</a>)</cite> to include a temporal dimension) to generate small clips for a single person, and leverages temporal information within the small clips to produce more accurate predictions.
<cite class="ltx_cite ltx_citemacro_cite">Zhou et al. (<a href="#bib.bib201" title="" class="ltx_ref">2020a</a>)</cite> proposes a pose estimation framework which consists of a temporal keypoint matching module and a temporal keypoint refinement module. Specifically, the temporal keypoint matching module gives reliable single-person pose sequences according to the keypoint similarities, and the temporal keypoint refinement module aggregates poses within the sequence to correct original poses.
<cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib172" title="" class="ltx_ref">2020b</a>)</cite> designs a Clip Tracking Network and a Video Tracking Pipeline to establish the tracklet for each person, and extends the HRNet <cite class="ltx_cite ltx_citemacro_cite">Sun et al. (<a href="#bib.bib153" title="" class="ltx_ref">2019</a>)</cite> to 3D-HRNet to perform temporal pose estimation for all tracklets.
<cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a href="#bib.bib186" title="" class="ltx_ref">2021</a>)</cite> employs a graph neural network to learn the pose dynamics from the historical pose sequence,
and incorporates the pose dynamics into the pose detection of the current frame.</span></span></span></span></span></p>
</div>
<div id="S3.SS1.SSS3.p8" class="ltx_para">
<p id="S3.SS1.SSS3.p8.1" class="ltx_p"><span id="S3.SS1.SSS3.p8.1.1" class="ltx_text">Pose tracking-based methods show strong adaptation in the scene of multi-person. However, these models require computing feature similarity or pose similarity to create tracklets, which invokes an extra overhead for pose estimation.<span id="S3.SS1.SSS3.p8.1.1.1" class="ltx_text"></span></span></p>
</div>
<div id="S3.SS1.SSS3.p9" class="ltx_para">
<p id="S3.SS1.SSS3.p9.1" class="ltx_p"><span id="S3.SS1.SSS3.p9.1.1" class="ltx_text ltx_font_bold">Key Frame Optimization<span id="S3.SS1.SSS3.p9.1.1.1" class="ltx_text ltx_font_medium"> <span id="S3.SS1.SSS3.p9.1.1.1.1" class="ltx_text">
In addition to exploiting temporal information from tracklets, it is also beneficial to select some key frames to refine the pose estimation of the current frame, what we refer to as <em id="S3.SS1.SSS3.p9.1.1.1.1.1" class="ltx_emph ltx_font_italic">keyframe-based</em> approaches.
<cite class="ltx_cite ltx_citemacro_cite">Charles et al. (<a href="#bib.bib15" title="" class="ltx_ref">2016</a>)</cite> proposes a personalized video pose estimation framework, which leverages a few key frames with high-precision pose estimates to fine-tune the model.
<cite class="ltx_cite ltx_citemacro_cite">Bertasius et al. (<a href="#bib.bib5" title="" class="ltx_ref">2019</a>)</cite> proposes a PoseWarper network which first warps poses of the labeled frames to the unlabeled (current) frame, and then aggregates all warped poses to predict the pose heatmaps of the current frame.
<cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib198" title="" class="ltx_ref">2020b</a>)</cite> presents a keyframe proposal network to select the effective key frames, and proposes a learnable dictionary to reconstruct entire pose sequence from the selected key frames.
The work in <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib99" title="" class="ltx_ref">2021a</a>)</cite> builds a dual consecutive framework for video pose estimation, termed DCPose<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Link of DCPose Project: <a target="_blank" href="https://github.com/Pose-Group/DCPose" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/Pose-Group/DCPose</a></span></span></span>, which incorporates consecutive frames from dual temporal directions to improve the pose estimation in videos. Specifically, three modular components are designed. A Pose Temporal Merger encodes keypoint spatiotemporal context to generate effective searching scopes while a Pose Residual Fusion module computes weighted pose residuals in dual directions. These are then processed via a Pose Correction Network for efficient refining of pose estimations. <span id="S3.SS1.SSS3.p9.1.1.1.1.2" class="ltx_text">It is worthy mentioning that the DCPose <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib99" title="" class="ltx_ref">2021a</a>)</cite> is able to fully leverage the temporal information from neighboring frames and achieves state-of-the-art performance on video-based human pose estimation.<span id="S3.SS1.SSS3.p9.1.1.1.1.2.1" class="ltx_text"></span></span></span></span></span></p>
</div>
</section>
<section id="S3.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.4 </span>Model Compression-Based Methods</h4>

<div id="S3.SS1.SSS4.p1" class="ltx_para">
<p id="S3.SS1.SSS4.p1.1" class="ltx_p">For practical applications on lightweight devices such as mobiles, a low-consumption and high-accuracy HPE method is urgently demanded.
However, the majority of existing pose estimation models are oversized, which require extensive computational resources and fail to reach real-time computation.
Consequently, these methods are usually low-efficient, which limits their potential usage especially for mobiles or wearable equipments.
To alleviate this problem, many model compression based methods <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a href="#bib.bib187" title="" class="ltx_ref">2021</a>); Li and Lee (<a href="#bib.bib82" title="" class="ltx_ref">2021</a>); Zhang et al. (<a href="#bib.bib194" title="" class="ltx_ref">2019</a>); Nie et al. (<a href="#bib.bib126" title="" class="ltx_ref">2019b</a>); Luo et al. (<a href="#bib.bib104" title="" class="ltx_ref">2018a</a>)</cite> have been proposed to achieve the trade-off between accuracy and efficiency. These methods are able to significantly reduce model parameters with small accuracy decline.</p>
</div>
<div id="S3.SS1.SSS4.p2" class="ltx_para">
<p id="S3.SS1.SSS4.p2.3" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib194" title="" class="ltx_ref">2019</a>)</cite> proposes a Fast Pose Distillation model that built upon the Teacher-Student network <cite class="ltx_cite ltx_citemacro_cite">Hinton et al. (<a href="#bib.bib56" title="" class="ltx_ref">2015</a>); Zhou et al. (<a href="#bib.bib202" title="" class="ltx_ref">2018</a>); Romero et al. (<a href="#bib.bib139" title="" class="ltx_ref">2014</a>); Wang et al. (<a href="#bib.bib168" title="" class="ltx_ref">2019c</a>); Mirzadeh et al. (<a href="#bib.bib113" title="" class="ltx_ref">2020</a>)</cite>, effectively transferring the human body structure knowledge from a strong teacher network (<em id="S3.SS1.SSS4.p2.3.1" class="ltx_emph ltx_font_italic">large model</em>) to a <em id="S3.SS1.SSS4.p2.3.2" class="ltx_emph ltx_font_italic">lightweight</em> student network. Specifically, the <math id="S3.SS1.SSS4.p2.1.m1.1" class="ltx_Math" alttext="8" display="inline"><semantics id="S3.SS1.SSS4.p2.1.m1.1a"><mn id="S3.SS1.SSS4.p2.1.m1.1.1" xref="S3.SS1.SSS4.p2.1.m1.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS4.p2.1.m1.1b"><cn type="integer" id="S3.SS1.SSS4.p2.1.m1.1.1.cmml" xref="S3.SS1.SSS4.p2.1.m1.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS4.p2.1.m1.1c">8</annotation></semantics></math>-stage Hourglass model is employed as the teacher network while a compact counterpart (<math id="S3.SS1.SSS4.p2.2.m2.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S3.SS1.SSS4.p2.2.m2.1a"><mn id="S3.SS1.SSS4.p2.2.m2.1.1" xref="S3.SS1.SSS4.p2.2.m2.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS4.p2.2.m2.1b"><cn type="integer" id="S3.SS1.SSS4.p2.2.m2.1.1.cmml" xref="S3.SS1.SSS4.p2.2.m2.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS4.p2.2.m2.1c">4</annotation></semantics></math>-stage Hourglass) is adopted as the student network. <cite class="ltx_cite ltx_citemacro_cite">Luo et al. (<a href="#bib.bib104" title="" class="ltx_ref">2018a</a>)</cite> proposes a lightweight LSTM architecture to perform video pose estimation. <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a href="#bib.bib187" title="" class="ltx_ref">2021</a>)</cite> proposes two schemes to reduce the parameters of HRNet: i) Simply applying the Shuffle-Block <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib197" title="" class="ltx_ref">2018b</a>)</cite> to replace the basic block in <em id="S3.SS1.SSS4.p2.3.3" class="ltx_emph ltx_font_italic">vanilla</em> HRNet. ii) Designing a conditional channel weighting module, which learns the weights across multiple resolutions to replace the costly point-wise (<math id="S3.SS1.SSS4.p2.3.m3.1" class="ltx_Math" alttext="1\times 1" display="inline"><semantics id="S3.SS1.SSS4.p2.3.m3.1a"><mrow id="S3.SS1.SSS4.p2.3.m3.1.1" xref="S3.SS1.SSS4.p2.3.m3.1.1.cmml"><mn id="S3.SS1.SSS4.p2.3.m3.1.1.2" xref="S3.SS1.SSS4.p2.3.m3.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS4.p2.3.m3.1.1.1" xref="S3.SS1.SSS4.p2.3.m3.1.1.1.cmml">×</mo><mn id="S3.SS1.SSS4.p2.3.m3.1.1.3" xref="S3.SS1.SSS4.p2.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS4.p2.3.m3.1b"><apply id="S3.SS1.SSS4.p2.3.m3.1.1.cmml" xref="S3.SS1.SSS4.p2.3.m3.1.1"><times id="S3.SS1.SSS4.p2.3.m3.1.1.1.cmml" xref="S3.SS1.SSS4.p2.3.m3.1.1.1"></times><cn type="integer" id="S3.SS1.SSS4.p2.3.m3.1.1.2.cmml" xref="S3.SS1.SSS4.p2.3.m3.1.1.2">1</cn><cn type="integer" id="S3.SS1.SSS4.p2.3.m3.1.1.3.cmml" xref="S3.SS1.SSS4.p2.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS4.p2.3.m3.1c">1\times 1</annotation></semantics></math>) convolutions. By simplifying the original HRNet <cite class="ltx_cite ltx_citemacro_cite">Sun et al. (<a href="#bib.bib153" title="" class="ltx_ref">2019</a>)</cite>, the Lite-HRNet <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a href="#bib.bib187" title="" class="ltx_ref">2021</a>)</cite> shows good performance with relatively fewer parameters.<span id="S3.SS1.SSS4.p2.3.4" class="ltx_text"></span></p>
</div>
</section>
<section id="S3.SS1.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.5 </span>Summary of Top-Down Framework</h4>

<div id="S3.SS1.SSS5.p1" class="ltx_para">
<p id="S3.SS1.SSS5.p1.1" class="ltx_p">The architecture of top-down framework comprises the following key components: an object detector for producing human bounding boxes, and a pose estimator for detecting human keypoint locations.
The object detector determines the performance of human proposal detection, and further influences pose estimation. The pose detector, on the other hand, is the core of the framework and directly determines the accuracy of pose estimation.
In summary, the top-down framework is highly scalable that can be constantly improved with advances of object detectors as well as pose detectors.
</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Bottom-Up Framework</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The major discrepancy between bottom-up and top-down frameworks is whether the human detector is employed to detect the human bounding boxes.
Compared to the top-down approaches, bottom-up approaches do not rely on human detection and directly perform keypoint estimation in the original image, thus reducing the computational overhead.
However, this procedure opens up a new challenge: How to judge the identities of estimated joints?
According to the way of determining the identities of estimated keypoints, we divide the bottom-up methods into <em id="S3.SS2.p1.1.1" class="ltx_emph ltx_font_italic">human center regression-based</em> <cite class="ltx_cite ltx_citemacro_cite">Nie et al. (<a href="#bib.bib123" title="" class="ltx_ref">2018a</a>); Geng et al. (<a href="#bib.bib40" title="" class="ltx_ref">2021</a>); Nie et al. (<a href="#bib.bib125" title="" class="ltx_ref">2019a</a>, <a href="#bib.bib124" title="" class="ltx_ref">2018b</a>)</cite> , <em id="S3.SS2.p1.1.2" class="ltx_emph ltx_font_italic">associate embedding-based</em> <cite class="ltx_cite ltx_citemacro_cite">Luo et al. (<a href="#bib.bib106" title="" class="ltx_ref">2021</a>); Cheng et al. (<a href="#bib.bib19" title="" class="ltx_ref">2020</a>); Jin et al. (<a href="#bib.bib69" title="" class="ltx_ref">2019</a>); Newell et al. (<a href="#bib.bib121" title="" class="ltx_ref">2016a</a>)</cite>, and <em id="S3.SS2.p1.1.3" class="ltx_emph ltx_font_italic">part field-based</em> <cite class="ltx_cite ltx_citemacro_cite">Hidalgo et al. (<a href="#bib.bib55" title="" class="ltx_ref">2019</a>); Raaj et al. (<a href="#bib.bib136" title="" class="ltx_ref">2019</a>); Kreiss et al. (<a href="#bib.bib78" title="" class="ltx_ref">2019</a>); Cao et al. (<a href="#bib.bib9" title="" class="ltx_ref">2017a</a>); Jin et al. (<a href="#bib.bib70" title="" class="ltx_ref">2020</a>); Wang et al. (<a href="#bib.bib173" title="" class="ltx_ref">2020c</a>); Li et al. (<a href="#bib.bib85" title="" class="ltx_ref">2020b</a>); Luo et al. (<a href="#bib.bib105" title="" class="ltx_ref">2018b</a>); Kocabas et al. (<a href="#bib.bib77" title="" class="ltx_ref">2018</a>); Pishchulin et al. (<a href="#bib.bib133" title="" class="ltx_ref">2016</a>); Insafutdinov et al. (<a href="#bib.bib62" title="" class="ltx_ref">2016</a>); Pishchulin et al. (<a href="#bib.bib132" title="" class="ltx_ref">2013</a>)</cite> approaches.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold">Human Center Regression</span> The <em id="S3.SS2.p2.1.2" class="ltx_emph ltx_font_italic">human center regression-based</em> approaches utilize a human center point to represent the person instance.
<cite class="ltx_cite ltx_citemacro_cite">Nie et al. (<a href="#bib.bib125" title="" class="ltx_ref">2019a</a>)</cite> proposes a Single-stage multi-person Pose Machine that unifies person instance and body joint position representations. In <cite class="ltx_cite ltx_citemacro_cite">Nie et al. (<a href="#bib.bib125" title="" class="ltx_ref">2019a</a>)</cite>, the root joints (center-biased points) are introduced to denote the person instances, and body joint locations are encoded into their displacements <em id="S3.SS2.p2.1.3" class="ltx_emph ltx_font_italic">w.r.t.</em> the roots.
<cite class="ltx_cite ltx_citemacro_cite">Geng et al. (<a href="#bib.bib40" title="" class="ltx_ref">2021</a>)</cite> predicts a human center map that indicates the person instance, and densely estimates a candidate pose at each pixel <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">q</annotation></semantics></math> within the center map.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p"><span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_bold">Associate Embedding</span> The <em id="S3.SS2.p3.1.2" class="ltx_emph ltx_font_italic">associate embedding-based</em> approaches assign each keypoint an associate embedding, which is an instance representation for distinguishing different persons.
<cite class="ltx_cite ltx_citemacro_cite">Newell et al. (<a href="#bib.bib121" title="" class="ltx_ref">2016a</a>)</cite> pioneers the embedding representation, where each predicted keypoint has an additional embedding vector that serves as a <em id="S3.SS2.p3.1.3" class="ltx_emph ltx_font_italic">tag</em> to identify its human instance assignment.
<cite class="ltx_cite ltx_citemacro_cite">Jin et al. (<a href="#bib.bib69" title="" class="ltx_ref">2019</a>)</cite> proposes a SpatialNet to detect body part heatmaps and predict part-level data association in the input image. Specifically, the part-level data association is parameterized by the keypoint embedding.
<cite class="ltx_cite ltx_citemacro_cite">Cheng et al. (<a href="#bib.bib19" title="" class="ltx_ref">2020</a>)</cite> follows the keypoints grouping in <cite class="ltx_cite ltx_citemacro_cite">Newell et al. (<a href="#bib.bib121" title="" class="ltx_ref">2016a</a>)</cite> and further proposes a Higher-Resolution Network to learn high-resolution feature pyramids, improving the pose estimation of small persons.
<cite class="ltx_cite ltx_citemacro_cite">Luo et al. (<a href="#bib.bib106" title="" class="ltx_ref">2021</a>)</cite> focuses on the problems of large variance of human scales and labeling ambiguities. This approach <cite class="ltx_cite ltx_citemacro_cite">Luo et al. (<a href="#bib.bib106" title="" class="ltx_ref">2021</a>)</cite> proposes a scale-adaptive heatmap regression model, which is able to adaptively adjust the standard deviation of the ground-truth gaussian kernels for each keypoint, and achieves high tolerance for different human scales and labeling ambiguities.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p"><span id="S3.SS2.p4.1.1" class="ltx_text ltx_font_bold">Part Field</span>
The <em id="S3.SS2.p4.1.2" class="ltx_emph ltx_font_italic">part field-based</em> methods first detect keypoints and connections between them, and then perform keypoint grouping according to the keypoint connections.
<span id="S3.SS2.p4.1.3" class="ltx_text">The representative work <cite class="ltx_cite ltx_citemacro_cite">Cao et al. (<a href="#bib.bib9" title="" class="ltx_ref">2017a</a>)</cite> proposes a two-branch multi-stage CNN architecture, where one branch predicts the confident maps to denote the locations of keypoints and another branch predicts the Part Affinity Fields to indicate the connective intensity between keypoints.
Then, <cite class="ltx_cite ltx_citemacro_cite">Cao et al. (<a href="#bib.bib9" title="" class="ltx_ref">2017a</a>)</cite> applies a greedy algorithm to assemble different joints of the same person, according to the connective intensity between joints.
Inspired by <cite class="ltx_cite ltx_citemacro_cite">Cao et al. (<a href="#bib.bib9" title="" class="ltx_ref">2017a</a>)</cite>, various attempts have been proposed.
<cite class="ltx_cite ltx_citemacro_cite">Kreiss et al. (<a href="#bib.bib78" title="" class="ltx_ref">2019</a>)</cite> utilizes a part intensity field to localize body parts, and employs a part association field to associate body parts with each other.
<cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib85" title="" class="ltx_ref">2020b</a>)</cite> presents a novel keypoint associated representation of <em id="S3.SS2.p4.1.3.1" class="ltx_emph ltx_font_italic">body part heatmaps</em> based on the Part Affinity Field <cite class="ltx_cite ltx_citemacro_cite">Cao et al. (<a href="#bib.bib9" title="" class="ltx_ref">2017a</a>)</cite> for effective keypoint grouping.
<span id="S3.SS2.p4.1.3.2" class="ltx_text">
Some approaches explore alternative representations of keypoint connection for keypoint grouping.
<cite class="ltx_cite ltx_citemacro_cite">Luo et al. (<a href="#bib.bib105" title="" class="ltx_ref">2018b</a>)</cite> proposes a multi-layer fractal network, which regresses the keypoint location heatmaps and infers kinships among adjacent joints to determine the optimal matched joint pairs.
<cite class="ltx_cite ltx_citemacro_cite">Jin et al. (<a href="#bib.bib70" title="" class="ltx_ref">2020</a>)</cite> proposes a differentiable Hierarchical Graph Grouping network that converts the keypoint grouping into a graph grouping problem, and can be trained end-to-end with the keypoint detection network.</span></span></p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p"><span id="S3.SS2.p5.1.1" class="ltx_text ltx_font_bold">Summary</span> Overall, the bottom-up approaches improve the efficiency of pose detection by eliminating the usage of additional object detection techniques.
Due to the high efficiency, the bottom-up methods are promising in practice applications. For example, the open source project<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Link of OpenPose Project: <a target="_blank" href="https://github.com/CMU-Perceptual-Computing-Lab/openpose" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/CMU-Perceptual-Computing-Lab/openpose</a></span></span></span> of OpenPose <cite class="ltx_cite ltx_citemacro_cite">Cao et al. (<a href="#bib.bib10" title="" class="ltx_ref">2017b</a>)</cite> has been extensively adopted in the industry.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Network Training Refinement</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">From the perspective of the overall training pipeline in neural networks, the quantity and quality of data, training strategy, and loss function will impact the model performance.
According to the above key phases during training, we classify the network training refinement approaches into <em id="S4.p1.1.1" class="ltx_emph ltx_font_italic">data augmentation techniques</em>, <em id="S4.p1.1.2" class="ltx_emph ltx_font_italic">multi-task training strategies</em>, <em id="S4.p1.1.3" class="ltx_emph ltx_font_italic">loss function constraints</em>, and <em id="S4.p1.1.4" class="ltx_emph ltx_font_italic">domain adaption methods</em>.
Data augmentation techniques aim to increase the amount and diversity of the data.
Multi-task training strategies seek to capture informative features by sharing representations among related visual tasks.
Loss function constraints determine the optimization objective of the network. Domain adaption methods aim to help the network adapt different datasets.
In this section, we introduce these methods in detail.
</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Data Augmentation Techniques</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text">Deep learning is typically data-driven, therefore data plays a crucial role in model training. A <em id="S4.SS1.p1.1.1.1" class="ltx_emph ltx_font_italic">large-scale</em> and <em id="S4.SS1.p1.1.1.2" class="ltx_emph ltx_font_italic">high-quality</em> dataset contributes to the robustness of models.
However, building such a wonderful dataset is time-consuming and expensive.<span id="S4.SS1.p1.1.1.3" class="ltx_text">
To alleviate this problem, data augmentation techniques are adopted to increase the number and diversity of samples in datasets.
</span></span></p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">In 2D human pose estimation, common data augmentation techniques include random rotation, random scale, random truncation, horizontal flipping, random information dropping, and illumination variations.
Apart from the above random schemes, several works <cite class="ltx_cite ltx_citemacro_cite">Peng et al. (<a href="#bib.bib130" title="" class="ltx_ref">2018</a>); Moon et al. (<a href="#bib.bib118" title="" class="ltx_ref">2019</a>); Huang et al. (<a href="#bib.bib59" title="" class="ltx_ref">2020b</a>); Bin et al. (<a href="#bib.bib6" title="" class="ltx_ref">2020</a>); Zhou et al. (<a href="#bib.bib204" title="" class="ltx_ref">2017</a>); Wang et al. (<a href="#bib.bib171" title="" class="ltx_ref">2021</a>)</cite> have been studying learnable data augmentation.
<cite class="ltx_cite ltx_citemacro_cite">Peng et al. (<a href="#bib.bib130" title="" class="ltx_ref">2018</a>)</cite> proposes an enhancement network that generates difficult pose samples to compete against the pose estimator.
<cite class="ltx_cite ltx_citemacro_cite">Tang and Wu (<a href="#bib.bib157" title="" class="ltx_ref">2019</a>)</cite> points out that state-of-the-art human pose estimation approaches have similar error distributions.
<cite class="ltx_cite ltx_citemacro_cite">Moon et al. (<a href="#bib.bib118" title="" class="ltx_ref">2019</a>)</cite> generates synthetic poses based on the error statics in <cite class="ltx_cite ltx_citemacro_cite">Tang and Wu (<a href="#bib.bib157" title="" class="ltx_ref">2019</a>)</cite> and employs the synthesized poses to train human pose estimation networks.
<cite class="ltx_cite ltx_citemacro_cite">Bin et al. (<a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite> presents an adversarial semantic data augmentation using the generative adversarial network (GAN <cite class="ltx_cite ltx_citemacro_cite">Goodfellow et al. (<a href="#bib.bib46" title="" class="ltx_ref">2014</a>)</cite>), which enhances original images by pasting segmented body parts with different semantic granularities.
<cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib171" title="" class="ltx_ref">2021</a>)</cite> introduces an AdvMix algorithm, in which a generator network confuses pose estimators by mixing various corrupted images, and a knowledge distillation network transfers clean pose structure knowledge to the target pose detector.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Multi-Task Training Strategies</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Most of the human pose estimation models are designed for single-task learning. In this subsection, we focus on the multi-task learning models related to 2D human pose estimation.
<em id="S4.SS2.p1.1.1" class="ltx_emph ltx_font_italic">Multi-task learning</em> aims at capturing informative features by sharing representations among related visual tasks.
Human parsing is a closely related task to human pose estimation, with the goal of segmenting the human body into semantic parts such as head, arms, and legs, etc.
Previous works <cite class="ltx_cite ltx_citemacro_cite">Ladicky et al. (<a href="#bib.bib80" title="" class="ltx_ref">2013</a>); Dong et al. (<a href="#bib.bib25" title="" class="ltx_ref">2014</a>); Xia et al. (<a href="#bib.bib180" title="" class="ltx_ref">2017</a>); Nie et al. (<a href="#bib.bib124" title="" class="ltx_ref">2018b</a>); Liang et al. (<a href="#bib.bib92" title="" class="ltx_ref">2018</a>); Duan et al. (<a href="#bib.bib27" title="" class="ltx_ref">2019</a>)</cite> employ the human parsing information to improve the performance of 2D HPE.
<cite class="ltx_cite ltx_citemacro_cite">Xia et al. (<a href="#bib.bib180" title="" class="ltx_ref">2017</a>)</cite> jointly solves the two tasks of human parsing and pose estimation, and utilizes the part-level segments to guide the keypoint localization.
<cite class="ltx_cite ltx_citemacro_cite">Nie et al. (<a href="#bib.bib124" title="" class="ltx_ref">2018b</a>)</cite> presents a parsing encoder and a pose model parameter adapter, which together learn to predict parameters of the pose model to extract complementary features for human pose estimation.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Loss Function Constraints</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Loss function determines the learning objective of the network, and greatly affects the performance of the model.
In this subsection, we summarize and discuss existing loss functions <cite class="ltx_cite ltx_citemacro_cite">Pishchulin et al. (<a href="#bib.bib133" title="" class="ltx_ref">2016</a>); Carreira et al. (<a href="#bib.bib12" title="" class="ltx_ref">2016</a>); He et al. (<a href="#bib.bib54" title="" class="ltx_ref">2017b</a>); Sun et al. (<a href="#bib.bib154" title="" class="ltx_ref">2017</a>); Ke et al. (<a href="#bib.bib75" title="" class="ltx_ref">2018</a>); Chen et al. (<a href="#bib.bib17" title="" class="ltx_ref">2018</a>); Li et al. (<a href="#bib.bib85" title="" class="ltx_ref">2020b</a>); Yuan et al. (<a href="#bib.bib189" title="" class="ltx_ref">2020</a>); Zhou et al. (<a href="#bib.bib203" title="" class="ltx_ref">2020b</a>); Luo et al. (<a href="#bib.bib106" title="" class="ltx_ref">2021</a>)</cite> of 2D HPE.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">The standard and common loss function of human pose estimation is the <math id="S4.SS3.p2.1.m1.1" class="ltx_Math" alttext="L_{2}" display="inline"><semantics id="S4.SS3.p2.1.m1.1a"><msub id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml"><mi id="S4.SS3.p2.1.m1.1.1.2" xref="S4.SS3.p2.1.m1.1.1.2.cmml">L</mi><mn id="S4.SS3.p2.1.m1.1.1.3" xref="S4.SS3.p2.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><apply id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.p2.1.m1.1.1.2.cmml" xref="S4.SS3.p2.1.m1.1.1.2">𝐿</ci><cn type="integer" id="S4.SS3.p2.1.m1.1.1.3.cmml" xref="S4.SS3.p2.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">L_{2}</annotation></semantics></math> distance. Training aims to minimize the total L2 distance between prediction and ground truth heatmaps for all joints. The cost function is defined as:</p>
<table id="S9.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S4.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.E1.m1.3" class="ltx_Math" alttext="\displaystyle L=\frac{1}{N}*\sum_{j=1}^{N}v_{j}\times||G\left(j\right)-P\left(j\right)||^{2}" display="inline"><semantics id="S4.E1.m1.3a"><mrow id="S4.E1.m1.3.3" xref="S4.E1.m1.3.3.cmml"><mi id="S4.E1.m1.3.3.3" xref="S4.E1.m1.3.3.3.cmml">L</mi><mo id="S4.E1.m1.3.3.2" xref="S4.E1.m1.3.3.2.cmml">=</mo><mrow id="S4.E1.m1.3.3.1" xref="S4.E1.m1.3.3.1.cmml"><mstyle displaystyle="true" id="S4.E1.m1.3.3.1.3" xref="S4.E1.m1.3.3.1.3.cmml"><mfrac id="S4.E1.m1.3.3.1.3a" xref="S4.E1.m1.3.3.1.3.cmml"><mn id="S4.E1.m1.3.3.1.3.2" xref="S4.E1.m1.3.3.1.3.2.cmml">1</mn><mi id="S4.E1.m1.3.3.1.3.3" xref="S4.E1.m1.3.3.1.3.3.cmml">N</mi></mfrac></mstyle><mo lspace="0.222em" rspace="0.222em" id="S4.E1.m1.3.3.1.2" xref="S4.E1.m1.3.3.1.2.cmml">∗</mo><mrow id="S4.E1.m1.3.3.1.1" xref="S4.E1.m1.3.3.1.1.cmml"><mstyle displaystyle="true" id="S4.E1.m1.3.3.1.1.2" xref="S4.E1.m1.3.3.1.1.2.cmml"><munderover id="S4.E1.m1.3.3.1.1.2a" xref="S4.E1.m1.3.3.1.1.2.cmml"><mo movablelimits="false" id="S4.E1.m1.3.3.1.1.2.2.2" xref="S4.E1.m1.3.3.1.1.2.2.2.cmml">∑</mo><mrow id="S4.E1.m1.3.3.1.1.2.2.3" xref="S4.E1.m1.3.3.1.1.2.2.3.cmml"><mi id="S4.E1.m1.3.3.1.1.2.2.3.2" xref="S4.E1.m1.3.3.1.1.2.2.3.2.cmml">j</mi><mo id="S4.E1.m1.3.3.1.1.2.2.3.1" xref="S4.E1.m1.3.3.1.1.2.2.3.1.cmml">=</mo><mn id="S4.E1.m1.3.3.1.1.2.2.3.3" xref="S4.E1.m1.3.3.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S4.E1.m1.3.3.1.1.2.3" xref="S4.E1.m1.3.3.1.1.2.3.cmml">N</mi></munderover></mstyle><mrow id="S4.E1.m1.3.3.1.1.1" xref="S4.E1.m1.3.3.1.1.1.cmml"><msub id="S4.E1.m1.3.3.1.1.1.3" xref="S4.E1.m1.3.3.1.1.1.3.cmml"><mi id="S4.E1.m1.3.3.1.1.1.3.2" xref="S4.E1.m1.3.3.1.1.1.3.2.cmml">v</mi><mi id="S4.E1.m1.3.3.1.1.1.3.3" xref="S4.E1.m1.3.3.1.1.1.3.3.cmml">j</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S4.E1.m1.3.3.1.1.1.2" xref="S4.E1.m1.3.3.1.1.1.2.cmml">×</mo><msup id="S4.E1.m1.3.3.1.1.1.1" xref="S4.E1.m1.3.3.1.1.1.1.cmml"><mrow id="S4.E1.m1.3.3.1.1.1.1.1.1" xref="S4.E1.m1.3.3.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S4.E1.m1.3.3.1.1.1.1.1.1.2" xref="S4.E1.m1.3.3.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S4.E1.m1.3.3.1.1.1.1.1.1.1" xref="S4.E1.m1.3.3.1.1.1.1.1.1.1.cmml"><mrow id="S4.E1.m1.3.3.1.1.1.1.1.1.1.2" xref="S4.E1.m1.3.3.1.1.1.1.1.1.1.2.cmml"><mi id="S4.E1.m1.3.3.1.1.1.1.1.1.1.2.2" xref="S4.E1.m1.3.3.1.1.1.1.1.1.1.2.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.3.3.1.1.1.1.1.1.1.2.1" xref="S4.E1.m1.3.3.1.1.1.1.1.1.1.2.1.cmml">​</mo><mrow id="S4.E1.m1.3.3.1.1.1.1.1.1.1.2.3.2" xref="S4.E1.m1.3.3.1.1.1.1.1.1.1.2.cmml"><mo id="S4.E1.m1.3.3.1.1.1.1.1.1.1.2.3.2.1" xref="S4.E1.m1.3.3.1.1.1.1.1.1.1.2.cmml">(</mo><mi id="S4.E1.m1.1.1" xref="S4.E1.m1.1.1.cmml">j</mi><mo id="S4.E1.m1.3.3.1.1.1.1.1.1.1.2.3.2.2" xref="S4.E1.m1.3.3.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S4.E1.m1.3.3.1.1.1.1.1.1.1.1" xref="S4.E1.m1.3.3.1.1.1.1.1.1.1.1.cmml">−</mo><mrow id="S4.E1.m1.3.3.1.1.1.1.1.1.1.3" xref="S4.E1.m1.3.3.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E1.m1.3.3.1.1.1.1.1.1.1.3.2" xref="S4.E1.m1.3.3.1.1.1.1.1.1.1.3.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.3.3.1.1.1.1.1.1.1.3.1" xref="S4.E1.m1.3.3.1.1.1.1.1.1.1.3.1.cmml">​</mo><mrow id="S4.E1.m1.3.3.1.1.1.1.1.1.1.3.3.2" xref="S4.E1.m1.3.3.1.1.1.1.1.1.1.3.cmml"><mo id="S4.E1.m1.3.3.1.1.1.1.1.1.1.3.3.2.1" xref="S4.E1.m1.3.3.1.1.1.1.1.1.1.3.cmml">(</mo><mi id="S4.E1.m1.2.2" xref="S4.E1.m1.2.2.cmml">j</mi><mo id="S4.E1.m1.3.3.1.1.1.1.1.1.1.3.3.2.2" xref="S4.E1.m1.3.3.1.1.1.1.1.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S4.E1.m1.3.3.1.1.1.1.1.1.3" xref="S4.E1.m1.3.3.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S4.E1.m1.3.3.1.1.1.1.3" xref="S4.E1.m1.3.3.1.1.1.1.3.cmml">2</mn></msup></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.3b"><apply id="S4.E1.m1.3.3.cmml" xref="S4.E1.m1.3.3"><eq id="S4.E1.m1.3.3.2.cmml" xref="S4.E1.m1.3.3.2"></eq><ci id="S4.E1.m1.3.3.3.cmml" xref="S4.E1.m1.3.3.3">𝐿</ci><apply id="S4.E1.m1.3.3.1.cmml" xref="S4.E1.m1.3.3.1"><times id="S4.E1.m1.3.3.1.2.cmml" xref="S4.E1.m1.3.3.1.2"></times><apply id="S4.E1.m1.3.3.1.3.cmml" xref="S4.E1.m1.3.3.1.3"><divide id="S4.E1.m1.3.3.1.3.1.cmml" xref="S4.E1.m1.3.3.1.3"></divide><cn type="integer" id="S4.E1.m1.3.3.1.3.2.cmml" xref="S4.E1.m1.3.3.1.3.2">1</cn><ci id="S4.E1.m1.3.3.1.3.3.cmml" xref="S4.E1.m1.3.3.1.3.3">𝑁</ci></apply><apply id="S4.E1.m1.3.3.1.1.cmml" xref="S4.E1.m1.3.3.1.1"><apply id="S4.E1.m1.3.3.1.1.2.cmml" xref="S4.E1.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.3.3.1.1.2.1.cmml" xref="S4.E1.m1.3.3.1.1.2">superscript</csymbol><apply id="S4.E1.m1.3.3.1.1.2.2.cmml" xref="S4.E1.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.3.3.1.1.2.2.1.cmml" xref="S4.E1.m1.3.3.1.1.2">subscript</csymbol><sum id="S4.E1.m1.3.3.1.1.2.2.2.cmml" xref="S4.E1.m1.3.3.1.1.2.2.2"></sum><apply id="S4.E1.m1.3.3.1.1.2.2.3.cmml" xref="S4.E1.m1.3.3.1.1.2.2.3"><eq id="S4.E1.m1.3.3.1.1.2.2.3.1.cmml" xref="S4.E1.m1.3.3.1.1.2.2.3.1"></eq><ci id="S4.E1.m1.3.3.1.1.2.2.3.2.cmml" xref="S4.E1.m1.3.3.1.1.2.2.3.2">𝑗</ci><cn type="integer" id="S4.E1.m1.3.3.1.1.2.2.3.3.cmml" xref="S4.E1.m1.3.3.1.1.2.2.3.3">1</cn></apply></apply><ci id="S4.E1.m1.3.3.1.1.2.3.cmml" xref="S4.E1.m1.3.3.1.1.2.3">𝑁</ci></apply><apply id="S4.E1.m1.3.3.1.1.1.cmml" xref="S4.E1.m1.3.3.1.1.1"><times id="S4.E1.m1.3.3.1.1.1.2.cmml" xref="S4.E1.m1.3.3.1.1.1.2"></times><apply id="S4.E1.m1.3.3.1.1.1.3.cmml" xref="S4.E1.m1.3.3.1.1.1.3"><csymbol cd="ambiguous" id="S4.E1.m1.3.3.1.1.1.3.1.cmml" xref="S4.E1.m1.3.3.1.1.1.3">subscript</csymbol><ci id="S4.E1.m1.3.3.1.1.1.3.2.cmml" xref="S4.E1.m1.3.3.1.1.1.3.2">𝑣</ci><ci id="S4.E1.m1.3.3.1.1.1.3.3.cmml" xref="S4.E1.m1.3.3.1.1.1.3.3">𝑗</ci></apply><apply id="S4.E1.m1.3.3.1.1.1.1.cmml" xref="S4.E1.m1.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.3.3.1.1.1.1.2.cmml" xref="S4.E1.m1.3.3.1.1.1.1">superscript</csymbol><apply id="S4.E1.m1.3.3.1.1.1.1.1.2.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1"><csymbol cd="latexml" id="S4.E1.m1.3.3.1.1.1.1.1.2.1.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1.2">norm</csymbol><apply id="S4.E1.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1.1"><minus id="S4.E1.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1.1.1"></minus><apply id="S4.E1.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1.1.2"><times id="S4.E1.m1.3.3.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1.1.2.1"></times><ci id="S4.E1.m1.3.3.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1.1.2.2">𝐺</ci><ci id="S4.E1.m1.1.1.cmml" xref="S4.E1.m1.1.1">𝑗</ci></apply><apply id="S4.E1.m1.3.3.1.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1.1.3"><times id="S4.E1.m1.3.3.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1.1.3.1"></times><ci id="S4.E1.m1.3.3.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1.1.3.2">𝑃</ci><ci id="S4.E1.m1.2.2.cmml" xref="S4.E1.m1.2.2">𝑗</ci></apply></apply></apply><cn type="integer" id="S4.E1.m1.3.3.1.1.1.1.3.cmml" xref="S4.E1.m1.3.3.1.1.1.1.3">2</cn></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.3c">\displaystyle L=\frac{1}{N}*\sum_{j=1}^{N}v_{j}\times||G\left(j\right)-P\left(j\right)||^{2}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S4.SS3.p2.6" class="ltx_p">Where <math id="S4.SS3.p2.2.m1.1" class="ltx_Math" alttext="G(j)" display="inline"><semantics id="S4.SS3.p2.2.m1.1a"><mrow id="S4.SS3.p2.2.m1.1.2" xref="S4.SS3.p2.2.m1.1.2.cmml"><mi id="S4.SS3.p2.2.m1.1.2.2" xref="S4.SS3.p2.2.m1.1.2.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p2.2.m1.1.2.1" xref="S4.SS3.p2.2.m1.1.2.1.cmml">​</mo><mrow id="S4.SS3.p2.2.m1.1.2.3.2" xref="S4.SS3.p2.2.m1.1.2.cmml"><mo stretchy="false" id="S4.SS3.p2.2.m1.1.2.3.2.1" xref="S4.SS3.p2.2.m1.1.2.cmml">(</mo><mi id="S4.SS3.p2.2.m1.1.1" xref="S4.SS3.p2.2.m1.1.1.cmml">j</mi><mo stretchy="false" id="S4.SS3.p2.2.m1.1.2.3.2.2" xref="S4.SS3.p2.2.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m1.1b"><apply id="S4.SS3.p2.2.m1.1.2.cmml" xref="S4.SS3.p2.2.m1.1.2"><times id="S4.SS3.p2.2.m1.1.2.1.cmml" xref="S4.SS3.p2.2.m1.1.2.1"></times><ci id="S4.SS3.p2.2.m1.1.2.2.cmml" xref="S4.SS3.p2.2.m1.1.2.2">𝐺</ci><ci id="S4.SS3.p2.2.m1.1.1.cmml" xref="S4.SS3.p2.2.m1.1.1">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m1.1c">G(j)</annotation></semantics></math>, <math id="S4.SS3.p2.3.m2.1" class="ltx_Math" alttext="P(j)" display="inline"><semantics id="S4.SS3.p2.3.m2.1a"><mrow id="S4.SS3.p2.3.m2.1.2" xref="S4.SS3.p2.3.m2.1.2.cmml"><mi id="S4.SS3.p2.3.m2.1.2.2" xref="S4.SS3.p2.3.m2.1.2.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p2.3.m2.1.2.1" xref="S4.SS3.p2.3.m2.1.2.1.cmml">​</mo><mrow id="S4.SS3.p2.3.m2.1.2.3.2" xref="S4.SS3.p2.3.m2.1.2.cmml"><mo stretchy="false" id="S4.SS3.p2.3.m2.1.2.3.2.1" xref="S4.SS3.p2.3.m2.1.2.cmml">(</mo><mi id="S4.SS3.p2.3.m2.1.1" xref="S4.SS3.p2.3.m2.1.1.cmml">j</mi><mo stretchy="false" id="S4.SS3.p2.3.m2.1.2.3.2.2" xref="S4.SS3.p2.3.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.3.m2.1b"><apply id="S4.SS3.p2.3.m2.1.2.cmml" xref="S4.SS3.p2.3.m2.1.2"><times id="S4.SS3.p2.3.m2.1.2.1.cmml" xref="S4.SS3.p2.3.m2.1.2.1"></times><ci id="S4.SS3.p2.3.m2.1.2.2.cmml" xref="S4.SS3.p2.3.m2.1.2.2">𝑃</ci><ci id="S4.SS3.p2.3.m2.1.1.cmml" xref="S4.SS3.p2.3.m2.1.1">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.3.m2.1c">P(j)</annotation></semantics></math> and <math id="S4.SS3.p2.4.m3.1" class="ltx_Math" alttext="v_{j}" display="inline"><semantics id="S4.SS3.p2.4.m3.1a"><msub id="S4.SS3.p2.4.m3.1.1" xref="S4.SS3.p2.4.m3.1.1.cmml"><mi id="S4.SS3.p2.4.m3.1.1.2" xref="S4.SS3.p2.4.m3.1.1.2.cmml">v</mi><mi id="S4.SS3.p2.4.m3.1.1.3" xref="S4.SS3.p2.4.m3.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.4.m3.1b"><apply id="S4.SS3.p2.4.m3.1.1.cmml" xref="S4.SS3.p2.4.m3.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.4.m3.1.1.1.cmml" xref="S4.SS3.p2.4.m3.1.1">subscript</csymbol><ci id="S4.SS3.p2.4.m3.1.1.2.cmml" xref="S4.SS3.p2.4.m3.1.1.2">𝑣</ci><ci id="S4.SS3.p2.4.m3.1.1.3.cmml" xref="S4.SS3.p2.4.m3.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.4.m3.1c">v_{j}</annotation></semantics></math> respectively denote the ground truth heatmap, prediction heatmap and visibility for joint <math id="S4.SS3.p2.5.m4.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S4.SS3.p2.5.m4.1a"><mi id="S4.SS3.p2.5.m4.1.1" xref="S4.SS3.p2.5.m4.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.5.m4.1b"><ci id="S4.SS3.p2.5.m4.1.1.cmml" xref="S4.SS3.p2.5.m4.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.5.m4.1c">j</annotation></semantics></math>.
The symbol <math id="S4.SS3.p2.6.m5.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.SS3.p2.6.m5.1a"><mi id="S4.SS3.p2.6.m5.1.1" xref="S4.SS3.p2.6.m5.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.6.m5.1b"><ci id="S4.SS3.p2.6.m5.1.1.cmml" xref="S4.SS3.p2.6.m5.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.6.m5.1c">N</annotation></semantics></math> denotes the number of joints.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Ke et al. (<a href="#bib.bib75" title="" class="ltx_ref">2018</a>)</cite> presents a multi-scale human structure-aware loss which captures the structural information of the human body.
The <em id="S4.SS3.p3.1.1" class="ltx_emph ltx_font_italic">structure-aware loss</em> at the <math id="S4.SS3.p3.1.m1.1" class="ltx_Math" alttext="i^{th}" display="inline"><semantics id="S4.SS3.p3.1.m1.1a"><msup id="S4.SS3.p3.1.m1.1.1" xref="S4.SS3.p3.1.m1.1.1.cmml"><mi id="S4.SS3.p3.1.m1.1.1.2" xref="S4.SS3.p3.1.m1.1.1.2.cmml">i</mi><mrow id="S4.SS3.p3.1.m1.1.1.3" xref="S4.SS3.p3.1.m1.1.1.3.cmml"><mi id="S4.SS3.p3.1.m1.1.1.3.2" xref="S4.SS3.p3.1.m1.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p3.1.m1.1.1.3.1" xref="S4.SS3.p3.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS3.p3.1.m1.1.1.3.3" xref="S4.SS3.p3.1.m1.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.1b"><apply id="S4.SS3.p3.1.m1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p3.1.m1.1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1">superscript</csymbol><ci id="S4.SS3.p3.1.m1.1.1.2.cmml" xref="S4.SS3.p3.1.m1.1.1.2">𝑖</ci><apply id="S4.SS3.p3.1.m1.1.1.3.cmml" xref="S4.SS3.p3.1.m1.1.1.3"><times id="S4.SS3.p3.1.m1.1.1.3.1.cmml" xref="S4.SS3.p3.1.m1.1.1.3.1"></times><ci id="S4.SS3.p3.1.m1.1.1.3.2.cmml" xref="S4.SS3.p3.1.m1.1.1.3.2">𝑡</ci><ci id="S4.SS3.p3.1.m1.1.1.3.3.cmml" xref="S4.SS3.p3.1.m1.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">i^{th}</annotation></semantics></math> feature scale can be expressed as follows:</p>
<table id="S9.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S4.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.E2.m1.1" class="ltx_Math" alttext="\displaystyle L^{i}=\frac{1}{N}\sum_{j=1}^{N}||P_{j}^{i}-G_{j}^{i}||_{2}+\alpha\sum_{i=1}^{N}||P_{S_{j}}^{i}-G_{S_{j}}^{i}||_{2}," display="inline"><semantics id="S4.E2.m1.1a"><mrow id="S4.E2.m1.1.1.1" xref="S4.E2.m1.1.1.1.1.cmml"><mrow id="S4.E2.m1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.cmml"><msup id="S4.E2.m1.1.1.1.1.4" xref="S4.E2.m1.1.1.1.1.4.cmml"><mi id="S4.E2.m1.1.1.1.1.4.2" xref="S4.E2.m1.1.1.1.1.4.2.cmml">L</mi><mi id="S4.E2.m1.1.1.1.1.4.3" xref="S4.E2.m1.1.1.1.1.4.3.cmml">i</mi></msup><mo id="S4.E2.m1.1.1.1.1.3" xref="S4.E2.m1.1.1.1.1.3.cmml">=</mo><mrow id="S4.E2.m1.1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.2.cmml"><mrow id="S4.E2.m1.1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.1.1.cmml"><mstyle displaystyle="true" id="S4.E2.m1.1.1.1.1.1.1.3" xref="S4.E2.m1.1.1.1.1.1.1.3.cmml"><mfrac id="S4.E2.m1.1.1.1.1.1.1.3a" xref="S4.E2.m1.1.1.1.1.1.1.3.cmml"><mn id="S4.E2.m1.1.1.1.1.1.1.3.2" xref="S4.E2.m1.1.1.1.1.1.1.3.2.cmml">1</mn><mi id="S4.E2.m1.1.1.1.1.1.1.3.3" xref="S4.E2.m1.1.1.1.1.1.1.3.3.cmml">N</mi></mfrac></mstyle><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S4.E2.m1.1.1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.1.1.1.cmml"><mstyle displaystyle="true" id="S4.E2.m1.1.1.1.1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.1.1.1.2.cmml"><munderover id="S4.E2.m1.1.1.1.1.1.1.1.2a" xref="S4.E2.m1.1.1.1.1.1.1.1.2.cmml"><mo movablelimits="false" id="S4.E2.m1.1.1.1.1.1.1.1.2.2.2" xref="S4.E2.m1.1.1.1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S4.E2.m1.1.1.1.1.1.1.1.2.2.3" xref="S4.E2.m1.1.1.1.1.1.1.1.2.2.3.cmml"><mi id="S4.E2.m1.1.1.1.1.1.1.1.2.2.3.2" xref="S4.E2.m1.1.1.1.1.1.1.1.2.2.3.2.cmml">j</mi><mo id="S4.E2.m1.1.1.1.1.1.1.1.2.2.3.1" xref="S4.E2.m1.1.1.1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S4.E2.m1.1.1.1.1.1.1.1.2.2.3.3" xref="S4.E2.m1.1.1.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S4.E2.m1.1.1.1.1.1.1.1.2.3" xref="S4.E2.m1.1.1.1.1.1.1.1.2.3.cmml">N</mi></munderover></mstyle><msub id="S4.E2.m1.1.1.1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.1.1.1.1.cmml"><mrow id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><msubsup id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml">P</mi><mi id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.3" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml">j</mi><mi id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msubsup><mo id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">−</mo><msubsup id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.2" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.2.cmml">G</mi><mi id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.3" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.3.cmml">j</mi><mi id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></msubsup></mrow><mo stretchy="false" id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S4.E2.m1.1.1.1.1.1.1.1.1.3" xref="S4.E2.m1.1.1.1.1.1.1.1.1.3.cmml">2</mn></msub></mrow></mrow><mo id="S4.E2.m1.1.1.1.1.2.3" xref="S4.E2.m1.1.1.1.1.2.3.cmml">+</mo><mrow id="S4.E2.m1.1.1.1.1.2.2" xref="S4.E2.m1.1.1.1.1.2.2.cmml"><mi id="S4.E2.m1.1.1.1.1.2.2.3" xref="S4.E2.m1.1.1.1.1.2.2.3.cmml">α</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.1.1.2.2.2" xref="S4.E2.m1.1.1.1.1.2.2.2.cmml">​</mo><mrow id="S4.E2.m1.1.1.1.1.2.2.1" xref="S4.E2.m1.1.1.1.1.2.2.1.cmml"><mstyle displaystyle="true" id="S4.E2.m1.1.1.1.1.2.2.1.2" xref="S4.E2.m1.1.1.1.1.2.2.1.2.cmml"><munderover id="S4.E2.m1.1.1.1.1.2.2.1.2a" xref="S4.E2.m1.1.1.1.1.2.2.1.2.cmml"><mo movablelimits="false" id="S4.E2.m1.1.1.1.1.2.2.1.2.2.2" xref="S4.E2.m1.1.1.1.1.2.2.1.2.2.2.cmml">∑</mo><mrow id="S4.E2.m1.1.1.1.1.2.2.1.2.2.3" xref="S4.E2.m1.1.1.1.1.2.2.1.2.2.3.cmml"><mi id="S4.E2.m1.1.1.1.1.2.2.1.2.2.3.2" xref="S4.E2.m1.1.1.1.1.2.2.1.2.2.3.2.cmml">i</mi><mo id="S4.E2.m1.1.1.1.1.2.2.1.2.2.3.1" xref="S4.E2.m1.1.1.1.1.2.2.1.2.2.3.1.cmml">=</mo><mn id="S4.E2.m1.1.1.1.1.2.2.1.2.2.3.3" xref="S4.E2.m1.1.1.1.1.2.2.1.2.2.3.3.cmml">1</mn></mrow><mi id="S4.E2.m1.1.1.1.1.2.2.1.2.3" xref="S4.E2.m1.1.1.1.1.2.2.1.2.3.cmml">N</mi></munderover></mstyle><msub id="S4.E2.m1.1.1.1.1.2.2.1.1" xref="S4.E2.m1.1.1.1.1.2.2.1.1.cmml"><mrow id="S4.E2.m1.1.1.1.1.2.2.1.1.1.1" xref="S4.E2.m1.1.1.1.1.2.2.1.1.1.2.cmml"><mo stretchy="false" id="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.2.2.1.1.1.2.1.cmml">‖</mo><mrow id="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.cmml"><msubsup id="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.2.cmml"><mi id="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.2.2.2" xref="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.2.2.2.cmml">P</mi><msub id="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.2.2.3" xref="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.2.2.3.cmml"><mi id="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.2.2.3.2" xref="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.2.2.3.2.cmml">S</mi><mi id="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.2.2.3.3" xref="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.2.2.3.3.cmml">j</mi></msub><mi id="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.2.3" xref="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.2.3.cmml">i</mi></msubsup><mo id="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.1.cmml">−</mo><msubsup id="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.3" xref="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.3.cmml"><mi id="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.3.2.2" xref="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.3.2.2.cmml">G</mi><msub id="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.3.2.3" xref="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.3.2.3.cmml"><mi id="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.3.2.3.2" xref="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.3.2.3.2.cmml">S</mi><mi id="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.3.2.3.3" xref="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.3.2.3.3.cmml">j</mi></msub><mi id="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.3.3" xref="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.3.3.cmml">i</mi></msubsup></mrow><mo stretchy="false" id="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.3" xref="S4.E2.m1.1.1.1.1.2.2.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S4.E2.m1.1.1.1.1.2.2.1.1.3" xref="S4.E2.m1.1.1.1.1.2.2.1.1.3.cmml">2</mn></msub></mrow></mrow></mrow></mrow><mo id="S4.E2.m1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.1b"><apply id="S4.E2.m1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1"><eq id="S4.E2.m1.1.1.1.1.3.cmml" xref="S4.E2.m1.1.1.1.1.3"></eq><apply id="S4.E2.m1.1.1.1.1.4.cmml" xref="S4.E2.m1.1.1.1.1.4"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.4.1.cmml" xref="S4.E2.m1.1.1.1.1.4">superscript</csymbol><ci id="S4.E2.m1.1.1.1.1.4.2.cmml" xref="S4.E2.m1.1.1.1.1.4.2">𝐿</ci><ci id="S4.E2.m1.1.1.1.1.4.3.cmml" xref="S4.E2.m1.1.1.1.1.4.3">𝑖</ci></apply><apply id="S4.E2.m1.1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.2"><plus id="S4.E2.m1.1.1.1.1.2.3.cmml" xref="S4.E2.m1.1.1.1.1.2.3"></plus><apply id="S4.E2.m1.1.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1"><times id="S4.E2.m1.1.1.1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.2"></times><apply id="S4.E2.m1.1.1.1.1.1.1.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.3"><divide id="S4.E2.m1.1.1.1.1.1.1.3.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.3"></divide><cn type="integer" id="S4.E2.m1.1.1.1.1.1.1.3.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.3.2">1</cn><ci id="S4.E2.m1.1.1.1.1.1.1.3.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.3.3">𝑁</ci></apply><apply id="S4.E2.m1.1.1.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1"><apply id="S4.E2.m1.1.1.1.1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S4.E2.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.2">subscript</csymbol><sum id="S4.E2.m1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.2.2.2"></sum><apply id="S4.E2.m1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.2.2.3"><eq id="S4.E2.m1.1.1.1.1.1.1.1.2.2.3.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.2.2.3.1"></eq><ci id="S4.E2.m1.1.1.1.1.1.1.1.2.2.3.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.2.2.3.2">𝑗</ci><cn type="integer" id="S4.E2.m1.1.1.1.1.1.1.1.2.2.3.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S4.E2.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.2.3">𝑁</ci></apply><apply id="S4.E2.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S4.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S4.E2.m1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1"><minus id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1"></minus><apply id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2">𝑃</ci><ci id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.3">𝑗</ci></apply><ci id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3">superscript</csymbol><apply id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.2">𝐺</ci><ci id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.3">𝑗</ci></apply><ci id="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply></apply><cn type="integer" id="S4.E2.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1.3">2</cn></apply></apply></apply><apply id="S4.E2.m1.1.1.1.1.2.2.cmml" xref="S4.E2.m1.1.1.1.1.2.2"><times id="S4.E2.m1.1.1.1.1.2.2.2.cmml" xref="S4.E2.m1.1.1.1.1.2.2.2"></times><ci id="S4.E2.m1.1.1.1.1.2.2.3.cmml" xref="S4.E2.m1.1.1.1.1.2.2.3">𝛼</ci><apply id="S4.E2.m1.1.1.1.1.2.2.1.cmml" xref="S4.E2.m1.1.1.1.1.2.2.1"><apply id="S4.E2.m1.1.1.1.1.2.2.1.2.cmml" xref="S4.E2.m1.1.1.1.1.2.2.1.2"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.2.2.1.2.1.cmml" xref="S4.E2.m1.1.1.1.1.2.2.1.2">superscript</csymbol><apply id="S4.E2.m1.1.1.1.1.2.2.1.2.2.cmml" xref="S4.E2.m1.1.1.1.1.2.2.1.2"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.2.2.1.2.2.1.cmml" xref="S4.E2.m1.1.1.1.1.2.2.1.2">subscript</csymbol><sum id="S4.E2.m1.1.1.1.1.2.2.1.2.2.2.cmml" xref="S4.E2.m1.1.1.1.1.2.2.1.2.2.2"></sum><apply id="S4.E2.m1.1.1.1.1.2.2.1.2.2.3.cmml" xref="S4.E2.m1.1.1.1.1.2.2.1.2.2.3"><eq id="S4.E2.m1.1.1.1.1.2.2.1.2.2.3.1.cmml" xref="S4.E2.m1.1.1.1.1.2.2.1.2.2.3.1"></eq><ci id="S4.E2.m1.1.1.1.1.2.2.1.2.2.3.2.cmml" xref="S4.E2.m1.1.1.1.1.2.2.1.2.2.3.2">𝑖</ci><cn type="integer" id="S4.E2.m1.1.1.1.1.2.2.1.2.2.3.3.cmml" xref="S4.E2.m1.1.1.1.1.2.2.1.2.2.3.3">1</cn></apply></apply><ci id="S4.E2.m1.1.1.1.1.2.2.1.2.3.cmml" xref="S4.E2.m1.1.1.1.1.2.2.1.2.3">𝑁</ci></apply><apply id="S4.E2.m1.1.1.1.1.2.2.1.1.cmml" xref="S4.E2.m1.1.1.1.1.2.2.1.1"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.2.2.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.2.2.1.1">subscript</csymbol><apply id="S4.E2.m1.1.1.1.1.2.2.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.2.2.1.1.1.1"><csymbol cd="latexml" id="S4.E2.m1.1.1.1.1.2.2.1.1.1.2.1.cmml" xref="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.2">norm</csymbol><apply id="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1"><minus id="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.1"></minus><apply id="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.2.1.cmml" xref="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.2">superscript</csymbol><apply id="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.2.2.cmml" xref="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.2.2.1.cmml" xref="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.2">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.2.2.2.cmml" xref="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.2.2.2">𝑃</ci><apply id="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.2.2.3.cmml" xref="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.2.2.3"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.2.2.3.1.cmml" xref="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.2.2.3">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.2.2.3.2.cmml" xref="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.2.2.3.2">𝑆</ci><ci id="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.2.2.3.3.cmml" xref="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.2.2.3.3">𝑗</ci></apply></apply><ci id="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.2.3.cmml" xref="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.3.cmml" xref="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.3.1.cmml" xref="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.3">superscript</csymbol><apply id="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.3.2.cmml" xref="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.3.2.1.cmml" xref="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.3.2.2.cmml" xref="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.3.2.2">𝐺</ci><apply id="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.3.2.3.cmml" xref="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.3.2.3"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.3.2.3.1.cmml" xref="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.3.2.3">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.3.2.3.2.cmml" xref="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.3.2.3.2">𝑆</ci><ci id="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.3.2.3.3.cmml" xref="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.3.2.3.3">𝑗</ci></apply></apply><ci id="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.3.3.cmml" xref="S4.E2.m1.1.1.1.1.2.2.1.1.1.1.1.3.3">𝑖</ci></apply></apply></apply><cn type="integer" id="S4.E2.m1.1.1.1.1.2.2.1.1.3.cmml" xref="S4.E2.m1.1.1.1.1.2.2.1.1.3">2</cn></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.1c">\displaystyle L^{i}=\frac{1}{N}\sum_{j=1}^{N}||P_{j}^{i}-G_{j}^{i}||_{2}+\alpha\sum_{i=1}^{N}||P_{S_{j}}^{i}-G_{S_{j}}^{i}||_{2},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S4.SS3.p3.7" class="ltx_p">where <math id="S4.SS3.p3.2.m1.1" class="ltx_Math" alttext="P_{j}" display="inline"><semantics id="S4.SS3.p3.2.m1.1a"><msub id="S4.SS3.p3.2.m1.1.1" xref="S4.SS3.p3.2.m1.1.1.cmml"><mi id="S4.SS3.p3.2.m1.1.1.2" xref="S4.SS3.p3.2.m1.1.1.2.cmml">P</mi><mi id="S4.SS3.p3.2.m1.1.1.3" xref="S4.SS3.p3.2.m1.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.2.m1.1b"><apply id="S4.SS3.p3.2.m1.1.1.cmml" xref="S4.SS3.p3.2.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p3.2.m1.1.1.1.cmml" xref="S4.SS3.p3.2.m1.1.1">subscript</csymbol><ci id="S4.SS3.p3.2.m1.1.1.2.cmml" xref="S4.SS3.p3.2.m1.1.1.2">𝑃</ci><ci id="S4.SS3.p3.2.m1.1.1.3.cmml" xref="S4.SS3.p3.2.m1.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.2.m1.1c">P_{j}</annotation></semantics></math> and <math id="S4.SS3.p3.3.m2.1" class="ltx_Math" alttext="G_{j}" display="inline"><semantics id="S4.SS3.p3.3.m2.1a"><msub id="S4.SS3.p3.3.m2.1.1" xref="S4.SS3.p3.3.m2.1.1.cmml"><mi id="S4.SS3.p3.3.m2.1.1.2" xref="S4.SS3.p3.3.m2.1.1.2.cmml">G</mi><mi id="S4.SS3.p3.3.m2.1.1.3" xref="S4.SS3.p3.3.m2.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.3.m2.1b"><apply id="S4.SS3.p3.3.m2.1.1.cmml" xref="S4.SS3.p3.3.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.p3.3.m2.1.1.1.cmml" xref="S4.SS3.p3.3.m2.1.1">subscript</csymbol><ci id="S4.SS3.p3.3.m2.1.1.2.cmml" xref="S4.SS3.p3.3.m2.1.1.2">𝐺</ci><ci id="S4.SS3.p3.3.m2.1.1.3.cmml" xref="S4.SS3.p3.3.m2.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.3.m2.1c">G_{j}</annotation></semantics></math> denote the predicted and labeled <math id="S4.SS3.p3.4.m3.1" class="ltx_Math" alttext="j^{th}" display="inline"><semantics id="S4.SS3.p3.4.m3.1a"><msup id="S4.SS3.p3.4.m3.1.1" xref="S4.SS3.p3.4.m3.1.1.cmml"><mi id="S4.SS3.p3.4.m3.1.1.2" xref="S4.SS3.p3.4.m3.1.1.2.cmml">j</mi><mrow id="S4.SS3.p3.4.m3.1.1.3" xref="S4.SS3.p3.4.m3.1.1.3.cmml"><mi id="S4.SS3.p3.4.m3.1.1.3.2" xref="S4.SS3.p3.4.m3.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p3.4.m3.1.1.3.1" xref="S4.SS3.p3.4.m3.1.1.3.1.cmml">​</mo><mi id="S4.SS3.p3.4.m3.1.1.3.3" xref="S4.SS3.p3.4.m3.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.4.m3.1b"><apply id="S4.SS3.p3.4.m3.1.1.cmml" xref="S4.SS3.p3.4.m3.1.1"><csymbol cd="ambiguous" id="S4.SS3.p3.4.m3.1.1.1.cmml" xref="S4.SS3.p3.4.m3.1.1">superscript</csymbol><ci id="S4.SS3.p3.4.m3.1.1.2.cmml" xref="S4.SS3.p3.4.m3.1.1.2">𝑗</ci><apply id="S4.SS3.p3.4.m3.1.1.3.cmml" xref="S4.SS3.p3.4.m3.1.1.3"><times id="S4.SS3.p3.4.m3.1.1.3.1.cmml" xref="S4.SS3.p3.4.m3.1.1.3.1"></times><ci id="S4.SS3.p3.4.m3.1.1.3.2.cmml" xref="S4.SS3.p3.4.m3.1.1.3.2">𝑡</ci><ci id="S4.SS3.p3.4.m3.1.1.3.3.cmml" xref="S4.SS3.p3.4.m3.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.4.m3.1c">j^{th}</annotation></semantics></math> keypoint heatmaps, <math id="S4.SS3.p3.5.m4.1" class="ltx_Math" alttext="P_{S_{j}}" display="inline"><semantics id="S4.SS3.p3.5.m4.1a"><msub id="S4.SS3.p3.5.m4.1.1" xref="S4.SS3.p3.5.m4.1.1.cmml"><mi id="S4.SS3.p3.5.m4.1.1.2" xref="S4.SS3.p3.5.m4.1.1.2.cmml">P</mi><msub id="S4.SS3.p3.5.m4.1.1.3" xref="S4.SS3.p3.5.m4.1.1.3.cmml"><mi id="S4.SS3.p3.5.m4.1.1.3.2" xref="S4.SS3.p3.5.m4.1.1.3.2.cmml">S</mi><mi id="S4.SS3.p3.5.m4.1.1.3.3" xref="S4.SS3.p3.5.m4.1.1.3.3.cmml">j</mi></msub></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.5.m4.1b"><apply id="S4.SS3.p3.5.m4.1.1.cmml" xref="S4.SS3.p3.5.m4.1.1"><csymbol cd="ambiguous" id="S4.SS3.p3.5.m4.1.1.1.cmml" xref="S4.SS3.p3.5.m4.1.1">subscript</csymbol><ci id="S4.SS3.p3.5.m4.1.1.2.cmml" xref="S4.SS3.p3.5.m4.1.1.2">𝑃</ci><apply id="S4.SS3.p3.5.m4.1.1.3.cmml" xref="S4.SS3.p3.5.m4.1.1.3"><csymbol cd="ambiguous" id="S4.SS3.p3.5.m4.1.1.3.1.cmml" xref="S4.SS3.p3.5.m4.1.1.3">subscript</csymbol><ci id="S4.SS3.p3.5.m4.1.1.3.2.cmml" xref="S4.SS3.p3.5.m4.1.1.3.2">𝑆</ci><ci id="S4.SS3.p3.5.m4.1.1.3.3.cmml" xref="S4.SS3.p3.5.m4.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.5.m4.1c">P_{S_{j}}</annotation></semantics></math> and <math id="S4.SS3.p3.6.m5.1" class="ltx_Math" alttext="G_{S_{j}}" display="inline"><semantics id="S4.SS3.p3.6.m5.1a"><msub id="S4.SS3.p3.6.m5.1.1" xref="S4.SS3.p3.6.m5.1.1.cmml"><mi id="S4.SS3.p3.6.m5.1.1.2" xref="S4.SS3.p3.6.m5.1.1.2.cmml">G</mi><msub id="S4.SS3.p3.6.m5.1.1.3" xref="S4.SS3.p3.6.m5.1.1.3.cmml"><mi id="S4.SS3.p3.6.m5.1.1.3.2" xref="S4.SS3.p3.6.m5.1.1.3.2.cmml">S</mi><mi id="S4.SS3.p3.6.m5.1.1.3.3" xref="S4.SS3.p3.6.m5.1.1.3.3.cmml">j</mi></msub></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.6.m5.1b"><apply id="S4.SS3.p3.6.m5.1.1.cmml" xref="S4.SS3.p3.6.m5.1.1"><csymbol cd="ambiguous" id="S4.SS3.p3.6.m5.1.1.1.cmml" xref="S4.SS3.p3.6.m5.1.1">subscript</csymbol><ci id="S4.SS3.p3.6.m5.1.1.2.cmml" xref="S4.SS3.p3.6.m5.1.1.2">𝐺</ci><apply id="S4.SS3.p3.6.m5.1.1.3.cmml" xref="S4.SS3.p3.6.m5.1.1.3"><csymbol cd="ambiguous" id="S4.SS3.p3.6.m5.1.1.3.1.cmml" xref="S4.SS3.p3.6.m5.1.1.3">subscript</csymbol><ci id="S4.SS3.p3.6.m5.1.1.3.2.cmml" xref="S4.SS3.p3.6.m5.1.1.3.2">𝑆</ci><ci id="S4.SS3.p3.6.m5.1.1.3.3.cmml" xref="S4.SS3.p3.6.m5.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.6.m5.1c">G_{S_{j}}</annotation></semantics></math> are the group of the heatmaps from keypoint <math id="S4.SS3.p3.7.m6.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S4.SS3.p3.7.m6.1a"><mi id="S4.SS3.p3.7.m6.1.1" xref="S4.SS3.p3.7.m6.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.7.m6.1b"><ci id="S4.SS3.p3.7.m6.1.1.cmml" xref="S4.SS3.p3.7.m6.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.7.m6.1c">j</annotation></semantics></math> and its neighbors, respectively.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.2" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib17" title="" class="ltx_ref">2018</a>)</cite> proposes an online hard keypoints mining, which first computes the regular <math id="S4.SS3.p4.1.m1.1" class="ltx_Math" alttext="L_{2}" display="inline"><semantics id="S4.SS3.p4.1.m1.1a"><msub id="S4.SS3.p4.1.m1.1.1" xref="S4.SS3.p4.1.m1.1.1.cmml"><mi id="S4.SS3.p4.1.m1.1.1.2" xref="S4.SS3.p4.1.m1.1.1.2.cmml">L</mi><mn id="S4.SS3.p4.1.m1.1.1.3" xref="S4.SS3.p4.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.1.m1.1b"><apply id="S4.SS3.p4.1.m1.1.1.cmml" xref="S4.SS3.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p4.1.m1.1.1.1.cmml" xref="S4.SS3.p4.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.p4.1.m1.1.1.2.cmml" xref="S4.SS3.p4.1.m1.1.1.2">𝐿</ci><cn type="integer" id="S4.SS3.p4.1.m1.1.1.3.cmml" xref="S4.SS3.p4.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.1.m1.1c">L_{2}</annotation></semantics></math> loss for all keypoints, and then additionally punishes top-<math id="S4.SS3.p4.2.m2.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S4.SS3.p4.2.m2.1a"><mi id="S4.SS3.p4.2.m2.1.1" xref="S4.SS3.p4.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.2.m2.1b"><ci id="S4.SS3.p4.2.m2.1.1.cmml" xref="S4.SS3.p4.2.m2.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.2.m2.1c">M</annotation></semantics></math> hard keypoints.
This loss function increases the penalty of the difficult keypoints, and improves the network performance.</p>
</div>
<div id="S4.SS3.p5" class="ltx_para">
<p id="S4.SS3.p5.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Yuan et al. (<a href="#bib.bib189" title="" class="ltx_ref">2020</a>)</cite> presents a combined distillation loss for the HRNet, which consists of a structure loss (STLoss), a pairwise inhibition loss (PairLoss), and a probability distribution loss (PDLoss).
Specifically, the STLoss enforces the network to learn human structures at earlier phase to combat against pose occlusions, and the PairLoss alleviates the problem of similar joint misclassification especially in crowded scenarios.
The PDLoss guides the learning of the distribution of final heatmaps.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Domain Adaption Methods</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">Human pose estimation has been widely investigated with much focus on supervised learning that requires sufficient pose annotations. However, in real applications, pretrained pose estimation models usually need be adapted to a new domain with no labels or sparse labels.
Therefore, several domain adaptation methods <cite class="ltx_cite ltx_citemacro_cite">Guo et al. (<a href="#bib.bib47" title="" class="ltx_ref">2018</a>); Hidalgo et al. (<a href="#bib.bib55" title="" class="ltx_ref">2019</a>); Xu et al. (<a href="#bib.bib183" title="" class="ltx_ref">2020</a>); Li and Lee (<a href="#bib.bib82" title="" class="ltx_ref">2021</a>)</cite> leverage a labeled source domain to learn a model that performs well on an unlabeled or
sparse labeled target domain.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Xu et al. (<a href="#bib.bib183" title="" class="ltx_ref">2020</a>)</cite> proposes a domain adaptation method for 2D HPE, which accomplishes both the human body-level topological structure alignment and fine-grained feature alignment in different datasets.
<cite class="ltx_cite ltx_citemacro_cite">Guo et al. (<a href="#bib.bib47" title="" class="ltx_ref">2018</a>)</cite> proposes a multi-domain pose network that is able to train the model on multiple dataset simultaneously, which obtains a better pose representation in a multi-domain learning fashion.
<cite class="ltx_cite ltx_citemacro_cite">Li and Lee (<a href="#bib.bib82" title="" class="ltx_ref">2021</a>)</cite> proposes an online coarse-to-fine pseudo label updating strategy to reduce the gap between the synthetic and real data, which have demonstrated strong generalization ability for animal pose estimation.
<cite class="ltx_cite ltx_citemacro_cite">Li and Lee (<a href="#bib.bib82" title="" class="ltx_ref">2021</a>)</cite> is able to softens the label noises and thereby delivers state-of-the-art results on multiple animal benchmark datasets.<span id="S4.SS4.p2.1.1" class="ltx_text"></span></p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Post Processing Approaches</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p"><span id="S5.p1.1.1" class="ltx_text">Instead of predicting the final keypoint locations at once, some approaches first estimate an initial pose and then optimize it with some post-processing operations, which we refer to as <em id="S5.p1.1.1.1" class="ltx_emph ltx_font_italic">post processing</em> methods.
We divide these methods into two categories, <em id="S5.p1.1.1.2" class="ltx_emph ltx_font_italic">i.e.</em>, quantization error and pose resampling.<span id="S5.p1.1.1.3" class="ltx_text">
For the heatmap representation of keypoints, the conversion from heatmap to coordinate space inevitably occurs errors, which leads to <em id="S5.p1.1.1.3.1" class="ltx_emph ltx_font_italic">quantization errors</em>.
Suppressing such quantization errors will boost the performance of numerous heatmap-based models.
<span id="S5.p1.1.1.3.2" class="ltx_text">On the other hand, an out-of-the-box pose refinement technique, <em id="S5.p1.1.1.3.2.1" class="ltx_emph ltx_font_italic">pose resampling</em>, aims at resampling favorable pose representations to improve the initial estimations.<span id="S5.p1.1.1.3.2.2" class="ltx_text">
In what follows, we elaborate on the above approaches.</span></span></span></span></p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Quantization Error</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p"><span id="S5.SS1.p1.1.1" class="ltx_text">The extensively adopted heatmap based pose representation requires decoding the 2D coordinates <math id="S5.SS1.p1.1.1.m1.2" class="ltx_Math" alttext="(x,y)" display="inline"><semantics id="S5.SS1.p1.1.1.m1.2a"><mrow id="S5.SS1.p1.1.1.m1.2.3.2" xref="S5.SS1.p1.1.1.m1.2.3.1.cmml"><mo stretchy="false" id="S5.SS1.p1.1.1.m1.2.3.2.1" xref="S5.SS1.p1.1.1.m1.2.3.1.cmml">(</mo><mi id="S5.SS1.p1.1.1.m1.1.1" xref="S5.SS1.p1.1.1.m1.1.1.cmml">x</mi><mo id="S5.SS1.p1.1.1.m1.2.3.2.2" xref="S5.SS1.p1.1.1.m1.2.3.1.cmml">,</mo><mi id="S5.SS1.p1.1.1.m1.2.2" xref="S5.SS1.p1.1.1.m1.2.2.cmml">y</mi><mo stretchy="false" id="S5.SS1.p1.1.1.m1.2.3.2.3" xref="S5.SS1.p1.1.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.1.m1.2b"><interval closure="open" id="S5.SS1.p1.1.1.m1.2.3.1.cmml" xref="S5.SS1.p1.1.1.m1.2.3.2"><ci id="S5.SS1.p1.1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.1.m1.1.1">𝑥</ci><ci id="S5.SS1.p1.1.1.m1.2.2.cmml" xref="S5.SS1.p1.1.1.m1.2.2">𝑦</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.1.m1.2c">(x,y)</annotation></semantics></math> of joints from estimated keypoint heatmaps.
In particular, we take the position of the maximum activation value from the predicted heatmap as the keypoint coordinates.
However, the predicted gaussian heatmaps do not always conform to the standard gaussian distribution and potentially contain multiple peak values, which degrades the accuracy of the coordinate computation.
To address the issue, <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib193" title="" class="ltx_ref">2020a</a>)</cite> proposes a distribution-aware architecture that first performs heatmap distribution modulation to adjust the shape of predicted heatmaps and then employs a new coordinate decoding method to accurately obtain the final keypoint locations. This approach reduces mistakes of the conversion from heatmaps to coordinates, and improves the performance of existing heatmap-based models.<span id="S5.SS1.p1.1.1.1" class="ltx_text">
<cite class="ltx_cite ltx_citemacro_cite">Huang et al. (<a href="#bib.bib58" title="" class="ltx_ref">2020a</a>)</cite> quantitatively analyzes the common biased data processing on 2D HPE, and further processes data based on unit length instead of pixel, which obtains aligned pose results when flipping is performed in inference.
<span id="S5.SS1.p1.1.1.1.1" class="ltx_text">Furthermore, this approach introduces an encoding-decoding method, which is theoretically error-free for the transformation of keypoint locations between heatmaps and coordinates.<span id="S5.SS1.p1.1.1.1.1.1" class="ltx_text"></span></span></span></span></p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p"><span id="S5.SS1.p2.1.1" class="ltx_text">On the other hand, the non-differentiable property of the maximum operation in the decoding process also introduces quantization errors.
To address this problem, a group of researches <cite class="ltx_cite ltx_citemacro_cite">Luvizon et al. (<a href="#bib.bib107" title="" class="ltx_ref">2019</a>); Sun et al. (<a href="#bib.bib155" title="" class="ltx_ref">2018</a>)</cite> attempt to design differentiable algorithms.
<cite class="ltx_cite ltx_citemacro_cite">Luvizon et al. (<a href="#bib.bib107" title="" class="ltx_ref">2019</a>)</cite> proposes a fully differentiable and end-to-end trainable regression approach, which utilizes the novel Soft-argmax function to convert feature maps directly to keypoint coordinates.
<cite class="ltx_cite ltx_citemacro_cite">Sun et al. (<a href="#bib.bib155" title="" class="ltx_ref">2018</a>)</cite> proposes an integral method to tackle the problem of non-differentiable from heatmaps to coordinates.<span id="S5.SS1.p2.1.1.1" class="ltx_text"></span></span></p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Pose Resampling</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p"><span id="S5.SS2.p1.1.1" class="ltx_text">A wide spectrum of pose estimators <cite class="ltx_cite ltx_citemacro_cite">Sun et al. (<a href="#bib.bib153" title="" class="ltx_ref">2019</a>); Xiao et al. (<a href="#bib.bib181" title="" class="ltx_ref">2018</a>)</cite> directly take the model output as final estimates.
However, these estimations can be further improved by a model-agnostic pose resampling technique.
A line of work considers fine-tuning of the initial estimation with additional pose cues.
<cite class="ltx_cite ltx_citemacro_cite">Moon et al. (<a href="#bib.bib118" title="" class="ltx_ref">2019</a>)</cite> proposes a model-agnostic PoseFix method that estimates a refined pose from a tuple of an input image and an input pose, where the input pose is derived from the estimations of existing methods.<span id="S5.SS2.p1.1.1.1" class="ltx_text">
<cite class="ltx_cite ltx_citemacro_cite">Qiu et al. (<a href="#bib.bib135" title="" class="ltx_ref">2020</a>)</cite> proposes to first localize the visible joints based on visual information by an existing pose estimator, and then estimate the invisible joints by an Image-Guided Progressive GCN module that combines image context and pose structure cues.
<cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib170" title="" class="ltx_ref">2020a</a>)</cite> proposes a two-stage and model-agnostic framework, namely Graph-PCNN, which employs an existing pose estimator for coarse keypoint localization, and designs a graph pose refinement module to produce more accurate localization results.</span></span></p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p"><span id="S5.SS2.p2.1.1" class="ltx_text">The above pose resampling methods are designed for static images, and
some approaches explore the pose resampling techniques for videos.
Specifically, these methods <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib172" title="" class="ltx_ref">2020b</a>); Liu et al. (<a href="#bib.bib99" title="" class="ltx_ref">2021a</a>); Yang et al. (<a href="#bib.bib186" title="" class="ltx_ref">2021</a>); Bertasius et al. (<a href="#bib.bib5" title="" class="ltx_ref">2019</a>); Zhou et al. (<a href="#bib.bib201" title="" class="ltx_ref">2020a</a>)</cite> perform pose aggregation to integrate multiple estimated poses of current frame to refine estimations.
Normalization is commonly leveraged to aggregate multiple pose predictions <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib99" title="" class="ltx_ref">2021a</a>); Bertasius et al. (<a href="#bib.bib5" title="" class="ltx_ref">2019</a>); Yang et al. (<a href="#bib.bib186" title="" class="ltx_ref">2021</a>)</cite>, where the various predictions are treated equally.<span id="S5.SS2.p2.1.1.1" class="ltx_text">
<cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib172" title="" class="ltx_ref">2020b</a>)</cite> introduces the Dijkstra algorithm <cite class="ltx_cite ltx_citemacro_cite">Dijkstra et al. (<a href="#bib.bib23" title="" class="ltx_ref">1959</a>)</cite> to solve the problem of optimal keypoint locations, which first employs the mean shift algorithm <cite class="ltx_cite ltx_citemacro_cite">Comaniciu and Meer (<a href="#bib.bib21" title="" class="ltx_ref">2002</a>)</cite> to group all pose hypotheses into various clusters, and subsequently selects the keypoint with closest distance to the cluster center as the optimal result.
<cite class="ltx_cite ltx_citemacro_cite">Zhou et al. (<a href="#bib.bib201" title="" class="ltx_ref">2020a</a>)</cite> utilizes the pose similarity between the neighboring frames and the current frame to biasedly aggregate features, and then employs a convolutional neural network to decode current heatmaps from the aggregated features.</span></span></p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Datasets and Evaluation</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Benchmark datasets form the basis of deep learning models, and also provide a common foundation for measuring and comparing the performance of competing approaches.
In this section, we present the major <em id="S6.p1.1.1" class="ltx_emph ltx_font_italic">benchmark datasets</em>, <em id="S6.p1.1.2" class="ltx_emph ltx_font_italic">evaluation metrics</em>, and <em id="S6.p1.1.3" class="ltx_emph ltx_font_italic">performance comparisons</em> for human pose estimation.</p>
</div>
<figure id="S6.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>A summary of 2D human pose estimation benchmark datasets. <em id="S6.T2.26.1" class="ltx_emph ltx_font_italic">Upper Poses</em>, <em id="S6.T2.27.2" class="ltx_emph ltx_font_italic">Full Poses</em>, <em id="S6.T2.28.3" class="ltx_emph ltx_font_italic">Various Poses</em> denotes the upper body poses, singular full body poses and various body poses, respectively.</figcaption>
<div id="S6.T2.22" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:146.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-289.1pt,97.7pt) scale(0.428583459344768,0.428583459344768) ;">
<table id="S6.T2.22.22" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T2.22.22.23.1" class="ltx_tr">
<td id="S6.T2.22.22.23.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;" rowspan="2"><span id="S6.T2.22.22.23.1.1.1" class="ltx_text">Dataset Name</span></td>
<td id="S6.T2.22.22.23.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;" rowspan="2"><span id="S6.T2.22.22.23.1.2.1" class="ltx_text">Year</span></td>
<td id="S6.T2.22.22.23.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;" rowspan="2"><span id="S6.T2.22.22.23.1.3.1" class="ltx_text">Single-Person</span></td>
<td id="S6.T2.22.22.23.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;" rowspan="2"><span id="S6.T2.22.22.23.1.4.1" class="ltx_text">Multi-Person</span></td>
<td id="S6.T2.22.22.23.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;" rowspan="2"><span id="S6.T2.22.22.23.1.5.1" class="ltx_text">Upper Poses</span></td>
<td id="S6.T2.22.22.23.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;" rowspan="2"><span id="S6.T2.22.22.23.1.6.1" class="ltx_text">Full Poses</span></td>
<td id="S6.T2.22.22.23.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;" rowspan="2"><span id="S6.T2.22.22.23.1.7.1" class="ltx_text">Various Poses</span></td>
<td id="S6.T2.22.22.23.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;" rowspan="2"><span id="S6.T2.22.22.23.1.8.1" class="ltx_text">Number of Joints</span></td>
<td id="S6.T2.22.22.23.1.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;" rowspan="2"><span id="S6.T2.22.22.23.1.9.1" class="ltx_text">Evaluation Metric</span></td>
<td id="S6.T2.22.22.23.1.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;" colspan="3">Number of Images / Videos</td>
</tr>
<tr id="S6.T2.22.22.24.2" class="ltx_tr">
<td id="S6.T2.22.22.24.2.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">Train</td>
<td id="S6.T2.22.22.24.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">Val</td>
<td id="S6.T2.22.22.24.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">Test</td>
</tr>
<tr id="S6.T2.22.22.25.3" class="ltx_tr">
<td id="S6.T2.22.22.25.3.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;" colspan="12">
<span id="S6.T2.22.22.25.3.1.1" class="ltx_text ltx_font_bold">Image</span>-Based Datasets for Human Pose Estimation.</td>
</tr>
<tr id="S6.T2.2.2.2" class="ltx_tr">
<td id="S6.T2.2.2.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">LSP <cite class="ltx_cite ltx_citemacro_cite">Johnson and Everingham (<a href="#bib.bib71" title="" class="ltx_ref">2010</a>)</cite>
</td>
<td id="S6.T2.2.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">2010</td>
<td id="S6.T2.2.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">✓</td>
<td id="S6.T2.2.2.2.6" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.2.2.2.7" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.2.2.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">✓</td>
<td id="S6.T2.2.2.2.9" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.2.2.2.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">14</td>
<td id="S6.T2.2.2.2.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">PCP</td>
<td id="S6.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T2.1.1.1.1.m1.2" class="ltx_Math" alttext="1,000" display="inline"><semantics id="S6.T2.1.1.1.1.m1.2a"><mrow id="S6.T2.1.1.1.1.m1.2.3.2" xref="S6.T2.1.1.1.1.m1.2.3.1.cmml"><mn id="S6.T2.1.1.1.1.m1.1.1" xref="S6.T2.1.1.1.1.m1.1.1.cmml">1</mn><mo id="S6.T2.1.1.1.1.m1.2.3.2.1" xref="S6.T2.1.1.1.1.m1.2.3.1.cmml">,</mo><mn id="S6.T2.1.1.1.1.m1.2.2" xref="S6.T2.1.1.1.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T2.1.1.1.1.m1.2b"><list id="S6.T2.1.1.1.1.m1.2.3.1.cmml" xref="S6.T2.1.1.1.1.m1.2.3.2"><cn type="integer" id="S6.T2.1.1.1.1.m1.1.1.cmml" xref="S6.T2.1.1.1.1.m1.1.1">1</cn><cn type="integer" id="S6.T2.1.1.1.1.m1.2.2.cmml" xref="S6.T2.1.1.1.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.1.1.1.1.m1.2c">1,000</annotation></semantics></math></td>
<td id="S6.T2.2.2.2.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T2.2.2.2.2.m1.2" class="ltx_Math" alttext="1,000" display="inline"><semantics id="S6.T2.2.2.2.2.m1.2a"><mrow id="S6.T2.2.2.2.2.m1.2.3.2" xref="S6.T2.2.2.2.2.m1.2.3.1.cmml"><mn id="S6.T2.2.2.2.2.m1.1.1" xref="S6.T2.2.2.2.2.m1.1.1.cmml">1</mn><mo id="S6.T2.2.2.2.2.m1.2.3.2.1" xref="S6.T2.2.2.2.2.m1.2.3.1.cmml">,</mo><mn id="S6.T2.2.2.2.2.m1.2.2" xref="S6.T2.2.2.2.2.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T2.2.2.2.2.m1.2b"><list id="S6.T2.2.2.2.2.m1.2.3.1.cmml" xref="S6.T2.2.2.2.2.m1.2.3.2"><cn type="integer" id="S6.T2.2.2.2.2.m1.1.1.cmml" xref="S6.T2.2.2.2.2.m1.1.1">1</cn><cn type="integer" id="S6.T2.2.2.2.2.m1.2.2.cmml" xref="S6.T2.2.2.2.2.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.2.2.2.2.m1.2c">1,000</annotation></semantics></math></td>
</tr>
<tr id="S6.T2.3.3.3" class="ltx_tr">
<td id="S6.T2.3.3.3.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">LSP-Extended <cite class="ltx_cite ltx_citemacro_cite">Johnson and Everingham (<a href="#bib.bib72" title="" class="ltx_ref">2011</a>)</cite>
</td>
<td id="S6.T2.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">2011</td>
<td id="S6.T2.3.3.3.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">✓</td>
<td id="S6.T2.3.3.3.5" class="ltx_td ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.3.3.3.6" class="ltx_td ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.3.3.3.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">✓</td>
<td id="S6.T2.3.3.3.8" class="ltx_td ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.3.3.3.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">14</td>
<td id="S6.T2.3.3.3.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">PCP</td>
<td id="S6.T2.3.3.3.1" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T2.3.3.3.1.m1.2" class="ltx_Math" alttext="10,000" display="inline"><semantics id="S6.T2.3.3.3.1.m1.2a"><mrow id="S6.T2.3.3.3.1.m1.2.3.2" xref="S6.T2.3.3.3.1.m1.2.3.1.cmml"><mn id="S6.T2.3.3.3.1.m1.1.1" xref="S6.T2.3.3.3.1.m1.1.1.cmml">10</mn><mo id="S6.T2.3.3.3.1.m1.2.3.2.1" xref="S6.T2.3.3.3.1.m1.2.3.1.cmml">,</mo><mn id="S6.T2.3.3.3.1.m1.2.2" xref="S6.T2.3.3.3.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T2.3.3.3.1.m1.2b"><list id="S6.T2.3.3.3.1.m1.2.3.1.cmml" xref="S6.T2.3.3.3.1.m1.2.3.2"><cn type="integer" id="S6.T2.3.3.3.1.m1.1.1.cmml" xref="S6.T2.3.3.3.1.m1.1.1">10</cn><cn type="integer" id="S6.T2.3.3.3.1.m1.2.2.cmml" xref="S6.T2.3.3.3.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.3.3.3.1.m1.2c">10,000</annotation></semantics></math></td>
<td id="S6.T2.3.3.3.11" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T2.3.3.3.12" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
</tr>
<tr id="S6.T2.5.5.5" class="ltx_tr">
<td id="S6.T2.5.5.5.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">Flic <cite class="ltx_cite ltx_citemacro_cite">Sapp and Taskar (<a href="#bib.bib142" title="" class="ltx_ref">2013</a>)</cite>
</td>
<td id="S6.T2.5.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">2013</td>
<td id="S6.T2.5.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">✓</td>
<td id="S6.T2.5.5.5.6" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.5.5.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">✓</td>
<td id="S6.T2.5.5.5.8" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.5.5.5.9" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.5.5.5.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">10</td>
<td id="S6.T2.5.5.5.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">PCP</td>
<td id="S6.T2.4.4.4.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T2.4.4.4.1.m1.2" class="ltx_Math" alttext="5,000" display="inline"><semantics id="S6.T2.4.4.4.1.m1.2a"><mrow id="S6.T2.4.4.4.1.m1.2.3.2" xref="S6.T2.4.4.4.1.m1.2.3.1.cmml"><mn id="S6.T2.4.4.4.1.m1.1.1" xref="S6.T2.4.4.4.1.m1.1.1.cmml">5</mn><mo id="S6.T2.4.4.4.1.m1.2.3.2.1" xref="S6.T2.4.4.4.1.m1.2.3.1.cmml">,</mo><mn id="S6.T2.4.4.4.1.m1.2.2" xref="S6.T2.4.4.4.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T2.4.4.4.1.m1.2b"><list id="S6.T2.4.4.4.1.m1.2.3.1.cmml" xref="S6.T2.4.4.4.1.m1.2.3.2"><cn type="integer" id="S6.T2.4.4.4.1.m1.1.1.cmml" xref="S6.T2.4.4.4.1.m1.1.1">5</cn><cn type="integer" id="S6.T2.4.4.4.1.m1.2.2.cmml" xref="S6.T2.4.4.4.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.4.4.4.1.m1.2c">5,000</annotation></semantics></math></td>
<td id="S6.T2.5.5.5.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T2.5.5.5.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T2.5.5.5.2.m1.2" class="ltx_Math" alttext="1,016" display="inline"><semantics id="S6.T2.5.5.5.2.m1.2a"><mrow id="S6.T2.5.5.5.2.m1.2.3.2" xref="S6.T2.5.5.5.2.m1.2.3.1.cmml"><mn id="S6.T2.5.5.5.2.m1.1.1" xref="S6.T2.5.5.5.2.m1.1.1.cmml">1</mn><mo id="S6.T2.5.5.5.2.m1.2.3.2.1" xref="S6.T2.5.5.5.2.m1.2.3.1.cmml">,</mo><mn id="S6.T2.5.5.5.2.m1.2.2" xref="S6.T2.5.5.5.2.m1.2.2.cmml">016</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T2.5.5.5.2.m1.2b"><list id="S6.T2.5.5.5.2.m1.2.3.1.cmml" xref="S6.T2.5.5.5.2.m1.2.3.2"><cn type="integer" id="S6.T2.5.5.5.2.m1.1.1.cmml" xref="S6.T2.5.5.5.2.m1.1.1">1</cn><cn type="integer" id="S6.T2.5.5.5.2.m1.2.2.cmml" xref="S6.T2.5.5.5.2.m1.2.2">016</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.5.5.5.2.m1.2c">1,016</annotation></semantics></math></td>
</tr>
<tr id="S6.T2.6.6.6" class="ltx_tr">
<td id="S6.T2.6.6.6.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">Flic-Full <cite class="ltx_cite ltx_citemacro_cite">Sapp and Taskar (<a href="#bib.bib142" title="" class="ltx_ref">2013</a>)</cite>
</td>
<td id="S6.T2.6.6.6.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">2013</td>
<td id="S6.T2.6.6.6.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">✓</td>
<td id="S6.T2.6.6.6.5" class="ltx_td ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.6.6.6.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">✓</td>
<td id="S6.T2.6.6.6.7" class="ltx_td ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.6.6.6.8" class="ltx_td ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.6.6.6.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">10</td>
<td id="S6.T2.6.6.6.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">PCP</td>
<td id="S6.T2.6.6.6.1" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T2.6.6.6.1.m1.2" class="ltx_Math" alttext="20,928" display="inline"><semantics id="S6.T2.6.6.6.1.m1.2a"><mrow id="S6.T2.6.6.6.1.m1.2.3.2" xref="S6.T2.6.6.6.1.m1.2.3.1.cmml"><mn id="S6.T2.6.6.6.1.m1.1.1" xref="S6.T2.6.6.6.1.m1.1.1.cmml">20</mn><mo id="S6.T2.6.6.6.1.m1.2.3.2.1" xref="S6.T2.6.6.6.1.m1.2.3.1.cmml">,</mo><mn id="S6.T2.6.6.6.1.m1.2.2" xref="S6.T2.6.6.6.1.m1.2.2.cmml">928</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T2.6.6.6.1.m1.2b"><list id="S6.T2.6.6.6.1.m1.2.3.1.cmml" xref="S6.T2.6.6.6.1.m1.2.3.2"><cn type="integer" id="S6.T2.6.6.6.1.m1.1.1.cmml" xref="S6.T2.6.6.6.1.m1.1.1">20</cn><cn type="integer" id="S6.T2.6.6.6.1.m1.2.2.cmml" xref="S6.T2.6.6.6.1.m1.2.2">928</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.6.6.6.1.m1.2c">20,928</annotation></semantics></math></td>
<td id="S6.T2.6.6.6.11" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T2.6.6.6.12" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
</tr>
<tr id="S6.T2.7.7.7" class="ltx_tr">
<td id="S6.T2.7.7.7.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">Flic-Plus <cite class="ltx_cite ltx_citemacro_cite">Tompson et al. (<a href="#bib.bib160" title="" class="ltx_ref">2014</a>)</cite>
</td>
<td id="S6.T2.7.7.7.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">2013</td>
<td id="S6.T2.7.7.7.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">✓</td>
<td id="S6.T2.7.7.7.5" class="ltx_td ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.7.7.7.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">✓</td>
<td id="S6.T2.7.7.7.7" class="ltx_td ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.7.7.7.8" class="ltx_td ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.7.7.7.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">10</td>
<td id="S6.T2.7.7.7.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">PCP</td>
<td id="S6.T2.7.7.7.1" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T2.7.7.7.1.m1.2" class="ltx_Math" alttext="17,380" display="inline"><semantics id="S6.T2.7.7.7.1.m1.2a"><mrow id="S6.T2.7.7.7.1.m1.2.3.2" xref="S6.T2.7.7.7.1.m1.2.3.1.cmml"><mn id="S6.T2.7.7.7.1.m1.1.1" xref="S6.T2.7.7.7.1.m1.1.1.cmml">17</mn><mo id="S6.T2.7.7.7.1.m1.2.3.2.1" xref="S6.T2.7.7.7.1.m1.2.3.1.cmml">,</mo><mn id="S6.T2.7.7.7.1.m1.2.2" xref="S6.T2.7.7.7.1.m1.2.2.cmml">380</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T2.7.7.7.1.m1.2b"><list id="S6.T2.7.7.7.1.m1.2.3.1.cmml" xref="S6.T2.7.7.7.1.m1.2.3.2"><cn type="integer" id="S6.T2.7.7.7.1.m1.1.1.cmml" xref="S6.T2.7.7.7.1.m1.1.1">17</cn><cn type="integer" id="S6.T2.7.7.7.1.m1.2.2.cmml" xref="S6.T2.7.7.7.1.m1.2.2">380</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.7.7.7.1.m1.2c">17,380</annotation></semantics></math></td>
<td id="S6.T2.7.7.7.11" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T2.7.7.7.12" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
</tr>
<tr id="S6.T2.9.9.9" class="ltx_tr">
<td id="S6.T2.9.9.9.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">MPII <cite class="ltx_cite ltx_citemacro_cite">Andriluka et al. (<a href="#bib.bib1" title="" class="ltx_ref">2014</a>)</cite>
</td>
<td id="S6.T2.9.9.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">2014</td>
<td id="S6.T2.9.9.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">✓</td>
<td id="S6.T2.9.9.9.6" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.9.9.9.7" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.9.9.9.8" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.9.9.9.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">✓</td>
<td id="S6.T2.9.9.9.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">16</td>
<td id="S6.T2.9.9.9.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">PCPm/PCKh</td>
<td id="S6.T2.8.8.8.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T2.8.8.8.1.m1.2" class="ltx_Math" alttext="28,821" display="inline"><semantics id="S6.T2.8.8.8.1.m1.2a"><mrow id="S6.T2.8.8.8.1.m1.2.3.2" xref="S6.T2.8.8.8.1.m1.2.3.1.cmml"><mn id="S6.T2.8.8.8.1.m1.1.1" xref="S6.T2.8.8.8.1.m1.1.1.cmml">28</mn><mo id="S6.T2.8.8.8.1.m1.2.3.2.1" xref="S6.T2.8.8.8.1.m1.2.3.1.cmml">,</mo><mn id="S6.T2.8.8.8.1.m1.2.2" xref="S6.T2.8.8.8.1.m1.2.2.cmml">821</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T2.8.8.8.1.m1.2b"><list id="S6.T2.8.8.8.1.m1.2.3.1.cmml" xref="S6.T2.8.8.8.1.m1.2.3.2"><cn type="integer" id="S6.T2.8.8.8.1.m1.1.1.cmml" xref="S6.T2.8.8.8.1.m1.1.1">28</cn><cn type="integer" id="S6.T2.8.8.8.1.m1.2.2.cmml" xref="S6.T2.8.8.8.1.m1.2.2">821</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.8.8.8.1.m1.2c">28,821</annotation></semantics></math></td>
<td id="S6.T2.9.9.9.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T2.9.9.9.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T2.9.9.9.2.m1.2" class="ltx_Math" alttext="11,701" display="inline"><semantics id="S6.T2.9.9.9.2.m1.2a"><mrow id="S6.T2.9.9.9.2.m1.2.3.2" xref="S6.T2.9.9.9.2.m1.2.3.1.cmml"><mn id="S6.T2.9.9.9.2.m1.1.1" xref="S6.T2.9.9.9.2.m1.1.1.cmml">11</mn><mo id="S6.T2.9.9.9.2.m1.2.3.2.1" xref="S6.T2.9.9.9.2.m1.2.3.1.cmml">,</mo><mn id="S6.T2.9.9.9.2.m1.2.2" xref="S6.T2.9.9.9.2.m1.2.2.cmml">701</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T2.9.9.9.2.m1.2b"><list id="S6.T2.9.9.9.2.m1.2.3.1.cmml" xref="S6.T2.9.9.9.2.m1.2.3.2"><cn type="integer" id="S6.T2.9.9.9.2.m1.1.1.cmml" xref="S6.T2.9.9.9.2.m1.1.1">11</cn><cn type="integer" id="S6.T2.9.9.9.2.m1.2.2.cmml" xref="S6.T2.9.9.9.2.m1.2.2">701</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.9.9.9.2.m1.2c">11,701</annotation></semantics></math></td>
</tr>
<tr id="S6.T2.11.11.11" class="ltx_tr">
<td id="S6.T2.11.11.11.3" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">MPII <cite class="ltx_cite ltx_citemacro_cite">Andriluka et al. (<a href="#bib.bib1" title="" class="ltx_ref">2014</a>)</cite>
</td>
<td id="S6.T2.11.11.11.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">2014</td>
<td id="S6.T2.11.11.11.5" class="ltx_td ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.11.11.11.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">✓</td>
<td id="S6.T2.11.11.11.7" class="ltx_td ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.11.11.11.8" class="ltx_td ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.11.11.11.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">✓</td>
<td id="S6.T2.11.11.11.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">16</td>
<td id="S6.T2.11.11.11.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">PCKh</td>
<td id="S6.T2.10.10.10.1" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T2.10.10.10.1.m1.2" class="ltx_Math" alttext="3,800" display="inline"><semantics id="S6.T2.10.10.10.1.m1.2a"><mrow id="S6.T2.10.10.10.1.m1.2.3.2" xref="S6.T2.10.10.10.1.m1.2.3.1.cmml"><mn id="S6.T2.10.10.10.1.m1.1.1" xref="S6.T2.10.10.10.1.m1.1.1.cmml">3</mn><mo id="S6.T2.10.10.10.1.m1.2.3.2.1" xref="S6.T2.10.10.10.1.m1.2.3.1.cmml">,</mo><mn id="S6.T2.10.10.10.1.m1.2.2" xref="S6.T2.10.10.10.1.m1.2.2.cmml">800</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T2.10.10.10.1.m1.2b"><list id="S6.T2.10.10.10.1.m1.2.3.1.cmml" xref="S6.T2.10.10.10.1.m1.2.3.2"><cn type="integer" id="S6.T2.10.10.10.1.m1.1.1.cmml" xref="S6.T2.10.10.10.1.m1.1.1">3</cn><cn type="integer" id="S6.T2.10.10.10.1.m1.2.2.cmml" xref="S6.T2.10.10.10.1.m1.2.2">800</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.10.10.10.1.m1.2c">3,800</annotation></semantics></math></td>
<td id="S6.T2.11.11.11.12" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T2.11.11.11.2" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T2.11.11.11.2.m1.2" class="ltx_Math" alttext="1,700" display="inline"><semantics id="S6.T2.11.11.11.2.m1.2a"><mrow id="S6.T2.11.11.11.2.m1.2.3.2" xref="S6.T2.11.11.11.2.m1.2.3.1.cmml"><mn id="S6.T2.11.11.11.2.m1.1.1" xref="S6.T2.11.11.11.2.m1.1.1.cmml">1</mn><mo id="S6.T2.11.11.11.2.m1.2.3.2.1" xref="S6.T2.11.11.11.2.m1.2.3.1.cmml">,</mo><mn id="S6.T2.11.11.11.2.m1.2.2" xref="S6.T2.11.11.11.2.m1.2.2.cmml">700</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T2.11.11.11.2.m1.2b"><list id="S6.T2.11.11.11.2.m1.2.3.1.cmml" xref="S6.T2.11.11.11.2.m1.2.3.2"><cn type="integer" id="S6.T2.11.11.11.2.m1.1.1.cmml" xref="S6.T2.11.11.11.2.m1.1.1">1</cn><cn type="integer" id="S6.T2.11.11.11.2.m1.2.2.cmml" xref="S6.T2.11.11.11.2.m1.2.2">700</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.11.11.11.2.m1.2c">1,700</annotation></semantics></math></td>
</tr>
<tr id="S6.T2.14.14.14" class="ltx_tr">
<td id="S6.T2.14.14.14.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">COCO <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib93" title="" class="ltx_ref">2014</a>)</cite>
</td>
<td id="S6.T2.14.14.14.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">2017</td>
<td id="S6.T2.14.14.14.6" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.14.14.14.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">✓</td>
<td id="S6.T2.14.14.14.8" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.14.14.14.9" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.14.14.14.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">✓</td>
<td id="S6.T2.14.14.14.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">17</td>
<td id="S6.T2.14.14.14.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">AP</td>
<td id="S6.T2.12.12.12.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T2.12.12.12.1.m1.2" class="ltx_Math" alttext="57,000" display="inline"><semantics id="S6.T2.12.12.12.1.m1.2a"><mrow id="S6.T2.12.12.12.1.m1.2.3.2" xref="S6.T2.12.12.12.1.m1.2.3.1.cmml"><mn id="S6.T2.12.12.12.1.m1.1.1" xref="S6.T2.12.12.12.1.m1.1.1.cmml">57</mn><mo id="S6.T2.12.12.12.1.m1.2.3.2.1" xref="S6.T2.12.12.12.1.m1.2.3.1.cmml">,</mo><mn id="S6.T2.12.12.12.1.m1.2.2" xref="S6.T2.12.12.12.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T2.12.12.12.1.m1.2b"><list id="S6.T2.12.12.12.1.m1.2.3.1.cmml" xref="S6.T2.12.12.12.1.m1.2.3.2"><cn type="integer" id="S6.T2.12.12.12.1.m1.1.1.cmml" xref="S6.T2.12.12.12.1.m1.1.1">57</cn><cn type="integer" id="S6.T2.12.12.12.1.m1.2.2.cmml" xref="S6.T2.12.12.12.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.12.12.12.1.m1.2c">57,000</annotation></semantics></math></td>
<td id="S6.T2.13.13.13.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T2.13.13.13.2.m1.2" class="ltx_Math" alttext="5,000" display="inline"><semantics id="S6.T2.13.13.13.2.m1.2a"><mrow id="S6.T2.13.13.13.2.m1.2.3.2" xref="S6.T2.13.13.13.2.m1.2.3.1.cmml"><mn id="S6.T2.13.13.13.2.m1.1.1" xref="S6.T2.13.13.13.2.m1.1.1.cmml">5</mn><mo id="S6.T2.13.13.13.2.m1.2.3.2.1" xref="S6.T2.13.13.13.2.m1.2.3.1.cmml">,</mo><mn id="S6.T2.13.13.13.2.m1.2.2" xref="S6.T2.13.13.13.2.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T2.13.13.13.2.m1.2b"><list id="S6.T2.13.13.13.2.m1.2.3.1.cmml" xref="S6.T2.13.13.13.2.m1.2.3.2"><cn type="integer" id="S6.T2.13.13.13.2.m1.1.1.cmml" xref="S6.T2.13.13.13.2.m1.1.1">5</cn><cn type="integer" id="S6.T2.13.13.13.2.m1.2.2.cmml" xref="S6.T2.13.13.13.2.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.13.13.13.2.m1.2c">5,000</annotation></semantics></math></td>
<td id="S6.T2.14.14.14.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T2.14.14.14.3.m1.2" class="ltx_Math" alttext="20,000" display="inline"><semantics id="S6.T2.14.14.14.3.m1.2a"><mrow id="S6.T2.14.14.14.3.m1.2.3.2" xref="S6.T2.14.14.14.3.m1.2.3.1.cmml"><mn id="S6.T2.14.14.14.3.m1.1.1" xref="S6.T2.14.14.14.3.m1.1.1.cmml">20</mn><mo id="S6.T2.14.14.14.3.m1.2.3.2.1" xref="S6.T2.14.14.14.3.m1.2.3.1.cmml">,</mo><mn id="S6.T2.14.14.14.3.m1.2.2" xref="S6.T2.14.14.14.3.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T2.14.14.14.3.m1.2b"><list id="S6.T2.14.14.14.3.m1.2.3.1.cmml" xref="S6.T2.14.14.14.3.m1.2.3.2"><cn type="integer" id="S6.T2.14.14.14.3.m1.1.1.cmml" xref="S6.T2.14.14.14.3.m1.1.1">20</cn><cn type="integer" id="S6.T2.14.14.14.3.m1.2.2.cmml" xref="S6.T2.14.14.14.3.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.14.14.14.3.m1.2c">20,000</annotation></semantics></math></td>
</tr>
<tr id="S6.T2.17.17.17" class="ltx_tr">
<td id="S6.T2.17.17.17.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">AIC-HKD <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a href="#bib.bib179" title="" class="ltx_ref">2017</a>)</cite>
</td>
<td id="S6.T2.17.17.17.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">2017</td>
<td id="S6.T2.17.17.17.6" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.17.17.17.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">✓</td>
<td id="S6.T2.17.17.17.8" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.17.17.17.9" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.17.17.17.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">✓</td>
<td id="S6.T2.17.17.17.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">14</td>
<td id="S6.T2.17.17.17.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">mAP</td>
<td id="S6.T2.15.15.15.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T2.15.15.15.1.m1.2" class="ltx_Math" alttext="210,000" display="inline"><semantics id="S6.T2.15.15.15.1.m1.2a"><mrow id="S6.T2.15.15.15.1.m1.2.3.2" xref="S6.T2.15.15.15.1.m1.2.3.1.cmml"><mn id="S6.T2.15.15.15.1.m1.1.1" xref="S6.T2.15.15.15.1.m1.1.1.cmml">210</mn><mo id="S6.T2.15.15.15.1.m1.2.3.2.1" xref="S6.T2.15.15.15.1.m1.2.3.1.cmml">,</mo><mn id="S6.T2.15.15.15.1.m1.2.2" xref="S6.T2.15.15.15.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T2.15.15.15.1.m1.2b"><list id="S6.T2.15.15.15.1.m1.2.3.1.cmml" xref="S6.T2.15.15.15.1.m1.2.3.2"><cn type="integer" id="S6.T2.15.15.15.1.m1.1.1.cmml" xref="S6.T2.15.15.15.1.m1.1.1">210</cn><cn type="integer" id="S6.T2.15.15.15.1.m1.2.2.cmml" xref="S6.T2.15.15.15.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.15.15.15.1.m1.2c">210,000</annotation></semantics></math></td>
<td id="S6.T2.16.16.16.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T2.16.16.16.2.m1.2" class="ltx_Math" alttext="30,000" display="inline"><semantics id="S6.T2.16.16.16.2.m1.2a"><mrow id="S6.T2.16.16.16.2.m1.2.3.2" xref="S6.T2.16.16.16.2.m1.2.3.1.cmml"><mn id="S6.T2.16.16.16.2.m1.1.1" xref="S6.T2.16.16.16.2.m1.1.1.cmml">30</mn><mo id="S6.T2.16.16.16.2.m1.2.3.2.1" xref="S6.T2.16.16.16.2.m1.2.3.1.cmml">,</mo><mn id="S6.T2.16.16.16.2.m1.2.2" xref="S6.T2.16.16.16.2.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T2.16.16.16.2.m1.2b"><list id="S6.T2.16.16.16.2.m1.2.3.1.cmml" xref="S6.T2.16.16.16.2.m1.2.3.2"><cn type="integer" id="S6.T2.16.16.16.2.m1.1.1.cmml" xref="S6.T2.16.16.16.2.m1.1.1">30</cn><cn type="integer" id="S6.T2.16.16.16.2.m1.2.2.cmml" xref="S6.T2.16.16.16.2.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.16.16.16.2.m1.2c">30,000</annotation></semantics></math></td>
<td id="S6.T2.17.17.17.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T2.17.17.17.3.m1.2" class="ltx_Math" alttext="60,000" display="inline"><semantics id="S6.T2.17.17.17.3.m1.2a"><mrow id="S6.T2.17.17.17.3.m1.2.3.2" xref="S6.T2.17.17.17.3.m1.2.3.1.cmml"><mn id="S6.T2.17.17.17.3.m1.1.1" xref="S6.T2.17.17.17.3.m1.1.1.cmml">60</mn><mo id="S6.T2.17.17.17.3.m1.2.3.2.1" xref="S6.T2.17.17.17.3.m1.2.3.1.cmml">,</mo><mn id="S6.T2.17.17.17.3.m1.2.2" xref="S6.T2.17.17.17.3.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T2.17.17.17.3.m1.2b"><list id="S6.T2.17.17.17.3.m1.2.3.1.cmml" xref="S6.T2.17.17.17.3.m1.2.3.2"><cn type="integer" id="S6.T2.17.17.17.3.m1.1.1.cmml" xref="S6.T2.17.17.17.3.m1.1.1">60</cn><cn type="integer" id="S6.T2.17.17.17.3.m1.2.2.cmml" xref="S6.T2.17.17.17.3.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.17.17.17.3.m1.2c">60,000</annotation></semantics></math></td>
</tr>
<tr id="S6.T2.20.20.20" class="ltx_tr">
<td id="S6.T2.20.20.20.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">CrowdedPose <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib84" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="S6.T2.20.20.20.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">2019</td>
<td id="S6.T2.20.20.20.6" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.20.20.20.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">✓</td>
<td id="S6.T2.20.20.20.8" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.20.20.20.9" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.20.20.20.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">✓</td>
<td id="S6.T2.20.20.20.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">14</td>
<td id="S6.T2.20.20.20.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">mAP</td>
<td id="S6.T2.18.18.18.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T2.18.18.18.1.m1.2" class="ltx_Math" alttext="10,000" display="inline"><semantics id="S6.T2.18.18.18.1.m1.2a"><mrow id="S6.T2.18.18.18.1.m1.2.3.2" xref="S6.T2.18.18.18.1.m1.2.3.1.cmml"><mn id="S6.T2.18.18.18.1.m1.1.1" xref="S6.T2.18.18.18.1.m1.1.1.cmml">10</mn><mo id="S6.T2.18.18.18.1.m1.2.3.2.1" xref="S6.T2.18.18.18.1.m1.2.3.1.cmml">,</mo><mn id="S6.T2.18.18.18.1.m1.2.2" xref="S6.T2.18.18.18.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T2.18.18.18.1.m1.2b"><list id="S6.T2.18.18.18.1.m1.2.3.1.cmml" xref="S6.T2.18.18.18.1.m1.2.3.2"><cn type="integer" id="S6.T2.18.18.18.1.m1.1.1.cmml" xref="S6.T2.18.18.18.1.m1.1.1">10</cn><cn type="integer" id="S6.T2.18.18.18.1.m1.2.2.cmml" xref="S6.T2.18.18.18.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.18.18.18.1.m1.2c">10,000</annotation></semantics></math></td>
<td id="S6.T2.19.19.19.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T2.19.19.19.2.m1.2" class="ltx_Math" alttext="2,000" display="inline"><semantics id="S6.T2.19.19.19.2.m1.2a"><mrow id="S6.T2.19.19.19.2.m1.2.3.2" xref="S6.T2.19.19.19.2.m1.2.3.1.cmml"><mn id="S6.T2.19.19.19.2.m1.1.1" xref="S6.T2.19.19.19.2.m1.1.1.cmml">2</mn><mo id="S6.T2.19.19.19.2.m1.2.3.2.1" xref="S6.T2.19.19.19.2.m1.2.3.1.cmml">,</mo><mn id="S6.T2.19.19.19.2.m1.2.2" xref="S6.T2.19.19.19.2.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T2.19.19.19.2.m1.2b"><list id="S6.T2.19.19.19.2.m1.2.3.1.cmml" xref="S6.T2.19.19.19.2.m1.2.3.2"><cn type="integer" id="S6.T2.19.19.19.2.m1.1.1.cmml" xref="S6.T2.19.19.19.2.m1.1.1">2</cn><cn type="integer" id="S6.T2.19.19.19.2.m1.2.2.cmml" xref="S6.T2.19.19.19.2.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.19.19.19.2.m1.2c">2,000</annotation></semantics></math></td>
<td id="S6.T2.20.20.20.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T2.20.20.20.3.m1.2" class="ltx_Math" alttext="8,000" display="inline"><semantics id="S6.T2.20.20.20.3.m1.2a"><mrow id="S6.T2.20.20.20.3.m1.2.3.2" xref="S6.T2.20.20.20.3.m1.2.3.1.cmml"><mn id="S6.T2.20.20.20.3.m1.1.1" xref="S6.T2.20.20.20.3.m1.1.1.cmml">8</mn><mo id="S6.T2.20.20.20.3.m1.2.3.2.1" xref="S6.T2.20.20.20.3.m1.2.3.1.cmml">,</mo><mn id="S6.T2.20.20.20.3.m1.2.2" xref="S6.T2.20.20.20.3.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T2.20.20.20.3.m1.2b"><list id="S6.T2.20.20.20.3.m1.2.3.1.cmml" xref="S6.T2.20.20.20.3.m1.2.3.2"><cn type="integer" id="S6.T2.20.20.20.3.m1.1.1.cmml" xref="S6.T2.20.20.20.3.m1.1.1">8</cn><cn type="integer" id="S6.T2.20.20.20.3.m1.2.2.cmml" xref="S6.T2.20.20.20.3.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.20.20.20.3.m1.2c">8,000</annotation></semantics></math></td>
</tr>
<tr id="S6.T2.22.22.26.4" class="ltx_tr">
<td id="S6.T2.22.22.26.4.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;" colspan="12">
<span id="S6.T2.22.22.26.4.1.1" class="ltx_text ltx_font_bold">Video</span>-Based Datasets for Human Pose Estimation.</td>
</tr>
<tr id="S6.T2.22.22.22" class="ltx_tr">
<td id="S6.T2.22.22.22.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">Penn Action <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib195" title="" class="ltx_ref">2013</a>)</cite>
</td>
<td id="S6.T2.22.22.22.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">2013</td>
<td id="S6.T2.22.22.22.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">✓</td>
<td id="S6.T2.22.22.22.6" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.22.22.22.7" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.22.22.22.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">✓</td>
<td id="S6.T2.22.22.22.9" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.22.22.22.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">13</td>
<td id="S6.T2.22.22.22.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">mAP</td>
<td id="S6.T2.21.21.21.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T2.21.21.21.1.m1.2" class="ltx_Math" alttext="1,000" display="inline"><semantics id="S6.T2.21.21.21.1.m1.2a"><mrow id="S6.T2.21.21.21.1.m1.2.3.2" xref="S6.T2.21.21.21.1.m1.2.3.1.cmml"><mn id="S6.T2.21.21.21.1.m1.1.1" xref="S6.T2.21.21.21.1.m1.1.1.cmml">1</mn><mo id="S6.T2.21.21.21.1.m1.2.3.2.1" xref="S6.T2.21.21.21.1.m1.2.3.1.cmml">,</mo><mn id="S6.T2.21.21.21.1.m1.2.2" xref="S6.T2.21.21.21.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T2.21.21.21.1.m1.2b"><list id="S6.T2.21.21.21.1.m1.2.3.1.cmml" xref="S6.T2.21.21.21.1.m1.2.3.2"><cn type="integer" id="S6.T2.21.21.21.1.m1.1.1.cmml" xref="S6.T2.21.21.21.1.m1.1.1">1</cn><cn type="integer" id="S6.T2.21.21.21.1.m1.2.2.cmml" xref="S6.T2.21.21.21.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.21.21.21.1.m1.2c">1,000</annotation></semantics></math></td>
<td id="S6.T2.22.22.22.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T2.22.22.22.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T2.22.22.22.2.m1.2" class="ltx_Math" alttext="1,000" display="inline"><semantics id="S6.T2.22.22.22.2.m1.2a"><mrow id="S6.T2.22.22.22.2.m1.2.3.2" xref="S6.T2.22.22.22.2.m1.2.3.1.cmml"><mn id="S6.T2.22.22.22.2.m1.1.1" xref="S6.T2.22.22.22.2.m1.1.1.cmml">1</mn><mo id="S6.T2.22.22.22.2.m1.2.3.2.1" xref="S6.T2.22.22.22.2.m1.2.3.1.cmml">,</mo><mn id="S6.T2.22.22.22.2.m1.2.2" xref="S6.T2.22.22.22.2.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T2.22.22.22.2.m1.2b"><list id="S6.T2.22.22.22.2.m1.2.3.1.cmml" xref="S6.T2.22.22.22.2.m1.2.3.2"><cn type="integer" id="S6.T2.22.22.22.2.m1.1.1.cmml" xref="S6.T2.22.22.22.2.m1.1.1">1</cn><cn type="integer" id="S6.T2.22.22.22.2.m1.2.2.cmml" xref="S6.T2.22.22.22.2.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.22.22.22.2.m1.2c">1,000</annotation></semantics></math></td>
</tr>
<tr id="S6.T2.22.22.27.5" class="ltx_tr">
<td id="S6.T2.22.22.27.5.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">JHMDB <cite class="ltx_cite ltx_citemacro_cite">Jhuang et al. (<a href="#bib.bib65" title="" class="ltx_ref">2013</a>)</cite>
</td>
<td id="S6.T2.22.22.27.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">2013</td>
<td id="S6.T2.22.22.27.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">✓</td>
<td id="S6.T2.22.22.27.5.4" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.22.22.27.5.5" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.22.22.27.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">✓</td>
<td id="S6.T2.22.22.27.5.7" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.22.22.27.5.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">15</td>
<td id="S6.T2.22.22.27.5.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">mAP</td>
<td id="S6.T2.22.22.27.5.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">600</td>
<td id="S6.T2.22.22.27.5.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T2.22.22.27.5.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">300</td>
</tr>
<tr id="S6.T2.22.22.28.6" class="ltx_tr">
<td id="S6.T2.22.22.28.6.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">PoseTrack2017 <cite class="ltx_cite ltx_citemacro_cite">Iqbal et al. (<a href="#bib.bib63" title="" class="ltx_ref">2017</a>)</cite>
</td>
<td id="S6.T2.22.22.28.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">2017</td>
<td id="S6.T2.22.22.28.6.3" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.22.22.28.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">✓</td>
<td id="S6.T2.22.22.28.6.5" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.22.22.28.6.6" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.22.22.28.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">✓</td>
<td id="S6.T2.22.22.28.6.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">15</td>
<td id="S6.T2.22.22.28.6.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">mAP</td>
<td id="S6.T2.22.22.28.6.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">250</td>
<td id="S6.T2.22.22.28.6.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">50</td>
<td id="S6.T2.22.22.28.6.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">214</td>
</tr>
<tr id="S6.T2.22.22.29.7" class="ltx_tr">
<td id="S6.T2.22.22.29.7.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">PoseTrack2018 <cite class="ltx_cite ltx_citemacro_cite">Andriluka et al. (<a href="#bib.bib2" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="S6.T2.22.22.29.7.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">2018</td>
<td id="S6.T2.22.22.29.7.3" class="ltx_td ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.22.22.29.7.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">✓</td>
<td id="S6.T2.22.22.29.7.5" class="ltx_td ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.22.22.29.7.6" class="ltx_td ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.22.22.29.7.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">✓</td>
<td id="S6.T2.22.22.29.7.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">15</td>
<td id="S6.T2.22.22.29.7.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">mAP</td>
<td id="S6.T2.22.22.29.7.10" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">593</td>
<td id="S6.T2.22.22.29.7.11" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">170</td>
<td id="S6.T2.22.22.29.7.12" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">375</td>
</tr>
<tr id="S6.T2.22.22.30.8" class="ltx_tr">
<td id="S6.T2.22.22.30.8.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">HiEve <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib94" title="" class="ltx_ref">2020</a>)</cite>
</td>
<td id="S6.T2.22.22.30.8.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">2020</td>
<td id="S6.T2.22.22.30.8.3" class="ltx_td ltx_border_b ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.22.22.30.8.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">✓</td>
<td id="S6.T2.22.22.30.8.5" class="ltx_td ltx_border_b ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.22.22.30.8.6" class="ltx_td ltx_border_b ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T2.22.22.30.8.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">✓</td>
<td id="S6.T2.22.22.30.8.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">14</td>
<td id="S6.T2.22.22.30.8.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">mAP</td>
<td id="S6.T2.22.22.30.8.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">19</td>
<td id="S6.T2.22.22.30.8.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T2.22.22.30.8.12" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">13</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S6.F5" class="ltx_figure"><img src="/html/2204.07370/assets/Figures/Fig5-label.jpg" id="S6.F5.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="342" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Illustration of pose annotations for different benchmark datasets including LSP, FLIC, MPII, COCO, AIC-HKD, CrowdedPose, Penn Action, J-HMDB, HiEve, and PoseTrack.</figcaption>
</figure>
<figure id="S6.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Performance comparisons of state-of-the-art methods including <em id="S6.T3.35.1" class="ltx_emph ltx_font_italic">top-down</em> approaches, <em id="S6.T3.36.2" class="ltx_emph ltx_font_italic">bottom-up</em> approaches and <em id="S6.T3.37.3" class="ltx_emph ltx_font_italic">small networks</em> on <span id="S6.T3.38.4" class="ltx_text ltx_font_bold">COCO</span> benchmark dataset (test-dev2017).</figcaption>
<div id="S6.T3.30" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:296.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-217.3pt,148.7pt) scale(0.499422068528947,0.499422068528947) ;">
<table id="S6.T3.30.30" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T3.6.6.6" class="ltx_tr">
<td id="S6.T3.6.6.6.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">Method</td>
<td id="S6.T3.6.6.6.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">Backbone</td>
<td id="S6.T3.6.6.6.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">Input size</td>
<td id="S6.T3.6.6.6.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">Parameters</td>
<td id="S6.T3.6.6.6.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">GFLOPs</td>
<td id="S6.T3.6.6.6.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">AP</td>
<td id="S6.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">AP<sup id="S6.T3.1.1.1.1.1" class="ltx_sup"><span id="S6.T3.1.1.1.1.1.1" class="ltx_text ltx_font_italic">50</span></sup>
</td>
<td id="S6.T3.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">AP<sup id="S6.T3.2.2.2.2.1" class="ltx_sup"><span id="S6.T3.2.2.2.2.1.1" class="ltx_text ltx_font_italic">75</span></sup>
</td>
<td id="S6.T3.3.3.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">AP<sup id="S6.T3.3.3.3.3.1" class="ltx_sup"><span id="S6.T3.3.3.3.3.1.1" class="ltx_text ltx_font_italic">M</span></sup>
</td>
<td id="S6.T3.4.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">AP<sup id="S6.T3.4.4.4.4.1" class="ltx_sup"><span id="S6.T3.4.4.4.4.1.1" class="ltx_text ltx_font_italic">L</span></sup>
</td>
<td id="S6.T3.6.6.6.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">AR</td>
<td id="S6.T3.5.5.5.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">AR<sup id="S6.T3.5.5.5.5.1" class="ltx_sup"><span id="S6.T3.5.5.5.5.1.1" class="ltx_text ltx_font_italic">M</span></sup>
</td>
<td id="S6.T3.6.6.6.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">AR<sup id="S6.T3.6.6.6.6.1" class="ltx_sup"><span id="S6.T3.6.6.6.6.1.1" class="ltx_text ltx_font_italic">L</span></sup>
</td>
</tr>
<tr id="S6.T3.30.30.31.1" class="ltx_tr">
<td id="S6.T3.30.30.31.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;" colspan="11">Top-down framework: human detection and individual keypoint detection.</td>
<td id="S6.T3.30.30.31.1.2" class="ltx_td ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T3.30.30.31.1.3" class="ltx_td ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
</tr>
<tr id="S6.T3.30.30.32.2" class="ltx_tr">
<td id="S6.T3.30.30.32.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">Mask-RCNN <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib54" title="" class="ltx_ref">2017b</a>)</cite>
</td>
<td id="S6.T3.30.30.32.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">ResNet-50</td>
<td id="S6.T3.30.30.32.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.30.30.32.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.30.30.32.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.30.30.32.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">63.1</td>
<td id="S6.T3.30.30.32.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">87.3</td>
<td id="S6.T3.30.30.32.2.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">68.7</td>
<td id="S6.T3.30.30.32.2.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">57.8</td>
<td id="S6.T3.30.30.32.2.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">71.4</td>
<td id="S6.T3.30.30.32.2.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.30.30.32.2.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.30.30.32.2.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
</tr>
<tr id="S6.T3.7.7.7" class="ltx_tr">
<td id="S6.T3.7.7.7.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">G-RMI <cite class="ltx_cite ltx_citemacro_cite">Papandreou et al. (<a href="#bib.bib128" title="" class="ltx_ref">2017</a>)</cite>
</td>
<td id="S6.T3.7.7.7.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">ResNet-101</td>
<td id="S6.T3.7.7.7.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T3.7.7.7.1.m1.1" class="ltx_Math" alttext="353\times 257" display="inline"><semantics id="S6.T3.7.7.7.1.m1.1a"><mrow id="S6.T3.7.7.7.1.m1.1.1" xref="S6.T3.7.7.7.1.m1.1.1.cmml"><mn id="S6.T3.7.7.7.1.m1.1.1.2" xref="S6.T3.7.7.7.1.m1.1.1.2.cmml">353</mn><mo lspace="0.222em" rspace="0.222em" id="S6.T3.7.7.7.1.m1.1.1.1" xref="S6.T3.7.7.7.1.m1.1.1.1.cmml">×</mo><mn id="S6.T3.7.7.7.1.m1.1.1.3" xref="S6.T3.7.7.7.1.m1.1.1.3.cmml">257</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T3.7.7.7.1.m1.1b"><apply id="S6.T3.7.7.7.1.m1.1.1.cmml" xref="S6.T3.7.7.7.1.m1.1.1"><times id="S6.T3.7.7.7.1.m1.1.1.1.cmml" xref="S6.T3.7.7.7.1.m1.1.1.1"></times><cn type="integer" id="S6.T3.7.7.7.1.m1.1.1.2.cmml" xref="S6.T3.7.7.7.1.m1.1.1.2">353</cn><cn type="integer" id="S6.T3.7.7.7.1.m1.1.1.3.cmml" xref="S6.T3.7.7.7.1.m1.1.1.3">257</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.7.7.7.1.m1.1c">353\times 257</annotation></semantics></math></td>
<td id="S6.T3.7.7.7.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">42.6M</td>
<td id="S6.T3.7.7.7.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">57.0</td>
<td id="S6.T3.7.7.7.6" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">64.9</td>
<td id="S6.T3.7.7.7.7" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">85.5</td>
<td id="S6.T3.7.7.7.8" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">71.3</td>
<td id="S6.T3.7.7.7.9" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">62.3</td>
<td id="S6.T3.7.7.7.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">70.0</td>
<td id="S6.T3.7.7.7.11" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">69.7</td>
<td id="S6.T3.7.7.7.12" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.7.7.7.13" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
</tr>
<tr id="S6.T3.8.8.8" class="ltx_tr">
<td id="S6.T3.8.8.8.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">Integral Pose <cite class="ltx_cite ltx_citemacro_cite">Sun et al. (<a href="#bib.bib155" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="S6.T3.8.8.8.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">ResNet-101</td>
<td id="S6.T3.8.8.8.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T3.8.8.8.1.m1.1" class="ltx_Math" alttext="256\times 256" display="inline"><semantics id="S6.T3.8.8.8.1.m1.1a"><mrow id="S6.T3.8.8.8.1.m1.1.1" xref="S6.T3.8.8.8.1.m1.1.1.cmml"><mn id="S6.T3.8.8.8.1.m1.1.1.2" xref="S6.T3.8.8.8.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S6.T3.8.8.8.1.m1.1.1.1" xref="S6.T3.8.8.8.1.m1.1.1.1.cmml">×</mo><mn id="S6.T3.8.8.8.1.m1.1.1.3" xref="S6.T3.8.8.8.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T3.8.8.8.1.m1.1b"><apply id="S6.T3.8.8.8.1.m1.1.1.cmml" xref="S6.T3.8.8.8.1.m1.1.1"><times id="S6.T3.8.8.8.1.m1.1.1.1.cmml" xref="S6.T3.8.8.8.1.m1.1.1.1"></times><cn type="integer" id="S6.T3.8.8.8.1.m1.1.1.2.cmml" xref="S6.T3.8.8.8.1.m1.1.1.2">256</cn><cn type="integer" id="S6.T3.8.8.8.1.m1.1.1.3.cmml" xref="S6.T3.8.8.8.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.8.8.8.1.m1.1c">256\times 256</annotation></semantics></math></td>
<td id="S6.T3.8.8.8.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">45.0M</td>
<td id="S6.T3.8.8.8.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">11.0</td>
<td id="S6.T3.8.8.8.6" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">67.8</td>
<td id="S6.T3.8.8.8.7" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">88.2</td>
<td id="S6.T3.8.8.8.8" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">74.8</td>
<td id="S6.T3.8.8.8.9" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">63.9</td>
<td id="S6.T3.8.8.8.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">74.0</td>
<td id="S6.T3.8.8.8.11" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.8.8.8.12" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.8.8.8.13" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
</tr>
<tr id="S6.T3.10.10.10" class="ltx_tr">
<td id="S6.T3.9.9.9.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">G-RMI<math id="S6.T3.9.9.9.1.m1.1" class="ltx_Math" alttext="+" display="inline"><semantics id="S6.T3.9.9.9.1.m1.1a"><mo id="S6.T3.9.9.9.1.m1.1.1" xref="S6.T3.9.9.9.1.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S6.T3.9.9.9.1.m1.1b"><plus id="S6.T3.9.9.9.1.m1.1.1.cmml" xref="S6.T3.9.9.9.1.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.9.9.9.1.m1.1c">+</annotation></semantics></math>extra data <cite class="ltx_cite ltx_citemacro_cite">Papandreou et al. (<a href="#bib.bib128" title="" class="ltx_ref">2017</a>)</cite>
</td>
<td id="S6.T3.10.10.10.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">ResNet-101</td>
<td id="S6.T3.10.10.10.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T3.10.10.10.2.m1.1" class="ltx_Math" alttext="353\times 257" display="inline"><semantics id="S6.T3.10.10.10.2.m1.1a"><mrow id="S6.T3.10.10.10.2.m1.1.1" xref="S6.T3.10.10.10.2.m1.1.1.cmml"><mn id="S6.T3.10.10.10.2.m1.1.1.2" xref="S6.T3.10.10.10.2.m1.1.1.2.cmml">353</mn><mo lspace="0.222em" rspace="0.222em" id="S6.T3.10.10.10.2.m1.1.1.1" xref="S6.T3.10.10.10.2.m1.1.1.1.cmml">×</mo><mn id="S6.T3.10.10.10.2.m1.1.1.3" xref="S6.T3.10.10.10.2.m1.1.1.3.cmml">257</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T3.10.10.10.2.m1.1b"><apply id="S6.T3.10.10.10.2.m1.1.1.cmml" xref="S6.T3.10.10.10.2.m1.1.1"><times id="S6.T3.10.10.10.2.m1.1.1.1.cmml" xref="S6.T3.10.10.10.2.m1.1.1.1"></times><cn type="integer" id="S6.T3.10.10.10.2.m1.1.1.2.cmml" xref="S6.T3.10.10.10.2.m1.1.1.2">353</cn><cn type="integer" id="S6.T3.10.10.10.2.m1.1.1.3.cmml" xref="S6.T3.10.10.10.2.m1.1.1.3">257</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.10.10.10.2.m1.1c">353\times 257</annotation></semantics></math></td>
<td id="S6.T3.10.10.10.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">42.6M</td>
<td id="S6.T3.10.10.10.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">57.0</td>
<td id="S6.T3.10.10.10.6" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">68.5</td>
<td id="S6.T3.10.10.10.7" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">87.1</td>
<td id="S6.T3.10.10.10.8" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">75.5</td>
<td id="S6.T3.10.10.10.9" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">65.8</td>
<td id="S6.T3.10.10.10.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">73.3</td>
<td id="S6.T3.10.10.10.11" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">73.3</td>
<td id="S6.T3.10.10.10.12" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.10.10.10.13" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
</tr>
<tr id="S6.T3.11.11.11" class="ltx_tr">
<td id="S6.T3.11.11.11.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">CPN <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib17" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="S6.T3.11.11.11.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">Resnet-Inception</td>
<td id="S6.T3.11.11.11.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T3.11.11.11.1.m1.1" class="ltx_Math" alttext="384\times 288" display="inline"><semantics id="S6.T3.11.11.11.1.m1.1a"><mrow id="S6.T3.11.11.11.1.m1.1.1" xref="S6.T3.11.11.11.1.m1.1.1.cmml"><mn id="S6.T3.11.11.11.1.m1.1.1.2" xref="S6.T3.11.11.11.1.m1.1.1.2.cmml">384</mn><mo lspace="0.222em" rspace="0.222em" id="S6.T3.11.11.11.1.m1.1.1.1" xref="S6.T3.11.11.11.1.m1.1.1.1.cmml">×</mo><mn id="S6.T3.11.11.11.1.m1.1.1.3" xref="S6.T3.11.11.11.1.m1.1.1.3.cmml">288</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T3.11.11.11.1.m1.1b"><apply id="S6.T3.11.11.11.1.m1.1.1.cmml" xref="S6.T3.11.11.11.1.m1.1.1"><times id="S6.T3.11.11.11.1.m1.1.1.1.cmml" xref="S6.T3.11.11.11.1.m1.1.1.1"></times><cn type="integer" id="S6.T3.11.11.11.1.m1.1.1.2.cmml" xref="S6.T3.11.11.11.1.m1.1.1.2">384</cn><cn type="integer" id="S6.T3.11.11.11.1.m1.1.1.3.cmml" xref="S6.T3.11.11.11.1.m1.1.1.3">288</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.11.11.11.1.m1.1c">384\times 288</annotation></semantics></math></td>
<td id="S6.T3.11.11.11.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.11.11.11.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.11.11.11.6" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">72.1</td>
<td id="S6.T3.11.11.11.7" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">91.4</td>
<td id="S6.T3.11.11.11.8" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">80.0</td>
<td id="S6.T3.11.11.11.9" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">68.7</td>
<td id="S6.T3.11.11.11.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">77.2</td>
<td id="S6.T3.11.11.11.11" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">78.5</td>
<td id="S6.T3.11.11.11.12" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.11.11.11.13" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
</tr>
<tr id="S6.T3.12.12.12" class="ltx_tr">
<td id="S6.T3.12.12.12.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">RMPE <cite class="ltx_cite ltx_citemacro_cite">Fang et al. (<a href="#bib.bib34" title="" class="ltx_ref">2017</a>)</cite>
</td>
<td id="S6.T3.12.12.12.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">Stacked Hourglass</td>
<td id="S6.T3.12.12.12.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T3.12.12.12.1.m1.1" class="ltx_Math" alttext="320\times 256" display="inline"><semantics id="S6.T3.12.12.12.1.m1.1a"><mrow id="S6.T3.12.12.12.1.m1.1.1" xref="S6.T3.12.12.12.1.m1.1.1.cmml"><mn id="S6.T3.12.12.12.1.m1.1.1.2" xref="S6.T3.12.12.12.1.m1.1.1.2.cmml">320</mn><mo lspace="0.222em" rspace="0.222em" id="S6.T3.12.12.12.1.m1.1.1.1" xref="S6.T3.12.12.12.1.m1.1.1.1.cmml">×</mo><mn id="S6.T3.12.12.12.1.m1.1.1.3" xref="S6.T3.12.12.12.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T3.12.12.12.1.m1.1b"><apply id="S6.T3.12.12.12.1.m1.1.1.cmml" xref="S6.T3.12.12.12.1.m1.1.1"><times id="S6.T3.12.12.12.1.m1.1.1.1.cmml" xref="S6.T3.12.12.12.1.m1.1.1.1"></times><cn type="integer" id="S6.T3.12.12.12.1.m1.1.1.2.cmml" xref="S6.T3.12.12.12.1.m1.1.1.2">320</cn><cn type="integer" id="S6.T3.12.12.12.1.m1.1.1.3.cmml" xref="S6.T3.12.12.12.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.12.12.12.1.m1.1c">320\times 256</annotation></semantics></math></td>
<td id="S6.T3.12.12.12.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">28.1M</td>
<td id="S6.T3.12.12.12.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">26.7</td>
<td id="S6.T3.12.12.12.6" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">72.3</td>
<td id="S6.T3.12.12.12.7" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">89.2</td>
<td id="S6.T3.12.12.12.8" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">79.1</td>
<td id="S6.T3.12.12.12.9" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">68.0</td>
<td id="S6.T3.12.12.12.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">78.6</td>
<td id="S6.T3.12.12.12.11" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.12.12.12.12" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.12.12.12.13" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
</tr>
<tr id="S6.T3.30.30.33.3" class="ltx_tr">
<td id="S6.T3.30.30.33.3.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">CFN <cite class="ltx_cite ltx_citemacro_cite">Huang et al. (<a href="#bib.bib60" title="" class="ltx_ref">2017</a>)</cite>
</td>
<td id="S6.T3.30.30.33.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.30.30.33.3.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.30.30.33.3.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.30.30.33.3.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.30.30.33.3.6" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">72.6</td>
<td id="S6.T3.30.30.33.3.7" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">86.1</td>
<td id="S6.T3.30.30.33.3.8" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">69.7</td>
<td id="S6.T3.30.30.33.3.9" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">78.3</td>
<td id="S6.T3.30.30.33.3.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">64.1</td>
<td id="S6.T3.30.30.33.3.11" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.30.30.33.3.12" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.30.30.33.3.13" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
</tr>
<tr id="S6.T3.13.13.13" class="ltx_tr">
<td id="S6.T3.13.13.13.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">CPN (ensemble) <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib17" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="S6.T3.13.13.13.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">Resnet-Inception</td>
<td id="S6.T3.13.13.13.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T3.13.13.13.1.m1.1" class="ltx_Math" alttext="384\times 288" display="inline"><semantics id="S6.T3.13.13.13.1.m1.1a"><mrow id="S6.T3.13.13.13.1.m1.1.1" xref="S6.T3.13.13.13.1.m1.1.1.cmml"><mn id="S6.T3.13.13.13.1.m1.1.1.2" xref="S6.T3.13.13.13.1.m1.1.1.2.cmml">384</mn><mo lspace="0.222em" rspace="0.222em" id="S6.T3.13.13.13.1.m1.1.1.1" xref="S6.T3.13.13.13.1.m1.1.1.1.cmml">×</mo><mn id="S6.T3.13.13.13.1.m1.1.1.3" xref="S6.T3.13.13.13.1.m1.1.1.3.cmml">288</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T3.13.13.13.1.m1.1b"><apply id="S6.T3.13.13.13.1.m1.1.1.cmml" xref="S6.T3.13.13.13.1.m1.1.1"><times id="S6.T3.13.13.13.1.m1.1.1.1.cmml" xref="S6.T3.13.13.13.1.m1.1.1.1"></times><cn type="integer" id="S6.T3.13.13.13.1.m1.1.1.2.cmml" xref="S6.T3.13.13.13.1.m1.1.1.2">384</cn><cn type="integer" id="S6.T3.13.13.13.1.m1.1.1.3.cmml" xref="S6.T3.13.13.13.1.m1.1.1.3">288</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.13.13.13.1.m1.1c">384\times 288</annotation></semantics></math></td>
<td id="S6.T3.13.13.13.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.13.13.13.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.13.13.13.6" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">73.0</td>
<td id="S6.T3.13.13.13.7" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">91.7</td>
<td id="S6.T3.13.13.13.8" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">80.9</td>
<td id="S6.T3.13.13.13.9" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">69.5</td>
<td id="S6.T3.13.13.13.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">78.1</td>
<td id="S6.T3.13.13.13.11" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">79.0</td>
<td id="S6.T3.13.13.13.12" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.13.13.13.13" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
</tr>
<tr id="S6.T3.14.14.14" class="ltx_tr">
<td id="S6.T3.14.14.14.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">SimpleBaseline <cite class="ltx_cite ltx_citemacro_cite">Xiao et al. (<a href="#bib.bib181" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="S6.T3.14.14.14.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">ResNet-152</td>
<td id="S6.T3.14.14.14.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T3.14.14.14.1.m1.1" class="ltx_Math" alttext="384\times 288" display="inline"><semantics id="S6.T3.14.14.14.1.m1.1a"><mrow id="S6.T3.14.14.14.1.m1.1.1" xref="S6.T3.14.14.14.1.m1.1.1.cmml"><mn id="S6.T3.14.14.14.1.m1.1.1.2" xref="S6.T3.14.14.14.1.m1.1.1.2.cmml">384</mn><mo lspace="0.222em" rspace="0.222em" id="S6.T3.14.14.14.1.m1.1.1.1" xref="S6.T3.14.14.14.1.m1.1.1.1.cmml">×</mo><mn id="S6.T3.14.14.14.1.m1.1.1.3" xref="S6.T3.14.14.14.1.m1.1.1.3.cmml">288</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T3.14.14.14.1.m1.1b"><apply id="S6.T3.14.14.14.1.m1.1.1.cmml" xref="S6.T3.14.14.14.1.m1.1.1"><times id="S6.T3.14.14.14.1.m1.1.1.1.cmml" xref="S6.T3.14.14.14.1.m1.1.1.1"></times><cn type="integer" id="S6.T3.14.14.14.1.m1.1.1.2.cmml" xref="S6.T3.14.14.14.1.m1.1.1.2">384</cn><cn type="integer" id="S6.T3.14.14.14.1.m1.1.1.3.cmml" xref="S6.T3.14.14.14.1.m1.1.1.3">288</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.14.14.14.1.m1.1c">384\times 288</annotation></semantics></math></td>
<td id="S6.T3.14.14.14.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">68.6M</td>
<td id="S6.T3.14.14.14.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">35.6</td>
<td id="S6.T3.14.14.14.6" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">73.7</td>
<td id="S6.T3.14.14.14.7" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">91.9</td>
<td id="S6.T3.14.14.14.8" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">81.1</td>
<td id="S6.T3.14.14.14.9" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">70.3</td>
<td id="S6.T3.14.14.14.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">80.0</td>
<td id="S6.T3.14.14.14.11" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">79.0</td>
<td id="S6.T3.14.14.14.12" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.14.14.14.13" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
</tr>
<tr id="S6.T3.15.15.15" class="ltx_tr">
<td id="S6.T3.15.15.15.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">HRNet-W32 <cite class="ltx_cite ltx_citemacro_cite">Sun et al. (<a href="#bib.bib153" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="S6.T3.15.15.15.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">HRNet-W32</td>
<td id="S6.T3.15.15.15.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T3.15.15.15.1.m1.1" class="ltx_Math" alttext="384\times 288" display="inline"><semantics id="S6.T3.15.15.15.1.m1.1a"><mrow id="S6.T3.15.15.15.1.m1.1.1" xref="S6.T3.15.15.15.1.m1.1.1.cmml"><mn id="S6.T3.15.15.15.1.m1.1.1.2" xref="S6.T3.15.15.15.1.m1.1.1.2.cmml">384</mn><mo lspace="0.222em" rspace="0.222em" id="S6.T3.15.15.15.1.m1.1.1.1" xref="S6.T3.15.15.15.1.m1.1.1.1.cmml">×</mo><mn id="S6.T3.15.15.15.1.m1.1.1.3" xref="S6.T3.15.15.15.1.m1.1.1.3.cmml">288</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T3.15.15.15.1.m1.1b"><apply id="S6.T3.15.15.15.1.m1.1.1.cmml" xref="S6.T3.15.15.15.1.m1.1.1"><times id="S6.T3.15.15.15.1.m1.1.1.1.cmml" xref="S6.T3.15.15.15.1.m1.1.1.1"></times><cn type="integer" id="S6.T3.15.15.15.1.m1.1.1.2.cmml" xref="S6.T3.15.15.15.1.m1.1.1.2">384</cn><cn type="integer" id="S6.T3.15.15.15.1.m1.1.1.3.cmml" xref="S6.T3.15.15.15.1.m1.1.1.3">288</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.15.15.15.1.m1.1c">384\times 288</annotation></semantics></math></td>
<td id="S6.T3.15.15.15.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">28.5M</td>
<td id="S6.T3.15.15.15.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">16.0</td>
<td id="S6.T3.15.15.15.6" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">74.9</td>
<td id="S6.T3.15.15.15.7" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">92.5</td>
<td id="S6.T3.15.15.15.8" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">82.8</td>
<td id="S6.T3.15.15.15.9" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">71.3</td>
<td id="S6.T3.15.15.15.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">80.9</td>
<td id="S6.T3.15.15.15.11" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">80.1</td>
<td id="S6.T3.15.15.15.12" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.15.15.15.13" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
</tr>
<tr id="S6.T3.16.16.16" class="ltx_tr">
<td id="S6.T3.16.16.16.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">HRNet-W48 <cite class="ltx_cite ltx_citemacro_cite">Sun et al. (<a href="#bib.bib153" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="S6.T3.16.16.16.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">HRNet-W48</td>
<td id="S6.T3.16.16.16.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T3.16.16.16.1.m1.1" class="ltx_Math" alttext="384\times 288" display="inline"><semantics id="S6.T3.16.16.16.1.m1.1a"><mrow id="S6.T3.16.16.16.1.m1.1.1" xref="S6.T3.16.16.16.1.m1.1.1.cmml"><mn id="S6.T3.16.16.16.1.m1.1.1.2" xref="S6.T3.16.16.16.1.m1.1.1.2.cmml">384</mn><mo lspace="0.222em" rspace="0.222em" id="S6.T3.16.16.16.1.m1.1.1.1" xref="S6.T3.16.16.16.1.m1.1.1.1.cmml">×</mo><mn id="S6.T3.16.16.16.1.m1.1.1.3" xref="S6.T3.16.16.16.1.m1.1.1.3.cmml">288</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T3.16.16.16.1.m1.1b"><apply id="S6.T3.16.16.16.1.m1.1.1.cmml" xref="S6.T3.16.16.16.1.m1.1.1"><times id="S6.T3.16.16.16.1.m1.1.1.1.cmml" xref="S6.T3.16.16.16.1.m1.1.1.1"></times><cn type="integer" id="S6.T3.16.16.16.1.m1.1.1.2.cmml" xref="S6.T3.16.16.16.1.m1.1.1.2">384</cn><cn type="integer" id="S6.T3.16.16.16.1.m1.1.1.3.cmml" xref="S6.T3.16.16.16.1.m1.1.1.3">288</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.16.16.16.1.m1.1c">384\times 288</annotation></semantics></math></td>
<td id="S6.T3.16.16.16.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">63.6M</td>
<td id="S6.T3.16.16.16.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">32.9</td>
<td id="S6.T3.16.16.16.6" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">75.5</td>
<td id="S6.T3.16.16.16.7" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">92.5</td>
<td id="S6.T3.16.16.16.8" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">83.3</td>
<td id="S6.T3.16.16.16.9" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">71.9</td>
<td id="S6.T3.16.16.16.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">81.5</td>
<td id="S6.T3.16.16.16.11" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">80.5</td>
<td id="S6.T3.16.16.16.12" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.16.16.16.13" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
</tr>
<tr id="S6.T3.17.17.17" class="ltx_tr">
<td id="S6.T3.17.17.17.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">DARK <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib193" title="" class="ltx_ref">2020a</a>)</cite>
</td>
<td id="S6.T3.17.17.17.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">HRNet-W48</td>
<td id="S6.T3.17.17.17.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T3.17.17.17.1.m1.1" class="ltx_Math" alttext="384\times 288" display="inline"><semantics id="S6.T3.17.17.17.1.m1.1a"><mrow id="S6.T3.17.17.17.1.m1.1.1" xref="S6.T3.17.17.17.1.m1.1.1.cmml"><mn id="S6.T3.17.17.17.1.m1.1.1.2" xref="S6.T3.17.17.17.1.m1.1.1.2.cmml">384</mn><mo lspace="0.222em" rspace="0.222em" id="S6.T3.17.17.17.1.m1.1.1.1" xref="S6.T3.17.17.17.1.m1.1.1.1.cmml">×</mo><mn id="S6.T3.17.17.17.1.m1.1.1.3" xref="S6.T3.17.17.17.1.m1.1.1.3.cmml">288</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T3.17.17.17.1.m1.1b"><apply id="S6.T3.17.17.17.1.m1.1.1.cmml" xref="S6.T3.17.17.17.1.m1.1.1"><times id="S6.T3.17.17.17.1.m1.1.1.1.cmml" xref="S6.T3.17.17.17.1.m1.1.1.1"></times><cn type="integer" id="S6.T3.17.17.17.1.m1.1.1.2.cmml" xref="S6.T3.17.17.17.1.m1.1.1.2">384</cn><cn type="integer" id="S6.T3.17.17.17.1.m1.1.1.3.cmml" xref="S6.T3.17.17.17.1.m1.1.1.3">288</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.17.17.17.1.m1.1c">384\times 288</annotation></semantics></math></td>
<td id="S6.T3.17.17.17.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">63.6M</td>
<td id="S6.T3.17.17.17.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">32.9</td>
<td id="S6.T3.17.17.17.6" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">76.2</td>
<td id="S6.T3.17.17.17.7" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">92.5</td>
<td id="S6.T3.17.17.17.8" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">83.6</td>
<td id="S6.T3.17.17.17.9" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">72.5</td>
<td id="S6.T3.17.17.17.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">82.4</td>
<td id="S6.T3.17.17.17.11" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">81.1</td>
<td id="S6.T3.17.17.17.12" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.17.17.17.13" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
</tr>
<tr id="S6.T3.18.18.18" class="ltx_tr">
<td id="S6.T3.18.18.18.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">UDP <cite class="ltx_cite ltx_citemacro_cite">Huang et al. (<a href="#bib.bib58" title="" class="ltx_ref">2020a</a>)</cite>
</td>
<td id="S6.T3.18.18.18.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">HRNet-W48</td>
<td id="S6.T3.18.18.18.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T3.18.18.18.1.m1.1" class="ltx_Math" alttext="384\times 288" display="inline"><semantics id="S6.T3.18.18.18.1.m1.1a"><mrow id="S6.T3.18.18.18.1.m1.1.1" xref="S6.T3.18.18.18.1.m1.1.1.cmml"><mn id="S6.T3.18.18.18.1.m1.1.1.2" xref="S6.T3.18.18.18.1.m1.1.1.2.cmml">384</mn><mo lspace="0.222em" rspace="0.222em" id="S6.T3.18.18.18.1.m1.1.1.1" xref="S6.T3.18.18.18.1.m1.1.1.1.cmml">×</mo><mn id="S6.T3.18.18.18.1.m1.1.1.3" xref="S6.T3.18.18.18.1.m1.1.1.3.cmml">288</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T3.18.18.18.1.m1.1b"><apply id="S6.T3.18.18.18.1.m1.1.1.cmml" xref="S6.T3.18.18.18.1.m1.1.1"><times id="S6.T3.18.18.18.1.m1.1.1.1.cmml" xref="S6.T3.18.18.18.1.m1.1.1.1"></times><cn type="integer" id="S6.T3.18.18.18.1.m1.1.1.2.cmml" xref="S6.T3.18.18.18.1.m1.1.1.2">384</cn><cn type="integer" id="S6.T3.18.18.18.1.m1.1.1.3.cmml" xref="S6.T3.18.18.18.1.m1.1.1.3">288</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.18.18.18.1.m1.1c">384\times 288</annotation></semantics></math></td>
<td id="S6.T3.18.18.18.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">63.6M</td>
<td id="S6.T3.18.18.18.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">33.0</td>
<td id="S6.T3.18.18.18.6" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;"><span id="S6.T3.18.18.18.6.1" class="ltx_text ltx_font_bold">76.5</span></td>
<td id="S6.T3.18.18.18.7" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">92.7</td>
<td id="S6.T3.18.18.18.8" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">84.0</td>
<td id="S6.T3.18.18.18.9" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">73.0</td>
<td id="S6.T3.18.18.18.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">82.4</td>
<td id="S6.T3.18.18.18.11" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">81.6</td>
<td id="S6.T3.18.18.18.12" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.18.18.18.13" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
</tr>
<tr id="S6.T3.20.20.20" class="ltx_tr">
<td id="S6.T3.19.19.19.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">HRNet-W48<math id="S6.T3.19.19.19.1.m1.1" class="ltx_Math" alttext="+" display="inline"><semantics id="S6.T3.19.19.19.1.m1.1a"><mo id="S6.T3.19.19.19.1.m1.1.1" xref="S6.T3.19.19.19.1.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S6.T3.19.19.19.1.m1.1b"><plus id="S6.T3.19.19.19.1.m1.1.1.cmml" xref="S6.T3.19.19.19.1.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.19.19.19.1.m1.1c">+</annotation></semantics></math>extra data <cite class="ltx_cite ltx_citemacro_cite">Sun et al. (<a href="#bib.bib153" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="S6.T3.20.20.20.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">HRNet-W48</td>
<td id="S6.T3.20.20.20.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T3.20.20.20.2.m1.1" class="ltx_Math" alttext="384\times 288" display="inline"><semantics id="S6.T3.20.20.20.2.m1.1a"><mrow id="S6.T3.20.20.20.2.m1.1.1" xref="S6.T3.20.20.20.2.m1.1.1.cmml"><mn id="S6.T3.20.20.20.2.m1.1.1.2" xref="S6.T3.20.20.20.2.m1.1.1.2.cmml">384</mn><mo lspace="0.222em" rspace="0.222em" id="S6.T3.20.20.20.2.m1.1.1.1" xref="S6.T3.20.20.20.2.m1.1.1.1.cmml">×</mo><mn id="S6.T3.20.20.20.2.m1.1.1.3" xref="S6.T3.20.20.20.2.m1.1.1.3.cmml">288</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T3.20.20.20.2.m1.1b"><apply id="S6.T3.20.20.20.2.m1.1.1.cmml" xref="S6.T3.20.20.20.2.m1.1.1"><times id="S6.T3.20.20.20.2.m1.1.1.1.cmml" xref="S6.T3.20.20.20.2.m1.1.1.1"></times><cn type="integer" id="S6.T3.20.20.20.2.m1.1.1.2.cmml" xref="S6.T3.20.20.20.2.m1.1.1.2">384</cn><cn type="integer" id="S6.T3.20.20.20.2.m1.1.1.3.cmml" xref="S6.T3.20.20.20.2.m1.1.1.3">288</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.20.20.20.2.m1.1c">384\times 288</annotation></semantics></math></td>
<td id="S6.T3.20.20.20.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">63.6M</td>
<td id="S6.T3.20.20.20.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">32.9</td>
<td id="S6.T3.20.20.20.6" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">77.0</td>
<td id="S6.T3.20.20.20.7" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">92.7</td>
<td id="S6.T3.20.20.20.8" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">84.5</td>
<td id="S6.T3.20.20.20.9" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">73.4</td>
<td id="S6.T3.20.20.20.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">83.1</td>
<td id="S6.T3.20.20.20.11" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">82.0</td>
<td id="S6.T3.20.20.20.12" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.20.20.20.13" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
</tr>
<tr id="S6.T3.22.22.22" class="ltx_tr">
<td id="S6.T3.21.21.21.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">DARK<math id="S6.T3.21.21.21.1.m1.1" class="ltx_Math" alttext="+" display="inline"><semantics id="S6.T3.21.21.21.1.m1.1a"><mo id="S6.T3.21.21.21.1.m1.1.1" xref="S6.T3.21.21.21.1.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S6.T3.21.21.21.1.m1.1b"><plus id="S6.T3.21.21.21.1.m1.1.1.cmml" xref="S6.T3.21.21.21.1.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.21.21.21.1.m1.1c">+</annotation></semantics></math>extra data <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib193" title="" class="ltx_ref">2020a</a>)</cite>
</td>
<td id="S6.T3.22.22.22.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">HRNet-W48</td>
<td id="S6.T3.22.22.22.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T3.22.22.22.2.m1.1" class="ltx_Math" alttext="384\times 288" display="inline"><semantics id="S6.T3.22.22.22.2.m1.1a"><mrow id="S6.T3.22.22.22.2.m1.1.1" xref="S6.T3.22.22.22.2.m1.1.1.cmml"><mn id="S6.T3.22.22.22.2.m1.1.1.2" xref="S6.T3.22.22.22.2.m1.1.1.2.cmml">384</mn><mo lspace="0.222em" rspace="0.222em" id="S6.T3.22.22.22.2.m1.1.1.1" xref="S6.T3.22.22.22.2.m1.1.1.1.cmml">×</mo><mn id="S6.T3.22.22.22.2.m1.1.1.3" xref="S6.T3.22.22.22.2.m1.1.1.3.cmml">288</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T3.22.22.22.2.m1.1b"><apply id="S6.T3.22.22.22.2.m1.1.1.cmml" xref="S6.T3.22.22.22.2.m1.1.1"><times id="S6.T3.22.22.22.2.m1.1.1.1.cmml" xref="S6.T3.22.22.22.2.m1.1.1.1"></times><cn type="integer" id="S6.T3.22.22.22.2.m1.1.1.2.cmml" xref="S6.T3.22.22.22.2.m1.1.1.2">384</cn><cn type="integer" id="S6.T3.22.22.22.2.m1.1.1.3.cmml" xref="S6.T3.22.22.22.2.m1.1.1.3">288</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.22.22.22.2.m1.1c">384\times 288</annotation></semantics></math></td>
<td id="S6.T3.22.22.22.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">63.6M</td>
<td id="S6.T3.22.22.22.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">32.9</td>
<td id="S6.T3.22.22.22.6" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;"><span id="S6.T3.22.22.22.6.1" class="ltx_text ltx_font_bold">77.4</span></td>
<td id="S6.T3.22.22.22.7" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">92.6</td>
<td id="S6.T3.22.22.22.8" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">84.6</td>
<td id="S6.T3.22.22.22.9" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">73.6</td>
<td id="S6.T3.22.22.22.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">83.7</td>
<td id="S6.T3.22.22.22.11" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">82.3</td>
<td id="S6.T3.22.22.22.12" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.22.22.22.13" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
</tr>
<tr id="S6.T3.30.30.34.4" class="ltx_tr">
<td id="S6.T3.30.30.34.4.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;" colspan="11">Bottom-up framework: keypoint detection and grouping.</td>
<td id="S6.T3.30.30.34.4.2" class="ltx_td ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T3.30.30.34.4.3" class="ltx_td ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
</tr>
<tr id="S6.T3.30.30.35.5" class="ltx_tr">
<td id="S6.T3.30.30.35.5.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">AE <cite class="ltx_cite ltx_citemacro_cite">Newell et al. (<a href="#bib.bib121" title="" class="ltx_ref">2016a</a>)</cite>
</td>
<td id="S6.T3.30.30.35.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.30.30.35.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">512</td>
<td id="S6.T3.30.30.35.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.30.30.35.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.30.30.35.5.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">63.0</td>
<td id="S6.T3.30.30.35.5.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">85.7</td>
<td id="S6.T3.30.30.35.5.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">68.9</td>
<td id="S6.T3.30.30.35.5.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">58.0</td>
<td id="S6.T3.30.30.35.5.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">70.4</td>
<td id="S6.T3.30.30.35.5.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.30.30.35.5.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.30.30.35.5.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
</tr>
<tr id="S6.T3.30.30.36.6" class="ltx_tr">
<td id="S6.T3.30.30.36.6.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">AE+refinement <cite class="ltx_cite ltx_citemacro_cite">Newell et al. (<a href="#bib.bib121" title="" class="ltx_ref">2016a</a>)</cite>
</td>
<td id="S6.T3.30.30.36.6.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.30.30.36.6.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">512</td>
<td id="S6.T3.30.30.36.6.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.30.30.36.6.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.30.30.36.6.6" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">65.5</td>
<td id="S6.T3.30.30.36.6.7" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">86.8</td>
<td id="S6.T3.30.30.36.6.8" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">72.3</td>
<td id="S6.T3.30.30.36.6.9" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">60.6</td>
<td id="S6.T3.30.30.36.6.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">72.6</td>
<td id="S6.T3.30.30.36.6.11" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">70.2</td>
<td id="S6.T3.30.30.36.6.12" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">64.6</td>
<td id="S6.T3.30.30.36.6.13" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">78.1</td>
</tr>
<tr id="S6.T3.30.30.37.7" class="ltx_tr">
<td id="S6.T3.30.30.37.7.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">DirectPose <cite class="ltx_cite ltx_citemacro_cite">Tian et al. (<a href="#bib.bib159" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="S6.T3.30.30.37.7.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.30.30.37.7.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">800</td>
<td id="S6.T3.30.30.37.7.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.30.30.37.7.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.30.30.37.7.6" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">64.8</td>
<td id="S6.T3.30.30.37.7.7" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">87.8</td>
<td id="S6.T3.30.30.37.7.8" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">71.1</td>
<td id="S6.T3.30.30.37.7.9" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">60.4</td>
<td id="S6.T3.30.30.37.7.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">71.5</td>
<td id="S6.T3.30.30.37.7.11" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.30.30.37.7.12" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.30.30.37.7.13" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
</tr>
<tr id="S6.T3.30.30.38.8" class="ltx_tr">
<td id="S6.T3.30.30.38.8.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">SimplePose <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib85" title="" class="ltx_ref">2020b</a>)</cite>
</td>
<td id="S6.T3.30.30.38.8.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.30.30.38.8.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">512</td>
<td id="S6.T3.30.30.38.8.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.30.30.38.8.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.30.30.38.8.6" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">68.1</td>
<td id="S6.T3.30.30.38.8.7" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.30.30.38.8.8" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.30.30.38.8.9" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">66.8</td>
<td id="S6.T3.30.30.38.8.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">70.5</td>
<td id="S6.T3.30.30.38.8.11" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">72.1</td>
<td id="S6.T3.30.30.38.8.12" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.30.30.38.8.13" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
</tr>
<tr id="S6.T3.30.30.39.9" class="ltx_tr">
<td id="S6.T3.30.30.39.9.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">HGG <cite class="ltx_cite ltx_citemacro_cite">Jin et al. (<a href="#bib.bib70" title="" class="ltx_ref">2020</a>)</cite>
</td>
<td id="S6.T3.30.30.39.9.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.30.30.39.9.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">512</td>
<td id="S6.T3.30.30.39.9.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.30.30.39.9.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.30.30.39.9.6" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">67.6</td>
<td id="S6.T3.30.30.39.9.7" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">85.1</td>
<td id="S6.T3.30.30.39.9.8" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">73.7</td>
<td id="S6.T3.30.30.39.9.9" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">62.7</td>
<td id="S6.T3.30.30.39.9.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">74.6</td>
<td id="S6.T3.30.30.39.9.11" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">71.3</td>
<td id="S6.T3.30.30.39.9.12" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.30.30.39.9.13" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
</tr>
<tr id="S6.T3.30.30.40.10" class="ltx_tr">
<td id="S6.T3.30.30.40.10.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">PersonLab <cite class="ltx_cite ltx_citemacro_cite">Papandreou et al. (<a href="#bib.bib129" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="S6.T3.30.30.40.10.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.30.30.40.10.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">1401</td>
<td id="S6.T3.30.30.40.10.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.30.30.40.10.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.30.30.40.10.6" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">68.7</td>
<td id="S6.T3.30.30.40.10.7" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">89.0</td>
<td id="S6.T3.30.30.40.10.8" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">75.4</td>
<td id="S6.T3.30.30.40.10.9" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">64.1</td>
<td id="S6.T3.30.30.40.10.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">75.5</td>
<td id="S6.T3.30.30.40.10.11" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">75.4</td>
<td id="S6.T3.30.30.40.10.12" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">69.7</td>
<td id="S6.T3.30.30.40.10.13" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">83.0</td>
</tr>
<tr id="S6.T3.30.30.41.11" class="ltx_tr">
<td id="S6.T3.30.30.41.11.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">Point-set Anchors <cite class="ltx_cite ltx_citemacro_cite">Wei et al. (<a href="#bib.bib177" title="" class="ltx_ref">2020</a>)</cite>
</td>
<td id="S6.T3.30.30.41.11.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.30.30.41.11.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">640</td>
<td id="S6.T3.30.30.41.11.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.30.30.41.11.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.30.30.41.11.6" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">68.7</td>
<td id="S6.T3.30.30.41.11.7" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">89.9</td>
<td id="S6.T3.30.30.41.11.8" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">76.3</td>
<td id="S6.T3.30.30.41.11.9" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">64.8</td>
<td id="S6.T3.30.30.41.11.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">75.3</td>
<td id="S6.T3.30.30.41.11.11" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">74.8</td>
<td id="S6.T3.30.30.41.11.12" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">69.6</td>
<td id="S6.T3.30.30.41.11.13" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">82.1</td>
</tr>
<tr id="S6.T3.23.23.23" class="ltx_tr">
<td id="S6.T3.23.23.23.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">HrHRNet-W48<math id="S6.T3.23.23.23.1.m1.1" class="ltx_Math" alttext="+" display="inline"><semantics id="S6.T3.23.23.23.1.m1.1a"><mo id="S6.T3.23.23.23.1.m1.1.1" xref="S6.T3.23.23.23.1.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S6.T3.23.23.23.1.m1.1b"><plus id="S6.T3.23.23.23.1.m1.1.1.cmml" xref="S6.T3.23.23.23.1.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.23.23.23.1.m1.1c">+</annotation></semantics></math>AE <cite class="ltx_cite ltx_citemacro_cite">Cheng et al. (<a href="#bib.bib19" title="" class="ltx_ref">2020</a>)</cite>
</td>
<td id="S6.T3.23.23.23.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">HRNet-W48</td>
<td id="S6.T3.23.23.23.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">640</td>
<td id="S6.T3.23.23.23.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.23.23.23.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.23.23.23.6" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">70.5</td>
<td id="S6.T3.23.23.23.7" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">89.3</td>
<td id="S6.T3.23.23.23.8" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">77.2</td>
<td id="S6.T3.23.23.23.9" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">66.6</td>
<td id="S6.T3.23.23.23.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">75.8</td>
<td id="S6.T3.23.23.23.11" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.23.23.23.12" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.23.23.23.13" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
</tr>
<tr id="S6.T3.30.30.42.12" class="ltx_tr">
<td id="S6.T3.30.30.42.12.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">DEKR-W48 <cite class="ltx_cite ltx_citemacro_cite">Geng et al. (<a href="#bib.bib40" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="S6.T3.30.30.42.12.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">HRNet-W48</td>
<td id="S6.T3.30.30.42.12.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">640</td>
<td id="S6.T3.30.30.42.12.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.30.30.42.12.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.30.30.42.12.6" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">71.0</td>
<td id="S6.T3.30.30.42.12.7" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">89.2</td>
<td id="S6.T3.30.30.42.12.8" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">78.0</td>
<td id="S6.T3.30.30.42.12.9" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">67.1</td>
<td id="S6.T3.30.30.42.12.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">76.9</td>
<td id="S6.T3.30.30.42.12.11" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">76.7</td>
<td id="S6.T3.30.30.42.12.12" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">71.5</td>
<td id="S6.T3.30.30.42.12.13" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">83.9</td>
</tr>
<tr id="S6.T3.24.24.24" class="ltx_tr">
<td id="S6.T3.24.24.24.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">SWAHR<math id="S6.T3.24.24.24.1.m1.1" class="ltx_Math" alttext="+" display="inline"><semantics id="S6.T3.24.24.24.1.m1.1a"><mo id="S6.T3.24.24.24.1.m1.1.1" xref="S6.T3.24.24.24.1.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S6.T3.24.24.24.1.m1.1b"><plus id="S6.T3.24.24.24.1.m1.1.1.cmml" xref="S6.T3.24.24.24.1.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.24.24.24.1.m1.1c">+</annotation></semantics></math>HrHRNet-W48 <cite class="ltx_cite ltx_citemacro_cite">Luo et al. (<a href="#bib.bib106" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="S6.T3.24.24.24.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">HRNet-W48</td>
<td id="S6.T3.24.24.24.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.24.24.24.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.24.24.24.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.24.24.24.6" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;"><span id="S6.T3.24.24.24.6.1" class="ltx_text ltx_font_bold">72.0</span></td>
<td id="S6.T3.24.24.24.7" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">90.7</td>
<td id="S6.T3.24.24.24.8" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">78.8</td>
<td id="S6.T3.24.24.24.9" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">67.8</td>
<td id="S6.T3.24.24.24.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">77.7</td>
<td id="S6.T3.24.24.24.11" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.24.24.24.12" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.24.24.24.13" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
</tr>
<tr id="S6.T3.30.30.43.13" class="ltx_tr">
<td id="S6.T3.30.30.43.13.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;" colspan="11">Small networks</td>
<td id="S6.T3.30.30.43.13.2" class="ltx_td ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
<td id="S6.T3.30.30.43.13.3" class="ltx_td ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"></td>
</tr>
<tr id="S6.T3.25.25.25" class="ltx_tr">
<td id="S6.T3.25.25.25.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">Small HRNet <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a href="#bib.bib187" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="S6.T3.25.25.25.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">HRNet-W16</td>
<td id="S6.T3.25.25.25.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T3.25.25.25.1.m1.1" class="ltx_Math" alttext="384\times 288" display="inline"><semantics id="S6.T3.25.25.25.1.m1.1a"><mrow id="S6.T3.25.25.25.1.m1.1.1" xref="S6.T3.25.25.25.1.m1.1.1.cmml"><mn id="S6.T3.25.25.25.1.m1.1.1.2" xref="S6.T3.25.25.25.1.m1.1.1.2.cmml">384</mn><mo lspace="0.222em" rspace="0.222em" id="S6.T3.25.25.25.1.m1.1.1.1" xref="S6.T3.25.25.25.1.m1.1.1.1.cmml">×</mo><mn id="S6.T3.25.25.25.1.m1.1.1.3" xref="S6.T3.25.25.25.1.m1.1.1.3.cmml">288</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T3.25.25.25.1.m1.1b"><apply id="S6.T3.25.25.25.1.m1.1.1.cmml" xref="S6.T3.25.25.25.1.m1.1.1"><times id="S6.T3.25.25.25.1.m1.1.1.1.cmml" xref="S6.T3.25.25.25.1.m1.1.1.1"></times><cn type="integer" id="S6.T3.25.25.25.1.m1.1.1.2.cmml" xref="S6.T3.25.25.25.1.m1.1.1.2">384</cn><cn type="integer" id="S6.T3.25.25.25.1.m1.1.1.3.cmml" xref="S6.T3.25.25.25.1.m1.1.1.3">288</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.25.25.25.1.m1.1c">384\times 288</annotation></semantics></math></td>
<td id="S6.T3.25.25.25.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">1.3M</td>
<td id="S6.T3.25.25.25.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">1.21</td>
<td id="S6.T3.25.25.25.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">55.2</td>
<td id="S6.T3.25.25.25.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">85.8</td>
<td id="S6.T3.25.25.25.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">61.4</td>
<td id="S6.T3.25.25.25.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">51.7</td>
<td id="S6.T3.25.25.25.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">61.2</td>
<td id="S6.T3.25.25.25.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">61.5</td>
<td id="S6.T3.25.25.25.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.25.25.25.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
</tr>
<tr id="S6.T3.27.27.27" class="ltx_tr">
<td id="S6.T3.26.26.26.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">MobileNetV2 <math id="S6.T3.26.26.26.1.m1.1" class="ltx_math_unparsed" alttext="1\times" display="inline"><semantics id="S6.T3.26.26.26.1.m1.1a"><mrow id="S6.T3.26.26.26.1.m1.1b"><mn id="S6.T3.26.26.26.1.m1.1.1">1</mn><mo lspace="0.222em" id="S6.T3.26.26.26.1.m1.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S6.T3.26.26.26.1.m1.1c">1\times</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">Sandler et al. (<a href="#bib.bib141" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="S6.T3.27.27.27.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">MobileNetV2</td>
<td id="S6.T3.27.27.27.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T3.27.27.27.2.m1.1" class="ltx_Math" alttext="384\times 288" display="inline"><semantics id="S6.T3.27.27.27.2.m1.1a"><mrow id="S6.T3.27.27.27.2.m1.1.1" xref="S6.T3.27.27.27.2.m1.1.1.cmml"><mn id="S6.T3.27.27.27.2.m1.1.1.2" xref="S6.T3.27.27.27.2.m1.1.1.2.cmml">384</mn><mo lspace="0.222em" rspace="0.222em" id="S6.T3.27.27.27.2.m1.1.1.1" xref="S6.T3.27.27.27.2.m1.1.1.1.cmml">×</mo><mn id="S6.T3.27.27.27.2.m1.1.1.3" xref="S6.T3.27.27.27.2.m1.1.1.3.cmml">288</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T3.27.27.27.2.m1.1b"><apply id="S6.T3.27.27.27.2.m1.1.1.cmml" xref="S6.T3.27.27.27.2.m1.1.1"><times id="S6.T3.27.27.27.2.m1.1.1.1.cmml" xref="S6.T3.27.27.27.2.m1.1.1.1"></times><cn type="integer" id="S6.T3.27.27.27.2.m1.1.1.2.cmml" xref="S6.T3.27.27.27.2.m1.1.1.2">384</cn><cn type="integer" id="S6.T3.27.27.27.2.m1.1.1.3.cmml" xref="S6.T3.27.27.27.2.m1.1.1.3">288</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.27.27.27.2.m1.1c">384\times 288</annotation></semantics></math></td>
<td id="S6.T3.27.27.27.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">9.8M</td>
<td id="S6.T3.27.27.27.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">3.33</td>
<td id="S6.T3.27.27.27.6" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">66.8</td>
<td id="S6.T3.27.27.27.7" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">90.0</td>
<td id="S6.T3.27.27.27.8" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">74.0</td>
<td id="S6.T3.27.27.27.9" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">62.6</td>
<td id="S6.T3.27.27.27.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">73.3</td>
<td id="S6.T3.27.27.27.11" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">72.3</td>
<td id="S6.T3.27.27.27.12" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.27.27.27.13" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
</tr>
<tr id="S6.T3.29.29.29" class="ltx_tr">
<td id="S6.T3.28.28.28.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">ShuffleNetV2 <math id="S6.T3.28.28.28.1.m1.1" class="ltx_math_unparsed" alttext="1\times" display="inline"><semantics id="S6.T3.28.28.28.1.m1.1a"><mrow id="S6.T3.28.28.28.1.m1.1b"><mn id="S6.T3.28.28.28.1.m1.1.1">1</mn><mo lspace="0.222em" id="S6.T3.28.28.28.1.m1.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S6.T3.28.28.28.1.m1.1c">1\times</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">Ma et al. (<a href="#bib.bib108" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="S6.T3.29.29.29.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">ShuffleNetV2</td>
<td id="S6.T3.29.29.29.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T3.29.29.29.2.m1.1" class="ltx_Math" alttext="384\times 288" display="inline"><semantics id="S6.T3.29.29.29.2.m1.1a"><mrow id="S6.T3.29.29.29.2.m1.1.1" xref="S6.T3.29.29.29.2.m1.1.1.cmml"><mn id="S6.T3.29.29.29.2.m1.1.1.2" xref="S6.T3.29.29.29.2.m1.1.1.2.cmml">384</mn><mo lspace="0.222em" rspace="0.222em" id="S6.T3.29.29.29.2.m1.1.1.1" xref="S6.T3.29.29.29.2.m1.1.1.1.cmml">×</mo><mn id="S6.T3.29.29.29.2.m1.1.1.3" xref="S6.T3.29.29.29.2.m1.1.1.3.cmml">288</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T3.29.29.29.2.m1.1b"><apply id="S6.T3.29.29.29.2.m1.1.1.cmml" xref="S6.T3.29.29.29.2.m1.1.1"><times id="S6.T3.29.29.29.2.m1.1.1.1.cmml" xref="S6.T3.29.29.29.2.m1.1.1.1"></times><cn type="integer" id="S6.T3.29.29.29.2.m1.1.1.2.cmml" xref="S6.T3.29.29.29.2.m1.1.1.2">384</cn><cn type="integer" id="S6.T3.29.29.29.2.m1.1.1.3.cmml" xref="S6.T3.29.29.29.2.m1.1.1.3">288</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.29.29.29.2.m1.1c">384\times 288</annotation></semantics></math></td>
<td id="S6.T3.29.29.29.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">7.6M</td>
<td id="S6.T3.29.29.29.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">2.87</td>
<td id="S6.T3.29.29.29.6" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">62.9</td>
<td id="S6.T3.29.29.29.7" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">88.5</td>
<td id="S6.T3.29.29.29.8" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">69.4</td>
<td id="S6.T3.29.29.29.9" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">58.9</td>
<td id="S6.T3.29.29.29.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">69.3</td>
<td id="S6.T3.29.29.29.11" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">68.9</td>
<td id="S6.T3.29.29.29.12" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.29.29.29.13" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
</tr>
<tr id="S6.T3.30.30.30" class="ltx_tr">
<td id="S6.T3.30.30.30.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">Lite-HRNet <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a href="#bib.bib187" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="S6.T3.30.30.30.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">Lite-HRNet-30</td>
<td id="S6.T3.30.30.30.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T3.30.30.30.1.m1.1" class="ltx_Math" alttext="384\times 288" display="inline"><semantics id="S6.T3.30.30.30.1.m1.1a"><mrow id="S6.T3.30.30.30.1.m1.1.1" xref="S6.T3.30.30.30.1.m1.1.1.cmml"><mn id="S6.T3.30.30.30.1.m1.1.1.2" xref="S6.T3.30.30.30.1.m1.1.1.2.cmml">384</mn><mo lspace="0.222em" rspace="0.222em" id="S6.T3.30.30.30.1.m1.1.1.1" xref="S6.T3.30.30.30.1.m1.1.1.1.cmml">×</mo><mn id="S6.T3.30.30.30.1.m1.1.1.3" xref="S6.T3.30.30.30.1.m1.1.1.3.cmml">288</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T3.30.30.30.1.m1.1b"><apply id="S6.T3.30.30.30.1.m1.1.1.cmml" xref="S6.T3.30.30.30.1.m1.1.1"><times id="S6.T3.30.30.30.1.m1.1.1.1.cmml" xref="S6.T3.30.30.30.1.m1.1.1.1"></times><cn type="integer" id="S6.T3.30.30.30.1.m1.1.1.2.cmml" xref="S6.T3.30.30.30.1.m1.1.1.2">384</cn><cn type="integer" id="S6.T3.30.30.30.1.m1.1.1.3.cmml" xref="S6.T3.30.30.30.1.m1.1.1.3">288</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.30.30.30.1.m1.1c">384\times 288</annotation></semantics></math></td>
<td id="S6.T3.30.30.30.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">1.8M</td>
<td id="S6.T3.30.30.30.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">0.70</td>
<td id="S6.T3.30.30.30.6" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:3.5pt;padding-bottom:3.5pt;"><span id="S6.T3.30.30.30.6.1" class="ltx_text ltx_font_bold">69.7</span></td>
<td id="S6.T3.30.30.30.7" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:3.5pt;padding-bottom:3.5pt;">90.7</td>
<td id="S6.T3.30.30.30.8" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:3.5pt;padding-bottom:3.5pt;">77.5</td>
<td id="S6.T3.30.30.30.9" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:3.5pt;padding-bottom:3.5pt;">66.9</td>
<td id="S6.T3.30.30.30.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">75.0</td>
<td id="S6.T3.30.30.30.11" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:3.5pt;padding-bottom:3.5pt;">75.4</td>
<td id="S6.T3.30.30.30.12" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T3.30.30.30.13" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S6.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Performance comparisons of state-of-the-art methods on <span id="S6.T4.144.1" class="ltx_text ltx_font_bold">PoseTrack2017</span> benchmark dataset (validation and test sets). Pretrain denotes the backbone model has been pretrained on COCO keypoint detection dataset.</figcaption>
<div id="S6.T4.142" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:205.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-238.4pt,113.1pt) scale(0.476243026573192,0.476243026573192) ;">
<table id="S6.T4.142.142" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T4.142.142.143.1" class="ltx_tr">
<td id="S6.T4.142.142.143.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">Method</td>
<td id="S6.T4.142.142.143.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">Backbone</td>
<td id="S6.T4.142.142.143.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">Pretrain</td>
<td id="S6.T4.142.142.143.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">Additional Training Data</td>
<td id="S6.T4.142.142.143.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">Head</td>
<td id="S6.T4.142.142.143.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">Shoulder</td>
<td id="S6.T4.142.142.143.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">Elbow</td>
<td id="S6.T4.142.142.143.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">Wrist</td>
<td id="S6.T4.142.142.143.1.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">Hip</td>
<td id="S6.T4.142.142.143.1.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">Knee</td>
<td id="S6.T4.142.142.143.1.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">Ankle</td>
<td id="S6.T4.142.142.143.1.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"><span id="S6.T4.142.142.143.1.12.1" class="ltx_text ltx_font_bold">Mean</span></td>
</tr>
<tr id="S6.T4.142.142.144.2" class="ltx_tr">
<td id="S6.T4.142.142.144.2.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;" colspan="12">Dataset: <span id="S6.T4.142.142.144.2.1.1" class="ltx_text ltx_font_bold">PoseTrack2017 Validation</span> set.</td>
</tr>
<tr id="S6.T4.8.8.8" class="ltx_tr">
<td id="S6.T4.8.8.8.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">PoseTracker<cite class="ltx_cite ltx_citemacro_cite">Girdhar et al. (<a href="#bib.bib41" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="S6.T4.8.8.8.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">ResNet-3D</td>
<td id="S6.T4.8.8.8.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">Y</td>
<td id="S6.T4.8.8.8.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">COCO</td>
<td id="S6.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.1.1.1.1.m1.1" class="ltx_Math" alttext="67.5" display="inline"><semantics id="S6.T4.1.1.1.1.m1.1a"><mn id="S6.T4.1.1.1.1.m1.1.1" xref="S6.T4.1.1.1.1.m1.1.1.cmml">67.5</mn><annotation-xml encoding="MathML-Content" id="S6.T4.1.1.1.1.m1.1b"><cn type="float" id="S6.T4.1.1.1.1.m1.1.1.cmml" xref="S6.T4.1.1.1.1.m1.1.1">67.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.1.1.1.1.m1.1c">67.5</annotation></semantics></math></td>
<td id="S6.T4.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.2.2.2.2.m1.1" class="ltx_Math" alttext="70.2" display="inline"><semantics id="S6.T4.2.2.2.2.m1.1a"><mn id="S6.T4.2.2.2.2.m1.1.1" xref="S6.T4.2.2.2.2.m1.1.1.cmml">70.2</mn><annotation-xml encoding="MathML-Content" id="S6.T4.2.2.2.2.m1.1b"><cn type="float" id="S6.T4.2.2.2.2.m1.1.1.cmml" xref="S6.T4.2.2.2.2.m1.1.1">70.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.2.2.2.2.m1.1c">70.2</annotation></semantics></math></td>
<td id="S6.T4.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.3.3.3.3.m1.1" class="ltx_Math" alttext="62.0" display="inline"><semantics id="S6.T4.3.3.3.3.m1.1a"><mn id="S6.T4.3.3.3.3.m1.1.1" xref="S6.T4.3.3.3.3.m1.1.1.cmml">62.0</mn><annotation-xml encoding="MathML-Content" id="S6.T4.3.3.3.3.m1.1b"><cn type="float" id="S6.T4.3.3.3.3.m1.1.1.cmml" xref="S6.T4.3.3.3.3.m1.1.1">62.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.3.3.3.3.m1.1c">62.0</annotation></semantics></math></td>
<td id="S6.T4.4.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.4.4.4.4.m1.1" class="ltx_Math" alttext="51.7" display="inline"><semantics id="S6.T4.4.4.4.4.m1.1a"><mn id="S6.T4.4.4.4.4.m1.1.1" xref="S6.T4.4.4.4.4.m1.1.1.cmml">51.7</mn><annotation-xml encoding="MathML-Content" id="S6.T4.4.4.4.4.m1.1b"><cn type="float" id="S6.T4.4.4.4.4.m1.1.1.cmml" xref="S6.T4.4.4.4.4.m1.1.1">51.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.4.4.4.4.m1.1c">51.7</annotation></semantics></math></td>
<td id="S6.T4.5.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.5.5.5.5.m1.1" class="ltx_Math" alttext="60.7" display="inline"><semantics id="S6.T4.5.5.5.5.m1.1a"><mn id="S6.T4.5.5.5.5.m1.1.1" xref="S6.T4.5.5.5.5.m1.1.1.cmml">60.7</mn><annotation-xml encoding="MathML-Content" id="S6.T4.5.5.5.5.m1.1b"><cn type="float" id="S6.T4.5.5.5.5.m1.1.1.cmml" xref="S6.T4.5.5.5.5.m1.1.1">60.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.5.5.5.5.m1.1c">60.7</annotation></semantics></math></td>
<td id="S6.T4.6.6.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.6.6.6.6.m1.1" class="ltx_Math" alttext="58.7" display="inline"><semantics id="S6.T4.6.6.6.6.m1.1a"><mn id="S6.T4.6.6.6.6.m1.1.1" xref="S6.T4.6.6.6.6.m1.1.1.cmml">58.7</mn><annotation-xml encoding="MathML-Content" id="S6.T4.6.6.6.6.m1.1b"><cn type="float" id="S6.T4.6.6.6.6.m1.1.1.cmml" xref="S6.T4.6.6.6.6.m1.1.1">58.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.6.6.6.6.m1.1c">58.7</annotation></semantics></math></td>
<td id="S6.T4.7.7.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.7.7.7.7.m1.1" class="ltx_Math" alttext="49.8" display="inline"><semantics id="S6.T4.7.7.7.7.m1.1a"><mn id="S6.T4.7.7.7.7.m1.1.1" xref="S6.T4.7.7.7.7.m1.1.1.cmml">49.8</mn><annotation-xml encoding="MathML-Content" id="S6.T4.7.7.7.7.m1.1b"><cn type="float" id="S6.T4.7.7.7.7.m1.1.1.cmml" xref="S6.T4.7.7.7.7.m1.1.1">49.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.7.7.7.7.m1.1c">49.8</annotation></semantics></math></td>
<td id="S6.T4.8.8.8.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.8.8.8.8.m1.1" class="ltx_Math" alttext="60.6" display="inline"><semantics id="S6.T4.8.8.8.8.m1.1a"><mn id="S6.T4.8.8.8.8.m1.1.1" xref="S6.T4.8.8.8.8.m1.1.1.cmml">60.6</mn><annotation-xml encoding="MathML-Content" id="S6.T4.8.8.8.8.m1.1b"><cn type="float" id="S6.T4.8.8.8.8.m1.1.1.cmml" xref="S6.T4.8.8.8.8.m1.1.1">60.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.8.8.8.8.m1.1c">60.6</annotation></semantics></math></td>
</tr>
<tr id="S6.T4.16.16.16" class="ltx_tr">
<td id="S6.T4.16.16.16.9" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">PoseFlow<cite class="ltx_cite ltx_citemacro_cite">Xiu et al. (<a href="#bib.bib182" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="S6.T4.16.16.16.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T4.16.16.16.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T4.16.16.16.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">MPII Pose + COCO</td>
<td id="S6.T4.9.9.9.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.9.9.9.1.m1.1" class="ltx_Math" alttext="66.7" display="inline"><semantics id="S6.T4.9.9.9.1.m1.1a"><mn id="S6.T4.9.9.9.1.m1.1.1" xref="S6.T4.9.9.9.1.m1.1.1.cmml">66.7</mn><annotation-xml encoding="MathML-Content" id="S6.T4.9.9.9.1.m1.1b"><cn type="float" id="S6.T4.9.9.9.1.m1.1.1.cmml" xref="S6.T4.9.9.9.1.m1.1.1">66.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.9.9.9.1.m1.1c">66.7</annotation></semantics></math></td>
<td id="S6.T4.10.10.10.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.10.10.10.2.m1.1" class="ltx_Math" alttext="73.3" display="inline"><semantics id="S6.T4.10.10.10.2.m1.1a"><mn id="S6.T4.10.10.10.2.m1.1.1" xref="S6.T4.10.10.10.2.m1.1.1.cmml">73.3</mn><annotation-xml encoding="MathML-Content" id="S6.T4.10.10.10.2.m1.1b"><cn type="float" id="S6.T4.10.10.10.2.m1.1.1.cmml" xref="S6.T4.10.10.10.2.m1.1.1">73.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.10.10.10.2.m1.1c">73.3</annotation></semantics></math></td>
<td id="S6.T4.11.11.11.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.11.11.11.3.m1.1" class="ltx_Math" alttext="68.3" display="inline"><semantics id="S6.T4.11.11.11.3.m1.1a"><mn id="S6.T4.11.11.11.3.m1.1.1" xref="S6.T4.11.11.11.3.m1.1.1.cmml">68.3</mn><annotation-xml encoding="MathML-Content" id="S6.T4.11.11.11.3.m1.1b"><cn type="float" id="S6.T4.11.11.11.3.m1.1.1.cmml" xref="S6.T4.11.11.11.3.m1.1.1">68.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.11.11.11.3.m1.1c">68.3</annotation></semantics></math></td>
<td id="S6.T4.12.12.12.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.12.12.12.4.m1.1" class="ltx_Math" alttext="61.1" display="inline"><semantics id="S6.T4.12.12.12.4.m1.1a"><mn id="S6.T4.12.12.12.4.m1.1.1" xref="S6.T4.12.12.12.4.m1.1.1.cmml">61.1</mn><annotation-xml encoding="MathML-Content" id="S6.T4.12.12.12.4.m1.1b"><cn type="float" id="S6.T4.12.12.12.4.m1.1.1.cmml" xref="S6.T4.12.12.12.4.m1.1.1">61.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.12.12.12.4.m1.1c">61.1</annotation></semantics></math></td>
<td id="S6.T4.13.13.13.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.13.13.13.5.m1.1" class="ltx_Math" alttext="67.5" display="inline"><semantics id="S6.T4.13.13.13.5.m1.1a"><mn id="S6.T4.13.13.13.5.m1.1.1" xref="S6.T4.13.13.13.5.m1.1.1.cmml">67.5</mn><annotation-xml encoding="MathML-Content" id="S6.T4.13.13.13.5.m1.1b"><cn type="float" id="S6.T4.13.13.13.5.m1.1.1.cmml" xref="S6.T4.13.13.13.5.m1.1.1">67.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.13.13.13.5.m1.1c">67.5</annotation></semantics></math></td>
<td id="S6.T4.14.14.14.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.14.14.14.6.m1.1" class="ltx_Math" alttext="67.0" display="inline"><semantics id="S6.T4.14.14.14.6.m1.1a"><mn id="S6.T4.14.14.14.6.m1.1.1" xref="S6.T4.14.14.14.6.m1.1.1.cmml">67.0</mn><annotation-xml encoding="MathML-Content" id="S6.T4.14.14.14.6.m1.1b"><cn type="float" id="S6.T4.14.14.14.6.m1.1.1.cmml" xref="S6.T4.14.14.14.6.m1.1.1">67.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.14.14.14.6.m1.1c">67.0</annotation></semantics></math></td>
<td id="S6.T4.15.15.15.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.15.15.15.7.m1.1" class="ltx_Math" alttext="61.3" display="inline"><semantics id="S6.T4.15.15.15.7.m1.1a"><mn id="S6.T4.15.15.15.7.m1.1.1" xref="S6.T4.15.15.15.7.m1.1.1.cmml">61.3</mn><annotation-xml encoding="MathML-Content" id="S6.T4.15.15.15.7.m1.1b"><cn type="float" id="S6.T4.15.15.15.7.m1.1.1.cmml" xref="S6.T4.15.15.15.7.m1.1.1">61.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.15.15.15.7.m1.1c">61.3</annotation></semantics></math></td>
<td id="S6.T4.16.16.16.8" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.16.16.16.8.m1.1" class="ltx_Math" alttext="66.5" display="inline"><semantics id="S6.T4.16.16.16.8.m1.1a"><mn id="S6.T4.16.16.16.8.m1.1.1" xref="S6.T4.16.16.16.8.m1.1.1.cmml">66.5</mn><annotation-xml encoding="MathML-Content" id="S6.T4.16.16.16.8.m1.1b"><cn type="float" id="S6.T4.16.16.16.8.m1.1.1.cmml" xref="S6.T4.16.16.16.8.m1.1.1">66.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.16.16.16.8.m1.1c">66.5</annotation></semantics></math></td>
</tr>
<tr id="S6.T4.17.17.17" class="ltx_tr">
<td id="S6.T4.17.17.17.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">JointFlow<cite class="ltx_cite ltx_citemacro_cite">Doering et al. (<a href="#bib.bib24" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="S6.T4.17.17.17.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T4.17.17.17.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T4.17.17.17.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T4.17.17.17.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T4.17.17.17.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T4.17.17.17.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T4.17.17.17.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T4.17.17.17.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T4.17.17.17.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T4.17.17.17.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T4.17.17.17.1" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.17.17.17.1.m1.1" class="ltx_Math" alttext="69.3" display="inline"><semantics id="S6.T4.17.17.17.1.m1.1a"><mn id="S6.T4.17.17.17.1.m1.1.1" xref="S6.T4.17.17.17.1.m1.1.1.cmml">69.3</mn><annotation-xml encoding="MathML-Content" id="S6.T4.17.17.17.1.m1.1b"><cn type="float" id="S6.T4.17.17.17.1.m1.1.1.cmml" xref="S6.T4.17.17.17.1.m1.1.1">69.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.17.17.17.1.m1.1c">69.3</annotation></semantics></math></td>
</tr>
<tr id="S6.T4.25.25.25" class="ltx_tr">
<td id="S6.T4.25.25.25.9" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">FastPose<cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib194" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="S6.T4.25.25.25.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T4.25.25.25.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T4.25.25.25.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T4.18.18.18.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.18.18.18.1.m1.1" class="ltx_Math" alttext="80.0" display="inline"><semantics id="S6.T4.18.18.18.1.m1.1a"><mn id="S6.T4.18.18.18.1.m1.1.1" xref="S6.T4.18.18.18.1.m1.1.1.cmml">80.0</mn><annotation-xml encoding="MathML-Content" id="S6.T4.18.18.18.1.m1.1b"><cn type="float" id="S6.T4.18.18.18.1.m1.1.1.cmml" xref="S6.T4.18.18.18.1.m1.1.1">80.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.18.18.18.1.m1.1c">80.0</annotation></semantics></math></td>
<td id="S6.T4.19.19.19.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.19.19.19.2.m1.1" class="ltx_Math" alttext="80.3" display="inline"><semantics id="S6.T4.19.19.19.2.m1.1a"><mn id="S6.T4.19.19.19.2.m1.1.1" xref="S6.T4.19.19.19.2.m1.1.1.cmml">80.3</mn><annotation-xml encoding="MathML-Content" id="S6.T4.19.19.19.2.m1.1b"><cn type="float" id="S6.T4.19.19.19.2.m1.1.1.cmml" xref="S6.T4.19.19.19.2.m1.1.1">80.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.19.19.19.2.m1.1c">80.3</annotation></semantics></math></td>
<td id="S6.T4.20.20.20.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.20.20.20.3.m1.1" class="ltx_Math" alttext="69.5" display="inline"><semantics id="S6.T4.20.20.20.3.m1.1a"><mn id="S6.T4.20.20.20.3.m1.1.1" xref="S6.T4.20.20.20.3.m1.1.1.cmml">69.5</mn><annotation-xml encoding="MathML-Content" id="S6.T4.20.20.20.3.m1.1b"><cn type="float" id="S6.T4.20.20.20.3.m1.1.1.cmml" xref="S6.T4.20.20.20.3.m1.1.1">69.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.20.20.20.3.m1.1c">69.5</annotation></semantics></math></td>
<td id="S6.T4.21.21.21.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.21.21.21.4.m1.1" class="ltx_Math" alttext="59.1" display="inline"><semantics id="S6.T4.21.21.21.4.m1.1a"><mn id="S6.T4.21.21.21.4.m1.1.1" xref="S6.T4.21.21.21.4.m1.1.1.cmml">59.1</mn><annotation-xml encoding="MathML-Content" id="S6.T4.21.21.21.4.m1.1b"><cn type="float" id="S6.T4.21.21.21.4.m1.1.1.cmml" xref="S6.T4.21.21.21.4.m1.1.1">59.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.21.21.21.4.m1.1c">59.1</annotation></semantics></math></td>
<td id="S6.T4.22.22.22.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.22.22.22.5.m1.1" class="ltx_Math" alttext="71.4" display="inline"><semantics id="S6.T4.22.22.22.5.m1.1a"><mn id="S6.T4.22.22.22.5.m1.1.1" xref="S6.T4.22.22.22.5.m1.1.1.cmml">71.4</mn><annotation-xml encoding="MathML-Content" id="S6.T4.22.22.22.5.m1.1b"><cn type="float" id="S6.T4.22.22.22.5.m1.1.1.cmml" xref="S6.T4.22.22.22.5.m1.1.1">71.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.22.22.22.5.m1.1c">71.4</annotation></semantics></math></td>
<td id="S6.T4.23.23.23.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.23.23.23.6.m1.1" class="ltx_Math" alttext="67.5" display="inline"><semantics id="S6.T4.23.23.23.6.m1.1a"><mn id="S6.T4.23.23.23.6.m1.1.1" xref="S6.T4.23.23.23.6.m1.1.1.cmml">67.5</mn><annotation-xml encoding="MathML-Content" id="S6.T4.23.23.23.6.m1.1b"><cn type="float" id="S6.T4.23.23.23.6.m1.1.1.cmml" xref="S6.T4.23.23.23.6.m1.1.1">67.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.23.23.23.6.m1.1c">67.5</annotation></semantics></math></td>
<td id="S6.T4.24.24.24.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.24.24.24.7.m1.1" class="ltx_Math" alttext="59.4" display="inline"><semantics id="S6.T4.24.24.24.7.m1.1a"><mn id="S6.T4.24.24.24.7.m1.1.1" xref="S6.T4.24.24.24.7.m1.1.1.cmml">59.4</mn><annotation-xml encoding="MathML-Content" id="S6.T4.24.24.24.7.m1.1b"><cn type="float" id="S6.T4.24.24.24.7.m1.1.1.cmml" xref="S6.T4.24.24.24.7.m1.1.1">59.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.24.24.24.7.m1.1c">59.4</annotation></semantics></math></td>
<td id="S6.T4.25.25.25.8" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.25.25.25.8.m1.1" class="ltx_Math" alttext="70.3" display="inline"><semantics id="S6.T4.25.25.25.8.m1.1a"><mn id="S6.T4.25.25.25.8.m1.1.1" xref="S6.T4.25.25.25.8.m1.1.1.cmml">70.3</mn><annotation-xml encoding="MathML-Content" id="S6.T4.25.25.25.8.m1.1b"><cn type="float" id="S6.T4.25.25.25.8.m1.1.1.cmml" xref="S6.T4.25.25.25.8.m1.1.1">70.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.25.25.25.8.m1.1c">70.3</annotation></semantics></math></td>
</tr>
<tr id="S6.T4.33.33.33" class="ltx_tr">
<td id="S6.T4.33.33.33.9" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">SimpleBaseline<cite class="ltx_cite ltx_citemacro_cite">Xiao et al. (<a href="#bib.bib181" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="S6.T4.33.33.33.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">ResNet-50</td>
<td id="S6.T4.33.33.33.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">N</td>
<td id="S6.T4.33.33.33.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">COCO</td>
<td id="S6.T4.26.26.26.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.26.26.26.1.m1.1" class="ltx_Math" alttext="79.1" display="inline"><semantics id="S6.T4.26.26.26.1.m1.1a"><mn id="S6.T4.26.26.26.1.m1.1.1" xref="S6.T4.26.26.26.1.m1.1.1.cmml">79.1</mn><annotation-xml encoding="MathML-Content" id="S6.T4.26.26.26.1.m1.1b"><cn type="float" id="S6.T4.26.26.26.1.m1.1.1.cmml" xref="S6.T4.26.26.26.1.m1.1.1">79.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.26.26.26.1.m1.1c">79.1</annotation></semantics></math></td>
<td id="S6.T4.27.27.27.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.27.27.27.2.m1.1" class="ltx_Math" alttext="80.5" display="inline"><semantics id="S6.T4.27.27.27.2.m1.1a"><mn id="S6.T4.27.27.27.2.m1.1.1" xref="S6.T4.27.27.27.2.m1.1.1.cmml">80.5</mn><annotation-xml encoding="MathML-Content" id="S6.T4.27.27.27.2.m1.1b"><cn type="float" id="S6.T4.27.27.27.2.m1.1.1.cmml" xref="S6.T4.27.27.27.2.m1.1.1">80.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.27.27.27.2.m1.1c">80.5</annotation></semantics></math></td>
<td id="S6.T4.28.28.28.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.28.28.28.3.m1.1" class="ltx_Math" alttext="75.5" display="inline"><semantics id="S6.T4.28.28.28.3.m1.1a"><mn id="S6.T4.28.28.28.3.m1.1.1" xref="S6.T4.28.28.28.3.m1.1.1.cmml">75.5</mn><annotation-xml encoding="MathML-Content" id="S6.T4.28.28.28.3.m1.1b"><cn type="float" id="S6.T4.28.28.28.3.m1.1.1.cmml" xref="S6.T4.28.28.28.3.m1.1.1">75.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.28.28.28.3.m1.1c">75.5</annotation></semantics></math></td>
<td id="S6.T4.29.29.29.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.29.29.29.4.m1.1" class="ltx_Math" alttext="66.0" display="inline"><semantics id="S6.T4.29.29.29.4.m1.1a"><mn id="S6.T4.29.29.29.4.m1.1.1" xref="S6.T4.29.29.29.4.m1.1.1.cmml">66.0</mn><annotation-xml encoding="MathML-Content" id="S6.T4.29.29.29.4.m1.1b"><cn type="float" id="S6.T4.29.29.29.4.m1.1.1.cmml" xref="S6.T4.29.29.29.4.m1.1.1">66.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.29.29.29.4.m1.1c">66.0</annotation></semantics></math></td>
<td id="S6.T4.30.30.30.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.30.30.30.5.m1.1" class="ltx_Math" alttext="70.8" display="inline"><semantics id="S6.T4.30.30.30.5.m1.1a"><mn id="S6.T4.30.30.30.5.m1.1.1" xref="S6.T4.30.30.30.5.m1.1.1.cmml">70.8</mn><annotation-xml encoding="MathML-Content" id="S6.T4.30.30.30.5.m1.1b"><cn type="float" id="S6.T4.30.30.30.5.m1.1.1.cmml" xref="S6.T4.30.30.30.5.m1.1.1">70.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.30.30.30.5.m1.1c">70.8</annotation></semantics></math></td>
<td id="S6.T4.31.31.31.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.31.31.31.6.m1.1" class="ltx_Math" alttext="70.0" display="inline"><semantics id="S6.T4.31.31.31.6.m1.1a"><mn id="S6.T4.31.31.31.6.m1.1.1" xref="S6.T4.31.31.31.6.m1.1.1.cmml">70.0</mn><annotation-xml encoding="MathML-Content" id="S6.T4.31.31.31.6.m1.1b"><cn type="float" id="S6.T4.31.31.31.6.m1.1.1.cmml" xref="S6.T4.31.31.31.6.m1.1.1">70.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.31.31.31.6.m1.1c">70.0</annotation></semantics></math></td>
<td id="S6.T4.32.32.32.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.32.32.32.7.m1.1" class="ltx_Math" alttext="61.7" display="inline"><semantics id="S6.T4.32.32.32.7.m1.1a"><mn id="S6.T4.32.32.32.7.m1.1.1" xref="S6.T4.32.32.32.7.m1.1.1.cmml">61.7</mn><annotation-xml encoding="MathML-Content" id="S6.T4.32.32.32.7.m1.1b"><cn type="float" id="S6.T4.32.32.32.7.m1.1.1.cmml" xref="S6.T4.32.32.32.7.m1.1.1">61.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.32.32.32.7.m1.1c">61.7</annotation></semantics></math></td>
<td id="S6.T4.33.33.33.8" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.33.33.33.8.m1.1" class="ltx_Math" alttext="72.4" display="inline"><semantics id="S6.T4.33.33.33.8.m1.1a"><mn id="S6.T4.33.33.33.8.m1.1.1" xref="S6.T4.33.33.33.8.m1.1.1.cmml">72.4</mn><annotation-xml encoding="MathML-Content" id="S6.T4.33.33.33.8.m1.1b"><cn type="float" id="S6.T4.33.33.33.8.m1.1.1.cmml" xref="S6.T4.33.33.33.8.m1.1.1">72.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.33.33.33.8.m1.1c">72.4</annotation></semantics></math></td>
</tr>
<tr id="S6.T4.41.41.41" class="ltx_tr">
<td id="S6.T4.41.41.41.9" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">SimpleBaseline<cite class="ltx_cite ltx_citemacro_cite">Xiao et al. (<a href="#bib.bib181" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="S6.T4.41.41.41.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">ResNet-152</td>
<td id="S6.T4.41.41.41.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">N</td>
<td id="S6.T4.41.41.41.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">COCO</td>
<td id="S6.T4.34.34.34.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.34.34.34.1.m1.1" class="ltx_Math" alttext="81.7" display="inline"><semantics id="S6.T4.34.34.34.1.m1.1a"><mn id="S6.T4.34.34.34.1.m1.1.1" xref="S6.T4.34.34.34.1.m1.1.1.cmml">81.7</mn><annotation-xml encoding="MathML-Content" id="S6.T4.34.34.34.1.m1.1b"><cn type="float" id="S6.T4.34.34.34.1.m1.1.1.cmml" xref="S6.T4.34.34.34.1.m1.1.1">81.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.34.34.34.1.m1.1c">81.7</annotation></semantics></math></td>
<td id="S6.T4.35.35.35.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.35.35.35.2.m1.1" class="ltx_Math" alttext="83.4" display="inline"><semantics id="S6.T4.35.35.35.2.m1.1a"><mn id="S6.T4.35.35.35.2.m1.1.1" xref="S6.T4.35.35.35.2.m1.1.1.cmml">83.4</mn><annotation-xml encoding="MathML-Content" id="S6.T4.35.35.35.2.m1.1b"><cn type="float" id="S6.T4.35.35.35.2.m1.1.1.cmml" xref="S6.T4.35.35.35.2.m1.1.1">83.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.35.35.35.2.m1.1c">83.4</annotation></semantics></math></td>
<td id="S6.T4.36.36.36.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.36.36.36.3.m1.1" class="ltx_Math" alttext="80.0" display="inline"><semantics id="S6.T4.36.36.36.3.m1.1a"><mn id="S6.T4.36.36.36.3.m1.1.1" xref="S6.T4.36.36.36.3.m1.1.1.cmml">80.0</mn><annotation-xml encoding="MathML-Content" id="S6.T4.36.36.36.3.m1.1b"><cn type="float" id="S6.T4.36.36.36.3.m1.1.1.cmml" xref="S6.T4.36.36.36.3.m1.1.1">80.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.36.36.36.3.m1.1c">80.0</annotation></semantics></math></td>
<td id="S6.T4.37.37.37.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.37.37.37.4.m1.1" class="ltx_Math" alttext="72.4" display="inline"><semantics id="S6.T4.37.37.37.4.m1.1a"><mn id="S6.T4.37.37.37.4.m1.1.1" xref="S6.T4.37.37.37.4.m1.1.1.cmml">72.4</mn><annotation-xml encoding="MathML-Content" id="S6.T4.37.37.37.4.m1.1b"><cn type="float" id="S6.T4.37.37.37.4.m1.1.1.cmml" xref="S6.T4.37.37.37.4.m1.1.1">72.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.37.37.37.4.m1.1c">72.4</annotation></semantics></math></td>
<td id="S6.T4.38.38.38.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.38.38.38.5.m1.1" class="ltx_Math" alttext="75.3" display="inline"><semantics id="S6.T4.38.38.38.5.m1.1a"><mn id="S6.T4.38.38.38.5.m1.1.1" xref="S6.T4.38.38.38.5.m1.1.1.cmml">75.3</mn><annotation-xml encoding="MathML-Content" id="S6.T4.38.38.38.5.m1.1b"><cn type="float" id="S6.T4.38.38.38.5.m1.1.1.cmml" xref="S6.T4.38.38.38.5.m1.1.1">75.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.38.38.38.5.m1.1c">75.3</annotation></semantics></math></td>
<td id="S6.T4.39.39.39.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.39.39.39.6.m1.1" class="ltx_Math" alttext="74.8" display="inline"><semantics id="S6.T4.39.39.39.6.m1.1a"><mn id="S6.T4.39.39.39.6.m1.1.1" xref="S6.T4.39.39.39.6.m1.1.1.cmml">74.8</mn><annotation-xml encoding="MathML-Content" id="S6.T4.39.39.39.6.m1.1b"><cn type="float" id="S6.T4.39.39.39.6.m1.1.1.cmml" xref="S6.T4.39.39.39.6.m1.1.1">74.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.39.39.39.6.m1.1c">74.8</annotation></semantics></math></td>
<td id="S6.T4.40.40.40.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.40.40.40.7.m1.1" class="ltx_Math" alttext="67.1" display="inline"><semantics id="S6.T4.40.40.40.7.m1.1a"><mn id="S6.T4.40.40.40.7.m1.1.1" xref="S6.T4.40.40.40.7.m1.1.1.cmml">67.1</mn><annotation-xml encoding="MathML-Content" id="S6.T4.40.40.40.7.m1.1b"><cn type="float" id="S6.T4.40.40.40.7.m1.1.1.cmml" xref="S6.T4.40.40.40.7.m1.1.1">67.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.40.40.40.7.m1.1c">67.1</annotation></semantics></math></td>
<td id="S6.T4.41.41.41.8" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.41.41.41.8.m1.1" class="ltx_Math" alttext="76.7" display="inline"><semantics id="S6.T4.41.41.41.8.m1.1a"><mn id="S6.T4.41.41.41.8.m1.1.1" xref="S6.T4.41.41.41.8.m1.1.1.cmml">76.7</mn><annotation-xml encoding="MathML-Content" id="S6.T4.41.41.41.8.m1.1b"><cn type="float" id="S6.T4.41.41.41.8.m1.1.1.cmml" xref="S6.T4.41.41.41.8.m1.1.1">76.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.41.41.41.8.m1.1c">76.7</annotation></semantics></math></td>
</tr>
<tr id="S6.T4.50.50.50" class="ltx_tr">
<td id="S6.T4.50.50.50.10" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">STEmbedding<cite class="ltx_cite ltx_citemacro_cite">Jin et al. (<a href="#bib.bib69" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="S6.T4.42.42.42.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">
<math id="S6.T4.42.42.42.1.m1.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S6.T4.42.42.42.1.m1.1a"><mn id="S6.T4.42.42.42.1.m1.1.1" xref="S6.T4.42.42.42.1.m1.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S6.T4.42.42.42.1.m1.1b"><cn type="integer" id="S6.T4.42.42.42.1.m1.1.1.cmml" xref="S6.T4.42.42.42.1.m1.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.42.42.42.1.m1.1c">4</annotation></semantics></math>-stage Stacked Hourglass</td>
<td id="S6.T4.50.50.50.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">Y</td>
<td id="S6.T4.50.50.50.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T4.43.43.43.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.43.43.43.2.m1.1" class="ltx_Math" alttext="83.8" display="inline"><semantics id="S6.T4.43.43.43.2.m1.1a"><mn id="S6.T4.43.43.43.2.m1.1.1" xref="S6.T4.43.43.43.2.m1.1.1.cmml">83.8</mn><annotation-xml encoding="MathML-Content" id="S6.T4.43.43.43.2.m1.1b"><cn type="float" id="S6.T4.43.43.43.2.m1.1.1.cmml" xref="S6.T4.43.43.43.2.m1.1.1">83.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.43.43.43.2.m1.1c">83.8</annotation></semantics></math></td>
<td id="S6.T4.44.44.44.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.44.44.44.3.m1.1" class="ltx_Math" alttext="81.6" display="inline"><semantics id="S6.T4.44.44.44.3.m1.1a"><mn id="S6.T4.44.44.44.3.m1.1.1" xref="S6.T4.44.44.44.3.m1.1.1.cmml">81.6</mn><annotation-xml encoding="MathML-Content" id="S6.T4.44.44.44.3.m1.1b"><cn type="float" id="S6.T4.44.44.44.3.m1.1.1.cmml" xref="S6.T4.44.44.44.3.m1.1.1">81.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.44.44.44.3.m1.1c">81.6</annotation></semantics></math></td>
<td id="S6.T4.45.45.45.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.45.45.45.4.m1.1" class="ltx_Math" alttext="77.1" display="inline"><semantics id="S6.T4.45.45.45.4.m1.1a"><mn id="S6.T4.45.45.45.4.m1.1.1" xref="S6.T4.45.45.45.4.m1.1.1.cmml">77.1</mn><annotation-xml encoding="MathML-Content" id="S6.T4.45.45.45.4.m1.1b"><cn type="float" id="S6.T4.45.45.45.4.m1.1.1.cmml" xref="S6.T4.45.45.45.4.m1.1.1">77.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.45.45.45.4.m1.1c">77.1</annotation></semantics></math></td>
<td id="S6.T4.46.46.46.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.46.46.46.5.m1.1" class="ltx_Math" alttext="70.0" display="inline"><semantics id="S6.T4.46.46.46.5.m1.1a"><mn id="S6.T4.46.46.46.5.m1.1.1" xref="S6.T4.46.46.46.5.m1.1.1.cmml">70.0</mn><annotation-xml encoding="MathML-Content" id="S6.T4.46.46.46.5.m1.1b"><cn type="float" id="S6.T4.46.46.46.5.m1.1.1.cmml" xref="S6.T4.46.46.46.5.m1.1.1">70.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.46.46.46.5.m1.1c">70.0</annotation></semantics></math></td>
<td id="S6.T4.47.47.47.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.47.47.47.6.m1.1" class="ltx_Math" alttext="77.4" display="inline"><semantics id="S6.T4.47.47.47.6.m1.1a"><mn id="S6.T4.47.47.47.6.m1.1.1" xref="S6.T4.47.47.47.6.m1.1.1.cmml">77.4</mn><annotation-xml encoding="MathML-Content" id="S6.T4.47.47.47.6.m1.1b"><cn type="float" id="S6.T4.47.47.47.6.m1.1.1.cmml" xref="S6.T4.47.47.47.6.m1.1.1">77.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.47.47.47.6.m1.1c">77.4</annotation></semantics></math></td>
<td id="S6.T4.48.48.48.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.48.48.48.7.m1.1" class="ltx_Math" alttext="74.5" display="inline"><semantics id="S6.T4.48.48.48.7.m1.1a"><mn id="S6.T4.48.48.48.7.m1.1.1" xref="S6.T4.48.48.48.7.m1.1.1.cmml">74.5</mn><annotation-xml encoding="MathML-Content" id="S6.T4.48.48.48.7.m1.1b"><cn type="float" id="S6.T4.48.48.48.7.m1.1.1.cmml" xref="S6.T4.48.48.48.7.m1.1.1">74.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.48.48.48.7.m1.1c">74.5</annotation></semantics></math></td>
<td id="S6.T4.49.49.49.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.49.49.49.8.m1.1" class="ltx_Math" alttext="70.8" display="inline"><semantics id="S6.T4.49.49.49.8.m1.1a"><mn id="S6.T4.49.49.49.8.m1.1.1" xref="S6.T4.49.49.49.8.m1.1.1.cmml">70.8</mn><annotation-xml encoding="MathML-Content" id="S6.T4.49.49.49.8.m1.1b"><cn type="float" id="S6.T4.49.49.49.8.m1.1.1.cmml" xref="S6.T4.49.49.49.8.m1.1.1">70.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.49.49.49.8.m1.1c">70.8</annotation></semantics></math></td>
<td id="S6.T4.50.50.50.9" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.50.50.50.9.m1.1" class="ltx_Math" alttext="77.0" display="inline"><semantics id="S6.T4.50.50.50.9.m1.1a"><mn id="S6.T4.50.50.50.9.m1.1.1" xref="S6.T4.50.50.50.9.m1.1.1.cmml">77.0</mn><annotation-xml encoding="MathML-Content" id="S6.T4.50.50.50.9.m1.1b"><cn type="float" id="S6.T4.50.50.50.9.m1.1.1.cmml" xref="S6.T4.50.50.50.9.m1.1.1">77.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.50.50.50.9.m1.1c">77.0</annotation></semantics></math></td>
</tr>
<tr id="S6.T4.58.58.58" class="ltx_tr">
<td id="S6.T4.58.58.58.9" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">HRNet<cite class="ltx_cite ltx_citemacro_cite">Sun et al. (<a href="#bib.bib153" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="S6.T4.58.58.58.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">HRNet-W48</td>
<td id="S6.T4.58.58.58.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">Y</td>
<td id="S6.T4.58.58.58.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">COCO</td>
<td id="S6.T4.51.51.51.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.51.51.51.1.m1.1" class="ltx_Math" alttext="82.1" display="inline"><semantics id="S6.T4.51.51.51.1.m1.1a"><mn id="S6.T4.51.51.51.1.m1.1.1" xref="S6.T4.51.51.51.1.m1.1.1.cmml">82.1</mn><annotation-xml encoding="MathML-Content" id="S6.T4.51.51.51.1.m1.1b"><cn type="float" id="S6.T4.51.51.51.1.m1.1.1.cmml" xref="S6.T4.51.51.51.1.m1.1.1">82.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.51.51.51.1.m1.1c">82.1</annotation></semantics></math></td>
<td id="S6.T4.52.52.52.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.52.52.52.2.m1.1" class="ltx_Math" alttext="83.6" display="inline"><semantics id="S6.T4.52.52.52.2.m1.1a"><mn id="S6.T4.52.52.52.2.m1.1.1" xref="S6.T4.52.52.52.2.m1.1.1.cmml">83.6</mn><annotation-xml encoding="MathML-Content" id="S6.T4.52.52.52.2.m1.1b"><cn type="float" id="S6.T4.52.52.52.2.m1.1.1.cmml" xref="S6.T4.52.52.52.2.m1.1.1">83.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.52.52.52.2.m1.1c">83.6</annotation></semantics></math></td>
<td id="S6.T4.53.53.53.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.53.53.53.3.m1.1" class="ltx_Math" alttext="80.4" display="inline"><semantics id="S6.T4.53.53.53.3.m1.1a"><mn id="S6.T4.53.53.53.3.m1.1.1" xref="S6.T4.53.53.53.3.m1.1.1.cmml">80.4</mn><annotation-xml encoding="MathML-Content" id="S6.T4.53.53.53.3.m1.1b"><cn type="float" id="S6.T4.53.53.53.3.m1.1.1.cmml" xref="S6.T4.53.53.53.3.m1.1.1">80.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.53.53.53.3.m1.1c">80.4</annotation></semantics></math></td>
<td id="S6.T4.54.54.54.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.54.54.54.4.m1.1" class="ltx_Math" alttext="73.3" display="inline"><semantics id="S6.T4.54.54.54.4.m1.1a"><mn id="S6.T4.54.54.54.4.m1.1.1" xref="S6.T4.54.54.54.4.m1.1.1.cmml">73.3</mn><annotation-xml encoding="MathML-Content" id="S6.T4.54.54.54.4.m1.1b"><cn type="float" id="S6.T4.54.54.54.4.m1.1.1.cmml" xref="S6.T4.54.54.54.4.m1.1.1">73.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.54.54.54.4.m1.1c">73.3</annotation></semantics></math></td>
<td id="S6.T4.55.55.55.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.55.55.55.5.m1.1" class="ltx_Math" alttext="75.5" display="inline"><semantics id="S6.T4.55.55.55.5.m1.1a"><mn id="S6.T4.55.55.55.5.m1.1.1" xref="S6.T4.55.55.55.5.m1.1.1.cmml">75.5</mn><annotation-xml encoding="MathML-Content" id="S6.T4.55.55.55.5.m1.1b"><cn type="float" id="S6.T4.55.55.55.5.m1.1.1.cmml" xref="S6.T4.55.55.55.5.m1.1.1">75.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.55.55.55.5.m1.1c">75.5</annotation></semantics></math></td>
<td id="S6.T4.56.56.56.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.56.56.56.6.m1.1" class="ltx_Math" alttext="75.3" display="inline"><semantics id="S6.T4.56.56.56.6.m1.1a"><mn id="S6.T4.56.56.56.6.m1.1.1" xref="S6.T4.56.56.56.6.m1.1.1.cmml">75.3</mn><annotation-xml encoding="MathML-Content" id="S6.T4.56.56.56.6.m1.1b"><cn type="float" id="S6.T4.56.56.56.6.m1.1.1.cmml" xref="S6.T4.56.56.56.6.m1.1.1">75.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.56.56.56.6.m1.1c">75.3</annotation></semantics></math></td>
<td id="S6.T4.57.57.57.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.57.57.57.7.m1.1" class="ltx_Math" alttext="68.5" display="inline"><semantics id="S6.T4.57.57.57.7.m1.1a"><mn id="S6.T4.57.57.57.7.m1.1.1" xref="S6.T4.57.57.57.7.m1.1.1.cmml">68.5</mn><annotation-xml encoding="MathML-Content" id="S6.T4.57.57.57.7.m1.1b"><cn type="float" id="S6.T4.57.57.57.7.m1.1.1.cmml" xref="S6.T4.57.57.57.7.m1.1.1">68.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.57.57.57.7.m1.1c">68.5</annotation></semantics></math></td>
<td id="S6.T4.58.58.58.8" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.58.58.58.8.m1.1" class="ltx_Math" alttext="77.3" display="inline"><semantics id="S6.T4.58.58.58.8.m1.1a"><mn id="S6.T4.58.58.58.8.m1.1.1" xref="S6.T4.58.58.58.8.m1.1.1.cmml">77.3</mn><annotation-xml encoding="MathML-Content" id="S6.T4.58.58.58.8.m1.1b"><cn type="float" id="S6.T4.58.58.58.8.m1.1.1.cmml" xref="S6.T4.58.58.58.8.m1.1.1">77.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.58.58.58.8.m1.1c">77.3</annotation></semantics></math></td>
</tr>
<tr id="S6.T4.66.66.66" class="ltx_tr">
<td id="S6.T4.66.66.66.9" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">MDPN<cite class="ltx_cite ltx_citemacro_cite">Guo et al. (<a href="#bib.bib47" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="S6.T4.66.66.66.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">SimpleBaseline</td>
<td id="S6.T4.66.66.66.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">Y</td>
<td id="S6.T4.66.66.66.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">MPII Pose + COCO</td>
<td id="S6.T4.59.59.59.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.59.59.59.1.m1.1" class="ltx_Math" alttext="85.2" display="inline"><semantics id="S6.T4.59.59.59.1.m1.1a"><mn id="S6.T4.59.59.59.1.m1.1.1" xref="S6.T4.59.59.59.1.m1.1.1.cmml">85.2</mn><annotation-xml encoding="MathML-Content" id="S6.T4.59.59.59.1.m1.1b"><cn type="float" id="S6.T4.59.59.59.1.m1.1.1.cmml" xref="S6.T4.59.59.59.1.m1.1.1">85.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.59.59.59.1.m1.1c">85.2</annotation></semantics></math></td>
<td id="S6.T4.60.60.60.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.60.60.60.2.m1.1" class="ltx_Math" alttext="88.5" display="inline"><semantics id="S6.T4.60.60.60.2.m1.1a"><mn id="S6.T4.60.60.60.2.m1.1.1" xref="S6.T4.60.60.60.2.m1.1.1.cmml">88.5</mn><annotation-xml encoding="MathML-Content" id="S6.T4.60.60.60.2.m1.1b"><cn type="float" id="S6.T4.60.60.60.2.m1.1.1.cmml" xref="S6.T4.60.60.60.2.m1.1.1">88.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.60.60.60.2.m1.1c">88.5</annotation></semantics></math></td>
<td id="S6.T4.61.61.61.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.61.61.61.3.m1.1" class="ltx_Math" alttext="83.9" display="inline"><semantics id="S6.T4.61.61.61.3.m1.1a"><mn id="S6.T4.61.61.61.3.m1.1.1" xref="S6.T4.61.61.61.3.m1.1.1.cmml">83.9</mn><annotation-xml encoding="MathML-Content" id="S6.T4.61.61.61.3.m1.1b"><cn type="float" id="S6.T4.61.61.61.3.m1.1.1.cmml" xref="S6.T4.61.61.61.3.m1.1.1">83.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.61.61.61.3.m1.1c">83.9</annotation></semantics></math></td>
<td id="S6.T4.62.62.62.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.62.62.62.4.m1.1" class="ltx_Math" alttext="77.5" display="inline"><semantics id="S6.T4.62.62.62.4.m1.1a"><mn id="S6.T4.62.62.62.4.m1.1.1" xref="S6.T4.62.62.62.4.m1.1.1.cmml">77.5</mn><annotation-xml encoding="MathML-Content" id="S6.T4.62.62.62.4.m1.1b"><cn type="float" id="S6.T4.62.62.62.4.m1.1.1.cmml" xref="S6.T4.62.62.62.4.m1.1.1">77.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.62.62.62.4.m1.1c">77.5</annotation></semantics></math></td>
<td id="S6.T4.63.63.63.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.63.63.63.5.m1.1" class="ltx_Math" alttext="79.0" display="inline"><semantics id="S6.T4.63.63.63.5.m1.1a"><mn id="S6.T4.63.63.63.5.m1.1.1" xref="S6.T4.63.63.63.5.m1.1.1.cmml">79.0</mn><annotation-xml encoding="MathML-Content" id="S6.T4.63.63.63.5.m1.1b"><cn type="float" id="S6.T4.63.63.63.5.m1.1.1.cmml" xref="S6.T4.63.63.63.5.m1.1.1">79.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.63.63.63.5.m1.1c">79.0</annotation></semantics></math></td>
<td id="S6.T4.64.64.64.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.64.64.64.6.m1.1" class="ltx_Math" alttext="77.0" display="inline"><semantics id="S6.T4.64.64.64.6.m1.1a"><mn id="S6.T4.64.64.64.6.m1.1.1" xref="S6.T4.64.64.64.6.m1.1.1.cmml">77.0</mn><annotation-xml encoding="MathML-Content" id="S6.T4.64.64.64.6.m1.1b"><cn type="float" id="S6.T4.64.64.64.6.m1.1.1.cmml" xref="S6.T4.64.64.64.6.m1.1.1">77.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.64.64.64.6.m1.1c">77.0</annotation></semantics></math></td>
<td id="S6.T4.65.65.65.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.65.65.65.7.m1.1" class="ltx_Math" alttext="71.4" display="inline"><semantics id="S6.T4.65.65.65.7.m1.1a"><mn id="S6.T4.65.65.65.7.m1.1.1" xref="S6.T4.65.65.65.7.m1.1.1.cmml">71.4</mn><annotation-xml encoding="MathML-Content" id="S6.T4.65.65.65.7.m1.1b"><cn type="float" id="S6.T4.65.65.65.7.m1.1.1.cmml" xref="S6.T4.65.65.65.7.m1.1.1">71.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.65.65.65.7.m1.1c">71.4</annotation></semantics></math></td>
<td id="S6.T4.66.66.66.8" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.66.66.66.8.m1.1" class="ltx_Math" alttext="80.7" display="inline"><semantics id="S6.T4.66.66.66.8.m1.1a"><mn id="S6.T4.66.66.66.8.m1.1.1" xref="S6.T4.66.66.66.8.m1.1.1.cmml">80.7</mn><annotation-xml encoding="MathML-Content" id="S6.T4.66.66.66.8.m1.1b"><cn type="float" id="S6.T4.66.66.66.8.m1.1.1.cmml" xref="S6.T4.66.66.66.8.m1.1.1">80.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.66.66.66.8.m1.1c">80.7</annotation></semantics></math></td>
</tr>
<tr id="S6.T4.74.74.74" class="ltx_tr">
<td id="S6.T4.74.74.74.9" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">Dynamic<cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a href="#bib.bib186" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="S6.T4.74.74.74.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">HRNet-W48</td>
<td id="S6.T4.74.74.74.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">Y</td>
<td id="S6.T4.74.74.74.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">COCO</td>
<td id="S6.T4.67.67.67.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.67.67.67.1.m1.1" class="ltx_Math" alttext="88.4" display="inline"><semantics id="S6.T4.67.67.67.1.m1.1a"><mn id="S6.T4.67.67.67.1.m1.1.1" xref="S6.T4.67.67.67.1.m1.1.1.cmml">88.4</mn><annotation-xml encoding="MathML-Content" id="S6.T4.67.67.67.1.m1.1b"><cn type="float" id="S6.T4.67.67.67.1.m1.1.1.cmml" xref="S6.T4.67.67.67.1.m1.1.1">88.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.67.67.67.1.m1.1c">88.4</annotation></semantics></math></td>
<td id="S6.T4.68.68.68.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.68.68.68.2.m1.1" class="ltx_Math" alttext="88.4" display="inline"><semantics id="S6.T4.68.68.68.2.m1.1a"><mn id="S6.T4.68.68.68.2.m1.1.1" xref="S6.T4.68.68.68.2.m1.1.1.cmml">88.4</mn><annotation-xml encoding="MathML-Content" id="S6.T4.68.68.68.2.m1.1b"><cn type="float" id="S6.T4.68.68.68.2.m1.1.1.cmml" xref="S6.T4.68.68.68.2.m1.1.1">88.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.68.68.68.2.m1.1c">88.4</annotation></semantics></math></td>
<td id="S6.T4.69.69.69.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.69.69.69.3.m1.1" class="ltx_Math" alttext="82.0" display="inline"><semantics id="S6.T4.69.69.69.3.m1.1a"><mn id="S6.T4.69.69.69.3.m1.1.1" xref="S6.T4.69.69.69.3.m1.1.1.cmml">82.0</mn><annotation-xml encoding="MathML-Content" id="S6.T4.69.69.69.3.m1.1b"><cn type="float" id="S6.T4.69.69.69.3.m1.1.1.cmml" xref="S6.T4.69.69.69.3.m1.1.1">82.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.69.69.69.3.m1.1c">82.0</annotation></semantics></math></td>
<td id="S6.T4.70.70.70.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.70.70.70.4.m1.1" class="ltx_Math" alttext="74.5" display="inline"><semantics id="S6.T4.70.70.70.4.m1.1a"><mn id="S6.T4.70.70.70.4.m1.1.1" xref="S6.T4.70.70.70.4.m1.1.1.cmml">74.5</mn><annotation-xml encoding="MathML-Content" id="S6.T4.70.70.70.4.m1.1b"><cn type="float" id="S6.T4.70.70.70.4.m1.1.1.cmml" xref="S6.T4.70.70.70.4.m1.1.1">74.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.70.70.70.4.m1.1c">74.5</annotation></semantics></math></td>
<td id="S6.T4.71.71.71.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.71.71.71.5.m1.1" class="ltx_Math" alttext="79.1" display="inline"><semantics id="S6.T4.71.71.71.5.m1.1a"><mn id="S6.T4.71.71.71.5.m1.1.1" xref="S6.T4.71.71.71.5.m1.1.1.cmml">79.1</mn><annotation-xml encoding="MathML-Content" id="S6.T4.71.71.71.5.m1.1b"><cn type="float" id="S6.T4.71.71.71.5.m1.1.1.cmml" xref="S6.T4.71.71.71.5.m1.1.1">79.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.71.71.71.5.m1.1c">79.1</annotation></semantics></math></td>
<td id="S6.T4.72.72.72.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.72.72.72.6.m1.1" class="ltx_Math" alttext="78.3" display="inline"><semantics id="S6.T4.72.72.72.6.m1.1a"><mn id="S6.T4.72.72.72.6.m1.1.1" xref="S6.T4.72.72.72.6.m1.1.1.cmml">78.3</mn><annotation-xml encoding="MathML-Content" id="S6.T4.72.72.72.6.m1.1b"><cn type="float" id="S6.T4.72.72.72.6.m1.1.1.cmml" xref="S6.T4.72.72.72.6.m1.1.1">78.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.72.72.72.6.m1.1c">78.3</annotation></semantics></math></td>
<td id="S6.T4.73.73.73.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.73.73.73.7.m1.1" class="ltx_Math" alttext="73.1" display="inline"><semantics id="S6.T4.73.73.73.7.m1.1a"><mn id="S6.T4.73.73.73.7.m1.1.1" xref="S6.T4.73.73.73.7.m1.1.1.cmml">73.1</mn><annotation-xml encoding="MathML-Content" id="S6.T4.73.73.73.7.m1.1b"><cn type="float" id="S6.T4.73.73.73.7.m1.1.1.cmml" xref="S6.T4.73.73.73.7.m1.1.1">73.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.73.73.73.7.m1.1c">73.1</annotation></semantics></math></td>
<td id="S6.T4.74.74.74.8" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.74.74.74.8.m1.1" class="ltx_Math" alttext="81.1" display="inline"><semantics id="S6.T4.74.74.74.8.m1.1a"><mn id="S6.T4.74.74.74.8.m1.1.1" xref="S6.T4.74.74.74.8.m1.1.1.cmml">81.1</mn><annotation-xml encoding="MathML-Content" id="S6.T4.74.74.74.8.m1.1b"><cn type="float" id="S6.T4.74.74.74.8.m1.1.1.cmml" xref="S6.T4.74.74.74.8.m1.1.1">81.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.74.74.74.8.m1.1c">81.1</annotation></semantics></math>
</td>
</tr>
<tr id="S6.T4.82.82.82" class="ltx_tr">
<td id="S6.T4.82.82.82.9" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">PoseWarper<cite class="ltx_cite ltx_citemacro_cite">Bertasius et al. (<a href="#bib.bib5" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="S6.T4.82.82.82.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">HRNet-W48</td>
<td id="S6.T4.82.82.82.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">Y</td>
<td id="S6.T4.82.82.82.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">COCO</td>
<td id="S6.T4.75.75.75.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.75.75.75.1.m1.1" class="ltx_Math" alttext="81.4" display="inline"><semantics id="S6.T4.75.75.75.1.m1.1a"><mn id="S6.T4.75.75.75.1.m1.1.1" xref="S6.T4.75.75.75.1.m1.1.1.cmml">81.4</mn><annotation-xml encoding="MathML-Content" id="S6.T4.75.75.75.1.m1.1b"><cn type="float" id="S6.T4.75.75.75.1.m1.1.1.cmml" xref="S6.T4.75.75.75.1.m1.1.1">81.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.75.75.75.1.m1.1c">81.4</annotation></semantics></math></td>
<td id="S6.T4.76.76.76.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.76.76.76.2.m1.1" class="ltx_Math" alttext="88.3" display="inline"><semantics id="S6.T4.76.76.76.2.m1.1a"><mn id="S6.T4.76.76.76.2.m1.1.1" xref="S6.T4.76.76.76.2.m1.1.1.cmml">88.3</mn><annotation-xml encoding="MathML-Content" id="S6.T4.76.76.76.2.m1.1b"><cn type="float" id="S6.T4.76.76.76.2.m1.1.1.cmml" xref="S6.T4.76.76.76.2.m1.1.1">88.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.76.76.76.2.m1.1c">88.3</annotation></semantics></math></td>
<td id="S6.T4.77.77.77.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.77.77.77.3.m1.1" class="ltx_Math" alttext="83.9" display="inline"><semantics id="S6.T4.77.77.77.3.m1.1a"><mn id="S6.T4.77.77.77.3.m1.1.1" xref="S6.T4.77.77.77.3.m1.1.1.cmml">83.9</mn><annotation-xml encoding="MathML-Content" id="S6.T4.77.77.77.3.m1.1b"><cn type="float" id="S6.T4.77.77.77.3.m1.1.1.cmml" xref="S6.T4.77.77.77.3.m1.1.1">83.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.77.77.77.3.m1.1c">83.9</annotation></semantics></math></td>
<td id="S6.T4.78.78.78.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.78.78.78.4.m1.1" class="ltx_Math" alttext="78.0" display="inline"><semantics id="S6.T4.78.78.78.4.m1.1a"><mn id="S6.T4.78.78.78.4.m1.1.1" xref="S6.T4.78.78.78.4.m1.1.1.cmml">78.0</mn><annotation-xml encoding="MathML-Content" id="S6.T4.78.78.78.4.m1.1b"><cn type="float" id="S6.T4.78.78.78.4.m1.1.1.cmml" xref="S6.T4.78.78.78.4.m1.1.1">78.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.78.78.78.4.m1.1c">78.0</annotation></semantics></math></td>
<td id="S6.T4.79.79.79.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.79.79.79.5.m1.1" class="ltx_Math" alttext="82.4" display="inline"><semantics id="S6.T4.79.79.79.5.m1.1a"><mn id="S6.T4.79.79.79.5.m1.1.1" xref="S6.T4.79.79.79.5.m1.1.1.cmml">82.4</mn><annotation-xml encoding="MathML-Content" id="S6.T4.79.79.79.5.m1.1b"><cn type="float" id="S6.T4.79.79.79.5.m1.1.1.cmml" xref="S6.T4.79.79.79.5.m1.1.1">82.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.79.79.79.5.m1.1c">82.4</annotation></semantics></math></td>
<td id="S6.T4.80.80.80.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.80.80.80.6.m1.1" class="ltx_Math" alttext="80.5" display="inline"><semantics id="S6.T4.80.80.80.6.m1.1a"><mn id="S6.T4.80.80.80.6.m1.1.1" xref="S6.T4.80.80.80.6.m1.1.1.cmml">80.5</mn><annotation-xml encoding="MathML-Content" id="S6.T4.80.80.80.6.m1.1b"><cn type="float" id="S6.T4.80.80.80.6.m1.1.1.cmml" xref="S6.T4.80.80.80.6.m1.1.1">80.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.80.80.80.6.m1.1c">80.5</annotation></semantics></math></td>
<td id="S6.T4.81.81.81.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.81.81.81.7.m1.1" class="ltx_Math" alttext="73.6" display="inline"><semantics id="S6.T4.81.81.81.7.m1.1a"><mn id="S6.T4.81.81.81.7.m1.1.1" xref="S6.T4.81.81.81.7.m1.1.1.cmml">73.6</mn><annotation-xml encoding="MathML-Content" id="S6.T4.81.81.81.7.m1.1b"><cn type="float" id="S6.T4.81.81.81.7.m1.1.1.cmml" xref="S6.T4.81.81.81.7.m1.1.1">73.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.81.81.81.7.m1.1c">73.6</annotation></semantics></math></td>
<td id="S6.T4.82.82.82.8" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.82.82.82.8.m1.1" class="ltx_Math" alttext="81.2" display="inline"><semantics id="S6.T4.82.82.82.8.m1.1a"><mn id="S6.T4.82.82.82.8.m1.1.1" xref="S6.T4.82.82.82.8.m1.1.1.cmml">81.2</mn><annotation-xml encoding="MathML-Content" id="S6.T4.82.82.82.8.m1.1b"><cn type="float" id="S6.T4.82.82.82.8.m1.1.1.cmml" xref="S6.T4.82.82.82.8.m1.1.1">81.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.82.82.82.8.m1.1c">81.2</annotation></semantics></math>
</td>
</tr>
<tr id="S6.T4.90.90.90" class="ltx_tr">
<td id="S6.T4.90.90.90.9" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><span id="S6.T4.90.90.90.9.1" class="ltx_text ltx_font_bold">DCPose<cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib99" title="" class="ltx_ref">2021a</a>)</cite></span></td>
<td id="S6.T4.90.90.90.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">HRNet-W48</td>
<td id="S6.T4.90.90.90.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">Y</td>
<td id="S6.T4.90.90.90.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">COCO</td>
<td id="S6.T4.83.83.83.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.83.83.83.1.m1.1" class="ltx_Math" alttext="\bf 88.0" display="inline"><semantics id="S6.T4.83.83.83.1.m1.1a"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S6.T4.83.83.83.1.m1.1.1" xref="S6.T4.83.83.83.1.m1.1.1.cmml">88.0</mn><annotation-xml encoding="MathML-Content" id="S6.T4.83.83.83.1.m1.1b"><cn type="float" id="S6.T4.83.83.83.1.m1.1.1.cmml" xref="S6.T4.83.83.83.1.m1.1.1">88.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.83.83.83.1.m1.1c">\bf 88.0</annotation></semantics></math></td>
<td id="S6.T4.84.84.84.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.84.84.84.2.m1.1" class="ltx_Math" alttext="\bf 88.7" display="inline"><semantics id="S6.T4.84.84.84.2.m1.1a"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S6.T4.84.84.84.2.m1.1.1" xref="S6.T4.84.84.84.2.m1.1.1.cmml">88.7</mn><annotation-xml encoding="MathML-Content" id="S6.T4.84.84.84.2.m1.1b"><cn type="float" id="S6.T4.84.84.84.2.m1.1.1.cmml" xref="S6.T4.84.84.84.2.m1.1.1">88.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.84.84.84.2.m1.1c">\bf 88.7</annotation></semantics></math></td>
<td id="S6.T4.85.85.85.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.85.85.85.3.m1.1" class="ltx_Math" alttext="\bf 84.1" display="inline"><semantics id="S6.T4.85.85.85.3.m1.1a"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S6.T4.85.85.85.3.m1.1.1" xref="S6.T4.85.85.85.3.m1.1.1.cmml">84.1</mn><annotation-xml encoding="MathML-Content" id="S6.T4.85.85.85.3.m1.1b"><cn type="float" id="S6.T4.85.85.85.3.m1.1.1.cmml" xref="S6.T4.85.85.85.3.m1.1.1">84.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.85.85.85.3.m1.1c">\bf 84.1</annotation></semantics></math></td>
<td id="S6.T4.86.86.86.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.86.86.86.4.m1.1" class="ltx_Math" alttext="\bf 78.4" display="inline"><semantics id="S6.T4.86.86.86.4.m1.1a"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S6.T4.86.86.86.4.m1.1.1" xref="S6.T4.86.86.86.4.m1.1.1.cmml">78.4</mn><annotation-xml encoding="MathML-Content" id="S6.T4.86.86.86.4.m1.1b"><cn type="float" id="S6.T4.86.86.86.4.m1.1.1.cmml" xref="S6.T4.86.86.86.4.m1.1.1">78.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.86.86.86.4.m1.1c">\bf 78.4</annotation></semantics></math></td>
<td id="S6.T4.87.87.87.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.87.87.87.5.m1.1" class="ltx_Math" alttext="\bf 83.0" display="inline"><semantics id="S6.T4.87.87.87.5.m1.1a"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S6.T4.87.87.87.5.m1.1.1" xref="S6.T4.87.87.87.5.m1.1.1.cmml">83.0</mn><annotation-xml encoding="MathML-Content" id="S6.T4.87.87.87.5.m1.1b"><cn type="float" id="S6.T4.87.87.87.5.m1.1.1.cmml" xref="S6.T4.87.87.87.5.m1.1.1">83.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.87.87.87.5.m1.1c">\bf 83.0</annotation></semantics></math></td>
<td id="S6.T4.88.88.88.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.88.88.88.6.m1.1" class="ltx_Math" alttext="\bf 81.4" display="inline"><semantics id="S6.T4.88.88.88.6.m1.1a"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S6.T4.88.88.88.6.m1.1.1" xref="S6.T4.88.88.88.6.m1.1.1.cmml">81.4</mn><annotation-xml encoding="MathML-Content" id="S6.T4.88.88.88.6.m1.1b"><cn type="float" id="S6.T4.88.88.88.6.m1.1.1.cmml" xref="S6.T4.88.88.88.6.m1.1.1">81.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.88.88.88.6.m1.1c">\bf 81.4</annotation></semantics></math></td>
<td id="S6.T4.89.89.89.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.89.89.89.7.m1.1" class="ltx_Math" alttext="\bf 74.2" display="inline"><semantics id="S6.T4.89.89.89.7.m1.1a"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S6.T4.89.89.89.7.m1.1.1" xref="S6.T4.89.89.89.7.m1.1.1.cmml">74.2</mn><annotation-xml encoding="MathML-Content" id="S6.T4.89.89.89.7.m1.1b"><cn type="float" id="S6.T4.89.89.89.7.m1.1.1.cmml" xref="S6.T4.89.89.89.7.m1.1.1">74.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.89.89.89.7.m1.1c">\bf 74.2</annotation></semantics></math></td>
<td id="S6.T4.90.90.90.8" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.90.90.90.8.m1.1" class="ltx_Math" alttext="\bf 82.8" display="inline"><semantics id="S6.T4.90.90.90.8.m1.1a"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S6.T4.90.90.90.8.m1.1.1" xref="S6.T4.90.90.90.8.m1.1.1.cmml">82.8</mn><annotation-xml encoding="MathML-Content" id="S6.T4.90.90.90.8.m1.1b"><cn type="float" id="S6.T4.90.90.90.8.m1.1.1.cmml" xref="S6.T4.90.90.90.8.m1.1.1">82.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.90.90.90.8.m1.1c">\bf 82.8</annotation></semantics></math></td>
</tr>
<tr id="S6.T4.142.142.145.3" class="ltx_tr">
<td id="S6.T4.142.142.145.3.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;" colspan="12">Dataset: <span id="S6.T4.142.142.145.3.1.1" class="ltx_text ltx_font_bold">PoseTrack2017 Test</span> set ( Results from the PoseTrack official leaderboard).</td>
</tr>
<tr id="S6.T4.93.93.93" class="ltx_tr">
<td id="S6.T4.93.93.93.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">PoseTracker<cite class="ltx_cite ltx_citemacro_cite">Girdhar et al. (<a href="#bib.bib41" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="S6.T4.93.93.93.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">ResNet-3D</td>
<td id="S6.T4.93.93.93.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">Y</td>
<td id="S6.T4.93.93.93.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">COCO</td>
<td id="S6.T4.93.93.93.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T4.93.93.93.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T4.93.93.93.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T4.91.91.91.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.91.91.91.1.m1.1" class="ltx_Math" alttext="51.5" display="inline"><semantics id="S6.T4.91.91.91.1.m1.1a"><mn id="S6.T4.91.91.91.1.m1.1.1" xref="S6.T4.91.91.91.1.m1.1.1.cmml">51.5</mn><annotation-xml encoding="MathML-Content" id="S6.T4.91.91.91.1.m1.1b"><cn type="float" id="S6.T4.91.91.91.1.m1.1.1.cmml" xref="S6.T4.91.91.91.1.m1.1.1">51.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.91.91.91.1.m1.1c">51.5</annotation></semantics></math></td>
<td id="S6.T4.93.93.93.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T4.93.93.93.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T4.92.92.92.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.92.92.92.2.m1.1" class="ltx_Math" alttext="50.17" display="inline"><semantics id="S6.T4.92.92.92.2.m1.1a"><mn id="S6.T4.92.92.92.2.m1.1.1" xref="S6.T4.92.92.92.2.m1.1.1.cmml">50.17</mn><annotation-xml encoding="MathML-Content" id="S6.T4.92.92.92.2.m1.1b"><cn type="float" id="S6.T4.92.92.92.2.m1.1.1.cmml" xref="S6.T4.92.92.92.2.m1.1.1">50.17</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.92.92.92.2.m1.1c">50.17</annotation></semantics></math></td>
<td id="S6.T4.93.93.93.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.93.93.93.3.m1.1" class="ltx_Math" alttext="59.6" display="inline"><semantics id="S6.T4.93.93.93.3.m1.1a"><mn id="S6.T4.93.93.93.3.m1.1.1" xref="S6.T4.93.93.93.3.m1.1.1.cmml">59.6</mn><annotation-xml encoding="MathML-Content" id="S6.T4.93.93.93.3.m1.1b"><cn type="float" id="S6.T4.93.93.93.3.m1.1.1.cmml" xref="S6.T4.93.93.93.3.m1.1.1">59.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.93.93.93.3.m1.1c">59.6</annotation></semantics></math></td>
</tr>
<tr id="S6.T4.101.101.101" class="ltx_tr">
<td id="S6.T4.101.101.101.9" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">PoseFlow<cite class="ltx_cite ltx_citemacro_cite">Xiu et al. (<a href="#bib.bib182" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="S6.T4.101.101.101.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T4.101.101.101.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T4.101.101.101.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">MPII Pose + COCO</td>
<td id="S6.T4.94.94.94.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.94.94.94.1.m1.1" class="ltx_Math" alttext="64.9" display="inline"><semantics id="S6.T4.94.94.94.1.m1.1a"><mn id="S6.T4.94.94.94.1.m1.1.1" xref="S6.T4.94.94.94.1.m1.1.1.cmml">64.9</mn><annotation-xml encoding="MathML-Content" id="S6.T4.94.94.94.1.m1.1b"><cn type="float" id="S6.T4.94.94.94.1.m1.1.1.cmml" xref="S6.T4.94.94.94.1.m1.1.1">64.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.94.94.94.1.m1.1c">64.9</annotation></semantics></math></td>
<td id="S6.T4.95.95.95.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.95.95.95.2.m1.1" class="ltx_Math" alttext="67.5" display="inline"><semantics id="S6.T4.95.95.95.2.m1.1a"><mn id="S6.T4.95.95.95.2.m1.1.1" xref="S6.T4.95.95.95.2.m1.1.1.cmml">67.5</mn><annotation-xml encoding="MathML-Content" id="S6.T4.95.95.95.2.m1.1b"><cn type="float" id="S6.T4.95.95.95.2.m1.1.1.cmml" xref="S6.T4.95.95.95.2.m1.1.1">67.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.95.95.95.2.m1.1c">67.5</annotation></semantics></math></td>
<td id="S6.T4.96.96.96.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.96.96.96.3.m1.1" class="ltx_Math" alttext="65.0" display="inline"><semantics id="S6.T4.96.96.96.3.m1.1a"><mn id="S6.T4.96.96.96.3.m1.1.1" xref="S6.T4.96.96.96.3.m1.1.1.cmml">65.0</mn><annotation-xml encoding="MathML-Content" id="S6.T4.96.96.96.3.m1.1b"><cn type="float" id="S6.T4.96.96.96.3.m1.1.1.cmml" xref="S6.T4.96.96.96.3.m1.1.1">65.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.96.96.96.3.m1.1c">65.0</annotation></semantics></math></td>
<td id="S6.T4.97.97.97.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.97.97.97.4.m1.1" class="ltx_Math" alttext="59.0" display="inline"><semantics id="S6.T4.97.97.97.4.m1.1a"><mn id="S6.T4.97.97.97.4.m1.1.1" xref="S6.T4.97.97.97.4.m1.1.1.cmml">59.0</mn><annotation-xml encoding="MathML-Content" id="S6.T4.97.97.97.4.m1.1b"><cn type="float" id="S6.T4.97.97.97.4.m1.1.1.cmml" xref="S6.T4.97.97.97.4.m1.1.1">59.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.97.97.97.4.m1.1c">59.0</annotation></semantics></math></td>
<td id="S6.T4.98.98.98.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.98.98.98.5.m1.1" class="ltx_Math" alttext="62.5" display="inline"><semantics id="S6.T4.98.98.98.5.m1.1a"><mn id="S6.T4.98.98.98.5.m1.1.1" xref="S6.T4.98.98.98.5.m1.1.1.cmml">62.5</mn><annotation-xml encoding="MathML-Content" id="S6.T4.98.98.98.5.m1.1b"><cn type="float" id="S6.T4.98.98.98.5.m1.1.1.cmml" xref="S6.T4.98.98.98.5.m1.1.1">62.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.98.98.98.5.m1.1c">62.5</annotation></semantics></math></td>
<td id="S6.T4.99.99.99.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.99.99.99.6.m1.1" class="ltx_Math" alttext="62.8" display="inline"><semantics id="S6.T4.99.99.99.6.m1.1a"><mn id="S6.T4.99.99.99.6.m1.1.1" xref="S6.T4.99.99.99.6.m1.1.1.cmml">62.8</mn><annotation-xml encoding="MathML-Content" id="S6.T4.99.99.99.6.m1.1b"><cn type="float" id="S6.T4.99.99.99.6.m1.1.1.cmml" xref="S6.T4.99.99.99.6.m1.1.1">62.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.99.99.99.6.m1.1c">62.8</annotation></semantics></math></td>
<td id="S6.T4.100.100.100.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.100.100.100.7.m1.1" class="ltx_Math" alttext="57.9" display="inline"><semantics id="S6.T4.100.100.100.7.m1.1a"><mn id="S6.T4.100.100.100.7.m1.1.1" xref="S6.T4.100.100.100.7.m1.1.1.cmml">57.9</mn><annotation-xml encoding="MathML-Content" id="S6.T4.100.100.100.7.m1.1b"><cn type="float" id="S6.T4.100.100.100.7.m1.1.1.cmml" xref="S6.T4.100.100.100.7.m1.1.1">57.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.100.100.100.7.m1.1c">57.9</annotation></semantics></math></td>
<td id="S6.T4.101.101.101.8" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.101.101.101.8.m1.1" class="ltx_Math" alttext="63.0" display="inline"><semantics id="S6.T4.101.101.101.8.m1.1a"><mn id="S6.T4.101.101.101.8.m1.1.1" xref="S6.T4.101.101.101.8.m1.1.1.cmml">63.0</mn><annotation-xml encoding="MathML-Content" id="S6.T4.101.101.101.8.m1.1b"><cn type="float" id="S6.T4.101.101.101.8.m1.1.1.cmml" xref="S6.T4.101.101.101.8.m1.1.1">63.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.101.101.101.8.m1.1c">63.0</annotation></semantics></math></td>
</tr>
<tr id="S6.T4.104.104.104" class="ltx_tr">
<td id="S6.T4.104.104.104.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">JointFlow<cite class="ltx_cite ltx_citemacro_cite">Doering et al. (<a href="#bib.bib24" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="S6.T4.104.104.104.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T4.104.104.104.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T4.104.104.104.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T4.104.104.104.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T4.104.104.104.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T4.104.104.104.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T4.102.102.102.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.102.102.102.1.m1.1" class="ltx_Math" alttext="53.1" display="inline"><semantics id="S6.T4.102.102.102.1.m1.1a"><mn id="S6.T4.102.102.102.1.m1.1.1" xref="S6.T4.102.102.102.1.m1.1.1.cmml">53.1</mn><annotation-xml encoding="MathML-Content" id="S6.T4.102.102.102.1.m1.1b"><cn type="float" id="S6.T4.102.102.102.1.m1.1.1.cmml" xref="S6.T4.102.102.102.1.m1.1.1">53.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.102.102.102.1.m1.1c">53.1</annotation></semantics></math></td>
<td id="S6.T4.104.104.104.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T4.104.104.104.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T4.103.103.103.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.103.103.103.2.m1.1" class="ltx_Math" alttext="50.4" display="inline"><semantics id="S6.T4.103.103.103.2.m1.1a"><mn id="S6.T4.103.103.103.2.m1.1.1" xref="S6.T4.103.103.103.2.m1.1.1.cmml">50.4</mn><annotation-xml encoding="MathML-Content" id="S6.T4.103.103.103.2.m1.1b"><cn type="float" id="S6.T4.103.103.103.2.m1.1.1.cmml" xref="S6.T4.103.103.103.2.m1.1.1">50.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.103.103.103.2.m1.1c">50.4</annotation></semantics></math></td>
<td id="S6.T4.104.104.104.3" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.104.104.104.3.m1.1" class="ltx_Math" alttext="63.4" display="inline"><semantics id="S6.T4.104.104.104.3.m1.1a"><mn id="S6.T4.104.104.104.3.m1.1.1" xref="S6.T4.104.104.104.3.m1.1.1.cmml">63.4</mn><annotation-xml encoding="MathML-Content" id="S6.T4.104.104.104.3.m1.1b"><cn type="float" id="S6.T4.104.104.104.3.m1.1.1.cmml" xref="S6.T4.104.104.104.3.m1.1.1">63.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.104.104.104.3.m1.1c">63.4</annotation></semantics></math></td>
</tr>
<tr id="S6.T4.107.107.107" class="ltx_tr">
<td id="S6.T4.107.107.107.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">KeyTrack<cite class="ltx_cite ltx_citemacro_cite">Snower et al. (<a href="#bib.bib150" title="" class="ltx_ref">2020</a>)</cite>
</td>
<td id="S6.T4.107.107.107.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T4.107.107.107.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T4.107.107.107.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">COCO</td>
<td id="S6.T4.107.107.107.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T4.107.107.107.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T4.107.107.107.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T4.105.105.105.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.105.105.105.1.m1.1" class="ltx_Math" alttext="71.9" display="inline"><semantics id="S6.T4.105.105.105.1.m1.1a"><mn id="S6.T4.105.105.105.1.m1.1.1" xref="S6.T4.105.105.105.1.m1.1.1.cmml">71.9</mn><annotation-xml encoding="MathML-Content" id="S6.T4.105.105.105.1.m1.1b"><cn type="float" id="S6.T4.105.105.105.1.m1.1.1.cmml" xref="S6.T4.105.105.105.1.m1.1.1">71.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.105.105.105.1.m1.1c">71.9</annotation></semantics></math></td>
<td id="S6.T4.107.107.107.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T4.107.107.107.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T4.106.106.106.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.106.106.106.2.m1.1" class="ltx_Math" alttext="65.0" display="inline"><semantics id="S6.T4.106.106.106.2.m1.1a"><mn id="S6.T4.106.106.106.2.m1.1.1" xref="S6.T4.106.106.106.2.m1.1.1.cmml">65.0</mn><annotation-xml encoding="MathML-Content" id="S6.T4.106.106.106.2.m1.1b"><cn type="float" id="S6.T4.106.106.106.2.m1.1.1.cmml" xref="S6.T4.106.106.106.2.m1.1.1">65.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.106.106.106.2.m1.1c">65.0</annotation></semantics></math></td>
<td id="S6.T4.107.107.107.3" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.107.107.107.3.m1.1" class="ltx_Math" alttext="74.0" display="inline"><semantics id="S6.T4.107.107.107.3.m1.1a"><mn id="S6.T4.107.107.107.3.m1.1.1" xref="S6.T4.107.107.107.3.m1.1.1.cmml">74.0</mn><annotation-xml encoding="MathML-Content" id="S6.T4.107.107.107.3.m1.1b"><cn type="float" id="S6.T4.107.107.107.3.m1.1.1.cmml" xref="S6.T4.107.107.107.3.m1.1.1">74.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.107.107.107.3.m1.1c">74.0</annotation></semantics></math></td>
</tr>
<tr id="S6.T4.110.110.110" class="ltx_tr">
<td id="S6.T4.110.110.110.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">DetTrack<cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib172" title="" class="ltx_ref">2020b</a>)</cite>
</td>
<td id="S6.T4.110.110.110.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">3D-HRNet</td>
<td id="S6.T4.110.110.110.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">Y</td>
<td id="S6.T4.110.110.110.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">COCO</td>
<td id="S6.T4.110.110.110.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T4.110.110.110.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T4.110.110.110.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T4.108.108.108.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.108.108.108.1.m1.1" class="ltx_Math" alttext="69.8" display="inline"><semantics id="S6.T4.108.108.108.1.m1.1a"><mn id="S6.T4.108.108.108.1.m1.1.1" xref="S6.T4.108.108.108.1.m1.1.1.cmml">69.8</mn><annotation-xml encoding="MathML-Content" id="S6.T4.108.108.108.1.m1.1b"><cn type="float" id="S6.T4.108.108.108.1.m1.1.1.cmml" xref="S6.T4.108.108.108.1.m1.1.1">69.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.108.108.108.1.m1.1c">69.8</annotation></semantics></math></td>
<td id="S6.T4.110.110.110.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T4.110.110.110.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">-</td>
<td id="S6.T4.109.109.109.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.109.109.109.2.m1.1" class="ltx_Math" alttext="65.9" display="inline"><semantics id="S6.T4.109.109.109.2.m1.1a"><mn id="S6.T4.109.109.109.2.m1.1.1" xref="S6.T4.109.109.109.2.m1.1.1.cmml">65.9</mn><annotation-xml encoding="MathML-Content" id="S6.T4.109.109.109.2.m1.1b"><cn type="float" id="S6.T4.109.109.109.2.m1.1.1.cmml" xref="S6.T4.109.109.109.2.m1.1.1">65.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.109.109.109.2.m1.1c">65.9</annotation></semantics></math></td>
<td id="S6.T4.110.110.110.3" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.110.110.110.3.m1.1" class="ltx_Math" alttext="74.1" display="inline"><semantics id="S6.T4.110.110.110.3.m1.1a"><mn id="S6.T4.110.110.110.3.m1.1.1" xref="S6.T4.110.110.110.3.m1.1.1.cmml">74.1</mn><annotation-xml encoding="MathML-Content" id="S6.T4.110.110.110.3.m1.1b"><cn type="float" id="S6.T4.110.110.110.3.m1.1.1.cmml" xref="S6.T4.110.110.110.3.m1.1.1">74.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.110.110.110.3.m1.1c">74.1</annotation></semantics></math></td>
</tr>
<tr id="S6.T4.118.118.118" class="ltx_tr">
<td id="S6.T4.118.118.118.9" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">SimpleBaseline<cite class="ltx_cite ltx_citemacro_cite">Xiao et al. (<a href="#bib.bib181" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="S6.T4.118.118.118.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">ResNet-152</td>
<td id="S6.T4.118.118.118.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">N</td>
<td id="S6.T4.118.118.118.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">COCO</td>
<td id="S6.T4.111.111.111.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.111.111.111.1.m1.1" class="ltx_Math" alttext="80.1" display="inline"><semantics id="S6.T4.111.111.111.1.m1.1a"><mn id="S6.T4.111.111.111.1.m1.1.1" xref="S6.T4.111.111.111.1.m1.1.1.cmml">80.1</mn><annotation-xml encoding="MathML-Content" id="S6.T4.111.111.111.1.m1.1b"><cn type="float" id="S6.T4.111.111.111.1.m1.1.1.cmml" xref="S6.T4.111.111.111.1.m1.1.1">80.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.111.111.111.1.m1.1c">80.1</annotation></semantics></math></td>
<td id="S6.T4.112.112.112.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.112.112.112.2.m1.1" class="ltx_Math" alttext="80.2" display="inline"><semantics id="S6.T4.112.112.112.2.m1.1a"><mn id="S6.T4.112.112.112.2.m1.1.1" xref="S6.T4.112.112.112.2.m1.1.1.cmml">80.2</mn><annotation-xml encoding="MathML-Content" id="S6.T4.112.112.112.2.m1.1b"><cn type="float" id="S6.T4.112.112.112.2.m1.1.1.cmml" xref="S6.T4.112.112.112.2.m1.1.1">80.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.112.112.112.2.m1.1c">80.2</annotation></semantics></math></td>
<td id="S6.T4.113.113.113.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.113.113.113.3.m1.1" class="ltx_Math" alttext="76.9" display="inline"><semantics id="S6.T4.113.113.113.3.m1.1a"><mn id="S6.T4.113.113.113.3.m1.1.1" xref="S6.T4.113.113.113.3.m1.1.1.cmml">76.9</mn><annotation-xml encoding="MathML-Content" id="S6.T4.113.113.113.3.m1.1b"><cn type="float" id="S6.T4.113.113.113.3.m1.1.1.cmml" xref="S6.T4.113.113.113.3.m1.1.1">76.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.113.113.113.3.m1.1c">76.9</annotation></semantics></math></td>
<td id="S6.T4.114.114.114.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.114.114.114.4.m1.1" class="ltx_Math" alttext="71.5" display="inline"><semantics id="S6.T4.114.114.114.4.m1.1a"><mn id="S6.T4.114.114.114.4.m1.1.1" xref="S6.T4.114.114.114.4.m1.1.1.cmml">71.5</mn><annotation-xml encoding="MathML-Content" id="S6.T4.114.114.114.4.m1.1b"><cn type="float" id="S6.T4.114.114.114.4.m1.1.1.cmml" xref="S6.T4.114.114.114.4.m1.1.1">71.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.114.114.114.4.m1.1c">71.5</annotation></semantics></math></td>
<td id="S6.T4.115.115.115.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.115.115.115.5.m1.1" class="ltx_Math" alttext="72.5" display="inline"><semantics id="S6.T4.115.115.115.5.m1.1a"><mn id="S6.T4.115.115.115.5.m1.1.1" xref="S6.T4.115.115.115.5.m1.1.1.cmml">72.5</mn><annotation-xml encoding="MathML-Content" id="S6.T4.115.115.115.5.m1.1b"><cn type="float" id="S6.T4.115.115.115.5.m1.1.1.cmml" xref="S6.T4.115.115.115.5.m1.1.1">72.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.115.115.115.5.m1.1c">72.5</annotation></semantics></math></td>
<td id="S6.T4.116.116.116.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.116.116.116.6.m1.1" class="ltx_Math" alttext="72.4" display="inline"><semantics id="S6.T4.116.116.116.6.m1.1a"><mn id="S6.T4.116.116.116.6.m1.1.1" xref="S6.T4.116.116.116.6.m1.1.1.cmml">72.4</mn><annotation-xml encoding="MathML-Content" id="S6.T4.116.116.116.6.m1.1b"><cn type="float" id="S6.T4.116.116.116.6.m1.1.1.cmml" xref="S6.T4.116.116.116.6.m1.1.1">72.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.116.116.116.6.m1.1c">72.4</annotation></semantics></math></td>
<td id="S6.T4.117.117.117.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.117.117.117.7.m1.1" class="ltx_Math" alttext="65.7" display="inline"><semantics id="S6.T4.117.117.117.7.m1.1a"><mn id="S6.T4.117.117.117.7.m1.1.1" xref="S6.T4.117.117.117.7.m1.1.1.cmml">65.7</mn><annotation-xml encoding="MathML-Content" id="S6.T4.117.117.117.7.m1.1b"><cn type="float" id="S6.T4.117.117.117.7.m1.1.1.cmml" xref="S6.T4.117.117.117.7.m1.1.1">65.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.117.117.117.7.m1.1c">65.7</annotation></semantics></math></td>
<td id="S6.T4.118.118.118.8" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.118.118.118.8.m1.1" class="ltx_Math" alttext="74.6" display="inline"><semantics id="S6.T4.118.118.118.8.m1.1a"><mn id="S6.T4.118.118.118.8.m1.1.1" xref="S6.T4.118.118.118.8.m1.1.1.cmml">74.6</mn><annotation-xml encoding="MathML-Content" id="S6.T4.118.118.118.8.m1.1b"><cn type="float" id="S6.T4.118.118.118.8.m1.1.1.cmml" xref="S6.T4.118.118.118.8.m1.1.1">74.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.118.118.118.8.m1.1c">74.6</annotation></semantics></math></td>
</tr>
<tr id="S6.T4.126.126.126" class="ltx_tr">
<td id="S6.T4.126.126.126.9" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">HRNet<cite class="ltx_cite ltx_citemacro_cite">Sun et al. (<a href="#bib.bib153" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="S6.T4.126.126.126.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">HRNet-W48</td>
<td id="S6.T4.126.126.126.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">Y</td>
<td id="S6.T4.126.126.126.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">COCO</td>
<td id="S6.T4.119.119.119.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.119.119.119.1.m1.1" class="ltx_Math" alttext="80.1" display="inline"><semantics id="S6.T4.119.119.119.1.m1.1a"><mn id="S6.T4.119.119.119.1.m1.1.1" xref="S6.T4.119.119.119.1.m1.1.1.cmml">80.1</mn><annotation-xml encoding="MathML-Content" id="S6.T4.119.119.119.1.m1.1b"><cn type="float" id="S6.T4.119.119.119.1.m1.1.1.cmml" xref="S6.T4.119.119.119.1.m1.1.1">80.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.119.119.119.1.m1.1c">80.1</annotation></semantics></math></td>
<td id="S6.T4.120.120.120.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.120.120.120.2.m1.1" class="ltx_Math" alttext="80.2" display="inline"><semantics id="S6.T4.120.120.120.2.m1.1a"><mn id="S6.T4.120.120.120.2.m1.1.1" xref="S6.T4.120.120.120.2.m1.1.1.cmml">80.2</mn><annotation-xml encoding="MathML-Content" id="S6.T4.120.120.120.2.m1.1b"><cn type="float" id="S6.T4.120.120.120.2.m1.1.1.cmml" xref="S6.T4.120.120.120.2.m1.1.1">80.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.120.120.120.2.m1.1c">80.2</annotation></semantics></math></td>
<td id="S6.T4.121.121.121.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.121.121.121.3.m1.1" class="ltx_Math" alttext="76.9" display="inline"><semantics id="S6.T4.121.121.121.3.m1.1a"><mn id="S6.T4.121.121.121.3.m1.1.1" xref="S6.T4.121.121.121.3.m1.1.1.cmml">76.9</mn><annotation-xml encoding="MathML-Content" id="S6.T4.121.121.121.3.m1.1b"><cn type="float" id="S6.T4.121.121.121.3.m1.1.1.cmml" xref="S6.T4.121.121.121.3.m1.1.1">76.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.121.121.121.3.m1.1c">76.9</annotation></semantics></math></td>
<td id="S6.T4.122.122.122.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.122.122.122.4.m1.1" class="ltx_Math" alttext="72.0" display="inline"><semantics id="S6.T4.122.122.122.4.m1.1a"><mn id="S6.T4.122.122.122.4.m1.1.1" xref="S6.T4.122.122.122.4.m1.1.1.cmml">72.0</mn><annotation-xml encoding="MathML-Content" id="S6.T4.122.122.122.4.m1.1b"><cn type="float" id="S6.T4.122.122.122.4.m1.1.1.cmml" xref="S6.T4.122.122.122.4.m1.1.1">72.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.122.122.122.4.m1.1c">72.0</annotation></semantics></math></td>
<td id="S6.T4.123.123.123.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.123.123.123.5.m1.1" class="ltx_Math" alttext="73.4" display="inline"><semantics id="S6.T4.123.123.123.5.m1.1a"><mn id="S6.T4.123.123.123.5.m1.1.1" xref="S6.T4.123.123.123.5.m1.1.1.cmml">73.4</mn><annotation-xml encoding="MathML-Content" id="S6.T4.123.123.123.5.m1.1b"><cn type="float" id="S6.T4.123.123.123.5.m1.1.1.cmml" xref="S6.T4.123.123.123.5.m1.1.1">73.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.123.123.123.5.m1.1c">73.4</annotation></semantics></math></td>
<td id="S6.T4.124.124.124.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.124.124.124.6.m1.1" class="ltx_Math" alttext="72.5" display="inline"><semantics id="S6.T4.124.124.124.6.m1.1a"><mn id="S6.T4.124.124.124.6.m1.1.1" xref="S6.T4.124.124.124.6.m1.1.1.cmml">72.5</mn><annotation-xml encoding="MathML-Content" id="S6.T4.124.124.124.6.m1.1b"><cn type="float" id="S6.T4.124.124.124.6.m1.1.1.cmml" xref="S6.T4.124.124.124.6.m1.1.1">72.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.124.124.124.6.m1.1c">72.5</annotation></semantics></math></td>
<td id="S6.T4.125.125.125.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.125.125.125.7.m1.1" class="ltx_Math" alttext="67.0" display="inline"><semantics id="S6.T4.125.125.125.7.m1.1a"><mn id="S6.T4.125.125.125.7.m1.1.1" xref="S6.T4.125.125.125.7.m1.1.1.cmml">67.0</mn><annotation-xml encoding="MathML-Content" id="S6.T4.125.125.125.7.m1.1b"><cn type="float" id="S6.T4.125.125.125.7.m1.1.1.cmml" xref="S6.T4.125.125.125.7.m1.1.1">67.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.125.125.125.7.m1.1c">67.0</annotation></semantics></math></td>
<td id="S6.T4.126.126.126.8" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.126.126.126.8.m1.1" class="ltx_Math" alttext="74.9" display="inline"><semantics id="S6.T4.126.126.126.8.m1.1a"><mn id="S6.T4.126.126.126.8.m1.1.1" xref="S6.T4.126.126.126.8.m1.1.1.cmml">74.9</mn><annotation-xml encoding="MathML-Content" id="S6.T4.126.126.126.8.m1.1b"><cn type="float" id="S6.T4.126.126.126.8.m1.1.1.cmml" xref="S6.T4.126.126.126.8.m1.1.1">74.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.126.126.126.8.m1.1c">74.9</annotation></semantics></math></td>
</tr>
<tr id="S6.T4.134.134.134" class="ltx_tr">
<td id="S6.T4.134.134.134.9" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">PoseWarper<cite class="ltx_cite ltx_citemacro_cite">Bertasius et al. (<a href="#bib.bib5" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="S6.T4.134.134.134.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">HRNet-W48</td>
<td id="S6.T4.134.134.134.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">Y</td>
<td id="S6.T4.134.134.134.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">COCO</td>
<td id="S6.T4.127.127.127.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.127.127.127.1.m1.1" class="ltx_Math" alttext="79.5" display="inline"><semantics id="S6.T4.127.127.127.1.m1.1a"><mn id="S6.T4.127.127.127.1.m1.1.1" xref="S6.T4.127.127.127.1.m1.1.1.cmml">79.5</mn><annotation-xml encoding="MathML-Content" id="S6.T4.127.127.127.1.m1.1b"><cn type="float" id="S6.T4.127.127.127.1.m1.1.1.cmml" xref="S6.T4.127.127.127.1.m1.1.1">79.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.127.127.127.1.m1.1c">79.5</annotation></semantics></math></td>
<td id="S6.T4.128.128.128.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.128.128.128.2.m1.1" class="ltx_Math" alttext="84.3" display="inline"><semantics id="S6.T4.128.128.128.2.m1.1a"><mn id="S6.T4.128.128.128.2.m1.1.1" xref="S6.T4.128.128.128.2.m1.1.1.cmml">84.3</mn><annotation-xml encoding="MathML-Content" id="S6.T4.128.128.128.2.m1.1b"><cn type="float" id="S6.T4.128.128.128.2.m1.1.1.cmml" xref="S6.T4.128.128.128.2.m1.1.1">84.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.128.128.128.2.m1.1c">84.3</annotation></semantics></math></td>
<td id="S6.T4.129.129.129.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.129.129.129.3.m1.1" class="ltx_Math" alttext="80.1" display="inline"><semantics id="S6.T4.129.129.129.3.m1.1a"><mn id="S6.T4.129.129.129.3.m1.1.1" xref="S6.T4.129.129.129.3.m1.1.1.cmml">80.1</mn><annotation-xml encoding="MathML-Content" id="S6.T4.129.129.129.3.m1.1b"><cn type="float" id="S6.T4.129.129.129.3.m1.1.1.cmml" xref="S6.T4.129.129.129.3.m1.1.1">80.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.129.129.129.3.m1.1c">80.1</annotation></semantics></math></td>
<td id="S6.T4.130.130.130.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.130.130.130.4.m1.1" class="ltx_Math" alttext="75.8" display="inline"><semantics id="S6.T4.130.130.130.4.m1.1a"><mn id="S6.T4.130.130.130.4.m1.1.1" xref="S6.T4.130.130.130.4.m1.1.1.cmml">75.8</mn><annotation-xml encoding="MathML-Content" id="S6.T4.130.130.130.4.m1.1b"><cn type="float" id="S6.T4.130.130.130.4.m1.1.1.cmml" xref="S6.T4.130.130.130.4.m1.1.1">75.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.130.130.130.4.m1.1c">75.8</annotation></semantics></math></td>
<td id="S6.T4.131.131.131.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.131.131.131.5.m1.1" class="ltx_Math" alttext="77.6" display="inline"><semantics id="S6.T4.131.131.131.5.m1.1a"><mn id="S6.T4.131.131.131.5.m1.1.1" xref="S6.T4.131.131.131.5.m1.1.1.cmml">77.6</mn><annotation-xml encoding="MathML-Content" id="S6.T4.131.131.131.5.m1.1b"><cn type="float" id="S6.T4.131.131.131.5.m1.1.1.cmml" xref="S6.T4.131.131.131.5.m1.1.1">77.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.131.131.131.5.m1.1c">77.6</annotation></semantics></math></td>
<td id="S6.T4.132.132.132.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.132.132.132.6.m1.1" class="ltx_Math" alttext="76.8" display="inline"><semantics id="S6.T4.132.132.132.6.m1.1a"><mn id="S6.T4.132.132.132.6.m1.1.1" xref="S6.T4.132.132.132.6.m1.1.1.cmml">76.8</mn><annotation-xml encoding="MathML-Content" id="S6.T4.132.132.132.6.m1.1b"><cn type="float" id="S6.T4.132.132.132.6.m1.1.1.cmml" xref="S6.T4.132.132.132.6.m1.1.1">76.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.132.132.132.6.m1.1c">76.8</annotation></semantics></math></td>
<td id="S6.T4.133.133.133.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.133.133.133.7.m1.1" class="ltx_Math" alttext="70.8" display="inline"><semantics id="S6.T4.133.133.133.7.m1.1a"><mn id="S6.T4.133.133.133.7.m1.1.1" xref="S6.T4.133.133.133.7.m1.1.1.cmml">70.8</mn><annotation-xml encoding="MathML-Content" id="S6.T4.133.133.133.7.m1.1b"><cn type="float" id="S6.T4.133.133.133.7.m1.1.1.cmml" xref="S6.T4.133.133.133.7.m1.1.1">70.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.133.133.133.7.m1.1c">70.8</annotation></semantics></math></td>
<td id="S6.T4.134.134.134.8" class="ltx_td ltx_align_center" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.134.134.134.8.m1.1" class="ltx_Math" alttext="77.9" display="inline"><semantics id="S6.T4.134.134.134.8.m1.1a"><mn id="S6.T4.134.134.134.8.m1.1.1" xref="S6.T4.134.134.134.8.m1.1.1.cmml">77.9</mn><annotation-xml encoding="MathML-Content" id="S6.T4.134.134.134.8.m1.1b"><cn type="float" id="S6.T4.134.134.134.8.m1.1.1.cmml" xref="S6.T4.134.134.134.8.m1.1.1">77.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.134.134.134.8.m1.1c">77.9</annotation></semantics></math></td>
</tr>
<tr id="S6.T4.142.142.142" class="ltx_tr">
<td id="S6.T4.142.142.142.9" class="ltx_td ltx_align_left ltx_border_b ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><span id="S6.T4.142.142.142.9.1" class="ltx_text ltx_font_bold">DCPose<cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib99" title="" class="ltx_ref">2021a</a>)</cite></span></td>
<td id="S6.T4.142.142.142.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">HRNet-W48</td>
<td id="S6.T4.142.142.142.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">Y</td>
<td id="S6.T4.142.142.142.12" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;">COCO</td>
<td id="S6.T4.135.135.135.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.135.135.135.1.m1.1" class="ltx_Math" alttext="\bf 84.3" display="inline"><semantics id="S6.T4.135.135.135.1.m1.1a"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S6.T4.135.135.135.1.m1.1.1" xref="S6.T4.135.135.135.1.m1.1.1.cmml">84.3</mn><annotation-xml encoding="MathML-Content" id="S6.T4.135.135.135.1.m1.1b"><cn type="float" id="S6.T4.135.135.135.1.m1.1.1.cmml" xref="S6.T4.135.135.135.1.m1.1.1">84.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.135.135.135.1.m1.1c">\bf 84.3</annotation></semantics></math></td>
<td id="S6.T4.136.136.136.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.136.136.136.2.m1.1" class="ltx_Math" alttext="\bf 84.9" display="inline"><semantics id="S6.T4.136.136.136.2.m1.1a"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S6.T4.136.136.136.2.m1.1.1" xref="S6.T4.136.136.136.2.m1.1.1.cmml">84.9</mn><annotation-xml encoding="MathML-Content" id="S6.T4.136.136.136.2.m1.1b"><cn type="float" id="S6.T4.136.136.136.2.m1.1.1.cmml" xref="S6.T4.136.136.136.2.m1.1.1">84.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.136.136.136.2.m1.1c">\bf 84.9</annotation></semantics></math></td>
<td id="S6.T4.137.137.137.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.137.137.137.3.m1.1" class="ltx_Math" alttext="\bf 80.5" display="inline"><semantics id="S6.T4.137.137.137.3.m1.1a"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S6.T4.137.137.137.3.m1.1.1" xref="S6.T4.137.137.137.3.m1.1.1.cmml">80.5</mn><annotation-xml encoding="MathML-Content" id="S6.T4.137.137.137.3.m1.1b"><cn type="float" id="S6.T4.137.137.137.3.m1.1.1.cmml" xref="S6.T4.137.137.137.3.m1.1.1">80.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.137.137.137.3.m1.1c">\bf 80.5</annotation></semantics></math></td>
<td id="S6.T4.138.138.138.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.138.138.138.4.m1.1" class="ltx_Math" alttext="\bf 76.1" display="inline"><semantics id="S6.T4.138.138.138.4.m1.1a"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S6.T4.138.138.138.4.m1.1.1" xref="S6.T4.138.138.138.4.m1.1.1.cmml">76.1</mn><annotation-xml encoding="MathML-Content" id="S6.T4.138.138.138.4.m1.1b"><cn type="float" id="S6.T4.138.138.138.4.m1.1.1.cmml" xref="S6.T4.138.138.138.4.m1.1.1">76.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.138.138.138.4.m1.1c">\bf 76.1</annotation></semantics></math></td>
<td id="S6.T4.139.139.139.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.139.139.139.5.m1.1" class="ltx_Math" alttext="\bf 77.9" display="inline"><semantics id="S6.T4.139.139.139.5.m1.1a"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S6.T4.139.139.139.5.m1.1.1" xref="S6.T4.139.139.139.5.m1.1.1.cmml">77.9</mn><annotation-xml encoding="MathML-Content" id="S6.T4.139.139.139.5.m1.1b"><cn type="float" id="S6.T4.139.139.139.5.m1.1.1.cmml" xref="S6.T4.139.139.139.5.m1.1.1">77.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.139.139.139.5.m1.1c">\bf 77.9</annotation></semantics></math></td>
<td id="S6.T4.140.140.140.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.140.140.140.6.m1.1" class="ltx_Math" alttext="\bf 77.1" display="inline"><semantics id="S6.T4.140.140.140.6.m1.1a"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S6.T4.140.140.140.6.m1.1.1" xref="S6.T4.140.140.140.6.m1.1.1.cmml">77.1</mn><annotation-xml encoding="MathML-Content" id="S6.T4.140.140.140.6.m1.1b"><cn type="float" id="S6.T4.140.140.140.6.m1.1.1.cmml" xref="S6.T4.140.140.140.6.m1.1.1">77.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.140.140.140.6.m1.1c">\bf 77.1</annotation></semantics></math></td>
<td id="S6.T4.141.141.141.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.141.141.141.7.m1.1" class="ltx_Math" alttext="\bf 71.2" display="inline"><semantics id="S6.T4.141.141.141.7.m1.1a"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S6.T4.141.141.141.7.m1.1.1" xref="S6.T4.141.141.141.7.m1.1.1.cmml">71.2</mn><annotation-xml encoding="MathML-Content" id="S6.T4.141.141.141.7.m1.1b"><cn type="float" id="S6.T4.141.141.141.7.m1.1.1.cmml" xref="S6.T4.141.141.141.7.m1.1.1">71.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.141.141.141.7.m1.1c">\bf 71.2</annotation></semantics></math></td>
<td id="S6.T4.142.142.142.8" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:3.5pt;padding-bottom:3.5pt;"><math id="S6.T4.142.142.142.8.m1.1" class="ltx_Math" alttext="\bf 79.2" display="inline"><semantics id="S6.T4.142.142.142.8.m1.1a"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S6.T4.142.142.142.8.m1.1.1" xref="S6.T4.142.142.142.8.m1.1.1.cmml">79.2</mn><annotation-xml encoding="MathML-Content" id="S6.T4.142.142.142.8.m1.1b"><cn type="float" id="S6.T4.142.142.142.8.m1.1.1.cmml" xref="S6.T4.142.142.142.8.m1.1.1">79.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.142.142.142.8.m1.1c">\bf 79.2</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Benchmark Datasets</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">Prior to the flourishing of deep learning, there are plenty of human pose datasets for specific task scenarios, including upper body pose datasets <cite class="ltx_cite ltx_citemacro_cite">Marin-Jimenez et al. (<a href="#bib.bib110" title="" class="ltx_ref">2014</a>); Eichner et al. (<a href="#bib.bib30" title="" class="ltx_ref">2009</a>); Everingham et al. (<a href="#bib.bib32" title="" class="ltx_ref">2010</a>); Eichner and Ferrari (<a href="#bib.bib28" title="" class="ltx_ref">2010</a>); Sapp et al. (<a href="#bib.bib144" title="" class="ltx_ref">2011</a>); Eichner and Ferrari (<a href="#bib.bib29" title="" class="ltx_ref">2012</a>)</cite> and full-body pose dataset <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib175" title="" class="ltx_ref">2011</a>); Li and Fei-Fei (<a href="#bib.bib88" title="" class="ltx_ref">2007</a>); Andriluka et al. (<a href="#bib.bib1" title="" class="ltx_ref">2014</a>); Gong et al. (<a href="#bib.bib45" title="" class="ltx_ref">2016</a>)</cite>.
In this section, we investigate the datasets that are commonly used for deep learning, as summarized in Table <a href="#S6.T2" title="Table 2 ‣ 6 Datasets and Evaluation ‣ 2D Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The corresponding pose annotations are depicted in Fig. <a href="#S6.F5" title="Figure 5 ‣ 6 Datasets and Evaluation ‣ 2D Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p id="S6.SS1.p2.3" class="ltx_p"><span id="S6.SS1.p2.3.1" class="ltx_text ltx_font_bold">Leeds Sports Pose (LSP) Dataset</span> The LSP dataset contains a total number of <math id="S6.SS1.p2.1.m1.2" class="ltx_Math" alttext="2,000" display="inline"><semantics id="S6.SS1.p2.1.m1.2a"><mrow id="S6.SS1.p2.1.m1.2.3.2" xref="S6.SS1.p2.1.m1.2.3.1.cmml"><mn id="S6.SS1.p2.1.m1.1.1" xref="S6.SS1.p2.1.m1.1.1.cmml">2</mn><mo id="S6.SS1.p2.1.m1.2.3.2.1" xref="S6.SS1.p2.1.m1.2.3.1.cmml">,</mo><mn id="S6.SS1.p2.1.m1.2.2" xref="S6.SS1.p2.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.1.m1.2b"><list id="S6.SS1.p2.1.m1.2.3.1.cmml" xref="S6.SS1.p2.1.m1.2.3.2"><cn type="integer" id="S6.SS1.p2.1.m1.1.1.cmml" xref="S6.SS1.p2.1.m1.1.1">2</cn><cn type="integer" id="S6.SS1.p2.1.m1.2.2.cmml" xref="S6.SS1.p2.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.1.m1.2c">2,000</annotation></semantics></math> images of full body poses (including 14 joints), <math id="S6.SS1.p2.2.m2.2" class="ltx_Math" alttext="1,000" display="inline"><semantics id="S6.SS1.p2.2.m2.2a"><mrow id="S6.SS1.p2.2.m2.2.3.2" xref="S6.SS1.p2.2.m2.2.3.1.cmml"><mn id="S6.SS1.p2.2.m2.1.1" xref="S6.SS1.p2.2.m2.1.1.cmml">1</mn><mo id="S6.SS1.p2.2.m2.2.3.2.1" xref="S6.SS1.p2.2.m2.2.3.1.cmml">,</mo><mn id="S6.SS1.p2.2.m2.2.2" xref="S6.SS1.p2.2.m2.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.2.m2.2b"><list id="S6.SS1.p2.2.m2.2.3.1.cmml" xref="S6.SS1.p2.2.m2.2.3.2"><cn type="integer" id="S6.SS1.p2.2.m2.1.1.cmml" xref="S6.SS1.p2.2.m2.1.1">1</cn><cn type="integer" id="S6.SS1.p2.2.m2.2.2.cmml" xref="S6.SS1.p2.2.m2.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.2.m2.2c">1,000</annotation></semantics></math> images for training and test, respectively.
This database is collected from the images tagged <em id="S6.SS1.p2.3.2" class="ltx_emph ltx_font_italic">athletics, badminton, baseball, gymnastics, parkour, soccer, tennis, and volleyball</em> in the Flickr<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>Link of Flickr: <a target="_blank" href="https://www.flickr.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.flickr.com/</a></span></span></span>.
The LSP dataset is subsequently extended to the LSP-Extended dataset which contains over <math id="S6.SS1.p2.3.m3.2" class="ltx_Math" alttext="10,000" display="inline"><semantics id="S6.SS1.p2.3.m3.2a"><mrow id="S6.SS1.p2.3.m3.2.3.2" xref="S6.SS1.p2.3.m3.2.3.1.cmml"><mn id="S6.SS1.p2.3.m3.1.1" xref="S6.SS1.p2.3.m3.1.1.cmml">10</mn><mo id="S6.SS1.p2.3.m3.2.3.2.1" xref="S6.SS1.p2.3.m3.2.3.1.cmml">,</mo><mn id="S6.SS1.p2.3.m3.2.2" xref="S6.SS1.p2.3.m3.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.3.m3.2b"><list id="S6.SS1.p2.3.m3.2.3.1.cmml" xref="S6.SS1.p2.3.m3.2.3.2"><cn type="integer" id="S6.SS1.p2.3.m3.1.1.cmml" xref="S6.SS1.p2.3.m3.1.1">10</cn><cn type="integer" id="S6.SS1.p2.3.m3.2.2.cmml" xref="S6.SS1.p2.3.m3.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.3.m3.2c">10,000</annotation></semantics></math> training images.
Datasets have been publicly available at <a target="_blank" href="https://sam.johnson.io/research/lsp.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://sam.johnson.io/research/lsp.html</a>.</p>
</div>
<div id="S6.SS1.p3" class="ltx_para">
<p id="S6.SS1.p3.5" class="ltx_p"><span id="S6.SS1.p3.5.1" class="ltx_text ltx_font_bold">Frames Labeled in Cinema (FLIC) Dataset</span> The FL-IC dataset consists of about <math id="S6.SS1.p3.1.m1.2" class="ltx_Math" alttext="5,000" display="inline"><semantics id="S6.SS1.p3.1.m1.2a"><mrow id="S6.SS1.p3.1.m1.2.3.2" xref="S6.SS1.p3.1.m1.2.3.1.cmml"><mn id="S6.SS1.p3.1.m1.1.1" xref="S6.SS1.p3.1.m1.1.1.cmml">5</mn><mo id="S6.SS1.p3.1.m1.2.3.2.1" xref="S6.SS1.p3.1.m1.2.3.1.cmml">,</mo><mn id="S6.SS1.p3.1.m1.2.2" xref="S6.SS1.p3.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.1.m1.2b"><list id="S6.SS1.p3.1.m1.2.3.1.cmml" xref="S6.SS1.p3.1.m1.2.3.2"><cn type="integer" id="S6.SS1.p3.1.m1.1.1.cmml" xref="S6.SS1.p3.1.m1.1.1">5</cn><cn type="integer" id="S6.SS1.p3.1.m1.2.2.cmml" xref="S6.SS1.p3.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.1.m1.2c">5,000</annotation></semantics></math> images drawn from popular Hollywood movies, with <math id="S6.SS1.p3.2.m2.2" class="ltx_Math" alttext="4,000" display="inline"><semantics id="S6.SS1.p3.2.m2.2a"><mrow id="S6.SS1.p3.2.m2.2.3.2" xref="S6.SS1.p3.2.m2.2.3.1.cmml"><mn id="S6.SS1.p3.2.m2.1.1" xref="S6.SS1.p3.2.m2.1.1.cmml">4</mn><mo id="S6.SS1.p3.2.m2.2.3.2.1" xref="S6.SS1.p3.2.m2.2.3.1.cmml">,</mo><mn id="S6.SS1.p3.2.m2.2.2" xref="S6.SS1.p3.2.m2.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.2.m2.2b"><list id="S6.SS1.p3.2.m2.2.3.1.cmml" xref="S6.SS1.p3.2.m2.2.3.2"><cn type="integer" id="S6.SS1.p3.2.m2.1.1.cmml" xref="S6.SS1.p3.2.m2.1.1">4</cn><cn type="integer" id="S6.SS1.p3.2.m2.2.2.cmml" xref="S6.SS1.p3.2.m2.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.2.m2.2c">4,000</annotation></semantics></math> images for training and <math id="S6.SS1.p3.3.m3.2" class="ltx_Math" alttext="1,000" display="inline"><semantics id="S6.SS1.p3.3.m3.2a"><mrow id="S6.SS1.p3.3.m3.2.3.2" xref="S6.SS1.p3.3.m3.2.3.1.cmml"><mn id="S6.SS1.p3.3.m3.1.1" xref="S6.SS1.p3.3.m3.1.1.cmml">1</mn><mo id="S6.SS1.p3.3.m3.2.3.2.1" xref="S6.SS1.p3.3.m3.2.3.1.cmml">,</mo><mn id="S6.SS1.p3.3.m3.2.2" xref="S6.SS1.p3.3.m3.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.3.m3.2b"><list id="S6.SS1.p3.3.m3.2.3.1.cmml" xref="S6.SS1.p3.3.m3.2.3.2"><cn type="integer" id="S6.SS1.p3.3.m3.1.1.cmml" xref="S6.SS1.p3.3.m3.1.1">1</cn><cn type="integer" id="S6.SS1.p3.3.m3.2.2.cmml" xref="S6.SS1.p3.3.m3.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.3.m3.2c">1,000</annotation></semantics></math> images for test.
During labeling the keypoints, an object detector <cite class="ltx_cite ltx_citemacro_cite">Bourdev and Malik (<a href="#bib.bib7" title="" class="ltx_ref">2009</a>)</cite> is first leveraged on the Flic dataset to give the human candidates (roughly <math id="S6.SS1.p3.4.m4.2" class="ltx_Math" alttext="20,000" display="inline"><semantics id="S6.SS1.p3.4.m4.2a"><mrow id="S6.SS1.p3.4.m4.2.3.2" xref="S6.SS1.p3.4.m4.2.3.1.cmml"><mn id="S6.SS1.p3.4.m4.1.1" xref="S6.SS1.p3.4.m4.1.1.cmml">20</mn><mo id="S6.SS1.p3.4.m4.2.3.2.1" xref="S6.SS1.p3.4.m4.2.3.1.cmml">,</mo><mn id="S6.SS1.p3.4.m4.2.2" xref="S6.SS1.p3.4.m4.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.4.m4.2b"><list id="S6.SS1.p3.4.m4.2.3.1.cmml" xref="S6.SS1.p3.4.m4.2.3.2"><cn type="integer" id="S6.SS1.p3.4.m4.1.1.cmml" xref="S6.SS1.p3.4.m4.1.1">20</cn><cn type="integer" id="S6.SS1.p3.4.m4.2.2.cmml" xref="S6.SS1.p3.4.m4.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.4.m4.2c">20,000</annotation></semantics></math> examples). These are then sent to the crowdsourcing marketplace <em id="S6.SS1.p3.5.2" class="ltx_emph ltx_font_italic">Amazon Mechanical Turk</em> to obtain the ground truth poses including <math id="S6.SS1.p3.5.m5.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S6.SS1.p3.5.m5.1a"><mn id="S6.SS1.p3.5.m5.1.1" xref="S6.SS1.p3.5.m5.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.5.m5.1b"><cn type="integer" id="S6.SS1.p3.5.m5.1.1.cmml" xref="S6.SS1.p3.5.m5.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.5.m5.1c">10</annotation></semantics></math> upper body joints. Severely occluded or non-frontal persons are manually cleaned to form the Flic-Full dataset.
These datasets have been publicly available at <a target="_blank" href="https://bensapp.github.io/flic-dataset.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://bensapp.github.io/flic-dataset.html</a>.</p>
</div>
<div id="S6.SS1.p4" class="ltx_para">
<p id="S6.SS1.p4.3" class="ltx_p"><span id="S6.SS1.p4.3.1" class="ltx_text ltx_font_bold">MPII Human Pose Dataset</span> The MPII dataset contains <math id="S6.SS1.p4.1.m1.2" class="ltx_Math" alttext="28,821" display="inline"><semantics id="S6.SS1.p4.1.m1.2a"><mrow id="S6.SS1.p4.1.m1.2.3.2" xref="S6.SS1.p4.1.m1.2.3.1.cmml"><mn id="S6.SS1.p4.1.m1.1.1" xref="S6.SS1.p4.1.m1.1.1.cmml">28</mn><mo id="S6.SS1.p4.1.m1.2.3.2.1" xref="S6.SS1.p4.1.m1.2.3.1.cmml">,</mo><mn id="S6.SS1.p4.1.m1.2.2" xref="S6.SS1.p4.1.m1.2.2.cmml">821</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p4.1.m1.2b"><list id="S6.SS1.p4.1.m1.2.3.1.cmml" xref="S6.SS1.p4.1.m1.2.3.2"><cn type="integer" id="S6.SS1.p4.1.m1.1.1.cmml" xref="S6.SS1.p4.1.m1.1.1">28</cn><cn type="integer" id="S6.SS1.p4.1.m1.2.2.cmml" xref="S6.SS1.p4.1.m1.2.2">821</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p4.1.m1.2c">28,821</annotation></semantics></math> images for training and <math id="S6.SS1.p4.2.m2.2" class="ltx_Math" alttext="11,701" display="inline"><semantics id="S6.SS1.p4.2.m2.2a"><mrow id="S6.SS1.p4.2.m2.2.3.2" xref="S6.SS1.p4.2.m2.2.3.1.cmml"><mn id="S6.SS1.p4.2.m2.1.1" xref="S6.SS1.p4.2.m2.1.1.cmml">11</mn><mo id="S6.SS1.p4.2.m2.2.3.2.1" xref="S6.SS1.p4.2.m2.2.3.1.cmml">,</mo><mn id="S6.SS1.p4.2.m2.2.2" xref="S6.SS1.p4.2.m2.2.2.cmml">701</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p4.2.m2.2b"><list id="S6.SS1.p4.2.m2.2.3.1.cmml" xref="S6.SS1.p4.2.m2.2.3.2"><cn type="integer" id="S6.SS1.p4.2.m2.1.1.cmml" xref="S6.SS1.p4.2.m2.1.1">11</cn><cn type="integer" id="S6.SS1.p4.2.m2.2.2.cmml" xref="S6.SS1.p4.2.m2.2.2">701</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p4.2.m2.2c">11,701</annotation></semantics></math> images for test.
This dataset covers various human activities including recreational, occupational, house holding activities, and involves over <math id="S6.SS1.p4.3.m3.2" class="ltx_Math" alttext="40,000" display="inline"><semantics id="S6.SS1.p4.3.m3.2a"><mrow id="S6.SS1.p4.3.m3.2.3.2" xref="S6.SS1.p4.3.m3.2.3.1.cmml"><mn id="S6.SS1.p4.3.m3.1.1" xref="S6.SS1.p4.3.m3.1.1.cmml">40</mn><mo id="S6.SS1.p4.3.m3.2.3.2.1" xref="S6.SS1.p4.3.m3.2.3.1.cmml">,</mo><mn id="S6.SS1.p4.3.m3.2.2" xref="S6.SS1.p4.3.m3.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p4.3.m3.2b"><list id="S6.SS1.p4.3.m3.2.3.1.cmml" xref="S6.SS1.p4.3.m3.2.3.2"><cn type="integer" id="S6.SS1.p4.3.m3.1.1.cmml" xref="S6.SS1.p4.3.m3.1.1">40</cn><cn type="integer" id="S6.SS1.p4.3.m3.2.2.cmml" xref="S6.SS1.p4.3.m3.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p4.3.m3.2c">40,000</annotation></semantics></math> individual persons under a wide spectrum of viewpoints.
The pose annotations include 15 human joints and occlusion labels.
This dataset has been publicly available at <a target="_blank" href="http://human-pose.mpi-inf.mpg.de/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://human-pose.mpi-inf.mpg.de/</a>.</p>
</div>
<div id="S6.SS1.p5" class="ltx_para">
<p id="S6.SS1.p5.2" class="ltx_p"><span id="S6.SS1.p5.2.1" class="ltx_text ltx_font_bold">Common Objects in Context (COCO) Dataset</span> Microsoft COCO dataset is one of the most commonly used large-scale vision benchmark datasets, containing a total number of <math id="S6.SS1.p5.1.m1.2" class="ltx_Math" alttext="330,000" display="inline"><semantics id="S6.SS1.p5.1.m1.2a"><mrow id="S6.SS1.p5.1.m1.2.3.2" xref="S6.SS1.p5.1.m1.2.3.1.cmml"><mn id="S6.SS1.p5.1.m1.1.1" xref="S6.SS1.p5.1.m1.1.1.cmml">330</mn><mo id="S6.SS1.p5.1.m1.2.3.2.1" xref="S6.SS1.p5.1.m1.2.3.1.cmml">,</mo><mn id="S6.SS1.p5.1.m1.2.2" xref="S6.SS1.p5.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p5.1.m1.2b"><list id="S6.SS1.p5.1.m1.2.3.1.cmml" xref="S6.SS1.p5.1.m1.2.3.2"><cn type="integer" id="S6.SS1.p5.1.m1.1.1.cmml" xref="S6.SS1.p5.1.m1.1.1">330</cn><cn type="integer" id="S6.SS1.p5.1.m1.2.2.cmml" xref="S6.SS1.p5.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p5.1.m1.2c">330,000</annotation></semantics></math> images with over <math id="S6.SS1.p5.2.m2.2" class="ltx_Math" alttext="200,000" display="inline"><semantics id="S6.SS1.p5.2.m2.2a"><mrow id="S6.SS1.p5.2.m2.2.3.2" xref="S6.SS1.p5.2.m2.2.3.1.cmml"><mn id="S6.SS1.p5.2.m2.1.1" xref="S6.SS1.p5.2.m2.1.1.cmml">200</mn><mo id="S6.SS1.p5.2.m2.2.3.2.1" xref="S6.SS1.p5.2.m2.2.3.1.cmml">,</mo><mn id="S6.SS1.p5.2.m2.2.2" xref="S6.SS1.p5.2.m2.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p5.2.m2.2b"><list id="S6.SS1.p5.2.m2.2.3.1.cmml" xref="S6.SS1.p5.2.m2.2.3.2"><cn type="integer" id="S6.SS1.p5.2.m2.1.1.cmml" xref="S6.SS1.p5.2.m2.1.1">200</cn><cn type="integer" id="S6.SS1.p5.2.m2.2.2.cmml" xref="S6.SS1.p5.2.m2.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p5.2.m2.2c">200,000</annotation></semantics></math> annotated images for vision tasks such as object detection, segmentation, captioning, superpixel stuff segmentation and pose estimation, <em id="S6.SS1.p5.2.2" class="ltx_emph ltx_font_italic">etc</em>.
For 2D human pose estimation, 200,000 labeled images with 250,000 pose annotations are included.
Pose annotations with 17 joints on training and validation sets are publicly available, and labels of test set are unavailable.
The COCO dataset has become the most popular benchmark in image-based human pose estimation. Therefore, we subsequently report performance comparisons among different algorithms in this dataset.
The COCO dataset for 2D human pose estimation can be obtained in <a target="_blank" href="https://cocodataset.org/#keypoints-2020" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://cocodataset.org/#keypoints-2020</a>.</p>
</div>
<div id="S6.SS1.p6" class="ltx_para">
<p id="S6.SS1.p6.2" class="ltx_p"><span id="S6.SS1.p6.2.1" class="ltx_text ltx_font_bold">AI Challenger (AIC) Dataset</span> The AIC dataset consists of three sub-datasets: human keypoint detection (HKD), large-scale attribute dataset and image Chinese captioning, respectively.
HKD contains <math id="S6.SS1.p6.1.m1.2" class="ltx_Math" alttext="300,000" display="inline"><semantics id="S6.SS1.p6.1.m1.2a"><mrow id="S6.SS1.p6.1.m1.2.3.2" xref="S6.SS1.p6.1.m1.2.3.1.cmml"><mn id="S6.SS1.p6.1.m1.1.1" xref="S6.SS1.p6.1.m1.1.1.cmml">300</mn><mo id="S6.SS1.p6.1.m1.2.3.2.1" xref="S6.SS1.p6.1.m1.2.3.1.cmml">,</mo><mn id="S6.SS1.p6.1.m1.2.2" xref="S6.SS1.p6.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p6.1.m1.2b"><list id="S6.SS1.p6.1.m1.2.3.1.cmml" xref="S6.SS1.p6.1.m1.2.3.2"><cn type="integer" id="S6.SS1.p6.1.m1.1.1.cmml" xref="S6.SS1.p6.1.m1.1.1">300</cn><cn type="integer" id="S6.SS1.p6.1.m1.2.2.cmml" xref="S6.SS1.p6.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p6.1.m1.2c">300,000</annotation></semantics></math> images with a total of <math id="S6.SS1.p6.2.m2.2" class="ltx_Math" alttext="700,000" display="inline"><semantics id="S6.SS1.p6.2.m2.2a"><mrow id="S6.SS1.p6.2.m2.2.3.2" xref="S6.SS1.p6.2.m2.2.3.1.cmml"><mn id="S6.SS1.p6.2.m2.1.1" xref="S6.SS1.p6.2.m2.1.1.cmml">700</mn><mo id="S6.SS1.p6.2.m2.2.3.2.1" xref="S6.SS1.p6.2.m2.2.3.1.cmml">,</mo><mn id="S6.SS1.p6.2.m2.2.2" xref="S6.SS1.p6.2.m2.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p6.2.m2.2b"><list id="S6.SS1.p6.2.m2.2.3.1.cmml" xref="S6.SS1.p6.2.m2.2.3.2"><cn type="integer" id="S6.SS1.p6.2.m2.1.1.cmml" xref="S6.SS1.p6.2.m2.1.1">700</cn><cn type="integer" id="S6.SS1.p6.2.m2.2.2.cmml" xref="S6.SS1.p6.2.m2.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p6.2.m2.2c">700,000</annotation></semantics></math> human instances labeled by 14 keypoints.
These images are collected from the Internet search engine with an emphasis on daily activates for ordinary people.
The link of official website is: <a target="_blank" href="https://challenger.ai/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://challenger.ai/</a>.</p>
</div>
<div id="S6.SS1.p7" class="ltx_para">
<p id="S6.SS1.p7.3" class="ltx_p"><span id="S6.SS1.p7.3.1" class="ltx_text ltx_font_bold">CrowdedPose Dataset</span> The CrowdedPose dataset is designed for the crowded scenarios, which contains <math id="S6.SS1.p7.1.m1.2" class="ltx_Math" alttext="20,000" display="inline"><semantics id="S6.SS1.p7.1.m1.2a"><mrow id="S6.SS1.p7.1.m1.2.3.2" xref="S6.SS1.p7.1.m1.2.3.1.cmml"><mn id="S6.SS1.p7.1.m1.1.1" xref="S6.SS1.p7.1.m1.1.1.cmml">20</mn><mo id="S6.SS1.p7.1.m1.2.3.2.1" xref="S6.SS1.p7.1.m1.2.3.1.cmml">,</mo><mn id="S6.SS1.p7.1.m1.2.2" xref="S6.SS1.p7.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p7.1.m1.2b"><list id="S6.SS1.p7.1.m1.2.3.1.cmml" xref="S6.SS1.p7.1.m1.2.3.2"><cn type="integer" id="S6.SS1.p7.1.m1.1.1.cmml" xref="S6.SS1.p7.1.m1.1.1">20</cn><cn type="integer" id="S6.SS1.p7.1.m1.2.2.cmml" xref="S6.SS1.p7.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p7.1.m1.2c">20,000</annotation></semantics></math> images about <math id="S6.SS1.p7.2.m2.2" class="ltx_Math" alttext="80,000" display="inline"><semantics id="S6.SS1.p7.2.m2.2a"><mrow id="S6.SS1.p7.2.m2.2.3.2" xref="S6.SS1.p7.2.m2.2.3.1.cmml"><mn id="S6.SS1.p7.2.m2.1.1" xref="S6.SS1.p7.2.m2.1.1.cmml">80</mn><mo id="S6.SS1.p7.2.m2.2.3.2.1" xref="S6.SS1.p7.2.m2.2.3.1.cmml">,</mo><mn id="S6.SS1.p7.2.m2.2.2" xref="S6.SS1.p7.2.m2.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p7.2.m2.2b"><list id="S6.SS1.p7.2.m2.2.3.1.cmml" xref="S6.SS1.p7.2.m2.2.3.2"><cn type="integer" id="S6.SS1.p7.2.m2.1.1.cmml" xref="S6.SS1.p7.2.m2.1.1">80</cn><cn type="integer" id="S6.SS1.p7.2.m2.2.2.cmml" xref="S6.SS1.p7.2.m2.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p7.2.m2.2c">80,000</annotation></semantics></math> individual persons. This dataset has a split ratio of <math id="S6.SS1.p7.3.m3.1" class="ltx_Math" alttext="5:1:4" display="inline"><semantics id="S6.SS1.p7.3.m3.1a"><mrow id="S6.SS1.p7.3.m3.1.1" xref="S6.SS1.p7.3.m3.1.1.cmml"><mn id="S6.SS1.p7.3.m3.1.1.2" xref="S6.SS1.p7.3.m3.1.1.2.cmml">5</mn><mo lspace="0.278em" rspace="0.278em" id="S6.SS1.p7.3.m3.1.1.3" xref="S6.SS1.p7.3.m3.1.1.3.cmml">:</mo><mn id="S6.SS1.p7.3.m3.1.1.4" xref="S6.SS1.p7.3.m3.1.1.4.cmml">1</mn><mo lspace="0.278em" rspace="0.278em" id="S6.SS1.p7.3.m3.1.1.5" xref="S6.SS1.p7.3.m3.1.1.5.cmml">:</mo><mn id="S6.SS1.p7.3.m3.1.1.6" xref="S6.SS1.p7.3.m3.1.1.6.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p7.3.m3.1b"><apply id="S6.SS1.p7.3.m3.1.1.cmml" xref="S6.SS1.p7.3.m3.1.1"><and id="S6.SS1.p7.3.m3.1.1a.cmml" xref="S6.SS1.p7.3.m3.1.1"></and><apply id="S6.SS1.p7.3.m3.1.1b.cmml" xref="S6.SS1.p7.3.m3.1.1"><ci id="S6.SS1.p7.3.m3.1.1.3.cmml" xref="S6.SS1.p7.3.m3.1.1.3">:</ci><cn type="integer" id="S6.SS1.p7.3.m3.1.1.2.cmml" xref="S6.SS1.p7.3.m3.1.1.2">5</cn><cn type="integer" id="S6.SS1.p7.3.m3.1.1.4.cmml" xref="S6.SS1.p7.3.m3.1.1.4">1</cn></apply><apply id="S6.SS1.p7.3.m3.1.1c.cmml" xref="S6.SS1.p7.3.m3.1.1"><ci id="S6.SS1.p7.3.m3.1.1.5.cmml" xref="S6.SS1.p7.3.m3.1.1.5">:</ci><share href="#S6.SS1.p7.3.m3.1.1.4.cmml" id="S6.SS1.p7.3.m3.1.1d.cmml" xref="S6.SS1.p7.3.m3.1.1"></share><cn type="integer" id="S6.SS1.p7.3.m3.1.1.6.cmml" xref="S6.SS1.p7.3.m3.1.1.6">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p7.3.m3.1c">5:1:4</annotation></semantics></math> for training, validation, and test sets.
The dataset is collected by randomly sampling 30,000 images from three public benchmarks according to the <em id="S6.SS1.p7.3.2" class="ltx_emph ltx_font_italic">Crowd Index</em> (a measurement of crowding level for a given image).
This dataset is available at <a target="_blank" href="https://github.com/Jeff-sjtu/CrowdPose" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/Jeff-sjtu/CrowdPose</a>.</p>
</div>
<div id="S6.SS1.p8" class="ltx_para">
<p id="S6.SS1.p8.3" class="ltx_p"><span id="S6.SS1.p8.3.1" class="ltx_text ltx_font_bold">Penn Action Dataset</span> The Penn Action dataset is an unconstrained human action dataset, which contains <math id="S6.SS1.p8.1.m1.2" class="ltx_Math" alttext="2,326" display="inline"><semantics id="S6.SS1.p8.1.m1.2a"><mrow id="S6.SS1.p8.1.m1.2.3.2" xref="S6.SS1.p8.1.m1.2.3.1.cmml"><mn id="S6.SS1.p8.1.m1.1.1" xref="S6.SS1.p8.1.m1.1.1.cmml">2</mn><mo id="S6.SS1.p8.1.m1.2.3.2.1" xref="S6.SS1.p8.1.m1.2.3.1.cmml">,</mo><mn id="S6.SS1.p8.1.m1.2.2" xref="S6.SS1.p8.1.m1.2.2.cmml">326</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p8.1.m1.2b"><list id="S6.SS1.p8.1.m1.2.3.1.cmml" xref="S6.SS1.p8.1.m1.2.3.2"><cn type="integer" id="S6.SS1.p8.1.m1.1.1.cmml" xref="S6.SS1.p8.1.m1.1.1">2</cn><cn type="integer" id="S6.SS1.p8.1.m1.2.2.cmml" xref="S6.SS1.p8.1.m1.2.2">326</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p8.1.m1.2c">2,326</annotation></semantics></math> video clips derived from YouTuBe, and covers 15 type of actions. There are <math id="S6.SS1.p8.2.m2.2" class="ltx_Math" alttext="1,258" display="inline"><semantics id="S6.SS1.p8.2.m2.2a"><mrow id="S6.SS1.p8.2.m2.2.3.2" xref="S6.SS1.p8.2.m2.2.3.1.cmml"><mn id="S6.SS1.p8.2.m2.1.1" xref="S6.SS1.p8.2.m2.1.1.cmml">1</mn><mo id="S6.SS1.p8.2.m2.2.3.2.1" xref="S6.SS1.p8.2.m2.2.3.1.cmml">,</mo><mn id="S6.SS1.p8.2.m2.2.2" xref="S6.SS1.p8.2.m2.2.2.cmml">258</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p8.2.m2.2b"><list id="S6.SS1.p8.2.m2.2.3.1.cmml" xref="S6.SS1.p8.2.m2.2.3.2"><cn type="integer" id="S6.SS1.p8.2.m2.1.1.cmml" xref="S6.SS1.p8.2.m2.1.1">1</cn><cn type="integer" id="S6.SS1.p8.2.m2.2.2.cmml" xref="S6.SS1.p8.2.m2.2.2">258</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p8.2.m2.2c">1,258</annotation></semantics></math> videos for training and <math id="S6.SS1.p8.3.m3.2" class="ltx_Math" alttext="1,068" display="inline"><semantics id="S6.SS1.p8.3.m3.2a"><mrow id="S6.SS1.p8.3.m3.2.3.2" xref="S6.SS1.p8.3.m3.2.3.1.cmml"><mn id="S6.SS1.p8.3.m3.1.1" xref="S6.SS1.p8.3.m3.1.1.cmml">1</mn><mo id="S6.SS1.p8.3.m3.2.3.2.1" xref="S6.SS1.p8.3.m3.2.3.1.cmml">,</mo><mn id="S6.SS1.p8.3.m3.2.2" xref="S6.SS1.p8.3.m3.2.2.cmml">068</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p8.3.m3.2b"><list id="S6.SS1.p8.3.m3.2.3.1.cmml" xref="S6.SS1.p8.3.m3.2.3.2"><cn type="integer" id="S6.SS1.p8.3.m3.1.1.cmml" xref="S6.SS1.p8.3.m3.1.1">1</cn><cn type="integer" id="S6.SS1.p8.3.m3.2.2.cmml" xref="S6.SS1.p8.3.m3.2.2">068</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p8.3.m3.2c">1,068</annotation></semantics></math> videos for test.
Each person in images is labeled with 13 keypoints, and both joint coordinates and visibility are provided.
This dataset is available at <a target="_blank" href="http://dreamdragon.github.io/PennAction/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://dreamdragon.github.io/PennAction/</a>.</p>
</div>
<div id="S6.SS1.p9" class="ltx_para">
<p id="S6.SS1.p9.1" class="ltx_p"><span id="S6.SS1.p9.1.1" class="ltx_text ltx_font_bold">Joint-Annotated Human Motion DataBase (JHMDB) Dataset</span> JHMDB dataset is a fully annotated dataset for human action recognition and human pose estimation, which contains 21 action categories including <em id="S6.SS1.p9.1.2" class="ltx_emph ltx_font_italic">bru-sh hair, catch, clap, climb stairs,</em> and so on.
A subset of JHMDB that involves all visible joints, termed sub-JHMDB, are used for video-based 2D HPE.
This subset contains 316 video clips with 12 action categories, and each person is annotated with 15 joints.
These datasets are available at <a target="_blank" href="http://jhmdb.is.tue.mpg.de/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://jhmdb.is.tue.mpg.de/</a>.</p>
</div>
<div id="S6.SS1.p10" class="ltx_para">
<p id="S6.SS1.p10.3" class="ltx_p"><span id="S6.SS1.p10.3.1" class="ltx_text ltx_font_bold">PoseTrack Dataset</span> PoseTrack is a large-scale public dataset for human pose estimation and articulated tracking, which includes challenging situations with complicated movement of highly occluded people in crowded environments. The PoseTrack2017 dataset contains 514 video clips with <math id="S6.SS1.p10.1.m1.2" class="ltx_Math" alttext="16,219" display="inline"><semantics id="S6.SS1.p10.1.m1.2a"><mrow id="S6.SS1.p10.1.m1.2.3.2" xref="S6.SS1.p10.1.m1.2.3.1.cmml"><mn id="S6.SS1.p10.1.m1.1.1" xref="S6.SS1.p10.1.m1.1.1.cmml">16</mn><mo id="S6.SS1.p10.1.m1.2.3.2.1" xref="S6.SS1.p10.1.m1.2.3.1.cmml">,</mo><mn id="S6.SS1.p10.1.m1.2.2" xref="S6.SS1.p10.1.m1.2.2.cmml">219</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p10.1.m1.2b"><list id="S6.SS1.p10.1.m1.2.3.1.cmml" xref="S6.SS1.p10.1.m1.2.3.2"><cn type="integer" id="S6.SS1.p10.1.m1.1.1.cmml" xref="S6.SS1.p10.1.m1.1.1">16</cn><cn type="integer" id="S6.SS1.p10.1.m1.2.2.cmml" xref="S6.SS1.p10.1.m1.2.2">219</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p10.1.m1.2c">16,219</annotation></semantics></math> pose annotations, and the PoseTrack2018 dataset greatly increased the number of video clips to <math id="S6.SS1.p10.2.m2.2" class="ltx_Math" alttext="1,138" display="inline"><semantics id="S6.SS1.p10.2.m2.2a"><mrow id="S6.SS1.p10.2.m2.2.3.2" xref="S6.SS1.p10.2.m2.2.3.1.cmml"><mn id="S6.SS1.p10.2.m2.1.1" xref="S6.SS1.p10.2.m2.1.1.cmml">1</mn><mo id="S6.SS1.p10.2.m2.2.3.2.1" xref="S6.SS1.p10.2.m2.2.3.1.cmml">,</mo><mn id="S6.SS1.p10.2.m2.2.2" xref="S6.SS1.p10.2.m2.2.2.cmml">138</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p10.2.m2.2b"><list id="S6.SS1.p10.2.m2.2.3.1.cmml" xref="S6.SS1.p10.2.m2.2.3.2"><cn type="integer" id="S6.SS1.p10.2.m2.1.1.cmml" xref="S6.SS1.p10.2.m2.1.1">1</cn><cn type="integer" id="S6.SS1.p10.2.m2.2.2.cmml" xref="S6.SS1.p10.2.m2.2.2">138</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p10.2.m2.2c">1,138</annotation></semantics></math> with a total of <math id="S6.SS1.p10.3.m3.2" class="ltx_Math" alttext="153,615" display="inline"><semantics id="S6.SS1.p10.3.m3.2a"><mrow id="S6.SS1.p10.3.m3.2.3.2" xref="S6.SS1.p10.3.m3.2.3.1.cmml"><mn id="S6.SS1.p10.3.m3.1.1" xref="S6.SS1.p10.3.m3.1.1.cmml">153</mn><mo id="S6.SS1.p10.3.m3.2.3.2.1" xref="S6.SS1.p10.3.m3.2.3.1.cmml">,</mo><mn id="S6.SS1.p10.3.m3.2.2" xref="S6.SS1.p10.3.m3.2.2.cmml">615</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p10.3.m3.2b"><list id="S6.SS1.p10.3.m3.2.3.1.cmml" xref="S6.SS1.p10.3.m3.2.3.2"><cn type="integer" id="S6.SS1.p10.3.m3.1.1.cmml" xref="S6.SS1.p10.3.m3.1.1">153</cn><cn type="integer" id="S6.SS1.p10.3.m3.2.2.cmml" xref="S6.SS1.p10.3.m3.2.2">615</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p10.3.m3.2c">153,615</annotation></semantics></math> pose annotations.
In training videos, dense annotations for 30 center frames
of a video are provided. In validation videos, human poses are annotated every four frames. Both datasets label 15 joints, with an additional annotation label for joint visibility.
These datasets are available at <a target="_blank" href="https://posetrack.net" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://posetrack.net</a>.</p>
</div>
<div id="S6.SS1.p11" class="ltx_para">
<p id="S6.SS1.p11.1" class="ltx_p"><span id="S6.SS1.p11.1.1" class="ltx_text ltx_font_bold">Human-Centric Video Analysis in Complex Events (HiEve) Dataset</span> HiEve is the largest dataset for video-based human pose estimation, which contains 31 videos with a total of <math id="S6.SS1.p11.1.m1.3" class="ltx_Math" alttext="1,099,357" display="inline"><semantics id="S6.SS1.p11.1.m1.3a"><mrow id="S6.SS1.p11.1.m1.3.4.2" xref="S6.SS1.p11.1.m1.3.4.1.cmml"><mn id="S6.SS1.p11.1.m1.1.1" xref="S6.SS1.p11.1.m1.1.1.cmml">1</mn><mo id="S6.SS1.p11.1.m1.3.4.2.1" xref="S6.SS1.p11.1.m1.3.4.1.cmml">,</mo><mn id="S6.SS1.p11.1.m1.2.2" xref="S6.SS1.p11.1.m1.2.2.cmml">099</mn><mo id="S6.SS1.p11.1.m1.3.4.2.2" xref="S6.SS1.p11.1.m1.3.4.1.cmml">,</mo><mn id="S6.SS1.p11.1.m1.3.3" xref="S6.SS1.p11.1.m1.3.3.cmml">357</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p11.1.m1.3b"><list id="S6.SS1.p11.1.m1.3.4.1.cmml" xref="S6.SS1.p11.1.m1.3.4.2"><cn type="integer" id="S6.SS1.p11.1.m1.1.1.cmml" xref="S6.SS1.p11.1.m1.1.1">1</cn><cn type="integer" id="S6.SS1.p11.1.m1.2.2.cmml" xref="S6.SS1.p11.1.m1.2.2">099</cn><cn type="integer" id="S6.SS1.p11.1.m1.3.3.cmml" xref="S6.SS1.p11.1.m1.3.3">357</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p11.1.m1.3c">1,099,357</annotation></semantics></math> annotated poses, and labels 14 keypoints.
The HiEve dataset incorporates three human-centered understanding tasks, including human pose estimation, pose tracking, and action recognition.
The HiEve dataset is publicly available at <a target="_blank" href="http://humaninevents.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://humaninevents.org/</a>.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Evaluation Metrics</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">Accuracy is the fundamental measurement of performance comparisons between different methods.
In Table <a href="#S6.T2" title="Table 2 ‣ 6 Datasets and Evaluation ‣ 2D Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we list the metrics used to compute the accuracy of models in different datasets.
In what follows, we focus on the evaluation metrics of model accuracy.
</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.1" class="ltx_p"><span id="S6.SS2.p2.1.1" class="ltx_text ltx_font_bold">Percentage of Correctly Estimated Body Parts (PCP)</span> The PCP metric reflects the accuracy of localized body parts. An estimated part is considered correct if its endpoints lie within a threshold, which can be a fraction of the length of the ground truth segment at its annotated location <cite class="ltx_cite ltx_citemacro_cite">Eichner et al. (<a href="#bib.bib31" title="" class="ltx_ref">2012</a>)</cite>.
In addition to the mean PCP of all body parts, separate body limbs PCP such as torso, upper legs and head are also usually reported.
Similar to the PCP metric, PCPm utilizes <math id="S6.SS2.p2.1.m1.1" class="ltx_Math" alttext="50\%" display="inline"><semantics id="S6.SS2.p2.1.m1.1a"><mrow id="S6.SS2.p2.1.m1.1.1" xref="S6.SS2.p2.1.m1.1.1.cmml"><mn id="S6.SS2.p2.1.m1.1.1.2" xref="S6.SS2.p2.1.m1.1.1.2.cmml">50</mn><mo id="S6.SS2.p2.1.m1.1.1.1" xref="S6.SS2.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.1.m1.1b"><apply id="S6.SS2.p2.1.m1.1.1.cmml" xref="S6.SS2.p2.1.m1.1.1"><csymbol cd="latexml" id="S6.SS2.p2.1.m1.1.1.1.cmml" xref="S6.SS2.p2.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S6.SS2.p2.1.m1.1.1.2.cmml" xref="S6.SS2.p2.1.m1.1.1.2">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.1.m1.1c">50\%</annotation></semantics></math> of the mean ground-truth segment length over the entire test as the matching threshold <cite class="ltx_cite ltx_citemacro_cite">Andriluka et al. (<a href="#bib.bib1" title="" class="ltx_ref">2014</a>)</cite>.</p>
</div>
<div id="S6.SS2.p3" class="ltx_para">
<p id="S6.SS2.p3.1" class="ltx_p"><span id="S6.SS2.p3.1.1" class="ltx_text ltx_font_bold">Percentage of Correct Keypoints (PCK)</span> PCK <cite class="ltx_cite ltx_citemacro_cite">Yang and Ramanan (<a href="#bib.bib185" title="" class="ltx_ref">2012</a>)</cite> measures the accuracy of the localized body keypoints, and a candidate joint is considered correct if it lies within a matching threshold.
The threshold for matching of the keypoint position to the ground-truth can be defined as a fraction of the human bounding box size (denoted as <em id="S6.SS2.p3.1.2" class="ltx_emph ltx_font_italic">PCK</em>), and <math id="S6.SS2.p3.1.m1.1" class="ltx_Math" alttext="50\%" display="inline"><semantics id="S6.SS2.p3.1.m1.1a"><mrow id="S6.SS2.p3.1.m1.1.1" xref="S6.SS2.p3.1.m1.1.1.cmml"><mn id="S6.SS2.p3.1.m1.1.1.2" xref="S6.SS2.p3.1.m1.1.1.2.cmml">50</mn><mo id="S6.SS2.p3.1.m1.1.1.1" xref="S6.SS2.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p3.1.m1.1b"><apply id="S6.SS2.p3.1.m1.1.1.cmml" xref="S6.SS2.p3.1.m1.1.1"><csymbol cd="latexml" id="S6.SS2.p3.1.m1.1.1.1.cmml" xref="S6.SS2.p3.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S6.SS2.p3.1.m1.1.1.2.cmml" xref="S6.SS2.p3.1.m1.1.1.2">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p3.1.m1.1c">50\%</annotation></semantics></math> of the head segment length (denoted as <em id="S6.SS2.p3.1.3" class="ltx_emph ltx_font_italic">PCKh</em>).</p>
</div>
<div id="S6.SS2.p4" class="ltx_para">
<p id="S6.SS2.p4.1" class="ltx_p"><span id="S6.SS2.p4.1.1" class="ltx_text ltx_font_bold">Average Precision (AP)</span> The AP metric is defined on the basis of the Object Keypoint Similarity (OKS) <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib93" title="" class="ltx_ref">2014</a>)</cite> that evaluates the similarity between predicted and ground-truth keypoints.
The Average Precision score under different OKS thresholds <em id="S6.SS2.p4.1.2" class="ltx_emph ltx_font_italic">N</em> is denoted as AP@<em id="S6.SS2.p4.1.3" class="ltx_emph ltx_font_italic">N</em>.
For the image-based human pose estimation, mean average precision (<span id="S6.SS2.p4.1.4" class="ltx_text ltx_font_bold">mAP</span>) is the mean value of AP scores at all OKS thresholds.
In video-based human pose estimation, mAP averages the AP scores of each joint.</p>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Performance Comparisons</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.2" class="ltx_p">In order to comprehensively provide a performance comparison for different human pose estimation algorithms, we pick two representative benchmark datasets: <em id="S6.SS3.p1.2.1" class="ltx_emph ltx_font_italic">COCO</em> and <em id="S6.SS3.p1.2.2" class="ltx_emph ltx_font_italic">PoseTrack2017</em>.
The performance of image-level human pose estimation models on COCO dataset are presented in Table <a href="#S6.T3" title="Table 3 ‣ 6 Datasets and Evaluation ‣ 2D Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
HRNet-W48 is a powerful backbone network with excellent performance for keypoint localiztion, and UDP network that builds upon the HRNet achieves state-of-the-art results without extra training data.
The bottom-up approaches remain a wide gap (5.4 mAP) compared to the top-down approaches.
In additional to the model accuracy, the efficiency is also important especially for practical applications.
To this end, we report some approaches that aim at designing small networks, as summarized in Table <a href="#S6.T3" title="Table 3 ‣ 6 Datasets and Evaluation ‣ 2D Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
Lite-HRNet achieves a better trade-off between accuracy and speed, which obtains the accuracy of <math id="S6.SS3.p1.1.m1.1" class="ltx_Math" alttext="69.7" display="inline"><semantics id="S6.SS3.p1.1.m1.1a"><mn id="S6.SS3.p1.1.m1.1.1" xref="S6.SS3.p1.1.m1.1.1.cmml">69.7</mn><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.1.m1.1b"><cn type="float" id="S6.SS3.p1.1.m1.1.1.cmml" xref="S6.SS3.p1.1.m1.1.1">69.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.1.m1.1c">69.7</annotation></semantics></math> mAP with parameters of <math id="S6.SS3.p1.2.m2.1" class="ltx_Math" alttext="1.8" display="inline"><semantics id="S6.SS3.p1.2.m2.1a"><mn id="S6.SS3.p1.2.m2.1.1" xref="S6.SS3.p1.2.m2.1.1.cmml">1.8</mn><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.2.m2.1b"><cn type="float" id="S6.SS3.p1.2.m2.1.1.cmml" xref="S6.SS3.p1.2.m2.1.1">1.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.2.m2.1c">1.8</annotation></semantics></math> M.</p>
</div>
<div id="S6.SS3.p2" class="ltx_para">
<p id="S6.SS3.p2.1" class="ltx_p">We also report the performance of various video-based models on PoseTrack2017 dataset in Table <a href="#S6.T4" title="Table 4 ‣ 6 Datasets and Evaluation ‣ 2D Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
The DCPose employs abundant temporal information from adjacent frame to facilitate the current pose estimation, consistently establishing new state-of-the-arts on both validation and test sets.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Discussion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In this section, we first discuss the open questions of the current 2D human pose estimation, including model generalization and datasets.
Subsequently, we introduce the incompletely explored domain of estimating human pose from signal data.
Finally, we provide future research directions in terms of unsupervised learning, pose representations, and model explainability.</p>
</div>
<section id="S7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1 </span>Open Questions</h3>

<div id="S7.SS1.p1" class="ltx_para">
<p id="S7.SS1.p1.1" class="ltx_p">Human pose estimation has been greatly advanced by the deep learning. However, there are still numerous challenges that prevent models from achieving perfect performance.
Such challenges mainly arise from two aspects: question of models and shortcoming of datasets.</p>
</div>
<div id="S7.SS1.p2" class="ltx_para">
<p id="S7.SS1.p2.1" class="ltx_p"><span id="S7.SS1.p2.1.1" class="ltx_text ltx_font_bold">Model Capacity</span> Regarding both <em id="S7.SS1.p2.1.2" class="ltx_emph ltx_font_italic">image-based</em> and <em id="S7.SS1.p2.1.3" class="ltx_emph ltx_font_italic">video-based</em> human pose estimation, modern deep models have difficulties in tackling pose occlusions, person entanglement, and motion blur in complex scenarios.
In such cases, the absence of keypoint visual feature leads to difficulties in localizing joints according to visual information.
For image-based human pose estimation, models require the prior knowledge of human structure to cope with the lack of visual cues in static images.
In terms of video-based human pose estimation,
the models need to fully use temporal cues to recover human poses from the frames with insufficient visual information.
Additional cues from adjacent frames can be employed to reconstruct the pose of the current frame.
</p>
</div>
<div id="S7.SS1.p3" class="ltx_para">
<p id="S7.SS1.p3.1" class="ltx_p"><span id="S7.SS1.p3.1.1" class="ltx_text ltx_font_bold">Training Data Shortage</span> Large-scale annotated image datasets are currently available, yet video datasets still suffer from some shortcomings such as singular scenes and insufficient quantity.
On the other hand, the high-quality position labels of occluded joints are missing in the video dataset. Most of existing video datasets only label the joint visibility to indicate that whether a joint is occluded.
In this configuration, the models are hard to learn to detect the occluded or entangled joints, which greatly increases the difficulty in handling pose occlusions.</p>
</div>
<div id="S7.SS1.p4" class="ltx_para">
<p id="S7.SS1.p4.1" class="ltx_p">In addition, lacking of domain-specific datasets is also a shortcoming.
For particular scenes such as dancing and swimming, datasets of the corresponding domains are necessary for the practical application.
Therefore, building specialized datasets for various domains is essential to facilitate the application of 2D HPE.</p>
</div>
</section>
<section id="S7.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2 </span>Signal-Based Human Pose Estimation</h3>

<div id="S7.SS2.p1" class="ltx_para">
<p id="S7.SS2.p1.1" class="ltx_p">The corruption of visual features leads to challenges in handling <em id="S7.SS2.p1.1.1" class="ltx_emph ltx_font_italic">hard</em> joints, and non-visual data such as <em id="S7.SS2.p1.1.2" class="ltx_emph ltx_font_italic">WIFI signals</em> provides another way to overcome this problem.
Previous works <cite class="ltx_cite ltx_citemacro_cite">Zhao et al. (<a href="#bib.bib199" title="" class="ltx_ref">2018</a>); Wang et al. (<a href="#bib.bib165" title="" class="ltx_ref">2019a</a>); Li et al. (<a href="#bib.bib83" title="" class="ltx_ref">2020a</a>); Wang et al. (<a href="#bib.bib166" title="" class="ltx_ref">2019b</a>); Guo et al. (<a href="#bib.bib48" title="" class="ltx_ref">2019a</a>)</cite> propose to recover human poses from the radio signals or radar.
<cite class="ltx_cite ltx_citemacro_cite">Zhao et al. (<a href="#bib.bib199" title="" class="ltx_ref">2018</a>)</cite> leverages WIFI signals to traverse walls and reflect off the human body for accurately estimating human pose when the person is occluded by the wall.
Specifically, a deep neural network is proposed to parse keypoint locations from WiFi signals.
<cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib166" title="" class="ltx_ref">2019b</a>)</cite> presents a WiFi antennas-based method which takes the WiFi signals as input, and performs pose estimation in an end-to-end fashion.
<cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib83" title="" class="ltx_ref">2020a</a>)</cite> proposes a human pose estimation system using <math id="S7.SS2.p1.1.m1.1" class="ltx_Math" alttext="77GHz" display="inline"><semantics id="S7.SS2.p1.1.m1.1a"><mrow id="S7.SS2.p1.1.m1.1.1" xref="S7.SS2.p1.1.m1.1.1.cmml"><mn id="S7.SS2.p1.1.m1.1.1.2" xref="S7.SS2.p1.1.m1.1.1.2.cmml">77</mn><mo lspace="0em" rspace="0em" id="S7.SS2.p1.1.m1.1.1.1" xref="S7.SS2.p1.1.m1.1.1.1.cmml">​</mo><mi id="S7.SS2.p1.1.m1.1.1.3" xref="S7.SS2.p1.1.m1.1.1.3.cmml">G</mi><mo lspace="0em" rspace="0em" id="S7.SS2.p1.1.m1.1.1.1a" xref="S7.SS2.p1.1.m1.1.1.1.cmml">​</mo><mi id="S7.SS2.p1.1.m1.1.1.4" xref="S7.SS2.p1.1.m1.1.1.4.cmml">H</mi><mo lspace="0em" rspace="0em" id="S7.SS2.p1.1.m1.1.1.1b" xref="S7.SS2.p1.1.m1.1.1.1.cmml">​</mo><mi id="S7.SS2.p1.1.m1.1.1.5" xref="S7.SS2.p1.1.m1.1.1.5.cmml">z</mi></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.p1.1.m1.1b"><apply id="S7.SS2.p1.1.m1.1.1.cmml" xref="S7.SS2.p1.1.m1.1.1"><times id="S7.SS2.p1.1.m1.1.1.1.cmml" xref="S7.SS2.p1.1.m1.1.1.1"></times><cn type="integer" id="S7.SS2.p1.1.m1.1.1.2.cmml" xref="S7.SS2.p1.1.m1.1.1.2">77</cn><ci id="S7.SS2.p1.1.m1.1.1.3.cmml" xref="S7.SS2.p1.1.m1.1.1.3">𝐺</ci><ci id="S7.SS2.p1.1.m1.1.1.4.cmml" xref="S7.SS2.p1.1.m1.1.1.4">𝐻</ci><ci id="S7.SS2.p1.1.m1.1.1.5.cmml" xref="S7.SS2.p1.1.m1.1.1.5">𝑧</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p1.1.m1.1c">77GHz</annotation></semantics></math> millimeter wave radar, which first employs two radar data to generate heatmaps, and then employs a CNN to transform two-dimensional heatmaps into human poses.</p>
</div>
</section>
<section id="S7.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3 </span>Future Directions</h3>

<div id="S7.SS3.p1" class="ltx_para">
<p id="S7.SS3.p1.1" class="ltx_p"><span id="S7.SS3.p1.1.1" class="ltx_text">We expect that future researches would dive deeper into three aspects: unsupervised learning, pose representation, and model interpretability.<span id="S7.SS3.p1.1.1.1" class="ltx_text"></span></span></p>
</div>
<div id="S7.SS3.p2" class="ltx_para">
<p id="S7.SS3.p2.1" class="ltx_p"><span id="S7.SS3.p2.1.1" class="ltx_text ltx_font_bold">Unsupervised Learning</span> The fully-supervised methods currently dominate the field of human pose estimation since their superior performance.
Their success stems from the rich pose annotations in large-scale datasets.
However, unlabeled images and videos are an almost endless source, and providing full annotations for these data is impossible.
Therefore, unsupervised learning that can automatically learn knowledge of human body from an infinite amount of data has been an important direction.
</p>
</div>
<div id="S7.SS3.p3" class="ltx_para">
<p id="S7.SS3.p3.1" class="ltx_p"><span id="S7.SS3.p3.1.1" class="ltx_text ltx_font_bold">Pose Representation</span> The heatmap-based pose representation has demonstrated superior performance. However, quantization errors in encoding heatmap from coordinates and decoding coordinates from heatmaps are inevitable.
Simultaneously, the encoding and decoding processes of the heatmap are influenced by its resolution.
The high resolution brings good accuracy, but also increases the computational load.
Therefore, a novel unbiased pose representation for addressing such issues is necessary.
</p>
</div>
<div id="S7.SS3.p4" class="ltx_para">
<p id="S7.SS3.p4.1" class="ltx_p"><span id="S7.SS3.p4.1.1" class="ltx_text ltx_font_bold">Model Explainability</span> A drawback of deep learning methods is uninterpretability.
So far, there is no comprehensive and formal theory for interpretability.
As a result, there is limited systematic guidance in designing the deep learning models.
With respect to human pose estimation, we also fail to clearly understand how the visual features of the input image impact the final keypoint localization, which is detrimental to future investigations.
Given the potential shortcoming, it is highly desirable to advance works on the interpretability of human pose estimation models.</p>
</div>
</section>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Conclusion</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">In this paper, we present a comprehensive and systematic review of human pose estimation methods.
We present a coarse-level taxonomy with three categories: <em id="S8.p1.1.1" class="ltx_emph ltx_font_italic">network architecture design</em>, <em id="S8.p1.1.2" class="ltx_emph ltx_font_italic">network training refinement</em>, and <em id="S8.p1.1.3" class="ltx_emph ltx_font_italic">post processing</em>.
The network architecture design methods focus on the model architecture, the network training refinement methods revolve around the training of networks, and the post processing methods consider the model-agnostic optimization strategies.
On a finer level, we split the <em id="S8.p1.1.4" class="ltx_emph ltx_font_italic">network architecture design</em> methods (Section <a href="#S3" title="3 Network Architecture Design Methods ‣ 2D Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) into top-down framework and bottom-up framework.
We divide the <em id="S8.p1.1.5" class="ltx_emph ltx_font_italic">network training refinement</em> approaches (Section <a href="#S4" title="4 Network Training Refinement ‣ 2D Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>) into data augmentation techniques, multi-task learning strategies, loss function constraints, and domain adaption methods.
The <em id="S8.p1.1.6" class="ltx_emph ltx_font_italic">post processing</em> methods (Section <a href="#S5" title="5 Post Processing Approaches ‣ 2D Human Pose Estimation: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>) consists of quantization error and pose resampling.
Ultimately, we summarize popular benchmark datasets and evaluation metrics, conduct model performance comparisons, and discuss the potential future research directions. <span id="S8.p1.1.7" class="ltx_text">Hope this would be beneficial for researchers in the community and would inspire future research.<span id="S8.p1.1.7.1" class="ltx_text"></span></span></p>
</div>
</section>
<section id="S9" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9 </span>Acknowledgements</h2>

<div id="S9.p1" class="ltx_para">
<p id="S9.p1.1" class="ltx_p">This paper is supported by the National Key R&amp;D Program of China (Grant no.2018YFB1404102), the Key R&amp;D Program of Zhejiang Province (No. 2021C01104), and the National Natural Science Foundation of China (No. 61902348).</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Andriluka et al. (2014)</span>
<span class="ltx_bibblock">
Andriluka M, Pishchulin L, Gehler P, Schiele B (2014) 2d human pose estimation:
New benchmark and state of the art analysis. In: Proceedings of the IEEE
Conference on computer Vision and Pattern Recognition, pp 3686–3693

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Andriluka et al. (2018)</span>
<span class="ltx_bibblock">
Andriluka M, Iqbal U, Insafutdinov E, Pishchulin L, Milan A, Gall J, Schiele B
(2018) Posetrack: A benchmark for human pose estimation and tracking. In:
Proceedings of the IEEE conference on computer vision and pattern
recognition, pp 5167–5176

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Artacho and Savakis (2020)</span>
<span class="ltx_bibblock">
Artacho B, Savakis A (2020) Unipose: Unified human pose estimation in single
images and videos. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp 7035–7044

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baccouche et al. (2011)</span>
<span class="ltx_bibblock">
Baccouche M, Mamalet F, Wolf C, Garcia C, Baskurt A (2011) Sequential deep
learning for human action recognition. In: International workshop on human
behavior understanding, Springer, pp 29–39

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bertasius et al. (2019)</span>
<span class="ltx_bibblock">
Bertasius G, Feichtenhofer C, Tran D, Shi J, Torresani L (2019) Learning
temporal pose estimation from sparsely-labeled videos. In: Advances in Neural
Information Processing Systems, pp 3027–3038

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bin et al. (2020)</span>
<span class="ltx_bibblock">
Bin Y, Cao X, Chen X, Ge Y, Tai Y, Wang C, Li J, Huang F, Gao C, Sang N (2020)
Adversarial semantic data augmentation for human pose estimation. In:
European Conference on Computer Vision, Springer, pp 606–622

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bourdev and Malik (2009)</span>
<span class="ltx_bibblock">
Bourdev L, Malik J (2009) Poselets: Body part detectors trained using 3d human
pose annotations. In: 2009 IEEE 12th International Conference on Computer
Vision, IEEE, pp 1365–1372

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai et al. (2020)</span>
<span class="ltx_bibblock">
Cai Y, Wang Z, Luo Z, Yin B, Du A, Wang H, Zhou X, Zhou E, Zhang X, Sun J
(2020) Learning delicate local representations for multi-person pose
estimation. arXiv preprint arXiv:200304030

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al. (2017a)</span>
<span class="ltx_bibblock">
Cao Z, Simon T, Wei SE, Sheikh Y (2017a) Realtime multi-person 2d
pose estimation using part affinity fields. In: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al. (2017b)</span>
<span class="ltx_bibblock">
Cao Z, Simon T, Wei SE, Sheikh Y (2017b) Realtime multi-person 2d
pose estimation using part affinity fields. In: Proceedings of the IEEE
conference on computer vision and pattern recognition, pp 7291–7299

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carion et al. (2020)</span>
<span class="ltx_bibblock">
Carion N, Massa F, Synnaeve G, Usunier N, Kirillov A, Zagoruyko S (2020)
End-to-end object detection with transformers. In: European Conference on
Computer Vision, Springer, pp 213–229

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carreira et al. (2016)</span>
<span class="ltx_bibblock">
Carreira J, Agrawal P, Fragkiadaki K, Malik J (2016) Human pose estimation with
iterative error feedback. In: Proceedings of the IEEE conference on computer
vision and pattern recognition, pp 4733–4742

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chan et al. (2019)</span>
<span class="ltx_bibblock">
Chan C, Ginosar S, Zhou T, Efros AA (2019) Everybody dance now. In: Proceedings
of the IEEE/CVF International Conference on Computer Vision, pp 5933–5942

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chang et al. (2020)</span>
<span class="ltx_bibblock">
Chang S, Yuan L, Nie X, Huang Z, Zhou Y, Chen Y, Feng J, Yan S (2020) Towards
accurate human pose estimation in videos of crowded scenes. In: Proceedings
of the 28th ACM International Conference on Multimedia, pp 4630–4634

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Charles et al. (2016)</span>
<span class="ltx_bibblock">
Charles J, Pfister T, Magee D, Hogg D, Zisserman A (2016) Personalizing human
video pose estimation. In: Proceedings of the IEEE conference on computer
vision and pattern recognition, pp 3063–3072

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen and Ramanan (2017)</span>
<span class="ltx_bibblock">
Chen CH, Ramanan D (2017) 3d human pose estimation= 2d pose estimation+
matching. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp 7035–7043

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2018)</span>
<span class="ltx_bibblock">
Chen Y, Wang Z, Peng Y, Zhang Z, Yu G, Sun J (2018) Cascaded pyramid network
for multi-person pose estimation. In: Proceedings of the IEEE conference on
computer vision and pattern recognition, pp 7103–7112

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2020)</span>
<span class="ltx_bibblock">
Chen Y, Tian Y, He M (2020) Monocular human pose estimation: A survey of deep
learning-based methods. Computer Vision and Image Understanding 192:102897

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al. (2020)</span>
<span class="ltx_bibblock">
Cheng B, Xiao B, Wang J, Shi H, Huang TS, Zhang L (2020) Higherhrnet:
Scale-aware representation learning for bottom-up human pose estimation. In:
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp 5386–5395

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chu et al. (2017)</span>
<span class="ltx_bibblock">
Chu X, Yang W, Ouyang W, Ma C, Yuille AL, Wang X (2017) Multi-context attention
for human pose estimation. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp 1831–1840

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Comaniciu and Meer (2002)</span>
<span class="ltx_bibblock">
Comaniciu D, Meer P (2002) Mean shift: A robust approach toward feature space
analysis. IEEE Transactions on pattern analysis and machine intelligence
24(5):603–619

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Datta et al. (2019)</span>
<span class="ltx_bibblock">
Datta S, Sikka K, Roy A, Ahuja K, Parikh D, Divakaran A (2019) Align2ground:
Weakly supervised phrase grounding guided by image-caption alignment. In:
Proceedings of the IEEE/CVF International Conference on Computer Vision
(ICCV)

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dijkstra et al. (1959)</span>
<span class="ltx_bibblock">
Dijkstra EW, et al. (1959) A note on two problems in connexion with graphs.
Numerische mathematik 1(1):269–271

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Doering et al. (2018)</span>
<span class="ltx_bibblock">
Doering A, Iqbal U, Gall J (2018) Joint flow: Temporal flow fields for multi
person tracking. arXiv preprint arXiv:180504596

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et al. (2014)</span>
<span class="ltx_bibblock">
Dong J, Chen Q, Shen X, Yang J, Yan S (2014) Towards unified human parsing and
pose estimation. In: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp 843–850

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy et al. (2015)</span>
<span class="ltx_bibblock">
Dosovitskiy A, Fischer P, Ilg E, Hausser P, Hazirbas C, Golkov V, van der Smagt
P, Cremers D, Brox T (2015) Flownet: Learning optical flow with convolutional
networks. In: Proceedings of the IEEE International Conference on Computer
Vision (ICCV)

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Duan et al. (2019)</span>
<span class="ltx_bibblock">
Duan H, Lin KY, Jin S, Liu W, Qian C, Ouyang W (2019) Trb: a novel triplet
representation for understanding 2d human body. In: Proceedings of the
IEEE/CVF International Conference on Computer Vision, pp 9479–9488

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eichner and Ferrari (2010)</span>
<span class="ltx_bibblock">
Eichner M, Ferrari V (2010) We are family: Joint pose estimation of multiple
persons. In: European conference on computer vision, Springer, pp 228–242

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eichner and Ferrari (2012)</span>
<span class="ltx_bibblock">
Eichner M, Ferrari V (2012) Human pose co-estimation and applications. IEEE
transactions on pattern analysis and machine intelligence 34(11):2282–2288

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eichner et al. (2009)</span>
<span class="ltx_bibblock">
Eichner M, Ferrari V, Zurich S (2009) Better appearance models for pictorial
structures. In: Bmvc, Citeseer, vol 2, p 5

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eichner et al. (2012)</span>
<span class="ltx_bibblock">
Eichner M, Marin-Jimenez M, Zisserman A, Ferrari V (2012) 2d articulated human
pose estimation and retrieval in (almost) unconstrained still images.
International journal of computer vision 99(2):190–214

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Everingham et al. (2010)</span>
<span class="ltx_bibblock">
Everingham M, Van Gool L, Williams CK, Winn J, Zisserman A (2010) The pascal
visual object classes (voc) challenge. International journal of computer
vision 88(2):303–338

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et al. (2015)</span>
<span class="ltx_bibblock">
Fan X, Zheng K, Lin Y, Wang S (2015) Combining local appearance and holistic
view: Dual-source deep neural networks for human pose estimation. In:
Proceedings of the IEEE conference on computer vision and pattern
recognition, pp 1347–1355

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang et al. (2017)</span>
<span class="ltx_bibblock">
Fang HS, Xie S, Tai YW, Lu C (2017) Rmpe: Regional multi-person pose
estimation. In: Proceedings of the IEEE International Conference on Computer
Vision, pp 2334–2343

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Felzenszwalb and Huttenlocher (2005)</span>
<span class="ltx_bibblock">
Felzenszwalb PF, Huttenlocher DP (2005) Pictorial structures for object
recognition. International journal of computer vision 61(1):55–79

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fieraru et al. (2018)</span>
<span class="ltx_bibblock">
Fieraru M, Khoreva A, Pishchulin L, Schiele B (2018) Learning to refine human
pose estimation. In: Proceedings of the IEEE conference on computer vision
and pattern recognition workshops, pp 205–214

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2015)</span>
<span class="ltx_bibblock">
Gao Y, Chang HJ, Demiris Y (2015) User modelling for personalised dressing
assistance by humanoid robots. In: 2015 IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS), IEEE, pp 1840–1845

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2016)</span>
<span class="ltx_bibblock">
Gao Y, Chang HJ, Demiris Y (2016) Iterative path optimisation for personalised
dressing assistance using vision and force information. In: 2016 IEEE/RSJ
international conference on intelligent robots and systems (IROS), IEEE, pp
4398–4403

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Garau et al. (2021)</span>
<span class="ltx_bibblock">
Garau N, Bisagno N, Bródka P, Conci N (2021) Deca: Deep
viewpoint-equivariant human pose estimation using capsule autoencoders. arXiv
preprint arXiv:210808557

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geng et al. (2021)</span>
<span class="ltx_bibblock">
Geng Z, Sun K, Xiao B, Zhang Z, Wang J (2021) Bottom-up human pose estimation
via disentangled keypoint regression. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp 14676–14686

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Girdhar et al. (2018)</span>
<span class="ltx_bibblock">
Girdhar R, Gkioxari G, Torresani L, Paluri M, Tran D (2018) Detect-and-track:
Efficient pose estimation in videos. In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp 350–359

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gkioxari et al. (2016)</span>
<span class="ltx_bibblock">
Gkioxari G, Toshev A, Jaitly N (2016) Chained predictions using convolutional
neural networks. In: European Conference on Computer Vision, Springer, pp
728–743

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gong et al. (2017)</span>
<span class="ltx_bibblock">
Gong K, Liang X, Zhang D, Shen X, Lin L (2017) Look into person:
Self-supervised structure-sensitive learning and a new benchmark for human
parsing. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp 932–940

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gong et al. (2018)</span>
<span class="ltx_bibblock">
Gong K, Liang X, Li Y, Chen Y, Yang M, Lin L (2018) Instance-level human
parsing via part grouping network. In: Proceedings of the European Conference
on Computer Vision (ECCV), pp 770–785

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gong et al. (2016)</span>
<span class="ltx_bibblock">
Gong W, Zhang X, Gonzàlez J, Sobral A, Bouwmans T, Tu C, Zahzah Eh (2016)
Human pose estimation from monocular images: A comprehensive survey. Sensors
16(12):1966

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goodfellow et al. (2014)</span>
<span class="ltx_bibblock">
Goodfellow I, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley D, Ozair S,
Courville A, Bengio Y (2014) Generative adversarial nets. Advances in neural
information processing systems 27

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al. (2018)</span>
<span class="ltx_bibblock">
Guo H, Tang T, Luo G, Chen R, Lu Y, Wen L (2018) Multi-domain pose network for
multi-person pose estimation and tracking. In: Proceedings of the European
Conference on Computer Vision (ECCV), pp 0–0

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al. (2019a)</span>
<span class="ltx_bibblock">
Guo L, Lu Z, Wen X, Zhou S, Han Z (2019a) From signal to image:
Capturing fine-grained human poses with commodity wi-fi. IEEE Communications
Letters 24(4):802–806

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al. (2019b)</span>
<span class="ltx_bibblock">
Guo Y, Cheng Z, Nie L, Liu Y, Wang Y, Kankanhalli MS (2019b)
Quantifying and alleviating the language prior problem in visual question
answering. In: SIGIR, ACM, pp 75–84

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al. (2021a)</span>
<span class="ltx_bibblock">
Guo Y, Nie L, Cheng Z, Ji F, Zhang J, Bimbo AD (2021a) Adavqa:
Overcoming language priors with adapted margin cosine loss. In: IJCAI,
ijcai.org, pp 708–714

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al. (2021b)</span>
<span class="ltx_bibblock">
Guo Y, Nie L, Cheng Z, Ji F, Zhang J, Del Bimbo A (2021b) Adavqa:
Overcoming language priors with adapted margin cosine loss. arXiv preprint
arXiv:210501993

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2016)</span>
<span class="ltx_bibblock">
He K, Zhang X, Ren S, Sun J (2016) Deep residual learning for image
recognition. In: Proceedings of the IEEE conference on computer vision and
pattern recognition, pp 770–778

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2017a)</span>
<span class="ltx_bibblock">
He K, Gkioxari G, Dollar P, Girshick R (2017a) Mask r-cnn. In:
Proceedings of the IEEE International Conference on Computer Vision (ICCV)

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2017b)</span>
<span class="ltx_bibblock">
He K, Gkioxari G, Dollár P, Girshick R (2017b) Mask r-cnn. In:
Proceedings of the IEEE international conference on computer vision, pp
2961–2969

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hidalgo et al. (2019)</span>
<span class="ltx_bibblock">
Hidalgo G, Raaj Y, Idrees H, Xiang D, Joo H, Simon T, Sheikh Y (2019)
Single-network whole-body pose estimation. In: Proceedings of the IEEE/CVF
International Conference on Computer Vision, pp 6982–6991

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hinton et al. (2015)</span>
<span class="ltx_bibblock">
Hinton G, Vinyals O, Dean J (2015) Distilling the knowledge in a neural
network. arXiv preprint arXiv:150302531

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Holte et al. (2012)</span>
<span class="ltx_bibblock">
Holte MB, Tran C, Trivedi MM, Moeslund TB (2012) Human pose estimation and
activity recognition from multi-view videos: Comparative explorations of
recent developments. IEEE Journal of selected topics in signal processing
6(5):538–552

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2020a)</span>
<span class="ltx_bibblock">
Huang J, Zhu Z, Guo F, Huang G (2020a) The devil is in the
details: Delving into unbiased data processing for human pose estimation. In:
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp 5700–5709

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2020b)</span>
<span class="ltx_bibblock">
Huang J, Zhu Z, Huang G, Du D (2020b) Aid: Pushing the performance
boundary of human pose estimation with information dropping augmentation.
arXiv preprint arXiv:200807139

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2017)</span>
<span class="ltx_bibblock">
Huang S, Gong M, Tao D (2017) A coarse-fine network for keypoint localization.
In: Proceedings of the IEEE international conference on computer vision, pp
3028–3037

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ilg et al. (2017)</span>
<span class="ltx_bibblock">
Ilg E, Mayer N, Saikia T, Keuper M, Dosovitskiy A, Brox T (2017) Flownet 2.0:
Evolution of optical flow estimation with deep networks. In: Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Insafutdinov et al. (2016)</span>
<span class="ltx_bibblock">
Insafutdinov E, Pishchulin L, Andres B, Andriluka M, Schiele B (2016)
Deepercut: A deeper, stronger, and faster multi-person pose estimation model.
In: European Conference on Computer Vision, Springer, pp 34–50

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Iqbal et al. (2017)</span>
<span class="ltx_bibblock">
Iqbal U, Garbade M, Gall J (2017) Pose for action-action for pose. In: 2017
12th IEEE International Conference on Automatic Face &amp; Gesture Recognition
(FG 2017), IEEE, pp 438–445

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jaderberg et al. (2015)</span>
<span class="ltx_bibblock">
Jaderberg M, Simonyan K, Zisserman A, et al. (2015) Spatial transformer
networks. Advances in neural information processing systems 28:2017–2025

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jhuang et al. (2013)</span>
<span class="ltx_bibblock">
Jhuang H, Gall J, Zuffi S, Schmid C, Black MJ (2013) Towards understanding
action recognition. In: Proceedings of the IEEE international conference on
computer vision, pp 3192–3199

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et al. (2012)</span>
<span class="ltx_bibblock">
Ji S, Xu W, Yang M, Yu K (2012) 3d convolutional neural networks for human
action recognition. IEEE transactions on pattern analysis and machine
intelligence 35(1):221–231

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji and Liu (2009)</span>
<span class="ltx_bibblock">
Ji X, Liu H (2009) Advances in view-invariant human motion analysis: A review.
IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and
Reviews) 40(1):13–24

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2020)</span>
<span class="ltx_bibblock">
Jiang C, Huang K, Zhang S, Wang X, Xiao J (2020) Pay attention selectively and
comprehensively: Pyramid gating network for human pose estimation without
pre-training. In: Proceedings of the 28th ACM International Conference on
Multimedia, pp 2364–2371

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al. (2019)</span>
<span class="ltx_bibblock">
Jin S, Liu W, Ouyang W, Qian C (2019) Multi-person articulated tracking with
spatial and temporal embeddings. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp 5664–5673

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al. (2020)</span>
<span class="ltx_bibblock">
Jin S, Liu W, Xie E, Wang W, Qian C, Ouyang W, Luo P (2020) Differentiable
hierarchical graph grouping for multi-person pose estimation. In: European
Conference on Computer Vision, Springer, pp 718–734

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson and Everingham (2010)</span>
<span class="ltx_bibblock">
Johnson S, Everingham M (2010) Clustered pose and nonlinear appearance models
for human pose estimation. In: bmvc, Citeseer, vol 2, p 5

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson and Everingham (2011)</span>
<span class="ltx_bibblock">
Johnson S, Everingham M (2011) Learning effective human pose estimation from
inaccurate annotation. In: CVPR 2011, IEEE, pp 1465–1472

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ju et al. (1996)</span>
<span class="ltx_bibblock">
Ju SX, Black MJ, Yacoob Y (1996) Cardboard people: A parameterized model of
articulated image motion. In: Proceedings of the Second International
Conference on Automatic Face and Gesture Recognition, IEEE, pp 38–44

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kappel et al. (2021)</span>
<span class="ltx_bibblock">
Kappel M, Golyanik V, Elgharib M, Henningson JO, Seidel HP, Castillo S,
Theobalt C, Magnor M (2021) High-fidelity neural human motion transfer from
monocular video. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp 1541–1550

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ke et al. (2018)</span>
<span class="ltx_bibblock">
Ke L, Chang MC, Qi H, Lyu S (2018) Multi-scale structure-aware network for
human pose estimation. In: Proceedings of the european conference on computer
vision (ECCV), pp 713–728

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kipf and Welling (2016)</span>
<span class="ltx_bibblock">
Kipf TN, Welling M (2016) Semi-supervised classification with graph
convolutional networks. arXiv preprint arXiv:160902907

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kocabas et al. (2018)</span>
<span class="ltx_bibblock">
Kocabas M, Karagoz S, Akbas E (2018) Multiposenet: Fast multi-person pose
estimation using pose residual network. In: Proceedings of the European
conference on computer vision (ECCV), pp 417–433

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kreiss et al. (2019)</span>
<span class="ltx_bibblock">
Kreiss S, Bertoni L, Alahi A (2019) Pifpaf: Composite fields for human pose
estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp 11977–11986

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krizhevsky et al. (2012)</span>
<span class="ltx_bibblock">
Krizhevsky A, Sutskever I, Hinton GE (2012) Imagenet classification with deep
convolutional neural networks. Advances in neural information processing
systems 25:1097–1105

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ladicky et al. (2013)</span>
<span class="ltx_bibblock">
Ladicky L, Torr PH, Zisserman A (2013) Human pose estimation using a joint
pixel-wise and part-wise formulation. In: proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp 3578–3585

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">LeCun et al. (1998)</span>
<span class="ltx_bibblock">
LeCun Y, Bottou L, Bengio Y, Haffner P (1998) Gradient-based learning applied
to document recognition. Proceedings of the IEEE 86(11):2278–2324

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Lee (2021)</span>
<span class="ltx_bibblock">
Li C, Lee GH (2021) From synthetic to real: Unsupervised domain adaptation for
animal pose estimation. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp 1482–1491

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2020a)</span>
<span class="ltx_bibblock">
Li G, Zhang Z, Yang H, Pan J, Chen D, Zhang J (2020a) Capturing
human pose using mmwave radar. In: 2020 IEEE International Conference on
Pervasive Computing and Communications Workshops (PerCom Workshops), IEEE, pp
1–6

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2019)</span>
<span class="ltx_bibblock">
Li J, Wang C, Zhu H, Mao Y, Fang HS, Lu C (2019) Crowdpose: Efficient crowded
scenes pose estimation and a new benchmark. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp 10863–10872

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2020b)</span>
<span class="ltx_bibblock">
Li J, Su W, Wang Z (2020b) Simple pose: Rethinking and improving a
bottom-up approach for multi-person pose estimation. In: Proceedings of the
AAAI conference on artificial intelligence, vol 34, pp 11354–11361

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2021a)</span>
<span class="ltx_bibblock">
Li J, Bian S, Zeng A, Wang C, Pang B, Liu W, Lu C (2021a) Human
pose regression with residual log-likelihood estimation. arXiv preprint
arXiv:210711291

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2021b)</span>
<span class="ltx_bibblock">
Li K, Wang S, Zhang X, Xu Y, Xu W, Tu Z (2021b) Pose recognition
with cascade transformers. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp 1944–1953

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Fei-Fei (2007)</span>
<span class="ltx_bibblock">
Li LJ, Fei-Fei L (2007) What, where and who? classifying events by scene and
object recognition. In: 2007 IEEE 11th international conference on computer
vision, IEEE, pp 1–8

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2014)</span>
<span class="ltx_bibblock">
Li S, Liu ZQ, Chan AB (2014) Heterogeneous multi-task learning for human pose
estimation with deep convolutional neural network. In: Proceedings of the
IEEE conference on computer vision and pattern recognition workshops, pp
482–489

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2021c)</span>
<span class="ltx_bibblock">
Li Y, Yang X, Shang X, Chua TS (2021c) Interventional video
relation detection. In: Proceedings of the 29th ACM International Conference
on Multimedia, pp 4091–4099

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2021d)</span>
<span class="ltx_bibblock">
Li Z, Ye J, Song M, Huang Y, Pan Z (2021d) Online knowledge
distillation for efficient pose estimation. arXiv preprint arXiv:210802092

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al. (2018)</span>
<span class="ltx_bibblock">
Liang X, Gong K, Shen X, Lin L (2018) Look into person: Joint body parsing &amp;
pose estimation network and a new benchmark. IEEE transactions on pattern
analysis and machine intelligence 41(4):871–885

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2014)</span>
<span class="ltx_bibblock">
Lin TY, Maire M, Belongie S, Hays J, Perona P, Ramanan D, Dollár P, Zitnick
CL (2014) Microsoft coco: Common objects in context. In: European conference
on computer vision, Springer, pp 740–755

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2020)</span>
<span class="ltx_bibblock">
Lin W, Liu H, Liu S, Li Y, Qian R, Wang T, Xu N, Xiong H, Qi GJ, Sebe N (2020)
Human in events: A large-scale benchmark for human-centric video analysis in
complex events. arXiv preprint arXiv:200504490

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2016)</span>
<span class="ltx_bibblock">
Liu W, Anguelov D, Erhan D, Szegedy C, Reed S, Fu CY, Berg AC (2016) Ssd:
Single shot multibox detector. In: European conference on computer vision,
Springer, pp 21–37

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2018)</span>
<span class="ltx_bibblock">
Liu W, Chen J, Li C, Qian C, Chu X, Hu X (2018) A cascaded inception of
inception network with attention modulated feature fusion for human pose
estimation. In: Thirty-Second AAAI Conference on Artificial Intelligence

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2015)</span>
<span class="ltx_bibblock">
Liu Z, Zhu J, Bu J, Chen C (2015) A survey of human pose estimation: the body
parts parsing based methods. Journal of Visual Communication and Image
Representation 32:10–19

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2019)</span>
<span class="ltx_bibblock">
Liu Z, Wu S, Jin S, Liu Q, Lu S, Zimmermann R, Cheng L (2019) Towards natural
and accurate future motion prediction of humans and animals. In: Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp
10004–10012

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2021a)</span>
<span class="ltx_bibblock">
Liu Z, Chen H, Feng R, Wu S, Ji S, Yang B, Wang X (2021a) Deep
dual consecutive network for human pose estimation. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp 525–534

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2021b)</span>
<span class="ltx_bibblock">
Liu Z, Lyu K, Wu S, Chen H, Hao Y, Ji S (2021b) Aggregated
multi-gans for controlled 3d human motion prediction. In: Proceedings of the
AAAI Conference on Artificial Intelligence, vol 35, pp 2225–2232

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2021c)</span>
<span class="ltx_bibblock">
Liu Z, Qian P, Wang X, Zhuang Y, Qiu L, Wang X (2021c) Combining
graph neural networks with expert knowledge for smart contract vulnerability
detection. IEEE Transactions on Knowledge and Data Engineering

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2021d)</span>
<span class="ltx_bibblock">
Liu Z, Su P, Wu S, Shen X, Chen H, Hao Y, Wang M (2021d) Motion
prediction using trajectory cues. IEEE International Conference on Computer
Vision

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2022)</span>
<span class="ltx_bibblock">
Liu Z, Mao H, Wu CY, Feichtenhofer C, Darrell T, Xie S (2022) A convnet for the
2020s. arXiv preprint arXiv:220103545

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al. (2018a)</span>
<span class="ltx_bibblock">
Luo Y, Ren J, Wang Z, Sun W, Pan J, Liu J, Pang J, Lin L (2018a)
Lstm pose machines. In: Proceedings of the IEEE conference on computer vision
and pattern recognition, pp 5207–5215

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al. (2018b)</span>
<span class="ltx_bibblock">
Luo Y, Xu Z, Liu P, Du Y, Guo JM (2018b) Multi-person pose
estimation via multi-layer fractal network and joints kinship pattern. IEEE
Transactions on Image Processing 28(1):142–155

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al. (2021)</span>
<span class="ltx_bibblock">
Luo Z, Wang Z, Huang Y, Wang L, Tan T, Zhou E (2021) Rethinking the heatmap
regression for bottom-up human pose estimation. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp
13264–13273

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luvizon et al. (2019)</span>
<span class="ltx_bibblock">
Luvizon DC, Tabia H, Picard D (2019) Human pose regression by combining
indirect part detection and contextual information. Computers &amp; Graphics
85:15–22

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. (2018)</span>
<span class="ltx_bibblock">
Ma N, Zhang X, Zheng HT, Sun J (2018) Shufflenet v2: Practical guidelines for
efficient cnn architecture design. In: Proceedings of the European conference
on computer vision (ECCV), pp 116–131

</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mao et al. (2021)</span>
<span class="ltx_bibblock">
Mao W, Tian Z, Wang X, Shen C (2021) Fcpose: Fully convolutional multi-person
pose estimation with dynamic instance-aware convolutions. In: Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp
9034–9043

</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marin-Jimenez et al. (2014)</span>
<span class="ltx_bibblock">
Marin-Jimenez MJ, Zisserman A, Eichner M, Ferrari V (2014) Detecting people
looking at each other in videos. International Journal of Computer Vision
106(3):282–296

</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Martinez et al. (2017)</span>
<span class="ltx_bibblock">
Martinez J, Hossain R, Romero J, Little JJ (2017) A simple yet effective
baseline for 3d human pose estimation. In: Proceedings of the IEEE
International Conference on Computer Vision, pp 2640–2649

</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mehta et al. (2017)</span>
<span class="ltx_bibblock">
Mehta D, Sridhar S, Sotnychenko O, Rhodin H, Shafiei M, Seidel HP, Xu W, Casas
D, Theobalt C (2017) Vnect: Real-time 3d human pose estimation with a single
rgb camera. ACM Transactions on Graphics (TOG) 36(4):1–14

</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mirzadeh et al. (2020)</span>
<span class="ltx_bibblock">
Mirzadeh SI, Farajtabar M, Li A, Levine N, Matsukawa A, Ghasemzadeh H (2020)
Improved knowledge distillation via teacher assistant. In: Proceedings of the
AAAI Conference on Artificial Intelligence, vol 34, pp 5191–5198

</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moeslund and Granum (2001)</span>
<span class="ltx_bibblock">
Moeslund TB, Granum E (2001) A survey of computer vision-based human motion
capture. Computer vision and image understanding 81(3):231–268

</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moeslund et al. (2006)</span>
<span class="ltx_bibblock">
Moeslund TB, Hilton A, Krüger V (2006) A survey of advances in vision-based
human motion capture and analysis. Computer vision and image understanding
104(2-3):90–126

</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moeslund et al. (2011)</span>
<span class="ltx_bibblock">
Moeslund TB, Hilton A, Krüger V, Sigal L (2011) Visual analysis of humans.
Springer

</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mogadala et al. (2021)</span>
<span class="ltx_bibblock">
Mogadala A, Kalimuthu M, Klakow D (2021) Trends in integration of vision and
language research: A survey of tasks, datasets, and methods. Journal of
Artificial Intelligence Research

</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moon et al. (2019)</span>
<span class="ltx_bibblock">
Moon G, Chang JY, Lee KM (2019) Posefix: Model-agnostic general human pose
refinement network. In: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp 7773–7781

</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Munea et al. (2020)</span>
<span class="ltx_bibblock">
Munea TL, Jembre YZ, Weldegebriel HT, Chen L, Huang C, Yang C (2020) The
progress of human pose estimation: a survey and taxonomy of models applied in
2d human pose estimation. IEEE Access 8:133330–133348

</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Naksuk et al. (2005)</span>
<span class="ltx_bibblock">
Naksuk N, Lee CG, Rietdyk S (2005) Whole-body human-to-humanoid motion
transfer. In: 5th IEEE-RAS International Conference on Humanoid Robots,
2005., IEEE, pp 104–109

</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Newell et al. (2016a)</span>
<span class="ltx_bibblock">
Newell A, Huang Z, Deng J (2016a) Associative embedding:
End-to-end learning for joint detection and grouping. arXiv preprint
arXiv:161105424

</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Newell et al. (2016b)</span>
<span class="ltx_bibblock">
Newell A, Yang K, Deng J (2016b) Stacked hourglass networks for
human pose estimation. In: European conference on computer vision, Springer,
pp 483–499

</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nie et al. (2018a)</span>
<span class="ltx_bibblock">
Nie X, Feng J, Xing J, Yan S (2018a) Pose partition networks for
multi-person pose estimation. In: Proceedings of the european conference on
computer vision (eccv), pp 684–699

</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nie et al. (2018b)</span>
<span class="ltx_bibblock">
Nie X, Feng J, Zuo Y, Yan S (2018b) Human pose estimation with
parsing induced learner. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp 2100–2108

</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nie et al. (2019a)</span>
<span class="ltx_bibblock">
Nie X, Feng J, Zhang J, Yan S (2019a) Single-stage multi-person
pose machines. In: Proceedings of the IEEE/CVF International Conference on
Computer Vision, pp 6951–6960

</span>
</li>
<li id="bib.bib126" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nie et al. (2019b)</span>
<span class="ltx_bibblock">
Nie X, Li Y, Luo L, Zhang N, Feng J (2019b) Dynamic kernel
distillation for efficient pose estimation in videos. In: Proceedings of the
IEEE/CVF International Conference on Computer Vision, pp 6942–6950

</span>
</li>
<li id="bib.bib127" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nie et al. (2020)</span>
<span class="ltx_bibblock">
Nie X, Feng J, Zhang J, Yan S (2020) Single-stage multi-person pose machines.
In: 2019 IEEE/CVF International Conference on Computer Vision (ICCV)

</span>
</li>
<li id="bib.bib128" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papandreou et al. (2017)</span>
<span class="ltx_bibblock">
Papandreou G, Zhu T, Kanazawa N, Toshev A, Tompson J, Bregler C, Murphy K
(2017) Towards accurate multi-person pose estimation in the wild. In:
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp 4903–4911

</span>
</li>
<li id="bib.bib129" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papandreou et al. (2018)</span>
<span class="ltx_bibblock">
Papandreou G, Zhu T, Chen LC, Gidaris S, Tompson J, Murphy K (2018) Personlab:
Person pose estimation and instance segmentation with a bottom-up,
part-based, geometric embedding model. In: Proceedings of the European
Conference on Computer Vision (ECCV), pp 269–286

</span>
</li>
<li id="bib.bib130" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et al. (2018)</span>
<span class="ltx_bibblock">
Peng X, Tang Z, Yang F, Feris RS, Metaxas D (2018) Jointly optimize data
augmentation and network training: Adversarial data augmentation in human
pose estimation. In: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp 2226–2234

</span>
</li>
<li id="bib.bib131" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pfister et al. (2015)</span>
<span class="ltx_bibblock">
Pfister T, Charles J, Zisserman A (2015) Flowing convnets for human pose
estimation in videos. In: Proceedings of the IEEE International Conference on
Computer Vision, pp 1913–1921

</span>
</li>
<li id="bib.bib132" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pishchulin et al. (2013)</span>
<span class="ltx_bibblock">
Pishchulin L, Andriluka M, Gehler P, Schiele B (2013) Poselet conditioned
pictorial structures. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp 588–595

</span>
</li>
<li id="bib.bib133" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pishchulin et al. (2016)</span>
<span class="ltx_bibblock">
Pishchulin L, Insafutdinov E, Tang S, Andres B, Andriluka M, Gehler PV, Schiele
B (2016) Deepcut: Joint subset partition and labeling for multi person pose
estimation. In: Proceedings of the IEEE conference on computer vision and
pattern recognition, pp 4929–4937

</span>
</li>
<li id="bib.bib134" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Poppe (2007)</span>
<span class="ltx_bibblock">
Poppe R (2007) Vision-based human motion analysis: An overview. Computer vision
and image understanding 108(1-2):4–18

</span>
</li>
<li id="bib.bib135" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qiu et al. (2020)</span>
<span class="ltx_bibblock">
Qiu L, Zhang X, Li Y, Li G, Wu X, Xiong Z, Han X, Cui S (2020) Peeking into
occluded joints: A novel framework for crowd pose estimation. In: European
Conference on Computer Vision, Springer, pp 488–504

</span>
</li>
<li id="bib.bib136" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raaj et al. (2019)</span>
<span class="ltx_bibblock">
Raaj Y, Idrees H, Hidalgo G, Sheikh Y (2019) Efficient online multi-person 2d
pose tracking with recurrent spatio-temporal affinity fields. In: Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp
4620–4628

</span>
</li>
<li id="bib.bib137" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramakrishna et al. (2014)</span>
<span class="ltx_bibblock">
Ramakrishna V, Munoz D, Hebert M, Bagnell JA, Sheikh Y (2014) Pose machines:
Articulated pose estimation via inference machines. In: European Conference
on Computer Vision, Springer, pp 33–47

</span>
</li>
<li id="bib.bib138" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al. (2015)</span>
<span class="ltx_bibblock">
Ren S, He K, Girshick R, Sun J (2015) Faster r-cnn: Towards real-time object
detection with region proposal networks. Advances in neural information
processing systems 28:91–99

</span>
</li>
<li id="bib.bib139" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Romero et al. (2014)</span>
<span class="ltx_bibblock">
Romero A, Ballas N, Kahou SE, Chassang A, Gatta C, Bengio Y (2014) Fitnets:
Hints for thin deep nets. arXiv preprint arXiv:14126550

</span>
</li>
<li id="bib.bib140" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ruan et al. (2019)</span>
<span class="ltx_bibblock">
Ruan T, Liu T, Huang Z, Wei Y, Wei S, Zhao Y (2019) Devil in the details:
Towards accurate single and multiple human parsing. In: Proceedings of the
AAAI Conference on Artificial Intelligence, vol 33, pp 4814–4821

</span>
</li>
<li id="bib.bib141" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sandler et al. (2018)</span>
<span class="ltx_bibblock">
Sandler M, Howard A, Zhu M, Zhmoginov A, Chen LC (2018) Mobilenetv2: Inverted
residuals and linear bottlenecks. In: Proceedings of the IEEE conference on
computer vision and pattern recognition, pp 4510–4520

</span>
</li>
<li id="bib.bib142" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sapp and Taskar (2013)</span>
<span class="ltx_bibblock">
Sapp B, Taskar B (2013) Modec: Multimodal decomposable models for human pose
estimation. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp 3674–3681

</span>
</li>
<li id="bib.bib143" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sapp et al. (2010)</span>
<span class="ltx_bibblock">
Sapp B, Toshev A, Taskar B (2010) Cascaded models for articulated pose
estimation. In: European conference on computer vision, Springer, pp 406–420

</span>
</li>
<li id="bib.bib144" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sapp et al. (2011)</span>
<span class="ltx_bibblock">
Sapp B, Weiss D, Taskar B (2011) Parsing human motion with stretchable models.
In: CVPR 2011, IEEE, pp 1281–1288

</span>
</li>
<li id="bib.bib145" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sarafianos et al. (2016)</span>
<span class="ltx_bibblock">
Sarafianos N, Boteanu B, Ionescu B, Kakadiaris IA (2016) 3d human pose
estimation: A review of the literature and analysis of covariates. Computer
Vision and Image Understanding 152:1–20

</span>
</li>
<li id="bib.bib146" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schmidtke et al. (2021)</span>
<span class="ltx_bibblock">
Schmidtke L, Vlontzos A, Ellershaw S, Lukens A, Arichi T, Kainz B (2021)
Unsupervised human pose estimation through transforming shape templates. In:
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp 2484–2494

</span>
</li>
<li id="bib.bib147" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shang et al. (2019)</span>
<span class="ltx_bibblock">
Shang X, Di D, Xiao J, Cao Y, Yang X, Chua TS (2019) Annotating objects and
relations in user-generated videos. In: Proceedings of the 2019 on
International Conference on Multimedia Retrieval, pp 279–287

</span>
</li>
<li id="bib.bib148" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sidenbladh et al. (2000)</span>
<span class="ltx_bibblock">
Sidenbladh H, De la Torre F, Black MJ (2000) A framework for modeling the
appearance of 3d articulated figures. In: Proceedings Fourth IEEE
International Conference on Automatic Face and Gesture Recognition (Cat. No.
PR00580), IEEE, pp 368–375

</span>
</li>
<li id="bib.bib149" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Simonyan and Zisserman (2014)</span>
<span class="ltx_bibblock">
Simonyan K, Zisserman A (2014) Very deep convolutional networks for large-scale
image recognition. arXiv preprint arXiv:14091556

</span>
</li>
<li id="bib.bib150" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Snower et al. (2020)</span>
<span class="ltx_bibblock">
Snower M, Kadav A, Lai F, Graf HP (2020) 15 keypoints is all you need. In:
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp 6738–6748

</span>
</li>
<li id="bib.bib151" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al. (2017)</span>
<span class="ltx_bibblock">
Song J, Wang L, Van Gool L, Hilliges O (2017) Thin-slicing network: A deep
structured model for pose estimation in videos. In: Proceedings of the IEEE
conference on computer vision and pattern recognition, pp 4220–4229

</span>
</li>
<li id="bib.bib152" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al. (2019)</span>
<span class="ltx_bibblock">
Su K, Yu D, Xu Z, Geng X, Wang C (2019) Multi-person pose estimation with
enhanced channel-wise and spatial information. In: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp 5674–5682

</span>
</li>
<li id="bib.bib153" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2019)</span>
<span class="ltx_bibblock">
Sun K, Xiao B, Liu D, Wang J (2019) Deep high-resolution representation
learning for human pose estimation. In: Proceedings of the IEEE conference on
computer vision and pattern recognition, pp 5693–5703

</span>
</li>
<li id="bib.bib154" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2017)</span>
<span class="ltx_bibblock">
Sun X, Shang J, Liang S, Wei Y (2017) Compositional human pose regression. In:
Proceedings of the IEEE International Conference on Computer Vision, pp
2602–2611

</span>
</li>
<li id="bib.bib155" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2018)</span>
<span class="ltx_bibblock">
Sun X, Xiao B, Wei F, Liang S, Wei Y (2018) Integral human pose regression. In:
Proceedings of the European Conference on Computer Vision (ECCV), pp 529–545

</span>
</li>
<li id="bib.bib156" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Szegedy et al. (2015)</span>
<span class="ltx_bibblock">
Szegedy C, Liu W, Jia Y, Sermanet P, Reed S, Anguelov D, Erhan D, Vanhoucke V,
Rabinovich A (2015) Going deeper with convolutions. In: Proceedings of the
IEEE conference on computer vision and pattern recognition, pp 1–9

</span>
</li>
<li id="bib.bib157" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang and Wu (2019)</span>
<span class="ltx_bibblock">
Tang W, Wu Y (2019) Does learning specific features for related parts help
human pose estimation? In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp 1107–1116

</span>
</li>
<li id="bib.bib158" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. (2018)</span>
<span class="ltx_bibblock">
Tang W, Yu P, Wu Y (2018) Deeply learned compositional models for human pose
estimation. In: Proceedings of the European conference on computer vision
(ECCV), pp 190–206

</span>
</li>
<li id="bib.bib159" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tian et al. (2019)</span>
<span class="ltx_bibblock">
Tian Z, Chen H, Shen C (2019) Directpose: Direct end-to-end multi-person pose
estimation. arXiv preprint arXiv:191107451

</span>
</li>
<li id="bib.bib160" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tompson et al. (2014)</span>
<span class="ltx_bibblock">
Tompson JJ, Jain A, LeCun Y, Bregler C (2014) Joint training of a convolutional
network and a graphical model for human pose estimation. Advances in neural
information processing systems 27:1799–1807

</span>
</li>
<li id="bib.bib161" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Toshev and Szegedy (2014)</span>
<span class="ltx_bibblock">
Toshev A, Szegedy C (2014) Deeppose: Human pose estimation via deep neural
networks. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)

</span>
</li>
<li id="bib.bib162" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Varamesh and Tuytelaars (2020)</span>
<span class="ltx_bibblock">
Varamesh A, Tuytelaars T (2020) Mixture dense regression for object detection
and human pose estimation. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp 13086–13095

</span>
</li>
<li id="bib.bib163" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Ł,
Polosukhin I (2017) Attention is all you need. In: Advances in neural
information processing systems, pp 5998–6008

</span>
</li>
<li id="bib.bib164" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang and Li (2013)</span>
<span class="ltx_bibblock">
Wang F, Li Y (2013) Beyond physical connections: Tree models in human pose
estimation. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp 596–603

</span>
</li>
<li id="bib.bib165" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2019a)</span>
<span class="ltx_bibblock">
Wang F, Panev S, Dai Z, Han J, Huang D (2019a) Can wifi estimate
person pose? arXiv preprint arXiv:190400277

</span>
</li>
<li id="bib.bib166" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2019b)</span>
<span class="ltx_bibblock">
Wang F, Zhou S, Panev S, Han J, Huang D (2019b) Person-in-wifi:
Fine-grained person perception using wifi. In: Proceedings of the IEEE/CVF
International Conference on Computer Vision, pp 5452–5461

</span>
</li>
<li id="bib.bib167" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang and Schmid (2013)</span>
<span class="ltx_bibblock">
Wang H, Schmid C (2013) Action recognition with improved trajectories. In:
Proceedings of the IEEE international conference on computer vision, pp
3551–3558

</span>
</li>
<li id="bib.bib168" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2019c)</span>
<span class="ltx_bibblock">
Wang J, Gou L, Zhang W, Yang H, Shen HW (2019c) Deepvid: Deep
visual interpretation and diagnosis for image classifiers via knowledge
distillation. IEEE transactions on visualization and computer graphics
25(6):2168–2180

</span>
</li>
<li id="bib.bib169" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2019d)</span>
<span class="ltx_bibblock">
Wang J, Qiu K, Peng H, Fu J, Zhu J (2019d) Ai coach: Deep human
pose estimation and analysis for personalized athletic training assistance.
In: Proceedings of the 27th ACM International Conference on Multimedia, pp
374–382

</span>
</li>
<li id="bib.bib170" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2020a)</span>
<span class="ltx_bibblock">
Wang J, Long X, Gao Y, Ding E, Wen S (2020a) Graph-pcnn: Two stage
human pose estimation with graph pose refinement. In: European Conference on
Computer Vision, Springer, pp 492–508

</span>
</li>
<li id="bib.bib171" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2021)</span>
<span class="ltx_bibblock">
Wang J, Jin S, Liu W, Liu W, Qian C, Luo P (2021) When human pose estimation
meets robustness: Adversarial algorithms and benchmarks. In: Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp
11855–11864

</span>
</li>
<li id="bib.bib172" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2020b)</span>
<span class="ltx_bibblock">
Wang M, Tighe J, Modolo D (2020b) Combining detection and tracking
for human pose estimation in videos. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp 11088–11096

</span>
</li>
<li id="bib.bib173" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2020c)</span>
<span class="ltx_bibblock">
Wang X, Gao L, Song J, Shen HT (2020c) Ktn: Knowledge transfer
network for multi-person densepose estimation. In: Proceedings of the 28th
ACM International Conference on Multimedia, pp 3780–3788

</span>
</li>
<li id="bib.bib174" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang and Mori (2008)</span>
<span class="ltx_bibblock">
Wang Y, Mori G (2008) Multiple tree models for occlusion and spatial
constraints in human pose estimation. In: European Conference on Computer
Vision, Springer, pp 710–724

</span>
</li>
<li id="bib.bib175" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2011)</span>
<span class="ltx_bibblock">
Wang Y, Tran D, Liao Z (2011) Learning hierarchical poselets for human parsing.
In: CVPR 2011, IEEE, pp 1705–1712

</span>
</li>
<li id="bib.bib176" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wehrbein et al. (2021)</span>
<span class="ltx_bibblock">
Wehrbein T, Rudolph M, Rosenhahn B, Wandt B (2021) Probabilistic monocular 3d
human pose estimation with normalizing flows. arXiv preprint arXiv:210713788

</span>
</li>
<li id="bib.bib177" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2020)</span>
<span class="ltx_bibblock">
Wei F, Sun X, Li H, Wang J, Lin S (2020) Point-set anchors for object
detection, instance segmentation and pose estimation. In: European Conference
on Computer Vision, Springer, pp 527–544

</span>
</li>
<li id="bib.bib178" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2016)</span>
<span class="ltx_bibblock">
Wei SE, Ramakrishna V, Kanade T, Sheikh Y (2016) Convolutional pose machines.
In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)

</span>
</li>
<li id="bib.bib179" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2017)</span>
<span class="ltx_bibblock">
Wu J, Zheng H, Zhao B, Li Y, Yan B, Liang R, Wang W, Zhou S, Lin G, Fu Y,
et al. (2017) Ai challenger: A large-scale dataset for going deeper in image
understanding. arXiv preprint arXiv:171106475

</span>
</li>
<li id="bib.bib180" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xia et al. (2017)</span>
<span class="ltx_bibblock">
Xia F, Wang P, Chen X, Yuille AL (2017) Joint multi-person pose estimation and
semantic part segmentation. In: Proceedings of the IEEE conference on
computer vision and pattern recognition, pp 6769–6778

</span>
</li>
<li id="bib.bib181" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao et al. (2018)</span>
<span class="ltx_bibblock">
Xiao B, Wu H, Wei Y (2018) Simple baselines for human pose estimation and
tracking. In: Proceedings of the European conference on computer vision
(ECCV), pp 466–481

</span>
</li>
<li id="bib.bib182" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiu et al. (2018)</span>
<span class="ltx_bibblock">
Xiu Y, Li J, Wang H, Fang Y, Lu C (2018) Pose flow: Efficient online pose
tracking. arXiv preprint arXiv:180200977

</span>
</li>
<li id="bib.bib183" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2020)</span>
<span class="ltx_bibblock">
Xu X, Zou Q, Lin X (2020) Alleviating human-level shift: A robust domain
adaptation method for multi-person pose estimation. In: Proceedings of the
28th ACM International Conference on Multimedia, pp 2326–2335

</span>
</li>
<li id="bib.bib184" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2017)</span>
<span class="ltx_bibblock">
Yang W, Li S, Ouyang W, Li H, Wang X (2017) Learning feature pyramids for human
pose estimation. In: proceedings of the IEEE international conference on
computer vision, pp 1281–1290

</span>
</li>
<li id="bib.bib185" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang and Ramanan (2012)</span>
<span class="ltx_bibblock">
Yang Y, Ramanan D (2012) Articulated human detection with flexible mixtures of
parts. IEEE transactions on pattern analysis and machine intelligence
35(12):2878–2890

</span>
</li>
<li id="bib.bib186" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2021)</span>
<span class="ltx_bibblock">
Yang Y, Ren Z, Li H, Zhou C, Wang X, Hua G (2021) Learning dynamics via graph
neural networks for human pose estimation and tracking. In: Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp
8074–8084

</span>
</li>
<li id="bib.bib187" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2021)</span>
<span class="ltx_bibblock">
Yu C, Xiao B, Gao C, Yuan L, Zhang L, Sang N, Wang J (2021) Lite-hrnet: A
lightweight high-resolution network. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp 10440–10450

</span>
</li>
<li id="bib.bib188" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2018)</span>
<span class="ltx_bibblock">
Yu D, Su K, Sun J, Wang C (2018) Multi-person pose estimation for pose tracking
with enhanced cascaded pyramid network. In: Proceedings of the European
Conference on Computer Vision (ECCV), pp 0–0

</span>
</li>
<li id="bib.bib189" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al. (2020)</span>
<span class="ltx_bibblock">
Yuan L, Zhang S, Fubiao F, Wei N, Pan H (2020) Combined distillation pose. In:
Proceedings of the 28th ACM International Conference on Multimedia, pp
4635–4639

</span>
</li>
<li id="bib.bib190" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et al. (2021)</span>
<span class="ltx_bibblock">
Zeng A, Sun X, Yang L, Zhao N, Liu M, Xu Q (2021) Learning skeletal graph
neural networks for hard 3d pose estimation. arXiv preprint arXiv:210807181

</span>
</li>
<li id="bib.bib191" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang and Shah (2015)</span>
<span class="ltx_bibblock">
Zhang D, Shah M (2015) Human pose estimation in videos. In: Proceedings of the
IEEE International Conference on Computer Vision (ICCV)

</span>
</li>
<li id="bib.bib192" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2018a)</span>
<span class="ltx_bibblock">
Zhang D, Guo G, Huang D, Han J (2018a) Poseflow: A deep motion
representation for understanding human behaviors in videos. In: Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, pp
6762–6770

</span>
</li>
<li id="bib.bib193" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2020a)</span>
<span class="ltx_bibblock">
Zhang F, Zhu X, Dai H, Ye M, Zhu C (2020a) Distribution-aware
coordinate representation for human pose estimation. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp 7093–7102

</span>
</li>
<li id="bib.bib194" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2019)</span>
<span class="ltx_bibblock">
Zhang J, Zhu Z, Zou W, Li P, Li Y, Su H, Huang G (2019) Fastpose: Towards
real-time pose estimation and tracking via scale-normalized multi-task
networks. arXiv preprint arXiv:190805593

</span>
</li>
<li id="bib.bib195" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2013)</span>
<span class="ltx_bibblock">
Zhang W, Zhu M, Derpanis KG (2013) From actemes to action: A
strongly-supervised representation for detailed action understanding. In:
Proceedings of the IEEE International Conference on Computer Vision, pp
2248–2255

</span>
</li>
<li id="bib.bib196" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2009)</span>
<span class="ltx_bibblock">
Zhang X, Li C, Tong X, Hu W, Maybank S, Zhang Y (2009) Efficient human pose
estimation via parsing a tree structure based human model. In: 2009 IEEE 12th
International Conference on Computer Vision, IEEE, pp 1349–1356

</span>
</li>
<li id="bib.bib197" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2018b)</span>
<span class="ltx_bibblock">
Zhang X, Zhou X, Lin M, Sun J (2018b) Shufflenet: An extremely
efficient convolutional neural network for mobile devices. In: Proceedings of
the IEEE conference on computer vision and pattern recognition, pp 6848–6856

</span>
</li>
<li id="bib.bib198" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2020b)</span>
<span class="ltx_bibblock">
Zhang Y, Wang Y, Camps O, Sznaier M (2020b) Key frame proposal
network for efficient pose estimation in videos. In: European Conference on
Computer Vision, Springer, pp 609–625

</span>
</li>
<li id="bib.bib199" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2018)</span>
<span class="ltx_bibblock">
Zhao M, Li T, Abu Alsheikh M, Tian Y, Zhao H, Torralba A, Katabi D (2018)
Through-wall human pose estimation using radio signals. In: Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition, pp 7356–7365

</span>
</li>
<li id="bib.bib200" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. (2020)</span>
<span class="ltx_bibblock">
Zheng C, Wu W, Yang T, Zhu S, Chen C, Liu R, Shen J, Kehtarnavaz N, Shah M
(2020) Deep learning-based human pose estimation: A survey. arXiv preprint
arXiv:201213392

</span>
</li>
<li id="bib.bib201" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2020a)</span>
<span class="ltx_bibblock">
Zhou C, Ren Z, Hua G (2020a) Temporal keypoint matching and
refinement network for pose estimation and tracking. In: European Conference
on Computer Vision, Springer, pp 680–695

</span>
</li>
<li id="bib.bib202" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2018)</span>
<span class="ltx_bibblock">
Zhou G, Fan Y, Cui R, Bian W, Zhu X, Gai K (2018) Rocket launching: A universal
and efficient framework for training well-performing light net. In:
Thirty-second AAAI conference on artificial intelligence

</span>
</li>
<li id="bib.bib203" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2020b)</span>
<span class="ltx_bibblock">
Zhou L, Chen Y, Gao Y, Wang J, Lu H (2020b) Occlusion-aware
siamese network for human pose estimation. In: European Conference on
Computer Vision, Springer, pp 396–412

</span>
</li>
<li id="bib.bib204" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2017)</span>
<span class="ltx_bibblock">
Zhou X, Huang Q, Sun X, Xue X, Wei Y (2017) Towards 3d human pose estimation in
the wild: a weakly-supervised approach. In: Proceedings of the IEEE
International Conference on Computer Vision, pp 398–407

</span>
</li>
<li id="bib.bib205" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2020)</span>
<span class="ltx_bibblock">
Zhu X, Su W, Lu L, Li B, Wang X, Dai J (2020) Deformable detr: Deformable
transformers for end-to-end object detection. arXiv preprint arXiv:201004159

</span>
</li>
<li id="bib.bib206" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zou et al. (2021)</span>
<span class="ltx_bibblock">
Zou S, Guo C, Zuo X, Wang S, Wang P, Hu X, Chen S, Gong M, Cheng L (2021)
Eventhpe: Event-based 3d human pose and shape estimation. arXiv preprint
arXiv:210806819

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2204.07369" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2204.07370" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2204.07370">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2204.07370" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2204.07371" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Mar 11 12:10:17 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
