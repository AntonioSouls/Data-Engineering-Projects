<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2307.00724] LXL: LiDAR Excluded Lean 3D Object Detection with 4D Imaging Radar and Camera Fusion</title><meta property="og:description" content="As an emerging technology and a relatively affordable device, the 4D imaging radar has already been confirmed effective in performing 3D object detection in autonomous driving. Nevertheless, the sparsity and noisiness …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="LXL: LiDAR Excluded Lean 3D Object Detection with 4D Imaging Radar and Camera Fusion">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="LXL: LiDAR Excluded Lean 3D Object Detection with 4D Imaging Radar and Camera Fusion">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2307.00724">

<!--Generated on Wed Feb 28 20:10:48 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
4D imaging radar,  camera,  multi-modal fusion,  3D object detection,  deep learning,  autonomous driving
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">LXL: LiDAR Excluded Lean 3D Object Detection with 4D Imaging Radar and Camera Fusion</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
</span><span class="ltx_author_notes">W. Xiong and B. Zhu are with the School of Automation Science and Electrical Engineering, Beihang University, Beijing 100191, P.R. China. Email:
weiyixiong@buaa.edu.cn (W. Xiong);
zhubing@buaa.edu.cn (B. Zhu).J. Liu is with Vitalent Consulting, Gothenburg, Sweden. Email: jianan.liu@vitalent.se.T. Huang is with the College of Science and Engineering, James Cook University, Cairns QLD 4878, Australia. Email: tao.huang1@jcu.edu.au.Q.-L. Han is with the School of Science, Computing and Engineering Technologies, Swinburne University of Technology, Melbourne, VIC 3122, Australia. Email: qhan@swin.edu.au.Y. Xia is with the Department of Electrical Engineering, Chalmers University of Technology, Gothenburg, Sweden. Email: yuxuan.xia@chalmers.se.1Both authors contribute equally to the work and are co-first authors.2Corresponding author.This paper has been accepted by IEEE Transactions on Intelligent Vehicles. Digital Object Identifier 10.1109/TIV.2023.3321240
<span class="ltx_contact ltx_role_affiliation">
Weiyi Xiong1,
Jianan Liu1,
Tao Huang, 
<br class="ltx_break">Qing-Long Han, 
Yuxuan Xia, and
Bing Zhu2, 

</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">As an emerging technology and a relatively affordable device, the 4D imaging radar has already been confirmed effective in performing 3D object detection in autonomous driving. Nevertheless, the sparsity and noisiness of 4D radar point clouds hinder further performance improvement, and in-depth studies about its fusion with other modalities are lacking.
On the other hand, as a new image view transformation strategy, “sampling” has been applied in a few image-based detectors and shown to outperform the widely applied “depth-based splatting” proposed in Lift-Splat-Shoot (LSS), even without image depth prediction.
However, the potential of “sampling” is not fully unleashed.
This paper investigates the “sampling” view transformation strategy on
the camera and 4D imaging radar fusion-based 3D object detection.
LiDAR Excluded Lean (LXL) model, predicted image depth distribution maps and radar 3D occupancy grids are generated from image perspective view (PV) features and radar bird’s eye view (BEV) features, respectively. They are sent to the core of LXL, called “radar occupancy-assisted depth-based sampling”, to aid image view transformation.
We demonstrated that more accurate view transformation can be performed by introducing image depths and radar information to enhance the “sampling” strategy.
Experiments on VoD and TJ4DRadSet datasets show that the proposed method outperforms the state-of-the-art 3D object detection methods by a significant margin without bells and whistles. Ablation studies demonstrate that our method performs the best among different enhancement settings.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
4D imaging radar, camera, multi-modal fusion, 3D object detection, deep learning, autonomous driving

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Perception plays a pivotal role in autonomous driving, since its subsequent procedures (e.g., trajectory prediction, motion planning and control) rely on accurately perceiving the environment. Key tasks in this domain encompass segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite>, object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite>, and tracking <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite>, with 3D object detection being the most widely researched area.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The approach to performing 3D object detection in autonomous driving varies based on the type of sensor employed. LiDARs, cameras, and radars are commonly utilized sensors characterized by distinct data structures and properties. LiDAR data is in the form of point clouds, providing precise 3D geometric information regarding an object’s shape, size, and position. Meanwhile, camera images offer dense and regular data, supplying rich semantic information.
However, the high cost of LiDARs prohibits their widespread adoption in household vehicles, and cameras are susceptible to challenging lighting and weather conditions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>The comparison of conventional automotive radars (3D radars) and 4D imaging radars (4D radars) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>]</cite>.</figcaption>
<table id="S1.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S1.T1.1.1" class="ltx_tr">
<td id="S1.T1.1.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Characteristics</td>
<td id="S1.T1.1.1.2" class="ltx_td ltx_align_left ltx_border_t">3D Radars</td>
<td id="S1.T1.1.1.3" class="ltx_td ltx_align_left ltx_border_t">4D Radars</td>
</tr>
<tr id="S1.T1.1.2" class="ltx_tr">
<td id="S1.T1.1.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Ability to Measure Elevation Angles</td>
<td id="S1.T1.1.2.2" class="ltx_td ltx_align_left ltx_border_t">Weaker</td>
<td id="S1.T1.1.2.3" class="ltx_td ltx_align_left ltx_border_t">Stronger</td>
</tr>
<tr id="S1.T1.1.3" class="ltx_tr">
<td id="S1.T1.1.3.1" class="ltx_td ltx_align_left ltx_border_r">Resolution</td>
<td id="S1.T1.1.3.2" class="ltx_td ltx_align_left">Lower</td>
<td id="S1.T1.1.3.3" class="ltx_td ltx_align_left">Higher</td>
</tr>
<tr id="S1.T1.1.4" class="ltx_tr">
<td id="S1.T1.1.4.1" class="ltx_td ltx_align_left ltx_border_r">Clutter / Noise</td>
<td id="S1.T1.1.4.2" class="ltx_td ltx_align_left">More</td>
<td id="S1.T1.1.4.3" class="ltx_td ltx_align_left">Less</td>
</tr>
<tr id="S1.T1.1.5" class="ltx_tr">
<td id="S1.T1.1.5.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">Number of Points</td>
<td id="S1.T1.1.5.2" class="ltx_td ltx_align_left ltx_border_b">Smaller</td>
<td id="S1.T1.1.5.3" class="ltx_td ltx_align_left ltx_border_b">Larger</td>
</tr>
</table>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In contrast, radars are cost-effective and resilient to external factors, making them vital for robust detection in current advanced driver assistance systems (ADAS) and autonomous driving <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite>. Moreover, radars hold promise for future applications in cooperative perception <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite>.
However, as shown in Table <a href="#S1.T1" title="TABLE I ‣ I Introduction ‣ LXL: LiDAR Excluded Lean 3D Object Detection with 4D Imaging Radar and Camera Fusion" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, conventional automotive radars, when used alone, lack height information and generate sparse point clouds, posing challenges for 3D object detection. The emergence of 4D imaging radars has led to the generation of higher-resolution 3D point clouds <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>]</cite>. Although there is still a notable disparity in density and quality compared to LiDAR point clouds, several studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite> have explored 4D radar-based detection and demonstrated its feasibility.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In 3D object detection, researchers increasingly turn to multi-modal fusion techniques to overcome the limitations associated with single-modal data to improve the overall performance. One prominent approach involves independently extracting bird’s-eye-view (BEV) features from different sensor modalities and integrating them into a unified feature map.
The utilization of BEV representation offers numerous advantages. Firstly, it enables more efficient processing compared to point-based or voxel-based methods. Additionally, leveraging mature 2D detection techniques can facilitate learning processes. Furthermore, occlusion, a common challenge in other representations like the range-view, is mitigated in BEV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a>]</cite>. Notably, using BEV representation simplifies and enhances the effectiveness of multi-modal fusion strategies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Despite the benefits of using BEV representation for multi-modal fusion in 3D object detection, transforming images from a perspective view (PV) to BEV is intricate. Current approaches can be categorized into geometry-based <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite> and network-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite>. Geometry-based approaches, relying on explicit utilization of calibration matrices, offer a more straightforward learning process than network-based approaches. One widely employed geometry-based method is “depth-based splatting”. Initially introduced in Lift-Splat-Shoot (LSS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite>, this method lifts image pixels into 3D space guided by predicted pixel depth distributions.
Several enhancements have been proposed to improve its performance. For example, BEVDepth <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite> generates a “ground-truth” depth map from LiDAR points to supervise image depth prediction, whereas CRN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite> employs a 2D radar occupancy map to assist view transformation. Another approach, called “sampling”, has demonstrated superior performance even without explicit depth prediction, as exemplified by Simple-BEV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite>.
However, unlike the common practice of “splatting”, few studies explored the combination of “sampling” with predicted depths. Moreover, the potential of “sampling” in conjunction with other modalities remains largely unexplored, indicating untapped opportunities for further improvement in this area.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Despite growing interest in multi-modal fusion techniques for 3D object detection, the specific integration of 4D imaging radar and cameras has received limited attention in the existing literature.
Existing methods designed for LiDAR-based fusion, such as the popular “splatting” approach, is applicable to 4D imaging radar and camera fusion, but their enhancements like BEV-Depth <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite> may fail due to the distinct characteristics of radar point clouds. Specifically, when point clouds from 4D radars instead of LiDARs can be accessed, the generated depth maps in BEV-Depth may suffer from sparsity and imprecision of radar points.
Additionally, methods devised specifically for radars, such as the technique presented in CRN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite>, may introduce computational complexity and hinder the real-time inference capabilities of the model.
Therefore, there is a clear need to address this research gap by developing novel fusion methods tailored for 4D imaging radar and camera fusion.
In this study, we aim to enhance the existing “sampling” method by leveraging the unique advantages of 4D imaging radar.
By conducting extensive ablation studies, we show how 4D imaging radar can assist in image view transformation and demonstrate its impact on the overall 3D object detection performance.
The contributions of this work are threefold:</p>
</div>
<div id="S1.p7" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">A LiDAR Excluded Lean (LXL) model is proposed
to perform 4D imaging radar and camera fusion-based 3D object detection.
This is an early attempt in this field, and serves as the latest benchmark for subsequent studies.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">A “radar occupancy-assisted depth-based sampling” feature lifting strategy is proposed in our view transformation module. It
utilizes bi-linear sampling to get image features for pre-defined voxels, followed by two parallel operations: one combines image 3D features with the information from predicted image depth distribution maps, and the other exploits estimated radar 3D occupancy grids.
This design enhances the underdeveloped “sampling” strategy by introducing predicted depth distribution maps and radar 3D occupancy grids as assistance, leading to more precise feature lifting results.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Experiments show that LXL outperforms state-of-the-art models on the View-of-Delft (VoD) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite> and TJ4DRadSet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite> datasets by 6.7% and 2.5%, respectively, demonstrating the effectiveness of LXL.
In addition, comparisons of different feature lifting and radar assistance strategies are made through ablation studies, showing the superiority of the proposed view transformation module.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">The rest of the paper is organized as follows. Section <a href="#S2" title="II Related Work ‣ LXL: LiDAR Excluded Lean 3D Object Detection with 4D Imaging Radar and Camera Fusion" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> reviews recent works on camera-based, camera and ordinary automotive radar fusion-based, and 4D imaging radar-based 3D object detection methods. Section <a href="#S3" title="III Proposed Method ‣ LXL: LiDAR Excluded Lean 3D Object Detection with 4D Imaging Radar and Camera Fusion" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> details our proposed model, with focus on the view transformation module. Experimental settings and performances of our model and the corresponding analysis are provided in section <a href="#S4" title="IV Experiments and Analysis ‣ LXL: LiDAR Excluded Lean 3D Object Detection with 4D Imaging Radar and Camera Fusion" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>. Finally, we summarize the work in this paper and point out the future research direction in Section <a href="#S5" title="V Conclusion ‣ LXL: LiDAR Excluded Lean 3D Object Detection with 4D Imaging Radar and Camera Fusion" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Work</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">3D Object Detection with Cameras</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The camera-based 3D object detection work can be mainly categorized into three types.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">The first type involves directly estimating 3D bounding boxes based on image PV features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite>. However, due to the inherent lack of depth information in images, the performances of these methods are limited.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">The second type focuses on <span id="S2.SS1.p3.1.1" class="ltx_text ltx_font_italic">directly</span> transforming PV features into BEV and predicting bounding boxes on the top-down view <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite>. This approach requires additional information, e.g. pixel heights or depths, to achieve accurate view transformation. Inverse perspective mapping (IPM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite> projects PV features onto the BEV with the assumption that all pixels lie on the ground. However, its performance is limited as the assumption is not always true.
Other methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite> utilize transformers to learn view transformation and reduce the impact of inaccurate depth estimation.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p">The third type involves lifting pixels into a point cloud or voxels <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite> and applying networks designed for LiDAR-based 3D object detection. These approaches assume or estimate pixel depth or depth distributions to guide the 2D-to-3D projection. Pseudo-LiDAR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite> is a pioneering work that transforms images into point clouds based on regressed depth. However, these networks cannot be trained end-to-end, limiting their performance. CaDDN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite>, BEVDet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite>, and BEVDepth <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite> employ a technique where they discretize the depth space into bins, and treat the depth estimation as depth bin classification. Subsequently, image features are lifted into voxels based on the estimated depth distribution. In contrast, M<sup id="S2.SS1.p4.1.1" class="ltx_sup">2</sup>BEV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite> adopts a different strategy by assuming a uniform depth distribution instead of predicting depth probabilities. This approach mitigates the computational burden while allowing for effectively lifting image features into voxels.</p>
</div>
<div id="S2.SS1.p5" class="ltx_para">
<p id="S2.SS1.p5.1" class="ltx_p">It is important to note that most of the methods belonging to the third type also detect objects on the BEV. Projecting image features from PV to BEV alleviates the occlusion problem and facilitates multi-modal fusion. As a result, these types of methods have become more prevalent in recent years.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">3D Object Detection with Camera and Ordinary Automotive Radar Fusion</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Ordinary automotive radars cannot measure height information, posing a challenge for models to estimate 3D bounding boxes accurately from 2D radar points solely. Consequently, researchers have explored the fusion of 3D radar and camera data for improved 3D object detection, with earlier works including <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Recent advancements in this field have introduced novel approaches to fuse camera and radar data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>]</cite>. For instance, Simple-BEV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite> lifts image pixels to 3D voxels and concatenates them with radar BEV features before reducing the height dimension. To enhance the model’s resilience against modal failure, CramNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite> transforms the image foreground into a point cloud and utilizes it along with radar points for subsequent bounding box prediction. Given the relatively low reliability of image depth estimation, ray-constrained cross-attention is proposed in CramNet to refine the 3D location of pixels. Another work, RCBEV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">33</span></a>]</cite>, employs a spatial-temporal encoder to extract features from accumulated radar sweeps and introduces a two-stage multi-modal fusion strategy.
In contrast to many previous approaches focusing on feature-level fusion, RADIANT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a>]</cite> adopts a result-level fusion strategy, merging depth predictions from radar and camera heads to achieve lower localization errors.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">Attention mechanisms and transformers have been leveraged to enhance camera and radar fusion performance. MVFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite> and CRN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite> utilize cross-attention to fuse camera and radar features. Additionally, they employ one modal as a guide to process the other modal. Specifically, MVFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite> derives a Semantic Indicator from images to extract image-guided radar features, while CRN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite> projects radar points onto images and generates occupancy maps within the view frustum, facilitating the depth-based PV to BEV transformation of image features. CRAFT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">36</span></a>]</cite> primarily focuses on image detection and utilizes radar measurements to refine image proposals through the Spatio-Contextual Fusion Transformer. TransCAR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">37</span></a>]</cite> incorporates a transformer decoder where vision-updated queries interact with radar features.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.5.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.6.2" class="ltx_text ltx_font_italic">3D Object Detection with 4D Imaging Radars</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">With advancements in 4D imaging radars, which can generate 3D point clouds, there is a possibility of regressing 3D bounding boxes using radar modality alone. However, despite the availability of a few datasets providing 4D radar point clouds <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite> and even 4D radar tensors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">39</span></a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite>, the research in this area remains limited. Most of the existing works in this field incorporate modules from LiDAR-based models, such as using the backbone of SECOND <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite> or the detection head of CenterPoint <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a>]</cite>. Nevertheless, 3D point clouds generated by 4D imaging radars are typically sparser and noisier than LiDAR point clouds, often leading to lower model performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite>.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">For example, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite> applies PointPillars <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite> to perform 3D object detection using 4D imaging radars and achieves reasonable results. RPFA-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite> modifies the pillarization operation of PointPillars <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite> by replacing the PointNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>]</cite> with a self-attention mechanism to extract global features, thereby enhancing the model’s orientation estimation capability. Using a spatial-temporal feature extractor on multi-frame radar point clouds and employing an anchor-based detection head, RadarMFNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite> achieves more accurate detection than single-frame methods. In the most recent work, SMURF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">45</span></a>]</cite>, a multi-representation fusion strategy is adopted where the PointPillars <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite> backbone and kernel density estimation (KDE) extract different radar features in parallel, enabling the acquisition of enhanced feature maps of radar points.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">To leverage the data from other sensors such as LiDARs and cameras, researchers start to explore the fusion of these modalities with 4D imaging radars to achieve improved results. For instance, InterFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite> and M<sup id="S2.SS3.p3.1.1" class="ltx_sup">2</sup>-Fusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">47</span></a>]</cite> employ attention mechanisms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">48</span></a>]</cite> to fuse pillarized LiDAR and radar features. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">49</span></a>]</cite> utilizes self-supervised model adaptation (SSMA) blocks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">50</span></a>]</cite> for a pixel-level fusion of image, radar BEV, and radar front view (FV) features. Similarly, RCFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite> incorporates an interactive attention module to fuse radar and image BEV features.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p id="S2.SS3.p4.1" class="ltx_p">As 4D radar and camera fusion-based 3D object detection remains an area requiring further investigation, this paper aims to inspire subsequent researchers to explore this domain.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2307.00724/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="380" height="173" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The overall architecture of LXL. The radar branch and image branch extract features of the corresponding modal, resulting in radar BEV feature maps and image PV feature maps. After that, radar 3D occupancy grids and image depth distribution maps are generated and sent to the view transformation module, assisting the image PV-to-BEV transformation. The resulted image BEV feature maps are fused with radar BEV feature maps, and bounding boxes are predicted from the output feature map of the fusion module.</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Proposed Method</span>
</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">Overall Architecture</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The overall architecture of our model is depicted in Fig. <a href="#S2.F1" title="Figure 1 ‣ II-C 3D Object Detection with 4D Imaging Radars ‣ II Related Work ‣ LXL: LiDAR Excluded Lean 3D Object Detection with 4D Imaging Radar and Camera Fusion" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The model comprises four main components: the radar branch with an occupancy net, image branch with a depth net and a specially designed view transformation module, fusion module, and detection head. Each component plays a crucial role in the 3D object detection process:</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.ix1.p1" class="ltx_para">
<p id="S3.I1.ix1.p1.1" class="ltx_p">The radar branch is responsible for processing radar point clouds as an input. It extracts radar BEV features, which capture essential information from the radar modality. Additionally, the radar branch generates 3D radar occupancy grids, representing the radar points’ occupancy status within the scene.</p>
</div>
</li>
<li id="S3.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.ix2.p1" class="ltx_para">
<p id="S3.I1.ix2.p1.1" class="ltx_p">The image branch focuses on extracting multi-scale image PV features. These features encode relevant visual information from the image modality. We employ predicted image depth distribution maps and 3D radar occupancy grids to assist in transforming the image PV features into the BEV domain. Aligning the image features with the radar BEV representation enables effective fusion with the radar features.</p>
</div>
</li>
<li id="S3.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I1.ix3.p1" class="ltx_para">
<p id="S3.I1.ix3.p1.1" class="ltx_p">The fusion module is a key component in integrating the BEV features from both radar and image branches. It combines the complementary information each modality provides, allowing for enhanced object detection performance. The fusion process leverages the BEV features to generate a unified representation that captures the combined strengths of radar and image data.</p>
</div>
</li>
<li id="S3.I1.ix4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S3.I1.ix4.p1" class="ltx_para">
<p id="S3.I1.ix4.p1.1" class="ltx_p">The detection head is responsible for bounding box regression and classification for each potential object in the scene. It utilizes the fused features to estimate the 3D position, dimensions, orientation, and category of the objects. By leveraging the comprehensive information from both radar and image modalities, the detection head produces accurate predictions.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">Further details regarding the proposed method are elaborated in the subsequent subsections.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Radar Branch</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In the radar branch, the input radar point cloud is initially
pillarized via the process employed in PointPillars <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite>.
Subsequently, the pillar representation is fed into the radar backbone and neck modules to extract relevant features. Following the widely referenced network SECOND <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite>, the radar backbone and neck are constructed. The radar backbone extracts multi-level BEV features from the voxelized pillars. These features capture important spatial and contextual information inherent in the radar modality. The radar neck module then combines these multi-level features into a unified single-scale representation, facilitating subsequent fusion and analysis.
The obtained radar BEV feature maps serve two primary purposes within our model. Firstly, they are forwarded to the fusion module, where they are integrated with the image BEV features for effective object detection. Secondly, the radar BEV feature maps are utilized to predict radar 3D occupancy grids. The motivation behind the occupancy generation is discussed further in Section <a href="#S3.SS4" title="III-D View Transformation ‣ III Proposed Method ‣ LXL: LiDAR Excluded Lean 3D Object Detection with 4D Imaging Radar and Camera Fusion" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-D</span></span></a>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.6" class="ltx_p">To generate radar 3D occupancy grids, we employ an occupancy net. The radar BEV feature map, denoted as <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="\mathbf{F}_{\text{BEV}}^{P}" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><msubsup id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2.2" xref="S3.SS2.p2.1.m1.1.1.2.2.cmml">𝐅</mi><mtext id="S3.SS2.p2.1.m1.1.1.2.3" xref="S3.SS2.p2.1.m1.1.1.2.3a.cmml">BEV</mtext><mi id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">P</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">superscript</csymbol><apply id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.2.1.cmml" xref="S3.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.2.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2.2">𝐅</ci><ci id="S3.SS2.p2.1.m1.1.1.2.3a.cmml" xref="S3.SS2.p2.1.m1.1.1.2.3"><mtext mathsize="70%" id="S3.SS2.p2.1.m1.1.1.2.3.cmml" xref="S3.SS2.p2.1.m1.1.1.2.3">BEV</mtext></ci></apply><ci id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">\mathbf{F}_{\text{BEV}}^{P}</annotation></semantics></math> with a shape <math id="S3.SS2.p2.2.m2.3" class="ltx_Math" alttext="(X,Y,C_{P})" display="inline"><semantics id="S3.SS2.p2.2.m2.3a"><mrow id="S3.SS2.p2.2.m2.3.3.1" xref="S3.SS2.p2.2.m2.3.3.2.cmml"><mo stretchy="false" id="S3.SS2.p2.2.m2.3.3.1.2" xref="S3.SS2.p2.2.m2.3.3.2.cmml">(</mo><mi id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml">X</mi><mo id="S3.SS2.p2.2.m2.3.3.1.3" xref="S3.SS2.p2.2.m2.3.3.2.cmml">,</mo><mi id="S3.SS2.p2.2.m2.2.2" xref="S3.SS2.p2.2.m2.2.2.cmml">Y</mi><mo id="S3.SS2.p2.2.m2.3.3.1.4" xref="S3.SS2.p2.2.m2.3.3.2.cmml">,</mo><msub id="S3.SS2.p2.2.m2.3.3.1.1" xref="S3.SS2.p2.2.m2.3.3.1.1.cmml"><mi id="S3.SS2.p2.2.m2.3.3.1.1.2" xref="S3.SS2.p2.2.m2.3.3.1.1.2.cmml">C</mi><mi id="S3.SS2.p2.2.m2.3.3.1.1.3" xref="S3.SS2.p2.2.m2.3.3.1.1.3.cmml">P</mi></msub><mo stretchy="false" id="S3.SS2.p2.2.m2.3.3.1.5" xref="S3.SS2.p2.2.m2.3.3.2.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.3b"><vector id="S3.SS2.p2.2.m2.3.3.2.cmml" xref="S3.SS2.p2.2.m2.3.3.1"><ci id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">𝑋</ci><ci id="S3.SS2.p2.2.m2.2.2.cmml" xref="S3.SS2.p2.2.m2.2.2">𝑌</ci><apply id="S3.SS2.p2.2.m2.3.3.1.1.cmml" xref="S3.SS2.p2.2.m2.3.3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.3.3.1.1.1.cmml" xref="S3.SS2.p2.2.m2.3.3.1.1">subscript</csymbol><ci id="S3.SS2.p2.2.m2.3.3.1.1.2.cmml" xref="S3.SS2.p2.2.m2.3.3.1.1.2">𝐶</ci><ci id="S3.SS2.p2.2.m2.3.3.1.1.3.cmml" xref="S3.SS2.p2.2.m2.3.3.1.1.3">𝑃</ci></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.3c">(X,Y,C_{P})</annotation></semantics></math>, is fed into the occupancy net. Here, <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><mi id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><ci id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">X</annotation></semantics></math> and <math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><mi id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><ci id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">Y</annotation></semantics></math> represent the dimensions of the feature map, and <math id="S3.SS2.p2.5.m5.1" class="ltx_Math" alttext="C_{P}" display="inline"><semantics id="S3.SS2.p2.5.m5.1a"><msub id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml"><mi id="S3.SS2.p2.5.m5.1.1.2" xref="S3.SS2.p2.5.m5.1.1.2.cmml">C</mi><mi id="S3.SS2.p2.5.m5.1.1.3" xref="S3.SS2.p2.5.m5.1.1.3.cmml">P</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><apply id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.5.m5.1.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.p2.5.m5.1.1.2.cmml" xref="S3.SS2.p2.5.m5.1.1.2">𝐶</ci><ci id="S3.SS2.p2.5.m5.1.1.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">C_{P}</annotation></semantics></math> corresponds to the number of channels. In our framework, the height of the 3D occupancy grid, denoted as <math id="S3.SS2.p2.6.m6.1" class="ltx_Math" alttext="Z" display="inline"><semantics id="S3.SS2.p2.6.m6.1a"><mi id="S3.SS2.p2.6.m6.1.1" xref="S3.SS2.p2.6.m6.1.1.cmml">Z</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m6.1b"><ci id="S3.SS2.p2.6.m6.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1">𝑍</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m6.1c">Z</annotation></semantics></math>, is predefined. The occupancy net can be formulated as follows:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="\mathbf{O}_{\text{3D}}^{P}=\mathtt{Sigmoid}(\mathtt{Conv}_{C_{P}\rightarrow Z}(\mathbf{F}_{\text{BEV}}^{P}))," display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><msubsup id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.3.2.2" xref="S3.E1.m1.1.1.1.1.3.2.2.cmml">𝐎</mi><mtext id="S3.E1.m1.1.1.1.1.3.2.3" xref="S3.E1.m1.1.1.1.1.3.2.3a.cmml">3D</mtext><mi id="S3.E1.m1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.3.3.cmml">P</mi></msubsup><mo id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.3.cmml">𝚂𝚒𝚐𝚖𝚘𝚒𝚍</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E1.m1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.2.cmml">𝙲𝚘𝚗𝚟</mi><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.cmml"><msub id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.2.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.2.2.cmml">C</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.2.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.2.3.cmml">P</mi></msub><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.1.cmml">→</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3.cmml">Z</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msubsup id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">𝐅</mi><mtext id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.3a.cmml">BEV</mtext><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">P</mi></msubsup><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><eq id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"></eq><apply id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3">superscript</csymbol><apply id="S3.E1.m1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2.2">𝐎</ci><ci id="S3.E1.m1.1.1.1.1.3.2.3a.cmml" xref="S3.E1.m1.1.1.1.1.3.2.3"><mtext mathsize="70%" id="S3.E1.m1.1.1.1.1.3.2.3.cmml" xref="S3.E1.m1.1.1.1.1.3.2.3">3D</mtext></ci></apply><ci id="S3.E1.m1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3">𝑃</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"><times id="S3.E1.m1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.2"></times><ci id="S3.E1.m1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.3">𝚂𝚒𝚐𝚖𝚘𝚒𝚍</ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1"><times id="S3.E1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2"></times><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.2">𝙲𝚘𝚗𝚟</ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3"><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.1">→</ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.2.2">𝐶</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.2.3">𝑃</ci></apply><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3">𝑍</ci></apply></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.2">𝐅</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.3a.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.3"><mtext mathsize="70%" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.3">BEV</mtext></ci></apply><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3">𝑃</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\mathbf{O}_{\text{3D}}^{P}=\mathtt{Sigmoid}(\mathtt{Conv}_{C_{P}\rightarrow Z}(\mathbf{F}_{\text{BEV}}^{P})),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p2.11" class="ltx_p">where <math id="S3.SS2.p2.7.m1.1" class="ltx_Math" alttext="\mathbf{O}_{\text{3D}}^{P}\in\mathbb{R}^{X\times Y\times Z}" display="inline"><semantics id="S3.SS2.p2.7.m1.1a"><mrow id="S3.SS2.p2.7.m1.1.1" xref="S3.SS2.p2.7.m1.1.1.cmml"><msubsup id="S3.SS2.p2.7.m1.1.1.2" xref="S3.SS2.p2.7.m1.1.1.2.cmml"><mi id="S3.SS2.p2.7.m1.1.1.2.2.2" xref="S3.SS2.p2.7.m1.1.1.2.2.2.cmml">𝐎</mi><mtext id="S3.SS2.p2.7.m1.1.1.2.2.3" xref="S3.SS2.p2.7.m1.1.1.2.2.3a.cmml">3D</mtext><mi id="S3.SS2.p2.7.m1.1.1.2.3" xref="S3.SS2.p2.7.m1.1.1.2.3.cmml">P</mi></msubsup><mo id="S3.SS2.p2.7.m1.1.1.1" xref="S3.SS2.p2.7.m1.1.1.1.cmml">∈</mo><msup id="S3.SS2.p2.7.m1.1.1.3" xref="S3.SS2.p2.7.m1.1.1.3.cmml"><mi id="S3.SS2.p2.7.m1.1.1.3.2" xref="S3.SS2.p2.7.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS2.p2.7.m1.1.1.3.3" xref="S3.SS2.p2.7.m1.1.1.3.3.cmml"><mi id="S3.SS2.p2.7.m1.1.1.3.3.2" xref="S3.SS2.p2.7.m1.1.1.3.3.2.cmml">X</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.7.m1.1.1.3.3.1" xref="S3.SS2.p2.7.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.SS2.p2.7.m1.1.1.3.3.3" xref="S3.SS2.p2.7.m1.1.1.3.3.3.cmml">Y</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.7.m1.1.1.3.3.1a" xref="S3.SS2.p2.7.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.SS2.p2.7.m1.1.1.3.3.4" xref="S3.SS2.p2.7.m1.1.1.3.3.4.cmml">Z</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.7.m1.1b"><apply id="S3.SS2.p2.7.m1.1.1.cmml" xref="S3.SS2.p2.7.m1.1.1"><in id="S3.SS2.p2.7.m1.1.1.1.cmml" xref="S3.SS2.p2.7.m1.1.1.1"></in><apply id="S3.SS2.p2.7.m1.1.1.2.cmml" xref="S3.SS2.p2.7.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.7.m1.1.1.2.1.cmml" xref="S3.SS2.p2.7.m1.1.1.2">superscript</csymbol><apply id="S3.SS2.p2.7.m1.1.1.2.2.cmml" xref="S3.SS2.p2.7.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.7.m1.1.1.2.2.1.cmml" xref="S3.SS2.p2.7.m1.1.1.2">subscript</csymbol><ci id="S3.SS2.p2.7.m1.1.1.2.2.2.cmml" xref="S3.SS2.p2.7.m1.1.1.2.2.2">𝐎</ci><ci id="S3.SS2.p2.7.m1.1.1.2.2.3a.cmml" xref="S3.SS2.p2.7.m1.1.1.2.2.3"><mtext mathsize="70%" id="S3.SS2.p2.7.m1.1.1.2.2.3.cmml" xref="S3.SS2.p2.7.m1.1.1.2.2.3">3D</mtext></ci></apply><ci id="S3.SS2.p2.7.m1.1.1.2.3.cmml" xref="S3.SS2.p2.7.m1.1.1.2.3">𝑃</ci></apply><apply id="S3.SS2.p2.7.m1.1.1.3.cmml" xref="S3.SS2.p2.7.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.7.m1.1.1.3.1.cmml" xref="S3.SS2.p2.7.m1.1.1.3">superscript</csymbol><ci id="S3.SS2.p2.7.m1.1.1.3.2.cmml" xref="S3.SS2.p2.7.m1.1.1.3.2">ℝ</ci><apply id="S3.SS2.p2.7.m1.1.1.3.3.cmml" xref="S3.SS2.p2.7.m1.1.1.3.3"><times id="S3.SS2.p2.7.m1.1.1.3.3.1.cmml" xref="S3.SS2.p2.7.m1.1.1.3.3.1"></times><ci id="S3.SS2.p2.7.m1.1.1.3.3.2.cmml" xref="S3.SS2.p2.7.m1.1.1.3.3.2">𝑋</ci><ci id="S3.SS2.p2.7.m1.1.1.3.3.3.cmml" xref="S3.SS2.p2.7.m1.1.1.3.3.3">𝑌</ci><ci id="S3.SS2.p2.7.m1.1.1.3.3.4.cmml" xref="S3.SS2.p2.7.m1.1.1.3.3.4">𝑍</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.7.m1.1c">\mathbf{O}_{\text{3D}}^{P}\in\mathbb{R}^{X\times Y\times Z}</annotation></semantics></math> is the predicted 3D occupancy grid and <math id="S3.SS2.p2.8.m2.1" class="ltx_Math" alttext="\mathtt{Conv}_{a\rightarrow b}" display="inline"><semantics id="S3.SS2.p2.8.m2.1a"><msub id="S3.SS2.p2.8.m2.1.1" xref="S3.SS2.p2.8.m2.1.1.cmml"><mi id="S3.SS2.p2.8.m2.1.1.2" xref="S3.SS2.p2.8.m2.1.1.2.cmml">𝙲𝚘𝚗𝚟</mi><mrow id="S3.SS2.p2.8.m2.1.1.3" xref="S3.SS2.p2.8.m2.1.1.3.cmml"><mi id="S3.SS2.p2.8.m2.1.1.3.2" xref="S3.SS2.p2.8.m2.1.1.3.2.cmml">a</mi><mo stretchy="false" id="S3.SS2.p2.8.m2.1.1.3.1" xref="S3.SS2.p2.8.m2.1.1.3.1.cmml">→</mo><mi id="S3.SS2.p2.8.m2.1.1.3.3" xref="S3.SS2.p2.8.m2.1.1.3.3.cmml">b</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.8.m2.1b"><apply id="S3.SS2.p2.8.m2.1.1.cmml" xref="S3.SS2.p2.8.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.8.m2.1.1.1.cmml" xref="S3.SS2.p2.8.m2.1.1">subscript</csymbol><ci id="S3.SS2.p2.8.m2.1.1.2.cmml" xref="S3.SS2.p2.8.m2.1.1.2">𝙲𝚘𝚗𝚟</ci><apply id="S3.SS2.p2.8.m2.1.1.3.cmml" xref="S3.SS2.p2.8.m2.1.1.3"><ci id="S3.SS2.p2.8.m2.1.1.3.1.cmml" xref="S3.SS2.p2.8.m2.1.1.3.1">→</ci><ci id="S3.SS2.p2.8.m2.1.1.3.2.cmml" xref="S3.SS2.p2.8.m2.1.1.3.2">𝑎</ci><ci id="S3.SS2.p2.8.m2.1.1.3.3.cmml" xref="S3.SS2.p2.8.m2.1.1.3.3">𝑏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.8.m2.1c">\mathtt{Conv}_{a\rightarrow b}</annotation></semantics></math> represents a <math id="S3.SS2.p2.9.m3.1" class="ltx_Math" alttext="1\times 1" display="inline"><semantics id="S3.SS2.p2.9.m3.1a"><mrow id="S3.SS2.p2.9.m3.1.1" xref="S3.SS2.p2.9.m3.1.1.cmml"><mn id="S3.SS2.p2.9.m3.1.1.2" xref="S3.SS2.p2.9.m3.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.9.m3.1.1.1" xref="S3.SS2.p2.9.m3.1.1.1.cmml">×</mo><mn id="S3.SS2.p2.9.m3.1.1.3" xref="S3.SS2.p2.9.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.9.m3.1b"><apply id="S3.SS2.p2.9.m3.1.1.cmml" xref="S3.SS2.p2.9.m3.1.1"><times id="S3.SS2.p2.9.m3.1.1.1.cmml" xref="S3.SS2.p2.9.m3.1.1.1"></times><cn type="integer" id="S3.SS2.p2.9.m3.1.1.2.cmml" xref="S3.SS2.p2.9.m3.1.1.2">1</cn><cn type="integer" id="S3.SS2.p2.9.m3.1.1.3.cmml" xref="S3.SS2.p2.9.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.9.m3.1c">1\times 1</annotation></semantics></math> convolution layer with <math id="S3.SS2.p2.10.m4.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S3.SS2.p2.10.m4.1a"><mi id="S3.SS2.p2.10.m4.1.1" xref="S3.SS2.p2.10.m4.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.10.m4.1b"><ci id="S3.SS2.p2.10.m4.1.1.cmml" xref="S3.SS2.p2.10.m4.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.10.m4.1c">a</annotation></semantics></math> input channels and <math id="S3.SS2.p2.11.m5.1" class="ltx_Math" alttext="b" display="inline"><semantics id="S3.SS2.p2.11.m5.1a"><mi id="S3.SS2.p2.11.m5.1.1" xref="S3.SS2.p2.11.m5.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.11.m5.1b"><ci id="S3.SS2.p2.11.m5.1.1.cmml" xref="S3.SS2.p2.11.m5.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.11.m5.1c">b</annotation></semantics></math> output channels. Note that during training, there are no direct supervise signals for occupancy prediction.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">To transform the image view from PV to BEV, we leverage the radar 3D occupancy grids as an assistance. The specific details and the process of this transformation will be elaborated upon in Section <a href="#S3.SS4" title="III-D View Transformation ‣ III Proposed Method ‣ LXL: LiDAR Excluded Lean 3D Object Detection with 4D Imaging Radar and Camera Fusion" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-D</span></span></a>.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.5.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.6.2" class="ltx_text ltx_font_italic">Image Branch</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The image branch consists of several key modules: image backbone, neck, depth net, and view transformation module.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">The image backbone extracts multi-level image PV features. The image neck is embraced to further enhance the features by mixing them at different scales. In our model, we employ the same architecture as YOLOX <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">52</span></a>]</cite> to achieve this design, utilizing CSPNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">53</span></a>]</cite> and PAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">54</span></a>]</cite>.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.4" class="ltx_p">The depth net is implemented as a <math id="S3.SS3.p3.1.m1.1" class="ltx_Math" alttext="1\times 1" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><mrow id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml"><mn id="S3.SS3.p3.1.m1.1.1.2" xref="S3.SS3.p3.1.m1.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p3.1.m1.1.1.1" xref="S3.SS3.p3.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS3.p3.1.m1.1.1.3" xref="S3.SS3.p3.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><apply id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1"><times id="S3.SS3.p3.1.m1.1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1.1"></times><cn type="integer" id="S3.SS3.p3.1.m1.1.1.2.cmml" xref="S3.SS3.p3.1.m1.1.1.2">1</cn><cn type="integer" id="S3.SS3.p3.1.m1.1.1.3.cmml" xref="S3.SS3.p3.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">1\times 1</annotation></semantics></math> convolutional layer for each multi-level image PV feature. Similar to many existing methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite>, we discretize the depth space into multiple bins and treat the depth estimation task as a depth bin classification task. Consequently, the depth net outputs a depth probability distribution for each pixel.
Given the image PV feature map of the <math id="S3.SS3.p3.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS3.p3.2.m2.1a"><mi id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><ci id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">i</annotation></semantics></math>-th level, denoted as <math id="S3.SS3.p3.3.m3.2" class="ltx_Math" alttext="\mathbf{F}_{i,\text{PV}}^{I}\in\mathbb{R}^{H_{i}\times W_{i}\times C_{I}}" display="inline"><semantics id="S3.SS3.p3.3.m3.2a"><mrow id="S3.SS3.p3.3.m3.2.3" xref="S3.SS3.p3.3.m3.2.3.cmml"><msubsup id="S3.SS3.p3.3.m3.2.3.2" xref="S3.SS3.p3.3.m3.2.3.2.cmml"><mi id="S3.SS3.p3.3.m3.2.3.2.2.2" xref="S3.SS3.p3.3.m3.2.3.2.2.2.cmml">𝐅</mi><mrow id="S3.SS3.p3.3.m3.2.2.2.4" xref="S3.SS3.p3.3.m3.2.2.2.3.cmml"><mi id="S3.SS3.p3.3.m3.1.1.1.1" xref="S3.SS3.p3.3.m3.1.1.1.1.cmml">i</mi><mo id="S3.SS3.p3.3.m3.2.2.2.4.1" xref="S3.SS3.p3.3.m3.2.2.2.3.cmml">,</mo><mtext id="S3.SS3.p3.3.m3.2.2.2.2" xref="S3.SS3.p3.3.m3.2.2.2.2a.cmml">PV</mtext></mrow><mi id="S3.SS3.p3.3.m3.2.3.2.3" xref="S3.SS3.p3.3.m3.2.3.2.3.cmml">I</mi></msubsup><mo id="S3.SS3.p3.3.m3.2.3.1" xref="S3.SS3.p3.3.m3.2.3.1.cmml">∈</mo><msup id="S3.SS3.p3.3.m3.2.3.3" xref="S3.SS3.p3.3.m3.2.3.3.cmml"><mi id="S3.SS3.p3.3.m3.2.3.3.2" xref="S3.SS3.p3.3.m3.2.3.3.2.cmml">ℝ</mi><mrow id="S3.SS3.p3.3.m3.2.3.3.3" xref="S3.SS3.p3.3.m3.2.3.3.3.cmml"><msub id="S3.SS3.p3.3.m3.2.3.3.3.2" xref="S3.SS3.p3.3.m3.2.3.3.3.2.cmml"><mi id="S3.SS3.p3.3.m3.2.3.3.3.2.2" xref="S3.SS3.p3.3.m3.2.3.3.3.2.2.cmml">H</mi><mi id="S3.SS3.p3.3.m3.2.3.3.3.2.3" xref="S3.SS3.p3.3.m3.2.3.3.3.2.3.cmml">i</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p3.3.m3.2.3.3.3.1" xref="S3.SS3.p3.3.m3.2.3.3.3.1.cmml">×</mo><msub id="S3.SS3.p3.3.m3.2.3.3.3.3" xref="S3.SS3.p3.3.m3.2.3.3.3.3.cmml"><mi id="S3.SS3.p3.3.m3.2.3.3.3.3.2" xref="S3.SS3.p3.3.m3.2.3.3.3.3.2.cmml">W</mi><mi id="S3.SS3.p3.3.m3.2.3.3.3.3.3" xref="S3.SS3.p3.3.m3.2.3.3.3.3.3.cmml">i</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p3.3.m3.2.3.3.3.1a" xref="S3.SS3.p3.3.m3.2.3.3.3.1.cmml">×</mo><msub id="S3.SS3.p3.3.m3.2.3.3.3.4" xref="S3.SS3.p3.3.m3.2.3.3.3.4.cmml"><mi id="S3.SS3.p3.3.m3.2.3.3.3.4.2" xref="S3.SS3.p3.3.m3.2.3.3.3.4.2.cmml">C</mi><mi id="S3.SS3.p3.3.m3.2.3.3.3.4.3" xref="S3.SS3.p3.3.m3.2.3.3.3.4.3.cmml">I</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.3.m3.2b"><apply id="S3.SS3.p3.3.m3.2.3.cmml" xref="S3.SS3.p3.3.m3.2.3"><in id="S3.SS3.p3.3.m3.2.3.1.cmml" xref="S3.SS3.p3.3.m3.2.3.1"></in><apply id="S3.SS3.p3.3.m3.2.3.2.cmml" xref="S3.SS3.p3.3.m3.2.3.2"><csymbol cd="ambiguous" id="S3.SS3.p3.3.m3.2.3.2.1.cmml" xref="S3.SS3.p3.3.m3.2.3.2">superscript</csymbol><apply id="S3.SS3.p3.3.m3.2.3.2.2.cmml" xref="S3.SS3.p3.3.m3.2.3.2"><csymbol cd="ambiguous" id="S3.SS3.p3.3.m3.2.3.2.2.1.cmml" xref="S3.SS3.p3.3.m3.2.3.2">subscript</csymbol><ci id="S3.SS3.p3.3.m3.2.3.2.2.2.cmml" xref="S3.SS3.p3.3.m3.2.3.2.2.2">𝐅</ci><list id="S3.SS3.p3.3.m3.2.2.2.3.cmml" xref="S3.SS3.p3.3.m3.2.2.2.4"><ci id="S3.SS3.p3.3.m3.1.1.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1.1.1">𝑖</ci><ci id="S3.SS3.p3.3.m3.2.2.2.2a.cmml" xref="S3.SS3.p3.3.m3.2.2.2.2"><mtext mathsize="70%" id="S3.SS3.p3.3.m3.2.2.2.2.cmml" xref="S3.SS3.p3.3.m3.2.2.2.2">PV</mtext></ci></list></apply><ci id="S3.SS3.p3.3.m3.2.3.2.3.cmml" xref="S3.SS3.p3.3.m3.2.3.2.3">𝐼</ci></apply><apply id="S3.SS3.p3.3.m3.2.3.3.cmml" xref="S3.SS3.p3.3.m3.2.3.3"><csymbol cd="ambiguous" id="S3.SS3.p3.3.m3.2.3.3.1.cmml" xref="S3.SS3.p3.3.m3.2.3.3">superscript</csymbol><ci id="S3.SS3.p3.3.m3.2.3.3.2.cmml" xref="S3.SS3.p3.3.m3.2.3.3.2">ℝ</ci><apply id="S3.SS3.p3.3.m3.2.3.3.3.cmml" xref="S3.SS3.p3.3.m3.2.3.3.3"><times id="S3.SS3.p3.3.m3.2.3.3.3.1.cmml" xref="S3.SS3.p3.3.m3.2.3.3.3.1"></times><apply id="S3.SS3.p3.3.m3.2.3.3.3.2.cmml" xref="S3.SS3.p3.3.m3.2.3.3.3.2"><csymbol cd="ambiguous" id="S3.SS3.p3.3.m3.2.3.3.3.2.1.cmml" xref="S3.SS3.p3.3.m3.2.3.3.3.2">subscript</csymbol><ci id="S3.SS3.p3.3.m3.2.3.3.3.2.2.cmml" xref="S3.SS3.p3.3.m3.2.3.3.3.2.2">𝐻</ci><ci id="S3.SS3.p3.3.m3.2.3.3.3.2.3.cmml" xref="S3.SS3.p3.3.m3.2.3.3.3.2.3">𝑖</ci></apply><apply id="S3.SS3.p3.3.m3.2.3.3.3.3.cmml" xref="S3.SS3.p3.3.m3.2.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS3.p3.3.m3.2.3.3.3.3.1.cmml" xref="S3.SS3.p3.3.m3.2.3.3.3.3">subscript</csymbol><ci id="S3.SS3.p3.3.m3.2.3.3.3.3.2.cmml" xref="S3.SS3.p3.3.m3.2.3.3.3.3.2">𝑊</ci><ci id="S3.SS3.p3.3.m3.2.3.3.3.3.3.cmml" xref="S3.SS3.p3.3.m3.2.3.3.3.3.3">𝑖</ci></apply><apply id="S3.SS3.p3.3.m3.2.3.3.3.4.cmml" xref="S3.SS3.p3.3.m3.2.3.3.3.4"><csymbol cd="ambiguous" id="S3.SS3.p3.3.m3.2.3.3.3.4.1.cmml" xref="S3.SS3.p3.3.m3.2.3.3.3.4">subscript</csymbol><ci id="S3.SS3.p3.3.m3.2.3.3.3.4.2.cmml" xref="S3.SS3.p3.3.m3.2.3.3.3.4.2">𝐶</ci><ci id="S3.SS3.p3.3.m3.2.3.3.3.4.3.cmml" xref="S3.SS3.p3.3.m3.2.3.3.3.4.3">𝐼</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.3.m3.2c">\mathbf{F}_{i,\text{PV}}^{I}\in\mathbb{R}^{H_{i}\times W_{i}\times C_{I}}</annotation></semantics></math>, the depth distribution map <math id="S3.SS3.p3.4.m4.1" class="ltx_Math" alttext="\mathbf{D}_{i}^{I}\in\mathbb{R}^{H_{i}\times W_{i}\times D}" display="inline"><semantics id="S3.SS3.p3.4.m4.1a"><mrow id="S3.SS3.p3.4.m4.1.1" xref="S3.SS3.p3.4.m4.1.1.cmml"><msubsup id="S3.SS3.p3.4.m4.1.1.2" xref="S3.SS3.p3.4.m4.1.1.2.cmml"><mi id="S3.SS3.p3.4.m4.1.1.2.2.2" xref="S3.SS3.p3.4.m4.1.1.2.2.2.cmml">𝐃</mi><mi id="S3.SS3.p3.4.m4.1.1.2.2.3" xref="S3.SS3.p3.4.m4.1.1.2.2.3.cmml">i</mi><mi id="S3.SS3.p3.4.m4.1.1.2.3" xref="S3.SS3.p3.4.m4.1.1.2.3.cmml">I</mi></msubsup><mo id="S3.SS3.p3.4.m4.1.1.1" xref="S3.SS3.p3.4.m4.1.1.1.cmml">∈</mo><msup id="S3.SS3.p3.4.m4.1.1.3" xref="S3.SS3.p3.4.m4.1.1.3.cmml"><mi id="S3.SS3.p3.4.m4.1.1.3.2" xref="S3.SS3.p3.4.m4.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS3.p3.4.m4.1.1.3.3" xref="S3.SS3.p3.4.m4.1.1.3.3.cmml"><msub id="S3.SS3.p3.4.m4.1.1.3.3.2" xref="S3.SS3.p3.4.m4.1.1.3.3.2.cmml"><mi id="S3.SS3.p3.4.m4.1.1.3.3.2.2" xref="S3.SS3.p3.4.m4.1.1.3.3.2.2.cmml">H</mi><mi id="S3.SS3.p3.4.m4.1.1.3.3.2.3" xref="S3.SS3.p3.4.m4.1.1.3.3.2.3.cmml">i</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p3.4.m4.1.1.3.3.1" xref="S3.SS3.p3.4.m4.1.1.3.3.1.cmml">×</mo><msub id="S3.SS3.p3.4.m4.1.1.3.3.3" xref="S3.SS3.p3.4.m4.1.1.3.3.3.cmml"><mi id="S3.SS3.p3.4.m4.1.1.3.3.3.2" xref="S3.SS3.p3.4.m4.1.1.3.3.3.2.cmml">W</mi><mi id="S3.SS3.p3.4.m4.1.1.3.3.3.3" xref="S3.SS3.p3.4.m4.1.1.3.3.3.3.cmml">i</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p3.4.m4.1.1.3.3.1a" xref="S3.SS3.p3.4.m4.1.1.3.3.1.cmml">×</mo><mi id="S3.SS3.p3.4.m4.1.1.3.3.4" xref="S3.SS3.p3.4.m4.1.1.3.3.4.cmml">D</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.4.m4.1b"><apply id="S3.SS3.p3.4.m4.1.1.cmml" xref="S3.SS3.p3.4.m4.1.1"><in id="S3.SS3.p3.4.m4.1.1.1.cmml" xref="S3.SS3.p3.4.m4.1.1.1"></in><apply id="S3.SS3.p3.4.m4.1.1.2.cmml" xref="S3.SS3.p3.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p3.4.m4.1.1.2.1.cmml" xref="S3.SS3.p3.4.m4.1.1.2">superscript</csymbol><apply id="S3.SS3.p3.4.m4.1.1.2.2.cmml" xref="S3.SS3.p3.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p3.4.m4.1.1.2.2.1.cmml" xref="S3.SS3.p3.4.m4.1.1.2">subscript</csymbol><ci id="S3.SS3.p3.4.m4.1.1.2.2.2.cmml" xref="S3.SS3.p3.4.m4.1.1.2.2.2">𝐃</ci><ci id="S3.SS3.p3.4.m4.1.1.2.2.3.cmml" xref="S3.SS3.p3.4.m4.1.1.2.2.3">𝑖</ci></apply><ci id="S3.SS3.p3.4.m4.1.1.2.3.cmml" xref="S3.SS3.p3.4.m4.1.1.2.3">𝐼</ci></apply><apply id="S3.SS3.p3.4.m4.1.1.3.cmml" xref="S3.SS3.p3.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p3.4.m4.1.1.3.1.cmml" xref="S3.SS3.p3.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS3.p3.4.m4.1.1.3.2.cmml" xref="S3.SS3.p3.4.m4.1.1.3.2">ℝ</ci><apply id="S3.SS3.p3.4.m4.1.1.3.3.cmml" xref="S3.SS3.p3.4.m4.1.1.3.3"><times id="S3.SS3.p3.4.m4.1.1.3.3.1.cmml" xref="S3.SS3.p3.4.m4.1.1.3.3.1"></times><apply id="S3.SS3.p3.4.m4.1.1.3.3.2.cmml" xref="S3.SS3.p3.4.m4.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS3.p3.4.m4.1.1.3.3.2.1.cmml" xref="S3.SS3.p3.4.m4.1.1.3.3.2">subscript</csymbol><ci id="S3.SS3.p3.4.m4.1.1.3.3.2.2.cmml" xref="S3.SS3.p3.4.m4.1.1.3.3.2.2">𝐻</ci><ci id="S3.SS3.p3.4.m4.1.1.3.3.2.3.cmml" xref="S3.SS3.p3.4.m4.1.1.3.3.2.3">𝑖</ci></apply><apply id="S3.SS3.p3.4.m4.1.1.3.3.3.cmml" xref="S3.SS3.p3.4.m4.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS3.p3.4.m4.1.1.3.3.3.1.cmml" xref="S3.SS3.p3.4.m4.1.1.3.3.3">subscript</csymbol><ci id="S3.SS3.p3.4.m4.1.1.3.3.3.2.cmml" xref="S3.SS3.p3.4.m4.1.1.3.3.3.2">𝑊</ci><ci id="S3.SS3.p3.4.m4.1.1.3.3.3.3.cmml" xref="S3.SS3.p3.4.m4.1.1.3.3.3.3">𝑖</ci></apply><ci id="S3.SS3.p3.4.m4.1.1.3.3.4.cmml" xref="S3.SS3.p3.4.m4.1.1.3.3.4">𝐷</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.4.m4.1c">\mathbf{D}_{i}^{I}\in\mathbb{R}^{H_{i}\times W_{i}\times D}</annotation></semantics></math> can be obtained as</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.6" class="ltx_Math" alttext="\mathbf{D}_{i}^{I}=\mathtt{Softmax}(\mathtt{Conv}_{C_{I}\rightarrow D}(\mathbf{F}_{i,\text{PV}}^{I})),i=1,2,\cdots,N_{\text{lvl}}," display="block"><semantics id="S3.E2.m1.6a"><mrow id="S3.E2.m1.6.6.1"><mrow id="S3.E2.m1.6.6.1.1.2" xref="S3.E2.m1.6.6.1.1.3.cmml"><mrow id="S3.E2.m1.6.6.1.1.1.1" xref="S3.E2.m1.6.6.1.1.1.1.cmml"><msubsup id="S3.E2.m1.6.6.1.1.1.1.3" xref="S3.E2.m1.6.6.1.1.1.1.3.cmml"><mi id="S3.E2.m1.6.6.1.1.1.1.3.2.2" xref="S3.E2.m1.6.6.1.1.1.1.3.2.2.cmml">𝐃</mi><mi id="S3.E2.m1.6.6.1.1.1.1.3.2.3" xref="S3.E2.m1.6.6.1.1.1.1.3.2.3.cmml">i</mi><mi id="S3.E2.m1.6.6.1.1.1.1.3.3" xref="S3.E2.m1.6.6.1.1.1.1.3.3.cmml">I</mi></msubsup><mo id="S3.E2.m1.6.6.1.1.1.1.2" xref="S3.E2.m1.6.6.1.1.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.6.6.1.1.1.1.1" xref="S3.E2.m1.6.6.1.1.1.1.1.cmml"><mi id="S3.E2.m1.6.6.1.1.1.1.1.3" xref="S3.E2.m1.6.6.1.1.1.1.1.3.cmml">𝚂𝚘𝚏𝚝𝚖𝚊𝚡</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.6.6.1.1.1.1.1.2" xref="S3.E2.m1.6.6.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m1.6.6.1.1.1.1.1.1.1" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.6.6.1.1.1.1.1.1.1.2" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.3.2.cmml">𝙲𝚘𝚗𝚟</mi><mrow id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.3.3.cmml"><msub id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.3.3.2" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.3.3.2.cmml"><mi id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.3.3.2.2" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.3.3.2.2.cmml">C</mi><mi id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.3.3.2.3" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.3.3.2.3.cmml">I</mi></msub><mo stretchy="false" id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.3.3.1" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.3.3.1.cmml">→</mo><mi id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.3.3.3" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.3.3.3.cmml">D</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msubsup id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">𝐅</mi><mrow id="S3.E2.m1.2.2.2.4" xref="S3.E2.m1.2.2.2.3.cmml"><mi id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml">i</mi><mo id="S3.E2.m1.2.2.2.4.1" xref="S3.E2.m1.2.2.2.3.cmml">,</mo><mtext id="S3.E2.m1.2.2.2.2" xref="S3.E2.m1.2.2.2.2a.cmml">PV</mtext></mrow><mi id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.3.cmml">I</mi></msubsup><mo stretchy="false" id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E2.m1.6.6.1.1.1.1.1.1.1.3" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E2.m1.6.6.1.1.2.3" xref="S3.E2.m1.6.6.1.1.3a.cmml">,</mo><mrow id="S3.E2.m1.6.6.1.1.2.2" xref="S3.E2.m1.6.6.1.1.2.2.cmml"><mi id="S3.E2.m1.6.6.1.1.2.2.3" xref="S3.E2.m1.6.6.1.1.2.2.3.cmml">i</mi><mo id="S3.E2.m1.6.6.1.1.2.2.2" xref="S3.E2.m1.6.6.1.1.2.2.2.cmml">=</mo><mrow id="S3.E2.m1.6.6.1.1.2.2.1.1" xref="S3.E2.m1.6.6.1.1.2.2.1.2.cmml"><mn id="S3.E2.m1.3.3" xref="S3.E2.m1.3.3.cmml">1</mn><mo id="S3.E2.m1.6.6.1.1.2.2.1.1.2" xref="S3.E2.m1.6.6.1.1.2.2.1.2.cmml">,</mo><mn id="S3.E2.m1.4.4" xref="S3.E2.m1.4.4.cmml">2</mn><mo id="S3.E2.m1.6.6.1.1.2.2.1.1.3" xref="S3.E2.m1.6.6.1.1.2.2.1.2.cmml">,</mo><mi mathvariant="normal" id="S3.E2.m1.5.5" xref="S3.E2.m1.5.5.cmml">⋯</mi><mo id="S3.E2.m1.6.6.1.1.2.2.1.1.4" xref="S3.E2.m1.6.6.1.1.2.2.1.2.cmml">,</mo><msub id="S3.E2.m1.6.6.1.1.2.2.1.1.1" xref="S3.E2.m1.6.6.1.1.2.2.1.1.1.cmml"><mi id="S3.E2.m1.6.6.1.1.2.2.1.1.1.2" xref="S3.E2.m1.6.6.1.1.2.2.1.1.1.2.cmml">N</mi><mtext id="S3.E2.m1.6.6.1.1.2.2.1.1.1.3" xref="S3.E2.m1.6.6.1.1.2.2.1.1.1.3a.cmml">lvl</mtext></msub></mrow></mrow></mrow><mo id="S3.E2.m1.6.6.1.2">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.6b"><apply id="S3.E2.m1.6.6.1.1.3.cmml" xref="S3.E2.m1.6.6.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.1.1.3a.cmml" xref="S3.E2.m1.6.6.1.1.2.3">formulae-sequence</csymbol><apply id="S3.E2.m1.6.6.1.1.1.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1"><eq id="S3.E2.m1.6.6.1.1.1.1.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.2"></eq><apply id="S3.E2.m1.6.6.1.1.1.1.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.1.1.1.1.3.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.3">superscript</csymbol><apply id="S3.E2.m1.6.6.1.1.1.1.3.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.1.1.1.1.3.2.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.6.6.1.1.1.1.3.2.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.3.2.2">𝐃</ci><ci id="S3.E2.m1.6.6.1.1.1.1.3.2.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.3.2.3">𝑖</ci></apply><ci id="S3.E2.m1.6.6.1.1.1.1.3.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.3.3">𝐼</ci></apply><apply id="S3.E2.m1.6.6.1.1.1.1.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1"><times id="S3.E2.m1.6.6.1.1.1.1.1.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.2"></times><ci id="S3.E2.m1.6.6.1.1.1.1.1.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.3">𝚂𝚘𝚏𝚝𝚖𝚊𝚡</ci><apply id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1"><times id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.2"></times><apply id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.3.2">𝙲𝚘𝚗𝚟</ci><apply id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.3.3"><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.3.3.1">→</ci><apply id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.3.3.2.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.3.3.2">subscript</csymbol><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.3.3.2.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.3.3.2.2">𝐶</ci><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.3.3.2.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.3.3.2.3">𝐼</ci></apply><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.3.3.3">𝐷</ci></apply></apply><apply id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.2.2">𝐅</ci><list id="S3.E2.m1.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.4"><ci id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1">𝑖</ci><ci id="S3.E2.m1.2.2.2.2a.cmml" xref="S3.E2.m1.2.2.2.2"><mtext mathsize="70%" id="S3.E2.m1.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2">PV</mtext></ci></list></apply><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.3">𝐼</ci></apply></apply></apply></apply><apply id="S3.E2.m1.6.6.1.1.2.2.cmml" xref="S3.E2.m1.6.6.1.1.2.2"><eq id="S3.E2.m1.6.6.1.1.2.2.2.cmml" xref="S3.E2.m1.6.6.1.1.2.2.2"></eq><ci id="S3.E2.m1.6.6.1.1.2.2.3.cmml" xref="S3.E2.m1.6.6.1.1.2.2.3">𝑖</ci><list id="S3.E2.m1.6.6.1.1.2.2.1.2.cmml" xref="S3.E2.m1.6.6.1.1.2.2.1.1"><cn type="integer" id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3">1</cn><cn type="integer" id="S3.E2.m1.4.4.cmml" xref="S3.E2.m1.4.4">2</cn><ci id="S3.E2.m1.5.5.cmml" xref="S3.E2.m1.5.5">⋯</ci><apply id="S3.E2.m1.6.6.1.1.2.2.1.1.1.cmml" xref="S3.E2.m1.6.6.1.1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.1.1.2.2.1.1.1.1.cmml" xref="S3.E2.m1.6.6.1.1.2.2.1.1.1">subscript</csymbol><ci id="S3.E2.m1.6.6.1.1.2.2.1.1.1.2.cmml" xref="S3.E2.m1.6.6.1.1.2.2.1.1.1.2">𝑁</ci><ci id="S3.E2.m1.6.6.1.1.2.2.1.1.1.3a.cmml" xref="S3.E2.m1.6.6.1.1.2.2.1.1.1.3"><mtext mathsize="70%" id="S3.E2.m1.6.6.1.1.2.2.1.1.1.3.cmml" xref="S3.E2.m1.6.6.1.1.2.2.1.1.1.3">lvl</mtext></ci></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.6c">\mathbf{D}_{i}^{I}=\mathtt{Softmax}(\mathtt{Conv}_{C_{I}\rightarrow D}(\mathbf{F}_{i,\text{PV}}^{I})),i=1,2,\cdots,N_{\text{lvl}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p3.7" class="ltx_p">where <math id="S3.SS3.p3.5.m1.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.SS3.p3.5.m1.1a"><mi id="S3.SS3.p3.5.m1.1.1" xref="S3.SS3.p3.5.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.5.m1.1b"><ci id="S3.SS3.p3.5.m1.1.1.cmml" xref="S3.SS3.p3.5.m1.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.5.m1.1c">D</annotation></semantics></math> represents the pre-defined number of depth bins, <math id="S3.SS3.p3.6.m2.1" class="ltx_Math" alttext="N_{\text{lvl}}" display="inline"><semantics id="S3.SS3.p3.6.m2.1a"><msub id="S3.SS3.p3.6.m2.1.1" xref="S3.SS3.p3.6.m2.1.1.cmml"><mi id="S3.SS3.p3.6.m2.1.1.2" xref="S3.SS3.p3.6.m2.1.1.2.cmml">N</mi><mtext id="S3.SS3.p3.6.m2.1.1.3" xref="S3.SS3.p3.6.m2.1.1.3a.cmml">lvl</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.6.m2.1b"><apply id="S3.SS3.p3.6.m2.1.1.cmml" xref="S3.SS3.p3.6.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.6.m2.1.1.1.cmml" xref="S3.SS3.p3.6.m2.1.1">subscript</csymbol><ci id="S3.SS3.p3.6.m2.1.1.2.cmml" xref="S3.SS3.p3.6.m2.1.1.2">𝑁</ci><ci id="S3.SS3.p3.6.m2.1.1.3a.cmml" xref="S3.SS3.p3.6.m2.1.1.3"><mtext mathsize="70%" id="S3.SS3.p3.6.m2.1.1.3.cmml" xref="S3.SS3.p3.6.m2.1.1.3">lvl</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.6.m2.1c">N_{\text{lvl}}</annotation></semantics></math> denotes the number of levels and <math id="S3.SS3.p3.7.m3.1" class="ltx_Math" alttext="\mathtt{Softmax}(\cdot)" display="inline"><semantics id="S3.SS3.p3.7.m3.1a"><mrow id="S3.SS3.p3.7.m3.1.2" xref="S3.SS3.p3.7.m3.1.2.cmml"><mi id="S3.SS3.p3.7.m3.1.2.2" xref="S3.SS3.p3.7.m3.1.2.2.cmml">𝚂𝚘𝚏𝚝𝚖𝚊𝚡</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.7.m3.1.2.1" xref="S3.SS3.p3.7.m3.1.2.1.cmml">​</mo><mrow id="S3.SS3.p3.7.m3.1.2.3.2" xref="S3.SS3.p3.7.m3.1.2.cmml"><mo stretchy="false" id="S3.SS3.p3.7.m3.1.2.3.2.1" xref="S3.SS3.p3.7.m3.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS3.p3.7.m3.1.1" xref="S3.SS3.p3.7.m3.1.1.cmml">⋅</mo><mo stretchy="false" id="S3.SS3.p3.7.m3.1.2.3.2.2" xref="S3.SS3.p3.7.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.7.m3.1b"><apply id="S3.SS3.p3.7.m3.1.2.cmml" xref="S3.SS3.p3.7.m3.1.2"><times id="S3.SS3.p3.7.m3.1.2.1.cmml" xref="S3.SS3.p3.7.m3.1.2.1"></times><ci id="S3.SS3.p3.7.m3.1.2.2.cmml" xref="S3.SS3.p3.7.m3.1.2.2">𝚂𝚘𝚏𝚝𝚖𝚊𝚡</ci><ci id="S3.SS3.p3.7.m3.1.1.cmml" xref="S3.SS3.p3.7.m3.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.7.m3.1c">\mathtt{Softmax}(\cdot)</annotation></semantics></math> is applied along the depth dimension.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">The final module in the image branch is the view transformation module which is also the core of LXL. Its primary objective is to lift the image PV features into a 3D space and compress the height dimension. The detailed workings of this module will be elaborated upon in Section <a href="#S3.SS4" title="III-D View Transformation ‣ III Proposed Method ‣ LXL: LiDAR Excluded Lean 3D Object Detection with 4D Imaging Radar and Camera Fusion" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-D</span></span></a>.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2307.00724/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="390" height="214" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The view transformation module in LXL. During the feature lifting process, image PV features are transformed to 3D voxel features through the “sampling” strategy. The result is then multiplied by sampled depth probabilities and radar 3D occupancy grids respectively, obtaining depth-assisted image 3D voxel features and radar occupancy-assisted image 3D voxel features. Finally, the two aforementioned image 3D features are concatenated before height compression.</figcaption>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.5.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.6.2" class="ltx_text ltx_font_italic">View Transformation</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">The process of view transformation, which incorporates predicted multi-scale depth distribution maps and radar 3D occupancy grids, is illustrated in Fig. <a href="#S3.F2" title="Figure 2 ‣ III-C Image Branch ‣ III Proposed Method ‣ LXL: LiDAR Excluded Lean 3D Object Detection with 4D Imaging Radar and Camera Fusion" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. This method is referred to as “<span id="S3.SS4.p1.1.1" class="ltx_text ltx_font_italic">radar occupancy-assisted depth-based sampling</span>” in this paper. To the best of the authors’ knowledge, this is the first work to enhance the “sampling” feature lifting strategy with both predicted image depth distribution maps and radar 3D occupancy grids.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p"><span id="S3.SS4.p2.1.1" class="ltx_text ltx_font_bold">Feature Lifting:</span>
There are two primary strategies for geometrically lifting image features into 3D space.
The first strategy is “sampling”, where pre-defined 3D voxels are projected onto the image plane, and the features of nearby pixels in the projected region are combined to form the voxel feature.
Representative models utilizing this strategy include <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">55</span></a>]</cite>.
The second strategy, “splatting”, involves transforming each image pixel into points or frustum voxels along a straight line in 3D space based on the calibration matrix. The features of these points or frustum voxels are determined by their corresponding pixel features.
Subsequently, the points are voxelized, or the frustum voxels are transformed into cubic voxels. Prominent models that employ the “splatting” technique for view transformation include <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite>.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.3" class="ltx_p">Simple-BEV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite> has demonstrated that the “sampling” approach outperforms “splatting”, and our experiments in Section <a href="#S4.SS4" title="IV-D Ablation Study ‣ IV Experiments and Analysis ‣ LXL: LiDAR Excluded Lean 3D Object Detection with 4D Imaging Radar and Camera Fusion" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-D</span></span></a> corroborate this conclusion.
Therefore, we have chosen the “sampling” strategy for view transformation in our model.
Specifically, given the 3-dimensional coordinates of the pre-defined 3D voxels, denoted as <math id="S3.SS4.p3.1.m1.1" class="ltx_Math" alttext="\mathbf{V}^{P}\in\mathbb{R}^{X\times Y\times Z\times 3}" display="inline"><semantics id="S3.SS4.p3.1.m1.1a"><mrow id="S3.SS4.p3.1.m1.1.1" xref="S3.SS4.p3.1.m1.1.1.cmml"><msup id="S3.SS4.p3.1.m1.1.1.2" xref="S3.SS4.p3.1.m1.1.1.2.cmml"><mi id="S3.SS4.p3.1.m1.1.1.2.2" xref="S3.SS4.p3.1.m1.1.1.2.2.cmml">𝐕</mi><mi id="S3.SS4.p3.1.m1.1.1.2.3" xref="S3.SS4.p3.1.m1.1.1.2.3.cmml">P</mi></msup><mo id="S3.SS4.p3.1.m1.1.1.1" xref="S3.SS4.p3.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS4.p3.1.m1.1.1.3" xref="S3.SS4.p3.1.m1.1.1.3.cmml"><mi id="S3.SS4.p3.1.m1.1.1.3.2" xref="S3.SS4.p3.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS4.p3.1.m1.1.1.3.3" xref="S3.SS4.p3.1.m1.1.1.3.3.cmml"><mi id="S3.SS4.p3.1.m1.1.1.3.3.2" xref="S3.SS4.p3.1.m1.1.1.3.3.2.cmml">X</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p3.1.m1.1.1.3.3.1" xref="S3.SS4.p3.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.SS4.p3.1.m1.1.1.3.3.3" xref="S3.SS4.p3.1.m1.1.1.3.3.3.cmml">Y</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p3.1.m1.1.1.3.3.1a" xref="S3.SS4.p3.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.SS4.p3.1.m1.1.1.3.3.4" xref="S3.SS4.p3.1.m1.1.1.3.3.4.cmml">Z</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p3.1.m1.1.1.3.3.1b" xref="S3.SS4.p3.1.m1.1.1.3.3.1.cmml">×</mo><mn id="S3.SS4.p3.1.m1.1.1.3.3.5" xref="S3.SS4.p3.1.m1.1.1.3.3.5.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.1.m1.1b"><apply id="S3.SS4.p3.1.m1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1"><in id="S3.SS4.p3.1.m1.1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1.1"></in><apply id="S3.SS4.p3.1.m1.1.1.2.cmml" xref="S3.SS4.p3.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p3.1.m1.1.1.2.1.cmml" xref="S3.SS4.p3.1.m1.1.1.2">superscript</csymbol><ci id="S3.SS4.p3.1.m1.1.1.2.2.cmml" xref="S3.SS4.p3.1.m1.1.1.2.2">𝐕</ci><ci id="S3.SS4.p3.1.m1.1.1.2.3.cmml" xref="S3.SS4.p3.1.m1.1.1.2.3">𝑃</ci></apply><apply id="S3.SS4.p3.1.m1.1.1.3.cmml" xref="S3.SS4.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.p3.1.m1.1.1.3.1.cmml" xref="S3.SS4.p3.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS4.p3.1.m1.1.1.3.2.cmml" xref="S3.SS4.p3.1.m1.1.1.3.2">ℝ</ci><apply id="S3.SS4.p3.1.m1.1.1.3.3.cmml" xref="S3.SS4.p3.1.m1.1.1.3.3"><times id="S3.SS4.p3.1.m1.1.1.3.3.1.cmml" xref="S3.SS4.p3.1.m1.1.1.3.3.1"></times><ci id="S3.SS4.p3.1.m1.1.1.3.3.2.cmml" xref="S3.SS4.p3.1.m1.1.1.3.3.2">𝑋</ci><ci id="S3.SS4.p3.1.m1.1.1.3.3.3.cmml" xref="S3.SS4.p3.1.m1.1.1.3.3.3">𝑌</ci><ci id="S3.SS4.p3.1.m1.1.1.3.3.4.cmml" xref="S3.SS4.p3.1.m1.1.1.3.3.4">𝑍</ci><cn type="integer" id="S3.SS4.p3.1.m1.1.1.3.3.5.cmml" xref="S3.SS4.p3.1.m1.1.1.3.3.5">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.1.m1.1c">\mathbf{V}^{P}\in\mathbb{R}^{X\times Y\times Z\times 3}</annotation></semantics></math> in the radar coordinate system, the radar-to-image coordinate transformation matrix <math id="S3.SS4.p3.2.m2.1" class="ltx_Math" alttext="\mathbf{T}_{\text{r2c}}\in\mathbb{R}^{3\times 3}" display="inline"><semantics id="S3.SS4.p3.2.m2.1a"><mrow id="S3.SS4.p3.2.m2.1.1" xref="S3.SS4.p3.2.m2.1.1.cmml"><msub id="S3.SS4.p3.2.m2.1.1.2" xref="S3.SS4.p3.2.m2.1.1.2.cmml"><mi id="S3.SS4.p3.2.m2.1.1.2.2" xref="S3.SS4.p3.2.m2.1.1.2.2.cmml">𝐓</mi><mtext id="S3.SS4.p3.2.m2.1.1.2.3" xref="S3.SS4.p3.2.m2.1.1.2.3a.cmml">r2c</mtext></msub><mo id="S3.SS4.p3.2.m2.1.1.1" xref="S3.SS4.p3.2.m2.1.1.1.cmml">∈</mo><msup id="S3.SS4.p3.2.m2.1.1.3" xref="S3.SS4.p3.2.m2.1.1.3.cmml"><mi id="S3.SS4.p3.2.m2.1.1.3.2" xref="S3.SS4.p3.2.m2.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS4.p3.2.m2.1.1.3.3" xref="S3.SS4.p3.2.m2.1.1.3.3.cmml"><mn id="S3.SS4.p3.2.m2.1.1.3.3.2" xref="S3.SS4.p3.2.m2.1.1.3.3.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p3.2.m2.1.1.3.3.1" xref="S3.SS4.p3.2.m2.1.1.3.3.1.cmml">×</mo><mn id="S3.SS4.p3.2.m2.1.1.3.3.3" xref="S3.SS4.p3.2.m2.1.1.3.3.3.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.2.m2.1b"><apply id="S3.SS4.p3.2.m2.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1"><in id="S3.SS4.p3.2.m2.1.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1.1"></in><apply id="S3.SS4.p3.2.m2.1.1.2.cmml" xref="S3.SS4.p3.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p3.2.m2.1.1.2.1.cmml" xref="S3.SS4.p3.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS4.p3.2.m2.1.1.2.2.cmml" xref="S3.SS4.p3.2.m2.1.1.2.2">𝐓</ci><ci id="S3.SS4.p3.2.m2.1.1.2.3a.cmml" xref="S3.SS4.p3.2.m2.1.1.2.3"><mtext mathsize="70%" id="S3.SS4.p3.2.m2.1.1.2.3.cmml" xref="S3.SS4.p3.2.m2.1.1.2.3">r2c</mtext></ci></apply><apply id="S3.SS4.p3.2.m2.1.1.3.cmml" xref="S3.SS4.p3.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.p3.2.m2.1.1.3.1.cmml" xref="S3.SS4.p3.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS4.p3.2.m2.1.1.3.2.cmml" xref="S3.SS4.p3.2.m2.1.1.3.2">ℝ</ci><apply id="S3.SS4.p3.2.m2.1.1.3.3.cmml" xref="S3.SS4.p3.2.m2.1.1.3.3"><times id="S3.SS4.p3.2.m2.1.1.3.3.1.cmml" xref="S3.SS4.p3.2.m2.1.1.3.3.1"></times><cn type="integer" id="S3.SS4.p3.2.m2.1.1.3.3.2.cmml" xref="S3.SS4.p3.2.m2.1.1.3.3.2">3</cn><cn type="integer" id="S3.SS4.p3.2.m2.1.1.3.3.3.cmml" xref="S3.SS4.p3.2.m2.1.1.3.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.2.m2.1c">\mathbf{T}_{\text{r2c}}\in\mathbb{R}^{3\times 3}</annotation></semantics></math>, and the camera intrinsic matrix <math id="S3.SS4.p3.3.m3.1" class="ltx_Math" alttext="\mathbf{I}\in\mathbb{R}^{3\times 4}" display="inline"><semantics id="S3.SS4.p3.3.m3.1a"><mrow id="S3.SS4.p3.3.m3.1.1" xref="S3.SS4.p3.3.m3.1.1.cmml"><mi id="S3.SS4.p3.3.m3.1.1.2" xref="S3.SS4.p3.3.m3.1.1.2.cmml">𝐈</mi><mo id="S3.SS4.p3.3.m3.1.1.1" xref="S3.SS4.p3.3.m3.1.1.1.cmml">∈</mo><msup id="S3.SS4.p3.3.m3.1.1.3" xref="S3.SS4.p3.3.m3.1.1.3.cmml"><mi id="S3.SS4.p3.3.m3.1.1.3.2" xref="S3.SS4.p3.3.m3.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS4.p3.3.m3.1.1.3.3" xref="S3.SS4.p3.3.m3.1.1.3.3.cmml"><mn id="S3.SS4.p3.3.m3.1.1.3.3.2" xref="S3.SS4.p3.3.m3.1.1.3.3.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p3.3.m3.1.1.3.3.1" xref="S3.SS4.p3.3.m3.1.1.3.3.1.cmml">×</mo><mn id="S3.SS4.p3.3.m3.1.1.3.3.3" xref="S3.SS4.p3.3.m3.1.1.3.3.3.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.3.m3.1b"><apply id="S3.SS4.p3.3.m3.1.1.cmml" xref="S3.SS4.p3.3.m3.1.1"><in id="S3.SS4.p3.3.m3.1.1.1.cmml" xref="S3.SS4.p3.3.m3.1.1.1"></in><ci id="S3.SS4.p3.3.m3.1.1.2.cmml" xref="S3.SS4.p3.3.m3.1.1.2">𝐈</ci><apply id="S3.SS4.p3.3.m3.1.1.3.cmml" xref="S3.SS4.p3.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.p3.3.m3.1.1.3.1.cmml" xref="S3.SS4.p3.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS4.p3.3.m3.1.1.3.2.cmml" xref="S3.SS4.p3.3.m3.1.1.3.2">ℝ</ci><apply id="S3.SS4.p3.3.m3.1.1.3.3.cmml" xref="S3.SS4.p3.3.m3.1.1.3.3"><times id="S3.SS4.p3.3.m3.1.1.3.3.1.cmml" xref="S3.SS4.p3.3.m3.1.1.3.3.1"></times><cn type="integer" id="S3.SS4.p3.3.m3.1.1.3.3.2.cmml" xref="S3.SS4.p3.3.m3.1.1.3.3.2">3</cn><cn type="integer" id="S3.SS4.p3.3.m3.1.1.3.3.3.cmml" xref="S3.SS4.p3.3.m3.1.1.3.3.3">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.3.m3.1c">\mathbf{I}\in\mathbb{R}^{3\times 4}</annotation></semantics></math>, we first project the voxel centers onto the image plane using the following equation:</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.7" class="ltx_Math" alttext="\mathbf{V}^{I}_{i,j,k}=\mathbf{I}\cdot\mathbf{\bar{T}}_{\text{r2c}}\cdot\mathbf{\bar{V}}^{P}_{i,j,k}," display="block"><semantics id="S3.E3.m1.7a"><mrow id="S3.E3.m1.7.7.1" xref="S3.E3.m1.7.7.1.1.cmml"><mrow id="S3.E3.m1.7.7.1.1" xref="S3.E3.m1.7.7.1.1.cmml"><msubsup id="S3.E3.m1.7.7.1.1.2" xref="S3.E3.m1.7.7.1.1.2.cmml"><mi id="S3.E3.m1.7.7.1.1.2.2.2" xref="S3.E3.m1.7.7.1.1.2.2.2.cmml">𝐕</mi><mrow id="S3.E3.m1.3.3.3.5" xref="S3.E3.m1.3.3.3.4.cmml"><mi id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml">i</mi><mo id="S3.E3.m1.3.3.3.5.1" xref="S3.E3.m1.3.3.3.4.cmml">,</mo><mi id="S3.E3.m1.2.2.2.2" xref="S3.E3.m1.2.2.2.2.cmml">j</mi><mo id="S3.E3.m1.3.3.3.5.2" xref="S3.E3.m1.3.3.3.4.cmml">,</mo><mi id="S3.E3.m1.3.3.3.3" xref="S3.E3.m1.3.3.3.3.cmml">k</mi></mrow><mi id="S3.E3.m1.7.7.1.1.2.2.3" xref="S3.E3.m1.7.7.1.1.2.2.3.cmml">I</mi></msubsup><mo id="S3.E3.m1.7.7.1.1.1" xref="S3.E3.m1.7.7.1.1.1.cmml">=</mo><mrow id="S3.E3.m1.7.7.1.1.3" xref="S3.E3.m1.7.7.1.1.3.cmml"><mi id="S3.E3.m1.7.7.1.1.3.2" xref="S3.E3.m1.7.7.1.1.3.2.cmml">𝐈</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E3.m1.7.7.1.1.3.1" xref="S3.E3.m1.7.7.1.1.3.1.cmml">⋅</mo><msub id="S3.E3.m1.7.7.1.1.3.3" xref="S3.E3.m1.7.7.1.1.3.3.cmml"><mover accent="true" id="S3.E3.m1.7.7.1.1.3.3.2" xref="S3.E3.m1.7.7.1.1.3.3.2.cmml"><mi id="S3.E3.m1.7.7.1.1.3.3.2.2" xref="S3.E3.m1.7.7.1.1.3.3.2.2.cmml">𝐓</mi><mo id="S3.E3.m1.7.7.1.1.3.3.2.1" xref="S3.E3.m1.7.7.1.1.3.3.2.1.cmml">¯</mo></mover><mtext id="S3.E3.m1.7.7.1.1.3.3.3" xref="S3.E3.m1.7.7.1.1.3.3.3a.cmml">r2c</mtext></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E3.m1.7.7.1.1.3.1a" xref="S3.E3.m1.7.7.1.1.3.1.cmml">⋅</mo><msubsup id="S3.E3.m1.7.7.1.1.3.4" xref="S3.E3.m1.7.7.1.1.3.4.cmml"><mover accent="true" id="S3.E3.m1.7.7.1.1.3.4.2.2" xref="S3.E3.m1.7.7.1.1.3.4.2.2.cmml"><mi id="S3.E3.m1.7.7.1.1.3.4.2.2.2" xref="S3.E3.m1.7.7.1.1.3.4.2.2.2.cmml">𝐕</mi><mo id="S3.E3.m1.7.7.1.1.3.4.2.2.1" xref="S3.E3.m1.7.7.1.1.3.4.2.2.1.cmml">¯</mo></mover><mrow id="S3.E3.m1.6.6.3.5" xref="S3.E3.m1.6.6.3.4.cmml"><mi id="S3.E3.m1.4.4.1.1" xref="S3.E3.m1.4.4.1.1.cmml">i</mi><mo id="S3.E3.m1.6.6.3.5.1" xref="S3.E3.m1.6.6.3.4.cmml">,</mo><mi id="S3.E3.m1.5.5.2.2" xref="S3.E3.m1.5.5.2.2.cmml">j</mi><mo id="S3.E3.m1.6.6.3.5.2" xref="S3.E3.m1.6.6.3.4.cmml">,</mo><mi id="S3.E3.m1.6.6.3.3" xref="S3.E3.m1.6.6.3.3.cmml">k</mi></mrow><mi id="S3.E3.m1.7.7.1.1.3.4.2.3" xref="S3.E3.m1.7.7.1.1.3.4.2.3.cmml">P</mi></msubsup></mrow></mrow><mo id="S3.E3.m1.7.7.1.2" xref="S3.E3.m1.7.7.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.7b"><apply id="S3.E3.m1.7.7.1.1.cmml" xref="S3.E3.m1.7.7.1"><eq id="S3.E3.m1.7.7.1.1.1.cmml" xref="S3.E3.m1.7.7.1.1.1"></eq><apply id="S3.E3.m1.7.7.1.1.2.cmml" xref="S3.E3.m1.7.7.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.7.7.1.1.2.1.cmml" xref="S3.E3.m1.7.7.1.1.2">subscript</csymbol><apply id="S3.E3.m1.7.7.1.1.2.2.cmml" xref="S3.E3.m1.7.7.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.7.7.1.1.2.2.1.cmml" xref="S3.E3.m1.7.7.1.1.2">superscript</csymbol><ci id="S3.E3.m1.7.7.1.1.2.2.2.cmml" xref="S3.E3.m1.7.7.1.1.2.2.2">𝐕</ci><ci id="S3.E3.m1.7.7.1.1.2.2.3.cmml" xref="S3.E3.m1.7.7.1.1.2.2.3">𝐼</ci></apply><list id="S3.E3.m1.3.3.3.4.cmml" xref="S3.E3.m1.3.3.3.5"><ci id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1">𝑖</ci><ci id="S3.E3.m1.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2">𝑗</ci><ci id="S3.E3.m1.3.3.3.3.cmml" xref="S3.E3.m1.3.3.3.3">𝑘</ci></list></apply><apply id="S3.E3.m1.7.7.1.1.3.cmml" xref="S3.E3.m1.7.7.1.1.3"><ci id="S3.E3.m1.7.7.1.1.3.1.cmml" xref="S3.E3.m1.7.7.1.1.3.1">⋅</ci><ci id="S3.E3.m1.7.7.1.1.3.2.cmml" xref="S3.E3.m1.7.7.1.1.3.2">𝐈</ci><apply id="S3.E3.m1.7.7.1.1.3.3.cmml" xref="S3.E3.m1.7.7.1.1.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.7.7.1.1.3.3.1.cmml" xref="S3.E3.m1.7.7.1.1.3.3">subscript</csymbol><apply id="S3.E3.m1.7.7.1.1.3.3.2.cmml" xref="S3.E3.m1.7.7.1.1.3.3.2"><ci id="S3.E3.m1.7.7.1.1.3.3.2.1.cmml" xref="S3.E3.m1.7.7.1.1.3.3.2.1">¯</ci><ci id="S3.E3.m1.7.7.1.1.3.3.2.2.cmml" xref="S3.E3.m1.7.7.1.1.3.3.2.2">𝐓</ci></apply><ci id="S3.E3.m1.7.7.1.1.3.3.3a.cmml" xref="S3.E3.m1.7.7.1.1.3.3.3"><mtext mathsize="70%" id="S3.E3.m1.7.7.1.1.3.3.3.cmml" xref="S3.E3.m1.7.7.1.1.3.3.3">r2c</mtext></ci></apply><apply id="S3.E3.m1.7.7.1.1.3.4.cmml" xref="S3.E3.m1.7.7.1.1.3.4"><csymbol cd="ambiguous" id="S3.E3.m1.7.7.1.1.3.4.1.cmml" xref="S3.E3.m1.7.7.1.1.3.4">subscript</csymbol><apply id="S3.E3.m1.7.7.1.1.3.4.2.cmml" xref="S3.E3.m1.7.7.1.1.3.4"><csymbol cd="ambiguous" id="S3.E3.m1.7.7.1.1.3.4.2.1.cmml" xref="S3.E3.m1.7.7.1.1.3.4">superscript</csymbol><apply id="S3.E3.m1.7.7.1.1.3.4.2.2.cmml" xref="S3.E3.m1.7.7.1.1.3.4.2.2"><ci id="S3.E3.m1.7.7.1.1.3.4.2.2.1.cmml" xref="S3.E3.m1.7.7.1.1.3.4.2.2.1">¯</ci><ci id="S3.E3.m1.7.7.1.1.3.4.2.2.2.cmml" xref="S3.E3.m1.7.7.1.1.3.4.2.2.2">𝐕</ci></apply><ci id="S3.E3.m1.7.7.1.1.3.4.2.3.cmml" xref="S3.E3.m1.7.7.1.1.3.4.2.3">𝑃</ci></apply><list id="S3.E3.m1.6.6.3.4.cmml" xref="S3.E3.m1.6.6.3.5"><ci id="S3.E3.m1.4.4.1.1.cmml" xref="S3.E3.m1.4.4.1.1">𝑖</ci><ci id="S3.E3.m1.5.5.2.2.cmml" xref="S3.E3.m1.5.5.2.2">𝑗</ci><ci id="S3.E3.m1.6.6.3.3.cmml" xref="S3.E3.m1.6.6.3.3">𝑘</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.7c">\mathbf{V}^{I}_{i,j,k}=\mathbf{I}\cdot\mathbf{\bar{T}}_{\text{r2c}}\cdot\mathbf{\bar{V}}^{P}_{i,j,k},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS4.p3.4" class="ltx_p">where <math id="S3.SS4.p3.4.m1.8" class="ltx_Math" alttext="\mathbf{\bar{V}}^{P}_{i,j,k}=[\mathbf{V}^{P}_{i,j,k},1]\in\mathbb{R}^{4}" display="inline"><semantics id="S3.SS4.p3.4.m1.8a"><mrow id="S3.SS4.p3.4.m1.8.8" xref="S3.SS4.p3.4.m1.8.8.cmml"><msubsup id="S3.SS4.p3.4.m1.8.8.3" xref="S3.SS4.p3.4.m1.8.8.3.cmml"><mover accent="true" id="S3.SS4.p3.4.m1.8.8.3.2.2" xref="S3.SS4.p3.4.m1.8.8.3.2.2.cmml"><mi id="S3.SS4.p3.4.m1.8.8.3.2.2.2" xref="S3.SS4.p3.4.m1.8.8.3.2.2.2.cmml">𝐕</mi><mo id="S3.SS4.p3.4.m1.8.8.3.2.2.1" xref="S3.SS4.p3.4.m1.8.8.3.2.2.1.cmml">¯</mo></mover><mrow id="S3.SS4.p3.4.m1.3.3.3.5" xref="S3.SS4.p3.4.m1.3.3.3.4.cmml"><mi id="S3.SS4.p3.4.m1.1.1.1.1" xref="S3.SS4.p3.4.m1.1.1.1.1.cmml">i</mi><mo id="S3.SS4.p3.4.m1.3.3.3.5.1" xref="S3.SS4.p3.4.m1.3.3.3.4.cmml">,</mo><mi id="S3.SS4.p3.4.m1.2.2.2.2" xref="S3.SS4.p3.4.m1.2.2.2.2.cmml">j</mi><mo id="S3.SS4.p3.4.m1.3.3.3.5.2" xref="S3.SS4.p3.4.m1.3.3.3.4.cmml">,</mo><mi id="S3.SS4.p3.4.m1.3.3.3.3" xref="S3.SS4.p3.4.m1.3.3.3.3.cmml">k</mi></mrow><mi id="S3.SS4.p3.4.m1.8.8.3.2.3" xref="S3.SS4.p3.4.m1.8.8.3.2.3.cmml">P</mi></msubsup><mo id="S3.SS4.p3.4.m1.8.8.4" xref="S3.SS4.p3.4.m1.8.8.4.cmml">=</mo><mrow id="S3.SS4.p3.4.m1.8.8.1.1" xref="S3.SS4.p3.4.m1.8.8.1.2.cmml"><mo stretchy="false" id="S3.SS4.p3.4.m1.8.8.1.1.2" xref="S3.SS4.p3.4.m1.8.8.1.2.cmml">[</mo><msubsup id="S3.SS4.p3.4.m1.8.8.1.1.1" xref="S3.SS4.p3.4.m1.8.8.1.1.1.cmml"><mi id="S3.SS4.p3.4.m1.8.8.1.1.1.2.2" xref="S3.SS4.p3.4.m1.8.8.1.1.1.2.2.cmml">𝐕</mi><mrow id="S3.SS4.p3.4.m1.6.6.3.5" xref="S3.SS4.p3.4.m1.6.6.3.4.cmml"><mi id="S3.SS4.p3.4.m1.4.4.1.1" xref="S3.SS4.p3.4.m1.4.4.1.1.cmml">i</mi><mo id="S3.SS4.p3.4.m1.6.6.3.5.1" xref="S3.SS4.p3.4.m1.6.6.3.4.cmml">,</mo><mi id="S3.SS4.p3.4.m1.5.5.2.2" xref="S3.SS4.p3.4.m1.5.5.2.2.cmml">j</mi><mo id="S3.SS4.p3.4.m1.6.6.3.5.2" xref="S3.SS4.p3.4.m1.6.6.3.4.cmml">,</mo><mi id="S3.SS4.p3.4.m1.6.6.3.3" xref="S3.SS4.p3.4.m1.6.6.3.3.cmml">k</mi></mrow><mi id="S3.SS4.p3.4.m1.8.8.1.1.1.2.3" xref="S3.SS4.p3.4.m1.8.8.1.1.1.2.3.cmml">P</mi></msubsup><mo id="S3.SS4.p3.4.m1.8.8.1.1.3" xref="S3.SS4.p3.4.m1.8.8.1.2.cmml">,</mo><mn id="S3.SS4.p3.4.m1.7.7" xref="S3.SS4.p3.4.m1.7.7.cmml">1</mn><mo stretchy="false" id="S3.SS4.p3.4.m1.8.8.1.1.4" xref="S3.SS4.p3.4.m1.8.8.1.2.cmml">]</mo></mrow><mo id="S3.SS4.p3.4.m1.8.8.5" xref="S3.SS4.p3.4.m1.8.8.5.cmml">∈</mo><msup id="S3.SS4.p3.4.m1.8.8.6" xref="S3.SS4.p3.4.m1.8.8.6.cmml"><mi id="S3.SS4.p3.4.m1.8.8.6.2" xref="S3.SS4.p3.4.m1.8.8.6.2.cmml">ℝ</mi><mn id="S3.SS4.p3.4.m1.8.8.6.3" xref="S3.SS4.p3.4.m1.8.8.6.3.cmml">4</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.4.m1.8b"><apply id="S3.SS4.p3.4.m1.8.8.cmml" xref="S3.SS4.p3.4.m1.8.8"><and id="S3.SS4.p3.4.m1.8.8a.cmml" xref="S3.SS4.p3.4.m1.8.8"></and><apply id="S3.SS4.p3.4.m1.8.8b.cmml" xref="S3.SS4.p3.4.m1.8.8"><eq id="S3.SS4.p3.4.m1.8.8.4.cmml" xref="S3.SS4.p3.4.m1.8.8.4"></eq><apply id="S3.SS4.p3.4.m1.8.8.3.cmml" xref="S3.SS4.p3.4.m1.8.8.3"><csymbol cd="ambiguous" id="S3.SS4.p3.4.m1.8.8.3.1.cmml" xref="S3.SS4.p3.4.m1.8.8.3">subscript</csymbol><apply id="S3.SS4.p3.4.m1.8.8.3.2.cmml" xref="S3.SS4.p3.4.m1.8.8.3"><csymbol cd="ambiguous" id="S3.SS4.p3.4.m1.8.8.3.2.1.cmml" xref="S3.SS4.p3.4.m1.8.8.3">superscript</csymbol><apply id="S3.SS4.p3.4.m1.8.8.3.2.2.cmml" xref="S3.SS4.p3.4.m1.8.8.3.2.2"><ci id="S3.SS4.p3.4.m1.8.8.3.2.2.1.cmml" xref="S3.SS4.p3.4.m1.8.8.3.2.2.1">¯</ci><ci id="S3.SS4.p3.4.m1.8.8.3.2.2.2.cmml" xref="S3.SS4.p3.4.m1.8.8.3.2.2.2">𝐕</ci></apply><ci id="S3.SS4.p3.4.m1.8.8.3.2.3.cmml" xref="S3.SS4.p3.4.m1.8.8.3.2.3">𝑃</ci></apply><list id="S3.SS4.p3.4.m1.3.3.3.4.cmml" xref="S3.SS4.p3.4.m1.3.3.3.5"><ci id="S3.SS4.p3.4.m1.1.1.1.1.cmml" xref="S3.SS4.p3.4.m1.1.1.1.1">𝑖</ci><ci id="S3.SS4.p3.4.m1.2.2.2.2.cmml" xref="S3.SS4.p3.4.m1.2.2.2.2">𝑗</ci><ci id="S3.SS4.p3.4.m1.3.3.3.3.cmml" xref="S3.SS4.p3.4.m1.3.3.3.3">𝑘</ci></list></apply><interval closure="closed" id="S3.SS4.p3.4.m1.8.8.1.2.cmml" xref="S3.SS4.p3.4.m1.8.8.1.1"><apply id="S3.SS4.p3.4.m1.8.8.1.1.1.cmml" xref="S3.SS4.p3.4.m1.8.8.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.4.m1.8.8.1.1.1.1.cmml" xref="S3.SS4.p3.4.m1.8.8.1.1.1">subscript</csymbol><apply id="S3.SS4.p3.4.m1.8.8.1.1.1.2.cmml" xref="S3.SS4.p3.4.m1.8.8.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.4.m1.8.8.1.1.1.2.1.cmml" xref="S3.SS4.p3.4.m1.8.8.1.1.1">superscript</csymbol><ci id="S3.SS4.p3.4.m1.8.8.1.1.1.2.2.cmml" xref="S3.SS4.p3.4.m1.8.8.1.1.1.2.2">𝐕</ci><ci id="S3.SS4.p3.4.m1.8.8.1.1.1.2.3.cmml" xref="S3.SS4.p3.4.m1.8.8.1.1.1.2.3">𝑃</ci></apply><list id="S3.SS4.p3.4.m1.6.6.3.4.cmml" xref="S3.SS4.p3.4.m1.6.6.3.5"><ci id="S3.SS4.p3.4.m1.4.4.1.1.cmml" xref="S3.SS4.p3.4.m1.4.4.1.1">𝑖</ci><ci id="S3.SS4.p3.4.m1.5.5.2.2.cmml" xref="S3.SS4.p3.4.m1.5.5.2.2">𝑗</ci><ci id="S3.SS4.p3.4.m1.6.6.3.3.cmml" xref="S3.SS4.p3.4.m1.6.6.3.3">𝑘</ci></list></apply><cn type="integer" id="S3.SS4.p3.4.m1.7.7.cmml" xref="S3.SS4.p3.4.m1.7.7">1</cn></interval></apply><apply id="S3.SS4.p3.4.m1.8.8c.cmml" xref="S3.SS4.p3.4.m1.8.8"><in id="S3.SS4.p3.4.m1.8.8.5.cmml" xref="S3.SS4.p3.4.m1.8.8.5"></in><share href="#S3.SS4.p3.4.m1.8.8.1.cmml" id="S3.SS4.p3.4.m1.8.8d.cmml" xref="S3.SS4.p3.4.m1.8.8"></share><apply id="S3.SS4.p3.4.m1.8.8.6.cmml" xref="S3.SS4.p3.4.m1.8.8.6"><csymbol cd="ambiguous" id="S3.SS4.p3.4.m1.8.8.6.1.cmml" xref="S3.SS4.p3.4.m1.8.8.6">superscript</csymbol><ci id="S3.SS4.p3.4.m1.8.8.6.2.cmml" xref="S3.SS4.p3.4.m1.8.8.6.2">ℝ</ci><cn type="integer" id="S3.SS4.p3.4.m1.8.8.6.3.cmml" xref="S3.SS4.p3.4.m1.8.8.6.3">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.4.m1.8c">\mathbf{\bar{V}}^{P}_{i,j,k}=[\mathbf{V}^{P}_{i,j,k},1]\in\mathbb{R}^{4}</annotation></semantics></math> is the extended coordinates, and</p>
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.1" class="ltx_Math" alttext="\mathbf{\bar{T}}_{\text{r2c}}=\begin{bmatrix}\mathbf{T}_{\text{r2c}}&amp;\mathbf{0}\\
\mathbf{0}&amp;1\end{bmatrix}\in\mathbb{R}^{4\times 4}" display="block"><semantics id="S3.E4.m1.1a"><mrow id="S3.E4.m1.1.2" xref="S3.E4.m1.1.2.cmml"><msub id="S3.E4.m1.1.2.2" xref="S3.E4.m1.1.2.2.cmml"><mover accent="true" id="S3.E4.m1.1.2.2.2" xref="S3.E4.m1.1.2.2.2.cmml"><mi id="S3.E4.m1.1.2.2.2.2" xref="S3.E4.m1.1.2.2.2.2.cmml">𝐓</mi><mo id="S3.E4.m1.1.2.2.2.1" xref="S3.E4.m1.1.2.2.2.1.cmml">¯</mo></mover><mtext id="S3.E4.m1.1.2.2.3" xref="S3.E4.m1.1.2.2.3a.cmml">r2c</mtext></msub><mo id="S3.E4.m1.1.2.3" xref="S3.E4.m1.1.2.3.cmml">=</mo><mrow id="S3.E4.m1.1.1.3" xref="S3.E4.m1.1.1.2.cmml"><mo id="S3.E4.m1.1.1.3.1" xref="S3.E4.m1.1.1.2.1.cmml">[</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S3.E4.m1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml"><mtr id="S3.E4.m1.1.1.1.1a" xref="S3.E4.m1.1.1.1.1.cmml"><mtd id="S3.E4.m1.1.1.1.1b" xref="S3.E4.m1.1.1.1.1.cmml"><msub id="S3.E4.m1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.1.2.cmml">𝐓</mi><mtext id="S3.E4.m1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.1.3a.cmml">r2c</mtext></msub></mtd><mtd id="S3.E4.m1.1.1.1.1c" xref="S3.E4.m1.1.1.1.1.cmml"><mn id="S3.E4.m1.1.1.1.1.1.2.1" xref="S3.E4.m1.1.1.1.1.1.2.1.cmml">𝟎</mn></mtd></mtr><mtr id="S3.E4.m1.1.1.1.1d" xref="S3.E4.m1.1.1.1.1.cmml"><mtd id="S3.E4.m1.1.1.1.1e" xref="S3.E4.m1.1.1.1.1.cmml"><mn id="S3.E4.m1.1.1.1.1.2.1.1" xref="S3.E4.m1.1.1.1.1.2.1.1.cmml">𝟎</mn></mtd><mtd id="S3.E4.m1.1.1.1.1f" xref="S3.E4.m1.1.1.1.1.cmml"><mn id="S3.E4.m1.1.1.1.1.2.2.1" xref="S3.E4.m1.1.1.1.1.2.2.1.cmml">1</mn></mtd></mtr></mtable><mo id="S3.E4.m1.1.1.3.2" xref="S3.E4.m1.1.1.2.1.cmml">]</mo></mrow><mo id="S3.E4.m1.1.2.4" xref="S3.E4.m1.1.2.4.cmml">∈</mo><msup id="S3.E4.m1.1.2.5" xref="S3.E4.m1.1.2.5.cmml"><mi id="S3.E4.m1.1.2.5.2" xref="S3.E4.m1.1.2.5.2.cmml">ℝ</mi><mrow id="S3.E4.m1.1.2.5.3" xref="S3.E4.m1.1.2.5.3.cmml"><mn id="S3.E4.m1.1.2.5.3.2" xref="S3.E4.m1.1.2.5.3.2.cmml">4</mn><mo lspace="0.222em" rspace="0.222em" id="S3.E4.m1.1.2.5.3.1" xref="S3.E4.m1.1.2.5.3.1.cmml">×</mo><mn id="S3.E4.m1.1.2.5.3.3" xref="S3.E4.m1.1.2.5.3.3.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.2.cmml" xref="S3.E4.m1.1.2"><and id="S3.E4.m1.1.2a.cmml" xref="S3.E4.m1.1.2"></and><apply id="S3.E4.m1.1.2b.cmml" xref="S3.E4.m1.1.2"><eq id="S3.E4.m1.1.2.3.cmml" xref="S3.E4.m1.1.2.3"></eq><apply id="S3.E4.m1.1.2.2.cmml" xref="S3.E4.m1.1.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.2.2.1.cmml" xref="S3.E4.m1.1.2.2">subscript</csymbol><apply id="S3.E4.m1.1.2.2.2.cmml" xref="S3.E4.m1.1.2.2.2"><ci id="S3.E4.m1.1.2.2.2.1.cmml" xref="S3.E4.m1.1.2.2.2.1">¯</ci><ci id="S3.E4.m1.1.2.2.2.2.cmml" xref="S3.E4.m1.1.2.2.2.2">𝐓</ci></apply><ci id="S3.E4.m1.1.2.2.3a.cmml" xref="S3.E4.m1.1.2.2.3"><mtext mathsize="70%" id="S3.E4.m1.1.2.2.3.cmml" xref="S3.E4.m1.1.2.2.3">r2c</mtext></ci></apply><apply id="S3.E4.m1.1.1.2.cmml" xref="S3.E4.m1.1.1.3"><csymbol cd="latexml" id="S3.E4.m1.1.1.2.1.cmml" xref="S3.E4.m1.1.1.3.1">matrix</csymbol><matrix id="S3.E4.m1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1"><matrixrow id="S3.E4.m1.1.1.1.1a.cmml" xref="S3.E4.m1.1.1.1.1"><apply id="S3.E4.m1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.2">𝐓</ci><ci id="S3.E4.m1.1.1.1.1.1.1.1.3a.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.3"><mtext mathsize="70%" id="S3.E4.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.3">r2c</mtext></ci></apply><cn type="integer" id="S3.E4.m1.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.1.1.1.1.1.2.1">0</cn></matrixrow><matrixrow id="S3.E4.m1.1.1.1.1b.cmml" xref="S3.E4.m1.1.1.1.1"><cn type="integer" id="S3.E4.m1.1.1.1.1.2.1.1.cmml" xref="S3.E4.m1.1.1.1.1.2.1.1">0</cn><cn type="integer" id="S3.E4.m1.1.1.1.1.2.2.1.cmml" xref="S3.E4.m1.1.1.1.1.2.2.1">1</cn></matrixrow></matrix></apply></apply><apply id="S3.E4.m1.1.2c.cmml" xref="S3.E4.m1.1.2"><in id="S3.E4.m1.1.2.4.cmml" xref="S3.E4.m1.1.2.4"></in><share href="#S3.E4.m1.1.1.cmml" id="S3.E4.m1.1.2d.cmml" xref="S3.E4.m1.1.2"></share><apply id="S3.E4.m1.1.2.5.cmml" xref="S3.E4.m1.1.2.5"><csymbol cd="ambiguous" id="S3.E4.m1.1.2.5.1.cmml" xref="S3.E4.m1.1.2.5">superscript</csymbol><ci id="S3.E4.m1.1.2.5.2.cmml" xref="S3.E4.m1.1.2.5.2">ℝ</ci><apply id="S3.E4.m1.1.2.5.3.cmml" xref="S3.E4.m1.1.2.5.3"><times id="S3.E4.m1.1.2.5.3.1.cmml" xref="S3.E4.m1.1.2.5.3.1"></times><cn type="integer" id="S3.E4.m1.1.2.5.3.2.cmml" xref="S3.E4.m1.1.2.5.3.2">4</cn><cn type="integer" id="S3.E4.m1.1.2.5.3.3.cmml" xref="S3.E4.m1.1.2.5.3.3">4</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">\mathbf{\bar{T}}_{\text{r2c}}=\begin{bmatrix}\mathbf{T}_{\text{r2c}}&amp;\mathbf{0}\\
\mathbf{0}&amp;1\end{bmatrix}\in\mathbb{R}^{4\times 4}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S3.SS4.p3.7" class="ltx_p">is the extended coordinate transformation matrix. <math id="S3.SS4.p3.5.m1.6" class="ltx_Math" alttext="\mathbf{V}^{I}_{i,j,k}=[ud,vd,d]" display="inline"><semantics id="S3.SS4.p3.5.m1.6a"><mrow id="S3.SS4.p3.5.m1.6.6" xref="S3.SS4.p3.5.m1.6.6.cmml"><msubsup id="S3.SS4.p3.5.m1.6.6.4" xref="S3.SS4.p3.5.m1.6.6.4.cmml"><mi id="S3.SS4.p3.5.m1.6.6.4.2.2" xref="S3.SS4.p3.5.m1.6.6.4.2.2.cmml">𝐕</mi><mrow id="S3.SS4.p3.5.m1.3.3.3.5" xref="S3.SS4.p3.5.m1.3.3.3.4.cmml"><mi id="S3.SS4.p3.5.m1.1.1.1.1" xref="S3.SS4.p3.5.m1.1.1.1.1.cmml">i</mi><mo id="S3.SS4.p3.5.m1.3.3.3.5.1" xref="S3.SS4.p3.5.m1.3.3.3.4.cmml">,</mo><mi id="S3.SS4.p3.5.m1.2.2.2.2" xref="S3.SS4.p3.5.m1.2.2.2.2.cmml">j</mi><mo id="S3.SS4.p3.5.m1.3.3.3.5.2" xref="S3.SS4.p3.5.m1.3.3.3.4.cmml">,</mo><mi id="S3.SS4.p3.5.m1.3.3.3.3" xref="S3.SS4.p3.5.m1.3.3.3.3.cmml">k</mi></mrow><mi id="S3.SS4.p3.5.m1.6.6.4.2.3" xref="S3.SS4.p3.5.m1.6.6.4.2.3.cmml">I</mi></msubsup><mo id="S3.SS4.p3.5.m1.6.6.3" xref="S3.SS4.p3.5.m1.6.6.3.cmml">=</mo><mrow id="S3.SS4.p3.5.m1.6.6.2.2" xref="S3.SS4.p3.5.m1.6.6.2.3.cmml"><mo stretchy="false" id="S3.SS4.p3.5.m1.6.6.2.2.3" xref="S3.SS4.p3.5.m1.6.6.2.3.cmml">[</mo><mrow id="S3.SS4.p3.5.m1.5.5.1.1.1" xref="S3.SS4.p3.5.m1.5.5.1.1.1.cmml"><mi id="S3.SS4.p3.5.m1.5.5.1.1.1.2" xref="S3.SS4.p3.5.m1.5.5.1.1.1.2.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p3.5.m1.5.5.1.1.1.1" xref="S3.SS4.p3.5.m1.5.5.1.1.1.1.cmml">​</mo><mi id="S3.SS4.p3.5.m1.5.5.1.1.1.3" xref="S3.SS4.p3.5.m1.5.5.1.1.1.3.cmml">d</mi></mrow><mo id="S3.SS4.p3.5.m1.6.6.2.2.4" xref="S3.SS4.p3.5.m1.6.6.2.3.cmml">,</mo><mrow id="S3.SS4.p3.5.m1.6.6.2.2.2" xref="S3.SS4.p3.5.m1.6.6.2.2.2.cmml"><mi id="S3.SS4.p3.5.m1.6.6.2.2.2.2" xref="S3.SS4.p3.5.m1.6.6.2.2.2.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p3.5.m1.6.6.2.2.2.1" xref="S3.SS4.p3.5.m1.6.6.2.2.2.1.cmml">​</mo><mi id="S3.SS4.p3.5.m1.6.6.2.2.2.3" xref="S3.SS4.p3.5.m1.6.6.2.2.2.3.cmml">d</mi></mrow><mo id="S3.SS4.p3.5.m1.6.6.2.2.5" xref="S3.SS4.p3.5.m1.6.6.2.3.cmml">,</mo><mi id="S3.SS4.p3.5.m1.4.4" xref="S3.SS4.p3.5.m1.4.4.cmml">d</mi><mo stretchy="false" id="S3.SS4.p3.5.m1.6.6.2.2.6" xref="S3.SS4.p3.5.m1.6.6.2.3.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.5.m1.6b"><apply id="S3.SS4.p3.5.m1.6.6.cmml" xref="S3.SS4.p3.5.m1.6.6"><eq id="S3.SS4.p3.5.m1.6.6.3.cmml" xref="S3.SS4.p3.5.m1.6.6.3"></eq><apply id="S3.SS4.p3.5.m1.6.6.4.cmml" xref="S3.SS4.p3.5.m1.6.6.4"><csymbol cd="ambiguous" id="S3.SS4.p3.5.m1.6.6.4.1.cmml" xref="S3.SS4.p3.5.m1.6.6.4">subscript</csymbol><apply id="S3.SS4.p3.5.m1.6.6.4.2.cmml" xref="S3.SS4.p3.5.m1.6.6.4"><csymbol cd="ambiguous" id="S3.SS4.p3.5.m1.6.6.4.2.1.cmml" xref="S3.SS4.p3.5.m1.6.6.4">superscript</csymbol><ci id="S3.SS4.p3.5.m1.6.6.4.2.2.cmml" xref="S3.SS4.p3.5.m1.6.6.4.2.2">𝐕</ci><ci id="S3.SS4.p3.5.m1.6.6.4.2.3.cmml" xref="S3.SS4.p3.5.m1.6.6.4.2.3">𝐼</ci></apply><list id="S3.SS4.p3.5.m1.3.3.3.4.cmml" xref="S3.SS4.p3.5.m1.3.3.3.5"><ci id="S3.SS4.p3.5.m1.1.1.1.1.cmml" xref="S3.SS4.p3.5.m1.1.1.1.1">𝑖</ci><ci id="S3.SS4.p3.5.m1.2.2.2.2.cmml" xref="S3.SS4.p3.5.m1.2.2.2.2">𝑗</ci><ci id="S3.SS4.p3.5.m1.3.3.3.3.cmml" xref="S3.SS4.p3.5.m1.3.3.3.3">𝑘</ci></list></apply><list id="S3.SS4.p3.5.m1.6.6.2.3.cmml" xref="S3.SS4.p3.5.m1.6.6.2.2"><apply id="S3.SS4.p3.5.m1.5.5.1.1.1.cmml" xref="S3.SS4.p3.5.m1.5.5.1.1.1"><times id="S3.SS4.p3.5.m1.5.5.1.1.1.1.cmml" xref="S3.SS4.p3.5.m1.5.5.1.1.1.1"></times><ci id="S3.SS4.p3.5.m1.5.5.1.1.1.2.cmml" xref="S3.SS4.p3.5.m1.5.5.1.1.1.2">𝑢</ci><ci id="S3.SS4.p3.5.m1.5.5.1.1.1.3.cmml" xref="S3.SS4.p3.5.m1.5.5.1.1.1.3">𝑑</ci></apply><apply id="S3.SS4.p3.5.m1.6.6.2.2.2.cmml" xref="S3.SS4.p3.5.m1.6.6.2.2.2"><times id="S3.SS4.p3.5.m1.6.6.2.2.2.1.cmml" xref="S3.SS4.p3.5.m1.6.6.2.2.2.1"></times><ci id="S3.SS4.p3.5.m1.6.6.2.2.2.2.cmml" xref="S3.SS4.p3.5.m1.6.6.2.2.2.2">𝑣</ci><ci id="S3.SS4.p3.5.m1.6.6.2.2.2.3.cmml" xref="S3.SS4.p3.5.m1.6.6.2.2.2.3">𝑑</ci></apply><ci id="S3.SS4.p3.5.m1.4.4.cmml" xref="S3.SS4.p3.5.m1.4.4">𝑑</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.5.m1.6c">\mathbf{V}^{I}_{i,j,k}=[ud,vd,d]</annotation></semantics></math> is the projected coordinate in the image coordinate system, where <math id="S3.SS4.p3.6.m2.2" class="ltx_Math" alttext="(u,v)" display="inline"><semantics id="S3.SS4.p3.6.m2.2a"><mrow id="S3.SS4.p3.6.m2.2.3.2" xref="S3.SS4.p3.6.m2.2.3.1.cmml"><mo stretchy="false" id="S3.SS4.p3.6.m2.2.3.2.1" xref="S3.SS4.p3.6.m2.2.3.1.cmml">(</mo><mi id="S3.SS4.p3.6.m2.1.1" xref="S3.SS4.p3.6.m2.1.1.cmml">u</mi><mo id="S3.SS4.p3.6.m2.2.3.2.2" xref="S3.SS4.p3.6.m2.2.3.1.cmml">,</mo><mi id="S3.SS4.p3.6.m2.2.2" xref="S3.SS4.p3.6.m2.2.2.cmml">v</mi><mo stretchy="false" id="S3.SS4.p3.6.m2.2.3.2.3" xref="S3.SS4.p3.6.m2.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.6.m2.2b"><interval closure="open" id="S3.SS4.p3.6.m2.2.3.1.cmml" xref="S3.SS4.p3.6.m2.2.3.2"><ci id="S3.SS4.p3.6.m2.1.1.cmml" xref="S3.SS4.p3.6.m2.1.1">𝑢</ci><ci id="S3.SS4.p3.6.m2.2.2.cmml" xref="S3.SS4.p3.6.m2.2.2">𝑣</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.6.m2.2c">(u,v)</annotation></semantics></math> and <math id="S3.SS4.p3.7.m3.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S3.SS4.p3.7.m3.1a"><mi id="S3.SS4.p3.7.m3.1.1" xref="S3.SS4.p3.7.m3.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.7.m3.1b"><ci id="S3.SS4.p3.7.m3.1.1.cmml" xref="S3.SS4.p3.7.m3.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.7.m3.1c">d</annotation></semantics></math> denotes the pixel index and the image depth, respectively.</p>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<p id="S3.SS4.p4.4" class="ltx_p">Subsequently, the feature of each pre-defined voxel can be obtained through bi-linear sampling on each multi-level image PV feature map. Specifically, we select the <math id="S3.SS4.p4.1.m1.1" class="ltx_Math" alttext="2\times 2" display="inline"><semantics id="S3.SS4.p4.1.m1.1a"><mrow id="S3.SS4.p4.1.m1.1.1" xref="S3.SS4.p4.1.m1.1.1.cmml"><mn id="S3.SS4.p4.1.m1.1.1.2" xref="S3.SS4.p4.1.m1.1.1.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p4.1.m1.1.1.1" xref="S3.SS4.p4.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS4.p4.1.m1.1.1.3" xref="S3.SS4.p4.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.1.m1.1b"><apply id="S3.SS4.p4.1.m1.1.1.cmml" xref="S3.SS4.p4.1.m1.1.1"><times id="S3.SS4.p4.1.m1.1.1.1.cmml" xref="S3.SS4.p4.1.m1.1.1.1"></times><cn type="integer" id="S3.SS4.p4.1.m1.1.1.2.cmml" xref="S3.SS4.p4.1.m1.1.1.2">2</cn><cn type="integer" id="S3.SS4.p4.1.m1.1.1.3.cmml" xref="S3.SS4.p4.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.1.m1.1c">2\times 2</annotation></semantics></math> pixels closest to <math id="S3.SS4.p4.2.m2.2" class="ltx_Math" alttext="(u,v)" display="inline"><semantics id="S3.SS4.p4.2.m2.2a"><mrow id="S3.SS4.p4.2.m2.2.3.2" xref="S3.SS4.p4.2.m2.2.3.1.cmml"><mo stretchy="false" id="S3.SS4.p4.2.m2.2.3.2.1" xref="S3.SS4.p4.2.m2.2.3.1.cmml">(</mo><mi id="S3.SS4.p4.2.m2.1.1" xref="S3.SS4.p4.2.m2.1.1.cmml">u</mi><mo id="S3.SS4.p4.2.m2.2.3.2.2" xref="S3.SS4.p4.2.m2.2.3.1.cmml">,</mo><mi id="S3.SS4.p4.2.m2.2.2" xref="S3.SS4.p4.2.m2.2.2.cmml">v</mi><mo stretchy="false" id="S3.SS4.p4.2.m2.2.3.2.3" xref="S3.SS4.p4.2.m2.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.2.m2.2b"><interval closure="open" id="S3.SS4.p4.2.m2.2.3.1.cmml" xref="S3.SS4.p4.2.m2.2.3.2"><ci id="S3.SS4.p4.2.m2.1.1.cmml" xref="S3.SS4.p4.2.m2.1.1">𝑢</ci><ci id="S3.SS4.p4.2.m2.2.2.cmml" xref="S3.SS4.p4.2.m2.2.2">𝑣</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.2.m2.2c">(u,v)</annotation></semantics></math> and compute the weighted sum of their features, which is then assigned to the corresponding voxel as its feature.
This step is accomplished using the “<math id="S3.SS4.p4.3.m3.4" class="ltx_Math" alttext="\mathtt{torch.nn.functional.grid\_sample}" display="inline"><semantics id="S3.SS4.p4.3.m3.4a"><mrow id="S3.SS4.p4.3.m3.4.4.1" xref="S3.SS4.p4.3.m3.4.4.2.cmml"><mi id="S3.SS4.p4.3.m3.1.1" xref="S3.SS4.p4.3.m3.1.1.cmml">𝚝𝚘𝚛𝚌𝚑</mi><mo lspace="0em" rspace="0.167em" id="S3.SS4.p4.3.m3.4.4.1.2" xref="S3.SS4.p4.3.m3.4.4.2a.cmml">.</mo><mi id="S3.SS4.p4.3.m3.2.2" xref="S3.SS4.p4.3.m3.2.2.cmml">𝚗𝚗</mi><mo lspace="0em" rspace="0.167em" id="S3.SS4.p4.3.m3.4.4.1.3" xref="S3.SS4.p4.3.m3.4.4.2a.cmml">.</mo><mi id="S3.SS4.p4.3.m3.3.3" xref="S3.SS4.p4.3.m3.3.3.cmml">𝚏𝚞𝚗𝚌𝚝𝚒𝚘𝚗𝚊𝚕</mi><mo lspace="0em" rspace="0.167em" id="S3.SS4.p4.3.m3.4.4.1.4" xref="S3.SS4.p4.3.m3.4.4.2a.cmml">.</mo><mrow id="S3.SS4.p4.3.m3.4.4.1.1" xref="S3.SS4.p4.3.m3.4.4.1.1.cmml"><mi id="S3.SS4.p4.3.m3.4.4.1.1.2" xref="S3.SS4.p4.3.m3.4.4.1.1.2.cmml">𝚐𝚛𝚒𝚍</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p4.3.m3.4.4.1.1.1" xref="S3.SS4.p4.3.m3.4.4.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S3.SS4.p4.3.m3.4.4.1.1.3" xref="S3.SS4.p4.3.m3.4.4.1.1.3.cmml">_</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p4.3.m3.4.4.1.1.1a" xref="S3.SS4.p4.3.m3.4.4.1.1.1.cmml">​</mo><mi id="S3.SS4.p4.3.m3.4.4.1.1.4" xref="S3.SS4.p4.3.m3.4.4.1.1.4.cmml">𝚜𝚊𝚖𝚙𝚕𝚎</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.3.m3.4b"><apply id="S3.SS4.p4.3.m3.4.4.2.cmml" xref="S3.SS4.p4.3.m3.4.4.1"><csymbol cd="ambiguous" id="S3.SS4.p4.3.m3.4.4.2a.cmml" xref="S3.SS4.p4.3.m3.4.4.1.2">formulae-sequence</csymbol><ci id="S3.SS4.p4.3.m3.1.1.cmml" xref="S3.SS4.p4.3.m3.1.1">𝚝𝚘𝚛𝚌𝚑</ci><ci id="S3.SS4.p4.3.m3.2.2.cmml" xref="S3.SS4.p4.3.m3.2.2">𝚗𝚗</ci><ci id="S3.SS4.p4.3.m3.3.3.cmml" xref="S3.SS4.p4.3.m3.3.3">𝚏𝚞𝚗𝚌𝚝𝚒𝚘𝚗𝚊𝚕</ci><apply id="S3.SS4.p4.3.m3.4.4.1.1.cmml" xref="S3.SS4.p4.3.m3.4.4.1.1"><times id="S3.SS4.p4.3.m3.4.4.1.1.1.cmml" xref="S3.SS4.p4.3.m3.4.4.1.1.1"></times><ci id="S3.SS4.p4.3.m3.4.4.1.1.2.cmml" xref="S3.SS4.p4.3.m3.4.4.1.1.2">𝚐𝚛𝚒𝚍</ci><ci id="S3.SS4.p4.3.m3.4.4.1.1.3.cmml" xref="S3.SS4.p4.3.m3.4.4.1.1.3">_</ci><ci id="S3.SS4.p4.3.m3.4.4.1.1.4.cmml" xref="S3.SS4.p4.3.m3.4.4.1.1.4">𝚜𝚊𝚖𝚙𝚕𝚎</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.3.m3.4c">\mathtt{torch.nn.functional.grid\_sample}</annotation></semantics></math>” operation, resulting in image 3D voxel features <math id="S3.SS4.p4.4.m4.1" class="ltx_Math" alttext="\mathbf{F}^{I}_{\text{3D}}\in\mathbb{R}^{N_{\text{lvl}}\times X\times Y\times Z\times C_{I}}" display="inline"><semantics id="S3.SS4.p4.4.m4.1a"><mrow id="S3.SS4.p4.4.m4.1.1" xref="S3.SS4.p4.4.m4.1.1.cmml"><msubsup id="S3.SS4.p4.4.m4.1.1.2" xref="S3.SS4.p4.4.m4.1.1.2.cmml"><mi id="S3.SS4.p4.4.m4.1.1.2.2.2" xref="S3.SS4.p4.4.m4.1.1.2.2.2.cmml">𝐅</mi><mtext id="S3.SS4.p4.4.m4.1.1.2.3" xref="S3.SS4.p4.4.m4.1.1.2.3a.cmml">3D</mtext><mi id="S3.SS4.p4.4.m4.1.1.2.2.3" xref="S3.SS4.p4.4.m4.1.1.2.2.3.cmml">I</mi></msubsup><mo id="S3.SS4.p4.4.m4.1.1.1" xref="S3.SS4.p4.4.m4.1.1.1.cmml">∈</mo><msup id="S3.SS4.p4.4.m4.1.1.3" xref="S3.SS4.p4.4.m4.1.1.3.cmml"><mi id="S3.SS4.p4.4.m4.1.1.3.2" xref="S3.SS4.p4.4.m4.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS4.p4.4.m4.1.1.3.3" xref="S3.SS4.p4.4.m4.1.1.3.3.cmml"><msub id="S3.SS4.p4.4.m4.1.1.3.3.2" xref="S3.SS4.p4.4.m4.1.1.3.3.2.cmml"><mi id="S3.SS4.p4.4.m4.1.1.3.3.2.2" xref="S3.SS4.p4.4.m4.1.1.3.3.2.2.cmml">N</mi><mtext id="S3.SS4.p4.4.m4.1.1.3.3.2.3" xref="S3.SS4.p4.4.m4.1.1.3.3.2.3a.cmml">lvl</mtext></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p4.4.m4.1.1.3.3.1" xref="S3.SS4.p4.4.m4.1.1.3.3.1.cmml">×</mo><mi id="S3.SS4.p4.4.m4.1.1.3.3.3" xref="S3.SS4.p4.4.m4.1.1.3.3.3.cmml">X</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p4.4.m4.1.1.3.3.1a" xref="S3.SS4.p4.4.m4.1.1.3.3.1.cmml">×</mo><mi id="S3.SS4.p4.4.m4.1.1.3.3.4" xref="S3.SS4.p4.4.m4.1.1.3.3.4.cmml">Y</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p4.4.m4.1.1.3.3.1b" xref="S3.SS4.p4.4.m4.1.1.3.3.1.cmml">×</mo><mi id="S3.SS4.p4.4.m4.1.1.3.3.5" xref="S3.SS4.p4.4.m4.1.1.3.3.5.cmml">Z</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p4.4.m4.1.1.3.3.1c" xref="S3.SS4.p4.4.m4.1.1.3.3.1.cmml">×</mo><msub id="S3.SS4.p4.4.m4.1.1.3.3.6" xref="S3.SS4.p4.4.m4.1.1.3.3.6.cmml"><mi id="S3.SS4.p4.4.m4.1.1.3.3.6.2" xref="S3.SS4.p4.4.m4.1.1.3.3.6.2.cmml">C</mi><mi id="S3.SS4.p4.4.m4.1.1.3.3.6.3" xref="S3.SS4.p4.4.m4.1.1.3.3.6.3.cmml">I</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.4.m4.1b"><apply id="S3.SS4.p4.4.m4.1.1.cmml" xref="S3.SS4.p4.4.m4.1.1"><in id="S3.SS4.p4.4.m4.1.1.1.cmml" xref="S3.SS4.p4.4.m4.1.1.1"></in><apply id="S3.SS4.p4.4.m4.1.1.2.cmml" xref="S3.SS4.p4.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p4.4.m4.1.1.2.1.cmml" xref="S3.SS4.p4.4.m4.1.1.2">subscript</csymbol><apply id="S3.SS4.p4.4.m4.1.1.2.2.cmml" xref="S3.SS4.p4.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p4.4.m4.1.1.2.2.1.cmml" xref="S3.SS4.p4.4.m4.1.1.2">superscript</csymbol><ci id="S3.SS4.p4.4.m4.1.1.2.2.2.cmml" xref="S3.SS4.p4.4.m4.1.1.2.2.2">𝐅</ci><ci id="S3.SS4.p4.4.m4.1.1.2.2.3.cmml" xref="S3.SS4.p4.4.m4.1.1.2.2.3">𝐼</ci></apply><ci id="S3.SS4.p4.4.m4.1.1.2.3a.cmml" xref="S3.SS4.p4.4.m4.1.1.2.3"><mtext mathsize="70%" id="S3.SS4.p4.4.m4.1.1.2.3.cmml" xref="S3.SS4.p4.4.m4.1.1.2.3">3D</mtext></ci></apply><apply id="S3.SS4.p4.4.m4.1.1.3.cmml" xref="S3.SS4.p4.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.p4.4.m4.1.1.3.1.cmml" xref="S3.SS4.p4.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS4.p4.4.m4.1.1.3.2.cmml" xref="S3.SS4.p4.4.m4.1.1.3.2">ℝ</ci><apply id="S3.SS4.p4.4.m4.1.1.3.3.cmml" xref="S3.SS4.p4.4.m4.1.1.3.3"><times id="S3.SS4.p4.4.m4.1.1.3.3.1.cmml" xref="S3.SS4.p4.4.m4.1.1.3.3.1"></times><apply id="S3.SS4.p4.4.m4.1.1.3.3.2.cmml" xref="S3.SS4.p4.4.m4.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS4.p4.4.m4.1.1.3.3.2.1.cmml" xref="S3.SS4.p4.4.m4.1.1.3.3.2">subscript</csymbol><ci id="S3.SS4.p4.4.m4.1.1.3.3.2.2.cmml" xref="S3.SS4.p4.4.m4.1.1.3.3.2.2">𝑁</ci><ci id="S3.SS4.p4.4.m4.1.1.3.3.2.3a.cmml" xref="S3.SS4.p4.4.m4.1.1.3.3.2.3"><mtext mathsize="50%" id="S3.SS4.p4.4.m4.1.1.3.3.2.3.cmml" xref="S3.SS4.p4.4.m4.1.1.3.3.2.3">lvl</mtext></ci></apply><ci id="S3.SS4.p4.4.m4.1.1.3.3.3.cmml" xref="S3.SS4.p4.4.m4.1.1.3.3.3">𝑋</ci><ci id="S3.SS4.p4.4.m4.1.1.3.3.4.cmml" xref="S3.SS4.p4.4.m4.1.1.3.3.4">𝑌</ci><ci id="S3.SS4.p4.4.m4.1.1.3.3.5.cmml" xref="S3.SS4.p4.4.m4.1.1.3.3.5">𝑍</ci><apply id="S3.SS4.p4.4.m4.1.1.3.3.6.cmml" xref="S3.SS4.p4.4.m4.1.1.3.3.6"><csymbol cd="ambiguous" id="S3.SS4.p4.4.m4.1.1.3.3.6.1.cmml" xref="S3.SS4.p4.4.m4.1.1.3.3.6">subscript</csymbol><ci id="S3.SS4.p4.4.m4.1.1.3.3.6.2.cmml" xref="S3.SS4.p4.4.m4.1.1.3.3.6.2">𝐶</ci><ci id="S3.SS4.p4.4.m4.1.1.3.3.6.3.cmml" xref="S3.SS4.p4.4.m4.1.1.3.3.6.3">𝐼</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.4.m4.1c">\mathbf{F}^{I}_{\text{3D}}\in\mathbb{R}^{N_{\text{lvl}}\times X\times Y\times Z\times C_{I}}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS4.p5" class="ltx_para">
<p id="S3.SS4.p5.3" class="ltx_p"><span id="S3.SS4.p5.3.1" class="ltx_text ltx_font_bold">Depth-Based Sampling:</span> However, the aforementioned operations do not consider the predicted image depths, which may lead to sub-optimal feature lifting. While LSS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite> employs the outer product for “depth-based splatting”, this approach is not directly applicable in our “sampling” case due to the different coordinate systems of the predicted depth distribution maps and image 3D features.
To address this issue, we leverage tri-linear sampling, the 3D extension of bi-linear sampling, on the predicted multi-scale image depth distribution maps in the image coordinate system, denoted as <math id="S3.SS4.p5.1.m1.1" class="ltx_Math" alttext="\mathbf{D}_{i}^{I}" display="inline"><semantics id="S3.SS4.p5.1.m1.1a"><msubsup id="S3.SS4.p5.1.m1.1.1" xref="S3.SS4.p5.1.m1.1.1.cmml"><mi id="S3.SS4.p5.1.m1.1.1.2.2" xref="S3.SS4.p5.1.m1.1.1.2.2.cmml">𝐃</mi><mi id="S3.SS4.p5.1.m1.1.1.2.3" xref="S3.SS4.p5.1.m1.1.1.2.3.cmml">i</mi><mi id="S3.SS4.p5.1.m1.1.1.3" xref="S3.SS4.p5.1.m1.1.1.3.cmml">I</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS4.p5.1.m1.1b"><apply id="S3.SS4.p5.1.m1.1.1.cmml" xref="S3.SS4.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p5.1.m1.1.1.1.cmml" xref="S3.SS4.p5.1.m1.1.1">superscript</csymbol><apply id="S3.SS4.p5.1.m1.1.1.2.cmml" xref="S3.SS4.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p5.1.m1.1.1.2.1.cmml" xref="S3.SS4.p5.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p5.1.m1.1.1.2.2.cmml" xref="S3.SS4.p5.1.m1.1.1.2.2">𝐃</ci><ci id="S3.SS4.p5.1.m1.1.1.2.3.cmml" xref="S3.SS4.p5.1.m1.1.1.2.3">𝑖</ci></apply><ci id="S3.SS4.p5.1.m1.1.1.3.cmml" xref="S3.SS4.p5.1.m1.1.1.3">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p5.1.m1.1c">\mathbf{D}_{i}^{I}</annotation></semantics></math>, to obtain depth probabilities <math id="S3.SS4.p5.2.m2.1" class="ltx_Math" alttext="\mathbf{D}^{I}_{\text{3D}}\in\mathbb{R}^{N_{\text{lvl}}\times X\times Y\times Z}" display="inline"><semantics id="S3.SS4.p5.2.m2.1a"><mrow id="S3.SS4.p5.2.m2.1.1" xref="S3.SS4.p5.2.m2.1.1.cmml"><msubsup id="S3.SS4.p5.2.m2.1.1.2" xref="S3.SS4.p5.2.m2.1.1.2.cmml"><mi id="S3.SS4.p5.2.m2.1.1.2.2.2" xref="S3.SS4.p5.2.m2.1.1.2.2.2.cmml">𝐃</mi><mtext id="S3.SS4.p5.2.m2.1.1.2.3" xref="S3.SS4.p5.2.m2.1.1.2.3a.cmml">3D</mtext><mi id="S3.SS4.p5.2.m2.1.1.2.2.3" xref="S3.SS4.p5.2.m2.1.1.2.2.3.cmml">I</mi></msubsup><mo id="S3.SS4.p5.2.m2.1.1.1" xref="S3.SS4.p5.2.m2.1.1.1.cmml">∈</mo><msup id="S3.SS4.p5.2.m2.1.1.3" xref="S3.SS4.p5.2.m2.1.1.3.cmml"><mi id="S3.SS4.p5.2.m2.1.1.3.2" xref="S3.SS4.p5.2.m2.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS4.p5.2.m2.1.1.3.3" xref="S3.SS4.p5.2.m2.1.1.3.3.cmml"><msub id="S3.SS4.p5.2.m2.1.1.3.3.2" xref="S3.SS4.p5.2.m2.1.1.3.3.2.cmml"><mi id="S3.SS4.p5.2.m2.1.1.3.3.2.2" xref="S3.SS4.p5.2.m2.1.1.3.3.2.2.cmml">N</mi><mtext id="S3.SS4.p5.2.m2.1.1.3.3.2.3" xref="S3.SS4.p5.2.m2.1.1.3.3.2.3a.cmml">lvl</mtext></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p5.2.m2.1.1.3.3.1" xref="S3.SS4.p5.2.m2.1.1.3.3.1.cmml">×</mo><mi id="S3.SS4.p5.2.m2.1.1.3.3.3" xref="S3.SS4.p5.2.m2.1.1.3.3.3.cmml">X</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p5.2.m2.1.1.3.3.1a" xref="S3.SS4.p5.2.m2.1.1.3.3.1.cmml">×</mo><mi id="S3.SS4.p5.2.m2.1.1.3.3.4" xref="S3.SS4.p5.2.m2.1.1.3.3.4.cmml">Y</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p5.2.m2.1.1.3.3.1b" xref="S3.SS4.p5.2.m2.1.1.3.3.1.cmml">×</mo><mi id="S3.SS4.p5.2.m2.1.1.3.3.5" xref="S3.SS4.p5.2.m2.1.1.3.3.5.cmml">Z</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p5.2.m2.1b"><apply id="S3.SS4.p5.2.m2.1.1.cmml" xref="S3.SS4.p5.2.m2.1.1"><in id="S3.SS4.p5.2.m2.1.1.1.cmml" xref="S3.SS4.p5.2.m2.1.1.1"></in><apply id="S3.SS4.p5.2.m2.1.1.2.cmml" xref="S3.SS4.p5.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p5.2.m2.1.1.2.1.cmml" xref="S3.SS4.p5.2.m2.1.1.2">subscript</csymbol><apply id="S3.SS4.p5.2.m2.1.1.2.2.cmml" xref="S3.SS4.p5.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p5.2.m2.1.1.2.2.1.cmml" xref="S3.SS4.p5.2.m2.1.1.2">superscript</csymbol><ci id="S3.SS4.p5.2.m2.1.1.2.2.2.cmml" xref="S3.SS4.p5.2.m2.1.1.2.2.2">𝐃</ci><ci id="S3.SS4.p5.2.m2.1.1.2.2.3.cmml" xref="S3.SS4.p5.2.m2.1.1.2.2.3">𝐼</ci></apply><ci id="S3.SS4.p5.2.m2.1.1.2.3a.cmml" xref="S3.SS4.p5.2.m2.1.1.2.3"><mtext mathsize="70%" id="S3.SS4.p5.2.m2.1.1.2.3.cmml" xref="S3.SS4.p5.2.m2.1.1.2.3">3D</mtext></ci></apply><apply id="S3.SS4.p5.2.m2.1.1.3.cmml" xref="S3.SS4.p5.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.p5.2.m2.1.1.3.1.cmml" xref="S3.SS4.p5.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS4.p5.2.m2.1.1.3.2.cmml" xref="S3.SS4.p5.2.m2.1.1.3.2">ℝ</ci><apply id="S3.SS4.p5.2.m2.1.1.3.3.cmml" xref="S3.SS4.p5.2.m2.1.1.3.3"><times id="S3.SS4.p5.2.m2.1.1.3.3.1.cmml" xref="S3.SS4.p5.2.m2.1.1.3.3.1"></times><apply id="S3.SS4.p5.2.m2.1.1.3.3.2.cmml" xref="S3.SS4.p5.2.m2.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS4.p5.2.m2.1.1.3.3.2.1.cmml" xref="S3.SS4.p5.2.m2.1.1.3.3.2">subscript</csymbol><ci id="S3.SS4.p5.2.m2.1.1.3.3.2.2.cmml" xref="S3.SS4.p5.2.m2.1.1.3.3.2.2">𝑁</ci><ci id="S3.SS4.p5.2.m2.1.1.3.3.2.3a.cmml" xref="S3.SS4.p5.2.m2.1.1.3.3.2.3"><mtext mathsize="50%" id="S3.SS4.p5.2.m2.1.1.3.3.2.3.cmml" xref="S3.SS4.p5.2.m2.1.1.3.3.2.3">lvl</mtext></ci></apply><ci id="S3.SS4.p5.2.m2.1.1.3.3.3.cmml" xref="S3.SS4.p5.2.m2.1.1.3.3.3">𝑋</ci><ci id="S3.SS4.p5.2.m2.1.1.3.3.4.cmml" xref="S3.SS4.p5.2.m2.1.1.3.3.4">𝑌</ci><ci id="S3.SS4.p5.2.m2.1.1.3.3.5.cmml" xref="S3.SS4.p5.2.m2.1.1.3.3.5">𝑍</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p5.2.m2.1c">\mathbf{D}^{I}_{\text{3D}}\in\mathbb{R}^{N_{\text{lvl}}\times X\times Y\times Z}</annotation></semantics></math> for the pre-defined voxels in the radar coordinate system. Concretely, the pre-defined voxels in the radar coordinate system are projected onto the image plane to determine the nearest <math id="S3.SS4.p5.3.m3.1" class="ltx_Math" alttext="2\times 2\times 2" display="inline"><semantics id="S3.SS4.p5.3.m3.1a"><mrow id="S3.SS4.p5.3.m3.1.1" xref="S3.SS4.p5.3.m3.1.1.cmml"><mn id="S3.SS4.p5.3.m3.1.1.2" xref="S3.SS4.p5.3.m3.1.1.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p5.3.m3.1.1.1" xref="S3.SS4.p5.3.m3.1.1.1.cmml">×</mo><mn id="S3.SS4.p5.3.m3.1.1.3" xref="S3.SS4.p5.3.m3.1.1.3.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p5.3.m3.1.1.1a" xref="S3.SS4.p5.3.m3.1.1.1.cmml">×</mo><mn id="S3.SS4.p5.3.m3.1.1.4" xref="S3.SS4.p5.3.m3.1.1.4.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p5.3.m3.1b"><apply id="S3.SS4.p5.3.m3.1.1.cmml" xref="S3.SS4.p5.3.m3.1.1"><times id="S3.SS4.p5.3.m3.1.1.1.cmml" xref="S3.SS4.p5.3.m3.1.1.1"></times><cn type="integer" id="S3.SS4.p5.3.m3.1.1.2.cmml" xref="S3.SS4.p5.3.m3.1.1.2">2</cn><cn type="integer" id="S3.SS4.p5.3.m3.1.1.3.cmml" xref="S3.SS4.p5.3.m3.1.1.3">2</cn><cn type="integer" id="S3.SS4.p5.3.m3.1.1.4.cmml" xref="S3.SS4.p5.3.m3.1.1.4">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p5.3.m3.1c">2\times 2\times 2</annotation></semantics></math> image frustum voxels. As each image depth probability corresponds to a frustum voxel, the weighted sum of depth probabilities from the selected frustum voxels is calculated as the sampled depth probability of the pre-defined voxel.</p>
</div>
<div id="S3.SS4.p6" class="ltx_para">
<p id="S3.SS4.p6.3" class="ltx_p">Subsequently, the image 3D voxel features are multiplied by the sampled depth probabilities using the following equation:</p>
<table id="S3.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E5.m1.1" class="ltx_Math" alttext="\mathbf{F^{\prime}}^{I}_{\text{3D}}=\mathbf{F}^{I}_{\text{3D}}\odot\mathbf{D}^{I}_{\text{3D}}." display="block"><semantics id="S3.E5.m1.1a"><mrow id="S3.E5.m1.1.1.1" xref="S3.E5.m1.1.1.1.1.cmml"><mrow id="S3.E5.m1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.cmml"><mmultiscripts id="S3.E5.m1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.2.cmml"><mi id="S3.E5.m1.1.1.1.1.2.2.2.2" xref="S3.E5.m1.1.1.1.1.2.2.2.2.cmml">𝐅</mi><mrow id="S3.E5.m1.1.1.1.1.2a" xref="S3.E5.m1.1.1.1.1.2.cmml"></mrow><mo id="S3.E5.m1.1.1.1.1.2.2.2.3" xref="S3.E5.m1.1.1.1.1.2.2.2.3.cmml">′</mo><mtext id="S3.E5.m1.1.1.1.1.2.3" xref="S3.E5.m1.1.1.1.1.2.3a.cmml">3D</mtext><mi id="S3.E5.m1.1.1.1.1.2.2.3" xref="S3.E5.m1.1.1.1.1.2.2.3.cmml">I</mi></mmultiscripts><mo id="S3.E5.m1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.cmml">=</mo><mrow id="S3.E5.m1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.3.cmml"><msubsup id="S3.E5.m1.1.1.1.1.3.2" xref="S3.E5.m1.1.1.1.1.3.2.cmml"><mi id="S3.E5.m1.1.1.1.1.3.2.2.2" xref="S3.E5.m1.1.1.1.1.3.2.2.2.cmml">𝐅</mi><mtext id="S3.E5.m1.1.1.1.1.3.2.3" xref="S3.E5.m1.1.1.1.1.3.2.3a.cmml">3D</mtext><mi id="S3.E5.m1.1.1.1.1.3.2.2.3" xref="S3.E5.m1.1.1.1.1.3.2.2.3.cmml">I</mi></msubsup><mo lspace="0.222em" rspace="0.222em" id="S3.E5.m1.1.1.1.1.3.1" xref="S3.E5.m1.1.1.1.1.3.1.cmml">⊙</mo><msubsup id="S3.E5.m1.1.1.1.1.3.3" xref="S3.E5.m1.1.1.1.1.3.3.cmml"><mi id="S3.E5.m1.1.1.1.1.3.3.2.2" xref="S3.E5.m1.1.1.1.1.3.3.2.2.cmml">𝐃</mi><mtext id="S3.E5.m1.1.1.1.1.3.3.3" xref="S3.E5.m1.1.1.1.1.3.3.3a.cmml">3D</mtext><mi id="S3.E5.m1.1.1.1.1.3.3.2.3" xref="S3.E5.m1.1.1.1.1.3.3.2.3.cmml">I</mi></msubsup></mrow></mrow><mo lspace="0em" id="S3.E5.m1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.1b"><apply id="S3.E5.m1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1"><eq id="S3.E5.m1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1"></eq><apply id="S3.E5.m1.1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.2.1.cmml" xref="S3.E5.m1.1.1.1.1.2">subscript</csymbol><apply id="S3.E5.m1.1.1.1.1.2.2.cmml" xref="S3.E5.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.2.2.1.cmml" xref="S3.E5.m1.1.1.1.1.2">superscript</csymbol><apply id="S3.E5.m1.1.1.1.1.2.2.2.cmml" xref="S3.E5.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.2.2.2.1.cmml" xref="S3.E5.m1.1.1.1.1.2">superscript</csymbol><ci id="S3.E5.m1.1.1.1.1.2.2.2.2.cmml" xref="S3.E5.m1.1.1.1.1.2.2.2.2">𝐅</ci><ci id="S3.E5.m1.1.1.1.1.2.2.2.3.cmml" xref="S3.E5.m1.1.1.1.1.2.2.2.3">′</ci></apply><ci id="S3.E5.m1.1.1.1.1.2.2.3.cmml" xref="S3.E5.m1.1.1.1.1.2.2.3">𝐼</ci></apply><ci id="S3.E5.m1.1.1.1.1.2.3a.cmml" xref="S3.E5.m1.1.1.1.1.2.3"><mtext mathsize="70%" id="S3.E5.m1.1.1.1.1.2.3.cmml" xref="S3.E5.m1.1.1.1.1.2.3">3D</mtext></ci></apply><apply id="S3.E5.m1.1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.1.3"><csymbol cd="latexml" id="S3.E5.m1.1.1.1.1.3.1.cmml" xref="S3.E5.m1.1.1.1.1.3.1">direct-product</csymbol><apply id="S3.E5.m1.1.1.1.1.3.2.cmml" xref="S3.E5.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.3.2.1.cmml" xref="S3.E5.m1.1.1.1.1.3.2">subscript</csymbol><apply id="S3.E5.m1.1.1.1.1.3.2.2.cmml" xref="S3.E5.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.3.2.2.1.cmml" xref="S3.E5.m1.1.1.1.1.3.2">superscript</csymbol><ci id="S3.E5.m1.1.1.1.1.3.2.2.2.cmml" xref="S3.E5.m1.1.1.1.1.3.2.2.2">𝐅</ci><ci id="S3.E5.m1.1.1.1.1.3.2.2.3.cmml" xref="S3.E5.m1.1.1.1.1.3.2.2.3">𝐼</ci></apply><ci id="S3.E5.m1.1.1.1.1.3.2.3a.cmml" xref="S3.E5.m1.1.1.1.1.3.2.3"><mtext mathsize="70%" id="S3.E5.m1.1.1.1.1.3.2.3.cmml" xref="S3.E5.m1.1.1.1.1.3.2.3">3D</mtext></ci></apply><apply id="S3.E5.m1.1.1.1.1.3.3.cmml" xref="S3.E5.m1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.3.3.1.cmml" xref="S3.E5.m1.1.1.1.1.3.3">subscript</csymbol><apply id="S3.E5.m1.1.1.1.1.3.3.2.cmml" xref="S3.E5.m1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.3.3.2.1.cmml" xref="S3.E5.m1.1.1.1.1.3.3">superscript</csymbol><ci id="S3.E5.m1.1.1.1.1.3.3.2.2.cmml" xref="S3.E5.m1.1.1.1.1.3.3.2.2">𝐃</ci><ci id="S3.E5.m1.1.1.1.1.3.3.2.3.cmml" xref="S3.E5.m1.1.1.1.1.3.3.2.3">𝐼</ci></apply><ci id="S3.E5.m1.1.1.1.1.3.3.3a.cmml" xref="S3.E5.m1.1.1.1.1.3.3.3"><mtext mathsize="70%" id="S3.E5.m1.1.1.1.1.3.3.3.cmml" xref="S3.E5.m1.1.1.1.1.3.3.3">3D</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.1c">\mathbf{F^{\prime}}^{I}_{\text{3D}}=\mathbf{F}^{I}_{\text{3D}}\odot\mathbf{D}^{I}_{\text{3D}}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p id="S3.SS4.p6.2" class="ltx_p">Here, <math id="S3.SS4.p6.1.m1.1" class="ltx_Math" alttext="\odot" display="inline"><semantics id="S3.SS4.p6.1.m1.1a"><mo id="S3.SS4.p6.1.m1.1.1" xref="S3.SS4.p6.1.m1.1.1.cmml">⊙</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.p6.1.m1.1b"><csymbol cd="latexml" id="S3.SS4.p6.1.m1.1.1.cmml" xref="S3.SS4.p6.1.m1.1.1">direct-product</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p6.1.m1.1c">\odot</annotation></semantics></math> represents element-wise multiplication with broadcasting, and <math id="S3.SS4.p6.2.m2.1" class="ltx_Math" alttext="\mathbf{F^{\prime}}^{I}_{\text{3D}}\in\mathbb{R}^{N_{\text{lvl}}\times X\times Y\times Z\times C_{I}}" display="inline"><semantics id="S3.SS4.p6.2.m2.1a"><mrow id="S3.SS4.p6.2.m2.1.1" xref="S3.SS4.p6.2.m2.1.1.cmml"><mmultiscripts id="S3.SS4.p6.2.m2.1.1.2" xref="S3.SS4.p6.2.m2.1.1.2.cmml"><mi id="S3.SS4.p6.2.m2.1.1.2.2.2.2" xref="S3.SS4.p6.2.m2.1.1.2.2.2.2.cmml">𝐅</mi><mrow id="S3.SS4.p6.2.m2.1.1.2a" xref="S3.SS4.p6.2.m2.1.1.2.cmml"></mrow><mo id="S3.SS4.p6.2.m2.1.1.2.2.2.3" xref="S3.SS4.p6.2.m2.1.1.2.2.2.3.cmml">′</mo><mtext id="S3.SS4.p6.2.m2.1.1.2.3" xref="S3.SS4.p6.2.m2.1.1.2.3a.cmml">3D</mtext><mi id="S3.SS4.p6.2.m2.1.1.2.2.3" xref="S3.SS4.p6.2.m2.1.1.2.2.3.cmml">I</mi></mmultiscripts><mo id="S3.SS4.p6.2.m2.1.1.1" xref="S3.SS4.p6.2.m2.1.1.1.cmml">∈</mo><msup id="S3.SS4.p6.2.m2.1.1.3" xref="S3.SS4.p6.2.m2.1.1.3.cmml"><mi id="S3.SS4.p6.2.m2.1.1.3.2" xref="S3.SS4.p6.2.m2.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS4.p6.2.m2.1.1.3.3" xref="S3.SS4.p6.2.m2.1.1.3.3.cmml"><msub id="S3.SS4.p6.2.m2.1.1.3.3.2" xref="S3.SS4.p6.2.m2.1.1.3.3.2.cmml"><mi id="S3.SS4.p6.2.m2.1.1.3.3.2.2" xref="S3.SS4.p6.2.m2.1.1.3.3.2.2.cmml">N</mi><mtext id="S3.SS4.p6.2.m2.1.1.3.3.2.3" xref="S3.SS4.p6.2.m2.1.1.3.3.2.3a.cmml">lvl</mtext></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p6.2.m2.1.1.3.3.1" xref="S3.SS4.p6.2.m2.1.1.3.3.1.cmml">×</mo><mi id="S3.SS4.p6.2.m2.1.1.3.3.3" xref="S3.SS4.p6.2.m2.1.1.3.3.3.cmml">X</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p6.2.m2.1.1.3.3.1a" xref="S3.SS4.p6.2.m2.1.1.3.3.1.cmml">×</mo><mi id="S3.SS4.p6.2.m2.1.1.3.3.4" xref="S3.SS4.p6.2.m2.1.1.3.3.4.cmml">Y</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p6.2.m2.1.1.3.3.1b" xref="S3.SS4.p6.2.m2.1.1.3.3.1.cmml">×</mo><mi id="S3.SS4.p6.2.m2.1.1.3.3.5" xref="S3.SS4.p6.2.m2.1.1.3.3.5.cmml">Z</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p6.2.m2.1.1.3.3.1c" xref="S3.SS4.p6.2.m2.1.1.3.3.1.cmml">×</mo><msub id="S3.SS4.p6.2.m2.1.1.3.3.6" xref="S3.SS4.p6.2.m2.1.1.3.3.6.cmml"><mi id="S3.SS4.p6.2.m2.1.1.3.3.6.2" xref="S3.SS4.p6.2.m2.1.1.3.3.6.2.cmml">C</mi><mi id="S3.SS4.p6.2.m2.1.1.3.3.6.3" xref="S3.SS4.p6.2.m2.1.1.3.3.6.3.cmml">I</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p6.2.m2.1b"><apply id="S3.SS4.p6.2.m2.1.1.cmml" xref="S3.SS4.p6.2.m2.1.1"><in id="S3.SS4.p6.2.m2.1.1.1.cmml" xref="S3.SS4.p6.2.m2.1.1.1"></in><apply id="S3.SS4.p6.2.m2.1.1.2.cmml" xref="S3.SS4.p6.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p6.2.m2.1.1.2.1.cmml" xref="S3.SS4.p6.2.m2.1.1.2">subscript</csymbol><apply id="S3.SS4.p6.2.m2.1.1.2.2.cmml" xref="S3.SS4.p6.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p6.2.m2.1.1.2.2.1.cmml" xref="S3.SS4.p6.2.m2.1.1.2">superscript</csymbol><apply id="S3.SS4.p6.2.m2.1.1.2.2.2.cmml" xref="S3.SS4.p6.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p6.2.m2.1.1.2.2.2.1.cmml" xref="S3.SS4.p6.2.m2.1.1.2">superscript</csymbol><ci id="S3.SS4.p6.2.m2.1.1.2.2.2.2.cmml" xref="S3.SS4.p6.2.m2.1.1.2.2.2.2">𝐅</ci><ci id="S3.SS4.p6.2.m2.1.1.2.2.2.3.cmml" xref="S3.SS4.p6.2.m2.1.1.2.2.2.3">′</ci></apply><ci id="S3.SS4.p6.2.m2.1.1.2.2.3.cmml" xref="S3.SS4.p6.2.m2.1.1.2.2.3">𝐼</ci></apply><ci id="S3.SS4.p6.2.m2.1.1.2.3a.cmml" xref="S3.SS4.p6.2.m2.1.1.2.3"><mtext mathsize="70%" id="S3.SS4.p6.2.m2.1.1.2.3.cmml" xref="S3.SS4.p6.2.m2.1.1.2.3">3D</mtext></ci></apply><apply id="S3.SS4.p6.2.m2.1.1.3.cmml" xref="S3.SS4.p6.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.p6.2.m2.1.1.3.1.cmml" xref="S3.SS4.p6.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS4.p6.2.m2.1.1.3.2.cmml" xref="S3.SS4.p6.2.m2.1.1.3.2">ℝ</ci><apply id="S3.SS4.p6.2.m2.1.1.3.3.cmml" xref="S3.SS4.p6.2.m2.1.1.3.3"><times id="S3.SS4.p6.2.m2.1.1.3.3.1.cmml" xref="S3.SS4.p6.2.m2.1.1.3.3.1"></times><apply id="S3.SS4.p6.2.m2.1.1.3.3.2.cmml" xref="S3.SS4.p6.2.m2.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS4.p6.2.m2.1.1.3.3.2.1.cmml" xref="S3.SS4.p6.2.m2.1.1.3.3.2">subscript</csymbol><ci id="S3.SS4.p6.2.m2.1.1.3.3.2.2.cmml" xref="S3.SS4.p6.2.m2.1.1.3.3.2.2">𝑁</ci><ci id="S3.SS4.p6.2.m2.1.1.3.3.2.3a.cmml" xref="S3.SS4.p6.2.m2.1.1.3.3.2.3"><mtext mathsize="50%" id="S3.SS4.p6.2.m2.1.1.3.3.2.3.cmml" xref="S3.SS4.p6.2.m2.1.1.3.3.2.3">lvl</mtext></ci></apply><ci id="S3.SS4.p6.2.m2.1.1.3.3.3.cmml" xref="S3.SS4.p6.2.m2.1.1.3.3.3">𝑋</ci><ci id="S3.SS4.p6.2.m2.1.1.3.3.4.cmml" xref="S3.SS4.p6.2.m2.1.1.3.3.4">𝑌</ci><ci id="S3.SS4.p6.2.m2.1.1.3.3.5.cmml" xref="S3.SS4.p6.2.m2.1.1.3.3.5">𝑍</ci><apply id="S3.SS4.p6.2.m2.1.1.3.3.6.cmml" xref="S3.SS4.p6.2.m2.1.1.3.3.6"><csymbol cd="ambiguous" id="S3.SS4.p6.2.m2.1.1.3.3.6.1.cmml" xref="S3.SS4.p6.2.m2.1.1.3.3.6">subscript</csymbol><ci id="S3.SS4.p6.2.m2.1.1.3.3.6.2.cmml" xref="S3.SS4.p6.2.m2.1.1.3.3.6.2">𝐶</ci><ci id="S3.SS4.p6.2.m2.1.1.3.3.6.3.cmml" xref="S3.SS4.p6.2.m2.1.1.3.3.6.3">𝐼</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p6.2.m2.1c">\mathbf{F^{\prime}}^{I}_{\text{3D}}\in\mathbb{R}^{N_{\text{lvl}}\times X\times Y\times Z\times C_{I}}</annotation></semantics></math> denotes the result of the “depth-assisted image feature lifting” process.</p>
</div>
<div id="S3.SS4.p7" class="ltx_para">
<p id="S3.SS4.p7.1" class="ltx_p"><span id="S3.SS4.p7.1.1" class="ltx_text ltx_font_bold">Radar Occupancy-Assisted Sampling:</span> The model faces challenges in learning accurate depth prediction without direct supervision, as the image depth information is often ambiguous <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite>.
To address this issue, one possible approach is to “generate” depth supervision using radar points. This method involves projecting radar points onto images and assigning their depths as the ground-truth depths of the nearest pixels. However, due to the sparsity of radar point cloud, only a few pixels have ground-truth depth information, and the accuracy of the ground-truth depths are limited because of the noise inherent in radar measurements.</p>
</div>
<div id="S3.SS4.p8" class="ltx_para">
<p id="S3.SS4.p8.1" class="ltx_p">As a result, we choose another approach, leveraging the radar modality differently by adding a branch for lifting the image PV features and fusing with the aforementioned lifted features <math id="S3.SS4.p8.1.m1.1" class="ltx_Math" alttext="\mathbf{F^{\prime}}^{I}_{\text{3D}}" display="inline"><semantics id="S3.SS4.p8.1.m1.1a"><mmultiscripts id="S3.SS4.p8.1.m1.1.1" xref="S3.SS4.p8.1.m1.1.1.cmml"><mi id="S3.SS4.p8.1.m1.1.1.2.2.2" xref="S3.SS4.p8.1.m1.1.1.2.2.2.cmml">𝐅</mi><mrow id="S3.SS4.p8.1.m1.1.1a" xref="S3.SS4.p8.1.m1.1.1.cmml"></mrow><mo id="S3.SS4.p8.1.m1.1.1.2.2.3" xref="S3.SS4.p8.1.m1.1.1.2.2.3.cmml">′</mo><mtext id="S3.SS4.p8.1.m1.1.1.3" xref="S3.SS4.p8.1.m1.1.1.3a.cmml">3D</mtext><mi id="S3.SS4.p8.1.m1.1.1.2.3" xref="S3.SS4.p8.1.m1.1.1.2.3.cmml">I</mi></mmultiscripts><annotation-xml encoding="MathML-Content" id="S3.SS4.p8.1.m1.1b"><apply id="S3.SS4.p8.1.m1.1.1.cmml" xref="S3.SS4.p8.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p8.1.m1.1.1.1.cmml" xref="S3.SS4.p8.1.m1.1.1">subscript</csymbol><apply id="S3.SS4.p8.1.m1.1.1.2.cmml" xref="S3.SS4.p8.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p8.1.m1.1.1.2.1.cmml" xref="S3.SS4.p8.1.m1.1.1">superscript</csymbol><apply id="S3.SS4.p8.1.m1.1.1.2.2.cmml" xref="S3.SS4.p8.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p8.1.m1.1.1.2.2.1.cmml" xref="S3.SS4.p8.1.m1.1.1">superscript</csymbol><ci id="S3.SS4.p8.1.m1.1.1.2.2.2.cmml" xref="S3.SS4.p8.1.m1.1.1.2.2.2">𝐅</ci><ci id="S3.SS4.p8.1.m1.1.1.2.2.3.cmml" xref="S3.SS4.p8.1.m1.1.1.2.2.3">′</ci></apply><ci id="S3.SS4.p8.1.m1.1.1.2.3.cmml" xref="S3.SS4.p8.1.m1.1.1.2.3">𝐼</ci></apply><ci id="S3.SS4.p8.1.m1.1.1.3a.cmml" xref="S3.SS4.p8.1.m1.1.1.3"><mtext mathsize="70%" id="S3.SS4.p8.1.m1.1.1.3.cmml" xref="S3.SS4.p8.1.m1.1.1.3">3D</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p8.1.m1.1c">\mathbf{F^{\prime}}^{I}_{\text{3D}}</annotation></semantics></math>. The latest work of this approach, CRN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite>, projects the 2D radar points onto the image plane and applies convolutional operations after pillarization. The resulting convolution output, referred to as the radar occupancy map, is in the image coordinate system, and aids in the view transformation process.
However, the coordinate transformation and pillarization procedures are time-consuming. In addition, when combing CRN with our “sampling” strategy, the radar occupancy map must be re-sampled to the radar coordinate system, which further increases the complexity. Thus, our proposed method generates radar occupancy grids in the radar coordinate system directly, as explained in Section <a href="#S3.SS2" title="III-B Radar Branch ‣ III Proposed Method ‣ LXL: LiDAR Excluded Lean 3D Object Detection with 4D Imaging Radar and Camera Fusion" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a>.</p>
</div>
<div id="S3.SS4.p9" class="ltx_para">
<p id="S3.SS4.p9.1" class="ltx_p">It is worth noting that in our model, radar 3D occupancy grids are predicted instead of radar 2D occupancy maps, as 4D radar is capable of capturing height information. Moreover, since the required occupancy grids and radar BEV features share the same BEV resolution, they are generated from the radar BEV features for simplicity.</p>
</div>
<div id="S3.SS4.p10" class="ltx_para">
<p id="S3.SS4.p10.3" class="ltx_p">Specifically, in this “radar-occupancy assisted sampling” step, the predicted radar 3D occupancy grids <math id="S3.SS4.p10.1.m1.1" class="ltx_Math" alttext="\mathbf{O}^{P}_{\text{3D}}" display="inline"><semantics id="S3.SS4.p10.1.m1.1a"><msubsup id="S3.SS4.p10.1.m1.1.1" xref="S3.SS4.p10.1.m1.1.1.cmml"><mi id="S3.SS4.p10.1.m1.1.1.2.2" xref="S3.SS4.p10.1.m1.1.1.2.2.cmml">𝐎</mi><mtext id="S3.SS4.p10.1.m1.1.1.3" xref="S3.SS4.p10.1.m1.1.1.3a.cmml">3D</mtext><mi id="S3.SS4.p10.1.m1.1.1.2.3" xref="S3.SS4.p10.1.m1.1.1.2.3.cmml">P</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS4.p10.1.m1.1b"><apply id="S3.SS4.p10.1.m1.1.1.cmml" xref="S3.SS4.p10.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p10.1.m1.1.1.1.cmml" xref="S3.SS4.p10.1.m1.1.1">subscript</csymbol><apply id="S3.SS4.p10.1.m1.1.1.2.cmml" xref="S3.SS4.p10.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p10.1.m1.1.1.2.1.cmml" xref="S3.SS4.p10.1.m1.1.1">superscript</csymbol><ci id="S3.SS4.p10.1.m1.1.1.2.2.cmml" xref="S3.SS4.p10.1.m1.1.1.2.2">𝐎</ci><ci id="S3.SS4.p10.1.m1.1.1.2.3.cmml" xref="S3.SS4.p10.1.m1.1.1.2.3">𝑃</ci></apply><ci id="S3.SS4.p10.1.m1.1.1.3a.cmml" xref="S3.SS4.p10.1.m1.1.1.3"><mtext mathsize="70%" id="S3.SS4.p10.1.m1.1.1.3.cmml" xref="S3.SS4.p10.1.m1.1.1.3">3D</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p10.1.m1.1c">\mathbf{O}^{P}_{\text{3D}}</annotation></semantics></math> are multiplied by <math id="S3.SS4.p10.2.m2.1" class="ltx_Math" alttext="\mathbf{F}^{I}_{\text{3D}}" display="inline"><semantics id="S3.SS4.p10.2.m2.1a"><msubsup id="S3.SS4.p10.2.m2.1.1" xref="S3.SS4.p10.2.m2.1.1.cmml"><mi id="S3.SS4.p10.2.m2.1.1.2.2" xref="S3.SS4.p10.2.m2.1.1.2.2.cmml">𝐅</mi><mtext id="S3.SS4.p10.2.m2.1.1.3" xref="S3.SS4.p10.2.m2.1.1.3a.cmml">3D</mtext><mi id="S3.SS4.p10.2.m2.1.1.2.3" xref="S3.SS4.p10.2.m2.1.1.2.3.cmml">I</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS4.p10.2.m2.1b"><apply id="S3.SS4.p10.2.m2.1.1.cmml" xref="S3.SS4.p10.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p10.2.m2.1.1.1.cmml" xref="S3.SS4.p10.2.m2.1.1">subscript</csymbol><apply id="S3.SS4.p10.2.m2.1.1.2.cmml" xref="S3.SS4.p10.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p10.2.m2.1.1.2.1.cmml" xref="S3.SS4.p10.2.m2.1.1">superscript</csymbol><ci id="S3.SS4.p10.2.m2.1.1.2.2.cmml" xref="S3.SS4.p10.2.m2.1.1.2.2">𝐅</ci><ci id="S3.SS4.p10.2.m2.1.1.2.3.cmml" xref="S3.SS4.p10.2.m2.1.1.2.3">𝐼</ci></apply><ci id="S3.SS4.p10.2.m2.1.1.3a.cmml" xref="S3.SS4.p10.2.m2.1.1.3"><mtext mathsize="70%" id="S3.SS4.p10.2.m2.1.1.3.cmml" xref="S3.SS4.p10.2.m2.1.1.3">3D</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p10.2.m2.1c">\mathbf{F}^{I}_{\text{3D}}</annotation></semantics></math> to obtain the radar-assisted image 3D features <math id="S3.SS4.p10.3.m3.1" class="ltx_Math" alttext="\mathbf{F^{\prime\prime}}^{I}_{\text{3D}}\in\mathbb{R}^{N_{\text{lvl}}\times X\times Y\times Z\times C_{I}}" display="inline"><semantics id="S3.SS4.p10.3.m3.1a"><mrow id="S3.SS4.p10.3.m3.1.1" xref="S3.SS4.p10.3.m3.1.1.cmml"><mmultiscripts id="S3.SS4.p10.3.m3.1.1.2" xref="S3.SS4.p10.3.m3.1.1.2.cmml"><mi id="S3.SS4.p10.3.m3.1.1.2.2.2.2" xref="S3.SS4.p10.3.m3.1.1.2.2.2.2.cmml">𝐅</mi><mrow id="S3.SS4.p10.3.m3.1.1.2a" xref="S3.SS4.p10.3.m3.1.1.2.cmml"></mrow><mo id="S3.SS4.p10.3.m3.1.1.2.2.2.3" xref="S3.SS4.p10.3.m3.1.1.2.2.2.3.cmml">′′</mo><mtext id="S3.SS4.p10.3.m3.1.1.2.3" xref="S3.SS4.p10.3.m3.1.1.2.3a.cmml">3D</mtext><mi id="S3.SS4.p10.3.m3.1.1.2.2.3" xref="S3.SS4.p10.3.m3.1.1.2.2.3.cmml">I</mi></mmultiscripts><mo id="S3.SS4.p10.3.m3.1.1.1" xref="S3.SS4.p10.3.m3.1.1.1.cmml">∈</mo><msup id="S3.SS4.p10.3.m3.1.1.3" xref="S3.SS4.p10.3.m3.1.1.3.cmml"><mi id="S3.SS4.p10.3.m3.1.1.3.2" xref="S3.SS4.p10.3.m3.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS4.p10.3.m3.1.1.3.3" xref="S3.SS4.p10.3.m3.1.1.3.3.cmml"><msub id="S3.SS4.p10.3.m3.1.1.3.3.2" xref="S3.SS4.p10.3.m3.1.1.3.3.2.cmml"><mi id="S3.SS4.p10.3.m3.1.1.3.3.2.2" xref="S3.SS4.p10.3.m3.1.1.3.3.2.2.cmml">N</mi><mtext id="S3.SS4.p10.3.m3.1.1.3.3.2.3" xref="S3.SS4.p10.3.m3.1.1.3.3.2.3a.cmml">lvl</mtext></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p10.3.m3.1.1.3.3.1" xref="S3.SS4.p10.3.m3.1.1.3.3.1.cmml">×</mo><mi id="S3.SS4.p10.3.m3.1.1.3.3.3" xref="S3.SS4.p10.3.m3.1.1.3.3.3.cmml">X</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p10.3.m3.1.1.3.3.1a" xref="S3.SS4.p10.3.m3.1.1.3.3.1.cmml">×</mo><mi id="S3.SS4.p10.3.m3.1.1.3.3.4" xref="S3.SS4.p10.3.m3.1.1.3.3.4.cmml">Y</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p10.3.m3.1.1.3.3.1b" xref="S3.SS4.p10.3.m3.1.1.3.3.1.cmml">×</mo><mi id="S3.SS4.p10.3.m3.1.1.3.3.5" xref="S3.SS4.p10.3.m3.1.1.3.3.5.cmml">Z</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p10.3.m3.1.1.3.3.1c" xref="S3.SS4.p10.3.m3.1.1.3.3.1.cmml">×</mo><msub id="S3.SS4.p10.3.m3.1.1.3.3.6" xref="S3.SS4.p10.3.m3.1.1.3.3.6.cmml"><mi id="S3.SS4.p10.3.m3.1.1.3.3.6.2" xref="S3.SS4.p10.3.m3.1.1.3.3.6.2.cmml">C</mi><mi id="S3.SS4.p10.3.m3.1.1.3.3.6.3" xref="S3.SS4.p10.3.m3.1.1.3.3.6.3.cmml">I</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p10.3.m3.1b"><apply id="S3.SS4.p10.3.m3.1.1.cmml" xref="S3.SS4.p10.3.m3.1.1"><in id="S3.SS4.p10.3.m3.1.1.1.cmml" xref="S3.SS4.p10.3.m3.1.1.1"></in><apply id="S3.SS4.p10.3.m3.1.1.2.cmml" xref="S3.SS4.p10.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p10.3.m3.1.1.2.1.cmml" xref="S3.SS4.p10.3.m3.1.1.2">subscript</csymbol><apply id="S3.SS4.p10.3.m3.1.1.2.2.cmml" xref="S3.SS4.p10.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p10.3.m3.1.1.2.2.1.cmml" xref="S3.SS4.p10.3.m3.1.1.2">superscript</csymbol><apply id="S3.SS4.p10.3.m3.1.1.2.2.2.cmml" xref="S3.SS4.p10.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p10.3.m3.1.1.2.2.2.1.cmml" xref="S3.SS4.p10.3.m3.1.1.2">superscript</csymbol><ci id="S3.SS4.p10.3.m3.1.1.2.2.2.2.cmml" xref="S3.SS4.p10.3.m3.1.1.2.2.2.2">𝐅</ci><ci id="S3.SS4.p10.3.m3.1.1.2.2.2.3.cmml" xref="S3.SS4.p10.3.m3.1.1.2.2.2.3">′′</ci></apply><ci id="S3.SS4.p10.3.m3.1.1.2.2.3.cmml" xref="S3.SS4.p10.3.m3.1.1.2.2.3">𝐼</ci></apply><ci id="S3.SS4.p10.3.m3.1.1.2.3a.cmml" xref="S3.SS4.p10.3.m3.1.1.2.3"><mtext mathsize="70%" id="S3.SS4.p10.3.m3.1.1.2.3.cmml" xref="S3.SS4.p10.3.m3.1.1.2.3">3D</mtext></ci></apply><apply id="S3.SS4.p10.3.m3.1.1.3.cmml" xref="S3.SS4.p10.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.p10.3.m3.1.1.3.1.cmml" xref="S3.SS4.p10.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS4.p10.3.m3.1.1.3.2.cmml" xref="S3.SS4.p10.3.m3.1.1.3.2">ℝ</ci><apply id="S3.SS4.p10.3.m3.1.1.3.3.cmml" xref="S3.SS4.p10.3.m3.1.1.3.3"><times id="S3.SS4.p10.3.m3.1.1.3.3.1.cmml" xref="S3.SS4.p10.3.m3.1.1.3.3.1"></times><apply id="S3.SS4.p10.3.m3.1.1.3.3.2.cmml" xref="S3.SS4.p10.3.m3.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS4.p10.3.m3.1.1.3.3.2.1.cmml" xref="S3.SS4.p10.3.m3.1.1.3.3.2">subscript</csymbol><ci id="S3.SS4.p10.3.m3.1.1.3.3.2.2.cmml" xref="S3.SS4.p10.3.m3.1.1.3.3.2.2">𝑁</ci><ci id="S3.SS4.p10.3.m3.1.1.3.3.2.3a.cmml" xref="S3.SS4.p10.3.m3.1.1.3.3.2.3"><mtext mathsize="50%" id="S3.SS4.p10.3.m3.1.1.3.3.2.3.cmml" xref="S3.SS4.p10.3.m3.1.1.3.3.2.3">lvl</mtext></ci></apply><ci id="S3.SS4.p10.3.m3.1.1.3.3.3.cmml" xref="S3.SS4.p10.3.m3.1.1.3.3.3">𝑋</ci><ci id="S3.SS4.p10.3.m3.1.1.3.3.4.cmml" xref="S3.SS4.p10.3.m3.1.1.3.3.4">𝑌</ci><ci id="S3.SS4.p10.3.m3.1.1.3.3.5.cmml" xref="S3.SS4.p10.3.m3.1.1.3.3.5">𝑍</ci><apply id="S3.SS4.p10.3.m3.1.1.3.3.6.cmml" xref="S3.SS4.p10.3.m3.1.1.3.3.6"><csymbol cd="ambiguous" id="S3.SS4.p10.3.m3.1.1.3.3.6.1.cmml" xref="S3.SS4.p10.3.m3.1.1.3.3.6">subscript</csymbol><ci id="S3.SS4.p10.3.m3.1.1.3.3.6.2.cmml" xref="S3.SS4.p10.3.m3.1.1.3.3.6.2">𝐶</ci><ci id="S3.SS4.p10.3.m3.1.1.3.3.6.3.cmml" xref="S3.SS4.p10.3.m3.1.1.3.3.6.3">𝐼</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p10.3.m3.1c">\mathbf{F^{\prime\prime}}^{I}_{\text{3D}}\in\mathbb{R}^{N_{\text{lvl}}\times X\times Y\times Z\times C_{I}}</annotation></semantics></math> using the following equation:</p>
<table id="S3.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E6.m1.1" class="ltx_Math" alttext="\mathbf{F^{\prime\prime}}^{I}_{\text{3D}}=\mathbf{F}^{I}_{\text{3D}}\odot\mathbf{O}^{P}_{\text{3D}}." display="block"><semantics id="S3.E6.m1.1a"><mrow id="S3.E6.m1.1.1.1" xref="S3.E6.m1.1.1.1.1.cmml"><mrow id="S3.E6.m1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.cmml"><mmultiscripts id="S3.E6.m1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.2.cmml"><mi id="S3.E6.m1.1.1.1.1.2.2.2.2" xref="S3.E6.m1.1.1.1.1.2.2.2.2.cmml">𝐅</mi><mrow id="S3.E6.m1.1.1.1.1.2a" xref="S3.E6.m1.1.1.1.1.2.cmml"></mrow><mo id="S3.E6.m1.1.1.1.1.2.2.2.3" xref="S3.E6.m1.1.1.1.1.2.2.2.3.cmml">′′</mo><mtext id="S3.E6.m1.1.1.1.1.2.3" xref="S3.E6.m1.1.1.1.1.2.3a.cmml">3D</mtext><mi id="S3.E6.m1.1.1.1.1.2.2.3" xref="S3.E6.m1.1.1.1.1.2.2.3.cmml">I</mi></mmultiscripts><mo id="S3.E6.m1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.cmml">=</mo><mrow id="S3.E6.m1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.3.cmml"><msubsup id="S3.E6.m1.1.1.1.1.3.2" xref="S3.E6.m1.1.1.1.1.3.2.cmml"><mi id="S3.E6.m1.1.1.1.1.3.2.2.2" xref="S3.E6.m1.1.1.1.1.3.2.2.2.cmml">𝐅</mi><mtext id="S3.E6.m1.1.1.1.1.3.2.3" xref="S3.E6.m1.1.1.1.1.3.2.3a.cmml">3D</mtext><mi id="S3.E6.m1.1.1.1.1.3.2.2.3" xref="S3.E6.m1.1.1.1.1.3.2.2.3.cmml">I</mi></msubsup><mo lspace="0.222em" rspace="0.222em" id="S3.E6.m1.1.1.1.1.3.1" xref="S3.E6.m1.1.1.1.1.3.1.cmml">⊙</mo><msubsup id="S3.E6.m1.1.1.1.1.3.3" xref="S3.E6.m1.1.1.1.1.3.3.cmml"><mi id="S3.E6.m1.1.1.1.1.3.3.2.2" xref="S3.E6.m1.1.1.1.1.3.3.2.2.cmml">𝐎</mi><mtext id="S3.E6.m1.1.1.1.1.3.3.3" xref="S3.E6.m1.1.1.1.1.3.3.3a.cmml">3D</mtext><mi id="S3.E6.m1.1.1.1.1.3.3.2.3" xref="S3.E6.m1.1.1.1.1.3.3.2.3.cmml">P</mi></msubsup></mrow></mrow><mo lspace="0em" id="S3.E6.m1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m1.1b"><apply id="S3.E6.m1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1"><eq id="S3.E6.m1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1"></eq><apply id="S3.E6.m1.1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.2.1.cmml" xref="S3.E6.m1.1.1.1.1.2">subscript</csymbol><apply id="S3.E6.m1.1.1.1.1.2.2.cmml" xref="S3.E6.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.2.2.1.cmml" xref="S3.E6.m1.1.1.1.1.2">superscript</csymbol><apply id="S3.E6.m1.1.1.1.1.2.2.2.cmml" xref="S3.E6.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.2.2.2.1.cmml" xref="S3.E6.m1.1.1.1.1.2">superscript</csymbol><ci id="S3.E6.m1.1.1.1.1.2.2.2.2.cmml" xref="S3.E6.m1.1.1.1.1.2.2.2.2">𝐅</ci><ci id="S3.E6.m1.1.1.1.1.2.2.2.3.cmml" xref="S3.E6.m1.1.1.1.1.2.2.2.3">′′</ci></apply><ci id="S3.E6.m1.1.1.1.1.2.2.3.cmml" xref="S3.E6.m1.1.1.1.1.2.2.3">𝐼</ci></apply><ci id="S3.E6.m1.1.1.1.1.2.3a.cmml" xref="S3.E6.m1.1.1.1.1.2.3"><mtext mathsize="70%" id="S3.E6.m1.1.1.1.1.2.3.cmml" xref="S3.E6.m1.1.1.1.1.2.3">3D</mtext></ci></apply><apply id="S3.E6.m1.1.1.1.1.3.cmml" xref="S3.E6.m1.1.1.1.1.3"><csymbol cd="latexml" id="S3.E6.m1.1.1.1.1.3.1.cmml" xref="S3.E6.m1.1.1.1.1.3.1">direct-product</csymbol><apply id="S3.E6.m1.1.1.1.1.3.2.cmml" xref="S3.E6.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.3.2.1.cmml" xref="S3.E6.m1.1.1.1.1.3.2">subscript</csymbol><apply id="S3.E6.m1.1.1.1.1.3.2.2.cmml" xref="S3.E6.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.3.2.2.1.cmml" xref="S3.E6.m1.1.1.1.1.3.2">superscript</csymbol><ci id="S3.E6.m1.1.1.1.1.3.2.2.2.cmml" xref="S3.E6.m1.1.1.1.1.3.2.2.2">𝐅</ci><ci id="S3.E6.m1.1.1.1.1.3.2.2.3.cmml" xref="S3.E6.m1.1.1.1.1.3.2.2.3">𝐼</ci></apply><ci id="S3.E6.m1.1.1.1.1.3.2.3a.cmml" xref="S3.E6.m1.1.1.1.1.3.2.3"><mtext mathsize="70%" id="S3.E6.m1.1.1.1.1.3.2.3.cmml" xref="S3.E6.m1.1.1.1.1.3.2.3">3D</mtext></ci></apply><apply id="S3.E6.m1.1.1.1.1.3.3.cmml" xref="S3.E6.m1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.3.3.1.cmml" xref="S3.E6.m1.1.1.1.1.3.3">subscript</csymbol><apply id="S3.E6.m1.1.1.1.1.3.3.2.cmml" xref="S3.E6.m1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.3.3.2.1.cmml" xref="S3.E6.m1.1.1.1.1.3.3">superscript</csymbol><ci id="S3.E6.m1.1.1.1.1.3.3.2.2.cmml" xref="S3.E6.m1.1.1.1.1.3.3.2.2">𝐎</ci><ci id="S3.E6.m1.1.1.1.1.3.3.2.3.cmml" xref="S3.E6.m1.1.1.1.1.3.3.2.3">𝑃</ci></apply><ci id="S3.E6.m1.1.1.1.1.3.3.3a.cmml" xref="S3.E6.m1.1.1.1.1.3.3.3"><mtext mathsize="70%" id="S3.E6.m1.1.1.1.1.3.3.3.cmml" xref="S3.E6.m1.1.1.1.1.3.3.3">3D</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.1c">\mathbf{F^{\prime\prime}}^{I}_{\text{3D}}=\mathbf{F}^{I}_{\text{3D}}\odot\mathbf{O}^{P}_{\text{3D}}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS4.p11" class="ltx_para">
<p id="S3.SS4.p11.4" class="ltx_p"><span id="S3.SS4.p11.4.1" class="ltx_text ltx_font_bold">Height Compression:</span> The resulting radar-assisted image 3D features, denoted as <math id="S3.SS4.p11.1.m1.1" class="ltx_Math" alttext="\mathbf{F^{\prime\prime}}^{I}_{\text{3D}}" display="inline"><semantics id="S3.SS4.p11.1.m1.1a"><mmultiscripts id="S3.SS4.p11.1.m1.1.1" xref="S3.SS4.p11.1.m1.1.1.cmml"><mi id="S3.SS4.p11.1.m1.1.1.2.2.2" xref="S3.SS4.p11.1.m1.1.1.2.2.2.cmml">𝐅</mi><mrow id="S3.SS4.p11.1.m1.1.1a" xref="S3.SS4.p11.1.m1.1.1.cmml"></mrow><mo id="S3.SS4.p11.1.m1.1.1.2.2.3" xref="S3.SS4.p11.1.m1.1.1.2.2.3.cmml">′′</mo><mtext id="S3.SS4.p11.1.m1.1.1.3" xref="S3.SS4.p11.1.m1.1.1.3a.cmml">3D</mtext><mi id="S3.SS4.p11.1.m1.1.1.2.3" xref="S3.SS4.p11.1.m1.1.1.2.3.cmml">I</mi></mmultiscripts><annotation-xml encoding="MathML-Content" id="S3.SS4.p11.1.m1.1b"><apply id="S3.SS4.p11.1.m1.1.1.cmml" xref="S3.SS4.p11.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p11.1.m1.1.1.1.cmml" xref="S3.SS4.p11.1.m1.1.1">subscript</csymbol><apply id="S3.SS4.p11.1.m1.1.1.2.cmml" xref="S3.SS4.p11.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p11.1.m1.1.1.2.1.cmml" xref="S3.SS4.p11.1.m1.1.1">superscript</csymbol><apply id="S3.SS4.p11.1.m1.1.1.2.2.cmml" xref="S3.SS4.p11.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p11.1.m1.1.1.2.2.1.cmml" xref="S3.SS4.p11.1.m1.1.1">superscript</csymbol><ci id="S3.SS4.p11.1.m1.1.1.2.2.2.cmml" xref="S3.SS4.p11.1.m1.1.1.2.2.2">𝐅</ci><ci id="S3.SS4.p11.1.m1.1.1.2.2.3.cmml" xref="S3.SS4.p11.1.m1.1.1.2.2.3">′′</ci></apply><ci id="S3.SS4.p11.1.m1.1.1.2.3.cmml" xref="S3.SS4.p11.1.m1.1.1.2.3">𝐼</ci></apply><ci id="S3.SS4.p11.1.m1.1.1.3a.cmml" xref="S3.SS4.p11.1.m1.1.1.3"><mtext mathsize="70%" id="S3.SS4.p11.1.m1.1.1.3.cmml" xref="S3.SS4.p11.1.m1.1.1.3">3D</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p11.1.m1.1c">\mathbf{F^{\prime\prime}}^{I}_{\text{3D}}</annotation></semantics></math>, and the depth-assisted image 3D features, denoted as <math id="S3.SS4.p11.2.m2.1" class="ltx_Math" alttext="\mathbf{F^{\prime}}^{I}_{\text{3D}}" display="inline"><semantics id="S3.SS4.p11.2.m2.1a"><mmultiscripts id="S3.SS4.p11.2.m2.1.1" xref="S3.SS4.p11.2.m2.1.1.cmml"><mi id="S3.SS4.p11.2.m2.1.1.2.2.2" xref="S3.SS4.p11.2.m2.1.1.2.2.2.cmml">𝐅</mi><mrow id="S3.SS4.p11.2.m2.1.1a" xref="S3.SS4.p11.2.m2.1.1.cmml"></mrow><mo id="S3.SS4.p11.2.m2.1.1.2.2.3" xref="S3.SS4.p11.2.m2.1.1.2.2.3.cmml">′</mo><mtext id="S3.SS4.p11.2.m2.1.1.3" xref="S3.SS4.p11.2.m2.1.1.3a.cmml">3D</mtext><mi id="S3.SS4.p11.2.m2.1.1.2.3" xref="S3.SS4.p11.2.m2.1.1.2.3.cmml">I</mi></mmultiscripts><annotation-xml encoding="MathML-Content" id="S3.SS4.p11.2.m2.1b"><apply id="S3.SS4.p11.2.m2.1.1.cmml" xref="S3.SS4.p11.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p11.2.m2.1.1.1.cmml" xref="S3.SS4.p11.2.m2.1.1">subscript</csymbol><apply id="S3.SS4.p11.2.m2.1.1.2.cmml" xref="S3.SS4.p11.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p11.2.m2.1.1.2.1.cmml" xref="S3.SS4.p11.2.m2.1.1">superscript</csymbol><apply id="S3.SS4.p11.2.m2.1.1.2.2.cmml" xref="S3.SS4.p11.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p11.2.m2.1.1.2.2.1.cmml" xref="S3.SS4.p11.2.m2.1.1">superscript</csymbol><ci id="S3.SS4.p11.2.m2.1.1.2.2.2.cmml" xref="S3.SS4.p11.2.m2.1.1.2.2.2">𝐅</ci><ci id="S3.SS4.p11.2.m2.1.1.2.2.3.cmml" xref="S3.SS4.p11.2.m2.1.1.2.2.3">′</ci></apply><ci id="S3.SS4.p11.2.m2.1.1.2.3.cmml" xref="S3.SS4.p11.2.m2.1.1.2.3">𝐼</ci></apply><ci id="S3.SS4.p11.2.m2.1.1.3a.cmml" xref="S3.SS4.p11.2.m2.1.1.3"><mtext mathsize="70%" id="S3.SS4.p11.2.m2.1.1.3.cmml" xref="S3.SS4.p11.2.m2.1.1.3">3D</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p11.2.m2.1c">\mathbf{F^{\prime}}^{I}_{\text{3D}}</annotation></semantics></math> are concatenated along the channel dimension and summed along the level dimension. Subsequently, the tensor is reshaped from <math id="S3.SS4.p11.3.m3.1" class="ltx_Math" alttext="X\times Y\times Z\times 2C_{I}" display="inline"><semantics id="S3.SS4.p11.3.m3.1a"><mrow id="S3.SS4.p11.3.m3.1.1" xref="S3.SS4.p11.3.m3.1.1.cmml"><mrow id="S3.SS4.p11.3.m3.1.1.2" xref="S3.SS4.p11.3.m3.1.1.2.cmml"><mi id="S3.SS4.p11.3.m3.1.1.2.2" xref="S3.SS4.p11.3.m3.1.1.2.2.cmml">X</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p11.3.m3.1.1.2.1" xref="S3.SS4.p11.3.m3.1.1.2.1.cmml">×</mo><mi id="S3.SS4.p11.3.m3.1.1.2.3" xref="S3.SS4.p11.3.m3.1.1.2.3.cmml">Y</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p11.3.m3.1.1.2.1a" xref="S3.SS4.p11.3.m3.1.1.2.1.cmml">×</mo><mi id="S3.SS4.p11.3.m3.1.1.2.4" xref="S3.SS4.p11.3.m3.1.1.2.4.cmml">Z</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p11.3.m3.1.1.2.1b" xref="S3.SS4.p11.3.m3.1.1.2.1.cmml">×</mo><mn id="S3.SS4.p11.3.m3.1.1.2.5" xref="S3.SS4.p11.3.m3.1.1.2.5.cmml">2</mn></mrow><mo lspace="0em" rspace="0em" id="S3.SS4.p11.3.m3.1.1.1" xref="S3.SS4.p11.3.m3.1.1.1.cmml">​</mo><msub id="S3.SS4.p11.3.m3.1.1.3" xref="S3.SS4.p11.3.m3.1.1.3.cmml"><mi id="S3.SS4.p11.3.m3.1.1.3.2" xref="S3.SS4.p11.3.m3.1.1.3.2.cmml">C</mi><mi id="S3.SS4.p11.3.m3.1.1.3.3" xref="S3.SS4.p11.3.m3.1.1.3.3.cmml">I</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p11.3.m3.1b"><apply id="S3.SS4.p11.3.m3.1.1.cmml" xref="S3.SS4.p11.3.m3.1.1"><times id="S3.SS4.p11.3.m3.1.1.1.cmml" xref="S3.SS4.p11.3.m3.1.1.1"></times><apply id="S3.SS4.p11.3.m3.1.1.2.cmml" xref="S3.SS4.p11.3.m3.1.1.2"><times id="S3.SS4.p11.3.m3.1.1.2.1.cmml" xref="S3.SS4.p11.3.m3.1.1.2.1"></times><ci id="S3.SS4.p11.3.m3.1.1.2.2.cmml" xref="S3.SS4.p11.3.m3.1.1.2.2">𝑋</ci><ci id="S3.SS4.p11.3.m3.1.1.2.3.cmml" xref="S3.SS4.p11.3.m3.1.1.2.3">𝑌</ci><ci id="S3.SS4.p11.3.m3.1.1.2.4.cmml" xref="S3.SS4.p11.3.m3.1.1.2.4">𝑍</ci><cn type="integer" id="S3.SS4.p11.3.m3.1.1.2.5.cmml" xref="S3.SS4.p11.3.m3.1.1.2.5">2</cn></apply><apply id="S3.SS4.p11.3.m3.1.1.3.cmml" xref="S3.SS4.p11.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.p11.3.m3.1.1.3.1.cmml" xref="S3.SS4.p11.3.m3.1.1.3">subscript</csymbol><ci id="S3.SS4.p11.3.m3.1.1.3.2.cmml" xref="S3.SS4.p11.3.m3.1.1.3.2">𝐶</ci><ci id="S3.SS4.p11.3.m3.1.1.3.3.cmml" xref="S3.SS4.p11.3.m3.1.1.3.3">𝐼</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p11.3.m3.1c">X\times Y\times Z\times 2C_{I}</annotation></semantics></math> to <math id="S3.SS4.p11.4.m4.1" class="ltx_Math" alttext="X\times Y\times(Z\cdot 2C_{I})" display="inline"><semantics id="S3.SS4.p11.4.m4.1a"><mrow id="S3.SS4.p11.4.m4.1.1" xref="S3.SS4.p11.4.m4.1.1.cmml"><mi id="S3.SS4.p11.4.m4.1.1.3" xref="S3.SS4.p11.4.m4.1.1.3.cmml">X</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p11.4.m4.1.1.2" xref="S3.SS4.p11.4.m4.1.1.2.cmml">×</mo><mi id="S3.SS4.p11.4.m4.1.1.4" xref="S3.SS4.p11.4.m4.1.1.4.cmml">Y</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p11.4.m4.1.1.2a" xref="S3.SS4.p11.4.m4.1.1.2.cmml">×</mo><mrow id="S3.SS4.p11.4.m4.1.1.1.1" xref="S3.SS4.p11.4.m4.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS4.p11.4.m4.1.1.1.1.2" xref="S3.SS4.p11.4.m4.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS4.p11.4.m4.1.1.1.1.1" xref="S3.SS4.p11.4.m4.1.1.1.1.1.cmml"><mrow id="S3.SS4.p11.4.m4.1.1.1.1.1.2" xref="S3.SS4.p11.4.m4.1.1.1.1.1.2.cmml"><mi id="S3.SS4.p11.4.m4.1.1.1.1.1.2.2" xref="S3.SS4.p11.4.m4.1.1.1.1.1.2.2.cmml">Z</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p11.4.m4.1.1.1.1.1.2.1" xref="S3.SS4.p11.4.m4.1.1.1.1.1.2.1.cmml">⋅</mo><mn id="S3.SS4.p11.4.m4.1.1.1.1.1.2.3" xref="S3.SS4.p11.4.m4.1.1.1.1.1.2.3.cmml">2</mn></mrow><mo lspace="0em" rspace="0em" id="S3.SS4.p11.4.m4.1.1.1.1.1.1" xref="S3.SS4.p11.4.m4.1.1.1.1.1.1.cmml">​</mo><msub id="S3.SS4.p11.4.m4.1.1.1.1.1.3" xref="S3.SS4.p11.4.m4.1.1.1.1.1.3.cmml"><mi id="S3.SS4.p11.4.m4.1.1.1.1.1.3.2" xref="S3.SS4.p11.4.m4.1.1.1.1.1.3.2.cmml">C</mi><mi id="S3.SS4.p11.4.m4.1.1.1.1.1.3.3" xref="S3.SS4.p11.4.m4.1.1.1.1.1.3.3.cmml">I</mi></msub></mrow><mo stretchy="false" id="S3.SS4.p11.4.m4.1.1.1.1.3" xref="S3.SS4.p11.4.m4.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p11.4.m4.1b"><apply id="S3.SS4.p11.4.m4.1.1.cmml" xref="S3.SS4.p11.4.m4.1.1"><times id="S3.SS4.p11.4.m4.1.1.2.cmml" xref="S3.SS4.p11.4.m4.1.1.2"></times><ci id="S3.SS4.p11.4.m4.1.1.3.cmml" xref="S3.SS4.p11.4.m4.1.1.3">𝑋</ci><ci id="S3.SS4.p11.4.m4.1.1.4.cmml" xref="S3.SS4.p11.4.m4.1.1.4">𝑌</ci><apply id="S3.SS4.p11.4.m4.1.1.1.1.1.cmml" xref="S3.SS4.p11.4.m4.1.1.1.1"><times id="S3.SS4.p11.4.m4.1.1.1.1.1.1.cmml" xref="S3.SS4.p11.4.m4.1.1.1.1.1.1"></times><apply id="S3.SS4.p11.4.m4.1.1.1.1.1.2.cmml" xref="S3.SS4.p11.4.m4.1.1.1.1.1.2"><ci id="S3.SS4.p11.4.m4.1.1.1.1.1.2.1.cmml" xref="S3.SS4.p11.4.m4.1.1.1.1.1.2.1">⋅</ci><ci id="S3.SS4.p11.4.m4.1.1.1.1.1.2.2.cmml" xref="S3.SS4.p11.4.m4.1.1.1.1.1.2.2">𝑍</ci><cn type="integer" id="S3.SS4.p11.4.m4.1.1.1.1.1.2.3.cmml" xref="S3.SS4.p11.4.m4.1.1.1.1.1.2.3">2</cn></apply><apply id="S3.SS4.p11.4.m4.1.1.1.1.1.3.cmml" xref="S3.SS4.p11.4.m4.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.p11.4.m4.1.1.1.1.1.3.1.cmml" xref="S3.SS4.p11.4.m4.1.1.1.1.1.3">subscript</csymbol><ci id="S3.SS4.p11.4.m4.1.1.1.1.1.3.2.cmml" xref="S3.SS4.p11.4.m4.1.1.1.1.1.3.2">𝐶</ci><ci id="S3.SS4.p11.4.m4.1.1.1.1.1.3.3.cmml" xref="S3.SS4.p11.4.m4.1.1.1.1.1.3.3">𝐼</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p11.4.m4.1c">X\times Y\times(Z\cdot 2C_{I})</annotation></semantics></math>, enabling the application of convolutional layers to facilitate spatial interaction. The process can be mathematically expressed as</p>
<table id="S3.E7" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E7.m1.1" class="ltx_Math" alttext="\mathbf{F}^{I}_{\text{BEV}}=\mathtt{Convs}(\mathtt{Reshape}(\mathtt{Concat}(\mathbf{F^{\prime}}^{I}_{\text{3D}},\mathbf{F^{\prime\prime}}^{I}_{\text{3D}})))," display="block"><semantics id="S3.E7.m1.1a"><mrow id="S3.E7.m1.1.1.1" xref="S3.E7.m1.1.1.1.1.cmml"><mrow id="S3.E7.m1.1.1.1.1" xref="S3.E7.m1.1.1.1.1.cmml"><msubsup id="S3.E7.m1.1.1.1.1.3" xref="S3.E7.m1.1.1.1.1.3.cmml"><mi id="S3.E7.m1.1.1.1.1.3.2.2" xref="S3.E7.m1.1.1.1.1.3.2.2.cmml">𝐅</mi><mtext id="S3.E7.m1.1.1.1.1.3.3" xref="S3.E7.m1.1.1.1.1.3.3a.cmml">BEV</mtext><mi id="S3.E7.m1.1.1.1.1.3.2.3" xref="S3.E7.m1.1.1.1.1.3.2.3.cmml">I</mi></msubsup><mo id="S3.E7.m1.1.1.1.1.2" xref="S3.E7.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E7.m1.1.1.1.1.1" xref="S3.E7.m1.1.1.1.1.1.cmml"><mi id="S3.E7.m1.1.1.1.1.1.3" xref="S3.E7.m1.1.1.1.1.1.3.cmml">𝙲𝚘𝚗𝚟𝚜</mi><mo lspace="0em" rspace="0em" id="S3.E7.m1.1.1.1.1.1.2" xref="S3.E7.m1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E7.m1.1.1.1.1.1.1.1" xref="S3.E7.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E7.m1.1.1.1.1.1.1.1.2" xref="S3.E7.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E7.m1.1.1.1.1.1.1.1.1" xref="S3.E7.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E7.m1.1.1.1.1.1.1.1.1.3" xref="S3.E7.m1.1.1.1.1.1.1.1.1.3.cmml">𝚁𝚎𝚜𝚑𝚊𝚙𝚎</mi><mo lspace="0em" rspace="0em" id="S3.E7.m1.1.1.1.1.1.1.1.1.2" xref="S3.E7.m1.1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.4" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.4.cmml">𝙲𝚘𝚗𝚌𝚊𝚝</mi><mo lspace="0em" rspace="0em" id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">​</mo><mrow id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml"><mo stretchy="false" id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.3" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">(</mo><mmultiscripts id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml">𝐅</mi><mrow id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1a" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"></mrow><mo id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml">′</mo><mtext id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3a.cmml">3D</mtext><mi id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">I</mi></mmultiscripts><mo id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.4" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">,</mo><mmultiscripts id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.2" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.2.cmml">𝐅</mi><mrow id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2a" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml"></mrow><mo id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.3" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.cmml">′′</mo><mtext id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3a.cmml">3D</mtext><mi id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.3" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.3.cmml">I</mi></mmultiscripts><mo stretchy="false" id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.5" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E7.m1.1.1.1.1.1.1.1.3" xref="S3.E7.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E7.m1.1.1.1.2" xref="S3.E7.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E7.m1.1b"><apply id="S3.E7.m1.1.1.1.1.cmml" xref="S3.E7.m1.1.1.1"><eq id="S3.E7.m1.1.1.1.1.2.cmml" xref="S3.E7.m1.1.1.1.1.2"></eq><apply id="S3.E7.m1.1.1.1.1.3.cmml" xref="S3.E7.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.1.1.3.1.cmml" xref="S3.E7.m1.1.1.1.1.3">subscript</csymbol><apply id="S3.E7.m1.1.1.1.1.3.2.cmml" xref="S3.E7.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.1.1.3.2.1.cmml" xref="S3.E7.m1.1.1.1.1.3">superscript</csymbol><ci id="S3.E7.m1.1.1.1.1.3.2.2.cmml" xref="S3.E7.m1.1.1.1.1.3.2.2">𝐅</ci><ci id="S3.E7.m1.1.1.1.1.3.2.3.cmml" xref="S3.E7.m1.1.1.1.1.3.2.3">𝐼</ci></apply><ci id="S3.E7.m1.1.1.1.1.3.3a.cmml" xref="S3.E7.m1.1.1.1.1.3.3"><mtext mathsize="70%" id="S3.E7.m1.1.1.1.1.3.3.cmml" xref="S3.E7.m1.1.1.1.1.3.3">BEV</mtext></ci></apply><apply id="S3.E7.m1.1.1.1.1.1.cmml" xref="S3.E7.m1.1.1.1.1.1"><times id="S3.E7.m1.1.1.1.1.1.2.cmml" xref="S3.E7.m1.1.1.1.1.1.2"></times><ci id="S3.E7.m1.1.1.1.1.1.3.cmml" xref="S3.E7.m1.1.1.1.1.1.3">𝙲𝚘𝚗𝚟𝚜</ci><apply id="S3.E7.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1"><times id="S3.E7.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.2"></times><ci id="S3.E7.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.3">𝚁𝚎𝚜𝚑𝚊𝚙𝚎</ci><apply id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1"><times id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.3"></times><ci id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.4.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.4">𝙲𝚘𝚗𝚌𝚊𝚝</ci><interval closure="open" id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2.2"><apply id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2">𝐅</ci><ci id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3">′</ci></apply><ci id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3">𝐼</ci></apply><ci id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3a.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3"><mtext mathsize="70%" id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3">3D</mtext></ci></apply><apply id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2">subscript</csymbol><apply id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.1.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2">superscript</csymbol><apply id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.1.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2">superscript</csymbol><ci id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.2.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.2">𝐅</ci><ci id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.3">′′</ci></apply><ci id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.3.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.3">𝐼</ci></apply><ci id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3a.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3"><mtext mathsize="70%" id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3">3D</mtext></ci></apply></interval></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7.m1.1c">\mathbf{F}^{I}_{\text{BEV}}=\mathtt{Convs}(\mathtt{Reshape}(\mathtt{Concat}(\mathbf{F^{\prime}}^{I}_{\text{3D}},\mathbf{F^{\prime\prime}}^{I}_{\text{3D}}))),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<p id="S3.SS4.p11.5" class="ltx_p">where <math id="S3.SS4.p11.5.m1.1" class="ltx_Math" alttext="\mathbf{F}^{I}_{\text{BEV}}\in\mathbb{R}^{X\times Y\times C_{I}}" display="inline"><semantics id="S3.SS4.p11.5.m1.1a"><mrow id="S3.SS4.p11.5.m1.1.1" xref="S3.SS4.p11.5.m1.1.1.cmml"><msubsup id="S3.SS4.p11.5.m1.1.1.2" xref="S3.SS4.p11.5.m1.1.1.2.cmml"><mi id="S3.SS4.p11.5.m1.1.1.2.2.2" xref="S3.SS4.p11.5.m1.1.1.2.2.2.cmml">𝐅</mi><mtext id="S3.SS4.p11.5.m1.1.1.2.3" xref="S3.SS4.p11.5.m1.1.1.2.3a.cmml">BEV</mtext><mi id="S3.SS4.p11.5.m1.1.1.2.2.3" xref="S3.SS4.p11.5.m1.1.1.2.2.3.cmml">I</mi></msubsup><mo id="S3.SS4.p11.5.m1.1.1.1" xref="S3.SS4.p11.5.m1.1.1.1.cmml">∈</mo><msup id="S3.SS4.p11.5.m1.1.1.3" xref="S3.SS4.p11.5.m1.1.1.3.cmml"><mi id="S3.SS4.p11.5.m1.1.1.3.2" xref="S3.SS4.p11.5.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS4.p11.5.m1.1.1.3.3" xref="S3.SS4.p11.5.m1.1.1.3.3.cmml"><mi id="S3.SS4.p11.5.m1.1.1.3.3.2" xref="S3.SS4.p11.5.m1.1.1.3.3.2.cmml">X</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p11.5.m1.1.1.3.3.1" xref="S3.SS4.p11.5.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.SS4.p11.5.m1.1.1.3.3.3" xref="S3.SS4.p11.5.m1.1.1.3.3.3.cmml">Y</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p11.5.m1.1.1.3.3.1a" xref="S3.SS4.p11.5.m1.1.1.3.3.1.cmml">×</mo><msub id="S3.SS4.p11.5.m1.1.1.3.3.4" xref="S3.SS4.p11.5.m1.1.1.3.3.4.cmml"><mi id="S3.SS4.p11.5.m1.1.1.3.3.4.2" xref="S3.SS4.p11.5.m1.1.1.3.3.4.2.cmml">C</mi><mi id="S3.SS4.p11.5.m1.1.1.3.3.4.3" xref="S3.SS4.p11.5.m1.1.1.3.3.4.3.cmml">I</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p11.5.m1.1b"><apply id="S3.SS4.p11.5.m1.1.1.cmml" xref="S3.SS4.p11.5.m1.1.1"><in id="S3.SS4.p11.5.m1.1.1.1.cmml" xref="S3.SS4.p11.5.m1.1.1.1"></in><apply id="S3.SS4.p11.5.m1.1.1.2.cmml" xref="S3.SS4.p11.5.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p11.5.m1.1.1.2.1.cmml" xref="S3.SS4.p11.5.m1.1.1.2">subscript</csymbol><apply id="S3.SS4.p11.5.m1.1.1.2.2.cmml" xref="S3.SS4.p11.5.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p11.5.m1.1.1.2.2.1.cmml" xref="S3.SS4.p11.5.m1.1.1.2">superscript</csymbol><ci id="S3.SS4.p11.5.m1.1.1.2.2.2.cmml" xref="S3.SS4.p11.5.m1.1.1.2.2.2">𝐅</ci><ci id="S3.SS4.p11.5.m1.1.1.2.2.3.cmml" xref="S3.SS4.p11.5.m1.1.1.2.2.3">𝐼</ci></apply><ci id="S3.SS4.p11.5.m1.1.1.2.3a.cmml" xref="S3.SS4.p11.5.m1.1.1.2.3"><mtext mathsize="70%" id="S3.SS4.p11.5.m1.1.1.2.3.cmml" xref="S3.SS4.p11.5.m1.1.1.2.3">BEV</mtext></ci></apply><apply id="S3.SS4.p11.5.m1.1.1.3.cmml" xref="S3.SS4.p11.5.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.p11.5.m1.1.1.3.1.cmml" xref="S3.SS4.p11.5.m1.1.1.3">superscript</csymbol><ci id="S3.SS4.p11.5.m1.1.1.3.2.cmml" xref="S3.SS4.p11.5.m1.1.1.3.2">ℝ</ci><apply id="S3.SS4.p11.5.m1.1.1.3.3.cmml" xref="S3.SS4.p11.5.m1.1.1.3.3"><times id="S3.SS4.p11.5.m1.1.1.3.3.1.cmml" xref="S3.SS4.p11.5.m1.1.1.3.3.1"></times><ci id="S3.SS4.p11.5.m1.1.1.3.3.2.cmml" xref="S3.SS4.p11.5.m1.1.1.3.3.2">𝑋</ci><ci id="S3.SS4.p11.5.m1.1.1.3.3.3.cmml" xref="S3.SS4.p11.5.m1.1.1.3.3.3">𝑌</ci><apply id="S3.SS4.p11.5.m1.1.1.3.3.4.cmml" xref="S3.SS4.p11.5.m1.1.1.3.3.4"><csymbol cd="ambiguous" id="S3.SS4.p11.5.m1.1.1.3.3.4.1.cmml" xref="S3.SS4.p11.5.m1.1.1.3.3.4">subscript</csymbol><ci id="S3.SS4.p11.5.m1.1.1.3.3.4.2.cmml" xref="S3.SS4.p11.5.m1.1.1.3.3.4.2">𝐶</ci><ci id="S3.SS4.p11.5.m1.1.1.3.3.4.3.cmml" xref="S3.SS4.p11.5.m1.1.1.3.3.4.3">𝐼</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p11.5.m1.1c">\mathbf{F}^{I}_{\text{BEV}}\in\mathbb{R}^{X\times Y\times C_{I}}</annotation></semantics></math> represents the final image BEV features, which are the output obtained from the view transformation module utilizing the <span id="S3.SS4.p11.5.1" class="ltx_text ltx_font_italic">radar occupancy-assisted depth-based sampling</span> method.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS5.5.1.1" class="ltx_text">III-E</span> </span><span id="S3.SS5.6.2" class="ltx_text ltx_font_italic">Multi-modal Fusion and Detection Head</span>
</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.2" class="ltx_p">After acquiring the radar and image BEV features, the fusion module integrates their information and produces fused BEV feature maps. In our approach, the radar BEV features, denoted as <math id="S3.SS5.p1.1.m1.1" class="ltx_Math" alttext="\mathbf{F}_{\text{BEV}}^{P}" display="inline"><semantics id="S3.SS5.p1.1.m1.1a"><msubsup id="S3.SS5.p1.1.m1.1.1" xref="S3.SS5.p1.1.m1.1.1.cmml"><mi id="S3.SS5.p1.1.m1.1.1.2.2" xref="S3.SS5.p1.1.m1.1.1.2.2.cmml">𝐅</mi><mtext id="S3.SS5.p1.1.m1.1.1.2.3" xref="S3.SS5.p1.1.m1.1.1.2.3a.cmml">BEV</mtext><mi id="S3.SS5.p1.1.m1.1.1.3" xref="S3.SS5.p1.1.m1.1.1.3.cmml">P</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.1.m1.1b"><apply id="S3.SS5.p1.1.m1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS5.p1.1.m1.1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1">superscript</csymbol><apply id="S3.SS5.p1.1.m1.1.1.2.cmml" xref="S3.SS5.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS5.p1.1.m1.1.1.2.1.cmml" xref="S3.SS5.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS5.p1.1.m1.1.1.2.2.cmml" xref="S3.SS5.p1.1.m1.1.1.2.2">𝐅</ci><ci id="S3.SS5.p1.1.m1.1.1.2.3a.cmml" xref="S3.SS5.p1.1.m1.1.1.2.3"><mtext mathsize="70%" id="S3.SS5.p1.1.m1.1.1.2.3.cmml" xref="S3.SS5.p1.1.m1.1.1.2.3">BEV</mtext></ci></apply><ci id="S3.SS5.p1.1.m1.1.1.3.cmml" xref="S3.SS5.p1.1.m1.1.1.3">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.1.m1.1c">\mathbf{F}_{\text{BEV}}^{P}</annotation></semantics></math>, and the image BEV features, denoted as <math id="S3.SS5.p1.2.m2.1" class="ltx_Math" alttext="\mathbf{F}^{I}_{\text{BEV}}" display="inline"><semantics id="S3.SS5.p1.2.m2.1a"><msubsup id="S3.SS5.p1.2.m2.1.1" xref="S3.SS5.p1.2.m2.1.1.cmml"><mi id="S3.SS5.p1.2.m2.1.1.2.2" xref="S3.SS5.p1.2.m2.1.1.2.2.cmml">𝐅</mi><mtext id="S3.SS5.p1.2.m2.1.1.3" xref="S3.SS5.p1.2.m2.1.1.3a.cmml">BEV</mtext><mi id="S3.SS5.p1.2.m2.1.1.2.3" xref="S3.SS5.p1.2.m2.1.1.2.3.cmml">I</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.2.m2.1b"><apply id="S3.SS5.p1.2.m2.1.1.cmml" xref="S3.SS5.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS5.p1.2.m2.1.1.1.cmml" xref="S3.SS5.p1.2.m2.1.1">subscript</csymbol><apply id="S3.SS5.p1.2.m2.1.1.2.cmml" xref="S3.SS5.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS5.p1.2.m2.1.1.2.1.cmml" xref="S3.SS5.p1.2.m2.1.1">superscript</csymbol><ci id="S3.SS5.p1.2.m2.1.1.2.2.cmml" xref="S3.SS5.p1.2.m2.1.1.2.2">𝐅</ci><ci id="S3.SS5.p1.2.m2.1.1.2.3.cmml" xref="S3.SS5.p1.2.m2.1.1.2.3">𝐼</ci></apply><ci id="S3.SS5.p1.2.m2.1.1.3a.cmml" xref="S3.SS5.p1.2.m2.1.1.3"><mtext mathsize="70%" id="S3.SS5.p1.2.m2.1.1.3.cmml" xref="S3.SS5.p1.2.m2.1.1.3">BEV</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.2.m2.1c">\mathbf{F}^{I}_{\text{BEV}}</annotation></semantics></math>, have the same resolution, allowing for concatenation and fusion through convolutional operations. The resulting fused BEV features are subsequently fed into the detection head to predict 3D bounding boxes. In this work, we adopt the methodology of CenterPoint <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a>]</cite> to generate category-wise heatmaps and perform object detection. <span id="S3.SS5.p1.2.1" class="ltx_text ltx_font_italic">It is important to note that our fusion strategy and detection head are not limited to specific methods. For instance, our model can also incorporate attention-based fusion techniques and employ an anchor-based detection head.</span></p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Experiments and Analysis</span>
</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">Dataset and Evaluation Metrics</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">Dataset:</span> In this study, we utilize two datasets, View-of-Delft (VoD) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite> and TJ4DRadSet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>, to evaluate the performance of our proposed model. These datasets are designed for autonomous driving and encompass data from various sensors, including LiDAR points, 4D radar points, and camera images. Each object in the datasets is annotated with its corresponding category, a 3D bounding box, and a tracking ID. Moreover, the datasets provide coordinate transformation matrices between different sensors.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">The VoD dataset encompasses three object categories we used in experiments: Car, pedestrian, and cyclist. On the other hand, the TJ4DRadSet includes an additional class, truck. It also presents a more diverse range of driving scenarios than VoD. Notably, it exhibits significant variations in lighting conditions throughout the dataset, as well as different road types such as crossroads and elevated roads. Consequently, the 3D object detection task becomes considerably more challenging when working with the TJ4DRadSet dataset.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">For both datasets, we adopt the official data splits provided. Specifically, the VoD dataset comprises 5139 frames for training and 1296 frames for validation. Since the official test server for the VoD dataset is not yet released, evaluations and analyses are performed solely on the validation set. In the case of the TJ4DRadSet, the training set consists of 5717 frames, while the test set encompasses 2040 frames.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p"><span id="S4.SS1.p4.1.1" class="ltx_text ltx_font_bold">Evaluation Metrics:</span> Our proposed model is evaluated using specific metrics for each dataset.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.4" class="ltx_p">For the VoD dataset, there are two official evaluation metrics: AP in the entire annotated area (EAA AP) and AP in the driving corridor (RoI AP). The driving corridor, considered as a region of interest (RoI), is located close to the ego-vehicle and is defined as a specific area, <math id="S4.SS1.p5.1.m1.6" class="ltx_Math" alttext="D_{\text{RoI}}=\{(x,y,z)|-4\text{m}&lt;x&lt;4\text{m},z&lt;25\text{m}\}" display="inline"><semantics id="S4.SS1.p5.1.m1.6a"><mrow id="S4.SS1.p5.1.m1.6.6" xref="S4.SS1.p5.1.m1.6.6.cmml"><msub id="S4.SS1.p5.1.m1.6.6.4" xref="S4.SS1.p5.1.m1.6.6.4.cmml"><mi id="S4.SS1.p5.1.m1.6.6.4.2" xref="S4.SS1.p5.1.m1.6.6.4.2.cmml">D</mi><mtext id="S4.SS1.p5.1.m1.6.6.4.3" xref="S4.SS1.p5.1.m1.6.6.4.3a.cmml">RoI</mtext></msub><mo id="S4.SS1.p5.1.m1.6.6.3" xref="S4.SS1.p5.1.m1.6.6.3.cmml">=</mo><mrow id="S4.SS1.p5.1.m1.6.6.2.2" xref="S4.SS1.p5.1.m1.6.6.2.3.cmml"><mo stretchy="false" id="S4.SS1.p5.1.m1.6.6.2.2.3" xref="S4.SS1.p5.1.m1.6.6.2.3.1.cmml">{</mo><mrow id="S4.SS1.p5.1.m1.5.5.1.1.1.2" xref="S4.SS1.p5.1.m1.5.5.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS1.p5.1.m1.5.5.1.1.1.2.1" xref="S4.SS1.p5.1.m1.5.5.1.1.1.1.cmml">(</mo><mi id="S4.SS1.p5.1.m1.1.1" xref="S4.SS1.p5.1.m1.1.1.cmml">x</mi><mo id="S4.SS1.p5.1.m1.5.5.1.1.1.2.2" xref="S4.SS1.p5.1.m1.5.5.1.1.1.1.cmml">,</mo><mi id="S4.SS1.p5.1.m1.2.2" xref="S4.SS1.p5.1.m1.2.2.cmml">y</mi><mo id="S4.SS1.p5.1.m1.5.5.1.1.1.2.3" xref="S4.SS1.p5.1.m1.5.5.1.1.1.1.cmml">,</mo><mi id="S4.SS1.p5.1.m1.3.3" xref="S4.SS1.p5.1.m1.3.3.cmml">z</mi><mo stretchy="false" id="S4.SS1.p5.1.m1.5.5.1.1.1.2.4" xref="S4.SS1.p5.1.m1.5.5.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S4.SS1.p5.1.m1.6.6.2.2.4" xref="S4.SS1.p5.1.m1.6.6.2.3.1.cmml">|</mo><mrow id="S4.SS1.p5.1.m1.6.6.2.2.2.2" xref="S4.SS1.p5.1.m1.6.6.2.2.2.3.cmml"><mrow id="S4.SS1.p5.1.m1.6.6.2.2.2.1.1" xref="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.cmml"><mrow id="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.2" xref="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.2.cmml"><mo id="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.2a" xref="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.2.cmml">−</mo><mrow id="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.2.2" xref="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.2.2.cmml"><mn id="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.2.2.2" xref="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.2.2.2.cmml">4</mn><mo lspace="0em" rspace="0em" id="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.2.2.1" xref="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.2.2.1.cmml">​</mo><mtext id="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.2.2.3" xref="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.2.2.3a.cmml">m</mtext></mrow></mrow><mo id="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.3" xref="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.3.cmml">&lt;</mo><mi id="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.4" xref="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.4.cmml">x</mi><mo id="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.5" xref="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.5.cmml">&lt;</mo><mrow id="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.6" xref="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.6.cmml"><mn id="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.6.2" xref="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.6.2.cmml">4</mn><mo lspace="0em" rspace="0em" id="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.6.1" xref="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.6.1.cmml">​</mo><mtext id="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.6.3" xref="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.6.3a.cmml">m</mtext></mrow></mrow><mo id="S4.SS1.p5.1.m1.6.6.2.2.2.2.3" xref="S4.SS1.p5.1.m1.6.6.2.2.2.3a.cmml">,</mo><mrow id="S4.SS1.p5.1.m1.6.6.2.2.2.2.2" xref="S4.SS1.p5.1.m1.6.6.2.2.2.2.2.cmml"><mi id="S4.SS1.p5.1.m1.4.4" xref="S4.SS1.p5.1.m1.4.4.cmml">z</mi><mo id="S4.SS1.p5.1.m1.6.6.2.2.2.2.2.1" xref="S4.SS1.p5.1.m1.6.6.2.2.2.2.2.1.cmml">&lt;</mo><mrow id="S4.SS1.p5.1.m1.6.6.2.2.2.2.2.2" xref="S4.SS1.p5.1.m1.6.6.2.2.2.2.2.2.cmml"><mn id="S4.SS1.p5.1.m1.6.6.2.2.2.2.2.2.2" xref="S4.SS1.p5.1.m1.6.6.2.2.2.2.2.2.2.cmml">25</mn><mo lspace="0em" rspace="0em" id="S4.SS1.p5.1.m1.6.6.2.2.2.2.2.2.1" xref="S4.SS1.p5.1.m1.6.6.2.2.2.2.2.2.1.cmml">​</mo><mtext id="S4.SS1.p5.1.m1.6.6.2.2.2.2.2.2.3" xref="S4.SS1.p5.1.m1.6.6.2.2.2.2.2.2.3a.cmml">m</mtext></mrow></mrow></mrow><mo stretchy="false" id="S4.SS1.p5.1.m1.6.6.2.2.5" xref="S4.SS1.p5.1.m1.6.6.2.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.1.m1.6b"><apply id="S4.SS1.p5.1.m1.6.6.cmml" xref="S4.SS1.p5.1.m1.6.6"><eq id="S4.SS1.p5.1.m1.6.6.3.cmml" xref="S4.SS1.p5.1.m1.6.6.3"></eq><apply id="S4.SS1.p5.1.m1.6.6.4.cmml" xref="S4.SS1.p5.1.m1.6.6.4"><csymbol cd="ambiguous" id="S4.SS1.p5.1.m1.6.6.4.1.cmml" xref="S4.SS1.p5.1.m1.6.6.4">subscript</csymbol><ci id="S4.SS1.p5.1.m1.6.6.4.2.cmml" xref="S4.SS1.p5.1.m1.6.6.4.2">𝐷</ci><ci id="S4.SS1.p5.1.m1.6.6.4.3a.cmml" xref="S4.SS1.p5.1.m1.6.6.4.3"><mtext mathsize="70%" id="S4.SS1.p5.1.m1.6.6.4.3.cmml" xref="S4.SS1.p5.1.m1.6.6.4.3">RoI</mtext></ci></apply><apply id="S4.SS1.p5.1.m1.6.6.2.3.cmml" xref="S4.SS1.p5.1.m1.6.6.2.2"><csymbol cd="latexml" id="S4.SS1.p5.1.m1.6.6.2.3.1.cmml" xref="S4.SS1.p5.1.m1.6.6.2.2.3">conditional-set</csymbol><vector id="S4.SS1.p5.1.m1.5.5.1.1.1.1.cmml" xref="S4.SS1.p5.1.m1.5.5.1.1.1.2"><ci id="S4.SS1.p5.1.m1.1.1.cmml" xref="S4.SS1.p5.1.m1.1.1">𝑥</ci><ci id="S4.SS1.p5.1.m1.2.2.cmml" xref="S4.SS1.p5.1.m1.2.2">𝑦</ci><ci id="S4.SS1.p5.1.m1.3.3.cmml" xref="S4.SS1.p5.1.m1.3.3">𝑧</ci></vector><apply id="S4.SS1.p5.1.m1.6.6.2.2.2.3.cmml" xref="S4.SS1.p5.1.m1.6.6.2.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.p5.1.m1.6.6.2.2.2.3a.cmml" xref="S4.SS1.p5.1.m1.6.6.2.2.2.2.3">formulae-sequence</csymbol><apply id="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.cmml" xref="S4.SS1.p5.1.m1.6.6.2.2.2.1.1"><and id="S4.SS1.p5.1.m1.6.6.2.2.2.1.1a.cmml" xref="S4.SS1.p5.1.m1.6.6.2.2.2.1.1"></and><apply id="S4.SS1.p5.1.m1.6.6.2.2.2.1.1b.cmml" xref="S4.SS1.p5.1.m1.6.6.2.2.2.1.1"><lt id="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.3.cmml" xref="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.3"></lt><apply id="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.2.cmml" xref="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.2"><minus id="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.2.1.cmml" xref="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.2"></minus><apply id="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.2.2.cmml" xref="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.2.2"><times id="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.2.2.1.cmml" xref="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.2.2.1"></times><cn type="integer" id="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.2.2.2.cmml" xref="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.2.2.2">4</cn><ci id="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.2.2.3a.cmml" xref="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.2.2.3"><mtext id="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.2.2.3.cmml" xref="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.2.2.3">m</mtext></ci></apply></apply><ci id="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.4.cmml" xref="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.4">𝑥</ci></apply><apply id="S4.SS1.p5.1.m1.6.6.2.2.2.1.1c.cmml" xref="S4.SS1.p5.1.m1.6.6.2.2.2.1.1"><lt id="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.5.cmml" xref="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.5"></lt><share href="#S4.SS1.p5.1.m1.6.6.2.2.2.1.1.4.cmml" id="S4.SS1.p5.1.m1.6.6.2.2.2.1.1d.cmml" xref="S4.SS1.p5.1.m1.6.6.2.2.2.1.1"></share><apply id="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.6.cmml" xref="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.6"><times id="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.6.1.cmml" xref="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.6.1"></times><cn type="integer" id="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.6.2.cmml" xref="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.6.2">4</cn><ci id="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.6.3a.cmml" xref="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.6.3"><mtext id="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.6.3.cmml" xref="S4.SS1.p5.1.m1.6.6.2.2.2.1.1.6.3">m</mtext></ci></apply></apply></apply><apply id="S4.SS1.p5.1.m1.6.6.2.2.2.2.2.cmml" xref="S4.SS1.p5.1.m1.6.6.2.2.2.2.2"><lt id="S4.SS1.p5.1.m1.6.6.2.2.2.2.2.1.cmml" xref="S4.SS1.p5.1.m1.6.6.2.2.2.2.2.1"></lt><ci id="S4.SS1.p5.1.m1.4.4.cmml" xref="S4.SS1.p5.1.m1.4.4">𝑧</ci><apply id="S4.SS1.p5.1.m1.6.6.2.2.2.2.2.2.cmml" xref="S4.SS1.p5.1.m1.6.6.2.2.2.2.2.2"><times id="S4.SS1.p5.1.m1.6.6.2.2.2.2.2.2.1.cmml" xref="S4.SS1.p5.1.m1.6.6.2.2.2.2.2.2.1"></times><cn type="integer" id="S4.SS1.p5.1.m1.6.6.2.2.2.2.2.2.2.cmml" xref="S4.SS1.p5.1.m1.6.6.2.2.2.2.2.2.2">25</cn><ci id="S4.SS1.p5.1.m1.6.6.2.2.2.2.2.2.3a.cmml" xref="S4.SS1.p5.1.m1.6.6.2.2.2.2.2.2.3"><mtext id="S4.SS1.p5.1.m1.6.6.2.2.2.2.2.2.3.cmml" xref="S4.SS1.p5.1.m1.6.6.2.2.2.2.2.2.3">m</mtext></ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.1.m1.6c">D_{\text{RoI}}=\{(x,y,z)|-4\text{m}&lt;x&lt;4\text{m},z&lt;25\text{m}\}</annotation></semantics></math>, within the camera coordinate system. The Intersection over Union (IoU) thresholds used in the calculation of AP are <math id="S4.SS1.p5.2.m2.1" class="ltx_Math" alttext="0.5" display="inline"><semantics id="S4.SS1.p5.2.m2.1a"><mn id="S4.SS1.p5.2.m2.1.1" xref="S4.SS1.p5.2.m2.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.2.m2.1b"><cn type="float" id="S4.SS1.p5.2.m2.1.1.cmml" xref="S4.SS1.p5.2.m2.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.2.m2.1c">0.5</annotation></semantics></math>, <math id="S4.SS1.p5.3.m3.1" class="ltx_Math" alttext="0.25" display="inline"><semantics id="S4.SS1.p5.3.m3.1a"><mn id="S4.SS1.p5.3.m3.1.1" xref="S4.SS1.p5.3.m3.1.1.cmml">0.25</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.3.m3.1b"><cn type="float" id="S4.SS1.p5.3.m3.1.1.cmml" xref="S4.SS1.p5.3.m3.1.1">0.25</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.3.m3.1c">0.25</annotation></semantics></math>, and <math id="S4.SS1.p5.4.m4.1" class="ltx_Math" alttext="0.25" display="inline"><semantics id="S4.SS1.p5.4.m4.1a"><mn id="S4.SS1.p5.4.m4.1.1" xref="S4.SS1.p5.4.m4.1.1.cmml">0.25</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.4.m4.1b"><cn type="float" id="S4.SS1.p5.4.m4.1.1.cmml" xref="S4.SS1.p5.4.m4.1.1">0.25</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.4.m4.1c">0.25</annotation></semantics></math> for cars, pedestrians, and cyclists, respectively. For each predicted bounding box defined as a True Positive (TP), these thresholds determine the minimum overlap required between it and the ground truth.</p>
</div>
<div id="S4.SS1.p6" class="ltx_para">
<p id="S4.SS1.p6.2" class="ltx_p">In the case of the TJ4DRadSet dataset, evaluation metrics include 3D AP and BEV AP for different object classes within a range of <math id="S4.SS1.p6.1.m1.1" class="ltx_Math" alttext="70" display="inline"><semantics id="S4.SS1.p6.1.m1.1a"><mn id="S4.SS1.p6.1.m1.1.1" xref="S4.SS1.p6.1.m1.1.1.cmml">70</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p6.1.m1.1b"><cn type="integer" id="S4.SS1.p6.1.m1.1.1.cmml" xref="S4.SS1.p6.1.m1.1.1">70</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p6.1.m1.1c">70</annotation></semantics></math> meters. The IoU thresholds for cars, pedestrians, and cyclists follow the same values as those used in the VoD dataset. Additionally, for the truck class, the IoU threshold is set to <math id="S4.SS1.p6.2.m2.1" class="ltx_Math" alttext="0.5" display="inline"><semantics id="S4.SS1.p6.2.m2.1a"><mn id="S4.SS1.p6.2.m2.1.1" xref="S4.SS1.p6.2.m2.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p6.2.m2.1b"><cn type="float" id="S4.SS1.p6.2.m2.1.1.cmml" xref="S4.SS1.p6.2.m2.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p6.2.m2.1c">0.5</annotation></semantics></math>.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Comparison with state-of-the-art methods on the validation set of VoD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite>, where R denotes 4D imaging radar and C indicates camera. The results of methods marked with † are inherited from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite>. Note that BEVFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">58</span></a>]</cite> is implemented according to the radar + camera configuration file on its official github.</figcaption>
<table id="S4.T2.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T2.3.4" class="ltx_tr">
<td id="S4.T2.3.4.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S4.T2.3.4.1.1" class="ltx_text">Method</span></td>
<td id="S4.T2.3.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T2.3.4.2.1" class="ltx_text">Modality</span></td>
<td id="S4.T2.3.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="4">AP in the Entire Annotated Area (%)</td>
<td id="S4.T2.3.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="4">AP in the Region of Interest (%)</td>
<td id="S4.T2.3.4.5" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S4.T2.3.4.5.1" class="ltx_text">FPS</span></td>
</tr>
<tr id="S4.T2.3.5" class="ltx_tr">
<td id="S4.T2.3.5.1" class="ltx_td ltx_align_center ltx_border_t">Car</td>
<td id="S4.T2.3.5.2" class="ltx_td ltx_align_center ltx_border_t">Pedestrian</td>
<td id="S4.T2.3.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Cyclist</td>
<td id="S4.T2.3.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">mAP</td>
<td id="S4.T2.3.5.5" class="ltx_td ltx_align_center ltx_border_t">Car</td>
<td id="S4.T2.3.5.6" class="ltx_td ltx_align_center ltx_border_t">Pedestrian</td>
<td id="S4.T2.3.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Cyclist</td>
<td id="S4.T2.3.5.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">mAP</td>
</tr>
<tr id="S4.T2.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1" class="ltx_td ltx_align_center ltx_border_t">PointPillars<sup id="S4.T2.1.1.1.1" class="ltx_sup"><span id="S4.T2.1.1.1.1.1" class="ltx_text ltx_font_italic">†</span></sup> (CVPR 2019) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite>
</td>
<td id="S4.T2.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">R</td>
<td id="S4.T2.1.1.3" class="ltx_td ltx_align_center ltx_border_t">37.06</td>
<td id="S4.T2.1.1.4" class="ltx_td ltx_align_center ltx_border_t">35.04</td>
<td id="S4.T2.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">63.44</td>
<td id="S4.T2.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">45.18</td>
<td id="S4.T2.1.1.7" class="ltx_td ltx_align_center ltx_border_t">70.15</td>
<td id="S4.T2.1.1.8" class="ltx_td ltx_align_center ltx_border_t">47.22</td>
<td id="S4.T2.1.1.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">85.07</td>
<td id="S4.T2.1.1.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">67.48</td>
<td id="S4.T2.1.1.11" class="ltx_td ltx_align_center ltx_border_t">N/A</td>
</tr>
<tr id="S4.T2.2.2" class="ltx_tr">
<td id="S4.T2.2.2.1" class="ltx_td ltx_align_center">RadarPillarNet<sup id="S4.T2.2.2.1.1" class="ltx_sup"><span id="S4.T2.2.2.1.1.1" class="ltx_text ltx_font_italic">†</span></sup> (IEEE T-IM 2023)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite>
</td>
<td id="S4.T2.2.2.2" class="ltx_td ltx_align_center ltx_border_r">R</td>
<td id="S4.T2.2.2.3" class="ltx_td ltx_align_center">39.30</td>
<td id="S4.T2.2.2.4" class="ltx_td ltx_align_center">35.10</td>
<td id="S4.T2.2.2.5" class="ltx_td ltx_align_center ltx_border_r">63.63</td>
<td id="S4.T2.2.2.6" class="ltx_td ltx_align_center ltx_border_r">46.01</td>
<td id="S4.T2.2.2.7" class="ltx_td ltx_align_center">71.65</td>
<td id="S4.T2.2.2.8" class="ltx_td ltx_align_center">42.80</td>
<td id="S4.T2.2.2.9" class="ltx_td ltx_align_center ltx_border_r">83.14</td>
<td id="S4.T2.2.2.10" class="ltx_td ltx_align_center ltx_border_r">65.86</td>
<td id="S4.T2.2.2.11" class="ltx_td ltx_align_center">N/A</td>
</tr>
<tr id="S4.T2.3.6" class="ltx_tr">
<td id="S4.T2.3.6.1" class="ltx_td ltx_align_center">LXL - R</td>
<td id="S4.T2.3.6.2" class="ltx_td ltx_align_center ltx_border_r">R</td>
<td id="S4.T2.3.6.3" class="ltx_td ltx_align_center">32.75</td>
<td id="S4.T2.3.6.4" class="ltx_td ltx_align_center">39.65</td>
<td id="S4.T2.3.6.5" class="ltx_td ltx_align_center ltx_border_r">68.13</td>
<td id="S4.T2.3.6.6" class="ltx_td ltx_align_center ltx_border_r">46.84</td>
<td id="S4.T2.3.6.7" class="ltx_td ltx_align_center">70.26</td>
<td id="S4.T2.3.6.8" class="ltx_td ltx_align_center">47.34</td>
<td id="S4.T2.3.6.9" class="ltx_td ltx_align_center ltx_border_r">87.93</td>
<td id="S4.T2.3.6.10" class="ltx_td ltx_align_center ltx_border_r">68.51</td>
<td id="S4.T2.3.6.11" class="ltx_td ltx_align_center">44.7</td>
</tr>
<tr id="S4.T2.3.7" class="ltx_tr">
<td id="S4.T2.3.7.1" class="ltx_td ltx_align_center">FUTR3D (CVPR 2023) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">57</span></a>]</cite>
</td>
<td id="S4.T2.3.7.2" class="ltx_td ltx_align_center ltx_border_r">R+C</td>
<td id="S4.T2.3.7.3" class="ltx_td ltx_align_center"><span id="S4.T2.3.7.3.1" class="ltx_text ltx_font_bold">46.01</span></td>
<td id="S4.T2.3.7.4" class="ltx_td ltx_align_center">35.11</td>
<td id="S4.T2.3.7.5" class="ltx_td ltx_align_center ltx_border_r">65.98</td>
<td id="S4.T2.3.7.6" class="ltx_td ltx_align_center ltx_border_r">49.03</td>
<td id="S4.T2.3.7.7" class="ltx_td ltx_align_center"><span id="S4.T2.3.7.7.1" class="ltx_text ltx_font_bold">78.66</span></td>
<td id="S4.T2.3.7.8" class="ltx_td ltx_align_center">43.10</td>
<td id="S4.T2.3.7.9" class="ltx_td ltx_align_center ltx_border_r">86.19</td>
<td id="S4.T2.3.7.10" class="ltx_td ltx_align_center ltx_border_r">69.32</td>
<td id="S4.T2.3.7.11" class="ltx_td ltx_align_center">7.3</td>
</tr>
<tr id="S4.T2.3.8" class="ltx_tr">
<td id="S4.T2.3.8.1" class="ltx_td ltx_align_center">BEVFusion (ICRA 2023) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">58</span></a>]</cite>
</td>
<td id="S4.T2.3.8.2" class="ltx_td ltx_align_center ltx_border_r">R+C</td>
<td id="S4.T2.3.8.3" class="ltx_td ltx_align_center">37.85</td>
<td id="S4.T2.3.8.4" class="ltx_td ltx_align_center">40.96</td>
<td id="S4.T2.3.8.5" class="ltx_td ltx_align_center ltx_border_r">68.95</td>
<td id="S4.T2.3.8.6" class="ltx_td ltx_align_center ltx_border_r">49.25</td>
<td id="S4.T2.3.8.7" class="ltx_td ltx_align_center">70.21</td>
<td id="S4.T2.3.8.8" class="ltx_td ltx_align_center">45.86</td>
<td id="S4.T2.3.8.9" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.3.8.9.1" class="ltx_text ltx_font_bold">89.48</span></td>
<td id="S4.T2.3.8.10" class="ltx_td ltx_align_center ltx_border_r">68.52</td>
<td id="S4.T2.3.8.11" class="ltx_td ltx_align_center">7.1</td>
</tr>
<tr id="S4.T2.3.3" class="ltx_tr">
<td id="S4.T2.3.3.1" class="ltx_td ltx_align_center">RCFusion<sup id="S4.T2.3.3.1.1" class="ltx_sup"><span id="S4.T2.3.3.1.1.1" class="ltx_text ltx_font_italic">†</span></sup> (IEEE T-IM 2023) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite>
</td>
<td id="S4.T2.3.3.2" class="ltx_td ltx_align_center ltx_border_r">R+C</td>
<td id="S4.T2.3.3.3" class="ltx_td ltx_align_center">41.70</td>
<td id="S4.T2.3.3.4" class="ltx_td ltx_align_center">38.95</td>
<td id="S4.T2.3.3.5" class="ltx_td ltx_align_center ltx_border_r">68.31</td>
<td id="S4.T2.3.3.6" class="ltx_td ltx_align_center ltx_border_r">49.65</td>
<td id="S4.T2.3.3.7" class="ltx_td ltx_align_center">71.87</td>
<td id="S4.T2.3.3.8" class="ltx_td ltx_align_center">47.50</td>
<td id="S4.T2.3.3.9" class="ltx_td ltx_align_center ltx_border_r">88.33</td>
<td id="S4.T2.3.3.10" class="ltx_td ltx_align_center ltx_border_r">69.23</td>
<td id="S4.T2.3.3.11" class="ltx_td ltx_align_center">N/A</td>
</tr>
<tr id="S4.T2.3.9" class="ltx_tr">
<td id="S4.T2.3.9.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">LXL (<span id="S4.T2.3.9.1.1" class="ltx_text ltx_font_bold">Ours</span>)</td>
<td id="S4.T2.3.9.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">R+C</td>
<td id="S4.T2.3.9.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">42.33</td>
<td id="S4.T2.3.9.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T2.3.9.4.1" class="ltx_text ltx_font_bold">49.48</span></td>
<td id="S4.T2.3.9.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T2.3.9.5.1" class="ltx_text ltx_font_bold">77.12</span></td>
<td id="S4.T2.3.9.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T2.3.9.6.1" class="ltx_text ltx_font_bold">56.31</span></td>
<td id="S4.T2.3.9.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">72.18</td>
<td id="S4.T2.3.9.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T2.3.9.8.1" class="ltx_text ltx_font_bold">58.30</span></td>
<td id="S4.T2.3.9.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">88.31</td>
<td id="S4.T2.3.9.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T2.3.9.10.1" class="ltx_text ltx_font_bold">72.93</span></td>
<td id="S4.T2.3.9.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">6.1</td>
</tr>
</table>
</figure>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2307.00724/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="396" height="357" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Some visualization results on the VoD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite> validation set (best viewed in color and zoom). Each column corresponds to a frame of data containing an image and radar points (gray points) in BEV, where the red triangle denotes the position of the ego-vehicle, and orange boxes represent ground-truths. Blue boxes in the second row stand for predicted bounding boxes from LXL. Note that in the third row, the detection results of LXL-R are also shown as green boxes for comparison.</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.5.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.6.2" class="ltx_text ltx_font_italic">Implementation Details</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The model implementation is based on MMDetection3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">56</span></a>]</cite>, an open-source framework for 3D object detection tasks.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.4" class="ltx_p"><span id="S4.SS2.p2.4.1" class="ltx_text ltx_font_bold">Hyper-parameter Settings:</span> The hyper-parameters are determined following the official guidelines of the VoD dataset. The point cloud range (PCR) is set to a specific range, <math id="S4.SS2.p2.1.m1.5" class="ltx_Math" alttext="D_{\text{PCR}}=\{(x,y,z)|0&lt;x&lt;51.2\text{m},-25.6\text{m}&lt;y&lt;25.6\text{m},-3\text{m}&lt;z&lt;2\text{m}\}" display="inline"><semantics id="S4.SS2.p2.1.m1.5a"><mrow id="S4.SS2.p2.1.m1.5.5" xref="S4.SS2.p2.1.m1.5.5.cmml"><msub id="S4.SS2.p2.1.m1.5.5.4" xref="S4.SS2.p2.1.m1.5.5.4.cmml"><mi id="S4.SS2.p2.1.m1.5.5.4.2" xref="S4.SS2.p2.1.m1.5.5.4.2.cmml">D</mi><mtext id="S4.SS2.p2.1.m1.5.5.4.3" xref="S4.SS2.p2.1.m1.5.5.4.3a.cmml">PCR</mtext></msub><mo id="S4.SS2.p2.1.m1.5.5.3" xref="S4.SS2.p2.1.m1.5.5.3.cmml">=</mo><mrow id="S4.SS2.p2.1.m1.5.5.2.2" xref="S4.SS2.p2.1.m1.5.5.2.3.cmml"><mo stretchy="false" id="S4.SS2.p2.1.m1.5.5.2.2.3" xref="S4.SS2.p2.1.m1.5.5.2.3.1.cmml">{</mo><mrow id="S4.SS2.p2.1.m1.4.4.1.1.1.2" xref="S4.SS2.p2.1.m1.4.4.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS2.p2.1.m1.4.4.1.1.1.2.1" xref="S4.SS2.p2.1.m1.4.4.1.1.1.1.cmml">(</mo><mi id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">x</mi><mo id="S4.SS2.p2.1.m1.4.4.1.1.1.2.2" xref="S4.SS2.p2.1.m1.4.4.1.1.1.1.cmml">,</mo><mi id="S4.SS2.p2.1.m1.2.2" xref="S4.SS2.p2.1.m1.2.2.cmml">y</mi><mo id="S4.SS2.p2.1.m1.4.4.1.1.1.2.3" xref="S4.SS2.p2.1.m1.4.4.1.1.1.1.cmml">,</mo><mi id="S4.SS2.p2.1.m1.3.3" xref="S4.SS2.p2.1.m1.3.3.cmml">z</mi><mo stretchy="false" id="S4.SS2.p2.1.m1.4.4.1.1.1.2.4" xref="S4.SS2.p2.1.m1.4.4.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S4.SS2.p2.1.m1.5.5.2.2.4" xref="S4.SS2.p2.1.m1.5.5.2.3.1.cmml">|</mo><mrow id="S4.SS2.p2.1.m1.5.5.2.2.2.3" xref="S4.SS2.p2.1.m1.5.5.2.2.2.4.cmml"><mrow id="S4.SS2.p2.1.m1.5.5.2.2.2.1.1" xref="S4.SS2.p2.1.m1.5.5.2.2.2.1.1.cmml"><mn id="S4.SS2.p2.1.m1.5.5.2.2.2.1.1.2" xref="S4.SS2.p2.1.m1.5.5.2.2.2.1.1.2.cmml">0</mn><mo id="S4.SS2.p2.1.m1.5.5.2.2.2.1.1.3" xref="S4.SS2.p2.1.m1.5.5.2.2.2.1.1.3.cmml">&lt;</mo><mi id="S4.SS2.p2.1.m1.5.5.2.2.2.1.1.4" xref="S4.SS2.p2.1.m1.5.5.2.2.2.1.1.4.cmml">x</mi><mo id="S4.SS2.p2.1.m1.5.5.2.2.2.1.1.5" xref="S4.SS2.p2.1.m1.5.5.2.2.2.1.1.5.cmml">&lt;</mo><mrow id="S4.SS2.p2.1.m1.5.5.2.2.2.1.1.6" xref="S4.SS2.p2.1.m1.5.5.2.2.2.1.1.6.cmml"><mn id="S4.SS2.p2.1.m1.5.5.2.2.2.1.1.6.2" xref="S4.SS2.p2.1.m1.5.5.2.2.2.1.1.6.2.cmml">51.2</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p2.1.m1.5.5.2.2.2.1.1.6.1" xref="S4.SS2.p2.1.m1.5.5.2.2.2.1.1.6.1.cmml">​</mo><mtext id="S4.SS2.p2.1.m1.5.5.2.2.2.1.1.6.3" xref="S4.SS2.p2.1.m1.5.5.2.2.2.1.1.6.3a.cmml">m</mtext></mrow></mrow><mo id="S4.SS2.p2.1.m1.5.5.2.2.2.3.4" xref="S4.SS2.p2.1.m1.5.5.2.2.2.4a.cmml">,</mo><mrow id="S4.SS2.p2.1.m1.5.5.2.2.2.2.2" xref="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.cmml"><mrow id="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.2" xref="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.2.cmml"><mo id="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.2a" xref="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.2.cmml">−</mo><mrow id="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.2.2" xref="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.2.2.cmml"><mn id="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.2.2.2" xref="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.2.2.2.cmml">25.6</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.2.2.1" xref="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.2.2.1.cmml">​</mo><mtext id="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.2.2.3" xref="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.2.2.3a.cmml">m</mtext></mrow></mrow><mo id="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.3" xref="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.3.cmml">&lt;</mo><mi id="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.4" xref="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.4.cmml">y</mi><mo id="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.5" xref="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.5.cmml">&lt;</mo><mrow id="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.6" xref="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.6.cmml"><mn id="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.6.2" xref="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.6.2.cmml">25.6</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.6.1" xref="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.6.1.cmml">​</mo><mtext id="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.6.3" xref="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.6.3a.cmml">m</mtext></mrow></mrow><mo id="S4.SS2.p2.1.m1.5.5.2.2.2.3.5" xref="S4.SS2.p2.1.m1.5.5.2.2.2.4a.cmml">,</mo><mrow id="S4.SS2.p2.1.m1.5.5.2.2.2.3.3" xref="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.cmml"><mrow id="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.2" xref="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.2.cmml"><mo id="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.2a" xref="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.2.cmml">−</mo><mrow id="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.2.2" xref="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.2.2.cmml"><mn id="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.2.2.2" xref="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.2.2.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.2.2.1" xref="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.2.2.1.cmml">​</mo><mtext id="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.2.2.3" xref="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.2.2.3a.cmml">m</mtext></mrow></mrow><mo id="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.3" xref="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.3.cmml">&lt;</mo><mi id="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.4" xref="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.4.cmml">z</mi><mo id="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.5" xref="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.5.cmml">&lt;</mo><mrow id="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.6" xref="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.6.cmml"><mn id="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.6.2" xref="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.6.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.6.1" xref="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.6.1.cmml">​</mo><mtext id="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.6.3" xref="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.6.3a.cmml">m</mtext></mrow></mrow></mrow><mo stretchy="false" id="S4.SS2.p2.1.m1.5.5.2.2.5" xref="S4.SS2.p2.1.m1.5.5.2.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.5b"><apply id="S4.SS2.p2.1.m1.5.5.cmml" xref="S4.SS2.p2.1.m1.5.5"><eq id="S4.SS2.p2.1.m1.5.5.3.cmml" xref="S4.SS2.p2.1.m1.5.5.3"></eq><apply id="S4.SS2.p2.1.m1.5.5.4.cmml" xref="S4.SS2.p2.1.m1.5.5.4"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.5.5.4.1.cmml" xref="S4.SS2.p2.1.m1.5.5.4">subscript</csymbol><ci id="S4.SS2.p2.1.m1.5.5.4.2.cmml" xref="S4.SS2.p2.1.m1.5.5.4.2">𝐷</ci><ci id="S4.SS2.p2.1.m1.5.5.4.3a.cmml" xref="S4.SS2.p2.1.m1.5.5.4.3"><mtext mathsize="70%" id="S4.SS2.p2.1.m1.5.5.4.3.cmml" xref="S4.SS2.p2.1.m1.5.5.4.3">PCR</mtext></ci></apply><apply id="S4.SS2.p2.1.m1.5.5.2.3.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2"><csymbol cd="latexml" id="S4.SS2.p2.1.m1.5.5.2.3.1.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.3">conditional-set</csymbol><vector id="S4.SS2.p2.1.m1.4.4.1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.4.4.1.1.1.2"><ci id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">𝑥</ci><ci id="S4.SS2.p2.1.m1.2.2.cmml" xref="S4.SS2.p2.1.m1.2.2">𝑦</ci><ci id="S4.SS2.p2.1.m1.3.3.cmml" xref="S4.SS2.p2.1.m1.3.3">𝑧</ci></vector><apply id="S4.SS2.p2.1.m1.5.5.2.2.2.4.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.3"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.5.5.2.2.2.4a.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.3.4">formulae-sequence</csymbol><apply id="S4.SS2.p2.1.m1.5.5.2.2.2.1.1.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.1.1"><and id="S4.SS2.p2.1.m1.5.5.2.2.2.1.1a.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.1.1"></and><apply id="S4.SS2.p2.1.m1.5.5.2.2.2.1.1b.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.1.1"><lt id="S4.SS2.p2.1.m1.5.5.2.2.2.1.1.3.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.1.1.3"></lt><cn type="integer" id="S4.SS2.p2.1.m1.5.5.2.2.2.1.1.2.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.1.1.2">0</cn><ci id="S4.SS2.p2.1.m1.5.5.2.2.2.1.1.4.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.1.1.4">𝑥</ci></apply><apply id="S4.SS2.p2.1.m1.5.5.2.2.2.1.1c.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.1.1"><lt id="S4.SS2.p2.1.m1.5.5.2.2.2.1.1.5.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.1.1.5"></lt><share href="#S4.SS2.p2.1.m1.5.5.2.2.2.1.1.4.cmml" id="S4.SS2.p2.1.m1.5.5.2.2.2.1.1d.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.1.1"></share><apply id="S4.SS2.p2.1.m1.5.5.2.2.2.1.1.6.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.1.1.6"><times id="S4.SS2.p2.1.m1.5.5.2.2.2.1.1.6.1.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.1.1.6.1"></times><cn type="float" id="S4.SS2.p2.1.m1.5.5.2.2.2.1.1.6.2.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.1.1.6.2">51.2</cn><ci id="S4.SS2.p2.1.m1.5.5.2.2.2.1.1.6.3a.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.1.1.6.3"><mtext id="S4.SS2.p2.1.m1.5.5.2.2.2.1.1.6.3.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.1.1.6.3">m</mtext></ci></apply></apply></apply><apply id="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.2.2"><and id="S4.SS2.p2.1.m1.5.5.2.2.2.2.2a.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.2.2"></and><apply id="S4.SS2.p2.1.m1.5.5.2.2.2.2.2b.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.2.2"><lt id="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.3.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.3"></lt><apply id="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.2.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.2"><minus id="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.2.1.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.2"></minus><apply id="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.2.2.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.2.2"><times id="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.2.2.1.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.2.2.1"></times><cn type="float" id="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.2.2.2.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.2.2.2">25.6</cn><ci id="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.2.2.3a.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.2.2.3"><mtext id="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.2.2.3.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.2.2.3">m</mtext></ci></apply></apply><ci id="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.4.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.4">𝑦</ci></apply><apply id="S4.SS2.p2.1.m1.5.5.2.2.2.2.2c.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.2.2"><lt id="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.5.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.5"></lt><share href="#S4.SS2.p2.1.m1.5.5.2.2.2.2.2.4.cmml" id="S4.SS2.p2.1.m1.5.5.2.2.2.2.2d.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.2.2"></share><apply id="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.6.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.6"><times id="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.6.1.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.6.1"></times><cn type="float" id="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.6.2.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.6.2">25.6</cn><ci id="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.6.3a.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.6.3"><mtext id="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.6.3.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.2.2.6.3">m</mtext></ci></apply></apply></apply><apply id="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.3.3"><and id="S4.SS2.p2.1.m1.5.5.2.2.2.3.3a.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.3.3"></and><apply id="S4.SS2.p2.1.m1.5.5.2.2.2.3.3b.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.3.3"><lt id="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.3.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.3"></lt><apply id="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.2.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.2"><minus id="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.2.1.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.2"></minus><apply id="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.2.2.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.2.2"><times id="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.2.2.1.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.2.2.1"></times><cn type="integer" id="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.2.2.2.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.2.2.2">3</cn><ci id="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.2.2.3a.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.2.2.3"><mtext id="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.2.2.3.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.2.2.3">m</mtext></ci></apply></apply><ci id="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.4.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.4">𝑧</ci></apply><apply id="S4.SS2.p2.1.m1.5.5.2.2.2.3.3c.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.3.3"><lt id="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.5.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.5"></lt><share href="#S4.SS2.p2.1.m1.5.5.2.2.2.3.3.4.cmml" id="S4.SS2.p2.1.m1.5.5.2.2.2.3.3d.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.3.3"></share><apply id="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.6.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.6"><times id="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.6.1.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.6.1"></times><cn type="integer" id="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.6.2.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.6.2">2</cn><ci id="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.6.3a.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.6.3"><mtext id="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.6.3.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2.3.3.6.3">m</mtext></ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.5c">D_{\text{PCR}}=\{(x,y,z)|0&lt;x&lt;51.2\text{m},-25.6\text{m}&lt;y&lt;25.6\text{m},-3\text{m}&lt;z&lt;2\text{m}\}</annotation></semantics></math>, in the radar coordinate system. The pillar size in the voxelization process of radar points is defined as <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="0.16\text{m}\times 0.16\text{m}" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mrow id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml"><mrow id="S4.SS2.p2.2.m2.1.1.2" xref="S4.SS2.p2.2.m2.1.1.2.cmml"><mrow id="S4.SS2.p2.2.m2.1.1.2.2" xref="S4.SS2.p2.2.m2.1.1.2.2.cmml"><mn id="S4.SS2.p2.2.m2.1.1.2.2.2" xref="S4.SS2.p2.2.m2.1.1.2.2.2.cmml">0.16</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p2.2.m2.1.1.2.2.1" xref="S4.SS2.p2.2.m2.1.1.2.2.1.cmml">​</mo><mtext id="S4.SS2.p2.2.m2.1.1.2.2.3" xref="S4.SS2.p2.2.m2.1.1.2.2.3a.cmml">m</mtext></mrow><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p2.2.m2.1.1.2.1" xref="S4.SS2.p2.2.m2.1.1.2.1.cmml">×</mo><mn id="S4.SS2.p2.2.m2.1.1.2.3" xref="S4.SS2.p2.2.m2.1.1.2.3.cmml">0.16</mn></mrow><mo lspace="0em" rspace="0em" id="S4.SS2.p2.2.m2.1.1.1" xref="S4.SS2.p2.2.m2.1.1.1.cmml">​</mo><mtext id="S4.SS2.p2.2.m2.1.1.3" xref="S4.SS2.p2.2.m2.1.1.3a.cmml">m</mtext></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><apply id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1"><times id="S4.SS2.p2.2.m2.1.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1.1"></times><apply id="S4.SS2.p2.2.m2.1.1.2.cmml" xref="S4.SS2.p2.2.m2.1.1.2"><times id="S4.SS2.p2.2.m2.1.1.2.1.cmml" xref="S4.SS2.p2.2.m2.1.1.2.1"></times><apply id="S4.SS2.p2.2.m2.1.1.2.2.cmml" xref="S4.SS2.p2.2.m2.1.1.2.2"><times id="S4.SS2.p2.2.m2.1.1.2.2.1.cmml" xref="S4.SS2.p2.2.m2.1.1.2.2.1"></times><cn type="float" id="S4.SS2.p2.2.m2.1.1.2.2.2.cmml" xref="S4.SS2.p2.2.m2.1.1.2.2.2">0.16</cn><ci id="S4.SS2.p2.2.m2.1.1.2.2.3a.cmml" xref="S4.SS2.p2.2.m2.1.1.2.2.3"><mtext id="S4.SS2.p2.2.m2.1.1.2.2.3.cmml" xref="S4.SS2.p2.2.m2.1.1.2.2.3">m</mtext></ci></apply><cn type="float" id="S4.SS2.p2.2.m2.1.1.2.3.cmml" xref="S4.SS2.p2.2.m2.1.1.2.3">0.16</cn></apply><ci id="S4.SS2.p2.2.m2.1.1.3a.cmml" xref="S4.SS2.p2.2.m2.1.1.3"><mtext id="S4.SS2.p2.2.m2.1.1.3.cmml" xref="S4.SS2.p2.2.m2.1.1.3">m</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">0.16\text{m}\times 0.16\text{m}</annotation></semantics></math>. The stride of the radar feature extractor, which consists of the backbone and neck, is adjusted to <math id="S4.SS2.p2.3.m3.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S4.SS2.p2.3.m3.1a"><mn id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.1b"><cn type="integer" id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.1c">2</annotation></semantics></math> to achieve a final BEV resolution of <math id="S4.SS2.p2.4.m4.1" class="ltx_Math" alttext="160\times 160" display="inline"><semantics id="S4.SS2.p2.4.m4.1a"><mrow id="S4.SS2.p2.4.m4.1.1" xref="S4.SS2.p2.4.m4.1.1.cmml"><mn id="S4.SS2.p2.4.m4.1.1.2" xref="S4.SS2.p2.4.m4.1.1.2.cmml">160</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p2.4.m4.1.1.1" xref="S4.SS2.p2.4.m4.1.1.1.cmml">×</mo><mn id="S4.SS2.p2.4.m4.1.1.3" xref="S4.SS2.p2.4.m4.1.1.3.cmml">160</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.4.m4.1b"><apply id="S4.SS2.p2.4.m4.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1"><times id="S4.SS2.p2.4.m4.1.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1.1"></times><cn type="integer" id="S4.SS2.p2.4.m4.1.1.2.cmml" xref="S4.SS2.p2.4.m4.1.1.2">160</cn><cn type="integer" id="S4.SS2.p2.4.m4.1.1.3.cmml" xref="S4.SS2.p2.4.m4.1.1.3">160</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.4.m4.1c">160\times 160</annotation></semantics></math>.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.3" class="ltx_p">For the detection head, we utilize the CenterPoint <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a>]</cite> framework. During training, the minimum Gaussian radius for generating ground-truth heatmaps is set to 2. During inference, the top 1000 detections are considered, and a post-processing step with non-maximum suppression (NMS) is applied. The distance thresholds for NMS are set to <math id="S4.SS2.p3.1.m1.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S4.SS2.p3.1.m1.1a"><mn id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><cn type="integer" id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">4</annotation></semantics></math>m, <math id="S4.SS2.p3.2.m2.1" class="ltx_Math" alttext="0.3" display="inline"><semantics id="S4.SS2.p3.2.m2.1a"><mn id="S4.SS2.p3.2.m2.1.1" xref="S4.SS2.p3.2.m2.1.1.cmml">0.3</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.m2.1b"><cn type="float" id="S4.SS2.p3.2.m2.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1">0.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.m2.1c">0.3</annotation></semantics></math>m, and <math id="S4.SS2.p3.3.m3.1" class="ltx_Math" alttext="0.85" display="inline"><semantics id="S4.SS2.p3.3.m3.1a"><mn id="S4.SS2.p3.3.m3.1.1" xref="S4.SS2.p3.3.m3.1.1.cmml">0.85</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.3.m3.1b"><cn type="float" id="S4.SS2.p3.3.m3.1.1.cmml" xref="S4.SS2.p3.3.m3.1.1">0.85</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.3.m3.1c">0.85</annotation></semantics></math>m for cars, pedestrians, and cyclists, respectively.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.2" class="ltx_p">Regarding the TJ4DRadSet dataset, the PCR is set to <math id="S4.SS2.p4.1.m1.5" class="ltx_Math" alttext="D_{\text{PCR}}=\{(x,y,z)|0&lt;x&lt;69.12\text{m},-39.68\text{m}&lt;y&lt;39.68\text{m},-4\text{m}&lt;z&lt;2\text{m}\}" display="inline"><semantics id="S4.SS2.p4.1.m1.5a"><mrow id="S4.SS2.p4.1.m1.5.5" xref="S4.SS2.p4.1.m1.5.5.cmml"><msub id="S4.SS2.p4.1.m1.5.5.4" xref="S4.SS2.p4.1.m1.5.5.4.cmml"><mi id="S4.SS2.p4.1.m1.5.5.4.2" xref="S4.SS2.p4.1.m1.5.5.4.2.cmml">D</mi><mtext id="S4.SS2.p4.1.m1.5.5.4.3" xref="S4.SS2.p4.1.m1.5.5.4.3a.cmml">PCR</mtext></msub><mo id="S4.SS2.p4.1.m1.5.5.3" xref="S4.SS2.p4.1.m1.5.5.3.cmml">=</mo><mrow id="S4.SS2.p4.1.m1.5.5.2.2" xref="S4.SS2.p4.1.m1.5.5.2.3.cmml"><mo stretchy="false" id="S4.SS2.p4.1.m1.5.5.2.2.3" xref="S4.SS2.p4.1.m1.5.5.2.3.1.cmml">{</mo><mrow id="S4.SS2.p4.1.m1.4.4.1.1.1.2" xref="S4.SS2.p4.1.m1.4.4.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS2.p4.1.m1.4.4.1.1.1.2.1" xref="S4.SS2.p4.1.m1.4.4.1.1.1.1.cmml">(</mo><mi id="S4.SS2.p4.1.m1.1.1" xref="S4.SS2.p4.1.m1.1.1.cmml">x</mi><mo id="S4.SS2.p4.1.m1.4.4.1.1.1.2.2" xref="S4.SS2.p4.1.m1.4.4.1.1.1.1.cmml">,</mo><mi id="S4.SS2.p4.1.m1.2.2" xref="S4.SS2.p4.1.m1.2.2.cmml">y</mi><mo id="S4.SS2.p4.1.m1.4.4.1.1.1.2.3" xref="S4.SS2.p4.1.m1.4.4.1.1.1.1.cmml">,</mo><mi id="S4.SS2.p4.1.m1.3.3" xref="S4.SS2.p4.1.m1.3.3.cmml">z</mi><mo stretchy="false" id="S4.SS2.p4.1.m1.4.4.1.1.1.2.4" xref="S4.SS2.p4.1.m1.4.4.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S4.SS2.p4.1.m1.5.5.2.2.4" xref="S4.SS2.p4.1.m1.5.5.2.3.1.cmml">|</mo><mrow id="S4.SS2.p4.1.m1.5.5.2.2.2.3" xref="S4.SS2.p4.1.m1.5.5.2.2.2.4.cmml"><mrow id="S4.SS2.p4.1.m1.5.5.2.2.2.1.1" xref="S4.SS2.p4.1.m1.5.5.2.2.2.1.1.cmml"><mn id="S4.SS2.p4.1.m1.5.5.2.2.2.1.1.2" xref="S4.SS2.p4.1.m1.5.5.2.2.2.1.1.2.cmml">0</mn><mo id="S4.SS2.p4.1.m1.5.5.2.2.2.1.1.3" xref="S4.SS2.p4.1.m1.5.5.2.2.2.1.1.3.cmml">&lt;</mo><mi id="S4.SS2.p4.1.m1.5.5.2.2.2.1.1.4" xref="S4.SS2.p4.1.m1.5.5.2.2.2.1.1.4.cmml">x</mi><mo id="S4.SS2.p4.1.m1.5.5.2.2.2.1.1.5" xref="S4.SS2.p4.1.m1.5.5.2.2.2.1.1.5.cmml">&lt;</mo><mrow id="S4.SS2.p4.1.m1.5.5.2.2.2.1.1.6" xref="S4.SS2.p4.1.m1.5.5.2.2.2.1.1.6.cmml"><mn id="S4.SS2.p4.1.m1.5.5.2.2.2.1.1.6.2" xref="S4.SS2.p4.1.m1.5.5.2.2.2.1.1.6.2.cmml">69.12</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p4.1.m1.5.5.2.2.2.1.1.6.1" xref="S4.SS2.p4.1.m1.5.5.2.2.2.1.1.6.1.cmml">​</mo><mtext id="S4.SS2.p4.1.m1.5.5.2.2.2.1.1.6.3" xref="S4.SS2.p4.1.m1.5.5.2.2.2.1.1.6.3a.cmml">m</mtext></mrow></mrow><mo id="S4.SS2.p4.1.m1.5.5.2.2.2.3.4" xref="S4.SS2.p4.1.m1.5.5.2.2.2.4a.cmml">,</mo><mrow id="S4.SS2.p4.1.m1.5.5.2.2.2.2.2" xref="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.cmml"><mrow id="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.2" xref="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.2.cmml"><mo id="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.2a" xref="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.2.cmml">−</mo><mrow id="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.2.2" xref="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.2.2.cmml"><mn id="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.2.2.2" xref="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.2.2.2.cmml">39.68</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.2.2.1" xref="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.2.2.1.cmml">​</mo><mtext id="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.2.2.3" xref="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.2.2.3a.cmml">m</mtext></mrow></mrow><mo id="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.3" xref="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.3.cmml">&lt;</mo><mi id="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.4" xref="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.4.cmml">y</mi><mo id="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.5" xref="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.5.cmml">&lt;</mo><mrow id="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.6" xref="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.6.cmml"><mn id="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.6.2" xref="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.6.2.cmml">39.68</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.6.1" xref="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.6.1.cmml">​</mo><mtext id="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.6.3" xref="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.6.3a.cmml">m</mtext></mrow></mrow><mo id="S4.SS2.p4.1.m1.5.5.2.2.2.3.5" xref="S4.SS2.p4.1.m1.5.5.2.2.2.4a.cmml">,</mo><mrow id="S4.SS2.p4.1.m1.5.5.2.2.2.3.3" xref="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.cmml"><mrow id="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.2" xref="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.2.cmml"><mo id="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.2a" xref="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.2.cmml">−</mo><mrow id="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.2.2" xref="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.2.2.cmml"><mn id="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.2.2.2" xref="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.2.2.2.cmml">4</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.2.2.1" xref="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.2.2.1.cmml">​</mo><mtext id="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.2.2.3" xref="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.2.2.3a.cmml">m</mtext></mrow></mrow><mo id="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.3" xref="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.3.cmml">&lt;</mo><mi id="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.4" xref="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.4.cmml">z</mi><mo id="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.5" xref="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.5.cmml">&lt;</mo><mrow id="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.6" xref="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.6.cmml"><mn id="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.6.2" xref="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.6.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.6.1" xref="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.6.1.cmml">​</mo><mtext id="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.6.3" xref="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.6.3a.cmml">m</mtext></mrow></mrow></mrow><mo stretchy="false" id="S4.SS2.p4.1.m1.5.5.2.2.5" xref="S4.SS2.p4.1.m1.5.5.2.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.1.m1.5b"><apply id="S4.SS2.p4.1.m1.5.5.cmml" xref="S4.SS2.p4.1.m1.5.5"><eq id="S4.SS2.p4.1.m1.5.5.3.cmml" xref="S4.SS2.p4.1.m1.5.5.3"></eq><apply id="S4.SS2.p4.1.m1.5.5.4.cmml" xref="S4.SS2.p4.1.m1.5.5.4"><csymbol cd="ambiguous" id="S4.SS2.p4.1.m1.5.5.4.1.cmml" xref="S4.SS2.p4.1.m1.5.5.4">subscript</csymbol><ci id="S4.SS2.p4.1.m1.5.5.4.2.cmml" xref="S4.SS2.p4.1.m1.5.5.4.2">𝐷</ci><ci id="S4.SS2.p4.1.m1.5.5.4.3a.cmml" xref="S4.SS2.p4.1.m1.5.5.4.3"><mtext mathsize="70%" id="S4.SS2.p4.1.m1.5.5.4.3.cmml" xref="S4.SS2.p4.1.m1.5.5.4.3">PCR</mtext></ci></apply><apply id="S4.SS2.p4.1.m1.5.5.2.3.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2"><csymbol cd="latexml" id="S4.SS2.p4.1.m1.5.5.2.3.1.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.3">conditional-set</csymbol><vector id="S4.SS2.p4.1.m1.4.4.1.1.1.1.cmml" xref="S4.SS2.p4.1.m1.4.4.1.1.1.2"><ci id="S4.SS2.p4.1.m1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1">𝑥</ci><ci id="S4.SS2.p4.1.m1.2.2.cmml" xref="S4.SS2.p4.1.m1.2.2">𝑦</ci><ci id="S4.SS2.p4.1.m1.3.3.cmml" xref="S4.SS2.p4.1.m1.3.3">𝑧</ci></vector><apply id="S4.SS2.p4.1.m1.5.5.2.2.2.4.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.3"><csymbol cd="ambiguous" id="S4.SS2.p4.1.m1.5.5.2.2.2.4a.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.3.4">formulae-sequence</csymbol><apply id="S4.SS2.p4.1.m1.5.5.2.2.2.1.1.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.1.1"><and id="S4.SS2.p4.1.m1.5.5.2.2.2.1.1a.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.1.1"></and><apply id="S4.SS2.p4.1.m1.5.5.2.2.2.1.1b.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.1.1"><lt id="S4.SS2.p4.1.m1.5.5.2.2.2.1.1.3.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.1.1.3"></lt><cn type="integer" id="S4.SS2.p4.1.m1.5.5.2.2.2.1.1.2.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.1.1.2">0</cn><ci id="S4.SS2.p4.1.m1.5.5.2.2.2.1.1.4.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.1.1.4">𝑥</ci></apply><apply id="S4.SS2.p4.1.m1.5.5.2.2.2.1.1c.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.1.1"><lt id="S4.SS2.p4.1.m1.5.5.2.2.2.1.1.5.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.1.1.5"></lt><share href="#S4.SS2.p4.1.m1.5.5.2.2.2.1.1.4.cmml" id="S4.SS2.p4.1.m1.5.5.2.2.2.1.1d.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.1.1"></share><apply id="S4.SS2.p4.1.m1.5.5.2.2.2.1.1.6.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.1.1.6"><times id="S4.SS2.p4.1.m1.5.5.2.2.2.1.1.6.1.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.1.1.6.1"></times><cn type="float" id="S4.SS2.p4.1.m1.5.5.2.2.2.1.1.6.2.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.1.1.6.2">69.12</cn><ci id="S4.SS2.p4.1.m1.5.5.2.2.2.1.1.6.3a.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.1.1.6.3"><mtext id="S4.SS2.p4.1.m1.5.5.2.2.2.1.1.6.3.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.1.1.6.3">m</mtext></ci></apply></apply></apply><apply id="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.2.2"><and id="S4.SS2.p4.1.m1.5.5.2.2.2.2.2a.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.2.2"></and><apply id="S4.SS2.p4.1.m1.5.5.2.2.2.2.2b.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.2.2"><lt id="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.3.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.3"></lt><apply id="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.2.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.2"><minus id="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.2.1.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.2"></minus><apply id="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.2.2.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.2.2"><times id="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.2.2.1.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.2.2.1"></times><cn type="float" id="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.2.2.2.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.2.2.2">39.68</cn><ci id="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.2.2.3a.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.2.2.3"><mtext id="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.2.2.3.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.2.2.3">m</mtext></ci></apply></apply><ci id="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.4.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.4">𝑦</ci></apply><apply id="S4.SS2.p4.1.m1.5.5.2.2.2.2.2c.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.2.2"><lt id="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.5.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.5"></lt><share href="#S4.SS2.p4.1.m1.5.5.2.2.2.2.2.4.cmml" id="S4.SS2.p4.1.m1.5.5.2.2.2.2.2d.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.2.2"></share><apply id="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.6.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.6"><times id="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.6.1.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.6.1"></times><cn type="float" id="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.6.2.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.6.2">39.68</cn><ci id="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.6.3a.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.6.3"><mtext id="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.6.3.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.2.2.6.3">m</mtext></ci></apply></apply></apply><apply id="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.3.3"><and id="S4.SS2.p4.1.m1.5.5.2.2.2.3.3a.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.3.3"></and><apply id="S4.SS2.p4.1.m1.5.5.2.2.2.3.3b.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.3.3"><lt id="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.3.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.3"></lt><apply id="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.2.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.2"><minus id="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.2.1.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.2"></minus><apply id="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.2.2.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.2.2"><times id="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.2.2.1.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.2.2.1"></times><cn type="integer" id="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.2.2.2.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.2.2.2">4</cn><ci id="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.2.2.3a.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.2.2.3"><mtext id="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.2.2.3.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.2.2.3">m</mtext></ci></apply></apply><ci id="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.4.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.4">𝑧</ci></apply><apply id="S4.SS2.p4.1.m1.5.5.2.2.2.3.3c.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.3.3"><lt id="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.5.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.5"></lt><share href="#S4.SS2.p4.1.m1.5.5.2.2.2.3.3.4.cmml" id="S4.SS2.p4.1.m1.5.5.2.2.2.3.3d.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.3.3"></share><apply id="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.6.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.6"><times id="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.6.1.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.6.1"></times><cn type="integer" id="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.6.2.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.6.2">2</cn><ci id="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.6.3a.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.6.3"><mtext id="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.6.3.cmml" xref="S4.SS2.p4.1.m1.5.5.2.2.2.3.3.6.3">m</mtext></ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.1.m1.5c">D_{\text{PCR}}=\{(x,y,z)|0&lt;x&lt;69.12\text{m},-39.68\text{m}&lt;y&lt;39.68\text{m},-4\text{m}&lt;z&lt;2\text{m}\}</annotation></semantics></math>, and the other hyper-parameters remain consistent with those used in the VoD dataset. For the truck category, the distance threshold for NMS is <math id="S4.SS2.p4.2.m2.1" class="ltx_Math" alttext="12" display="inline"><semantics id="S4.SS2.p4.2.m2.1a"><mn id="S4.SS2.p4.2.m2.1.1" xref="S4.SS2.p4.2.m2.1.1.cmml">12</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.2.m2.1b"><cn type="integer" id="S4.SS2.p4.2.m2.1.1.cmml" xref="S4.SS2.p4.2.m2.1.1">12</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.2.m2.1c">12</annotation></semantics></math>m.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.11" class="ltx_p"><span id="S4.SS2.p5.11.1" class="ltx_text ltx_font_bold">Training Details:</span> During training, both images and radar points are normalized with the mean and standard deviation values of the corresponding data in the whole training set before being fed into the model:</p>
<table id="S4.E8" class="ltx_equationgroup ltx_eqn_table">
<tbody>
<tr id="S4.E8X" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.E8X.2.1.1.m1.1" class="ltx_Math" alttext="\displaystyle i" display="inline"><semantics id="S4.E8X.2.1.1.m1.1a"><mi id="S4.E8X.2.1.1.m1.1.1" xref="S4.E8X.2.1.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.E8X.2.1.1.m1.1b"><ci id="S4.E8X.2.1.1.m1.1.1.cmml" xref="S4.E8X.2.1.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.E8X.2.1.1.m1.1c">\displaystyle i</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S4.E8X.3.2.2.m1.1" class="ltx_Math" alttext="\displaystyle=(i-\mu_{I})/\sigma_{I}," display="inline"><semantics id="S4.E8X.3.2.2.m1.1a"><mrow id="S4.E8X.3.2.2.m1.1.1.1" xref="S4.E8X.3.2.2.m1.1.1.1.1.cmml"><mrow id="S4.E8X.3.2.2.m1.1.1.1.1" xref="S4.E8X.3.2.2.m1.1.1.1.1.cmml"><mi id="S4.E8X.3.2.2.m1.1.1.1.1.3" xref="S4.E8X.3.2.2.m1.1.1.1.1.3.cmml"></mi><mo id="S4.E8X.3.2.2.m1.1.1.1.1.2" xref="S4.E8X.3.2.2.m1.1.1.1.1.2.cmml">=</mo><mrow id="S4.E8X.3.2.2.m1.1.1.1.1.1" xref="S4.E8X.3.2.2.m1.1.1.1.1.1.cmml"><mrow id="S4.E8X.3.2.2.m1.1.1.1.1.1.1.1" xref="S4.E8X.3.2.2.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E8X.3.2.2.m1.1.1.1.1.1.1.1.2" xref="S4.E8X.3.2.2.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E8X.3.2.2.m1.1.1.1.1.1.1.1.1" xref="S4.E8X.3.2.2.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E8X.3.2.2.m1.1.1.1.1.1.1.1.1.2" xref="S4.E8X.3.2.2.m1.1.1.1.1.1.1.1.1.2.cmml">i</mi><mo id="S4.E8X.3.2.2.m1.1.1.1.1.1.1.1.1.1" xref="S4.E8X.3.2.2.m1.1.1.1.1.1.1.1.1.1.cmml">−</mo><msub id="S4.E8X.3.2.2.m1.1.1.1.1.1.1.1.1.3" xref="S4.E8X.3.2.2.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E8X.3.2.2.m1.1.1.1.1.1.1.1.1.3.2" xref="S4.E8X.3.2.2.m1.1.1.1.1.1.1.1.1.3.2.cmml">μ</mi><mi id="S4.E8X.3.2.2.m1.1.1.1.1.1.1.1.1.3.3" xref="S4.E8X.3.2.2.m1.1.1.1.1.1.1.1.1.3.3.cmml">I</mi></msub></mrow><mo stretchy="false" id="S4.E8X.3.2.2.m1.1.1.1.1.1.1.1.3" xref="S4.E8X.3.2.2.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S4.E8X.3.2.2.m1.1.1.1.1.1.2" xref="S4.E8X.3.2.2.m1.1.1.1.1.1.2.cmml">/</mo><msub id="S4.E8X.3.2.2.m1.1.1.1.1.1.3" xref="S4.E8X.3.2.2.m1.1.1.1.1.1.3.cmml"><mi id="S4.E8X.3.2.2.m1.1.1.1.1.1.3.2" xref="S4.E8X.3.2.2.m1.1.1.1.1.1.3.2.cmml">σ</mi><mi id="S4.E8X.3.2.2.m1.1.1.1.1.1.3.3" xref="S4.E8X.3.2.2.m1.1.1.1.1.1.3.3.cmml">I</mi></msub></mrow></mrow><mo id="S4.E8X.3.2.2.m1.1.1.1.2" xref="S4.E8X.3.2.2.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E8X.3.2.2.m1.1b"><apply id="S4.E8X.3.2.2.m1.1.1.1.1.cmml" xref="S4.E8X.3.2.2.m1.1.1.1"><eq id="S4.E8X.3.2.2.m1.1.1.1.1.2.cmml" xref="S4.E8X.3.2.2.m1.1.1.1.1.2"></eq><csymbol cd="latexml" id="S4.E8X.3.2.2.m1.1.1.1.1.3.cmml" xref="S4.E8X.3.2.2.m1.1.1.1.1.3">absent</csymbol><apply id="S4.E8X.3.2.2.m1.1.1.1.1.1.cmml" xref="S4.E8X.3.2.2.m1.1.1.1.1.1"><divide id="S4.E8X.3.2.2.m1.1.1.1.1.1.2.cmml" xref="S4.E8X.3.2.2.m1.1.1.1.1.1.2"></divide><apply id="S4.E8X.3.2.2.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.E8X.3.2.2.m1.1.1.1.1.1.1.1"><minus id="S4.E8X.3.2.2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E8X.3.2.2.m1.1.1.1.1.1.1.1.1.1"></minus><ci id="S4.E8X.3.2.2.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E8X.3.2.2.m1.1.1.1.1.1.1.1.1.2">𝑖</ci><apply id="S4.E8X.3.2.2.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E8X.3.2.2.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E8X.3.2.2.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E8X.3.2.2.m1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E8X.3.2.2.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E8X.3.2.2.m1.1.1.1.1.1.1.1.1.3.2">𝜇</ci><ci id="S4.E8X.3.2.2.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E8X.3.2.2.m1.1.1.1.1.1.1.1.1.3.3">𝐼</ci></apply></apply><apply id="S4.E8X.3.2.2.m1.1.1.1.1.1.3.cmml" xref="S4.E8X.3.2.2.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E8X.3.2.2.m1.1.1.1.1.1.3.1.cmml" xref="S4.E8X.3.2.2.m1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E8X.3.2.2.m1.1.1.1.1.1.3.2.cmml" xref="S4.E8X.3.2.2.m1.1.1.1.1.1.3.2">𝜎</ci><ci id="S4.E8X.3.2.2.m1.1.1.1.1.1.3.3.cmml" xref="S4.E8X.3.2.2.m1.1.1.1.1.1.3.3">𝐼</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E8X.3.2.2.m1.1c">\displaystyle=(i-\mu_{I})/\sigma_{I},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="2" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(8)</span></td>
</tr>
<tr id="S4.E8Xa" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.E8Xa.2.1.1.m1.1" class="ltx_Math" alttext="\displaystyle p_{m}" display="inline"><semantics id="S4.E8Xa.2.1.1.m1.1a"><msub id="S4.E8Xa.2.1.1.m1.1.1" xref="S4.E8Xa.2.1.1.m1.1.1.cmml"><mi id="S4.E8Xa.2.1.1.m1.1.1.2" xref="S4.E8Xa.2.1.1.m1.1.1.2.cmml">p</mi><mi id="S4.E8Xa.2.1.1.m1.1.1.3" xref="S4.E8Xa.2.1.1.m1.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S4.E8Xa.2.1.1.m1.1b"><apply id="S4.E8Xa.2.1.1.m1.1.1.cmml" xref="S4.E8Xa.2.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.E8Xa.2.1.1.m1.1.1.1.cmml" xref="S4.E8Xa.2.1.1.m1.1.1">subscript</csymbol><ci id="S4.E8Xa.2.1.1.m1.1.1.2.cmml" xref="S4.E8Xa.2.1.1.m1.1.1.2">𝑝</ci><ci id="S4.E8Xa.2.1.1.m1.1.1.3.cmml" xref="S4.E8Xa.2.1.1.m1.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E8Xa.2.1.1.m1.1c">\displaystyle p_{m}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S4.E8Xa.3.2.2.m1.5" class="ltx_Math" alttext="\displaystyle=(p_{m}-(\mu_{P})_{m})/(\sigma_{P})_{m},m\notin\{x,y,z,t\}," display="inline"><semantics id="S4.E8Xa.3.2.2.m1.5a"><mrow id="S4.E8Xa.3.2.2.m1.5.5.1"><mrow id="S4.E8Xa.3.2.2.m1.5.5.1.1.2" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.3.cmml"><mrow id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.cmml"><mi id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.4" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.4.cmml"></mi><mo id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.3" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.3.cmml">=</mo><mrow id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.2" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.2.cmml"><mrow id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.2" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.cmml"><msub id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.3" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.3.2" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.3.2.cmml">p</mi><mi id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.3.3" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.3.3.cmml">m</mi></msub><mo id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.2" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.2.cmml">−</mo><msub id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.1" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.1.cmml"><mrow id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">μ</mi><mi id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">P</mi></msub><mo stretchy="false" id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mi id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.1.3" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.1.3.cmml">m</mi></msub></mrow><mo stretchy="false" id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.3" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.2.3" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.2.3.cmml">/</mo><msub id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.2.2" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.2.2.cmml"><mrow id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.2.2.1.1" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.2.2.1.1.1.cmml"><mo stretchy="false" id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.2.2.1.1.2" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.2.2.1.1.1.cmml">(</mo><msub id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.2.2.1.1.1" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.2.2.1.1.1.cmml"><mi id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.2.2.1.1.1.2" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.2.2.1.1.1.2.cmml">σ</mi><mi id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.2.2.1.1.1.3" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.2.2.1.1.1.3.cmml">P</mi></msub><mo stretchy="false" id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.2.2.1.1.3" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.2.2.1.1.1.cmml">)</mo></mrow><mi id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.2.2.3" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.2.2.3.cmml">m</mi></msub></mrow></mrow><mo id="S4.E8Xa.3.2.2.m1.5.5.1.1.2.3" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.3a.cmml">,</mo><mrow id="S4.E8Xa.3.2.2.m1.5.5.1.1.2.2" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.2.2.cmml"><mi id="S4.E8Xa.3.2.2.m1.5.5.1.1.2.2.2" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.2.2.2.cmml">m</mi><mo id="S4.E8Xa.3.2.2.m1.5.5.1.1.2.2.1" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.2.2.1.cmml">∉</mo><mrow id="S4.E8Xa.3.2.2.m1.5.5.1.1.2.2.3.2" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.2.2.3.1.cmml"><mo stretchy="false" id="S4.E8Xa.3.2.2.m1.5.5.1.1.2.2.3.2.1" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.2.2.3.1.cmml">{</mo><mi id="S4.E8Xa.3.2.2.m1.1.1" xref="S4.E8Xa.3.2.2.m1.1.1.cmml">x</mi><mo id="S4.E8Xa.3.2.2.m1.5.5.1.1.2.2.3.2.2" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.2.2.3.1.cmml">,</mo><mi id="S4.E8Xa.3.2.2.m1.2.2" xref="S4.E8Xa.3.2.2.m1.2.2.cmml">y</mi><mo id="S4.E8Xa.3.2.2.m1.5.5.1.1.2.2.3.2.3" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.2.2.3.1.cmml">,</mo><mi id="S4.E8Xa.3.2.2.m1.3.3" xref="S4.E8Xa.3.2.2.m1.3.3.cmml">z</mi><mo id="S4.E8Xa.3.2.2.m1.5.5.1.1.2.2.3.2.4" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.2.2.3.1.cmml">,</mo><mi id="S4.E8Xa.3.2.2.m1.4.4" xref="S4.E8Xa.3.2.2.m1.4.4.cmml">t</mi><mo stretchy="false" id="S4.E8Xa.3.2.2.m1.5.5.1.1.2.2.3.2.5" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.2.2.3.1.cmml">}</mo></mrow></mrow></mrow><mo id="S4.E8Xa.3.2.2.m1.5.5.1.2">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E8Xa.3.2.2.m1.5b"><apply id="S4.E8Xa.3.2.2.m1.5.5.1.1.3.cmml" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.2"><csymbol cd="ambiguous" id="S4.E8Xa.3.2.2.m1.5.5.1.1.3a.cmml" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.2.3">formulae-sequence</csymbol><apply id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.cmml" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1"><eq id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.3.cmml" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.3"></eq><csymbol cd="latexml" id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.4.cmml" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.4">absent</csymbol><apply id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.2.cmml" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.2"><divide id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.2.3.cmml" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.2.3"></divide><apply id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.cmml" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1"><minus id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.2"></minus><apply id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.3.2">𝑝</ci><ci id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.3.3">𝑚</ci></apply><apply id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.2">𝜇</ci><ci id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.3">𝑃</ci></apply><ci id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.1.1.1.1.1.3">𝑚</ci></apply></apply><apply id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.2.2.cmml" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.2.2.2.cmml" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.2.2">subscript</csymbol><apply id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.2.2.1.1.1.cmml" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.2.2.1.1"><csymbol cd="ambiguous" id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.2.2.1.1.1.1.cmml" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.2.2.1.1">subscript</csymbol><ci id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.2.2.1.1.1.2.cmml" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.2.2.1.1.1.2">𝜎</ci><ci id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.2.2.1.1.1.3.cmml" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.2.2.1.1.1.3">𝑃</ci></apply><ci id="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.2.2.3.cmml" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.1.1.2.2.3">𝑚</ci></apply></apply></apply><apply id="S4.E8Xa.3.2.2.m1.5.5.1.1.2.2.cmml" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.2.2"><notin id="S4.E8Xa.3.2.2.m1.5.5.1.1.2.2.1.cmml" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.2.2.1"></notin><ci id="S4.E8Xa.3.2.2.m1.5.5.1.1.2.2.2.cmml" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.2.2.2">𝑚</ci><set id="S4.E8Xa.3.2.2.m1.5.5.1.1.2.2.3.1.cmml" xref="S4.E8Xa.3.2.2.m1.5.5.1.1.2.2.3.2"><ci id="S4.E8Xa.3.2.2.m1.1.1.cmml" xref="S4.E8Xa.3.2.2.m1.1.1">𝑥</ci><ci id="S4.E8Xa.3.2.2.m1.2.2.cmml" xref="S4.E8Xa.3.2.2.m1.2.2">𝑦</ci><ci id="S4.E8Xa.3.2.2.m1.3.3.cmml" xref="S4.E8Xa.3.2.2.m1.3.3">𝑧</ci><ci id="S4.E8Xa.3.2.2.m1.4.4.cmml" xref="S4.E8Xa.3.2.2.m1.4.4">𝑡</ci></set></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E8Xa.3.2.2.m1.5c">\displaystyle=(p_{m}-(\mu_{P})_{m})/(\sigma_{P})_{m},m\notin\{x,y,z,t\},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p id="S4.SS2.p5.10" class="ltx_p">where <math id="S4.SS2.p5.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.SS2.p5.1.m1.1a"><mi id="S4.SS2.p5.1.m1.1.1" xref="S4.SS2.p5.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.1.m1.1b"><ci id="S4.SS2.p5.1.m1.1.1.cmml" xref="S4.SS2.p5.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.1.m1.1c">i</annotation></semantics></math> is the RGB value of an image pixel, and <math id="S4.SS2.p5.2.m2.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S4.SS2.p5.2.m2.1a"><mi id="S4.SS2.p5.2.m2.1.1" xref="S4.SS2.p5.2.m2.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.2.m2.1b"><ci id="S4.SS2.p5.2.m2.1.1.cmml" xref="S4.SS2.p5.2.m2.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.2.m2.1c">p</annotation></semantics></math> is the original feature of a radar point; <math id="S4.SS2.p5.3.m3.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S4.SS2.p5.3.m3.1a"><mi id="S4.SS2.p5.3.m3.1.1" xref="S4.SS2.p5.3.m3.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.3.m3.1b"><ci id="S4.SS2.p5.3.m3.1.1.cmml" xref="S4.SS2.p5.3.m3.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.3.m3.1c">I</annotation></semantics></math> and <math id="S4.SS2.p5.4.m4.1" class="ltx_Math" alttext="P" display="inline"><semantics id="S4.SS2.p5.4.m4.1a"><mi id="S4.SS2.p5.4.m4.1.1" xref="S4.SS2.p5.4.m4.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.4.m4.1b"><ci id="S4.SS2.p5.4.m4.1.1.cmml" xref="S4.SS2.p5.4.m4.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.4.m4.1c">P</annotation></semantics></math> represent the whole set of image pixels and radar points, and the subscript <math id="S4.SS2.p5.5.m5.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S4.SS2.p5.5.m5.1a"><mi id="S4.SS2.p5.5.m5.1.1" xref="S4.SS2.p5.5.m5.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.5.m5.1b"><ci id="S4.SS2.p5.5.m5.1.1.cmml" xref="S4.SS2.p5.5.m5.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.5.m5.1c">m</annotation></semantics></math> denotes the corresponding feature vector component. <math id="S4.SS2.p5.6.m6.1" class="ltx_Math" alttext="\mu_{X}" display="inline"><semantics id="S4.SS2.p5.6.m6.1a"><msub id="S4.SS2.p5.6.m6.1.1" xref="S4.SS2.p5.6.m6.1.1.cmml"><mi id="S4.SS2.p5.6.m6.1.1.2" xref="S4.SS2.p5.6.m6.1.1.2.cmml">μ</mi><mi id="S4.SS2.p5.6.m6.1.1.3" xref="S4.SS2.p5.6.m6.1.1.3.cmml">X</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.6.m6.1b"><apply id="S4.SS2.p5.6.m6.1.1.cmml" xref="S4.SS2.p5.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS2.p5.6.m6.1.1.1.cmml" xref="S4.SS2.p5.6.m6.1.1">subscript</csymbol><ci id="S4.SS2.p5.6.m6.1.1.2.cmml" xref="S4.SS2.p5.6.m6.1.1.2">𝜇</ci><ci id="S4.SS2.p5.6.m6.1.1.3.cmml" xref="S4.SS2.p5.6.m6.1.1.3">𝑋</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.6.m6.1c">\mu_{X}</annotation></semantics></math> and <math id="S4.SS2.p5.7.m7.1" class="ltx_Math" alttext="\sigma_{X}" display="inline"><semantics id="S4.SS2.p5.7.m7.1a"><msub id="S4.SS2.p5.7.m7.1.1" xref="S4.SS2.p5.7.m7.1.1.cmml"><mi id="S4.SS2.p5.7.m7.1.1.2" xref="S4.SS2.p5.7.m7.1.1.2.cmml">σ</mi><mi id="S4.SS2.p5.7.m7.1.1.3" xref="S4.SS2.p5.7.m7.1.1.3.cmml">X</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.7.m7.1b"><apply id="S4.SS2.p5.7.m7.1.1.cmml" xref="S4.SS2.p5.7.m7.1.1"><csymbol cd="ambiguous" id="S4.SS2.p5.7.m7.1.1.1.cmml" xref="S4.SS2.p5.7.m7.1.1">subscript</csymbol><ci id="S4.SS2.p5.7.m7.1.1.2.cmml" xref="S4.SS2.p5.7.m7.1.1.2">𝜎</ci><ci id="S4.SS2.p5.7.m7.1.1.3.cmml" xref="S4.SS2.p5.7.m7.1.1.3">𝑋</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.7.m7.1c">\sigma_{X}</annotation></semantics></math> is the mean and standard deviation of set <math id="S4.SS2.p5.8.m8.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S4.SS2.p5.8.m8.1a"><mi id="S4.SS2.p5.8.m8.1.1" xref="S4.SS2.p5.8.m8.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.8.m8.1b"><ci id="S4.SS2.p5.8.m8.1.1.cmml" xref="S4.SS2.p5.8.m8.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.8.m8.1c">X</annotation></semantics></math> (<math id="S4.SS2.p5.9.m9.1" class="ltx_Math" alttext="X=I" display="inline"><semantics id="S4.SS2.p5.9.m9.1a"><mrow id="S4.SS2.p5.9.m9.1.1" xref="S4.SS2.p5.9.m9.1.1.cmml"><mi id="S4.SS2.p5.9.m9.1.1.2" xref="S4.SS2.p5.9.m9.1.1.2.cmml">X</mi><mo id="S4.SS2.p5.9.m9.1.1.1" xref="S4.SS2.p5.9.m9.1.1.1.cmml">=</mo><mi id="S4.SS2.p5.9.m9.1.1.3" xref="S4.SS2.p5.9.m9.1.1.3.cmml">I</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.9.m9.1b"><apply id="S4.SS2.p5.9.m9.1.1.cmml" xref="S4.SS2.p5.9.m9.1.1"><eq id="S4.SS2.p5.9.m9.1.1.1.cmml" xref="S4.SS2.p5.9.m9.1.1.1"></eq><ci id="S4.SS2.p5.9.m9.1.1.2.cmml" xref="S4.SS2.p5.9.m9.1.1.2">𝑋</ci><ci id="S4.SS2.p5.9.m9.1.1.3.cmml" xref="S4.SS2.p5.9.m9.1.1.3">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.9.m9.1c">X=I</annotation></semantics></math> or <math id="S4.SS2.p5.10.m10.1" class="ltx_Math" alttext="P" display="inline"><semantics id="S4.SS2.p5.10.m10.1a"><mi id="S4.SS2.p5.10.m10.1.1" xref="S4.SS2.p5.10.m10.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.10.m10.1b"><ci id="S4.SS2.p5.10.m10.1.1.cmml" xref="S4.SS2.p5.10.m10.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.10.m10.1c">P</annotation></semantics></math>), respectively. In addition, radar points and ground-truth bounding boxes outside the image view are filtered out to ensure data consistency. This is achieved by projecting radar points and centers of bounding boxes onto the image plane and discarding those falling outside the image. Moreover, random horizontal flipping is applied as a data augmentation technique for both input data and BEV features.</p>
</div>
<div id="S4.SS2.p6" class="ltx_para">
<p id="S4.SS2.p6.1" class="ltx_p">The model is trained for 80 epochs using the AdamW optimizer and StepLR scheduler. The batch size is set to 6, and the initial learning rate is set to 1e-3. It is important to note that the image backbone and neck are loaded from a pre-trained model, and their parameters are frozen to prevent overfitting. The pre-trained model is from MMDetection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">59</span></a>]</cite>, which is YOLOX <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">52</span></a>]</cite> trained on the COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">60</span></a>]</cite> dataset for the 2D object detection task.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.5.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.6.2" class="ltx_text ltx_font_italic">Results and Analysis</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p"><span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_bold">Results on VoD:</span> The experimental results on the VoD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite> validation set are presented in Table <a href="#S4.T2" title="TABLE II ‣ IV-A Dataset and Evaluation Metrics ‣ IV Experiments and Analysis ‣ LXL: LiDAR Excluded Lean 3D Object Detection with 4D Imaging Radar and Camera Fusion" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>.
We also present the detection accuracy of LXL-R, which is our single-modal baseline without the image branch, occupancy net, and fusion module.
The RoI AP for cars and cyclists is relatively high for LXL-R, indicating that 4D radar alone is effective in perceiving the environment at close range.
However, the RoI AP for pedestrians is limited due to two main reasons. Firstly, pedestrians are small in the BEV representation, often occupying only a single grid or even a fraction of it, making it challenging for the network to accurately regress bounding boxes.
Additionally, millimeter waves have weak reflections on non-metallic objects, resulting in sparse and less accurate measurements from pedestrians.
Another observation is that the radar-modal-only model performs poorly in terms of EAA AP for all categories, highlighting the challenges of detecting far-away objects due to the sparsity and noise in radar points.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">Upon fusing camera images with radar data, the detection results of different models are improved, particularly in the EAA metric and on pedestrians and cyclists.
These improvements suggest that dense images with rich semantic information can compensate for the sparsity and noise in radar points, enhancing radar perception for objects that are porous, non-metallic, or located at a distance.
Compared to RCFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite>, the latest benchmark on 3D object detection with 4D imaging radar and camera fusion, our LXL model achieves higher detection accuracy across almost all categories and evaluation regions.
This indicates that the proposed “radar occupancy-assisted depth-based samping” strategy is able to achieve precise image view transformation and amplifies the effectiveness of fusion with images.
As our LXL utilize a center-based detection head that is skilled in detecting small objects <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite> while RCFusion adopts an anchor-based head, the performance improvement is higher on pedestrians (+10.5%) and cyclists (+8.8%) than that on cars (+0.6%).
FUTR3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">57</span></a>]</cite> and BEVFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">58</span></a>]</cite> are also implemented on VoD according to the code on their official github, getting 7% lower results than LXL as these methods are not specially designed for 3D points from 4D radars.
It is interesting to notice that FUTR3D outperforms LXL in the car category. This can be attributed to the sampling strategy in its transformer decoder head that samples features from the <math id="S4.SS3.p2.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S4.SS3.p2.1.m1.1a"><mi id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><ci id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">K</annotation></semantics></math> nearest radar points. However, when the object size is small, there are often not enough points to be sampled, and the sampled features will be polluted by other irrelevant sampled points. Thus, the detection accuracy of pedestrians and cyclists is limited.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">The inference speed of LXL and some other models are also listed in Table <a href="#S4.T2" title="TABLE II ‣ IV-A Dataset and Evaluation Metrics ‣ IV Experiments and Analysis ‣ LXL: LiDAR Excluded Lean 3D Object Detection with 4D Imaging Radar and Camera Fusion" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, measured on a single V100 GPU. The radar modal-only model, LXL-R, can detect objects in real time, while multi-modal methods all require more than 100ms (FPS <math id="S4.SS3.p3.1.m1.1" class="ltx_Math" alttext="&lt;10" display="inline"><semantics id="S4.SS3.p3.1.m1.1a"><mrow id="S4.SS3.p3.1.m1.1.1" xref="S4.SS3.p3.1.m1.1.1.cmml"><mi id="S4.SS3.p3.1.m1.1.1.2" xref="S4.SS3.p3.1.m1.1.1.2.cmml"></mi><mo id="S4.SS3.p3.1.m1.1.1.1" xref="S4.SS3.p3.1.m1.1.1.1.cmml">&lt;</mo><mn id="S4.SS3.p3.1.m1.1.1.3" xref="S4.SS3.p3.1.m1.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.1b"><apply id="S4.SS3.p3.1.m1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1"><lt id="S4.SS3.p3.1.m1.1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1.1"></lt><csymbol cd="latexml" id="S4.SS3.p3.1.m1.1.1.2.cmml" xref="S4.SS3.p3.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S4.SS3.p3.1.m1.1.1.3.cmml" xref="S4.SS3.p3.1.m1.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">&lt;10</annotation></semantics></math>) to generate predictions due to the introduction of images. Although LXL runs slightly slower than FUTR3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">57</span></a>]</cite> and BEVFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">58</span></a>]</cite>, the performance gain of more than 7% makes it relatively acceptable.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p">Fig. <a href="#S4.F3" title="Figure 3 ‣ IV-A Dataset and Evaluation Metrics ‣ IV Experiments and Analysis ‣ LXL: LiDAR Excluded Lean 3D Object Detection with 4D Imaging Radar and Camera Fusion" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> showcases the visualization results of our LXL model, demonstrating its accurate detection of various object classes. The predictions of LXL-R are also provided for comparison.
Notably, in some cases, LXL even detects true objects that are not labeled (e.g., the bottom-right cyclist in the second column of the image).
Moreover, when the radar point cloud is sparse, LXL has the ability to leverage camera information to detect objects that are missed by LXL-R. It is also able to utilize radar measurements to detect objects occluded in the camera view. Therefore, our model effectively leverages the advantages of both modalities to reduce missed detections and improve detection accuracy.</p>
</div>
<div id="S4.SS3.p5" class="ltx_para">
<p id="S4.SS3.p5.1" class="ltx_p">To compare the performance of LiDAR-based 3D object detection and 4D radar-based 3D object detection, Table <a href="#S4.T3" title="TABLE III ‣ IV-C Results and Analysis ‣ IV Experiments and Analysis ‣ LXL: LiDAR Excluded Lean 3D Object Detection with 4D Imaging Radar and Camera Fusion" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> exhibits the experimental results of LXL and LiDAR-based PointPillars <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite>. As expected, the detection accuracy of PointPillars-LiDAR is 19.1% higher than that of LXL-R, because LiDAR points are much denser and less noisy than 4D radar points, as mentioned in Section <a href="#S1" title="I Introduction ‣ LXL: LiDAR Excluded Lean 3D Object Detection with 4D Imaging Radar and Camera Fusion" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>. However, when fused 4D radar points with camera images, the performance gap is significantly narrowed to 9.6%, and LXL even outperforms PointPillars-LiDAR in the cyclist category. This result shows that the mutual compensation of low-cost sensors can make 4D radar-camera fusion-based methods have the potential to replace high-cost LiDARs in some aspect.</p>
</div>
<div id="S4.SS3.p6" class="ltx_para">
<p id="S4.SS3.p6.1" class="ltx_p"><span id="S4.SS3.p6.1.1" class="ltx_text ltx_font_bold">Results on TJ4DRadSet:</span> To evaluate the generalization ability of our proposed model, we conduct additional experiments on the TJ4DRadSet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite> dataset. Table <a href="#S4.T4" title="TABLE IV ‣ IV-C Results and Analysis ‣ IV Experiments and Analysis ‣ LXL: LiDAR Excluded Lean 3D Object Detection with 4D Imaging Radar and Camera Fusion" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> presents the performance of different methods on the test set of TJ4DRadSet, where LXL outperforms other 4D radar-camera fusion-based models by more than 2.4% in 3D mAP. Fig. <a href="#S4.F4" title="Figure 4 ‣ IV-C Results and Analysis ‣ IV Experiments and Analysis ‣ LXL: LiDAR Excluded Lean 3D Object Detection with 4D Imaging Radar and Camera Fusion" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> provides visualizations of the detection results in various scenarios. These results demonstrate the effectiveness of our model in fusing radar and camera information for 3D object detection, even under challenging lighting conditions such as darkness or excessive illumination.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Comparison between classical LiDAR-based PointPillars (the result is inherited from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">61</span></a>]</cite>) and 4D imaging radar and camera fusion-based LXL on the validation set of VoD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite>, where L denotes LiDAR, R denotes 4D imaging radar, C indicates camera.</figcaption>
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T3.1.1" class="ltx_tr">
<td id="S4.T3.1.1.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S4.T3.1.1.1.1" class="ltx_text">Method</span></td>
<td id="S4.T3.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T3.1.1.2.1" class="ltx_text">Modality</span></td>
<td id="S4.T3.1.1.3" class="ltx_td ltx_align_center ltx_border_t" colspan="4">AP in the Entire Annotated Area (%)</td>
</tr>
<tr id="S4.T3.1.2" class="ltx_tr">
<td id="S4.T3.1.2.1" class="ltx_td ltx_align_center ltx_border_t">Car</td>
<td id="S4.T3.1.2.2" class="ltx_td ltx_align_center ltx_border_t">Pedestrian</td>
<td id="S4.T3.1.2.3" class="ltx_td ltx_align_center ltx_border_t">Cyclist</td>
<td id="S4.T3.1.2.4" class="ltx_td ltx_align_center ltx_border_t">mAP</td>
</tr>
<tr id="S4.T3.1.3" class="ltx_tr">
<td id="S4.T3.1.3.1" class="ltx_td ltx_align_center ltx_border_t">LXL - R</td>
<td id="S4.T3.1.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">R</td>
<td id="S4.T3.1.3.3" class="ltx_td ltx_align_center ltx_border_t">32.8</td>
<td id="S4.T3.1.3.4" class="ltx_td ltx_align_center ltx_border_t">39.6</td>
<td id="S4.T3.1.3.5" class="ltx_td ltx_align_center ltx_border_t">68.1</td>
<td id="S4.T3.1.3.6" class="ltx_td ltx_align_center ltx_border_t">46.8</td>
</tr>
<tr id="S4.T3.1.4" class="ltx_tr">
<td id="S4.T3.1.4.1" class="ltx_td ltx_align_center">LXL (<span id="S4.T3.1.4.1.1" class="ltx_text ltx_font_bold">Ours</span>)</td>
<td id="S4.T3.1.4.2" class="ltx_td ltx_align_center ltx_border_r">R+C</td>
<td id="S4.T3.1.4.3" class="ltx_td ltx_align_center">42.3</td>
<td id="S4.T3.1.4.4" class="ltx_td ltx_align_center">49.5</td>
<td id="S4.T3.1.4.5" class="ltx_td ltx_align_center"><span id="S4.T3.1.4.5.1" class="ltx_text ltx_font_bold">77.1</span></td>
<td id="S4.T3.1.4.6" class="ltx_td ltx_align_center">56.3</td>
</tr>
<tr id="S4.T3.1.5" class="ltx_tr">
<td id="S4.T3.1.5.1" class="ltx_td ltx_align_center ltx_border_b">PointPillars (CVPR 2019) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite>
</td>
<td id="S4.T3.1.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">L</td>
<td id="S4.T3.1.5.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T3.1.5.3.1" class="ltx_text ltx_font_bold">66.6</span></td>
<td id="S4.T3.1.5.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T3.1.5.4.1" class="ltx_text ltx_font_bold">56.1</span></td>
<td id="S4.T3.1.5.5" class="ltx_td ltx_align_center ltx_border_b">75.1</td>
<td id="S4.T3.1.5.6" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T3.1.5.6.1" class="ltx_text ltx_font_bold">65.9</span></td>
</tr>
</table>
</figure>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>Comparison with state-of-the-art methods on the test set of TJ4DRadSet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>, where R denotes 4D imaging radar and C indicates camera. The results of methods marked with † are inherited from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite>. Note that BEVFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">58</span></a>]</cite> is implemented according to the radar + camera configuration file on its official github.</figcaption>
<table id="S4.T4.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T4.3.4" class="ltx_tr">
<td id="S4.T4.3.4.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T4.3.4.1.1" class="ltx_text" style="font-size:70%;">Method</span></td>
<td id="S4.T4.3.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T4.3.4.2.1" class="ltx_text" style="font-size:70%;">Modality</span></td>
<td id="S4.T4.3.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T4.3.4.3.1" class="ltx_text" style="font-size:70%;">3D mAP (%)</span></td>
<td id="S4.T4.3.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T4.3.4.4.1" class="ltx_text" style="font-size:70%;">BEV mAP (%)</span></td>
</tr>
<tr id="S4.T4.1.1" class="ltx_tr">
<td id="S4.T4.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;">
<span id="S4.T4.1.1.1.1" class="ltx_text" style="font-size:70%;">RPFA-Net</span><sup id="S4.T4.1.1.1.2" class="ltx_sup"><span id="S4.T4.1.1.1.2.1" class="ltx_text ltx_font_italic" style="font-size:70%;">†</span></sup><span id="S4.T4.1.1.1.3" class="ltx_text" style="font-size:70%;"> (ITSC 2021) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T4.1.1.1.4.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a><span id="S4.T4.1.1.1.5.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S4.T4.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T4.1.1.2.1" class="ltx_text" style="font-size:70%;">R</span></td>
<td id="S4.T4.1.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T4.1.1.3.1" class="ltx_text" style="font-size:70%;">29.91</span></td>
<td id="S4.T4.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T4.1.1.4.1" class="ltx_text" style="font-size:70%;">38.94</span></td>
</tr>
<tr id="S4.T4.2.2" class="ltx_tr">
<td id="S4.T4.2.2.1" class="ltx_td ltx_align_center" style="padding-top:0.7pt;padding-bottom:0.7pt;">
<span id="S4.T4.2.2.1.1" class="ltx_text" style="font-size:70%;">RadarPillarNet</span><sup id="S4.T4.2.2.1.2" class="ltx_sup"><span id="S4.T4.2.2.1.2.1" class="ltx_text ltx_font_italic" style="font-size:70%;">†</span></sup><span id="S4.T4.2.2.1.3" class="ltx_text" style="font-size:70%;"> (T-IM 2023) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T4.2.2.1.4.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a><span id="S4.T4.2.2.1.5.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S4.T4.2.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T4.2.2.2.1" class="ltx_text" style="font-size:70%;">R</span></td>
<td id="S4.T4.2.2.3" class="ltx_td ltx_align_center" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T4.2.2.3.1" class="ltx_text" style="font-size:70%;">30.37</span></td>
<td id="S4.T4.2.2.4" class="ltx_td ltx_align_center" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T4.2.2.4.1" class="ltx_text" style="font-size:70%;">39.24</span></td>
</tr>
<tr id="S4.T4.3.5" class="ltx_tr">
<td id="S4.T4.3.5.1" class="ltx_td ltx_align_center" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T4.3.5.1.1" class="ltx_text" style="font-size:70%;">LXL - R</span></td>
<td id="S4.T4.3.5.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T4.3.5.2.1" class="ltx_text" style="font-size:70%;">R</span></td>
<td id="S4.T4.3.5.3" class="ltx_td ltx_align_center" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T4.3.5.3.1" class="ltx_text" style="font-size:70%;">30.79</span></td>
<td id="S4.T4.3.5.4" class="ltx_td ltx_align_center" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T4.3.5.4.1" class="ltx_text" style="font-size:70%;">38.42</span></td>
</tr>
<tr id="S4.T4.3.6" class="ltx_tr">
<td id="S4.T4.3.6.1" class="ltx_td ltx_align_center" style="padding-top:0.7pt;padding-bottom:0.7pt;">
<span id="S4.T4.3.6.1.1" class="ltx_text" style="font-size:70%;">FUTR3D (CVPR 2023) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T4.3.6.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib57" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">57</span></a><span id="S4.T4.3.6.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S4.T4.3.6.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T4.3.6.2.1" class="ltx_text" style="font-size:70%;">R+C</span></td>
<td id="S4.T4.3.6.3" class="ltx_td ltx_align_center" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T4.3.6.3.1" class="ltx_text" style="font-size:70%;">32.42</span></td>
<td id="S4.T4.3.6.4" class="ltx_td ltx_align_center" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T4.3.6.4.1" class="ltx_text" style="font-size:70%;">37.51</span></td>
</tr>
<tr id="S4.T4.3.7" class="ltx_tr">
<td id="S4.T4.3.7.1" class="ltx_td ltx_align_center" style="padding-top:0.7pt;padding-bottom:0.7pt;">
<span id="S4.T4.3.7.1.1" class="ltx_text" style="font-size:70%;">BEVFusion (ICRA 2023) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T4.3.7.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">58</span></a><span id="S4.T4.3.7.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S4.T4.3.7.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T4.3.7.2.1" class="ltx_text" style="font-size:70%;">R+C</span></td>
<td id="S4.T4.3.7.3" class="ltx_td ltx_align_center" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T4.3.7.3.1" class="ltx_text" style="font-size:70%;">32.71</span></td>
<td id="S4.T4.3.7.4" class="ltx_td ltx_align_center" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T4.3.7.4.1" class="ltx_text" style="font-size:70%;">41.12</span></td>
</tr>
<tr id="S4.T4.3.3" class="ltx_tr">
<td id="S4.T4.3.3.1" class="ltx_td ltx_align_center" style="padding-top:0.7pt;padding-bottom:0.7pt;">
<span id="S4.T4.3.3.1.1" class="ltx_text" style="font-size:70%;">RCFusion</span><sup id="S4.T4.3.3.1.2" class="ltx_sup"><span id="S4.T4.3.3.1.2.1" class="ltx_text ltx_font_italic" style="font-size:70%;">†</span></sup><span id="S4.T4.3.3.1.3" class="ltx_text" style="font-size:70%;"> (T-IM 2023) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T4.3.3.1.4.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a><span id="S4.T4.3.3.1.5.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S4.T4.3.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T4.3.3.2.1" class="ltx_text" style="font-size:70%;">R+C</span></td>
<td id="S4.T4.3.3.3" class="ltx_td ltx_align_center" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T4.3.3.3.1" class="ltx_text" style="font-size:70%;">33.85</span></td>
<td id="S4.T4.3.3.4" class="ltx_td ltx_align_center" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T4.3.3.4.1" class="ltx_text" style="font-size:70%;">39.76</span></td>
</tr>
<tr id="S4.T4.3.8" class="ltx_tr">
<td id="S4.T4.3.8.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;">
<span id="S4.T4.3.8.1.1" class="ltx_text" style="font-size:70%;">LXL (</span><span id="S4.T4.3.8.1.2" class="ltx_text ltx_font_bold" style="font-size:70%;">Ours</span><span id="S4.T4.3.8.1.3" class="ltx_text" style="font-size:70%;">)</span>
</td>
<td id="S4.T4.3.8.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T4.3.8.2.1" class="ltx_text" style="font-size:70%;">R+C</span></td>
<td id="S4.T4.3.8.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T4.3.8.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">36.32</span></td>
<td id="S4.T4.3.8.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T4.3.8.4.1" class="ltx_text ltx_font_bold" style="font-size:70%;">41.20</span></td>
</tr>
</table>
</figure>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE V: </span>The performances of our model on different lighting conditions. Note that there are no pedestrians in the “dark” subset, and the AP is considered as 0 in the computation of the mAP metric.</figcaption>
<table id="S4.T5.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T5.1.1" class="ltx_tr">
<td id="S4.T5.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T5.1.1.1.1" class="ltx_text">Model</span></td>
<td id="S4.T5.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3">3D mAP (%)</td>
<td id="S4.T5.1.1.3" class="ltx_td ltx_align_center ltx_border_t" colspan="3">BEV mAP (%)</td>
</tr>
<tr id="S4.T5.1.2" class="ltx_tr">
<td id="S4.T5.1.2.1" class="ltx_td ltx_align_center ltx_border_t">Dark</td>
<td id="S4.T5.1.2.2" class="ltx_td ltx_align_center ltx_border_t">Standard</td>
<td id="S4.T5.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Shiny</td>
<td id="S4.T5.1.2.4" class="ltx_td ltx_align_center ltx_border_t">Dark</td>
<td id="S4.T5.1.2.5" class="ltx_td ltx_align_center ltx_border_t">Standard</td>
<td id="S4.T5.1.2.6" class="ltx_td ltx_align_center ltx_border_t">Shiny</td>
</tr>
<tr id="S4.T5.1.3" class="ltx_tr">
<td id="S4.T5.1.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">LXL-R</td>
<td id="S4.T5.1.3.2" class="ltx_td ltx_align_center ltx_border_t">20.17</td>
<td id="S4.T5.1.3.3" class="ltx_td ltx_align_center ltx_border_t">30.76</td>
<td id="S4.T5.1.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.1.3.4.1" class="ltx_text ltx_font_bold">22.68</span></td>
<td id="S4.T5.1.3.5" class="ltx_td ltx_align_center ltx_border_t">22.36</td>
<td id="S4.T5.1.3.6" class="ltx_td ltx_align_center ltx_border_t">35.47</td>
<td id="S4.T5.1.3.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.3.7.1" class="ltx_text ltx_font_bold">36.66</span></td>
</tr>
<tr id="S4.T5.1.4" class="ltx_tr">
<td id="S4.T5.1.4.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">LXL</td>
<td id="S4.T5.1.4.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T5.1.4.2.1" class="ltx_text ltx_font_bold">22.50</span></td>
<td id="S4.T5.1.4.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T5.1.4.3.1" class="ltx_text ltx_font_bold">45.74</span></td>
<td id="S4.T5.1.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">20.33</td>
<td id="S4.T5.1.4.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T5.1.4.5.1" class="ltx_text ltx_font_bold">25.19</span></td>
<td id="S4.T5.1.4.6" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T5.1.4.6.1" class="ltx_text ltx_font_bold">49.55</span></td>
<td id="S4.T5.1.4.7" class="ltx_td ltx_align_center ltx_border_b">28.13</td>
</tr>
</table>
</figure>
<figure id="S4.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE VI: </span>The evaluating results of our model on different distances of objects.</figcaption>
<table id="S4.T6.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T6.1.1" class="ltx_tr">
<td id="S4.T6.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T6.1.1.1.1" class="ltx_text">Model</span></td>
<td id="S4.T6.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3">3D mAP (%)</td>
<td id="S4.T6.1.1.3" class="ltx_td ltx_align_center ltx_border_t" colspan="3">BEV mAP (%)</td>
</tr>
<tr id="S4.T6.1.2" class="ltx_tr">
<td id="S4.T6.1.2.1" class="ltx_td ltx_align_center ltx_border_t">0-25m</td>
<td id="S4.T6.1.2.2" class="ltx_td ltx_align_center ltx_border_t">25-50m</td>
<td id="S4.T6.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">50-70m</td>
<td id="S4.T6.1.2.4" class="ltx_td ltx_align_center ltx_border_t">0-25m</td>
<td id="S4.T6.1.2.5" class="ltx_td ltx_align_center ltx_border_t">25-50m</td>
<td id="S4.T6.1.2.6" class="ltx_td ltx_align_center ltx_border_t">50-70m</td>
</tr>
<tr id="S4.T6.1.3" class="ltx_tr">
<td id="S4.T6.1.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">LXL-R</td>
<td id="S4.T6.1.3.2" class="ltx_td ltx_align_center ltx_border_t">36.62</td>
<td id="S4.T6.1.3.3" class="ltx_td ltx_align_center ltx_border_t">20.93</td>
<td id="S4.T6.1.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">9.48</td>
<td id="S4.T6.1.3.5" class="ltx_td ltx_align_center ltx_border_t">43.69</td>
<td id="S4.T6.1.3.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.1.3.6.1" class="ltx_text ltx_font_bold">28.68</span></td>
<td id="S4.T6.1.3.7" class="ltx_td ltx_align_center ltx_border_t">14.81</td>
</tr>
<tr id="S4.T6.1.4" class="ltx_tr">
<td id="S4.T6.1.4.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">LXL</td>
<td id="S4.T6.1.4.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T6.1.4.2.1" class="ltx_text ltx_font_bold">47.30</span></td>
<td id="S4.T6.1.4.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T6.1.4.3.1" class="ltx_text ltx_font_bold">22.00</span></td>
<td id="S4.T6.1.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T6.1.4.4.1" class="ltx_text ltx_font_bold">14.55</span></td>
<td id="S4.T6.1.4.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T6.1.4.5.1" class="ltx_text ltx_font_bold">51.69</span></td>
<td id="S4.T6.1.4.6" class="ltx_td ltx_align_center ltx_border_b">27.08</td>
<td id="S4.T6.1.4.7" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T6.1.4.7.1" class="ltx_text ltx_font_bold">17.85</span></td>
</tr>
</table>
</figure>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2307.00724/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="379" height="372" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Some visualization results on the TJ4DRadSet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite> test set (best viewed in color and zoom). Each column corresponds to a frame of data containing an image and radar points (gray points) in BEV, where the red triangle denotes the position of the ego-vehicle, and orange boxes represent ground-truths. Blue boxes in the second row stand for predicted bounding boxes from LXL. Note that in the third row, the detection results of LXL-R are also shown as green boxes for comparison.</figcaption>
</figure>
<div id="S4.SS3.p7" class="ltx_para">
<p id="S4.SS3.p7.1" class="ltx_p">To further investigate the influence of lighting conditions and object distances on our LXL model, we analyze the detection results on TJ4DRadSet.
Specifically, we divide the test set into three subsets based on the brightness of the scenarios: dark, standard, and over-illuminated (referred to as “Shiny” in Table <a href="#S4.T5" title="TABLE V ‣ IV-C Results and Analysis ‣ IV Experiments and Analysis ‣ LXL: LiDAR Excluded Lean 3D Object Detection with 4D Imaging Radar and Camera Fusion" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>). These subsets account for approximately 15%, 60%, and 25% of the entire test set, respectively.
We report the detection accuracy on these subsets in Table <a href="#S4.T5" title="TABLE V ‣ IV-C Results and Analysis ‣ IV Experiments and Analysis ‣ LXL: LiDAR Excluded Lean 3D Object Detection with 4D Imaging Radar and Camera Fusion" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>. To mitigate the influence of road conditions across subsets, we also include the performance of LXL-R, which is less affected by lighting conditions, in the table.
By comparing the results of LXL with LXL-R on the same subset, we observe that image information is beneficial in normal lighting conditions, as expected.</p>
</div>
<div id="S4.SS3.p8" class="ltx_para">
<p id="S4.SS3.p8.1" class="ltx_p">Interestingly, even in dark scenarios, fusion with images a performance gain of 2.3% on the 3D mAP because the headlights and taillights of vehicles provide valuable cues for object classification and localization. However, in cases of excessive illumination, the performance deteriorates due to unclear images under such conditions.
To address this issue, a simple rule-based approach could be employed, such as switching to LXL-R with a single 4D imaging radar mode when the image quality illumination factor falls below a certain threshold.
As there are few studies about camera and 4D radar fusion-based 3D object detection, we aim to improve the overall performance here, and robustness against image quality degradation is not the primary focus of this work. Improving model robustness will be a subject of our future research.</p>
</div>
<div id="S4.SS3.p9" class="ltx_para">
<p id="S4.SS3.p9.1" class="ltx_p">Furthermore, we evaluate our model on objects at different distances from the ego-vehicle and present the results in Table <a href="#S4.T6" title="TABLE VI ‣ IV-C Results and Analysis ‣ IV Experiments and Analysis ‣ LXL: LiDAR Excluded Lean 3D Object Detection with 4D Imaging Radar and Camera Fusion" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a>. The LXL model exhibits higher detection accuracy than LXL-R for objects at almost all distances, and the performance decreases as the distance increases due to the sparsity of radar points. Moreover, as the semantic information from images aids in identifying objects at a distance, there are fewer missed detections and more TPs for far-away objects. In contrast, the radar-only modality demonstrates some ability to detect objects at medium range, and the introduction of images primarily improves bounding box regression accuracy. Since the number of TPs significantly impacts the AP, long-range objects benefit more from multi-modal fusion than medium-range objects.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS4.5.1.1" class="ltx_text">IV-D</span> </span><span id="S4.SS4.6.2" class="ltx_text ltx_font_italic">Ablation Study</span>
</h3>

<figure id="S4.T7" class="ltx_table">

<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE VII: </span>Ablation studies on image feature lifting and radar assistance. Experiments are conducted on VoD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite> dataset.
</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.T7.1" class="ltx_p ltx_figure_panel ltx_align_center">[b]

<span id="S4.T7.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T7.1.1.1" class="ltx_tr">
<span id="S4.T7.1.1.1.1" class="ltx_td ltx_border_t"></span>
<span id="S4.T7.1.1.1.2" class="ltx_td ltx_border_r ltx_border_t"></span>
<span id="S4.T7.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_colspan ltx_colspan_4">AP in the Entire Annotated Area (%)</span>
<span id="S4.T7.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_4">AP in the Region of Interest (%)</span></span>
<span id="S4.T7.1.1.2" class="ltx_tr">
<span id="S4.T7.1.1.2.1" class="ltx_td ltx_align_center"><span id="S4.T7.1.1.2.1.1" class="ltx_text">
<span id="S4.T7.1.1.2.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T7.1.1.2.1.1.1.1" class="ltx_tr">
<span id="S4.T7.1.1.2.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Image Feature</span></span>
<span id="S4.T7.1.1.2.1.1.1.2" class="ltx_tr">
<span id="S4.T7.1.1.2.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Lifting Method</span></span>
</span></span></span>
<span id="S4.T7.1.1.2.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T7.1.1.2.2.1" class="ltx_text">Radar Assistance</span></span>
<span id="S4.T7.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t">Car</span>
<span id="S4.T7.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t">Pedestrian</span>
<span id="S4.T7.1.1.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Cyclist</span>
<span id="S4.T7.1.1.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">mAP</span>
<span id="S4.T7.1.1.2.7" class="ltx_td ltx_align_center ltx_border_t">Car</span>
<span id="S4.T7.1.1.2.8" class="ltx_td ltx_align_center ltx_border_t">Pedestrian</span>
<span id="S4.T7.1.1.2.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Cyclist</span>
<span id="S4.T7.1.1.2.10" class="ltx_td ltx_align_center ltx_border_t">mAP</span></span>
<span id="S4.T7.1.1.3" class="ltx_tr">
<span id="S4.T7.1.1.3.1" class="ltx_td ltx_align_center ltx_border_t">Splatting</span>
<span id="S4.T7.1.1.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">None</span>
<span id="S4.T7.1.1.3.3" class="ltx_td ltx_align_center ltx_border_t">38.05</span>
<span id="S4.T7.1.1.3.4" class="ltx_td ltx_align_center ltx_border_t">45.01</span>
<span id="S4.T7.1.1.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">68.12</span>
<span id="S4.T7.1.1.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">50.39</span>
<span id="S4.T7.1.1.3.7" class="ltx_td ltx_align_center ltx_border_t">70.73</span>
<span id="S4.T7.1.1.3.8" class="ltx_td ltx_align_center ltx_border_t">54.92</span>
<span id="S4.T7.1.1.3.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">87.28</span>
<span id="S4.T7.1.1.3.10" class="ltx_td ltx_align_center ltx_border_t">70.98</span></span>
<span id="S4.T7.1.1.4" class="ltx_tr">
<span id="S4.T7.1.1.4.1" class="ltx_td ltx_align_center">Sampling</span>
<span id="S4.T7.1.1.4.2" class="ltx_td ltx_align_center ltx_border_r">None</span>
<span id="S4.T7.1.1.4.3" class="ltx_td ltx_align_center"><span id="S4.T7.1.1.4.3.1" class="ltx_text ltx_font_bold">42.63</span></span>
<span id="S4.T7.1.1.4.4" class="ltx_td ltx_align_center">45.74</span>
<span id="S4.T7.1.1.4.5" class="ltx_td ltx_align_center ltx_border_r">74.30</span>
<span id="S4.T7.1.1.4.6" class="ltx_td ltx_align_center ltx_border_r">54.22</span>
<span id="S4.T7.1.1.4.7" class="ltx_td ltx_align_center">71.66</span>
<span id="S4.T7.1.1.4.8" class="ltx_td ltx_align_center">52.05</span>
<span id="S4.T7.1.1.4.9" class="ltx_td ltx_align_center ltx_border_r">87.09</span>
<span id="S4.T7.1.1.4.10" class="ltx_td ltx_align_center">70.27</span></span>
<span id="S4.T7.1.1.5" class="ltx_tr">
<span id="S4.T7.1.1.5.1" class="ltx_td ltx_align_center">Sampling</span>
<span id="S4.T7.1.1.5.2" class="ltx_td ltx_align_center ltx_border_r">Depth Supervise</span>
<span id="S4.T7.1.1.5.3" class="ltx_td ltx_align_center">41.79</span>
<span id="S4.T7.1.1.5.4" class="ltx_td ltx_align_center">48.71</span>
<span id="S4.T7.1.1.5.5" class="ltx_td ltx_align_center ltx_border_r">74.65</span>
<span id="S4.T7.1.1.5.6" class="ltx_td ltx_align_center ltx_border_r">55.05</span>
<span id="S4.T7.1.1.5.7" class="ltx_td ltx_align_center">71.05</span>
<span id="S4.T7.1.1.5.8" class="ltx_td ltx_align_center">53.93</span>
<span id="S4.T7.1.1.5.9" class="ltx_td ltx_align_center ltx_border_r">88.18</span>
<span id="S4.T7.1.1.5.10" class="ltx_td ltx_align_center">71.05</span></span>
<span id="S4.T7.1.1.6" class="ltx_tr">
<span id="S4.T7.1.1.6.1" class="ltx_td ltx_align_center">Sampling</span>
<span id="S4.T7.1.1.6.2" class="ltx_td ltx_align_center ltx_border_r">3D Occupancy Grids (CRN)<sup id="S4.T7.1.1.6.2.1" class="ltx_sup">*</sup></span>
<span id="S4.T7.1.1.6.3" class="ltx_td ltx_align_center">42.01</span>
<span id="S4.T7.1.1.6.4" class="ltx_td ltx_align_center">45.86</span>
<span id="S4.T7.1.1.6.5" class="ltx_td ltx_align_center ltx_border_r">76.07</span>
<span id="S4.T7.1.1.6.6" class="ltx_td ltx_align_center ltx_border_r">54.65</span>
<span id="S4.T7.1.1.6.7" class="ltx_td ltx_align_center">71.00</span>
<span id="S4.T7.1.1.6.8" class="ltx_td ltx_align_center">50.70</span>
<span id="S4.T7.1.1.6.9" class="ltx_td ltx_align_center ltx_border_r">87.52</span>
<span id="S4.T7.1.1.6.10" class="ltx_td ltx_align_center">69.74</span></span>
<span id="S4.T7.1.1.7" class="ltx_tr">
<span id="S4.T7.1.1.7.1" class="ltx_td ltx_align_center ltx_border_b">Sampling</span>
<span id="S4.T7.1.1.7.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">3D Occupancy Grids (<span id="S4.T7.1.1.7.2.1" class="ltx_text ltx_font_bold">Ours</span>)</span>
<span id="S4.T7.1.1.7.3" class="ltx_td ltx_align_center ltx_border_b">42.33</span>
<span id="S4.T7.1.1.7.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T7.1.1.7.4.1" class="ltx_text ltx_font_bold">49.48</span></span>
<span id="S4.T7.1.1.7.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T7.1.1.7.5.1" class="ltx_text ltx_font_bold">77.12</span></span>
<span id="S4.T7.1.1.7.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T7.1.1.7.6.1" class="ltx_text ltx_font_bold">56.31</span></span>
<span id="S4.T7.1.1.7.7" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T7.1.1.7.7.1" class="ltx_text ltx_font_bold">72.18</span></span>
<span id="S4.T7.1.1.7.8" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T7.1.1.7.8.1" class="ltx_text ltx_font_bold">58.30</span></span>
<span id="S4.T7.1.1.7.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T7.1.1.7.9.1" class="ltx_text ltx_font_bold">88.31</span></span>
<span id="S4.T7.1.1.7.10" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T7.1.1.7.10.1" class="ltx_text ltx_font_bold">72.93</span></span></span>
</span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<ul id="S4.I1" class="ltx_itemize ltx_centering ltx_figure_panel">
<li id="S4.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">*</span> 
<div id="S4.I1.ix1.p1" class="ltx_para">
<p id="S4.I1.ix1.p1.1" class="ltx_p">Note: Since additional elevation could be measured by 4D imaging radar but ordinary automotive radar used in CRN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite> can only measure range and azimuth of objects, 3D occupancy grids rather than the 2D occupancy maps used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite> are employed.</p>
</div>
</li>
</ul>
</div>
</div>
</figure>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2307.00724/assets/x5.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="180" height="110" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The illustration of <span id="S4.F5.2.1" class="ltx_text ltx_font_italic">the basic idea</span> of “splatting” strategy. For ease of understanding, it is shown in the overhead view. As each image pixel is transformed into radar coordinate system with indefinite depth, it corresponds to a ray in 3D space, named pixel ray. The feature of a BEV grid is related to that of a pixel, if the BEV projection of the pixel ray passes through the grid. Otherwise, the BEV grid is empty. In practice, descretized depth values are predefined and each image pixel only corresponds to several points on the pixel ray, leading to more empty grids in the far distance.</figcaption>
</figure>
<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">In this subsection, we perform several experiments to validate the effectiveness of key design choices in our model on the VoD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite> dataset. Specifically, we focus on two aspects: the image feature lifting strategy and the utilization of radar in the image branch.
We investigate the commonly used geometrical feature lifting strategies, “sampling” and “splatting”, as described in Section <a href="#S3.SS4" title="III-D View Transformation ‣ III Proposed Method ‣ LXL: LiDAR Excluded Lean 3D Object Detection with 4D Imaging Radar and Camera Fusion" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-D</span></span></a>. For the “splatting” process, we follow the implementation approach in LSS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite>.
Table <a href="#S4.T7" title="TABLE VII ‣ IV-D Ablation Study ‣ IV Experiments and Analysis ‣ LXL: LiDAR Excluded Lean 3D Object Detection with 4D Imaging Radar and Camera Fusion" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VII</span></a> presents results of these experiments. While “sampling” exhibits slightly lower performance in terms of RoI AP compared to “splatting”, it significantly outperforms “splatting” in terms of EAA APs. It suggests that while “splatting” may have a slight advantage at short distances, its performance deteriorates significantly as the distance increases, leading to lower performance compared to “sampling” in a broader range.
This observation can be attributed to sparsity of splatted point clouds with increasing distance. After pillarization, a considerable number of far-away BEV grids may remain empty, as illustrated in Fig. <a href="#S4.F5" title="Figure 5 ‣ IV-D Ablation Study ‣ IV Experiments and Analysis ‣ LXL: LiDAR Excluded Lean 3D Object Detection with 4D Imaging Radar and Camera Fusion" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. In contrast, “sampling” ensures that each 3D voxel is associated with a sampled image feature, as long as the corresponding grid falls within the camera view. Consequently, “sampling” proves to be more effective in capturing information across a wide range.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">Regarding radar assistance in the image branch, we compare our “radar occupancy-assisted sampling” method with two alternative approaches. One alternative, referred to as “Depth Supervise” in Table <a href="#S4.T7" title="TABLE VII ‣ IV-D Ablation Study ‣ IV Experiments and Analysis ‣ LXL: LiDAR Excluded Lean 3D Object Detection with 4D Imaging Radar and Camera Fusion" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VII</span></a>, is similar to the approach used in BEVDepth <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite>. It leverages radar points to generate supervision signals for image depth distribution prediction. Specifically, the radar points are first transformed into the image coordinate system. Subsequently, for each projected radar point, we identify the nearest pixel and assign the radar depth as the ground-truth depth for that pixel.
In cases where multiple radar points correspond to a single pixel, we compute the average depth to determine its ground-truth value.
However, we find that reducing the depth loss during training using this method is challenging. This difficulty arises due to the inherent noisiness and sparsity of radar points.
The noise in the radar measurements leads to inaccuracies in the derived ground-truth depths, while the sparsity of radar points poses challenges for the convergence of the depth estimation network.
Consequently, this alternative method only yields a slight improvement in detection accuracy compared to the approach without radar assistance.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p">An alternative approach involves generating radar 3D occupancy grids by following CRN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite>, namely “3D Occupancy Grids (CRN)” in Table <a href="#S4.T7" title="TABLE VII ‣ IV-D Ablation Study ‣ IV Experiments and Analysis ‣ LXL: LiDAR Excluded Lean 3D Object Detection with 4D Imaging Radar and Camera Fusion" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VII</span></a>, which differs from our method. For “3D Occupancy Grids (CRN)”, the radar points are projected onto the image plane and voxelized to match the shape of the image depth distribution maps. Subsequently, sparse convolutions are employed to generate radar 3D occupancy grids in the image coordinate system, and tri-linear sampling is applied for image-to-radar coordinate transformation. It is important to note two differences between the aforementioned method and the original approach in CRN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite>. Firstly, since the radar points come from 4D radars and contain height information, 3D occupancy grids are generated instead of 2D occupancy maps. Secondly, the feature lifting method here is “sampling” rather than “splatting”, so the occupancy grids need to be resampled to transform back to the radar coordinate system before being multiplied with the 3D image features. Nonetheless, the underlying idea is the same as CRN.</p>
</div>
<div id="S4.SS4.p4" class="ltx_para">
<p id="S4.SS4.p4.1" class="ltx_p">Compared to the “Depth Supervise” approach discussed earlier, the performance of the “3D Occupancy Grids (CRN)” method is even more significantly affected by the sparsity of radar points, due to the larger number of empty grids in 3D space. Furthermore, this method requires a time-consuming projection and voxelization process, whereas our approach only relies on a simple occupancy net to predict radar 3D occupancy grids in the radar coordinate system directly. Consequently, our “radar occupancy-assisted sampling” strategy offers performance and inference speed advantages.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, a new camera and 4D imaging radar fusion model, namely LXL, is proposed for 3D object detection. It is shown that LXL outperforms existing works by a large margin, mainly because its elaborate “radar occupancy-assisted depth-based sampling” view transformation strategy can effectively transform image PV features into BEV with the aid of predicted image depth distribution maps and radar 3D occupancy grids. This design demonstrates that there is a large room for improving the “sampling” strategy, where a small enhancement can boost the view transformation significantly.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">The proposed LXL provides a framework capable of inspiring subsequent researches in camera and 4D imaging radar fusion-based 3D object detection.
Future works includes improving the robustness of LXL via attention-based transformer to achieve adaptive interaction between modals, and its applications in subsequent planning and control tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">62</span></a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">63</span></a>]</cite>.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib1.2.2.1" class="ltx_text" style="font-size:90%;">[1]</span></span>
<span class="ltx_bibblock"><span id="bib.bib1.4.1" class="ltx_text" style="font-size:90%;"> J. Liu, W. Xiong, L. Bai, Y. Xia, T. Huang, W. Ouyang, and B. Zhu, “Deep instance segmentation with automotive radar detection points,” </span><em id="bib.bib1.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Intelligent Vehicles</em><span id="bib.bib1.6.3" class="ltx_text" style="font-size:90%;">, vol. 8, no. 1, pp. 84-94, 2023.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib2.2.2.1" class="ltx_text" style="font-size:90%;">[2]</span></span>
<span class="ltx_bibblock"><span id="bib.bib2.4.1" class="ltx_text" style="font-size:90%;"> W. Xiong, J. Liu, Y. Xia, T. Huang, B. Zhu, and W. Xiang, “Contrastive learning for automotive mmWave radar detection points based instance segmentation,” in </span><em id="bib.bib2.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Intelligent Transportation Systems (ITSC)</em><span id="bib.bib2.6.3" class="ltx_text" style="font-size:90%;">, 2022, pp. 1255-1261.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib3.2.2.1" class="ltx_text" style="font-size:90%;">[3]</span></span>
<span class="ltx_bibblock"><span id="bib.bib3.4.1" class="ltx_text" style="font-size:90%;"> Y. Yang, J. Liu, T. Huang, Q.-L. Han, G. Ma, and B. Zhu, “RaLiBEV: Radar and LiDAR BEV Fusion Learning for Anchor Box Free Object Detection Systems,” 2022, </span><em id="bib.bib3.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv:2211.06108.</em><span id="bib.bib3.6.3" class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib4.2.2.1" class="ltx_text" style="font-size:90%;">[4]</span></span>
<span class="ltx_bibblock"><span id="bib.bib4.4.1" class="ltx_text" style="font-size:90%;">
J. Liu, L. Bai, Y. Xia, T. Huang, B. Zhu, and Q.-L. Han, “GNN-PMB: A simple but effective online 3D multi-object tracker without bells and whistles,” </span><em id="bib.bib4.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Intelligent Vehicles</em><span id="bib.bib4.6.3" class="ltx_text" style="font-size:90%;">, vol. 8, no. 2, pp. 1176-1189, 2023.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib5.2.2.1" class="ltx_text" style="font-size:90%;">[5]</span></span>
<span class="ltx_bibblock"><span id="bib.bib5.4.1" class="ltx_text" style="font-size:90%;">
P. Sun, S. Li, B. Zhu, Z. Zuo, X. Xia, “Vision-Based Fixed-Time Uncooperative Aerial Target Tracking for UAV,” </span><em id="bib.bib5.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE/CAA Journal of Automatica Sinica</em><span id="bib.bib5.6.3" class="ltx_text" style="font-size:90%;">, vol. 10, no. 5, pp. 1322-1324, 2023.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib6.2.2.1" class="ltx_text" style="font-size:90%;">[6]</span></span>
<span class="ltx_bibblock"><span id="bib.bib6.4.1" class="ltx_text" style="font-size:90%;">
J. Mao, S. Shi, X. Wang, and H. Li, “3D object detection for autonomous driving: A review and new outlooks,” </span><em id="bib.bib6.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Journal of Computer Vision (IJCV)</em><span id="bib.bib6.6.3" class="ltx_text" style="font-size:90%;">, vol. 131, pp. 1909–1963, 2023.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib7.2.2.1" class="ltx_text" style="font-size:90%;">[7]</span></span>
<span class="ltx_bibblock"><span id="bib.bib7.4.1" class="ltx_text" style="font-size:90%;">
S. Sun, A. P. Petropulu, and H. V. Poor, “MIMO radar for advanced driver-assistance systems and autonomous driving: Advantages and challenges,” </span><em id="bib.bib7.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Signal Processing Magazine</em><span id="bib.bib7.6.3" class="ltx_text" style="font-size:90%;">, vol. 37, no. 4, pp. 98–117, 2020.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib8.2.2.1" class="ltx_text" style="font-size:90%;">[8]</span></span>
<span class="ltx_bibblock"><span id="bib.bib8.4.1" class="ltx_text" style="font-size:90%;">
A. Caillot, S. Ouerghi, P. Vasseur, R. Boutteau, and Y. Dupuis, “Survey on cooperative perception in an automotive context,” </span><em id="bib.bib8.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Intelligent Transportation Systems</em><span id="bib.bib8.6.3" class="ltx_text" style="font-size:90%;">, vol. 23, no. 9, pp. 14204-14223, 2022.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib9.2.2.1" class="ltx_text" style="font-size:90%;">[9]</span></span>
<span class="ltx_bibblock"><span id="bib.bib9.4.1" class="ltx_text" style="font-size:90%;">
Z. Han, J. Wang, Z. Xu, S. Yang, L. He, S. Xu, and J. Wang, “4D millimeter-wave radar in autonomous driving: A survey,” 2023, </span><em id="bib.bib9.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv:2306.04242.</em><span id="bib.bib9.6.3" class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib10.2.2.1" class="ltx_text" style="font-size:90%;">[10]</span></span>
<span class="ltx_bibblock"><span id="bib.bib10.4.1" class="ltx_text" style="font-size:90%;">
A. Palffy, E. Pool, S. Baratam, J. F. Kooij, and D. M. Gavrila, “Multi-class road user detection with 3+1D radar in the View-of-Delft dataset,” </span><em id="bib.bib10.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Robotics and Automation Letters</em><span id="bib.bib10.6.3" class="ltx_text" style="font-size:90%;">, vol. 7, no. 2, pp. 4961–4968, 2022.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib11.2.2.1" class="ltx_text" style="font-size:90%;">[11]</span></span>
<span class="ltx_bibblock"><span id="bib.bib11.4.1" class="ltx_text" style="font-size:90%;">
B. Xu, X. Zhang, L. Wang, X. Hu, Z. Li, S. Pan, J. Li, and Y. Deng, “RPFA-Net: A 4D radar pillar feature attention network for 3D object detection,” in </span><em id="bib.bib11.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Intelligent Transportation Systems Conference (ITSC)</em><span id="bib.bib11.6.3" class="ltx_text" style="font-size:90%;">, 2021, pp. 3061–3066.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib12.2.2.1" class="ltx_text" style="font-size:90%;">[12]</span></span>
<span class="ltx_bibblock"><span id="bib.bib12.4.1" class="ltx_text" style="font-size:90%;">
B. Tan, Z. Ma, X. Zhu, S. Li, L. Zheng, S. Chen, L. Huang, and J. Bai, “3D object detection for multi-frame 4D automotive millimeter-wave radar point cloud,” </span><em id="bib.bib12.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Sensors Journal</em><span id="bib.bib12.6.3" class="ltx_text" style="font-size:90%;">, 2022, doi: 10.1109/JSEN.2022.3219643.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib13.2.2.1" class="ltx_text" style="font-size:90%;">[13]</span></span>
<span class="ltx_bibblock"><span id="bib.bib13.4.1" class="ltx_text" style="font-size:90%;">
M. Drobnitzky, J. Friederich, B. Egger, and P. Zschech, “Survey and systematization of 3D object detection models and methods,” </span><em id="bib.bib13.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">The Visual Computer</em><span id="bib.bib13.6.3" class="ltx_text" style="font-size:90%;">, pp. 1-47, 2023.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib14.2.2.1" class="ltx_text" style="font-size:90%;">[14]</span></span>
<span class="ltx_bibblock"><span id="bib.bib14.4.1" class="ltx_text" style="font-size:90%;">
Y. Ma, T. Wang, X. Bai, H. Yang, Y. Hou, Y. Wang, Y. Qiao, R. Yang, D. Manocha, and X. Zhu, “Vision-centric BEV perception: A survey,” 2022, </span><em id="bib.bib14.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv:2208.02797.</em><span id="bib.bib14.6.3" class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib15.2.2.1" class="ltx_text" style="font-size:90%;">[15]</span></span>
<span class="ltx_bibblock"><span id="bib.bib15.4.1" class="ltx_text" style="font-size:90%;">
C. Reading, A. Harakeh, J. Chae, and S. L. Waslander, “Categorical depth distribution network for monocular 3D object detection,” in </span><em id="bib.bib15.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em><span id="bib.bib15.6.3" class="ltx_text" style="font-size:90%;">, 2021, pp. 8555–8564.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib16.2.2.1" class="ltx_text" style="font-size:90%;">[16]</span></span>
<span class="ltx_bibblock"><span id="bib.bib16.4.1" class="ltx_text" style="font-size:90%;">
J. Huang, G. Huang, Z. Zhu, and D. Du, “BEVDet: High-performance multi-camera 3D object detection in bird-eye-view,” 2023, </span><em id="bib.bib16.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv:2112.11790.</em><span id="bib.bib16.6.3" class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib17.2.2.1" class="ltx_text" style="font-size:90%;">[17]</span></span>
<span class="ltx_bibblock"><span id="bib.bib17.4.1" class="ltx_text" style="font-size:90%;">
E. Xie, Z. Yu, D. Zhou, J. Philion, A. Anandkumar, S. Fidler, P. Luo, and J. M. Alvarez, “M2BEV: Multi-camera joint 3D detection and segmentation with unified birds-eye view representation,” 2022, </span><em id="bib.bib17.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv:2204.05088.</em><span id="bib.bib17.6.3" class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib18.2.2.1" class="ltx_text" style="font-size:90%;">[18]</span></span>
<span class="ltx_bibblock"><span id="bib.bib18.4.1" class="ltx_text" style="font-size:90%;">
Z. Li, W. Wang, H. Li, E. Xie, C. Sima, T. Lu, Y. Qiao, and J. Dai, “BEVFormer: Learning bird’s-eye-view representation from multi-camera images via spatio-temporal transformers,” in </span><em id="bib.bib18.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 17th European Conference on Computer Vision (ECCV)</em><span id="bib.bib18.6.3" class="ltx_text" style="font-size:90%;">. Springer, 2022, pp. 1–18.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib19.2.2.1" class="ltx_text" style="font-size:90%;">[19]</span></span>
<span class="ltx_bibblock"><span id="bib.bib19.4.1" class="ltx_text" style="font-size:90%;">
Y. Jiang, L. Zhang, Z. Miao, X. Zhu, J. Gao, W. Hu, and Y.-G. Jiang, “PolarFormer: Multi-camera 3D object detection with polar transformers,” 2022, </span><em id="bib.bib19.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv:2206.15398.</em><span id="bib.bib19.6.3" class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib20.2.2.1" class="ltx_text" style="font-size:90%;">[20]</span></span>
<span class="ltx_bibblock"><span id="bib.bib20.4.1" class="ltx_text" style="font-size:90%;">
J. Philion and S. Fidler, “Lift, Splat, Shoot: Encoding images from arbitrary camera rigs by implicitly unprojecting to 3D,” in </span><em id="bib.bib20.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 16th European Conference on Computer Vision (ECCV)</em><span id="bib.bib20.6.3" class="ltx_text" style="font-size:90%;">. Springer, 2020, pp. 194–210.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib21.2.2.1" class="ltx_text" style="font-size:90%;">[21]</span></span>
<span class="ltx_bibblock"><span id="bib.bib21.4.1" class="ltx_text" style="font-size:90%;">
Y. Li, Z. Ge, G. Yu, J. Yang, Z. Wang, Y. Shi, J. Sun, and Z. Li, “BEVDepth: Acquisition of reliable depth for multi-view 3D object detection,” 2022, </span><em id="bib.bib21.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv:2206.10092.</em><span id="bib.bib21.6.3" class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib22.2.2.1" class="ltx_text" style="font-size:90%;">[22]</span></span>
<span class="ltx_bibblock"><span id="bib.bib22.4.1" class="ltx_text" style="font-size:90%;">
Y. Kim, S. Kim, J. Shin, J. W. Choi, and D. Kum, “CRN: Camera radar net for accurate, robust, efficient 3D perception,” in
</span><em id="bib.bib22.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em><span id="bib.bib22.6.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib23.2.2.1" class="ltx_text" style="font-size:90%;">[23]</span></span>
<span class="ltx_bibblock"><span id="bib.bib23.4.1" class="ltx_text" style="font-size:90%;">
A. W. Harley, Z. Fang, J. Li, R. Ambrus, and K. Fragkiadaki, “Simple-BEV: What really matters for multi-sensor BEV perception?” in </span><em id="bib.bib23.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)</em><span id="bib.bib23.6.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib24.2.2.1" class="ltx_text" style="font-size:90%;">[24]</span></span>
<span class="ltx_bibblock"><span id="bib.bib24.4.1" class="ltx_text" style="font-size:90%;">
L. Zheng, Z. Ma, X. Zhu, B. Tan, S. Li, K. Long, W. Sun, S. Chen, L. Zhang, M. Wan, et al., “TJ4DRadSet: A 4D radar dataset for autonomous driving,” in </span><em id="bib.bib24.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE 25th International Conference on Intelligent Transportation Systems (ITSC)</em><span id="bib.bib24.6.3" class="ltx_text" style="font-size:90%;">. IEEE, 2022, pp. 493–498.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib25.2.2.1" class="ltx_text" style="font-size:90%;">[25]</span></span>
<span class="ltx_bibblock"><span id="bib.bib25.4.1" class="ltx_text" style="font-size:90%;">
G. Brazil and X. Liu, “M3D-RPN: Monocular 3D region proposal network for object detection,” in </span><em id="bib.bib25.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em><span id="bib.bib25.6.3" class="ltx_text" style="font-size:90%;">, 2019, pp. 9287–9296.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib26.2.2.1" class="ltx_text" style="font-size:90%;">[26]</span></span>
<span class="ltx_bibblock"><span id="bib.bib26.4.1" class="ltx_text" style="font-size:90%;">
H. A. Mallot, H. H. Bülthoff, J. Little, and S. Bohrer, “Inverse perspective mapping simplifies optical flow computation and obstacle detection,” </span><em id="bib.bib26.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Biological Cybernetics</em><span id="bib.bib26.6.3" class="ltx_text" style="font-size:90%;">, vol. 64, no. 3, pp. 177–185, 1991.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib27.2.2.1" class="ltx_text" style="font-size:90%;">[27]</span></span>
<span class="ltx_bibblock"><span id="bib.bib27.4.1" class="ltx_text" style="font-size:90%;">
Y. Wang, W.-L. Chao, D. Garg, B. Hariharan, M. Campbell, and K. Q. Weinberger, “Pseudo-LiDAR from visual depth estimation: Bridging the gap in 3D object detection for autonomous driving,” in </span><em id="bib.bib27.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em><span id="bib.bib27.6.3" class="ltx_text" style="font-size:90%;">, 2019, pp. 8445–8453.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib28.2.2.1" class="ltx_text" style="font-size:90%;">[28]</span></span>
<span class="ltx_bibblock"><span id="bib.bib28.4.1" class="ltx_text" style="font-size:90%;">
T.-Y. Lim, A. Ansari, B. Major, D. Fontijne, M. Hamilton, R. Gowaikar, and S. Subramanian, “Radar and camera early fusion for vehicle detection in advanced driver assistance systems,” in </span><em id="bib.bib28.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 33rd Conference on Neural Information Processing Systems (NeurIPS), Machine Learning for Autonomous Driving Workshop</em><span id="bib.bib28.6.3" class="ltx_text" style="font-size:90%;">, vol. 2, 2019, p. 7.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib29.2.2.1" class="ltx_text" style="font-size:90%;">[29]</span></span>
<span class="ltx_bibblock"><span id="bib.bib29.4.1" class="ltx_text" style="font-size:90%;">
R. Nabati and H. Qi, “Radar-camera sensor fusion for joint object detection and distance estimation in autonomous vehicles,” 2020, </span><em id="bib.bib29.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv:2009.08428.</em><span id="bib.bib29.6.3" class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib30.2.2.1" class="ltx_text" style="font-size:90%;">[30]</span></span>
<span class="ltx_bibblock"><span id="bib.bib30.4.1" class="ltx_text" style="font-size:90%;">
R. Nabati and H. Qi, “CenterFusion: Center-based radar and camera fusion for 3D object detection,” in </span><em id="bib.bib30.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</em><span id="bib.bib30.6.3" class="ltx_text" style="font-size:90%;">, 2021, pp. 1527–1536.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib31.2.2.1" class="ltx_text" style="font-size:90%;">[31]</span></span>
<span class="ltx_bibblock"><span id="bib.bib31.4.1" class="ltx_text" style="font-size:90%;">
S. Yao, R. Guan, X. Huang, Z. Li, X. Sha, Y. Yue, E. G. Lim, H. Seo, K. L. Man, X. Zhu, and Y. Yue, “Radar-camera fusion for object detection and semantic segmentation in autonomous driving: A comprehensive review,” </span><em id="bib.bib31.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Intelligent Vehicles</em><span id="bib.bib31.6.3" class="ltx_text" style="font-size:90%;">, 2023. doi: 10.1109/TIV.2023.3307157.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib32.2.2.1" class="ltx_text" style="font-size:90%;">[32]</span></span>
<span class="ltx_bibblock"><span id="bib.bib32.4.1" class="ltx_text" style="font-size:90%;">
J.-J. Hwang, H. Kretzschmar, J. Manela, S. Rafferty, N. Armstrong-Crews, T. Chen, and D. Anguelov, “CramNet: Camera-radar fusion with ray-constrained cross-attention for robust 3D object detection,” in </span><em id="bib.bib32.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 17th European Conference on Computer Vision (ECCV)</em><span id="bib.bib32.6.3" class="ltx_text" style="font-size:90%;">. Springer, 2022, pp. 388–405.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib33.2.2.1" class="ltx_text" style="font-size:90%;">[33]</span></span>
<span class="ltx_bibblock"><span id="bib.bib33.4.1" class="ltx_text" style="font-size:90%;">
T. Zhou, J. Chen, Y. Shi, K. Jiang, M. Yang, and D. Yang, “Bridging the view disparity between radar and camera features for multi-modal fusion 3D object detection,” </span><em id="bib.bib33.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Intelligent Vehicles</em><span id="bib.bib33.6.3" class="ltx_text" style="font-size:90%;">, vol. 8, no. 2, pp. 1523–1535, 2023.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib34.2.2.1" class="ltx_text" style="font-size:90%;">[34]</span></span>
<span class="ltx_bibblock"><span id="bib.bib34.4.1" class="ltx_text" style="font-size:90%;">
Y. Long, A. Kumar, D. Morris, X. Liu, M. Castro, and P. Chakravarty, “RADIANT: Radar-image association network for 3D object detection,” in </span><em id="bib.bib34.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 37th AAAI Conference on Artificial Intelligence</em><span id="bib.bib34.6.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib35.2.2.1" class="ltx_text" style="font-size:90%;">[35]</span></span>
<span class="ltx_bibblock"><span id="bib.bib35.4.1" class="ltx_text" style="font-size:90%;">
Z. Wu, G. Chen, Y. Gan, L. Wang, and J. Pu, “MVFusion: Multi-view 3D object detection with semantic-aligned radar and camera fusion,” in </span><em id="bib.bib35.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)</em><span id="bib.bib35.6.3" class="ltx_text" style="font-size:90%;">, 2023, pp. 2766-2773.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib36.2.2.1" class="ltx_text" style="font-size:90%;">[36]</span></span>
<span class="ltx_bibblock"><span id="bib.bib36.4.1" class="ltx_text" style="font-size:90%;">
Y. Kim, S. Kim, J. W. Choi, and D. Kum, “CRAFT: Camera-radar 3D object detection with spatio-contextual fusion transformer,” 2022, </span><em id="bib.bib36.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv:2209.06535. Accepted by the 37th AAAI Conference on Artificial Intelligence.</em><span id="bib.bib36.6.3" class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib37.2.2.1" class="ltx_text" style="font-size:90%;">[37]</span></span>
<span class="ltx_bibblock"><span id="bib.bib37.4.1" class="ltx_text" style="font-size:90%;">
P. Su, M. Daniel, and R. Hayder, “TransCAR: Transformer-based camera-and-radar fusion for 3D object detection,” 2023, </span><em id="bib.bib37.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv:2305.00397. Accepted by the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS).</em><span id="bib.bib37.6.3" class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib38.2.2.1" class="ltx_text" style="font-size:90%;">[38]</span></span>
<span class="ltx_bibblock"><span id="bib.bib38.4.1" class="ltx_text" style="font-size:90%;">
M. Meyer and G. Kuschk, “Automotive radar dataset for deep learning based 3D object detection,” in </span><em id="bib.bib38.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE 16th European Radar Conference (EuRAD)</em><span id="bib.bib38.6.3" class="ltx_text" style="font-size:90%;">, 2019, pp. 129–132.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib39.2.2.1" class="ltx_text" style="font-size:90%;">[39]</span></span>
<span class="ltx_bibblock"><span id="bib.bib39.4.1" class="ltx_text" style="font-size:90%;">
J. Rebut, A. Ouaknine, W. Malik, and P. Pérez, “Raw high-definition radar for multi-task learning,” in </span><em id="bib.bib39.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em><span id="bib.bib39.6.3" class="ltx_text" style="font-size:90%;">, 2022, pp. 17021–17030.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib40.2.2.1" class="ltx_text" style="font-size:90%;">[40]</span></span>
<span class="ltx_bibblock"><span id="bib.bib40.4.1" class="ltx_text" style="font-size:90%;">
D.-H. Paek, S.-H. Kong, and K. T. Wijaya, “K-radar: 4D radar object detection for autonomous driving in various weather conditions,” in </span><em id="bib.bib40.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 36th Conference on Neural Information Processing Systems (NeuIPS), Datasets and Benchmarks Track</em><span id="bib.bib40.6.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib41.2.2.1" class="ltx_text" style="font-size:90%;">[41]</span></span>
<span class="ltx_bibblock"><span id="bib.bib41.4.1" class="ltx_text" style="font-size:90%;">
Y. Yan, Y. Mao, and B. Li, “SECOND: Sparsely embedded convolutional detection,” </span><em id="bib.bib41.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Sensors</em><span id="bib.bib41.6.3" class="ltx_text" style="font-size:90%;">, vol. 18, no. 10, p. 3337, 2018.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib42.2.2.1" class="ltx_text" style="font-size:90%;">[42]</span></span>
<span class="ltx_bibblock"><span id="bib.bib42.4.1" class="ltx_text" style="font-size:90%;">
T. Yin, X. Zhou, and P. Krahenbuhl, “Center-based 3D object detection and tracking,” in </span><em id="bib.bib42.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em><span id="bib.bib42.6.3" class="ltx_text" style="font-size:90%;">, 2021, pp. 11784–11793.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib43.2.2.1" class="ltx_text" style="font-size:90%;">[43]</span></span>
<span class="ltx_bibblock"><span id="bib.bib43.4.1" class="ltx_text" style="font-size:90%;">
A. H. Lang, S. Vora, H. Caesar, L. Zhou, J. Yang, and O. Beijbom, “PointPillars: Fast encoders for object detection from point clouds,” in </span><em id="bib.bib43.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em><span id="bib.bib43.6.3" class="ltx_text" style="font-size:90%;">, 2019, pp. 12697–12705.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib44.2.2.1" class="ltx_text" style="font-size:90%;">[44]</span></span>
<span class="ltx_bibblock"><span id="bib.bib44.4.1" class="ltx_text" style="font-size:90%;">
C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “PointNet: Deep learning on point sets for 3D classification and segmentation,” in </span><em id="bib.bib44.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em><span id="bib.bib44.6.3" class="ltx_text" style="font-size:90%;">, 2017, pp. 652–660.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib45.2.2.1" class="ltx_text" style="font-size:90%;">[45]</span></span>
<span class="ltx_bibblock"><span id="bib.bib45.4.1" class="ltx_text" style="font-size:90%;">
J. Liu, Q. Zhao, W. Xiong, T. Huang, Q.-L. Han, and B. Zhu, “SMURF: Spatial multi-representation fusion for 3D object detection with 4D imaging radar,” 2023, </span><em id="bib.bib45.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv:2307.10784.</em><span id="bib.bib45.6.3" class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib46.2.2.1" class="ltx_text" style="font-size:90%;">[46]</span></span>
<span class="ltx_bibblock"><span id="bib.bib46.4.1" class="ltx_text" style="font-size:90%;">
L. Wang, X. Zhang, B. Xv, J. Zhang, R. Fu, X. Wang, L. Zhu, H. Ren, P. Lu, J. Li, and H. Liu, “InterFusion: Interaction-based 4D radar and LiDAR fusion for 3D object detection,” in </span><em id="bib.bib46.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em><span id="bib.bib46.6.3" class="ltx_text" style="font-size:90%;">, 2022, pp. 12247–12253.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib47.2.2.1" class="ltx_text" style="font-size:90%;">[47]</span></span>
<span class="ltx_bibblock"><span id="bib.bib47.4.1" class="ltx_text" style="font-size:90%;">
L. Wang, X. Zhang, J. Li, B. Xv, R. Fu, H. Chen, L. Yang, D. Jin, and L. Zhao, “Multi-modal and multi-scale fusion 3D object detection of 4D radar and LiDAR for autonomous driving,” </span><em id="bib.bib47.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Vehicular Technology</em><span id="bib.bib47.6.3" class="ltx_text" style="font-size:90%;">, pp. 1–15, 2022.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib48.2.2.1" class="ltx_text" style="font-size:90%;">[48]</span></span>
<span class="ltx_bibblock"><span id="bib.bib48.4.1" class="ltx_text" style="font-size:90%;">
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, and I. Polosukhin, “Attention is all you need,” </span><em id="bib.bib48.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib48.6.3" class="ltx_text" style="font-size:90%;">, 2017, pp. 30.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib49.2.2.1" class="ltx_text" style="font-size:90%;">[49]</span></span>
<span class="ltx_bibblock"><span id="bib.bib49.4.1" class="ltx_text" style="font-size:90%;">
H. Cui, J. Wu, J. Zhang, G. Chowdhary, and W. R. Norris, “3D detection and tracking for on-road vehicles with a monovision camera and dual low-cost 4D mmwave radars,” in </span><em id="bib.bib49.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Intelligent Transportation Systems Conference (ITSC)</em><span id="bib.bib49.6.3" class="ltx_text" style="font-size:90%;">. IEEE, 2021, pp. 2931–2937.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib50.2.2.1" class="ltx_text" style="font-size:90%;">[50]</span></span>
<span class="ltx_bibblock"><span id="bib.bib50.4.1" class="ltx_text" style="font-size:90%;">
A. Valada, R. Mohan, and W. Burgard, “Self-supervised model adaptation for multi-modal semantic segmentation,” </span><em id="bib.bib50.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Journal of Computer Vision</em><span id="bib.bib50.6.3" class="ltx_text" style="font-size:90%;">, vol. 128, no. 5, pp. 1239–1285, 2020.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib51.2.2.1" class="ltx_text" style="font-size:90%;">[51]</span></span>
<span class="ltx_bibblock"><span id="bib.bib51.4.1" class="ltx_text" style="font-size:90%;">
L. Zheng, S. Li, B. Tan, L. Yang, S. Chen, L. Huang, J. Bai, X. Zhu, and Z. Ma, “RCFusion: Fusing 4D radar and camera with bird’s-eye view features for 3D object detection,” </span><em id="bib.bib51.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Instrumentation and Measurement</em><span id="bib.bib51.6.3" class="ltx_text" style="font-size:90%;">, 2023, doi: 10.1109/TIM.2023.3280525.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib52.2.2.1" class="ltx_text" style="font-size:90%;">[52]</span></span>
<span class="ltx_bibblock"><span id="bib.bib52.4.1" class="ltx_text" style="font-size:90%;">
Z. Ge, S. Liu, F. Wang, Z. Li, and J. Sun, “YOLOX: Exceeding YOLO series in 2021,” 2021, </span><em id="bib.bib52.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv:2107.08430.</em><span id="bib.bib52.6.3" class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib53.2.2.1" class="ltx_text" style="font-size:90%;">[53]</span></span>
<span class="ltx_bibblock"><span id="bib.bib53.4.1" class="ltx_text" style="font-size:90%;">
C.-Y. Wang, H.-Y. M. Liao, Y.-H. Wu, P.-Y. Chen, J.-W. Hsieh, and I.-H. Yeh, “CSPNet: A new backbone that can enhance learning capability of CNN,” in </span><em id="bib.bib53.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) workshops</em><span id="bib.bib53.6.3" class="ltx_text" style="font-size:90%;">, 2020, pp. 390–391.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib54.2.2.1" class="ltx_text" style="font-size:90%;">[54]</span></span>
<span class="ltx_bibblock"><span id="bib.bib54.4.1" class="ltx_text" style="font-size:90%;">
S. Liu, L. Qi, H. Qin, J. Shi, and J. Jia, “Path aggregation network for instance segmentation,” in </span><em id="bib.bib54.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em><span id="bib.bib54.6.3" class="ltx_text" style="font-size:90%;">, 2018, pp. 8759–8768.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib55.2.2.1" class="ltx_text" style="font-size:90%;">[55]</span></span>
<span class="ltx_bibblock"><span id="bib.bib55.4.1" class="ltx_text" style="font-size:90%;">
T. Roddick, A. Kendall, and R. Cipolla, “Orthographic feature transform for monocular 3D object detection,” in </span><em id="bib.bib55.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the British Machine Vision Conference (BMVC)</em><span id="bib.bib55.6.3" class="ltx_text" style="font-size:90%;">. BMVA Press, September 2019, pp. 59.1–59.13.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib56.2.2.1" class="ltx_text" style="font-size:90%;">[56]</span></span>
<span class="ltx_bibblock"><span id="bib.bib56.4.1" class="ltx_text" style="font-size:90%;">
MMDetection3D Contributors, “MMDetection3D: OpenMMLab next generation platform for general 3D object detection,” https://github.com/open-mmlab/mmdetection3d, 2020.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib57.2.2.1" class="ltx_text" style="font-size:90%;">[57]</span></span>
<span class="ltx_bibblock"><span id="bib.bib57.4.1" class="ltx_text" style="font-size:90%;">
X. Chen, T. Zhang, Y. Wang, Y. Wang, and H. Zhao, “FUTR3D: A unified sensor fusion framework for 3D detection,” in </span><em id="bib.bib57.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em><span id="bib.bib57.6.3" class="ltx_text" style="font-size:90%;">, 2023, pp. 172–181.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib58.2.2.1" class="ltx_text" style="font-size:90%;">[58]</span></span>
<span class="ltx_bibblock"><span id="bib.bib58.4.1" class="ltx_text" style="font-size:90%;">
Z. Liu, H. Tang, A. Amini, X. Yang, H. Mao, D. Rus, and S. Han,
“BEVFusion: Multi-task multi-sensor fusion with unified bird’s-eye view representation,” in </span><em id="bib.bib58.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)</em><span id="bib.bib58.6.3" class="ltx_text" style="font-size:90%;">, 2023, pp. 2774-2781.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib59.2.2.1" class="ltx_text" style="font-size:90%;">[59]</span></span>
<span class="ltx_bibblock"><span id="bib.bib59.4.1" class="ltx_text" style="font-size:90%;">
K. Chen, J. Wang, J. Pang, Y. Cao, Y. Xiong, X. Li, S. Sun, W. Feng, Z. Liu, J, Xu, Z. Zhang, D. Cheng, C. Zhu, T. Cheng, Q. Zhao, B. Li, X. Lu, R. Zhu, Y. Wu, J. Dai, J. Wang, J. Shi, W. Ouyang, C. Loy, and D. Lin “MMDetection: Open MMLab detection toolbox and benchmark,” 2019, </span><em id="bib.bib59.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv:1906.07155.</em><span id="bib.bib59.6.3" class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib60.2.2.1" class="ltx_text" style="font-size:90%;">[60]</span></span>
<span class="ltx_bibblock"><span id="bib.bib60.4.1" class="ltx_text" style="font-size:90%;">
T.-Y. Lin, M, Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick, “Microsoft COCO: Common objects in context,” in </span><em id="bib.bib60.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 13th European Conference on Computer Vision (ECCV)</em><span id="bib.bib60.6.3" class="ltx_text" style="font-size:90%;">. Springer, 2014, pp. 740-755.
</span>
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib61.2.2.1" class="ltx_text" style="font-size:90%;">[61]</span></span>
<span class="ltx_bibblock"><span id="bib.bib61.4.1" class="ltx_text" style="font-size:90%;">
P. Palmer, M. Krueger, R. Altendorfer, G. Adam, and T. Bertram, “Reviewing 3D object detectors in the context of high-resolution 3+1D radar,” in </span><em id="bib.bib61.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), Workshop on 3D Vision and Robotics</em><span id="bib.bib61.6.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib62.2.2.1" class="ltx_text" style="font-size:90%;">[62]</span></span>
<span class="ltx_bibblock"><span id="bib.bib62.4.1" class="ltx_text" style="font-size:90%;">
J. Wang, B. Zhu, and Z. Zheng, “Robust Adaptive Control for a Quadrotor UAV With Uncertain Aerodynamic Parameters,” </span><em id="bib.bib62.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Aerospace and Electronic Systems</em><span id="bib.bib62.6.3" class="ltx_text" style="font-size:90%;"> (early access), 2023, doi: 10.1109/TAES.2023.3303133.
</span>
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib63.2.2.1" class="ltx_text" style="font-size:90%;">[63]</span></span>
<span class="ltx_bibblock"><span id="bib.bib63.4.1" class="ltx_text" style="font-size:90%;">
W. Zheng, and B. Zhu, “Control Lyapunov–Barrier function based model predictive control for stochastic nonlinear affine systems,” </span><em id="bib.bib63.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE International Journal of Robust and Nonlinear Control</em><span id="bib.bib63.6.3" class="ltx_text" style="font-size:90%;"> (early access), 2023, doi: doi.org/10.1002/rnc.6962.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2307.00723" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2307.00724" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2307.00724">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2307.00724" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2307.00725" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 20:10:48 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
