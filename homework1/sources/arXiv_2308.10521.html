<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2308.10521] PHE-SICH-CT-IDS: A Benchmark CT Image Dataset for Evaluation Semantic Segmentation, Object Detection and Radiomic Feature Extraction of Perihematomal Edema in Spontaneous Intracerebral Hemorrhage</title><meta property="og:description" content="Background and objective: Intracerebral hemorrhage is one of the diseases with the highest mortality and poorest prognosis worldwide. Spontaneous intracerebral hemorrhage (SICH) typically presents acutely, prompt and e…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="PHE-SICH-CT-IDS: A Benchmark CT Image Dataset for Evaluation Semantic Segmentation, Object Detection and Radiomic Feature Extraction of Perihematomal Edema in Spontaneous Intracerebral Hemorrhage">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="PHE-SICH-CT-IDS: A Benchmark CT Image Dataset for Evaluation Semantic Segmentation, Object Detection and Radiomic Feature Extraction of Perihematomal Edema in Spontaneous Intracerebral Hemorrhage">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2308.10521">

<!--Generated on Wed Feb 28 12:03:58 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">PHE-SICH-CT-IDS: A Benchmark CT Image Dataset for Evaluation Semantic Segmentation, Object Detection and Radiomic Feature Extraction of Perihematomal Edema in Spontaneous Intracerebral Hemorrhage</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">

Deguo Ma1,
Chen Li 1,
Lin Qiao2,
Tianming Du1,
Dechao Tang1,
Zhiyu Ma1,
Liyu Shi1,


Marcin Grzegorzek3 and
Hongzan Sun2 




</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">1 Microscopic Image and Medical Image Analysis Group, College of Medicine and Biological Information Engineering, 
<br class="ltx_break">Northeastern University, Shenyang, China
</span>
<span class="ltx_contact ltx_role_affiliation">2 Shengjing Hospital, China Medical University, Shenyang, China
</span>
<span class="ltx_contact ltx_role_affiliation">3 Institute of Medical Informatics, University of Luebeck, Luebeck, Germany
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id3.id1" class="ltx_p">Background and objective: Intracerebral hemorrhage is one of the diseases with the highest mortality and poorest prognosis worldwide. Spontaneous intracerebral hemorrhage (SICH) typically presents acutely, prompt and expedited radiological examination is crucial for diagnosis, localization, and quantification of the hemorrhage. Early detection and accurate segmentation of perihematomal edema (PHE) play a critical role in guiding appropriate clinical intervention and enhancing patient prognosis. However, the progress and assessment of computer-aided diagnostic methods for PHE segmentation and detection face challenges due to the scarcity of publicly accessible brain CT image datasets.</p>
<p id="id4.id2" class="ltx_p">Methods: This study establishes a publicly available CT dataset named PHE-SICH-CT-IDS for perihematomal edema in spontaneous intracerebral hemorrhage. The dataset comprises 120 brain CT scans and 7,022 CT images, along with corresponding medical information of the patients. To demonstrate its effectiveness, classical algorithms for semantic segmentation, object detection, and radiomic feature
extraction are evaluated. The experimental results confirm the suitability of PHE-SICH-CT-IDS for assessing the performance of segmentation, detection and radiomic feature extraction methods.</p>
<p id="id2.2" class="ltx_p">Results: This study conducts numerous experiments using machine learning and deep learning methods, demonstrating the differences in various segmentation and detection methods on the PHE-SICH-CT-IDS. The highest precision achieved in semantic segmentation is 76.31<math id="id1.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="id1.1.m1.1a"><mo id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><csymbol cd="latexml" id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">\%</annotation></semantics></math>, while object detection attains a maximum precision of 97.62<math id="id2.2.m2.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="id2.2.m2.1a"><mo id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><csymbol cd="latexml" id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">\%</annotation></semantics></math>. The experimental results on radiomic feature extraction and analysis prove the suitability of PHE-SICH-CT-IDS for evaluating image features and highlight the predictive value of these features for the prognosis of SICH patients.</p>
<p id="id5.id3" class="ltx_p">Conclusion: To the best of our knowledge, this is the first publicly available dataset for PHE in SICH, comprising various data formats suitable for applications across diverse medical scenarios. We believe that PHE-SICH-CT-IDS will allure researchers to explore novel algorithms, providing valuable support for clinicians and patients in the clinical setting. PHE-SICH-CT-IDS is freely published for non-commercial purpose at: <a target="_blank" href="https://figshare.com/articles/dataset/PHE-SICH-CT-IDS/23957937" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://figshare.com/articles/dataset/PHE-SICH-CT-IDS/23957937</a>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S1.SS1.5.1.1" class="ltx_text">I-A</span> </span><span id="S1.SS1.6.2" class="ltx_text ltx_font_italic">Research background and motivation</span>
</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">Intracerebral hemorrhage (ICH) is a type of cerebrovascular accident resulting from bleeding within the brain tissue, leading to the accumulation of blood.
This disorder is known to be caused by various factors, among which hypertension is the most common, accounting for about 65% of spontaneous cases. Other causes include amyloid angiopathy, brain tumors, aneurysms, arteriovenous malformations, cerebral cavernous malformations, and arteriovenous fistulae. These underlying causes may lead to the rupture of blood vessels, resulting in the formation of a hematoma within the brain parenchyma <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.
ICH remains a significant cause of morbidity and mortality worldwide, with an estimated incidence of 2.8 million cases annually. ICH accounts for approximately 10% to 15% of all strokes in the USA, Europe, and Australia, and up to 20% to 30% of strokes in Asia <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. The incidence rate of ICH is 24.6 per 100,000 person-years, making it a considerable public health concern <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. The consequences of ICH can be severe, including long-term disability and death. The financial burden of ICH is attributed in part to its high mortality, with up to 40% of patients succumbing to the condition within 30 days of onset, often after prolonged stays in the intensive care unit <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Moreover, the incidence of ICH is projected to rise due to the increasing use of anticoagulation, antiplatelet drugs, and an aging population.</p>
</div>
<div id="S1.SS1.p2" class="ltx_para">
<p id="S1.SS1.p2.1" class="ltx_p">Spontaneous intracerebral hemorrhage (SICH) is a frequent subtype of ICH, frequently occurring in the basal ganglia. Patients with SICH may experience early re-bleeding. SICH typically presents acutely, and timely imaging is crucial for accurate diagnosis, determination of the location and volume of bleeding.</p>
</div>
<div id="S1.SS1.p3" class="ltx_para">
<p id="S1.SS1.p3.1" class="ltx_p">The cranial computed tomography (CT) imaging is preferred in the stroke unit due to its prevalence, non-invasiveness, and affordability in nearly all hospitals, while offering good quality visual information on human organs.
In CT images, the hemorrhage appears as a hyper-intense bright region with sharp contrast against its surroundings and the perihematomal edema (PHE) as a low-density area around the hemorrhage.
Numerous preclinical and clinical studies have demonstrated that the kinetics and peak volume of PHE have been shown to cause secondary brain injury (SBI) after SICH and is associated with a poor prognosis. Thus, PHE has been considered a promising therapeutic target for SICH.
Furthermore, the CT images of PHE also provide several radiomic feature parameters that can predict hematoma expansion (HE).
Patients with SICH may experience HE in the early stage, which can increase the mortality rate of the patient. HE is a critical determinant of disease progression and poor prognosis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.
Practical scoring schemas have been developed based on these parameters and clinical criteria to predict HE accurately.
However, localization of PHE regions in CT images is extremely challenging due to significant overlap in CT values between HE and other brain tissues, such as cerebrospinal fluid and microvascular diseases. For experienced radiologists, detecting intracerebral hemorrhage using pixel labeling requires an average of 10 minutes per CT scan, while PHE labeling requires at least twice as much time.
The present diagnosis process involves the examine of CT images by an expert radiologist to determine the presence of ICH and identify its type and location.
However, the diagnosis is dependent on the availability of subspecialty-trained neuroradiologists, leading to potential time inefficiencies and inaccuracies in remote areas where access to specialized care is limited.
The current research status reveals several critical issues in the field: medical professionals face challenges in accurately diagnosing and localizing Spontaneous Intracerebral Hemorrhage (SICH) due to the subjective nature of results, excessive workload, and extended working hours. Furthermore, medical doctors lack radiomic features parameters to predict HE. Therefore, there is a pressing need to address these relevant issues more effectively.</p>
</div>
<div id="S1.SS1.p4" class="ltx_para">
<p id="S1.SS1.p4.1" class="ltx_p">With the development of medical image processing technology, the primary goal is to achieve high accuracy and good performance in computer-assisted diagnosis. To accomplish this objective, several deep learning-based methods have been proposed and explored <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Specifically, medical image segmentation and detection are crucial for achieving this goal. The rapid development of object detection methods has enabled the efficient and rapid identification of hemorrhage and PHE locations. In addition, the results of image segmentation methods can provide important reference for clinical doctors in predicting the risk of HE and predicting patient’s cycle survival rate. Overall, the medical image processing technology holds great potential for improving the accuracy and efficiency of medical diagnosis for SICH.</p>
</div>
<div id="S1.SS1.p5" class="ltx_para">
<p id="S1.SS1.p5.1" class="ltx_p">In this paper, a benchmark CT image dataset for evaluation semantic segmentation, object detection, and radiomic feature extraction of perihematomal edema in spontaneous intracerebral hemorrhage is introduced, namely PHE-SICH-CT-IDS, which is constructed 120 CT scans of patients with SICH. PHE-SICH-CT-IDS contains 3,511 CT images of SICH occurring in the basal ganglia region, with associated labels for the surrounding edematous zone around the hematoma. Additionally, PHE-SICH-CT-IDS provides SICH detection labels for the edematous zone and radiomic features of the edematous zone, which are valuable for clinical research and diagnosis. The evaluation results are obtained by using different traditional machine learning and new deep learning methods for segmentation and detection on the images of the CT dataset. Dataset is available at the URL:<a target="_blank" href="https://figshare.com/articles/dataset/PHE-SICH-CT-IDS/23957937" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://figshare.com/articles/dataset/PHE-SICH-CT-IDS/23957937</a>.</p>
</div>
<div id="S1.SS1.p6" class="ltx_para">
<p id="S1.SS1.p6.1" class="ltx_p">The main contributions of this paper are as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1)</span> 
<div id="S1.I1.ix1.p1" class="ltx_para">
<p id="S1.I1.ix1.p1.1" class="ltx_p">The first open-source CT dataset of spontaneous intracerebral hemorrhage with perihematomal edema in the basal ganglia region (PHE-SICH-CT-IDS) is developed and released.</p>
</div>
</li>
<li id="S1.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2)</span> 
<div id="S1.I1.ix2.p1" class="ltx_para">
<p id="S1.I1.ix2.p1.1" class="ltx_p">PHE-SICH-CT-IDS provides segmentation and detection capabilities for edematous areas, as well as radiomic features of the edematous areas around intracerebral hemorrhage, which can be used by medical researchers to study the correlation between PHE and HE within a short period of time.</p>
</div>
</li>
<li id="S1.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3)</span> 
<div id="S1.I1.ix3.p1" class="ltx_para">
<p id="S1.I1.ix3.p1.1" class="ltx_p">The validation of the related edematous area segmentation and detection methods proposed by individuals highlights the distinguishability of PHE-SICH-CT-IDS for commonly used machine learning and deep learning methods.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S1.SS2.5.1.1" class="ltx_text">I-B</span> </span><span id="S1.SS2.6.2" class="ltx_text ltx_font_italic">Related work</span>
</h3>

<div id="S1.SS2.p1" class="ltx_para">
<p id="S1.SS2.p1.1" class="ltx_p">This study analyzes and compares existing public CT datasets on ICH, and explores in-depth the currently known research results. Additionally, it highlights the limitations of the relevant datasets currently available.</p>
</div>
<div id="S1.SS2.p2" class="ltx_para">
<p id="S1.SS2.p2.1" class="ltx_p">Two commonly used public datasets for ICH detection are the CQ500 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> and the RSNA Intracranial Hemorrhage Detection Challenge dataset(RSNA ICHD dataset). The CQ500 dataset, originating from the Centre for Advanced Research in Imaging, Neurosciences, and Genomics in New Delhi, India, encompasses a diverse range of CT scanners with slices per rotation varying from 16 to 128. CQ500 includes 491 head CT scans with each of intraparenchymal, subdural, extradural, and subarachnoid hemorrhages, as well as calvarial fractures.
RSNA ICHD dataset, which was released by the Radiological Society of North America (RSNA) in collaboration with members of the American Society of Neuroradiology and MD.ai for the RSNA Challenge on Kaggle, and contains over 25,000 CT scans.</p>
</div>
<div id="S1.SS2.p3" class="ltx_para">
<p id="S1.SS2.p3.1" class="ltx_p">For ICH segmentation, the PhysioNet ICH dataset contains 82 CT scans. Among these, 36 scans are from patients diagnosed with ICH by radiologists, including types such as intraventricular, intraparenchymal, subarachnoid, epidural, and subdural <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. Each CT scan has around 30 slices with a 5 mm slice thickness.The number of images in this dataset is small and the variety is insufficient.</p>
</div>
<div id="S1.SS2.p4" class="ltx_para">
<p id="S1.SS2.p4.1" class="ltx_p">In addition to ICH detection and segmentation, many approaches have been proposed for PHE segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. However, a lack of public or private datasets with PHE masks has prevented the validation of many of these approaches. Others have been validated on private datasets with different characteristics, such as the region of onset in the brain and the type of ICH diagnosed. Given these differences, it is not possible to objectively compare different approaches. Therefore, a dataset is required to benchmark and extend the work on PHE segmentation in ICH.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Recent datasets for segmentation and detection of intracerebral
hemorrhage.</figcaption>
<table id="S1.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S1.T1.1.1" class="ltx_tr">
<td id="S1.T1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt">Year</td>
<td id="S1.T1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">Name</td>
<td id="S1.T1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">Reference</td>
<td id="S1.T1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">Category</td>
<td id="S1.T1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt">Amount</td>
<td id="S1.T1.1.1.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt">modality</td>
</tr>
<tr id="S1.T1.1.2" class="ltx_tr">
<td id="S1.T1.1.2.1" class="ltx_td ltx_align_center ltx_border_t">2018</td>
<td id="S1.T1.1.2.2" class="ltx_td ltx_align_center ltx_border_t">CQ500</td>
<td id="S1.T1.1.2.3" class="ltx_td ltx_align_center ltx_border_t"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>
</td>
<td id="S1.T1.1.2.4" class="ltx_td ltx_align_center ltx_border_t">Detection, Classification</td>
<td id="S1.T1.1.2.5" class="ltx_td ltx_align_center ltx_border_t">491 scans</td>
<td id="S1.T1.1.2.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">CT</td>
</tr>
<tr id="S1.T1.1.3" class="ltx_tr">
<td id="S1.T1.1.3.1" class="ltx_td ltx_align_center">2019</td>
<td id="S1.T1.1.3.2" class="ltx_td ltx_align_center">RSNA Intracranial Hemorrhage Detection Challenge dataset</td>
<td id="S1.T1.1.3.3" class="ltx_td ltx_align_center"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>
</td>
<td id="S1.T1.1.3.4" class="ltx_td ltx_align_center">Detection</td>
<td id="S1.T1.1.3.5" class="ltx_td ltx_align_center">25,000 scans</td>
<td id="S1.T1.1.3.6" class="ltx_td ltx_nopad_r ltx_align_center">CT</td>
</tr>
<tr id="S1.T1.1.4" class="ltx_tr">
<td id="S1.T1.1.4.1" class="ltx_td ltx_align_center">2020</td>
<td id="S1.T1.1.4.2" class="ltx_td ltx_align_center">PhysioNet ICH dataset</td>
<td id="S1.T1.1.4.3" class="ltx_td ltx_align_center"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>
</td>
<td id="S1.T1.1.4.4" class="ltx_td ltx_align_center">Segmentation</td>
<td id="S1.T1.1.4.5" class="ltx_td ltx_align_center">36 scans</td>
<td id="S1.T1.1.4.6" class="ltx_td ltx_nopad_r ltx_align_center">CT</td>
</tr>
<tr id="S1.T1.1.5" class="ltx_tr">
<td id="S1.T1.1.5.1" class="ltx_td ltx_align_center ltx_border_bb">2022</td>
<td id="S1.T1.1.5.2" class="ltx_td ltx_align_center ltx_border_bb">ATLAS V2.0</td>
<td id="S1.T1.1.5.3" class="ltx_td ltx_align_center ltx_border_bb"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>
</td>
<td id="S1.T1.1.5.4" class="ltx_td ltx_align_center ltx_border_bb">Segmentation</td>
<td id="S1.T1.1.5.5" class="ltx_td ltx_align_center ltx_border_bb">955 scans</td>
<td id="S1.T1.1.5.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">MRI</td>
</tr>
</table>
</figure>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2308.10521/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="233" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>: Data preparation workflow of PHE-SICH-CT-IDS.</figcaption>
</figure>
</section>
<section id="S1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S1.SS3.5.1.1" class="ltx_text">I-C</span> </span><span id="S1.SS3.6.2" class="ltx_text ltx_font_italic">Structure of this paper</span>
</h3>

<div id="S1.SS3.p1" class="ltx_para">
<p id="S1.SS3.p1.1" class="ltx_p">In this section, the background and motivation for the dataset preparation are presented along with a review of related research papers. Detailed methods for preparing and evaluating the dataset, encompassing information about each individual element. Section <a href="#S3" title="III Result ‣ PHE-SICH-CT-IDS: A Benchmark CT Image Dataset for Evaluation Semantic Segmentation, Object Detection and Radiomic Feature Extraction of Perihematomal Edema in Spontaneous Intracerebral Hemorrhage" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> presents the results of the dataset evaluation, followed by a discussion of these results in section <a href="#S4" title="IV Discussion ‣ PHE-SICH-CT-IDS: A Benchmark CT Image Dataset for Evaluation Semantic Segmentation, Object Detection and Radiomic Feature Extraction of Perihematomal Edema in Spontaneous Intracerebral Hemorrhage" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>. Finally, a summary of the findings and potential future work are presented.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Materials and Methods</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">Dataset preparation</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">PHE-SICH-CT-IDS includes 7,022 CT images, consisting of 3,511 CT slice images and 3,511 ground truth images of PHE. Moreover, 3,511 object detection labels in XML format are provided. Additionally, PHE-SICH-CT-IDS contains 120 CT scans and corresponding PHE labels in NIFTI format for eligible patients. Radiomic features of the PHE are also available for these 120 patients. The dataset includes medical information for these patients such as presence of the subsequent hematoma expansion, gender, age, diagnostic haemorrhagic area and previous medical history (e.g., hypertension and diabetes). In response to the different functions of the dataset and the diverse requirements of experiments, the dataset is divided into three sub-datasets, with specific information provided in Table <a href="#S2.T2" title="TABLE II ‣ II-A Dataset preparation ‣ II Materials and Methods ‣ PHE-SICH-CT-IDS: A Benchmark CT Image Dataset for Evaluation Semantic Segmentation, Object Detection and Radiomic Feature Extraction of Perihematomal Edema in Spontaneous Intracerebral Hemorrhage" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>. The subsequent section provides an introduction to the specifics of the applied datasets.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">The complete process, starting from data solicitation and concluding with the compilation of the final dataset, is depicted in Figure <a href="#S1.F1" title="Figure 1 ‣ I-B Related work ‣ I Introduction ‣ PHE-SICH-CT-IDS: A Benchmark CT Image Dataset for Evaluation Semantic Segmentation, Object Detection and Radiomic Feature Extraction of Perihematomal Edema in Spontaneous Intracerebral Hemorrhage" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">PHE-SICH-CT-IDS:</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<ol id="S2.I1" class="ltx_enumerate">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">Data source:</p>
</div>
<div id="S2.I1.i1.p2" class="ltx_para">
<p id="S2.I1.i1.p2.1" class="ltx_p">The head CT scan data of SICH was collected between December 2020 and April 2023 from Shengjing Hospital of China Medical University. The CT scans were rigorously screened by three senior radiologists with 15, 17, and 20 years of experience in cranial CT interpretation, according to strict criteria(See section <a href="#S2.I1.i2" title="item 2 ‣ II-A Dataset preparation ‣ II Materials and Methods ‣ PHE-SICH-CT-IDS: A Benchmark CT Image Dataset for Evaluation Semantic Segmentation, Object Detection and Radiomic Feature Extraction of Perihematomal Edema in Spontaneous Intracerebral Hemorrhage" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> for details). The CT scans were performed using a PHILIPS Brilliance iCT scanner with the following parameters: tube voltage 120 kV, tube current 300 mA, matrix <math id="S2.I1.i1.p2.1.m1.1" class="ltx_Math" alttext="512\times 512" display="inline"><semantics id="S2.I1.i1.p2.1.m1.1a"><mrow id="S2.I1.i1.p2.1.m1.1.1" xref="S2.I1.i1.p2.1.m1.1.1.cmml"><mn id="S2.I1.i1.p2.1.m1.1.1.2" xref="S2.I1.i1.p2.1.m1.1.1.2.cmml">512</mn><mo lspace="0.222em" rspace="0.222em" id="S2.I1.i1.p2.1.m1.1.1.1" xref="S2.I1.i1.p2.1.m1.1.1.1.cmml">×</mo><mn id="S2.I1.i1.p2.1.m1.1.1.3" xref="S2.I1.i1.p2.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p2.1.m1.1b"><apply id="S2.I1.i1.p2.1.m1.1.1.cmml" xref="S2.I1.i1.p2.1.m1.1.1"><times id="S2.I1.i1.p2.1.m1.1.1.1.cmml" xref="S2.I1.i1.p2.1.m1.1.1.1"></times><cn type="integer" id="S2.I1.i1.p2.1.m1.1.1.2.cmml" xref="S2.I1.i1.p2.1.m1.1.1.2">512</cn><cn type="integer" id="S2.I1.i1.p2.1.m1.1.1.3.cmml" xref="S2.I1.i1.p2.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p2.1.m1.1c">512\times 512</annotation></semantics></math>, slice thickness 5.00 mm, and slice spacing 5.00 mm. Each CT scan consisted of an average of approximately 35 slices.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p">Data selection criteria:</p>
</div>
<div id="S2.I1.i2.p2" class="ltx_para">
<p id="S2.I1.i2.p2.1" class="ltx_p">Inclusion criteria:</p>
<ul id="S2.I1.i2.I1" class="ltx_itemize">
<li id="S2.I1.i2.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1)</span> 
<div id="S2.I1.i2.I1.ix1.p1" class="ltx_para">
<p id="S2.I1.i2.I1.ix1.p1.1" class="ltx_p">Diagnosis of spontaneous intracranial hemorrhage.</p>
</div>
</li>
<li id="S2.I1.i2.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2)</span> 
<div id="S2.I1.i2.I1.ix2.p1" class="ltx_para">
<p id="S2.I1.i2.I1.ix2.p1.1" class="ltx_p">Hemorrhage occurring in the basal ganglia region of the brain.</p>
</div>
</li>
<li id="S2.I1.i2.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3)</span> 
<div id="S2.I1.i2.I1.ix3.p1" class="ltx_para">
<p id="S2.I1.i2.I1.ix3.p1.1" class="ltx_p">Time from SICH symptom onset to hospital admission and scanning within 12 hours.</p>
</div>
</li>
<li id="S2.I1.i2.I1.ix4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4)</span> 
<div id="S2.I1.i2.I1.ix4.p1" class="ltx_para">
<p id="S2.I1.i2.I1.ix4.p1.1" class="ltx_p">Age over 18 years.</p>
</div>
</li>
<li id="S2.I1.i2.I1.ix5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5)</span> 
<div id="S2.I1.i2.I1.ix5.p1" class="ltx_para">
<p id="S2.I1.i2.I1.ix5.p1.1" class="ltx_p">Patients who did not undergo surgical treatment after admission and reexamined their head CT within 72 hours.</p>
</div>
</li>
</ul>
</div>
<div id="S2.I1.i2.p3" class="ltx_para">
<p id="S2.I1.i2.p3.1" class="ltx_p">Exclusion criteria:</p>
<ul id="S2.I1.i2.I2" class="ltx_itemize">
<li id="S2.I1.i2.I2.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1)</span> 
<div id="S2.I1.i2.I2.ix1.p1" class="ltx_para">
<p id="S2.I1.i2.I2.ix1.p1.1" class="ltx_p">Poor image quality.</p>
</div>
</li>
<li id="S2.I1.i2.I2.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2)</span> 
<div id="S2.I1.i2.I2.ix2.p1" class="ltx_para">
<p id="S2.I1.i2.I2.ix2.p1.1" class="ltx_p">History of neurosurgical procedures.</p>
</div>
</li>
<li id="S2.I1.i2.I2.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3)</span> 
<div id="S2.I1.i2.I2.ix3.p1" class="ltx_para">
<p id="S2.I1.i2.I2.ix3.p1.1" class="ltx_p">Coagulation disorders or a history of anticoagulant use.</p>
</div>
</li>
<li id="S2.I1.i2.I2.ix4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4)</span> 
<div id="S2.I1.i2.I2.ix4.p1" class="ltx_para">
<p id="S2.I1.i2.I2.ix4.p1.1" class="ltx_p">Secondary ICH or bleeding into the ventricles</p>
</div>
</li>
</ul>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p">Perihematomal edema segmentation labels:</p>
</div>
<div id="S2.I1.i3.p2" class="ltx_para">
<p id="S2.I1.i3.p2.1" class="ltx_p">During the data collection process, if the selection criteria are met, the open-source software LIFEx v5.10 is used for PHE segmentation. Using the cerebral window settings (Level=40, Width=120), radiologists manually outline the PHE range on each CT image layer. The outlining is based on the density of the PHE. Notably, this study exclusively focuses on outlining areas with low-density PHE. The specific segmentation labels are shown in Figure <a href="#S2.F2" title="Figure 2 ‣ II-A Dataset preparation ‣ II Materials and Methods ‣ PHE-SICH-CT-IDS: A Benchmark CT Image Dataset for Evaluation Semantic Segmentation, Object Detection and Radiomic Feature Extraction of Perihematomal Edema in Spontaneous Intracerebral Hemorrhage" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</li>
<li id="S2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S2.I1.i4.p1" class="ltx_para">
<p id="S2.I1.i4.p1.1" class="ltx_p">Object detection labels:</p>
</div>
<div id="S2.I1.i4.p2" class="ltx_para">
<p id="S2.I1.i4.p2.1" class="ltx_p">Annotations for object detection are created using the Labelme annotation tool and saved in PASCAL VOC format as XML files with the same name as the corresponding images. The annotated regions include not only the area of intracranial hemorrhage but also the surrounding PHE. The detection labels in the dataset are presented in Figure <a href="#S2.F2" title="Figure 2 ‣ II-A Dataset preparation ‣ II Materials and Methods ‣ PHE-SICH-CT-IDS: A Benchmark CT Image Dataset for Evaluation Semantic Segmentation, Object Detection and Radiomic Feature Extraction of Perihematomal Edema in Spontaneous Intracerebral Hemorrhage" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</li>
<li id="S2.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S2.I1.i5.p1" class="ltx_para">
<p id="S2.I1.i5.p1.1" class="ltx_p">Radiomic features:</p>
</div>
<div id="S2.I1.i5.p2" class="ltx_para">
<p id="S2.I1.i5.p2.1" class="ltx_p">Based on the segmented PHE regions, the open-source software LIFEx v5.1 is used for radiomic feature extraction, which yield a total of seven texture features. These include Homogeneity, Energy, Contrast, Correlation, Entropylog2, Entropylog10, and Dissimilarity, all derived from the gray-level co-occurrence matrix (GLCM).</p>
</div>
</li>
<li id="S2.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span> 
<div id="S2.I1.i6.p1" class="ltx_para">
<p id="S2.I1.i6.p1.1" class="ltx_p">Dataset preparation workflow:</p>
<ul id="S2.I1.i6.I1" class="ltx_itemize">
<li id="S2.I1.i6.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1)</span> 
<div id="S2.I1.i6.I1.ix1.p1" class="ltx_para">
<p id="S2.I1.i6.I1.ix1.p1.1" class="ltx_p">Selecting data that meets the criteria.</p>
</div>
</li>
<li id="S2.I1.i6.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2)</span> 
<div id="S2.I1.i6.I1.ix2.p1" class="ltx_para">
<p id="S2.I1.i6.I1.ix2.p1.1" class="ltx_p">Retaining patient-related medical information.</p>
</div>
</li>
<li id="S2.I1.i6.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3)</span> 
<div id="S2.I1.i6.I1.ix3.p1" class="ltx_para">
<p id="S2.I1.i6.I1.ix3.p1.1" class="ltx_p">Creating segmentation labels. (Figure <a href="#S1.F1" title="Figure 1 ‣ I-B Related work ‣ I Introduction ‣ PHE-SICH-CT-IDS: A Benchmark CT Image Dataset for Evaluation Semantic Segmentation, Object Detection and Radiomic Feature Extraction of Perihematomal Edema in Spontaneous Intracerebral Hemorrhage" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>-(b))</p>
</div>
</li>
<li id="S2.I1.i6.I1.ix4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4)</span> 
<div id="S2.I1.i6.I1.ix4.p1" class="ltx_para">
<p id="S2.I1.i6.I1.ix4.p1.1" class="ltx_p">Extracting radiomic features. (Figure <a href="#S1.F1" title="Figure 1 ‣ I-B Related work ‣ I Introduction ‣ PHE-SICH-CT-IDS: A Benchmark CT Image Dataset for Evaluation Semantic Segmentation, Object Detection and Radiomic Feature Extraction of Perihematomal Edema in Spontaneous Intracerebral Hemorrhage" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>-(c))</p>
</div>
</li>
<li id="S2.I1.i6.I1.ix5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5)</span> 
<div id="S2.I1.i6.I1.ix5.p1" class="ltx_para">
<p id="S2.I1.i6.I1.ix5.p1.1" class="ltx_p">Converting original data and segmentation label data in NIFIT format to PNG format slices with adjusted window level and width. (Figure <a href="#S1.F1" title="Figure 1 ‣ I-B Related work ‣ I Introduction ‣ PHE-SICH-CT-IDS: A Benchmark CT Image Dataset for Evaluation Semantic Segmentation, Object Detection and Radiomic Feature Extraction of Perihematomal Edema in Spontaneous Intracerebral Hemorrhage" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>-(d))</p>
</div>
</li>
<li id="S2.I1.i6.I1.ix6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6)</span> 
<div id="S2.I1.i6.I1.ix6.p1" class="ltx_para">
<p id="S2.I1.i6.I1.ix6.p1.1" class="ltx_p">Creating object detection labels. (Figure <a href="#S1.F1" title="Figure 1 ‣ I-B Related work ‣ I Introduction ‣ PHE-SICH-CT-IDS: A Benchmark CT Image Dataset for Evaluation Semantic Segmentation, Object Detection and Radiomic Feature Extraction of Perihematomal Edema in Spontaneous Intracerebral Hemorrhage" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>-(f)(h))</p>
</div>
</li>
</ul>
</div>
</li>
<li id="S2.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span> 
<div id="S2.I1.i7.p1" class="ltx_para">
<p id="S2.I1.i7.p1.1" class="ltx_p">Sub-dataset and image format:</p>
<ul id="S2.I1.i7.I1" class="ltx_itemize">
<li id="S2.I1.i7.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1)</span> 
<div id="S2.I1.i7.I1.ix1.p1" class="ltx_para">
<p id="S2.I1.i7.I1.ix1.p1.1" class="ltx_p">Sub-dataset A: NIFIT format.</p>
</div>
</li>
<li id="S2.I1.i7.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2)</span> 
<div id="S2.I1.i7.I1.ix2.p1" class="ltx_para">
<p id="S2.I1.i7.I1.ix2.p1.1" class="ltx_p">Sub-dataset B: PNG format and <math id="S2.I1.i7.I1.ix2.p1.1.m1.1" class="ltx_Math" alttext="512\times 512" display="inline"><semantics id="S2.I1.i7.I1.ix2.p1.1.m1.1a"><mrow id="S2.I1.i7.I1.ix2.p1.1.m1.1.1" xref="S2.I1.i7.I1.ix2.p1.1.m1.1.1.cmml"><mn id="S2.I1.i7.I1.ix2.p1.1.m1.1.1.2" xref="S2.I1.i7.I1.ix2.p1.1.m1.1.1.2.cmml">512</mn><mo lspace="0.222em" rspace="0.222em" id="S2.I1.i7.I1.ix2.p1.1.m1.1.1.1" xref="S2.I1.i7.I1.ix2.p1.1.m1.1.1.1.cmml">×</mo><mn id="S2.I1.i7.I1.ix2.p1.1.m1.1.1.3" xref="S2.I1.i7.I1.ix2.p1.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i7.I1.ix2.p1.1.m1.1b"><apply id="S2.I1.i7.I1.ix2.p1.1.m1.1.1.cmml" xref="S2.I1.i7.I1.ix2.p1.1.m1.1.1"><times id="S2.I1.i7.I1.ix2.p1.1.m1.1.1.1.cmml" xref="S2.I1.i7.I1.ix2.p1.1.m1.1.1.1"></times><cn type="integer" id="S2.I1.i7.I1.ix2.p1.1.m1.1.1.2.cmml" xref="S2.I1.i7.I1.ix2.p1.1.m1.1.1.2">512</cn><cn type="integer" id="S2.I1.i7.I1.ix2.p1.1.m1.1.1.3.cmml" xref="S2.I1.i7.I1.ix2.p1.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i7.I1.ix2.p1.1.m1.1c">512\times 512</annotation></semantics></math> pixels.</p>
</div>
</li>
<li id="S2.I1.i7.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3)</span> 
<div id="S2.I1.i7.I1.ix3.p1" class="ltx_para">
<p id="S2.I1.i7.I1.ix3.p1.1" class="ltx_p">Sub-dataset C: JPG format and <math id="S2.I1.i7.I1.ix3.p1.1.m1.1" class="ltx_Math" alttext="512\times 512" display="inline"><semantics id="S2.I1.i7.I1.ix3.p1.1.m1.1a"><mrow id="S2.I1.i7.I1.ix3.p1.1.m1.1.1" xref="S2.I1.i7.I1.ix3.p1.1.m1.1.1.cmml"><mn id="S2.I1.i7.I1.ix3.p1.1.m1.1.1.2" xref="S2.I1.i7.I1.ix3.p1.1.m1.1.1.2.cmml">512</mn><mo lspace="0.222em" rspace="0.222em" id="S2.I1.i7.I1.ix3.p1.1.m1.1.1.1" xref="S2.I1.i7.I1.ix3.p1.1.m1.1.1.1.cmml">×</mo><mn id="S2.I1.i7.I1.ix3.p1.1.m1.1.1.3" xref="S2.I1.i7.I1.ix3.p1.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i7.I1.ix3.p1.1.m1.1b"><apply id="S2.I1.i7.I1.ix3.p1.1.m1.1.1.cmml" xref="S2.I1.i7.I1.ix3.p1.1.m1.1.1"><times id="S2.I1.i7.I1.ix3.p1.1.m1.1.1.1.cmml" xref="S2.I1.i7.I1.ix3.p1.1.m1.1.1.1"></times><cn type="integer" id="S2.I1.i7.I1.ix3.p1.1.m1.1.1.2.cmml" xref="S2.I1.i7.I1.ix3.p1.1.m1.1.1.2">512</cn><cn type="integer" id="S2.I1.i7.I1.ix3.p1.1.m1.1.1.3.cmml" xref="S2.I1.i7.I1.ix3.p1.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i7.I1.ix3.p1.1.m1.1c">512\times 512</annotation></semantics></math> pixels.</p>
</div>
</li>
</ul>
</div>
</li>
</ol>
</div>
<figure id="S2.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Dataset content of PHE-SICH-CT-IDS.</figcaption>
<table id="S2.T2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S2.T2.1.1" class="ltx_tr">
<td id="S2.T2.1.1.1" class="ltx_td ltx_align_center ltx_border_tt">Subdataset name</td>
<td id="S2.T2.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">Format</td>
<td id="S2.T2.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">Amount</td>
<td id="S2.T2.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">Function</td>
<td id="S2.T2.1.1.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt">Others</td>
</tr>
<tr id="S2.T2.1.2" class="ltx_tr">
<td id="S2.T2.1.2.1" class="ltx_td ltx_align_center ltx_border_t">Subdataset A</td>
<td id="S2.T2.1.2.2" class="ltx_td ltx_align_center ltx_border_t">NIFIT</td>
<td id="S2.T2.1.2.3" class="ltx_td ltx_align_center ltx_border_t">120 scans</td>
<td id="S2.T2.1.2.4" class="ltx_td ltx_align_center ltx_border_t">Segmentation and Feature extraction</td>
<td id="S2.T2.1.2.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">Radiomic features Medical information</td>
</tr>
<tr id="S2.T2.1.3" class="ltx_tr">
<td id="S2.T2.1.3.1" class="ltx_td ltx_align_center">Subdataset B</td>
<td id="S2.T2.1.3.2" class="ltx_td ltx_align_center">PNG</td>
<td id="S2.T2.1.3.3" class="ltx_td ltx_align_center">120 scans / 7,022 images</td>
<td id="S2.T2.1.3.4" class="ltx_td ltx_align_center">Segmentation and Detection</td>
<td id="S2.T2.1.3.5" class="ltx_td ltx_nopad_r ltx_align_center">Medical information</td>
</tr>
<tr id="S2.T2.1.4" class="ltx_tr">
<td id="S2.T2.1.4.1" class="ltx_td ltx_align_center ltx_border_bb">Subdataset C</td>
<td id="S2.T2.1.4.2" class="ltx_td ltx_align_center ltx_border_bb">JPG</td>
<td id="S2.T2.1.4.3" class="ltx_td ltx_align_center ltx_border_bb">120 scans / 7,022 images</td>
<td id="S2.T2.1.4.4" class="ltx_td ltx_align_center ltx_border_bb">Segmentation and Detection</td>
<td id="S2.T2.1.4.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">Medical information</td>
</tr>
</table>
</figure>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2308.10521/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="305" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The labels of SICH-CT-IDS. The first row represents the original CT images, the second row represents the segmentation labels, and the third row represents the detection labels.</figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">Dataset description</span>
</h3>

<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS1.5.1.1" class="ltx_text">II-B</span>1 </span>Normal CT Image</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.1" class="ltx_p">A brain CT image of a healthy individual reveals normal brain structure and tissue characteristics. The cortex and white matter regions exhibit normal density and contrast, with well-defined gray-white matter boundaries. The ventricular system appears in a normal size and shape, and the sulcal patterns and cerebral gyri are intact and clearly visible. No signs of abnormal hemorrhage, masses, edema, or other pathological changes were observed. Overall, the image depicts a healthy brain with normal anatomical structure and tissue integrity. Some examples are shown in Figure <a href="#S2.F3" title="Figure 3 ‣ II-B2 CT image of SICH ‣ II-B Dataset description ‣ II Materials and Methods ‣ PHE-SICH-CT-IDS: A Benchmark CT Image Dataset for Evaluation Semantic Segmentation, Object Detection and Radiomic Feature Extraction of Perihematomal Edema in Spontaneous Intracerebral Hemorrhage" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS2.5.1.1" class="ltx_text">II-B</span>2 </span>CT image of SICH</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.1" class="ltx_p">During CT examinations, ICH is characterized by uniformly high-density images in an elongated or circular shape, while PHE manifests as low-density images surrounding the site of hemorrhage. Examples of the related content can be observed in Figure <a href="#S2.F3" title="Figure 3 ‣ II-B2 CT image of SICH ‣ II-B Dataset description ‣ II Materials and Methods ‣ PHE-SICH-CT-IDS: A Benchmark CT Image Dataset for Evaluation Semantic Segmentation, Object Detection and Radiomic Feature Extraction of Perihematomal Edema in Spontaneous Intracerebral Hemorrhage" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2308.10521/assets/x3.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="150" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Example of CT images: (a) (b) (c) (d) (e) (f) Normal CT images without SICH, (g) (h) (i) (j) (k) (l) CT images of SICH.</figcaption>
</figure>
</section>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.5.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.6.2" class="ltx_text ltx_font_italic">Methods of segmentation</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Five classical machine learning methods, namely <math id="S2.SS3.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS3.p1.1.m1.1a"><mi id="S2.SS3.p1.1.m1.1.1" xref="S2.SS3.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.1.m1.1b"><ci id="S2.SS3.p1.1.m1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.1.m1.1c">k</annotation></semantics></math>-means, MRF, Otsu and Watershed, are employed for segmenting PHE-SICH-CT-IDS. Additionally, this study employs four distinct deep learning techniques for the segmentation of PHE-SICH-CT-IDS. These methods, classified as either classical or novel, encompass U-Net, SegNet, SwinUNet, and TransUNet.</p>
</div>
<section id="S2.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS3.SSS1.5.1.1" class="ltx_text">II-C</span>1 </span>Segmentation machine learning methods</h4>

<div id="S2.SS3.SSS1.p1" class="ltx_para">
<p id="S2.SS3.SSS1.p1.2" class="ltx_p"><math id="S2.SS3.SSS1.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS3.SSS1.p1.1.m1.1a"><mi id="S2.SS3.SSS1.p1.1.m1.1.1" xref="S2.SS3.SSS1.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p1.1.m1.1b"><ci id="S2.SS3.SSS1.p1.1.m1.1.1.cmml" xref="S2.SS3.SSS1.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p1.1.m1.1c">k</annotation></semantics></math>-means clustering algorithm is a well-known partitioning clustering and segmentation method. It is widely used in image segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.
<math id="S2.SS3.SSS1.p1.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS3.SSS1.p1.2.m2.1a"><mi id="S2.SS3.SSS1.p1.2.m2.1.1" xref="S2.SS3.SSS1.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p1.2.m2.1b"><ci id="S2.SS3.SSS1.p1.2.m2.1.1.cmml" xref="S2.SS3.SSS1.p1.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p1.2.m2.1c">k</annotation></semantics></math>-means clustering for extracting shape features from images characterized by low contrast and backgrounds exhibiting multiple levels of variation brings forth several advantages. These encompass clear contours, efficient algorithmic processing speed, and optimal utilization of memory resources. Therefore, it is an effective algorithm for grayscale image segmentation.</p>
</div>
<div id="S2.SS3.SSS1.p2" class="ltx_para">
<p id="S2.SS3.SSS1.p2.1" class="ltx_p">Markov Random Field (MRF) is a model described using an undirected graph <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. It consists of a set of nodes, where each node corresponds to a single variable or a group of variables. The links between nodes are undirected. In the context of image segmentation, it can be viewed as an image clustering problem. It involves grouping pixels with similar properties into the same class.</p>
</div>
<div id="S2.SS3.SSS1.p3" class="ltx_para">
<p id="S2.SS3.SSS1.p3.1" class="ltx_p">Otsu’s method, also known as the Otsu’s thresholding algorithm, was proposed in 1979 for determining the threshold value for image binarization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. In terms of the principle of Otsu’s method, it is also known as the maximum between-class variance method. This is because the threshold obtained through Otsu’s method results in the maximum between-class variance when applied to image binarization, separating the foreground and background. It is considered the optimal algorithm for threshold selection in image segmentation due to its simplicity, independence from image brightness and contrast, and wide application in digital image processing.</p>
</div>
<div id="S2.SS3.SSS1.p4" class="ltx_para">
<p id="S2.SS3.SSS1.p4.1" class="ltx_p">Watershed segmentation is a mathematical morphology-based method rooted in topological theory <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. Its fundamental concept is to treat an image as a topographic landscape in geodesy, where the grayscale value of each pixel represents its elevation. Each local minimum and its influence region are referred to as a catchment basin, while the boundaries between catchment basins form the watershed.</p>
</div>
</section>
<section id="S2.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS3.SSS2.5.1.1" class="ltx_text">II-C</span>2 </span>Deep learning methods</h4>

<div id="S2.SS3.SSS2.p1" class="ltx_para">
<p id="S2.SS3.SSS2.p1.1" class="ltx_p">U-Net is primarily used for addressing challenges in biomedical image analysis and has gained popularity in the field of medical image segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. It employs classical design methods such as downsampling, upsampling, and skip connections. Due to its U-shaped network structure, it is referred to as U-Net. The left half represents the feature extraction part, while the right half represents the upsampling part, also known as the encoder-decoder structure. The left side can be viewed as an encoder, and the right side can be viewed as a decoder. Additionally, the network employs skip connections by connecting the upsampling results with the outputs of the encoder submodules that have the same resolution. These connections serve as inputs for the subsequent submodules in the decoder.</p>
</div>
<div id="S2.SS3.SSS2.p2" class="ltx_para">
<p id="S2.SS3.SSS2.p2.1" class="ltx_p">UNet++ incorporates a series of short connections instead of the long connections that the original U-Net architecture includes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. By combining both long and short connections with varying receptive fields, UNet++ has the advantage of capturing features at different levels and integrating them through feature concatenation. This approach elevates its accuracy in performance. Furthermore, the flexible network structure, when coupled with deep supervision, allows for significant reduction in parameter count of deep networks without compromising the acceptable range of accuracy.</p>
</div>
<div id="S2.SS3.SSS2.p3" class="ltx_para">
<p id="S2.SS3.SSS2.p3.1" class="ltx_p">SegNet was introduced to address the challenges of semantic image segmentation in autonomous driving and intelligent robotics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. It consists of two parts: the Encoder and the Decoder. There exists a symmetric relationship between the Encoder and the Decoder.
The Encoder undertakes feature extraction via convolutions, while pooling enlarges the receptive field, thereby diminishing the image dimensions. Contrarily, the Decoder encompasses deconvolution and upsampling that reconstruct features post image classification, restoring them to their initial dimensions. Finally, the output is passed through softmax, yielding the maximum values for different classes and generating the final segmentation map.</p>
</div>
<div id="S2.SS3.SSS2.p4" class="ltx_para">
<p id="S2.SS3.SSS2.p4.1" class="ltx_p">TransUNet utilizes the encoder structure of the transformer in its encoder, allowing for better feature extraction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. The transformer was originally designed for sequence-to-sequence prediction, and its inherent global attention mechanism makes it a viable network structure. However, the transformer is deficient in capturing low-level details, which can result in limited localization capabilities. Due to the limited receptive field, CNNs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> struggle to effectively utilize global information, although they excel in extracting local details. In contrast, TransUNet combines both approaches, leveraging the advantages of both Transformers and U-Net. The transformer encodes image patches from CNN feature maps as input sequences to extract global features. The decoder upsamples the encoded features and combines them with high-resolution CNN features for precise localization.</p>
</div>
<div id="S2.SS3.SSS2.p5" class="ltx_para">
<p id="S2.SS3.SSS2.p5.1" class="ltx_p">Swin-UNet is a pure Transformer architecture similar to U-Net, specifically designed for medical image segmentation, comprising of an Encoder, Bottleneck, Decoder, and Skip Connections <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. The tokenized image blocks are fed into the Transformer-based U-shaped En-Decoder architecture through skip connections to facilitate local and global semantic feature learning. The Swin Transformer blocks are responsible for feature representation learning, while the decoder consists of Swin Transformer blocks and patch expansion layers. The extracted contextual features are fused with multi-scale features from the encoder through skip connections to compensate for spatial information loss caused by downsampling.</p>
</div>
</section>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS4.5.1.1" class="ltx_text">II-D</span> </span><span id="S2.SS4.6.2" class="ltx_text ltx_font_italic">Methods of detection</span>
</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">Faster R-CNN is the pioneering framework that achieved end-to-end implementation of object detection tasks using deep learning models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. It inherits the technical trajectory of both R-CNN and Fast R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> and further introduces the RPN network for efficient and batch-wise generation of region proposals. The algorithm consists of three components: feature extraction by the backbone network, region proposal generation using RPN, and object classification and regression by the RCNN.</p>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<p id="S2.SS4.p2.1" class="ltx_p">SSD (Single Shot MultiBox Detector) enables multi-scale object detection and has achieved superior performance compared to Faster R-CNN on various datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. Furthermore, in scenarios where the input image size is small, SSD exhibits higher accuracy compared to other one-stage methods. It introduces a single-stage detector that is more accurate and faster than previous algorithms such as YOLO, without employing RPN and pooling operations. As a result, SSD is simpler than two-stage object detection algorithms</p>
</div>
<div id="S2.SS4.p3" class="ltx_para">
<p id="S2.SS4.p3.1" class="ltx_p">YOLO algorithm is an example of a one-stage object detection algorithm, differing significantly from two-stage object detection algorithms in terms of computational speed. The YOLO series of algorithms divide an image into multiple grids and generate prior bounding boxes based on the anchor mechanism. By generating detection boxes in a single step, this approach greatly enhances the algorithm’s prediction speed.</p>
</div>
<div id="S2.SS4.p4" class="ltx_para">
<p id="S2.SS4.p4.1" class="ltx_p">The first improvement in YOLOv3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> is the replacement of the backbone network with a more effective one, DarkNet53, which extracts relevant features from images to achieve our desired objectives. Compared to DarkNet19 used in YOLOv2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, the new network employs a greater number of convolutions, specifically 53 layers of convolutions. Additionally, it incorporates residual connections from the residual network, thereby enhancing the network’s performance.</p>
</div>
<div id="S2.SS4.p5" class="ltx_para">
<p id="S2.SS4.p5.1" class="ltx_p">YOLOv4 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> improves the Average Precision (AP) and Frames Per Second (FPS) of YOLOv3 by 10% and 12% respectively. It incorporates the CSPDarkNet-53 network, which offers excellent trade-offs between speed and accuracy. The core of YOLOv4 is the adoption of the Cross Stage Partial Network structure. Additionally, several improvements are made to the input during training, including Mosaic data augmentation, cmBN (Cross mini-Batch Normalization), and SAT (Self-Adversarial Training).</p>
</div>
<div id="S2.SS4.p6" class="ltx_para">
<p id="S2.SS4.p6.1" class="ltx_p">YOLOv5 demonstrates further improvements in object detection performance compared to YOLOv4. It introduces new techniques such as the PAN structure and adaptive augmentation, resulting in enhanced detection accuracy. YOLOv5 incorporates a novel backbone network architecture called CSPNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, which outperforms the DarkNet53 in terms of computational efficiency and accuracy. As a result, YOLOv5 exhibits improvements in both speed and precision.</p>
</div>
</section>
<section id="S2.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS5.5.1.1" class="ltx_text">II-E</span> </span><span id="S2.SS5.6.2" class="ltx_text ltx_font_italic">Methods of feature extraction</span>
</h3>

<div id="S2.SS5.p1" class="ltx_para">
<p id="S2.SS5.p1.1" class="ltx_p">A total of 1316 texture features were extracted in this study. These features were automatically extracted from the expert-segmented PHE region. They included first-order statistics features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, shape features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, gray-level co-occurrence matrix (GLCM) features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, gray-level run-length matrix (GLRLM) features, gray-level size-zone matrix (GLSZM) features, gray-level dependence matrix (GLDM) features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> and neighborhood gray-tone difference matrix (NGTDM) features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>.
A consistency test was performed on the extracted radiomics features using the intra-class correlation coefficient (ICC), and only features with ICC &gt; 0.80 were retained for subsequent modeling.
The purpose of this study is to construct a model based on CT images, using the radiomics features of the PHE around hemorrhage, to predict the risk of HE in the short term in patients with SICH, and to provide better individualized treatment options for the clinic.</p>
</div>
<section id="S2.SS5.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS5.SSS1.5.1.1" class="ltx_text">II-E</span>1 </span>Radiomic features</h4>

<div id="S2.SS5.SSS1.p1" class="ltx_para">
<p id="S2.SS5.SSS1.p1.1" class="ltx_p">First-order statistics describe the distribution of voxel intensities within an image region defined by a mask, utilizing commonly used and fundamental measures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. These features provide information about the distribution and intensity levels of pixels in the image, with a total of 19 features. First-order features encompass several essential attributes of the image. These include Energy, measures the magnitude of voxel values in the image; Entropy, which specifies the uncertainty/randomness of values and quantifies the average information required to encode the image values; Minimum, representing the minimum grayscale value within the region of interest (ROI); Maximum, representing the maximum grayscale intensity within ROI, and others.</p>
</div>
<div id="S2.SS5.SSS1.p2" class="ltx_para">
<p id="S2.SS5.SSS1.p2.1" class="ltx_p">Shape features include descriptors for the three-dimensional size and shape of the ROI. These features are independent of the grayscale intensity distribution within the ROI and provide information about the shape, size, and spatial relationships of the objects or regions of interest <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. Shape features are calculated based on the boundaries or contours of objects. Common shape features include Mesh Volume, Voxel Volume, Surface Area, and others.</p>
</div>
<div id="S2.SS5.SSS1.p3" class="ltx_para">
<p id="S2.SS5.SSS1.p3.1" class="ltx_p">Gray Level Co-occurrence Matrix (GLCM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> is a statistical method used to describe image texture features. It quantifies the spatial relationships between grayscale values of pixels at varying directions and distances within the image <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>. By constructing a matrix that represents the co-occurrence of gray levels, diverse texture features can be extracted, including joint average, contrast, correlation, and more.</p>
</div>
<div id="S2.SS5.SSS1.p4" class="ltx_para">
<p id="S2.SS5.SSS1.p4.1" class="ltx_p">Similarly, the Gray Level Run Length Matrix (GLRLM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> finds application as a statistical method to elucidate the distribution of consecutive gray levels within an image. It computes the frequency of pixel runs with varying lengths for different gray levels, capturing distribution patterns and texture features. Notable GLRLM features encompass short run emphasis, long run emphasis, gray level non-uniformity, and more.</p>
</div>
<div id="S2.SS5.SSS1.p5" class="ltx_para">
<p id="S2.SS5.SSS1.p5.1" class="ltx_p">Furthermore, the Gray Level Size Zone Matrix (GLSZM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> operates as a statistical tool aimed at capturing the distribution of gray levels within regions of diverse sizes in an image. It computes the frequency of gray levels within these differently sized regions, thereby extracting texture features associated with size variation in the image. Notable GLSZM features include small area emphasis, gray level non-uniformity, normalized size-zone non-uniformity, and more <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>.</p>
</div>
<div id="S2.SS5.SSS1.p6" class="ltx_para">
<p id="S2.SS5.SSS1.p6.1" class="ltx_p">The Gray Level Dependence Matrix (GLDM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> takes on the role of a statistical method employed to uncover the interdependence among different gray levels within an image. It quantifies the differences and frequency of pixel-level changes across different gray levels, thereby extracting features related to the dependence of gray levels in the image. Prominent GLDM features comprise small dependence emphasis, large dependence emphasis, and more.</p>
</div>
<div id="S2.SS5.SSS1.p7" class="ltx_para">
<p id="S2.SS5.SSS1.p7.1" class="ltx_p">Lastly, the Neighborhood Gray Tone Difference Matrix (NGTDM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> emerges as a statistical approach used to illustrate the variations in gray levels among neighboring pixels within an image. It computes the differences and frequency of gray level changes within diverse pixel neighborhoods, thereby extracting texture features of the image. Noteworthy NGTDM features encompass small dependence emphasis, large dependence emphasis, dependence non-uniformity, and more.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Result</span>
</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">Results of segmentation</span>
</h3>

<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS1.5.1.1" class="ltx_text">III-A</span>1 </span>Results of segmentation machine learning methods</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">Four classical machine learning segmentation methods are experimentally evaluated on the PHE-SICH-CT-IDS, comparing and analyzing the segmentation images and performance under different machine learning methods. The experimental segmentation results are depicted in the Figure <a href="#S3.F4" title="Figure 4 ‣ III-A1 Results of segmentation machine learning methods ‣ III-A Results of segmentation ‣ III Result ‣ PHE-SICH-CT-IDS: A Benchmark CT Image Dataset for Evaluation Semantic Segmentation, Object Detection and Radiomic Feature Extraction of Perihematomal Edema in Spontaneous Intracerebral Hemorrhage" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. The distinct variations in results obtained by applying different classical machine learning segmentation methods demonstrate the effectiveness of PHE-SICH-CT-IDS in evaluating the performance of various segmentation methods. Additionally, the challenging segmentation of PHE highlights the urgent need for a dedicated edema segmentation dataset.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2308.10521/assets/x4.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="233" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Segmentation results of classical machine learning methods on the PHE-SICH-CT-IDS.</figcaption>
</figure>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS2.5.1.1" class="ltx_text">III-A</span>2 </span>Results of deep learning methods</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">The segmentation performance is evaluated on the current dataset using four different deep learning models. In the experiments, each model is trained using a ratio of 4:4:2 for the training, validation, and test sets, respectively. The learning rate is set to 0.000 05, the number of epochs is set to 100, and the batch size is set to 4. Figure <a href="#S3.F5" title="Figure 5 ‣ III-A2 Results of deep learning methods ‣ III-A Results of segmentation ‣ III Result ‣ PHE-SICH-CT-IDS: A Benchmark CT Image Dataset for Evaluation Semantic Segmentation, Object Detection and Radiomic Feature Extraction of Perihematomal Edema in Spontaneous Intracerebral Hemorrhage" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> presents the segmentation results obtained using three different models. The evaluation employs five commonly used metrics, including Dice coefficient, Jaccard index, Hausdorff distance, precision, and recall. The evaluation metrics for the segmentation experiments are shown in Table <a href="#S3.T3" title="TABLE III ‣ III-A2 Results of deep learning methods ‣ III-A Results of segmentation ‣ III Result ‣ PHE-SICH-CT-IDS: A Benchmark CT Image Dataset for Evaluation Semantic Segmentation, Object Detection and Radiomic Feature Extraction of Perihematomal Edema in Spontaneous Intracerebral Hemorrhage" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>.</p>
</div>
<div id="S3.SS1.SSS2.p2" class="ltx_para">
<p id="S3.SS1.SSS2.p2.1" class="ltx_p">The experiments were conducted on an NVIDIA GeForce RTX 2080 GPU with 8 GB of memory. In terms of software, the programming was done in Python 3.8, and the PyTorch framework version 1.7.0 was utilized.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2308.10521/assets/x5.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="309" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Segmentation results of deep learning methods on the PHE-SICH-CT-IDS.</figcaption>
</figure>
<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Evaluation metrics for different segmentation methods based on deep learning.</figcaption>
<table id="S3.T3.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T3.1.1" class="ltx_tr">
<td id="S3.T3.1.1.1" class="ltx_td ltx_align_center ltx_border_tt">Methods</td>
<td id="S3.T3.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">Dice</td>
<td id="S3.T3.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">Jaccard index</td>
<td id="S3.T3.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">Hausdorff distance</td>
<td id="S3.T3.1.1.5" class="ltx_td ltx_align_center ltx_border_tt">Precision</td>
<td id="S3.T3.1.1.6" class="ltx_td ltx_align_center ltx_border_tt">Recall</td>
</tr>
<tr id="S3.T3.1.2" class="ltx_tr">
<td id="S3.T3.1.2.1" class="ltx_td ltx_align_center ltx_border_t">U-Net</td>
<td id="S3.T3.1.2.2" class="ltx_td ltx_align_center ltx_border_t">0.2269</td>
<td id="S3.T3.1.2.3" class="ltx_td ltx_align_center ltx_border_t">0.1488</td>
<td id="S3.T3.1.2.4" class="ltx_td ltx_align_center ltx_border_t">68.2550</td>
<td id="S3.T3.1.2.5" class="ltx_td ltx_align_center ltx_border_t">0.6332</td>
<td id="S3.T3.1.2.6" class="ltx_td ltx_align_center ltx_border_t">0.1582</td>
</tr>
<tr id="S3.T3.1.3" class="ltx_tr">
<td id="S3.T3.1.3.1" class="ltx_td ltx_align_center">UNet++</td>
<td id="S3.T3.1.3.2" class="ltx_td ltx_align_center">0.2178</td>
<td id="S3.T3.1.3.3" class="ltx_td ltx_align_center">0.1456</td>
<td id="S3.T3.1.3.4" class="ltx_td ltx_align_center">54.0571</td>
<td id="S3.T3.1.3.5" class="ltx_td ltx_align_center">0.6086</td>
<td id="S3.T3.1.3.6" class="ltx_td ltx_align_center">0.1519</td>
</tr>
<tr id="S3.T3.1.4" class="ltx_tr">
<td id="S3.T3.1.4.1" class="ltx_td ltx_align_center">SegNet</td>
<td id="S3.T3.1.4.2" class="ltx_td ltx_align_center">0.1863</td>
<td id="S3.T3.1.4.3" class="ltx_td ltx_align_center">0.1208</td>
<td id="S3.T3.1.4.4" class="ltx_td ltx_align_center">48.0345</td>
<td id="S3.T3.1.4.5" class="ltx_td ltx_align_center">0.5887</td>
<td id="S3.T3.1.4.6" class="ltx_td ltx_align_center">0.1224</td>
</tr>
<tr id="S3.T3.1.5" class="ltx_tr">
<td id="S3.T3.1.5.1" class="ltx_td ltx_align_center">TransUNet</td>
<td id="S3.T3.1.5.2" class="ltx_td ltx_align_center">0.2451</td>
<td id="S3.T3.1.5.3" class="ltx_td ltx_align_center">0.1552</td>
<td id="S3.T3.1.5.4" class="ltx_td ltx_align_center">52.4001</td>
<td id="S3.T3.1.5.5" class="ltx_td ltx_align_center">0.7631</td>
<td id="S3.T3.1.5.6" class="ltx_td ltx_align_center">0.1591</td>
</tr>
<tr id="S3.T3.1.6" class="ltx_tr">
<td id="S3.T3.1.6.1" class="ltx_td ltx_align_center ltx_border_bb">Swin-UNet</td>
<td id="S3.T3.1.6.2" class="ltx_td ltx_align_center ltx_border_bb">0.3512</td>
<td id="S3.T3.1.6.3" class="ltx_td ltx_align_center ltx_border_bb">0.2285</td>
<td id="S3.T3.1.6.4" class="ltx_td ltx_align_center ltx_border_bb">47.8789</td>
<td id="S3.T3.1.6.5" class="ltx_td ltx_align_center ltx_border_bb">0.4720</td>
<td id="S3.T3.1.6.6" class="ltx_td ltx_align_center ltx_border_bb">0.3321</td>
</tr>
</table>
</figure>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Results of detection</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In this study, five commonly used object detection models with different architectures are employed to test the feasibility of using the PHE-SICH-CT-IDS for object detection. The targets of detection include not only the hemorrhagic regions but also the edematous zones. The detection results are depicted in Figure <a href="#S3.F6" title="Figure 6 ‣ III-B Results of detection ‣ III Result ‣ PHE-SICH-CT-IDS: A Benchmark CT Image Dataset for Evaluation Semantic Segmentation, Object Detection and Radiomic Feature Extraction of Perihematomal Edema in Spontaneous Intracerebral Hemorrhage" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. The performance is evaluated using four widely adopted metrics: AP (Average Precision), F1-score, precision, and recall, as shown in Table <a href="#S3.T4" title="TABLE IV ‣ III-B Results of detection ‣ III Result ‣ PHE-SICH-CT-IDS: A Benchmark CT Image Dataset for Evaluation Semantic Segmentation, Object Detection and Radiomic Feature Extraction of Perihematomal Edema in Spontaneous Intracerebral Hemorrhage" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>. For this experiment, the models are trained for 300 epochs with a learning rate of 0.000 1 and a batch size of 4.</p>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2308.10521/assets/x6.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="178" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>The object detection experiment results on the PHE-SICH-CT-IDS are presented. The first column shows the original CT images, the second column displays the overlay of the segmented edematous regions on the original images, and the third column shows the ground truth bounding boxes of both edematous regions and hemorrhagic lesions. The fourth to eighth columns illustrate the detection results of Faster R-CNN, SSD, YOLOv3, YOLOv4, and YOLOv5, respectively.</figcaption>
</figure>
<figure id="S3.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>The evaluation results of the object detection experiments on the PHE-SICH-CT-IDS.</figcaption>
<table id="S3.T4.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T4.1.1" class="ltx_tr">
<td id="S3.T4.1.1.1" class="ltx_td ltx_align_center ltx_border_tt">Methods</td>
<td id="S3.T4.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">AP</td>
<td id="S3.T4.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">F1-score</td>
<td id="S3.T4.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">Precision</td>
<td id="S3.T4.1.1.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt">Recall</td>
</tr>
<tr id="S3.T4.1.2" class="ltx_tr">
<td id="S3.T4.1.2.1" class="ltx_td ltx_align_center ltx_border_t">Faster R-CNN</td>
<td id="S3.T4.1.2.2" class="ltx_td ltx_align_center ltx_border_t">96.56</td>
<td id="S3.T4.1.2.3" class="ltx_td ltx_align_center ltx_border_t">0.95</td>
<td id="S3.T4.1.2.4" class="ltx_td ltx_align_center ltx_border_t">91.55</td>
<td id="S3.T4.1.2.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">96.55</td>
</tr>
<tr id="S3.T4.1.3" class="ltx_tr">
<td id="S3.T4.1.3.1" class="ltx_td ltx_align_center">SSD</td>
<td id="S3.T4.1.3.2" class="ltx_td ltx_align_center">95.44</td>
<td id="S3.T4.1.3.3" class="ltx_td ltx_align_center">0.94</td>
<td id="S3.T4.1.3.4" class="ltx_td ltx_align_center">91.89</td>
<td id="S3.T4.1.3.5" class="ltx_td ltx_nopad_r ltx_align_center">95.77</td>
</tr>
<tr id="S3.T4.1.4" class="ltx_tr">
<td id="S3.T4.1.4.1" class="ltx_td ltx_align_center">YOLOv3</td>
<td id="S3.T4.1.4.2" class="ltx_td ltx_align_center">92.73</td>
<td id="S3.T4.1.4.3" class="ltx_td ltx_align_center">0.77</td>
<td id="S3.T4.1.4.4" class="ltx_td ltx_align_center">97.62</td>
<td id="S3.T4.1.4.5" class="ltx_td ltx_nopad_r ltx_align_center">62.35</td>
</tr>
<tr id="S3.T4.1.5" class="ltx_tr">
<td id="S3.T4.1.5.1" class="ltx_td ltx_align_center">YOLOv4</td>
<td id="S3.T4.1.5.2" class="ltx_td ltx_align_center">94.33</td>
<td id="S3.T4.1.5.3" class="ltx_td ltx_align_center">0.91</td>
<td id="S3.T4.1.5.4" class="ltx_td ltx_align_center">94.03</td>
<td id="S3.T4.1.5.5" class="ltx_td ltx_nopad_r ltx_align_center">88.73</td>
</tr>
<tr id="S3.T4.1.6" class="ltx_tr">
<td id="S3.T4.1.6.1" class="ltx_td ltx_align_center ltx_border_bb">YOLOv5</td>
<td id="S3.T4.1.6.2" class="ltx_td ltx_align_center ltx_border_bb">96.37</td>
<td id="S3.T4.1.6.3" class="ltx_td ltx_align_center ltx_border_bb">0.96</td>
<td id="S3.T4.1.6.4" class="ltx_td ltx_align_center ltx_border_bb">97.10</td>
<td id="S3.T4.1.6.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">94.37</td>
</tr>
</table>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.5.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.6.2" class="ltx_text ltx_font_italic">Results of feature extraction</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The sub-dataset A was randomly partitioned into a training set and a validation set in a 7:3 ratio. The features were normalized using the Z-score algorithm, dimensionality reduction was performed using pearson correlation coefficient (PCC) algorithm. Eight radiomic features were selected using analysis of variance (ANOVA), recursive feature elimination (RFE), and relief algorithms, with their weighted coefficients shown in Table <a href="#S3.T5" title="TABLE V ‣ III-C Results of feature extraction ‣ III Result ‣ PHE-SICH-CT-IDS: A Benchmark CT Image Dataset for Evaluation Semantic Segmentation, Object Detection and Radiomic Feature Extraction of Perihematomal Edema in Spontaneous Intracerebral Hemorrhage" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>. The machine learning model employed logistic regression and incorporated clinical parameters for modeling analysis. The training set achieved an area under curve (AUC) value of 0.835. The validation set yielded an AUC value of 0.745.</p>
</div>
<figure id="S3.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE V: </span>The refined features and their corresponding weights after the selection process.</figcaption>
<table id="S3.T5.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T5.1.1" class="ltx_tr">
<td id="S3.T5.1.1.1" class="ltx_td ltx_align_center ltx_border_tt">Feature</td>
<td id="S3.T5.1.1.2" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt">Weight</td>
</tr>
<tr id="S3.T5.1.2" class="ltx_tr">
<td id="S3.T5.1.2.1" class="ltx_td ltx_align_center ltx_border_t">CT_gradient_firstorder_Skewness</td>
<td id="S3.T5.1.2.2" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">-0.745</td>
</tr>
<tr id="S3.T5.1.3" class="ltx_tr">
<td id="S3.T5.1.3.1" class="ltx_td ltx_align_center">CT_square_firstorder_RobustMeanAbsoluteDeviation</td>
<td id="S3.T5.1.3.2" class="ltx_td ltx_nopad_r ltx_align_center">1.366</td>
</tr>
<tr id="S3.T5.1.4" class="ltx_tr">
<td id="S3.T5.1.4.1" class="ltx_td ltx_align_center">CT_squareroot_glcm_ClusterShade</td>
<td id="S3.T5.1.4.2" class="ltx_td ltx_nopad_r ltx_align_center">-0.601</td>
</tr>
<tr id="S3.T5.1.5" class="ltx_tr">
<td id="S3.T5.1.5.1" class="ltx_td ltx_align_center">CT_squareroot_gldm_LargeDependenceLowGrayLevelEmphasis</td>
<td id="S3.T5.1.5.2" class="ltx_td ltx_nopad_r ltx_align_center">0.651</td>
</tr>
<tr id="S3.T5.1.6" class="ltx_tr">
<td id="S3.T5.1.6.1" class="ltx_td ltx_align_center">CT_squareroot_glszm_LowGrayLevelZoneEmphasis</td>
<td id="S3.T5.1.6.2" class="ltx_td ltx_nopad_r ltx_align_center">0.092</td>
</tr>
<tr id="S3.T5.1.7" class="ltx_tr">
<td id="S3.T5.1.7.1" class="ltx_td ltx_align_center">CT_wavelet-HLH_glszm_SmallAreaHighGrayLevelEmphasis</td>
<td id="S3.T5.1.7.2" class="ltx_td ltx_nopad_r ltx_align_center">0.229</td>
</tr>
<tr id="S3.T5.1.8" class="ltx_tr">
<td id="S3.T5.1.8.1" class="ltx_td ltx_align_center">CT_wavelet-HLL_glcm_Correlation</td>
<td id="S3.T5.1.8.2" class="ltx_td ltx_nopad_r ltx_align_center">-0.707</td>
</tr>
<tr id="S3.T5.1.9" class="ltx_tr">
<td id="S3.T5.1.9.1" class="ltx_td ltx_align_center ltx_border_bb">CT_wavelet-LLL_firstorder_Maximum</td>
<td id="S3.T5.1.9.2" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">-1.069</td>
</tr>
</table>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Discussion</span>
</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">Segmentation results discussion</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">According to Figure <a href="#S3.F4" title="Figure 4 ‣ III-A1 Results of segmentation machine learning methods ‣ III-A Results of segmentation ‣ III Result ‣ PHE-SICH-CT-IDS: A Benchmark CT Image Dataset for Evaluation Semantic Segmentation, Object Detection and Radiomic Feature Extraction of Perihematomal Edema in Spontaneous Intracerebral Hemorrhage" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, the PHE-SICH-CT-IDS exhibits noticeable variations in segmentation results when different classic machine learning segmentation methods are employed. However, it is challenging to accurately segment the PHE in CT images due to several reasons. Firstly, the edematous region associated with cerebral hemorrhage exhibits similarity in morphology and density with surrounding tissues, making it difficult to achieve clear segmentation from the surrounding structures. This similarity can result in errors in machine learning models, particularly in boundary regions or in edematous areas with complex shapes. Secondly, the edematous region displays significant variability across different CT images, including differences in size, shape, and location. This variability poses a challenge in building a universal model capable of accommodating different patients, scanners, and scanning parameters, thereby limiting the model’s generalizability. As a result, classical machine learning methods face significant difficulties in accurately segmenting the edematous region.
However, in this study, distinct segmentation results were achieved for CT images. There are noticeable differences among different classic machine learning segmentation methods. Hence, the PHE-SICH-CT-IDS proves to be effective in assessing the segmentation performance of different classic machine learning segmentation methods.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Overall, deep learning methods outperform classical machine learning methods. Comparing the results in Figure <a href="#S3.F4" title="Figure 4 ‣ III-A1 Results of segmentation machine learning methods ‣ III-A Results of segmentation ‣ III Result ‣ PHE-SICH-CT-IDS: A Benchmark CT Image Dataset for Evaluation Semantic Segmentation, Object Detection and Radiomic Feature Extraction of Perihematomal Edema in Spontaneous Intracerebral Hemorrhage" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and Figure <a href="#S3.F5" title="Figure 5 ‣ III-A2 Results of deep learning methods ‣ III-A Results of segmentation ‣ III Result ‣ PHE-SICH-CT-IDS: A Benchmark CT Image Dataset for Evaluation Semantic Segmentation, Object Detection and Radiomic Feature Extraction of Perihematomal Edema in Spontaneous Intracerebral Hemorrhage" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, it is evident that deep learning methods accurately locate and segment the objects. In the PHE-SICH-CT-IDS, the Swin-UNet achieves the highest Dice coefficient, Jaccard index, and recall, reaching 0.35, 0.23, and 0.33, respectively. The TransUNet achieves the highest precision at 0.76. It can be observed that the two models combining the Transformer <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">vaswani2017attention</span>]</cite> with the U-Net architecture perform the best. However, these models have larger sizes and longer training times. Among them, SegNet has the shortest training time and the smallest model size, but it also has the poorest performance among the five models. U-Net and UNet++ exhibit comparable performance. However, they demonstrate significant dissimilarity in terms of Hausdorff distance.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.5.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.6.2" class="ltx_text ltx_font_italic">Detection results discussion</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.2" class="ltx_p">Based on the observation in Figure <a href="#S3.F6" title="Figure 6 ‣ III-B Results of detection ‣ III Result ‣ PHE-SICH-CT-IDS: A Benchmark CT Image Dataset for Evaluation Semantic Segmentation, Object Detection and Radiomic Feature Extraction of Perihematomal Edema in Spontaneous Intracerebral Hemorrhage" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, the object detection algorithms demonstrate excellent performance on the PHE-SICH-CT-IDS, accurately detecting almost all targets, including hemorrhagic regions and PHE zones. Table <a href="#S3.T4" title="TABLE IV ‣ III-B Results of detection ‣ III Result ‣ PHE-SICH-CT-IDS: A Benchmark CT Image Dataset for Evaluation Semantic Segmentation, Object Detection and Radiomic Feature Extraction of Perihematomal Edema in Spontaneous Intracerebral Hemorrhage" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> presents significant variations in various evaluation metrics among the different algorithms. Notably, YOLOv5 exhibits the best overall performance in object detection, achieving the highest precision of around 97<math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mo id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><csymbol cd="latexml" id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">\%</annotation></semantics></math>. However, YOLOv3 shows the lowest F1-score and recall, at 0.77 and 62.35<math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mo id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><csymbol cd="latexml" id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">\%</annotation></semantics></math>, respectively. Additionally, SSD exhibits the fastest detection speed among the tested methods. Therefore, the PHE-SICH-CT-IDS proves to be effectively utilized for image object detection.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.5.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.6.2" class="ltx_text ltx_font_italic">Feature extraction results discussion</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">This study demonstrates the value of a combined model constructed using texture features extracted from CT images for prognostic predictions in patients with SICH. By applying this combined model, frontline clinicians can self-assess the risk of hematoma expansion in SICH patients and develop personalized treatment strategies. Based on these findings, it is concluded that PHE-SICH-CT-IDS can be utilized to evaluate image features.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion and futures works</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This study developed the first open-source CT image dataset, named PHE-SICH-CT-IDS, specifically designed for segmenting PHE in patients with spontaneous basal ganglia intracerebral hemorrhage. In addition to the segmentation of PHE, the dataset provides functionality for hemorrhage detection and radiomic feature extraction. PHE-SICH-CT-IDS consists of three sub-datasets with different data types, allowing users to select the appropriate dataset based on their specific needs. To evaluate the segmentation performance, we employ commonly used machine learning methods as well as five deep learning segmentation methods, including U-Net and Swin-UNet. The experimental results demonstrated that PHE-SICH-CT-IDS can discern between different segmentation methods, supported by multiple evaluation metrics. Furthermore, five object detection methods have been used during the experiments such as Faster R-CNN and YOLOv5, which accurately identify the hemorrhage region and the surrounding edema. Thus, PHE-SICH-CT-IDS holds promise for effective image-based object detection. Additionally, we extract a wide range of radiomic features and construct a joint model combining logistic regression and clinical parameters. The classification performance of this model was significantly improved, providing valuable assistance to frontline clinicians in assessing the HE risk in patients with SICH.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">In the forthcoming future, the dataset will undergo expansion to encompass a broader scope of medical scenarios. Concurrently, efforts will focus on refining and advancing the segmentation methods to provide a more sophisticated and robust resource for the medical community. This ongoing pursuit of excellence aims to empower medical professionals with cutting-edge tools and foster groundbreaking advancements in the realm of medical research.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This work is supported by the “National Natural Science Foundation of China” (No. 82220108007). We thank B.A. Qi Qiu, from Foreign Studies College in Northeastern University, China, for her professional English proofreading in this paper. We also thank Miss. Zixian Li and Mr. Guoxian Li for their important discussion in this work.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Declaration of Competing Interest</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">The authors declare that they have no conflict of interest in this paper.</p>
</div>
</section>
<section id="Sx3" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Data Cvailability Statement</h2>

<div id="Sx3.p1" class="ltx_para">
<p id="Sx3.p1.1" class="ltx_p">The datasets featured in this study are available in online repositories.
The repository name and accession number(s) can be found below: <a target="_blank" href="https://figshare.com/articles/dataset/PHE-SICH-CT-IDS/23957937" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://figshare.com/articles/dataset/PHE-SICH-CT-IDS/23957937</a>.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
R. F. Keep, Y. Hua, and G. Xi, “Intracerebral haemorrhage: mechanisms of
injury and therapeutic targets,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">The Lancet Neurology</em>, vol. 11,
no. 8, pp. 720–731, 2012.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
E. J. Benjamin, P. Muntner, and A. Alonso, “Heart disease and stroke
statistics-2019 update: A report from the american heart association,”
<em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Circulation</em>, vol. 139, no. 10, p. e56–528, 2019.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
O. Adeoye and J. P. Broderick, “Advances in the management of intracerebral
hemorrhage,” <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Nature Reviews Neurology</em>, vol. 6, no. 11, pp. 593–601,
2010.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
E. C. Jauch, J. A. Pineda, and J. C. Hemphill, “Emergency neurological life
support: Intracerebral hemorrhage,” <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Neurocritical Care</em>, vol. 23, no.
2 Supplement, pp. 83–93, 2015.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
C. Asch, M. J. Luitse, G. J. Rinkel, I. Tweel, and C. J. Klijn, “Incidence,
case fatality, and functional outcome of intracerebral haemorrhage over time,
according to age, sex, and ethnic origin: a systematic review and
meta-analysis.” <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Lancet Neurology</em>, vol. 9, no. 2, pp. 167–176, 2010.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
D. Dowlatshahi, A. M. Demchuk, M. L. Flaherty, M. Ali, P. L. Lyden, and E. E.
Smith, “Defining hematoma expansion in intracerebral hemorrhage:
Relationship with patient outcomes,” <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Neurology</em>, vol. 76, no. 14, pp.
1238–1244, 2011.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks for
biomedical image segmentation,” 2015.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
S. Chilamkurthy, R. Ghosh, S. Tanamala, M. Biviji, N. G. Campeau, V. K.
Venugopal, V. Mahajan, P. Rao, and P. Warier, “Development and validation of
deep learning algorithms for detection of critical findings in head ct
scans,” 2018.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
S. Chilamkurthy, R. Ghosh, S. Tanamala, M. Biviji, and Campeau, “Deep learning
algorithms for detection of critical findings in head ct scans: a
retrospective study,” <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">The Lancet</em>, vol. 392, no. 10162, pp.
2388–2396, 2018.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
M. D. Hssayeni, M. S. Croock, A. D. Salman, H. F. Al-khafaji, Z. A. Yahya, and
B. Ghoraani, “Intracranial hemorrhage segmentation using a deep
convolutional model,” <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Data</em>, vol. 5, no. 1, p. 14, 2020.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
M. Hssayeni, M. Croock, A. Salman, H. Al-khafaji, Z. Yahya, and B. Ghoraani,
“Computed tomography images for intracranial hemorrhage detection and
segmentation,” <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Intracranial Hemorrhage Segmentation Using A Deep
Convolutional Model. Data</em>, vol. 5, no. 1, p. 14, 2020.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Y. Chen, C. Qin, J. Chang, Y. Liu, Q. Zhang, Z. Ye, Z. Li, F. Tian, W. Ma,
J. Wei <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Defining delayed perihematomal edema expansion in
intracerebral hemorrhage: segmentation, time course, risk factors and
clinical outcome,” <em id="bib.bib12.2.2" class="ltx_emph ltx_font_italic">Frontiers in Immunology</em>, vol. 13, 2022.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Z. Kuang, Z. Yan, L. Yu, X. Deng, Y. Hua, and S. Li, “Uncertainty-aware deep
learning with cross-task supervision for phe segmentation on ct images,”
<em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">IEEE Journal of Biomedical and Health Informatics</em>, vol. 26, no. 6, pp.
2615–2626, 2022.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
A. E. Flanders, L. M. Prevedello, G. Shih, S. S. Halabi, J. Kalpathy-Cramer,
R. Ball, J. T. Mongan, A. Stein, F. C. Kitamura, M. P. Lungren <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">et al.</em>,
“Construction of a machine learning dataset through collaboration: the rsna
2019 brain ct hemorrhage challenge,” <em id="bib.bib14.2.2" class="ltx_emph ltx_font_italic">Radiology: Artificial
Intelligence</em>, vol. 2, no. 3, p. e190211, 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
S.-L. Liew, B. P. Lo, M. R. Donnelly, A. Zavaliangos-Petropulu, J. N. Jeong,
G. Barisano, A. Hutton, J. P. Simon, J. M. Juliano, A. Suri <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">et al.</em>,
“A large, curated, open-source stroke neuroimaging dataset to improve lesion
segmentation algorithms,” <em id="bib.bib15.2.2" class="ltx_emph ltx_font_italic">Scientific data</em>, vol. 9, no. 1, p. 320,
2022.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
N. Dhanachandra, K. Manglem, and Y. J. Chanu, “Image segmentation using
k-means clustering algorithm and subtractive clustering algorithm,”
<em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Procedia Computer Science</em>, vol. 54, pp. 764–771, 2015.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
H. Deng and D. A. Clausi, “Unsupervised image segmentation using a simple mrf
model with a new implementation scheme,” 2004, pp. 2323–2335.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
D. Liu and J. Yu, “Otsu method and k-means,” in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">2009 Ninth
International conference on hybrid intelligent systems</em>, vol. 1.   IEEE, 2009, pp. 344–349.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
R. Shojaii, J. Alirezaie, and P. Babyn, “Automatic lung segmentation in ct
images using watershed transform,” in <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">IEEE international conference on
image processing 2005</em>, vol. 2.   IEEE,
2005, pp. II–1270.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Z. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, “Unet++: A nested
u-net architecture for medical image segmentation,” in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Deep Learning
in Medical Image Analysis and Multimodal Learning for Clinical Decision
Support: 4th International Workshop, DLMIA 2018, and 8th International
Workshop, ML-CDS 2018, Held in Conjunction with MICCAI 2018, Granada, Spain,
September 20, 2018, Proceedings 4</em>.   Springer, 2018, pp. 3–11.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
V. Badrinarayanan, A. Kendall, and R. Cipolla, “Segnet: A deep convolutional
encoder-decoder architecture for image segmentation,” 2016.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
J. Chen, Y. Lu, Q. Yu, X. Luo, E. Adeli, Y. Wang, L. Lu, A. L. Yuille, and
Y. Zhou, “Transunet: Transformers make strong encoders for medical image
segmentation,” 2021.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning
applied to document recognition,” <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE</em>, vol. 86,
no. 11, pp. 2278–2324, 1998.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification with
deep convolutional neural networks,” <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Communications of the ACM</em>,
vol. 60, no. 6, pp. 84–90, 2017.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
H. Cao, Y. Wang, J. Chen, D. Jiang, X. Zhang, Q. Tian, and M. Wang,
“Swin-unet: Unet-like pure transformer for medical image segmentation,” in
<em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">European conference on computer vision</em>.   Springer, 2022, pp. 205–218.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time
object detection with region proposal networks,” <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Advances in neural
information processing systems</em>, vol. 28, 2015.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
R. Girshick, “Fast r-cnn,” in <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international
conference on computer vision</em>, 2015, pp. 1440–1448.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C. Berg,
“Ssd: Single shot multibox detector,” in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Computer Vision–ECCV 2016:
14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016,
Proceedings, Part I 14</em>.   Springer,
2016, pp. 21–37.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
——, “Yolov3: An incremental improvement,” <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1804.02767</em>, 2018.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
J. Redmon and A. Farhadi, “Yolo9000: better, faster, stronger,” in
<em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern
recognition</em>, 2017, pp. 7263–7271.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
A. Bochkovskiy, C.-Y. Wang, and H.-Y. M. Liao, “Yolov4: Optimal speed and
accuracy of object detection,” <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2004.10934</em>, 2020.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
C.-Y. Wang, H.-Y. M. Liao, Y.-H. Wu, P.-Y. Chen, J.-W. Hsieh, and I.-H. Yeh,
“Cspnet: A new backbone that can enhance learning capability of cnn,” in
<em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition workshops</em>, 2020, pp. 390–391.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Y. Peng, P. Luan, H. Tu, X. Li, and P. Zhou, “Pulmonary fissure segmentation
in ct images based on odos filter and shape features,” <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Multimedia
Tools and Applications</em>, pp. 1–22, 2023.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Q. Firdaus, R. Sigit, T. Harsono, and A. Anwar, “Lung cancer detection based
on ct-scan images with detection features using gray level co-occurrence
matrix (glcm) and support vector machine (svm) methods,” in <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">2020
International Electronics Symposium (IES)</em>.   IEEE, 2020, pp. 643–648.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
N. Ahmadi and G. Akbarizadeh, “Iris tissue recognition based on gldm feature
extraction and hybrid mlpnn-ica classifier,” <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Neural Computing and
Applications</em>, vol. 32, pp. 2267–2281, 2020.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
B. Pathak and D. Barooah, “Texture analysis based on the gray-level
co-occurrence matrix considering possible orientations,” <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">International
Journal of Advanced Research in Electrical, Electronics and Instrumentation
Engineering</em>, vol. 2, no. 9, pp. 4206–4212, 2013.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Ş. Öztürk and B. Akdemir, “Application of feature extraction and
classification methods for histopathological image using glcm, lbp, lbglcm,
glrlm and sfta,” <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Procedia computer science</em>, vol. 132, pp. 40–46,
2018.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
G. Thibault, J. Angulo, and F. Meyer, “Advanced statistical matrices for
texture characterization: application to cell classification,” <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">IEEE
Transactions on Biomedical Engineering</em>, vol. 61, no. 3, pp. 630–637, 2013.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
H. J. Aerts, E. R. Velazquez, R. T. Leijenaar, C. Parmar, P. Grossmann,
S. Carvalho, J. Bussink, R. Monshouwer, B. Haibe-Kains, D. Rietveld
<em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Decoding tumour phenotype by noninvasive imaging using a
quantitative radiomics approach,” <em id="bib.bib39.2.2" class="ltx_emph ltx_font_italic">Nature communications</em>, vol. 5,
no. 1, p. 4006, 2014.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Computer Science</em>, 2014.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2308.10520" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2308.10521" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2308.10521">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2308.10521" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2308.10522" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 12:03:58 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
