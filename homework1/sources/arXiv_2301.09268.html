<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2301.09268] PCBDet: An Efficient Deep Neural Network Object Detection Architecture for Automatic PCB Component Detection on the Edge</title><meta property="og:description" content="There can be numerous electronic components on a given PCB, making the task of visual inspection to detect defects very time-consuming and prone to error, especially at scale. There has thus been significant interest iâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="PCBDet: An Efficient Deep Neural Network Object Detection Architecture for Automatic PCB Component Detection on the Edge">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="PCBDet: An Efficient Deep Neural Network Object Detection Architecture for Automatic PCB Component Detection on the Edge">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2301.09268">

<!--Generated on Fri Mar  1 05:58:23 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">PCBDet: An Efficient Deep Neural Network Object Detection Architecture for Automatic PCB Component Detection on the Edge</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Brian Li
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Steven Palayew
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Francis Li
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">DarwinAI, Waterloo, Ontario, Canada
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Saad Abbasi
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">DarwinAI, Waterloo, Ontario, Canada
</span>
<span class="ltx_contact ltx_role_affiliation">University of Waterloo, Waterloo, Ontario, Canada
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Saeejith Nair
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">University of Waterloo, Waterloo, Ontario, Canada
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Alexander Wong
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">DarwinAI, Waterloo, Ontario, Canada
</span>
<span class="ltx_contact ltx_role_affiliation">University of Waterloo, Waterloo, Ontario, Canada
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.2" class="ltx_p">There can be numerous electronic components on a given PCB, making the task of visual inspection to detect defects very time-consuming and prone to error, especially at scale. There has thus been significant interest in automatic PCB component detection, particularly leveraging deep learning. However, deep neural networks typically require high computational resources, possibly limiting their feasibility in real-world use cases in manufacturing, which often involve high-volume and high-throughput detection with constrained edge computing resource availability. As a result of an exploration of efficient deep neural network architectures for this use case, we introduce PCBDet, an attention condenser network design that provides state-of-the-art inference throughput while achieving superior PCB component detection performance compared to other state-of-the-art efficient architecture designs. Experimental results show that PCBDet can achieve up to 2<math id="id1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="id1.1.m1.1a"><mo id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><times id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">\times</annotation></semantics></math> inference speed-up on an ARM Cortex A72 processor when compared to an EfficientNet-based design while achieving <math id="id2.2.m2.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="id2.2.m2.1a"><mo id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><csymbol cd="latexml" id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">\sim</annotation></semantics></math>2-4% higher mAP on the FICS-PCB benchmark dataset.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">A crucial process in printed circuit board assembly is the visual inspection of electronic components for potential defects. This can help avoid functional failure of devices, user data leakage, or even system control taken by adversaries Â <cite class="ltx_cite ltx_citemacro_citet">Lu etÂ al. (<a href="#bib.bib4" title="" class="ltx_ref">2020</a>)</cite>. Given that there can be hundreds of electronic components on a given PCB, the task of visual inspection can be extremely time-consuming and prone to operator error, especially during large assembly runs. Therefore, the ability to automatically detect different electronic components on a PCB board for automated inspection purposes is highly desired. As a result, there has been significant interest in the research community in automatic PCB component detection, particularly leveraging deep learning Â <cite class="ltx_cite ltx_citemacro_citet">Kuo etÂ al. (<a href="#bib.bib1" title="" class="ltx_ref">2019</a>); Li etÂ al. (<a href="#bib.bib2" title="" class="ltx_ref">2022</a>)</cite>. However, one consideration that has been largely left unexplored in research literature in this area is computational efficiency, which is particularly critical for real-world visual quality inspection scenarios involving high-volume, high-throughput electronics manufacturing use-cases under constrained edge computing resources.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Motivated by the need for both high efficiency and high accuracy for automatic PCB component detection, this study explores efficient deep neural network object detection architectures for the purpose of automatic PCB component detection on the edge. As a result of this exploration, we introduce PCBDet, a highly efficient, performant self-attention deep neural network architecture design. This architecture notably makes use of the recently introduced AttendNeXt backbone, which has been shown to achieve state-of-the-art performance for TinyML, and is integrated here into RetinaNetÂ <cite class="ltx_cite ltx_citemacro_citet">Wong etÂ al. (<a href="#bib.bib8" title="" class="ltx_ref">2022</a>); Lin etÂ al. (<a href="#bib.bib3" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The paper is organized as follows. In Section 2, details about the architecture of PCBDet, the training procedure, the evaluation procedure, and the data used for training and evaluation are described. In Section 3, experimental results in terms of component detection performance, model size, and inference speed on different computing hardware are presented. Finally, conclusions are drawn and future directions are discussed in Section 4.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methods</h2>

<figure id="S2.F1" class="ltx_figure">
<p id="S2.F1.1" class="ltx_p"><span id="S2.F1.1.1" class="ltx_text ltx_inline-block" style="width:433.6pt;"><img src="/html/2301.09268/assets/PCBDet.jpg" id="S2.F1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="529" height="268" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Overview of PCBDet architecture. PCBDet consists of A) a double-condensing attention condenser feature encoder feeding a FPN, and B) classification and box regression convolutional sub-nets for bounding box prediction. </figcaption>
</figure>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Dataset</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">To appropriately explore the quality and impact of our network design, a dataset that can facilitate the training and validation of robust models is essential. With this in mind, models were trained and tested using the DSLR images of the FICS-PCB dataset, a public, comprehensive, and diverse PCB component dataset that includes a number of challenging cases. The dataset itself consists of a total of 31 PCB boards containing over 77 thousand components to detect, with capacitors and resistors being the most widely represented piecesÂ <cite class="ltx_cite ltx_citemacro_citet">Lu etÂ al. (<a href="#bib.bib4" title="" class="ltx_ref">2020</a>)</cite>. Each board in the FICS-PCB dataset was further truncated into square patches, reducing train-time resource demand but maintaining component-wise resolution, with each patch being reshaped for additional size reduction further in the input pipeline. Figure 2 demonstrates several examples of such patches with annotated ground truth component labels.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Division of the dataset into distinct train, validation, and test splits is another crucial element in confirming the soundness of our experimentation. The preservation of exclusivity in the test set here is integral, since it allows for performance evaluation on a strict holdout set, and thus all of the image patches extracted from seven of the 31 PCBs were used as the test set. With the exception of one of the boards, which was excluded due to a lack of DSLR pictures, patches from the remaining boards (which total to 23) were used for the train/validation splits. From this set, 87.5% of the patches were taken for the train set, while the other 12.5% were used for the validation set (for post-epoch performance validation).</p>
</div>
<figure id="S2.F2" class="ltx_figure">
<p id="S2.F2.1" class="ltx_p"><span id="S2.F2.1.1" class="ltx_text ltx_inline-block" style="width:433.6pt;"><img src="/html/2301.09268/assets/6_pcbs.jpg" id="S2.F2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="488" height="324" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Examples of PCB patches annotated with ground truth bounding boxes.</figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Architecture</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">As seen in Figure 1, the proposed PCBDet possesses an efficient self-attention architecture design inspired by two different architecture design paradigms: 1) RetinaNet bounding box prediction structureÂ <cite class="ltx_cite ltx_citemacro_citet">Lin etÂ al. (<a href="#bib.bib3" title="" class="ltx_ref">2017</a>)</cite>, and 2) double-condensing attention condenser architectureÂ <cite class="ltx_cite ltx_citemacro_citet">Wong etÂ al. (<a href="#bib.bib8" title="" class="ltx_ref">2022</a>)</cite>. As a single-stage detector architecture design, the RetinaNet structure encompasses a more efficient object detection process when compared to state-of-the-art two-stage object detectors like R-CNNÂ <cite class="ltx_cite ltx_citemacro_citet">Lin etÂ al. (<a href="#bib.bib3" title="" class="ltx_ref">2017</a>)</cite>. RetinaNet has also seen increased performance when compared to one-stage detectors such as SSD or YOLOÂ <cite class="ltx_cite ltx_citemacro_citet">Lin etÂ al. (<a href="#bib.bib3" title="" class="ltx_ref">2017</a>)</cite>. As such, the proposed PCBDet architectural design takes inspiration from the RetinaNet structure, looking to adopt an efficient single-stage approach without seeing substantial tradeoffs in performance.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Without an efficient backbone, however, our network cannot maximize on the possible efficiency-based benefits of the RetinaNet framework. More specifically, the backbone architecture within a RetinaNet structure is the feature encoder that feeds into the convolutional sub-nets, and while a larger, complex backbone may enable increased performance gains, this can lead to substantial losses in efficiency. The design of a small, efficient backbone architecture is thus crucial in creating an effective and efficient object detection network.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">PCBDetâ€™s backbone architecture takes inspiration from the AttendNeXt double-condensing attention condenser architecture design, which has shown top of the line performance among other state-of-the-art efficient architectures for ImageNet classificationÂ <cite class="ltx_cite ltx_citemacro_citet">Wong etÂ al. (<a href="#bib.bib8" title="" class="ltx_ref">2022</a>)</cite>. This self-attention architecture design features double-condensing attention condenser modules, applied within a convolutional network structure, to increase the speed and efficiency of standard convolutional architectures for feature extraction, thus serving as the basis for an efficient backbone for RetinaNetÂ <cite class="ltx_cite ltx_citemacro_citet">Wong etÂ al. (<a href="#bib.bib8" title="" class="ltx_ref">2022</a>)</cite>. The AttendNeXt feature encoder used in our study was first pretrained on ImageNet, establishing a basis for the weights to be used in our object detection task. The classification head was then removed, and stage outputs were taken as inputs for a feature pyramid network (FPN), whose layers respectively feed into classification and regression subnets as per general RetinaNet structure. To further increase the efficiency of our designed network, the first of four stages of the feature encoder was omitted from the construction of the FPN, decreasing the amount of subnet operations performed per pass. The resultant network from this design process is dubbed PCBDet.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">We also tried integrating an EfficientNetB4-based backbone into RetinaNet and compared the performance of this architecture with PCBDet. EfficientNets are a family of convolutional models generally designed for, as the name implies, efficiency, with the model seeing upscaling as it progresses from B0 through B7Â <cite class="ltx_cite ltx_citemacro_citet">Tan and Le (<a href="#bib.bib5" title="" class="ltx_ref">2019</a>)</cite>. EfficientNet-B4 shows improved top-1 ImageNet performance when compared to other state-of-the-art convolutional classifiers while maintaining a lower number of parameters, and was thus chosen as an efficient but potent backbone to explore with RetinaNetÂ <cite class="ltx_cite ltx_citemacro_citet">Tan and Le (<a href="#bib.bib5" title="" class="ltx_ref">2019</a>)</cite>. As with PCBDetâ€™s AttendNeXt backbone, the EfficientNet feature encoder had its classification head removed and block outputs were used as inputs for an FPN. To provide a fair point of comparison for PCBDet, the integration of this feature encoder with the FPN once again seeks to achieve greater efficiency, with the first three of seven blocks of the EfficientNet-B4 feature encoder being excluded from FPN construction. The network designed here is referred to as EfficientNet-Det.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Training</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Proper exploration of our architectures requires thorough training, and for compact networks such as PCBDet in particular, slower, gradual weight learning is crucial to appropriately search for effective weights in the training process.
As such, PCBDet and EfficientNet-Det were each trained for 300 epochs, with a base learning rate of 2e-4 and a proprietary learning rate scheduler, along with Adam optimization. While potential overfitting could arise from slow, gradual learning, this issue was combated with the use of image augmentation, including vertical and horizontal flipping and translation, colour degeneration, and random cutouts (patch removal), as well as the monitoring of network performance on the validation set. It is also essential to address the disproportionate representation of components in the FICS-PCB dataset. To do so, network training uses the focal loss metric, which accounts for class imbalances by adding a focusing parameter to the standard cross-entropy loss, resulting in greatly decreased weighting for easy, well-classified data pointsÂ <cite class="ltx_cite ltx_citemacro_citet">Lin etÂ al. (<a href="#bib.bib3" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">During training, the first and second blocks of the AttendNeXt feature encoder in PCBDet were frozen, allowing for the encoder to retain its memories of low-level features from ImageNet pretraining while also tuning higher-level feature blocks to better recognize the shapes and objects associated with PCB components. Similarly, the first four of seven blocks were frozen for the EfficientNet-B4 encoder in EfficientNet-Det.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Evaluation</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">Given the goal of efficient model design, we need a method that can effectively measure the complexity and compactness of a model. Inference time, or the time taken per forward pass, is a method that can reveal how quickly a network can perform as a predictor; in our work, inference time was taken for both PCBDet and EfficientNet-Det using an Intel Core i7-10750H processor and a NVIDIA GeForce RTX 1650 Ti, both within a Dell XPS 15 9500 laptop, as well as a Jetson Nano and a 64-bit ARM Cortex A72 processor, altogether providing an image of the on-the-edge inference speeds of the two networks. The number of parameters was also taken for each of the two networks, providing an additional measure for model compactness.</p>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<p id="S2.SS4.p2.1" class="ltx_p">The predictive power for bounding boxes of our networks is another necessary measure to analyze, in order to compare the model performance of the PCBDet and EfficientNet-Det architectures. This performance assessment was realized using the mean average precision (mAP) for bounding box predictions, over IOU thresholds from 0.5 to 0.95 with a 0.05 step size, commonly known as mAP@[0.5:0.95]; this mAP metric is also known as COCO mAP, the standard performance metric for COCO challengesÂ <cite class="ltx_cite ltx_citemacro_citet">Tong etÂ al. (<a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite>.
Averaging performance over a range of IOU thresholds provides a more generalized sense of object detection performance across resolutions, as lower IOU thresholds test for more roughly correct box predictions while higher thresholds solely reward exact bounding box location. The validation and test performances of PCBDet and EfficientNet-Det were taken to be the mAP@[0.5:0.95] on the validation and holdout test sets respectively; this general AP metric helps to determine the predictive accuracy of our networks during and after training.</p>
</div>
<div id="S2.SS4.p3" class="ltx_para">
<p id="S2.SS4.p3.3" class="ltx_p">While individual differences in network performance and compactness can be seen through inference time and COCO mAP measures, a collective assessment can provide a better picture of the difference in the accuracy-complexity balance achieved in PCBDet and EfficientNet-Det. This unified analysis can be performed using the NetScore metric, which acts as a quantitative method of assessing this very balanceÂ <cite class="ltx_cite ltx_citemacro_citet">Wong (<a href="#bib.bib7" title="" class="ltx_ref">2018</a>)</cite>. In this calculation, the coefficient values used were <math id="S2.SS4.p3.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S2.SS4.p3.1.m1.1a"><mi id="S2.SS4.p3.1.m1.1.1" xref="S2.SS4.p3.1.m1.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.p3.1.m1.1b"><ci id="S2.SS4.p3.1.m1.1.1.cmml" xref="S2.SS4.p3.1.m1.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p3.1.m1.1c">\alpha</annotation></semantics></math> = 2, <math id="S2.SS4.p3.2.m2.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S2.SS4.p3.2.m2.1a"><mi id="S2.SS4.p3.2.m2.1.1" xref="S2.SS4.p3.2.m2.1.1.cmml">Î²</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.p3.2.m2.1b"><ci id="S2.SS4.p3.2.m2.1.1.cmml" xref="S2.SS4.p3.2.m2.1.1">ğ›½</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p3.2.m2.1c">\beta</annotation></semantics></math> = 1, and <math id="S2.SS4.p3.3.m3.1" class="ltx_Math" alttext="\gamma" display="inline"><semantics id="S2.SS4.p3.3.m3.1a"><mi id="S2.SS4.p3.3.m3.1.1" xref="S2.SS4.p3.3.m3.1.1.cmml">Î³</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.p3.3.m3.1b"><ci id="S2.SS4.p3.3.m3.1.1.cmml" xref="S2.SS4.p3.3.m3.1.1">ğ›¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p3.3.m3.1c">\gamma</annotation></semantics></math> = 1, in accordance with the original NetScore studyÂ <cite class="ltx_cite ltx_citemacro_citet">Wong (<a href="#bib.bib7" title="" class="ltx_ref">2018</a>)</cite>. The inference-time multiply-accumulate (MAC) operations measure was also replaced with the experimental inference time (in seconds) using the ARM Cortex A72 in the calculation, as this experimental metric of complexity provides a more practical measure for edge performance than MAC operations, while the COCO mAP was used as the accuracy metric for the calculation. The calculation used for NetScore was</p>
</div>
<div id="S2.SS4.p4" class="ltx_para">
<table id="S2.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.Ex1.m1.4" class="ltx_Math" alttext="NetScore=\frac{(mAP*100)^{2}}{(MParams)(Inference\;\,time\;(s))}" display="block"><semantics id="S2.Ex1.m1.4a"><mrow id="S2.Ex1.m1.4.5" xref="S2.Ex1.m1.4.5.cmml"><mrow id="S2.Ex1.m1.4.5.2" xref="S2.Ex1.m1.4.5.2.cmml"><mi id="S2.Ex1.m1.4.5.2.2" xref="S2.Ex1.m1.4.5.2.2.cmml">N</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.4.5.2.1" xref="S2.Ex1.m1.4.5.2.1.cmml">â€‹</mo><mi id="S2.Ex1.m1.4.5.2.3" xref="S2.Ex1.m1.4.5.2.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.4.5.2.1a" xref="S2.Ex1.m1.4.5.2.1.cmml">â€‹</mo><mi id="S2.Ex1.m1.4.5.2.4" xref="S2.Ex1.m1.4.5.2.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.4.5.2.1b" xref="S2.Ex1.m1.4.5.2.1.cmml">â€‹</mo><mi id="S2.Ex1.m1.4.5.2.5" xref="S2.Ex1.m1.4.5.2.5.cmml">S</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.4.5.2.1c" xref="S2.Ex1.m1.4.5.2.1.cmml">â€‹</mo><mi id="S2.Ex1.m1.4.5.2.6" xref="S2.Ex1.m1.4.5.2.6.cmml">c</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.4.5.2.1d" xref="S2.Ex1.m1.4.5.2.1.cmml">â€‹</mo><mi id="S2.Ex1.m1.4.5.2.7" xref="S2.Ex1.m1.4.5.2.7.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.4.5.2.1e" xref="S2.Ex1.m1.4.5.2.1.cmml">â€‹</mo><mi id="S2.Ex1.m1.4.5.2.8" xref="S2.Ex1.m1.4.5.2.8.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.4.5.2.1f" xref="S2.Ex1.m1.4.5.2.1.cmml">â€‹</mo><mi id="S2.Ex1.m1.4.5.2.9" xref="S2.Ex1.m1.4.5.2.9.cmml">e</mi></mrow><mo id="S2.Ex1.m1.4.5.1" xref="S2.Ex1.m1.4.5.1.cmml">=</mo><mfrac id="S2.Ex1.m1.4.4" xref="S2.Ex1.m1.4.4.cmml"><msup id="S2.Ex1.m1.1.1.1" xref="S2.Ex1.m1.1.1.1.cmml"><mrow id="S2.Ex1.m1.1.1.1.1.1" xref="S2.Ex1.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.Ex1.m1.1.1.1.1.1.2" xref="S2.Ex1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.Ex1.m1.1.1.1.1.1.1" xref="S2.Ex1.m1.1.1.1.1.1.1.cmml"><mrow id="S2.Ex1.m1.1.1.1.1.1.1.2" xref="S2.Ex1.m1.1.1.1.1.1.1.2.cmml"><mi id="S2.Ex1.m1.1.1.1.1.1.1.2.2" xref="S2.Ex1.m1.1.1.1.1.1.1.2.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.1.1.1.1.1.1.2.1" xref="S2.Ex1.m1.1.1.1.1.1.1.2.1.cmml">â€‹</mo><mi id="S2.Ex1.m1.1.1.1.1.1.1.2.3" xref="S2.Ex1.m1.1.1.1.1.1.1.2.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.1.1.1.1.1.1.2.1a" xref="S2.Ex1.m1.1.1.1.1.1.1.2.1.cmml">â€‹</mo><mi id="S2.Ex1.m1.1.1.1.1.1.1.2.4" xref="S2.Ex1.m1.1.1.1.1.1.1.2.4.cmml">P</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S2.Ex1.m1.1.1.1.1.1.1.1" xref="S2.Ex1.m1.1.1.1.1.1.1.1.cmml">âˆ—</mo><mn id="S2.Ex1.m1.1.1.1.1.1.1.3" xref="S2.Ex1.m1.1.1.1.1.1.1.3.cmml">100</mn></mrow><mo stretchy="false" id="S2.Ex1.m1.1.1.1.1.1.3" xref="S2.Ex1.m1.1.1.1.1.1.1.cmml">)</mo></mrow><mn id="S2.Ex1.m1.1.1.1.3" xref="S2.Ex1.m1.1.1.1.3.cmml">2</mn></msup><mrow id="S2.Ex1.m1.4.4.4" xref="S2.Ex1.m1.4.4.4.cmml"><mrow id="S2.Ex1.m1.3.3.3.2.1" xref="S2.Ex1.m1.3.3.3.2.1.1.cmml"><mo stretchy="false" id="S2.Ex1.m1.3.3.3.2.1.2" xref="S2.Ex1.m1.3.3.3.2.1.1.cmml">(</mo><mrow id="S2.Ex1.m1.3.3.3.2.1.1" xref="S2.Ex1.m1.3.3.3.2.1.1.cmml"><mi id="S2.Ex1.m1.3.3.3.2.1.1.2" xref="S2.Ex1.m1.3.3.3.2.1.1.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.3.3.3.2.1.1.1" xref="S2.Ex1.m1.3.3.3.2.1.1.1.cmml">â€‹</mo><mi id="S2.Ex1.m1.3.3.3.2.1.1.3" xref="S2.Ex1.m1.3.3.3.2.1.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.3.3.3.2.1.1.1a" xref="S2.Ex1.m1.3.3.3.2.1.1.1.cmml">â€‹</mo><mi id="S2.Ex1.m1.3.3.3.2.1.1.4" xref="S2.Ex1.m1.3.3.3.2.1.1.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.3.3.3.2.1.1.1b" xref="S2.Ex1.m1.3.3.3.2.1.1.1.cmml">â€‹</mo><mi id="S2.Ex1.m1.3.3.3.2.1.1.5" xref="S2.Ex1.m1.3.3.3.2.1.1.5.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.3.3.3.2.1.1.1c" xref="S2.Ex1.m1.3.3.3.2.1.1.1.cmml">â€‹</mo><mi id="S2.Ex1.m1.3.3.3.2.1.1.6" xref="S2.Ex1.m1.3.3.3.2.1.1.6.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.3.3.3.2.1.1.1d" xref="S2.Ex1.m1.3.3.3.2.1.1.1.cmml">â€‹</mo><mi id="S2.Ex1.m1.3.3.3.2.1.1.7" xref="S2.Ex1.m1.3.3.3.2.1.1.7.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.3.3.3.2.1.1.1e" xref="S2.Ex1.m1.3.3.3.2.1.1.1.cmml">â€‹</mo><mi id="S2.Ex1.m1.3.3.3.2.1.1.8" xref="S2.Ex1.m1.3.3.3.2.1.1.8.cmml">s</mi></mrow><mo stretchy="false" id="S2.Ex1.m1.3.3.3.2.1.3" xref="S2.Ex1.m1.3.3.3.2.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.4.4.4.4" xref="S2.Ex1.m1.4.4.4.4.cmml">â€‹</mo><mrow id="S2.Ex1.m1.4.4.4.3.1" xref="S2.Ex1.m1.4.4.4.3.1.1.cmml"><mo stretchy="false" id="S2.Ex1.m1.4.4.4.3.1.2" xref="S2.Ex1.m1.4.4.4.3.1.1.cmml">(</mo><mrow id="S2.Ex1.m1.4.4.4.3.1.1" xref="S2.Ex1.m1.4.4.4.3.1.1.cmml"><mi id="S2.Ex1.m1.4.4.4.3.1.1.2" xref="S2.Ex1.m1.4.4.4.3.1.1.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.4.4.4.3.1.1.1" xref="S2.Ex1.m1.4.4.4.3.1.1.1.cmml">â€‹</mo><mi id="S2.Ex1.m1.4.4.4.3.1.1.3" xref="S2.Ex1.m1.4.4.4.3.1.1.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.4.4.4.3.1.1.1a" xref="S2.Ex1.m1.4.4.4.3.1.1.1.cmml">â€‹</mo><mi id="S2.Ex1.m1.4.4.4.3.1.1.4" xref="S2.Ex1.m1.4.4.4.3.1.1.4.cmml">f</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.4.4.4.3.1.1.1b" xref="S2.Ex1.m1.4.4.4.3.1.1.1.cmml">â€‹</mo><mi id="S2.Ex1.m1.4.4.4.3.1.1.5" xref="S2.Ex1.m1.4.4.4.3.1.1.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.4.4.4.3.1.1.1c" xref="S2.Ex1.m1.4.4.4.3.1.1.1.cmml">â€‹</mo><mi id="S2.Ex1.m1.4.4.4.3.1.1.6" xref="S2.Ex1.m1.4.4.4.3.1.1.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.4.4.4.3.1.1.1d" xref="S2.Ex1.m1.4.4.4.3.1.1.1.cmml">â€‹</mo><mi id="S2.Ex1.m1.4.4.4.3.1.1.7" xref="S2.Ex1.m1.4.4.4.3.1.1.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.4.4.4.3.1.1.1e" xref="S2.Ex1.m1.4.4.4.3.1.1.1.cmml">â€‹</mo><mi id="S2.Ex1.m1.4.4.4.3.1.1.8" xref="S2.Ex1.m1.4.4.4.3.1.1.8.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.4.4.4.3.1.1.1f" xref="S2.Ex1.m1.4.4.4.3.1.1.1.cmml">â€‹</mo><mi id="S2.Ex1.m1.4.4.4.3.1.1.9" xref="S2.Ex1.m1.4.4.4.3.1.1.9.cmml">c</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.4.4.4.3.1.1.1g" xref="S2.Ex1.m1.4.4.4.3.1.1.1.cmml">â€‹</mo><mi id="S2.Ex1.m1.4.4.4.3.1.1.10" xref="S2.Ex1.m1.4.4.4.3.1.1.10.cmml">e</mi><mo lspace="0.440em" rspace="0em" id="S2.Ex1.m1.4.4.4.3.1.1.1h" xref="S2.Ex1.m1.4.4.4.3.1.1.1.cmml">â€‹</mo><mi id="S2.Ex1.m1.4.4.4.3.1.1.11" xref="S2.Ex1.m1.4.4.4.3.1.1.11.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.4.4.4.3.1.1.1i" xref="S2.Ex1.m1.4.4.4.3.1.1.1.cmml">â€‹</mo><mi id="S2.Ex1.m1.4.4.4.3.1.1.12" xref="S2.Ex1.m1.4.4.4.3.1.1.12.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.4.4.4.3.1.1.1j" xref="S2.Ex1.m1.4.4.4.3.1.1.1.cmml">â€‹</mo><mi id="S2.Ex1.m1.4.4.4.3.1.1.13" xref="S2.Ex1.m1.4.4.4.3.1.1.13.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.4.4.4.3.1.1.1k" xref="S2.Ex1.m1.4.4.4.3.1.1.1.cmml">â€‹</mo><mi id="S2.Ex1.m1.4.4.4.3.1.1.14" xref="S2.Ex1.m1.4.4.4.3.1.1.14.cmml">e</mi><mo lspace="0.280em" rspace="0em" id="S2.Ex1.m1.4.4.4.3.1.1.1l" xref="S2.Ex1.m1.4.4.4.3.1.1.1.cmml">â€‹</mo><mrow id="S2.Ex1.m1.4.4.4.3.1.1.15.2" xref="S2.Ex1.m1.4.4.4.3.1.1.cmml"><mo stretchy="false" id="S2.Ex1.m1.4.4.4.3.1.1.15.2.1" xref="S2.Ex1.m1.4.4.4.3.1.1.cmml">(</mo><mi id="S2.Ex1.m1.2.2.2.1" xref="S2.Ex1.m1.2.2.2.1.cmml">s</mi><mo stretchy="false" id="S2.Ex1.m1.4.4.4.3.1.1.15.2.2" xref="S2.Ex1.m1.4.4.4.3.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S2.Ex1.m1.4.4.4.3.1.3" xref="S2.Ex1.m1.4.4.4.3.1.1.cmml">)</mo></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex1.m1.4b"><apply id="S2.Ex1.m1.4.5.cmml" xref="S2.Ex1.m1.4.5"><eq id="S2.Ex1.m1.4.5.1.cmml" xref="S2.Ex1.m1.4.5.1"></eq><apply id="S2.Ex1.m1.4.5.2.cmml" xref="S2.Ex1.m1.4.5.2"><times id="S2.Ex1.m1.4.5.2.1.cmml" xref="S2.Ex1.m1.4.5.2.1"></times><ci id="S2.Ex1.m1.4.5.2.2.cmml" xref="S2.Ex1.m1.4.5.2.2">ğ‘</ci><ci id="S2.Ex1.m1.4.5.2.3.cmml" xref="S2.Ex1.m1.4.5.2.3">ğ‘’</ci><ci id="S2.Ex1.m1.4.5.2.4.cmml" xref="S2.Ex1.m1.4.5.2.4">ğ‘¡</ci><ci id="S2.Ex1.m1.4.5.2.5.cmml" xref="S2.Ex1.m1.4.5.2.5">ğ‘†</ci><ci id="S2.Ex1.m1.4.5.2.6.cmml" xref="S2.Ex1.m1.4.5.2.6">ğ‘</ci><ci id="S2.Ex1.m1.4.5.2.7.cmml" xref="S2.Ex1.m1.4.5.2.7">ğ‘œ</ci><ci id="S2.Ex1.m1.4.5.2.8.cmml" xref="S2.Ex1.m1.4.5.2.8">ğ‘Ÿ</ci><ci id="S2.Ex1.m1.4.5.2.9.cmml" xref="S2.Ex1.m1.4.5.2.9">ğ‘’</ci></apply><apply id="S2.Ex1.m1.4.4.cmml" xref="S2.Ex1.m1.4.4"><divide id="S2.Ex1.m1.4.4.5.cmml" xref="S2.Ex1.m1.4.4"></divide><apply id="S2.Ex1.m1.1.1.1.cmml" xref="S2.Ex1.m1.1.1.1"><csymbol cd="ambiguous" id="S2.Ex1.m1.1.1.1.2.cmml" xref="S2.Ex1.m1.1.1.1">superscript</csymbol><apply id="S2.Ex1.m1.1.1.1.1.1.1.cmml" xref="S2.Ex1.m1.1.1.1.1.1"><times id="S2.Ex1.m1.1.1.1.1.1.1.1.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1.1"></times><apply id="S2.Ex1.m1.1.1.1.1.1.1.2.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1.2"><times id="S2.Ex1.m1.1.1.1.1.1.1.2.1.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1.2.1"></times><ci id="S2.Ex1.m1.1.1.1.1.1.1.2.2.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1.2.2">ğ‘š</ci><ci id="S2.Ex1.m1.1.1.1.1.1.1.2.3.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1.2.3">ğ´</ci><ci id="S2.Ex1.m1.1.1.1.1.1.1.2.4.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1.2.4">ğ‘ƒ</ci></apply><cn type="integer" id="S2.Ex1.m1.1.1.1.1.1.1.3.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1.3">100</cn></apply><cn type="integer" id="S2.Ex1.m1.1.1.1.3.cmml" xref="S2.Ex1.m1.1.1.1.3">2</cn></apply><apply id="S2.Ex1.m1.4.4.4.cmml" xref="S2.Ex1.m1.4.4.4"><times id="S2.Ex1.m1.4.4.4.4.cmml" xref="S2.Ex1.m1.4.4.4.4"></times><apply id="S2.Ex1.m1.3.3.3.2.1.1.cmml" xref="S2.Ex1.m1.3.3.3.2.1"><times id="S2.Ex1.m1.3.3.3.2.1.1.1.cmml" xref="S2.Ex1.m1.3.3.3.2.1.1.1"></times><ci id="S2.Ex1.m1.3.3.3.2.1.1.2.cmml" xref="S2.Ex1.m1.3.3.3.2.1.1.2">ğ‘€</ci><ci id="S2.Ex1.m1.3.3.3.2.1.1.3.cmml" xref="S2.Ex1.m1.3.3.3.2.1.1.3">ğ‘ƒ</ci><ci id="S2.Ex1.m1.3.3.3.2.1.1.4.cmml" xref="S2.Ex1.m1.3.3.3.2.1.1.4">ğ‘</ci><ci id="S2.Ex1.m1.3.3.3.2.1.1.5.cmml" xref="S2.Ex1.m1.3.3.3.2.1.1.5">ğ‘Ÿ</ci><ci id="S2.Ex1.m1.3.3.3.2.1.1.6.cmml" xref="S2.Ex1.m1.3.3.3.2.1.1.6">ğ‘</ci><ci id="S2.Ex1.m1.3.3.3.2.1.1.7.cmml" xref="S2.Ex1.m1.3.3.3.2.1.1.7">ğ‘š</ci><ci id="S2.Ex1.m1.3.3.3.2.1.1.8.cmml" xref="S2.Ex1.m1.3.3.3.2.1.1.8">ğ‘ </ci></apply><apply id="S2.Ex1.m1.4.4.4.3.1.1.cmml" xref="S2.Ex1.m1.4.4.4.3.1"><times id="S2.Ex1.m1.4.4.4.3.1.1.1.cmml" xref="S2.Ex1.m1.4.4.4.3.1.1.1"></times><ci id="S2.Ex1.m1.4.4.4.3.1.1.2.cmml" xref="S2.Ex1.m1.4.4.4.3.1.1.2">ğ¼</ci><ci id="S2.Ex1.m1.4.4.4.3.1.1.3.cmml" xref="S2.Ex1.m1.4.4.4.3.1.1.3">ğ‘›</ci><ci id="S2.Ex1.m1.4.4.4.3.1.1.4.cmml" xref="S2.Ex1.m1.4.4.4.3.1.1.4">ğ‘“</ci><ci id="S2.Ex1.m1.4.4.4.3.1.1.5.cmml" xref="S2.Ex1.m1.4.4.4.3.1.1.5">ğ‘’</ci><ci id="S2.Ex1.m1.4.4.4.3.1.1.6.cmml" xref="S2.Ex1.m1.4.4.4.3.1.1.6">ğ‘Ÿ</ci><ci id="S2.Ex1.m1.4.4.4.3.1.1.7.cmml" xref="S2.Ex1.m1.4.4.4.3.1.1.7">ğ‘’</ci><ci id="S2.Ex1.m1.4.4.4.3.1.1.8.cmml" xref="S2.Ex1.m1.4.4.4.3.1.1.8">ğ‘›</ci><ci id="S2.Ex1.m1.4.4.4.3.1.1.9.cmml" xref="S2.Ex1.m1.4.4.4.3.1.1.9">ğ‘</ci><ci id="S2.Ex1.m1.4.4.4.3.1.1.10.cmml" xref="S2.Ex1.m1.4.4.4.3.1.1.10">ğ‘’</ci><ci id="S2.Ex1.m1.4.4.4.3.1.1.11.cmml" xref="S2.Ex1.m1.4.4.4.3.1.1.11">ğ‘¡</ci><ci id="S2.Ex1.m1.4.4.4.3.1.1.12.cmml" xref="S2.Ex1.m1.4.4.4.3.1.1.12">ğ‘–</ci><ci id="S2.Ex1.m1.4.4.4.3.1.1.13.cmml" xref="S2.Ex1.m1.4.4.4.3.1.1.13">ğ‘š</ci><ci id="S2.Ex1.m1.4.4.4.3.1.1.14.cmml" xref="S2.Ex1.m1.4.4.4.3.1.1.14">ğ‘’</ci><ci id="S2.Ex1.m1.2.2.2.1.cmml" xref="S2.Ex1.m1.2.2.2.1">ğ‘ </ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex1.m1.4c">NetScore=\frac{(mAP*100)^{2}}{(MParams)(Inference\;\,time\;(s))}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Results</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The efficacy of the proposed PCBDet for PCB component detection is compared here with EfficientNet-Det across the following metrics: 1) COCO mAP, 2) model size, and 3) inference speed on various low-power computing hardware.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2301.09268/assets/pcbdet_map.jpg" id="S3.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="1332" height="800" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>COCO mAP on validation and test data for PCBDet and EfficientNet-Det.</figcaption>
</figure>
<div id="S3.p2" class="ltx_para ltx_noindent">
<p id="S3.p2.1" class="ltx_p"><span id="S3.p2.1.1" class="ltx_text ltx_font_bold">COCO mAP</span>. It can be observed in Figure 3 that the proposed PCBDet achieves noticeable gains in terms of test and validation COCO mAP by approximately 4 % and 2 %, respectively, when compared to EfficientNet-Det. This gain in mAP is particularly interesting especially given the fact that PCBDet is significantly smaller and faster than EfficientNet-Det, which we will discuss in greater detail. As such, these results illustrate that a high level of performance can be achieved with the proposed PCBDet for the purpose of automatic PCB component detection on the edge.</p>
</div>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p id="S3.p3.1" class="ltx_p"><span id="S3.p3.1.1" class="ltx_text ltx_font_bold">Model Size</span>. As shown in Figure 4, it can be observed that the proposed PCBDet possesses less than half the number of total parameters, as well as trainable parameters, when compared to EfficientNet-Det. This is particularly important for edge based applications such as automatic on-the-edge PCB component detection where memory resources are limited.</p>
</div>
<div id="S3.p4" class="ltx_para ltx_noindent">
<p id="S3.p4.1" class="ltx_p"><span id="S3.p4.1.1" class="ltx_text ltx_font_bold">Inference Speed</span>. As shown in Figure 5, it can be observed that the proposed PCBDet is more than 30% faster than EfficientNet-Det on the NVIDIA Geforce RTX 1650 Ti, with an even greater speed gain on slower hardware such as the Jetson Nano (almost 65% faster) and Intel Core i7-10750H (over 45% faster). As seen in Figure 6, the performance gains of the proposed PCBDet on lower-power hardware were especially apparent when evaluated on an ARM Cortex A72, where PCBDet was more than 2<math id="S3.p4.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.p4.1.m1.1a"><mo id="S3.p4.1.m1.1.1" xref="S3.p4.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S3.p4.1.m1.1b"><times id="S3.p4.1.m1.1.1.cmml" xref="S3.p4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.1.m1.1c">\times</annotation></semantics></math> faster than EfficientNet-Det. These inference speed results demonstrate the efficacy of the proposed PCBDet for high-throughput PCB component detection on the edge.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">Finally, using the above results, PCBDet was found to achieve a NetScore of 28.2670 while EfficientNet-Det achieved a NetScore of 13.5749, supporting our findings that PCBDet achieves a superior accuracy-complexity balance when compared to EfficientNet-Det.</p>
</div>
<div id="S3.p6" class="ltx_para">
<p id="S3.p6.1" class="ltx_p">These results demonstrate overall that PCBDet shows significantly greater efficacy than RetinaNet with EfficientNet-B4, which is currently considered a state-of-the-art backbone for TinyML. Ultimately, we have developed a model for PCB object detection that shows very strong performance despite its small size and high inference throughput.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2301.09268/assets/x1.png" id="S3.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="290" height="172" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Number of total and trainable parameters for PCBDet and EfficientNet-Det.</figcaption>
</figure>
<div id="S3.p7" class="ltx_para">
<br class="ltx_break">
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2301.09268/assets/x2.png" id="S3.F5.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="315" height="190" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Inference time for PCBDet and EfficientNet-Det across different hardware.</figcaption>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Here, we conducted an exploration of efficient deep neural network object detection architectures for the purpose of automatic PCB component detection on the edge. The resulting network architecture, which we coin PCBDet, methodically integrates the recently introduced AttendNeXt backbone into RetinaNet. This results in an architecture which can achieve up to a 2x inference speed-up on low power hardware compared to other state-of-the-art efficient architectures, while still achieving a higher mAP on the FICS-PCB benchmark dataset. This makes PCBDet very well-suited for component detection in high-throughput manufacturing scenarios with limited computational resources. Future work may include seeing if a similar strategy involving the methodical use of the AttendNeXt backbone could be employed to develop high performance, efficient deep neural network object detection architectures for other applications.</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2301.09268/assets/x3.png" id="S4.F6.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="246" height="144" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Inference time for PCBDet and EfficientNet-Det on ARM Cortex A72 processor.</figcaption>
</figure>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kuo etÂ al. [2019]</span>
<span class="ltx_bibblock">
Chia-Wen Kuo, Jacob Ashmore, David Huggins, and Zsolt Kira.

</span>
<span class="ltx_bibblock">Data-efficient graph embedding learning for pcb component detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">2019 IEEE Winter Conference on Applications of Computer
Vision (WACV)</em>, 2019.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. [2022]</span>
<span class="ltx_bibblock">
Jing Li, Yingqian Chen, Weiye Li, and Jinan Gu.

</span>
<span class="ltx_bibblock">Balanced-yolov3: Addressing the imbalance problem of object detection
in pcb assembly scene.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Electronics</em>, 2022.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin etÂ al. [2017]</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Priya Goyal, RossÂ B. Girshick, Kaiming He, and Piotr
DollÃ¡r.

</span>
<span class="ltx_bibblock">Focal loss for dense object detection.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, 2017.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu etÂ al. [2020]</span>
<span class="ltx_bibblock">
Hangwei Lu, Dhwani Mehta, Olivia Paradis, Navid Asadizanjani, Mark Tehranipoor,
and DamonÂ L. Woodard.

</span>
<span class="ltx_bibblock">Fics-pcb: A multi-modal image dataset for automated printed circuit
board visual inspection.

</span>
<span class="ltx_bibblock">2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan and Le [2019]</span>
<span class="ltx_bibblock">
Mingxing Tan and QuocÂ V. Le.

</span>
<span class="ltx_bibblock">Efficientnet: Rethinking model scaling for convolutional neural
networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, 2019.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tong etÂ al. [2020]</span>
<span class="ltx_bibblock">
Kang Tong, Yiquan Wu, and Fei Zhou.

</span>
<span class="ltx_bibblock">Recent advances in small object detection based on deep learning: A
review.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Image and Vision Computing</em>, 2020.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wong [2018]</span>
<span class="ltx_bibblock">
Alexander Wong.

</span>
<span class="ltx_bibblock">Netscore: Towards universal metrics for large-scale performance
analysis of deep neural networks for practical usage.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, 2018.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wong etÂ al. [2022]</span>
<span class="ltx_bibblock">
Alexander Wong, MohammadÂ Javad Shafiee, Saad Abbasi, Saeejith Nair, and Mahmoud
Famouri.

</span>
<span class="ltx_bibblock">Faster attention is what you need: A fast self-attention neural
network backbone architecture for the edge via double-condensing attention
condensers.

</span>
<span class="ltx_bibblock">2022.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2301.09267" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2301.09268" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2301.09268">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2301.09268" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2301.09269" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 05:58:23 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
