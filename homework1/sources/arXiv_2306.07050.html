<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2306.07050] Revisiting Token Pruning for Object Detection and Instance Segmentation</title><meta property="og:description" content="Vision Transformers (ViTs) have shown impressive performance in computer vision, but their high computational cost, quadratic in the number of tokens, limits their adoption in computation-constrained applications. Howe…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Revisiting Token Pruning for Object Detection and Instance Segmentation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Revisiting Token Pruning for Object Detection and Instance Segmentation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2306.07050">

<!--Generated on Thu Feb 29 00:56:06 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Revisiting Token Pruning for Object Detection and Instance Segmentation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yifei Liu
 Mathias Gehrig
 Nico Messikommer
 Marco Cannici
 Davide Scaramuzza
<br class="ltx_break">Robotics and Perception Group, University of Zurich, Switzerland
<br class="ltx_break"><span id="id3.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{yifei.liu@, mgehrig@ifi., nmessi@ifi, cannici@ifi., sdavide@ifi.}uzh.ch</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.2" class="ltx_p">Vision Transformers (ViTs) have shown impressive performance in computer vision, but their high computational cost, quadratic in the number of tokens, limits their adoption in computation-constrained applications. However, this large number of tokens may not be necessary, as not all tokens are equally important. In this paper, we investigate token pruning to accelerate inference for object detection and instance segmentation, extending prior works from image classification. Through extensive experiments, we offer four insights for dense tasks: (i) tokens should not be completely pruned and discarded, but rather preserved in the feature maps for later use. (ii) reactivating previously pruned tokens can further enhance model performance. (iii) a dynamic pruning rate based on images is better than a fixed pruning rate. (iv) a lightweight, 2-layer MLP can effectively prune tokens, achieving accuracy comparable with complex gating networks with a simpler design.
We assess the effects of these design decisions on the COCO dataset and introduce an approach that incorporates these findings, showing a reduction in performance decline from <math id="id1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="id1.1.m1.1a"><mo id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><csymbol cd="latexml" id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">\sim</annotation></semantics></math>1.5 mAP to <math id="id2.2.m2.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="id2.2.m2.1a"><mo id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><csymbol cd="latexml" id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">\sim</annotation></semantics></math>0.3 mAP in both boxes and masks, compared to existing token pruning methods.
In relation to the dense counterpart that utilizes all tokens, our method realizes an increase in inference speed, achieving up to 34% faster performance for the entire network and 46% for the backbone.</p>
</div>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text ltx_font_bold">Code: <a target="_blank" href="https://github.com/uzh-rpg/svit/" title="" class="ltx_ref ltx_url ltx_font_typewriter ltx_font_medium">https://github.com/uzh-rpg/svit/</a></span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure"><img src="/html/2306.07050/assets/imgs/eye_catching.png" id="S1.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="341" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.3.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.4.2" class="ltx_text" style="font-size:90%;">Top: high-level workflow of SViT, the MLP selectively chooses tokens to be processed in the transformer block, and the pruned tokens are preserved in feature maps and can be reactivated in later layers. Bottom: the token useage heatmap represents the number of layers using the tokens, and shows that the computational distribution highly aligns with interested objects.</span></figcaption>
</figure>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2306.07050/assets/imgs/general_workflow.png" id="S1.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="220" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.3.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S1.F2.4.2" class="ltx_text" style="font-size:90%;">A high-level comparison of the overall workflows for various token pruning methods. ① Selection Module: may utilize a gating module (before self-attention) or be attention-based (after self-attention). ② Number of pruned tokens: can be either dynamic or fixed. ③ Treatment of pruned tokens: either removing or preserving them. If they are preserved within feature maps, there is an additional option to reactivate them.</span></figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Transformers and multi-head self-attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> have revolutionized the field of computer vision.
Since their first introduction, Vision Transformers (ViTs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> have quickly become the leading model architecture for a number of vision tasks, including image classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite>, semantic segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib74" title="" class="ltx_ref">74</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>, and others <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>.
Their unique ability to perform global reasoning through pair-wise token attention is, however, both a strength and a weakness.
Although it enhances the representational power of these architectures, it also leads to a significant increase in computational footprint, limiting the adoption of ViTs in resource-constrained settings.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">A viable strategy for mitigating the substantial computational demands involves leveraging input-aware inference to prune less critical features within the input space.
While this strategy has previously been applied to CNNs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, resulting in improved FLOP measurements, the intrinsic regularity of convolution operations makes it difficult to obtain noticeable speedup on hardware.
However, the advent of ViTs paves the way for input-space pruning,
as the MLPs in ViTs operate pointwise and self-attention inherently accommodates an arbitrary number of tokens.
Consequently, pruning tokens can readily attain remarkable speedup without necessitating any additional hardware adaptations.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Initial investigations in the domain of token pruning have encompassed the utilization of gating networks to identify less significant tokens <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> or eliminating tokens receiving minimal attention from the class token <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>.
These approaches, while having demonstrated their effectiveness,
were only applied to classification and
have yet to be applied to other tasks such as object detection and instance segmentation.
To the best of our knowledge, the exploration of token pruning in the context of dense tasks remains still notably scarce (Section <a href="#S2" title="2 Related Work ‣ Revisiting Token Pruning for Object Detection and Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>)<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The most related work on dense tasks is an extended version <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> of DynamicViT. However, it focuses on skipping MLPs in hierarchical models for dense tasks.</span></span></span>.
In this paper, we investigate token pruning for object detection and instance segmentation on isotropic vision transformers, with the aim of bridging the gap between classification and dense tasks.
During our preliminary experiments, we adapted prior methods to dense tasks and discovered they have apparent performance loss (Section <a href="#S4.SS1" title="4.1 Evaluation of the insights and observations ‣ 4 Experiments ‣ Revisiting Token Pruning for Object Detection and Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>).
With extensive experiments, we identified four key insights that are beneficial for improving model performance and simplifying model designs (Section <a href="#S3.SS2" title="3.2 Insights and Observations ‣ 3 Token Pruning on dense prediction tasks ‣ Revisiting Token Pruning for Object Detection and Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>),
leading to a method that outperforms previous state-of-the-art token pruning methods by a significant margin on object detection and instance segmentation (Section <a href="#S4.SS2" title="4.2 Comparison with state-of-the-art models ‣ 4 Experiments ‣ Revisiting Token Pruning for Object Detection and Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>). Our insights are as follows:</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p"><span id="S1.p4.1.1" class="ltx_text ltx_font_bold">Token preserving on dense tasks.</span>
Unlike classification, where pruned tokens can be removed permanently, dense prediction tasks benefit from preserving them in feature maps for subsequent utilization by the detection head.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p"><span id="S1.p5.1.1" class="ltx_text ltx_font_bold">Token reactivation as needed.</span>
In addition to preserving them, reactivating pruned tokens in the backbone on demand can improve model performance by adapting to layer-wise attention and recovering mis-pruned tokens for better robustness. A token once pruned has the flexibility to be reused at any subsequent layer, including the immediately succeeding one.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p"><span id="S1.p6.1.1" class="ltx_text ltx_font_bold">Pruning with a dynamic rate.</span>
The concept of a dynamic pruning rate, previously introduced for classification tasks in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, optimizes model performance within the same computation resource by allocating more tokens for complex images and fewer for simple images. It gains additional efficacy when integrated with token reactivation on dense prediciton tasks.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p"><span id="S1.p7.1.1" class="ltx_text ltx_font_bold">2-layer MLP is sufficient.</span>
A lightweight MLP is sufficient to select which tokens should be pruned, delivering almost the same accuracy as more complex gating networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> used for classification.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.2" class="ltx_p">We evaluate these design choices and build upon them to introduce a straightforward model to selectively prune tokens, which we refer to as SViT.
We demonstrate that this model surpasses previous state-of-the-art token pruning models by reducing loss in mAP from <math id="S1.p8.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S1.p8.1.m1.1a"><mo id="S1.p8.1.m1.1.1" xref="S1.p8.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S1.p8.1.m1.1b"><csymbol cd="latexml" id="S1.p8.1.m1.1.1.cmml" xref="S1.p8.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.p8.1.m1.1c">\sim</annotation></semantics></math>1.5 to <math id="S1.p8.2.m2.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S1.p8.2.m2.1a"><mo id="S1.p8.2.m2.1.1" xref="S1.p8.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S1.p8.2.m2.1b"><csymbol cd="latexml" id="S1.p8.2.m2.1.1.cmml" xref="S1.p8.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.p8.2.m2.1c">\sim</annotation></semantics></math>0.3 for both boxes and masks, and accelerates the inference speed of the dense counterpart by up to 34% for the whole network and 46% for the backbone.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Vision  Transformer</span>   
Originating in the NLP community <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, Transformers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> have lately acquired popularity also in the field of computer vision for their ability to capture long-range relations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.
The seminal work on Vision Transformers (ViTs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> demonstrated state-of-the-art classification performance, when pre-trained on large-scale datasets.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Since then, several improvements have been proposed to the ViT architecture, including improved tokens’ aggregation schemes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, multi-scale hierarchical designs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib58" title="" class="ltx_ref">58</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>, and hybrid architectures combining CNNs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>, <a href="#bib.bib70" title="" class="ltx_ref">70</a>, <a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>
.
Apart from design improvements, researchers have also investigated their use in more complex vision tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib73" title="" class="ltx_ref">73</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>, <a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite>.
This paper fits in between these two lines of research, as we not only focus on architecture design choices, but also extend their usage to dense prediction tasks such as object detection and instance segmentation.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Transformer  Acceleration</span>   
Various methods have been explored for optimizing Transformers’ high computational cost, including designing alternative lightweight attention formulations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>, removing unnecessary network modules <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> approximating attention multiplications with low-rank decompositions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, distilling knowledge into a more efficient student network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>, and extending network quantization techniques for Transformers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Furthermore, acceleration techniques specific to ViTs have been proposed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib61" title="" class="ltx_ref">61</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> by exploiting the redundancy in the input patches to early drop tokens for saving computation.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text ltx_font_bold">Input   Space   Pruning</span>   
As not all regions in the input image are equally important, pruning redundant areas can save computation without apparent accuracy loss. Spatially ACT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> prunes pixels for CNNs.
Numerous token pruning methods for ViTs have been developed on classification, including using gating networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>, attention scores <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib61" title="" class="ltx_ref">61</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, reinforcement learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> and others <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Among them, ToMe <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> proposes to merge tokens rather than remove them. A few works also consider dense tasks: SparseViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> prunes coarse windows for pyramid transformers, while we prune finer-grained tokens for isotropic transformers. SparseDETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> focuses on improving the efficiency of DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> architecture, while we focus on improving transformer-based backbones. STViT-R <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> sparsifies tokens by repeatedly clustering them into a few semantic tokens and restoring the spatial resolution, while we keep the spatial resolution with detailed position information.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Token Pruning on dense prediction tasks</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Revisit prior token pruning approaches</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We review the majority of token pruning techniques by illustrating the high-level distinctions in their workflows.
As shown in Table <a href="#S3.T1" title="Table 1 ‣ 3.1 Revisit prior token pruning approaches ‣ 3 Token Pruning on dense prediction tasks ‣ Revisiting Token Pruning for Object Detection and Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, these approaches can be classified along four dimensions: the selection module, use of dynamic pruning rate, preservation of pruned tokens, and reactivation of pruned tokens.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">The overall workflow of token pruning is depicted in Figure <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Revisiting Token Pruning for Object Detection and Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and can be summarized as follows: initially, the input image is partitioned into non-overlapping patches, which are linearly transformed into tokens and subsequently processed by the initial ViT blocks to obtain comprehensive enough feature representations. Then, token selection modules are introduced to identify tokens for pruning, consequently accelerating computations due to the reduced number of tokens. Note that, here, acceleration comes out-of-the-box as self-attention can adaptively process fewer number of tokens without any modification.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S3.T1.2" class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.2.1.1" class="ltx_tr">
<th id="S3.T1.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Selection Module</th>
<th id="S3.T1.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<table id="S3.T1.2.1.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.2.1.1.2.1.1" class="ltx_tr">
<td id="S3.T1.2.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Dynamic</td>
</tr>
<tr id="S3.T1.2.1.1.2.1.2" class="ltx_tr">
<td id="S3.T1.2.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Pruning</td>
</tr>
<tr id="S3.T1.2.1.1.2.1.3" class="ltx_tr">
<td id="S3.T1.2.1.1.2.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center">Rate</td>
</tr>
</table>
</th>
<th id="S3.T1.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<table id="S3.T1.2.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.2.1.1.3.1.1" class="ltx_tr">
<td id="S3.T1.2.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Preserve</td>
</tr>
<tr id="S3.T1.2.1.1.3.1.2" class="ltx_tr">
<td id="S3.T1.2.1.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Pruned</td>
</tr>
<tr id="S3.T1.2.1.1.3.1.3" class="ltx_tr">
<td id="S3.T1.2.1.1.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center">Tokens</td>
</tr>
</table>
</th>
<th id="S3.T1.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<table id="S3.T1.2.1.1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.2.1.1.4.1.1" class="ltx_tr">
<td id="S3.T1.2.1.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Reactivate</td>
</tr>
<tr id="S3.T1.2.1.1.4.1.2" class="ltx_tr">
<td id="S3.T1.2.1.1.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Pruned</td>
</tr>
<tr id="S3.T1.2.1.1.4.1.3" class="ltx_tr">
<td id="S3.T1.2.1.1.4.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center">Tokens</td>
</tr>
</table>
</th>
<th id="S3.T1.2.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Model</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.2.2.1" class="ltx_tr">
<td id="S3.T1.2.2.1.1" class="ltx_td ltx_align_center ltx_border_tt">gating module</td>
<td id="S3.T1.2.2.1.2" class="ltx_td ltx_align_center ltx_border_tt">✓</td>
<td id="S3.T1.2.2.1.3" class="ltx_td ltx_align_center ltx_border_tt">✓</td>
<td id="S3.T1.2.2.1.4" class="ltx_td ltx_align_center ltx_border_tt">✓</td>
<td id="S3.T1.2.2.1.5" class="ltx_td ltx_align_left ltx_border_tt">SViT (Ours)</td>
</tr>
<tr id="S3.T1.2.3.2" class="ltx_tr">
<td id="S3.T1.2.3.2.1" class="ltx_td ltx_align_center">gating module</td>
<td id="S3.T1.2.3.2.2" class="ltx_td ltx_align_center">✗</td>
<td id="S3.T1.2.3.2.3" class="ltx_td ltx_align_center">✗</td>
<td id="S3.T1.2.3.2.4" class="ltx_td ltx_align_center">✗</td>
<td id="S3.T1.2.3.2.5" class="ltx_td ltx_align_left">DynamicViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>
</td>
</tr>
<tr id="S3.T1.2.4.3" class="ltx_tr">
<td id="S3.T1.2.4.3.1" class="ltx_td ltx_align_center">attention-based</td>
<td id="S3.T1.2.4.3.2" class="ltx_td ltx_align_center">✓</td>
<td id="S3.T1.2.4.3.3" class="ltx_td ltx_align_center">✗</td>
<td id="S3.T1.2.4.3.4" class="ltx_td ltx_align_center">✗</td>
<td id="S3.T1.2.4.3.5" class="ltx_td ltx_align_left">ATS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
</td>
</tr>
<tr id="S3.T1.2.5.4" class="ltx_tr">
<td id="S3.T1.2.5.4.1" class="ltx_td ltx_align_center">attention-based</td>
<td id="S3.T1.2.5.4.2" class="ltx_td ltx_align_center">✗</td>
<td id="S3.T1.2.5.4.3" class="ltx_td ltx_align_center">✓</td>
<td id="S3.T1.2.5.4.4" class="ltx_td ltx_align_center">✗<sup id="S3.T1.2.5.4.4.1" class="ltx_sup">2</sup>
</td>
<td id="S3.T1.2.5.4.5" class="ltx_td ltx_align_left">Evo-ViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>
</td>
</tr>
<tr id="S3.T1.2.6.5" class="ltx_tr">
<td id="S3.T1.2.6.5.1" class="ltx_td ltx_align_center ltx_border_b">attention-based</td>
<td id="S3.T1.2.6.5.2" class="ltx_td ltx_align_center ltx_border_b">✗</td>
<td id="S3.T1.2.6.5.3" class="ltx_td ltx_align_center ltx_border_b">✗</td>
<td id="S3.T1.2.6.5.4" class="ltx_td ltx_align_center ltx_border_b">✗</td>
<td id="S3.T1.2.6.5.5" class="ltx_td ltx_align_left ltx_border_b">EViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>
</td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<ul id="S3.I1" class="ltx_itemize ltx_centering ltx_figure_panel">
<li id="S3.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S3.I1.ix1.1.1.1" class="ltx_text" style="font-size:70%;">2</span></span> 
<div id="S3.I1.ix1.p1" class="ltx_para">
<p id="S3.I1.ix1.p1.1" class="ltx_p"><span id="S3.I1.ix1.p1.1.1" class="ltx_text" style="font-size:80%;">While Evo-ViT is theoretically capable of reusing tokens by design, it tends to use the same tokens throughout the network, details in the supplementary material.</span></p>
</div>
</li>
</ul>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.3.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S3.T1.4.2" class="ltx_text" style="font-size:90%;">A high-level examination of token pruning techniques. The gating module refers to an auxiliary compact network, designed to predict the tokens to be pruned. Attention-based selection involves pruning tokens that receive minimal attention from the class token.</span></figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Insights and Observations</h3>

<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Preserve pruned tokens within feature maps.</h4>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p1.1" class="ltx_p">A notable distinction between classification and dense prediction tasks is how the pruned tokens should be treated.
In classification, token pruning methods often remove tokens permanently because pruned tokens will no longer influence the result, as the classification solely depends on the class token, which is always kept.</p>
</div>
<div id="S3.SS2.SSS0.Px1.p2" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p2.1" class="ltx_p">However, on dense prediction tasks, the pruned tokens can still be utilized by subsequent detection heads, even if they are no longer updated in the backbone. Therefore, it is beneficial to keep the already computed features for pruned tokens for later use.
When pruned tokens are not preserved, we recover a dense feature map by placing remaining tokens in their original location, and zero-pad the pruned ones<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>. Preserving pruned tokes, instead, built the feature map incrementally, each time replacing updated tokens, but keeping pruned ones unchanged.
Preserving pruned tokens can be as fast as removing them (see Table <a href="#S4.T2" title="Table 2 ‣ Preserve pruned tokens within feature maps ‣ 4.1 Evaluation of the insights and observations ‣ 4 Experiments ‣ Revisiting Token Pruning for Object Detection and Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>), and improves model performance on various models on dense tasks.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Reuse preserved tokens on demand.</h4>

<div id="S3.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p1.1" class="ltx_p">As pruned tokens are preserved within feature maps, it is natural to consider whether they should be used again.
In the scope of this paper, ”token preserving” refers to the utilization of pruned tokens only by detection heads, whereas ”token reactivation” implies that these tokens can also be reintroduced into the backbone for subsequent layers.
A counterargument to token reactivation may say that ViT should prioritize allocating computing resources to informative tokens as much as possible <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>, and reactivating pruned tokens may potentially undermine this principle.
However, the definition of ”informative” may vary across different layers since ViT could concentrate on distinct regions at each layer, see supplementary material.
Thus, the ability to reactivate pruned tokens accommodates the distinct attentions of various ViT layers, enabling the model to prioritize its current focus before returning to other relevant tokens in subsequent blocks. Additionally, this makes pruning more robust, as mis-pruned tokens have the opportunity to become active again, see Figure <a href="#S4.F5.sf2" title="Figure 5(b) ‣ Figure 5 ‣ Reuse preserved tokens at demand ‣ 4.1 Evaluation of the insights and observations ‣ 4 Experiments ‣ Revisiting Token Pruning for Object Detection and Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5(b)</span></a>. Ultimately, these advantages lead to a more effective overall utilization of tokens under the same token usage per block. In Section <a href="#S4" title="4 Experiments ‣ Revisiting Token Pruning for Object Detection and Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we allow the model to learn whether and when to reuse pruned tokens by itself, and show that this ability can improve model accuracy by a 0.4 box AP and 0.3 mask AP.</p>
</div>
<figure id="S3.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2306.07050/assets/imgs/2layerMLP.png" id="S3.F3.sf1.g1" class="ltx_graphics ltx_img_landscape" width="685" height="370" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S3.F3.sf1.3.2" class="ltx_text" style="font-size:90%;">Two-layer gating network</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2306.07050/assets/imgs/complex_gating_net.png" id="S3.F3.sf2.g1" class="ltx_graphics ltx_img_landscape" width="685" height="178" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S3.F3.sf2.3.2" class="ltx_text" style="font-size:90%;">Gating network used in DynamicViT</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.4.2.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.2.1" class="ltx_text" style="font-size:90%;">Different types of gating networks for predicting tokens to be pruned. (a) is used by SViT, and (b) is used by DynamicViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>. Normalization and activation functions are omitted for conciseness. <math id="S3.F3.2.1.m1.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.F3.2.1.m1.1b"><mi id="S3.F3.2.1.m1.1.1" xref="S3.F3.2.1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.F3.2.1.m1.1c"><ci id="S3.F3.2.1.m1.1.1.cmml" xref="S3.F3.2.1.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.2.1.m1.1d">C</annotation></semantics></math> represents the dimension of tokens.</span></figcaption>
</figure>
</section>
<section id="S3.SS2.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">A 2-layer MLP can substitute complex gating networks for pruning tokens.</h4>

<div id="S3.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px3.p1.1" class="ltx_p">Prior token pruning approaches tend to employ complex gating networks for predicting the tokens to be pruned.
In DynamicViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, several MLPs are utilized in conjunction with mean and concatenation operations to learn both token-specific and global information for determining which tokens should be pruned, as illustrated in Figure <a href="#S3.F3.sf2" title="Figure 3(b) ‣ Figure 3 ‣ Reuse preserved tokens on demand. ‣ 3.2 Insights and Observations ‣ 3 Token Pruning on dense prediction tasks ‣ Revisiting Token Pruning for Object Detection and Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(b)</span></a>.
SPViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, introduces a more intricate gating network that incorporates an additional head branch to calculate score weights for each individual head.
However, in Section <a href="#S4.SS1" title="4.1 Evaluation of the insights and observations ‣ 4 Experiments ‣ Revisiting Token Pruning for Object Detection and Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>, our study shows that a simple 2-layer MLP in Figure <a href="#S3.F3.sf1" title="Figure 3(a) ‣ Figure 3 ‣ Reuse preserved tokens on demand. ‣ 3.2 Insights and Observations ‣ 3 Token Pruning on dense prediction tasks ‣ Revisiting Token Pruning for Object Detection and Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(a)</span></a> works equally well and simplifies the architecture design.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">A dynamic pruning rate is better than a fixed pruning rate.</h4>

<div id="S3.SS2.SSS0.Px4.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px4.p1.1" class="ltx_p">Several studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> in the context of classification have implemented dynamic pruning rates, adaptively pruning varying numbers of tokens based on the input images during inference.
We further validate its effectiveness in the context of object detection and instance segmentation, and show it is one key components to achieve optimal performance in Section <a href="#S4" title="4 Experiments ‣ Revisiting Token Pruning for Object Detection and Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
<br class="ltx_break"></p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>SViT: Selective Vision Transformer</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">In light of the insights, we introduce the Selective Vision Transformer (SViT), a simple yet effective token pruning model, which seamlessly integrates all prior findings.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">SViT is depicted in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Revisiting Token Pruning for Object Detection and Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. For the selection module, we employ a 2-layer perceptron followed by Gumbel Softmax <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> to make the discrete decision differentiable, as shown in Eq (<a href="#S3.E1" title="Equation 1 ‣ 3.3 SViT: Selective Vision Transformer ‣ 3 Token Pruning on dense prediction tasks ‣ Revisiting Token Pruning for Object Detection and Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). By placing this selection module before the entire ViT block, we facilitate acceleration for both self-attention and the MLP in the transformer encoder:</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<table id="S3.E1" class="ltx_equationgroup ltx_eqn_table">
<tbody>
<tr id="S3.E1X" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E1X.2.1.1.m1.3" class="ltx_Math" alttext="\displaystyle\mathbf{p}=\text{Softmax}(\text{MLP}(\mathbf{x}))\in\mathbb{R}^{N\times 2},\ \mathbf{x}\in\mathbb{R}^{N\times C}" display="inline"><semantics id="S3.E1X.2.1.1.m1.3a"><mrow id="S3.E1X.2.1.1.m1.3.3.2" xref="S3.E1X.2.1.1.m1.3.3.3.cmml"><mrow id="S3.E1X.2.1.1.m1.2.2.1.1" xref="S3.E1X.2.1.1.m1.2.2.1.1.cmml"><mi id="S3.E1X.2.1.1.m1.2.2.1.1.3" xref="S3.E1X.2.1.1.m1.2.2.1.1.3.cmml">𝐩</mi><mo id="S3.E1X.2.1.1.m1.2.2.1.1.4" xref="S3.E1X.2.1.1.m1.2.2.1.1.4.cmml">=</mo><mrow id="S3.E1X.2.1.1.m1.2.2.1.1.1" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.cmml"><mtext id="S3.E1X.2.1.1.m1.2.2.1.1.1.3" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.3a.cmml">Softmax</mtext><mo lspace="0em" rspace="0em" id="S3.E1X.2.1.1.m1.2.2.1.1.1.2" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.2.cmml">​</mo><mrow id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.2" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.cmml"><mtext id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.2" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.2a.cmml">MLP</mtext><mo lspace="0em" rspace="0em" id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.1" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.1.cmml">​</mo><mrow id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.3.2" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.3.2.1" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.cmml">(</mo><mi id="S3.E1X.2.1.1.m1.1.1" xref="S3.E1X.2.1.1.m1.1.1.cmml">𝐱</mi><mo stretchy="false" id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.3.2.2" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.3" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1X.2.1.1.m1.2.2.1.1.5" xref="S3.E1X.2.1.1.m1.2.2.1.1.5.cmml">∈</mo><msup id="S3.E1X.2.1.1.m1.2.2.1.1.6" xref="S3.E1X.2.1.1.m1.2.2.1.1.6.cmml"><mi id="S3.E1X.2.1.1.m1.2.2.1.1.6.2" xref="S3.E1X.2.1.1.m1.2.2.1.1.6.2.cmml">ℝ</mi><mrow id="S3.E1X.2.1.1.m1.2.2.1.1.6.3" xref="S3.E1X.2.1.1.m1.2.2.1.1.6.3.cmml"><mi id="S3.E1X.2.1.1.m1.2.2.1.1.6.3.2" xref="S3.E1X.2.1.1.m1.2.2.1.1.6.3.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E1X.2.1.1.m1.2.2.1.1.6.3.1" xref="S3.E1X.2.1.1.m1.2.2.1.1.6.3.1.cmml">×</mo><mn id="S3.E1X.2.1.1.m1.2.2.1.1.6.3.3" xref="S3.E1X.2.1.1.m1.2.2.1.1.6.3.3.cmml">2</mn></mrow></msup></mrow><mo rspace="0.667em" id="S3.E1X.2.1.1.m1.3.3.2.3" xref="S3.E1X.2.1.1.m1.3.3.3a.cmml">,</mo><mrow id="S3.E1X.2.1.1.m1.3.3.2.2" xref="S3.E1X.2.1.1.m1.3.3.2.2.cmml"><mi id="S3.E1X.2.1.1.m1.3.3.2.2.2" xref="S3.E1X.2.1.1.m1.3.3.2.2.2.cmml">𝐱</mi><mo id="S3.E1X.2.1.1.m1.3.3.2.2.1" xref="S3.E1X.2.1.1.m1.3.3.2.2.1.cmml">∈</mo><msup id="S3.E1X.2.1.1.m1.3.3.2.2.3" xref="S3.E1X.2.1.1.m1.3.3.2.2.3.cmml"><mi id="S3.E1X.2.1.1.m1.3.3.2.2.3.2" xref="S3.E1X.2.1.1.m1.3.3.2.2.3.2.cmml">ℝ</mi><mrow id="S3.E1X.2.1.1.m1.3.3.2.2.3.3" xref="S3.E1X.2.1.1.m1.3.3.2.2.3.3.cmml"><mi id="S3.E1X.2.1.1.m1.3.3.2.2.3.3.2" xref="S3.E1X.2.1.1.m1.3.3.2.2.3.3.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E1X.2.1.1.m1.3.3.2.2.3.3.1" xref="S3.E1X.2.1.1.m1.3.3.2.2.3.3.1.cmml">×</mo><mi id="S3.E1X.2.1.1.m1.3.3.2.2.3.3.3" xref="S3.E1X.2.1.1.m1.3.3.2.2.3.3.3.cmml">C</mi></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1X.2.1.1.m1.3b"><apply id="S3.E1X.2.1.1.m1.3.3.3.cmml" xref="S3.E1X.2.1.1.m1.3.3.2"><csymbol cd="ambiguous" id="S3.E1X.2.1.1.m1.3.3.3a.cmml" xref="S3.E1X.2.1.1.m1.3.3.2.3">formulae-sequence</csymbol><apply id="S3.E1X.2.1.1.m1.2.2.1.1.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1"><and id="S3.E1X.2.1.1.m1.2.2.1.1a.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1"></and><apply id="S3.E1X.2.1.1.m1.2.2.1.1b.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1"><eq id="S3.E1X.2.1.1.m1.2.2.1.1.4.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.4"></eq><ci id="S3.E1X.2.1.1.m1.2.2.1.1.3.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.3">𝐩</ci><apply id="S3.E1X.2.1.1.m1.2.2.1.1.1.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.1"><times id="S3.E1X.2.1.1.m1.2.2.1.1.1.2.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.2"></times><ci id="S3.E1X.2.1.1.m1.2.2.1.1.1.3a.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.3"><mtext id="S3.E1X.2.1.1.m1.2.2.1.1.1.3.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.3">Softmax</mtext></ci><apply id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1"><times id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.1"></times><ci id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.2a.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.2"><mtext id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.2">MLP</mtext></ci><ci id="S3.E1X.2.1.1.m1.1.1.cmml" xref="S3.E1X.2.1.1.m1.1.1">𝐱</ci></apply></apply></apply><apply id="S3.E1X.2.1.1.m1.2.2.1.1c.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1"><in id="S3.E1X.2.1.1.m1.2.2.1.1.5.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.5"></in><share href="#S3.E1X.2.1.1.m1.2.2.1.1.1.cmml" id="S3.E1X.2.1.1.m1.2.2.1.1d.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1"></share><apply id="S3.E1X.2.1.1.m1.2.2.1.1.6.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.6"><csymbol cd="ambiguous" id="S3.E1X.2.1.1.m1.2.2.1.1.6.1.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.6">superscript</csymbol><ci id="S3.E1X.2.1.1.m1.2.2.1.1.6.2.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.6.2">ℝ</ci><apply id="S3.E1X.2.1.1.m1.2.2.1.1.6.3.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.6.3"><times id="S3.E1X.2.1.1.m1.2.2.1.1.6.3.1.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.6.3.1"></times><ci id="S3.E1X.2.1.1.m1.2.2.1.1.6.3.2.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.6.3.2">𝑁</ci><cn type="integer" id="S3.E1X.2.1.1.m1.2.2.1.1.6.3.3.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.6.3.3">2</cn></apply></apply></apply></apply><apply id="S3.E1X.2.1.1.m1.3.3.2.2.cmml" xref="S3.E1X.2.1.1.m1.3.3.2.2"><in id="S3.E1X.2.1.1.m1.3.3.2.2.1.cmml" xref="S3.E1X.2.1.1.m1.3.3.2.2.1"></in><ci id="S3.E1X.2.1.1.m1.3.3.2.2.2.cmml" xref="S3.E1X.2.1.1.m1.3.3.2.2.2">𝐱</ci><apply id="S3.E1X.2.1.1.m1.3.3.2.2.3.cmml" xref="S3.E1X.2.1.1.m1.3.3.2.2.3"><csymbol cd="ambiguous" id="S3.E1X.2.1.1.m1.3.3.2.2.3.1.cmml" xref="S3.E1X.2.1.1.m1.3.3.2.2.3">superscript</csymbol><ci id="S3.E1X.2.1.1.m1.3.3.2.2.3.2.cmml" xref="S3.E1X.2.1.1.m1.3.3.2.2.3.2">ℝ</ci><apply id="S3.E1X.2.1.1.m1.3.3.2.2.3.3.cmml" xref="S3.E1X.2.1.1.m1.3.3.2.2.3.3"><times id="S3.E1X.2.1.1.m1.3.3.2.2.3.3.1.cmml" xref="S3.E1X.2.1.1.m1.3.3.2.2.3.3.1"></times><ci id="S3.E1X.2.1.1.m1.3.3.2.2.3.3.2.cmml" xref="S3.E1X.2.1.1.m1.3.3.2.2.3.3.2">𝑁</ci><ci id="S3.E1X.2.1.1.m1.3.3.2.2.3.3.3.cmml" xref="S3.E1X.2.1.1.m1.3.3.2.2.3.3.3">𝐶</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1X.2.1.1.m1.3c">\displaystyle\mathbf{p}=\text{Softmax}(\text{MLP}(\mathbf{x}))\in\mathbb{R}^{N\times 2},\ \mathbf{x}\in\mathbb{R}^{N\times C}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="3" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(1)</span></td>
</tr>
<tr id="S3.E1Xa" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E1Xa.2.1.1.m1.4" class="ltx_Math" alttext="\displaystyle\mathbf{M}=\text{GumbelSoftmax}(\mathbf{p})\in\{0,1\}^{N}," display="inline"><semantics id="S3.E1Xa.2.1.1.m1.4a"><mrow id="S3.E1Xa.2.1.1.m1.4.4.1" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.cmml"><mrow id="S3.E1Xa.2.1.1.m1.4.4.1.1" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.cmml"><mi id="S3.E1Xa.2.1.1.m1.4.4.1.1.2" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.2.cmml">𝐌</mi><mo id="S3.E1Xa.2.1.1.m1.4.4.1.1.3" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.3.cmml">=</mo><mrow id="S3.E1Xa.2.1.1.m1.4.4.1.1.4" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.4.cmml"><mtext id="S3.E1Xa.2.1.1.m1.4.4.1.1.4.2" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.4.2a.cmml">GumbelSoftmax</mtext><mo lspace="0em" rspace="0em" id="S3.E1Xa.2.1.1.m1.4.4.1.1.4.1" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.4.1.cmml">​</mo><mrow id="S3.E1Xa.2.1.1.m1.4.4.1.1.4.3.2" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.4.cmml"><mo stretchy="false" id="S3.E1Xa.2.1.1.m1.4.4.1.1.4.3.2.1" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.4.cmml">(</mo><mi id="S3.E1Xa.2.1.1.m1.1.1" xref="S3.E1Xa.2.1.1.m1.1.1.cmml">𝐩</mi><mo stretchy="false" id="S3.E1Xa.2.1.1.m1.4.4.1.1.4.3.2.2" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.4.cmml">)</mo></mrow></mrow><mo id="S3.E1Xa.2.1.1.m1.4.4.1.1.5" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.5.cmml">∈</mo><msup id="S3.E1Xa.2.1.1.m1.4.4.1.1.6" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.6.cmml"><mrow id="S3.E1Xa.2.1.1.m1.4.4.1.1.6.2.2" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.6.2.1.cmml"><mo stretchy="false" id="S3.E1Xa.2.1.1.m1.4.4.1.1.6.2.2.1" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.6.2.1.cmml">{</mo><mn id="S3.E1Xa.2.1.1.m1.2.2" xref="S3.E1Xa.2.1.1.m1.2.2.cmml">0</mn><mo id="S3.E1Xa.2.1.1.m1.4.4.1.1.6.2.2.2" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.6.2.1.cmml">,</mo><mn id="S3.E1Xa.2.1.1.m1.3.3" xref="S3.E1Xa.2.1.1.m1.3.3.cmml">1</mn><mo stretchy="false" id="S3.E1Xa.2.1.1.m1.4.4.1.1.6.2.2.3" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.6.2.1.cmml">}</mo></mrow><mi id="S3.E1Xa.2.1.1.m1.4.4.1.1.6.3" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.6.3.cmml">N</mi></msup></mrow><mo id="S3.E1Xa.2.1.1.m1.4.4.1.2" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1Xa.2.1.1.m1.4b"><apply id="S3.E1Xa.2.1.1.m1.4.4.1.1.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.1"><and id="S3.E1Xa.2.1.1.m1.4.4.1.1a.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.1"></and><apply id="S3.E1Xa.2.1.1.m1.4.4.1.1b.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.1"><eq id="S3.E1Xa.2.1.1.m1.4.4.1.1.3.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.3"></eq><ci id="S3.E1Xa.2.1.1.m1.4.4.1.1.2.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.2">𝐌</ci><apply id="S3.E1Xa.2.1.1.m1.4.4.1.1.4.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.4"><times id="S3.E1Xa.2.1.1.m1.4.4.1.1.4.1.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.4.1"></times><ci id="S3.E1Xa.2.1.1.m1.4.4.1.1.4.2a.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.4.2"><mtext id="S3.E1Xa.2.1.1.m1.4.4.1.1.4.2.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.4.2">GumbelSoftmax</mtext></ci><ci id="S3.E1Xa.2.1.1.m1.1.1.cmml" xref="S3.E1Xa.2.1.1.m1.1.1">𝐩</ci></apply></apply><apply id="S3.E1Xa.2.1.1.m1.4.4.1.1c.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.1"><in id="S3.E1Xa.2.1.1.m1.4.4.1.1.5.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.5"></in><share href="#S3.E1Xa.2.1.1.m1.4.4.1.1.4.cmml" id="S3.E1Xa.2.1.1.m1.4.4.1.1d.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.1"></share><apply id="S3.E1Xa.2.1.1.m1.4.4.1.1.6.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.6"><csymbol cd="ambiguous" id="S3.E1Xa.2.1.1.m1.4.4.1.1.6.1.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.6">superscript</csymbol><set id="S3.E1Xa.2.1.1.m1.4.4.1.1.6.2.1.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.6.2.2"><cn type="integer" id="S3.E1Xa.2.1.1.m1.2.2.cmml" xref="S3.E1Xa.2.1.1.m1.2.2">0</cn><cn type="integer" id="S3.E1Xa.2.1.1.m1.3.3.cmml" xref="S3.E1Xa.2.1.1.m1.3.3">1</cn></set><ci id="S3.E1Xa.2.1.1.m1.4.4.1.1.6.3.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.6.3">𝑁</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1Xa.2.1.1.m1.4c">\displaystyle\mathbf{M}=\text{GumbelSoftmax}(\mathbf{p})\in\{0,1\}^{N},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr id="S3.E1Xb" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E1Xb.2.1.1.m1.3" class="ltx_Math" alttext="\displaystyle\mathbf{x}\leftarrow\mathbf{M}\odot\text{ViTBlock}(\mathbf{x},\mathbf{M})+(1-\mathbf{M})\odot\mathbf{x}" display="inline"><semantics id="S3.E1Xb.2.1.1.m1.3a"><mrow id="S3.E1Xb.2.1.1.m1.3.3" xref="S3.E1Xb.2.1.1.m1.3.3.cmml"><mi id="S3.E1Xb.2.1.1.m1.3.3.3" xref="S3.E1Xb.2.1.1.m1.3.3.3.cmml">𝐱</mi><mo stretchy="false" id="S3.E1Xb.2.1.1.m1.3.3.2" xref="S3.E1Xb.2.1.1.m1.3.3.2.cmml">←</mo><mrow id="S3.E1Xb.2.1.1.m1.3.3.1" xref="S3.E1Xb.2.1.1.m1.3.3.1.cmml"><mrow id="S3.E1Xb.2.1.1.m1.3.3.1.3" xref="S3.E1Xb.2.1.1.m1.3.3.1.3.cmml"><mrow id="S3.E1Xb.2.1.1.m1.3.3.1.3.2" xref="S3.E1Xb.2.1.1.m1.3.3.1.3.2.cmml"><mi id="S3.E1Xb.2.1.1.m1.3.3.1.3.2.2" xref="S3.E1Xb.2.1.1.m1.3.3.1.3.2.2.cmml">𝐌</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E1Xb.2.1.1.m1.3.3.1.3.2.1" xref="S3.E1Xb.2.1.1.m1.3.3.1.3.2.1.cmml">⊙</mo><mtext id="S3.E1Xb.2.1.1.m1.3.3.1.3.2.3" xref="S3.E1Xb.2.1.1.m1.3.3.1.3.2.3a.cmml">ViTBlock</mtext></mrow><mo lspace="0em" rspace="0em" id="S3.E1Xb.2.1.1.m1.3.3.1.3.1" xref="S3.E1Xb.2.1.1.m1.3.3.1.3.1.cmml">​</mo><mrow id="S3.E1Xb.2.1.1.m1.3.3.1.3.3.2" xref="S3.E1Xb.2.1.1.m1.3.3.1.3.3.1.cmml"><mo stretchy="false" id="S3.E1Xb.2.1.1.m1.3.3.1.3.3.2.1" xref="S3.E1Xb.2.1.1.m1.3.3.1.3.3.1.cmml">(</mo><mi id="S3.E1Xb.2.1.1.m1.1.1" xref="S3.E1Xb.2.1.1.m1.1.1.cmml">𝐱</mi><mo id="S3.E1Xb.2.1.1.m1.3.3.1.3.3.2.2" xref="S3.E1Xb.2.1.1.m1.3.3.1.3.3.1.cmml">,</mo><mi id="S3.E1Xb.2.1.1.m1.2.2" xref="S3.E1Xb.2.1.1.m1.2.2.cmml">𝐌</mi><mo stretchy="false" id="S3.E1Xb.2.1.1.m1.3.3.1.3.3.2.3" xref="S3.E1Xb.2.1.1.m1.3.3.1.3.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E1Xb.2.1.1.m1.3.3.1.2" xref="S3.E1Xb.2.1.1.m1.3.3.1.2.cmml">+</mo><mrow id="S3.E1Xb.2.1.1.m1.3.3.1.1" xref="S3.E1Xb.2.1.1.m1.3.3.1.1.cmml"><mrow id="S3.E1Xb.2.1.1.m1.3.3.1.1.1.1" xref="S3.E1Xb.2.1.1.m1.3.3.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1Xb.2.1.1.m1.3.3.1.1.1.1.2" xref="S3.E1Xb.2.1.1.m1.3.3.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1Xb.2.1.1.m1.3.3.1.1.1.1.1" xref="S3.E1Xb.2.1.1.m1.3.3.1.1.1.1.1.cmml"><mn id="S3.E1Xb.2.1.1.m1.3.3.1.1.1.1.1.2" xref="S3.E1Xb.2.1.1.m1.3.3.1.1.1.1.1.2.cmml">1</mn><mo id="S3.E1Xb.2.1.1.m1.3.3.1.1.1.1.1.1" xref="S3.E1Xb.2.1.1.m1.3.3.1.1.1.1.1.1.cmml">−</mo><mi id="S3.E1Xb.2.1.1.m1.3.3.1.1.1.1.1.3" xref="S3.E1Xb.2.1.1.m1.3.3.1.1.1.1.1.3.cmml">𝐌</mi></mrow><mo rspace="0.055em" stretchy="false" id="S3.E1Xb.2.1.1.m1.3.3.1.1.1.1.3" xref="S3.E1Xb.2.1.1.m1.3.3.1.1.1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S3.E1Xb.2.1.1.m1.3.3.1.1.2" xref="S3.E1Xb.2.1.1.m1.3.3.1.1.2.cmml">⊙</mo><mi id="S3.E1Xb.2.1.1.m1.3.3.1.1.3" xref="S3.E1Xb.2.1.1.m1.3.3.1.1.3.cmml">𝐱</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1Xb.2.1.1.m1.3b"><apply id="S3.E1Xb.2.1.1.m1.3.3.cmml" xref="S3.E1Xb.2.1.1.m1.3.3"><ci id="S3.E1Xb.2.1.1.m1.3.3.2.cmml" xref="S3.E1Xb.2.1.1.m1.3.3.2">←</ci><ci id="S3.E1Xb.2.1.1.m1.3.3.3.cmml" xref="S3.E1Xb.2.1.1.m1.3.3.3">𝐱</ci><apply id="S3.E1Xb.2.1.1.m1.3.3.1.cmml" xref="S3.E1Xb.2.1.1.m1.3.3.1"><plus id="S3.E1Xb.2.1.1.m1.3.3.1.2.cmml" xref="S3.E1Xb.2.1.1.m1.3.3.1.2"></plus><apply id="S3.E1Xb.2.1.1.m1.3.3.1.3.cmml" xref="S3.E1Xb.2.1.1.m1.3.3.1.3"><times id="S3.E1Xb.2.1.1.m1.3.3.1.3.1.cmml" xref="S3.E1Xb.2.1.1.m1.3.3.1.3.1"></times><apply id="S3.E1Xb.2.1.1.m1.3.3.1.3.2.cmml" xref="S3.E1Xb.2.1.1.m1.3.3.1.3.2"><csymbol cd="latexml" id="S3.E1Xb.2.1.1.m1.3.3.1.3.2.1.cmml" xref="S3.E1Xb.2.1.1.m1.3.3.1.3.2.1">direct-product</csymbol><ci id="S3.E1Xb.2.1.1.m1.3.3.1.3.2.2.cmml" xref="S3.E1Xb.2.1.1.m1.3.3.1.3.2.2">𝐌</ci><ci id="S3.E1Xb.2.1.1.m1.3.3.1.3.2.3a.cmml" xref="S3.E1Xb.2.1.1.m1.3.3.1.3.2.3"><mtext id="S3.E1Xb.2.1.1.m1.3.3.1.3.2.3.cmml" xref="S3.E1Xb.2.1.1.m1.3.3.1.3.2.3">ViTBlock</mtext></ci></apply><interval closure="open" id="S3.E1Xb.2.1.1.m1.3.3.1.3.3.1.cmml" xref="S3.E1Xb.2.1.1.m1.3.3.1.3.3.2"><ci id="S3.E1Xb.2.1.1.m1.1.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.1">𝐱</ci><ci id="S3.E1Xb.2.1.1.m1.2.2.cmml" xref="S3.E1Xb.2.1.1.m1.2.2">𝐌</ci></interval></apply><apply id="S3.E1Xb.2.1.1.m1.3.3.1.1.cmml" xref="S3.E1Xb.2.1.1.m1.3.3.1.1"><csymbol cd="latexml" id="S3.E1Xb.2.1.1.m1.3.3.1.1.2.cmml" xref="S3.E1Xb.2.1.1.m1.3.3.1.1.2">direct-product</csymbol><apply id="S3.E1Xb.2.1.1.m1.3.3.1.1.1.1.1.cmml" xref="S3.E1Xb.2.1.1.m1.3.3.1.1.1.1"><minus id="S3.E1Xb.2.1.1.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E1Xb.2.1.1.m1.3.3.1.1.1.1.1.1"></minus><cn type="integer" id="S3.E1Xb.2.1.1.m1.3.3.1.1.1.1.1.2.cmml" xref="S3.E1Xb.2.1.1.m1.3.3.1.1.1.1.1.2">1</cn><ci id="S3.E1Xb.2.1.1.m1.3.3.1.1.1.1.1.3.cmml" xref="S3.E1Xb.2.1.1.m1.3.3.1.1.1.1.1.3">𝐌</ci></apply><ci id="S3.E1Xb.2.1.1.m1.3.3.1.1.3.cmml" xref="S3.E1Xb.2.1.1.m1.3.3.1.1.3">𝐱</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1Xb.2.1.1.m1.3c">\displaystyle\mathbf{x}\leftarrow\mathbf{M}\odot\text{ViTBlock}(\mathbf{x},\mathbf{M})+(1-\mathbf{M})\odot\mathbf{x}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p id="S3.SS3.p3.9" class="ltx_p">where <math id="S3.SS3.p3.1.m1.1" class="ltx_Math" alttext="\mathbf{x}" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><mi id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">𝐱</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><ci id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">𝐱</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">\mathbf{x}</annotation></semantics></math> represents the input tokens, <math id="S3.SS3.p3.2.m2.1" class="ltx_Math" alttext="\mathbf{p}" display="inline"><semantics id="S3.SS3.p3.2.m2.1a"><mi id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml">𝐩</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><ci id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1">𝐩</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">\mathbf{p}</annotation></semantics></math> is the intermediate sampling probability, <math id="S3.SS3.p3.3.m3.1" class="ltx_Math" alttext="\mathbf{M}" display="inline"><semantics id="S3.SS3.p3.3.m3.1a"><mi id="S3.SS3.p3.3.m3.1.1" xref="S3.SS3.p3.3.m3.1.1.cmml">𝐌</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.3.m3.1b"><ci id="S3.SS3.p3.3.m3.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1">𝐌</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.3.m3.1c">\mathbf{M}</annotation></semantics></math> signifies token masks, and <math id="S3.SS3.p3.4.m4.1" class="ltx_Math" alttext="\odot" display="inline"><semantics id="S3.SS3.p3.4.m4.1a"><mo id="S3.SS3.p3.4.m4.1.1" xref="S3.SS3.p3.4.m4.1.1.cmml">⊙</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.4.m4.1b"><csymbol cd="latexml" id="S3.SS3.p3.4.m4.1.1.cmml" xref="S3.SS3.p3.4.m4.1.1">direct-product</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.4.m4.1c">\odot</annotation></semantics></math> is Hadamard product. The MLP transforms token dimensions from <math id="S3.SS3.p3.5.m5.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.SS3.p3.5.m5.1a"><mi id="S3.SS3.p3.5.m5.1.1" xref="S3.SS3.p3.5.m5.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.5.m5.1b"><ci id="S3.SS3.p3.5.m5.1.1.cmml" xref="S3.SS3.p3.5.m5.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.5.m5.1c">C</annotation></semantics></math> to <math id="S3.SS3.p3.6.m6.1" class="ltx_Math" alttext="\frac{C}{4}" display="inline"><semantics id="S3.SS3.p3.6.m6.1a"><mfrac id="S3.SS3.p3.6.m6.1.1" xref="S3.SS3.p3.6.m6.1.1.cmml"><mi id="S3.SS3.p3.6.m6.1.1.2" xref="S3.SS3.p3.6.m6.1.1.2.cmml">C</mi><mn id="S3.SS3.p3.6.m6.1.1.3" xref="S3.SS3.p3.6.m6.1.1.3.cmml">4</mn></mfrac><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.6.m6.1b"><apply id="S3.SS3.p3.6.m6.1.1.cmml" xref="S3.SS3.p3.6.m6.1.1"><divide id="S3.SS3.p3.6.m6.1.1.1.cmml" xref="S3.SS3.p3.6.m6.1.1"></divide><ci id="S3.SS3.p3.6.m6.1.1.2.cmml" xref="S3.SS3.p3.6.m6.1.1.2">𝐶</ci><cn type="integer" id="S3.SS3.p3.6.m6.1.1.3.cmml" xref="S3.SS3.p3.6.m6.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.6.m6.1c">\frac{C}{4}</annotation></semantics></math>, and <math id="S3.SS3.p3.7.m7.1" class="ltx_Math" alttext="\frac{C}{4}" display="inline"><semantics id="S3.SS3.p3.7.m7.1a"><mfrac id="S3.SS3.p3.7.m7.1.1" xref="S3.SS3.p3.7.m7.1.1.cmml"><mi id="S3.SS3.p3.7.m7.1.1.2" xref="S3.SS3.p3.7.m7.1.1.2.cmml">C</mi><mn id="S3.SS3.p3.7.m7.1.1.3" xref="S3.SS3.p3.7.m7.1.1.3.cmml">4</mn></mfrac><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.7.m7.1b"><apply id="S3.SS3.p3.7.m7.1.1.cmml" xref="S3.SS3.p3.7.m7.1.1"><divide id="S3.SS3.p3.7.m7.1.1.1.cmml" xref="S3.SS3.p3.7.m7.1.1"></divide><ci id="S3.SS3.p3.7.m7.1.1.2.cmml" xref="S3.SS3.p3.7.m7.1.1.2">𝐶</ci><cn type="integer" id="S3.SS3.p3.7.m7.1.1.3.cmml" xref="S3.SS3.p3.7.m7.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.7.m7.1c">\frac{C}{4}</annotation></semantics></math> to <math id="S3.SS3.p3.8.m8.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S3.SS3.p3.8.m8.1a"><mn id="S3.SS3.p3.8.m8.1.1" xref="S3.SS3.p3.8.m8.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.8.m8.1b"><cn type="integer" id="S3.SS3.p3.8.m8.1.1.cmml" xref="S3.SS3.p3.8.m8.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.8.m8.1c">2</annotation></semantics></math>.
The ViTBlock takes in the masks <math id="S3.SS3.p3.9.m9.1" class="ltx_Math" alttext="\mathbf{M}" display="inline"><semantics id="S3.SS3.p3.9.m9.1a"><mi id="S3.SS3.p3.9.m9.1.1" xref="S3.SS3.p3.9.m9.1.1.cmml">𝐌</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.9.m9.1b"><ci id="S3.SS3.p3.9.m9.1.1.cmml" xref="S3.SS3.p3.9.m9.1.1">𝐌</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.9.m9.1c">\mathbf{M}</annotation></semantics></math> and eliminates the influence of pruned tokens on other tokens during training by setting the corresponding columns in the attention matrix to 0.
During inference, we simply gather the active tokens, feed them to the current ViT Block, and then scatter them back to the previous feature map.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2306.07050/assets/imgs/qualitative_results.png" id="S3.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="326" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.3.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.4.2" class="ltx_text" style="font-size:90%;">SViT learns to allocate computation to visually more important tokens. The token usage heatmap shows the number of layers used for each token and reflects computational distribution over the input space, which highly align with fine-grained object contours. More visualizations and dataset-level statistics are in the supplementary material.</span></figcaption>
</figure>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">For controlling the number of pruned tokens, similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, we use a dynamic pruning ratio loss during training as in Eq (<a href="#S3.E2" title="Equation 2 ‣ 3.3 SViT: Selective Vision Transformer ‣ 3 Token Pruning on dense prediction tasks ‣ Revisiting Token Pruning for Object Detection and Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>):</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<table id="S3.E2" class="ltx_equationgroup ltx_eqn_table">
<tbody>
<tr id="S3.E2X" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E2X.2.1.1.m1.3" class="ltx_Math" alttext="\displaystyle\mathcal{L}_{dynamic}=\frac{1}{L}\sum_{l\in L}((\frac{1}{BN}\sum_{b\in B}\sum_{n\in N}{\mathbf{M}}_{n}^{b,l})-\mathbf{t}^{l})^{2}," display="inline"><semantics id="S3.E2X.2.1.1.m1.3a"><mrow id="S3.E2X.2.1.1.m1.3.3.1" xref="S3.E2X.2.1.1.m1.3.3.1.1.cmml"><mrow id="S3.E2X.2.1.1.m1.3.3.1.1" xref="S3.E2X.2.1.1.m1.3.3.1.1.cmml"><msub id="S3.E2X.2.1.1.m1.3.3.1.1.3" xref="S3.E2X.2.1.1.m1.3.3.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2X.2.1.1.m1.3.3.1.1.3.2" xref="S3.E2X.2.1.1.m1.3.3.1.1.3.2.cmml">ℒ</mi><mrow id="S3.E2X.2.1.1.m1.3.3.1.1.3.3" xref="S3.E2X.2.1.1.m1.3.3.1.1.3.3.cmml"><mi id="S3.E2X.2.1.1.m1.3.3.1.1.3.3.2" xref="S3.E2X.2.1.1.m1.3.3.1.1.3.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.E2X.2.1.1.m1.3.3.1.1.3.3.1" xref="S3.E2X.2.1.1.m1.3.3.1.1.3.3.1.cmml">​</mo><mi id="S3.E2X.2.1.1.m1.3.3.1.1.3.3.3" xref="S3.E2X.2.1.1.m1.3.3.1.1.3.3.3.cmml">y</mi><mo lspace="0em" rspace="0em" id="S3.E2X.2.1.1.m1.3.3.1.1.3.3.1a" xref="S3.E2X.2.1.1.m1.3.3.1.1.3.3.1.cmml">​</mo><mi id="S3.E2X.2.1.1.m1.3.3.1.1.3.3.4" xref="S3.E2X.2.1.1.m1.3.3.1.1.3.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E2X.2.1.1.m1.3.3.1.1.3.3.1b" xref="S3.E2X.2.1.1.m1.3.3.1.1.3.3.1.cmml">​</mo><mi id="S3.E2X.2.1.1.m1.3.3.1.1.3.3.5" xref="S3.E2X.2.1.1.m1.3.3.1.1.3.3.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E2X.2.1.1.m1.3.3.1.1.3.3.1c" xref="S3.E2X.2.1.1.m1.3.3.1.1.3.3.1.cmml">​</mo><mi id="S3.E2X.2.1.1.m1.3.3.1.1.3.3.6" xref="S3.E2X.2.1.1.m1.3.3.1.1.3.3.6.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E2X.2.1.1.m1.3.3.1.1.3.3.1d" xref="S3.E2X.2.1.1.m1.3.3.1.1.3.3.1.cmml">​</mo><mi id="S3.E2X.2.1.1.m1.3.3.1.1.3.3.7" xref="S3.E2X.2.1.1.m1.3.3.1.1.3.3.7.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E2X.2.1.1.m1.3.3.1.1.3.3.1e" xref="S3.E2X.2.1.1.m1.3.3.1.1.3.3.1.cmml">​</mo><mi id="S3.E2X.2.1.1.m1.3.3.1.1.3.3.8" xref="S3.E2X.2.1.1.m1.3.3.1.1.3.3.8.cmml">c</mi></mrow></msub><mo id="S3.E2X.2.1.1.m1.3.3.1.1.2" xref="S3.E2X.2.1.1.m1.3.3.1.1.2.cmml">=</mo><mrow id="S3.E2X.2.1.1.m1.3.3.1.1.1" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.cmml"><mstyle displaystyle="true" id="S3.E2X.2.1.1.m1.3.3.1.1.1.3" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.3.cmml"><mfrac id="S3.E2X.2.1.1.m1.3.3.1.1.1.3a" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.3.cmml"><mn id="S3.E2X.2.1.1.m1.3.3.1.1.1.3.2" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.3.2.cmml">1</mn><mi id="S3.E2X.2.1.1.m1.3.3.1.1.1.3.3" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.3.3.cmml">L</mi></mfrac></mstyle><mo lspace="0em" rspace="0em" id="S3.E2X.2.1.1.m1.3.3.1.1.1.2" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.2.cmml">​</mo><mrow id="S3.E2X.2.1.1.m1.3.3.1.1.1.1" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.2" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.2.cmml"><munder id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.2a" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.2.cmml"><mo movablelimits="false" id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.2.2" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.2.2.cmml">∑</mo><mrow id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.2.3" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.2.3.cmml"><mi id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.2.3.2" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.2.3.2.cmml">l</mi><mo id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.2.3.1" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.2.3.1.cmml">∈</mo><mi id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.2.3.3" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.2.3.3.cmml">L</mi></mrow></munder></mstyle><msup id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.cmml"><mrow id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.2" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mfrac id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2a" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mn id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">1</mn><mrow id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.2" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.2.cmml">B</mi><mo lspace="0em" rspace="0em" id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.1" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.1.cmml">​</mo><mi id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.3" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.3.cmml">N</mi></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em" id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml">​</mo><mrow id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mstyle displaystyle="true" id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml"><munder id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1a" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml"><mo movablelimits="false" id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1.2" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1.2.cmml">∑</mo><mrow id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1.3" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1.3.cmml"><mi id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1.3.2" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1.3.2.cmml">b</mi><mo id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1.3.1" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1.3.1.cmml">∈</mo><mi id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1.3.3" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1.3.3.cmml">B</mi></mrow></munder></mstyle><mrow id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml"><mstyle displaystyle="true" id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.1" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.1.cmml"><munder id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.1a" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.1.cmml"><mo movablelimits="false" id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.1.2" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.1.2.cmml">∑</mo><mrow id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.1.3" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.1.3.cmml"><mi id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.1.3.2" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.1.3.2.cmml">n</mi><mo id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.1.3.1" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.1.3.1.cmml">∈</mo><mi id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.1.3.3" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.1.3.3.cmml">N</mi></mrow></munder></mstyle><msubsup id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.2" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.2.cmml"><mi id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.2.2.2" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.2.2.2.cmml">𝐌</mi><mi id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.2.2.3" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.2.2.3.cmml">n</mi><mrow id="S3.E2X.2.1.1.m1.2.2.2.4" xref="S3.E2X.2.1.1.m1.2.2.2.3.cmml"><mi id="S3.E2X.2.1.1.m1.1.1.1.1" xref="S3.E2X.2.1.1.m1.1.1.1.1.cmml">b</mi><mo id="S3.E2X.2.1.1.m1.2.2.2.4.1" xref="S3.E2X.2.1.1.m1.2.2.2.3.cmml">,</mo><mi id="S3.E2X.2.1.1.m1.2.2.2.2" xref="S3.E2X.2.1.1.m1.2.2.2.2.cmml">l</mi></mrow></msubsup></mrow></mrow></mrow><mo stretchy="false" id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.2" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.2.cmml">−</mo><msup id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.3" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.3.2" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.3.2.cmml">𝐭</mi><mi id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.3.3" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.3.3.cmml">l</mi></msup></mrow><mo stretchy="false" id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.3" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mn id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.3" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.3.cmml">2</mn></msup></mrow></mrow></mrow><mo id="S3.E2X.2.1.1.m1.3.3.1.2" xref="S3.E2X.2.1.1.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2X.2.1.1.m1.3b"><apply id="S3.E2X.2.1.1.m1.3.3.1.1.cmml" xref="S3.E2X.2.1.1.m1.3.3.1"><eq id="S3.E2X.2.1.1.m1.3.3.1.1.2.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.2"></eq><apply id="S3.E2X.2.1.1.m1.3.3.1.1.3.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.3"><csymbol cd="ambiguous" id="S3.E2X.2.1.1.m1.3.3.1.1.3.1.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.3">subscript</csymbol><ci id="S3.E2X.2.1.1.m1.3.3.1.1.3.2.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.3.2">ℒ</ci><apply id="S3.E2X.2.1.1.m1.3.3.1.1.3.3.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.3.3"><times id="S3.E2X.2.1.1.m1.3.3.1.1.3.3.1.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.3.3.1"></times><ci id="S3.E2X.2.1.1.m1.3.3.1.1.3.3.2.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.3.3.2">𝑑</ci><ci id="S3.E2X.2.1.1.m1.3.3.1.1.3.3.3.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.3.3.3">𝑦</ci><ci id="S3.E2X.2.1.1.m1.3.3.1.1.3.3.4.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.3.3.4">𝑛</ci><ci id="S3.E2X.2.1.1.m1.3.3.1.1.3.3.5.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.3.3.5">𝑎</ci><ci id="S3.E2X.2.1.1.m1.3.3.1.1.3.3.6.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.3.3.6">𝑚</ci><ci id="S3.E2X.2.1.1.m1.3.3.1.1.3.3.7.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.3.3.7">𝑖</ci><ci id="S3.E2X.2.1.1.m1.3.3.1.1.3.3.8.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.3.3.8">𝑐</ci></apply></apply><apply id="S3.E2X.2.1.1.m1.3.3.1.1.1.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1"><times id="S3.E2X.2.1.1.m1.3.3.1.1.1.2.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.2"></times><apply id="S3.E2X.2.1.1.m1.3.3.1.1.1.3.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.3"><divide id="S3.E2X.2.1.1.m1.3.3.1.1.1.3.1.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.3"></divide><cn type="integer" id="S3.E2X.2.1.1.m1.3.3.1.1.1.3.2.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.3.2">1</cn><ci id="S3.E2X.2.1.1.m1.3.3.1.1.1.3.3.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.3.3">𝐿</ci></apply><apply id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1"><apply id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.2.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.2.1.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.2">subscript</csymbol><sum id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.2.2.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.2.2"></sum><apply id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.2.3.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.2.3"><in id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.2.3.1.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.2.3.1"></in><ci id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.2.3.2.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.2.3.2">𝑙</ci><ci id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.2.3.3.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.2.3.3">𝐿</ci></apply></apply><apply id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.2.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1">superscript</csymbol><apply id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1"><minus id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.2"></minus><apply id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1"><times id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1"></times><apply id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2"><divide id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2"></divide><cn type="integer" id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2">1</cn><apply id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3"><times id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.1"></times><ci id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.2">𝐵</ci><ci id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.3">𝑁</ci></apply></apply><apply id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3"><apply id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1"><csymbol cd="ambiguous" id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1.1.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1">subscript</csymbol><sum id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1.2.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1.2"></sum><apply id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1.3.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1.3"><in id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1.3.1.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1.3.1"></in><ci id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1.3.2.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1.3.2">𝑏</ci><ci id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1.3.3.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1.3.3">𝐵</ci></apply></apply><apply id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2"><apply id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.1"><csymbol cd="ambiguous" id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.1.1.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.1">subscript</csymbol><sum id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.1.2.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.1.2"></sum><apply id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.1.3.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.1.3"><in id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.1.3.1.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.1.3.1"></in><ci id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.1.3.2.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.1.3.2">𝑛</ci><ci id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.1.3.3.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.1.3.3">𝑁</ci></apply></apply><apply id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.2.1.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.2">superscript</csymbol><apply id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.2.2.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.2.2.1.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.2">subscript</csymbol><ci id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.2.2.2.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.2.2.2">𝐌</ci><ci id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.2.2.3.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.2.2.3">𝑛</ci></apply><list id="S3.E2X.2.1.1.m1.2.2.2.3.cmml" xref="S3.E2X.2.1.1.m1.2.2.2.4"><ci id="S3.E2X.2.1.1.m1.1.1.1.1.cmml" xref="S3.E2X.2.1.1.m1.1.1.1.1">𝑏</ci><ci id="S3.E2X.2.1.1.m1.2.2.2.2.cmml" xref="S3.E2X.2.1.1.m1.2.2.2.2">𝑙</ci></list></apply></apply></apply></apply><apply id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.3.2">𝐭</ci><ci id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.3.3">𝑙</ci></apply></apply><cn type="integer" id="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.3.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.1.1.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2X.2.1.1.m1.3c">\displaystyle\mathcal{L}_{dynamic}=\frac{1}{L}\sum_{l\in L}((\frac{1}{BN}\sum_{b\in B}\sum_{n\in N}{\mathbf{M}}_{n}^{b,l})-\mathbf{t}^{l})^{2},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="2" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(2)</span></td>
</tr>
<tr id="S3.E2Xa" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E2Xa.2.1.1.m1.1" class="ltx_Math" alttext="\displaystyle\rm{\mathcal{L}_{total}=\mathcal{L}_{task}+\lambda\mathcal{L}_{dynamic}}" display="inline"><semantics id="S3.E2Xa.2.1.1.m1.1a"><mrow id="S3.E2Xa.2.1.1.m1.1.1" xref="S3.E2Xa.2.1.1.m1.1.1.cmml"><msub id="S3.E2Xa.2.1.1.m1.1.1.2" xref="S3.E2Xa.2.1.1.m1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2Xa.2.1.1.m1.1.1.2.2" xref="S3.E2Xa.2.1.1.m1.1.1.2.2.cmml">ℒ</mi><mi id="S3.E2Xa.2.1.1.m1.1.1.2.3" xref="S3.E2Xa.2.1.1.m1.1.1.2.3.cmml">total</mi></msub><mo id="S3.E2Xa.2.1.1.m1.1.1.1" xref="S3.E2Xa.2.1.1.m1.1.1.1.cmml">=</mo><mrow id="S3.E2Xa.2.1.1.m1.1.1.3" xref="S3.E2Xa.2.1.1.m1.1.1.3.cmml"><msub id="S3.E2Xa.2.1.1.m1.1.1.3.2" xref="S3.E2Xa.2.1.1.m1.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2Xa.2.1.1.m1.1.1.3.2.2" xref="S3.E2Xa.2.1.1.m1.1.1.3.2.2.cmml">ℒ</mi><mi id="S3.E2Xa.2.1.1.m1.1.1.3.2.3" xref="S3.E2Xa.2.1.1.m1.1.1.3.2.3.cmml">task</mi></msub><mo id="S3.E2Xa.2.1.1.m1.1.1.3.1" xref="S3.E2Xa.2.1.1.m1.1.1.3.1.cmml">+</mo><mrow id="S3.E2Xa.2.1.1.m1.1.1.3.3" xref="S3.E2Xa.2.1.1.m1.1.1.3.3.cmml"><mi id="S3.E2Xa.2.1.1.m1.1.1.3.3.2" xref="S3.E2Xa.2.1.1.m1.1.1.3.3.2.cmml">λ</mi><mo lspace="0em" rspace="0em" id="S3.E2Xa.2.1.1.m1.1.1.3.3.1" xref="S3.E2Xa.2.1.1.m1.1.1.3.3.1.cmml">​</mo><msub id="S3.E2Xa.2.1.1.m1.1.1.3.3.3" xref="S3.E2Xa.2.1.1.m1.1.1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2Xa.2.1.1.m1.1.1.3.3.3.2" xref="S3.E2Xa.2.1.1.m1.1.1.3.3.3.2.cmml">ℒ</mi><mi id="S3.E2Xa.2.1.1.m1.1.1.3.3.3.3" xref="S3.E2Xa.2.1.1.m1.1.1.3.3.3.3.cmml">dynamic</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2Xa.2.1.1.m1.1b"><apply id="S3.E2Xa.2.1.1.m1.1.1.cmml" xref="S3.E2Xa.2.1.1.m1.1.1"><eq id="S3.E2Xa.2.1.1.m1.1.1.1.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.1"></eq><apply id="S3.E2Xa.2.1.1.m1.1.1.2.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E2Xa.2.1.1.m1.1.1.2.1.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.2">subscript</csymbol><ci id="S3.E2Xa.2.1.1.m1.1.1.2.2.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.2.2">ℒ</ci><ci id="S3.E2Xa.2.1.1.m1.1.1.2.3.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.2.3">total</ci></apply><apply id="S3.E2Xa.2.1.1.m1.1.1.3.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.3"><plus id="S3.E2Xa.2.1.1.m1.1.1.3.1.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.3.1"></plus><apply id="S3.E2Xa.2.1.1.m1.1.1.3.2.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E2Xa.2.1.1.m1.1.1.3.2.1.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.3.2">subscript</csymbol><ci id="S3.E2Xa.2.1.1.m1.1.1.3.2.2.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.3.2.2">ℒ</ci><ci id="S3.E2Xa.2.1.1.m1.1.1.3.2.3.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.3.2.3">task</ci></apply><apply id="S3.E2Xa.2.1.1.m1.1.1.3.3.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.3.3"><times id="S3.E2Xa.2.1.1.m1.1.1.3.3.1.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.3.3.1"></times><ci id="S3.E2Xa.2.1.1.m1.1.1.3.3.2.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.3.3.2">𝜆</ci><apply id="S3.E2Xa.2.1.1.m1.1.1.3.3.3.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E2Xa.2.1.1.m1.1.1.3.3.3.1.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.3.3.3">subscript</csymbol><ci id="S3.E2Xa.2.1.1.m1.1.1.3.3.3.2.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.3.3.3.2">ℒ</ci><ci id="S3.E2Xa.2.1.1.m1.1.1.3.3.3.3.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.3.3.3.3">dynamic</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2Xa.2.1.1.m1.1c">\displaystyle\rm{\mathcal{L}_{total}=\mathcal{L}_{task}+\lambda\mathcal{L}_{dynamic}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p id="S3.SS3.p5.8" class="ltx_p">where <math id="S3.SS3.p5.1.m1.2" class="ltx_Math" alttext="\mathbf{M}_{n}^{b,l}" display="inline"><semantics id="S3.SS3.p5.1.m1.2a"><msubsup id="S3.SS3.p5.1.m1.2.3" xref="S3.SS3.p5.1.m1.2.3.cmml"><mi id="S3.SS3.p5.1.m1.2.3.2.2" xref="S3.SS3.p5.1.m1.2.3.2.2.cmml">𝐌</mi><mi id="S3.SS3.p5.1.m1.2.3.2.3" xref="S3.SS3.p5.1.m1.2.3.2.3.cmml">n</mi><mrow id="S3.SS3.p5.1.m1.2.2.2.4" xref="S3.SS3.p5.1.m1.2.2.2.3.cmml"><mi id="S3.SS3.p5.1.m1.1.1.1.1" xref="S3.SS3.p5.1.m1.1.1.1.1.cmml">b</mi><mo id="S3.SS3.p5.1.m1.2.2.2.4.1" xref="S3.SS3.p5.1.m1.2.2.2.3.cmml">,</mo><mi id="S3.SS3.p5.1.m1.2.2.2.2" xref="S3.SS3.p5.1.m1.2.2.2.2.cmml">l</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.1.m1.2b"><apply id="S3.SS3.p5.1.m1.2.3.cmml" xref="S3.SS3.p5.1.m1.2.3"><csymbol cd="ambiguous" id="S3.SS3.p5.1.m1.2.3.1.cmml" xref="S3.SS3.p5.1.m1.2.3">superscript</csymbol><apply id="S3.SS3.p5.1.m1.2.3.2.cmml" xref="S3.SS3.p5.1.m1.2.3"><csymbol cd="ambiguous" id="S3.SS3.p5.1.m1.2.3.2.1.cmml" xref="S3.SS3.p5.1.m1.2.3">subscript</csymbol><ci id="S3.SS3.p5.1.m1.2.3.2.2.cmml" xref="S3.SS3.p5.1.m1.2.3.2.2">𝐌</ci><ci id="S3.SS3.p5.1.m1.2.3.2.3.cmml" xref="S3.SS3.p5.1.m1.2.3.2.3">𝑛</ci></apply><list id="S3.SS3.p5.1.m1.2.2.2.3.cmml" xref="S3.SS3.p5.1.m1.2.2.2.4"><ci id="S3.SS3.p5.1.m1.1.1.1.1.cmml" xref="S3.SS3.p5.1.m1.1.1.1.1">𝑏</ci><ci id="S3.SS3.p5.1.m1.2.2.2.2.cmml" xref="S3.SS3.p5.1.m1.2.2.2.2">𝑙</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.1.m1.2c">\mathbf{M}_{n}^{b,l}</annotation></semantics></math> denotes the mask at batch <math id="S3.SS3.p5.2.m2.1" class="ltx_Math" alttext="b" display="inline"><semantics id="S3.SS3.p5.2.m2.1a"><mi id="S3.SS3.p5.2.m2.1.1" xref="S3.SS3.p5.2.m2.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.2.m2.1b"><ci id="S3.SS3.p5.2.m2.1.1.cmml" xref="S3.SS3.p5.2.m2.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.2.m2.1c">b</annotation></semantics></math> and layer <math id="S3.SS3.p5.3.m3.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S3.SS3.p5.3.m3.1a"><mi id="S3.SS3.p5.3.m3.1.1" xref="S3.SS3.p5.3.m3.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.3.m3.1b"><ci id="S3.SS3.p5.3.m3.1.1.cmml" xref="S3.SS3.p5.3.m3.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.3.m3.1c">l</annotation></semantics></math> for the <math id="S3.SS3.p5.4.m4.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS3.p5.4.m4.1a"><mi id="S3.SS3.p5.4.m4.1.1" xref="S3.SS3.p5.4.m4.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.4.m4.1b"><ci id="S3.SS3.p5.4.m4.1.1.cmml" xref="S3.SS3.p5.4.m4.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.4.m4.1c">n</annotation></semantics></math>-th token, <math id="S3.SS3.p5.5.m5.1" class="ltx_Math" alttext="\mathbf{t}^{l}" display="inline"><semantics id="S3.SS3.p5.5.m5.1a"><msup id="S3.SS3.p5.5.m5.1.1" xref="S3.SS3.p5.5.m5.1.1.cmml"><mi id="S3.SS3.p5.5.m5.1.1.2" xref="S3.SS3.p5.5.m5.1.1.2.cmml">𝐭</mi><mi id="S3.SS3.p5.5.m5.1.1.3" xref="S3.SS3.p5.5.m5.1.1.3.cmml">l</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.5.m5.1b"><apply id="S3.SS3.p5.5.m5.1.1.cmml" xref="S3.SS3.p5.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.p5.5.m5.1.1.1.cmml" xref="S3.SS3.p5.5.m5.1.1">superscript</csymbol><ci id="S3.SS3.p5.5.m5.1.1.2.cmml" xref="S3.SS3.p5.5.m5.1.1.2">𝐭</ci><ci id="S3.SS3.p5.5.m5.1.1.3.cmml" xref="S3.SS3.p5.5.m5.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.5.m5.1c">\mathbf{t}^{l}</annotation></semantics></math> represents the target keeping ratio at layer <math id="S3.SS3.p5.6.m6.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S3.SS3.p5.6.m6.1a"><mi id="S3.SS3.p5.6.m6.1.1" xref="S3.SS3.p5.6.m6.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.6.m6.1b"><ci id="S3.SS3.p5.6.m6.1.1.cmml" xref="S3.SS3.p5.6.m6.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.6.m6.1c">l</annotation></semantics></math>, and <math id="S3.SS3.p5.7.m7.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S3.SS3.p5.7.m7.1a"><mi id="S3.SS3.p5.7.m7.1.1" xref="S3.SS3.p5.7.m7.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.7.m7.1b"><ci id="S3.SS3.p5.7.m7.1.1.cmml" xref="S3.SS3.p5.7.m7.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.7.m7.1c">\lambda</annotation></semantics></math> is a hyper-parameter to weight losses. It is worth noting that the token usage, i.e averaged mask values: <math id="S3.SS3.p5.8.m8.2" class="ltx_Math" alttext="\frac{1}{BN}\sum_{b\in B}\sum_{n\in N}{\mathbf{M}}_{n}^{b,l}" display="inline"><semantics id="S3.SS3.p5.8.m8.2a"><mrow id="S3.SS3.p5.8.m8.2.3" xref="S3.SS3.p5.8.m8.2.3.cmml"><mfrac id="S3.SS3.p5.8.m8.2.3.2" xref="S3.SS3.p5.8.m8.2.3.2.cmml"><mn id="S3.SS3.p5.8.m8.2.3.2.2" xref="S3.SS3.p5.8.m8.2.3.2.2.cmml">1</mn><mrow id="S3.SS3.p5.8.m8.2.3.2.3" xref="S3.SS3.p5.8.m8.2.3.2.3.cmml"><mi id="S3.SS3.p5.8.m8.2.3.2.3.2" xref="S3.SS3.p5.8.m8.2.3.2.3.2.cmml">B</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p5.8.m8.2.3.2.3.1" xref="S3.SS3.p5.8.m8.2.3.2.3.1.cmml">​</mo><mi id="S3.SS3.p5.8.m8.2.3.2.3.3" xref="S3.SS3.p5.8.m8.2.3.2.3.3.cmml">N</mi></mrow></mfrac><mo lspace="0em" rspace="0em" id="S3.SS3.p5.8.m8.2.3.1" xref="S3.SS3.p5.8.m8.2.3.1.cmml">​</mo><mrow id="S3.SS3.p5.8.m8.2.3.3" xref="S3.SS3.p5.8.m8.2.3.3.cmml"><msub id="S3.SS3.p5.8.m8.2.3.3.1" xref="S3.SS3.p5.8.m8.2.3.3.1.cmml"><mo rspace="0em" id="S3.SS3.p5.8.m8.2.3.3.1.2" xref="S3.SS3.p5.8.m8.2.3.3.1.2.cmml">∑</mo><mrow id="S3.SS3.p5.8.m8.2.3.3.1.3" xref="S3.SS3.p5.8.m8.2.3.3.1.3.cmml"><mi id="S3.SS3.p5.8.m8.2.3.3.1.3.2" xref="S3.SS3.p5.8.m8.2.3.3.1.3.2.cmml">b</mi><mo id="S3.SS3.p5.8.m8.2.3.3.1.3.1" xref="S3.SS3.p5.8.m8.2.3.3.1.3.1.cmml">∈</mo><mi id="S3.SS3.p5.8.m8.2.3.3.1.3.3" xref="S3.SS3.p5.8.m8.2.3.3.1.3.3.cmml">B</mi></mrow></msub><mrow id="S3.SS3.p5.8.m8.2.3.3.2" xref="S3.SS3.p5.8.m8.2.3.3.2.cmml"><msub id="S3.SS3.p5.8.m8.2.3.3.2.1" xref="S3.SS3.p5.8.m8.2.3.3.2.1.cmml"><mo id="S3.SS3.p5.8.m8.2.3.3.2.1.2" xref="S3.SS3.p5.8.m8.2.3.3.2.1.2.cmml">∑</mo><mrow id="S3.SS3.p5.8.m8.2.3.3.2.1.3" xref="S3.SS3.p5.8.m8.2.3.3.2.1.3.cmml"><mi id="S3.SS3.p5.8.m8.2.3.3.2.1.3.2" xref="S3.SS3.p5.8.m8.2.3.3.2.1.3.2.cmml">n</mi><mo id="S3.SS3.p5.8.m8.2.3.3.2.1.3.1" xref="S3.SS3.p5.8.m8.2.3.3.2.1.3.1.cmml">∈</mo><mi id="S3.SS3.p5.8.m8.2.3.3.2.1.3.3" xref="S3.SS3.p5.8.m8.2.3.3.2.1.3.3.cmml">N</mi></mrow></msub><msubsup id="S3.SS3.p5.8.m8.2.3.3.2.2" xref="S3.SS3.p5.8.m8.2.3.3.2.2.cmml"><mi id="S3.SS3.p5.8.m8.2.3.3.2.2.2.2" xref="S3.SS3.p5.8.m8.2.3.3.2.2.2.2.cmml">𝐌</mi><mi id="S3.SS3.p5.8.m8.2.3.3.2.2.2.3" xref="S3.SS3.p5.8.m8.2.3.3.2.2.2.3.cmml">n</mi><mrow id="S3.SS3.p5.8.m8.2.2.2.4" xref="S3.SS3.p5.8.m8.2.2.2.3.cmml"><mi id="S3.SS3.p5.8.m8.1.1.1.1" xref="S3.SS3.p5.8.m8.1.1.1.1.cmml">b</mi><mo id="S3.SS3.p5.8.m8.2.2.2.4.1" xref="S3.SS3.p5.8.m8.2.2.2.3.cmml">,</mo><mi id="S3.SS3.p5.8.m8.2.2.2.2" xref="S3.SS3.p5.8.m8.2.2.2.2.cmml">l</mi></mrow></msubsup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.8.m8.2b"><apply id="S3.SS3.p5.8.m8.2.3.cmml" xref="S3.SS3.p5.8.m8.2.3"><times id="S3.SS3.p5.8.m8.2.3.1.cmml" xref="S3.SS3.p5.8.m8.2.3.1"></times><apply id="S3.SS3.p5.8.m8.2.3.2.cmml" xref="S3.SS3.p5.8.m8.2.3.2"><divide id="S3.SS3.p5.8.m8.2.3.2.1.cmml" xref="S3.SS3.p5.8.m8.2.3.2"></divide><cn type="integer" id="S3.SS3.p5.8.m8.2.3.2.2.cmml" xref="S3.SS3.p5.8.m8.2.3.2.2">1</cn><apply id="S3.SS3.p5.8.m8.2.3.2.3.cmml" xref="S3.SS3.p5.8.m8.2.3.2.3"><times id="S3.SS3.p5.8.m8.2.3.2.3.1.cmml" xref="S3.SS3.p5.8.m8.2.3.2.3.1"></times><ci id="S3.SS3.p5.8.m8.2.3.2.3.2.cmml" xref="S3.SS3.p5.8.m8.2.3.2.3.2">𝐵</ci><ci id="S3.SS3.p5.8.m8.2.3.2.3.3.cmml" xref="S3.SS3.p5.8.m8.2.3.2.3.3">𝑁</ci></apply></apply><apply id="S3.SS3.p5.8.m8.2.3.3.cmml" xref="S3.SS3.p5.8.m8.2.3.3"><apply id="S3.SS3.p5.8.m8.2.3.3.1.cmml" xref="S3.SS3.p5.8.m8.2.3.3.1"><csymbol cd="ambiguous" id="S3.SS3.p5.8.m8.2.3.3.1.1.cmml" xref="S3.SS3.p5.8.m8.2.3.3.1">subscript</csymbol><sum id="S3.SS3.p5.8.m8.2.3.3.1.2.cmml" xref="S3.SS3.p5.8.m8.2.3.3.1.2"></sum><apply id="S3.SS3.p5.8.m8.2.3.3.1.3.cmml" xref="S3.SS3.p5.8.m8.2.3.3.1.3"><in id="S3.SS3.p5.8.m8.2.3.3.1.3.1.cmml" xref="S3.SS3.p5.8.m8.2.3.3.1.3.1"></in><ci id="S3.SS3.p5.8.m8.2.3.3.1.3.2.cmml" xref="S3.SS3.p5.8.m8.2.3.3.1.3.2">𝑏</ci><ci id="S3.SS3.p5.8.m8.2.3.3.1.3.3.cmml" xref="S3.SS3.p5.8.m8.2.3.3.1.3.3">𝐵</ci></apply></apply><apply id="S3.SS3.p5.8.m8.2.3.3.2.cmml" xref="S3.SS3.p5.8.m8.2.3.3.2"><apply id="S3.SS3.p5.8.m8.2.3.3.2.1.cmml" xref="S3.SS3.p5.8.m8.2.3.3.2.1"><csymbol cd="ambiguous" id="S3.SS3.p5.8.m8.2.3.3.2.1.1.cmml" xref="S3.SS3.p5.8.m8.2.3.3.2.1">subscript</csymbol><sum id="S3.SS3.p5.8.m8.2.3.3.2.1.2.cmml" xref="S3.SS3.p5.8.m8.2.3.3.2.1.2"></sum><apply id="S3.SS3.p5.8.m8.2.3.3.2.1.3.cmml" xref="S3.SS3.p5.8.m8.2.3.3.2.1.3"><in id="S3.SS3.p5.8.m8.2.3.3.2.1.3.1.cmml" xref="S3.SS3.p5.8.m8.2.3.3.2.1.3.1"></in><ci id="S3.SS3.p5.8.m8.2.3.3.2.1.3.2.cmml" xref="S3.SS3.p5.8.m8.2.3.3.2.1.3.2">𝑛</ci><ci id="S3.SS3.p5.8.m8.2.3.3.2.1.3.3.cmml" xref="S3.SS3.p5.8.m8.2.3.3.2.1.3.3">𝑁</ci></apply></apply><apply id="S3.SS3.p5.8.m8.2.3.3.2.2.cmml" xref="S3.SS3.p5.8.m8.2.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS3.p5.8.m8.2.3.3.2.2.1.cmml" xref="S3.SS3.p5.8.m8.2.3.3.2.2">superscript</csymbol><apply id="S3.SS3.p5.8.m8.2.3.3.2.2.2.cmml" xref="S3.SS3.p5.8.m8.2.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS3.p5.8.m8.2.3.3.2.2.2.1.cmml" xref="S3.SS3.p5.8.m8.2.3.3.2.2">subscript</csymbol><ci id="S3.SS3.p5.8.m8.2.3.3.2.2.2.2.cmml" xref="S3.SS3.p5.8.m8.2.3.3.2.2.2.2">𝐌</ci><ci id="S3.SS3.p5.8.m8.2.3.3.2.2.2.3.cmml" xref="S3.SS3.p5.8.m8.2.3.3.2.2.2.3">𝑛</ci></apply><list id="S3.SS3.p5.8.m8.2.2.2.3.cmml" xref="S3.SS3.p5.8.m8.2.2.2.4"><ci id="S3.SS3.p5.8.m8.1.1.1.1.cmml" xref="S3.SS3.p5.8.m8.1.1.1.1">𝑏</ci><ci id="S3.SS3.p5.8.m8.2.2.2.2.cmml" xref="S3.SS3.p5.8.m8.2.2.2.2">𝑙</ci></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.8.m8.2c">\frac{1}{BN}\sum_{b\in B}\sum_{n\in N}{\mathbf{M}}_{n}^{b,l}</annotation></semantics></math>, is averaged not only across all tokens but also across images in a batch, making the loss aware of the trade-off between token usage and accuracy, resulting in more tokens allocated for complex images and fewer tokens for simpler images. For a comparison with a fixed pruning ratio loss, see Section <a href="#S4.SS1" title="4.1 Evaluation of the insights and observations ‣ 4 Experiments ‣ Revisiting Token Pruning for Object Detection and Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We conduct experiments on the COCO 2017 object detection and instance segmentation dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, which consists of 118K training images and 5K validation images, and provide experiments on ImageNet-1K <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> classification in the supplementary material A.
We use Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> as our object detection framework, and employ ViT-Adapter <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> to wrap a ViT as the backbone.
The dense backbone utilizes DeiT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> with global self-attention, while the sparse backbone adopts one of the token pruning models (DynamicViT, EViT, EvoViT, ATS, SViT) with a reduced number of tokens.
By default, SViT incorporates nine gating modules, ranging from the 4-th to the 12-th layer to prune tokens from the dense model, and adheres to the target keeping ratio of [70%, 70%, 70%, 49%, 49%, 49%, 34.3%, 34.3%, 34.3%] following conventions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.
For training, we follow the settings of ViT-Adapter <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> to train the dense model with a 3x schedule (36 epochs). Then we finetune each sparse model for 6 and 4 epochs for tiny and small models, respectively, with an initial learning rate of 1e-5 and the loss hyper-parameter <math id="S4.p1.1.m1.1" class="ltx_Math" alttext="\lambda=4" display="inline"><semantics id="S4.p1.1.m1.1a"><mrow id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml"><mi id="S4.p1.1.m1.1.1.2" xref="S4.p1.1.m1.1.1.2.cmml">λ</mi><mo id="S4.p1.1.m1.1.1.1" xref="S4.p1.1.m1.1.1.1.cmml">=</mo><mn id="S4.p1.1.m1.1.1.3" xref="S4.p1.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><apply id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1"><eq id="S4.p1.1.m1.1.1.1.cmml" xref="S4.p1.1.m1.1.1.1"></eq><ci id="S4.p1.1.m1.1.1.2.cmml" xref="S4.p1.1.m1.1.1.2">𝜆</ci><cn type="integer" id="S4.p1.1.m1.1.1.3.cmml" xref="S4.p1.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">\lambda=4</annotation></semantics></math> .
In the following, we first present experiments for each insight, and then compare the derived SViT with other state-of-the-art token pruning models on object detection and instance segmentation. Finally, we analyse the pattern of pruning and reactivation by providing qualitative and quantitative results.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Evaluation of the insights and observations</h3>

<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Preserve pruned tokens within feature maps</h4>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p1.1" class="ltx_p">We evaluate the difference between removing and preserving pruned tokens on four state-of-the-art models: EViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, Evo-ViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>, DynamicViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> and ATS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. Some of these models prune tokens via the attention score from the class token, which does not naturally exist on dense tasks, and we insert an artificial class token and find it still works well for these models. As shown in Table <a href="#S4.T2" title="Table 2 ‣ Preserve pruned tokens within feature maps ‣ 4.1 Evaluation of the insights and observations ‣ 4 Experiments ‣ Revisiting Token Pruning for Object Detection and Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, Evo-ViT, which inherently preserves pruned tokens, performs the best among the original models. In addition, by enabling preserving tokens, EViT and ATS both get small increase in performance, and DynamicViT has a boost increase, as gating networks learned end-to-end are sensitive to gradient information kept in pruned tokens, and the gradients cannot be back-propagated to the backbone if pruned tokens are dropped. Owing to this factor, DynamicViT-S experiences training divergence, as indicated in <a href="#S4.T5" title="Table 5 ‣ 4.2 Comparison with state-of-the-art models ‣ 4 Experiments ‣ Revisiting Token Pruning for Object Detection and Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.2.1.1" class="ltx_tr">
<th id="S4.T2.2.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<th id="S4.T2.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2">DynamicViT</th>
<th id="S4.T2.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2">EViT</th>
<th id="S4.T2.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2">ATS</th>
<th id="S4.T2.2.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" rowspan="2"><span id="S4.T2.2.1.1.5.1" class="ltx_text">Evo-ViT</span></th>
</tr>
<tr id="S4.T2.2.2.2" class="ltx_tr">
<th id="S4.T2.2.2.2.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S4.T2.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">remove</th>
<th id="S4.T2.2.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">prsv.</th>
<th id="S4.T2.2.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">remove</th>
<th id="S4.T2.2.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">prsv.</th>
<th id="S4.T2.2.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">remove</th>
<th id="S4.T2.2.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">prsv.</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.2.3.1" class="ltx_tr">
<th id="S4.T2.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">AP<sup id="S4.T2.2.3.1.1.1" class="ltx_sup">box</sup>
</th>
<td id="S4.T2.2.3.1.2" class="ltx_td ltx_align_center ltx_border_t">41.2</td>
<td id="S4.T2.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t">44.1</td>
<td id="S4.T2.2.3.1.4" class="ltx_td ltx_align_center ltx_border_t">44.5</td>
<td id="S4.T2.2.3.1.5" class="ltx_td ltx_align_center ltx_border_t">44.7</td>
<td id="S4.T2.2.3.1.6" class="ltx_td ltx_align_center ltx_border_t">43.9</td>
<td id="S4.T2.2.3.1.7" class="ltx_td ltx_align_center ltx_border_t">44.1</td>
<td id="S4.T2.2.3.1.8" class="ltx_td ltx_align_center ltx_border_t">44.8</td>
</tr>
<tr id="S4.T2.2.4.2" class="ltx_tr">
<th id="S4.T2.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">AP<sup id="S4.T2.2.4.2.1.1" class="ltx_sup">mask</sup>
</th>
<td id="S4.T2.2.4.2.2" class="ltx_td ltx_align_center">37.1</td>
<td id="S4.T2.2.4.2.3" class="ltx_td ltx_align_center">39.3</td>
<td id="S4.T2.2.4.2.4" class="ltx_td ltx_align_center">39.8</td>
<td id="S4.T2.2.4.2.5" class="ltx_td ltx_align_center">39.9</td>
<td id="S4.T2.2.4.2.6" class="ltx_td ltx_align_center">39.1</td>
<td id="S4.T2.2.4.2.7" class="ltx_td ltx_align_center">39.3</td>
<td id="S4.T2.2.4.2.8" class="ltx_td ltx_align_center">39.9</td>
</tr>
<tr id="S4.T2.2.5.3" class="ltx_tr">
<th id="S4.T2.2.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">FPS</th>
<td id="S4.T2.2.5.3.2" class="ltx_td ltx_align_center ltx_border_b">23.10</td>
<td id="S4.T2.2.5.3.3" class="ltx_td ltx_align_center ltx_border_b">22.95</td>
<td id="S4.T2.2.5.3.4" class="ltx_td ltx_align_center ltx_border_b">22.76</td>
<td id="S4.T2.2.5.3.5" class="ltx_td ltx_align_center ltx_border_b">22.81</td>
<td id="S4.T2.2.5.3.6" class="ltx_td ltx_align_center ltx_border_b">16.41</td>
<td id="S4.T2.2.5.3.7" class="ltx_td ltx_align_center ltx_border_b">16.52</td>
<td id="S4.T2.2.5.3.8" class="ltx_td ltx_align_center ltx_border_b">22.12</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.3.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.4.2" class="ltx_text" style="font-size:90%;">Effectiveness of removing tokens vs. preserving tokens on COCO 2017. Evo-ViT inherently preserves tokens, and performs the best among these models; Similarly, preserving tokens in feature maps increases the performance of the other three models. As anticipated, the token removal or preservation process has a negligible impact on inference speeds; scattering updated tokens onto either a zero feature map or the previous feature map consumes equivalent computational time.</span></figcaption>
</figure>
</section>
<section id="S4.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Reuse preserved tokens at demand</h4>

<div id="S4.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p1.3" class="ltx_p">We evaluate the influence of reusing / reactivating pruned tokens on SViT instead of the previous models, as models utilizing attention-based selection cannot really reuse tokens. Attention-based selection happens after Multi-Head Self-Attention (MHSA), and reusing tokens in such case requires all tokens to participate in MHSA, leading to no computational savings.
To construct our baseline that is restricted not to reuse pruned tokens, we multiply the mask at <math id="S4.SS1.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S4.SS1.SSS0.Px2.p1.1.m1.1a"><mi id="S4.SS1.SSS0.Px2.p1.1.m1.1.1" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px2.p1.1.m1.1b"><ci id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px2.p1.1.m1.1c">l</annotation></semantics></math>-th layer by its previous mask at <math id="S4.SS1.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="l-1" display="inline"><semantics id="S4.SS1.SSS0.Px2.p1.2.m2.1a"><mrow id="S4.SS1.SSS0.Px2.p1.2.m2.1.1" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.cmml"><mi id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.2" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.2.cmml">l</mi><mo id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.1" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.1.cmml">−</mo><mn id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.3" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px2.p1.2.m2.1b"><apply id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1"><minus id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.1"></minus><ci id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.2">𝑙</ci><cn type="integer" id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.3.cmml" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px2.p1.2.m2.1c">l-1</annotation></semantics></math>-th layer following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>: <math id="S4.SS1.SSS0.Px2.p1.3.m3.1" class="ltx_Math" alttext="\mathbf{M}^{l}\leftarrow\mathbf{M}^{l}\odot\mathbf{M}^{l-1}" display="inline"><semantics id="S4.SS1.SSS0.Px2.p1.3.m3.1a"><mrow id="S4.SS1.SSS0.Px2.p1.3.m3.1.1" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.cmml"><msup id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.2" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.2.cmml"><mi id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.2.2" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.2.2.cmml">𝐌</mi><mi id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.2.3" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.2.3.cmml">l</mi></msup><mo stretchy="false" id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.1" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.1.cmml">←</mo><mrow id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.cmml"><msup id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.2" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.2.cmml"><mi id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.2.2" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.2.2.cmml">𝐌</mi><mi id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.2.3" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.2.3.cmml">l</mi></msup><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.1" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.1.cmml">⊙</mo><msup id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.3" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.3.cmml"><mi id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.3.2" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.3.2.cmml">𝐌</mi><mrow id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.3.3" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.3.3.cmml"><mi id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.3.3.2" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.3.3.2.cmml">l</mi><mo id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.3.3.1" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.3.3.1.cmml">−</mo><mn id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.3.3.3" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.3.3.3.cmml">1</mn></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px2.p1.3.m3.1b"><apply id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1"><ci id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.1">←</ci><apply id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.2.cmml" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.2.1.cmml" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.2">superscript</csymbol><ci id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.2.2.cmml" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.2.2">𝐌</ci><ci id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.2.3.cmml" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.2.3">𝑙</ci></apply><apply id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.cmml" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3"><csymbol cd="latexml" id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.1.cmml" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.1">direct-product</csymbol><apply id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.2.cmml" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.2"><csymbol cd="ambiguous" id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.2.1.cmml" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.2">superscript</csymbol><ci id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.2.2.cmml" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.2.2">𝐌</ci><ci id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.2.3.cmml" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.2.3">𝑙</ci></apply><apply id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.3.cmml" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.3"><csymbol cd="ambiguous" id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.3.1.cmml" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.3">superscript</csymbol><ci id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.3.2.cmml" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.3.2">𝐌</ci><apply id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.3.3.cmml" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.3.3"><minus id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.3.3.1.cmml" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.3.3.1"></minus><ci id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.3.3.2.cmml" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.3.3.2">𝑙</ci><cn type="integer" id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.3.3.3.cmml" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.3.3.3">1</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px2.p1.3.m3.1c">\mathbf{M}^{l}\leftarrow\mathbf{M}^{l}\odot\mathbf{M}^{l-1}</annotation></semantics></math>.
This implies that active tokens will consistently be a subset of previous active tokens, and pruned tokens cannot be used again. As the set of active tokens is strictly decreasing, we merge selection modules with the same keeping ratios into one selection module.
Table <a href="#S4.T3" title="Table 3 ‣ Reuse preserved tokens at demand ‣ 4.1 Evaluation of the insights and observations ‣ 4 Experiments ‣ Revisiting Token Pruning for Object Detection and Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows that by reusing pruned tokens, SViT-T gets +0.4 box AP and +0.3 mask AP. Reactivation ratio and visualizations samples are in Figure <a href="#S4.F5" title="Figure 5 ‣ Reuse preserved tokens at demand ‣ 4.1 Evaluation of the insights and observations ‣ 4 Experiments ‣ Revisiting Token Pruning for Object Detection and Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.2.1.1" class="ltx_tr">
<th id="S4.T3.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">Model</th>
<th id="S4.T3.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">dynamic</th>
<th id="S4.T3.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">reactivation</th>
<th id="S4.T3.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AP<sup id="S4.T3.2.1.1.4.1" class="ltx_sup">box</sup>
</th>
<th id="S4.T3.2.1.1.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t">AP<sup id="S4.T3.2.1.1.5.1" class="ltx_sup">mask</sup>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.2.2.1" class="ltx_tr">
<th id="S4.T3.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">SViT-T</th>
<td id="S4.T3.2.2.1.2" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S4.T3.2.2.1.3" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S4.T3.2.2.1.4" class="ltx_td ltx_align_center ltx_border_t">45.5</td>
<td id="S4.T3.2.2.1.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">40.7</td>
</tr>
<tr id="S4.T3.2.3.2" class="ltx_tr">
<th id="S4.T3.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">SViT-T</th>
<td id="S4.T3.2.3.2.2" class="ltx_td ltx_align_center">✗</td>
<td id="S4.T3.2.3.2.3" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T3.2.3.2.4" class="ltx_td ltx_align_center">45.1</td>
<td id="S4.T3.2.3.2.5" class="ltx_td ltx_nopad_r ltx_align_center">40.2</td>
</tr>
<tr id="S4.T3.2.4.3" class="ltx_tr">
<th id="S4.T3.2.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">SViT-T</th>
<td id="S4.T3.2.4.3.2" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T3.2.4.3.3" class="ltx_td ltx_align_center">✗</td>
<td id="S4.T3.2.4.3.4" class="ltx_td ltx_align_center">45.1</td>
<td id="S4.T3.2.4.3.5" class="ltx_td ltx_nopad_r ltx_align_center">40.4</td>
</tr>
<tr id="S4.T3.2.5.4" class="ltx_tr">
<th id="S4.T3.2.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">SViT-T</th>
<td id="S4.T3.2.5.4.2" class="ltx_td ltx_align_center ltx_border_b">✗</td>
<td id="S4.T3.2.5.4.3" class="ltx_td ltx_align_center ltx_border_b">✗</td>
<td id="S4.T3.2.5.4.4" class="ltx_td ltx_align_center ltx_border_b">44.9</td>
<td id="S4.T3.2.5.4.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_b">40.2</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.3.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S4.T3.4.2" class="ltx_text" style="font-size:90%;">The effects of dynamic pruning rate and reactivating pruned tokens. Both can enhance performance individually, and their combination results in a larger improvement. </span></figcaption>
</figure>
<figure id="S4.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F5.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2306.07050/assets/imgs/reuse_ratio.png" id="S4.F5.sf1.g1" class="ltx_graphics ltx_img_landscape" width="685" height="524" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F5.sf1.3.2" class="ltx_text" style="font-size:90%;">reactivation ratio per layer</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F5.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2306.07050/assets/imgs/reuse.png" id="S4.F5.sf2.g1" class="ltx_graphics ltx_img_landscape" width="685" height="367" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F5.sf2.3.2" class="ltx_text" style="font-size:90%;">reactivation example</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.3.2" class="ltx_text" style="font-size:90%;">(a) Reactivation ratio at different layers, averaged on COCO validation set. (b) Visualization of reactivated tokens. Cyan tokens will be reactivated in later layers, while white tokens are not. Reactivated tokens are visually more important tokens.</span></figcaption>
</figure>
</section>
<section id="S4.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Dynamic pruning rate outperforms fixed pruning rate</h4>

<div id="S4.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px3.p1.2" class="ltx_p">To evaluate the influence of dynamic pruning rate vs. fixed pruning rate, we create a baseline by changing the dynamic ratio loss <math id="S4.SS1.SSS0.Px3.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{L}_{dynamic}" display="inline"><semantics id="S4.SS1.SSS0.Px3.p1.1.m1.1a"><msub id="S4.SS1.SSS0.Px3.p1.1.m1.1.1" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.2" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.2.cmml">ℒ</mi><mrow id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.cmml"><mi id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.2" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.1" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.3" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.3.cmml">y</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.1a" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.4" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.1b" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.5" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.1c" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.6" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.6.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.1d" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.7" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.7.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.1e" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.8" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.8.cmml">c</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px3.p1.1.m1.1b"><apply id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.2">ℒ</ci><apply id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.cmml" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3"><times id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.1.cmml" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.1"></times><ci id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.2.cmml" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.2">𝑑</ci><ci id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.3.cmml" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.3">𝑦</ci><ci id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.4.cmml" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.4">𝑛</ci><ci id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.5.cmml" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.5">𝑎</ci><ci id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.6.cmml" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.6">𝑚</ci><ci id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.7.cmml" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.7">𝑖</ci><ci id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.8.cmml" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.3.8">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px3.p1.1.m1.1c">\mathcal{L}_{dynamic}</annotation></semantics></math> from equation (<a href="#S3.E2" title="Equation 2 ‣ 3.3 SViT: Selective Vision Transformer ‣ 3 Token Pruning on dense prediction tasks ‣ Revisiting Token Pruning for Object Detection and Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) to the fixed ratio loss <math id="S4.SS1.SSS0.Px3.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{L}_{fixed}" display="inline"><semantics id="S4.SS1.SSS0.Px3.p1.2.m2.1a"><msub id="S4.SS1.SSS0.Px3.p1.2.m2.1.1" xref="S4.SS1.SSS0.Px3.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.SSS0.Px3.p1.2.m2.1.1.2" xref="S4.SS1.SSS0.Px3.p1.2.m2.1.1.2.cmml">ℒ</mi><mrow id="S4.SS1.SSS0.Px3.p1.2.m2.1.1.3" xref="S4.SS1.SSS0.Px3.p1.2.m2.1.1.3.cmml"><mi id="S4.SS1.SSS0.Px3.p1.2.m2.1.1.3.2" xref="S4.SS1.SSS0.Px3.p1.2.m2.1.1.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS0.Px3.p1.2.m2.1.1.3.1" xref="S4.SS1.SSS0.Px3.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.SS1.SSS0.Px3.p1.2.m2.1.1.3.3" xref="S4.SS1.SSS0.Px3.p1.2.m2.1.1.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS0.Px3.p1.2.m2.1.1.3.1a" xref="S4.SS1.SSS0.Px3.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.SS1.SSS0.Px3.p1.2.m2.1.1.3.4" xref="S4.SS1.SSS0.Px3.p1.2.m2.1.1.3.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS0.Px3.p1.2.m2.1.1.3.1b" xref="S4.SS1.SSS0.Px3.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.SS1.SSS0.Px3.p1.2.m2.1.1.3.5" xref="S4.SS1.SSS0.Px3.p1.2.m2.1.1.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS0.Px3.p1.2.m2.1.1.3.1c" xref="S4.SS1.SSS0.Px3.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.SS1.SSS0.Px3.p1.2.m2.1.1.3.6" xref="S4.SS1.SSS0.Px3.p1.2.m2.1.1.3.6.cmml">d</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px3.p1.2.m2.1b"><apply id="S4.SS1.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS0.Px3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS0.Px3.p1.2.m2.1.1.1.cmml" xref="S4.SS1.SSS0.Px3.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.SSS0.Px3.p1.2.m2.1.1.2.cmml" xref="S4.SS1.SSS0.Px3.p1.2.m2.1.1.2">ℒ</ci><apply id="S4.SS1.SSS0.Px3.p1.2.m2.1.1.3.cmml" xref="S4.SS1.SSS0.Px3.p1.2.m2.1.1.3"><times id="S4.SS1.SSS0.Px3.p1.2.m2.1.1.3.1.cmml" xref="S4.SS1.SSS0.Px3.p1.2.m2.1.1.3.1"></times><ci id="S4.SS1.SSS0.Px3.p1.2.m2.1.1.3.2.cmml" xref="S4.SS1.SSS0.Px3.p1.2.m2.1.1.3.2">𝑓</ci><ci id="S4.SS1.SSS0.Px3.p1.2.m2.1.1.3.3.cmml" xref="S4.SS1.SSS0.Px3.p1.2.m2.1.1.3.3">𝑖</ci><ci id="S4.SS1.SSS0.Px3.p1.2.m2.1.1.3.4.cmml" xref="S4.SS1.SSS0.Px3.p1.2.m2.1.1.3.4">𝑥</ci><ci id="S4.SS1.SSS0.Px3.p1.2.m2.1.1.3.5.cmml" xref="S4.SS1.SSS0.Px3.p1.2.m2.1.1.3.5">𝑒</ci><ci id="S4.SS1.SSS0.Px3.p1.2.m2.1.1.3.6.cmml" xref="S4.SS1.SSS0.Px3.p1.2.m2.1.1.3.6">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px3.p1.2.m2.1c">\mathcal{L}_{fixed}</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> as follows:</p>
<table id="S4.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E3.m1.3" class="ltx_Math" alttext="\mathcal{L}_{fixed}=\frac{1}{LB}\sum_{l\in L}\sum_{b\in B}\left(\left(\frac{1}{N}\sum_{n\in N}{\mathbf{M}}_{b,n}^{l}\right)-\mathbf{t}^{l}\right)^{2}," display="block"><semantics id="S4.E3.m1.3a"><mrow id="S4.E3.m1.3.3.1" xref="S4.E3.m1.3.3.1.1.cmml"><mrow id="S4.E3.m1.3.3.1.1" xref="S4.E3.m1.3.3.1.1.cmml"><msub id="S4.E3.m1.3.3.1.1.3" xref="S4.E3.m1.3.3.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E3.m1.3.3.1.1.3.2" xref="S4.E3.m1.3.3.1.1.3.2.cmml">ℒ</mi><mrow id="S4.E3.m1.3.3.1.1.3.3" xref="S4.E3.m1.3.3.1.1.3.3.cmml"><mi id="S4.E3.m1.3.3.1.1.3.3.2" xref="S4.E3.m1.3.3.1.1.3.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.3.3.1.1.3.3.1" xref="S4.E3.m1.3.3.1.1.3.3.1.cmml">​</mo><mi id="S4.E3.m1.3.3.1.1.3.3.3" xref="S4.E3.m1.3.3.1.1.3.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.3.3.1.1.3.3.1a" xref="S4.E3.m1.3.3.1.1.3.3.1.cmml">​</mo><mi id="S4.E3.m1.3.3.1.1.3.3.4" xref="S4.E3.m1.3.3.1.1.3.3.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.3.3.1.1.3.3.1b" xref="S4.E3.m1.3.3.1.1.3.3.1.cmml">​</mo><mi id="S4.E3.m1.3.3.1.1.3.3.5" xref="S4.E3.m1.3.3.1.1.3.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.3.3.1.1.3.3.1c" xref="S4.E3.m1.3.3.1.1.3.3.1.cmml">​</mo><mi id="S4.E3.m1.3.3.1.1.3.3.6" xref="S4.E3.m1.3.3.1.1.3.3.6.cmml">d</mi></mrow></msub><mo id="S4.E3.m1.3.3.1.1.2" xref="S4.E3.m1.3.3.1.1.2.cmml">=</mo><mrow id="S4.E3.m1.3.3.1.1.1" xref="S4.E3.m1.3.3.1.1.1.cmml"><mfrac id="S4.E3.m1.3.3.1.1.1.3" xref="S4.E3.m1.3.3.1.1.1.3.cmml"><mn id="S4.E3.m1.3.3.1.1.1.3.2" xref="S4.E3.m1.3.3.1.1.1.3.2.cmml">1</mn><mrow id="S4.E3.m1.3.3.1.1.1.3.3" xref="S4.E3.m1.3.3.1.1.1.3.3.cmml"><mi id="S4.E3.m1.3.3.1.1.1.3.3.2" xref="S4.E3.m1.3.3.1.1.1.3.3.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.3.3.1.1.1.3.3.1" xref="S4.E3.m1.3.3.1.1.1.3.3.1.cmml">​</mo><mi id="S4.E3.m1.3.3.1.1.1.3.3.3" xref="S4.E3.m1.3.3.1.1.1.3.3.3.cmml">B</mi></mrow></mfrac><mo lspace="0em" rspace="0em" id="S4.E3.m1.3.3.1.1.1.2" xref="S4.E3.m1.3.3.1.1.1.2.cmml">​</mo><mrow id="S4.E3.m1.3.3.1.1.1.1" xref="S4.E3.m1.3.3.1.1.1.1.cmml"><munder id="S4.E3.m1.3.3.1.1.1.1.2" xref="S4.E3.m1.3.3.1.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S4.E3.m1.3.3.1.1.1.1.2.2" xref="S4.E3.m1.3.3.1.1.1.1.2.2.cmml">∑</mo><mrow id="S4.E3.m1.3.3.1.1.1.1.2.3" xref="S4.E3.m1.3.3.1.1.1.1.2.3.cmml"><mi id="S4.E3.m1.3.3.1.1.1.1.2.3.2" xref="S4.E3.m1.3.3.1.1.1.1.2.3.2.cmml">l</mi><mo id="S4.E3.m1.3.3.1.1.1.1.2.3.1" xref="S4.E3.m1.3.3.1.1.1.1.2.3.1.cmml">∈</mo><mi id="S4.E3.m1.3.3.1.1.1.1.2.3.3" xref="S4.E3.m1.3.3.1.1.1.1.2.3.3.cmml">L</mi></mrow></munder><mrow id="S4.E3.m1.3.3.1.1.1.1.1" xref="S4.E3.m1.3.3.1.1.1.1.1.cmml"><munder id="S4.E3.m1.3.3.1.1.1.1.1.2" xref="S4.E3.m1.3.3.1.1.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S4.E3.m1.3.3.1.1.1.1.1.2.2" xref="S4.E3.m1.3.3.1.1.1.1.1.2.2.cmml">∑</mo><mrow id="S4.E3.m1.3.3.1.1.1.1.1.2.3" xref="S4.E3.m1.3.3.1.1.1.1.1.2.3.cmml"><mi id="S4.E3.m1.3.3.1.1.1.1.1.2.3.2" xref="S4.E3.m1.3.3.1.1.1.1.1.2.3.2.cmml">b</mi><mo id="S4.E3.m1.3.3.1.1.1.1.1.2.3.1" xref="S4.E3.m1.3.3.1.1.1.1.1.2.3.1.cmml">∈</mo><mi id="S4.E3.m1.3.3.1.1.1.1.1.2.3.3" xref="S4.E3.m1.3.3.1.1.1.1.1.2.3.3.cmml">B</mi></mrow></munder><msup id="S4.E3.m1.3.3.1.1.1.1.1.1" xref="S4.E3.m1.3.3.1.1.1.1.1.1.cmml"><mrow id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><mo id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.2" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><mrow id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mfrac id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mn id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">1</mn><mi id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">​</mo><mrow id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><munder id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.1" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml"><mo movablelimits="false" id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.1.2" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.1.2.cmml">∑</mo><mrow id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.1.3" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.1.3.cmml"><mi id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.1.3.2" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.1.3.2.cmml">n</mi><mo id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.1.3.1" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.1.3.1.cmml">∈</mo><mi id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.1.3.3" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.1.3.3.cmml">N</mi></mrow></munder><msubsup id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml"><mi id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.2.2.2" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.2.2.2.cmml">𝐌</mi><mrow id="S4.E3.m1.2.2.2.4" xref="S4.E3.m1.2.2.2.3.cmml"><mi id="S4.E3.m1.1.1.1.1" xref="S4.E3.m1.1.1.1.1.cmml">b</mi><mo id="S4.E3.m1.2.2.2.4.1" xref="S4.E3.m1.2.2.2.3.cmml">,</mo><mi id="S4.E3.m1.2.2.2.2" xref="S4.E3.m1.2.2.2.2.cmml">n</mi></mrow><mi id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3.cmml">l</mi></msubsup></mrow></mrow><mo id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.2" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml">−</mo><msup id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.2" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.2.cmml">𝐭</mi><mi id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.3" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.cmml">l</mi></msup></mrow><mo id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.3" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mn id="S4.E3.m1.3.3.1.1.1.1.1.1.3" xref="S4.E3.m1.3.3.1.1.1.1.1.1.3.cmml">2</mn></msup></mrow></mrow></mrow></mrow><mo id="S4.E3.m1.3.3.1.2" xref="S4.E3.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m1.3b"><apply id="S4.E3.m1.3.3.1.1.cmml" xref="S4.E3.m1.3.3.1"><eq id="S4.E3.m1.3.3.1.1.2.cmml" xref="S4.E3.m1.3.3.1.1.2"></eq><apply id="S4.E3.m1.3.3.1.1.3.cmml" xref="S4.E3.m1.3.3.1.1.3"><csymbol cd="ambiguous" id="S4.E3.m1.3.3.1.1.3.1.cmml" xref="S4.E3.m1.3.3.1.1.3">subscript</csymbol><ci id="S4.E3.m1.3.3.1.1.3.2.cmml" xref="S4.E3.m1.3.3.1.1.3.2">ℒ</ci><apply id="S4.E3.m1.3.3.1.1.3.3.cmml" xref="S4.E3.m1.3.3.1.1.3.3"><times id="S4.E3.m1.3.3.1.1.3.3.1.cmml" xref="S4.E3.m1.3.3.1.1.3.3.1"></times><ci id="S4.E3.m1.3.3.1.1.3.3.2.cmml" xref="S4.E3.m1.3.3.1.1.3.3.2">𝑓</ci><ci id="S4.E3.m1.3.3.1.1.3.3.3.cmml" xref="S4.E3.m1.3.3.1.1.3.3.3">𝑖</ci><ci id="S4.E3.m1.3.3.1.1.3.3.4.cmml" xref="S4.E3.m1.3.3.1.1.3.3.4">𝑥</ci><ci id="S4.E3.m1.3.3.1.1.3.3.5.cmml" xref="S4.E3.m1.3.3.1.1.3.3.5">𝑒</ci><ci id="S4.E3.m1.3.3.1.1.3.3.6.cmml" xref="S4.E3.m1.3.3.1.1.3.3.6">𝑑</ci></apply></apply><apply id="S4.E3.m1.3.3.1.1.1.cmml" xref="S4.E3.m1.3.3.1.1.1"><times id="S4.E3.m1.3.3.1.1.1.2.cmml" xref="S4.E3.m1.3.3.1.1.1.2"></times><apply id="S4.E3.m1.3.3.1.1.1.3.cmml" xref="S4.E3.m1.3.3.1.1.1.3"><divide id="S4.E3.m1.3.3.1.1.1.3.1.cmml" xref="S4.E3.m1.3.3.1.1.1.3"></divide><cn type="integer" id="S4.E3.m1.3.3.1.1.1.3.2.cmml" xref="S4.E3.m1.3.3.1.1.1.3.2">1</cn><apply id="S4.E3.m1.3.3.1.1.1.3.3.cmml" xref="S4.E3.m1.3.3.1.1.1.3.3"><times id="S4.E3.m1.3.3.1.1.1.3.3.1.cmml" xref="S4.E3.m1.3.3.1.1.1.3.3.1"></times><ci id="S4.E3.m1.3.3.1.1.1.3.3.2.cmml" xref="S4.E3.m1.3.3.1.1.1.3.3.2">𝐿</ci><ci id="S4.E3.m1.3.3.1.1.1.3.3.3.cmml" xref="S4.E3.m1.3.3.1.1.1.3.3.3">𝐵</ci></apply></apply><apply id="S4.E3.m1.3.3.1.1.1.1.cmml" xref="S4.E3.m1.3.3.1.1.1.1"><apply id="S4.E3.m1.3.3.1.1.1.1.2.cmml" xref="S4.E3.m1.3.3.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E3.m1.3.3.1.1.1.1.2.1.cmml" xref="S4.E3.m1.3.3.1.1.1.1.2">subscript</csymbol><sum id="S4.E3.m1.3.3.1.1.1.1.2.2.cmml" xref="S4.E3.m1.3.3.1.1.1.1.2.2"></sum><apply id="S4.E3.m1.3.3.1.1.1.1.2.3.cmml" xref="S4.E3.m1.3.3.1.1.1.1.2.3"><in id="S4.E3.m1.3.3.1.1.1.1.2.3.1.cmml" xref="S4.E3.m1.3.3.1.1.1.1.2.3.1"></in><ci id="S4.E3.m1.3.3.1.1.1.1.2.3.2.cmml" xref="S4.E3.m1.3.3.1.1.1.1.2.3.2">𝑙</ci><ci id="S4.E3.m1.3.3.1.1.1.1.2.3.3.cmml" xref="S4.E3.m1.3.3.1.1.1.1.2.3.3">𝐿</ci></apply></apply><apply id="S4.E3.m1.3.3.1.1.1.1.1.cmml" xref="S4.E3.m1.3.3.1.1.1.1.1"><apply id="S4.E3.m1.3.3.1.1.1.1.1.2.cmml" xref="S4.E3.m1.3.3.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E3.m1.3.3.1.1.1.1.1.2.1.cmml" xref="S4.E3.m1.3.3.1.1.1.1.1.2">subscript</csymbol><sum id="S4.E3.m1.3.3.1.1.1.1.1.2.2.cmml" xref="S4.E3.m1.3.3.1.1.1.1.1.2.2"></sum><apply id="S4.E3.m1.3.3.1.1.1.1.1.2.3.cmml" xref="S4.E3.m1.3.3.1.1.1.1.1.2.3"><in id="S4.E3.m1.3.3.1.1.1.1.1.2.3.1.cmml" xref="S4.E3.m1.3.3.1.1.1.1.1.2.3.1"></in><ci id="S4.E3.m1.3.3.1.1.1.1.1.2.3.2.cmml" xref="S4.E3.m1.3.3.1.1.1.1.1.2.3.2">𝑏</ci><ci id="S4.E3.m1.3.3.1.1.1.1.1.2.3.3.cmml" xref="S4.E3.m1.3.3.1.1.1.1.1.2.3.3">𝐵</ci></apply></apply><apply id="S4.E3.m1.3.3.1.1.1.1.1.1.cmml" xref="S4.E3.m1.3.3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E3.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S4.E3.m1.3.3.1.1.1.1.1.1">superscript</csymbol><apply id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1"><minus id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.2"></minus><apply id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1"><times id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1"></times><apply id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2"><divide id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2"></divide><cn type="integer" id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.2">1</cn><ci id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.3">𝑁</ci></apply><apply id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3"><apply id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.1"><csymbol cd="ambiguous" id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.1.1.cmml" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.1">subscript</csymbol><sum id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.1.2.cmml" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.1.2"></sum><apply id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.1.3.cmml" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.1.3"><in id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.1.3.1.cmml" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.1.3.1"></in><ci id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.1.3.2.cmml" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.1.3.2">𝑛</ci><ci id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.1.3.3.cmml" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.1.3.3">𝑁</ci></apply></apply><apply id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.2">superscript</csymbol><apply id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.2.2.1.cmml" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.2">subscript</csymbol><ci id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.2.2.2.cmml" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.2.2.2">𝐌</ci><list id="S4.E3.m1.2.2.2.3.cmml" xref="S4.E3.m1.2.2.2.4"><ci id="S4.E3.m1.1.1.1.1.cmml" xref="S4.E3.m1.1.1.1.1">𝑏</ci><ci id="S4.E3.m1.2.2.2.2.cmml" xref="S4.E3.m1.2.2.2.2">𝑛</ci></list></apply><ci id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3">𝑙</ci></apply></apply></apply><apply id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.2">𝐭</ci><ci id="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.3">𝑙</ci></apply></apply><cn type="integer" id="S4.E3.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S4.E3.m1.3.3.1.1.1.1.1.1.3">2</cn></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m1.3c">\mathcal{L}_{fixed}=\frac{1}{LB}\sum_{l\in L}\sum_{b\in B}\left(\left(\frac{1}{N}\sum_{n\in N}{\mathbf{M}}_{b,n}^{l}\right)-\mathbf{t}^{l}\right)^{2},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS1.SSS0.Px3.p2" class="ltx_para">
<p id="S4.SS1.SSS0.Px3.p2.1" class="ltx_p">This loss does not average token usage across images within a batch, thereby penalizing each image towards the same keeping ratio. As indicated in Table <a href="#S4.T3" title="Table 3 ‣ Reuse preserved tokens at demand ‣ 4.1 Evaluation of the insights and observations ‣ 4 Experiments ‣ Revisiting Token Pruning for Object Detection and Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, employing just a dynamic pruning rate yields a gain of +0.2 in both box AP and mask AP for SViT-T. When further augmented with token reactivation, these improvements escalate to +0.4 for box AP and +0.5 for mask AP. This substantiates both the efficacy of implementing a dynamic pruning rate in dense tasks and the added benefits of its integration with token reactivation. We also provide throughput experiments with different batch sizes in the supplementary material.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">A 2-layer MLP performs as good as complex gating networks</h4>

<div id="S4.SS1.SSS0.Px4.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px4.p1.1" class="ltx_p">We evaluate the designs of the different gating modules, as shown in Figure <a href="#S3.F3" title="Figure 3 ‣ Reuse preserved tokens on demand. ‣ 3.2 Insights and Observations ‣ 3 Token Pruning on dense prediction tasks ‣ Revisiting Token Pruning for Object Detection and Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, and experiment with both tiny and small models. Table <a href="#S4.T4" title="Table 4 ‣ A 2-layer MLP performs as good as complex gating networks ‣ 4.1 Evaluation of the insights and observations ‣ 4 Experiments ‣ Revisiting Token Pruning for Object Detection and Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows that using a 2-layer MLP to predict tokens for pruning achieves the same box AP and only -0.1 mask AP for tiny models, and -0.1 box AP and the same mask AP for small models. This verifies the role of 2-layer MLP as an effective and simple selection module.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.2.1.1" class="ltx_tr">
<th id="S4.T4.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" rowspan="2"><span id="S4.T4.2.1.1.1.1" class="ltx_text">Gating module</span></th>
<th id="S4.T4.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2">Tiny</th>
<th id="S4.T4.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2">Small</th>
</tr>
<tr id="S4.T4.2.2.2" class="ltx_tr">
<th id="S4.T4.2.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AP<sup id="S4.T4.2.2.2.1.1" class="ltx_sup">box</sup>
</th>
<th id="S4.T4.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AP<sup id="S4.T4.2.2.2.2.1" class="ltx_sup">mask</sup>
</th>
<th id="S4.T4.2.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AP<sup id="S4.T4.2.2.2.3.1" class="ltx_sup">box</sup>
</th>
<th id="S4.T4.2.2.2.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t">AP<sup id="S4.T4.2.2.2.4.1" class="ltx_sup">mask</sup>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.2.3.1" class="ltx_tr">
<th id="S4.T4.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">2-layer MLP</th>
<td id="S4.T4.2.3.1.2" class="ltx_td ltx_align_center ltx_border_t">48.2</td>
<td id="S4.T4.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t">48.5</td>
<td id="S4.T4.2.3.1.4" class="ltx_td ltx_align_center ltx_border_t">45.5</td>
<td id="S4.T4.2.3.1.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">40.7</td>
</tr>
<tr id="S4.T4.2.4.2" class="ltx_tr">
<th id="S4.T4.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">complex gating network</th>
<td id="S4.T4.2.4.2.2" class="ltx_td ltx_align_center ltx_border_b">48.2</td>
<td id="S4.T4.2.4.2.3" class="ltx_td ltx_align_center ltx_border_b">48.6</td>
<td id="S4.T4.2.4.2.4" class="ltx_td ltx_align_center ltx_border_b">45.6</td>
<td id="S4.T4.2.4.2.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_b">40.7</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.3.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S4.T4.4.2" class="ltx_text" style="font-size:90%;">Evaluation of designs for the gating module on SViT-T and SViT-S. A simple MLP can achieve similar performance with complex gating network, simplifying model design.</span></figcaption>
</figure>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Comparison with state-of-the-art models</h3>

<figure id="S4.T5" class="ltx_table">
<table id="S4.T5.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.2.1.1" class="ltx_tr">
<th id="S4.T5.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;" rowspan="2"><span id="S4.T5.2.1.1.1.1" class="ltx_text">
<span id="S4.T5.2.1.1.1.1.1" class="ltx_inline-block">
<span id="S4.T5.2.1.1.1.1.1.1" class="ltx_p"></span>
<span id="S4.T5.2.1.1.1.1.1.2" class="ltx_p">Model in</span>
<span id="S4.T5.2.1.1.1.1.1.3" class="ltx_p">ViT-Adapter</span>
</span></span></th>
<th id="S4.T5.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;" colspan="4">Tiny</th>
<th id="S4.T5.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;" colspan="4">Small</th>
</tr>
<tr id="S4.T5.2.2.2" class="ltx_tr">
<th id="S4.T5.2.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">AP<sup id="S4.T5.2.2.2.1.1" class="ltx_sup">box</sup>
</th>
<th id="S4.T5.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">AP<sup id="S4.T5.2.2.2.2.1" class="ltx_sup">mask</sup>
</th>
<th id="S4.T5.2.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">FPS<sup id="S4.T5.2.2.2.3.1" class="ltx_sup">w</sup>
</th>
<th id="S4.T5.2.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">FPS<sup id="S4.T5.2.2.2.4.1" class="ltx_sup">b</sup>
</th>
<th id="S4.T5.2.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">AP<sup id="S4.T5.2.2.2.5.1" class="ltx_sup">box</sup>
</th>
<th id="S4.T5.2.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">AP<sup id="S4.T5.2.2.2.6.1" class="ltx_sup">mask</sup>
</th>
<th id="S4.T5.2.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">FPS<sup id="S4.T5.2.2.2.7.1" class="ltx_sup">w</sup>
</th>
<th id="S4.T5.2.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">FPS<sup id="S4.T5.2.2.2.8.1" class="ltx_sup">b</sup>
</th>
</tr>
<tr id="S4.T5.2.3.3" class="ltx_tr">
<th id="S4.T5.2.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">DeiT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>
</th>
<th id="S4.T5.2.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">45.8</th>
<th id="S4.T5.2.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">40.9</th>
<th id="S4.T5.2.3.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">18.45</th>
<th id="S4.T5.2.3.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">27.61</th>
<th id="S4.T5.2.3.3.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">48.5</th>
<th id="S4.T5.2.3.3.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">42.8</th>
<th id="S4.T5.2.3.3.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">11.70</th>
<th id="S4.T5.2.3.3.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">14.20</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.2.4.1" class="ltx_tr">
<th id="S4.T5.2.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">EViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>
</th>
<td id="S4.T5.2.4.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">44.5 (-1.3)</td>
<td id="S4.T5.2.4.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">39.8 (-1.1)</td>
<td id="S4.T5.2.4.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">22.76</td>
<td id="S4.T5.2.4.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">35.80</td>
<td id="S4.T5.2.4.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">47.1 (-1.4)</td>
<td id="S4.T5.2.4.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">41.6 (-1.2)</td>
<td id="S4.T5.2.4.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">15.34</td>
<td id="S4.T5.2.4.1.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">20.01</td>
</tr>
<tr id="S4.T5.2.5.2" class="ltx_tr">
<th id="S4.T5.2.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;">EvoViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>
</th>
<td id="S4.T5.2.5.2.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">44.8 (-1.0)</td>
<td id="S4.T5.2.5.2.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">39.9 (-1.0)</td>
<td id="S4.T5.2.5.2.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">22.12</td>
<td id="S4.T5.2.5.2.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">34.33</td>
<td id="S4.T5.2.5.2.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">47.2 (-1.3)</td>
<td id="S4.T5.2.5.2.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">41.6 (-1.2)</td>
<td id="S4.T5.2.5.2.8" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">15.48</td>
<td id="S4.T5.2.5.2.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">20.26</td>
</tr>
<tr id="S4.T5.2.6.3" class="ltx_tr">
<th id="S4.T5.2.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;">ATS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
</th>
<td id="S4.T5.2.6.3.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">43.9 (-1.9)</td>
<td id="S4.T5.2.6.3.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">39.1 (-1.8)</td>
<td id="S4.T5.2.6.3.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">16.41</td>
<td id="S4.T5.2.6.3.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">22.38</td>
<td id="S4.T5.2.6.3.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">46.7 (-1.8)</td>
<td id="S4.T5.2.6.3.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">41.1 (-1.7)</td>
<td id="S4.T5.2.6.3.8" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">11.63</td>
<td id="S4.T5.2.6.3.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">14.24</td>
</tr>
<tr id="S4.T5.2.7.4" class="ltx_tr">
<th id="S4.T5.2.7.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;">DyViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>
</th>
<td id="S4.T5.2.7.4.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">41.2 (-4.6)</td>
<td id="S4.T5.2.7.4.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">37.1 (-3.8)</td>
<td id="S4.T5.2.7.4.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T5.2.7.4.4.1" class="ltx_text ltx_font_bold">23.10</span></td>
<td id="S4.T5.2.7.4.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T5.2.7.4.5.1" class="ltx_text ltx_font_bold">36.45</span></td>
<td id="S4.T5.2.7.4.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">diverge</td>
<td id="S4.T5.2.7.4.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">diverge</td>
<td id="S4.T5.2.7.4.8" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">/</td>
<td id="S4.T5.2.7.4.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">/</td>
</tr>
<tr id="S4.T5.2.8.5" class="ltx_tr">
<th id="S4.T5.2.8.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;">DyViT+prsv.</th>
<td id="S4.T5.2.8.5.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">44.1 (-1.7)</td>
<td id="S4.T5.2.8.5.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">39.3 (-1.6)</td>
<td id="S4.T5.2.8.5.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">22.95</td>
<td id="S4.T5.2.8.5.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">36.38</td>
<td id="S4.T5.2.8.5.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">47.2 (-1.3)</td>
<td id="S4.T5.2.8.5.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">41.6 (-1.2)</td>
<td id="S4.T5.2.8.5.8" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">15.66</td>
<td id="S4.T5.2.8.5.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T5.2.8.5.9.1" class="ltx_text ltx_font_bold">20.79</span></td>
</tr>
<tr id="S4.T5.2.9.6" class="ltx_tr">
<th id="S4.T5.2.9.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" style="padding-left:4.0pt;padding-right:4.0pt;">SViT (Ours)</th>
<td id="S4.T5.2.9.6.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T5.2.9.6.2.1" class="ltx_text ltx_font_bold">45.5 (-0.3)</span></td>
<td id="S4.T5.2.9.6.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T5.2.9.6.3.1" class="ltx_text ltx_font_bold">40.7 (-0.2)</span></td>
<td id="S4.T5.2.9.6.4" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.0pt;padding-right:4.0pt;">22.32</td>
<td id="S4.T5.2.9.6.5" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.0pt;padding-right:4.0pt;">34.69</td>
<td id="S4.T5.2.9.6.6" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T5.2.9.6.6.1" class="ltx_text ltx_font_bold">48.2 (-0.3)</span></td>
<td id="S4.T5.2.9.6.7" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T5.2.9.6.7.1" class="ltx_text ltx_font_bold">42.5 (-0.3)</span></td>
<td id="S4.T5.2.9.6.8" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T5.2.9.6.8.1" class="ltx_text ltx_font_bold">15.75</span></td>
<td id="S4.T5.2.9.6.9" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.0pt;padding-right:4.0pt;">20.78</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T5.5.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="S4.T5.6.2" class="ltx_text" style="font-size:90%;">Comparison of token pruning methods on COCO object detection and instance segmentation. DeiT is the dense model using all tokens. FPS<sup id="S4.T5.6.2.1" class="ltx_sup">w</sup> and FPS<sup id="S4.T5.6.2.2" class="ltx_sup">b</sup> represents the inference speeds for the whole network and the backbone, respectively, which are measured with batch size 1 on a single A100 GPU.</span></figcaption>
</figure>
<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.2" class="ltx_p">In this section, we compare SViT with prior art pruning models adapted for dense tasks. We evaluate inference speeds on a NVIDIA A100 GPU for both the backbones and the entire networks. As illustrated in Table <a href="#S4.T5" title="Table 5 ‣ 4.2 Comparison with state-of-the-art models ‣ 4 Experiments ‣ Revisiting Token Pruning for Object Detection and Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, sparse models exhibit comparable relative speed gains under identical total pruning rates, with the exception of ATS. The latter incurs computational overhead in its inverse transform sampling module for dealing with a large number of tokens in dense tasks. Among sparse models, SViT gets the highest performance for both tiny and small models. SViT-S significantly surpasses all baseline models, narrowing the performance drop with respect to the dense model from a range of -1.3 to -1.8 in box AP and -1.2 to -1.7 in mask AP, down to a mere -0.3 for both metrics. This performance advantage is consistently observed in SViT-T as well. In comparison with the dense counterpart, SViT-S improves inference speed by <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mo id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><csymbol cd="latexml" id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">\sim</annotation></semantics></math>34% and <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mo id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><csymbol cd="latexml" id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">\sim</annotation></semantics></math>46% for the entire network and the backbone, respectively, with negligible -0.3 drop in both box AP and mask AP.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Additional Analysis</h3>

<section id="S4.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Qualitative Results</h4>

<div id="S4.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px1.p1.1" class="ltx_p">We show qualitative results of the token pruning in SViT in Figure <a href="#S3.F4" title="Figure 4 ‣ 3.3 SViT: Selective Vision Transformer ‣ 3 Token Pruning on dense prediction tasks ‣ Revisiting Token Pruning for Object Detection and Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, and refer to supplementary material for more examples. The token-usage heatmap, created by quantifying the number of active layers for each token position, distinctly highlights not only the objects themselves but also their fine-grained contours. For example, the zebra’s feet and the contour of the donuts stand out clearly against their respective backgrounds. In the case of background tokens, uniform areas such as the ground in the zebra image are more prone to be pruned, whereas textured backgrounds near objects are kept processed more. We also present the averaged token-usage heatmap on COCO validation set in Figure <a href="#S4.F6" title="Figure 6 ‣ Qualitative Results ‣ 4.3 Additional Analysis ‣ 4 Experiments ‣ Revisiting Token Pruning for Object Detection and Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. The heatmap reveals a higher frequency of token usage in the central regions of images, due to the common photographic tendency to place objects at the center.</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2306.07050/assets/imgs/heatmap_coco2.png" id="S4.F6.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="318" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.3.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.4.2" class="ltx_text" style="font-size:90%;">Averaged token-usage heat map of SViT-T showing the number of active layers for each token position, averaged on COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> validation set. The resolution is interpolated to 50 x 80 tokens for all images.</span></figcaption>
</figure>
</section>
<section id="S4.SS3.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Different pruning rates</h4>

<div id="S4.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px2.p1.1" class="ltx_p">We adjust the pruning rate for SViT-S from the default 0.7 to {0.5, 0.6, 0.8, 0.9} and plot the mAP vs. pruning rate in Figure <a href="#S4.F7" title="Figure 7 ‣ Different pruning rates ‣ 4.3 Additional Analysis ‣ 4 Experiments ‣ Revisiting Token Pruning for Object Detection and Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. As shown in the plot, SViT consistently achieves better speed-accuracy trade-off than DeiT. However, we observe noticeable AP drop when base pruning rate is as low as 0.6 or 0.5, due to too aggressive pruning rates in the last three ViT blocks, i.e., 0.216 and 0.125, which is consistent with findings from classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>.</p>
</div>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2306.07050/assets/imgs/model_scaling.png" id="S4.F7.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="387" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.3.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S4.F7.4.2" class="ltx_text" style="font-size:90%;">Trade-off between speed and accuracy across various pruning rates in ViT-Adapter with dense DeiT and sparse SViT configurations.</span></figcaption>
</figure>
</section>
<section id="S4.SS3.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Reactivation distribution</h4>

<div id="S4.SS3.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px3.p1.1" class="ltx_p">To further understand the behavior of reactivation across transformer layers, we plot the reactivation ratio at each layer averaged on COCO validation set in Figure <a href="#S4.F5.sf1" title="Figure 5(a) ‣ Figure 5 ‣ Reuse preserved tokens at demand ‣ 4.1 Evaluation of the insights and observations ‣ 4 Experiments ‣ Revisiting Token Pruning for Object Detection and Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5(a)</span></a>. In the scope of this paper, the reactivation ratio at a layer (cyan colored) is defined as the ratio of current pruned tokens that are reused in at least one later layer in the backbone. As shown in the plot, most pruned tokens in early layers are reused in later layers. This indicates that it is harmful to fully drop tokens in early layers, and the model chooses to recover them in succeeding layers to alleviate the loss. In deeper layers, although the pruning rate is higher, the reactivation ratio is not apparently increased, as it is more tolerant to drop tokens. We also observe that over 50% of reactivated tokens are immediately reused in the succeeding layer. This observation aligns with the notion that the utility of a token diminishes if it remains unused for an extended period, given that feature characteristics often vary between deep and shallow layers.</p>
</div>
</section>
<section id="S4.SS3.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Reactivation areas</h4>

<div id="S4.SS3.SSS0.Px4.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px4.p1.1" class="ltx_p">In the previous section we analysed the reactivation ratio for different model layers, and here we show reactivation regions in images as visualized in Figure <a href="#S4.F5.sf2" title="Figure 5(b) ‣ Figure 5 ‣ Reuse preserved tokens at demand ‣ 4.1 Evaluation of the insights and observations ‣ 4 Experiments ‣ Revisiting Token Pruning for Object Detection and Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5(b)</span></a>. The token pruning is shown at middle layers of SViT. As anticipated, background tokens are predominantly not reactivated (white colored), while pruned tokens in interested objects, such as person, soccer and computers, are selectively reactivated (cyan colored). When faced with a high pruning rate that necessitates the temporary removal of tokens associated with objects of interest, the model strategically reactivates these tokens at later layers. This approach allows for a more expansive set of active tokens compared to scenarios where token reactivation is not an option.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Limitations and Societal Impacts</h2>

<section id="S5.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Limitations</h4>

<div id="S5.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px1.p1.1" class="ltx_p">The aim of our work is to bridge the gap of token pruning between classification and dense tasks for isotropic vision transformers. We do not focus on pyramidal vision transformers, nor on exploring better pruning rates. These topics are covered by some concurrent works and will be further studied in future works.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Societal Impact</h4>

<div id="S5.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px2.p1.1" class="ltx_p">The fintuning of sparse pruning is conducted after the model is fully trained and will introduce some additional energy consumption for training. However, this cost can be amortized once the model is deployed with improved inference efficiency. The proposed method predicts pruned tokens based on learned statistics from the training dataset, any bias inherent in the training data will be mirrored in the pruning process, and may result in the model disregarding biased content and exacerbating fairness issues.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this work, we revisit the designs of token pruning for vision transformers in the context of object detection and instance segmentation. We provide four insights that can enhance token pruning on dense tasks: the pruned tokens should not be removed but preserved in feature maps; reactivating pruned tokens at demand can boost model performance; a dynamic pruning rate is helpful on dense tasks; and a 2-layer MLP can be as effective as more complex gating networks. By incorporating these insights together, we present a token pruning method that outperforms prior state-of-the-arts by a significant margin and accelerates backbone inference by <math id="S6.p1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S6.p1.1.m1.1a"><mo id="S6.p1.1.m1.1.1" xref="S6.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S6.p1.1.m1.1b"><csymbol cd="latexml" id="S6.p1.1.m1.1.1.cmml" xref="S6.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.1.m1.1c">\sim</annotation></semantics></math>46% with negligible loss in accuracy. We hope these insights and encouraging results can inspire further research on ViT acceleration for dense prediction tasks beyond image classification.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Acknowledgements</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">This work was supported by the National Centre of Competence in Research (NCCR) Robotics (grant agreement No. 51NF40-185543) through the Swiss National Science Foundation (SNSF), and the European Research Council (ERC) under grant agreement No. 864042 (AGILEFLIGHT).</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Supplementary Material</h2>

</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">A    Results on ImageNet-1K classification</h2>

<section id="Sx2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Setup</h4>

<div id="Sx2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="Sx2.SS0.SSS0.Px1.p1.1" class="ltx_p">We also train and evaluate SViT-S on ImageNet-1K <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. We follow the training settings in DeiT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> and initialize our model from public pre-trained weights of DeiT-S. We use an AdamW optimizer to train the our model for 30 epochs and set the learning rate as <math id="Sx2.SS0.SSS0.Px1.p1.1.m1.1" class="ltx_math_unparsed" alttext="\frac{\text{batchsize}}{512}\times" display="inline"><semantics id="Sx2.SS0.SSS0.Px1.p1.1.m1.1a"><mrow id="Sx2.SS0.SSS0.Px1.p1.1.m1.1b"><mfrac id="Sx2.SS0.SSS0.Px1.p1.1.m1.1.1"><mtext id="Sx2.SS0.SSS0.Px1.p1.1.m1.1.1.2">batchsize</mtext><mn id="Sx2.SS0.SSS0.Px1.p1.1.m1.1.1.3">512</mn></mfrac><mo lspace="0.222em" id="Sx2.SS0.SSS0.Px1.p1.1.m1.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="Sx2.SS0.SSS0.Px1.p1.1.m1.1c">\frac{\text{batchsize}}{512}\times</annotation></semantics></math> 1e-5. The model is trained on a single machine with 4 V100 GPUs with a batch size of 1024.</p>
</div>
</section>
<section id="Sx2.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Results</h4>

<div id="Sx2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="Sx2.SS0.SSS0.Px2.p1.1" class="ltx_p">We compare the throughput of SViT-S and the dense counterpart DeiT-S in Table <a href="#Sx2.T6" title="Table 6 ‣ Results ‣ A Results on ImageNet-1K classification ‣ Revisiting Token Pruning for Object Detection and Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. SViT achieves 47% higher throughput than the dense counter part while only sacrificing -0.4% accuracy, effectively improving the accuracy-speed trade off. We also compare SViT-S with other token pruning models in Table <a href="#Sx2.T7" title="Table 7 ‣ Results ‣ A Results on ImageNet-1K classification ‣ Revisiting Token Pruning for Object Detection and Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. Although SViT is not originally targeted at classification tasks, it outperforms all models that use gating modules (DynamicViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, SPViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, AdaViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>), the models using special pruning techniques such as adaptive computation time <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> (A-ViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>) and reinforcement learning (IA-RED2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>), and a model that uses class token’s attention (Evo-ViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>).
However, when it comes to classification, EViT and ATS demonstrate superior performance over SViT. This is primarily due to their utilization of the class token, a feature specifically designed for the classification task.</p>
</div>
<figure id="Sx2.T6" class="ltx_table">
<table id="Sx2.T6.1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Sx2.T6.1.1.1" class="ltx_tr">
<th id="Sx2.T6.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="Sx2.T6.1.1.1.2.1" class="ltx_text" style="font-size:90%;">Model</span></th>
<th id="Sx2.T6.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="Sx2.T6.1.1.1.3.1" class="ltx_text" style="font-size:90%;">Top-1 Accuracy</span></th>
<th id="Sx2.T6.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="Sx2.T6.1.1.1.4.1" class="ltx_text" style="font-size:90%;">GFLOPS</span></th>
<th id="Sx2.T6.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><math id="Sx2.T6.1.1.1.1.m1.1" class="ltx_Math" alttext="\text{images}/s" display="inline"><semantics id="Sx2.T6.1.1.1.1.m1.1a"><mrow id="Sx2.T6.1.1.1.1.m1.1.1" xref="Sx2.T6.1.1.1.1.m1.1.1.cmml"><mtext mathsize="90%" id="Sx2.T6.1.1.1.1.m1.1.1.2" xref="Sx2.T6.1.1.1.1.m1.1.1.2a.cmml">images</mtext><mo maxsize="90%" minsize="90%" stretchy="true" symmetric="true" id="Sx2.T6.1.1.1.1.m1.1.1.1" xref="Sx2.T6.1.1.1.1.m1.1.1.1.cmml">/</mo><mi mathsize="90%" id="Sx2.T6.1.1.1.1.m1.1.1.3" xref="Sx2.T6.1.1.1.1.m1.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="Sx2.T6.1.1.1.1.m1.1b"><apply id="Sx2.T6.1.1.1.1.m1.1.1.cmml" xref="Sx2.T6.1.1.1.1.m1.1.1"><divide id="Sx2.T6.1.1.1.1.m1.1.1.1.cmml" xref="Sx2.T6.1.1.1.1.m1.1.1.1"></divide><ci id="Sx2.T6.1.1.1.1.m1.1.1.2a.cmml" xref="Sx2.T6.1.1.1.1.m1.1.1.2"><mtext mathsize="90%" id="Sx2.T6.1.1.1.1.m1.1.1.2.cmml" xref="Sx2.T6.1.1.1.1.m1.1.1.2">images</mtext></ci><ci id="Sx2.T6.1.1.1.1.m1.1.1.3.cmml" xref="Sx2.T6.1.1.1.1.m1.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.T6.1.1.1.1.m1.1c">\text{images}/s</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Sx2.T6.1.1.2.1" class="ltx_tr">
<td id="Sx2.T6.1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t">
<span id="Sx2.T6.1.1.2.1.1.1" class="ltx_text" style="font-size:90%;">DeiT-S </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx2.T6.1.1.2.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib51" title="" class="ltx_ref">51</a><span id="Sx2.T6.1.1.2.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="Sx2.T6.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="Sx2.T6.1.1.2.1.2.1" class="ltx_text" style="font-size:90%;">79.8</span></td>
<td id="Sx2.T6.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="Sx2.T6.1.1.2.1.3.1" class="ltx_text" style="font-size:90%;">4.6</span></td>
<td id="Sx2.T6.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="Sx2.T6.1.1.2.1.4.1" class="ltx_text" style="font-size:90%;">1524</span></td>
</tr>
<tr id="Sx2.T6.1.1.3.2" class="ltx_tr">
<td id="Sx2.T6.1.1.3.2.1" class="ltx_td ltx_align_left ltx_border_b"><span id="Sx2.T6.1.1.3.2.1.1" class="ltx_text" style="font-size:90%;">SViT-S</span></td>
<td id="Sx2.T6.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_b"><span id="Sx2.T6.1.1.3.2.2.1" class="ltx_text" style="font-size:90%;">79.4</span></td>
<td id="Sx2.T6.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_b"><span id="Sx2.T6.1.1.3.2.3.1" class="ltx_text" style="font-size:90%;">3.0</span></td>
<td id="Sx2.T6.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_b"><span id="Sx2.T6.1.1.3.2.4.1" class="ltx_text" style="font-size:90%;">2246</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 6: </span>Model Performance of DeiT-S and SViT-S. Throughput is measured on a single A100 GPU with batch size 512.</figcaption>
</figure>
<figure id="Sx2.T7" class="ltx_table">
<table id="Sx2.T7.3.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Sx2.T7.3.3.4.1" class="ltx_tr">
<th id="Sx2.T7.3.3.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t"><span id="Sx2.T7.3.3.4.1.1.1" class="ltx_text" style="font-size:90%;">Model</span></th>
<th id="Sx2.T7.3.3.4.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t"><span id="Sx2.T7.3.3.4.1.2.1" class="ltx_text" style="font-size:90%;">epochs</span></th>
<th id="Sx2.T7.3.3.4.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="Sx2.T7.3.3.4.1.3.1" class="ltx_text" style="font-size:90%;">GFLOPS</span></th>
<th id="Sx2.T7.3.3.4.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="Sx2.T7.3.3.4.1.4.1" class="ltx_text" style="font-size:90%;">Top-1 Acc(%)</span></th>
</tr>
<tr id="Sx2.T7.3.3.5.2" class="ltx_tr">
<th id="Sx2.T7.3.3.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">
<span id="Sx2.T7.3.3.5.2.1.1" class="ltx_text" style="font-size:90%;">DeiT-S </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx2.T7.3.3.5.2.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib51" title="" class="ltx_ref">51</a><span id="Sx2.T7.3.3.5.2.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="Sx2.T7.3.3.5.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="Sx2.T7.3.3.5.2.2.1" class="ltx_text" style="font-size:90%;">-</span></th>
<th id="Sx2.T7.3.3.5.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="Sx2.T7.3.3.5.2.3.1" class="ltx_text" style="font-size:90%;">4.6</span></th>
<th id="Sx2.T7.3.3.5.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="Sx2.T7.3.3.5.2.4.1" class="ltx_text" style="font-size:90%;">79.8</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Sx2.T7.1.1.1" class="ltx_tr">
<th id="Sx2.T7.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<span id="Sx2.T7.1.1.1.1.1" class="ltx_text" style="font-size:90%;">DynamicViT </span><math id="Sx2.T7.1.1.1.1.m1.1" class="ltx_Math" alttext="\ddagger" display="inline"><semantics id="Sx2.T7.1.1.1.1.m1.1a"><mo mathsize="90%" id="Sx2.T7.1.1.1.1.m1.1.1" xref="Sx2.T7.1.1.1.1.m1.1.1.cmml">‡</mo><annotation-xml encoding="MathML-Content" id="Sx2.T7.1.1.1.1.m1.1b"><ci id="Sx2.T7.1.1.1.1.m1.1.1.cmml" xref="Sx2.T7.1.1.1.1.m1.1.1">‡</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.T7.1.1.1.1.m1.1c">\ddagger</annotation></semantics></math><span id="Sx2.T7.1.1.1.1.2" class="ltx_text" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx2.T7.1.1.1.1.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib44" title="" class="ltx_ref">44</a><span id="Sx2.T7.1.1.1.1.4.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="Sx2.T7.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="Sx2.T7.1.1.1.2.1" class="ltx_text" style="font-size:90%;">30</span></th>
<td id="Sx2.T7.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="Sx2.T7.1.1.1.3.1" class="ltx_text" style="font-size:90%;">3.0</span></td>
<td id="Sx2.T7.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="Sx2.T7.1.1.1.4.1" class="ltx_text" style="font-size:90%;">79.3 (-0.5)</span></td>
</tr>
<tr id="Sx2.T7.3.3.6.1" class="ltx_tr">
<th id="Sx2.T7.3.3.6.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="Sx2.T7.3.3.6.1.1.1" class="ltx_text" style="font-size:90%;">EViT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx2.T7.3.3.6.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib34" title="" class="ltx_ref">34</a><span id="Sx2.T7.3.3.6.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="Sx2.T7.3.3.6.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="Sx2.T7.3.3.6.1.2.1" class="ltx_text" style="font-size:90%;">30</span></th>
<td id="Sx2.T7.3.3.6.1.3" class="ltx_td ltx_align_center"><span id="Sx2.T7.3.3.6.1.3.1" class="ltx_text" style="font-size:90%;">3.0</span></td>
<td id="Sx2.T7.3.3.6.1.4" class="ltx_td ltx_align_center"><span id="Sx2.T7.3.3.6.1.4.1" class="ltx_text" style="font-size:90%;">79.5 (-0.3)</span></td>
</tr>
<tr id="Sx2.T7.3.3.7.2" class="ltx_tr">
<th id="Sx2.T7.3.3.7.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="Sx2.T7.3.3.7.2.1.1" class="ltx_text" style="font-size:90%;">Evo-ViT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx2.T7.3.3.7.2.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib61" title="" class="ltx_ref">61</a><span id="Sx2.T7.3.3.7.2.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="Sx2.T7.3.3.7.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="Sx2.T7.3.3.7.2.2.1" class="ltx_text" style="font-size:90%;">300 *</span></th>
<td id="Sx2.T7.3.3.7.2.3" class="ltx_td ltx_align_center"><span id="Sx2.T7.3.3.7.2.3.1" class="ltx_text" style="font-size:90%;">3.0</span></td>
<td id="Sx2.T7.3.3.7.2.4" class="ltx_td ltx_align_center"><span id="Sx2.T7.3.3.7.2.4.1" class="ltx_text" style="font-size:90%;">79.4 (-0.4)</span></td>
</tr>
<tr id="Sx2.T7.2.2.2" class="ltx_tr">
<th id="Sx2.T7.2.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="Sx2.T7.2.2.2.2.1" class="ltx_text" style="font-size:90%;">Evo-ViT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx2.T7.2.2.2.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib61" title="" class="ltx_ref">61</a><span id="Sx2.T7.2.2.2.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="Sx2.T7.2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="Sx2.T7.2.2.2.1.1" class="ltx_text" style="font-size:90%;">30 </span><math id="Sx2.T7.2.2.2.1.m1.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="Sx2.T7.2.2.2.1.m1.1a"><mo mathsize="90%" id="Sx2.T7.2.2.2.1.m1.1.1" xref="Sx2.T7.2.2.2.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="Sx2.T7.2.2.2.1.m1.1b"><ci id="Sx2.T7.2.2.2.1.m1.1.1.cmml" xref="Sx2.T7.2.2.2.1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.T7.2.2.2.1.m1.1c">\dagger</annotation></semantics></math>
</th>
<td id="Sx2.T7.2.2.2.3" class="ltx_td ltx_align_center"><span id="Sx2.T7.2.2.2.3.1" class="ltx_text" style="font-size:90%;">3.0</span></td>
<td id="Sx2.T7.2.2.2.4" class="ltx_td ltx_align_center"><span id="Sx2.T7.2.2.2.4.1" class="ltx_text" style="font-size:90%;">79.2 (-0.6)</span></td>
</tr>
<tr id="Sx2.T7.3.3.8.3" class="ltx_tr">
<th id="Sx2.T7.3.3.8.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="Sx2.T7.3.3.8.3.1.1" class="ltx_text" style="font-size:90%;">A-ViT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx2.T7.3.3.8.3.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib63" title="" class="ltx_ref">63</a><span id="Sx2.T7.3.3.8.3.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="Sx2.T7.3.3.8.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="Sx2.T7.3.3.8.3.2.1" class="ltx_text" style="font-size:90%;">100</span></th>
<td id="Sx2.T7.3.3.8.3.3" class="ltx_td ltx_align_center"><span id="Sx2.T7.3.3.8.3.3.1" class="ltx_text" style="font-size:90%;">3.6</span></td>
<td id="Sx2.T7.3.3.8.3.4" class="ltx_td ltx_align_center"><span id="Sx2.T7.3.3.8.3.4.1" class="ltx_text" style="font-size:90%;">78.6 (-1.3)</span></td>
</tr>
<tr id="Sx2.T7.3.3.9.4" class="ltx_tr">
<th id="Sx2.T7.3.3.9.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="Sx2.T7.3.3.9.4.1.1" class="ltx_text" style="font-size:90%;">ATS </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx2.T7.3.3.9.4.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib19" title="" class="ltx_ref">19</a><span id="Sx2.T7.3.3.9.4.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="Sx2.T7.3.3.9.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="Sx2.T7.3.3.9.4.2.1" class="ltx_text" style="font-size:90%;">30</span></th>
<td id="Sx2.T7.3.3.9.4.3" class="ltx_td ltx_align_center"><span id="Sx2.T7.3.3.9.4.3.1" class="ltx_text" style="font-size:90%;">3.0</span></td>
<td id="Sx2.T7.3.3.9.4.4" class="ltx_td ltx_align_center"><span id="Sx2.T7.3.3.9.4.4.1" class="ltx_text" style="font-size:90%;">79.7 (-0.1)</span></td>
</tr>
<tr id="Sx2.T7.3.3.10.5" class="ltx_tr">
<th id="Sx2.T7.3.3.10.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="Sx2.T7.3.3.10.5.1.1" class="ltx_text" style="font-size:90%;">AdaViT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx2.T7.3.3.10.5.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib39" title="" class="ltx_ref">39</a><span id="Sx2.T7.3.3.10.5.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="Sx2.T7.3.3.10.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="Sx2.T7.3.3.10.5.2.1" class="ltx_text" style="font-size:90%;">150</span></th>
<td id="Sx2.T7.3.3.10.5.3" class="ltx_td ltx_align_center"><span id="Sx2.T7.3.3.10.5.3.1" class="ltx_text" style="font-size:90%;">2.3</span></td>
<td id="Sx2.T7.3.3.10.5.4" class="ltx_td ltx_align_center"><span id="Sx2.T7.3.3.10.5.4.1" class="ltx_text" style="font-size:90%;">77.3 (-2.5)</span></td>
</tr>
<tr id="Sx2.T7.3.3.11.6" class="ltx_tr">
<th id="Sx2.T7.3.3.11.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="Sx2.T7.3.3.11.6.1.1" class="ltx_text" style="font-size:90%;">IA-RED2 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx2.T7.3.3.11.6.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib41" title="" class="ltx_ref">41</a><span id="Sx2.T7.3.3.11.6.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="Sx2.T7.3.3.11.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="Sx2.T7.3.3.11.6.2.1" class="ltx_text" style="font-size:90%;">90</span></th>
<td id="Sx2.T7.3.3.11.6.3" class="ltx_td ltx_align_center"><span id="Sx2.T7.3.3.11.6.3.1" class="ltx_text" style="font-size:90%;">3.2</span></td>
<td id="Sx2.T7.3.3.11.6.4" class="ltx_td ltx_align_center"><span id="Sx2.T7.3.3.11.6.4.1" class="ltx_text" style="font-size:90%;">79.1 (-0.7)</span></td>
</tr>
<tr id="Sx2.T7.3.3.3" class="ltx_tr">
<th id="Sx2.T7.3.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="Sx2.T7.3.3.3.1.1" class="ltx_text" style="font-size:90%;">SPViT </span><math id="Sx2.T7.3.3.3.1.m1.1" class="ltx_Math" alttext="\ddagger" display="inline"><semantics id="Sx2.T7.3.3.3.1.m1.1a"><mo mathsize="90%" id="Sx2.T7.3.3.3.1.m1.1.1" xref="Sx2.T7.3.3.3.1.m1.1.1.cmml">‡</mo><annotation-xml encoding="MathML-Content" id="Sx2.T7.3.3.3.1.m1.1b"><ci id="Sx2.T7.3.3.3.1.m1.1.1.cmml" xref="Sx2.T7.3.3.3.1.m1.1.1">‡</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.T7.3.3.3.1.m1.1c">\ddagger</annotation></semantics></math><span id="Sx2.T7.3.3.3.1.2" class="ltx_text" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="Sx2.T7.3.3.3.1.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib32" title="" class="ltx_ref">32</a><span id="Sx2.T7.3.3.3.1.4.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="Sx2.T7.3.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="Sx2.T7.3.3.3.2.1" class="ltx_text" style="font-size:90%;">60</span></th>
<td id="Sx2.T7.3.3.3.3" class="ltx_td ltx_align_center"><span id="Sx2.T7.3.3.3.3.1" class="ltx_text" style="font-size:90%;">2.7</span></td>
<td id="Sx2.T7.3.3.3.4" class="ltx_td ltx_align_center"><span id="Sx2.T7.3.3.3.4.1" class="ltx_text" style="font-size:90%;">79.3 (-0.5)</span></td>
</tr>
<tr id="Sx2.T7.3.3.12.7" class="ltx_tr">
<th id="Sx2.T7.3.3.12.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b"><span id="Sx2.T7.3.3.12.7.1.1" class="ltx_text" style="font-size:90%;">SViT (Ours)</span></th>
<th id="Sx2.T7.3.3.12.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b"><span id="Sx2.T7.3.3.12.7.2.1" class="ltx_text" style="font-size:90%;">30</span></th>
<td id="Sx2.T7.3.3.12.7.3" class="ltx_td ltx_align_center ltx_border_b"><span id="Sx2.T7.3.3.12.7.3.1" class="ltx_text" style="font-size:90%;">3.0</span></td>
<td id="Sx2.T7.3.3.12.7.4" class="ltx_td ltx_align_center ltx_border_b"><span id="Sx2.T7.3.3.12.7.4.1" class="ltx_text" style="font-size:90%;">79.4 (-0.4)</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 7: </span>Model performance on ImageNet-1K. * means training from scratch. <math id="Sx2.T7.6.m1.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="Sx2.T7.6.m1.1b"><mo id="Sx2.T7.6.m1.1.1" xref="Sx2.T7.6.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="Sx2.T7.6.m1.1c"><ci id="Sx2.T7.6.m1.1.1.cmml" xref="Sx2.T7.6.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.T7.6.m1.1d">\dagger</annotation></semantics></math> indicates experiments trained by us. <math id="Sx2.T7.7.m2.1" class="ltx_Math" alttext="\ddagger" display="inline"><semantics id="Sx2.T7.7.m2.1b"><mo id="Sx2.T7.7.m2.1.1" xref="Sx2.T7.7.m2.1.1.cmml">‡</mo><annotation-xml encoding="MathML-Content" id="Sx2.T7.7.m2.1c"><ci id="Sx2.T7.7.m2.1.1.cmml" xref="Sx2.T7.7.m2.1.1">‡</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.T7.7.m2.1d">\ddagger</annotation></semantics></math> uses additional knowledge distillation. Note that EViT, Evo-ViT and ATS depend on the class token, designed specifically for classification, to help improve their performance. On the other hand, SViT targets more general tasks and does not rely on the attention map of the class token for token pruning.</figcaption>
</figure>
</section>
</section>
<section id="Sx3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">B    Influence of Batch Size on Throughput</h2>

<div id="Sx3.p1" class="ltx_para">
<p id="Sx3.p1.1" class="ltx_p">It is not straightforward to do batch inference with different number of tokens per image, as the tensor cannot be easily arranged in a regular shape, therefore, we use <span id="Sx3.p1.1.1" class="ltx_text ltx_font_italic">pytorch</span> <span id="Sx3.p1.1.2" class="ltx_text ltx_font_italic">nested tensor<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note"><span id="footnote2.1.1.1" class="ltx_text ltx_font_upright">2</span></span><a target="_blank" href="https://pytorch.org/docs/1.13/nested.html" title="" class="ltx_ref ltx_url ltx_font_typewriter ltx_font_upright">https://pytorch.org/docs/1.13/nested.html</a></span></span></span></span> to efficiently process the varying-length sequences of tokens. The tokens to be processed are first gathered into a nested tensor, then passed to a ViT block constructed with <span id="Sx3.p1.1.3" class="ltx_text ltx_font_italic">nested tensor</span> operations, and finally un-nested and scattered back to the feature map.</p>
</div>
<div id="Sx3.p2" class="ltx_para">
<p id="Sx3.p2.1" class="ltx_p">We test the throughput of SViT and DeiT on ImageNet-1k for varying batch sizes as follows: for each batch size, we randomly fetch 30 batches from the validation set, and for each batch we run the inference for 50 times and take the average throughput as the speed for this batch. Then we calculate the mean and standard deviation over the speeds of the batches. As seen in Figure <a href="#Sx3.F8" title="Figure 8 ‣ B Influence of Batch Size on Throughput ‣ Revisiting Token Pruning for Object Detection and Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, proportional throughput gains can be obtained from increased batch sizes for SViT, which verifies that <span id="Sx3.p2.1.1" class="ltx_text ltx_font_italic">nested tensor</span> could be a promising way of handling the varying-sized tensors in the dynamic scenario. However, note that the <span id="Sx3.p2.1.2" class="ltx_text ltx_font_italic">nested tensor</span> is not fully developed and is still in a prototype stage, and leads to some overhead when the batch size is small.</p>
</div>
<div id="Sx3.p3" class="ltx_para">
<p id="Sx3.p3.1" class="ltx_p">In the case of object detection and instance segmentation, we abstain from using <span id="Sx3.p3.1.1" class="ltx_text ltx_font_italic">nested tensors</span> due to the difficulties associated with creating a large batch size for high-resolution images. Consequently, we centered our efforts on inference with a batch size of 1 for these dense tasks. Future advancements in this technique, along with other related breakthroughs, may facilitate additional speedups. These improvements could be readily integrated into SViT, as previously demonstrated in classification tasks.</p>
</div>
<figure id="Sx3.F8" class="ltx_figure"><img src="/html/2306.07050/assets/imgs/bs_throughput.png" id="Sx3.F8.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="437" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Sx3.F8.3.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="Sx3.F8.4.2" class="ltx_text" style="font-size:90%;">Throughput vs. Batch Sizes of SViT-S and DeiT-S on ImageNet-1K.</span></figcaption>
</figure>
</section>
<section id="Sx4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">C    ViT has different layer-wise attention</h2>

<div id="Sx4.p1" class="ltx_para">
<p id="Sx4.p1.1" class="ltx_p">Vision Transformers do not always attend to the same set of tokens, even for the important ones. An illustrated in the example in Figure <a href="#Sx4.F9" title="Figure 9 ‣ C ViT has different layer-wise attention ‣ Revisiting Token Pruning for Object Detection and Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>, the dense DeiT-S <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> first attends to the background tokens in the 1st layer, and then attends to joint regions of the human face and the rabbit in the next three layers. After that, the human face, rabbit eyes, and rabbit ears are attended in different layers, respectively. This inspired us to reactivate previously pruned tokens, as each layer can have its customized preference on tokens.</p>
</div>
<figure id="Sx4.F9" class="ltx_figure"><img src="/html/2306.07050/assets/imgs/layer-wise_attention.png" id="Sx4.F9.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="361" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Sx4.F9.7.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="Sx4.F9.8.2" class="ltx_text" style="font-size:90%;">The first row: the attention from the <span id="Sx4.F9.8.2.1" class="ltx_ERROR undefined">\nth</span>1 layer to the <span id="Sx4.F9.8.2.2" class="ltx_ERROR undefined">\nth</span>6 layer; the second row: the attention maps from the <span id="Sx4.F9.8.2.3" class="ltx_ERROR undefined">\nth</span>7 layer to the <span id="Sx4.F9.8.2.4" class="ltx_ERROR undefined">\nth</span>12 layer. Compared to the dense DeiT-S model, SViT-S keeps the most important features for each layer, especially the human face, the rabbit eyes, and the rabbit ear. Since ViT’s attentions can be different across different layers, SViT does not keep the same tokens across different layers. The attention maps are visualized as the mean of the attention from class token’s heads. </span></figcaption>
</figure>
</section>
<section id="Sx5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">D    Discussion on Prior-Art Token Pruning Methods</h2>

<div id="Sx5.p1" class="ltx_para">
<p id="Sx5.p1.1" class="ltx_p">Since the class token does not originally exist for ViT models on dense tasks, we append a randomly initialize a class token for attention-based token pruning models (EViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, ATS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, and Evo-ViT<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>), which can help them prune tokens reasonably on dense tasks.</p>
</div>
<div id="Sx5.p2" class="ltx_para">
<p id="Sx5.p2.1" class="ltx_p">Among these models, Evo-ViT is unique because it preserves pruned tokens in the feature map. However, it has a tendency to converge to a consistent set of tokens and thus does not reuse pruned tokens, as shown in Figure <a href="#Sx5.F10" title="Figure 10 ‣ D Discussion on Prior-Art Token Pruning Methods ‣ Revisiting Token Pruning for Object Detection and Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>.
We conjecture this is because Evo-ViT uses a moving average to update the attention scores of processed tokens, which in turn is used to select tokens to be pruned. Since updating the scores is only done for processed tokens, the pruned tokens do not have a chance to change their scores, and thus causing the model to consistently use the same selection scheme.
As Figure <a href="#Sx5.F10" title="Figure 10 ‣ D Discussion on Prior-Art Token Pruning Methods ‣ Revisiting Token Pruning for Object Detection and Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> illustrates, Evo-ViT consistently selects bottom-left tokens from its first layer onward, even though they are irrelevant background tokens. In contrast, our model can dynamically choose different tokens for each layer, and reuse important ones.</p>
</div>
<figure id="Sx5.F10" class="ltx_figure"><img src="/html/2306.07050/assets/imgs/evo_not_reuse.png" id="Sx5.F10.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="302" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Sx5.F10.3.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span id="Sx5.F10.4.2" class="ltx_text" style="font-size:90%;">Top: token pruning for Evo-ViT-T. Bottom: token pruning for SViT-T. Evo-ViT has the same keep ratio per layer, and tends to use the same set of tokens for all its pruning layers, so the selection largely depends on its first pruning layer, which may not be optimal. SViT can prune tokens independently for each layer, which is learned by the gating MLPs, and can reuse tokens according as needed.</span></figcaption>
</figure>
</section>
<section id="Sx6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">E    Qualitative Examples</h2>

<div id="Sx6.p1" class="ltx_para">
<p id="Sx6.p1.1" class="ltx_p">We provide more visualizations of SViT-S on COCO in Figure <a href="#Sx6.F11" title="Figure 11 ‣ E Qualitative Examples ‣ Revisiting Token Pruning for Object Detection and Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>. To better understand the token selection of SViT, we split the tokens into two sets: object tokens and background tokens, according to the prediction mask.
Then we compute the token usage for them separately. The first observation is that SViT uses a larger ratio of foreground tokens than background tokens. When the proportion of the object in the image is small, the foreground token usage can be as high as 90%.
In cluttered images, such as the sheep example in the 4th row, not all foreground tokens are essential; less discriminative foreground tokens can still be pruned without affecting performance.</p>
</div>
<figure id="Sx6.F11" class="ltx_figure"><img src="/html/2306.07050/assets/imgs/more_visualization.png" id="Sx6.F11.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="371" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Sx6.F11.3.1.1" class="ltx_text" style="font-size:90%;">Figure 11</span>: </span><span id="Sx6.F11.4.2" class="ltx_text" style="font-size:90%;">More visualizations of SViT-S on COCO validation set. The token usage heat map shows the number of used layers per token. The tokens are further split into two sets: foreground tokens and background tokens, and token usages are computed for them separately.</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Aishwarya Bhandare, Vamsi Sripathi, Deepthi Karkada, Vivek Menon, Sun Choi,
Kushal Datta, and Vikram Saletore.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Efficient 8-bit quantization of transformer neural machine language
translation model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1906.00532</span><span id="bib.bib1.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph
Feichtenhofer, and Judy Hoffman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Token merging: Your ViT but faster.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. Conf. Learn. Representations (ICLR)</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Language models are few-shot learners.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib3.4.2" class="ltx_text" style="font-size:90%;">,
33:1877–1901, 2020.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander
Kirillov, and Sergey Zagoruyko.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">End-to-end object detection with transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Eur. Conf. Comput. Vis. (ECCV)</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Shuning Chang, Pichao Wang, Ming Lin, Fan Wang, David Junhao Zhang, Rong Jin,
and Mike Zheng Shou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Making vision transformers efficient from a token sparsification
view.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, pages
6195–6205, 2023.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher
Ré.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Scatterbrain: Unifying sparse and low-rank attention approximation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2110.15343</span><span id="bib.bib6.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Chun-Fu Chen, Rameswar Panda, and Quanfu Fan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Regionvit: Regional-to-local attention for vision transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2106.02689</span><span id="bib.bib7.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Xuanyao Chen, Zhijian Liu, Haotian Tang, Li Yi, Hang Zhao, and Song Han.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Sparsevit: Revisiting activation sparsity for efficient
high-resolution vision transformer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu
Qiao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Vision transformer adapter for dense predictions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. Conf. Learn. Representations (ICLR)</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Bowen Cheng, Alex Schwing, and Alexander Kirillov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Per-pixel classification is not all you need for semantic
segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib10.4.2" class="ltx_text" style="font-size:90%;">,
34:17864–17875, 2021.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Generating long sequences with sparse transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1904.10509</span><span id="bib.bib11.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,
Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,
Lukasz Kaiser, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Rethinking attention with performers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2009.14794</span><span id="bib.bib12.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei,
Huaxia Xia, and Chunhua Shen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Twins: Revisiting the design of spatial attention in vision
transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib13.4.2" class="ltx_text" style="font-size:90%;">,
34:9355–9366, 2021.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Imagenet: A large-scale hierarchical image database.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, 2009.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Bert: Pre-training of deep bidirectional transformers for language
understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1810.04805</span><span id="bib.bib15.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">An image is worth 16x16 words: Transformers for image recognition at
scale.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. Conf. Learn. Representations (ICLR)</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Angela Fan, Edouard Grave, and Armand Joulin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Reducing transformer depth on demand with structured dropout.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1909.11556</span><span id="bib.bib17.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, Rémi Gribonval,
Herve Jegou, and Armand Joulin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Training with quantization noise for extreme model compression.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2004.07320</span><span id="bib.bib18.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Mohsen Fayyaz, Soroush Abbasi Koohpayegani, Farnoush Rezaei Jafari, Sunando
Sengupta, Hamid Reza Vaezi Joze, Eric Sommerlade, Hamed Pirsiavash, and
Juergen Gall.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Adaptive token sampling for efficient vision transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Eur. Conf. Comput. Vis. (ECCV)</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Michael Figurnov, Maxwell D Collins, Yukun Zhu, Li Zhang, Jonathan Huang,
Dmitry Vetrov, and Ruslan Salakhutdinov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Spatially adaptive computation time for residual networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Benjamin Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin,
Hervé Jégou, and Matthijs Douze.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Levit: a vision transformer in convnet’s clothing for faster
inference.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. Conf. Comput. Vis. (ICCV)</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Alex Graves.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Adaptive computation time for recurrent neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1603.08983</span><span id="bib.bib22.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu,
Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">A survey on vision transformer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Trans. Pattern Anal. Mach. Intell.</span><span id="bib.bib23.4.2" class="ltx_text" style="font-size:90%;">, 45(1):87–110, 2022.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Transformer in transformer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib24.4.2" class="ltx_text" style="font-size:90%;">,
34:15908–15919, 2021.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Mask r-cnn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. Conf. Comput. Vis. (ICCV)</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Eric Jang, Shixiang Gu, and Ben Poole.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Categorical reparameterization with gumbel-softmax.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. Conf. Learn. Representations (ICLR)</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Zi-Hang Jiang, Qibin Hou, Li Yuan, Daquan Zhou, Yujun Shi, Xiaojie Jin, Anran
Wang, and Jiashi Feng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">All tokens matter: Token labeling for training better vision
transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib27.4.2" class="ltx_text" style="font-size:90%;">,
34:18590–18602, 2021.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François
Fleuret.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Transformers are rnns: Fast autoregressive transformers with linear
attention.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. Int. Conf. Mach. Learning (ICML)</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, pages 5156–5165.
PMLR, 2020.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz
Khan, and Mubarak Shah.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Transformers in vision: A survey.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ACM computing surveys (CSUR)</span><span id="bib.bib29.4.2" class="ltx_text" style="font-size:90%;">, 54(10s):1–41, 2022.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">I-bert: Integer-only bert quantization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. Int. Conf. Mach. Learning (ICML)</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, pages 5506–5518.
PMLR, 2021.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Reformer: The efficient transformer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2001.04451</span><span id="bib.bib31.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Zhenglun Kong, Peiyan Dong, Xiaolong Ma, Xin Meng, Wei Niu, Mengshu Sun, Bin
Ren, Minghai Qin, Hao Tang, and Yanzhi Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Spvit: Enabling faster vision transformers via soft token pruning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Eur. Conf. Comput. Vis. (ECCV)</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Exploring plain vision transformer backbones for object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Eur. Conf. Comput. Vis. (ECCV)</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:90%;">, pages 280–296, 2022.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Youwei Liang, Chongjian GE, Zhan Tong, Yibing Song, Jue Wang, and Pengtao Xie.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">EViT: Expediting vision transformers via token reorganizations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. Conf. Learn. Representations (ICLR)</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Microsoft coco: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Eur. Conf. Comput. Vis. (ECCV)</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Roberta: A robustly optimized bert pretraining approach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1907.11692</span><span id="bib.bib36.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
Baining Guo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Swin transformer: Hierarchical vision transformer using shifted
windows.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib37.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. Conf. Comput. Vis. (ICCV)</span><span id="bib.bib37.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Chris J. Maddison, Andriy Mnih, and Yee Whye Teh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">The concrete distribution: A continuous relaxation of discrete random
variables.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. Conf. Learn. Representations (ICLR)</span><span id="bib.bib38.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Lingchen Meng, Hengduo Li, Bor-Chun Chen, Shiyi Lan, Zuxuan Wu, Yu-Gang Jiang,
and Ser-Nam Lim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">Adavit: Adaptive vision transformers for efficient image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib39.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</span><span id="bib.bib39.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Paul Michel, Omer Levy, and Graham Neubig.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Are sixteen heads really better than one?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib40.4.2" class="ltx_text" style="font-size:90%;">, 32, 2019.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">
Bowen Pan, Rameswar Panda, Yifan Jiang, Zhangyang Wang, Rogerio Feris, and Aude
Oliva.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text" style="font-size:90%;">Ia-red</span><sup id="bib.bib41.4.2" class="ltx_sup"><span id="bib.bib41.4.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">2</span></sup><span id="bib.bib41.5.3" class="ltx_text" style="font-size:90%;">: Interpretability-aware redundancy reduction for
vision transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib41.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib41.8.3" class="ltx_text" style="font-size:90%;">,
volume 34, pages 24898–24911, 2021.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Improving language understanding by generative pre-training.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text" style="font-size:90%;">2018.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Yongming Rao, Zuyan Liu, Wenliang Zhao, Jie Zhou, and Jiwen Lu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Dynamic spatial sparsification for efficient vision transformers and
convolutional neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Trans. Pattern Anal. Mach. Intell.</span><span id="bib.bib43.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">DynamicVit: Efficient vision transformers with dynamic token
sparsification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib44.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib44.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Byungseok Roh, JaeWoong Shin, Wuhyun Shin, and Saehoon Kim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">Sparse detr: Efficient end-to-end object detection with learnable
sparsity.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib45.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. Conf. Learn. Representations (ICLR)</span><span id="bib.bib45.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">Efficient content-based sparse attention with routing transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Transactions of the Association for Computational Linguistics</span><span id="bib.bib46.4.2" class="ltx_text" style="font-size:90%;">,
9:53–68, 2021.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Michael S Ryoo, AJ Piergiovanni, Anurag Arnab, Mostafa Dehghani, and Anelia
Angelova.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">Tokenlearner: What can 8 learned tokens do for images and videos?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2106.11297</span><span id="bib.bib47.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">Distilbert, a distilled version of bert: smaller, faster, cheaper and
lighter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1910.01108</span><span id="bib.bib48.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami,
Michael W Mahoney, and Kurt Keutzer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">Q-bert: Hessian based ultra low precision quantization of bert.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib49.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">AAAI Conf. Artificial Intell.</span><span id="bib.bib49.5.3" class="ltx_text" style="font-size:90%;">, volume 34, pages
8815–8821, 2020.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">Sparse sinkhorn attention.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib50.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. Int. Conf. Mach. Learning (ICML)</span><span id="bib.bib50.5.3" class="ltx_text" style="font-size:90%;">, pages 9438–9447.
PMLR, 2020.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text" style="font-size:90%;">
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre
Sablayrolles, and Herve Jegou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.2.1" class="ltx_text" style="font-size:90%;">Training data-efficient image transformers &amp; distillation through
attention.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib51.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. Int. Conf. Mach. Learning (ICML)</span><span id="bib.bib51.5.3" class="ltx_text" style="font-size:90%;">, July 2021.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text" style="font-size:90%;">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.2.1" class="ltx_text" style="font-size:90%;">Attention is all you need.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib52.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib52.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text" style="font-size:90%;">
Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.2.1" class="ltx_text" style="font-size:90%;">Analyzing multi-head self-attention: Specialized heads do the heavy
lifting, the rest can be pruned.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib53.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 57th Annual Meeting of the Association for
Computational Linguistics</span><span id="bib.bib53.5.3" class="ltx_text" style="font-size:90%;">, pages 5797–5808, Florence, Italy, July 2019.
Association for Computational Linguistics.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text" style="font-size:90%;">
Apoorv Vyas, Angelos Katharopoulos, and François Fleuret.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.2.1" class="ltx_text" style="font-size:90%;">Fast transformers with clustered attention.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib54.4.2" class="ltx_text" style="font-size:90%;">,
33:21665–21674, 2020.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text" style="font-size:90%;">
Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.2.1" class="ltx_text" style="font-size:90%;">Linformer: Self-attention with linear complexity.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2006.04768</span><span id="bib.bib55.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text" style="font-size:90%;">
Wenxiao Wang, Wei Chen, Qibo Qiu, Long Chen, Boxi Wu, Binbin Lin, Xiaofei He,
and Wei Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.2.1" class="ltx_text" style="font-size:90%;">Crossformer++: A versatile vision transformer hinging on cross-scale
attention.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2303.06908</span><span id="bib.bib56.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text" style="font-size:90%;">
Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong
Lu, Ping Luo, and Ling Shao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.2.1" class="ltx_text" style="font-size:90%;">Pyramid vision transformer: A versatile backbone for dense prediction
without convolutions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib57.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. Conf. Comput. Vis. (ICCV)</span><span id="bib.bib57.5.3" class="ltx_text" style="font-size:90%;">, pages 568–578, 2021.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock"><span id="bib.bib58.1.1" class="ltx_text" style="font-size:90%;">
Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong
Lu, Ping Luo, and Ling Shao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.2.1" class="ltx_text" style="font-size:90%;">Pvt v2: Improved baselines with pyramid vision transformer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Computational Visual Media</span><span id="bib.bib58.4.2" class="ltx_text" style="font-size:90%;">, 8(3):415–424, 2022.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock"><span id="bib.bib59.1.1" class="ltx_text" style="font-size:90%;">
Wenxiao Wang, Lulian Yao, Long Chen, Binbin Lin, Deng Cai, Xiaofei He, and Wei
Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.2.1" class="ltx_text" style="font-size:90%;">Crossformer: A versatile vision transformer hinging on cross-scale
attention.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib59.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. Conf. Learn. Representations (ICLR)</span><span id="bib.bib59.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock"><span id="bib.bib60.1.1" class="ltx_text" style="font-size:90%;">
Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping
Luo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.2.1" class="ltx_text" style="font-size:90%;">Segformer: Simple and efficient design for semantic segmentation with
transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib60.4.2" class="ltx_text" style="font-size:90%;">,
34:12077–12090, 2021.
</span>
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock"><span id="bib.bib61.1.1" class="ltx_text" style="font-size:90%;">
Yifan Xu, Zhijie Zhang, Mengdan Zhang, Kekai Sheng, Ke Li, Weiming Dong, Liqing
Zhang, Changsheng Xu, and Xing Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.2.1" class="ltx_text" style="font-size:90%;">Evo-vit: Slow-fast token evolution for dynamic vision transformer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib61.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">AAAI Conf. Artificial Intell.</span><span id="bib.bib61.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock"><span id="bib.bib62.1.1" class="ltx_text" style="font-size:90%;">
Botao Ye, Hong Chang, Bingpeng Ma, Shiguang Shan, and Xilin Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.2.1" class="ltx_text" style="font-size:90%;">Joint feature learning and relation modeling for tracking: A
one-stream framework.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib62.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Eur. Conf. Comput. Vis. (ECCV)</span><span id="bib.bib62.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock"><span id="bib.bib63.1.1" class="ltx_text" style="font-size:90%;">
Hongxu Yin, Arash Vahdat, Jose M. Alvarez, Arun Mallya, Jan Kautz, and Pavlo
Molchanov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.2.1" class="ltx_text" style="font-size:90%;">A-ViT: Adaptive tokens for efficient vision transformer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib63.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</span><span id="bib.bib63.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock"><span id="bib.bib64.1.1" class="ltx_text" style="font-size:90%;">
Xumin Yu, Yongming Rao, Ziyi Wang, Zuyan Liu, Jiwen Lu, and Jie Zhou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.2.1" class="ltx_text" style="font-size:90%;">Pointr: Diverse point cloud completion with geometry-aware
transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib64.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. Conf. Comput. Vis. (ICCV)</span><span id="bib.bib64.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock"><span id="bib.bib65.1.1" class="ltx_text" style="font-size:90%;">
Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu, and Wei Wu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.2.1" class="ltx_text" style="font-size:90%;">Incorporating convolution designs into visual transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib65.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. Conf. Comput. Vis. (ICCV)</span><span id="bib.bib65.5.3" class="ltx_text" style="font-size:90%;">, pages 579–588, 2021.
</span>
</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock"><span id="bib.bib66.1.1" class="ltx_text" style="font-size:90%;">
Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang,
Francis EH Tay, Jiashi Feng, and Shuicheng Yan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.2.1" class="ltx_text" style="font-size:90%;">Tokens-to-token vit: Training vision transformers from scratch on
imagenet.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib66.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. Conf. Comput. Vis. (ICCV)</span><span id="bib.bib66.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock"><span id="bib.bib67.1.1" class="ltx_text" style="font-size:90%;">
Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.2.1" class="ltx_text" style="font-size:90%;">Q8bert: Quantized 8bit bert.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib67.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2019 Fifth Workshop on Energy Efficient Machine Learning and
Cognitive Computing-NeurIPS Edition (EMC2-NIPS)</span><span id="bib.bib67.5.3" class="ltx_text" style="font-size:90%;">, pages 36–39. IEEE, 2019.
</span>
</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock"><span id="bib.bib68.1.1" class="ltx_text" style="font-size:90%;">
Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris
Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang,
et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.2.1" class="ltx_text" style="font-size:90%;">Big bird: Transformers for longer sequences.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib68.4.2" class="ltx_text" style="font-size:90%;">,
33:17283–17297, 2020.
</span>
</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock"><span id="bib.bib69.1.1" class="ltx_text" style="font-size:90%;">
Jinnian Zhang, Houwen Peng, Kan Wu, Mengchen Liu, Bin Xiao, Jianlong Fu, and Lu
Yuan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.2.1" class="ltx_text" style="font-size:90%;">Minivit: Compressing vision transformers with weight multiplexing.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib69.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. Conf. Comput. Vis. (ICCV)</span><span id="bib.bib69.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock"><span id="bib.bib70.1.1" class="ltx_text" style="font-size:90%;">
Qinglong Zhang and Yu-Bin Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.2.1" class="ltx_text" style="font-size:90%;">Rest: An efficient transformer for visual recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib70.4.2" class="ltx_text" style="font-size:90%;">,
34:15475–15485, 2021.
</span>
</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock"><span id="bib.bib71.1.1" class="ltx_text" style="font-size:90%;">
Zizhao Zhang, Han Zhang, Long Zhao, Ting Chen, and Tomas Pfister.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.2.1" class="ltx_text" style="font-size:90%;">Aggregating nested transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2105.12723</span><span id="bib.bib71.4.2" class="ltx_text" style="font-size:90%;">, 2(3):5, 2021.
</span>
</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock"><span id="bib.bib72.1.1" class="ltx_text" style="font-size:90%;">
Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and Vladlen Koltun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.2.1" class="ltx_text" style="font-size:90%;">Point transformer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib72.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. Conf. Comput. Vis. (ICCV)</span><span id="bib.bib72.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock"><span id="bib.bib73.1.1" class="ltx_text" style="font-size:90%;">
Ce Zheng, Sijie Zhu, Matias Mendieta, Taojiannan Yang, Chen Chen, and Zhengming
Ding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.2.1" class="ltx_text" style="font-size:90%;">3d human pose estimation with spatial and temporal transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib73.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. Conf. Comput. Vis. (ICCV)</span><span id="bib.bib73.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock"><span id="bib.bib74.1.1" class="ltx_text" style="font-size:90%;">
Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang,
Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib74.2.1" class="ltx_text" style="font-size:90%;">Rethinking semantic segmentation from a sequence-to-sequence
perspective with transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib74.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib74.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</span><span id="bib.bib74.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock"><span id="bib.bib75.1.1" class="ltx_text" style="font-size:90%;">
Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib75.2.1" class="ltx_text" style="font-size:90%;">Deformable detr: Deformable transformers for end-to-end object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib75.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2010.04159</span><span id="bib.bib75.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2306.07049" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2306.07050" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2306.07050">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2306.07050" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2306.07051" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 00:56:06 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
