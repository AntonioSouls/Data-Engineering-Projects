<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2303.09252] GridCLIP: One-Stage Object Detection by Grid-Level CLIP Representation Learning</title><meta property="og:description" content="A vision-language foundation model pretrained on very large-scale image-text paired data
has the potential to provide generalizable knowledge representation for
downstream visual recognition and detection tasks, especiâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="GridCLIP: One-Stage Object Detection by Grid-Level CLIP Representation Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="GridCLIP: One-Stage Object Detection by Grid-Level CLIP Representation Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2303.09252">

<!--Generated on Thu Feb 29 19:27:16 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">GridCLIP: One-Stage Object Detection
by Grid-Level CLIP Representation Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jiayi Lin
<br class="ltx_break">Queen Mary University of London
<br class="ltx_break"><span id="id3.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">jiayi.lin@qmul.ac.uk</span>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shaogang Gong
<br class="ltx_break">Queen Mary University of London
<br class="ltx_break"><span id="id4.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">s.gong@qmul.ac.uk</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.2" class="ltx_p">A vision-language foundation model pretrained on very large-scale image-text paired data
has the potential to provide generalizable knowledge representation for
downstream visual recognition and detection tasks, especially on
supplementing the undersampled categories in downstream model training.
Recent studies utilizing
CLIP for object detection have shown
that a two-stage detector design typically
outperforms a one-stage detector,
while requiring more expensive
training resources and longer inference time.
In this work, we propose a one-stage detector <span id="id2.2.1" class="ltx_text" style="color:#000000;">GridCLIP</span>
<span id="id2.2.2" class="ltx_text" style="color:#000000;">that narrows its</span> <span id="id2.2.3" class="ltx_text" style="color:#000000;">performance</span> gap <span id="id2.2.4" class="ltx_text" style="color:#000000;">to those of</span> two-stage detectors,
with approximately 43<math id="id1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="id1.1.m1.1a"><mo id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><times id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">\times</annotation></semantics></math> and 5<math id="id2.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="id2.2.m2.1a"><mo id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><times id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">\times</annotation></semantics></math> faster than its two-stage counterpart (ViLD) in the training and test process respectively.
<span id="id2.2.5" class="ltx_text" style="color:#000000;">GridCLIP learns grid-level representations
to adapt to the intrinsic principle of one-stage detection learning
by expanding</span>
the conventional CLIP image-text holistic mapping
to a more fine-grained, grid-text alignment.
<span id="id2.2.6" class="ltx_text" style="color:#000000;">This differs</span>
<span id="id2.2.7" class="ltx_text" style="color:#000000;">from the region-text mapping
in two-stage detectors that apply CLIP directly by
treating regions as images.</span>
Specifically, GridCLIP <span id="id2.2.8" class="ltx_text" style="color:#000000;">performs Grid-level Alignment to adapt</span> the CLIP image-level representations
to grid-level representations
<span id="id2.2.9" class="ltx_text" style="color:#000000;">by aligning</span> to CLIP category representations to learn the annotated (especially frequent) categories.
To learn generalizable visual representations of broader
categories, especially undersampled ones,
we perform Image-level Alignment during training to
<span id="id2.2.10" class="ltx_text" style="color:#000000;">propagate broad pre-learned categories in the CLIP image encoder from
the image-level to the grid-level representations</span>.
Experiments show that the learned CLIP-based grid-level representations boost the performance of undersampled (infrequent and novel) categories,
reaching <span id="id2.2.11" class="ltx_text" style="color:#000000;">comparable</span> detection performance on the LVIS benchmark.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure"><img src="/html/2303.09252/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="217" height="192" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span> Comparison of approaches to applying visual pretrained and vision-language pretrained models (GridCLIP as an example)
to one-stage detectors.
The visual pretrained model is commonly used for extracting grid-level image representations (also called feature maps) which are aligned to the manual one-hot embeddings, so only base categories can be learned (top). While in a vision-language pretrained model, images are encoded into high-dimension embeddings, which can be aligned to the text embeddings of base categories as well as the whole image embedding of both base and novel categories (bottom).
</figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Simultaneous multi-category object detection aims to
both recognize (classify) and detect (locate) all instances of given
categories in an image.
A significant challenge in training a good detector
is the cost of labeling a large-scale dataset on a broad
range of object categories with balanced data distributions. Existing
detection datasets are often
imbalanced with a long-tail distribution across
categoriesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> where some object categories have only
a few or zero training sample(s).
To deal with these undersampled categories,
few-shot and zero-shot learning have been explored, but
they are inherently weaker models when compared to a fully supervised
learning based model.
</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p"><span id="S1.p2.1.1" class="ltx_text" style="color:#000000;">Elsewhere, Self-Supervised Learning (SSL) has received increasing research interest for</span> exploring widely available unlabelled data.
Self-supervised pretraining followed by supervised
fine-tuning for constructing a detector has been proposed recentlyÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>.
An example is Open Vocabulary Object Detection (OVOD)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>,
which pretrains a model on image-caption pairs containing a
substantial amount of broad categories,
followed by fine-tuning the model on a detection
specific dataset of only a few base categories.
For the pretraining stage that supplements knowledge for undersampled categories, Vision-Language pretrained Models (VLMs) are widely adopted.
As one of the most widely adopted VLMs, CLIPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> is pretrained
on a dataset of 400 million image-text pairs with a vocabulary of over 49,000 words, providing generalizable visual embeddings
of broad categories
that helps supplement the undersampled categories in a detection dataset.
</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.2" class="ltx_p">Recent approaches have been exploring the CLIP-based representation for object detection, mostly for Open-Vocabulary Object Detection (OVOD)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>.
These detectors can be broadly considered as two-stage detectorsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>
and one-stage detectorsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>.
Although a one-stage detector is inherently simpler and
less costly to compute, it suffers from poorer performance than
that of a two-stage detector using CLIPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>.
In this work, we propose a CLIP-based one-stage detector GridLCIP that narrows the
performance gap from typical two-stage detectors,
while requires a much shorter training time (43<math id="S1.p3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.p3.1.m1.1a"><mo id="S1.p3.1.m1.1.1" xref="S1.p3.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b"><times id="S1.p3.1.m1.1.1.cmml" xref="S1.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.1.m1.1c">\times</annotation></semantics></math> less compared to ViLDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>) and test time (5<math id="S1.p3.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.p3.2.m2.1a"><mo id="S1.p3.2.m2.1.1" xref="S1.p3.2.m2.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S1.p3.2.m2.1b"><times id="S1.p3.2.m2.1.1.cmml" xref="S1.p3.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.2.m2.1c">\times</annotation></semantics></math> faster) .</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Specifically, we <span id="S1.p4.1.1" class="ltx_text" style="color:#000000;">learn</span>
grid-level representations of images that can be further used for classification, since an object in one-stage detectors is noted by the category of a grid (a pixel in a feature map) and its corresponding bounding box.
Therefore, we expand the
conventional CLIP image-text holistic mapping to grid-text
mapping, namely <em id="S1.p4.1.2" class="ltx_emph ltx_font_italic">Grid-level Alignment</em>.
Although some other one-stage detectorsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>
also share similar spirits,
they align the image embeddings trained
from supervised or visual data only self-supervised learning methods
(rather than CLIP-like visual-language pretrained models) to align with CLIP text embeddings.
While we directly adapt the CLIP image embeddings to generate grid-level embeddings,
which directly benefits from the generalizability of the CLIP image encoder
and is intrinsically more consistent with CLIP text embeddings.
However, similar to DenseCLIPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>
which can only perform close-set detection,
grid-level alignment is only performed on the base categories
without the scope to learn knowledge of novel (unseen) categories
for open-set detection.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">To further exploit CLIP to learn representation for novel categories,
some approachesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> used extra
image-caption or labeled datasets like
CC3MÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> or
ImageNet-21KÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>,
making the training process complicated and resource-consuming.
In this work, we want to explore CLIP directly without the need
of extra image-caption
and/or labelled training data in order to
learn novel categories by applying knowledge distillation on
CLIP.
As revealed in HierKDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>,
the gap between one-stage and two-stage detectors is that the knowledge distillation on
two-stage detectors happen on image regions of both base and novel categoriesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>
given by a separate pretrained region proposal network (RPN), hence
two-stage, while the one-stage detectors mainly only use base categories for knowledge distillationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>.
Therefore, HierKD aligns the text embedding of the caption
to its paired image embedding,
and further
uses an attention layer for adapting the gap between captions and images.
However, HierKD requires the training images to have paired
captions in addition to detection annotations, making it
unscalable to training on other detection datasets for more general detection
tasks.
To avoid all these additional requirements on model training, we
propose to use visual-to-visual alignment instead of
caption-to-visual alignment. This is designed
to learn the visual embeddings of both base and novel categories
without the need for further adaptation.
We call it <em id="S1.p5.1.1" class="ltx_emph ltx_font_italic">Image-level Alignment</em>.
Specifically, we align the image-level embeddings with the one that is generated by a fixed CLIP image encoder (teacher).
Since the feature extractor of image-level embeddings and grid-level embeddings are mainly shared, the grid-level embeddings can implicitly obtain knowledge
of undersampled categories
from the teacher.
In this way, we <span id="S1.p5.1.2" class="ltx_text" style="color:#000000;">implicitly train</span> the grid-level embeddings
to align to both base and novel categories in CLIP space.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Overall, we propose a one-stage <span id="S1.p6.1.1" class="ltx_text" style="color:#000000;">detector</span>
GridCLIP,
which exploits CLIP to supplement the knowledge of undersampled categories in downstream detection datasets by simultaneously applying
grid-level and image-level alignments.
Our contributions are:
(1) We exploit CLIP to
supplement the missing knowledge of undersampled object
detection categories in training a <span id="S1.p6.1.2" class="ltx_text" style="color:#000000;">one-stage</span> detector, mitigating
the poor performance due to the long-tail data distribution in most
existing detection training data.
(2) We propose a simple yet effective visual-to-visual knowledge
distillation method for learning novel categories in
constructing a one-stage CLIP-based detector, providing
2.4 AP gains on novel categories compared to the baseline.
(3)
<span id="S1.p6.1.3" class="ltx_text" style="color:#000000;">GridCLIP is</span> capable of
handling Open-Vocabulary Object Detection with considerable
scalability and generalizability,
reaching the
comparable performance to two-stage detectors in Open-Vocabulary Object Detection with much higher training and inference speed,
without using extra pretraining processes or additional
fine-tuning datasets.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Vision-Language Pretrained Model (VLM).</span>
Visual-only pretrained model has dominated the pretraining process
for several years until VLMs have appeared.
In comparison,
Vision-Language Pretrained Models (VLMs) are able to
align more visual concepts out of manual predefined categories
to natural language, extending the generality of the model.
Recently,
a considerable number of vision-language pretrained modelsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> emerge,
training from large-scale image-text data
in an unsupervised way.
These models usually have both image and text encoders to generate corresponding features that can be aligned in a cross-modality representational space for corresponding image-text matching.
Utilizing these alignment spaces helps
zero-shot transfer
to a wide range of downstream visual recognition tasks, such as
object detectionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>,
segmentationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>,
image retrievalÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.
As one widely-used instance,
CLIPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> is trained on 400 million image-text pairs
through a contrastive objective, which extends significantly the
generalizability and usability of
the learned image
embeddings to
align to broad categories,
showing competitive performance
with its fully supervised counterparts.
CLIP is widely applied
both in downstream tasks oriented pretrainingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>
and in fine-tuning for downstream tasksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Object Detection using VLM.</span>
OVR-CNNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> is the first to utilize natural language (captions) for object detection.
<span id="S2.p2.1.2" class="ltx_text" style="color:#000000;">While recent detectors apply large-scale image-text datasets
to learn generalizable image representations.
Some VLMs like GLIPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> and DetCLIPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> utilize large-scale annotation datasets in addition to image-text pairs for pertaining, which requires relatively high annotation cost.
While we focus on utilizing unsupervised VLMs like CLIP and learn from annotation datasets of a limited number of categories to transfer to broader categories.
</span></p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">To learn knowledge of base categories,
most detectorsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> replace the classifiers of their detection heads with VLM text embeddings.
Recent detectors mainly improve the learning in two aspects: learning better text embeddings of categories and extracting image embeddings to align with these text embeddings.
(1) For generating better text embeddings,
also called <em id="S2.p3.1.1" class="ltx_emph ltx_font_italic">Prompt Learning</em>,
current approaches can be roughly classified as template-based and learnable prompting.
The template-based one uses fixed incomplete sentences
that can accept labels to build complete sentencesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>,
while the learnable prompting methods concatenate learnable parameters with category labels as the input,
where the prompt is implicitly learned during fine-tuningÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.
GridCLIP uses template-based prompting as in the original CLIP and some OVOD detectorsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> for simplicity and scalability.
(2) For extracting image embeddings to align with text embeddings, two-stage
detectorsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> use the embedding of cropped object bounding-box proposals.
However, they need to train a region proposal network first and require multiple inferences of the CLIP image encoder to compute the visual embedding for each region,
which is relatively inefficient.
In comparison, a one-stage
detectorÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> aligns parts
in an image
represented by grid-level embeddings.
DenseCLIPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>
adapt it as
aligning grid-level image features with text,
while can only perform under close-set settings.
While HierKDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> performs image-level, region-level and grid-level alignment to CLIP,
which however requires the match captions with the detection dataset,
limiting its deployment to other downstream detection datasets.
Our method uses grid-level alignment for efficiency
while preserving the original alignment space of CLIP to gain better generalization ability,
without requiring extra datasets.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">To learn knowledge of novel categories, some approachesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> use extract knowledge from external datasets extra image-caption or labeled datasets like CC3MÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> or ImageNet-21KÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>,
making the training process complicated and resource-consuming.
While we argue that CLIP has been trained over a broad vocabulary and
has the ability to provide visual embeddings of various categories.
Therefore, we explore the original CLIP representation space
to learn novel categories by applying knowledge distillation on CLIP as in ViLDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2303.09252/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="484" height="173" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The pipeline of our proposed GridCLIP. GridCLIP aligns to the CLIP representation in both image and grid levels. In image-level alignment, the image-level feature is aligned to the feature generated by a fixed CLIP image encoder. In grid-level alignment, the grid-level feature is aligned to the classification target generated from the ground truth labels and bounding boxes following the detector FCOSÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. Note that the grid-level alignment is performed in multiple scales, while only one scale is presented here for simplicity. </figcaption>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Approach</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The overall model design of GridCLIP is shown in
FigureÂ <a href="#S2.F2" title="Figure 2 â€£ 2 Related Works â€£ GridCLIP: One-Stage Object Detection by Grid-Level CLIP Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
We first introduce
the strategy of adopting the CLIP embeddings for a detection task, and
then present the approach to mapping simultaneously the CLIP
representation by both grid-level and image-level alignments
based on a one-stage detector FCOSÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Adapting CLIP for Detection</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">CLIP consists of an image encoder (ResNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> or
ViTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>)
and a text encoder (TransformerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>),
which together form
the alignment space of visual and language embeddings.
However, as the image
embedding in CLIP is a high-dimensional feature vector of an entire image
instead of a spatial region or pixels,
and the text embedding is encoded from a
sentence instead of a single category label in detection,
further adaptation for the detection task is needed.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.8" class="ltx_p"><span id="S3.SS1.p2.8.1" class="ltx_text ltx_font_bold">Generating Image Embedding.</span>
The original CLIP image feature <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="{\overline{z}}" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mover accent="true" id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><mi id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml">z</mi><mo id="S3.SS1.p2.1.m1.1.1.1" xref="S3.SS1.p2.1.m1.1.1.1.cmml">Â¯</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><ci id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1.1">Â¯</ci><ci id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2">ğ‘§</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">{\overline{z}}</annotation></semantics></math> is a single
high-dimensional feature vector representing an entire image
without spatial information.
To get the grid-level feature <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="{z}" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">{z}</annotation></semantics></math> , inspired by DenseCLIPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, we use the other
feature from the last layer
of the CLIP image encoder.
Specifically,
taking the ResNet50 encoder as an example, the final output feature in the 5-th stage <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="{C}_{5}\in\mathbf{R}^{H_{5}\times W_{5}\times D_{5}}" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><mrow id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml"><msub id="S3.SS1.p2.3.m3.1.1.2" xref="S3.SS1.p2.3.m3.1.1.2.cmml"><mi id="S3.SS1.p2.3.m3.1.1.2.2" xref="S3.SS1.p2.3.m3.1.1.2.2.cmml">C</mi><mn id="S3.SS1.p2.3.m3.1.1.2.3" xref="S3.SS1.p2.3.m3.1.1.2.3.cmml">5</mn></msub><mo id="S3.SS1.p2.3.m3.1.1.1" xref="S3.SS1.p2.3.m3.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS1.p2.3.m3.1.1.3" xref="S3.SS1.p2.3.m3.1.1.3.cmml"><mi id="S3.SS1.p2.3.m3.1.1.3.2" xref="S3.SS1.p2.3.m3.1.1.3.2.cmml">ğ‘</mi><mrow id="S3.SS1.p2.3.m3.1.1.3.3" xref="S3.SS1.p2.3.m3.1.1.3.3.cmml"><msub id="S3.SS1.p2.3.m3.1.1.3.3.2" xref="S3.SS1.p2.3.m3.1.1.3.3.2.cmml"><mi id="S3.SS1.p2.3.m3.1.1.3.3.2.2" xref="S3.SS1.p2.3.m3.1.1.3.3.2.2.cmml">H</mi><mn id="S3.SS1.p2.3.m3.1.1.3.3.2.3" xref="S3.SS1.p2.3.m3.1.1.3.3.2.3.cmml">5</mn></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.3.m3.1.1.3.3.1" xref="S3.SS1.p2.3.m3.1.1.3.3.1.cmml">Ã—</mo><msub id="S3.SS1.p2.3.m3.1.1.3.3.3" xref="S3.SS1.p2.3.m3.1.1.3.3.3.cmml"><mi id="S3.SS1.p2.3.m3.1.1.3.3.3.2" xref="S3.SS1.p2.3.m3.1.1.3.3.3.2.cmml">W</mi><mn id="S3.SS1.p2.3.m3.1.1.3.3.3.3" xref="S3.SS1.p2.3.m3.1.1.3.3.3.3.cmml">5</mn></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.3.m3.1.1.3.3.1a" xref="S3.SS1.p2.3.m3.1.1.3.3.1.cmml">Ã—</mo><msub id="S3.SS1.p2.3.m3.1.1.3.3.4" xref="S3.SS1.p2.3.m3.1.1.3.3.4.cmml"><mi id="S3.SS1.p2.3.m3.1.1.3.3.4.2" xref="S3.SS1.p2.3.m3.1.1.3.3.4.2.cmml">D</mi><mn id="S3.SS1.p2.3.m3.1.1.3.3.4.3" xref="S3.SS1.p2.3.m3.1.1.3.3.4.3.cmml">5</mn></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><apply id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1"><in id="S3.SS1.p2.3.m3.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1.1"></in><apply id="S3.SS1.p2.3.m3.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.1.1.2.1.cmml" xref="S3.SS1.p2.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS1.p2.3.m3.1.1.2.2.cmml" xref="S3.SS1.p2.3.m3.1.1.2.2">ğ¶</ci><cn type="integer" id="S3.SS1.p2.3.m3.1.1.2.3.cmml" xref="S3.SS1.p2.3.m3.1.1.2.3">5</cn></apply><apply id="S3.SS1.p2.3.m3.1.1.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.1.1.3.1.cmml" xref="S3.SS1.p2.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.3.m3.1.1.3.2.cmml" xref="S3.SS1.p2.3.m3.1.1.3.2">ğ‘</ci><apply id="S3.SS1.p2.3.m3.1.1.3.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3.3"><times id="S3.SS1.p2.3.m3.1.1.3.3.1.cmml" xref="S3.SS1.p2.3.m3.1.1.3.3.1"></times><apply id="S3.SS1.p2.3.m3.1.1.3.3.2.cmml" xref="S3.SS1.p2.3.m3.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.1.1.3.3.2.1.cmml" xref="S3.SS1.p2.3.m3.1.1.3.3.2">subscript</csymbol><ci id="S3.SS1.p2.3.m3.1.1.3.3.2.2.cmml" xref="S3.SS1.p2.3.m3.1.1.3.3.2.2">ğ»</ci><cn type="integer" id="S3.SS1.p2.3.m3.1.1.3.3.2.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3.3.2.3">5</cn></apply><apply id="S3.SS1.p2.3.m3.1.1.3.3.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.1.1.3.3.3.1.cmml" xref="S3.SS1.p2.3.m3.1.1.3.3.3">subscript</csymbol><ci id="S3.SS1.p2.3.m3.1.1.3.3.3.2.cmml" xref="S3.SS1.p2.3.m3.1.1.3.3.3.2">ğ‘Š</ci><cn type="integer" id="S3.SS1.p2.3.m3.1.1.3.3.3.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3.3.3.3">5</cn></apply><apply id="S3.SS1.p2.3.m3.1.1.3.3.4.cmml" xref="S3.SS1.p2.3.m3.1.1.3.3.4"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.1.1.3.3.4.1.cmml" xref="S3.SS1.p2.3.m3.1.1.3.3.4">subscript</csymbol><ci id="S3.SS1.p2.3.m3.1.1.3.3.4.2.cmml" xref="S3.SS1.p2.3.m3.1.1.3.3.4.2">ğ·</ci><cn type="integer" id="S3.SS1.p2.3.m3.1.1.3.3.4.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3.3.4.3">5</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">{C}_{5}\in\mathbf{R}^{H_{5}\times W_{5}\times D_{5}}</annotation></semantics></math> is
first performed global average pooling to
get the image-level feature <math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="\overline{{C}}_{5}\in\mathbf{R}^{1\times 1\times D_{5}}" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><mrow id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml"><msub id="S3.SS1.p2.4.m4.1.1.2" xref="S3.SS1.p2.4.m4.1.1.2.cmml"><mover accent="true" id="S3.SS1.p2.4.m4.1.1.2.2" xref="S3.SS1.p2.4.m4.1.1.2.2.cmml"><mi id="S3.SS1.p2.4.m4.1.1.2.2.2" xref="S3.SS1.p2.4.m4.1.1.2.2.2.cmml">C</mi><mo id="S3.SS1.p2.4.m4.1.1.2.2.1" xref="S3.SS1.p2.4.m4.1.1.2.2.1.cmml">Â¯</mo></mover><mn id="S3.SS1.p2.4.m4.1.1.2.3" xref="S3.SS1.p2.4.m4.1.1.2.3.cmml">5</mn></msub><mo id="S3.SS1.p2.4.m4.1.1.1" xref="S3.SS1.p2.4.m4.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS1.p2.4.m4.1.1.3" xref="S3.SS1.p2.4.m4.1.1.3.cmml"><mi id="S3.SS1.p2.4.m4.1.1.3.2" xref="S3.SS1.p2.4.m4.1.1.3.2.cmml">ğ‘</mi><mrow id="S3.SS1.p2.4.m4.1.1.3.3" xref="S3.SS1.p2.4.m4.1.1.3.3.cmml"><mn id="S3.SS1.p2.4.m4.1.1.3.3.2" xref="S3.SS1.p2.4.m4.1.1.3.3.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.4.m4.1.1.3.3.1" xref="S3.SS1.p2.4.m4.1.1.3.3.1.cmml">Ã—</mo><mn id="S3.SS1.p2.4.m4.1.1.3.3.3" xref="S3.SS1.p2.4.m4.1.1.3.3.3.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.4.m4.1.1.3.3.1a" xref="S3.SS1.p2.4.m4.1.1.3.3.1.cmml">Ã—</mo><msub id="S3.SS1.p2.4.m4.1.1.3.3.4" xref="S3.SS1.p2.4.m4.1.1.3.3.4.cmml"><mi id="S3.SS1.p2.4.m4.1.1.3.3.4.2" xref="S3.SS1.p2.4.m4.1.1.3.3.4.2.cmml">D</mi><mn id="S3.SS1.p2.4.m4.1.1.3.3.4.3" xref="S3.SS1.p2.4.m4.1.1.3.3.4.3.cmml">5</mn></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><apply id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1"><in id="S3.SS1.p2.4.m4.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1.1"></in><apply id="S3.SS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.2.1.cmml" xref="S3.SS1.p2.4.m4.1.1.2">subscript</csymbol><apply id="S3.SS1.p2.4.m4.1.1.2.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2.2"><ci id="S3.SS1.p2.4.m4.1.1.2.2.1.cmml" xref="S3.SS1.p2.4.m4.1.1.2.2.1">Â¯</ci><ci id="S3.SS1.p2.4.m4.1.1.2.2.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2.2.2">ğ¶</ci></apply><cn type="integer" id="S3.SS1.p2.4.m4.1.1.2.3.cmml" xref="S3.SS1.p2.4.m4.1.1.2.3">5</cn></apply><apply id="S3.SS1.p2.4.m4.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.3.1.cmml" xref="S3.SS1.p2.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.4.m4.1.1.3.2.cmml" xref="S3.SS1.p2.4.m4.1.1.3.2">ğ‘</ci><apply id="S3.SS1.p2.4.m4.1.1.3.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3"><times id="S3.SS1.p2.4.m4.1.1.3.3.1.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.1"></times><cn type="integer" id="S3.SS1.p2.4.m4.1.1.3.3.2.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.2">1</cn><cn type="integer" id="S3.SS1.p2.4.m4.1.1.3.3.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.3">1</cn><apply id="S3.SS1.p2.4.m4.1.1.3.3.4.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.4"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.3.3.4.1.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.4">subscript</csymbol><ci id="S3.SS1.p2.4.m4.1.1.3.3.4.2.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.4.2">ğ·</ci><cn type="integer" id="S3.SS1.p2.4.m4.1.1.3.3.4.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.4.3">5</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">\overline{{C}}_{5}\in\mathbf{R}^{1\times 1\times D_{5}}</annotation></semantics></math> ,
where <math id="S3.SS1.p2.5.m5.1" class="ltx_Math" alttext="H_{5}" display="inline"><semantics id="S3.SS1.p2.5.m5.1a"><msub id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml"><mi id="S3.SS1.p2.5.m5.1.1.2" xref="S3.SS1.p2.5.m5.1.1.2.cmml">H</mi><mn id="S3.SS1.p2.5.m5.1.1.3" xref="S3.SS1.p2.5.m5.1.1.3.cmml">5</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><apply id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.1.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p2.5.m5.1.1.2.cmml" xref="S3.SS1.p2.5.m5.1.1.2">ğ»</ci><cn type="integer" id="S3.SS1.p2.5.m5.1.1.3.cmml" xref="S3.SS1.p2.5.m5.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">H_{5}</annotation></semantics></math>, <math id="S3.SS1.p2.6.m6.1" class="ltx_Math" alttext="W_{5}" display="inline"><semantics id="S3.SS1.p2.6.m6.1a"><msub id="S3.SS1.p2.6.m6.1.1" xref="S3.SS1.p2.6.m6.1.1.cmml"><mi id="S3.SS1.p2.6.m6.1.1.2" xref="S3.SS1.p2.6.m6.1.1.2.cmml">W</mi><mn id="S3.SS1.p2.6.m6.1.1.3" xref="S3.SS1.p2.6.m6.1.1.3.cmml">5</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m6.1b"><apply id="S3.SS1.p2.6.m6.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.1.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.p2.6.m6.1.1.2.cmml" xref="S3.SS1.p2.6.m6.1.1.2">ğ‘Š</ci><cn type="integer" id="S3.SS1.p2.6.m6.1.1.3.cmml" xref="S3.SS1.p2.6.m6.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.1c">W_{5}</annotation></semantics></math>, <math id="S3.SS1.p2.7.m7.1" class="ltx_Math" alttext="D_{5}" display="inline"><semantics id="S3.SS1.p2.7.m7.1a"><msub id="S3.SS1.p2.7.m7.1.1" xref="S3.SS1.p2.7.m7.1.1.cmml"><mi id="S3.SS1.p2.7.m7.1.1.2" xref="S3.SS1.p2.7.m7.1.1.2.cmml">D</mi><mn id="S3.SS1.p2.7.m7.1.1.3" xref="S3.SS1.p2.7.m7.1.1.3.cmml">5</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m7.1b"><apply id="S3.SS1.p2.7.m7.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.1.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1">subscript</csymbol><ci id="S3.SS1.p2.7.m7.1.1.2.cmml" xref="S3.SS1.p2.7.m7.1.1.2">ğ·</ci><cn type="integer" id="S3.SS1.p2.7.m7.1.1.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m7.1c">D_{5}</annotation></semantics></math> are the height, width and number of channels of the feature in the 5-th stage of the ResNet50.
Then the concatenated features <math id="S3.SS1.p2.8.m8.2" class="ltx_Math" alttext="\left[\overline{{C}}_{5},{C}_{5}\right]" display="inline"><semantics id="S3.SS1.p2.8.m8.2a"><mrow id="S3.SS1.p2.8.m8.2.2.2" xref="S3.SS1.p2.8.m8.2.2.3.cmml"><mo id="S3.SS1.p2.8.m8.2.2.2.3" xref="S3.SS1.p2.8.m8.2.2.3.cmml">[</mo><msub id="S3.SS1.p2.8.m8.1.1.1.1" xref="S3.SS1.p2.8.m8.1.1.1.1.cmml"><mover accent="true" id="S3.SS1.p2.8.m8.1.1.1.1.2" xref="S3.SS1.p2.8.m8.1.1.1.1.2.cmml"><mi id="S3.SS1.p2.8.m8.1.1.1.1.2.2" xref="S3.SS1.p2.8.m8.1.1.1.1.2.2.cmml">C</mi><mo id="S3.SS1.p2.8.m8.1.1.1.1.2.1" xref="S3.SS1.p2.8.m8.1.1.1.1.2.1.cmml">Â¯</mo></mover><mn id="S3.SS1.p2.8.m8.1.1.1.1.3" xref="S3.SS1.p2.8.m8.1.1.1.1.3.cmml">5</mn></msub><mo id="S3.SS1.p2.8.m8.2.2.2.4" xref="S3.SS1.p2.8.m8.2.2.3.cmml">,</mo><msub id="S3.SS1.p2.8.m8.2.2.2.2" xref="S3.SS1.p2.8.m8.2.2.2.2.cmml"><mi id="S3.SS1.p2.8.m8.2.2.2.2.2" xref="S3.SS1.p2.8.m8.2.2.2.2.2.cmml">C</mi><mn id="S3.SS1.p2.8.m8.2.2.2.2.3" xref="S3.SS1.p2.8.m8.2.2.2.2.3.cmml">5</mn></msub><mo id="S3.SS1.p2.8.m8.2.2.2.5" xref="S3.SS1.p2.8.m8.2.2.3.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m8.2b"><interval closure="closed" id="S3.SS1.p2.8.m8.2.2.3.cmml" xref="S3.SS1.p2.8.m8.2.2.2"><apply id="S3.SS1.p2.8.m8.1.1.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.8.m8.1.1.1.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1.1.1">subscript</csymbol><apply id="S3.SS1.p2.8.m8.1.1.1.1.2.cmml" xref="S3.SS1.p2.8.m8.1.1.1.1.2"><ci id="S3.SS1.p2.8.m8.1.1.1.1.2.1.cmml" xref="S3.SS1.p2.8.m8.1.1.1.1.2.1">Â¯</ci><ci id="S3.SS1.p2.8.m8.1.1.1.1.2.2.cmml" xref="S3.SS1.p2.8.m8.1.1.1.1.2.2">ğ¶</ci></apply><cn type="integer" id="S3.SS1.p2.8.m8.1.1.1.1.3.cmml" xref="S3.SS1.p2.8.m8.1.1.1.1.3">5</cn></apply><apply id="S3.SS1.p2.8.m8.2.2.2.2.cmml" xref="S3.SS1.p2.8.m8.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.8.m8.2.2.2.2.1.cmml" xref="S3.SS1.p2.8.m8.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p2.8.m8.2.2.2.2.2.cmml" xref="S3.SS1.p2.8.m8.2.2.2.2.2">ğ¶</ci><cn type="integer" id="S3.SS1.p2.8.m8.2.2.2.2.3.cmml" xref="S3.SS1.p2.8.m8.2.2.2.2.3">5</cn></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m8.2c">\left[\overline{{C}}_{5},{C}_{5}\right]</annotation></semantics></math>
are fed into a Multi-Head Self-Attention (MHSA) layerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> as follows,</p>
<table id="S5.EGx1" class="ltx_equationgroup ltx_eqn_eqnarray ltx_eqn_table">

<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E1.m1.3" class="ltx_Math" alttext="\displaystyle[\overline{{z}},{z}]={\rm{MHSA}}\left(\left[\overline{{C}}_{5},{C}_{5}\right]\right)." display="inline"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3.3.1" xref="S3.E1.m1.3.3.1.1.cmml"><mrow id="S3.E1.m1.3.3.1.1" xref="S3.E1.m1.3.3.1.1.cmml"><mrow id="S3.E1.m1.3.3.1.1.3.2" xref="S3.E1.m1.3.3.1.1.3.1.cmml"><mo stretchy="false" id="S3.E1.m1.3.3.1.1.3.2.1" xref="S3.E1.m1.3.3.1.1.3.1.cmml">[</mo><mover accent="true" id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><mi id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml">z</mi><mo id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml">Â¯</mo></mover><mo id="S3.E1.m1.3.3.1.1.3.2.2" xref="S3.E1.m1.3.3.1.1.3.1.cmml">,</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">z</mi><mo stretchy="false" id="S3.E1.m1.3.3.1.1.3.2.3" xref="S3.E1.m1.3.3.1.1.3.1.cmml">]</mo></mrow><mo id="S3.E1.m1.3.3.1.1.2" xref="S3.E1.m1.3.3.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.3.3.1.1.1" xref="S3.E1.m1.3.3.1.1.1.cmml"><mi id="S3.E1.m1.3.3.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.3.cmml">MHSA</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E1.m1.3.3.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.cmml"><mo id="S3.E1.m1.3.3.1.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.3.3.1.1.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.3.cmml"><mo id="S3.E1.m1.3.3.1.1.1.1.1.1.2.3" xref="S3.E1.m1.3.3.1.1.1.1.1.1.3.cmml">[</mo><msub id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.2.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.2.2.cmml">C</mi><mo id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.2.1" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.2.1.cmml">Â¯</mo></mover><mn id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3.cmml">5</mn></msub><mo id="S3.E1.m1.3.3.1.1.1.1.1.1.2.4" xref="S3.E1.m1.3.3.1.1.1.1.1.1.3.cmml">,</mo><msub id="S3.E1.m1.3.3.1.1.1.1.1.1.2.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.2.2.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.1.1.2.2.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.2.2.2.cmml">C</mi><mn id="S3.E1.m1.3.3.1.1.1.1.1.1.2.2.3" xref="S3.E1.m1.3.3.1.1.1.1.1.1.2.2.3.cmml">5</mn></msub><mo id="S3.E1.m1.3.3.1.1.1.1.1.1.2.5" xref="S3.E1.m1.3.3.1.1.1.1.1.1.3.cmml">]</mo></mrow><mo id="S3.E1.m1.3.3.1.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S3.E1.m1.3.3.1.2" xref="S3.E1.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.3b"><apply id="S3.E1.m1.3.3.1.1.cmml" xref="S3.E1.m1.3.3.1"><eq id="S3.E1.m1.3.3.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.2"></eq><interval closure="closed" id="S3.E1.m1.3.3.1.1.3.1.cmml" xref="S3.E1.m1.3.3.1.1.3.2"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><ci id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1">Â¯</ci><ci id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2">ğ‘§</ci></apply><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">ğ‘§</ci></interval><apply id="S3.E1.m1.3.3.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1"><times id="S3.E1.m1.3.3.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.2"></times><ci id="S3.E1.m1.3.3.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.3">MHSA</ci><interval closure="closed" id="S3.E1.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.2"><apply id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.2"><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.2.1">Â¯</ci><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.2.2">ğ¶</ci></apply><cn type="integer" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3">5</cn></apply><apply id="S3.E1.m1.3.3.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.2.2.2">ğ¶</ci><cn type="integer" id="S3.E1.m1.3.3.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.2.2.3">5</cn></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.3c">\displaystyle[\overline{{z}},{z}]={\rm{MHSA}}\left(\left[\overline{{C}}_{5},{C}_{5}\right]\right).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.6" class="ltx_p">In CLIP, the output with spatial information <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="{z}" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><mi id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><ci id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">{z}</annotation></semantics></math> is dumped and <math id="S3.SS1.p3.2.m2.1" class="ltx_Math" alttext="\overline{{z}}" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><mover accent="true" id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml"><mi id="S3.SS1.p3.2.m2.1.1.2" xref="S3.SS1.p3.2.m2.1.1.2.cmml">z</mi><mo id="S3.SS1.p3.2.m2.1.1.1" xref="S3.SS1.p3.2.m2.1.1.1.cmml">Â¯</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><apply id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1"><ci id="S3.SS1.p3.2.m2.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1.1">Â¯</ci><ci id="S3.SS1.p3.2.m2.1.1.2.cmml" xref="S3.SS1.p3.2.m2.1.1.2">ğ‘§</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">\overline{{z}}</annotation></semantics></math> is used to match with the text embedding.
However, as illustrated in DenseCLIP, since the MHSA is symmetric to each input element, <math id="S3.SS1.p3.3.m3.1" class="ltx_Math" alttext="{z}" display="inline"><semantics id="S3.SS1.p3.3.m3.1a"><mi id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><ci id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">{z}</annotation></semantics></math> may behave similarly
to <math id="S3.SS1.p3.4.m4.1" class="ltx_Math" alttext="\overline{{z}}" display="inline"><semantics id="S3.SS1.p3.4.m4.1a"><mover accent="true" id="S3.SS1.p3.4.m4.1.1" xref="S3.SS1.p3.4.m4.1.1.cmml"><mi id="S3.SS1.p3.4.m4.1.1.2" xref="S3.SS1.p3.4.m4.1.1.2.cmml">z</mi><mo id="S3.SS1.p3.4.m4.1.1.1" xref="S3.SS1.p3.4.m4.1.1.1.cmml">Â¯</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m4.1b"><apply id="S3.SS1.p3.4.m4.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1"><ci id="S3.SS1.p3.4.m4.1.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1.1">Â¯</ci><ci id="S3.SS1.p3.4.m4.1.1.2.cmml" xref="S3.SS1.p3.4.m4.1.1.2">ğ‘§</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m4.1c">\overline{{z}}</annotation></semantics></math>, which aligns well with the text embedding.
Therefore, we adopt both <math id="S3.SS1.p3.5.m5.1" class="ltx_Math" alttext="{z}" display="inline"><semantics id="S3.SS1.p3.5.m5.1a"><mi id="S3.SS1.p3.5.m5.1.1" xref="S3.SS1.p3.5.m5.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.5.m5.1b"><ci id="S3.SS1.p3.5.m5.1.1.cmml" xref="S3.SS1.p3.5.m5.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.5.m5.1c">{z}</annotation></semantics></math> and <math id="S3.SS1.p3.6.m6.1" class="ltx_Math" alttext="\overline{{z}}" display="inline"><semantics id="S3.SS1.p3.6.m6.1a"><mover accent="true" id="S3.SS1.p3.6.m6.1.1" xref="S3.SS1.p3.6.m6.1.1.cmml"><mi id="S3.SS1.p3.6.m6.1.1.2" xref="S3.SS1.p3.6.m6.1.1.2.cmml">z</mi><mo id="S3.SS1.p3.6.m6.1.1.1" xref="S3.SS1.p3.6.m6.1.1.1.cmml">Â¯</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.6.m6.1b"><apply id="S3.SS1.p3.6.m6.1.1.cmml" xref="S3.SS1.p3.6.m6.1.1"><ci id="S3.SS1.p3.6.m6.1.1.1.cmml" xref="S3.SS1.p3.6.m6.1.1.1">Â¯</ci><ci id="S3.SS1.p3.6.m6.1.1.2.cmml" xref="S3.SS1.p3.6.m6.1.1.2">ğ‘§</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.6.m6.1c">\overline{{z}}</annotation></semantics></math> to generate our grid-level and image-level embeddings respectively with a few adaptation layers.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para ltx_noindent">
<p id="S3.SS1.p4.2" class="ltx_p"><span id="S3.SS1.p4.2.1" class="ltx_text ltx_font_bold">From Label to Text Embedding.</span>
In the original CLIP,
to create a dataset classifier from label text,
a set of template-based prompts
like â€œa photo of a {<em id="S3.SS1.p4.2.2" class="ltx_emph ltx_font_italic">object</em>}.â€ are applied,
where <em id="S3.SS1.p4.2.3" class="ltx_emph ltx_font_italic">object</em> is any of the target category names.
Then the multiple prompts for a single label are aggregated.
Although there are several learnable prompting methods for CLIPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> that fine-tune with the downstream tasks,
We follow the template-based one for simplicity and scalability.
We use one template-based variant in ViLDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> designed for object detection.
Here we note the final text embeddings of the categories in the target dataset ( base categories) as
<math id="S3.SS1.p4.1.m1.1" class="ltx_Math" alttext="\left\{T_{k}\right\}_{k=1}^{K}" display="inline"><semantics id="S3.SS1.p4.1.m1.1a"><msubsup id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml"><mrow id="S3.SS1.p4.1.m1.1.1.1.1.1" xref="S3.SS1.p4.1.m1.1.1.1.1.2.cmml"><mo id="S3.SS1.p4.1.m1.1.1.1.1.1.2" xref="S3.SS1.p4.1.m1.1.1.1.1.2.cmml">{</mo><msub id="S3.SS1.p4.1.m1.1.1.1.1.1.1" xref="S3.SS1.p4.1.m1.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p4.1.m1.1.1.1.1.1.1.2" xref="S3.SS1.p4.1.m1.1.1.1.1.1.1.2.cmml">T</mi><mi id="S3.SS1.p4.1.m1.1.1.1.1.1.1.3" xref="S3.SS1.p4.1.m1.1.1.1.1.1.1.3.cmml">k</mi></msub><mo id="S3.SS1.p4.1.m1.1.1.1.1.1.3" xref="S3.SS1.p4.1.m1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS1.p4.1.m1.1.1.1.3" xref="S3.SS1.p4.1.m1.1.1.1.3.cmml"><mi id="S3.SS1.p4.1.m1.1.1.1.3.2" xref="S3.SS1.p4.1.m1.1.1.1.3.2.cmml">k</mi><mo id="S3.SS1.p4.1.m1.1.1.1.3.1" xref="S3.SS1.p4.1.m1.1.1.1.3.1.cmml">=</mo><mn id="S3.SS1.p4.1.m1.1.1.1.3.3" xref="S3.SS1.p4.1.m1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.SS1.p4.1.m1.1.1.3" xref="S3.SS1.p4.1.m1.1.1.3.cmml">K</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><apply id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.1.m1.1.1.2.cmml" xref="S3.SS1.p4.1.m1.1.1">superscript</csymbol><apply id="S3.SS1.p4.1.m1.1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.1.m1.1.1.1.2.cmml" xref="S3.SS1.p4.1.m1.1.1">subscript</csymbol><set id="S3.SS1.p4.1.m1.1.1.1.1.2.cmml" xref="S3.SS1.p4.1.m1.1.1.1.1.1"><apply id="S3.SS1.p4.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p4.1.m1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p4.1.m1.1.1.1.1.1.1.2">ğ‘‡</ci><ci id="S3.SS1.p4.1.m1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p4.1.m1.1.1.1.1.1.1.3">ğ‘˜</ci></apply></set><apply id="S3.SS1.p4.1.m1.1.1.1.3.cmml" xref="S3.SS1.p4.1.m1.1.1.1.3"><eq id="S3.SS1.p4.1.m1.1.1.1.3.1.cmml" xref="S3.SS1.p4.1.m1.1.1.1.3.1"></eq><ci id="S3.SS1.p4.1.m1.1.1.1.3.2.cmml" xref="S3.SS1.p4.1.m1.1.1.1.3.2">ğ‘˜</ci><cn type="integer" id="S3.SS1.p4.1.m1.1.1.1.3.3.cmml" xref="S3.SS1.p4.1.m1.1.1.1.3.3">1</cn></apply></apply><ci id="S3.SS1.p4.1.m1.1.1.3.cmml" xref="S3.SS1.p4.1.m1.1.1.3">ğ¾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">\left\{T_{k}\right\}_{k=1}^{K}</annotation></semantics></math>, where <math id="S3.SS1.p4.2.m2.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS1.p4.2.m2.1a"><mi id="S3.SS1.p4.2.m2.1.1" xref="S3.SS1.p4.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.2.m2.1b"><ci id="S3.SS1.p4.2.m2.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.2.m2.1c">K</annotation></semantics></math> is the number of category.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Grid-level Alignment</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.14" class="ltx_p"><span id="S3.SS2.p1.14.1" class="ltx_text ltx_font_bold">Generating Grid-level Image Embedding.</span> Taking ResNet50 encoder as an example, in FCOSÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, the output feature map <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="{C}_{3}" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><msub id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">C</mi><mn id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">ğ¶</ci><cn type="integer" id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">{C}_{3}</annotation></semantics></math>, <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="{C}_{4}" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><msub id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">C</mi><mn id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml">4</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">ğ¶</ci><cn type="integer" id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">{C}_{4}</annotation></semantics></math>, <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="{C}_{5}" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><msub id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml"><mi id="S3.SS2.p1.3.m3.1.1.2" xref="S3.SS2.p1.3.m3.1.1.2.cmml">C</mi><mn id="S3.SS2.p1.3.m3.1.1.3" xref="S3.SS2.p1.3.m3.1.1.3.cmml">5</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><apply id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2">ğ¶</ci><cn type="integer" id="S3.SS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">{C}_{5}</annotation></semantics></math> of ResNet50 are inputted into FPN,
producing <math id="S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S3.SS2.p1.4.m4.1a"><mn id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><cn type="integer" id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">5</annotation></semantics></math> multi-scale image feature maps <math id="S3.SS2.p1.5.m5.1" class="ltx_Math" alttext="\left\{{P}_{i}\right\}_{i=3}^{7}" display="inline"><semantics id="S3.SS2.p1.5.m5.1a"><msubsup id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml"><mrow id="S3.SS2.p1.5.m5.1.1.1.1.1" xref="S3.SS2.p1.5.m5.1.1.1.1.2.cmml"><mo id="S3.SS2.p1.5.m5.1.1.1.1.1.2" xref="S3.SS2.p1.5.m5.1.1.1.1.2.cmml">{</mo><msub id="S3.SS2.p1.5.m5.1.1.1.1.1.1" xref="S3.SS2.p1.5.m5.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p1.5.m5.1.1.1.1.1.1.2" xref="S3.SS2.p1.5.m5.1.1.1.1.1.1.2.cmml">P</mi><mi id="S3.SS2.p1.5.m5.1.1.1.1.1.1.3" xref="S3.SS2.p1.5.m5.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS2.p1.5.m5.1.1.1.1.1.3" xref="S3.SS2.p1.5.m5.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS2.p1.5.m5.1.1.1.3" xref="S3.SS2.p1.5.m5.1.1.1.3.cmml"><mi id="S3.SS2.p1.5.m5.1.1.1.3.2" xref="S3.SS2.p1.5.m5.1.1.1.3.2.cmml">i</mi><mo id="S3.SS2.p1.5.m5.1.1.1.3.1" xref="S3.SS2.p1.5.m5.1.1.1.3.1.cmml">=</mo><mn id="S3.SS2.p1.5.m5.1.1.1.3.3" xref="S3.SS2.p1.5.m5.1.1.1.3.3.cmml">3</mn></mrow><mn id="S3.SS2.p1.5.m5.1.1.3" xref="S3.SS2.p1.5.m5.1.1.3.cmml">7</mn></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><apply id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m5.1.1.2.cmml" xref="S3.SS2.p1.5.m5.1.1">superscript</csymbol><apply id="S3.SS2.p1.5.m5.1.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m5.1.1.1.2.cmml" xref="S3.SS2.p1.5.m5.1.1">subscript</csymbol><set id="S3.SS2.p1.5.m5.1.1.1.1.2.cmml" xref="S3.SS2.p1.5.m5.1.1.1.1.1"><apply id="S3.SS2.p1.5.m5.1.1.1.1.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m5.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p1.5.m5.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p1.5.m5.1.1.1.1.1.1.2">ğ‘ƒ</ci><ci id="S3.SS2.p1.5.m5.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p1.5.m5.1.1.1.1.1.1.3">ğ‘–</ci></apply></set><apply id="S3.SS2.p1.5.m5.1.1.1.3.cmml" xref="S3.SS2.p1.5.m5.1.1.1.3"><eq id="S3.SS2.p1.5.m5.1.1.1.3.1.cmml" xref="S3.SS2.p1.5.m5.1.1.1.3.1"></eq><ci id="S3.SS2.p1.5.m5.1.1.1.3.2.cmml" xref="S3.SS2.p1.5.m5.1.1.1.3.2">ğ‘–</ci><cn type="integer" id="S3.SS2.p1.5.m5.1.1.1.3.3.cmml" xref="S3.SS2.p1.5.m5.1.1.1.3.3">3</cn></apply></apply><cn type="integer" id="S3.SS2.p1.5.m5.1.1.3.cmml" xref="S3.SS2.p1.5.m5.1.1.3">7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">\left\{{P}_{i}\right\}_{i=3}^{7}</annotation></semantics></math>. In FPN, <math id="S3.SS2.p1.6.m6.1" class="ltx_Math" alttext="{C}_{5}" display="inline"><semantics id="S3.SS2.p1.6.m6.1a"><msub id="S3.SS2.p1.6.m6.1.1" xref="S3.SS2.p1.6.m6.1.1.cmml"><mi id="S3.SS2.p1.6.m6.1.1.2" xref="S3.SS2.p1.6.m6.1.1.2.cmml">C</mi><mn id="S3.SS2.p1.6.m6.1.1.3" xref="S3.SS2.p1.6.m6.1.1.3.cmml">5</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m6.1b"><apply id="S3.SS2.p1.6.m6.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.6.m6.1.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.p1.6.m6.1.1.2.cmml" xref="S3.SS2.p1.6.m6.1.1.2">ğ¶</ci><cn type="integer" id="S3.SS2.p1.6.m6.1.1.3.cmml" xref="S3.SS2.p1.6.m6.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m6.1c">{C}_{5}</annotation></semantics></math> is fused with <math id="S3.SS2.p1.7.m7.1" class="ltx_Math" alttext="{C}_{3}" display="inline"><semantics id="S3.SS2.p1.7.m7.1a"><msub id="S3.SS2.p1.7.m7.1.1" xref="S3.SS2.p1.7.m7.1.1.cmml"><mi id="S3.SS2.p1.7.m7.1.1.2" xref="S3.SS2.p1.7.m7.1.1.2.cmml">C</mi><mn id="S3.SS2.p1.7.m7.1.1.3" xref="S3.SS2.p1.7.m7.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.7.m7.1b"><apply id="S3.SS2.p1.7.m7.1.1.cmml" xref="S3.SS2.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.7.m7.1.1.1.cmml" xref="S3.SS2.p1.7.m7.1.1">subscript</csymbol><ci id="S3.SS2.p1.7.m7.1.1.2.cmml" xref="S3.SS2.p1.7.m7.1.1.2">ğ¶</ci><cn type="integer" id="S3.SS2.p1.7.m7.1.1.3.cmml" xref="S3.SS2.p1.7.m7.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.7.m7.1c">{C}_{3}</annotation></semantics></math>, <math id="S3.SS2.p1.8.m8.1" class="ltx_Math" alttext="{C}_{4}" display="inline"><semantics id="S3.SS2.p1.8.m8.1a"><msub id="S3.SS2.p1.8.m8.1.1" xref="S3.SS2.p1.8.m8.1.1.cmml"><mi id="S3.SS2.p1.8.m8.1.1.2" xref="S3.SS2.p1.8.m8.1.1.2.cmml">C</mi><mn id="S3.SS2.p1.8.m8.1.1.3" xref="S3.SS2.p1.8.m8.1.1.3.cmml">4</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.8.m8.1b"><apply id="S3.SS2.p1.8.m8.1.1.cmml" xref="S3.SS2.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.8.m8.1.1.1.cmml" xref="S3.SS2.p1.8.m8.1.1">subscript</csymbol><ci id="S3.SS2.p1.8.m8.1.1.2.cmml" xref="S3.SS2.p1.8.m8.1.1.2">ğ¶</ci><cn type="integer" id="S3.SS2.p1.8.m8.1.1.3.cmml" xref="S3.SS2.p1.8.m8.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.8.m8.1c">{C}_{4}</annotation></semantics></math> to produce <math id="S3.SS2.p1.9.m9.1" class="ltx_Math" alttext="{P}_{3}" display="inline"><semantics id="S3.SS2.p1.9.m9.1a"><msub id="S3.SS2.p1.9.m9.1.1" xref="S3.SS2.p1.9.m9.1.1.cmml"><mi id="S3.SS2.p1.9.m9.1.1.2" xref="S3.SS2.p1.9.m9.1.1.2.cmml">P</mi><mn id="S3.SS2.p1.9.m9.1.1.3" xref="S3.SS2.p1.9.m9.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.9.m9.1b"><apply id="S3.SS2.p1.9.m9.1.1.cmml" xref="S3.SS2.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.9.m9.1.1.1.cmml" xref="S3.SS2.p1.9.m9.1.1">subscript</csymbol><ci id="S3.SS2.p1.9.m9.1.1.2.cmml" xref="S3.SS2.p1.9.m9.1.1.2">ğ‘ƒ</ci><cn type="integer" id="S3.SS2.p1.9.m9.1.1.3.cmml" xref="S3.SS2.p1.9.m9.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.9.m9.1c">{P}_{3}</annotation></semantics></math>, <math id="S3.SS2.p1.10.m10.1" class="ltx_Math" alttext="{P}_{4}" display="inline"><semantics id="S3.SS2.p1.10.m10.1a"><msub id="S3.SS2.p1.10.m10.1.1" xref="S3.SS2.p1.10.m10.1.1.cmml"><mi id="S3.SS2.p1.10.m10.1.1.2" xref="S3.SS2.p1.10.m10.1.1.2.cmml">P</mi><mn id="S3.SS2.p1.10.m10.1.1.3" xref="S3.SS2.p1.10.m10.1.1.3.cmml">4</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.10.m10.1b"><apply id="S3.SS2.p1.10.m10.1.1.cmml" xref="S3.SS2.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.10.m10.1.1.1.cmml" xref="S3.SS2.p1.10.m10.1.1">subscript</csymbol><ci id="S3.SS2.p1.10.m10.1.1.2.cmml" xref="S3.SS2.p1.10.m10.1.1.2">ğ‘ƒ</ci><cn type="integer" id="S3.SS2.p1.10.m10.1.1.3.cmml" xref="S3.SS2.p1.10.m10.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.10.m10.1c">{P}_{4}</annotation></semantics></math> as well as serves as the input of <math id="S3.SS2.p1.11.m11.1" class="ltx_Math" alttext="{P}_{6}" display="inline"><semantics id="S3.SS2.p1.11.m11.1a"><msub id="S3.SS2.p1.11.m11.1.1" xref="S3.SS2.p1.11.m11.1.1.cmml"><mi id="S3.SS2.p1.11.m11.1.1.2" xref="S3.SS2.p1.11.m11.1.1.2.cmml">P</mi><mn id="S3.SS2.p1.11.m11.1.1.3" xref="S3.SS2.p1.11.m11.1.1.3.cmml">6</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.11.m11.1b"><apply id="S3.SS2.p1.11.m11.1.1.cmml" xref="S3.SS2.p1.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.11.m11.1.1.1.cmml" xref="S3.SS2.p1.11.m11.1.1">subscript</csymbol><ci id="S3.SS2.p1.11.m11.1.1.2.cmml" xref="S3.SS2.p1.11.m11.1.1.2">ğ‘ƒ</ci><cn type="integer" id="S3.SS2.p1.11.m11.1.1.3.cmml" xref="S3.SS2.p1.11.m11.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.11.m11.1c">{P}_{6}</annotation></semantics></math>, <math id="S3.SS2.p1.12.m12.1" class="ltx_Math" alttext="{P}_{7}" display="inline"><semantics id="S3.SS2.p1.12.m12.1a"><msub id="S3.SS2.p1.12.m12.1.1" xref="S3.SS2.p1.12.m12.1.1.cmml"><mi id="S3.SS2.p1.12.m12.1.1.2" xref="S3.SS2.p1.12.m12.1.1.2.cmml">P</mi><mn id="S3.SS2.p1.12.m12.1.1.3" xref="S3.SS2.p1.12.m12.1.1.3.cmml">7</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.12.m12.1b"><apply id="S3.SS2.p1.12.m12.1.1.cmml" xref="S3.SS2.p1.12.m12.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.12.m12.1.1.1.cmml" xref="S3.SS2.p1.12.m12.1.1">subscript</csymbol><ci id="S3.SS2.p1.12.m12.1.1.2.cmml" xref="S3.SS2.p1.12.m12.1.1.2">ğ‘ƒ</ci><cn type="integer" id="S3.SS2.p1.12.m12.1.1.3.cmml" xref="S3.SS2.p1.12.m12.1.1.3">7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.12.m12.1c">{P}_{7}</annotation></semantics></math>.
Therefore, we fuse <math id="S3.SS2.p1.13.m13.1" class="ltx_Math" alttext="{z^{\prime}}" display="inline"><semantics id="S3.SS2.p1.13.m13.1a"><msup id="S3.SS2.p1.13.m13.1.1" xref="S3.SS2.p1.13.m13.1.1.cmml"><mi id="S3.SS2.p1.13.m13.1.1.2" xref="S3.SS2.p1.13.m13.1.1.2.cmml">z</mi><mo id="S3.SS2.p1.13.m13.1.1.3" xref="S3.SS2.p1.13.m13.1.1.3.cmml">â€²</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.13.m13.1b"><apply id="S3.SS2.p1.13.m13.1.1.cmml" xref="S3.SS2.p1.13.m13.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.13.m13.1.1.1.cmml" xref="S3.SS2.p1.13.m13.1.1">superscript</csymbol><ci id="S3.SS2.p1.13.m13.1.1.2.cmml" xref="S3.SS2.p1.13.m13.1.1.2">ğ‘§</ci><ci id="S3.SS2.p1.13.m13.1.1.3.cmml" xref="S3.SS2.p1.13.m13.1.1.3">â€²</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.13.m13.1c">{z^{\prime}}</annotation></semantics></math> into <math id="S3.SS2.p1.14.m14.1" class="ltx_Math" alttext="{C}_{5}" display="inline"><semantics id="S3.SS2.p1.14.m14.1a"><msub id="S3.SS2.p1.14.m14.1.1" xref="S3.SS2.p1.14.m14.1.1.cmml"><mi id="S3.SS2.p1.14.m14.1.1.2" xref="S3.SS2.p1.14.m14.1.1.2.cmml">C</mi><mn id="S3.SS2.p1.14.m14.1.1.3" xref="S3.SS2.p1.14.m14.1.1.3.cmml">5</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.14.m14.1b"><apply id="S3.SS2.p1.14.m14.1.1.cmml" xref="S3.SS2.p1.14.m14.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.14.m14.1.1.1.cmml" xref="S3.SS2.p1.14.m14.1.1">subscript</csymbol><ci id="S3.SS2.p1.14.m14.1.1.2.cmml" xref="S3.SS2.p1.14.m14.1.1.2">ğ¶</ci><cn type="integer" id="S3.SS2.p1.14.m14.1.1.3.cmml" xref="S3.SS2.p1.14.m14.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.14.m14.1c">{C}_{5}</annotation></semantics></math> to spread the image embeddings suitable for the text alignment to different scale image feature maps.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.7" class="ltx_p">To be specific, we first apply three consecutive 3<math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mo id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><times id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">\times</annotation></semantics></math>3 convolutional layers with ReLU activation function to adapt the MHSA grid-level output feature <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mi id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><ci id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">z</annotation></semantics></math>, reducing the number of channels from 1024 to 256 to generate <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="z^{\prime}" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><msup id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml"><mi id="S3.SS2.p2.3.m3.1.1.2" xref="S3.SS2.p2.3.m3.1.1.2.cmml">z</mi><mo id="S3.SS2.p2.3.m3.1.1.3" xref="S3.SS2.p2.3.m3.1.1.3.cmml">â€²</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">superscript</csymbol><ci id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2">ğ‘§</ci><ci id="S3.SS2.p2.3.m3.1.1.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3">â€²</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">z^{\prime}</annotation></semantics></math>, and concatenate it with <math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="{C}_{5}" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><msub id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml"><mi id="S3.SS2.p2.4.m4.1.1.2" xref="S3.SS2.p2.4.m4.1.1.2.cmml">C</mi><mn id="S3.SS2.p2.4.m4.1.1.3" xref="S3.SS2.p2.4.m4.1.1.3.cmml">5</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><apply id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p2.4.m4.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1.2">ğ¶</ci><cn type="integer" id="S3.SS2.p2.4.m4.1.1.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">{C}_{5}</annotation></semantics></math>.
Then the concatenated feature <math id="S3.SS2.p2.5.m5.2" class="ltx_Math" alttext="\left[{{z^{\prime}}},{C}_{5}\right]" display="inline"><semantics id="S3.SS2.p2.5.m5.2a"><mrow id="S3.SS2.p2.5.m5.2.2.2" xref="S3.SS2.p2.5.m5.2.2.3.cmml"><mo id="S3.SS2.p2.5.m5.2.2.2.3" xref="S3.SS2.p2.5.m5.2.2.3.cmml">[</mo><msup id="S3.SS2.p2.5.m5.1.1.1.1" xref="S3.SS2.p2.5.m5.1.1.1.1.cmml"><mi id="S3.SS2.p2.5.m5.1.1.1.1.2" xref="S3.SS2.p2.5.m5.1.1.1.1.2.cmml">z</mi><mo id="S3.SS2.p2.5.m5.1.1.1.1.3" xref="S3.SS2.p2.5.m5.1.1.1.1.3.cmml">â€²</mo></msup><mo id="S3.SS2.p2.5.m5.2.2.2.4" xref="S3.SS2.p2.5.m5.2.2.3.cmml">,</mo><msub id="S3.SS2.p2.5.m5.2.2.2.2" xref="S3.SS2.p2.5.m5.2.2.2.2.cmml"><mi id="S3.SS2.p2.5.m5.2.2.2.2.2" xref="S3.SS2.p2.5.m5.2.2.2.2.2.cmml">C</mi><mn id="S3.SS2.p2.5.m5.2.2.2.2.3" xref="S3.SS2.p2.5.m5.2.2.2.2.3.cmml">5</mn></msub><mo id="S3.SS2.p2.5.m5.2.2.2.5" xref="S3.SS2.p2.5.m5.2.2.3.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.2b"><interval closure="closed" id="S3.SS2.p2.5.m5.2.2.3.cmml" xref="S3.SS2.p2.5.m5.2.2.2"><apply id="S3.SS2.p2.5.m5.1.1.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.5.m5.1.1.1.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1.1.1">superscript</csymbol><ci id="S3.SS2.p2.5.m5.1.1.1.1.2.cmml" xref="S3.SS2.p2.5.m5.1.1.1.1.2">ğ‘§</ci><ci id="S3.SS2.p2.5.m5.1.1.1.1.3.cmml" xref="S3.SS2.p2.5.m5.1.1.1.1.3">â€²</ci></apply><apply id="S3.SS2.p2.5.m5.2.2.2.2.cmml" xref="S3.SS2.p2.5.m5.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p2.5.m5.2.2.2.2.1.cmml" xref="S3.SS2.p2.5.m5.2.2.2.2">subscript</csymbol><ci id="S3.SS2.p2.5.m5.2.2.2.2.2.cmml" xref="S3.SS2.p2.5.m5.2.2.2.2.2">ğ¶</ci><cn type="integer" id="S3.SS2.p2.5.m5.2.2.2.2.3.cmml" xref="S3.SS2.p2.5.m5.2.2.2.2.3">5</cn></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.2c">\left[{{z^{\prime}}},{C}_{5}\right]</annotation></semantics></math> replaces <math id="S3.SS2.p2.6.m6.1" class="ltx_Math" alttext="{C}_{5}" display="inline"><semantics id="S3.SS2.p2.6.m6.1a"><msub id="S3.SS2.p2.6.m6.1.1" xref="S3.SS2.p2.6.m6.1.1.cmml"><mi id="S3.SS2.p2.6.m6.1.1.2" xref="S3.SS2.p2.6.m6.1.1.2.cmml">C</mi><mn id="S3.SS2.p2.6.m6.1.1.3" xref="S3.SS2.p2.6.m6.1.1.3.cmml">5</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m6.1b"><apply id="S3.SS2.p2.6.m6.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.6.m6.1.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.p2.6.m6.1.1.2.cmml" xref="S3.SS2.p2.6.m6.1.1.2">ğ¶</ci><cn type="integer" id="S3.SS2.p2.6.m6.1.1.3.cmml" xref="S3.SS2.p2.6.m6.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m6.1c">{C}_{5}</annotation></semantics></math> and is fed into the FPN with a little modification in the input channel number.
In this way, the FPN is able to produce the multi-scale feature maps <math id="S3.SS2.p2.7.m7.1" class="ltx_Math" alttext="\left\{{P}_{i}\right\}_{i=3}^{7}" display="inline"><semantics id="S3.SS2.p2.7.m7.1a"><msubsup id="S3.SS2.p2.7.m7.1.1" xref="S3.SS2.p2.7.m7.1.1.cmml"><mrow id="S3.SS2.p2.7.m7.1.1.1.1.1" xref="S3.SS2.p2.7.m7.1.1.1.1.2.cmml"><mo id="S3.SS2.p2.7.m7.1.1.1.1.1.2" xref="S3.SS2.p2.7.m7.1.1.1.1.2.cmml">{</mo><msub id="S3.SS2.p2.7.m7.1.1.1.1.1.1" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p2.7.m7.1.1.1.1.1.1.2" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.2.cmml">P</mi><mi id="S3.SS2.p2.7.m7.1.1.1.1.1.1.3" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS2.p2.7.m7.1.1.1.1.1.3" xref="S3.SS2.p2.7.m7.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS2.p2.7.m7.1.1.1.3" xref="S3.SS2.p2.7.m7.1.1.1.3.cmml"><mi id="S3.SS2.p2.7.m7.1.1.1.3.2" xref="S3.SS2.p2.7.m7.1.1.1.3.2.cmml">i</mi><mo id="S3.SS2.p2.7.m7.1.1.1.3.1" xref="S3.SS2.p2.7.m7.1.1.1.3.1.cmml">=</mo><mn id="S3.SS2.p2.7.m7.1.1.1.3.3" xref="S3.SS2.p2.7.m7.1.1.1.3.3.cmml">3</mn></mrow><mn id="S3.SS2.p2.7.m7.1.1.3" xref="S3.SS2.p2.7.m7.1.1.3.cmml">7</mn></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.7.m7.1b"><apply id="S3.SS2.p2.7.m7.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.7.m7.1.1.2.cmml" xref="S3.SS2.p2.7.m7.1.1">superscript</csymbol><apply id="S3.SS2.p2.7.m7.1.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.7.m7.1.1.1.2.cmml" xref="S3.SS2.p2.7.m7.1.1">subscript</csymbol><set id="S3.SS2.p2.7.m7.1.1.1.1.2.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.1"><apply id="S3.SS2.p2.7.m7.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p2.7.m7.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.2">ğ‘ƒ</ci><ci id="S3.SS2.p2.7.m7.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.3">ğ‘–</ci></apply></set><apply id="S3.SS2.p2.7.m7.1.1.1.3.cmml" xref="S3.SS2.p2.7.m7.1.1.1.3"><eq id="S3.SS2.p2.7.m7.1.1.1.3.1.cmml" xref="S3.SS2.p2.7.m7.1.1.1.3.1"></eq><ci id="S3.SS2.p2.7.m7.1.1.1.3.2.cmml" xref="S3.SS2.p2.7.m7.1.1.1.3.2">ğ‘–</ci><cn type="integer" id="S3.SS2.p2.7.m7.1.1.1.3.3.cmml" xref="S3.SS2.p2.7.m7.1.1.1.3.3">3</cn></apply></apply><cn type="integer" id="S3.SS2.p2.7.m7.1.1.3.cmml" xref="S3.SS2.p2.7.m7.1.1.3">7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.7.m7.1c">\left\{{P}_{i}\right\}_{i=3}^{7}</annotation></semantics></math> inheriting from the CLIP image embeddings that can be aligned to the text embedding as formulated below,</p>
<table id="S5.EGx2" class="ltx_equationgroup ltx_eqn_eqnarray ltx_eqn_table">

<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E2.m1.1" class="ltx_Math" alttext="\displaystyle\left\{{P}_{i}\right\}_{i=3}^{7}={\rm{FPN}}({C}_{3},{C}_{4},\left[{{z^{\prime}}},{C}_{5}\right])." display="inline"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><msubsup id="S3.E2.m1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.2.cmml"><mo id="S3.E2.m1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.2.cmml">{</mo><msub id="S3.E2.m1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml">P</mi><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E2.m1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.E2.m1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.1.1.3.2.cmml">i</mi><mo id="S3.E2.m1.1.1.1.1.1.1.3.1" xref="S3.E2.m1.1.1.1.1.1.1.3.1.cmml">=</mo><mn id="S3.E2.m1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.1.1.3.3.cmml">3</mn></mrow><mn id="S3.E2.m1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.3.cmml">7</mn></msubsup><mo id="S3.E2.m1.1.1.1.1.5" xref="S3.E2.m1.1.1.1.1.5.cmml">=</mo><mrow id="S3.E2.m1.1.1.1.1.4" xref="S3.E2.m1.1.1.1.1.4.cmml"><mi id="S3.E2.m1.1.1.1.1.4.5" xref="S3.E2.m1.1.1.1.1.4.5.cmml">FPN</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.4.4" xref="S3.E2.m1.1.1.1.1.4.4.cmml">â€‹</mo><mrow id="S3.E2.m1.1.1.1.1.4.3.3" xref="S3.E2.m1.1.1.1.1.4.3.4.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.4.3.3.4" xref="S3.E2.m1.1.1.1.1.4.3.4.cmml">(</mo><msub id="S3.E2.m1.1.1.1.1.2.1.1.1" xref="S3.E2.m1.1.1.1.1.2.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.2.1.1.1.2" xref="S3.E2.m1.1.1.1.1.2.1.1.1.2.cmml">C</mi><mn id="S3.E2.m1.1.1.1.1.2.1.1.1.3" xref="S3.E2.m1.1.1.1.1.2.1.1.1.3.cmml">3</mn></msub><mo id="S3.E2.m1.1.1.1.1.4.3.3.5" xref="S3.E2.m1.1.1.1.1.4.3.4.cmml">,</mo><msub id="S3.E2.m1.1.1.1.1.3.2.2.2" xref="S3.E2.m1.1.1.1.1.3.2.2.2.cmml"><mi id="S3.E2.m1.1.1.1.1.3.2.2.2.2" xref="S3.E2.m1.1.1.1.1.3.2.2.2.2.cmml">C</mi><mn id="S3.E2.m1.1.1.1.1.3.2.2.2.3" xref="S3.E2.m1.1.1.1.1.3.2.2.2.3.cmml">4</mn></msub><mo id="S3.E2.m1.1.1.1.1.4.3.3.6" xref="S3.E2.m1.1.1.1.1.4.3.4.cmml">,</mo><mrow id="S3.E2.m1.1.1.1.1.4.3.3.3.2" xref="S3.E2.m1.1.1.1.1.4.3.3.3.3.cmml"><mo id="S3.E2.m1.1.1.1.1.4.3.3.3.2.3" xref="S3.E2.m1.1.1.1.1.4.3.3.3.3.cmml">[</mo><msup id="S3.E2.m1.1.1.1.1.4.3.3.3.1.1" xref="S3.E2.m1.1.1.1.1.4.3.3.3.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.4.3.3.3.1.1.2" xref="S3.E2.m1.1.1.1.1.4.3.3.3.1.1.2.cmml">z</mi><mo id="S3.E2.m1.1.1.1.1.4.3.3.3.1.1.3" xref="S3.E2.m1.1.1.1.1.4.3.3.3.1.1.3.cmml">â€²</mo></msup><mo id="S3.E2.m1.1.1.1.1.4.3.3.3.2.4" xref="S3.E2.m1.1.1.1.1.4.3.3.3.3.cmml">,</mo><msub id="S3.E2.m1.1.1.1.1.4.3.3.3.2.2" xref="S3.E2.m1.1.1.1.1.4.3.3.3.2.2.cmml"><mi id="S3.E2.m1.1.1.1.1.4.3.3.3.2.2.2" xref="S3.E2.m1.1.1.1.1.4.3.3.3.2.2.2.cmml">C</mi><mn id="S3.E2.m1.1.1.1.1.4.3.3.3.2.2.3" xref="S3.E2.m1.1.1.1.1.4.3.3.3.2.2.3.cmml">5</mn></msub><mo id="S3.E2.m1.1.1.1.1.4.3.3.3.2.5" xref="S3.E2.m1.1.1.1.1.4.3.3.3.3.cmml">]</mo></mrow><mo stretchy="false" id="S3.E2.m1.1.1.1.1.4.3.3.7" xref="S3.E2.m1.1.1.1.1.4.3.4.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><eq id="S3.E2.m1.1.1.1.1.5.cmml" xref="S3.E2.m1.1.1.1.1.5"></eq><apply id="S3.E2.m1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1">superscript</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1">subscript</csymbol><set id="S3.E2.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1"><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2">ğ‘ƒ</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply></set><apply id="S3.E2.m1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3"><eq id="S3.E2.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3.1"></eq><ci id="S3.E2.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3.2">ğ‘–</ci><cn type="integer" id="S3.E2.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3.3">3</cn></apply></apply><cn type="integer" id="S3.E2.m1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.3">7</cn></apply><apply id="S3.E2.m1.1.1.1.1.4.cmml" xref="S3.E2.m1.1.1.1.1.4"><times id="S3.E2.m1.1.1.1.1.4.4.cmml" xref="S3.E2.m1.1.1.1.1.4.4"></times><ci id="S3.E2.m1.1.1.1.1.4.5.cmml" xref="S3.E2.m1.1.1.1.1.4.5">FPN</ci><vector id="S3.E2.m1.1.1.1.1.4.3.4.cmml" xref="S3.E2.m1.1.1.1.1.4.3.3"><apply id="S3.E2.m1.1.1.1.1.2.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.2.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.2.1.1.1">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.2.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.2.1.1.1.2">ğ¶</ci><cn type="integer" id="S3.E2.m1.1.1.1.1.2.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.2.1.1.1.3">3</cn></apply><apply id="S3.E2.m1.1.1.1.1.3.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.2.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.3.2.2.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.2.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2.2.2.2">ğ¶</ci><cn type="integer" id="S3.E2.m1.1.1.1.1.3.2.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.3.2.2.2.3">4</cn></apply><interval closure="closed" id="S3.E2.m1.1.1.1.1.4.3.3.3.3.cmml" xref="S3.E2.m1.1.1.1.1.4.3.3.3.2"><apply id="S3.E2.m1.1.1.1.1.4.3.3.3.1.1.cmml" xref="S3.E2.m1.1.1.1.1.4.3.3.3.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.4.3.3.3.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.4.3.3.3.1.1">superscript</csymbol><ci id="S3.E2.m1.1.1.1.1.4.3.3.3.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.4.3.3.3.1.1.2">ğ‘§</ci><ci id="S3.E2.m1.1.1.1.1.4.3.3.3.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.4.3.3.3.1.1.3">â€²</ci></apply><apply id="S3.E2.m1.1.1.1.1.4.3.3.3.2.2.cmml" xref="S3.E2.m1.1.1.1.1.4.3.3.3.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.4.3.3.3.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.4.3.3.3.2.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.4.3.3.3.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.4.3.3.3.2.2.2">ğ¶</ci><cn type="integer" id="S3.E2.m1.1.1.1.1.4.3.3.3.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.4.3.3.3.2.2.3">5</cn></apply></interval></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\displaystyle\left\{{P}_{i}\right\}_{i=3}^{7}={\rm{FPN}}({C}_{3},{C}_{4},\left[{{z^{\prime}}},{C}_{5}\right]).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.6" class="ltx_p">The FPN output feature <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="\left\{{P}_{i}\right\}_{i=3}^{7}" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><msubsup id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml"><mrow id="S3.SS2.p3.1.m1.1.1.1.1.1" xref="S3.SS2.p3.1.m1.1.1.1.1.2.cmml"><mo id="S3.SS2.p3.1.m1.1.1.1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.1.1.2.cmml">{</mo><msub id="S3.SS2.p3.1.m1.1.1.1.1.1.1" xref="S3.SS2.p3.1.m1.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p3.1.m1.1.1.1.1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.1.1.1.1.2.cmml">P</mi><mi id="S3.SS2.p3.1.m1.1.1.1.1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS2.p3.1.m1.1.1.1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS2.p3.1.m1.1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.1.3.cmml"><mi id="S3.SS2.p3.1.m1.1.1.1.3.2" xref="S3.SS2.p3.1.m1.1.1.1.3.2.cmml">i</mi><mo id="S3.SS2.p3.1.m1.1.1.1.3.1" xref="S3.SS2.p3.1.m1.1.1.1.3.1.cmml">=</mo><mn id="S3.SS2.p3.1.m1.1.1.1.3.3" xref="S3.SS2.p3.1.m1.1.1.1.3.3.cmml">3</mn></mrow><mn id="S3.SS2.p3.1.m1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.3.cmml">7</mn></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1">superscript</csymbol><apply id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1">subscript</csymbol><set id="S3.SS2.p3.1.m1.1.1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.1.1.1"><apply id="S3.SS2.p3.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p3.1.m1.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.1.1.1.1.2">ğ‘ƒ</ci><ci id="S3.SS2.p3.1.m1.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.1.1.1.1.1.1.3">ğ‘–</ci></apply></set><apply id="S3.SS2.p3.1.m1.1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.1.1.1.3"><eq id="S3.SS2.p3.1.m1.1.1.1.3.1.cmml" xref="S3.SS2.p3.1.m1.1.1.1.3.1"></eq><ci id="S3.SS2.p3.1.m1.1.1.1.3.2.cmml" xref="S3.SS2.p3.1.m1.1.1.1.3.2">ğ‘–</ci><cn type="integer" id="S3.SS2.p3.1.m1.1.1.1.3.3.cmml" xref="S3.SS2.p3.1.m1.1.1.1.3.3">3</cn></apply></apply><cn type="integer" id="S3.SS2.p3.1.m1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3">7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">\left\{{P}_{i}\right\}_{i=3}^{7}</annotation></semantics></math> are then used to generate the final multi-scale grid-level features <math id="S3.SS2.p3.2.m2.1" class="ltx_Math" alttext="\left\{{G}_{i}\right\}_{i=3}^{7}" display="inline"><semantics id="S3.SS2.p3.2.m2.1a"><msubsup id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml"><mrow id="S3.SS2.p3.2.m2.1.1.1.1.1" xref="S3.SS2.p3.2.m2.1.1.1.1.2.cmml"><mo id="S3.SS2.p3.2.m2.1.1.1.1.1.2" xref="S3.SS2.p3.2.m2.1.1.1.1.2.cmml">{</mo><msub id="S3.SS2.p3.2.m2.1.1.1.1.1.1" xref="S3.SS2.p3.2.m2.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p3.2.m2.1.1.1.1.1.1.2" xref="S3.SS2.p3.2.m2.1.1.1.1.1.1.2.cmml">G</mi><mi id="S3.SS2.p3.2.m2.1.1.1.1.1.1.3" xref="S3.SS2.p3.2.m2.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS2.p3.2.m2.1.1.1.1.1.3" xref="S3.SS2.p3.2.m2.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS2.p3.2.m2.1.1.1.3" xref="S3.SS2.p3.2.m2.1.1.1.3.cmml"><mi id="S3.SS2.p3.2.m2.1.1.1.3.2" xref="S3.SS2.p3.2.m2.1.1.1.3.2.cmml">i</mi><mo id="S3.SS2.p3.2.m2.1.1.1.3.1" xref="S3.SS2.p3.2.m2.1.1.1.3.1.cmml">=</mo><mn id="S3.SS2.p3.2.m2.1.1.1.3.3" xref="S3.SS2.p3.2.m2.1.1.1.3.3.cmml">3</mn></mrow><mn id="S3.SS2.p3.2.m2.1.1.3" xref="S3.SS2.p3.2.m2.1.1.3.cmml">7</mn></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><apply id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1">superscript</csymbol><apply id="S3.SS2.p3.2.m2.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.1.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1">subscript</csymbol><set id="S3.SS2.p3.2.m2.1.1.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1.1.1.1"><apply id="S3.SS2.p3.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p3.2.m2.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1.1.1.1.1.2">ğº</ci><ci id="S3.SS2.p3.2.m2.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p3.2.m2.1.1.1.1.1.1.3">ğ‘–</ci></apply></set><apply id="S3.SS2.p3.2.m2.1.1.1.3.cmml" xref="S3.SS2.p3.2.m2.1.1.1.3"><eq id="S3.SS2.p3.2.m2.1.1.1.3.1.cmml" xref="S3.SS2.p3.2.m2.1.1.1.3.1"></eq><ci id="S3.SS2.p3.2.m2.1.1.1.3.2.cmml" xref="S3.SS2.p3.2.m2.1.1.1.3.2">ğ‘–</ci><cn type="integer" id="S3.SS2.p3.2.m2.1.1.1.3.3.cmml" xref="S3.SS2.p3.2.m2.1.1.1.3.3">3</cn></apply></apply><cn type="integer" id="S3.SS2.p3.2.m2.1.1.3.cmml" xref="S3.SS2.p3.2.m2.1.1.3">7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">\left\{{G}_{i}\right\}_{i=3}^{7}</annotation></semantics></math>
by going through the FCOS classification head.
In the original FCOS, the classification head contains 5 convolutional layers and the last layer outputs the features with the channel number equal to the category number.
While in GridCLIP,
We instead modify the output channel to be equal to the dimension of the CLIP text embeddings, to generate <math id="S3.SS2.p3.3.m3.1" class="ltx_Math" alttext="\left\{{G}_{i}\right\}_{i=3}^{7}" display="inline"><semantics id="S3.SS2.p3.3.m3.1a"><msubsup id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml"><mrow id="S3.SS2.p3.3.m3.1.1.1.1.1" xref="S3.SS2.p3.3.m3.1.1.1.1.2.cmml"><mo id="S3.SS2.p3.3.m3.1.1.1.1.1.2" xref="S3.SS2.p3.3.m3.1.1.1.1.2.cmml">{</mo><msub id="S3.SS2.p3.3.m3.1.1.1.1.1.1" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p3.3.m3.1.1.1.1.1.1.2" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.2.cmml">G</mi><mi id="S3.SS2.p3.3.m3.1.1.1.1.1.1.3" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS2.p3.3.m3.1.1.1.1.1.3" xref="S3.SS2.p3.3.m3.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS2.p3.3.m3.1.1.1.3" xref="S3.SS2.p3.3.m3.1.1.1.3.cmml"><mi id="S3.SS2.p3.3.m3.1.1.1.3.2" xref="S3.SS2.p3.3.m3.1.1.1.3.2.cmml">i</mi><mo id="S3.SS2.p3.3.m3.1.1.1.3.1" xref="S3.SS2.p3.3.m3.1.1.1.3.1.cmml">=</mo><mn id="S3.SS2.p3.3.m3.1.1.1.3.3" xref="S3.SS2.p3.3.m3.1.1.1.3.3.cmml">3</mn></mrow><mn id="S3.SS2.p3.3.m3.1.1.3" xref="S3.SS2.p3.3.m3.1.1.3.cmml">7</mn></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><apply id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.3.m3.1.1.2.cmml" xref="S3.SS2.p3.3.m3.1.1">superscript</csymbol><apply id="S3.SS2.p3.3.m3.1.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.3.m3.1.1.1.2.cmml" xref="S3.SS2.p3.3.m3.1.1">subscript</csymbol><set id="S3.SS2.p3.3.m3.1.1.1.1.2.cmml" xref="S3.SS2.p3.3.m3.1.1.1.1.1"><apply id="S3.SS2.p3.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.3.m3.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p3.3.m3.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.2">ğº</ci><ci id="S3.SS2.p3.3.m3.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.3">ğ‘–</ci></apply></set><apply id="S3.SS2.p3.3.m3.1.1.1.3.cmml" xref="S3.SS2.p3.3.m3.1.1.1.3"><eq id="S3.SS2.p3.3.m3.1.1.1.3.1.cmml" xref="S3.SS2.p3.3.m3.1.1.1.3.1"></eq><ci id="S3.SS2.p3.3.m3.1.1.1.3.2.cmml" xref="S3.SS2.p3.3.m3.1.1.1.3.2">ğ‘–</ci><cn type="integer" id="S3.SS2.p3.3.m3.1.1.1.3.3.cmml" xref="S3.SS2.p3.3.m3.1.1.1.3.3">3</cn></apply></apply><cn type="integer" id="S3.SS2.p3.3.m3.1.1.3.cmml" xref="S3.SS2.p3.3.m3.1.1.3">7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">\left\{{G}_{i}\right\}_{i=3}^{7}</annotation></semantics></math>.
Then for each scale,
the output feature map calculates the cosine similarities
with each text embedding (each category)
in pixel level
corresponding to grids in the original image,
to produce the multi-scale grid-level score with the Sigmoid activation function.
For any grid (pixel) <math id="S3.SS2.p3.4.m4.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.SS2.p3.4.m4.1a"><mi id="S3.SS2.p3.4.m4.1.1" xref="S3.SS2.p3.4.m4.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.m4.1b"><ci id="S3.SS2.p3.4.m4.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1">ğ‘—</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.4.m4.1c">j</annotation></semantics></math> in the <math id="S3.SS2.p3.5.m5.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS2.p3.5.m5.1a"><mi id="S3.SS2.p3.5.m5.1.1" xref="S3.SS2.p3.5.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.5.m5.1b"><ci id="S3.SS2.p3.5.m5.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.5.m5.1c">i</annotation></semantics></math>-th scale grid-level feature <math id="S3.SS2.p3.6.m6.1" class="ltx_Math" alttext="G_{i}(j)" display="inline"><semantics id="S3.SS2.p3.6.m6.1a"><mrow id="S3.SS2.p3.6.m6.1.2" xref="S3.SS2.p3.6.m6.1.2.cmml"><msub id="S3.SS2.p3.6.m6.1.2.2" xref="S3.SS2.p3.6.m6.1.2.2.cmml"><mi id="S3.SS2.p3.6.m6.1.2.2.2" xref="S3.SS2.p3.6.m6.1.2.2.2.cmml">G</mi><mi id="S3.SS2.p3.6.m6.1.2.2.3" xref="S3.SS2.p3.6.m6.1.2.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS2.p3.6.m6.1.2.1" xref="S3.SS2.p3.6.m6.1.2.1.cmml">â€‹</mo><mrow id="S3.SS2.p3.6.m6.1.2.3.2" xref="S3.SS2.p3.6.m6.1.2.cmml"><mo stretchy="false" id="S3.SS2.p3.6.m6.1.2.3.2.1" xref="S3.SS2.p3.6.m6.1.2.cmml">(</mo><mi id="S3.SS2.p3.6.m6.1.1" xref="S3.SS2.p3.6.m6.1.1.cmml">j</mi><mo stretchy="false" id="S3.SS2.p3.6.m6.1.2.3.2.2" xref="S3.SS2.p3.6.m6.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.6.m6.1b"><apply id="S3.SS2.p3.6.m6.1.2.cmml" xref="S3.SS2.p3.6.m6.1.2"><times id="S3.SS2.p3.6.m6.1.2.1.cmml" xref="S3.SS2.p3.6.m6.1.2.1"></times><apply id="S3.SS2.p3.6.m6.1.2.2.cmml" xref="S3.SS2.p3.6.m6.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p3.6.m6.1.2.2.1.cmml" xref="S3.SS2.p3.6.m6.1.2.2">subscript</csymbol><ci id="S3.SS2.p3.6.m6.1.2.2.2.cmml" xref="S3.SS2.p3.6.m6.1.2.2.2">ğº</ci><ci id="S3.SS2.p3.6.m6.1.2.2.3.cmml" xref="S3.SS2.p3.6.m6.1.2.2.3">ğ‘–</ci></apply><ci id="S3.SS2.p3.6.m6.1.1.cmml" xref="S3.SS2.p3.6.m6.1.1">ğ‘—</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.6.m6.1c">G_{i}(j)</annotation></semantics></math>, the matching score over all categories can be formulated as below,</p>
<table id="S5.EGx3" class="ltx_equationgroup ltx_eqn_eqnarray ltx_eqn_table">

<tbody id="S3.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E3.m1.6" class="ltx_Math" alttext="\displaystyle S_{i}(j)=\left\{\dfrac{G_{i}(j)\cdot T_{k}}{{\left\|G_{i}(j)\right\|_{2}\left\|T_{k}\right\|_{2}}}\right\}_{k=1}^{K}." display="inline"><semantics id="S3.E3.m1.6a"><mrow id="S3.E3.m1.6.6.1" xref="S3.E3.m1.6.6.1.1.cmml"><mrow id="S3.E3.m1.6.6.1.1" xref="S3.E3.m1.6.6.1.1.cmml"><mrow id="S3.E3.m1.6.6.1.1.2" xref="S3.E3.m1.6.6.1.1.2.cmml"><msub id="S3.E3.m1.6.6.1.1.2.2" xref="S3.E3.m1.6.6.1.1.2.2.cmml"><mi id="S3.E3.m1.6.6.1.1.2.2.2" xref="S3.E3.m1.6.6.1.1.2.2.2.cmml">S</mi><mi id="S3.E3.m1.6.6.1.1.2.2.3" xref="S3.E3.m1.6.6.1.1.2.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.6.6.1.1.2.1" xref="S3.E3.m1.6.6.1.1.2.1.cmml">â€‹</mo><mrow id="S3.E3.m1.6.6.1.1.2.3.2" xref="S3.E3.m1.6.6.1.1.2.cmml"><mo stretchy="false" id="S3.E3.m1.6.6.1.1.2.3.2.1" xref="S3.E3.m1.6.6.1.1.2.cmml">(</mo><mi id="S3.E3.m1.5.5" xref="S3.E3.m1.5.5.cmml">j</mi><mo stretchy="false" id="S3.E3.m1.6.6.1.1.2.3.2.2" xref="S3.E3.m1.6.6.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.6.6.1.1.1" xref="S3.E3.m1.6.6.1.1.1.cmml">=</mo><msubsup id="S3.E3.m1.6.6.1.1.3" xref="S3.E3.m1.6.6.1.1.3.cmml"><mrow id="S3.E3.m1.6.6.1.1.3.2.2.2" xref="S3.E3.m1.6.6.1.1.3.2.2.1.cmml"><mo id="S3.E3.m1.6.6.1.1.3.2.2.2.1" xref="S3.E3.m1.6.6.1.1.3.2.2.1.cmml">{</mo><mstyle displaystyle="true" id="S3.E3.m1.4.4" xref="S3.E3.m1.4.4.cmml"><mfrac id="S3.E3.m1.4.4a" xref="S3.E3.m1.4.4.cmml"><mrow id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.3" xref="S3.E3.m1.1.1.1.3.cmml"><msub id="S3.E3.m1.1.1.1.3.2" xref="S3.E3.m1.1.1.1.3.2.cmml"><mi id="S3.E3.m1.1.1.1.3.2.2" xref="S3.E3.m1.1.1.1.3.2.2.cmml">G</mi><mi id="S3.E3.m1.1.1.1.3.2.3" xref="S3.E3.m1.1.1.1.3.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.3.1" xref="S3.E3.m1.1.1.1.3.1.cmml">â€‹</mo><mrow id="S3.E3.m1.1.1.1.3.3.2" xref="S3.E3.m1.1.1.1.3.cmml"><mo stretchy="false" id="S3.E3.m1.1.1.1.3.3.2.1" xref="S3.E3.m1.1.1.1.3.cmml">(</mo><mi id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml">j</mi><mo rspace="0.055em" stretchy="false" id="S3.E3.m1.1.1.1.3.3.2.2" xref="S3.E3.m1.1.1.1.3.cmml">)</mo></mrow></mrow><mo rspace="0.222em" id="S3.E3.m1.1.1.1.2" xref="S3.E3.m1.1.1.1.2.cmml">â‹…</mo><msub id="S3.E3.m1.1.1.1.4" xref="S3.E3.m1.1.1.1.4.cmml"><mi id="S3.E3.m1.1.1.1.4.2" xref="S3.E3.m1.1.1.1.4.2.cmml">T</mi><mi id="S3.E3.m1.1.1.1.4.3" xref="S3.E3.m1.1.1.1.4.3.cmml">k</mi></msub></mrow><mrow id="S3.E3.m1.4.4.4" xref="S3.E3.m1.4.4.4.cmml"><msub id="S3.E3.m1.3.3.3.2" xref="S3.E3.m1.3.3.3.2.cmml"><mrow id="S3.E3.m1.3.3.3.2.1.1" xref="S3.E3.m1.3.3.3.2.1.2.cmml"><mo id="S3.E3.m1.3.3.3.2.1.1.2" xref="S3.E3.m1.3.3.3.2.1.2.1.cmml">â€–</mo><mrow id="S3.E3.m1.3.3.3.2.1.1.1" xref="S3.E3.m1.3.3.3.2.1.1.1.cmml"><msub id="S3.E3.m1.3.3.3.2.1.1.1.2" xref="S3.E3.m1.3.3.3.2.1.1.1.2.cmml"><mi id="S3.E3.m1.3.3.3.2.1.1.1.2.2" xref="S3.E3.m1.3.3.3.2.1.1.1.2.2.cmml">G</mi><mi id="S3.E3.m1.3.3.3.2.1.1.1.2.3" xref="S3.E3.m1.3.3.3.2.1.1.1.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.3.2.1.1.1.1" xref="S3.E3.m1.3.3.3.2.1.1.1.1.cmml">â€‹</mo><mrow id="S3.E3.m1.3.3.3.2.1.1.1.3.2" xref="S3.E3.m1.3.3.3.2.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.3.3.3.2.1.1.1.3.2.1" xref="S3.E3.m1.3.3.3.2.1.1.1.cmml">(</mo><mi id="S3.E3.m1.2.2.2.1" xref="S3.E3.m1.2.2.2.1.cmml">j</mi><mo stretchy="false" id="S3.E3.m1.3.3.3.2.1.1.1.3.2.2" xref="S3.E3.m1.3.3.3.2.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.3.3.3.2.1.1.3" xref="S3.E3.m1.3.3.3.2.1.2.1.cmml">â€–</mo></mrow><mn id="S3.E3.m1.3.3.3.2.3" xref="S3.E3.m1.3.3.3.2.3.cmml">2</mn></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.4.4.4" xref="S3.E3.m1.4.4.4.4.cmml">â€‹</mo><msub id="S3.E3.m1.4.4.4.3" xref="S3.E3.m1.4.4.4.3.cmml"><mrow id="S3.E3.m1.4.4.4.3.1.1" xref="S3.E3.m1.4.4.4.3.1.2.cmml"><mo id="S3.E3.m1.4.4.4.3.1.1.2" xref="S3.E3.m1.4.4.4.3.1.2.1.cmml">â€–</mo><msub id="S3.E3.m1.4.4.4.3.1.1.1" xref="S3.E3.m1.4.4.4.3.1.1.1.cmml"><mi id="S3.E3.m1.4.4.4.3.1.1.1.2" xref="S3.E3.m1.4.4.4.3.1.1.1.2.cmml">T</mi><mi id="S3.E3.m1.4.4.4.3.1.1.1.3" xref="S3.E3.m1.4.4.4.3.1.1.1.3.cmml">k</mi></msub><mo id="S3.E3.m1.4.4.4.3.1.1.3" xref="S3.E3.m1.4.4.4.3.1.2.1.cmml">â€–</mo></mrow><mn id="S3.E3.m1.4.4.4.3.3" xref="S3.E3.m1.4.4.4.3.3.cmml">2</mn></msub></mrow></mfrac></mstyle><mo id="S3.E3.m1.6.6.1.1.3.2.2.2.2" xref="S3.E3.m1.6.6.1.1.3.2.2.1.cmml">}</mo></mrow><mrow id="S3.E3.m1.6.6.1.1.3.2.3" xref="S3.E3.m1.6.6.1.1.3.2.3.cmml"><mi id="S3.E3.m1.6.6.1.1.3.2.3.2" xref="S3.E3.m1.6.6.1.1.3.2.3.2.cmml">k</mi><mo id="S3.E3.m1.6.6.1.1.3.2.3.1" xref="S3.E3.m1.6.6.1.1.3.2.3.1.cmml">=</mo><mn id="S3.E3.m1.6.6.1.1.3.2.3.3" xref="S3.E3.m1.6.6.1.1.3.2.3.3.cmml">1</mn></mrow><mi id="S3.E3.m1.6.6.1.1.3.3" xref="S3.E3.m1.6.6.1.1.3.3.cmml">K</mi></msubsup></mrow><mo lspace="0em" id="S3.E3.m1.6.6.1.2" xref="S3.E3.m1.6.6.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.6b"><apply id="S3.E3.m1.6.6.1.1.cmml" xref="S3.E3.m1.6.6.1"><eq id="S3.E3.m1.6.6.1.1.1.cmml" xref="S3.E3.m1.6.6.1.1.1"></eq><apply id="S3.E3.m1.6.6.1.1.2.cmml" xref="S3.E3.m1.6.6.1.1.2"><times id="S3.E3.m1.6.6.1.1.2.1.cmml" xref="S3.E3.m1.6.6.1.1.2.1"></times><apply id="S3.E3.m1.6.6.1.1.2.2.cmml" xref="S3.E3.m1.6.6.1.1.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.6.6.1.1.2.2.1.cmml" xref="S3.E3.m1.6.6.1.1.2.2">subscript</csymbol><ci id="S3.E3.m1.6.6.1.1.2.2.2.cmml" xref="S3.E3.m1.6.6.1.1.2.2.2">ğ‘†</ci><ci id="S3.E3.m1.6.6.1.1.2.2.3.cmml" xref="S3.E3.m1.6.6.1.1.2.2.3">ğ‘–</ci></apply><ci id="S3.E3.m1.5.5.cmml" xref="S3.E3.m1.5.5">ğ‘—</ci></apply><apply id="S3.E3.m1.6.6.1.1.3.cmml" xref="S3.E3.m1.6.6.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.6.6.1.1.3.1.cmml" xref="S3.E3.m1.6.6.1.1.3">superscript</csymbol><apply id="S3.E3.m1.6.6.1.1.3.2.cmml" xref="S3.E3.m1.6.6.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.6.6.1.1.3.2.1.cmml" xref="S3.E3.m1.6.6.1.1.3">subscript</csymbol><set id="S3.E3.m1.6.6.1.1.3.2.2.1.cmml" xref="S3.E3.m1.6.6.1.1.3.2.2.2"><apply id="S3.E3.m1.4.4.cmml" xref="S3.E3.m1.4.4"><divide id="S3.E3.m1.4.4.5.cmml" xref="S3.E3.m1.4.4"></divide><apply id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"><ci id="S3.E3.m1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.2">â‹…</ci><apply id="S3.E3.m1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.3"><times id="S3.E3.m1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.3.1"></times><apply id="S3.E3.m1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.3.2.1.cmml" xref="S3.E3.m1.1.1.1.3.2">subscript</csymbol><ci id="S3.E3.m1.1.1.1.3.2.2.cmml" xref="S3.E3.m1.1.1.1.3.2.2">ğº</ci><ci id="S3.E3.m1.1.1.1.3.2.3.cmml" xref="S3.E3.m1.1.1.1.3.2.3">ğ‘–</ci></apply><ci id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1">ğ‘—</ci></apply><apply id="S3.E3.m1.1.1.1.4.cmml" xref="S3.E3.m1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.4.1.cmml" xref="S3.E3.m1.1.1.1.4">subscript</csymbol><ci id="S3.E3.m1.1.1.1.4.2.cmml" xref="S3.E3.m1.1.1.1.4.2">ğ‘‡</ci><ci id="S3.E3.m1.1.1.1.4.3.cmml" xref="S3.E3.m1.1.1.1.4.3">ğ‘˜</ci></apply></apply><apply id="S3.E3.m1.4.4.4.cmml" xref="S3.E3.m1.4.4.4"><times id="S3.E3.m1.4.4.4.4.cmml" xref="S3.E3.m1.4.4.4.4"></times><apply id="S3.E3.m1.3.3.3.2.cmml" xref="S3.E3.m1.3.3.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.3.2.2.cmml" xref="S3.E3.m1.3.3.3.2">subscript</csymbol><apply id="S3.E3.m1.3.3.3.2.1.2.cmml" xref="S3.E3.m1.3.3.3.2.1.1"><csymbol cd="latexml" id="S3.E3.m1.3.3.3.2.1.2.1.cmml" xref="S3.E3.m1.3.3.3.2.1.1.2">norm</csymbol><apply id="S3.E3.m1.3.3.3.2.1.1.1.cmml" xref="S3.E3.m1.3.3.3.2.1.1.1"><times id="S3.E3.m1.3.3.3.2.1.1.1.1.cmml" xref="S3.E3.m1.3.3.3.2.1.1.1.1"></times><apply id="S3.E3.m1.3.3.3.2.1.1.1.2.cmml" xref="S3.E3.m1.3.3.3.2.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.3.2.1.1.1.2.1.cmml" xref="S3.E3.m1.3.3.3.2.1.1.1.2">subscript</csymbol><ci id="S3.E3.m1.3.3.3.2.1.1.1.2.2.cmml" xref="S3.E3.m1.3.3.3.2.1.1.1.2.2">ğº</ci><ci id="S3.E3.m1.3.3.3.2.1.1.1.2.3.cmml" xref="S3.E3.m1.3.3.3.2.1.1.1.2.3">ğ‘–</ci></apply><ci id="S3.E3.m1.2.2.2.1.cmml" xref="S3.E3.m1.2.2.2.1">ğ‘—</ci></apply></apply><cn type="integer" id="S3.E3.m1.3.3.3.2.3.cmml" xref="S3.E3.m1.3.3.3.2.3">2</cn></apply><apply id="S3.E3.m1.4.4.4.3.cmml" xref="S3.E3.m1.4.4.4.3"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.4.3.2.cmml" xref="S3.E3.m1.4.4.4.3">subscript</csymbol><apply id="S3.E3.m1.4.4.4.3.1.2.cmml" xref="S3.E3.m1.4.4.4.3.1.1"><csymbol cd="latexml" id="S3.E3.m1.4.4.4.3.1.2.1.cmml" xref="S3.E3.m1.4.4.4.3.1.1.2">norm</csymbol><apply id="S3.E3.m1.4.4.4.3.1.1.1.cmml" xref="S3.E3.m1.4.4.4.3.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.4.3.1.1.1.1.cmml" xref="S3.E3.m1.4.4.4.3.1.1.1">subscript</csymbol><ci id="S3.E3.m1.4.4.4.3.1.1.1.2.cmml" xref="S3.E3.m1.4.4.4.3.1.1.1.2">ğ‘‡</ci><ci id="S3.E3.m1.4.4.4.3.1.1.1.3.cmml" xref="S3.E3.m1.4.4.4.3.1.1.1.3">ğ‘˜</ci></apply></apply><cn type="integer" id="S3.E3.m1.4.4.4.3.3.cmml" xref="S3.E3.m1.4.4.4.3.3">2</cn></apply></apply></apply></set><apply id="S3.E3.m1.6.6.1.1.3.2.3.cmml" xref="S3.E3.m1.6.6.1.1.3.2.3"><eq id="S3.E3.m1.6.6.1.1.3.2.3.1.cmml" xref="S3.E3.m1.6.6.1.1.3.2.3.1"></eq><ci id="S3.E3.m1.6.6.1.1.3.2.3.2.cmml" xref="S3.E3.m1.6.6.1.1.3.2.3.2">ğ‘˜</ci><cn type="integer" id="S3.E3.m1.6.6.1.1.3.2.3.3.cmml" xref="S3.E3.m1.6.6.1.1.3.2.3.3">1</cn></apply></apply><ci id="S3.E3.m1.6.6.1.1.3.3.cmml" xref="S3.E3.m1.6.6.1.1.3.3">ğ¾</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.6c">\displaystyle S_{i}(j)=\left\{\dfrac{G_{i}(j)\cdot T_{k}}{{\left\|G_{i}(j)\right\|_{2}\left\|T_{k}\right\|_{2}}}\right\}_{k=1}^{K}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.p4" class="ltx_para ltx_noindent">
<p id="S3.SS2.p4.2" class="ltx_p">Finally, the grid-level score <math id="S3.SS2.p4.1.m1.1" class="ltx_Math" alttext="\left\{{S}_{i}\right\}_{i=3}^{7}" display="inline"><semantics id="S3.SS2.p4.1.m1.1a"><msubsup id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml"><mrow id="S3.SS2.p4.1.m1.1.1.1.1.1" xref="S3.SS2.p4.1.m1.1.1.1.1.2.cmml"><mo id="S3.SS2.p4.1.m1.1.1.1.1.1.2" xref="S3.SS2.p4.1.m1.1.1.1.1.2.cmml">{</mo><msub id="S3.SS2.p4.1.m1.1.1.1.1.1.1" xref="S3.SS2.p4.1.m1.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p4.1.m1.1.1.1.1.1.1.2" xref="S3.SS2.p4.1.m1.1.1.1.1.1.1.2.cmml">S</mi><mi id="S3.SS2.p4.1.m1.1.1.1.1.1.1.3" xref="S3.SS2.p4.1.m1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS2.p4.1.m1.1.1.1.1.1.3" xref="S3.SS2.p4.1.m1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS2.p4.1.m1.1.1.1.3" xref="S3.SS2.p4.1.m1.1.1.1.3.cmml"><mi id="S3.SS2.p4.1.m1.1.1.1.3.2" xref="S3.SS2.p4.1.m1.1.1.1.3.2.cmml">i</mi><mo id="S3.SS2.p4.1.m1.1.1.1.3.1" xref="S3.SS2.p4.1.m1.1.1.1.3.1.cmml">=</mo><mn id="S3.SS2.p4.1.m1.1.1.1.3.3" xref="S3.SS2.p4.1.m1.1.1.1.3.3.cmml">3</mn></mrow><mn id="S3.SS2.p4.1.m1.1.1.3" xref="S3.SS2.p4.1.m1.1.1.3.cmml">7</mn></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><apply id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.1.m1.1.1.2.cmml" xref="S3.SS2.p4.1.m1.1.1">superscript</csymbol><apply id="S3.SS2.p4.1.m1.1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.1.m1.1.1.1.2.cmml" xref="S3.SS2.p4.1.m1.1.1">subscript</csymbol><set id="S3.SS2.p4.1.m1.1.1.1.1.2.cmml" xref="S3.SS2.p4.1.m1.1.1.1.1.1"><apply id="S3.SS2.p4.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p4.1.m1.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p4.1.m1.1.1.1.1.1.1.2">ğ‘†</ci><ci id="S3.SS2.p4.1.m1.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p4.1.m1.1.1.1.1.1.1.3">ğ‘–</ci></apply></set><apply id="S3.SS2.p4.1.m1.1.1.1.3.cmml" xref="S3.SS2.p4.1.m1.1.1.1.3"><eq id="S3.SS2.p4.1.m1.1.1.1.3.1.cmml" xref="S3.SS2.p4.1.m1.1.1.1.3.1"></eq><ci id="S3.SS2.p4.1.m1.1.1.1.3.2.cmml" xref="S3.SS2.p4.1.m1.1.1.1.3.2">ğ‘–</ci><cn type="integer" id="S3.SS2.p4.1.m1.1.1.1.3.3.cmml" xref="S3.SS2.p4.1.m1.1.1.1.3.3">3</cn></apply></apply><cn type="integer" id="S3.SS2.p4.1.m1.1.1.3.cmml" xref="S3.SS2.p4.1.m1.1.1.3">7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">\left\{{S}_{i}\right\}_{i=3}^{7}</annotation></semantics></math> is treated as the original classification output and aligned to the ground-truth target <math id="S3.SS2.p4.2.m2.1" class="ltx_Math" alttext="\left\{{Target}_{i}\right\}_{i=3}^{7}" display="inline"><semantics id="S3.SS2.p4.2.m2.1a"><msubsup id="S3.SS2.p4.2.m2.1.1" xref="S3.SS2.p4.2.m2.1.1.cmml"><mrow id="S3.SS2.p4.2.m2.1.1.1.1.1" xref="S3.SS2.p4.2.m2.1.1.1.1.2.cmml"><mo id="S3.SS2.p4.2.m2.1.1.1.1.1.2" xref="S3.SS2.p4.2.m2.1.1.1.1.2.cmml">{</mo><mrow id="S3.SS2.p4.2.m2.1.1.1.1.1.1" xref="S3.SS2.p4.2.m2.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p4.2.m2.1.1.1.1.1.1.2" xref="S3.SS2.p4.2.m2.1.1.1.1.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.2.m2.1.1.1.1.1.1.1" xref="S3.SS2.p4.2.m2.1.1.1.1.1.1.1.cmml">â€‹</mo><mi id="S3.SS2.p4.2.m2.1.1.1.1.1.1.3" xref="S3.SS2.p4.2.m2.1.1.1.1.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.2.m2.1.1.1.1.1.1.1a" xref="S3.SS2.p4.2.m2.1.1.1.1.1.1.1.cmml">â€‹</mo><mi id="S3.SS2.p4.2.m2.1.1.1.1.1.1.4" xref="S3.SS2.p4.2.m2.1.1.1.1.1.1.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.2.m2.1.1.1.1.1.1.1b" xref="S3.SS2.p4.2.m2.1.1.1.1.1.1.1.cmml">â€‹</mo><mi id="S3.SS2.p4.2.m2.1.1.1.1.1.1.5" xref="S3.SS2.p4.2.m2.1.1.1.1.1.1.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.2.m2.1.1.1.1.1.1.1c" xref="S3.SS2.p4.2.m2.1.1.1.1.1.1.1.cmml">â€‹</mo><mi id="S3.SS2.p4.2.m2.1.1.1.1.1.1.6" xref="S3.SS2.p4.2.m2.1.1.1.1.1.1.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.2.m2.1.1.1.1.1.1.1d" xref="S3.SS2.p4.2.m2.1.1.1.1.1.1.1.cmml">â€‹</mo><msub id="S3.SS2.p4.2.m2.1.1.1.1.1.1.7" xref="S3.SS2.p4.2.m2.1.1.1.1.1.1.7.cmml"><mi id="S3.SS2.p4.2.m2.1.1.1.1.1.1.7.2" xref="S3.SS2.p4.2.m2.1.1.1.1.1.1.7.2.cmml">t</mi><mi id="S3.SS2.p4.2.m2.1.1.1.1.1.1.7.3" xref="S3.SS2.p4.2.m2.1.1.1.1.1.1.7.3.cmml">i</mi></msub></mrow><mo id="S3.SS2.p4.2.m2.1.1.1.1.1.3" xref="S3.SS2.p4.2.m2.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS2.p4.2.m2.1.1.1.3" xref="S3.SS2.p4.2.m2.1.1.1.3.cmml"><mi id="S3.SS2.p4.2.m2.1.1.1.3.2" xref="S3.SS2.p4.2.m2.1.1.1.3.2.cmml">i</mi><mo id="S3.SS2.p4.2.m2.1.1.1.3.1" xref="S3.SS2.p4.2.m2.1.1.1.3.1.cmml">=</mo><mn id="S3.SS2.p4.2.m2.1.1.1.3.3" xref="S3.SS2.p4.2.m2.1.1.1.3.3.cmml">3</mn></mrow><mn id="S3.SS2.p4.2.m2.1.1.3" xref="S3.SS2.p4.2.m2.1.1.3.cmml">7</mn></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.1b"><apply id="S3.SS2.p4.2.m2.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.2.m2.1.1.2.cmml" xref="S3.SS2.p4.2.m2.1.1">superscript</csymbol><apply id="S3.SS2.p4.2.m2.1.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.2.m2.1.1.1.2.cmml" xref="S3.SS2.p4.2.m2.1.1">subscript</csymbol><set id="S3.SS2.p4.2.m2.1.1.1.1.2.cmml" xref="S3.SS2.p4.2.m2.1.1.1.1.1"><apply id="S3.SS2.p4.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1.1.1.1.1"><times id="S3.SS2.p4.2.m2.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1.1.1.1.1.1"></times><ci id="S3.SS2.p4.2.m2.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p4.2.m2.1.1.1.1.1.1.2">ğ‘‡</ci><ci id="S3.SS2.p4.2.m2.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p4.2.m2.1.1.1.1.1.1.3">ğ‘</ci><ci id="S3.SS2.p4.2.m2.1.1.1.1.1.1.4.cmml" xref="S3.SS2.p4.2.m2.1.1.1.1.1.1.4">ğ‘Ÿ</ci><ci id="S3.SS2.p4.2.m2.1.1.1.1.1.1.5.cmml" xref="S3.SS2.p4.2.m2.1.1.1.1.1.1.5">ğ‘”</ci><ci id="S3.SS2.p4.2.m2.1.1.1.1.1.1.6.cmml" xref="S3.SS2.p4.2.m2.1.1.1.1.1.1.6">ğ‘’</ci><apply id="S3.SS2.p4.2.m2.1.1.1.1.1.1.7.cmml" xref="S3.SS2.p4.2.m2.1.1.1.1.1.1.7"><csymbol cd="ambiguous" id="S3.SS2.p4.2.m2.1.1.1.1.1.1.7.1.cmml" xref="S3.SS2.p4.2.m2.1.1.1.1.1.1.7">subscript</csymbol><ci id="S3.SS2.p4.2.m2.1.1.1.1.1.1.7.2.cmml" xref="S3.SS2.p4.2.m2.1.1.1.1.1.1.7.2">ğ‘¡</ci><ci id="S3.SS2.p4.2.m2.1.1.1.1.1.1.7.3.cmml" xref="S3.SS2.p4.2.m2.1.1.1.1.1.1.7.3">ğ‘–</ci></apply></apply></set><apply id="S3.SS2.p4.2.m2.1.1.1.3.cmml" xref="S3.SS2.p4.2.m2.1.1.1.3"><eq id="S3.SS2.p4.2.m2.1.1.1.3.1.cmml" xref="S3.SS2.p4.2.m2.1.1.1.3.1"></eq><ci id="S3.SS2.p4.2.m2.1.1.1.3.2.cmml" xref="S3.SS2.p4.2.m2.1.1.1.3.2">ğ‘–</ci><cn type="integer" id="S3.SS2.p4.2.m2.1.1.1.3.3.cmml" xref="S3.SS2.p4.2.m2.1.1.1.3.3">3</cn></apply></apply><cn type="integer" id="S3.SS2.p4.2.m2.1.1.3.cmml" xref="S3.SS2.p4.2.m2.1.1.3">7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.1c">\left\{{Target}_{i}\right\}_{i=3}^{7}</annotation></semantics></math> using Focal LossÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> as in the original FCOS.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Image-level Alignment</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.2" class="ltx_p">With grid-level alignment, the image
grids
of the base categories are mapped to the CLIP alignment space,
by aligning their embeddings to the corresponding text embeddings <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="\left\{T_{k}\right\}_{k=1}^{K}" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><msubsup id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mrow id="S3.SS3.p1.1.m1.1.1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.1.2.cmml"><mo id="S3.SS3.p1.1.m1.1.1.1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.1.1.2.cmml">{</mo><msub id="S3.SS3.p1.1.m1.1.1.1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.cmml"><mi id="S3.SS3.p1.1.m1.1.1.1.1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.2.cmml">T</mi><mi id="S3.SS3.p1.1.m1.1.1.1.1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.3.cmml">k</mi></msub><mo id="S3.SS3.p1.1.m1.1.1.1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS3.p1.1.m1.1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.1.3.cmml"><mi id="S3.SS3.p1.1.m1.1.1.1.3.2" xref="S3.SS3.p1.1.m1.1.1.1.3.2.cmml">k</mi><mo id="S3.SS3.p1.1.m1.1.1.1.3.1" xref="S3.SS3.p1.1.m1.1.1.1.3.1.cmml">=</mo><mn id="S3.SS3.p1.1.m1.1.1.1.3.3" xref="S3.SS3.p1.1.m1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml">K</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1">superscript</csymbol><apply id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1">subscript</csymbol><set id="S3.SS3.p1.1.m1.1.1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.1"><apply id="S3.SS3.p1.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.1.1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.2">ğ‘‡</ci><ci id="S3.SS3.p1.1.m1.1.1.1.1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.3">ğ‘˜</ci></apply></set><apply id="S3.SS3.p1.1.m1.1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.1.3"><eq id="S3.SS3.p1.1.m1.1.1.1.3.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1.3.1"></eq><ci id="S3.SS3.p1.1.m1.1.1.1.3.2.cmml" xref="S3.SS3.p1.1.m1.1.1.1.3.2">ğ‘˜</ci><cn type="integer" id="S3.SS3.p1.1.m1.1.1.1.3.3.cmml" xref="S3.SS3.p1.1.m1.1.1.1.3.3">1</cn></apply></apply><ci id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3">ğ¾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">\left\{T_{k}\right\}_{k=1}^{K}</annotation></semantics></math>.
While for
grids of novel categories,
there are no corresponding text embeddings to align to,
which can only learn their embeddings by minimizing their similarity to any of <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="\left\{T_{k}\right\}_{k=1}^{K}" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><msubsup id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml"><mrow id="S3.SS3.p1.2.m2.1.1.1.1.1" xref="S3.SS3.p1.2.m2.1.1.1.1.2.cmml"><mo id="S3.SS3.p1.2.m2.1.1.1.1.1.2" xref="S3.SS3.p1.2.m2.1.1.1.1.2.cmml">{</mo><msub id="S3.SS3.p1.2.m2.1.1.1.1.1.1" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.cmml"><mi id="S3.SS3.p1.2.m2.1.1.1.1.1.1.2" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.2.cmml">T</mi><mi id="S3.SS3.p1.2.m2.1.1.1.1.1.1.3" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.3.cmml">k</mi></msub><mo id="S3.SS3.p1.2.m2.1.1.1.1.1.3" xref="S3.SS3.p1.2.m2.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS3.p1.2.m2.1.1.1.3" xref="S3.SS3.p1.2.m2.1.1.1.3.cmml"><mi id="S3.SS3.p1.2.m2.1.1.1.3.2" xref="S3.SS3.p1.2.m2.1.1.1.3.2.cmml">k</mi><mo id="S3.SS3.p1.2.m2.1.1.1.3.1" xref="S3.SS3.p1.2.m2.1.1.1.3.1.cmml">=</mo><mn id="S3.SS3.p1.2.m2.1.1.1.3.3" xref="S3.SS3.p1.2.m2.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.SS3.p1.2.m2.1.1.3" xref="S3.SS3.p1.2.m2.1.1.3.cmml">K</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><apply id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1">superscript</csymbol><apply id="S3.SS3.p1.2.m2.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1">subscript</csymbol><set id="S3.SS3.p1.2.m2.1.1.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.1"><apply id="S3.SS3.p1.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p1.2.m2.1.1.1.1.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.2">ğ‘‡</ci><ci id="S3.SS3.p1.2.m2.1.1.1.1.1.1.3.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.3">ğ‘˜</ci></apply></set><apply id="S3.SS3.p1.2.m2.1.1.1.3.cmml" xref="S3.SS3.p1.2.m2.1.1.1.3"><eq id="S3.SS3.p1.2.m2.1.1.1.3.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1.3.1"></eq><ci id="S3.SS3.p1.2.m2.1.1.1.3.2.cmml" xref="S3.SS3.p1.2.m2.1.1.1.3.2">ğ‘˜</ci><cn type="integer" id="S3.SS3.p1.2.m2.1.1.1.3.3.cmml" xref="S3.SS3.p1.2.m2.1.1.1.3.3">1</cn></apply></apply><ci id="S3.SS3.p1.2.m2.1.1.3.cmml" xref="S3.SS3.p1.2.m2.1.1.3">ğ¾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">\left\{T_{k}\right\}_{k=1}^{K}</annotation></semantics></math> during training.
However, since the embeddings of different novel categories
can have different similarities to each base category,
simply minimizing the similarities
between the base and novel categories
is not consistent with the CLIP representation space
which presents a generalizable knowledge representation.
Therefore, ignoring the alignment of novel categories
may limit the ability to encode a wide range of novel visual concepts, which
harms the generalization ability of the model.
</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.9" class="ltx_p">In practice, inspired by ViLDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>,
we align the image-level embedding <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="{\overline{z}}" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mover accent="true" id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mi id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">z</mi><mo id="S3.SS3.p2.1.m1.1.1.1" xref="S3.SS3.p2.1.m1.1.1.1.cmml">Â¯</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><ci id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1">Â¯</ci><ci id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">ğ‘§</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">{\overline{z}}</annotation></semantics></math>
to the embedding <math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="{\overline{z}_{\rm CLIP}}" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><msub id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml"><mover accent="true" id="S3.SS3.p2.2.m2.1.1.2" xref="S3.SS3.p2.2.m2.1.1.2.cmml"><mi id="S3.SS3.p2.2.m2.1.1.2.2" xref="S3.SS3.p2.2.m2.1.1.2.2.cmml">z</mi><mo id="S3.SS3.p2.2.m2.1.1.2.1" xref="S3.SS3.p2.2.m2.1.1.2.1.cmml">Â¯</mo></mover><mi id="S3.SS3.p2.2.m2.1.1.3" xref="S3.SS3.p2.2.m2.1.1.3.cmml">CLIP</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><apply id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">subscript</csymbol><apply id="S3.SS3.p2.2.m2.1.1.2.cmml" xref="S3.SS3.p2.2.m2.1.1.2"><ci id="S3.SS3.p2.2.m2.1.1.2.1.cmml" xref="S3.SS3.p2.2.m2.1.1.2.1">Â¯</ci><ci id="S3.SS3.p2.2.m2.1.1.2.2.cmml" xref="S3.SS3.p2.2.m2.1.1.2.2">ğ‘§</ci></apply><ci id="S3.SS3.p2.2.m2.1.1.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3">CLIP</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">{\overline{z}_{\rm CLIP}}</annotation></semantics></math> produced by a fixed CLIP image encoder,
so that the regions of novel categories in an image can also
be projected to the CLIP alignment space.
Different from ViLD that
aligns the embedding of several proposed regions in an image
provided by a separate region proposal network (RPN), hence two-stage,
which is the source of significant extra computational costs due to
its requirement for multiple inferences of the image encoder for each
object proposal, we directly align the embedding of the whole
image <math id="S3.SS3.p2.3.m3.1" class="ltx_Math" alttext="{\overline{z}^{\prime}}" display="inline"><semantics id="S3.SS3.p2.3.m3.1a"><msup id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml"><mover accent="true" id="S3.SS3.p2.3.m3.1.1.2" xref="S3.SS3.p2.3.m3.1.1.2.cmml"><mi id="S3.SS3.p2.3.m3.1.1.2.2" xref="S3.SS3.p2.3.m3.1.1.2.2.cmml">z</mi><mo id="S3.SS3.p2.3.m3.1.1.2.1" xref="S3.SS3.p2.3.m3.1.1.2.1.cmml">Â¯</mo></mover><mo id="S3.SS3.p2.3.m3.1.1.3" xref="S3.SS3.p2.3.m3.1.1.3.cmml">â€²</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><apply id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.1.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1">superscript</csymbol><apply id="S3.SS3.p2.3.m3.1.1.2.cmml" xref="S3.SS3.p2.3.m3.1.1.2"><ci id="S3.SS3.p2.3.m3.1.1.2.1.cmml" xref="S3.SS3.p2.3.m3.1.1.2.1">Â¯</ci><ci id="S3.SS3.p2.3.m3.1.1.2.2.cmml" xref="S3.SS3.p2.3.m3.1.1.2.2">ğ‘§</ci></apply><ci id="S3.SS3.p2.3.m3.1.1.3.cmml" xref="S3.SS3.p2.3.m3.1.1.3">â€²</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">{\overline{z}^{\prime}}</annotation></semantics></math> without the need for multiple passes.
Specifically,
similar to grid-level alignment, we generate the image-level embedding <math id="S3.SS3.p2.4.m4.1" class="ltx_Math" alttext="\overline{{z}^{\prime}}" display="inline"><semantics id="S3.SS3.p2.4.m4.1a"><mover accent="true" id="S3.SS3.p2.4.m4.1.1" xref="S3.SS3.p2.4.m4.1.1.cmml"><msup id="S3.SS3.p2.4.m4.1.1.2" xref="S3.SS3.p2.4.m4.1.1.2.cmml"><mi id="S3.SS3.p2.4.m4.1.1.2.2" xref="S3.SS3.p2.4.m4.1.1.2.2.cmml">z</mi><mo id="S3.SS3.p2.4.m4.1.1.2.3" xref="S3.SS3.p2.4.m4.1.1.2.3.cmml">â€²</mo></msup><mo id="S3.SS3.p2.4.m4.1.1.1" xref="S3.SS3.p2.4.m4.1.1.1.cmml">Â¯</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.1b"><apply id="S3.SS3.p2.4.m4.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1"><ci id="S3.SS3.p2.4.m4.1.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1.1">Â¯</ci><apply id="S3.SS3.p2.4.m4.1.1.2.cmml" xref="S3.SS3.p2.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p2.4.m4.1.1.2.1.cmml" xref="S3.SS3.p2.4.m4.1.1.2">superscript</csymbol><ci id="S3.SS3.p2.4.m4.1.1.2.2.cmml" xref="S3.SS3.p2.4.m4.1.1.2.2">ğ‘§</ci><ci id="S3.SS3.p2.4.m4.1.1.2.3.cmml" xref="S3.SS3.p2.4.m4.1.1.2.3">â€²</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.1c">\overline{{z}^{\prime}}</annotation></semantics></math>
from <math id="S3.SS3.p2.5.m5.1" class="ltx_Math" alttext="\overline{{z}}" display="inline"><semantics id="S3.SS3.p2.5.m5.1a"><mover accent="true" id="S3.SS3.p2.5.m5.1.1" xref="S3.SS3.p2.5.m5.1.1.cmml"><mi id="S3.SS3.p2.5.m5.1.1.2" xref="S3.SS3.p2.5.m5.1.1.2.cmml">z</mi><mo id="S3.SS3.p2.5.m5.1.1.1" xref="S3.SS3.p2.5.m5.1.1.1.cmml">Â¯</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.5.m5.1b"><apply id="S3.SS3.p2.5.m5.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1"><ci id="S3.SS3.p2.5.m5.1.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1.1">Â¯</ci><ci id="S3.SS3.p2.5.m5.1.1.2.cmml" xref="S3.SS3.p2.5.m5.1.1.2">ğ‘§</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m5.1c">\overline{{z}}</annotation></semantics></math> going through three consecutive linear layers with the ReLU activation function.
Then we maximize the <math id="S3.SS3.p2.6.m6.1" class="ltx_Math" alttext="L_{1}" display="inline"><semantics id="S3.SS3.p2.6.m6.1a"><msub id="S3.SS3.p2.6.m6.1.1" xref="S3.SS3.p2.6.m6.1.1.cmml"><mi id="S3.SS3.p2.6.m6.1.1.2" xref="S3.SS3.p2.6.m6.1.1.2.cmml">L</mi><mn id="S3.SS3.p2.6.m6.1.1.3" xref="S3.SS3.p2.6.m6.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.6.m6.1b"><apply id="S3.SS3.p2.6.m6.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.6.m6.1.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1">subscript</csymbol><ci id="S3.SS3.p2.6.m6.1.1.2.cmml" xref="S3.SS3.p2.6.m6.1.1.2">ğ¿</ci><cn type="integer" id="S3.SS3.p2.6.m6.1.1.3.cmml" xref="S3.SS3.p2.6.m6.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.6.m6.1c">L_{1}</annotation></semantics></math> similarity between <math id="S3.SS3.p2.7.m7.1" class="ltx_Math" alttext="{\overline{z}}" display="inline"><semantics id="S3.SS3.p2.7.m7.1a"><mover accent="true" id="S3.SS3.p2.7.m7.1.1" xref="S3.SS3.p2.7.m7.1.1.cmml"><mi id="S3.SS3.p2.7.m7.1.1.2" xref="S3.SS3.p2.7.m7.1.1.2.cmml">z</mi><mo id="S3.SS3.p2.7.m7.1.1.1" xref="S3.SS3.p2.7.m7.1.1.1.cmml">Â¯</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.7.m7.1b"><apply id="S3.SS3.p2.7.m7.1.1.cmml" xref="S3.SS3.p2.7.m7.1.1"><ci id="S3.SS3.p2.7.m7.1.1.1.cmml" xref="S3.SS3.p2.7.m7.1.1.1">Â¯</ci><ci id="S3.SS3.p2.7.m7.1.1.2.cmml" xref="S3.SS3.p2.7.m7.1.1.2">ğ‘§</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.7.m7.1c">{\overline{z}}</annotation></semantics></math> and <math id="S3.SS3.p2.8.m8.1" class="ltx_Math" alttext="{\overline{z}_{\rm CLIP}}" display="inline"><semantics id="S3.SS3.p2.8.m8.1a"><msub id="S3.SS3.p2.8.m8.1.1" xref="S3.SS3.p2.8.m8.1.1.cmml"><mover accent="true" id="S3.SS3.p2.8.m8.1.1.2" xref="S3.SS3.p2.8.m8.1.1.2.cmml"><mi id="S3.SS3.p2.8.m8.1.1.2.2" xref="S3.SS3.p2.8.m8.1.1.2.2.cmml">z</mi><mo id="S3.SS3.p2.8.m8.1.1.2.1" xref="S3.SS3.p2.8.m8.1.1.2.1.cmml">Â¯</mo></mover><mi id="S3.SS3.p2.8.m8.1.1.3" xref="S3.SS3.p2.8.m8.1.1.3.cmml">CLIP</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.8.m8.1b"><apply id="S3.SS3.p2.8.m8.1.1.cmml" xref="S3.SS3.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.8.m8.1.1.1.cmml" xref="S3.SS3.p2.8.m8.1.1">subscript</csymbol><apply id="S3.SS3.p2.8.m8.1.1.2.cmml" xref="S3.SS3.p2.8.m8.1.1.2"><ci id="S3.SS3.p2.8.m8.1.1.2.1.cmml" xref="S3.SS3.p2.8.m8.1.1.2.1">Â¯</ci><ci id="S3.SS3.p2.8.m8.1.1.2.2.cmml" xref="S3.SS3.p2.8.m8.1.1.2.2">ğ‘§</ci></apply><ci id="S3.SS3.p2.8.m8.1.1.3.cmml" xref="S3.SS3.p2.8.m8.1.1.3">CLIP</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.8.m8.1c">{\overline{z}_{\rm CLIP}}</annotation></semantics></math> , the reverse of which is the image-level alignment loss <math id="S3.SS3.p2.9.m9.1" class="ltx_Math" alttext="L_{\rm image}" display="inline"><semantics id="S3.SS3.p2.9.m9.1a"><msub id="S3.SS3.p2.9.m9.1.1" xref="S3.SS3.p2.9.m9.1.1.cmml"><mi id="S3.SS3.p2.9.m9.1.1.2" xref="S3.SS3.p2.9.m9.1.1.2.cmml">L</mi><mi id="S3.SS3.p2.9.m9.1.1.3" xref="S3.SS3.p2.9.m9.1.1.3.cmml">image</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.9.m9.1b"><apply id="S3.SS3.p2.9.m9.1.1.cmml" xref="S3.SS3.p2.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.9.m9.1.1.1.cmml" xref="S3.SS3.p2.9.m9.1.1">subscript</csymbol><ci id="S3.SS3.p2.9.m9.1.1.2.cmml" xref="S3.SS3.p2.9.m9.1.1.2">ğ¿</ci><ci id="S3.SS3.p2.9.m9.1.1.3.cmml" xref="S3.SS3.p2.9.m9.1.1.3">image</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.9.m9.1c">L_{\rm image}</annotation></semantics></math>.
As for the fixed CLIP image encoder, we evaluate different published versions of pretrained models
and choose the ViT-B/32 version for alignment.
Note that image-level alignment is only performed during the training phase.
</p>
</div>
<div id="S3.SS3.p3" class="ltx_para ltx_noindent">
<p id="S3.SS3.p3.3" class="ltx_p">Finally, the total loss for training GridCLIP end-to-end is,</p>
<table id="S5.EGx4" class="ltx_equationgroup ltx_eqn_eqnarray ltx_eqn_table">

<tbody id="S3.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E4.m1.1" class="ltx_Math" alttext="\displaystyle L=w_{\rm grid}L_{\rm grid}+w_{\rm image}L_{\rm image}+L_{\rm R}+L_{\rm C}," display="inline"><semantics id="S3.E4.m1.1a"><mrow id="S3.E4.m1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml"><mrow id="S3.E4.m1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml"><mi id="S3.E4.m1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.2.cmml">L</mi><mo id="S3.E4.m1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.cmml">=</mo><mrow id="S3.E4.m1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.3.cmml"><mrow id="S3.E4.m1.1.1.1.1.3.2" xref="S3.E4.m1.1.1.1.1.3.2.cmml"><msub id="S3.E4.m1.1.1.1.1.3.2.2" xref="S3.E4.m1.1.1.1.1.3.2.2.cmml"><mi id="S3.E4.m1.1.1.1.1.3.2.2.2" xref="S3.E4.m1.1.1.1.1.3.2.2.2.cmml">w</mi><mi id="S3.E4.m1.1.1.1.1.3.2.2.3" xref="S3.E4.m1.1.1.1.1.3.2.2.3.cmml">grid</mi></msub><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.1.3.2.1" xref="S3.E4.m1.1.1.1.1.3.2.1.cmml">â€‹</mo><msub id="S3.E4.m1.1.1.1.1.3.2.3" xref="S3.E4.m1.1.1.1.1.3.2.3.cmml"><mi id="S3.E4.m1.1.1.1.1.3.2.3.2" xref="S3.E4.m1.1.1.1.1.3.2.3.2.cmml">L</mi><mi id="S3.E4.m1.1.1.1.1.3.2.3.3" xref="S3.E4.m1.1.1.1.1.3.2.3.3.cmml">grid</mi></msub></mrow><mo id="S3.E4.m1.1.1.1.1.3.1" xref="S3.E4.m1.1.1.1.1.3.1.cmml">+</mo><mrow id="S3.E4.m1.1.1.1.1.3.3" xref="S3.E4.m1.1.1.1.1.3.3.cmml"><msub id="S3.E4.m1.1.1.1.1.3.3.2" xref="S3.E4.m1.1.1.1.1.3.3.2.cmml"><mi id="S3.E4.m1.1.1.1.1.3.3.2.2" xref="S3.E4.m1.1.1.1.1.3.3.2.2.cmml">w</mi><mi id="S3.E4.m1.1.1.1.1.3.3.2.3" xref="S3.E4.m1.1.1.1.1.3.3.2.3.cmml">image</mi></msub><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.1.3.3.1" xref="S3.E4.m1.1.1.1.1.3.3.1.cmml">â€‹</mo><msub id="S3.E4.m1.1.1.1.1.3.3.3" xref="S3.E4.m1.1.1.1.1.3.3.3.cmml"><mi id="S3.E4.m1.1.1.1.1.3.3.3.2" xref="S3.E4.m1.1.1.1.1.3.3.3.2.cmml">L</mi><mi id="S3.E4.m1.1.1.1.1.3.3.3.3" xref="S3.E4.m1.1.1.1.1.3.3.3.3.cmml">image</mi></msub></mrow><mo id="S3.E4.m1.1.1.1.1.3.1a" xref="S3.E4.m1.1.1.1.1.3.1.cmml">+</mo><msub id="S3.E4.m1.1.1.1.1.3.4" xref="S3.E4.m1.1.1.1.1.3.4.cmml"><mi id="S3.E4.m1.1.1.1.1.3.4.2" xref="S3.E4.m1.1.1.1.1.3.4.2.cmml">L</mi><mi mathvariant="normal" id="S3.E4.m1.1.1.1.1.3.4.3" xref="S3.E4.m1.1.1.1.1.3.4.3.cmml">R</mi></msub><mo id="S3.E4.m1.1.1.1.1.3.1b" xref="S3.E4.m1.1.1.1.1.3.1.cmml">+</mo><msub id="S3.E4.m1.1.1.1.1.3.5" xref="S3.E4.m1.1.1.1.1.3.5.cmml"><mi id="S3.E4.m1.1.1.1.1.3.5.2" xref="S3.E4.m1.1.1.1.1.3.5.2.cmml">L</mi><mi mathvariant="normal" id="S3.E4.m1.1.1.1.1.3.5.3" xref="S3.E4.m1.1.1.1.1.3.5.3.cmml">C</mi></msub></mrow></mrow><mo id="S3.E4.m1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1"><eq id="S3.E4.m1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1"></eq><ci id="S3.E4.m1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.2">ğ¿</ci><apply id="S3.E4.m1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.3"><plus id="S3.E4.m1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.1.1.3.1"></plus><apply id="S3.E4.m1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.1.1.3.2"><times id="S3.E4.m1.1.1.1.1.3.2.1.cmml" xref="S3.E4.m1.1.1.1.1.3.2.1"></times><apply id="S3.E4.m1.1.1.1.1.3.2.2.cmml" xref="S3.E4.m1.1.1.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.3.2.2.1.cmml" xref="S3.E4.m1.1.1.1.1.3.2.2">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.3.2.2.2.cmml" xref="S3.E4.m1.1.1.1.1.3.2.2.2">ğ‘¤</ci><ci id="S3.E4.m1.1.1.1.1.3.2.2.3.cmml" xref="S3.E4.m1.1.1.1.1.3.2.2.3">grid</ci></apply><apply id="S3.E4.m1.1.1.1.1.3.2.3.cmml" xref="S3.E4.m1.1.1.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.3.2.3.1.cmml" xref="S3.E4.m1.1.1.1.1.3.2.3">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.3.2.3.2.cmml" xref="S3.E4.m1.1.1.1.1.3.2.3.2">ğ¿</ci><ci id="S3.E4.m1.1.1.1.1.3.2.3.3.cmml" xref="S3.E4.m1.1.1.1.1.3.2.3.3">grid</ci></apply></apply><apply id="S3.E4.m1.1.1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.1.1.3.3"><times id="S3.E4.m1.1.1.1.1.3.3.1.cmml" xref="S3.E4.m1.1.1.1.1.3.3.1"></times><apply id="S3.E4.m1.1.1.1.1.3.3.2.cmml" xref="S3.E4.m1.1.1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.3.3.2.1.cmml" xref="S3.E4.m1.1.1.1.1.3.3.2">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.3.3.2.2.cmml" xref="S3.E4.m1.1.1.1.1.3.3.2.2">ğ‘¤</ci><ci id="S3.E4.m1.1.1.1.1.3.3.2.3.cmml" xref="S3.E4.m1.1.1.1.1.3.3.2.3">image</ci></apply><apply id="S3.E4.m1.1.1.1.1.3.3.3.cmml" xref="S3.E4.m1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.3.3.3.1.cmml" xref="S3.E4.m1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.3.3.3.2.cmml" xref="S3.E4.m1.1.1.1.1.3.3.3.2">ğ¿</ci><ci id="S3.E4.m1.1.1.1.1.3.3.3.3.cmml" xref="S3.E4.m1.1.1.1.1.3.3.3.3">image</ci></apply></apply><apply id="S3.E4.m1.1.1.1.1.3.4.cmml" xref="S3.E4.m1.1.1.1.1.3.4"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.3.4.1.cmml" xref="S3.E4.m1.1.1.1.1.3.4">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.3.4.2.cmml" xref="S3.E4.m1.1.1.1.1.3.4.2">ğ¿</ci><ci id="S3.E4.m1.1.1.1.1.3.4.3.cmml" xref="S3.E4.m1.1.1.1.1.3.4.3">R</ci></apply><apply id="S3.E4.m1.1.1.1.1.3.5.cmml" xref="S3.E4.m1.1.1.1.1.3.5"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.3.5.1.cmml" xref="S3.E4.m1.1.1.1.1.3.5">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.3.5.2.cmml" xref="S3.E4.m1.1.1.1.1.3.5.2">ğ¿</ci><ci id="S3.E4.m1.1.1.1.1.3.5.3.cmml" xref="S3.E4.m1.1.1.1.1.3.5.3">C</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">\displaystyle L=w_{\rm grid}L_{\rm grid}+w_{\rm image}L_{\rm image}+L_{\rm R}+L_{\rm C},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p3.2" class="ltx_p">which includes the loss of two alignments as well as the original loss in the one-stage detector FCOS: Regression loss <math id="S3.SS3.p3.1.m1.1" class="ltx_Math" alttext="L_{\rm R}" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><msub id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml"><mi id="S3.SS3.p3.1.m1.1.1.2" xref="S3.SS3.p3.1.m1.1.1.2.cmml">L</mi><mi mathvariant="normal" id="S3.SS3.p3.1.m1.1.1.3" xref="S3.SS3.p3.1.m1.1.1.3.cmml">R</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><apply id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.1.m1.1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p3.1.m1.1.1.2.cmml" xref="S3.SS3.p3.1.m1.1.1.2">ğ¿</ci><ci id="S3.SS3.p3.1.m1.1.1.3.cmml" xref="S3.SS3.p3.1.m1.1.1.3">R</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">L_{\rm R}</annotation></semantics></math> for bounding boxes and centerness loss <math id="S3.SS3.p3.2.m2.1" class="ltx_Math" alttext="L_{\rm C}" display="inline"><semantics id="S3.SS3.p3.2.m2.1a"><msub id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml"><mi id="S3.SS3.p3.2.m2.1.1.2" xref="S3.SS3.p3.2.m2.1.1.2.cmml">L</mi><mi mathvariant="normal" id="S3.SS3.p3.2.m2.1.1.3" xref="S3.SS3.p3.2.m2.1.1.3.cmml">C</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><apply id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.2.m2.1.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p3.2.m2.1.1.2.cmml" xref="S3.SS3.p3.2.m2.1.1.2">ğ¿</ci><ci id="S3.SS3.p3.2.m2.1.1.3.cmml" xref="S3.SS3.p3.2.m2.1.1.3">C</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">L_{\rm C}</annotation></semantics></math> indicating the distance of a pixel to the center of the bounding box.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison with different object detectors on LVIS v1.0Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> with open-set settings. Multi-scale training is used. â€œCLIP on cropped regionsâ€ directly applies CLIP to classify cropped region proposals.
<span id="S4.T1.8.4" class="ltx_text" style="color:#000000;">Except for GridCLIP, all detectors use an RPN pretrained on base categories to get region proposals.
<math id="S4.T1.5.1.m1.1" class="ltx_Math" alttext="\ddagger" display="inline"><semantics id="S4.T1.5.1.m1.1b"><mo mathcolor="#000000" id="S4.T1.5.1.m1.1.1" xref="S4.T1.5.1.m1.1.1.cmml">â€¡</mo><annotation-xml encoding="MathML-Content" id="S4.T1.5.1.m1.1c"><ci id="S4.T1.5.1.m1.1.1.cmml" xref="S4.T1.5.1.m1.1.1">â€¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.1.m1.1d">\ddagger</annotation></semantics></math> denotes using mask annotations.
<math id="S4.T1.6.2.m2.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="S4.T1.6.2.m2.1b"><mo mathcolor="#000000" id="S4.T1.6.2.m2.1.1" xref="S4.T1.6.2.m2.1.1.cmml">â€ </mo><annotation-xml encoding="MathML-Content" id="S4.T1.6.2.m2.1c"><ci id="S4.T1.6.2.m2.1.1.cmml" xref="S4.T1.6.2.m2.1.1">â€ </ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.2.m2.1d">\dagger</annotation></semantics></math> denotes mask AP.
<math id="S4.T1.7.3.m3.1" class="ltx_Math" alttext="\nabla" display="inline"><semantics id="S4.T1.7.3.m3.1b"><mo mathcolor="#000000" id="S4.T1.7.3.m3.1.1" xref="S4.T1.7.3.m3.1.1.cmml">âˆ‡</mo><annotation-xml encoding="MathML-Content" id="S4.T1.7.3.m3.1c"><ci id="S4.T1.7.3.m3.1.1.cmml" xref="S4.T1.7.3.m3.1.1">âˆ‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.3.m3.1d">\nabla</annotation></semantics></math> denotes using learnable prompts instead of template prompts.
<math id="S4.T1.8.4.m4.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T1.8.4.m4.1b"><mo mathcolor="#000000" id="S4.T1.8.4.m4.1.1" xref="S4.T1.8.4.m4.1.1.cmml">â‹†</mo><annotation-xml encoding="MathML-Content" id="S4.T1.8.4.m4.1c"><ci id="S4.T1.8.4.m4.1.1.cmml" xref="S4.T1.8.4.m4.1.1">â‹†</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.8.4.m4.1d">\star</annotation></semantics></math> denotes that RegionCLIP use extra pretraining process of 600k iter on CC3M dataset with batch size of 96.
</span>
</figcaption>
<table id="S4.T1.18" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.11.3" class="ltx_tr">
<td id="S4.T1.11.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.11.3.4.1" class="ltx_text" style="font-size:80%;">Model</span></td>
<td id="S4.T1.11.3.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.11.3.5.1" class="ltx_text" style="font-size:80%;">backbone</span></td>
<td id="S4.T1.11.3.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.11.3.6.1" class="ltx_text" style="font-size:80%;">pretrained CLIP</span></td>
<td id="S4.T1.11.3.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.11.3.7.1" class="ltx_text" style="font-size:80%;">epochs</span></td>
<td id="S4.T1.11.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:8.0pt;padding-right:8.0pt;">
<table id="S4.T1.11.3.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.11.3.8.1.1" class="ltx_tr">
<td id="S4.T1.11.3.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.11.3.8.1.1.1.1" class="ltx_text" style="font-size:80%;">external</span></td>
</tr>
<tr id="S4.T1.11.3.8.1.2" class="ltx_tr">
<td id="S4.T1.11.3.8.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.11.3.8.1.2.1.1" class="ltx_text" style="font-size:80%;">dataset</span></td>
</tr>
</table>
</td>
<td id="S4.T1.9.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:8.0pt;padding-right:8.0pt;">
<span id="S4.T1.9.1.1.1" class="ltx_text" style="font-size:80%;">AP</span><sub id="S4.T1.9.1.1.2" class="ltx_sub"><span id="S4.T1.9.1.1.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">r</span></sub>
</td>
<td id="S4.T1.10.2.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:8.0pt;padding-right:8.0pt;">
<span id="S4.T1.10.2.2.1" class="ltx_text" style="font-size:80%;">AP</span><sub id="S4.T1.10.2.2.2" class="ltx_sub"><span id="S4.T1.10.2.2.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">c</span></sub>
</td>
<td id="S4.T1.11.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:8.0pt;padding-right:8.0pt;">
<span id="S4.T1.11.3.3.1" class="ltx_text" style="font-size:80%;">AP</span><sub id="S4.T1.11.3.3.2" class="ltx_sub"><span id="S4.T1.11.3.3.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">f</span></sub>
</td>
<td id="S4.T1.11.3.9" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.11.3.9.1" class="ltx_text" style="font-size:80%;">AP</span></td>
</tr>
<tr id="S4.T1.12.4" class="ltx_tr">
<td id="S4.T1.12.4.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">
<span id="S4.T1.12.4.1.1" class="ltx_text" style="font-size:80%;">CLIP on cropped regions</span><math id="S4.T1.12.4.1.m1.1" class="ltx_Math" alttext="\ddagger" display="inline"><semantics id="S4.T1.12.4.1.m1.1a"><mo mathsize="80%" id="S4.T1.12.4.1.m1.1.1" xref="S4.T1.12.4.1.m1.1.1.cmml">â€¡</mo><annotation-xml encoding="MathML-Content" id="S4.T1.12.4.1.m1.1b"><ci id="S4.T1.12.4.1.m1.1.1.cmml" xref="S4.T1.12.4.1.m1.1.1">â€¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.12.4.1.m1.1c">\ddagger</annotation></semantics></math><span id="S4.T1.12.4.1.2" class="ltx_text" style="font-size:80%;">Â </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.12.4.1.3.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib9" title="" class="ltx_ref">9</a><span id="S4.T1.12.4.1.4.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S4.T1.12.4.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.12.4.2.1" class="ltx_text" style="font-size:80%;">R50-FPN</span></td>
<td id="S4.T1.12.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.12.4.3.1" class="ltx_text" style="font-size:80%;">ViT-B/32</span></td>
<td id="S4.T1.12.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.12.4.4.1" class="ltx_text" style="font-size:80%;">0</span></td>
<td id="S4.T1.12.4.5" class="ltx_td ltx_border_r ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"></td>
<td id="S4.T1.12.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.12.4.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">19.5</span></td>
<td id="S4.T1.12.4.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.12.4.7.1" class="ltx_text" style="font-size:80%;">19.7</span></td>
<td id="S4.T1.12.4.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.12.4.8.1" class="ltx_text" style="font-size:80%;">17.0</span></td>
<td id="S4.T1.12.4.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.12.4.9.1" class="ltx_text" style="font-size:80%;">18.6</span></td>
</tr>
<tr id="S4.T1.13.5" class="ltx_tr">
<td id="S4.T1.13.5.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;">
<span id="S4.T1.13.5.1.1" class="ltx_text" style="font-size:80%;">ViLD</span><math id="S4.T1.13.5.1.m1.1" class="ltx_Math" alttext="\ddagger" display="inline"><semantics id="S4.T1.13.5.1.m1.1a"><mo mathsize="80%" id="S4.T1.13.5.1.m1.1.1" xref="S4.T1.13.5.1.m1.1.1.cmml">â€¡</mo><annotation-xml encoding="MathML-Content" id="S4.T1.13.5.1.m1.1b"><ci id="S4.T1.13.5.1.m1.1.1.cmml" xref="S4.T1.13.5.1.m1.1.1">â€¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.13.5.1.m1.1c">\ddagger</annotation></semantics></math><span id="S4.T1.13.5.1.2" class="ltx_text" style="font-size:80%;">Â </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.13.5.1.3.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib9" title="" class="ltx_ref">9</a><span id="S4.T1.13.5.1.4.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S4.T1.13.5.2" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.13.5.2.1" class="ltx_text" style="font-size:80%;">R50-FPN</span></td>
<td id="S4.T1.13.5.3" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.13.5.3.1" class="ltx_text" style="font-size:80%;">ViT-B/32</span></td>
<td id="S4.T1.13.5.4" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.13.5.4.1" class="ltx_text" style="font-size:80%;">384</span></td>
<td id="S4.T1.13.5.5" class="ltx_td ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;"></td>
<td id="S4.T1.13.5.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.13.5.6.1" class="ltx_text" style="font-size:80%;">16.3</span></td>
<td id="S4.T1.13.5.7" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.13.5.7.1" class="ltx_text" style="font-size:80%;">21.2</span></td>
<td id="S4.T1.13.5.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.13.5.8.1" class="ltx_text" style="font-size:80%;">31.6</span></td>
<td id="S4.T1.13.5.9" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.13.5.9.1" class="ltx_text" style="font-size:80%;">24.4</span></td>
</tr>
<tr id="S4.T1.14.6" class="ltx_tr">
<td id="S4.T1.14.6.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;">
<span id="S4.T1.14.6.2.1" class="ltx_text" style="font-size:80%;">RegionCLIPÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.14.6.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib38" title="" class="ltx_ref">38</a><span id="S4.T1.14.6.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S4.T1.14.6.3" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.14.6.3.1" class="ltx_text" style="font-size:80%;">CLIP R50-C4</span></td>
<td id="S4.T1.14.6.4" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.14.6.4.1" class="ltx_text" style="font-size:80%;">RN50</span></td>
<td id="S4.T1.14.6.1" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">
<span id="S4.T1.14.6.1.1" class="ltx_text" style="font-size:80%;">12</span><math id="S4.T1.14.6.1.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T1.14.6.1.m1.1a"><mo mathsize="80%" id="S4.T1.14.6.1.m1.1.1" xref="S4.T1.14.6.1.m1.1.1.cmml">â‹†</mo><annotation-xml encoding="MathML-Content" id="S4.T1.14.6.1.m1.1b"><ci id="S4.T1.14.6.1.m1.1.1.cmml" xref="S4.T1.14.6.1.m1.1.1">â‹†</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.14.6.1.m1.1c">\star</annotation></semantics></math>
</td>
<td id="S4.T1.14.6.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.14.6.5.1" class="ltx_text" style="font-size:80%;">CC3M</span></td>
<td id="S4.T1.14.6.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.14.6.6.1" class="ltx_text" style="font-size:80%;">17.1</span></td>
<td id="S4.T1.14.6.7" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.14.6.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">27.4</span></td>
<td id="S4.T1.14.6.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.14.6.8.1" class="ltx_text ltx_font_bold" style="font-size:80%;">34.0</span></td>
<td id="S4.T1.14.6.9" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.14.6.9.1" class="ltx_text ltx_font_bold" style="font-size:80%;">28.2</span></td>
</tr>
<tr id="S4.T1.16.8" class="ltx_tr">
<td id="S4.T1.16.8.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;">
<span id="S4.T1.16.8.2.1" class="ltx_text" style="font-size:80%;">Detic</span><math id="S4.T1.15.7.1.m1.1" class="ltx_Math" alttext="\ddagger" display="inline"><semantics id="S4.T1.15.7.1.m1.1a"><mo mathsize="80%" id="S4.T1.15.7.1.m1.1.1" xref="S4.T1.15.7.1.m1.1.1.cmml">â€¡</mo><annotation-xml encoding="MathML-Content" id="S4.T1.15.7.1.m1.1b"><ci id="S4.T1.15.7.1.m1.1.1.cmml" xref="S4.T1.15.7.1.m1.1.1">â€¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.15.7.1.m1.1c">\ddagger</annotation></semantics></math><math id="S4.T1.16.8.2.m2.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="S4.T1.16.8.2.m2.1a"><mo mathsize="80%" id="S4.T1.16.8.2.m2.1.1" xref="S4.T1.16.8.2.m2.1.1.cmml">â€ </mo><annotation-xml encoding="MathML-Content" id="S4.T1.16.8.2.m2.1b"><ci id="S4.T1.16.8.2.m2.1.1.cmml" xref="S4.T1.16.8.2.m2.1.1">â€ </ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.16.8.2.m2.1c">\dagger</annotation></semantics></math><span id="S4.T1.16.8.2.2" class="ltx_text" style="font-size:80%;">Â </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.16.8.2.3.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib41" title="" class="ltx_ref">41</a><span id="S4.T1.16.8.2.4.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S4.T1.16.8.3" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.16.8.3.1" class="ltx_text" style="font-size:80%;">R50-FPN</span></td>
<td id="S4.T1.16.8.4" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.16.8.4.1" class="ltx_text" style="font-size:80%;">ViT-B/32</span></td>
<td id="S4.T1.16.8.5" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.16.8.5.1" class="ltx_text" style="font-size:80%;">384</span></td>
<td id="S4.T1.16.8.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.16.8.6.1" class="ltx_text" style="font-size:80%;">ImageNet-21K</span></td>
<td id="S4.T1.16.8.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.16.8.7.1" class="ltx_text" style="font-size:80%;">17.8</span></td>
<td id="S4.T1.16.8.8" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.16.8.8.1" class="ltx_text ltx_framed ltx_framed_underline" style="font-size:80%;">26.3</span></td>
<td id="S4.T1.16.8.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.16.8.9.1" class="ltx_text" style="font-size:80%;">31.6</span></td>
<td id="S4.T1.16.8.10" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.16.8.10.1" class="ltx_text ltx_framed ltx_framed_underline" style="font-size:80%;">26.8</span></td>
</tr>
<tr id="S4.T1.18.10" class="ltx_tr">
<td id="S4.T1.18.10.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;">
<span id="S4.T1.18.10.2.1" class="ltx_text" style="font-size:80%;">PromptDet</span><math id="S4.T1.17.9.1.m1.1" class="ltx_Math" alttext="\nabla" display="inline"><semantics id="S4.T1.17.9.1.m1.1a"><mo mathsize="80%" id="S4.T1.17.9.1.m1.1.1" xref="S4.T1.17.9.1.m1.1.1.cmml">âˆ‡</mo><annotation-xml encoding="MathML-Content" id="S4.T1.17.9.1.m1.1b"><ci id="S4.T1.17.9.1.m1.1.1.cmml" xref="S4.T1.17.9.1.m1.1.1">âˆ‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.17.9.1.m1.1c">\nabla</annotation></semantics></math><math id="S4.T1.18.10.2.m2.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="S4.T1.18.10.2.m2.1a"><mo mathsize="80%" id="S4.T1.18.10.2.m2.1.1" xref="S4.T1.18.10.2.m2.1.1.cmml">â€ </mo><annotation-xml encoding="MathML-Content" id="S4.T1.18.10.2.m2.1b"><ci id="S4.T1.18.10.2.m2.1.1.cmml" xref="S4.T1.18.10.2.m2.1.1">â€ </ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.18.10.2.m2.1c">\dagger</annotation></semantics></math><span id="S4.T1.18.10.2.2" class="ltx_text" style="font-size:80%;">Â </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.18.10.2.3.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib7" title="" class="ltx_ref">7</a><span id="S4.T1.18.10.2.4.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S4.T1.18.10.3" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.18.10.3.1" class="ltx_text" style="font-size:80%;">R50-FPN</span></td>
<td id="S4.T1.18.10.4" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.18.10.4.1" class="ltx_text" style="font-size:80%;">ViT-B/32</span></td>
<td id="S4.T1.18.10.5" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.18.10.5.1" class="ltx_text" style="font-size:80%;">6+12</span></td>
<td id="S4.T1.18.10.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.18.10.6.1" class="ltx_text" style="font-size:80%;">LAION-novel</span></td>
<td id="S4.T1.18.10.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.18.10.7.1" class="ltx_text ltx_framed ltx_framed_underline" style="font-size:80%;">19.0</span></td>
<td id="S4.T1.18.10.8" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.18.10.8.1" class="ltx_text" style="font-size:80%;">18.5</span></td>
<td id="S4.T1.18.10.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.18.10.9.1" class="ltx_text" style="font-size:80%;">25.8</span></td>
<td id="S4.T1.18.10.10" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.18.10.10.1" class="ltx_text" style="font-size:80%;">21.4</span></td>
</tr>
<tr id="S4.T1.18.11.1" class="ltx_tr">
<td id="S4.T1.18.11.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.18.11.1.1.1" class="ltx_text" style="font-size:80%;">GridCLIP-R50</span></td>
<td id="S4.T1.18.11.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.18.11.1.2.1" class="ltx_text" style="font-size:80%;">CLIP R50-FPN</span></td>
<td id="S4.T1.18.11.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.18.11.1.3.1" class="ltx_text" style="font-size:80%;">ViT-B/32</span></td>
<td id="S4.T1.18.11.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.18.11.1.4.1" class="ltx_text" style="font-size:80%;">24</span></td>
<td id="S4.T1.18.11.1.5" class="ltx_td ltx_border_r ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"></td>
<td id="S4.T1.18.11.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.18.11.1.6.1" class="ltx_text" style="font-size:80%;">15.0</span></td>
<td id="S4.T1.18.11.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.18.11.1.7.1" class="ltx_text" style="font-size:80%;">22.7</span></td>
<td id="S4.T1.18.11.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.18.11.1.8.1" class="ltx_text" style="font-size:80%;">32.5</span></td>
<td id="S4.T1.18.11.1.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.18.11.1.9.1" class="ltx_text" style="font-size:80%;">25.2</span></td>
</tr>
<tr id="S4.T1.18.12.2" class="ltx_tr">
<td id="S4.T1.18.12.2.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.18.12.2.1.1" class="ltx_text" style="font-size:80%;">GridCLIP-R50-RN</span></td>
<td id="S4.T1.18.12.2.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.18.12.2.2.1" class="ltx_text" style="font-size:80%;">CLIP R50-FPN</span></td>
<td id="S4.T1.18.12.2.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.18.12.2.3.1" class="ltx_text" style="font-size:80%;">RN50x64</span></td>
<td id="S4.T1.18.12.2.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.18.12.2.4.1" class="ltx_text" style="font-size:80%;">24</span></td>
<td id="S4.T1.18.12.2.5" class="ltx_td ltx_border_bb ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;"></td>
<td id="S4.T1.18.12.2.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.18.12.2.6.1" class="ltx_text" style="font-size:80%;">13.7</span></td>
<td id="S4.T1.18.12.2.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.18.12.2.7.1" class="ltx_text" style="font-size:80%;">23.3</span></td>
<td id="S4.T1.18.12.2.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.18.12.2.8.1" class="ltx_text ltx_framed ltx_framed_underline" style="font-size:80%;">32.6</span></td>
<td id="S4.T1.18.12.2.9" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T1.18.12.2.9.1" class="ltx_text" style="font-size:80%;">25.3</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 2: </span><span id="S4.T2.3.1" class="ltx_text" style="color:#000000;">The training and test time on LVIS v1.0 and the model size comparisons of ViLD and GridCLIP. The resource usage of ViLD is measured based on the implementation of DetProÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. </span></figcaption>
<table id="S4.T2.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.4.1.1" class="ltx_tr">
<th id="S4.T2.4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.4.1.1.1.1" class="ltx_text" style="font-size:80%;">Model</span></th>
<th id="S4.T2.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;">
<table id="S4.T2.4.1.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.4.1.1.2.1.1" class="ltx_tr">
<td id="S4.T2.4.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.4.1.1.2.1.1.1.1" class="ltx_text" style="font-size:80%;">Parameters</span></td>
</tr>
<tr id="S4.T2.4.1.1.2.1.2" class="ltx_tr">
<td id="S4.T2.4.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.4.1.1.2.1.2.1.1" class="ltx_text" style="font-size:80%;">for Inference (M)</span></td>
</tr>
</table>
</th>
<th id="S4.T2.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.4.1.1.3.1" class="ltx_text" style="font-size:80%;">Epoch</span></th>
<th id="S4.T2.4.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;">
<table id="S4.T2.4.1.1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.4.1.1.4.1.1" class="ltx_tr">
<td id="S4.T2.4.1.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.4.1.1.4.1.1.1.1" class="ltx_text" style="font-size:80%;">Training Cost / Epoch</span></td>
</tr>
<tr id="S4.T2.4.1.1.4.1.2" class="ltx_tr">
<td id="S4.T2.4.1.1.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.4.1.1.4.1.2.1.1" class="ltx_text" style="font-size:80%;">(Per-GPU-Hour)</span></td>
</tr>
</table>
</th>
<th id="S4.T2.4.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;">
<table id="S4.T2.4.1.1.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.4.1.1.5.1.1" class="ltx_tr">
<td id="S4.T2.4.1.1.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.4.1.1.5.1.1.1.1" class="ltx_text" style="font-size:80%;">Total Training Cost</span></td>
</tr>
<tr id="S4.T2.4.1.1.5.1.2" class="ltx_tr">
<td id="S4.T2.4.1.1.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.4.1.1.5.1.2.1.1" class="ltx_text" style="font-size:80%;">(Per-GPU-Hour)</span></td>
</tr>
</table>
</th>
<th id="S4.T2.4.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.4.1.1.6.1" class="ltx_text" style="font-size:80%;">FPS</span></th>
</tr>
<tr id="S4.T2.4.2.2" class="ltx_tr">
<th id="S4.T2.4.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.4.2.2.1.1" class="ltx_text" style="font-size:80%;">ViLD</span></th>
<th id="S4.T2.4.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.4.2.2.2.1" class="ltx_text" style="font-size:80%;">60.5</span></th>
<th id="S4.T2.4.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.4.2.2.3.1" class="ltx_text" style="font-size:80%;">384</span></th>
<th id="S4.T2.4.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.4.2.2.4.1" class="ltx_text" style="font-size:80%;">7.98</span></th>
<th id="S4.T2.4.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.4.2.2.5.1" class="ltx_text" style="font-size:80%;">3064</span></th>
<th id="S4.T2.4.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.4.2.2.6.1" class="ltx_text" style="font-size:80%;">3.3</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.4.3.1" class="ltx_tr">
<th id="S4.T2.4.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.4.3.1.1.1" class="ltx_text" style="font-size:80%;">GridCLIP-R50</span></th>
<th id="S4.T2.4.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.4.3.1.2.1" class="ltx_text" style="font-size:80%;">56.4</span></th>
<td id="S4.T2.4.3.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.4.3.1.3.1" class="ltx_text" style="font-size:80%;">24</span></td>
<td id="S4.T2.4.3.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.4.3.1.4.1" class="ltx_text" style="font-size:80%;">2.94</span></td>
<td id="S4.T2.4.3.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.4.3.1.5.1" class="ltx_text" style="font-size:80%;">70</span></td>
<td id="S4.T2.4.3.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.4.3.1.6.1" class="ltx_text" style="font-size:80%;">19.5</span></td>
</tr>
<tr id="S4.T2.4.4.2" class="ltx_tr">
<th id="S4.T2.4.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.4.4.2.1.1" class="ltx_text" style="font-size:80%;">GridCLIP-R50-RN</span></th>
<th id="S4.T2.4.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.4.4.2.2.1" class="ltx_text" style="font-size:80%;">56.4</span></th>
<td id="S4.T2.4.4.2.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.4.4.2.3.1" class="ltx_text" style="font-size:80%;">24</span></td>
<td id="S4.T2.4.4.2.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.4.4.2.4.1" class="ltx_text" style="font-size:80%;">3.66</span></td>
<td id="S4.T2.4.4.2.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.4.4.2.5.1" class="ltx_text" style="font-size:80%;">88</span></td>
<td id="S4.T2.4.4.2.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S4.T2.4.4.2.6.1" class="ltx_text" style="font-size:80%;">19.5</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>Generalization ability of LVIS-trained detectors to PASCAL VOC 2007 test set and COCO validation set.</figcaption>
<table id="S4.T3.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.4.5.1" class="ltx_tr">
<th id="S4.T3.4.5.1.1" class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:8.0pt;padding-right:8.0pt;"></th>
<th id="S4.T3.4.5.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:8.0pt;padding-right:8.0pt;" colspan="2"><span id="S4.T3.4.5.1.2.1" class="ltx_text" style="font-size:80%;">PASCAL VOC</span></th>
<th id="S4.T3.4.5.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:8.0pt;padding-right:8.0pt;" colspan="3"><span id="S4.T3.4.5.1.3.1" class="ltx_text" style="font-size:80%;">COCO</span></th>
</tr>
<tr id="S4.T3.4.4" class="ltx_tr">
<th id="S4.T3.4.4.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T3.4.4.5.1" class="ltx_text" style="font-size:80%;">Model</span></th>
<th id="S4.T3.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:8.0pt;padding-right:8.0pt;">
<span id="S4.T3.1.1.1.1" class="ltx_text" style="font-size:80%;">AP</span><sub id="S4.T3.1.1.1.2" class="ltx_sub"><span id="S4.T3.1.1.1.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">50</span></sub>
</th>
<th id="S4.T3.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;">
<span id="S4.T3.2.2.2.1" class="ltx_text" style="font-size:80%;">AP</span><sub id="S4.T3.2.2.2.2" class="ltx_sub"><span id="S4.T3.2.2.2.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">75</span></sub>
</th>
<th id="S4.T3.4.4.6" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T3.4.4.6.1" class="ltx_text" style="font-size:80%;">AP</span></th>
<th id="S4.T3.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:8.0pt;padding-right:8.0pt;">
<span id="S4.T3.3.3.3.1" class="ltx_text" style="font-size:80%;">AP</span><sub id="S4.T3.3.3.3.2" class="ltx_sub"><span id="S4.T3.3.3.3.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">50</span></sub>
</th>
<th id="S4.T3.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:8.0pt;padding-right:8.0pt;">
<span id="S4.T3.4.4.4.1" class="ltx_text" style="font-size:80%;">AP</span><sub id="S4.T3.4.4.4.2" class="ltx_sub"><span id="S4.T3.4.4.4.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">75</span></sub>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.4.6.1" class="ltx_tr">
<td id="S4.T3.4.6.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T3.4.6.1.1.1" class="ltx_text" style="font-size:80%;">ViLD</span></td>
<td id="S4.T3.4.6.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T3.4.6.1.2.1" class="ltx_text" style="font-size:80%;">72.2</span></td>
<td id="S4.T3.4.6.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T3.4.6.1.3.1" class="ltx_text" style="font-size:80%;">56.7</span></td>
<td id="S4.T3.4.6.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T3.4.6.1.4.1" class="ltx_text" style="font-size:80%;">36.6</span></td>
<td id="S4.T3.4.6.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T3.4.6.1.5.1" class="ltx_text" style="font-size:80%;">55.6</span></td>
<td id="S4.T3.4.6.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T3.4.6.1.6.1" class="ltx_text" style="font-size:80%;">39.8</span></td>
</tr>
<tr id="S4.T3.4.7.2" class="ltx_tr">
<td id="S4.T3.4.7.2.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T3.4.7.2.1.1" class="ltx_text" style="font-size:80%;">DetPro</span></td>
<td id="S4.T3.4.7.2.2" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T3.4.7.2.2.1" class="ltx_text" style="font-size:80%;">74.6</span></td>
<td id="S4.T3.4.7.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T3.4.7.2.3.1" class="ltx_text" style="font-size:80%;">57.9</span></td>
<td id="S4.T3.4.7.2.4" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T3.4.7.2.4.1" class="ltx_text" style="font-size:80%;">34.9</span></td>
<td id="S4.T3.4.7.2.5" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T3.4.7.2.5.1" class="ltx_text" style="font-size:80%;">53.8</span></td>
<td id="S4.T3.4.7.2.6" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T3.4.7.2.6.1" class="ltx_text" style="font-size:80%;">37.4</span></td>
</tr>
<tr id="S4.T3.4.8.3" class="ltx_tr">
<td id="S4.T3.4.8.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T3.4.8.3.1.1" class="ltx_text" style="font-size:80%;">GridCLIP-R50</span></td>
<td id="S4.T3.4.8.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T3.4.8.3.2.1" class="ltx_text" style="font-size:80%;">70.9</span></td>
<td id="S4.T3.4.8.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T3.4.8.3.3.1" class="ltx_text" style="font-size:80%;">55.4</span></td>
<td id="S4.T3.4.8.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T3.4.8.3.4.1" class="ltx_text" style="font-size:80%;">34.7</span></td>
<td id="S4.T3.4.8.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T3.4.8.3.5.1" class="ltx_text" style="font-size:80%;">52.2</span></td>
<td id="S4.T3.4.8.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T3.4.8.3.6.1" class="ltx_text" style="font-size:80%;">37.1</span></td>
</tr>
<tr id="S4.T3.4.9.4" class="ltx_tr">
<td id="S4.T3.4.9.4.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T3.4.9.4.1.1" class="ltx_text" style="font-size:80%;">GridCLIP-R50-RN</span></td>
<td id="S4.T3.4.9.4.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T3.4.9.4.2.1" class="ltx_text" style="font-size:80%;">71.6</span></td>
<td id="S4.T3.4.9.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T3.4.9.4.3.1" class="ltx_text" style="font-size:80%;">55.7</span></td>
<td id="S4.T3.4.9.4.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T3.4.9.4.4.1" class="ltx_text" style="font-size:80%;">34.4</span></td>
<td id="S4.T3.4.9.4.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T3.4.9.4.5.1" class="ltx_text" style="font-size:80%;">51.8</span></td>
<td id="S4.T3.4.9.4.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T3.4.9.4.6.1" class="ltx_text" style="font-size:80%;">36.6</span></td>
</tr>
</tbody>
</table>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Implementational Details</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.2" class="ltx_p">GridCLIP uses the one-stage detector FCOSÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> as the detector, which can be replaced by other one-stage detectors like RetinaNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> or ATSSÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>.
The backbone uses the RN50 pretrained CLIP image encoder, which has two more convolutional layers than the original ResNet50Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> in the stem module
and a Multi-Head Self-Attention (MHSA) layer performing on the output of the 5th stage.
The adapting layers performed on the output features of MHSA use the embedding dimension of 256.
For image-level alignment, GridCLIP uses the ViT-B/32 version of CLIP.
The weights of the two alignment losses are: <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="w_{\rm grid}" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><msub id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mi id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml">w</mi><mi id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml">grid</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2">ğ‘¤</ci><ci id="S4.SS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3">grid</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">w_{\rm grid}</annotation></semantics></math>=1, <math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="w_{\rm image}" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><msub id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml"><mi id="S4.SS1.p1.2.m2.1.1.2" xref="S4.SS1.p1.2.m2.1.1.2.cmml">w</mi><mi id="S4.SS1.p1.2.m2.1.1.3" xref="S4.SS1.p1.2.m2.1.1.3.cmml">image</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><apply id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.2.m2.1.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.p1.2.m2.1.1.2.cmml" xref="S4.SS1.p1.2.m2.1.1.2">ğ‘¤</ci><ci id="S4.SS1.p1.2.m2.1.1.3.cmml" xref="S4.SS1.p1.2.m2.1.1.3">image</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">w_{\rm image}</annotation></semantics></math>=10.
Our implementation is based on the MMDetection frameworkÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.5" class="ltx_p">We conduct experiments on the detection benchmark
LVIS v1.0Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.
LVIS v1.0 is a long-tail detection dataset containing 1203 categories.
The categories are divided into three parts by how many images
they appear in: rare (1-10), common (11-100), and
frequent (<math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mo id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><gt id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">&gt;</annotation></semantics></math>100), respectively including 337, 461 and 405 categories, with corresponding metrics AP<sub id="S4.SS1.p2.5.1" class="ltx_sub"><span id="S4.SS1.p2.5.1.1" class="ltx_text ltx_font_italic">r</span></sub>, AP<sub id="S4.SS1.p2.5.2" class="ltx_sub"><span id="S4.SS1.p2.5.2.1" class="ltx_text ltx_font_italic">c</span></sub> and AP<sub id="S4.SS1.p2.5.3" class="ltx_sub"><span id="S4.SS1.p2.5.3.1" class="ltx_text ltx_font_italic">f</span></sub>.
Following the ViLDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, we use frequent and common categories as the base categories and rare categories as the novel categories
for open-set detection. For close-set detection, we use the common and frequent categories.
When comparing with other SOTA approaches, we adopt multi-scale training similar to ViLD and random cropping augmentation.
For the training process,
GridCLIP is trained for a 2<math id="S4.SS1.p2.5.m5.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.p2.5.m5.1a"><mo id="S4.SS1.p2.5.m5.1.1" xref="S4.SS1.p2.5.m5.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.5.m5.1b"><times id="S4.SS1.p2.5.m5.1.1.cmml" xref="S4.SS1.p2.5.m5.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.5.m5.1c">\times</annotation></semantics></math> (24 LVIS epochs) schedule with a batch size of 16.
During the inference stage, the maximum number of detection objects per image is 300, and the threshold of the classification score is set to 0.05.
The IOU threshold of NMS is 0.5.
Refer to the supplementary materials for more details.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Comparison with the State-of-the-Art</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We compare GridCLIP on the LVIS v1.0 validation set with other methods with comparable backbone, including ViLDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, RegionCLIPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, DeticÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, DetProÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and PromptDetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> in TableÂ <a href="#S4.T1" title="Table 1 â€£ 4 Experiments â€£ GridCLIP: One-Stage Object Detection by Grid-Level CLIP Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
<span id="S4.SS2.p1.1.1" class="ltx_text" style="color:#000000;">These methods use template-based prompts,
except that DetProÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and PromptDetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> use learnable prompts.</span>
Also, we do not compare to methods like GLIPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> and DetCLIPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> which use large-scale annotation data, since we focus on utilizing limited annotation data for the detection of broader categories.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.9" class="ltx_p"><span id="S4.SS2.p2.9.2" class="ltx_text ltx_font_bold">Performance Comparison.</span>
A totally fair comparison is not realistic, since external datasets or learnable prompts are widely used in most OVOD methods.
Therefore, we find it relatively fair to compare GridCLIP with ViLDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>
which only utilizes the knowledge of CLIP and the detection dataset without learnable prompts.
We observe that GridCLIP surpasses ViLD in overall AP by 0.8.
As a one-stage detector,
GridCLIP closes the gap to the two-stage detector ViLD in novel categories to 1.3 AP<sub id="S4.SS2.p2.9.3" class="ltx_sub"><span id="S4.SS2.p2.9.3.1" class="ltx_text ltx_font_italic">r</span></sub>,
while the current SOTA one-stage detector HierKD is still 8.2 AP behind ViLD on the COCO validation dataset.
Furthermore, GridCLIP outperforms ViLD by 1.5 AP<sub id="S4.SS2.p2.9.4" class="ltx_sub"><span id="S4.SS2.p2.9.4.1" class="ltx_text ltx_font_italic">c</span></sub> and 0.9 AP<sub id="S4.SS2.p2.9.5" class="ltx_sub"><span id="S4.SS2.p2.9.5.1" class="ltx_text ltx_font_italic">f</span></sub>.
Besides ViT-B/32, we also use the RN50<math id="S4.SS2.p2.4.m4.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS2.p2.4.m4.1a"><mo id="S4.SS2.p2.4.m4.1.1" xref="S4.SS2.p2.4.m4.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.4.m4.1b"><times id="S4.SS2.p2.4.m4.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.4.m4.1c">\times</annotation></semantics></math>64 version (the largest model of CLIP under ResNet architecture) of CLIP for image-level alignment
to explore the upper bound of the ResNet version. We observe that the RN50<math id="S4.SS2.p2.5.m5.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS2.p2.5.m5.1a"><mo id="S4.SS2.p2.5.m5.1.1" xref="S4.SS2.p2.5.m5.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.5.m5.1b"><times id="S4.SS2.p2.5.m5.1.1.cmml" xref="S4.SS2.p2.5.m5.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.5.m5.1c">\times</annotation></semantics></math>64 version has a worse generalization ability to novel categories compared to the ViT-B/32 one,
with lower AP<sub id="S4.SS2.p2.9.6" class="ltx_sub"><span id="S4.SS2.p2.9.6.1" class="ltx_text ltx_font_italic">r</span></sub> while higher AP<sub id="S4.SS2.p2.9.7" class="ltx_sub"><span id="S4.SS2.p2.9.7.1" class="ltx_text ltx_font_italic">c</span></sub> and AP<sub id="S4.SS2.p2.9.8" class="ltx_sub"><span id="S4.SS2.p2.9.8.1" class="ltx_text ltx_font_italic">f</span></sub>.
<span id="S4.SS2.p2.9.1" class="ltx_text" style="color:#000000;">We further observe the obvious gap between the base and novel categories in GridCLIP,
and try to understand and explain the gap based on the analysis from another one-stage detector HierKDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>.
In ViLD,
both the novel and base categories use the region-level alignment.
In GridCLIP,
whilst novel categories use the more coarse-grained image-level alignment,
the base categories in GridCLIP use <span id="S4.SS2.p2.9.1.1" class="ltx_text ltx_font_italic">both</span>
the more fine-grained grid-level alignment and the
image-level alignment. This makes the gap between base and novel categories larger than that of ViLD.
To verify this, we can replace image-level alignment with region-level alignment similar to ViLD to further improve AP<sub id="S4.SS2.p2.9.1.2" class="ltx_sub"><span id="S4.SS2.p2.9.1.2.1" class="ltx_text ltx_font_italic">r</span></sub>,
which however may require more training time as other two-stage detectors do. We leave it for future research. </span></p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 4: </span>Comparison of different backbones and alignment methods on LVIS v1.0Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> with close-set settings. The top section compares supervised (ImageNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>) and unsupervised (SwAVÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>) pretrained visual models with CLIP as unsupervised visual-language pretrained model. The bottom section compares different alignment methods based on the ResNet50 version of pretrained CLIP image encoder. â€œGridCLIP-R50<math id="S4.T4.2.m1.1" class="ltx_Math" alttext="\ast" display="inline"><semantics id="S4.T4.2.m1.1b"><mo id="S4.T4.2.m1.1.1" xref="S4.T4.2.m1.1.1.cmml">âˆ—</mo><annotation-xml encoding="MathML-Content" id="S4.T4.2.m1.1c"><ci id="S4.T4.2.m1.1.1.cmml" xref="S4.T4.2.m1.1.1">âˆ—</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.m1.1d">\ast</annotation></semantics></math>â€ uses the CLIP ResNet50 without the MHSA layer,
while â€œGridCLIP-R50â€ uses the whole image encoder of CLIP ResNet50.</figcaption>
<table id="S4.T4.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T4.4.2" class="ltx_tr">
<th id="S4.T4.4.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.4.2.3.1" class="ltx_text" style="font-size:80%;">Method</span></th>
<td id="S4.T4.4.2.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.4.2.4.1" class="ltx_text" style="font-size:80%;">Backbone</span></td>
<td id="S4.T4.4.2.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:8.0pt;padding-right:8.0pt;">
<table id="S4.T4.4.2.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T4.4.2.5.1.1" class="ltx_tr">
<td id="S4.T4.4.2.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.4.2.5.1.1.1.1" class="ltx_text" style="font-size:80%;">Grid-level</span></td>
</tr>
<tr id="S4.T4.4.2.5.1.2" class="ltx_tr">
<td id="S4.T4.4.2.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.4.2.5.1.2.1.1" class="ltx_text" style="font-size:80%;">Alignment</span></td>
</tr>
</table>
</td>
<td id="S4.T4.4.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:8.0pt;padding-right:8.0pt;">
<table id="S4.T4.4.2.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T4.4.2.6.1.1" class="ltx_tr">
<td id="S4.T4.4.2.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.4.2.6.1.1.1.1" class="ltx_text" style="font-size:80%;">Image-level</span></td>
</tr>
<tr id="S4.T4.4.2.6.1.2" class="ltx_tr">
<td id="S4.T4.4.2.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.4.2.6.1.2.1.1" class="ltx_text" style="font-size:80%;">Alignment</span></td>
</tr>
</table>
</td>
<td id="S4.T4.3.1.1" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:8.0pt;padding-right:8.0pt;">
<span id="S4.T4.3.1.1.1" class="ltx_text" style="font-size:80%;">AP</span><sub id="S4.T4.3.1.1.2" class="ltx_sub"><span id="S4.T4.3.1.1.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">c</span></sub>
</td>
<td id="S4.T4.4.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:8.0pt;padding-right:8.0pt;">
<span id="S4.T4.4.2.2.1" class="ltx_text" style="font-size:80%;">AP</span><sub id="S4.T4.4.2.2.2" class="ltx_sub"><span id="S4.T4.4.2.2.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">f</span></sub>
</td>
<td id="S4.T4.4.2.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.4.2.7.1" class="ltx_text" style="font-size:80%;">AP</span></td>
</tr>
<tr id="S4.T4.5.4.1" class="ltx_tr">
<th id="S4.T4.5.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.5.4.1.1.1" class="ltx_text" style="font-size:80%;">ImageNet-R50 w/o align</span></th>
<td id="S4.T4.5.4.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.5.4.1.2.1" class="ltx_text" style="font-size:80%;">ImageNet R50-FPN</span></td>
<td id="S4.T4.5.4.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.5.4.1.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T4.5.4.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.5.4.1.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T4.5.4.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.5.4.1.5.1" class="ltx_text" style="font-size:80%;">14.6</span></td>
<td id="S4.T4.5.4.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.5.4.1.6.1" class="ltx_text" style="font-size:80%;">26.2</span></td>
<td id="S4.T4.5.4.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.5.4.1.7.1" class="ltx_text" style="font-size:80%;">16.6</span></td>
</tr>
<tr id="S4.T4.5.5.2" class="ltx_tr">
<th id="S4.T4.5.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.5.5.2.1.1" class="ltx_text" style="font-size:80%;">SwAV-R50 w/o align</span></th>
<td id="S4.T4.5.5.2.2" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.5.5.2.2.1" class="ltx_text" style="font-size:80%;">SwAV R50-FPN</span></td>
<td id="S4.T4.5.5.2.3" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.5.5.2.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T4.5.5.2.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.5.5.2.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T4.5.5.2.5" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.5.5.2.5.1" class="ltx_text" style="font-size:80%;">19.5</span></td>
<td id="S4.T4.5.5.2.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.5.5.2.6.1" class="ltx_text" style="font-size:80%;">29.2</span></td>
<td id="S4.T4.5.5.2.7" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.5.5.2.7.1" class="ltx_text" style="font-size:80%;">20.0</span></td>
</tr>
<tr id="S4.T4.5.3" class="ltx_tr">
<th id="S4.T4.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;">
<span id="S4.T4.5.3.1.1" class="ltx_text" style="font-size:80%;">GridCLIP-R50</span><math id="S4.T4.5.3.1.m1.1" class="ltx_Math" alttext="\ast" display="inline"><semantics id="S4.T4.5.3.1.m1.1a"><mo mathsize="80%" id="S4.T4.5.3.1.m1.1.1" xref="S4.T4.5.3.1.m1.1.1.cmml">âˆ—</mo><annotation-xml encoding="MathML-Content" id="S4.T4.5.3.1.m1.1b"><ci id="S4.T4.5.3.1.m1.1.1.cmml" xref="S4.T4.5.3.1.m1.1.1">âˆ—</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.5.3.1.m1.1c">\ast</annotation></semantics></math><span id="S4.T4.5.3.1.2" class="ltx_text" style="font-size:80%;"> w/o align</span>
</th>
<td id="S4.T4.5.3.2" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.5.3.2.1" class="ltx_text" style="font-size:80%;">CLIP R50-FPN woMHSA</span></td>
<td id="S4.T4.5.3.3" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.5.3.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T4.5.3.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.5.3.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T4.5.3.5" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.5.3.5.1" class="ltx_text" style="font-size:80%;">20.2</span></td>
<td id="S4.T4.5.3.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.5.3.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">30.3</span></td>
<td id="S4.T4.5.3.7" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.5.3.7.1" class="ltx_text" style="font-size:80%;">20.7</span></td>
</tr>
<tr id="S4.T4.5.6.3" class="ltx_tr">
<th id="S4.T4.5.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.5.6.3.1.1" class="ltx_text" style="font-size:80%;">GridCLIP-R50 w/o align</span></th>
<td id="S4.T4.5.6.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.5.6.3.2.1" class="ltx_text" style="font-size:80%;">CLIP R50-FPN</span></td>
<td id="S4.T4.5.6.3.3" class="ltx_td ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"></td>
<td id="S4.T4.5.6.3.4" class="ltx_td ltx_border_r ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"></td>
<td id="S4.T4.5.6.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.5.6.3.5.1" class="ltx_text" style="font-size:80%;">19.4</span></td>
<td id="S4.T4.5.6.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.5.6.3.6.1" class="ltx_text" style="font-size:80%;">29.7</span></td>
<td id="S4.T4.5.6.3.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.5.6.3.7.1" class="ltx_text" style="font-size:80%;">20.1</span></td>
</tr>
<tr id="S4.T4.5.7.4" class="ltx_tr">
<th id="S4.T4.5.7.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.5.7.4.1.1" class="ltx_text" style="font-size:80%;">GridCLIP-R50 w grid-align</span></th>
<td id="S4.T4.5.7.4.2" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.5.7.4.2.1" class="ltx_text" style="font-size:80%;">CLIP R50-FPN</span></td>
<td id="S4.T4.5.7.4.3" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.5.7.4.3.1" class="ltx_text" style="font-size:80%;">âœ“</span></td>
<td id="S4.T4.5.7.4.4" class="ltx_td ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;"></td>
<td id="S4.T4.5.7.4.5" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.5.7.4.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">21.7</span></td>
<td id="S4.T4.5.7.4.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.5.7.4.6.1" class="ltx_text" style="font-size:80%;">30.0</span></td>
<td id="S4.T4.5.7.4.7" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.5.7.4.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">21.2</span></td>
</tr>
<tr id="S4.T4.5.8.5" class="ltx_tr">
<th id="S4.T4.5.8.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.5.8.5.1.1" class="ltx_text" style="font-size:80%;">GridCLIP-R50 w image-align</span></th>
<td id="S4.T4.5.8.5.2" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.5.8.5.2.1" class="ltx_text" style="font-size:80%;">CLIP R50-FPN</span></td>
<td id="S4.T4.5.8.5.3" class="ltx_td" style="padding-left:8.0pt;padding-right:8.0pt;"></td>
<td id="S4.T4.5.8.5.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.5.8.5.4.1" class="ltx_text" style="font-size:80%;">âœ“</span></td>
<td id="S4.T4.5.8.5.5" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.5.8.5.5.1" class="ltx_text" style="font-size:80%;">19.4</span></td>
<td id="S4.T4.5.8.5.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.5.8.5.6.1" class="ltx_text ltx_framed ltx_framed_underline" style="font-size:80%;">30.1</span></td>
<td id="S4.T4.5.8.5.7" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.5.8.5.7.1" class="ltx_text" style="font-size:80%;">20.2</span></td>
</tr>
<tr id="S4.T4.5.9.6" class="ltx_tr">
<th id="S4.T4.5.9.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.5.9.6.1.1" class="ltx_text" style="font-size:80%;">GridCLIP-R50</span></th>
<td id="S4.T4.5.9.6.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.5.9.6.2.1" class="ltx_text" style="font-size:80%;">CLIP R50-FPN</span></td>
<td id="S4.T4.5.9.6.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.5.9.6.3.1" class="ltx_text" style="font-size:80%;">âœ“</span></td>
<td id="S4.T4.5.9.6.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.5.9.6.4.1" class="ltx_text" style="font-size:80%;">âœ“</span></td>
<td id="S4.T4.5.9.6.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.5.9.6.5.1" class="ltx_text ltx_framed ltx_framed_underline" style="font-size:80%;">21.2</span></td>
<td id="S4.T4.5.9.6.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.5.9.6.6.1" class="ltx_text" style="font-size:80%;">30.0</span></td>
<td id="S4.T4.5.9.6.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.5.9.6.7.1" class="ltx_text ltx_framed ltx_framed_underline" style="font-size:80%;">21.0</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.F3" class="ltx_figure">
<table id="S4.F3.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.F3.2.2" class="ltx_tr">
<td id="S4.F3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><img src="/html/2303.09252/assets/x3.png" id="S4.F3.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="267" height="89" alt="Refer to caption"></td>
<td id="S4.F3.2.2.2" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><img src="/html/2303.09252/assets/x4.png" id="S4.F3.2.2.2.g1" class="ltx_graphics ltx_img_landscape" width="267" height="89" alt="Refer to caption"></td>
</tr>
<tr id="S4.F3.2.3.1" class="ltx_tr">
<td id="S4.F3.2.3.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.F3.2.3.1.1.1" class="ltx_text" style="font-size:90%;">(a) close-set detection</span></td>
<td id="S4.F3.2.3.1.2" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.F3.2.3.1.2.1" class="ltx_text" style="font-size:90%;">(b) open-set detection</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span> The AP on LVIS v1.0 over categories sorted by frequency in ascending order. (a) uses on the close-set setting, only containing 461 â€œcommonâ€ categories and 405 â€œfrequentâ€ categories. (b) uses the open-set setting, containing containing 337 â€œrareâ€ categories, 461 â€œcommonâ€ categories and 405 â€œfrequentâ€ categories. The value is smoothed using moving average with window [-10,10].</figcaption>
</figure>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 5: </span>The effect of image-level alignment on LVIS v1.0Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> with open-set settings.</figcaption>
<table id="S4.T5.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.3.3" class="ltx_tr">
<th id="S4.T5.3.3.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T5.3.3.4.1" class="ltx_text" style="font-size:80%;">Model</span></th>
<th id="S4.T5.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:8.0pt;padding-right:8.0pt;">
<span id="S4.T5.1.1.1.1" class="ltx_text" style="font-size:80%;">AP</span><sub id="S4.T5.1.1.1.2" class="ltx_sub"><span id="S4.T5.1.1.1.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">r</span></sub>
</th>
<th id="S4.T5.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:8.0pt;padding-right:8.0pt;">
<span id="S4.T5.2.2.2.1" class="ltx_text" style="font-size:80%;">AP</span><sub id="S4.T5.2.2.2.2" class="ltx_sub"><span id="S4.T5.2.2.2.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">c</span></sub>
</th>
<th id="S4.T5.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:8.0pt;padding-right:8.0pt;">
<span id="S4.T5.3.3.3.1" class="ltx_text" style="font-size:80%;">AP</span><sub id="S4.T5.3.3.3.2" class="ltx_sub"><span id="S4.T5.3.3.3.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">f</span></sub>
</th>
<th id="S4.T5.3.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T5.3.3.5.1" class="ltx_text" style="font-size:80%;">AP</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.3.4.1" class="ltx_tr">
<th id="S4.T5.3.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T5.3.4.1.1.1" class="ltx_text" style="font-size:80%;">GridCLIP-R50 w grid-align</span></th>
<th id="S4.T5.3.4.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T5.3.4.1.2.1" class="ltx_text" style="font-size:80%;">10.1</span></th>
<td id="S4.T5.3.4.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T5.3.4.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">21.0</span></td>
<td id="S4.T5.3.4.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T5.3.4.1.4.1" class="ltx_text" style="font-size:80%;">29.6</span></td>
<td id="S4.T5.3.4.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T5.3.4.1.5.1" class="ltx_text" style="font-size:80%;">22.5</span></td>
</tr>
<tr id="S4.T5.3.5.2" class="ltx_tr">
<th id="S4.T5.3.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T5.3.5.2.1.1" class="ltx_text" style="font-size:80%;">GridCLIP-R50</span></th>
<th id="S4.T5.3.5.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T5.3.5.2.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">12.7</span></th>
<td id="S4.T5.3.5.2.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T5.3.5.2.3.1" class="ltx_text" style="font-size:80%;">20.6</span></td>
<td id="S4.T5.3.5.2.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T5.3.5.2.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">29.7</span></td>
<td id="S4.T5.3.5.2.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T5.3.5.2.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">22.8</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.p3.5" class="ltx_p"><span id="S4.SS2.p3.5.3" class="ltx_text ltx_font_bold">Resources Comparison.</span>
We compare GridCLIP with ViLD on training and test time,
as well as the model size.
ViLD is originally trained on TPUv3 with a batch size of 256.
For fair comparison and due to resource limitation,
we train both ViLD and GridCLIP with a batch size of 16 on 2 A100 GPUs.
We train ViLD using the implementation of DetProÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.
Moreover, ViLD takes 1 day on 8 V100 GPUs to pre-compute the
CLIP image embedding of regions to accelerate training. <span id="S4.SS2.p3.5.4" class="ltx_text" style="color:#000000;">GridCLIP
does not require this.</span>
In TableÂ <a href="#S4.T2" title="Table 2 â€£ 4 Experiments â€£ GridCLIP: One-Stage Object Detection by Grid-Level CLIP Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we show that with comparable model size,
GridCLIP-R50 is approximately <span id="S4.SS2.p3.2.2" class="ltx_text" style="color:#000000;">43<math id="S4.SS2.p3.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS2.p3.1.1.m1.1a"><mo mathcolor="#000000" id="S4.SS2.p3.1.1.m1.1.1" xref="S4.SS2.p3.1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.1.m1.1b"><times id="S4.SS2.p3.1.1.m1.1.1.cmml" xref="S4.SS2.p3.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.1.m1.1c">\times</annotation></semantics></math> and 5<math id="S4.SS2.p3.2.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS2.p3.2.2.m2.1a"><mo mathcolor="#000000" id="S4.SS2.p3.2.2.m2.1.1" xref="S4.SS2.p3.2.2.m2.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.2.m2.1b"><times id="S4.SS2.p3.2.2.m2.1.1.cmml" xref="S4.SS2.p3.2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.2.m2.1c">\times</annotation></semantics></math></span> faster than ViLD in
<span id="S4.SS2.p3.5.5" class="ltx_text" style="color:#000000;">training and test respectively</span>.
Such significant advantages
remain when GridCLIP-R50-RN using RN50<math id="S4.SS2.p3.3.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS2.p3.3.m1.1a"><mo id="S4.SS2.p3.3.m1.1.1" xref="S4.SS2.p3.3.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.3.m1.1b"><times id="S4.SS2.p3.3.m1.1.1.cmml" xref="S4.SS2.p3.3.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.3.m1.1c">\times</annotation></semantics></math>64 (3<math id="S4.SS2.p3.4.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS2.p3.4.m2.1a"><mo id="S4.SS2.p3.4.m2.1.1" xref="S4.SS2.p3.4.m2.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.4.m2.1b"><times id="S4.SS2.p3.4.m2.1.1.cmml" xref="S4.SS2.p3.4.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.4.m2.1c">\times</annotation></semantics></math> larger in both input and parameters) for image-level alignment, with 34<math id="S4.SS2.p3.5.m3.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS2.p3.5.m3.1a"><mo id="S4.SS2.p3.5.m3.1.1" xref="S4.SS2.p3.5.m3.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.5.m3.1b"><times id="S4.SS2.p3.5.m3.1.1.cmml" xref="S4.SS2.p3.5.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.5.m3.1c">\times</annotation></semantics></math>
faster in training time than that of ViLD.
This validates
clearly the compute efficiency of the one-stage GridCLIP.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para ltx_noindent">
<p id="S4.SS2.p4.1" class="ltx_p"><span id="S4.SS2.p4.1.1" class="ltx_text ltx_font_bold">Transfer to Other Datasets.</span>
To further explore the generalizability of GridCLIP, we follow ViLDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> and evaluate the LVIS-trained GridCLIP on <span id="S4.SS2.p4.1.2" class="ltx_text" style="color:#000000;">both the</span> PASCAL VOC 2007 test setÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and the COCO validation setÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> by directly replacing the categories without any finetuning.
Note that there are overlaps of both category and image between LVIS and COCO (as well as PASCAL VOC).
The IOU threshold
of NMS is 0.6. On PASCAL VOC, we observe that
the gap between GridCLIP-R50 and ViLD is
1.2 to 1.3 AP,
and GridCLIP-R50-RN is comparable with ViLD on PASCAL VOC
with no more than <span id="S4.SS2.p4.1.3" class="ltx_text" style="color:#000000;">1 AP difference</span>.
Although the gap on COCO is still obvious with 2.2 AP falling behind but is comparable to that of DetPro which uses learn prompts based on ViLD.
Therefore, in the generalization ability, GridCLIP performs quite close to its two-stage counterparts.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Ablation Studies</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We verify the effectiveness of grid-level and image-level alignment for undersampled categories
for both close-set detection (TableÂ <a href="#S4.T4" title="Table 4 â€£ 4.2 Comparison with the State-of-the-Art â€£ 4 Experiments â€£ GridCLIP: One-Stage Object Detection by Grid-Level CLIP Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>)
and open-set detection (TableÂ <a href="#S4.T5" title="Table 5 â€£ 4.2 Comparison with the State-of-the-Art â€£ 4 Experiments â€£ GridCLIP: One-Stage Object Detection by Grid-Level CLIP Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>).
We follow the same settings in Sec.Â <a href="#S4.SS2" title="4.2 Comparison with the State-of-the-Art â€£ 4 Experiments â€£ GridCLIP: One-Stage Object Detection by Grid-Level CLIP Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a> except
that multi-scaling training and random cropping augmentation
are not used here.
<span id="S4.SS3.p1.1.1" class="ltx_text" style="color:#000000;">Among the experiments,
â€œwo alignâ€ denotes using the original design of FCOS only with different backbones that feed different image features to the FPN. â€œw grid-alignâ€ and â€œw image-alignâ€ denote only using grid-level or image-level alignment respectively.</span></p>
</div>
<figure id="S4.F4" class="ltx_figure">
<table id="S4.F4.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.F4.4.4" class="ltx_tr">
<td id="S4.F4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><img src="/html/2303.09252/assets/x5.png" id="S4.F4.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="120" height="90" alt="Refer to caption"></td>
<td id="S4.F4.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><img src="/html/2303.09252/assets/x6.png" id="S4.F4.2.2.2.g1" class="ltx_graphics ltx_img_landscape" width="120" height="90" alt="Refer to caption"></td>
<td id="S4.F4.3.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><img src="/html/2303.09252/assets/x7.png" id="S4.F4.3.3.3.g1" class="ltx_graphics ltx_img_landscape" width="120" height="90" alt="Refer to caption"></td>
<td id="S4.F4.4.4.4" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><img src="/html/2303.09252/assets/x8.png" id="S4.F4.4.4.4.g1" class="ltx_graphics ltx_img_landscape" width="120" height="90" alt="Refer to caption"></td>
</tr>
<tr id="S4.F4.4.5.1" class="ltx_tr">
<td id="S4.F4.4.5.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.F4.4.5.1.1.1" class="ltx_text" style="font-size:90%;">(a) Percent of images</span></td>
<td id="S4.F4.4.5.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.F4.4.5.1.2.1" class="ltx_text" style="font-size:90%;">(b) RC@10</span></td>
<td id="S4.F4.4.5.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.F4.4.5.1.3.1" class="ltx_text" style="font-size:90%;">(c) RC@100</span></td>
<td id="S4.F4.4.5.1.4" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.F4.4.5.1.4.1" class="ltx_text" style="font-size:90%;">(d) RC@300</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span> (a) shows the percent of images containing different numbers of categories in an image on LVIS v1.0 validation set.
(b), (c) and (d) are the recall of top k (k=10, 100, 300) predictions of different CLIP pretrained versions, using the image-level representation from the corresponding CLIP image encoder. </figcaption>
</figure>
<div id="S4.SS3.p2" class="ltx_para ltx_noindent">
<p id="S4.SS3.p2.1" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_bold">Close-set Detection.</span>
We first evaluate other visual pretrained models as the backbone to compare to the vision-language pretrained model CLIP,
including the ImageNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> pretrained ResNet50 on the classification task
and self-supervised pretrained ResNet50 using visual-only SSL method SwAVÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> pretrained on large-scale unlabeled images.
By comparing the methods using different pretrained ResNet50 without any alignment (the top section of TableÂ <a href="#S4.T4" title="Table 4 â€£ 4.2 Comparison with the State-of-the-Art â€£ 4 Experiments â€£ GridCLIP: One-Stage Object Detection by Grid-Level CLIP Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>), we notice that using the CLIP pretrained ResNet50 can bring notable improvements compared to the ImageNet and SwAV pretrained ones, with the similar architecture, which indicates the superiority of the vision-language pretrained model CLIP than other visual pretrained models in generalizing better image embeddings for detection.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.6" class="ltx_p">On the bottom section of TableÂ <a href="#S4.T4" title="Table 4 â€£ 4.2 Comparison with the State-of-the-Art â€£ 4 Experiments â€£ GridCLIP: One-Stage Object Detection by Grid-Level CLIP Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we introduce the MHSA layer whose output is aligned to the CLIP text encoder in the original training of CLIP. Therefore, the MHSA layer provides both grid-level and image-level features for CLIP-based alignment.
We first observe that introducing the MHSA layer without any alignment drops the overall performance by 0.7 AP,
while using both alignments can improve the performance in the infrequent common categories by 1.3 AP and preserves the performance in frequent categories.
Among the experiments that use the MHSA layer,
we find that applying grid-level alignment can significantly improve the common categories by 2.3 AP, while using
image-level alignment seems limited on the metrics AP<sub id="S4.SS3.p3.6.1" class="ltx_sub"><span id="S4.SS3.p3.6.1.1" class="ltx_text ltx_font_italic">c</span></sub> and AP<sub id="S4.SS3.p3.6.2" class="ltx_sub"><span id="S4.SS3.p3.6.2.1" class="ltx_text ltx_font_italic">f</span></sub>.
Using both alignments can bring 1.8 AP<sub id="S4.SS3.p3.6.3" class="ltx_sub"><span id="S4.SS3.p3.6.3.1" class="ltx_text ltx_font_italic">c</span></sub> and 0.3 AP<sub id="S4.SS3.p3.6.4" class="ltx_sub"><span id="S4.SS3.p3.6.4.1" class="ltx_text ltx_font_italic">f</span></sub> improvements which are lower than the one using only grid-level alignment.
We further explore the reasons behind that.
We find that the metric of AP<sub id="S4.SS3.p3.6.5" class="ltx_sub"><span id="S4.SS3.p3.6.5.1" class="ltx_text ltx_font_italic">c</span></sub> and AP<sub id="S4.SS3.p3.6.6" class="ltx_sub"><span id="S4.SS3.p3.6.6.1" class="ltx_text ltx_font_italic">f</span></sub> are too coarse-grained for
distinguishing categories with different frequencies.
So we further observe the performance in a more fine-grained way by the plot of AP over categories with different sample numbers (FigureÂ <a href="#S4.F3" title="Figure 3 â€£ 4.2 Comparison with the State-of-the-Art â€£ 4 Experiments â€£ GridCLIP: One-Stage Object Detection by Grid-Level CLIP Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>).
As shown in subfigure (a), in the 200 most infrequent categories, the improvement of applying one alignment is not stable, where the AP can be notably higher than â€œGridCLIP-R50 w/o alignâ€ in some categories while obviously worse in other categories.
By applying both alignments,
â€œGridCLIP-R50â€ primarily ranks top in the 100 most infrequent categories, while
its superiority is not obvious in frequent categories.
Therefore, we can conclude that using both alignments benefits the undersampled categories in close-set detection.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para ltx_noindent">
<p id="S4.SS3.p4.1" class="ltx_p"><span id="S4.SS3.p4.1.1" class="ltx_text ltx_font_bold">Open-set Detection.</span>
Since only models with grid-level alignment can be extended for open-set detection by extending the categories embedding list with novel categories and using the list match with the grid-level image embeddings, we compare two models from TableÂ <a href="#S4.T4" title="Table 4 â€£ 4.2 Comparison with the State-of-the-Art â€£ 4 Experiments â€£ GridCLIP: One-Stage Object Detection by Grid-Level CLIP Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
As shown in TableÂ <a href="#S4.T5" title="Table 5 â€£ 4.2 Comparison with the State-of-the-Art â€£ 4 Experiments â€£ GridCLIP: One-Stage Object Detection by Grid-Level CLIP Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>,
image-level alignment improves novel categories by <span id="S4.SS3.p4.1.2" class="ltx_text" style="color:#000000;">2.6</span> AP<sub id="S4.SS3.p4.1.3" class="ltx_sub"><span id="S4.SS3.p4.1.3.1" class="ltx_text ltx_font_italic">r</span></sub>, while preserving the performance on base categories.
Also, as shown in FigureÂ <a href="#S4.F3" title="Figure 3 â€£ 4.2 Comparison with the State-of-the-Art â€£ 4 Experiments â€£ GridCLIP: One-Stage Object Detection by Grid-Level CLIP Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (b), the performance rises over categories as their training sample number increases, which verifies that undersampled categories suffer from long-tail distribution.
In the 200 most infrequent categories which are novel (also rare) categories, â€œGridCLIP-R50â€ outperforms â€œGridCLIP-R50 w grid-alignâ€ significantly, which indicates the effectiveness of image-level alignment on novel categories. Therefore, it is verified the alignment of image-level representations also helps learn generalizable grid-level representations of undersampled categories.</p>
</div>
<div id="S4.SS3.p5" class="ltx_para">
<p id="S4.SS3.p5.1" class="ltx_p">In summary, grid-level alignment improves the performance notably on undersampled categories in close-set detection and allows the detector to be extended for open-set detection, while image-level alignment obviously benefits the novel categories in open-set detection.
Using both alignments enables a one-stage detector to detect novel categories and mitigate the deterioration of undersampled categories in long-tail datasets.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Further Analysis: Evaluating CLIP Image-Level Representation for Object Detection</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">We analyse how accurately the multiple categories in an image can be represented by the original CLIP image encoder.
This substantially affects the performance of a one-stage detector built upon the CLIP image-level representation
to detect multiple categories at the same time, which indicates how much image-level alignment can benefit GridCLIP.
We evaluate several pretrained versions of CLIP on LVIS v1.0 validation set, including the refined ResNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> (RN50, RN101, RN50<math id="S4.SS4.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS4.p1.1.m1.1a"><mo id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><times id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">\times</annotation></semantics></math>64) and those with the Transformer architectureÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> (ViT-B/32).
Specifically, We calculate the recall of categories by using the original CLIP image-level representation to match the text representation of each category (FigureÂ <a href="#S4.F4" title="Figure 4 â€£ 4.3 Ablation Studies â€£ 4 Experiments â€£ GridCLIP: One-Stage Object Detection by Grid-Level CLIP Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>).</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.2" class="ltx_p">For RC@10, all models perform poorly with no more than 2 recalls.
While for RC@100, we find that ViT-B/32 can recall 50% of the categories and RN50<math id="S4.SS4.p2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS4.p2.1.m1.1a"><mo id="S4.SS4.p2.1.m1.1.1" xref="S4.SS4.p2.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.1.m1.1b"><times id="S4.SS4.p2.1.m1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.1.m1.1c">\times</annotation></semantics></math>64 can recall more than 30% of the categories. In comparison, the other ResNet-based models perform poorly that reach less than 20% recall rate.
Furthermore, for RC@300, nearly 75% of the categories in an image are captured by ViT-B/32, and RN50<math id="S4.SS4.p2.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS4.p2.2.m2.1a"><mo id="S4.SS4.p2.2.m2.1.1" xref="S4.SS4.p2.2.m2.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.2.m2.1b"><times id="S4.SS4.p2.2.m2.1.1.cmml" xref="S4.SS4.p2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.2.m2.1c">\times</annotation></semantics></math>64 reaches about 60% recall rate.
Given that the maximum detection number for LVIS v1.0 in OVOD is usually set to 300 (objects), ViT-B/32 can at most help detect 75% of the objects if all the objects have different categories and 50% if every 3 of the objects share the same category.
This provides substantial knowledge of categories to help the detector build the representation for multiple object detection.
Therefore, the CLIP image encoder is able to capture multiple categories in an image at the same time with relatively high accuracy and provide substantial knowledge of categories for the detector during image-level alignment.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this work,
we introduce a one-stage detector GridCLIP,
which exploits the CLIP representation space to supplement the knowledge on undersampled categories in downstream detection datasets.
GridCLIP optimizes generalizable knowledge from the CLIP
pretrained representation to more fine-grained localized mapping,
by simultaneously
learning a localized grid-level CLIP
mapping to base categories (Grid-level Alignment) and a holistic
image-level knowledge distillation to base and novel
categories in a target
object detection domain (Image-level Alignment).
In our experiments, we verify that GridCLIP
suffers less from long-tail distributions with the help of both grid-level and image-level alignments,
reaching comparable performance
on the LVIS v1.0 benchmark <span id="S5.p1.1.1" class="ltx_text" style="color:#000000;">with higher training and inference speed.</span></p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and
Armand Joulin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Unsupervised learning of visual features by contrasting cluster
assignments.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib1.4.2" class="ltx_text" style="font-size:90%;">,
33:9912â€“9924, 2020.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li,
Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Mmdetection: Open mmlab detection toolbox and benchmark.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1906.07155</span><span id="bib.bib2.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Imagenet: A large-scale hierarchical image database.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2009 IEEE conference on computer vision and pattern
recognition</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, pages 248â€“255. Ieee, 2009.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">An image is worth 16x16 words: Transformers for image recognition at
scale.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">9th International Conference on Learning Representations,
ICLR 2021, Virtual Event, Austria, May 3-7, 2021</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">. OpenReview.net, 2021.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Yu Du, Fangyun Wei, Zihe Zhang, Miaojing Shi, Yue Gao, and Guoqi Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Learning to prompt for open-vocabulary object detection with
vision-language model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, pages 14084â€“14093, 2022.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Mark Everingham, Luc VanÂ Gool, ChristopherÂ KI Williams, John Winn, and Andrew
Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">The pascal visual object classes (voc) challenge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International journal of computer vision</span><span id="bib.bib6.4.2" class="ltx_text" style="font-size:90%;">, 88(2):303â€“338, 2010.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Chengjian Feng, Yujie Zhong, Zequn Jie, Xiangxiang Chu, Haibing Ren, Xiaolin
Wei, Weidi Xie, and Lin Ma.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Promptdet: Expand your detector vocabulary with uncurated images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2203.16513</span><span id="bib.bib7.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Mingfei Gao, Chen Xing, JuanÂ Carlos Niebles, Junnan Li, Ran Xu, Wenhao Liu, and
Caiming Xiong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Towards open vocabulary object detection without human-provided
bounding boxes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2111.09452</span><span id="bib.bib8.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Open-vocabulary object detection via vision and language knowledge
distillation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">The Tenth International Conference on Learning
Representations, ICLR 2022, Virtual Event, April 25-29, 2022</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">.
OpenReview.net, 2022.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Agrim Gupta, Piotr Dollar, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Lvis: A dataset for large vocabulary instance segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, pages 5356â€“5364, 2019.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, pages 770â€“778, 2016.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Tony Huang, Jack Chu, and Fangyun Wei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Unsupervised prompt learning for vision-language models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2204.03649</span><span id="bib.bib12.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Scaling up visual and vision-language representation learning with
noisy text supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, pages
4904â€“4916. PMLR, 2021.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Weicheng Kuo, Yin Cui, Xiuye Gu, AJ Piergiovanni, and Anelia Angelova.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">F-vlm: Open-vocabulary object detection upon frozen vision and
language models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2209.15639</span><span id="bib.bib14.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong,
and Steven ChuÂ Hong Hoi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Align before fuse: Vision and language representation learning with
momentum distillation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib15.4.2" class="ltx_text" style="font-size:90%;">,
34:9694â€“9705, 2021.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
LiunianÂ Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li,
Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Grounded language-image pre-training.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, pages 10965â€“10975, 2022.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr DollÃ¡r.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Focal loss for dense object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE international conference on computer
vision</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, pages 2980â€“2988, 2017.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr DollÃ¡r, and CÂ Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Microsoft coco: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European conference on computer vision</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, pages 740â€“755.
Springer, 2014.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Timo LÃ¼ddecke and AlexanderÂ S Ecker.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Prompt-based multi-modal image segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2112.10003</span><span id="bib.bib19.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Zongyang Ma, Guan Luo, Jin Gao, Liang Li, Yuxin Chen, Shaoru Wang, Congxuan
Zhang, and Weiming Hu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Open-vocabulary one-stage detection with hierarchical visual-language
knowledge distillation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, pages 14074â€“14083, 2022.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Alec Radford, JongÂ Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Learning transferable visual models from natural language
supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, pages
8748â€“8763. PMLR, 2021.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong Tang, Zheng Zhu, Guan Huang,
Jie Zhou, and Jiwen Lu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Denseclip: Language-guided dense prediction with context-aware
prompting.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE/CVF Conference on Computer Vision and Pattern
Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, pages
18061â€“18070. IEEE, 2022.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Faster r-cnn: Towards real-time object detection with region proposal
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib23.4.2" class="ltx_text" style="font-size:90%;">, 28, 2015.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
AlexanderÂ I Saichev, Yannick Malevergne, and Didier Sornette.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Theory of Zipfâ€™s law and beyond</span><span id="bib.bib24.3.2" class="ltx_text" style="font-size:90%;">, volume 632.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.4.1" class="ltx_text" style="font-size:90%;">Springer Science &amp; Business Media, 2009.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Conceptual captions: A cleaned, hypernymed, image alt-text dataset
for automatic image captioning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers)</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, pages 2556â€“2565, 2018.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech
Galuba, Marcus Rohrbach, and Douwe Kiela.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">FLAVA: A foundational language and vision alignment model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE/CVF Conference on Computer Vision and Pattern
Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, pages
15617â€“15629. IEEE, 2022.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng Xu, Wei Zhan, Masayoshi
Tomizuka, Lei Li, Zehuan Yuan, Changhu Wang, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Sparse r-cnn: End-to-end object detection with learnable proposals.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, pages 14454â€“14463, 2021.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Zhi Tian, Chunhua Shen, Hao Chen, and Tong He.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Fcos: Fully convolutional one-stage object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF international conference on
computer vision</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, pages 9627â€“9636, 2019.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
AidanÂ N Gomez, Åukasz Kaiser, and Illia Polosukhin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Attention is all you need.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib29.4.2" class="ltx_text" style="font-size:90%;">, 30, 2017.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Fangyun Wei, Yue Gao, Zhirong Wu, Han Hu, and Stephen Lin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Aligning pretraining for detection via object-level contrastive
learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib30.4.2" class="ltx_text" style="font-size:90%;">, 34, 2021.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Johnathan Xie and Shuai Zheng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Zsd-yolo: Zero-shot yolo detection using vision-language
knowledgedistillation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2109.12066</span><span id="bib.bib31.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Ceyuan Yang, Zhirong Wu, Bolei Zhou, and Stephen Lin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Instance localization for self-supervised detection pretraining.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">, pages 3987â€“3996, 2021.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao,
Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Florence: A new foundation model for computer vision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2111.11432</span><span id="bib.bib33.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Alireza Zareian, KevinÂ Dela Rosa, DerekÂ Hao Hu, and Shih-Fu Chang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Open-vocabulary object detection using captions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">, pages 14393â€“14402, 2021.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun Chen, LiunianÂ Harold Li,
Xiyang Dai, Lijuan Wang, Lu Yuan, Jenq-Neng Hwang, and Jianfeng Gao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Glipv2: Unifying localization and vision-language understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang,
Yejin Choi, and Jianfeng Gao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Vinvl: Revisiting visual representations in vision-language models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:90%;">, pages 5579â€“5588, 2021.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and StanÂ Z. Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Bridging the gap between anchor-based and anchor-free detection via
adaptive training sample selection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib37.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib37.5.3" class="ltx_text" style="font-size:90%;">, June 2020.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella,
LiunianÂ Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, and Jianfeng
Gao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Regionclip: Region-based language-image pretraining.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE/CVF Conference on Computer Vision and Pattern
Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022</span><span id="bib.bib38.5.3" class="ltx_text" style="font-size:90%;">, pages
16772â€“16782. IEEE, 2022.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Chong Zhou, ChenÂ Change Loy, and Bo Dai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">Denseclip: Extract free dense labels from clip.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2112.01071</span><span id="bib.bib39.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Kaiyang Zhou, Jingkang Yang, ChenÂ Change Loy, and Ziwei Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Learning to prompt for vision-language models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. J. Comput. Vis.</span><span id="bib.bib40.4.2" class="ltx_text" style="font-size:90%;">, 130(9):2337â€“2348, 2022.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Xingyi Zhou, Rohit Girdhar, Armand Joulin, Phillip KrÃ¤henbÃ¼hl, and
Ishan Misra.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">Detecting twenty-thousand classes using image-level supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2201.02605</span><span id="bib.bib41.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2303.09251" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2303.09252" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2303.09252">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2303.09252" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2303.09253" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 19:27:16 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
