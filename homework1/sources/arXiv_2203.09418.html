<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2203.09418] ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation</title><meta property="og:description" content="Establishing correspondences from image to 3D has been a key task of 6DoF object pose estimation for a long time. To predict pose more accurately, deeply learned dense maps replaced sparse templates. Dense methods alsoâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2203.09418">

<!--Generated on Mon Mar 11 10:15:22 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">

<span id="id1.1.id1" class="ltx_tabular ltx_align_middle">
<span id="id1.1.id1.1" class="ltx_tr">
<span id="id1.1.id1.1.1" class="ltx_td ltx_align_center">Yongzhi Su<sup id="id1.1.id1.1.1.1" class="ltx_sup">1,2*</sup></span>
<span id="id1.1.id1.1.2" class="ltx_td ltx_align_center">Mahdi Saleh<sup id="id1.1.id1.1.2.1" class="ltx_sup">3*</sup></span>
<span id="id1.1.id1.1.3" class="ltx_td ltx_align_center">Torben Fetzer<sup id="id1.1.id1.1.3.1" class="ltx_sup">2</sup></span>
<span id="id1.1.id1.1.4" class="ltx_td ltx_align_center">Jason Rambach<sup id="id1.1.id1.1.4.1" class="ltx_sup">1</sup></span></span>
<span id="id1.1.id1.2" class="ltx_tr">
<span id="id1.1.id1.2.1" class="ltx_td ltx_align_center">Nassir Navab<sup id="id1.1.id1.2.1.1" class="ltx_sup">3</sup></span>
<span id="id1.1.id1.2.2" class="ltx_td ltx_align_center">Benjamin Busam<sup id="id1.1.id1.2.2.1" class="ltx_sup">3</sup></span>
<span id="id1.1.id1.2.3" class="ltx_td ltx_align_center">Didier Stricker<sup id="id1.1.id1.2.3.1" class="ltx_sup">1,2</sup></span>
<span id="id1.1.id1.2.4" class="ltx_td ltx_align_center">Federico Tombari<sup id="id1.1.id1.2.4.1" class="ltx_sup">3,4</sup></span></span>
</span>

<br class="ltx_break"><sup id="id2.2.id2" class="ltx_sup">1</sup> German Research Center for Artificial Intelligence (DFKI)Â Â Â Â Â Â Â Â Â Â Â Â Â 
<sup id="id3.3.id3" class="ltx_sup">2</sup> TU Kaiserslautern
<br class="ltx_break"><sup id="id4.4.id4" class="ltx_sup">3</sup>Technische UniversitÃ¤t MÃ¼nchen Â Â Â Â Â Â Â Â Â Â Â Â Â 
<sup id="id5.5.id5" class="ltx_sup">4</sup>Google
<br class="ltx_break"><span id="id6.6.id6" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{yongzhi.su; jason.rambach; torben.fetzer; didier.stricker}@dfki.de
<br class="ltx_break">{m.saleh; b.busam; nassir.navab}@tum.de, tombari@in.tum.de


</span>
</span><span class="ltx_author_notes"><sup id="id7.7.id1" class="ltx_sup"><span id="id7.7.id1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">*</span></sup><span id="id8.8.id2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">The authors contributed equally to this paper</span><span id="id9.9.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Code: </span><a target="_blank" href="https://github.com/suyz526/ZebraPose" title="" class="ltx_ref ltx_href ltx_font_typewriter" style="font-size:90%;">https://github.com/suyz526/ZebraPose</a></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id10.id1" class="ltx_p">Establishing correspondences from image to 3D has been a key task of 6DoF object pose estimation for a long time. To predict pose more accurately, deeply learned dense maps replaced sparse templates. Dense methods also improved pose estimation in the presence of occlusion. More recently researchers have shown improvements by learning object fragments as segmentation. In this work, we present a discrete descriptor, which can represent the object surface densely. By incorporating a hierarchical binary grouping, we can encode the object surface very efficiently. Moreover, we propose a coarse to fine training strategy, which enables fine-grained correspondence prediction. Finally, by matching predicted codes with object surface and using a PnP solver, we estimate the 6DoF pose. Results on the public LM-O and YCB-V datasets show major improvement over the state of the art w.r.t. ADD(-S) metric, even surpassing RGB-D based methods in some cases.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Augmented reality and robotics are two of the main application fields of 3D computer vision. In many augmented reality applications, the location and pose of an object of interest has to be determined at a high precisionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>. Similarly, object grasping and manipulation is needed for many robotic applications (e.g. automatic manufacturingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>, cooperative assistanceÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>), and also demands accurate 6 Degree-of-Freedom (6DoF) object pose information. As the crucial element in both application domains, estimating the 6DoF object pose has received increasing attention from the computer vision research community.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2203.09418/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="329" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">ZebraPose assigns a discrete code to each surface vertex hierarchically. We project the code as binary black and white values (top) and learn them using deep neural networks. Our binary descriptor allows one-to-one correspondence for the problem of 6DoF object pose efficiently. </span></figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The correspondence problem is a classical problem in computer vision. While finding correspondences across the same domain is more straightforward, estimating the 6DoF object pose requires 2D-3D correspondences. In earlier object pose estimation research, depth maps came to help to match image pixels to 3D surface pointsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite>. Due to cost and setup complications, the detection of 6DoF pose without depth information can be advantageous. However, RGB approaches typically achieve a lower accuracy with respect to their depth-based counterpartsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Driven by the recent developments in deep learning and Convolutional Neural Networks (CNNs), various methods were proposed, which make 6DoF pose estimation from a single RGB image feasibleÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib70" title="" class="ltx_ref">70</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>. In a correspondence-based setting, to estimate the object pose, Perspective-n-Points (PnP) algorithms require at least 4 2D-3D point matchesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>. Therefore, sparse methods are applied to extract points of interest <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>. However such methods might fail to find object landmarks under viewpoint changes, occlusion, or lack of texture.
With the success of deep neural networks in image synthesis problems, researchers use such tools to generate dense correspondence maps. For instance, several methods learn UVÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite> or UVW Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> values in object local coordinates. Since the network produces dense smooth results, certain low-level geometry is lost. Moreover, neural networks tend to achieve a higher performance in classification tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To this aim, we propose a dense correspondence pipeline that combines the concepts of handcrafted features and image segmentation in a hierarchical fashion for RGB-based 6DoF pose estimation.
In order to design a descriptor that encodes surfaces efficiently, we use the binary numeral system. Binary-based descriptors are applied in ORB Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> and are still in use in robust SLAM applicationsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. In our work, we split the surface into halves in multiple iterations and define our vertex encoding by stacking the assigned group labels. By leveraging a hierarchical discrete representation, we guarantee a compact mapping and simple learning objective as a multi-label classification problem <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>.
Moreover, learning how to encode a full sequence at once might be challenging for neural networks. Therefore, we propose a coarse to fine learning scheme. By design, our encodings on the coarse levels are continuously shared in wider object regions. As the network learns to differentiate coarse splits, we focus on finer encoding positions. With a coarse to fine loss and training strategy, we then manage to predict fine-grained surface correspondences.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In contrast to previous works where there is no guaranteed putative correspondenceÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>, our encoding promotes direct pixel-to-surface matching just by means of a look-up table. With a simple matching and PnP-RANSAC scheme of Progressive-XÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, we outperform the state of the art in 6DoF pose on the most commonly used benchmarks w.r.t. ADD(-S) metric.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">In summary, we propose ZebraPose, a two-stage RGB-based approach that defines the matching of dense 2D-3D correspondence as a hierarchical classification task. We divide the general two-stage approach for 6DoF object pose estimation into three components: 1) assigning a unique descriptor to the 3D vertex; 2) predicting a dense correspondence between the 2D pixels and 3D vertices; 3) solving the object pose using the predicted correspondences. We can summarize our proposed contributions in this paper related to the first two components:
</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">A novel coarse to fine surface encoding method assigning the dense vertex descriptor in an efficient way, which also fully exploits traditional outlier filters used in computer vision task.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">A novel hierarchical training loss and strategy to automatically adjust the weights of each code position.</p>
</div>
</li>
</ul>
<p id="S1.p6.2" class="ltx_p">Extensive experiments on LM-OÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> and YCB-VÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> datasets show that our proposed approach achieves state of the art results.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">We limit our in-depth discussion of related work to the most relevant methods to our work, i.e. RGB-based 6DoF pose estimation, and object surface encoding techniques.
</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>RGB-based 6DoF Pose Estimation</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p"><span id="S2.SS1.p1.1.1" class="ltx_text ltx_font_bold">Traditional Methods.</span>
With the development of the feature descriptorÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, the object pose problem could be solved by feeding estimated 2D-3D correspondences into a RANSAC/PnP framework. However, dealing with texture-less objects remained a challenge. To overcome the lack of keypoints, Hinterstoisser <em id="S2.SS1.p1.1.2" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS1.p1.1.3" class="ltx_text"></span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> proposed to utilize the image gradient information and formulate the pose estimation task within a template matching pipeline. Later advancesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> avoided the template searching time by applying a statistical learning-based framework to regress object coordinates and object labels jointly.
However, the accuracy that handcrafted methods can achieve is far from that of deep learning methods nowadays.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p"><span id="S2.SS1.p2.1.1" class="ltx_text ltx_font_bold">End-to-End Methods.</span>
PoseNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> was the first work that attempted to regress the camera viewpoint with a CNN. Following works usually concatenated an object detector with the pose regression, making multi-object pose estimation possibleÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite>. Finding a suitable rotation representation for pose regression was a problem at that time and typical rotation parametrization did not populate Euclidean spacesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. SSD6DÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> avoided complex parameters by discretization of the rotation space thus treating the rotation estimation as a classification problem. Zhou et.alÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite> proposed a continuous 6-dimensional rotation representation that shows advantages over quaternionsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> or Lie algebraÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> parametrization for neural network training. This representation is utilized in several direct regression worksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">In parallel, several efforts have been made to integrate RANSAC and PnP modules to pose learning frameworks. Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> propose differentiable RANSAC variants, which are not applicable to object pose estimation as they require a good initialization and complex training strategy.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> proposes a network to solve the PnP problem, with a loss function reflecting pose metrics. At the same time, a new branch of methods has been developed with the growth of neural renderersÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> is able to define the loss according to the texture colour on pixel level. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite> used a differentiable depth map and achieved self-supervised network fine-tuning with unlabeled RGB-D data. In an effort to combine correspondence-based methods with direct regression of 6DoF parameters, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite> used correspondence maps as an intermediate geometric representation to regress the pose. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> further enhances <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite> by employing self-occlusion information that provides richer information to predict the object pose with the predicted 2D-3D correspondences.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p"><span id="S2.SS1.p4.1.1" class="ltx_text ltx_font_bold">Indirect Methods with Deep Learning.</span> While end-to-end methods have evolved through time by integrating differentiable modules, the performance of such methods are normally below geometrical and indirect methods. Combining learning features and geometrical fitting,
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite> uses metric learning to learn an implicit pose representation through triplet loss and finally looks for nearest neighbors in pose space. AAEÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite> learns to generate a latent vector based on the visual information of the object in discrete viewpoints. At inference stage, the rotation is obtained by comparing the latent code with the pre-generated rotation-latent code lookup table. The rest of the indirect methods usually estimate the 2D-3D correspondence, and solve the object pose using RANSAC/PnP. BB8Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> firstly defines the 3D object bounding box corners as the keypoints and PVNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> reaches high recall rate in LMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> dataset by predicting the keypoints with a dense pixel-wise voting for sampled keypoints on the object. The main drawback of such sparse 2D-3D correspondence methods is that the prediction of keypoints in the occluded area lacks in accuracy. HybridPoseÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite> proposed to leverage multiple geometric information to tackle this issue while other methodsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib72" title="" class="ltx_ref">72</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> predict pixel-wise dense 2D-3D correspondences.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Surface Encoding</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The binary surface encoding technique has been successfully used in the field of structured light reconstruction for many yearsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>, <a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>. For this purpose, a video projector illuminates the scene with several successively refined binary fringe patterns. The composition of the different stripe patterns provides an encoding of the surface points.
Surface coding using multiple classification problems
has proven to be highly reliable and competitive <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. Since neural networks are ideally suited for solving classification problems, a transfer of the approach as we presented in this work constitutes a logical step.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">In pose estimation domain, to estimate the dense 2D-3D correspondence, each 3D corresponding point must be assigned a unique descriptor. Pix2PoseÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> simply treats the 3D vertex coordinates as this descriptor. DPODÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite> textures the object with a 2-channel UV-map with discrete values
to learn the correspondences. EPOSÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> divides the object surface into multiple fragments, and estimates the corresponding points by combining fragment segmentation and local fragments coordinates prediction. Although most of these encodings are limited to local object coordinates, we propose a method that learns dense 2D-3D correspondence through a handcrafted code. Compared to methods that predict local coordinates space Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>, <a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite> in 2D or 3D grid, we encode the object surface in a coarse to fine manner. Moreover, unlike EPOSÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> that divides the object surface into multiple coarse bins at once, we divide the object surface iteratively until the fragments are fine enough to define the unique 3D corresponding point. This allows for gradual refinement of the correspondence through the hierarchical levels.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method: ZebraPose</h2>

<figure id="S3.F2" class="ltx_figure"><img src="/html/2203.09418/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="528" height="164" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.3.2" class="ltx_text" style="font-size:90%;">Left: Our hierarchical encoding is defined by grouping surface vertices in several iterations. In each iteration, object vertices are split into equally sized groups. In a binary setting, vertices are classified into two groups, 0 (white) and 1 (black). This process happens offline and the generated mapping between vertex code and the corresponding 3D vertex is stored in a look-up table. Right: Our training framework uses a detector to crop the object ROI and predicts a multi-layer code using a fully convolutional neural network. The predicted code is then matched to the 3D surface vertex and passed to RANSAC and PnP modules for pose estimation. </span></figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we present our approach for the problem of 6DoF object pose, which involves the entire process from our surface encoding to the final pose estimation.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Coarse to Fine Surface Encoding</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.5" class="ltx_p">Given a surface CAD model of an object and its vertices <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="\bm{v_{i}}\in\mathbb{R}^{3}" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><msub id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2.2" xref="S3.SS1.p1.1.m1.1.1.2.2.cmml">ğ’—</mi><mi id="S3.SS1.p1.1.m1.1.1.2.3" xref="S3.SS1.p1.1.m1.1.1.2.3.cmml">ğ’Š</mi></msub><mo id="S3.SS1.p1.1.m1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml"><mi id="S3.SS1.p1.1.m1.1.1.3.2" xref="S3.SS1.p1.1.m1.1.1.3.2.cmml">â„</mi><mn id="S3.SS1.p1.1.m1.1.1.3.3" xref="S3.SS1.p1.1.m1.1.1.3.3.cmml">3</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><in id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1"></in><apply id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.2.1.cmml" xref="S3.SS1.p1.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.2.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2.2">ğ’—</ci><ci id="S3.SS1.p1.1.m1.1.1.2.3.cmml" xref="S3.SS1.p1.1.m1.1.1.2.3">ğ’Š</ci></apply><apply id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.3.1.cmml" xref="S3.SS1.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.3.2.cmml" xref="S3.SS1.p1.1.m1.1.1.3.2">â„</ci><cn type="integer" id="S3.SS1.p1.1.m1.1.1.3.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\bm{v_{i}}\in\mathbb{R}^{3}</annotation></semantics></math> , where <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">i</annotation></semantics></math> stands for the vertex id, we want to represent each <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="\bm{v_{i}}" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><msub id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">ğ’—</mi><mi id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">ğ’Š</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2">ğ’—</ci><ci id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">ğ’Š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">\bm{v_{i}}</annotation></semantics></math> with vertex code <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="c_{i}\in\mathbb{N}^{d}" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><mrow id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml"><msub id="S3.SS1.p1.4.m4.1.1.2" xref="S3.SS1.p1.4.m4.1.1.2.cmml"><mi id="S3.SS1.p1.4.m4.1.1.2.2" xref="S3.SS1.p1.4.m4.1.1.2.2.cmml">c</mi><mi id="S3.SS1.p1.4.m4.1.1.2.3" xref="S3.SS1.p1.4.m4.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS1.p1.4.m4.1.1.1" xref="S3.SS1.p1.4.m4.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS1.p1.4.m4.1.1.3" xref="S3.SS1.p1.4.m4.1.1.3.cmml"><mi id="S3.SS1.p1.4.m4.1.1.3.2" xref="S3.SS1.p1.4.m4.1.1.3.2.cmml">â„•</mi><mi id="S3.SS1.p1.4.m4.1.1.3.3" xref="S3.SS1.p1.4.m4.1.1.3.3.cmml">d</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><apply id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1"><in id="S3.SS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1.1"></in><apply id="S3.SS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.1.1.2.1.cmml" xref="S3.SS1.p1.4.m4.1.1.2">subscript</csymbol><ci id="S3.SS1.p1.4.m4.1.1.2.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2.2">ğ‘</ci><ci id="S3.SS1.p1.4.m4.1.1.2.3.cmml" xref="S3.SS1.p1.4.m4.1.1.2.3">ğ‘–</ci></apply><apply id="S3.SS1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.1.1.3.1.cmml" xref="S3.SS1.p1.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS1.p1.4.m4.1.1.3.2.cmml" xref="S3.SS1.p1.4.m4.1.1.3.2">â„•</ci><ci id="S3.SS1.p1.4.m4.1.1.3.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3.3">ğ‘‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">c_{i}\in\mathbb{N}^{d}</annotation></semantics></math>, where <math id="S3.SS1.p1.5.m5.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S3.SS1.p1.5.m5.1a"><mi id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><ci id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">d</annotation></semantics></math> is the length of the vertex code. We need to define such encoding based on verticesâ€™ position relative to the given 3D object surface to enable coarse to fine learning. To enable this, we construct our codes in a non-decimal numeral system.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.18" class="ltx_p">Defining our encoding in a numeral system with lower radix makes the representation very efficient and provides easier grounds for coarse to fine grouping of the points. For a code of length <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">d</annotation></semantics></math>, we perform <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">d</annotation></semantics></math> iterations of grouping of the vertices. The collection of groups <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="G_{j}" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><msub id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml"><mi id="S3.SS1.p2.3.m3.1.1.2" xref="S3.SS1.p2.3.m3.1.1.2.cmml">G</mi><mi id="S3.SS1.p2.3.m3.1.1.3" xref="S3.SS1.p2.3.m3.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><apply id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p2.3.m3.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.2">ğº</ci><ci id="S3.SS1.p2.3.m3.1.1.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3">ğ‘—</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">G_{j}</annotation></semantics></math> in the <math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><mi id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><ci id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">ğ‘—</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">j</annotation></semantics></math>-th iteration (<math id="S3.SS1.p2.5.m5.3" class="ltx_Math" alttext="j\in\{0,...,d\}\subset\mathbb{N}" display="inline"><semantics id="S3.SS1.p2.5.m5.3a"><mrow id="S3.SS1.p2.5.m5.3.4" xref="S3.SS1.p2.5.m5.3.4.cmml"><mi id="S3.SS1.p2.5.m5.3.4.2" xref="S3.SS1.p2.5.m5.3.4.2.cmml">j</mi><mo id="S3.SS1.p2.5.m5.3.4.3" xref="S3.SS1.p2.5.m5.3.4.3.cmml">âˆˆ</mo><mrow id="S3.SS1.p2.5.m5.3.4.4.2" xref="S3.SS1.p2.5.m5.3.4.4.1.cmml"><mo stretchy="false" id="S3.SS1.p2.5.m5.3.4.4.2.1" xref="S3.SS1.p2.5.m5.3.4.4.1.cmml">{</mo><mn id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml">0</mn><mo id="S3.SS1.p2.5.m5.3.4.4.2.2" xref="S3.SS1.p2.5.m5.3.4.4.1.cmml">,</mo><mi mathvariant="normal" id="S3.SS1.p2.5.m5.2.2" xref="S3.SS1.p2.5.m5.2.2.cmml">â€¦</mi><mo id="S3.SS1.p2.5.m5.3.4.4.2.3" xref="S3.SS1.p2.5.m5.3.4.4.1.cmml">,</mo><mi id="S3.SS1.p2.5.m5.3.3" xref="S3.SS1.p2.5.m5.3.3.cmml">d</mi><mo stretchy="false" id="S3.SS1.p2.5.m5.3.4.4.2.4" xref="S3.SS1.p2.5.m5.3.4.4.1.cmml">}</mo></mrow><mo id="S3.SS1.p2.5.m5.3.4.5" xref="S3.SS1.p2.5.m5.3.4.5.cmml">âŠ‚</mo><mi id="S3.SS1.p2.5.m5.3.4.6" xref="S3.SS1.p2.5.m5.3.4.6.cmml">â„•</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.3b"><apply id="S3.SS1.p2.5.m5.3.4.cmml" xref="S3.SS1.p2.5.m5.3.4"><and id="S3.SS1.p2.5.m5.3.4a.cmml" xref="S3.SS1.p2.5.m5.3.4"></and><apply id="S3.SS1.p2.5.m5.3.4b.cmml" xref="S3.SS1.p2.5.m5.3.4"><in id="S3.SS1.p2.5.m5.3.4.3.cmml" xref="S3.SS1.p2.5.m5.3.4.3"></in><ci id="S3.SS1.p2.5.m5.3.4.2.cmml" xref="S3.SS1.p2.5.m5.3.4.2">ğ‘—</ci><set id="S3.SS1.p2.5.m5.3.4.4.1.cmml" xref="S3.SS1.p2.5.m5.3.4.4.2"><cn type="integer" id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">0</cn><ci id="S3.SS1.p2.5.m5.2.2.cmml" xref="S3.SS1.p2.5.m5.2.2">â€¦</ci><ci id="S3.SS1.p2.5.m5.3.3.cmml" xref="S3.SS1.p2.5.m5.3.3">ğ‘‘</ci></set></apply><apply id="S3.SS1.p2.5.m5.3.4c.cmml" xref="S3.SS1.p2.5.m5.3.4"><subset id="S3.SS1.p2.5.m5.3.4.5.cmml" xref="S3.SS1.p2.5.m5.3.4.5"></subset><share href="#S3.SS1.p2.5.m5.3.4.4.cmml" id="S3.SS1.p2.5.m5.3.4d.cmml" xref="S3.SS1.p2.5.m5.3.4"></share><ci id="S3.SS1.p2.5.m5.3.4.6.cmml" xref="S3.SS1.p2.5.m5.3.4.6">â„•</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.3c">j\in\{0,...,d\}\subset\mathbb{N}</annotation></semantics></math>) consist of <math id="S3.SS1.p2.6.m6.1" class="ltx_Math" alttext="r^{j}" display="inline"><semantics id="S3.SS1.p2.6.m6.1a"><msup id="S3.SS1.p2.6.m6.1.1" xref="S3.SS1.p2.6.m6.1.1.cmml"><mi id="S3.SS1.p2.6.m6.1.1.2" xref="S3.SS1.p2.6.m6.1.1.2.cmml">r</mi><mi id="S3.SS1.p2.6.m6.1.1.3" xref="S3.SS1.p2.6.m6.1.1.3.cmml">j</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m6.1b"><apply id="S3.SS1.p2.6.m6.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.1.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1">superscript</csymbol><ci id="S3.SS1.p2.6.m6.1.1.2.cmml" xref="S3.SS1.p2.6.m6.1.1.2">ğ‘Ÿ</ci><ci id="S3.SS1.p2.6.m6.1.1.3.cmml" xref="S3.SS1.p2.6.m6.1.1.3">ğ‘—</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.1c">r^{j}</annotation></semantics></math> groups. <math id="S3.SS1.p2.7.m7.1" class="ltx_Math" alttext="G_{0}" display="inline"><semantics id="S3.SS1.p2.7.m7.1a"><msub id="S3.SS1.p2.7.m7.1.1" xref="S3.SS1.p2.7.m7.1.1.cmml"><mi id="S3.SS1.p2.7.m7.1.1.2" xref="S3.SS1.p2.7.m7.1.1.2.cmml">G</mi><mn id="S3.SS1.p2.7.m7.1.1.3" xref="S3.SS1.p2.7.m7.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m7.1b"><apply id="S3.SS1.p2.7.m7.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.1.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1">subscript</csymbol><ci id="S3.SS1.p2.7.m7.1.1.2.cmml" xref="S3.SS1.p2.7.m7.1.1.2">ğº</ci><cn type="integer" id="S3.SS1.p2.7.m7.1.1.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m7.1c">G_{0}</annotation></semantics></math> defines the initial group, including only one group, i.e. the entire object vertices. <math id="S3.SS1.p2.8.m8.1" class="ltx_Math" alttext="G_{j}" display="inline"><semantics id="S3.SS1.p2.8.m8.1a"><msub id="S3.SS1.p2.8.m8.1.1" xref="S3.SS1.p2.8.m8.1.1.cmml"><mi id="S3.SS1.p2.8.m8.1.1.2" xref="S3.SS1.p2.8.m8.1.1.2.cmml">G</mi><mi id="S3.SS1.p2.8.m8.1.1.3" xref="S3.SS1.p2.8.m8.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m8.1b"><apply id="S3.SS1.p2.8.m8.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.8.m8.1.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1">subscript</csymbol><ci id="S3.SS1.p2.8.m8.1.1.2.cmml" xref="S3.SS1.p2.8.m8.1.1.2">ğº</ci><ci id="S3.SS1.p2.8.m8.1.1.3.cmml" xref="S3.SS1.p2.8.m8.1.1.3">ğ‘—</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m8.1c">G_{j}</annotation></semantics></math> with <math id="S3.SS1.p2.9.m9.1" class="ltx_Math" alttext="j&gt;1" display="inline"><semantics id="S3.SS1.p2.9.m9.1a"><mrow id="S3.SS1.p2.9.m9.1.1" xref="S3.SS1.p2.9.m9.1.1.cmml"><mi id="S3.SS1.p2.9.m9.1.1.2" xref="S3.SS1.p2.9.m9.1.1.2.cmml">j</mi><mo id="S3.SS1.p2.9.m9.1.1.1" xref="S3.SS1.p2.9.m9.1.1.1.cmml">&gt;</mo><mn id="S3.SS1.p2.9.m9.1.1.3" xref="S3.SS1.p2.9.m9.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.9.m9.1b"><apply id="S3.SS1.p2.9.m9.1.1.cmml" xref="S3.SS1.p2.9.m9.1.1"><gt id="S3.SS1.p2.9.m9.1.1.1.cmml" xref="S3.SS1.p2.9.m9.1.1.1"></gt><ci id="S3.SS1.p2.9.m9.1.1.2.cmml" xref="S3.SS1.p2.9.m9.1.1.2">ğ‘—</ci><cn type="integer" id="S3.SS1.p2.9.m9.1.1.3.cmml" xref="S3.SS1.p2.9.m9.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.9.m9.1c">j&gt;1</annotation></semantics></math> is obtained by splitting each group in <math id="S3.SS1.p2.10.m10.1" class="ltx_Math" alttext="G_{j-1}" display="inline"><semantics id="S3.SS1.p2.10.m10.1a"><msub id="S3.SS1.p2.10.m10.1.1" xref="S3.SS1.p2.10.m10.1.1.cmml"><mi id="S3.SS1.p2.10.m10.1.1.2" xref="S3.SS1.p2.10.m10.1.1.2.cmml">G</mi><mrow id="S3.SS1.p2.10.m10.1.1.3" xref="S3.SS1.p2.10.m10.1.1.3.cmml"><mi id="S3.SS1.p2.10.m10.1.1.3.2" xref="S3.SS1.p2.10.m10.1.1.3.2.cmml">j</mi><mo id="S3.SS1.p2.10.m10.1.1.3.1" xref="S3.SS1.p2.10.m10.1.1.3.1.cmml">âˆ’</mo><mn id="S3.SS1.p2.10.m10.1.1.3.3" xref="S3.SS1.p2.10.m10.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.10.m10.1b"><apply id="S3.SS1.p2.10.m10.1.1.cmml" xref="S3.SS1.p2.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.10.m10.1.1.1.cmml" xref="S3.SS1.p2.10.m10.1.1">subscript</csymbol><ci id="S3.SS1.p2.10.m10.1.1.2.cmml" xref="S3.SS1.p2.10.m10.1.1.2">ğº</ci><apply id="S3.SS1.p2.10.m10.1.1.3.cmml" xref="S3.SS1.p2.10.m10.1.1.3"><minus id="S3.SS1.p2.10.m10.1.1.3.1.cmml" xref="S3.SS1.p2.10.m10.1.1.3.1"></minus><ci id="S3.SS1.p2.10.m10.1.1.3.2.cmml" xref="S3.SS1.p2.10.m10.1.1.3.2">ğ‘—</ci><cn type="integer" id="S3.SS1.p2.10.m10.1.1.3.3.cmml" xref="S3.SS1.p2.10.m10.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.10.m10.1c">G_{j-1}</annotation></semantics></math> into <math id="S3.SS1.p2.11.m11.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S3.SS1.p2.11.m11.1a"><mi id="S3.SS1.p2.11.m11.1.1" xref="S3.SS1.p2.11.m11.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.11.m11.1b"><ci id="S3.SS1.p2.11.m11.1.1.cmml" xref="S3.SS1.p2.11.m11.1.1">ğ‘Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.11.m11.1c">r</annotation></semantics></math> groups. In a grouping iteration, each vertex <math id="S3.SS1.p2.12.m12.1" class="ltx_Math" alttext="\bm{v_{i}}" display="inline"><semantics id="S3.SS1.p2.12.m12.1a"><msub id="S3.SS1.p2.12.m12.1.1" xref="S3.SS1.p2.12.m12.1.1.cmml"><mi id="S3.SS1.p2.12.m12.1.1.2" xref="S3.SS1.p2.12.m12.1.1.2.cmml">ğ’—</mi><mi id="S3.SS1.p2.12.m12.1.1.3" xref="S3.SS1.p2.12.m12.1.1.3.cmml">ğ’Š</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.12.m12.1b"><apply id="S3.SS1.p2.12.m12.1.1.cmml" xref="S3.SS1.p2.12.m12.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.12.m12.1.1.1.cmml" xref="S3.SS1.p2.12.m12.1.1">subscript</csymbol><ci id="S3.SS1.p2.12.m12.1.1.2.cmml" xref="S3.SS1.p2.12.m12.1.1.2">ğ’—</ci><ci id="S3.SS1.p2.12.m12.1.1.3.cmml" xref="S3.SS1.p2.12.m12.1.1.3">ğ’Š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.12.m12.1c">\bm{v_{i}}</annotation></semantics></math> is assigned with a class id <math id="S3.SS1.p2.13.m13.2" class="ltx_Math" alttext="m_{i,j}" display="inline"><semantics id="S3.SS1.p2.13.m13.2a"><msub id="S3.SS1.p2.13.m13.2.3" xref="S3.SS1.p2.13.m13.2.3.cmml"><mi id="S3.SS1.p2.13.m13.2.3.2" xref="S3.SS1.p2.13.m13.2.3.2.cmml">m</mi><mrow id="S3.SS1.p2.13.m13.2.2.2.4" xref="S3.SS1.p2.13.m13.2.2.2.3.cmml"><mi id="S3.SS1.p2.13.m13.1.1.1.1" xref="S3.SS1.p2.13.m13.1.1.1.1.cmml">i</mi><mo id="S3.SS1.p2.13.m13.2.2.2.4.1" xref="S3.SS1.p2.13.m13.2.2.2.3.cmml">,</mo><mi id="S3.SS1.p2.13.m13.2.2.2.2" xref="S3.SS1.p2.13.m13.2.2.2.2.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.13.m13.2b"><apply id="S3.SS1.p2.13.m13.2.3.cmml" xref="S3.SS1.p2.13.m13.2.3"><csymbol cd="ambiguous" id="S3.SS1.p2.13.m13.2.3.1.cmml" xref="S3.SS1.p2.13.m13.2.3">subscript</csymbol><ci id="S3.SS1.p2.13.m13.2.3.2.cmml" xref="S3.SS1.p2.13.m13.2.3.2">ğ‘š</ci><list id="S3.SS1.p2.13.m13.2.2.2.3.cmml" xref="S3.SS1.p2.13.m13.2.2.2.4"><ci id="S3.SS1.p2.13.m13.1.1.1.1.cmml" xref="S3.SS1.p2.13.m13.1.1.1.1">ğ‘–</ci><ci id="S3.SS1.p2.13.m13.2.2.2.2.cmml" xref="S3.SS1.p2.13.m13.2.2.2.2">ğ‘—</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.13.m13.2c">m_{i,j}</annotation></semantics></math>, where <math id="S3.SS1.p2.14.m14.5" class="ltx_Math" alttext="m_{i,j}\in\{0,...,r-1\}\subset\mathbb{N}" display="inline"><semantics id="S3.SS1.p2.14.m14.5a"><mrow id="S3.SS1.p2.14.m14.5.5" xref="S3.SS1.p2.14.m14.5.5.cmml"><msub id="S3.SS1.p2.14.m14.5.5.3" xref="S3.SS1.p2.14.m14.5.5.3.cmml"><mi id="S3.SS1.p2.14.m14.5.5.3.2" xref="S3.SS1.p2.14.m14.5.5.3.2.cmml">m</mi><mrow id="S3.SS1.p2.14.m14.2.2.2.4" xref="S3.SS1.p2.14.m14.2.2.2.3.cmml"><mi id="S3.SS1.p2.14.m14.1.1.1.1" xref="S3.SS1.p2.14.m14.1.1.1.1.cmml">i</mi><mo id="S3.SS1.p2.14.m14.2.2.2.4.1" xref="S3.SS1.p2.14.m14.2.2.2.3.cmml">,</mo><mi id="S3.SS1.p2.14.m14.2.2.2.2" xref="S3.SS1.p2.14.m14.2.2.2.2.cmml">j</mi></mrow></msub><mo id="S3.SS1.p2.14.m14.5.5.4" xref="S3.SS1.p2.14.m14.5.5.4.cmml">âˆˆ</mo><mrow id="S3.SS1.p2.14.m14.5.5.1.1" xref="S3.SS1.p2.14.m14.5.5.1.2.cmml"><mo stretchy="false" id="S3.SS1.p2.14.m14.5.5.1.1.2" xref="S3.SS1.p2.14.m14.5.5.1.2.cmml">{</mo><mn id="S3.SS1.p2.14.m14.3.3" xref="S3.SS1.p2.14.m14.3.3.cmml">0</mn><mo id="S3.SS1.p2.14.m14.5.5.1.1.3" xref="S3.SS1.p2.14.m14.5.5.1.2.cmml">,</mo><mi mathvariant="normal" id="S3.SS1.p2.14.m14.4.4" xref="S3.SS1.p2.14.m14.4.4.cmml">â€¦</mi><mo id="S3.SS1.p2.14.m14.5.5.1.1.4" xref="S3.SS1.p2.14.m14.5.5.1.2.cmml">,</mo><mrow id="S3.SS1.p2.14.m14.5.5.1.1.1" xref="S3.SS1.p2.14.m14.5.5.1.1.1.cmml"><mi id="S3.SS1.p2.14.m14.5.5.1.1.1.2" xref="S3.SS1.p2.14.m14.5.5.1.1.1.2.cmml">r</mi><mo id="S3.SS1.p2.14.m14.5.5.1.1.1.1" xref="S3.SS1.p2.14.m14.5.5.1.1.1.1.cmml">âˆ’</mo><mn id="S3.SS1.p2.14.m14.5.5.1.1.1.3" xref="S3.SS1.p2.14.m14.5.5.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S3.SS1.p2.14.m14.5.5.1.1.5" xref="S3.SS1.p2.14.m14.5.5.1.2.cmml">}</mo></mrow><mo id="S3.SS1.p2.14.m14.5.5.5" xref="S3.SS1.p2.14.m14.5.5.5.cmml">âŠ‚</mo><mi id="S3.SS1.p2.14.m14.5.5.6" xref="S3.SS1.p2.14.m14.5.5.6.cmml">â„•</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.14.m14.5b"><apply id="S3.SS1.p2.14.m14.5.5.cmml" xref="S3.SS1.p2.14.m14.5.5"><and id="S3.SS1.p2.14.m14.5.5a.cmml" xref="S3.SS1.p2.14.m14.5.5"></and><apply id="S3.SS1.p2.14.m14.5.5b.cmml" xref="S3.SS1.p2.14.m14.5.5"><in id="S3.SS1.p2.14.m14.5.5.4.cmml" xref="S3.SS1.p2.14.m14.5.5.4"></in><apply id="S3.SS1.p2.14.m14.5.5.3.cmml" xref="S3.SS1.p2.14.m14.5.5.3"><csymbol cd="ambiguous" id="S3.SS1.p2.14.m14.5.5.3.1.cmml" xref="S3.SS1.p2.14.m14.5.5.3">subscript</csymbol><ci id="S3.SS1.p2.14.m14.5.5.3.2.cmml" xref="S3.SS1.p2.14.m14.5.5.3.2">ğ‘š</ci><list id="S3.SS1.p2.14.m14.2.2.2.3.cmml" xref="S3.SS1.p2.14.m14.2.2.2.4"><ci id="S3.SS1.p2.14.m14.1.1.1.1.cmml" xref="S3.SS1.p2.14.m14.1.1.1.1">ğ‘–</ci><ci id="S3.SS1.p2.14.m14.2.2.2.2.cmml" xref="S3.SS1.p2.14.m14.2.2.2.2">ğ‘—</ci></list></apply><set id="S3.SS1.p2.14.m14.5.5.1.2.cmml" xref="S3.SS1.p2.14.m14.5.5.1.1"><cn type="integer" id="S3.SS1.p2.14.m14.3.3.cmml" xref="S3.SS1.p2.14.m14.3.3">0</cn><ci id="S3.SS1.p2.14.m14.4.4.cmml" xref="S3.SS1.p2.14.m14.4.4">â€¦</ci><apply id="S3.SS1.p2.14.m14.5.5.1.1.1.cmml" xref="S3.SS1.p2.14.m14.5.5.1.1.1"><minus id="S3.SS1.p2.14.m14.5.5.1.1.1.1.cmml" xref="S3.SS1.p2.14.m14.5.5.1.1.1.1"></minus><ci id="S3.SS1.p2.14.m14.5.5.1.1.1.2.cmml" xref="S3.SS1.p2.14.m14.5.5.1.1.1.2">ğ‘Ÿ</ci><cn type="integer" id="S3.SS1.p2.14.m14.5.5.1.1.1.3.cmml" xref="S3.SS1.p2.14.m14.5.5.1.1.1.3">1</cn></apply></set></apply><apply id="S3.SS1.p2.14.m14.5.5c.cmml" xref="S3.SS1.p2.14.m14.5.5"><subset id="S3.SS1.p2.14.m14.5.5.5.cmml" xref="S3.SS1.p2.14.m14.5.5.5"></subset><share href="#S3.SS1.p2.14.m14.5.5.1.cmml" id="S3.SS1.p2.14.m14.5.5d.cmml" xref="S3.SS1.p2.14.m14.5.5"></share><ci id="S3.SS1.p2.14.m14.5.5.6.cmml" xref="S3.SS1.p2.14.m14.5.5.6">â„•</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.14.m14.5c">m_{i,j}\in\{0,...,r-1\}\subset\mathbb{N}</annotation></semantics></math> based on the group it belongs to in the <math id="S3.SS1.p2.15.m15.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.SS1.p2.15.m15.1a"><mi id="S3.SS1.p2.15.m15.1.1" xref="S3.SS1.p2.15.m15.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.15.m15.1b"><ci id="S3.SS1.p2.15.m15.1.1.cmml" xref="S3.SS1.p2.15.m15.1.1">ğ‘—</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.15.m15.1c">j</annotation></semantics></math>-th grouping. Finally, each vertex is assigned to a vertex code with <math id="S3.SS1.p2.16.m16.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S3.SS1.p2.16.m16.1a"><mi id="S3.SS1.p2.16.m16.1.1" xref="S3.SS1.p2.16.m16.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.16.m16.1b"><ci id="S3.SS1.p2.16.m16.1.1.cmml" xref="S3.SS1.p2.16.m16.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.16.m16.1c">d</annotation></semantics></math> digits by stacking the class id of each grouping operation. This representation is stored and fixed for every 3D object. The vertices in each group share the same code. We build the lookup table to map a code to the centroid of the each group in <math id="S3.SS1.p2.17.m17.1" class="ltx_Math" alttext="G_{d}" display="inline"><semantics id="S3.SS1.p2.17.m17.1a"><msub id="S3.SS1.p2.17.m17.1.1" xref="S3.SS1.p2.17.m17.1.1.cmml"><mi id="S3.SS1.p2.17.m17.1.1.2" xref="S3.SS1.p2.17.m17.1.1.2.cmml">G</mi><mi id="S3.SS1.p2.17.m17.1.1.3" xref="S3.SS1.p2.17.m17.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.17.m17.1b"><apply id="S3.SS1.p2.17.m17.1.1.cmml" xref="S3.SS1.p2.17.m17.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.17.m17.1.1.1.cmml" xref="S3.SS1.p2.17.m17.1.1">subscript</csymbol><ci id="S3.SS1.p2.17.m17.1.1.2.cmml" xref="S3.SS1.p2.17.m17.1.1.2">ğº</ci><ci id="S3.SS1.p2.17.m17.1.1.3.cmml" xref="S3.SS1.p2.17.m17.1.1.3">ğ‘‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.17.m17.1c">G_{d}</annotation></semantics></math>, which is further used to build 2D-3D correspondence and solve the pose as described in Sec.Â <a href="#S3.SS6" title="3.6 Pose estimation â€£ 3 Method: ZebraPose â€£ ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.6</span></a>. In this paper, we used k-means for the grouping, more details are in Sec.Â <a href="#S4.SS1" title="4.1 Experiments Setup â€£ 4 Experiments â€£ ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>. We illustrate this process in Fig.Â <a href="#S3.F2" title="Figure 2 â€£ 3 Method: ZebraPose â€£ ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> with <math id="S3.SS1.p2.18.m18.1" class="ltx_Math" alttext="r=2" display="inline"><semantics id="S3.SS1.p2.18.m18.1a"><mrow id="S3.SS1.p2.18.m18.1.1" xref="S3.SS1.p2.18.m18.1.1.cmml"><mi id="S3.SS1.p2.18.m18.1.1.2" xref="S3.SS1.p2.18.m18.1.1.2.cmml">r</mi><mo id="S3.SS1.p2.18.m18.1.1.1" xref="S3.SS1.p2.18.m18.1.1.1.cmml">=</mo><mn id="S3.SS1.p2.18.m18.1.1.3" xref="S3.SS1.p2.18.m18.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.18.m18.1b"><apply id="S3.SS1.p2.18.m18.1.1.cmml" xref="S3.SS1.p2.18.m18.1.1"><eq id="S3.SS1.p2.18.m18.1.1.1.cmml" xref="S3.SS1.p2.18.m18.1.1.1"></eq><ci id="S3.SS1.p2.18.m18.1.1.2.cmml" xref="S3.SS1.p2.18.m18.1.1.2">ğ‘Ÿ</ci><cn type="integer" id="S3.SS1.p2.18.m18.1.1.3.cmml" xref="S3.SS1.p2.18.m18.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.18.m18.1c">r=2</annotation></semantics></math> and break down the CAD model surface into discrete and equally sized groups.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Choice of the Radix for Vertex Code</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.4" class="ltx_p">Following our grouping described in Sec.Â <a href="#S3.SS1" title="3.1 Coarse to Fine Surface Encoding â€£ 3 Method: ZebraPose â€£ ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>, we would have <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">K</annotation></semantics></math> total number of classes, where <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="K=r^{d}" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mrow id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">K</mi><mo id="S3.SS2.p1.2.m2.1.1.1" xref="S3.SS2.p1.2.m2.1.1.1.cmml">=</mo><msup id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml"><mi id="S3.SS2.p1.2.m2.1.1.3.2" xref="S3.SS2.p1.2.m2.1.1.3.2.cmml">r</mi><mi id="S3.SS2.p1.2.m2.1.1.3.3" xref="S3.SS2.p1.2.m2.1.1.3.3.cmml">d</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><eq id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1"></eq><ci id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">ğ¾</ci><apply id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.3.1.cmml" xref="S3.SS2.p1.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.3.2.cmml" xref="S3.SS2.p1.2.m2.1.1.3.2">ğ‘Ÿ</ci><ci id="S3.SS2.p1.2.m2.1.1.3.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3">ğ‘‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">K=r^{d}</annotation></semantics></math>. In a classification problem we learn these maps using <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="o" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><mi id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">o</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><ci id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">ğ‘œ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">o</annotation></semantics></math> logits, where <math id="S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="o=r\cdot d" display="inline"><semantics id="S3.SS2.p1.4.m4.1a"><mrow id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml"><mi id="S3.SS2.p1.4.m4.1.1.2" xref="S3.SS2.p1.4.m4.1.1.2.cmml">o</mi><mo id="S3.SS2.p1.4.m4.1.1.1" xref="S3.SS2.p1.4.m4.1.1.1.cmml">=</mo><mrow id="S3.SS2.p1.4.m4.1.1.3" xref="S3.SS2.p1.4.m4.1.1.3.cmml"><mi id="S3.SS2.p1.4.m4.1.1.3.2" xref="S3.SS2.p1.4.m4.1.1.3.2.cmml">r</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p1.4.m4.1.1.3.1" xref="S3.SS2.p1.4.m4.1.1.3.1.cmml">â‹…</mo><mi id="S3.SS2.p1.4.m4.1.1.3.3" xref="S3.SS2.p1.4.m4.1.1.3.3.cmml">d</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><apply id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1"><eq id="S3.SS2.p1.4.m4.1.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1.1"></eq><ci id="S3.SS2.p1.4.m4.1.1.2.cmml" xref="S3.SS2.p1.4.m4.1.1.2">ğ‘œ</ci><apply id="S3.SS2.p1.4.m4.1.1.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3"><ci id="S3.SS2.p1.4.m4.1.1.3.1.cmml" xref="S3.SS2.p1.4.m4.1.1.3.1">â‹…</ci><ci id="S3.SS2.p1.4.m4.1.1.3.2.cmml" xref="S3.SS2.p1.4.m4.1.1.3.2">ğ‘Ÿ</ci><ci id="S3.SS2.p1.4.m4.1.1.3.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3.3">ğ‘‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">o=r\cdot d</annotation></semantics></math>. To minimize the number of outputs while learning the most number of classes we have:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="o_{min}=\min_{r}r\cdot d=\min_{r}r\cdot log_{r}K=2\cdot log_{2}K." display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><msub id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.2.2.cmml">o</mi><mrow id="S3.E1.m1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.2.3.cmml"><mi id="S3.E1.m1.1.1.1.1.2.3.2" xref="S3.E1.m1.1.1.1.1.2.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.2.3.1" xref="S3.E1.m1.1.1.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.1.1.1.1.2.3.3" xref="S3.E1.m1.1.1.1.1.2.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.2.3.1a" xref="S3.E1.m1.1.1.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.1.1.1.1.2.3.4" xref="S3.E1.m1.1.1.1.1.2.3.4.cmml">n</mi></mrow></msub><mo id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml">=</mo><mrow id="S3.E1.m1.1.1.1.1.4" xref="S3.E1.m1.1.1.1.1.4.cmml"><munder id="S3.E1.m1.1.1.1.1.4.1" xref="S3.E1.m1.1.1.1.1.4.1.cmml"><mi id="S3.E1.m1.1.1.1.1.4.1.2" xref="S3.E1.m1.1.1.1.1.4.1.2.cmml">min</mi><mi id="S3.E1.m1.1.1.1.1.4.1.3" xref="S3.E1.m1.1.1.1.1.4.1.3.cmml">r</mi></munder><mo lspace="0.167em" id="S3.E1.m1.1.1.1.1.4a" xref="S3.E1.m1.1.1.1.1.4.cmml">â¡</mo><mrow id="S3.E1.m1.1.1.1.1.4.2" xref="S3.E1.m1.1.1.1.1.4.2.cmml"><mi id="S3.E1.m1.1.1.1.1.4.2.2" xref="S3.E1.m1.1.1.1.1.4.2.2.cmml">r</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m1.1.1.1.1.4.2.1" xref="S3.E1.m1.1.1.1.1.4.2.1.cmml">â‹…</mo><mi id="S3.E1.m1.1.1.1.1.4.2.3" xref="S3.E1.m1.1.1.1.1.4.2.3.cmml">d</mi></mrow></mrow><mo id="S3.E1.m1.1.1.1.1.5" xref="S3.E1.m1.1.1.1.1.5.cmml">=</mo><mrow id="S3.E1.m1.1.1.1.1.6" xref="S3.E1.m1.1.1.1.1.6.cmml"><munder id="S3.E1.m1.1.1.1.1.6.1" xref="S3.E1.m1.1.1.1.1.6.1.cmml"><mi id="S3.E1.m1.1.1.1.1.6.1.2" xref="S3.E1.m1.1.1.1.1.6.1.2.cmml">min</mi><mi id="S3.E1.m1.1.1.1.1.6.1.3" xref="S3.E1.m1.1.1.1.1.6.1.3.cmml">r</mi></munder><mo lspace="0.167em" id="S3.E1.m1.1.1.1.1.6a" xref="S3.E1.m1.1.1.1.1.6.cmml">â¡</mo><mrow id="S3.E1.m1.1.1.1.1.6.2" xref="S3.E1.m1.1.1.1.1.6.2.cmml"><mrow id="S3.E1.m1.1.1.1.1.6.2.2" xref="S3.E1.m1.1.1.1.1.6.2.2.cmml"><mi id="S3.E1.m1.1.1.1.1.6.2.2.2" xref="S3.E1.m1.1.1.1.1.6.2.2.2.cmml">r</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m1.1.1.1.1.6.2.2.1" xref="S3.E1.m1.1.1.1.1.6.2.2.1.cmml">â‹…</mo><mi id="S3.E1.m1.1.1.1.1.6.2.2.3" xref="S3.E1.m1.1.1.1.1.6.2.2.3.cmml">l</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.6.2.1" xref="S3.E1.m1.1.1.1.1.6.2.1.cmml">â€‹</mo><mi id="S3.E1.m1.1.1.1.1.6.2.3" xref="S3.E1.m1.1.1.1.1.6.2.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.6.2.1a" xref="S3.E1.m1.1.1.1.1.6.2.1.cmml">â€‹</mo><msub id="S3.E1.m1.1.1.1.1.6.2.4" xref="S3.E1.m1.1.1.1.1.6.2.4.cmml"><mi id="S3.E1.m1.1.1.1.1.6.2.4.2" xref="S3.E1.m1.1.1.1.1.6.2.4.2.cmml">g</mi><mi id="S3.E1.m1.1.1.1.1.6.2.4.3" xref="S3.E1.m1.1.1.1.1.6.2.4.3.cmml">r</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.6.2.1b" xref="S3.E1.m1.1.1.1.1.6.2.1.cmml">â€‹</mo><mi id="S3.E1.m1.1.1.1.1.6.2.5" xref="S3.E1.m1.1.1.1.1.6.2.5.cmml">K</mi></mrow></mrow><mo id="S3.E1.m1.1.1.1.1.7" xref="S3.E1.m1.1.1.1.1.7.cmml">=</mo><mrow id="S3.E1.m1.1.1.1.1.8" xref="S3.E1.m1.1.1.1.1.8.cmml"><mrow id="S3.E1.m1.1.1.1.1.8.2" xref="S3.E1.m1.1.1.1.1.8.2.cmml"><mn id="S3.E1.m1.1.1.1.1.8.2.2" xref="S3.E1.m1.1.1.1.1.8.2.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m1.1.1.1.1.8.2.1" xref="S3.E1.m1.1.1.1.1.8.2.1.cmml">â‹…</mo><mi id="S3.E1.m1.1.1.1.1.8.2.3" xref="S3.E1.m1.1.1.1.1.8.2.3.cmml">l</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.8.1" xref="S3.E1.m1.1.1.1.1.8.1.cmml">â€‹</mo><mi id="S3.E1.m1.1.1.1.1.8.3" xref="S3.E1.m1.1.1.1.1.8.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.8.1a" xref="S3.E1.m1.1.1.1.1.8.1.cmml">â€‹</mo><msub id="S3.E1.m1.1.1.1.1.8.4" xref="S3.E1.m1.1.1.1.1.8.4.cmml"><mi id="S3.E1.m1.1.1.1.1.8.4.2" xref="S3.E1.m1.1.1.1.1.8.4.2.cmml">g</mi><mn id="S3.E1.m1.1.1.1.1.8.4.3" xref="S3.E1.m1.1.1.1.1.8.4.3.cmml">2</mn></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.8.1b" xref="S3.E1.m1.1.1.1.1.8.1.cmml">â€‹</mo><mi id="S3.E1.m1.1.1.1.1.8.5" xref="S3.E1.m1.1.1.1.1.8.5.cmml">K</mi></mrow></mrow><mo lspace="0em" id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><and id="S3.E1.m1.1.1.1.1a.cmml" xref="S3.E1.m1.1.1.1"></and><apply id="S3.E1.m1.1.1.1.1b.cmml" xref="S3.E1.m1.1.1.1"><eq id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3"></eq><apply id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.2.2">ğ‘œ</ci><apply id="S3.E1.m1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.2.3"><times id="S3.E1.m1.1.1.1.1.2.3.1.cmml" xref="S3.E1.m1.1.1.1.1.2.3.1"></times><ci id="S3.E1.m1.1.1.1.1.2.3.2.cmml" xref="S3.E1.m1.1.1.1.1.2.3.2">ğ‘š</ci><ci id="S3.E1.m1.1.1.1.1.2.3.3.cmml" xref="S3.E1.m1.1.1.1.1.2.3.3">ğ‘–</ci><ci id="S3.E1.m1.1.1.1.1.2.3.4.cmml" xref="S3.E1.m1.1.1.1.1.2.3.4">ğ‘›</ci></apply></apply><apply id="S3.E1.m1.1.1.1.1.4.cmml" xref="S3.E1.m1.1.1.1.1.4"><apply id="S3.E1.m1.1.1.1.1.4.1.cmml" xref="S3.E1.m1.1.1.1.1.4.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.4.1.1.cmml" xref="S3.E1.m1.1.1.1.1.4.1">subscript</csymbol><min id="S3.E1.m1.1.1.1.1.4.1.2.cmml" xref="S3.E1.m1.1.1.1.1.4.1.2"></min><ci id="S3.E1.m1.1.1.1.1.4.1.3.cmml" xref="S3.E1.m1.1.1.1.1.4.1.3">ğ‘Ÿ</ci></apply><apply id="S3.E1.m1.1.1.1.1.4.2.cmml" xref="S3.E1.m1.1.1.1.1.4.2"><ci id="S3.E1.m1.1.1.1.1.4.2.1.cmml" xref="S3.E1.m1.1.1.1.1.4.2.1">â‹…</ci><ci id="S3.E1.m1.1.1.1.1.4.2.2.cmml" xref="S3.E1.m1.1.1.1.1.4.2.2">ğ‘Ÿ</ci><ci id="S3.E1.m1.1.1.1.1.4.2.3.cmml" xref="S3.E1.m1.1.1.1.1.4.2.3">ğ‘‘</ci></apply></apply></apply><apply id="S3.E1.m1.1.1.1.1c.cmml" xref="S3.E1.m1.1.1.1"><eq id="S3.E1.m1.1.1.1.1.5.cmml" xref="S3.E1.m1.1.1.1.1.5"></eq><share href="#S3.E1.m1.1.1.1.1.4.cmml" id="S3.E1.m1.1.1.1.1d.cmml" xref="S3.E1.m1.1.1.1"></share><apply id="S3.E1.m1.1.1.1.1.6.cmml" xref="S3.E1.m1.1.1.1.1.6"><apply id="S3.E1.m1.1.1.1.1.6.1.cmml" xref="S3.E1.m1.1.1.1.1.6.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.6.1.1.cmml" xref="S3.E1.m1.1.1.1.1.6.1">subscript</csymbol><min id="S3.E1.m1.1.1.1.1.6.1.2.cmml" xref="S3.E1.m1.1.1.1.1.6.1.2"></min><ci id="S3.E1.m1.1.1.1.1.6.1.3.cmml" xref="S3.E1.m1.1.1.1.1.6.1.3">ğ‘Ÿ</ci></apply><apply id="S3.E1.m1.1.1.1.1.6.2.cmml" xref="S3.E1.m1.1.1.1.1.6.2"><times id="S3.E1.m1.1.1.1.1.6.2.1.cmml" xref="S3.E1.m1.1.1.1.1.6.2.1"></times><apply id="S3.E1.m1.1.1.1.1.6.2.2.cmml" xref="S3.E1.m1.1.1.1.1.6.2.2"><ci id="S3.E1.m1.1.1.1.1.6.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.6.2.2.1">â‹…</ci><ci id="S3.E1.m1.1.1.1.1.6.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.6.2.2.2">ğ‘Ÿ</ci><ci id="S3.E1.m1.1.1.1.1.6.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.6.2.2.3">ğ‘™</ci></apply><ci id="S3.E1.m1.1.1.1.1.6.2.3.cmml" xref="S3.E1.m1.1.1.1.1.6.2.3">ğ‘œ</ci><apply id="S3.E1.m1.1.1.1.1.6.2.4.cmml" xref="S3.E1.m1.1.1.1.1.6.2.4"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.6.2.4.1.cmml" xref="S3.E1.m1.1.1.1.1.6.2.4">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.6.2.4.2.cmml" xref="S3.E1.m1.1.1.1.1.6.2.4.2">ğ‘”</ci><ci id="S3.E1.m1.1.1.1.1.6.2.4.3.cmml" xref="S3.E1.m1.1.1.1.1.6.2.4.3">ğ‘Ÿ</ci></apply><ci id="S3.E1.m1.1.1.1.1.6.2.5.cmml" xref="S3.E1.m1.1.1.1.1.6.2.5">ğ¾</ci></apply></apply></apply><apply id="S3.E1.m1.1.1.1.1e.cmml" xref="S3.E1.m1.1.1.1"><eq id="S3.E1.m1.1.1.1.1.7.cmml" xref="S3.E1.m1.1.1.1.1.7"></eq><share href="#S3.E1.m1.1.1.1.1.6.cmml" id="S3.E1.m1.1.1.1.1f.cmml" xref="S3.E1.m1.1.1.1"></share><apply id="S3.E1.m1.1.1.1.1.8.cmml" xref="S3.E1.m1.1.1.1.1.8"><times id="S3.E1.m1.1.1.1.1.8.1.cmml" xref="S3.E1.m1.1.1.1.1.8.1"></times><apply id="S3.E1.m1.1.1.1.1.8.2.cmml" xref="S3.E1.m1.1.1.1.1.8.2"><ci id="S3.E1.m1.1.1.1.1.8.2.1.cmml" xref="S3.E1.m1.1.1.1.1.8.2.1">â‹…</ci><cn type="integer" id="S3.E1.m1.1.1.1.1.8.2.2.cmml" xref="S3.E1.m1.1.1.1.1.8.2.2">2</cn><ci id="S3.E1.m1.1.1.1.1.8.2.3.cmml" xref="S3.E1.m1.1.1.1.1.8.2.3">ğ‘™</ci></apply><ci id="S3.E1.m1.1.1.1.1.8.3.cmml" xref="S3.E1.m1.1.1.1.1.8.3">ğ‘œ</ci><apply id="S3.E1.m1.1.1.1.1.8.4.cmml" xref="S3.E1.m1.1.1.1.1.8.4"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.8.4.1.cmml" xref="S3.E1.m1.1.1.1.1.8.4">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.8.4.2.cmml" xref="S3.E1.m1.1.1.1.1.8.4.2">ğ‘”</ci><cn type="integer" id="S3.E1.m1.1.1.1.1.8.4.3.cmml" xref="S3.E1.m1.1.1.1.1.8.4.3">2</cn></apply><ci id="S3.E1.m1.1.1.1.1.8.5.cmml" xref="S3.E1.m1.1.1.1.1.8.5">ğ¾</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">o_{min}=\min_{r}r\cdot d=\min_{r}r\cdot log_{r}K=2\cdot log_{2}K.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p1.7" class="ltx_p">The best positive integer choice of <math id="S3.SS2.p1.5.m1.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S3.SS2.p1.5.m1.1a"><mi id="S3.SS2.p1.5.m1.1.1" xref="S3.SS2.p1.5.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m1.1b"><ci id="S3.SS2.p1.5.m1.1.1.cmml" xref="S3.SS2.p1.5.m1.1.1">ğ‘Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m1.1c">r</annotation></semantics></math> to minimize the number of network layers are 2 and 4. Since a value is classified either as positive or negative, we do not need to use the cross entropy loss with 2 explicit output layers for the binary classification. So we can reach <math id="S3.SS2.p1.6.m2.1" class="ltx_Math" alttext="log_{2}K" display="inline"><semantics id="S3.SS2.p1.6.m2.1a"><mrow id="S3.SS2.p1.6.m2.1.1" xref="S3.SS2.p1.6.m2.1.1.cmml"><mi id="S3.SS2.p1.6.m2.1.1.2" xref="S3.SS2.p1.6.m2.1.1.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.6.m2.1.1.1" xref="S3.SS2.p1.6.m2.1.1.1.cmml">â€‹</mo><mi id="S3.SS2.p1.6.m2.1.1.3" xref="S3.SS2.p1.6.m2.1.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.6.m2.1.1.1a" xref="S3.SS2.p1.6.m2.1.1.1.cmml">â€‹</mo><msub id="S3.SS2.p1.6.m2.1.1.4" xref="S3.SS2.p1.6.m2.1.1.4.cmml"><mi id="S3.SS2.p1.6.m2.1.1.4.2" xref="S3.SS2.p1.6.m2.1.1.4.2.cmml">g</mi><mn id="S3.SS2.p1.6.m2.1.1.4.3" xref="S3.SS2.p1.6.m2.1.1.4.3.cmml">2</mn></msub><mo lspace="0em" rspace="0em" id="S3.SS2.p1.6.m2.1.1.1b" xref="S3.SS2.p1.6.m2.1.1.1.cmml">â€‹</mo><mi id="S3.SS2.p1.6.m2.1.1.5" xref="S3.SS2.p1.6.m2.1.1.5.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m2.1b"><apply id="S3.SS2.p1.6.m2.1.1.cmml" xref="S3.SS2.p1.6.m2.1.1"><times id="S3.SS2.p1.6.m2.1.1.1.cmml" xref="S3.SS2.p1.6.m2.1.1.1"></times><ci id="S3.SS2.p1.6.m2.1.1.2.cmml" xref="S3.SS2.p1.6.m2.1.1.2">ğ‘™</ci><ci id="S3.SS2.p1.6.m2.1.1.3.cmml" xref="S3.SS2.p1.6.m2.1.1.3">ğ‘œ</ci><apply id="S3.SS2.p1.6.m2.1.1.4.cmml" xref="S3.SS2.p1.6.m2.1.1.4"><csymbol cd="ambiguous" id="S3.SS2.p1.6.m2.1.1.4.1.cmml" xref="S3.SS2.p1.6.m2.1.1.4">subscript</csymbol><ci id="S3.SS2.p1.6.m2.1.1.4.2.cmml" xref="S3.SS2.p1.6.m2.1.1.4.2">ğ‘”</ci><cn type="integer" id="S3.SS2.p1.6.m2.1.1.4.3.cmml" xref="S3.SS2.p1.6.m2.1.1.4.3">2</cn></apply><ci id="S3.SS2.p1.6.m2.1.1.5.cmml" xref="S3.SS2.p1.6.m2.1.1.5">ğ¾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m2.1c">log_{2}K</annotation></semantics></math> as the optimal number of output layers with <math id="S3.SS2.p1.7.m3.1" class="ltx_Math" alttext="r=2" display="inline"><semantics id="S3.SS2.p1.7.m3.1a"><mrow id="S3.SS2.p1.7.m3.1.1" xref="S3.SS2.p1.7.m3.1.1.cmml"><mi id="S3.SS2.p1.7.m3.1.1.2" xref="S3.SS2.p1.7.m3.1.1.2.cmml">r</mi><mo id="S3.SS2.p1.7.m3.1.1.1" xref="S3.SS2.p1.7.m3.1.1.1.cmml">=</mo><mn id="S3.SS2.p1.7.m3.1.1.3" xref="S3.SS2.p1.7.m3.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.7.m3.1b"><apply id="S3.SS2.p1.7.m3.1.1.cmml" xref="S3.SS2.p1.7.m3.1.1"><eq id="S3.SS2.p1.7.m3.1.1.1.cmml" xref="S3.SS2.p1.7.m3.1.1.1"></eq><ci id="S3.SS2.p1.7.m3.1.1.2.cmml" xref="S3.SS2.p1.7.m3.1.1.2">ğ‘Ÿ</ci><cn type="integer" id="S3.SS2.p1.7.m3.1.1.3.cmml" xref="S3.SS2.p1.7.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.7.m3.1c">r=2</annotation></semantics></math>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Besides the advantages of reduced GPU memory requirement, we show later in the ablation study (see Sec.Â <a href="#S4.SS2" title="4.2 Ablation Study on LM-O â€£ 4 Experiments â€£ ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>) that using the binary vertex code yields the most accurately predicted pose. Thus we choose a binary base for the vertex code.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Rendering the Training Labels</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Each object pixel in the image corresponds to a 3D object vertex.
The network predicts the class id that is assigned to this vertex in each grouping operation. Therefore, we still need to render the class id into the 2D image plane with a given pose for the training. For this purpose, we transfer the class id of vertices into the class id of the mesh faces using the following criteria: if two vertices of a face have the same class id, the face is assigned with this class id. Otherwise, the face has the class id of its first vertex. We repeat this rendering process for <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">d</annotation></semantics></math> times until the training label class id for each grouping is generated.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Network Architecture</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.3" class="ltx_p">In Sec.Â <a href="#S3.SS2" title="3.2 Choice of the Radix for Vertex Code â€£ 3 Method: ZebraPose â€£ ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a> we justify our choice of <math id="S3.SS4.p1.1.m1.1" class="ltx_Math" alttext="r=2" display="inline"><semantics id="S3.SS4.p1.1.m1.1a"><mrow id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml"><mi id="S3.SS4.p1.1.m1.1.1.2" xref="S3.SS4.p1.1.m1.1.1.2.cmml">r</mi><mo id="S3.SS4.p1.1.m1.1.1.1" xref="S3.SS4.p1.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS4.p1.1.m1.1.1.3" xref="S3.SS4.p1.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><apply id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1"><eq id="S3.SS4.p1.1.m1.1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1.1"></eq><ci id="S3.SS4.p1.1.m1.1.1.2.cmml" xref="S3.SS4.p1.1.m1.1.1.2">ğ‘Ÿ</ci><cn type="integer" id="S3.SS4.p1.1.m1.1.1.3.cmml" xref="S3.SS4.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">r=2</annotation></semantics></math>. In this regard, our goal is to classify <math id="S3.SS4.p1.2.m2.1" class="ltx_Math" alttext="2^{d}" display="inline"><semantics id="S3.SS4.p1.2.m2.1a"><msup id="S3.SS4.p1.2.m2.1.1" xref="S3.SS4.p1.2.m2.1.1.cmml"><mn id="S3.SS4.p1.2.m2.1.1.2" xref="S3.SS4.p1.2.m2.1.1.2.cmml">2</mn><mi id="S3.SS4.p1.2.m2.1.1.3" xref="S3.SS4.p1.2.m2.1.1.3.cmml">d</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.2.m2.1b"><apply id="S3.SS4.p1.2.m2.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.2.m2.1.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1">superscript</csymbol><cn type="integer" id="S3.SS4.p1.2.m2.1.1.2.cmml" xref="S3.SS4.p1.2.m2.1.1.2">2</cn><ci id="S3.SS4.p1.2.m2.1.1.3.cmml" xref="S3.SS4.p1.2.m2.1.1.3">ğ‘‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.2.m2.1c">2^{d}</annotation></semantics></math> regions with only <math id="S3.SS4.p1.3.m3.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S3.SS4.p1.3.m3.1a"><mi id="S3.SS4.p1.3.m3.1.1" xref="S3.SS4.p1.3.m3.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.3.m3.1b"><ci id="S3.SS4.p1.3.m3.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.3.m3.1c">d</annotation></semantics></math> binary values.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.3" class="ltx_p">During training, we use the object pose annotations to render the labels as layered black and white maps to image coordinates. This way, our objective learning maps are <math id="S3.SS4.p2.1.m1.1" class="ltx_Math" alttext="d+1" display="inline"><semantics id="S3.SS4.p2.1.m1.1a"><mrow id="S3.SS4.p2.1.m1.1.1" xref="S3.SS4.p2.1.m1.1.1.cmml"><mi id="S3.SS4.p2.1.m1.1.1.2" xref="S3.SS4.p2.1.m1.1.1.2.cmml">d</mi><mo id="S3.SS4.p2.1.m1.1.1.1" xref="S3.SS4.p2.1.m1.1.1.1.cmml">+</mo><mn id="S3.SS4.p2.1.m1.1.1.3" xref="S3.SS4.p2.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.1.m1.1b"><apply id="S3.SS4.p2.1.m1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1"><plus id="S3.SS4.p2.1.m1.1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1.1"></plus><ci id="S3.SS4.p2.1.m1.1.1.2.cmml" xref="S3.SS4.p2.1.m1.1.1.2">ğ‘‘</ci><cn type="integer" id="S3.SS4.p2.1.m1.1.1.3.cmml" xref="S3.SS4.p2.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.1.m1.1c">d+1</annotation></semantics></math> binary labels (<math id="S3.SS4.p2.2.m2.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S3.SS4.p2.2.m2.1a"><mi id="S3.SS4.p2.2.m2.1.1" xref="S3.SS4.p2.2.m2.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.2.m2.1b"><ci id="S3.SS4.p2.2.m2.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.2.m2.1c">d</annotation></semantics></math> for the binary vertex code and 1 for the object mask) for code and visible mask prediction. An encoder-decoder network generates <math id="S3.SS4.p2.3.m3.1" class="ltx_Math" alttext="d+1" display="inline"><semantics id="S3.SS4.p2.3.m3.1a"><mrow id="S3.SS4.p2.3.m3.1.1" xref="S3.SS4.p2.3.m3.1.1.cmml"><mi id="S3.SS4.p2.3.m3.1.1.2" xref="S3.SS4.p2.3.m3.1.1.2.cmml">d</mi><mo id="S3.SS4.p2.3.m3.1.1.1" xref="S3.SS4.p2.3.m3.1.1.1.cmml">+</mo><mn id="S3.SS4.p2.3.m3.1.1.3" xref="S3.SS4.p2.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.3.m3.1b"><apply id="S3.SS4.p2.3.m3.1.1.cmml" xref="S3.SS4.p2.3.m3.1.1"><plus id="S3.SS4.p2.3.m3.1.1.1.cmml" xref="S3.SS4.p2.3.m3.1.1.1"></plus><ci id="S3.SS4.p2.3.m3.1.1.2.cmml" xref="S3.SS4.p2.3.m3.1.1.2">ğ‘‘</ci><cn type="integer" id="S3.SS4.p2.3.m3.1.1.3.cmml" xref="S3.SS4.p2.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.3.m3.1c">d+1</annotation></semantics></math> outputs with a single decoder. We round the final output probabilities to represent our discrete vertex codes.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p">The entire process from input images to the predicted pose is presented in Fig.Â <a href="#S3.F2" title="Figure 2 â€£ 3 Method: ZebraPose â€£ ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. To predict the code per pixel in the frame with fine granularity, we process only a Region of Interest (ROI) around object pixels. Following the pipeline similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, we focus on the object pose and use the available 2D detector predictions to find the ROIs. We crop and resize the ROI from the prediction to a fixed dimension H<math id="S3.SS4.p3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS4.p3.1.m1.1a"><mo id="S3.SS4.p3.1.m1.1.1" xref="S3.SS4.p3.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.1.m1.1b"><times id="S3.SS4.p3.1.m1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.1.m1.1c">\times</annotation></semantics></math>W, and apply the exact process to the target vertex code maps during training. Our goal is to predict multiple labels per frame in the ROI.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Hierarchical Learning</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">Predicting correspondences directly from object pixels is a fine-grained task. On the other side, deep neural networks are commonly used for coarse level predictions. This means features predicted per pixel are very similar in a small vicinity. As our encoding is also hierarchical by design, we learn the codes in a coarse to fine manner. Therefore, the predictions are learned in different stages, from coarse groupings to fine ones. We use an error histogram for each position on hierarchical level and weight our Hamming-based loss given the error to design this.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.1" class="ltx_p"><span id="S3.SS5.p2.1.1" class="ltx_text ltx_font_bold">Mask loss.</span> Firstly, we predict the visible mask to segment the object area from the background. Here, we simply pass the predicted probability to the sigmoid function and use L1 loss as <math id="S3.SS5.p2.1.m1.1" class="ltx_Math" alttext="L_{mask}" display="inline"><semantics id="S3.SS5.p2.1.m1.1a"><msub id="S3.SS5.p2.1.m1.1.1" xref="S3.SS5.p2.1.m1.1.1.cmml"><mi id="S3.SS5.p2.1.m1.1.1.2" xref="S3.SS5.p2.1.m1.1.1.2.cmml">L</mi><mrow id="S3.SS5.p2.1.m1.1.1.3" xref="S3.SS5.p2.1.m1.1.1.3.cmml"><mi id="S3.SS5.p2.1.m1.1.1.3.2" xref="S3.SS5.p2.1.m1.1.1.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.1.m1.1.1.3.1" xref="S3.SS5.p2.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS5.p2.1.m1.1.1.3.3" xref="S3.SS5.p2.1.m1.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.1.m1.1.1.3.1a" xref="S3.SS5.p2.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS5.p2.1.m1.1.1.3.4" xref="S3.SS5.p2.1.m1.1.1.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.1.m1.1.1.3.1b" xref="S3.SS5.p2.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS5.p2.1.m1.1.1.3.5" xref="S3.SS5.p2.1.m1.1.1.3.5.cmml">k</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.1.m1.1b"><apply id="S3.SS5.p2.1.m1.1.1.cmml" xref="S3.SS5.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS5.p2.1.m1.1.1.1.cmml" xref="S3.SS5.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS5.p2.1.m1.1.1.2.cmml" xref="S3.SS5.p2.1.m1.1.1.2">ğ¿</ci><apply id="S3.SS5.p2.1.m1.1.1.3.cmml" xref="S3.SS5.p2.1.m1.1.1.3"><times id="S3.SS5.p2.1.m1.1.1.3.1.cmml" xref="S3.SS5.p2.1.m1.1.1.3.1"></times><ci id="S3.SS5.p2.1.m1.1.1.3.2.cmml" xref="S3.SS5.p2.1.m1.1.1.3.2">ğ‘š</ci><ci id="S3.SS5.p2.1.m1.1.1.3.3.cmml" xref="S3.SS5.p2.1.m1.1.1.3.3">ğ‘</ci><ci id="S3.SS5.p2.1.m1.1.1.3.4.cmml" xref="S3.SS5.p2.1.m1.1.1.3.4">ğ‘ </ci><ci id="S3.SS5.p2.1.m1.1.1.3.5.cmml" xref="S3.SS5.p2.1.m1.1.1.3.5">ğ‘˜</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.1.m1.1c">L_{mask}</annotation></semantics></math>. It is worth noting, for the binary vertex code prediction in the following, we only calculate loss of the pixels within the predicted object mask.</p>
</div>
<div id="S3.SS5.p3" class="ltx_para">
<p id="S3.SS5.p3.8" class="ltx_p"><span id="S3.SS5.p3.8.2" class="ltx_text ltx_font_bold">Hamming distance:</span> The CNN outputs the binary vertex code probabilities <math id="S3.SS5.p3.1.m1.1" class="ltx_Math" alttext="\hat{p}\in\mathbb{R}^{d}" display="inline"><semantics id="S3.SS5.p3.1.m1.1a"><mrow id="S3.SS5.p3.1.m1.1.1" xref="S3.SS5.p3.1.m1.1.1.cmml"><mover accent="true" id="S3.SS5.p3.1.m1.1.1.2" xref="S3.SS5.p3.1.m1.1.1.2.cmml"><mi id="S3.SS5.p3.1.m1.1.1.2.2" xref="S3.SS5.p3.1.m1.1.1.2.2.cmml">p</mi><mo id="S3.SS5.p3.1.m1.1.1.2.1" xref="S3.SS5.p3.1.m1.1.1.2.1.cmml">^</mo></mover><mo id="S3.SS5.p3.1.m1.1.1.1" xref="S3.SS5.p3.1.m1.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS5.p3.1.m1.1.1.3" xref="S3.SS5.p3.1.m1.1.1.3.cmml"><mi id="S3.SS5.p3.1.m1.1.1.3.2" xref="S3.SS5.p3.1.m1.1.1.3.2.cmml">â„</mi><mi id="S3.SS5.p3.1.m1.1.1.3.3" xref="S3.SS5.p3.1.m1.1.1.3.3.cmml">d</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.1.m1.1b"><apply id="S3.SS5.p3.1.m1.1.1.cmml" xref="S3.SS5.p3.1.m1.1.1"><in id="S3.SS5.p3.1.m1.1.1.1.cmml" xref="S3.SS5.p3.1.m1.1.1.1"></in><apply id="S3.SS5.p3.1.m1.1.1.2.cmml" xref="S3.SS5.p3.1.m1.1.1.2"><ci id="S3.SS5.p3.1.m1.1.1.2.1.cmml" xref="S3.SS5.p3.1.m1.1.1.2.1">^</ci><ci id="S3.SS5.p3.1.m1.1.1.2.2.cmml" xref="S3.SS5.p3.1.m1.1.1.2.2">ğ‘</ci></apply><apply id="S3.SS5.p3.1.m1.1.1.3.cmml" xref="S3.SS5.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS5.p3.1.m1.1.1.3.1.cmml" xref="S3.SS5.p3.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS5.p3.1.m1.1.1.3.2.cmml" xref="S3.SS5.p3.1.m1.1.1.3.2">â„</ci><ci id="S3.SS5.p3.1.m1.1.1.3.3.cmml" xref="S3.SS5.p3.1.m1.1.1.3.3">ğ‘‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.1.m1.1c">\hat{p}\in\mathbb{R}^{d}</annotation></semantics></math> for a pixel within a ROI, we obtain the predicted discrete binary code <math id="S3.SS5.p3.2.m2.1" class="ltx_Math" alttext="\hat{b}" display="inline"><semantics id="S3.SS5.p3.2.m2.1a"><mover accent="true" id="S3.SS5.p3.2.m2.1.1" xref="S3.SS5.p3.2.m2.1.1.cmml"><mi id="S3.SS5.p3.2.m2.1.1.2" xref="S3.SS5.p3.2.m2.1.1.2.cmml">b</mi><mo id="S3.SS5.p3.2.m2.1.1.1" xref="S3.SS5.p3.2.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.2.m2.1b"><apply id="S3.SS5.p3.2.m2.1.1.cmml" xref="S3.SS5.p3.2.m2.1.1"><ci id="S3.SS5.p3.2.m2.1.1.1.cmml" xref="S3.SS5.p3.2.m2.1.1.1">^</ci><ci id="S3.SS5.p3.2.m2.1.1.2.cmml" xref="S3.SS5.p3.2.m2.1.1.2">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.2.m2.1c">\hat{b}</annotation></semantics></math> by rounding <math id="S3.SS5.p3.3.m3.1" class="ltx_Math" alttext="\hat{p}" display="inline"><semantics id="S3.SS5.p3.3.m3.1a"><mover accent="true" id="S3.SS5.p3.3.m3.1.1" xref="S3.SS5.p3.3.m3.1.1.cmml"><mi id="S3.SS5.p3.3.m3.1.1.2" xref="S3.SS5.p3.3.m3.1.1.2.cmml">p</mi><mo id="S3.SS5.p3.3.m3.1.1.1" xref="S3.SS5.p3.3.m3.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.3.m3.1b"><apply id="S3.SS5.p3.3.m3.1.1.cmml" xref="S3.SS5.p3.3.m3.1.1"><ci id="S3.SS5.p3.3.m3.1.1.1.cmml" xref="S3.SS5.p3.3.m3.1.1.1">^</ci><ci id="S3.SS5.p3.3.m3.1.1.2.cmml" xref="S3.SS5.p3.3.m3.1.1.2">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.3.m3.1c">\hat{p}</annotation></semantics></math>. Given <math id="S3.SS5.p3.4.m4.1" class="ltx_Math" alttext="\hat{b}" display="inline"><semantics id="S3.SS5.p3.4.m4.1a"><mover accent="true" id="S3.SS5.p3.4.m4.1.1" xref="S3.SS5.p3.4.m4.1.1.cmml"><mi id="S3.SS5.p3.4.m4.1.1.2" xref="S3.SS5.p3.4.m4.1.1.2.cmml">b</mi><mo id="S3.SS5.p3.4.m4.1.1.1" xref="S3.SS5.p3.4.m4.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.4.m4.1b"><apply id="S3.SS5.p3.4.m4.1.1.cmml" xref="S3.SS5.p3.4.m4.1.1"><ci id="S3.SS5.p3.4.m4.1.1.1.cmml" xref="S3.SS5.p3.4.m4.1.1.1">^</ci><ci id="S3.SS5.p3.4.m4.1.1.2.cmml" xref="S3.SS5.p3.4.m4.1.1.2">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.4.m4.1c">\hat{b}</annotation></semantics></math> and its known ground truth binary vertex code <math id="S3.SS5.p3.5.m5.1" class="ltx_Math" alttext="b" display="inline"><semantics id="S3.SS5.p3.5.m5.1a"><mi id="S3.SS5.p3.5.m5.1.1" xref="S3.SS5.p3.5.m5.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.5.m5.1b"><ci id="S3.SS5.p3.5.m5.1.1.cmml" xref="S3.SS5.p3.5.m5.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.5.m5.1c">b</annotation></semantics></math>, the Hamming distance <span id="S3.SS5.p3.6.1" class="ltx_text"><math id="S3.SS5.p3.6.1.m1.1" class="ltx_Math" alttext="Hamm" display="inline"><semantics id="S3.SS5.p3.6.1.m1.1a"><mrow id="S3.SS5.p3.6.1.m1.1.1" xref="S3.SS5.p3.6.1.m1.1.1.cmml"><mi id="S3.SS5.p3.6.1.m1.1.1.2" xref="S3.SS5.p3.6.1.m1.1.1.2.cmml">H</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p3.6.1.m1.1.1.1" xref="S3.SS5.p3.6.1.m1.1.1.1.cmml">â€‹</mo><mi id="S3.SS5.p3.6.1.m1.1.1.3" xref="S3.SS5.p3.6.1.m1.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p3.6.1.m1.1.1.1a" xref="S3.SS5.p3.6.1.m1.1.1.1.cmml">â€‹</mo><mi id="S3.SS5.p3.6.1.m1.1.1.4" xref="S3.SS5.p3.6.1.m1.1.1.4.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p3.6.1.m1.1.1.1b" xref="S3.SS5.p3.6.1.m1.1.1.1.cmml">â€‹</mo><mi id="S3.SS5.p3.6.1.m1.1.1.5" xref="S3.SS5.p3.6.1.m1.1.1.5.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.6.1.m1.1b"><apply id="S3.SS5.p3.6.1.m1.1.1.cmml" xref="S3.SS5.p3.6.1.m1.1.1"><times id="S3.SS5.p3.6.1.m1.1.1.1.cmml" xref="S3.SS5.p3.6.1.m1.1.1.1"></times><ci id="S3.SS5.p3.6.1.m1.1.1.2.cmml" xref="S3.SS5.p3.6.1.m1.1.1.2">ğ»</ci><ci id="S3.SS5.p3.6.1.m1.1.1.3.cmml" xref="S3.SS5.p3.6.1.m1.1.1.3">ğ‘</ci><ci id="S3.SS5.p3.6.1.m1.1.1.4.cmml" xref="S3.SS5.p3.6.1.m1.1.1.4">ğ‘š</ci><ci id="S3.SS5.p3.6.1.m1.1.1.5.cmml" xref="S3.SS5.p3.6.1.m1.1.1.5">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.6.1.m1.1c">Hamm</annotation></semantics></math></span> is defined by counting the number of bits <math id="S3.SS5.p3.7.m6.1" class="ltx_Math" alttext="\hat{b}" display="inline"><semantics id="S3.SS5.p3.7.m6.1a"><mover accent="true" id="S3.SS5.p3.7.m6.1.1" xref="S3.SS5.p3.7.m6.1.1.cmml"><mi id="S3.SS5.p3.7.m6.1.1.2" xref="S3.SS5.p3.7.m6.1.1.2.cmml">b</mi><mo id="S3.SS5.p3.7.m6.1.1.1" xref="S3.SS5.p3.7.m6.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.7.m6.1b"><apply id="S3.SS5.p3.7.m6.1.1.cmml" xref="S3.SS5.p3.7.m6.1.1"><ci id="S3.SS5.p3.7.m6.1.1.1.cmml" xref="S3.SS5.p3.7.m6.1.1.1">^</ci><ci id="S3.SS5.p3.7.m6.1.1.2.cmml" xref="S3.SS5.p3.7.m6.1.1.2">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.7.m6.1c">\hat{b}</annotation></semantics></math> which are different from <math id="S3.SS5.p3.8.m7.1" class="ltx_Math" alttext="b" display="inline"><semantics id="S3.SS5.p3.8.m7.1a"><mi id="S3.SS5.p3.8.m7.1.1" xref="S3.SS5.p3.8.m7.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.8.m7.1b"><ci id="S3.SS5.p3.8.m7.1.1.cmml" xref="S3.SS5.p3.8.m7.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.8.m7.1c">b</annotation></semantics></math>. This formulation does not favor any of the positions and calculates the error without considering any hierarchical information explicitly.
As a common practice in deep learning, we use binary cross-entropy as an activation function for the distance:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.4" class="ltx_Math" alttext="\text{Hamm}(b,\hat{p})=\sum_{j=1}^{d}{b_{j}}\log\hat{p_{j}}+(1-b_{j})\log(1-\hat{p_{j}})," display="block"><semantics id="S3.E2.m1.4a"><mrow id="S3.E2.m1.4.4.1" xref="S3.E2.m1.4.4.1.1.cmml"><mrow id="S3.E2.m1.4.4.1.1" xref="S3.E2.m1.4.4.1.1.cmml"><mrow id="S3.E2.m1.4.4.1.1.4" xref="S3.E2.m1.4.4.1.1.4.cmml"><mtext id="S3.E2.m1.4.4.1.1.4.2" xref="S3.E2.m1.4.4.1.1.4.2a.cmml">Hamm</mtext><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.1.1.4.1" xref="S3.E2.m1.4.4.1.1.4.1.cmml">â€‹</mo><mrow id="S3.E2.m1.4.4.1.1.4.3.2" xref="S3.E2.m1.4.4.1.1.4.3.1.cmml"><mo stretchy="false" id="S3.E2.m1.4.4.1.1.4.3.2.1" xref="S3.E2.m1.4.4.1.1.4.3.1.cmml">(</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">b</mi><mo id="S3.E2.m1.4.4.1.1.4.3.2.2" xref="S3.E2.m1.4.4.1.1.4.3.1.cmml">,</mo><mover accent="true" id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml"><mi id="S3.E2.m1.2.2.2" xref="S3.E2.m1.2.2.2.cmml">p</mi><mo id="S3.E2.m1.2.2.1" xref="S3.E2.m1.2.2.1.cmml">^</mo></mover><mo stretchy="false" id="S3.E2.m1.4.4.1.1.4.3.2.3" xref="S3.E2.m1.4.4.1.1.4.3.1.cmml">)</mo></mrow></mrow><mo rspace="0.111em" id="S3.E2.m1.4.4.1.1.3" xref="S3.E2.m1.4.4.1.1.3.cmml">=</mo><mrow id="S3.E2.m1.4.4.1.1.2" xref="S3.E2.m1.4.4.1.1.2.cmml"><mrow id="S3.E2.m1.4.4.1.1.2.4" xref="S3.E2.m1.4.4.1.1.2.4.cmml"><munderover id="S3.E2.m1.4.4.1.1.2.4.1" xref="S3.E2.m1.4.4.1.1.2.4.1.cmml"><mo movablelimits="false" id="S3.E2.m1.4.4.1.1.2.4.1.2.2" xref="S3.E2.m1.4.4.1.1.2.4.1.2.2.cmml">âˆ‘</mo><mrow id="S3.E2.m1.4.4.1.1.2.4.1.2.3" xref="S3.E2.m1.4.4.1.1.2.4.1.2.3.cmml"><mi id="S3.E2.m1.4.4.1.1.2.4.1.2.3.2" xref="S3.E2.m1.4.4.1.1.2.4.1.2.3.2.cmml">j</mi><mo id="S3.E2.m1.4.4.1.1.2.4.1.2.3.1" xref="S3.E2.m1.4.4.1.1.2.4.1.2.3.1.cmml">=</mo><mn id="S3.E2.m1.4.4.1.1.2.4.1.2.3.3" xref="S3.E2.m1.4.4.1.1.2.4.1.2.3.3.cmml">1</mn></mrow><mi id="S3.E2.m1.4.4.1.1.2.4.1.3" xref="S3.E2.m1.4.4.1.1.2.4.1.3.cmml">d</mi></munderover><mrow id="S3.E2.m1.4.4.1.1.2.4.2" xref="S3.E2.m1.4.4.1.1.2.4.2.cmml"><msub id="S3.E2.m1.4.4.1.1.2.4.2.2" xref="S3.E2.m1.4.4.1.1.2.4.2.2.cmml"><mi id="S3.E2.m1.4.4.1.1.2.4.2.2.2" xref="S3.E2.m1.4.4.1.1.2.4.2.2.2.cmml">b</mi><mi id="S3.E2.m1.4.4.1.1.2.4.2.2.3" xref="S3.E2.m1.4.4.1.1.2.4.2.2.3.cmml">j</mi></msub><mo lspace="0.167em" rspace="0em" id="S3.E2.m1.4.4.1.1.2.4.2.1" xref="S3.E2.m1.4.4.1.1.2.4.2.1.cmml">â€‹</mo><mrow id="S3.E2.m1.4.4.1.1.2.4.2.3" xref="S3.E2.m1.4.4.1.1.2.4.2.3.cmml"><mi id="S3.E2.m1.4.4.1.1.2.4.2.3.1" xref="S3.E2.m1.4.4.1.1.2.4.2.3.1.cmml">log</mi><mo lspace="0.167em" id="S3.E2.m1.4.4.1.1.2.4.2.3a" xref="S3.E2.m1.4.4.1.1.2.4.2.3.cmml">â¡</mo><mover accent="true" id="S3.E2.m1.4.4.1.1.2.4.2.3.2" xref="S3.E2.m1.4.4.1.1.2.4.2.3.2.cmml"><msub id="S3.E2.m1.4.4.1.1.2.4.2.3.2.2" xref="S3.E2.m1.4.4.1.1.2.4.2.3.2.2.cmml"><mi id="S3.E2.m1.4.4.1.1.2.4.2.3.2.2.2" xref="S3.E2.m1.4.4.1.1.2.4.2.3.2.2.2.cmml">p</mi><mi id="S3.E2.m1.4.4.1.1.2.4.2.3.2.2.3" xref="S3.E2.m1.4.4.1.1.2.4.2.3.2.2.3.cmml">j</mi></msub><mo id="S3.E2.m1.4.4.1.1.2.4.2.3.2.1" xref="S3.E2.m1.4.4.1.1.2.4.2.3.2.1.cmml">^</mo></mover></mrow></mrow></mrow><mo id="S3.E2.m1.4.4.1.1.2.3" xref="S3.E2.m1.4.4.1.1.2.3.cmml">+</mo><mrow id="S3.E2.m1.4.4.1.1.2.2" xref="S3.E2.m1.4.4.1.1.2.2.cmml"><mrow id="S3.E2.m1.4.4.1.1.1.1.1.1" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.4.4.1.1.1.1.1.1.2" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.4.4.1.1.1.1.1.1.1" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.cmml"><mn id="S3.E2.m1.4.4.1.1.1.1.1.1.1.2" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><msub id="S3.E2.m1.4.4.1.1.1.1.1.1.1.3" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.4.4.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.3.2.cmml">b</mi><mi id="S3.E2.m1.4.4.1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.3.3.cmml">j</mi></msub></mrow><mo stretchy="false" id="S3.E2.m1.4.4.1.1.1.1.1.1.3" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0.167em" rspace="0em" id="S3.E2.m1.4.4.1.1.2.2.3" xref="S3.E2.m1.4.4.1.1.2.2.3.cmml">â€‹</mo><mrow id="S3.E2.m1.4.4.1.1.2.2.2.1" xref="S3.E2.m1.4.4.1.1.2.2.2.2.cmml"><mi id="S3.E2.m1.3.3" xref="S3.E2.m1.3.3.cmml">log</mi><mo id="S3.E2.m1.4.4.1.1.2.2.2.1a" xref="S3.E2.m1.4.4.1.1.2.2.2.2.cmml">â¡</mo><mrow id="S3.E2.m1.4.4.1.1.2.2.2.1.1" xref="S3.E2.m1.4.4.1.1.2.2.2.2.cmml"><mo stretchy="false" id="S3.E2.m1.4.4.1.1.2.2.2.1.1.2" xref="S3.E2.m1.4.4.1.1.2.2.2.2.cmml">(</mo><mrow id="S3.E2.m1.4.4.1.1.2.2.2.1.1.1" xref="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.cmml"><mn id="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.2" xref="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.2.cmml">1</mn><mo id="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.1" xref="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.1.cmml">âˆ’</mo><mover accent="true" id="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.3" xref="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.3.cmml"><msub id="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.3.2" xref="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.3.2.cmml"><mi id="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.3.2.2" xref="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.3.2.2.cmml">p</mi><mi id="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.3.2.3" xref="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.3.2.3.cmml">j</mi></msub><mo id="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.3.1" xref="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.3.1.cmml">^</mo></mover></mrow><mo stretchy="false" id="S3.E2.m1.4.4.1.1.2.2.2.1.1.3" xref="S3.E2.m1.4.4.1.1.2.2.2.2.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S3.E2.m1.4.4.1.2" xref="S3.E2.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.4b"><apply id="S3.E2.m1.4.4.1.1.cmml" xref="S3.E2.m1.4.4.1"><eq id="S3.E2.m1.4.4.1.1.3.cmml" xref="S3.E2.m1.4.4.1.1.3"></eq><apply id="S3.E2.m1.4.4.1.1.4.cmml" xref="S3.E2.m1.4.4.1.1.4"><times id="S3.E2.m1.4.4.1.1.4.1.cmml" xref="S3.E2.m1.4.4.1.1.4.1"></times><ci id="S3.E2.m1.4.4.1.1.4.2a.cmml" xref="S3.E2.m1.4.4.1.1.4.2"><mtext id="S3.E2.m1.4.4.1.1.4.2.cmml" xref="S3.E2.m1.4.4.1.1.4.2">Hamm</mtext></ci><interval closure="open" id="S3.E2.m1.4.4.1.1.4.3.1.cmml" xref="S3.E2.m1.4.4.1.1.4.3.2"><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">ğ‘</ci><apply id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2"><ci id="S3.E2.m1.2.2.1.cmml" xref="S3.E2.m1.2.2.1">^</ci><ci id="S3.E2.m1.2.2.2.cmml" xref="S3.E2.m1.2.2.2">ğ‘</ci></apply></interval></apply><apply id="S3.E2.m1.4.4.1.1.2.cmml" xref="S3.E2.m1.4.4.1.1.2"><plus id="S3.E2.m1.4.4.1.1.2.3.cmml" xref="S3.E2.m1.4.4.1.1.2.3"></plus><apply id="S3.E2.m1.4.4.1.1.2.4.cmml" xref="S3.E2.m1.4.4.1.1.2.4"><apply id="S3.E2.m1.4.4.1.1.2.4.1.cmml" xref="S3.E2.m1.4.4.1.1.2.4.1"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.2.4.1.1.cmml" xref="S3.E2.m1.4.4.1.1.2.4.1">superscript</csymbol><apply id="S3.E2.m1.4.4.1.1.2.4.1.2.cmml" xref="S3.E2.m1.4.4.1.1.2.4.1"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.2.4.1.2.1.cmml" xref="S3.E2.m1.4.4.1.1.2.4.1">subscript</csymbol><sum id="S3.E2.m1.4.4.1.1.2.4.1.2.2.cmml" xref="S3.E2.m1.4.4.1.1.2.4.1.2.2"></sum><apply id="S3.E2.m1.4.4.1.1.2.4.1.2.3.cmml" xref="S3.E2.m1.4.4.1.1.2.4.1.2.3"><eq id="S3.E2.m1.4.4.1.1.2.4.1.2.3.1.cmml" xref="S3.E2.m1.4.4.1.1.2.4.1.2.3.1"></eq><ci id="S3.E2.m1.4.4.1.1.2.4.1.2.3.2.cmml" xref="S3.E2.m1.4.4.1.1.2.4.1.2.3.2">ğ‘—</ci><cn type="integer" id="S3.E2.m1.4.4.1.1.2.4.1.2.3.3.cmml" xref="S3.E2.m1.4.4.1.1.2.4.1.2.3.3">1</cn></apply></apply><ci id="S3.E2.m1.4.4.1.1.2.4.1.3.cmml" xref="S3.E2.m1.4.4.1.1.2.4.1.3">ğ‘‘</ci></apply><apply id="S3.E2.m1.4.4.1.1.2.4.2.cmml" xref="S3.E2.m1.4.4.1.1.2.4.2"><times id="S3.E2.m1.4.4.1.1.2.4.2.1.cmml" xref="S3.E2.m1.4.4.1.1.2.4.2.1"></times><apply id="S3.E2.m1.4.4.1.1.2.4.2.2.cmml" xref="S3.E2.m1.4.4.1.1.2.4.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.2.4.2.2.1.cmml" xref="S3.E2.m1.4.4.1.1.2.4.2.2">subscript</csymbol><ci id="S3.E2.m1.4.4.1.1.2.4.2.2.2.cmml" xref="S3.E2.m1.4.4.1.1.2.4.2.2.2">ğ‘</ci><ci id="S3.E2.m1.4.4.1.1.2.4.2.2.3.cmml" xref="S3.E2.m1.4.4.1.1.2.4.2.2.3">ğ‘—</ci></apply><apply id="S3.E2.m1.4.4.1.1.2.4.2.3.cmml" xref="S3.E2.m1.4.4.1.1.2.4.2.3"><log id="S3.E2.m1.4.4.1.1.2.4.2.3.1.cmml" xref="S3.E2.m1.4.4.1.1.2.4.2.3.1"></log><apply id="S3.E2.m1.4.4.1.1.2.4.2.3.2.cmml" xref="S3.E2.m1.4.4.1.1.2.4.2.3.2"><ci id="S3.E2.m1.4.4.1.1.2.4.2.3.2.1.cmml" xref="S3.E2.m1.4.4.1.1.2.4.2.3.2.1">^</ci><apply id="S3.E2.m1.4.4.1.1.2.4.2.3.2.2.cmml" xref="S3.E2.m1.4.4.1.1.2.4.2.3.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.2.4.2.3.2.2.1.cmml" xref="S3.E2.m1.4.4.1.1.2.4.2.3.2.2">subscript</csymbol><ci id="S3.E2.m1.4.4.1.1.2.4.2.3.2.2.2.cmml" xref="S3.E2.m1.4.4.1.1.2.4.2.3.2.2.2">ğ‘</ci><ci id="S3.E2.m1.4.4.1.1.2.4.2.3.2.2.3.cmml" xref="S3.E2.m1.4.4.1.1.2.4.2.3.2.2.3">ğ‘—</ci></apply></apply></apply></apply></apply><apply id="S3.E2.m1.4.4.1.1.2.2.cmml" xref="S3.E2.m1.4.4.1.1.2.2"><times id="S3.E2.m1.4.4.1.1.2.2.3.cmml" xref="S3.E2.m1.4.4.1.1.2.2.3"></times><apply id="S3.E2.m1.4.4.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1"><minus id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1"></minus><cn type="integer" id="S3.E2.m1.4.4.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.2">1</cn><apply id="S3.E2.m1.4.4.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.4.4.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.3.2">ğ‘</ci><ci id="S3.E2.m1.4.4.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.3.3">ğ‘—</ci></apply></apply><apply id="S3.E2.m1.4.4.1.1.2.2.2.2.cmml" xref="S3.E2.m1.4.4.1.1.2.2.2.1"><log id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3"></log><apply id="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.2.2.2.1.1.1"><minus id="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.1"></minus><cn type="integer" id="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.2.cmml" xref="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.2">1</cn><apply id="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.3.cmml" xref="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.3"><ci id="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.3.1.cmml" xref="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.3.1">^</ci><apply id="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.3.2.cmml" xref="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.3.2.1.cmml" xref="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.3.2">subscript</csymbol><ci id="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.3.2.2.cmml" xref="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.3.2.2">ğ‘</ci><ci id="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.3.2.3.cmml" xref="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.3.2.3">ğ‘—</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.4c">\text{Hamm}(b,\hat{p})=\sum_{j=1}^{d}{b_{j}}\log\hat{p_{j}}+(1-b_{j})\log(1-\hat{p_{j}}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS5.p3.13" class="ltx_p">where <math id="S3.SS5.p3.9.m1.1" class="ltx_Math" alttext="b_{j}" display="inline"><semantics id="S3.SS5.p3.9.m1.1a"><msub id="S3.SS5.p3.9.m1.1.1" xref="S3.SS5.p3.9.m1.1.1.cmml"><mi id="S3.SS5.p3.9.m1.1.1.2" xref="S3.SS5.p3.9.m1.1.1.2.cmml">b</mi><mi id="S3.SS5.p3.9.m1.1.1.3" xref="S3.SS5.p3.9.m1.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.9.m1.1b"><apply id="S3.SS5.p3.9.m1.1.1.cmml" xref="S3.SS5.p3.9.m1.1.1"><csymbol cd="ambiguous" id="S3.SS5.p3.9.m1.1.1.1.cmml" xref="S3.SS5.p3.9.m1.1.1">subscript</csymbol><ci id="S3.SS5.p3.9.m1.1.1.2.cmml" xref="S3.SS5.p3.9.m1.1.1.2">ğ‘</ci><ci id="S3.SS5.p3.9.m1.1.1.3.cmml" xref="S3.SS5.p3.9.m1.1.1.3">ğ‘—</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.9.m1.1c">b_{j}</annotation></semantics></math> stands for the <math id="S3.SS5.p3.10.m2.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.SS5.p3.10.m2.1a"><mi id="S3.SS5.p3.10.m2.1.1" xref="S3.SS5.p3.10.m2.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.10.m2.1b"><ci id="S3.SS5.p3.10.m2.1.1.cmml" xref="S3.SS5.p3.10.m2.1.1">ğ‘—</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.10.m2.1c">j</annotation></semantics></math>-th bit in <math id="S3.SS5.p3.11.m3.1" class="ltx_Math" alttext="b" display="inline"><semantics id="S3.SS5.p3.11.m3.1a"><mi id="S3.SS5.p3.11.m3.1.1" xref="S3.SS5.p3.11.m3.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.11.m3.1b"><ci id="S3.SS5.p3.11.m3.1.1.cmml" xref="S3.SS5.p3.11.m3.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.11.m3.1c">b</annotation></semantics></math> (the <math id="S3.SS5.p3.12.m4.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.SS5.p3.12.m4.1a"><mi id="S3.SS5.p3.12.m4.1.1" xref="S3.SS5.p3.12.m4.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.12.m4.1b"><ci id="S3.SS5.p3.12.m4.1.1.cmml" xref="S3.SS5.p3.12.m4.1.1">ğ‘—</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.12.m4.1c">j</annotation></semantics></math>-th bit is generated in the <math id="S3.SS5.p3.13.m5.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.SS5.p3.13.m5.1a"><mi id="S3.SS5.p3.13.m5.1.1" xref="S3.SS5.p3.13.m5.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.13.m5.1b"><ci id="S3.SS5.p3.13.m5.1.1.cmml" xref="S3.SS5.p3.13.m5.1.1">ğ‘—</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.13.m5.1c">j</annotation></semantics></math>-th vertices grouping).</p>
</div>
<div id="S3.SS5.p4" class="ltx_para">
<p id="S3.SS5.p4.2" class="ltx_p"><span id="S3.SS5.p4.2.1" class="ltx_text ltx_font_bold">Active bits.</span> Lower bits in binary vertex code <math id="S3.SS5.p4.1.m1.1" class="ltx_Math" alttext="b" display="inline"><semantics id="S3.SS5.p4.1.m1.1a"><mi id="S3.SS5.p4.1.m1.1.1" xref="S3.SS5.p4.1.m1.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p4.1.m1.1b"><ci id="S3.SS5.p4.1.m1.1.1.cmml" xref="S3.SS5.p4.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p4.1.m1.1c">b</annotation></semantics></math> hold coarse correspondences, and higher bits define finer estimates. During the initial training phase, the network focuses on learning the coarse splits and has a higher error on fine bits. Therefore we adaptively weight the coarse bits by looking at the histogram of error of all bits. As the training proceeds and coarser predictions become more robust, finer bits are induced with more weights. We define our histogram at training step <math id="S3.SS5.p4.2.m2.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS5.p4.2.m2.1a"><mi id="S3.SS5.p4.2.m2.1.1" xref="S3.SS5.p4.2.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p4.2.m2.1b"><ci id="S3.SS5.p4.2.m2.1.1.cmml" xref="S3.SS5.p4.2.m2.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p4.2.m2.1c">t</annotation></semantics></math> by looking at the error at different bits:</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.2" class="ltx_Math" alttext="H_{j}(t)=avg(\lambda(b_{j}^{t}-\hat{b_{j}}^{t})+(1-\lambda)(b_{j}^{t-1}-\hat{b_{j}}^{t-1}))," display="block"><semantics id="S3.E3.m1.2a"><mrow id="S3.E3.m1.2.2.1" xref="S3.E3.m1.2.2.1.1.cmml"><mrow id="S3.E3.m1.2.2.1.1" xref="S3.E3.m1.2.2.1.1.cmml"><mrow id="S3.E3.m1.2.2.1.1.3" xref="S3.E3.m1.2.2.1.1.3.cmml"><msub id="S3.E3.m1.2.2.1.1.3.2" xref="S3.E3.m1.2.2.1.1.3.2.cmml"><mi id="S3.E3.m1.2.2.1.1.3.2.2" xref="S3.E3.m1.2.2.1.1.3.2.2.cmml">H</mi><mi id="S3.E3.m1.2.2.1.1.3.2.3" xref="S3.E3.m1.2.2.1.1.3.2.3.cmml">j</mi></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.1.1.3.1" xref="S3.E3.m1.2.2.1.1.3.1.cmml">â€‹</mo><mrow id="S3.E3.m1.2.2.1.1.3.3.2" xref="S3.E3.m1.2.2.1.1.3.cmml"><mo stretchy="false" id="S3.E3.m1.2.2.1.1.3.3.2.1" xref="S3.E3.m1.2.2.1.1.3.cmml">(</mo><mi id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml">t</mi><mo stretchy="false" id="S3.E3.m1.2.2.1.1.3.3.2.2" xref="S3.E3.m1.2.2.1.1.3.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.2.2.1.1.2" xref="S3.E3.m1.2.2.1.1.2.cmml">=</mo><mrow id="S3.E3.m1.2.2.1.1.1" xref="S3.E3.m1.2.2.1.1.1.cmml"><mi id="S3.E3.m1.2.2.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.1.1.1.2" xref="S3.E3.m1.2.2.1.1.1.2.cmml">â€‹</mo><mi id="S3.E3.m1.2.2.1.1.1.4" xref="S3.E3.m1.2.2.1.1.1.4.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.1.1.1.2a" xref="S3.E3.m1.2.2.1.1.1.2.cmml">â€‹</mo><mi id="S3.E3.m1.2.2.1.1.1.5" xref="S3.E3.m1.2.2.1.1.1.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.1.1.1.2b" xref="S3.E3.m1.2.2.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E3.m1.2.2.1.1.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.2.2.1.1.1.1.1.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.2.2.1.1.1.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.1.1.cmml"><mrow id="S3.E3.m1.2.2.1.1.1.1.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.2.2.1.1.1.1.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.3.cmml">Î»</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.1.1.1.1.1.1.1.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml"><msubsup id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml">b</mi><mi id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.2.3" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml">j</mi><mi id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.3.cmml">t</mi></msubsup><mo id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><msup id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.cmml"><mover accent="true" id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.2.cmml"><msub id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.2.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.2.2.cmml"><mi id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.2.2.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.2.2.2.cmml">b</mi><mi id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.2.2.3" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.2.2.3.cmml">j</mi></msub><mo id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.2.1" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.2.1.cmml">^</mo></mover><mi id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.3.cmml">t</mi></msup></mrow><mo stretchy="false" id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.2.2.1.1.1.1.1.1.4" xref="S3.E3.m1.2.2.1.1.1.1.1.1.4.cmml">+</mo><mrow id="S3.E3.m1.2.2.1.1.1.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.cmml"><mrow id="S3.E3.m1.2.2.1.1.1.1.1.1.2.1.1" xref="S3.E3.m1.2.2.1.1.1.1.1.1.2.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.2.2.1.1.1.1.1.1.2.1.1.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.2.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.2.2.1.1.1.1.1.1.2.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.1.1.2.1.1.1.cmml"><mn id="S3.E3.m1.2.2.1.1.1.1.1.1.2.1.1.1.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.2.1.1.1.2.cmml">1</mn><mo id="S3.E3.m1.2.2.1.1.1.1.1.1.2.1.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.cmml">âˆ’</mo><mi id="S3.E3.m1.2.2.1.1.1.1.1.1.2.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.1.1.1.2.1.1.1.3.cmml">Î»</mi></mrow><mo stretchy="false" id="S3.E3.m1.2.2.1.1.1.1.1.1.2.1.1.3" xref="S3.E3.m1.2.2.1.1.1.1.1.1.2.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.1.1.1.1.1.1.3.3" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.3.cmml">â€‹</mo><mrow id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.cmml">(</mo><mrow id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.cmml"><msubsup id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.2.cmml"><mi id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.2.2.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.2.2.2.cmml">b</mi><mi id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.2.2.3" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.2.2.3.cmml">j</mi><mrow id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.2.3" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.2.3.cmml"><mi id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.2.3.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.2.3.2.cmml">t</mi><mo id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.2.3.1" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.2.3.1.cmml">âˆ’</mo><mn id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.2.3.3" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.2.3.3.cmml">1</mn></mrow></msubsup><mo id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.1.cmml">âˆ’</mo><msup id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.3" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.3.cmml"><mover accent="true" id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.3.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.3.2.cmml"><msub id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.3.2.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.3.2.2.cmml"><mi id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.3.2.2.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.3.2.2.2.cmml">b</mi><mi id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.3.2.2.3" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.3.2.2.3.cmml">j</mi></msub><mo id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.3.2.1" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.3.2.1.cmml">^</mo></mover><mrow id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.3.3" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.3.3.cmml"><mi id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.3.3.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.3.3.2.cmml">t</mi><mo id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.3.3.1" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.3.3.1.cmml">âˆ’</mo><mn id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.3.3.3" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.3.3.3.cmml">1</mn></mrow></msup></mrow><mo stretchy="false" id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.3" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S3.E3.m1.2.2.1.1.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E3.m1.2.2.1.2" xref="S3.E3.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.2b"><apply id="S3.E3.m1.2.2.1.1.cmml" xref="S3.E3.m1.2.2.1"><eq id="S3.E3.m1.2.2.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.2"></eq><apply id="S3.E3.m1.2.2.1.1.3.cmml" xref="S3.E3.m1.2.2.1.1.3"><times id="S3.E3.m1.2.2.1.1.3.1.cmml" xref="S3.E3.m1.2.2.1.1.3.1"></times><apply id="S3.E3.m1.2.2.1.1.3.2.cmml" xref="S3.E3.m1.2.2.1.1.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.3.2.1.cmml" xref="S3.E3.m1.2.2.1.1.3.2">subscript</csymbol><ci id="S3.E3.m1.2.2.1.1.3.2.2.cmml" xref="S3.E3.m1.2.2.1.1.3.2.2">ğ»</ci><ci id="S3.E3.m1.2.2.1.1.3.2.3.cmml" xref="S3.E3.m1.2.2.1.1.3.2.3">ğ‘—</ci></apply><ci id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1">ğ‘¡</ci></apply><apply id="S3.E3.m1.2.2.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1"><times id="S3.E3.m1.2.2.1.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.1.2"></times><ci id="S3.E3.m1.2.2.1.1.1.3.cmml" xref="S3.E3.m1.2.2.1.1.1.3">ğ‘</ci><ci id="S3.E3.m1.2.2.1.1.1.4.cmml" xref="S3.E3.m1.2.2.1.1.1.4">ğ‘£</ci><ci id="S3.E3.m1.2.2.1.1.1.5.cmml" xref="S3.E3.m1.2.2.1.1.1.5">ğ‘”</ci><apply id="S3.E3.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1"><plus id="S3.E3.m1.2.2.1.1.1.1.1.1.4.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.4"></plus><apply id="S3.E3.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1"><times id="S3.E3.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.2"></times><ci id="S3.E3.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.3">ğœ†</ci><apply id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1"><minus id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.1"></minus><apply id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.2.2">ğ‘</ci><ci id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.2.3">ğ‘—</ci></apply><ci id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.3">ğ‘¡</ci></apply><apply id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3">superscript</csymbol><apply id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.2"><ci id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.2.1">^</ci><apply id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.2.2.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.2.2">subscript</csymbol><ci id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.2.2.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.2.2.2">ğ‘</ci><ci id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.2.2.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.2.2.3">ğ‘—</ci></apply></apply><ci id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.3">ğ‘¡</ci></apply></apply></apply><apply id="S3.E3.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3"><times id="S3.E3.m1.2.2.1.1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.3"></times><apply id="S3.E3.m1.2.2.1.1.1.1.1.1.2.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.2.1.1"><minus id="S3.E3.m1.2.2.1.1.1.1.1.1.2.1.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.2.1.1.1.1"></minus><cn type="integer" id="S3.E3.m1.2.2.1.1.1.1.1.1.2.1.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.2.1.1.1.2">1</cn><ci id="S3.E3.m1.2.2.1.1.1.1.1.1.2.1.1.1.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.2.1.1.1.3">ğœ†</ci></apply><apply id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1"><minus id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.1"></minus><apply id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.2.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.2">superscript</csymbol><apply id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.2.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.2.2.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.2">subscript</csymbol><ci id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.2.2.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.2.2.2">ğ‘</ci><ci id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.2.2.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.2.2.3">ğ‘—</ci></apply><apply id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.2.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.2.3"><minus id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.2.3.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.2.3.1"></minus><ci id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.2.3.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.2.3.2">ğ‘¡</ci><cn type="integer" id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.2.3.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.2.3.3">1</cn></apply></apply><apply id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.3.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.3">superscript</csymbol><apply id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.3.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.3.2"><ci id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.3.2.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.3.2.1">^</ci><apply id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.3.2.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.3.2.2.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.3.2.2">subscript</csymbol><ci id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.3.2.2.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.3.2.2.2">ğ‘</ci><ci id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.3.2.2.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.3.2.2.3">ğ‘—</ci></apply></apply><apply id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.3.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.3.3"><minus id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.3.3.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.3.3.1"></minus><ci id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.3.3.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.3.3.2">ğ‘¡</ci><cn type="integer" id="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.3.3.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.2.1.1.3.3.3">1</cn></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.2c">H_{j}(t)=avg(\lambda(b_{j}^{t}-\hat{b_{j}}^{t})+(1-\lambda)(b_{j}^{t-1}-\hat{b_{j}}^{t-1})),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS5.p4.10" class="ltx_p">where <math id="S3.SS5.p4.3.m1.1" class="ltx_Math" alttext="\hat{b_{j}}^{t}" display="inline"><semantics id="S3.SS5.p4.3.m1.1a"><msup id="S3.SS5.p4.3.m1.1.1" xref="S3.SS5.p4.3.m1.1.1.cmml"><mover accent="true" id="S3.SS5.p4.3.m1.1.1.2" xref="S3.SS5.p4.3.m1.1.1.2.cmml"><msub id="S3.SS5.p4.3.m1.1.1.2.2" xref="S3.SS5.p4.3.m1.1.1.2.2.cmml"><mi id="S3.SS5.p4.3.m1.1.1.2.2.2" xref="S3.SS5.p4.3.m1.1.1.2.2.2.cmml">b</mi><mi id="S3.SS5.p4.3.m1.1.1.2.2.3" xref="S3.SS5.p4.3.m1.1.1.2.2.3.cmml">j</mi></msub><mo id="S3.SS5.p4.3.m1.1.1.2.1" xref="S3.SS5.p4.3.m1.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS5.p4.3.m1.1.1.3" xref="S3.SS5.p4.3.m1.1.1.3.cmml">t</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS5.p4.3.m1.1b"><apply id="S3.SS5.p4.3.m1.1.1.cmml" xref="S3.SS5.p4.3.m1.1.1"><csymbol cd="ambiguous" id="S3.SS5.p4.3.m1.1.1.1.cmml" xref="S3.SS5.p4.3.m1.1.1">superscript</csymbol><apply id="S3.SS5.p4.3.m1.1.1.2.cmml" xref="S3.SS5.p4.3.m1.1.1.2"><ci id="S3.SS5.p4.3.m1.1.1.2.1.cmml" xref="S3.SS5.p4.3.m1.1.1.2.1">^</ci><apply id="S3.SS5.p4.3.m1.1.1.2.2.cmml" xref="S3.SS5.p4.3.m1.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS5.p4.3.m1.1.1.2.2.1.cmml" xref="S3.SS5.p4.3.m1.1.1.2.2">subscript</csymbol><ci id="S3.SS5.p4.3.m1.1.1.2.2.2.cmml" xref="S3.SS5.p4.3.m1.1.1.2.2.2">ğ‘</ci><ci id="S3.SS5.p4.3.m1.1.1.2.2.3.cmml" xref="S3.SS5.p4.3.m1.1.1.2.2.3">ğ‘—</ci></apply></apply><ci id="S3.SS5.p4.3.m1.1.1.3.cmml" xref="S3.SS5.p4.3.m1.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p4.3.m1.1c">\hat{b_{j}}^{t}</annotation></semantics></math> defines the predicted binary vertex code <math id="S3.SS5.p4.4.m2.1" class="ltx_Math" alttext="\hat{b_{j}}" display="inline"><semantics id="S3.SS5.p4.4.m2.1a"><mover accent="true" id="S3.SS5.p4.4.m2.1.1" xref="S3.SS5.p4.4.m2.1.1.cmml"><msub id="S3.SS5.p4.4.m2.1.1.2" xref="S3.SS5.p4.4.m2.1.1.2.cmml"><mi id="S3.SS5.p4.4.m2.1.1.2.2" xref="S3.SS5.p4.4.m2.1.1.2.2.cmml">b</mi><mi id="S3.SS5.p4.4.m2.1.1.2.3" xref="S3.SS5.p4.4.m2.1.1.2.3.cmml">j</mi></msub><mo id="S3.SS5.p4.4.m2.1.1.1" xref="S3.SS5.p4.4.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS5.p4.4.m2.1b"><apply id="S3.SS5.p4.4.m2.1.1.cmml" xref="S3.SS5.p4.4.m2.1.1"><ci id="S3.SS5.p4.4.m2.1.1.1.cmml" xref="S3.SS5.p4.4.m2.1.1.1">^</ci><apply id="S3.SS5.p4.4.m2.1.1.2.cmml" xref="S3.SS5.p4.4.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS5.p4.4.m2.1.1.2.1.cmml" xref="S3.SS5.p4.4.m2.1.1.2">subscript</csymbol><ci id="S3.SS5.p4.4.m2.1.1.2.2.cmml" xref="S3.SS5.p4.4.m2.1.1.2.2">ğ‘</ci><ci id="S3.SS5.p4.4.m2.1.1.2.3.cmml" xref="S3.SS5.p4.4.m2.1.1.2.3">ğ‘—</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p4.4.m2.1c">\hat{b_{j}}</annotation></semantics></math> at training step <math id="S3.SS5.p4.5.m3.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS5.p4.5.m3.1a"><mi id="S3.SS5.p4.5.m3.1.1" xref="S3.SS5.p4.5.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p4.5.m3.1b"><ci id="S3.SS5.p4.5.m3.1.1.cmml" xref="S3.SS5.p4.5.m3.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p4.5.m3.1c">t</annotation></semantics></math>, and <math id="S3.SS5.p4.6.m4.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S3.SS5.p4.6.m4.1a"><mi id="S3.SS5.p4.6.m4.1.1" xref="S3.SS5.p4.6.m4.1.1.cmml">Î»</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p4.6.m4.1b"><ci id="S3.SS5.p4.6.m4.1.1.cmml" xref="S3.SS5.p4.6.m4.1.1">ğœ†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p4.6.m4.1c">\lambda</annotation></semantics></math> is a constant. With the <math id="S3.SS5.p4.7.m5.1" class="ltx_Math" alttext="avg" display="inline"><semantics id="S3.SS5.p4.7.m5.1a"><mrow id="S3.SS5.p4.7.m5.1.1" xref="S3.SS5.p4.7.m5.1.1.cmml"><mi id="S3.SS5.p4.7.m5.1.1.2" xref="S3.SS5.p4.7.m5.1.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p4.7.m5.1.1.1" xref="S3.SS5.p4.7.m5.1.1.1.cmml">â€‹</mo><mi id="S3.SS5.p4.7.m5.1.1.3" xref="S3.SS5.p4.7.m5.1.1.3.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p4.7.m5.1.1.1a" xref="S3.SS5.p4.7.m5.1.1.1.cmml">â€‹</mo><mi id="S3.SS5.p4.7.m5.1.1.4" xref="S3.SS5.p4.7.m5.1.1.4.cmml">g</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p4.7.m5.1b"><apply id="S3.SS5.p4.7.m5.1.1.cmml" xref="S3.SS5.p4.7.m5.1.1"><times id="S3.SS5.p4.7.m5.1.1.1.cmml" xref="S3.SS5.p4.7.m5.1.1.1"></times><ci id="S3.SS5.p4.7.m5.1.1.2.cmml" xref="S3.SS5.p4.7.m5.1.1.2">ğ‘</ci><ci id="S3.SS5.p4.7.m5.1.1.3.cmml" xref="S3.SS5.p4.7.m5.1.1.3">ğ‘£</ci><ci id="S3.SS5.p4.7.m5.1.1.4.cmml" xref="S3.SS5.p4.7.m5.1.1.4">ğ‘”</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p4.7.m5.1c">avg</annotation></semantics></math> operator, we get the error ratio by calculating the average difference in <math id="S3.SS5.p4.8.m6.1" class="ltx_Math" alttext="b_{j}^{t}" display="inline"><semantics id="S3.SS5.p4.8.m6.1a"><msubsup id="S3.SS5.p4.8.m6.1.1" xref="S3.SS5.p4.8.m6.1.1.cmml"><mi id="S3.SS5.p4.8.m6.1.1.2.2" xref="S3.SS5.p4.8.m6.1.1.2.2.cmml">b</mi><mi id="S3.SS5.p4.8.m6.1.1.2.3" xref="S3.SS5.p4.8.m6.1.1.2.3.cmml">j</mi><mi id="S3.SS5.p4.8.m6.1.1.3" xref="S3.SS5.p4.8.m6.1.1.3.cmml">t</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS5.p4.8.m6.1b"><apply id="S3.SS5.p4.8.m6.1.1.cmml" xref="S3.SS5.p4.8.m6.1.1"><csymbol cd="ambiguous" id="S3.SS5.p4.8.m6.1.1.1.cmml" xref="S3.SS5.p4.8.m6.1.1">superscript</csymbol><apply id="S3.SS5.p4.8.m6.1.1.2.cmml" xref="S3.SS5.p4.8.m6.1.1"><csymbol cd="ambiguous" id="S3.SS5.p4.8.m6.1.1.2.1.cmml" xref="S3.SS5.p4.8.m6.1.1">subscript</csymbol><ci id="S3.SS5.p4.8.m6.1.1.2.2.cmml" xref="S3.SS5.p4.8.m6.1.1.2.2">ğ‘</ci><ci id="S3.SS5.p4.8.m6.1.1.2.3.cmml" xref="S3.SS5.p4.8.m6.1.1.2.3">ğ‘—</ci></apply><ci id="S3.SS5.p4.8.m6.1.1.3.cmml" xref="S3.SS5.p4.8.m6.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p4.8.m6.1c">b_{j}^{t}</annotation></semantics></math> and <math id="S3.SS5.p4.9.m7.1" class="ltx_Math" alttext="\hat{b_{j}}^{t}" display="inline"><semantics id="S3.SS5.p4.9.m7.1a"><msup id="S3.SS5.p4.9.m7.1.1" xref="S3.SS5.p4.9.m7.1.1.cmml"><mover accent="true" id="S3.SS5.p4.9.m7.1.1.2" xref="S3.SS5.p4.9.m7.1.1.2.cmml"><msub id="S3.SS5.p4.9.m7.1.1.2.2" xref="S3.SS5.p4.9.m7.1.1.2.2.cmml"><mi id="S3.SS5.p4.9.m7.1.1.2.2.2" xref="S3.SS5.p4.9.m7.1.1.2.2.2.cmml">b</mi><mi id="S3.SS5.p4.9.m7.1.1.2.2.3" xref="S3.SS5.p4.9.m7.1.1.2.2.3.cmml">j</mi></msub><mo id="S3.SS5.p4.9.m7.1.1.2.1" xref="S3.SS5.p4.9.m7.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS5.p4.9.m7.1.1.3" xref="S3.SS5.p4.9.m7.1.1.3.cmml">t</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS5.p4.9.m7.1b"><apply id="S3.SS5.p4.9.m7.1.1.cmml" xref="S3.SS5.p4.9.m7.1.1"><csymbol cd="ambiguous" id="S3.SS5.p4.9.m7.1.1.1.cmml" xref="S3.SS5.p4.9.m7.1.1">superscript</csymbol><apply id="S3.SS5.p4.9.m7.1.1.2.cmml" xref="S3.SS5.p4.9.m7.1.1.2"><ci id="S3.SS5.p4.9.m7.1.1.2.1.cmml" xref="S3.SS5.p4.9.m7.1.1.2.1">^</ci><apply id="S3.SS5.p4.9.m7.1.1.2.2.cmml" xref="S3.SS5.p4.9.m7.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS5.p4.9.m7.1.1.2.2.1.cmml" xref="S3.SS5.p4.9.m7.1.1.2.2">subscript</csymbol><ci id="S3.SS5.p4.9.m7.1.1.2.2.2.cmml" xref="S3.SS5.p4.9.m7.1.1.2.2.2">ğ‘</ci><ci id="S3.SS5.p4.9.m7.1.1.2.2.3.cmml" xref="S3.SS5.p4.9.m7.1.1.2.2.3">ğ‘—</ci></apply></apply><ci id="S3.SS5.p4.9.m7.1.1.3.cmml" xref="S3.SS5.p4.9.m7.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p4.9.m7.1c">\hat{b_{j}}^{t}</annotation></semantics></math> of all pixels within the predicted object mask in a mini-batch.
During training we update the histogram given the previous histogram in training step <math id="S3.SS5.p4.10.m8.1" class="ltx_Math" alttext="t-1" display="inline"><semantics id="S3.SS5.p4.10.m8.1a"><mrow id="S3.SS5.p4.10.m8.1.1" xref="S3.SS5.p4.10.m8.1.1.cmml"><mi id="S3.SS5.p4.10.m8.1.1.2" xref="S3.SS5.p4.10.m8.1.1.2.cmml">t</mi><mo id="S3.SS5.p4.10.m8.1.1.1" xref="S3.SS5.p4.10.m8.1.1.1.cmml">âˆ’</mo><mn id="S3.SS5.p4.10.m8.1.1.3" xref="S3.SS5.p4.10.m8.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p4.10.m8.1b"><apply id="S3.SS5.p4.10.m8.1.1.cmml" xref="S3.SS5.p4.10.m8.1.1"><minus id="S3.SS5.p4.10.m8.1.1.1.cmml" xref="S3.SS5.p4.10.m8.1.1.1"></minus><ci id="S3.SS5.p4.10.m8.1.1.2.cmml" xref="S3.SS5.p4.10.m8.1.1.2">ğ‘¡</ci><cn type="integer" id="S3.SS5.p4.10.m8.1.1.3.cmml" xref="S3.SS5.p4.10.m8.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p4.10.m8.1c">t-1</annotation></semantics></math> and the current error histogram. We show how to define a hierarchical loss based on the histogram in the following.</p>
</div>
<div id="S3.SS5.p5" class="ltx_para">
<p id="S3.SS5.p5.5" class="ltx_p"><span id="S3.SS5.p5.5.1" class="ltx_text ltx_font_bold">Hierarchical loss.</span>
We compute a weighting coefficient based on the error histogram, and use it on top of a Hamming distance to form our hierarchical loss with</p>
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.5" class="ltx_Math" alttext="w_{j}(t)=\exp(\sigma\cdot min\{H_{j}(t),0.5-H_{j}(t)\})," display="block"><semantics id="S3.E4.m1.5a"><mrow id="S3.E4.m1.5.5.1" xref="S3.E4.m1.5.5.1.1.cmml"><mrow id="S3.E4.m1.5.5.1.1" xref="S3.E4.m1.5.5.1.1.cmml"><mrow id="S3.E4.m1.5.5.1.1.3" xref="S3.E4.m1.5.5.1.1.3.cmml"><msub id="S3.E4.m1.5.5.1.1.3.2" xref="S3.E4.m1.5.5.1.1.3.2.cmml"><mi id="S3.E4.m1.5.5.1.1.3.2.2" xref="S3.E4.m1.5.5.1.1.3.2.2.cmml">w</mi><mi id="S3.E4.m1.5.5.1.1.3.2.3" xref="S3.E4.m1.5.5.1.1.3.2.3.cmml">j</mi></msub><mo lspace="0em" rspace="0em" id="S3.E4.m1.5.5.1.1.3.1" xref="S3.E4.m1.5.5.1.1.3.1.cmml">â€‹</mo><mrow id="S3.E4.m1.5.5.1.1.3.3.2" xref="S3.E4.m1.5.5.1.1.3.cmml"><mo stretchy="false" id="S3.E4.m1.5.5.1.1.3.3.2.1" xref="S3.E4.m1.5.5.1.1.3.cmml">(</mo><mi id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml">t</mi><mo stretchy="false" id="S3.E4.m1.5.5.1.1.3.3.2.2" xref="S3.E4.m1.5.5.1.1.3.cmml">)</mo></mrow></mrow><mo id="S3.E4.m1.5.5.1.1.2" xref="S3.E4.m1.5.5.1.1.2.cmml">=</mo><mrow id="S3.E4.m1.5.5.1.1.1.1" xref="S3.E4.m1.5.5.1.1.1.2.cmml"><mi id="S3.E4.m1.4.4" xref="S3.E4.m1.4.4.cmml">exp</mi><mo id="S3.E4.m1.5.5.1.1.1.1a" xref="S3.E4.m1.5.5.1.1.1.2.cmml">â¡</mo><mrow id="S3.E4.m1.5.5.1.1.1.1.1" xref="S3.E4.m1.5.5.1.1.1.2.cmml"><mo stretchy="false" id="S3.E4.m1.5.5.1.1.1.1.1.2" xref="S3.E4.m1.5.5.1.1.1.2.cmml">(</mo><mrow id="S3.E4.m1.5.5.1.1.1.1.1.1" xref="S3.E4.m1.5.5.1.1.1.1.1.1.cmml"><mrow id="S3.E4.m1.5.5.1.1.1.1.1.1.4" xref="S3.E4.m1.5.5.1.1.1.1.1.1.4.cmml"><mi id="S3.E4.m1.5.5.1.1.1.1.1.1.4.2" xref="S3.E4.m1.5.5.1.1.1.1.1.1.4.2.cmml">Ïƒ</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E4.m1.5.5.1.1.1.1.1.1.4.1" xref="S3.E4.m1.5.5.1.1.1.1.1.1.4.1.cmml">â‹…</mo><mi id="S3.E4.m1.5.5.1.1.1.1.1.1.4.3" xref="S3.E4.m1.5.5.1.1.1.1.1.1.4.3.cmml">m</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E4.m1.5.5.1.1.1.1.1.1.3" xref="S3.E4.m1.5.5.1.1.1.1.1.1.3.cmml">â€‹</mo><mi id="S3.E4.m1.5.5.1.1.1.1.1.1.5" xref="S3.E4.m1.5.5.1.1.1.1.1.1.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.5.5.1.1.1.1.1.1.3a" xref="S3.E4.m1.5.5.1.1.1.1.1.1.3.cmml">â€‹</mo><mi id="S3.E4.m1.5.5.1.1.1.1.1.1.6" xref="S3.E4.m1.5.5.1.1.1.1.1.1.6.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.5.5.1.1.1.1.1.1.3b" xref="S3.E4.m1.5.5.1.1.1.1.1.1.3.cmml">â€‹</mo><mrow id="S3.E4.m1.5.5.1.1.1.1.1.1.2.2" xref="S3.E4.m1.5.5.1.1.1.1.1.1.2.3.cmml"><mo stretchy="false" id="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.3" xref="S3.E4.m1.5.5.1.1.1.1.1.1.2.3.cmml">{</mo><mrow id="S3.E4.m1.5.5.1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.5.5.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E4.m1.5.5.1.1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.5.5.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E4.m1.5.5.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E4.m1.5.5.1.1.1.1.1.1.1.1.1.2.2.cmml">H</mi><mi id="S3.E4.m1.5.5.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E4.m1.5.5.1.1.1.1.1.1.1.1.1.2.3.cmml">j</mi></msub><mo lspace="0em" rspace="0em" id="S3.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.cmml">â€‹</mo><mrow id="S3.E4.m1.5.5.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E4.m1.5.5.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E4.m1.5.5.1.1.1.1.1.1.1.1.1.3.2.1" xref="S3.E4.m1.5.5.1.1.1.1.1.1.1.1.1.cmml">(</mo><mi id="S3.E4.m1.2.2" xref="S3.E4.m1.2.2.cmml">t</mi><mo stretchy="false" id="S3.E4.m1.5.5.1.1.1.1.1.1.1.1.1.3.2.2" xref="S3.E4.m1.5.5.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.4" xref="S3.E4.m1.5.5.1.1.1.1.1.1.2.3.cmml">,</mo><mrow id="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.2" xref="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.2.cmml"><mn id="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.2.2" xref="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.2.2.cmml">0.5</mn><mo id="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.2.1" xref="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.2.1.cmml">âˆ’</mo><mrow id="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.2.3" xref="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.2.3.cmml"><msub id="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.2.3.2" xref="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.2.3.2.cmml"><mi id="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.2.3.2.2" xref="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.2.3.2.2.cmml">H</mi><mi id="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.2.3.2.3" xref="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.2.3.2.3.cmml">j</mi></msub><mo lspace="0em" rspace="0em" id="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.2.3.1" xref="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.2.3.1.cmml">â€‹</mo><mrow id="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.2.3.3.2" xref="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.2.3.cmml"><mo stretchy="false" id="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.2.3.3.2.1" xref="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.2.3.cmml">(</mo><mi id="S3.E4.m1.3.3" xref="S3.E4.m1.3.3.cmml">t</mi><mo stretchy="false" id="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.2.3.3.2.2" xref="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.5" xref="S3.E4.m1.5.5.1.1.1.1.1.1.2.3.cmml">}</mo></mrow></mrow><mo stretchy="false" id="S3.E4.m1.5.5.1.1.1.1.1.3" xref="S3.E4.m1.5.5.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E4.m1.5.5.1.2" xref="S3.E4.m1.5.5.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.5b"><apply id="S3.E4.m1.5.5.1.1.cmml" xref="S3.E4.m1.5.5.1"><eq id="S3.E4.m1.5.5.1.1.2.cmml" xref="S3.E4.m1.5.5.1.1.2"></eq><apply id="S3.E4.m1.5.5.1.1.3.cmml" xref="S3.E4.m1.5.5.1.1.3"><times id="S3.E4.m1.5.5.1.1.3.1.cmml" xref="S3.E4.m1.5.5.1.1.3.1"></times><apply id="S3.E4.m1.5.5.1.1.3.2.cmml" xref="S3.E4.m1.5.5.1.1.3.2"><csymbol cd="ambiguous" id="S3.E4.m1.5.5.1.1.3.2.1.cmml" xref="S3.E4.m1.5.5.1.1.3.2">subscript</csymbol><ci id="S3.E4.m1.5.5.1.1.3.2.2.cmml" xref="S3.E4.m1.5.5.1.1.3.2.2">ğ‘¤</ci><ci id="S3.E4.m1.5.5.1.1.3.2.3.cmml" xref="S3.E4.m1.5.5.1.1.3.2.3">ğ‘—</ci></apply><ci id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1">ğ‘¡</ci></apply><apply id="S3.E4.m1.5.5.1.1.1.2.cmml" xref="S3.E4.m1.5.5.1.1.1.1"><exp id="S3.E4.m1.4.4.cmml" xref="S3.E4.m1.4.4"></exp><apply id="S3.E4.m1.5.5.1.1.1.1.1.1.cmml" xref="S3.E4.m1.5.5.1.1.1.1.1.1"><times id="S3.E4.m1.5.5.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.5.5.1.1.1.1.1.1.3"></times><apply id="S3.E4.m1.5.5.1.1.1.1.1.1.4.cmml" xref="S3.E4.m1.5.5.1.1.1.1.1.1.4"><ci id="S3.E4.m1.5.5.1.1.1.1.1.1.4.1.cmml" xref="S3.E4.m1.5.5.1.1.1.1.1.1.4.1">â‹…</ci><ci id="S3.E4.m1.5.5.1.1.1.1.1.1.4.2.cmml" xref="S3.E4.m1.5.5.1.1.1.1.1.1.4.2">ğœ</ci><ci id="S3.E4.m1.5.5.1.1.1.1.1.1.4.3.cmml" xref="S3.E4.m1.5.5.1.1.1.1.1.1.4.3">ğ‘š</ci></apply><ci id="S3.E4.m1.5.5.1.1.1.1.1.1.5.cmml" xref="S3.E4.m1.5.5.1.1.1.1.1.1.5">ğ‘–</ci><ci id="S3.E4.m1.5.5.1.1.1.1.1.1.6.cmml" xref="S3.E4.m1.5.5.1.1.1.1.1.1.6">ğ‘›</ci><set id="S3.E4.m1.5.5.1.1.1.1.1.1.2.3.cmml" xref="S3.E4.m1.5.5.1.1.1.1.1.1.2.2"><apply id="S3.E4.m1.5.5.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.5.5.1.1.1.1.1.1.1.1.1"><times id="S3.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1"></times><apply id="S3.E4.m1.5.5.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.5.5.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.5.5.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.5.5.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E4.m1.5.5.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E4.m1.5.5.1.1.1.1.1.1.1.1.1.2.2">ğ»</ci><ci id="S3.E4.m1.5.5.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E4.m1.5.5.1.1.1.1.1.1.1.1.1.2.3">ğ‘—</ci></apply><ci id="S3.E4.m1.2.2.cmml" xref="S3.E4.m1.2.2">ğ‘¡</ci></apply><apply id="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.2"><minus id="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.2.1"></minus><cn type="float" id="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.2.2">0.5</cn><apply id="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.2.3"><times id="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.2.3.1.cmml" xref="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.2.3.1"></times><apply id="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.2.3.2.cmml" xref="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.2.3.2"><csymbol cd="ambiguous" id="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.2.3.2.1.cmml" xref="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.2.3.2">subscript</csymbol><ci id="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.2.3.2.2.cmml" xref="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.2.3.2.2">ğ»</ci><ci id="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.2.3.2.3.cmml" xref="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.2.3.2.3">ğ‘—</ci></apply><ci id="S3.E4.m1.3.3.cmml" xref="S3.E4.m1.3.3">ğ‘¡</ci></apply></apply></set></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.5c">w_{j}(t)=\exp(\sigma\cdot min\{H_{j}(t),0.5-H_{j}(t)\}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S3.SS5.p5.4" class="ltx_p">where the function <math id="S3.SS5.p5.1.m1.1" class="ltx_Math" alttext="w" display="inline"><semantics id="S3.SS5.p5.1.m1.1a"><mi id="S3.SS5.p5.1.m1.1.1" xref="S3.SS5.p5.1.m1.1.1.cmml">w</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p5.1.m1.1b"><ci id="S3.SS5.p5.1.m1.1.1.cmml" xref="S3.SS5.p5.1.m1.1.1">ğ‘¤</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p5.1.m1.1c">w</annotation></semantics></math> uses an exponential term to softly define a weight for <math id="S3.SS5.p5.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS5.p5.2.m2.1a"><mi id="S3.SS5.p5.2.m2.1.1" xref="S3.SS5.p5.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p5.2.m2.1b"><ci id="S3.SS5.p5.2.m2.1.1.cmml" xref="S3.SS5.p5.2.m2.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p5.2.m2.1c">n</annotation></semantics></math>-th bit at training step <math id="S3.SS5.p5.3.m3.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS5.p5.3.m3.1a"><mi id="S3.SS5.p5.3.m3.1.1" xref="S3.SS5.p5.3.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p5.3.m3.1b"><ci id="S3.SS5.p5.3.m3.1.1.cmml" xref="S3.SS5.p5.3.m3.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p5.3.m3.1c">t</annotation></semantics></math>, <math id="S3.SS5.p5.4.m4.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S3.SS5.p5.4.m4.1a"><mi id="S3.SS5.p5.4.m4.1.1" xref="S3.SS5.p5.4.m4.1.1.cmml">Ïƒ</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p5.4.m4.1b"><ci id="S3.SS5.p5.4.m4.1.1.cmml" xref="S3.SS5.p5.4.m4.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p5.4.m4.1c">\sigma</annotation></semantics></math> is a constant. All object pixels in the mini-batch share the same weighting coefficients. We normalize the weights across all bits. We then define our hierarchical loss based on the weighting function of active bits and Hamming distance as below:</p>
<table id="S3.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E5.m1.3" class="ltx_Math" alttext="L_{hier}=\sum_{j=1}^{d}{w_{j}\cdot\text{$Hamm$}(b_{j},\hat{p_{j}})}." display="block"><semantics id="S3.E5.m1.3a"><mrow id="S3.E5.m1.3.3.1" xref="S3.E5.m1.3.3.1.1.cmml"><mrow id="S3.E5.m1.3.3.1.1" xref="S3.E5.m1.3.3.1.1.cmml"><msub id="S3.E5.m1.3.3.1.1.3" xref="S3.E5.m1.3.3.1.1.3.cmml"><mi id="S3.E5.m1.3.3.1.1.3.2" xref="S3.E5.m1.3.3.1.1.3.2.cmml">L</mi><mrow id="S3.E5.m1.3.3.1.1.3.3" xref="S3.E5.m1.3.3.1.1.3.3.cmml"><mi id="S3.E5.m1.3.3.1.1.3.3.2" xref="S3.E5.m1.3.3.1.1.3.3.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.3.3.1.1.3.3.1" xref="S3.E5.m1.3.3.1.1.3.3.1.cmml">â€‹</mo><mi id="S3.E5.m1.3.3.1.1.3.3.3" xref="S3.E5.m1.3.3.1.1.3.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.3.3.1.1.3.3.1a" xref="S3.E5.m1.3.3.1.1.3.3.1.cmml">â€‹</mo><mi id="S3.E5.m1.3.3.1.1.3.3.4" xref="S3.E5.m1.3.3.1.1.3.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.3.3.1.1.3.3.1b" xref="S3.E5.m1.3.3.1.1.3.3.1.cmml">â€‹</mo><mi id="S3.E5.m1.3.3.1.1.3.3.5" xref="S3.E5.m1.3.3.1.1.3.3.5.cmml">r</mi></mrow></msub><mo rspace="0.111em" id="S3.E5.m1.3.3.1.1.2" xref="S3.E5.m1.3.3.1.1.2.cmml">=</mo><mrow id="S3.E5.m1.3.3.1.1.1" xref="S3.E5.m1.3.3.1.1.1.cmml"><munderover id="S3.E5.m1.3.3.1.1.1.2" xref="S3.E5.m1.3.3.1.1.1.2.cmml"><mo movablelimits="false" id="S3.E5.m1.3.3.1.1.1.2.2.2" xref="S3.E5.m1.3.3.1.1.1.2.2.2.cmml">âˆ‘</mo><mrow id="S3.E5.m1.3.3.1.1.1.2.2.3" xref="S3.E5.m1.3.3.1.1.1.2.2.3.cmml"><mi id="S3.E5.m1.3.3.1.1.1.2.2.3.2" xref="S3.E5.m1.3.3.1.1.1.2.2.3.2.cmml">j</mi><mo id="S3.E5.m1.3.3.1.1.1.2.2.3.1" xref="S3.E5.m1.3.3.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E5.m1.3.3.1.1.1.2.2.3.3" xref="S3.E5.m1.3.3.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E5.m1.3.3.1.1.1.2.3" xref="S3.E5.m1.3.3.1.1.1.2.3.cmml">d</mi></munderover><mrow id="S3.E5.m1.3.3.1.1.1.1" xref="S3.E5.m1.3.3.1.1.1.1.cmml"><mrow id="S3.E5.m1.3.3.1.1.1.1.3" xref="S3.E5.m1.3.3.1.1.1.1.3.cmml"><msub id="S3.E5.m1.3.3.1.1.1.1.3.2" xref="S3.E5.m1.3.3.1.1.1.1.3.2.cmml"><mi id="S3.E5.m1.3.3.1.1.1.1.3.2.2" xref="S3.E5.m1.3.3.1.1.1.1.3.2.2.cmml">w</mi><mi id="S3.E5.m1.3.3.1.1.1.1.3.2.3" xref="S3.E5.m1.3.3.1.1.1.1.3.2.3.cmml">j</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E5.m1.3.3.1.1.1.1.3.1" xref="S3.E5.m1.3.3.1.1.1.1.3.1.cmml">â‹…</mo><mrow id="S3.E5.m1.1.1.1" xref="S3.E5.m1.1.1.1.cmml"><mi id="S3.E5.m1.1.1.1.3" xref="S3.E5.m1.1.1.1.3.cmml">H</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.1.1.1.2" xref="S3.E5.m1.1.1.1.2.cmml">â€‹</mo><mi id="S3.E5.m1.1.1.1.4" xref="S3.E5.m1.1.1.1.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.1.1.1.2a" xref="S3.E5.m1.1.1.1.2.cmml">â€‹</mo><mi id="S3.E5.m1.1.1.1.5" xref="S3.E5.m1.1.1.1.5.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.1.1.1.2b" xref="S3.E5.m1.1.1.1.2.cmml">â€‹</mo><mi id="S3.E5.m1.1.1.1.6" xref="S3.E5.m1.1.1.1.6.cmml">m</mi></mrow></mrow><mo lspace="0em" rspace="0em" id="S3.E5.m1.3.3.1.1.1.1.2" xref="S3.E5.m1.3.3.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E5.m1.3.3.1.1.1.1.1.1" xref="S3.E5.m1.3.3.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E5.m1.3.3.1.1.1.1.1.1.2" xref="S3.E5.m1.3.3.1.1.1.1.1.2.cmml">(</mo><msub id="S3.E5.m1.3.3.1.1.1.1.1.1.1" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.cmml"><mi id="S3.E5.m1.3.3.1.1.1.1.1.1.1.2" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.2.cmml">b</mi><mi id="S3.E5.m1.3.3.1.1.1.1.1.1.1.3" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.3.cmml">j</mi></msub><mo id="S3.E5.m1.3.3.1.1.1.1.1.1.3" xref="S3.E5.m1.3.3.1.1.1.1.1.2.cmml">,</mo><mover accent="true" id="S3.E5.m1.2.2" xref="S3.E5.m1.2.2.cmml"><msub id="S3.E5.m1.2.2.2" xref="S3.E5.m1.2.2.2.cmml"><mi id="S3.E5.m1.2.2.2.2" xref="S3.E5.m1.2.2.2.2.cmml">p</mi><mi id="S3.E5.m1.2.2.2.3" xref="S3.E5.m1.2.2.2.3.cmml">j</mi></msub><mo id="S3.E5.m1.2.2.1" xref="S3.E5.m1.2.2.1.cmml">^</mo></mover><mo stretchy="false" id="S3.E5.m1.3.3.1.1.1.1.1.1.4" xref="S3.E5.m1.3.3.1.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em" id="S3.E5.m1.3.3.1.2" xref="S3.E5.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.3b"><apply id="S3.E5.m1.3.3.1.1.cmml" xref="S3.E5.m1.3.3.1"><eq id="S3.E5.m1.3.3.1.1.2.cmml" xref="S3.E5.m1.3.3.1.1.2"></eq><apply id="S3.E5.m1.3.3.1.1.3.cmml" xref="S3.E5.m1.3.3.1.1.3"><csymbol cd="ambiguous" id="S3.E5.m1.3.3.1.1.3.1.cmml" xref="S3.E5.m1.3.3.1.1.3">subscript</csymbol><ci id="S3.E5.m1.3.3.1.1.3.2.cmml" xref="S3.E5.m1.3.3.1.1.3.2">ğ¿</ci><apply id="S3.E5.m1.3.3.1.1.3.3.cmml" xref="S3.E5.m1.3.3.1.1.3.3"><times id="S3.E5.m1.3.3.1.1.3.3.1.cmml" xref="S3.E5.m1.3.3.1.1.3.3.1"></times><ci id="S3.E5.m1.3.3.1.1.3.3.2.cmml" xref="S3.E5.m1.3.3.1.1.3.3.2">â„</ci><ci id="S3.E5.m1.3.3.1.1.3.3.3.cmml" xref="S3.E5.m1.3.3.1.1.3.3.3">ğ‘–</ci><ci id="S3.E5.m1.3.3.1.1.3.3.4.cmml" xref="S3.E5.m1.3.3.1.1.3.3.4">ğ‘’</ci><ci id="S3.E5.m1.3.3.1.1.3.3.5.cmml" xref="S3.E5.m1.3.3.1.1.3.3.5">ğ‘Ÿ</ci></apply></apply><apply id="S3.E5.m1.3.3.1.1.1.cmml" xref="S3.E5.m1.3.3.1.1.1"><apply id="S3.E5.m1.3.3.1.1.1.2.cmml" xref="S3.E5.m1.3.3.1.1.1.2"><csymbol cd="ambiguous" id="S3.E5.m1.3.3.1.1.1.2.1.cmml" xref="S3.E5.m1.3.3.1.1.1.2">superscript</csymbol><apply id="S3.E5.m1.3.3.1.1.1.2.2.cmml" xref="S3.E5.m1.3.3.1.1.1.2"><csymbol cd="ambiguous" id="S3.E5.m1.3.3.1.1.1.2.2.1.cmml" xref="S3.E5.m1.3.3.1.1.1.2">subscript</csymbol><sum id="S3.E5.m1.3.3.1.1.1.2.2.2.cmml" xref="S3.E5.m1.3.3.1.1.1.2.2.2"></sum><apply id="S3.E5.m1.3.3.1.1.1.2.2.3.cmml" xref="S3.E5.m1.3.3.1.1.1.2.2.3"><eq id="S3.E5.m1.3.3.1.1.1.2.2.3.1.cmml" xref="S3.E5.m1.3.3.1.1.1.2.2.3.1"></eq><ci id="S3.E5.m1.3.3.1.1.1.2.2.3.2.cmml" xref="S3.E5.m1.3.3.1.1.1.2.2.3.2">ğ‘—</ci><cn type="integer" id="S3.E5.m1.3.3.1.1.1.2.2.3.3.cmml" xref="S3.E5.m1.3.3.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E5.m1.3.3.1.1.1.2.3.cmml" xref="S3.E5.m1.3.3.1.1.1.2.3">ğ‘‘</ci></apply><apply id="S3.E5.m1.3.3.1.1.1.1.cmml" xref="S3.E5.m1.3.3.1.1.1.1"><times id="S3.E5.m1.3.3.1.1.1.1.2.cmml" xref="S3.E5.m1.3.3.1.1.1.1.2"></times><apply id="S3.E5.m1.3.3.1.1.1.1.3.cmml" xref="S3.E5.m1.3.3.1.1.1.1.3"><ci id="S3.E5.m1.3.3.1.1.1.1.3.1.cmml" xref="S3.E5.m1.3.3.1.1.1.1.3.1">â‹…</ci><apply id="S3.E5.m1.3.3.1.1.1.1.3.2.cmml" xref="S3.E5.m1.3.3.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E5.m1.3.3.1.1.1.1.3.2.1.cmml" xref="S3.E5.m1.3.3.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E5.m1.3.3.1.1.1.1.3.2.2.cmml" xref="S3.E5.m1.3.3.1.1.1.1.3.2.2">ğ‘¤</ci><ci id="S3.E5.m1.3.3.1.1.1.1.3.2.3.cmml" xref="S3.E5.m1.3.3.1.1.1.1.3.2.3">ğ‘—</ci></apply><apply id="S3.E5.m1.1.1.1.cmml" xref="S3.E5.m1.1.1.1"><times id="S3.E5.m1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.2"></times><ci id="S3.E5.m1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.3">ğ»</ci><ci id="S3.E5.m1.1.1.1.4.cmml" xref="S3.E5.m1.1.1.1.4">ğ‘</ci><ci id="S3.E5.m1.1.1.1.5.cmml" xref="S3.E5.m1.1.1.1.5">ğ‘š</ci><ci id="S3.E5.m1.1.1.1.6.cmml" xref="S3.E5.m1.1.1.1.6">ğ‘š</ci></apply></apply><interval closure="open" id="S3.E5.m1.3.3.1.1.1.1.1.2.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1"><apply id="S3.E5.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E5.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.2">ğ‘</ci><ci id="S3.E5.m1.3.3.1.1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.3">ğ‘—</ci></apply><apply id="S3.E5.m1.2.2.cmml" xref="S3.E5.m1.2.2"><ci id="S3.E5.m1.2.2.1.cmml" xref="S3.E5.m1.2.2.1">^</ci><apply id="S3.E5.m1.2.2.2.cmml" xref="S3.E5.m1.2.2.2"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.2.1.cmml" xref="S3.E5.m1.2.2.2">subscript</csymbol><ci id="S3.E5.m1.2.2.2.2.cmml" xref="S3.E5.m1.2.2.2.2">ğ‘</ci><ci id="S3.E5.m1.2.2.2.3.cmml" xref="S3.E5.m1.2.2.2.3">ğ‘—</ci></apply></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.3c">L_{hier}=\sum_{j=1}^{d}{w_{j}\cdot\text{$Hamm$}(b_{j},\hat{p_{j}})}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p id="S3.SS5.p5.6" class="ltx_p">With this loss we focus mainly on active bits which automatically change from coarse to fine during training. 
<br class="ltx_break"></p>
</div>
<div id="S3.SS5.p6" class="ltx_para">
<p id="S3.SS5.p6.5" class="ltx_p"><span id="S3.SS5.p6.5.1" class="ltx_text ltx_font_bold">Total loss to train the CNN.</span> We weight the <math id="S3.SS5.p6.1.m1.1" class="ltx_Math" alttext="L_{mask}" display="inline"><semantics id="S3.SS5.p6.1.m1.1a"><msub id="S3.SS5.p6.1.m1.1.1" xref="S3.SS5.p6.1.m1.1.1.cmml"><mi id="S3.SS5.p6.1.m1.1.1.2" xref="S3.SS5.p6.1.m1.1.1.2.cmml">L</mi><mrow id="S3.SS5.p6.1.m1.1.1.3" xref="S3.SS5.p6.1.m1.1.1.3.cmml"><mi id="S3.SS5.p6.1.m1.1.1.3.2" xref="S3.SS5.p6.1.m1.1.1.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p6.1.m1.1.1.3.1" xref="S3.SS5.p6.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS5.p6.1.m1.1.1.3.3" xref="S3.SS5.p6.1.m1.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p6.1.m1.1.1.3.1a" xref="S3.SS5.p6.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS5.p6.1.m1.1.1.3.4" xref="S3.SS5.p6.1.m1.1.1.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p6.1.m1.1.1.3.1b" xref="S3.SS5.p6.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS5.p6.1.m1.1.1.3.5" xref="S3.SS5.p6.1.m1.1.1.3.5.cmml">k</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p6.1.m1.1b"><apply id="S3.SS5.p6.1.m1.1.1.cmml" xref="S3.SS5.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS5.p6.1.m1.1.1.1.cmml" xref="S3.SS5.p6.1.m1.1.1">subscript</csymbol><ci id="S3.SS5.p6.1.m1.1.1.2.cmml" xref="S3.SS5.p6.1.m1.1.1.2">ğ¿</ci><apply id="S3.SS5.p6.1.m1.1.1.3.cmml" xref="S3.SS5.p6.1.m1.1.1.3"><times id="S3.SS5.p6.1.m1.1.1.3.1.cmml" xref="S3.SS5.p6.1.m1.1.1.3.1"></times><ci id="S3.SS5.p6.1.m1.1.1.3.2.cmml" xref="S3.SS5.p6.1.m1.1.1.3.2">ğ‘š</ci><ci id="S3.SS5.p6.1.m1.1.1.3.3.cmml" xref="S3.SS5.p6.1.m1.1.1.3.3">ğ‘</ci><ci id="S3.SS5.p6.1.m1.1.1.3.4.cmml" xref="S3.SS5.p6.1.m1.1.1.3.4">ğ‘ </ci><ci id="S3.SS5.p6.1.m1.1.1.3.5.cmml" xref="S3.SS5.p6.1.m1.1.1.3.5">ğ‘˜</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p6.1.m1.1c">L_{mask}</annotation></semantics></math> and <math id="S3.SS5.p6.2.m2.1" class="ltx_Math" alttext="L_{hier}" display="inline"><semantics id="S3.SS5.p6.2.m2.1a"><msub id="S3.SS5.p6.2.m2.1.1" xref="S3.SS5.p6.2.m2.1.1.cmml"><mi id="S3.SS5.p6.2.m2.1.1.2" xref="S3.SS5.p6.2.m2.1.1.2.cmml">L</mi><mrow id="S3.SS5.p6.2.m2.1.1.3" xref="S3.SS5.p6.2.m2.1.1.3.cmml"><mi id="S3.SS5.p6.2.m2.1.1.3.2" xref="S3.SS5.p6.2.m2.1.1.3.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p6.2.m2.1.1.3.1" xref="S3.SS5.p6.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS5.p6.2.m2.1.1.3.3" xref="S3.SS5.p6.2.m2.1.1.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p6.2.m2.1.1.3.1a" xref="S3.SS5.p6.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS5.p6.2.m2.1.1.3.4" xref="S3.SS5.p6.2.m2.1.1.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p6.2.m2.1.1.3.1b" xref="S3.SS5.p6.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS5.p6.2.m2.1.1.3.5" xref="S3.SS5.p6.2.m2.1.1.3.5.cmml">r</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p6.2.m2.1b"><apply id="S3.SS5.p6.2.m2.1.1.cmml" xref="S3.SS5.p6.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS5.p6.2.m2.1.1.1.cmml" xref="S3.SS5.p6.2.m2.1.1">subscript</csymbol><ci id="S3.SS5.p6.2.m2.1.1.2.cmml" xref="S3.SS5.p6.2.m2.1.1.2">ğ¿</ci><apply id="S3.SS5.p6.2.m2.1.1.3.cmml" xref="S3.SS5.p6.2.m2.1.1.3"><times id="S3.SS5.p6.2.m2.1.1.3.1.cmml" xref="S3.SS5.p6.2.m2.1.1.3.1"></times><ci id="S3.SS5.p6.2.m2.1.1.3.2.cmml" xref="S3.SS5.p6.2.m2.1.1.3.2">â„</ci><ci id="S3.SS5.p6.2.m2.1.1.3.3.cmml" xref="S3.SS5.p6.2.m2.1.1.3.3">ğ‘–</ci><ci id="S3.SS5.p6.2.m2.1.1.3.4.cmml" xref="S3.SS5.p6.2.m2.1.1.3.4">ğ‘’</ci><ci id="S3.SS5.p6.2.m2.1.1.3.5.cmml" xref="S3.SS5.p6.2.m2.1.1.3.5">ğ‘Ÿ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p6.2.m2.1c">L_{hier}</annotation></semantics></math> with a hyper-parameter <math id="S3.SS5.p6.3.m3.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS5.p6.3.m3.1a"><mi id="S3.SS5.p6.3.m3.1.1" xref="S3.SS5.p6.3.m3.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p6.3.m3.1b"><ci id="S3.SS5.p6.3.m3.1.1.cmml" xref="S3.SS5.p6.3.m3.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p6.3.m3.1c">\alpha</annotation></semantics></math> (<math id="S3.SS5.p6.4.m4.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS5.p6.4.m4.1a"><mi id="S3.SS5.p6.4.m4.1.1" xref="S3.SS5.p6.4.m4.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p6.4.m4.1b"><ci id="S3.SS5.p6.4.m4.1.1.cmml" xref="S3.SS5.p6.4.m4.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p6.4.m4.1c">\alpha</annotation></semantics></math> set as <math id="S3.SS5.p6.5.m5.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S3.SS5.p6.5.m5.1a"><mn id="S3.SS5.p6.5.m5.1.1" xref="S3.SS5.p6.5.m5.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S3.SS5.p6.5.m5.1b"><cn type="integer" id="S3.SS5.p6.5.m5.1.1.cmml" xref="S3.SS5.p6.5.m5.1.1">0</cn></annotation-xml></semantics></math> for pixels predicted as background), the total per pixel loss can be mathematically expressed as</p>
<table id="S3.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E6.m1.1" class="ltx_Math" alttext="L_{total}=L_{mask}+\alpha\cdot L_{hier}." display="block"><semantics id="S3.E6.m1.1a"><mrow id="S3.E6.m1.1.1.1" xref="S3.E6.m1.1.1.1.1.cmml"><mrow id="S3.E6.m1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.cmml"><msub id="S3.E6.m1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.2.cmml"><mi id="S3.E6.m1.1.1.1.1.2.2" xref="S3.E6.m1.1.1.1.1.2.2.cmml">L</mi><mrow id="S3.E6.m1.1.1.1.1.2.3" xref="S3.E6.m1.1.1.1.1.2.3.cmml"><mi id="S3.E6.m1.1.1.1.1.2.3.2" xref="S3.E6.m1.1.1.1.1.2.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.2.3.1" xref="S3.E6.m1.1.1.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.E6.m1.1.1.1.1.2.3.3" xref="S3.E6.m1.1.1.1.1.2.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.2.3.1a" xref="S3.E6.m1.1.1.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.E6.m1.1.1.1.1.2.3.4" xref="S3.E6.m1.1.1.1.1.2.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.2.3.1b" xref="S3.E6.m1.1.1.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.E6.m1.1.1.1.1.2.3.5" xref="S3.E6.m1.1.1.1.1.2.3.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.2.3.1c" xref="S3.E6.m1.1.1.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.E6.m1.1.1.1.1.2.3.6" xref="S3.E6.m1.1.1.1.1.2.3.6.cmml">l</mi></mrow></msub><mo id="S3.E6.m1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.cmml">=</mo><mrow id="S3.E6.m1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.3.cmml"><msub id="S3.E6.m1.1.1.1.1.3.2" xref="S3.E6.m1.1.1.1.1.3.2.cmml"><mi id="S3.E6.m1.1.1.1.1.3.2.2" xref="S3.E6.m1.1.1.1.1.3.2.2.cmml">L</mi><mrow id="S3.E6.m1.1.1.1.1.3.2.3" xref="S3.E6.m1.1.1.1.1.3.2.3.cmml"><mi id="S3.E6.m1.1.1.1.1.3.2.3.2" xref="S3.E6.m1.1.1.1.1.3.2.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.3.2.3.1" xref="S3.E6.m1.1.1.1.1.3.2.3.1.cmml">â€‹</mo><mi id="S3.E6.m1.1.1.1.1.3.2.3.3" xref="S3.E6.m1.1.1.1.1.3.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.3.2.3.1a" xref="S3.E6.m1.1.1.1.1.3.2.3.1.cmml">â€‹</mo><mi id="S3.E6.m1.1.1.1.1.3.2.3.4" xref="S3.E6.m1.1.1.1.1.3.2.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.3.2.3.1b" xref="S3.E6.m1.1.1.1.1.3.2.3.1.cmml">â€‹</mo><mi id="S3.E6.m1.1.1.1.1.3.2.3.5" xref="S3.E6.m1.1.1.1.1.3.2.3.5.cmml">k</mi></mrow></msub><mo id="S3.E6.m1.1.1.1.1.3.1" xref="S3.E6.m1.1.1.1.1.3.1.cmml">+</mo><mrow id="S3.E6.m1.1.1.1.1.3.3" xref="S3.E6.m1.1.1.1.1.3.3.cmml"><mi id="S3.E6.m1.1.1.1.1.3.3.2" xref="S3.E6.m1.1.1.1.1.3.3.2.cmml">Î±</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E6.m1.1.1.1.1.3.3.1" xref="S3.E6.m1.1.1.1.1.3.3.1.cmml">â‹…</mo><msub id="S3.E6.m1.1.1.1.1.3.3.3" xref="S3.E6.m1.1.1.1.1.3.3.3.cmml"><mi id="S3.E6.m1.1.1.1.1.3.3.3.2" xref="S3.E6.m1.1.1.1.1.3.3.3.2.cmml">L</mi><mrow id="S3.E6.m1.1.1.1.1.3.3.3.3" xref="S3.E6.m1.1.1.1.1.3.3.3.3.cmml"><mi id="S3.E6.m1.1.1.1.1.3.3.3.3.2" xref="S3.E6.m1.1.1.1.1.3.3.3.3.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.3.3.3.3.1" xref="S3.E6.m1.1.1.1.1.3.3.3.3.1.cmml">â€‹</mo><mi id="S3.E6.m1.1.1.1.1.3.3.3.3.3" xref="S3.E6.m1.1.1.1.1.3.3.3.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.3.3.3.3.1a" xref="S3.E6.m1.1.1.1.1.3.3.3.3.1.cmml">â€‹</mo><mi id="S3.E6.m1.1.1.1.1.3.3.3.3.4" xref="S3.E6.m1.1.1.1.1.3.3.3.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.3.3.3.3.1b" xref="S3.E6.m1.1.1.1.1.3.3.3.3.1.cmml">â€‹</mo><mi id="S3.E6.m1.1.1.1.1.3.3.3.3.5" xref="S3.E6.m1.1.1.1.1.3.3.3.3.5.cmml">r</mi></mrow></msub></mrow></mrow></mrow><mo lspace="0em" id="S3.E6.m1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m1.1b"><apply id="S3.E6.m1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1"><eq id="S3.E6.m1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1"></eq><apply id="S3.E6.m1.1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.2.1.cmml" xref="S3.E6.m1.1.1.1.1.2">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.2.2.cmml" xref="S3.E6.m1.1.1.1.1.2.2">ğ¿</ci><apply id="S3.E6.m1.1.1.1.1.2.3.cmml" xref="S3.E6.m1.1.1.1.1.2.3"><times id="S3.E6.m1.1.1.1.1.2.3.1.cmml" xref="S3.E6.m1.1.1.1.1.2.3.1"></times><ci id="S3.E6.m1.1.1.1.1.2.3.2.cmml" xref="S3.E6.m1.1.1.1.1.2.3.2">ğ‘¡</ci><ci id="S3.E6.m1.1.1.1.1.2.3.3.cmml" xref="S3.E6.m1.1.1.1.1.2.3.3">ğ‘œ</ci><ci id="S3.E6.m1.1.1.1.1.2.3.4.cmml" xref="S3.E6.m1.1.1.1.1.2.3.4">ğ‘¡</ci><ci id="S3.E6.m1.1.1.1.1.2.3.5.cmml" xref="S3.E6.m1.1.1.1.1.2.3.5">ğ‘</ci><ci id="S3.E6.m1.1.1.1.1.2.3.6.cmml" xref="S3.E6.m1.1.1.1.1.2.3.6">ğ‘™</ci></apply></apply><apply id="S3.E6.m1.1.1.1.1.3.cmml" xref="S3.E6.m1.1.1.1.1.3"><plus id="S3.E6.m1.1.1.1.1.3.1.cmml" xref="S3.E6.m1.1.1.1.1.3.1"></plus><apply id="S3.E6.m1.1.1.1.1.3.2.cmml" xref="S3.E6.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.3.2.1.cmml" xref="S3.E6.m1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.3.2.2.cmml" xref="S3.E6.m1.1.1.1.1.3.2.2">ğ¿</ci><apply id="S3.E6.m1.1.1.1.1.3.2.3.cmml" xref="S3.E6.m1.1.1.1.1.3.2.3"><times id="S3.E6.m1.1.1.1.1.3.2.3.1.cmml" xref="S3.E6.m1.1.1.1.1.3.2.3.1"></times><ci id="S3.E6.m1.1.1.1.1.3.2.3.2.cmml" xref="S3.E6.m1.1.1.1.1.3.2.3.2">ğ‘š</ci><ci id="S3.E6.m1.1.1.1.1.3.2.3.3.cmml" xref="S3.E6.m1.1.1.1.1.3.2.3.3">ğ‘</ci><ci id="S3.E6.m1.1.1.1.1.3.2.3.4.cmml" xref="S3.E6.m1.1.1.1.1.3.2.3.4">ğ‘ </ci><ci id="S3.E6.m1.1.1.1.1.3.2.3.5.cmml" xref="S3.E6.m1.1.1.1.1.3.2.3.5">ğ‘˜</ci></apply></apply><apply id="S3.E6.m1.1.1.1.1.3.3.cmml" xref="S3.E6.m1.1.1.1.1.3.3"><ci id="S3.E6.m1.1.1.1.1.3.3.1.cmml" xref="S3.E6.m1.1.1.1.1.3.3.1">â‹…</ci><ci id="S3.E6.m1.1.1.1.1.3.3.2.cmml" xref="S3.E6.m1.1.1.1.1.3.3.2">ğ›¼</ci><apply id="S3.E6.m1.1.1.1.1.3.3.3.cmml" xref="S3.E6.m1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.3.3.3.1.cmml" xref="S3.E6.m1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.3.3.3.2.cmml" xref="S3.E6.m1.1.1.1.1.3.3.3.2">ğ¿</ci><apply id="S3.E6.m1.1.1.1.1.3.3.3.3.cmml" xref="S3.E6.m1.1.1.1.1.3.3.3.3"><times id="S3.E6.m1.1.1.1.1.3.3.3.3.1.cmml" xref="S3.E6.m1.1.1.1.1.3.3.3.3.1"></times><ci id="S3.E6.m1.1.1.1.1.3.3.3.3.2.cmml" xref="S3.E6.m1.1.1.1.1.3.3.3.3.2">â„</ci><ci id="S3.E6.m1.1.1.1.1.3.3.3.3.3.cmml" xref="S3.E6.m1.1.1.1.1.3.3.3.3.3">ğ‘–</ci><ci id="S3.E6.m1.1.1.1.1.3.3.3.3.4.cmml" xref="S3.E6.m1.1.1.1.1.3.3.3.3.4">ğ‘’</ci><ci id="S3.E6.m1.1.1.1.1.3.3.3.3.5.cmml" xref="S3.E6.m1.1.1.1.1.3.3.3.3.5">ğ‘Ÿ</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.1c">L_{total}=L_{mask}+\alpha\cdot L_{hier}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Pose estimation</h3>

<div id="S3.SS6.p1" class="ltx_para">
<p id="S3.SS6.p1.2" class="ltx_p">In previous sections, we discussed how to generate our descriptor and learn to predict them using a fully convolutional neural network. Now we incorporate the predicted code and visible mask and the reference 3D model encoding to match correspondences.
Different from common dense correspondences such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib68" title="" class="ltx_ref">68</a>, <a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>, this compact representation also enables a bijective correspondence between the surface vertices and the descriptor space. That means, unlike the regressed 3D point which can be off the object surface, our estimated 3D correspondences always refers to a vertex on the object model, which eases the matching stage for the pose solver. For the matching, we use a look-up table that extracts the corresponding 2D and 3D points. Following that, we use Progressive-X <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> solver to calculate the rotation <math id="S3.SS6.p1.1.m1.1" class="ltx_Math" alttext="R" display="inline"><semantics id="S3.SS6.p1.1.m1.1a"><mi id="S3.SS6.p1.1.m1.1.1" xref="S3.SS6.p1.1.m1.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.1.m1.1b"><ci id="S3.SS6.p1.1.m1.1.1.cmml" xref="S3.SS6.p1.1.m1.1.1">ğ‘…</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.1.m1.1c">R</annotation></semantics></math> and translation <math id="S3.SS6.p1.2.m2.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS6.p1.2.m2.1a"><mi id="S3.SS6.p1.2.m2.1.1" xref="S3.SS6.p1.2.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.2.m2.1b"><ci id="S3.SS6.p1.2.m2.1.1.cmml" xref="S3.SS6.p1.2.m2.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.2.m2.1c">t</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we firstly introduce the implementation details, the datasets and metrics used for the evaluation. Subsequently, we present ablation study experiments on the LM-OÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> dataset. Finally, we compare our experimental results with state of the art methods on the LM-OÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and YCB-VÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> datasets. Please refer to supplementary materials for more qualitative results.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experiments Setup</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.4" class="ltx_p"><span id="S4.SS1.p1.4.1" class="ltx_text ltx_font_bold">Implementation Details.</span> In order to have the same number of classes as DPODÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite> (<math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="K=256^{2}" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mrow id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mi id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml">K</mi><mo id="S4.SS1.p1.1.m1.1.1.1" xref="S4.SS1.p1.1.m1.1.1.1.cmml">=</mo><msup id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml"><mn id="S4.SS1.p1.1.m1.1.1.3.2" xref="S4.SS1.p1.1.m1.1.1.3.2.cmml">256</mn><mn id="S4.SS1.p1.1.m1.1.1.3.3" xref="S4.SS1.p1.1.m1.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><eq id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1.1"></eq><ci id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2">ğ¾</ci><apply id="S4.SS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p1.1.m1.1.1.3.1.cmml" xref="S4.SS1.p1.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.SS1.p1.1.m1.1.1.3.2.cmml" xref="S4.SS1.p1.1.m1.1.1.3.2">256</cn><cn type="integer" id="S4.SS1.p1.1.m1.1.1.3.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">K=256^{2}</annotation></semantics></math>), we firstly upsample the mesh by subdivision of each face using the edgeâ€™s midpointÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> until the mesh has more than <math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="256^{2}" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><msup id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml"><mn id="S4.SS1.p1.2.m2.1.1.2" xref="S4.SS1.p1.2.m2.1.1.2.cmml">256</mn><mn id="S4.SS1.p1.2.m2.1.1.3" xref="S4.SS1.p1.2.m2.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><apply id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.2.m2.1.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">superscript</csymbol><cn type="integer" id="S4.SS1.p1.2.m2.1.1.2.cmml" xref="S4.SS1.p1.2.m2.1.1.2">256</cn><cn type="integer" id="S4.SS1.p1.2.m2.1.1.3.cmml" xref="S4.SS1.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">256^{2}</annotation></semantics></math> vertices. Subsequently, we group the 3D vertices of the object model as we described in Sec.Â <a href="#S3.SS1" title="3.1 Coarse to Fine Surface Encoding â€£ 3 Method: ZebraPose â€£ ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a> with <math id="S4.SS1.p1.3.m3.1" class="ltx_Math" alttext="r=2" display="inline"><semantics id="S4.SS1.p1.3.m3.1a"><mrow id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml"><mi id="S4.SS1.p1.3.m3.1.1.2" xref="S4.SS1.p1.3.m3.1.1.2.cmml">r</mi><mo id="S4.SS1.p1.3.m3.1.1.1" xref="S4.SS1.p1.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS1.p1.3.m3.1.1.3" xref="S4.SS1.p1.3.m3.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b"><apply id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1"><eq id="S4.SS1.p1.3.m3.1.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1.1"></eq><ci id="S4.SS1.p1.3.m3.1.1.2.cmml" xref="S4.SS1.p1.3.m3.1.1.2">ğ‘Ÿ</ci><cn type="integer" id="S4.SS1.p1.3.m3.1.1.3.cmml" xref="S4.SS1.p1.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">r=2</annotation></semantics></math> and <math id="S4.SS1.p1.4.m4.1" class="ltx_Math" alttext="d=16" display="inline"><semantics id="S4.SS1.p1.4.m4.1a"><mrow id="S4.SS1.p1.4.m4.1.1" xref="S4.SS1.p1.4.m4.1.1.cmml"><mi id="S4.SS1.p1.4.m4.1.1.2" xref="S4.SS1.p1.4.m4.1.1.2.cmml">d</mi><mo id="S4.SS1.p1.4.m4.1.1.1" xref="S4.SS1.p1.4.m4.1.1.1.cmml">=</mo><mn id="S4.SS1.p1.4.m4.1.1.3" xref="S4.SS1.p1.4.m4.1.1.3.cmml">16</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.4.m4.1b"><apply id="S4.SS1.p1.4.m4.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1"><eq id="S4.SS1.p1.4.m4.1.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1.1"></eq><ci id="S4.SS1.p1.4.m4.1.1.2.cmml" xref="S4.SS1.p1.4.m4.1.1.2">ğ‘‘</ci><cn type="integer" id="S4.SS1.p1.4.m4.1.1.3.cmml" xref="S4.SS1.p1.4.m4.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.4.m4.1c">d=16</annotation></semantics></math>. After several iterations of the grouping operation, a group could contain fewer points than 2 and cannot be grouped further. To avoid this, we modified the k-means++ clustering algorithmÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, to force both output groups of points to have equal sizes.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.6" class="ltx_p">We modified Deeplabv3Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> by adding skip connections and used Resnet34Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> as the backbone. The input ROI is resized to the shape of 256<math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mo id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><times id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">\times</annotation></semantics></math>256<math id="S4.SS1.p2.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.p2.2.m2.1a"><mo id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><times id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">\times</annotation></semantics></math>3, and the CNN output has a height and width of 128. We applied the same dynamic zoom-in strategy as CDPNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> to generate the noisy ROI for the training. The parameter <math id="S4.SS1.p2.3.m3.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S4.SS1.p2.3.m3.1a"><mi id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml">Î»</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.1b"><ci id="S4.SS1.p2.3.m3.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1">ğœ†</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.1c">\lambda</annotation></semantics></math> used in the histogram is 0.05. The parameter <math id="S4.SS1.p2.4.m4.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S4.SS1.p2.4.m4.1a"><mi id="S4.SS1.p2.4.m4.1.1" xref="S4.SS1.p2.4.m4.1.1.cmml">Ïƒ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.4.m4.1b"><ci id="S4.SS1.p2.4.m4.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.4.m4.1c">\sigma</annotation></semantics></math> used in the hierarchical loss is 0.5, and <math id="S4.SS1.p2.5.m5.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.SS1.p2.5.m5.1a"><mi id="S4.SS1.p2.5.m5.1.1" xref="S4.SS1.p2.5.m5.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.5.m5.1b"><ci id="S4.SS1.p2.5.m5.1.1.cmml" xref="S4.SS1.p2.5.m5.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.5.m5.1c">\alpha</annotation></semantics></math> has been set as 3 to balance the training for mask and vertex code prediction. The CNN has been trained <math id="S4.SS1.p2.6.m6.1" class="ltx_Math" alttext="380k" display="inline"><semantics id="S4.SS1.p2.6.m6.1a"><mrow id="S4.SS1.p2.6.m6.1.1" xref="S4.SS1.p2.6.m6.1.1.cmml"><mn id="S4.SS1.p2.6.m6.1.1.2" xref="S4.SS1.p2.6.m6.1.1.2.cmml">380</mn><mo lspace="0em" rspace="0em" id="S4.SS1.p2.6.m6.1.1.1" xref="S4.SS1.p2.6.m6.1.1.1.cmml">â€‹</mo><mi id="S4.SS1.p2.6.m6.1.1.3" xref="S4.SS1.p2.6.m6.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.6.m6.1b"><apply id="S4.SS1.p2.6.m6.1.1.cmml" xref="S4.SS1.p2.6.m6.1.1"><times id="S4.SS1.p2.6.m6.1.1.1.cmml" xref="S4.SS1.p2.6.m6.1.1.1"></times><cn type="integer" id="S4.SS1.p2.6.m6.1.1.2.cmml" xref="S4.SS1.p2.6.m6.1.1.2">380</cn><ci id="S4.SS1.p2.6.m6.1.1.3.cmml" xref="S4.SS1.p2.6.m6.1.1.3">ğ‘˜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.6.m6.1c">380k</annotation></semantics></math> steps using the Adam optimizerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> with a batch size of 32 and a fixed learning rate of 2e-4. During the inference stage, we utilize the detected bounding box with Faster R-CNNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> and FCOSÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite> provided by CDPNv2Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>. If not specified, we used detected bounding box from Faster R-CNN in the ablation study.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">Additionally, by changing any bit in the vertex code, the code refers to another 3D point, possibly even to a vertex on the other side of the object. To maintain the topology presented with the ground truth correspondence map, we disabled the interpolation during the rendering when we generated the ground truth. The resizing of ground truth is also done with nearest neighbourhood interpolation in the training stage.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.2" class="ltx_p"><span id="S4.SS1.p4.2.1" class="ltx_text ltx_font_bold">Datasets.</span>
The reported recall rate in LMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> dataset has lately been higher than <math id="S4.SS1.p4.1.m1.1" class="ltx_Math" alttext="95\%" display="inline"><semantics id="S4.SS1.p4.1.m1.1a"><mrow id="S4.SS1.p4.1.m1.1.1" xref="S4.SS1.p4.1.m1.1.1.cmml"><mn id="S4.SS1.p4.1.m1.1.1.2" xref="S4.SS1.p4.1.m1.1.1.2.cmml">95</mn><mo id="S4.SS1.p4.1.m1.1.1.1" xref="S4.SS1.p4.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.1.m1.1b"><apply id="S4.SS1.p4.1.m1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1"><csymbol cd="latexml" id="S4.SS1.p4.1.m1.1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S4.SS1.p4.1.m1.1.1.2.cmml" xref="S4.SS1.p4.1.m1.1.1.2">95</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.1c">95\%</annotation></semantics></math> and quite saturated, therefore we focus on the more challenging LM-OÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and YCB-VÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> dataset in this paper. LM-O consist of 1214 images and is only used as test images. LM-O annotated 8 objects poses in the images under partial occlusion, making pose estimation more challenging. About <math id="S4.SS1.p4.2.m2.1" class="ltx_Math" alttext="1.2k" display="inline"><semantics id="S4.SS1.p4.2.m2.1a"><mrow id="S4.SS1.p4.2.m2.1.1" xref="S4.SS1.p4.2.m2.1.1.cmml"><mn id="S4.SS1.p4.2.m2.1.1.2" xref="S4.SS1.p4.2.m2.1.1.2.cmml">1.2</mn><mo lspace="0em" rspace="0em" id="S4.SS1.p4.2.m2.1.1.1" xref="S4.SS1.p4.2.m2.1.1.1.cmml">â€‹</mo><mi id="S4.SS1.p4.2.m2.1.1.3" xref="S4.SS1.p4.2.m2.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.2.m2.1b"><apply id="S4.SS1.p4.2.m2.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1"><times id="S4.SS1.p4.2.m2.1.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1.1"></times><cn type="float" id="S4.SS1.p4.2.m2.1.1.2.cmml" xref="S4.SS1.p4.2.m2.1.1.2">1.2</cn><ci id="S4.SS1.p4.2.m2.1.1.3.cmml" xref="S4.SS1.p4.2.m2.1.1.3">ğ‘˜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.2.m2.1c">1.2k</annotation></semantics></math> images per object in LM are used as the real training images for LM-O. Compared to LM-O, YCB-V is a large dataset containing 21 objects.
Although YCB-V provides more real training images, the objects are strongly occluded in the scene, and many of the objects are geometrically symmetric.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p">Since the LM-O dataset includes only a limited number of training images, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> additionally render a large number of synthetic images for training. However, due to the domain gap between the synthetic and real images, the performance of the methods also heavily depends on the domain randomization and domain adaptation techniqueÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>, <a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>. As the physically-based rendering (pbr) training imagesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> for both datasets are publicly accessible now, using pbr images to support the training can help us focus on the pose estimation CNN itself. We use the pbr images together with the real images for the training in the same manner as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.</p>
</div>
<div id="S4.SS1.p6" class="ltx_para">
<p id="S4.SS1.p6.1" class="ltx_p"><span id="S4.SS1.p6.1.1" class="ltx_text ltx_font_bold">Error Metrics.</span>
We selected the ADD(-S) error metric as the most commonly used metric for the 6DoF pose estimation task. This metric calculates the average distance of model points projected to the camera domain using the predicted pose to the same model points projected using the ground truth pose. For symmetric objects, the metric matches the closest model points projected with the ground truth pose instead of the same model point. In all the experiments in this paper, if ADD(-S) error is smaller than 10% (most commonly used threshold) of the object diameter, the predicted pose is considered to be correct
For YCB-V, we also reported the AUC
(area under curve) of ADD(-S) with a maximum threshold of 10 cm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Ablation Study on LM-O</h3>

<figure id="S4.F3" class="ltx_figure"><img src="/html/2203.09418/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="116" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.4.2.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S4.F3.2.1" class="ltx_text" style="font-size:90%;">a) In the first row, a yellow dot has the ground truth binary vertex code beginning with 000, and the yellow circle refers to the neighborhood vertex of this yellow dot. If the vertex code has been predicted as 100 (the first bit is wrong, marked as red in the figure) during the inference stage, the estimated yellow dot lies somewhere on the head of the drill (marked in blue). The estimated 3D vertex is far away from its original neighborhood and can be easily found by checking the spatial coherency. We show four similar cases in this figure. b) We calculated ADD pose metrics only on the first <math id="S4.F3.2.1.m1.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S4.F3.2.1.m1.1b"><mi id="S4.F3.2.1.m1.1.1" xref="S4.F3.2.1.m1.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S4.F3.2.1.m1.1c"><ci id="S4.F3.2.1.m1.1.1.cmml" xref="S4.F3.2.1.m1.1.1">ğ‘—</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.2.1.m1.1d">j</annotation></semantics></math> bits of the predicted code to build the 2D-3D correspondence. Here we observe from which bit the predictions are stable. c) We present the average error rate at different bit positions on the LM-O dataset<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</span></figcaption>
</figure>
<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">In this section, we present the results of several ablation studies as follows:</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p"><span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_bold">Length of Binary Vertex Code.</span>
The object 3D surface is encoded through iterative k-means++ clustering until the size of the segmented cluster is small enough so that we can map the vertex code to the centroid of each cluster. We used the same total number of classes as DPODÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite>, which means each binary vertex code has 16 bits. However, if the objects are small or the distance of the object to the camera is too large, different clusters in the fine level could be rendered into the same pixel when we generate the ground truth data. This makes the binary code in the fine levels (the last few bits) redundant. Due to the distance variation of the object to the camera, we can not determine which bits are redundant.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">In this ablation study, we research which bits are the redundant bits. The models are trained without the hierarchical training strategy, and we use Progressive-XÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> to solve the pose. We ignore the last few bits of the predicted binary code in the inference stage. The new binary vertex code with fewer bits refers to a larger point cloud group (see Fig.Â <a href="#S3.F2" title="Figure 2 â€£ 3 Method: ZebraPose â€£ ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> left). We calculated the new centroid of the group and reassigned the centroid as the corresponding 3D points for the binary vertex code with fewer bits. From Fig.Â <a href="#S4.F3" title="Figure 3 â€£ 4.2 Ablation Study on LM-O â€£ 4 Experiments â€£ ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> b) we can see that using 10-bit code is already sufficient to yield an accurate prediction for the objects in LM-O, indicating the last 6 bits are redundant for those objects.
The results fluctuated a bit when we applied the redundant bits, which indicates that for some objects the best results is achieved by not using the full 16-bit code. However, in the following experiment, we always report the results with the full predicted vertex code.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.4" class="ltx_p"><span id="S4.SS2.p4.4.1" class="ltx_text ltx_font_bold">Radix used in Vertex Code.</span> The number of the clusters in each iteration decides the radix of the generated vertex code that describes the 3D vertex. Since our CNN predicts the vertex code, it is meaningful to compare which radix in vertex code suits the representation better. We do not need to generate all the vertex codes used in this ablation study from scratch. More specifically, by merging every <math id="S4.SS2.p4.1.m1.1" class="ltx_Math" alttext="log_{2}r" display="inline"><semantics id="S4.SS2.p4.1.m1.1a"><mrow id="S4.SS2.p4.1.m1.1.1" xref="S4.SS2.p4.1.m1.1.1.cmml"><mi id="S4.SS2.p4.1.m1.1.1.2" xref="S4.SS2.p4.1.m1.1.1.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p4.1.m1.1.1.1" xref="S4.SS2.p4.1.m1.1.1.1.cmml">â€‹</mo><mi id="S4.SS2.p4.1.m1.1.1.3" xref="S4.SS2.p4.1.m1.1.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p4.1.m1.1.1.1a" xref="S4.SS2.p4.1.m1.1.1.1.cmml">â€‹</mo><msub id="S4.SS2.p4.1.m1.1.1.4" xref="S4.SS2.p4.1.m1.1.1.4.cmml"><mi id="S4.SS2.p4.1.m1.1.1.4.2" xref="S4.SS2.p4.1.m1.1.1.4.2.cmml">g</mi><mn id="S4.SS2.p4.1.m1.1.1.4.3" xref="S4.SS2.p4.1.m1.1.1.4.3.cmml">2</mn></msub><mo lspace="0em" rspace="0em" id="S4.SS2.p4.1.m1.1.1.1b" xref="S4.SS2.p4.1.m1.1.1.1.cmml">â€‹</mo><mi id="S4.SS2.p4.1.m1.1.1.5" xref="S4.SS2.p4.1.m1.1.1.5.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.1.m1.1b"><apply id="S4.SS2.p4.1.m1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1"><times id="S4.SS2.p4.1.m1.1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1.1"></times><ci id="S4.SS2.p4.1.m1.1.1.2.cmml" xref="S4.SS2.p4.1.m1.1.1.2">ğ‘™</ci><ci id="S4.SS2.p4.1.m1.1.1.3.cmml" xref="S4.SS2.p4.1.m1.1.1.3">ğ‘œ</ci><apply id="S4.SS2.p4.1.m1.1.1.4.cmml" xref="S4.SS2.p4.1.m1.1.1.4"><csymbol cd="ambiguous" id="S4.SS2.p4.1.m1.1.1.4.1.cmml" xref="S4.SS2.p4.1.m1.1.1.4">subscript</csymbol><ci id="S4.SS2.p4.1.m1.1.1.4.2.cmml" xref="S4.SS2.p4.1.m1.1.1.4.2">ğ‘”</ci><cn type="integer" id="S4.SS2.p4.1.m1.1.1.4.3.cmml" xref="S4.SS2.p4.1.m1.1.1.4.3">2</cn></apply><ci id="S4.SS2.p4.1.m1.1.1.5.cmml" xref="S4.SS2.p4.1.m1.1.1.5">ğ‘Ÿ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.1.m1.1c">log_{2}r</annotation></semantics></math> bit of a vertex code, we get a code with a radix <math id="S4.SS2.p4.2.m2.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S4.SS2.p4.2.m2.1a"><mi id="S4.SS2.p4.2.m2.1.1" xref="S4.SS2.p4.2.m2.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.2.m2.1b"><ci id="S4.SS2.p4.2.m2.1.1.cmml" xref="S4.SS2.p4.2.m2.1.1">ğ‘Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.2.m2.1c">r</annotation></semantics></math>. For instance, a vertex with a binary code {11111110 11111111} can be transformed to {254 255} using 256 as the radix. We will get exactly the same code for this vertex if we split the object into 256 groups and split each group again into 256 groups. We use a fix <math id="S4.SS2.p4.3.m3.1" class="ltx_Math" alttext="w_{j}=1" display="inline"><semantics id="S4.SS2.p4.3.m3.1a"><mrow id="S4.SS2.p4.3.m3.1.1" xref="S4.SS2.p4.3.m3.1.1.cmml"><msub id="S4.SS2.p4.3.m3.1.1.2" xref="S4.SS2.p4.3.m3.1.1.2.cmml"><mi id="S4.SS2.p4.3.m3.1.1.2.2" xref="S4.SS2.p4.3.m3.1.1.2.2.cmml">w</mi><mi id="S4.SS2.p4.3.m3.1.1.2.3" xref="S4.SS2.p4.3.m3.1.1.2.3.cmml">j</mi></msub><mo id="S4.SS2.p4.3.m3.1.1.1" xref="S4.SS2.p4.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS2.p4.3.m3.1.1.3" xref="S4.SS2.p4.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.3.m3.1b"><apply id="S4.SS2.p4.3.m3.1.1.cmml" xref="S4.SS2.p4.3.m3.1.1"><eq id="S4.SS2.p4.3.m3.1.1.1.cmml" xref="S4.SS2.p4.3.m3.1.1.1"></eq><apply id="S4.SS2.p4.3.m3.1.1.2.cmml" xref="S4.SS2.p4.3.m3.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p4.3.m3.1.1.2.1.cmml" xref="S4.SS2.p4.3.m3.1.1.2">subscript</csymbol><ci id="S4.SS2.p4.3.m3.1.1.2.2.cmml" xref="S4.SS2.p4.3.m3.1.1.2.2">ğ‘¤</ci><ci id="S4.SS2.p4.3.m3.1.1.2.3.cmml" xref="S4.SS2.p4.3.m3.1.1.2.3">ğ‘—</ci></apply><cn type="integer" id="S4.SS2.p4.3.m3.1.1.3.cmml" xref="S4.SS2.p4.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.3.m3.1c">w_{j}=1</annotation></semantics></math> (see eq. <a href="#S3.E4" title="Equation 4 â€£ 3.5 Hierarchical Learning â€£ 3 Method: ZebraPose â€£ ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>) for all positions so the loss is essentially a binary cross entropy when <math id="S4.SS2.p4.4.m4.1" class="ltx_Math" alttext="r=2" display="inline"><semantics id="S4.SS2.p4.4.m4.1a"><mrow id="S4.SS2.p4.4.m4.1.1" xref="S4.SS2.p4.4.m4.1.1.cmml"><mi id="S4.SS2.p4.4.m4.1.1.2" xref="S4.SS2.p4.4.m4.1.1.2.cmml">r</mi><mo id="S4.SS2.p4.4.m4.1.1.1" xref="S4.SS2.p4.4.m4.1.1.1.cmml">=</mo><mn id="S4.SS2.p4.4.m4.1.1.3" xref="S4.SS2.p4.4.m4.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.4.m4.1b"><apply id="S4.SS2.p4.4.m4.1.1.cmml" xref="S4.SS2.p4.4.m4.1.1"><eq id="S4.SS2.p4.4.m4.1.1.1.cmml" xref="S4.SS2.p4.4.m4.1.1.1"></eq><ci id="S4.SS2.p4.4.m4.1.1.2.cmml" xref="S4.SS2.p4.4.m4.1.1.2">ğ‘Ÿ</ci><cn type="integer" id="S4.SS2.p4.4.m4.1.1.3.cmml" xref="S4.SS2.p4.4.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.4.m4.1c">r=2</annotation></semantics></math>, and cross-entropy loss for other radixes.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.1" class="ltx_p">We present the comparison results in Tab.Â <a href="#S4.T1" title="Table 1 â€£ 4.2 Ablation Study on LM-O â€£ 4 Experiments â€£ ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. If RANSAC/PnP is used to solve the pose, the results with different radices are quite similar. There is no clear indication whether using the small or large radix is better. If we switch the pose solver to Progressive-XÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, the code with small radix improves the most and yields the best accuracy.
Progressive-X solver includes a spatial coherence filter that checks neighboring 3D points with respect to its assigned 2D correspondences based on label cost energy minimization as introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.
It can therefore deal particularly well with the type of outliers, from our method as we mentioned in Fig.Â <a href="#S4.F3" title="Figure 3 â€£ 4.2 Ablation Study on LM-O â€£ 4 Experiments â€£ ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. We show in the Fig.Â <a href="#S4.F3" title="Figure 3 â€£ 4.2 Ablation Study on LM-O â€£ 4 Experiments â€£ ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> a), if the CNN predicts the first few bits wrong for the yellow dot, the estimated corresponding 3D points is far away from the ground truth position and totally incoherent with its original neighbourhood. Assuming that most predictions are correct, most neighbourhood vertices are posed within the yellow circle in the figure. In this case, this false estimated 3D corresponding can be easily filtered by calculating the coherency with its neighbourhood. This spatial coherency filter can not detect outliers well if the wrong prediction is in the last few bits, and the same for 256 as radix, as divide the vertices into 256 groups is already a fine grouping. Nevertheless, the Fig.Â <a href="#S4.F3" title="Figure 3 â€£ 4.2 Ablation Study on LM-O â€£ 4 Experiments â€£ ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> b) already shows that the last few bits do not affect the solved pose. So we argue that the binary vertex code suits this task the best. Moreover, the prediction of binary codes requires the least RAM in GPU, as we discussed in Sec.Â <a href="#S3.SS1" title="3.1 Coarse to Fine Surface Encoding â€£ 3 Method: ZebraPose â€£ ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>.</p>
</div>
<div id="S4.SS2.p6" class="ltx_para">
<p id="S4.SS2.p6.1" class="ltx_p"><span id="S4.SS2.p6.1.1" class="ltx_text ltx_font_bold">Effectiveness of Hierarchical Training.</span>
According to the first ablation study, the last few bits are redundant and may not be trainable (see Fig.Â <a href="#S4.F3" title="Figure 3 â€£ 4.2 Ablation Study on LM-O â€£ 4 Experiments â€£ ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>c)). During the training, we can recognize redundant bits based on the error histogram and focus on the decisive bits as described in Sec.Â <a href="#S3.SS5" title="3.5 Hierarchical Learning â€£ 3 Method: ZebraPose â€£ ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.5</span></a>. Tab.Â <a href="#S4.T2" title="Table 2 â€£ 4.2 Ablation Study on LM-O â€£ 4 Experiments â€£ ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows that the results are further improved by our proposed hierarchical training.</p>
</div>
<div id="S4.SS2.p7" class="ltx_para">
<p id="S4.SS2.p7.1" class="ltx_p"><span id="S4.SS2.p7.1.1" class="ltx_text ltx_font_bold">Influence of 2D detection.</span>
The CNN estimates the pose with the cropped ROI from the detected bounding box. The object pose estimation is meaningless with a false-positive detection, also the pose is not even estimated in the case of false-negative detection. By leveraging the detected bounding box with FCOSÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite> instead of the one from Faster R-CNNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>, the recall rate improved 1.05%.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T1.2.1" class="ltx_tr">
<td id="S4.T1.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Method</td>
<td id="S4.T1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">RANSAC/ PnpÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>
</td>
<td id="S4.T1.2.1.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt">Progressive-X <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>
</td>
</tr>
<tr id="S4.T1.2.2" class="ltx_tr">
<td id="S4.T1.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2 as radix</td>
<td id="S4.T1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">73.06</td>
<td id="S4.T1.2.2.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T1.2.2.3.1" class="ltx_text ltx_font_bold">75.23 (+2.17)</span></td>
</tr>
<tr id="S4.T1.2.3" class="ltx_tr">
<td id="S4.T1.2.3.1" class="ltx_td ltx_align_center ltx_border_r">4 as radix</td>
<td id="S4.T1.2.3.2" class="ltx_td ltx_align_center ltx_border_r">72.94</td>
<td id="S4.T1.2.3.3" class="ltx_td ltx_nopad_r ltx_align_center">74.59 (+1.65)</td>
</tr>
<tr id="S4.T1.2.4" class="ltx_tr">
<td id="S4.T1.2.4.1" class="ltx_td ltx_align_center ltx_border_r">16 as radix</td>
<td id="S4.T1.2.4.2" class="ltx_td ltx_align_center ltx_border_r">73.04</td>
<td id="S4.T1.2.4.3" class="ltx_td ltx_nopad_r ltx_align_center">74.98 (+1.94)</td>
</tr>
<tr id="S4.T1.2.5" class="ltx_tr">
<td id="S4.T1.2.5.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">256 as radix</td>
<td id="S4.T1.2.5.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">73.25</td>
<td id="S4.T1.2.5.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">74.52 (+1.27)</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.4.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.5.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Ablation study on LM-OÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.<span id="S4.T1.5.2.1" class="ltx_text ltx_font_medium"> We tested the use of different radices to encode the vertices, and using different solvers to calculate the pose. The results are presented in terms of average recall of ADD(-S) in %.</span></span></figcaption>
</figure>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T2.1.2" class="ltx_tr">
<td id="S4.T2.1.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Method</td>
<td id="S4.T2.1.2.2" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt">ADD</td>
</tr>
<tr id="S4.T2.1.3" class="ltx_tr">
<td id="S4.T2.1.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2 as radix</td>
<td id="S4.T2.1.3.2" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">75.23</td>
</tr>
<tr id="S4.T2.1.4" class="ltx_tr">
<td id="S4.T2.1.4.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2 as radix + Hierarchical Learning</td>
<td id="S4.T2.1.4.2" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">75.86</td>
</tr>
<tr id="S4.T2.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t">
<span id="S4.T2.1.1.1.2" class="ltx_text"></span><span id="S4.T2.1.1.1.1" class="ltx_text">
<span id="S4.T2.1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.1.1.1.1.1.2" class="ltx_tr">
<span id="S4.T2.1.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">2 as radix + Hierarchical Learning</span></span>
<span id="S4.T2.1.1.1.1.1.1" class="ltx_tr">
<span id="S4.T2.1.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">+ Faster R-CNNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> <math id="S4.T2.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T2.1.1.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T2.1.1.1.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.1.1.1.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.1.1.1.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.1.1.1.m1.1c">\rightarrow</annotation></semantics></math> FCOSÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite></span></span>
</span></span><span id="S4.T2.1.1.1.3" class="ltx_text"></span></td>
<td id="S4.T2.1.1.2" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.1.1.2.1" class="ltx_text ltx_font_bold">76.91</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.4.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.5.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Ablation study on LM-O<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.<span id="S4.T2.5.2.1" class="ltx_text ltx_font_medium"> We compare the result with and w/o applying our hierarchical loss, as well as the impact of the prior object detector. The results are presented with average recall of ADD(-S) in %.</span></span></figcaption>
</figure>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T3.2.1" class="ltx_tr">
<td id="S4.T3.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" rowspan="2"><span id="S4.T3.2.1.1.1" class="ltx_text">Method</span></td>
<td id="S4.T3.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="5">RGB Input</td>
<td id="S4.T3.2.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">RGB-D Input</td>
</tr>
<tr id="S4.T3.2.2" class="ltx_tr">
<td id="S4.T3.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">HybridPoseÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>
</td>
<td id="S4.T3.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">RePoseÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>
</td>
<td id="S4.T3.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">GDR-NetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>
</td>
<td id="S4.T3.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">SO-PoseÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
</td>
<td id="S4.T3.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.2.2.5.1" class="ltx_text ltx_font_bold">Ours</span></td>
<td id="S4.T3.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">PR-GCNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite>
</td>
<td id="S4.T3.2.2.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">FFB6DÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>
</td>
</tr>
<tr id="S4.T3.2.3" class="ltx_tr">
<td id="S4.T3.2.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">ape</td>
<td id="S4.T3.2.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">20.9</td>
<td id="S4.T3.2.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">31.1</td>
<td id="S4.T3.2.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">46.8</td>
<td id="S4.T3.2.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">48.4</td>
<td id="S4.T3.2.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.2.3.6.1" class="ltx_text ltx_font_bold">57.9</span></td>
<td id="S4.T3.2.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">40.2</td>
<td id="S4.T3.2.3.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">47.2</td>
</tr>
<tr id="S4.T3.2.4" class="ltx_tr">
<td id="S4.T3.2.4.1" class="ltx_td ltx_align_center ltx_border_r">can</td>
<td id="S4.T3.2.4.2" class="ltx_td ltx_align_center ltx_border_r">75.3</td>
<td id="S4.T3.2.4.3" class="ltx_td ltx_align_center ltx_border_r">80.0</td>
<td id="S4.T3.2.4.4" class="ltx_td ltx_align_center ltx_border_r">90.8</td>
<td id="S4.T3.2.4.5" class="ltx_td ltx_align_center ltx_border_r">85.8</td>
<td id="S4.T3.2.4.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.2.4.6.1" class="ltx_text ltx_font_bold">95.0</span></td>
<td id="S4.T3.2.4.7" class="ltx_td ltx_align_center ltx_border_r">76.2</td>
<td id="S4.T3.2.4.8" class="ltx_td ltx_nopad_r ltx_align_center">85.2</td>
</tr>
<tr id="S4.T3.2.5" class="ltx_tr">
<td id="S4.T3.2.5.1" class="ltx_td ltx_align_center ltx_border_r">cat</td>
<td id="S4.T3.2.5.2" class="ltx_td ltx_align_center ltx_border_r">24.9</td>
<td id="S4.T3.2.5.3" class="ltx_td ltx_align_center ltx_border_r">25.6</td>
<td id="S4.T3.2.5.4" class="ltx_td ltx_align_center ltx_border_r">40.5</td>
<td id="S4.T3.2.5.5" class="ltx_td ltx_align_center ltx_border_r">32.7</td>
<td id="S4.T3.2.5.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.2.5.6.1" class="ltx_text ltx_font_bold">60.6</span></td>
<td id="S4.T3.2.5.7" class="ltx_td ltx_align_center ltx_border_r">57.0</td>
<td id="S4.T3.2.5.8" class="ltx_td ltx_nopad_r ltx_align_center">45.7</td>
</tr>
<tr id="S4.T3.2.6" class="ltx_tr">
<td id="S4.T3.2.6.1" class="ltx_td ltx_align_center ltx_border_r">driller</td>
<td id="S4.T3.2.6.2" class="ltx_td ltx_align_center ltx_border_r">70.2</td>
<td id="S4.T3.2.6.3" class="ltx_td ltx_align_center ltx_border_r">73.1</td>
<td id="S4.T3.2.6.4" class="ltx_td ltx_align_center ltx_border_r">82.6</td>
<td id="S4.T3.2.6.5" class="ltx_td ltx_align_center ltx_border_r">77.4</td>
<td id="S4.T3.2.6.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.2.6.6.1" class="ltx_text ltx_font_bold">94.8</span></td>
<td id="S4.T3.2.6.7" class="ltx_td ltx_align_center ltx_border_r">82.3</td>
<td id="S4.T3.2.6.8" class="ltx_td ltx_nopad_r ltx_align_center">81.4</td>
</tr>
<tr id="S4.T3.2.7" class="ltx_tr">
<td id="S4.T3.2.7.1" class="ltx_td ltx_align_center ltx_border_r">duck</td>
<td id="S4.T3.2.7.2" class="ltx_td ltx_align_center ltx_border_r">27.9</td>
<td id="S4.T3.2.7.3" class="ltx_td ltx_align_center ltx_border_r">43.0</td>
<td id="S4.T3.2.7.4" class="ltx_td ltx_align_center ltx_border_r">46.9</td>
<td id="S4.T3.2.7.5" class="ltx_td ltx_align_center ltx_border_r">48.9</td>
<td id="S4.T3.2.7.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.2.7.6.1" class="ltx_text ltx_font_bold">64.5</span></td>
<td id="S4.T3.2.7.7" class="ltx_td ltx_align_center ltx_border_r">30.0</td>
<td id="S4.T3.2.7.8" class="ltx_td ltx_nopad_r ltx_align_center">53.9</td>
</tr>
<tr id="S4.T3.2.8" class="ltx_tr">
<td id="S4.T3.2.8.1" class="ltx_td ltx_align_center ltx_border_r">eggbox*</td>
<td id="S4.T3.2.8.2" class="ltx_td ltx_align_center ltx_border_r">52.4</td>
<td id="S4.T3.2.8.3" class="ltx_td ltx_align_center ltx_border_r">51.7</td>
<td id="S4.T3.2.8.4" class="ltx_td ltx_align_center ltx_border_r">54.2</td>
<td id="S4.T3.2.8.5" class="ltx_td ltx_align_center ltx_border_r">52.4</td>
<td id="S4.T3.2.8.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.2.8.6.1" class="ltx_text ltx_font_bold">70.9</span></td>
<td id="S4.T3.2.8.7" class="ltx_td ltx_align_center ltx_border_r">68.2</td>
<td id="S4.T3.2.8.8" class="ltx_td ltx_nopad_r ltx_align_center">70.2</td>
</tr>
<tr id="S4.T3.2.9" class="ltx_tr">
<td id="S4.T3.2.9.1" class="ltx_td ltx_align_center ltx_border_r">glue*</td>
<td id="S4.T3.2.9.2" class="ltx_td ltx_align_center ltx_border_r">53.8</td>
<td id="S4.T3.2.9.3" class="ltx_td ltx_align_center ltx_border_r">54.3</td>
<td id="S4.T3.2.9.4" class="ltx_td ltx_align_center ltx_border_r">75.8</td>
<td id="S4.T3.2.9.5" class="ltx_td ltx_align_center ltx_border_r">78.3</td>
<td id="S4.T3.2.9.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.2.9.6.1" class="ltx_text ltx_font_bold">88.7</span></td>
<td id="S4.T3.2.9.7" class="ltx_td ltx_align_center ltx_border_r">67.0</td>
<td id="S4.T3.2.9.8" class="ltx_td ltx_nopad_r ltx_align_center">60.1</td>
</tr>
<tr id="S4.T3.2.10" class="ltx_tr">
<td id="S4.T3.2.10.1" class="ltx_td ltx_align_center ltx_border_r">holepuncher</td>
<td id="S4.T3.2.10.2" class="ltx_td ltx_align_center ltx_border_r">54.2</td>
<td id="S4.T3.2.10.3" class="ltx_td ltx_align_center ltx_border_r">53.6</td>
<td id="S4.T3.2.10.4" class="ltx_td ltx_align_center ltx_border_r">60.1</td>
<td id="S4.T3.2.10.5" class="ltx_td ltx_align_center ltx_border_r">75.3</td>
<td id="S4.T3.2.10.6" class="ltx_td ltx_align_center ltx_border_r">83.0</td>
<td id="S4.T3.2.10.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.2.10.7.1" class="ltx_text ltx_font_bold">97.2</span></td>
<td id="S4.T3.2.10.8" class="ltx_td ltx_nopad_r ltx_align_center">85.9</td>
</tr>
<tr id="S4.T3.2.11" class="ltx_tr">
<td id="S4.T3.2.11.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">mean</td>
<td id="S4.T3.2.11.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">47.5</td>
<td id="S4.T3.2.11.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">51.6</td>
<td id="S4.T3.2.11.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">62.2</td>
<td id="S4.T3.2.11.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">62.3</td>
<td id="S4.T3.2.11.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t"><span id="S4.T3.2.11.6.1" class="ltx_text ltx_font_bold">76.9</span></td>
<td id="S4.T3.2.11.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">65</td>
<td id="S4.T3.2.11.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t">66.2</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.4.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S4.T3.5.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Comparison with State of the Art on LM-O<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite><span id="S4.T3.5.2.1" class="ltx_text ltx_font_medium">. We report the Recall of ADD(-S) in % and compare with state of the art. (*) denotes symmetric objects.</span></span></figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Comparison to State of the Art</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We use 2 as radix, i.e. binary vertex code and apply the hierarchical training strategy and Progressive-X pose solverÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> in our proposed ZebraPose to compare to state of the art on LM-OÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and YCB-VÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> datasets. The detected bounding box of FCOSÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite> detector are provided by CDPNv2Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_bold">Results on LM-O.</span> We report the recall of ADD(-S) metric in Tab.Â <a href="#S4.T3" title="Table 3 â€£ 4.2 Ablation Study on LM-O â€£ 4 Experiments â€£ ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. We ordered the methods according to the input modality. HybridPoseÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite> and RePoseÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> have been trained with synthetic and real images. GDR-NetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite> also reported their recall of 53% when trained with synthetic and real images. Therefore, GDR-Net outperforms HybridPose and RePose. In our Tab.<a href="#S4.T3" title="Table 3 â€£ 4.2 Ablation Study on LM-O â€£ 4 Experiments â€£ ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we report the best results that GDR-Net and SO-PoseÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> presented, which are also trained with pbr and real images. GDR-Net used faster R-CNNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> as the detector, ZebraPose yields a recall of 75.86% with faster R-CNN (see Tab.Â <a href="#S4.T2" title="Table 2 â€£ 4.2 Ablation Study on LM-O â€£ 4 Experiments â€£ ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>), which can be seen as a more fair comparison with GDR-Net.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">To summarize, our ZebraPose outperforms state of the art RGB based methods with a large margin on this dataset. Additionally, we found that our ZebraPose also outperforms state of the art RGB-D based methodsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. Most objects in the LM-O dataset are texture-less, meaning that RGB-D based methods should have more advantage in feature extraction on the objects with the help of depth image. Even in this case, our results still exceed theirs.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p"><span id="S4.SS3.p4.1.1" class="ltx_text ltx_font_bold">Results on YCB-V.</span>
We compare ZebraPose with other approaches in the YCB-V dataset in Tab.Â <a href="#S4.T4" title="Table 4 â€£ 4.3 Comparison to State of the Art â€£ 4 Experiments â€£ ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. The AUC reported in Tab.Â <a href="#S4.T4" title="Table 4 â€£ 4.3 Comparison to State of the Art â€£ 4 Experiments â€£ ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> has been calculated using all-points interpolation. Tab.Â <a href="#S4.T4" title="Table 4 â€£ 4.3 Comparison to State of the Art â€£ 4 Experiments â€£ ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows that ZebraPose is still better than state of the art w.r.t. ADD(-S) and AUC of ADD(-S) metrics and comparable to them w.r.t. the AUC of ADD-S metric.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T4.2.1" class="ltx_tr">
<td id="S4.T4.2.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">Method</td>
<td id="S4.T4.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">ADD(-S)</td>
<td id="S4.T4.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">
<span id="S4.T4.2.1.3.1" class="ltx_text"></span> <span id="S4.T4.2.1.3.2" class="ltx_text">
<span id="S4.T4.2.1.3.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T4.2.1.3.2.1.1" class="ltx_tr">
<span id="S4.T4.2.1.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">AUC of</span></span>
<span id="S4.T4.2.1.3.2.1.2" class="ltx_tr">
<span id="S4.T4.2.1.3.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">ADD-S</span></span>
</span></span><span id="S4.T4.2.1.3.3" class="ltx_text"></span></td>
<td id="S4.T4.2.1.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt">
<span id="S4.T4.2.1.4.1" class="ltx_text"></span> <span id="S4.T4.2.1.4.2" class="ltx_text">
<span id="S4.T4.2.1.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T4.2.1.4.2.1.1" class="ltx_tr">
<span id="S4.T4.2.1.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">AUC of</span></span>
<span id="S4.T4.2.1.4.2.1.2" class="ltx_tr">
<span id="S4.T4.2.1.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">ADD(-S)</span></span>
</span></span><span id="S4.T4.2.1.4.3" class="ltx_text"></span></td>
</tr>
<tr id="S4.T4.2.2" class="ltx_tr">
<td id="S4.T4.2.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">SegDriven<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>
</td>
<td id="S4.T4.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">39.0</td>
<td id="S4.T4.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S4.T4.2.2.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S4.T4.2.3" class="ltx_tr">
<td id="S4.T4.2.3.1" class="ltx_td ltx_align_left ltx_border_r">SingleStage<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>
</td>
<td id="S4.T4.2.3.2" class="ltx_td ltx_align_center ltx_border_r">53.9</td>
<td id="S4.T4.2.3.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T4.2.3.4" class="ltx_td ltx_nopad_r ltx_align_center">-</td>
</tr>
<tr id="S4.T4.2.4" class="ltx_tr">
<td id="S4.T4.2.4.1" class="ltx_td ltx_align_left ltx_border_r">CosyPoseÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>
</td>
<td id="S4.T4.2.4.2" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T4.2.4.3" class="ltx_td ltx_align_center ltx_border_r">89.8</td>
<td id="S4.T4.2.4.4" class="ltx_td ltx_nopad_r ltx_align_center">84.5</td>
</tr>
<tr id="S4.T4.2.5" class="ltx_tr">
<td id="S4.T4.2.5.1" class="ltx_td ltx_align_left ltx_border_r">RePoseÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>
</td>
<td id="S4.T4.2.5.2" class="ltx_td ltx_align_center ltx_border_r">62.1</td>
<td id="S4.T4.2.5.3" class="ltx_td ltx_align_center ltx_border_r">88.5</td>
<td id="S4.T4.2.5.4" class="ltx_td ltx_nopad_r ltx_align_center">82.0</td>
</tr>
<tr id="S4.T4.2.6" class="ltx_tr">
<td id="S4.T4.2.6.1" class="ltx_td ltx_align_left ltx_border_r">GDR-NetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>
</td>
<td id="S4.T4.2.6.2" class="ltx_td ltx_align_center ltx_border_r">60.1</td>
<td id="S4.T4.2.6.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T4.2.6.3.1" class="ltx_text ltx_font_bold">91.6</span></td>
<td id="S4.T4.2.6.4" class="ltx_td ltx_nopad_r ltx_align_center">84.4</td>
</tr>
<tr id="S4.T4.2.7" class="ltx_tr">
<td id="S4.T4.2.7.1" class="ltx_td ltx_align_left ltx_border_r">SO-PoseÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
</td>
<td id="S4.T4.2.7.2" class="ltx_td ltx_align_center ltx_border_r">56.8</td>
<td id="S4.T4.2.7.3" class="ltx_td ltx_align_center ltx_border_r">90.9</td>
<td id="S4.T4.2.7.4" class="ltx_td ltx_nopad_r ltx_align_center">83.9</td>
</tr>
<tr id="S4.T4.2.8" class="ltx_tr">
<td id="S4.T4.2.8.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">Ours</td>
<td id="S4.T4.2.8.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T4.2.8.2.1" class="ltx_text ltx_font_bold">80.5</span></td>
<td id="S4.T4.2.8.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">90.1</td>
<td id="S4.T4.2.8.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="S4.T4.2.8.4.1" class="ltx_text ltx_font_bold">85.3</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.4.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S4.T4.5.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Comparison with State of the Art on YCB-V<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite><span id="S4.T4.5.2.1" class="ltx_text ltx_font_medium">. We compare our ZebraPose with state of the art w.r.t ADD(-S), AUC of ADD(-S) and AUC of ADD-S in %. (-) denotes results missing from the original paper.</span></span></figcaption>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Runtime Analysis</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">We tested the runtime on a desktop with an Intel 3.50GHz CPU and an Nvidia 2080Ti GPU. The CNN runtime plus the time to build the 2D-3D correspondence is about 52 ms. The FCOS detector<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite> takes 55 ms. RANSAC/PnPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> needs only 4 ms to solve the pose, while Progressive-XÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> requires 150 ms to obtain the pose. So for ZebraPose used in Sec.<a href="#S4.SS3" title="4.3 Comparison to State of the Art â€£ 4 Experiments â€£ ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>, it totally needs about 250 ms to estimate the object pose. If we use RANSAC/PnP to solve the pose, the runtime reduces to 110 ms, while with about 2.6% recall drop on LM-O dataset.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this work, we proposed a novel coarse to fine surface encoding technique to provide 2D-3D correspondences for 6DoF object pose estimation. We also designed a specific hierarchical training strategy that maximizes the prediction accuracy for our proposed binary vertex code. Solving the object pose using a PnP solver based on our vertex code surpasses the state of the art on different benchmarks, proving our approachâ€™s effectiveness. In the future, we would like to extend our vertex code solution to the problem of category-level object poseÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This work was partially funded by the <em id="Sx1.p1.1.1" class="ltx_emph ltx_font_italic">Federal Ministry of Education and Research</em> of the Federal Republic of Germany (BMBF), under grant agreements 16SV8732 (GreifbAR) and 01IW21001 (DECODE). We are thankful to Rene Schuster, Fangwen Shu, Yaxu Xie and Ghazal Ghazaei for proofreading the paper.
<span id="Sx1.p1.1.2" class="ltx_text" style="font-size:90%;"></span></p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
David Arthur and Sergei Vassilvitskii.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">k-means++: The advantages of careful seeding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">Technical report, Stanford, 2006.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Daniel Barath and Jiri Matas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Progressive-x: Efficient, anytime, multi-model fitting algorithm.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, pages 3780â€“3788, 2019.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Eric Brachmann, Alexander Krull, Frank Michel, Stefan Gumhold, Jamie Shotton,
and Carsten Rother.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Learning 6d object pose estimation using 3d object coordinates.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European conference on computer vision</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, pages 536â€“551.
Springer, 2014.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Eric Brachmann, Alexander Krull, Sebastian Nowozin, Jamie Shotton, Frank
Michel, Stefan Gumhold, and Carsten Rother.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Dsac-differentiable ransac for camera localization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, pages 6684â€“6692, 2017.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Eric Brachmann, Frank Michel, Alexander Krull, Michael YingÂ Yang, Stefan
Gumhold, and others.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Uncertainty-driven 6d pose estimation of objects and scenes from a
single rgb image.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, pages 3364â€“3372, 2016.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Eric Brachmann and Carsten Rother.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Learning less is more-6d camera localization via 3d surface
regression.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, pages 4654â€“4662, 2018.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Eric Brachmann and Carsten Rother.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Neural-guided ransac: Learning where to sample model hypotheses.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, pages 4322â€“4331, 2019.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Benjamin Busam, Tolga Birdal, and Nassir Navab.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Camera pose filtering with local regression geodesics on the
riemannian manifold of dual quaternions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision Workshops</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, pages 2436â€“2445, 2017.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Benjamin Busam, Marco Esposito, Simon Cheâ€™Rose, Nassir Navab, and Benjamin
Frisch.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">A stereo vision approach for cooperative robotic movement therapy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision Workshops</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, pages 127â€“135, 2015.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Benjamin Busam, HyunÂ Jun Jung, and Nassir Navab.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">I like to move it: 6d pose estimation as an action decision process.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2009.12678</span><span id="bib.bib10.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Berk Calli, Arjun Singh, Aaron Walsman, Siddhartha Srinivasa, Pieter Abbeel,
and AaronÂ M Dollar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">The ycb object and model set: Towards common benchmarks for
manipulation research.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advanced Robotics (ICAR), 2015 International Conference on</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">,
pages 510â€“517. IEEE, 2015.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Carlos Campos, Richard Elvira, Juan JÂ GÃ³mez RodrÃ­guez, JosÃ©Â MM
Montiel, and JuanÂ D TardÃ³s.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Orb-slam3: An accurate open-source library for visual,
visualâ€“inertial, and multimap slam.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Robotics</span><span id="bib.bib12.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Rethinking atrous convolution for semantic image segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1706.05587</span><span id="bib.bib13.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Qi Chen and Hartmut Prautzsch.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">General midpoint subdivision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1208.3794</span><span id="bib.bib14.4.2" class="ltx_text" style="font-size:90%;">, 2012.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Wenzheng Chen, Huan Ling, Jun Gao, Edward Smith, Jaakko Lehtinen, Alec
Jacobson, and Sanja Fidler.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Learning to predict 3d objects with an interpolation-based
differentiable renderer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib15.4.2" class="ltx_text" style="font-size:90%;">,
32:9609â€“9619, 2019.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Alvaro Collet, Manuel Martinez, and SiddharthaÂ S Srinivasa.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">The moped framework: Object recognition and pose estimation for
manipulation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">The international journal of robotics research</span><span id="bib.bib16.4.2" class="ltx_text" style="font-size:90%;">,
30(10):1284â€“1306, 2011.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Andrew Delong, Anton Osokin, HossamÂ N Isack, and Yuri Boykov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Fast approximate energy minimization with label costs.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International journal of computer vision</span><span id="bib.bib17.4.2" class="ltx_text" style="font-size:90%;">, 96(1):1â€“27, 2012.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Maximilian Denninger, Martin Sundermeyer, Dominik Winkelbauer, Youssef Zidan,
Dmitry Olefir, Mohamad Elbadrawy, Ahsan Lodhi, and Harinandan Katam.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Blenderproc.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1911.01911</span><span id="bib.bib18.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Yan Di, Fabian Manhardt, Gu Wang, Xiangyang Ji, Nassir Navab, and Federico
Tombari.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">So-pose: Exploiting self-occlusion for direct 6d pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, pages 12396â€“12405, 2021.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
T Do, Trung Pham, Ming Cai, and Ian Reid.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Real-time monocular object instance 6d pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">2019.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Ghazal Ghazaei, Iro Laina, Christian Rupprecht, Federico Tombari, Nassir Navab,
and Kianoush Nazarpour.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Dealing with ambiguity in robotic grasping via multiple predictions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Asian Conference on Computer Vision</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, pages 38â€“55. Springer,
2018.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Silvio Giancola, Matteo Valenti, and Remo Sala.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">A survey on 3D cameras: Metrological comparison of
time-of-flight, structured-light and active stereoscopy technologies</span><span id="bib.bib22.3.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.4.1" class="ltx_text" style="font-size:90%;">Springer, 2018.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, pages 770â€“778, 2016.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Yisheng He, Haibin Huang, Haoqiang Fan, Qifeng Chen, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Ffb6d: A full flow bidirectional fusion network for 6d pose
estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, pages 3003â€“3013, 2021.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Stefan Hinterstoisser, Cedric Cagniart, Slobodan Ilic, Peter Sturm, Nassir
Navab, Pascal Fua, and Vincent Lepetit.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Gradient response maps for real-time detection of textureless
objects.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE transactions on pattern analysis and machine intelligence</span><span id="bib.bib25.4.2" class="ltx_text" style="font-size:90%;">,
34(5):876â€“888, 2011.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Stefan Hinterstoisser, Cedric Cagniart, Slobodan Ilic, Peter Sturm, Nassir
Navab, Pascal Fua, and Vincent Lepetit.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Gradient response maps for real-time detection of textureless
objects.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span id="bib.bib26.4.2" class="ltx_text" style="font-size:90%;">,
34(5):876â€“888, 2012.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Stefan Hinterstoisser, Stefan Holzer, Cedric Cagniart, Slobodan Ilic, Kurt
Konolige, Nassir Navab, and Vincent Lepetit.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Multimodal templates for real-time detection of texture-less objects
in heavily cluttered scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2011 international conference on computer vision</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, pages
858â€“865. IEEE, 2011.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Stefan Hinterstoisser, Vincent Lepetit, Slobodan Ilic, Stefan Holzer, Gary
Bradski, Kurt Konolige, and Nassir Navab.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Model based training, detection and pose estimation of texture-less
3d objects in heavily cluttered scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Asian conference on computer vision</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, pages 548â€“562.
Springer, 2012.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Tomas Hodan, Daniel Barath, and Jiri Matas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Epos: Estimating 6d pose of objects with symmetries.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, pages 11703â€“11712, 2020.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Yinlin Hu, Pascal Fua, Wei Wang, and Mathieu Salzmann.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Single-stage 6d object pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, pages 2930â€“2939, 2020.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Yinlin Hu, Joachim Hugonot, Pascal Fua, and Mathieu Salzmann.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Segmentation-driven 6d object pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, pages 3385â€“3394, 2019.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Shun Iwase, Xingyu Liu, Rawal Khirodkar, Rio Yokota, and KrisÂ M Kitani.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Repose: Fast 6d object pose refinement via deep texture rendering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">, pages 3303â€“3312, 2021.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Hiroharu Kato, Yoshitaka Ushiku, and Tatsuya Harada.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Neural 3d mesh renderer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:90%;">, pages 3907â€“3916, 2018.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Wadim Kehl, Fabian Manhardt, Federico Tombari, Slobodan Ilic, and Nassir Navab.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">SSD-6D: Making RGB-based 3D detection and 6D pose estimation great
again.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the International Conference on Computer
Vision (ICCV 2017), Venice, Italy</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">, pages 22â€“29, 2017.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Alex Kendall, Matthew Grimes, and Roberto Cipolla.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Posenet: A convolutional network for real-time 6-dof camera
relocalization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE international conference on computer
vision</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:90%;">, pages 2938â€“2946, 2015.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
DiederikÂ P Kingma and Jimmy Ba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Adam: A method for stochastic optimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1412.6980</span><span id="bib.bib36.4.2" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Yann LabbÃ©, Justin Carpentier, Mathieu Aubry, and Josef Sivic.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Cosypose: Consistent multi-view multi-object 6d pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib37.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</span><span id="bib.bib37.5.3" class="ltx_text" style="font-size:90%;">, pages 574â€“591.
Springer, 2020.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Vincent Lepetit, Francesc Moreno-Noguer, and Pascal Fua.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Epnp: An accurate o (n) solution to the pnp problem.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International journal of computer vision</span><span id="bib.bib38.4.2" class="ltx_text" style="font-size:90%;">, 81(2):155, 2009.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Vincent Lepetit, Francesc Moreno-Noguer, and Pascal Fua.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">EPnP: An Accurate O(n) Solution to the PnP Problem.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Computer Vision</span><span id="bib.bib39.4.2" class="ltx_text" style="font-size:90%;">, 81(2):155â€“166, 2
2009.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Xiaolong Li, He Wang, Li Yi, LeonidasÂ J Guibas, AÂ Lynn Abbott, and Shuran Song.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Category-level articulated object pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib40.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib40.5.3" class="ltx_text" style="font-size:90%;">, pages 3706â€“3715, 2020.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Zhigang Li, Gu Wang, and Xiangyang Ji.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">Cdpn: Coordinates-based disentangled pose network for real-time
rgb-based 6-dof object pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib41.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib41.5.3" class="ltx_text" style="font-size:90%;">, pages 7678â€“7687, 2019.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
MatthewÂ M Loper and MichaelÂ J Black.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Opendr: An approximate differentiable renderer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib42.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</span><span id="bib.bib42.5.3" class="ltx_text" style="font-size:90%;">, pages 154â€“169.
Springer, 2014.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
DavidÂ G Lowe.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Distinctive image features from scale-invariant keypoints.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International journal of computer vision</span><span id="bib.bib43.4.2" class="ltx_text" style="font-size:90%;">, 60(2):91â€“110, 2004.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Fabian Manhardt, DiegoÂ Martin Arroyo, Christian Rupprecht, Benjamin Busam,
Tolga Birdal, Nassir Navab, and Federico Tombari.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Explaining the ambiguity of object detection and 6d pose from visual
data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib44.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib44.5.3" class="ltx_text" style="font-size:90%;">, pages 6841â€“6850, 2019.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Fabian Manhardt, Wadim Kehl, Nassir Navab, and Federico Tombari.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">Deep model-based 6d pose refinement in rgb.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib45.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision
(ECCV)</span><span id="bib.bib45.5.3" class="ltx_text" style="font-size:90%;">, pages 800â€“815, 2018.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Eric Marchand, Hideaki Uchiyama, and Fabien Spindler.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">Pose estimation for augmented reality: a hands-on survey.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE transactions on visualization and computer graphics</span><span id="bib.bib46.4.2" class="ltx_text" style="font-size:90%;">,
22(12):2633â€“2651, 2016.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Michihiko MIMOU, Takeo Kanade, and Toshiyuki SAKAI.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">A method of time-coded parallel planes of light for depth
measurement.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEICE TRANSACTIONS (1976-1990)</span><span id="bib.bib47.4.2" class="ltx_text" style="font-size:90%;">, 64(8):521â€“528, 1981.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
Markus Oberweger, Mahdi Rad, and Vincent Lepetit.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">Making Deep Heatmaps Robust to Partial Occlusions for 3D Object Pose
Estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text" style="font-size:90%;">pages 2â€“4.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
Kiru Park, Timothy Patten, and Markus Vincze.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">Pix2pose: Pixel-wise coordinate regression of objects for 6d pose
estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib49.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib49.5.3" class="ltx_text" style="font-size:90%;">, pages 7668â€“7677, 2019.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">Pvnet: Pixel-wise voting network for 6dof pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib50.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib50.5.3" class="ltx_text" style="font-size:90%;">, pages 4561â€“4570, 2019.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text" style="font-size:90%;">
Luis PÃ©rez, ÃÃ±igo Rodr\â€™\iguez, Nuria
Rodr\â€™\iguez, RubÃ©n Usamentiaga, and Daniel
Garc\â€™\ia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.2.1" class="ltx_text" style="font-size:90%;">Robot guidance using machine vision techniques in industrial
environments: A comparative review.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Sensors</span><span id="bib.bib51.4.2" class="ltx_text" style="font-size:90%;">, 16(3):335, 2016.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text" style="font-size:90%;">
JeffreyÂ L Posdamer and MD Altschuler.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.2.1" class="ltx_text" style="font-size:90%;">Surface measurement by space-encoded projected beam systems.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer graphics and image processing</span><span id="bib.bib52.4.2" class="ltx_text" style="font-size:90%;">, 18(1):1â€“17, 1982.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text" style="font-size:90%;">
Mahdi Rad and Vincent Lepetit.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.2.1" class="ltx_text" style="font-size:90%;">BB8: A scalable, accurate, robust to partial occlusion method for
predicting the 3D poses of challenging objects without using depth.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib53.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib53.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text" style="font-size:90%;">
Jason Rambach, Chengbiao Deng, Alain Pagani, and Didier Stricker.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.2.1" class="ltx_text" style="font-size:90%;">Learning 6dof object poses from synthetic single channel images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib54.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2018 IEEE International Symposium on Mixed and Augmented
Reality Adjunct (ISMAR-Adjunct)</span><span id="bib.bib54.5.3" class="ltx_text" style="font-size:90%;">, pages 164â€“169. IEEE, 2018.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text" style="font-size:90%;">
Jason Rambach, Alain Pagani, Michael Schneider, Oleksandr Artemenko, and Didier
Stricker.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.2.1" class="ltx_text" style="font-size:90%;">6dof object tracking based on 3d scans for augmented reality remote
live support.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Computers</span><span id="bib.bib55.4.2" class="ltx_text" style="font-size:90%;">, 7(1):6, 2018.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text" style="font-size:90%;">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.2.1" class="ltx_text" style="font-size:90%;">Faster r-cnn: Towards real-time object detection with region
proposal networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib56.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib56.5.3" class="ltx_text" style="font-size:90%;">, pages
91â€“99, 2015.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text" style="font-size:90%;">
Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary Bradski.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.2.1" class="ltx_text" style="font-size:90%;">ORB: An efficient alternative to SIFT or SURF.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib57.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2011 International Conference on Computer Vision</span><span id="bib.bib57.5.3" class="ltx_text" style="font-size:90%;">, pages
2564â€“2571. IEEE, 11 2011.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock"><span id="bib.bib58.1.1" class="ltx_text" style="font-size:90%;">
Joaquim Salvi, Jordi Pages, and Joan Batlle.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.2.1" class="ltx_text" style="font-size:90%;">Pattern codification strategies in structured light systems.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Pattern recognition</span><span id="bib.bib58.4.2" class="ltx_text" style="font-size:90%;">, 37(4):827â€“849, 2004.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock"><span id="bib.bib59.1.1" class="ltx_text" style="font-size:90%;">
Juil Sock, Guillermo Garcia-Hernando, Anil Armagan, and Tae-Kyun Kim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.2.1" class="ltx_text" style="font-size:90%;">Introducing pose consistency and warp-alignment for self-supervised
6d object pose estimation in color images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib59.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2020 International Conference on 3D Vision (3DV)</span><span id="bib.bib59.5.3" class="ltx_text" style="font-size:90%;">, pages
291â€“300. IEEE, 2020.
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock"><span id="bib.bib60.1.1" class="ltx_text" style="font-size:90%;">
Chen Song, Jiaru Song, and Qixing Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.2.1" class="ltx_text" style="font-size:90%;">Hybridpose: 6d object pose estimation under hybrid representations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib60.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition</span><span id="bib.bib60.5.3" class="ltx_text" style="font-size:90%;">, pages 431â€“440, 2020.
</span>
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock"><span id="bib.bib61.1.1" class="ltx_text" style="font-size:90%;">
Yongzhi Su, Jason Rambach, Alain Pagani, and Didier Stricker.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.2.1" class="ltx_text" style="font-size:90%;">Synpo-netâ€”accurate and fast cnn-based 6dof object pose estimation
using synthetic training.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Sensors</span><span id="bib.bib61.4.2" class="ltx_text" style="font-size:90%;">, 21(1):300, 2021.
</span>
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock"><span id="bib.bib62.1.1" class="ltx_text" style="font-size:90%;">
Martin Sundermeyer, Zoltan-Csaba Marton, Maximilian Durner, Manuel Brucker, and
Rudolph Triebel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.2.1" class="ltx_text" style="font-size:90%;">Implicit 3d orientation learning for 6d object detection from rgb
images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib62.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision
(ECCV)</span><span id="bib.bib62.5.3" class="ltx_text" style="font-size:90%;">, pages 699â€“715, 2018.
</span>
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock"><span id="bib.bib63.1.1" class="ltx_text" style="font-size:90%;">
Zhi Tian, Chunhua Shen, Hao Chen, and Tong He.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.2.1" class="ltx_text" style="font-size:90%;">Fcos: Fully convolutional one-stage object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib63.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF international conference on
computer vision</span><span id="bib.bib63.5.3" class="ltx_text" style="font-size:90%;">, pages 9627â€“9636, 2019.
</span>
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock"><span id="bib.bib64.1.1" class="ltx_text" style="font-size:90%;">
Marjan Trobina.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.2.1" class="ltx_text" style="font-size:90%;">Error model of a coded-light range sensor.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Technical report</span><span id="bib.bib64.4.2" class="ltx_text" style="font-size:90%;">, 1995.
</span>
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock"><span id="bib.bib65.1.1" class="ltx_text" style="font-size:90%;">
Joel Vidal, Chyi-Yeu Lin, and Robert Mart\â€™\i.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.2.1" class="ltx_text" style="font-size:90%;">6D pose estimation using an improved method based on point pair
features.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib65.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2018 4th International Conference on Control, Automation and
Robotics (ICCAR)</span><span id="bib.bib65.5.3" class="ltx_text" style="font-size:90%;">, pages 405â€“409. IEEE, 2018.
</span>
</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock"><span id="bib.bib66.1.1" class="ltx_text" style="font-size:90%;">
Gu Wang, Fabian Manhardt, Jianzhun Shao, Xiangyang Ji, Nassir Navab, and
Federico Tombari.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.2.1" class="ltx_text" style="font-size:90%;">Self6d: Self-supervised monocular 6d object pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib66.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</span><span id="bib.bib66.5.3" class="ltx_text" style="font-size:90%;">, pages 108â€“125.
Springer, 2020.
</span>
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock"><span id="bib.bib67.1.1" class="ltx_text" style="font-size:90%;">
Gu Wang, Fabian Manhardt, Federico Tombari, and Xiangyang Ji.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.2.1" class="ltx_text" style="font-size:90%;">Gdr-net: Geometry-guided direct regression network for monocular 6d
object pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib67.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib67.5.3" class="ltx_text" style="font-size:90%;">, pages 16611â€“16621, 2021.
</span>
</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock"><span id="bib.bib68.1.1" class="ltx_text" style="font-size:90%;">
He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran Song, and
LeonidasÂ J Guibas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.2.1" class="ltx_text" style="font-size:90%;">Normalized object coordinate space for category-level 6d object pose
and size estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib68.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib68.5.3" class="ltx_text" style="font-size:90%;">, pages 2642â€“2651, 2019.
</span>
</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock"><span id="bib.bib69.1.1" class="ltx_text" style="font-size:90%;">
Paul Wohlhart and Vincent Lepetit.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.2.1" class="ltx_text" style="font-size:90%;">Learning descriptors for object recognition and 3d pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib69.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib69.5.3" class="ltx_text" style="font-size:90%;">, pages 3109â€“3118, 2015.
</span>
</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock"><span id="bib.bib70.1.1" class="ltx_text" style="font-size:90%;">
Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, and Dieter Fox.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.2.1" class="ltx_text" style="font-size:90%;">Posecnn: A convolutional neural network for 6d object pose
estimation in cluttered scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1711.00199</span><span id="bib.bib70.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock"><span id="bib.bib71.1.1" class="ltx_text" style="font-size:90%;">
Sergey Zakharov, Wadim Kehl, and Slobodan Ilic.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.2.1" class="ltx_text" style="font-size:90%;">Deceptionnet: Network-driven domain randomization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib71.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib71.5.3" class="ltx_text" style="font-size:90%;">, pages 532â€“541, 2019.
</span>
</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock"><span id="bib.bib72.1.1" class="ltx_text" style="font-size:90%;">
Sergey Zakharov, IvanÂ S. Shugurov, and Slobodan Ilic.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.2.1" class="ltx_text" style="font-size:90%;">Dpod: 6d pose object detector and refiner.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">2019 IEEE/CVF International Conference on Computer Vision
(ICCV)</span><span id="bib.bib72.4.2" class="ltx_text" style="font-size:90%;">, pages 1941â€“1950, 2019.
</span>
</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock"><span id="bib.bib73.1.1" class="ltx_text" style="font-size:90%;">
Guangyuan Zhou, Huiqun Wang, Jiaxin Chen, and Di Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.2.1" class="ltx_text" style="font-size:90%;">Pr-gcn: A deep graph convolutional network with point refinement for
6d pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib73.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib73.5.3" class="ltx_text" style="font-size:90%;">, pages 2793â€“2802, 2021.
</span>
</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock"><span id="bib.bib74.1.1" class="ltx_text" style="font-size:90%;">
Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib74.2.1" class="ltx_text" style="font-size:90%;">On the continuity of rotation representations in neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib74.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib74.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib74.5.3" class="ltx_text" style="font-size:90%;">, pages 5745â€“5753, 2019.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Supplementary Material </h2>

<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Hyper-parameters in the Pose Solver</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">For RANSAC/PnPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, we set the threshold value for reprojection error as 2 pixels, and execute 150 iterations. For Progressive-XÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, we
also set the threshold value for the reprojection error as 2 pixels, and execute 400 iterations. The additional parameters for Progressive-X are â€neighborhood_ball_radius=20â€, â€spatial_coherence_weight=0.1â€, â€maximum_tanimoto_similarity=0.9â€.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>BOP Challenge</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">We submitted the results on 4 datasets of the BOP challenge and will test our method on the rest 3 datasets. The results are online in <a href="/https://bop.felk.cvut.cz/leaderboards/" title="" class="ltx_ref ltx_href">BOP Leaderboards</a> with the submission name â€zebraposeâ€.</p>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>YCB-V Evaluation per Object</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">We present a more detailed result on the YCB-V datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> in Tab.Â <a href="#S6.T5" title="Table 5 â€£ 6.3 YCB-V Evaluation per Object â€£ 6 Supplementary Material â€£ ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> and Tab.Â <a href="#S6.T6" title="Table 6 â€£ 6.3 YCB-V Evaluation per Object â€£ 6 Supplementary Material â€£ ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. As the Tab.Â <a href="#S6.T5" title="Table 5 â€£ 6.3 YCB-V Evaluation per Object â€£ 6 Supplementary Material â€£ ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows, in the evaluation of the estimate pose w.r.t ADD(-S) metric, we show major improvement over the state of the art.</p>
</div>
<div id="S6.SS3.p2" class="ltx_para">
<p id="S6.SS3.p2.1" class="ltx_p">In Tab.Â <a href="#S6.T6" title="Table 6 â€£ 6.3 YCB-V Evaluation per Object â€£ 6 Supplementary Material â€£ ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, we carefully calculated the AUC with all-points interpolation algorithm with the maximum threshold of 10 cm. If we calculate the AUC with 11-points interpolation, we will reach AUC of ADD-S of 94%, and AUC of ADD(-S) of 89.8%.</p>
</div>
<figure id="S6.T5" class="ltx_table">
<table id="S6.T5.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S6.T5.2.1" class="ltx_tr">
<td id="S6.T5.2.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">Method</td>
<td id="S6.T5.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">SegDrivenÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>
</td>
<td id="S6.T5.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Single-StageÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>
</td>
<td id="S6.T5.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">RePoseÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>
</td>
<td id="S6.T5.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">GDR-NetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>
</td>
<td id="S6.T5.2.1.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt"><span id="S6.T5.2.1.6.1" class="ltx_text ltx_font_bold">Ours</span></td>
</tr>
<tr id="S6.T5.2.2" class="ltx_tr">
<td id="S6.T5.2.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">002_master_chef_can</td>
<td id="S6.T5.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">33.0</td>
<td id="S6.T5.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S6.T5.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S6.T5.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">41.5</td>
<td id="S6.T5.2.2.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S6.T5.2.2.6.1" class="ltx_text ltx_font_bold">62.6</span></td>
</tr>
<tr id="S6.T5.2.3" class="ltx_tr">
<td id="S6.T5.2.3.1" class="ltx_td ltx_align_left ltx_border_r">003_cracker_box</td>
<td id="S6.T5.2.3.2" class="ltx_td ltx_align_center ltx_border_r">44.6</td>
<td id="S6.T5.2.3.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T5.2.3.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T5.2.3.5" class="ltx_td ltx_align_center ltx_border_r">83.2</td>
<td id="S6.T5.2.3.6" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S6.T5.2.3.6.1" class="ltx_text ltx_font_bold">98.5</span></td>
</tr>
<tr id="S6.T5.2.4" class="ltx_tr">
<td id="S6.T5.2.4.1" class="ltx_td ltx_align_left ltx_border_r">004_sugar_box</td>
<td id="S6.T5.2.4.2" class="ltx_td ltx_align_center ltx_border_r">75.6</td>
<td id="S6.T5.2.4.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T5.2.4.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T5.2.4.5" class="ltx_td ltx_align_center ltx_border_r">91.5</td>
<td id="S6.T5.2.4.6" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S6.T5.2.4.6.1" class="ltx_text ltx_font_bold">96.3</span></td>
</tr>
<tr id="S6.T5.2.5" class="ltx_tr">
<td id="S6.T5.2.5.1" class="ltx_td ltx_align_left ltx_border_r">005_tomato_soup_can</td>
<td id="S6.T5.2.5.2" class="ltx_td ltx_align_center ltx_border_r">40.8</td>
<td id="S6.T5.2.5.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T5.2.5.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T5.2.5.5" class="ltx_td ltx_align_center ltx_border_r">65.9</td>
<td id="S6.T5.2.5.6" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S6.T5.2.5.6.1" class="ltx_text ltx_font_bold">80.5</span></td>
</tr>
<tr id="S6.T5.2.6" class="ltx_tr">
<td id="S6.T5.2.6.1" class="ltx_td ltx_align_left ltx_border_r">006_mustard_bottle</td>
<td id="S6.T5.2.6.2" class="ltx_td ltx_align_center ltx_border_r">70.6</td>
<td id="S6.T5.2.6.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T5.2.6.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T5.2.6.5" class="ltx_td ltx_align_center ltx_border_r">90.2</td>
<td id="S6.T5.2.6.6" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S6.T5.2.6.6.1" class="ltx_text ltx_font_bold">100.0</span></td>
</tr>
<tr id="S6.T5.2.7" class="ltx_tr">
<td id="S6.T5.2.7.1" class="ltx_td ltx_align_left ltx_border_r">007_tuna_fish_can</td>
<td id="S6.T5.2.7.2" class="ltx_td ltx_align_center ltx_border_r">18.1</td>
<td id="S6.T5.2.7.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T5.2.7.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T5.2.7.5" class="ltx_td ltx_align_center ltx_border_r">44.2</td>
<td id="S6.T5.2.7.6" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S6.T5.2.7.6.1" class="ltx_text ltx_font_bold">70.5</span></td>
</tr>
<tr id="S6.T5.2.8" class="ltx_tr">
<td id="S6.T5.2.8.1" class="ltx_td ltx_align_left ltx_border_r">008_pudding_box</td>
<td id="S6.T5.2.8.2" class="ltx_td ltx_align_center ltx_border_r">12.2</td>
<td id="S6.T5.2.8.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T5.2.8.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T5.2.8.5" class="ltx_td ltx_align_center ltx_border_r">2.8</td>
<td id="S6.T5.2.8.6" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S6.T5.2.8.6.1" class="ltx_text ltx_font_bold">99.5</span></td>
</tr>
<tr id="S6.T5.2.9" class="ltx_tr">
<td id="S6.T5.2.9.1" class="ltx_td ltx_align_left ltx_border_r">009_gelatin_box</td>
<td id="S6.T5.2.9.2" class="ltx_td ltx_align_center ltx_border_r">59.4</td>
<td id="S6.T5.2.9.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T5.2.9.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T5.2.9.5" class="ltx_td ltx_align_center ltx_border_r">61.7</td>
<td id="S6.T5.2.9.6" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S6.T5.2.9.6.1" class="ltx_text ltx_font_bold">97.2</span></td>
</tr>
<tr id="S6.T5.2.10" class="ltx_tr">
<td id="S6.T5.2.10.1" class="ltx_td ltx_align_left ltx_border_r">010_potted_meat_can</td>
<td id="S6.T5.2.10.2" class="ltx_td ltx_align_center ltx_border_r">33.3</td>
<td id="S6.T5.2.10.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T5.2.10.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T5.2.10.5" class="ltx_td ltx_align_center ltx_border_r">64.9</td>
<td id="S6.T5.2.10.6" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S6.T5.2.10.6.1" class="ltx_text ltx_font_bold">76.9</span></td>
</tr>
<tr id="S6.T5.2.11" class="ltx_tr">
<td id="S6.T5.2.11.1" class="ltx_td ltx_align_left ltx_border_r">011_banana</td>
<td id="S6.T5.2.11.2" class="ltx_td ltx_align_center ltx_border_r">16.6</td>
<td id="S6.T5.2.11.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T5.2.11.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T5.2.11.5" class="ltx_td ltx_align_center ltx_border_r">64.1</td>
<td id="S6.T5.2.11.6" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S6.T5.2.11.6.1" class="ltx_text ltx_font_bold">71.2</span></td>
</tr>
<tr id="S6.T5.2.12" class="ltx_tr">
<td id="S6.T5.2.12.1" class="ltx_td ltx_align_left ltx_border_r">019_pitcher_base</td>
<td id="S6.T5.2.12.2" class="ltx_td ltx_align_center ltx_border_r">90.0</td>
<td id="S6.T5.2.12.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T5.2.12.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T5.2.12.5" class="ltx_td ltx_align_center ltx_border_r">99.0</td>
<td id="S6.T5.2.12.6" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S6.T5.2.12.6.1" class="ltx_text ltx_font_bold">100.0</span></td>
</tr>
<tr id="S6.T5.2.13" class="ltx_tr">
<td id="S6.T5.2.13.1" class="ltx_td ltx_align_left ltx_border_r">021_bleach_cleanser</td>
<td id="S6.T5.2.13.2" class="ltx_td ltx_align_center ltx_border_r">70.9</td>
<td id="S6.T5.2.13.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T5.2.13.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T5.2.13.5" class="ltx_td ltx_align_center ltx_border_r">73.8</td>
<td id="S6.T5.2.13.6" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S6.T5.2.13.6.1" class="ltx_text ltx_font_bold">75.9</span></td>
</tr>
<tr id="S6.T5.2.14" class="ltx_tr">
<td id="S6.T5.2.14.1" class="ltx_td ltx_align_left ltx_border_r">024_bowl*</td>
<td id="S6.T5.2.14.2" class="ltx_td ltx_align_center ltx_border_r">30.5</td>
<td id="S6.T5.2.14.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T5.2.14.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T5.2.14.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S6.T5.2.14.5.1" class="ltx_text ltx_font_bold">37.7</span></td>
<td id="S6.T5.2.14.6" class="ltx_td ltx_nopad_r ltx_align_center">18.5</td>
</tr>
<tr id="S6.T5.2.15" class="ltx_tr">
<td id="S6.T5.2.15.1" class="ltx_td ltx_align_left ltx_border_r">025_mug</td>
<td id="S6.T5.2.15.2" class="ltx_td ltx_align_center ltx_border_r">40.7</td>
<td id="S6.T5.2.15.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T5.2.15.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T5.2.15.5" class="ltx_td ltx_align_center ltx_border_r">61.5</td>
<td id="S6.T5.2.15.6" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S6.T5.2.15.6.1" class="ltx_text ltx_font_bold">77.5</span></td>
</tr>
<tr id="S6.T5.2.16" class="ltx_tr">
<td id="S6.T5.2.16.1" class="ltx_td ltx_align_left ltx_border_r">035_power_drill</td>
<td id="S6.T5.2.16.2" class="ltx_td ltx_align_center ltx_border_r">63.5</td>
<td id="S6.T5.2.16.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T5.2.16.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T5.2.16.5" class="ltx_td ltx_align_center ltx_border_r">78.5</td>
<td id="S6.T5.2.16.6" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S6.T5.2.16.6.1" class="ltx_text ltx_font_bold">97.4</span></td>
</tr>
<tr id="S6.T5.2.17" class="ltx_tr">
<td id="S6.T5.2.17.1" class="ltx_td ltx_align_left ltx_border_r">036_wood_block*</td>
<td id="S6.T5.2.17.2" class="ltx_td ltx_align_center ltx_border_r">27.7</td>
<td id="S6.T5.2.17.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T5.2.17.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T5.2.17.5" class="ltx_td ltx_align_center ltx_border_r">59.5</td>
<td id="S6.T5.2.17.6" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S6.T5.2.17.6.1" class="ltx_text ltx_font_bold">87.6</span></td>
</tr>
<tr id="S6.T5.2.18" class="ltx_tr">
<td id="S6.T5.2.18.1" class="ltx_td ltx_align_left ltx_border_r">037_scissors</td>
<td id="S6.T5.2.18.2" class="ltx_td ltx_align_center ltx_border_r">17.1</td>
<td id="S6.T5.2.18.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T5.2.18.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T5.2.18.5" class="ltx_td ltx_align_center ltx_border_r">3.9</td>
<td id="S6.T5.2.18.6" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S6.T5.2.18.6.1" class="ltx_text ltx_font_bold">71.8</span></td>
</tr>
<tr id="S6.T5.2.19" class="ltx_tr">
<td id="S6.T5.2.19.1" class="ltx_td ltx_align_left ltx_border_r">040_large_marker</td>
<td id="S6.T5.2.19.2" class="ltx_td ltx_align_center ltx_border_r">4.8</td>
<td id="S6.T5.2.19.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T5.2.19.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T5.2.19.5" class="ltx_td ltx_align_center ltx_border_r">7.4</td>
<td id="S6.T5.2.19.6" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S6.T5.2.19.6.1" class="ltx_text ltx_font_bold">23.3</span></td>
</tr>
<tr id="S6.T5.2.20" class="ltx_tr">
<td id="S6.T5.2.20.1" class="ltx_td ltx_align_left ltx_border_r">051_large_clamp*</td>
<td id="S6.T5.2.20.2" class="ltx_td ltx_align_center ltx_border_r">25.6</td>
<td id="S6.T5.2.20.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T5.2.20.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T5.2.20.5" class="ltx_td ltx_align_center ltx_border_r">69.8</td>
<td id="S6.T5.2.20.6" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S6.T5.2.20.6.1" class="ltx_text ltx_font_bold">87.6</span></td>
</tr>
<tr id="S6.T5.2.21" class="ltx_tr">
<td id="S6.T5.2.21.1" class="ltx_td ltx_align_left ltx_border_r">052_extra_large_clamp*</td>
<td id="S6.T5.2.21.2" class="ltx_td ltx_align_center ltx_border_r">8.8</td>
<td id="S6.T5.2.21.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T5.2.21.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T5.2.21.5" class="ltx_td ltx_align_center ltx_border_r">90.0</td>
<td id="S6.T5.2.21.6" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S6.T5.2.21.6.1" class="ltx_text ltx_font_bold">98.0</span></td>
</tr>
<tr id="S6.T5.2.22" class="ltx_tr">
<td id="S6.T5.2.22.1" class="ltx_td ltx_align_left ltx_border_r">061_foam_brick*</td>
<td id="S6.T5.2.22.2" class="ltx_td ltx_align_center ltx_border_r">34.7</td>
<td id="S6.T5.2.22.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T5.2.22.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T5.2.22.5" class="ltx_td ltx_align_center ltx_border_r">71.9</td>
<td id="S6.T5.2.22.6" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S6.T5.2.22.6.1" class="ltx_text ltx_font_bold">99.3</span></td>
</tr>
<tr id="S6.T5.2.23" class="ltx_tr">
<td id="S6.T5.2.23.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t">mean</td>
<td id="S6.T5.2.23.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">39.0</td>
<td id="S6.T5.2.23.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">53.9</td>
<td id="S6.T5.2.23.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">62.1</td>
<td id="S6.T5.2.23.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">60.1</td>
<td id="S6.T5.2.23.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t"><span id="S6.T5.2.23.6.1" class="ltx_text ltx_font_bold">80.5</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S6.T5.4.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="S6.T5.5.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Comparison with State of the Art on YCB-V<span id="S6.T5.5.2.1" class="ltx_text ltx_font_medium">. We report the Average Recall of ADD(-S) in % and compare with state of the art. (*) denotes symmetric objects, (-) denotes the results missing from the original paper.</span></span></figcaption>
</figure>
<figure id="S6.T6" class="ltx_table">
<table id="S6.T6.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S6.T6.2.1" class="ltx_tr">
<td id="S6.T6.2.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">Method</td>
<td id="S6.T6.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2">PoseCNNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite>
</td>
<td id="S6.T6.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2">CosyPose<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>
</td>
<td id="S6.T6.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2">GDR-NetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>
</td>
<td id="S6.T6.2.1.5" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S6.T6.2.1.5.1" class="ltx_text ltx_font_bold">Ours</span></td>
</tr>
<tr id="S6.T6.2.2" class="ltx_tr">
<td id="S6.T6.2.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Metric</td>
<td id="S6.T6.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S6.T6.2.2.2.1" class="ltx_text"></span> <span id="S6.T6.2.2.2.2" class="ltx_text">
<span id="S6.T6.2.2.2.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T6.2.2.2.2.1.1" class="ltx_tr">
<span id="S6.T6.2.2.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">AUC of</span></span>
<span id="S6.T6.2.2.2.2.1.2" class="ltx_tr">
<span id="S6.T6.2.2.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">ADD-S</span></span>
</span></span><span id="S6.T6.2.2.2.3" class="ltx_text"></span></td>
<td id="S6.T6.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S6.T6.2.2.3.1" class="ltx_text"></span> <span id="S6.T6.2.2.3.2" class="ltx_text">
<span id="S6.T6.2.2.3.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T6.2.2.3.2.1.1" class="ltx_tr">
<span id="S6.T6.2.2.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">AUC of</span></span>
<span id="S6.T6.2.2.3.2.1.2" class="ltx_tr">
<span id="S6.T6.2.2.3.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">ADD(-S)</span></span>
</span></span><span id="S6.T6.2.2.3.3" class="ltx_text"></span></td>
<td id="S6.T6.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S6.T6.2.2.4.1" class="ltx_text"></span> <span id="S6.T6.2.2.4.2" class="ltx_text">
<span id="S6.T6.2.2.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T6.2.2.4.2.1.1" class="ltx_tr">
<span id="S6.T6.2.2.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">AUC of</span></span>
<span id="S6.T6.2.2.4.2.1.2" class="ltx_tr">
<span id="S6.T6.2.2.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">ADD-S</span></span>
</span></span><span id="S6.T6.2.2.4.3" class="ltx_text"></span></td>
<td id="S6.T6.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S6.T6.2.2.5.1" class="ltx_text"></span> <span id="S6.T6.2.2.5.2" class="ltx_text">
<span id="S6.T6.2.2.5.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T6.2.2.5.2.1.1" class="ltx_tr">
<span id="S6.T6.2.2.5.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">AUC of</span></span>
<span id="S6.T6.2.2.5.2.1.2" class="ltx_tr">
<span id="S6.T6.2.2.5.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">ADD(-S)</span></span>
</span></span><span id="S6.T6.2.2.5.3" class="ltx_text"></span></td>
<td id="S6.T6.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S6.T6.2.2.6.1" class="ltx_text"></span> <span id="S6.T6.2.2.6.2" class="ltx_text">
<span id="S6.T6.2.2.6.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T6.2.2.6.2.1.1" class="ltx_tr">
<span id="S6.T6.2.2.6.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">AUC of</span></span>
<span id="S6.T6.2.2.6.2.1.2" class="ltx_tr">
<span id="S6.T6.2.2.6.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">ADD-S</span></span>
</span></span><span id="S6.T6.2.2.6.3" class="ltx_text"></span></td>
<td id="S6.T6.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S6.T6.2.2.7.1" class="ltx_text"></span> <span id="S6.T6.2.2.7.2" class="ltx_text">
<span id="S6.T6.2.2.7.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T6.2.2.7.2.1.1" class="ltx_tr">
<span id="S6.T6.2.2.7.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">AUC of</span></span>
<span id="S6.T6.2.2.7.2.1.2" class="ltx_tr">
<span id="S6.T6.2.2.7.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">ADD(-S)</span></span>
</span></span><span id="S6.T6.2.2.7.3" class="ltx_text"></span></td>
<td id="S6.T6.2.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S6.T6.2.2.8.1" class="ltx_text"></span> <span id="S6.T6.2.2.8.2" class="ltx_text">
<span id="S6.T6.2.2.8.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T6.2.2.8.2.1.1" class="ltx_tr">
<span id="S6.T6.2.2.8.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">AUC of</span></span>
<span id="S6.T6.2.2.8.2.1.2" class="ltx_tr">
<span id="S6.T6.2.2.8.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">ADD-S</span></span>
</span></span><span id="S6.T6.2.2.8.3" class="ltx_text"></span></td>
<td id="S6.T6.2.2.9" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">
<span id="S6.T6.2.2.9.1" class="ltx_text"></span> <span id="S6.T6.2.2.9.2" class="ltx_text">
<span id="S6.T6.2.2.9.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T6.2.2.9.2.1.1" class="ltx_tr">
<span id="S6.T6.2.2.9.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">AUC of</span></span>
<span id="S6.T6.2.2.9.2.1.2" class="ltx_tr">
<span id="S6.T6.2.2.9.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">ADD(-S)</span></span>
</span></span><span id="S6.T6.2.2.9.3" class="ltx_text"></span></td>
</tr>
<tr id="S6.T6.2.3" class="ltx_tr">
<td id="S6.T6.2.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">002_master_chef_can</td>
<td id="S6.T6.2.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">84.0</td>
<td id="S6.T6.2.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">50.9</td>
<td id="S6.T6.2.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S6.T6.2.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S6.T6.2.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">96.3</td>
<td id="S6.T6.2.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">65.2</td>
<td id="S6.T6.2.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">93.7</td>
<td id="S6.T6.2.3.9" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">75.4</td>
</tr>
<tr id="S6.T6.2.4" class="ltx_tr">
<td id="S6.T6.2.4.1" class="ltx_td ltx_align_left ltx_border_r">003_cracker_box</td>
<td id="S6.T6.2.4.2" class="ltx_td ltx_align_center ltx_border_r">76.9</td>
<td id="S6.T6.2.4.3" class="ltx_td ltx_align_center ltx_border_r">51.7</td>
<td id="S6.T6.2.4.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T6.2.4.5" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T6.2.4.6" class="ltx_td ltx_align_center ltx_border_r">97.0</td>
<td id="S6.T6.2.4.7" class="ltx_td ltx_align_center ltx_border_r">88.8</td>
<td id="S6.T6.2.4.8" class="ltx_td ltx_align_center ltx_border_r">93.0</td>
<td id="S6.T6.2.4.9" class="ltx_td ltx_nopad_r ltx_align_center">87.8</td>
</tr>
<tr id="S6.T6.2.5" class="ltx_tr">
<td id="S6.T6.2.5.1" class="ltx_td ltx_align_left ltx_border_r">004_sugar_box</td>
<td id="S6.T6.2.5.2" class="ltx_td ltx_align_center ltx_border_r">84.3</td>
<td id="S6.T6.2.5.3" class="ltx_td ltx_align_center ltx_border_r">68.6</td>
<td id="S6.T6.2.5.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T6.2.5.5" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T6.2.5.6" class="ltx_td ltx_align_center ltx_border_r">98.9</td>
<td id="S6.T6.2.5.7" class="ltx_td ltx_align_center ltx_border_r">95.0</td>
<td id="S6.T6.2.5.8" class="ltx_td ltx_align_center ltx_border_r">95.1</td>
<td id="S6.T6.2.5.9" class="ltx_td ltx_nopad_r ltx_align_center">90.9</td>
</tr>
<tr id="S6.T6.2.6" class="ltx_tr">
<td id="S6.T6.2.6.1" class="ltx_td ltx_align_left ltx_border_r">005_tomato_soup_can</td>
<td id="S6.T6.2.6.2" class="ltx_td ltx_align_center ltx_border_r">80.9</td>
<td id="S6.T6.2.6.3" class="ltx_td ltx_align_center ltx_border_r">66.0</td>
<td id="S6.T6.2.6.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T6.2.6.5" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T6.2.6.6" class="ltx_td ltx_align_center ltx_border_r">96.5</td>
<td id="S6.T6.2.6.7" class="ltx_td ltx_align_center ltx_border_r">91.9</td>
<td id="S6.T6.2.6.8" class="ltx_td ltx_align_center ltx_border_r">94.4</td>
<td id="S6.T6.2.6.9" class="ltx_td ltx_nopad_r ltx_align_center">90.1</td>
</tr>
<tr id="S6.T6.2.7" class="ltx_tr">
<td id="S6.T6.2.7.1" class="ltx_td ltx_align_left ltx_border_r">006_mustard_bottle</td>
<td id="S6.T6.2.7.2" class="ltx_td ltx_align_center ltx_border_r">90.2</td>
<td id="S6.T6.2.7.3" class="ltx_td ltx_align_center ltx_border_r">79.9</td>
<td id="S6.T6.2.7.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T6.2.7.5" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T6.2.7.6" class="ltx_td ltx_align_center ltx_border_r">100</td>
<td id="S6.T6.2.7.7" class="ltx_td ltx_align_center ltx_border_r">92.8</td>
<td id="S6.T6.2.7.8" class="ltx_td ltx_align_center ltx_border_r">96.0</td>
<td id="S6.T6.2.7.9" class="ltx_td ltx_nopad_r ltx_align_center">92.6</td>
</tr>
<tr id="S6.T6.2.8" class="ltx_tr">
<td id="S6.T6.2.8.1" class="ltx_td ltx_align_left ltx_border_r">007_tuna_fish_can</td>
<td id="S6.T6.2.8.2" class="ltx_td ltx_align_center ltx_border_r">87.9</td>
<td id="S6.T6.2.8.3" class="ltx_td ltx_align_center ltx_border_r">70.4</td>
<td id="S6.T6.2.8.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T6.2.8.5" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T6.2.8.6" class="ltx_td ltx_align_center ltx_border_r">99.4</td>
<td id="S6.T6.2.8.7" class="ltx_td ltx_align_center ltx_border_r">94.2</td>
<td id="S6.T6.2.8.8" class="ltx_td ltx_align_center ltx_border_r">96.9</td>
<td id="S6.T6.2.8.9" class="ltx_td ltx_nopad_r ltx_align_center">92.6</td>
</tr>
<tr id="S6.T6.2.9" class="ltx_tr">
<td id="S6.T6.2.9.1" class="ltx_td ltx_align_left ltx_border_r">008_pudding_box</td>
<td id="S6.T6.2.9.2" class="ltx_td ltx_align_center ltx_border_r">79.0</td>
<td id="S6.T6.2.9.3" class="ltx_td ltx_align_center ltx_border_r">62.9</td>
<td id="S6.T6.2.9.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T6.2.9.5" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T6.2.9.6" class="ltx_td ltx_align_center ltx_border_r">64.6</td>
<td id="S6.T6.2.9.7" class="ltx_td ltx_align_center ltx_border_r">44.7</td>
<td id="S6.T6.2.9.8" class="ltx_td ltx_align_center ltx_border_r">97.2</td>
<td id="S6.T6.2.9.9" class="ltx_td ltx_nopad_r ltx_align_center">95.3</td>
</tr>
<tr id="S6.T6.2.10" class="ltx_tr">
<td id="S6.T6.2.10.1" class="ltx_td ltx_align_left ltx_border_r">009_gelatin_box</td>
<td id="S6.T6.2.10.2" class="ltx_td ltx_align_center ltx_border_r">87.1</td>
<td id="S6.T6.2.10.3" class="ltx_td ltx_align_center ltx_border_r">75.2</td>
<td id="S6.T6.2.10.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T6.2.10.5" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T6.2.10.6" class="ltx_td ltx_align_center ltx_border_r">97.1</td>
<td id="S6.T6.2.10.7" class="ltx_td ltx_align_center ltx_border_r">92.5</td>
<td id="S6.T6.2.10.8" class="ltx_td ltx_align_center ltx_border_r">96.8</td>
<td id="S6.T6.2.10.9" class="ltx_td ltx_nopad_r ltx_align_center">94.8</td>
</tr>
<tr id="S6.T6.2.11" class="ltx_tr">
<td id="S6.T6.2.11.1" class="ltx_td ltx_align_left ltx_border_r">010_potted_meat_can</td>
<td id="S6.T6.2.11.2" class="ltx_td ltx_align_center ltx_border_r">78.5</td>
<td id="S6.T6.2.11.3" class="ltx_td ltx_align_center ltx_border_r">59.6</td>
<td id="S6.T6.2.11.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T6.2.11.5" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T6.2.11.6" class="ltx_td ltx_align_center ltx_border_r">86.0</td>
<td id="S6.T6.2.11.7" class="ltx_td ltx_align_center ltx_border_r">80.2</td>
<td id="S6.T6.2.11.8" class="ltx_td ltx_align_center ltx_border_r">91.7</td>
<td id="S6.T6.2.11.9" class="ltx_td ltx_nopad_r ltx_align_center">83.6</td>
</tr>
<tr id="S6.T6.2.12" class="ltx_tr">
<td id="S6.T6.2.12.1" class="ltx_td ltx_align_left ltx_border_r">011_banana</td>
<td id="S6.T6.2.12.2" class="ltx_td ltx_align_center ltx_border_r">85.9</td>
<td id="S6.T6.2.12.3" class="ltx_td ltx_align_center ltx_border_r">72.3</td>
<td id="S6.T6.2.12.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T6.2.12.5" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T6.2.12.6" class="ltx_td ltx_align_center ltx_border_r">96.3</td>
<td id="S6.T6.2.12.7" class="ltx_td ltx_align_center ltx_border_r">85.8</td>
<td id="S6.T6.2.12.8" class="ltx_td ltx_align_center ltx_border_r">92.6</td>
<td id="S6.T6.2.12.9" class="ltx_td ltx_nopad_r ltx_align_center">84.6</td>
</tr>
<tr id="S6.T6.2.13" class="ltx_tr">
<td id="S6.T6.2.13.1" class="ltx_td ltx_align_left ltx_border_r">019_pitcher_base</td>
<td id="S6.T6.2.13.2" class="ltx_td ltx_align_center ltx_border_r">76.8</td>
<td id="S6.T6.2.13.3" class="ltx_td ltx_align_center ltx_border_r">52.5</td>
<td id="S6.T6.2.13.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T6.2.13.5" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T6.2.13.6" class="ltx_td ltx_align_center ltx_border_r">99.9</td>
<td id="S6.T6.2.13.7" class="ltx_td ltx_align_center ltx_border_r">98.5</td>
<td id="S6.T6.2.13.8" class="ltx_td ltx_align_center ltx_border_r">96.4</td>
<td id="S6.T6.2.13.9" class="ltx_td ltx_nopad_r ltx_align_center">93.4</td>
</tr>
<tr id="S6.T6.2.14" class="ltx_tr">
<td id="S6.T6.2.14.1" class="ltx_td ltx_align_left ltx_border_r">021_bleach_cleanser</td>
<td id="S6.T6.2.14.2" class="ltx_td ltx_align_center ltx_border_r">71.9</td>
<td id="S6.T6.2.14.3" class="ltx_td ltx_align_center ltx_border_r">50.5</td>
<td id="S6.T6.2.14.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T6.2.14.5" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T6.2.14.6" class="ltx_td ltx_align_center ltx_border_r">94.2</td>
<td id="S6.T6.2.14.7" class="ltx_td ltx_align_center ltx_border_r">84.3</td>
<td id="S6.T6.2.14.8" class="ltx_td ltx_align_center ltx_border_r">89.5</td>
<td id="S6.T6.2.14.9" class="ltx_td ltx_nopad_r ltx_align_center">80.0</td>
</tr>
<tr id="S6.T6.2.15" class="ltx_tr">
<td id="S6.T6.2.15.1" class="ltx_td ltx_align_left ltx_border_r">024_bowl*</td>
<td id="S6.T6.2.15.2" class="ltx_td ltx_align_center ltx_border_r">69.7</td>
<td id="S6.T6.2.15.3" class="ltx_td ltx_align_center ltx_border_r">69.7</td>
<td id="S6.T6.2.15.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T6.2.15.5" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T6.2.15.6" class="ltx_td ltx_align_center ltx_border_r">85.7</td>
<td id="S6.T6.2.15.7" class="ltx_td ltx_align_center ltx_border_r">85.7</td>
<td id="S6.T6.2.15.8" class="ltx_td ltx_align_center ltx_border_r">37.1</td>
<td id="S6.T6.2.15.9" class="ltx_td ltx_nopad_r ltx_align_center">37.1</td>
</tr>
<tr id="S6.T6.2.16" class="ltx_tr">
<td id="S6.T6.2.16.1" class="ltx_td ltx_align_left ltx_border_r">025_mug</td>
<td id="S6.T6.2.16.2" class="ltx_td ltx_align_center ltx_border_r">78.0</td>
<td id="S6.T6.2.16.3" class="ltx_td ltx_align_center ltx_border_r">57.7</td>
<td id="S6.T6.2.16.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T6.2.16.5" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T6.2.16.6" class="ltx_td ltx_align_center ltx_border_r">99.6</td>
<td id="S6.T6.2.16.7" class="ltx_td ltx_align_center ltx_border_r">94.0</td>
<td id="S6.T6.2.16.8" class="ltx_td ltx_align_center ltx_border_r">96.1</td>
<td id="S6.T6.2.16.9" class="ltx_td ltx_nopad_r ltx_align_center">90.8</td>
</tr>
<tr id="S6.T6.2.17" class="ltx_tr">
<td id="S6.T6.2.17.1" class="ltx_td ltx_align_left ltx_border_r">035_power_drill</td>
<td id="S6.T6.2.17.2" class="ltx_td ltx_align_center ltx_border_r">72.8</td>
<td id="S6.T6.2.17.3" class="ltx_td ltx_align_center ltx_border_r">55.1</td>
<td id="S6.T6.2.17.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T6.2.17.5" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T6.2.17.6" class="ltx_td ltx_align_center ltx_border_r">97.5</td>
<td id="S6.T6.2.17.7" class="ltx_td ltx_align_center ltx_border_r">90.1</td>
<td id="S6.T6.2.17.8" class="ltx_td ltx_align_center ltx_border_r">95.0</td>
<td id="S6.T6.2.17.9" class="ltx_td ltx_nopad_r ltx_align_center">89.7</td>
</tr>
<tr id="S6.T6.2.18" class="ltx_tr">
<td id="S6.T6.2.18.1" class="ltx_td ltx_align_left ltx_border_r">036_wood_block*</td>
<td id="S6.T6.2.18.2" class="ltx_td ltx_align_center ltx_border_r">65.8</td>
<td id="S6.T6.2.18.3" class="ltx_td ltx_align_center ltx_border_r">65.8</td>
<td id="S6.T6.2.18.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T6.2.18.5" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T6.2.18.6" class="ltx_td ltx_align_center ltx_border_r">82.5</td>
<td id="S6.T6.2.18.7" class="ltx_td ltx_align_center ltx_border_r">82.5</td>
<td id="S6.T6.2.18.8" class="ltx_td ltx_align_center ltx_border_r">84.5</td>
<td id="S6.T6.2.18.9" class="ltx_td ltx_nopad_r ltx_align_center">84.5</td>
</tr>
<tr id="S6.T6.2.19" class="ltx_tr">
<td id="S6.T6.2.19.1" class="ltx_td ltx_align_left ltx_border_r">037_scissors</td>
<td id="S6.T6.2.19.2" class="ltx_td ltx_align_center ltx_border_r">56.2</td>
<td id="S6.T6.2.19.3" class="ltx_td ltx_align_center ltx_border_r">35.8</td>
<td id="S6.T6.2.19.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T6.2.19.5" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T6.2.19.6" class="ltx_td ltx_align_center ltx_border_r">63.8</td>
<td id="S6.T6.2.19.7" class="ltx_td ltx_align_center ltx_border_r">49.5</td>
<td id="S6.T6.2.19.8" class="ltx_td ltx_align_center ltx_border_r">92.5</td>
<td id="S6.T6.2.19.9" class="ltx_td ltx_nopad_r ltx_align_center">84.5</td>
</tr>
<tr id="S6.T6.2.20" class="ltx_tr">
<td id="S6.T6.2.20.1" class="ltx_td ltx_align_left ltx_border_r">040_large_marker</td>
<td id="S6.T6.2.20.2" class="ltx_td ltx_align_center ltx_border_r">71.4</td>
<td id="S6.T6.2.20.3" class="ltx_td ltx_align_center ltx_border_r">58.0</td>
<td id="S6.T6.2.20.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T6.2.20.5" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T6.2.20.6" class="ltx_td ltx_align_center ltx_border_r">88.0</td>
<td id="S6.T6.2.20.7" class="ltx_td ltx_align_center ltx_border_r">76.1</td>
<td id="S6.T6.2.20.8" class="ltx_td ltx_align_center ltx_border_r">80.4</td>
<td id="S6.T6.2.20.9" class="ltx_td ltx_nopad_r ltx_align_center">69.5</td>
</tr>
<tr id="S6.T6.2.21" class="ltx_tr">
<td id="S6.T6.2.21.1" class="ltx_td ltx_align_left ltx_border_r">051_large_clamp*</td>
<td id="S6.T6.2.21.2" class="ltx_td ltx_align_center ltx_border_r">49.9</td>
<td id="S6.T6.2.21.3" class="ltx_td ltx_align_center ltx_border_r">49.9</td>
<td id="S6.T6.2.21.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T6.2.21.5" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T6.2.21.6" class="ltx_td ltx_align_center ltx_border_r">89.3</td>
<td id="S6.T6.2.21.7" class="ltx_td ltx_align_center ltx_border_r">89.3</td>
<td id="S6.T6.2.21.8" class="ltx_td ltx_align_center ltx_border_r">85.6</td>
<td id="S6.T6.2.21.9" class="ltx_td ltx_nopad_r ltx_align_center">85.6</td>
</tr>
<tr id="S6.T6.2.22" class="ltx_tr">
<td id="S6.T6.2.22.1" class="ltx_td ltx_align_left ltx_border_r">052_extra_large_clamp*</td>
<td id="S6.T6.2.22.2" class="ltx_td ltx_align_center ltx_border_r">47.0</td>
<td id="S6.T6.2.22.3" class="ltx_td ltx_align_center ltx_border_r">47.0</td>
<td id="S6.T6.2.22.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T6.2.22.5" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T6.2.22.6" class="ltx_td ltx_align_center ltx_border_r">93.5</td>
<td id="S6.T6.2.22.7" class="ltx_td ltx_align_center ltx_border_r">93.5</td>
<td id="S6.T6.2.22.8" class="ltx_td ltx_align_center ltx_border_r">92.5</td>
<td id="S6.T6.2.22.9" class="ltx_td ltx_nopad_r ltx_align_center">92.5</td>
</tr>
<tr id="S6.T6.2.23" class="ltx_tr">
<td id="S6.T6.2.23.1" class="ltx_td ltx_align_left ltx_border_r">061_foam_brick*</td>
<td id="S6.T6.2.23.2" class="ltx_td ltx_align_center ltx_border_r">87.8</td>
<td id="S6.T6.2.23.3" class="ltx_td ltx_align_center ltx_border_r">87.8</td>
<td id="S6.T6.2.23.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T6.2.23.5" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T6.2.23.6" class="ltx_td ltx_align_center ltx_border_r">96.9</td>
<td id="S6.T6.2.23.7" class="ltx_td ltx_align_center ltx_border_r">96.9</td>
<td id="S6.T6.2.23.8" class="ltx_td ltx_align_center ltx_border_r">95.3</td>
<td id="S6.T6.2.23.9" class="ltx_td ltx_nopad_r ltx_align_center">95.3</td>
</tr>
<tr id="S6.T6.2.24" class="ltx_tr">
<td id="S6.T6.2.24.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t">mean</td>
<td id="S6.T6.2.24.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">75.9</td>
<td id="S6.T6.2.24.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">61.3</td>
<td id="S6.T6.2.24.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">89.8</td>
<td id="S6.T6.2.24.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">84.5</td>
<td id="S6.T6.2.24.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">91.6</td>
<td id="S6.T6.2.24.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">84.3</td>
<td id="S6.T6.2.24.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">90.1</td>
<td id="S6.T6.2.24.9" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t">85.3</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S6.T6.4.1.1" class="ltx_text" style="font-size:90%;">Table 6</span>: </span><span id="S6.T6.5.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Comparison with State of the Art on YCB-V<span id="S6.T6.5.2.1" class="ltx_text ltx_font_medium">. We report the Average Recall w.r.t AUC of ADD(-S) and AUC of ADD-S in % and compare with state of the art. (*) denotes symmetric objects, (-) denotes the results missing from the original paper.</span></span></figcaption>
</figure>
</section>
<section id="S6.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4 </span>Qualitative Results</h3>

<section id="S6.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.4.1 </span>Vertex Code Prediction LM-O</h4>

<div id="S6.SS4.SSS1.p1" class="ltx_para">
<p id="S6.SS4.SSS1.p1.1" class="ltx_p">We visualized the predicted binary code of the â€duckâ€ object in LM-O dataset<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> with a few examples in Fig.Â <a href="#S6.F4" title="Figure 4 â€£ 6.4.1 Vertex Code Prediction LM-O â€£ 6.4 Qualitative Results â€£ 6 Supplementary Material â€£ ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Due to the size limits, we only show the predicted binary code till the 11-th bits. We render the object with the predicted pose on top of the original input ROI. To make the predicted pose more visible in the figure, we set the colour of the object model as red just for this figure. So the duck appears with the orange colour (red + yellow) in the last row. We can see that the rendered object overlapped the object in the original image quite well, indicating that our predicted pose is very accurate.</p>
</div>
<figure id="S6.F4" class="ltx_figure"><img src="/html/2203.09418/assets/x4.png" id="S6.F4.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="438" height="551" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S6.F4.3.2" class="ltx_text" style="font-size:90%;">We visualized the predicted binary code of the â€duckâ€ in LM-O dataset<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> with a few examples. Due to the size limits, we only show the predicted binary code till the 11-th bit. We set the colour of the object model as red and render the object with the predicted pose on the top of the input ROI. We can see that the rendered object overlaps the object in the image quite well.</span></figcaption>
</figure>
</section>
<section id="S6.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.4.2 </span>Pose Prediction LM-O</h4>

<div id="S6.SS4.SSS2.p1" class="ltx_para">
<p id="S6.SS4.SSS2.p1.1" class="ltx_p">Qualitative Results on LM-O<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> can be found in Fig.Â <a href="#S6.F5" title="Figure 5 â€£ 6.4.2 Pose Prediction LM-O â€£ 6.4 Qualitative Results â€£ 6 Supplementary Material â€£ ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. We render the objects with estimated pose on top of the original images. The presented confidence scores are from the 2D object detection with FCOS detectorÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>.</p>
</div>
<figure id="S6.F5" class="ltx_figure"><img src="/html/2203.09418/assets/x5.png" id="S6.F5.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="438" height="563" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F5.3.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S6.F5.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Qualitative Results on LM-O<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite><span id="S6.F5.4.2.1" class="ltx_text ltx_font_medium">: We render the objects with estimated pose on top of the original images. The presented confidence score are from the 2D object detection with FCOS detectorÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>.</span></span></figcaption>
</figure>
</section>
<section id="S6.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.4.3 </span>Pose Prediction YCB-V</h4>

<div id="S6.SS4.SSS3.p1" class="ltx_para">
<p id="S6.SS4.SSS3.p1.1" class="ltx_p">Qualitative Results on YCB-V<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> are available in Fig.Â <a href="#S6.F6" title="Figure 6 â€£ 6.4.3 Pose Prediction YCB-V â€£ 6.4 Qualitative Results â€£ 6 Supplementary Material â€£ ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. We render the objects with estimated pose on top of the original images. The presented confidence scores are from the 2D object detection with FCOS detectorÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>.</p>
</div>
<figure id="S6.F6" class="ltx_figure"><img src="/html/2203.09418/assets/x6.png" id="S6.F6.g1" class="ltx_graphics ltx_centering ltx_img_square" width="438" height="492" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F6.3.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S6.F6.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Qualitative Results on YCB-V<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite><span id="S6.F6.4.2.1" class="ltx_text ltx_font_medium">: We render the objects with estimated pose on top of the original images. The presented confidence score are from the 2D object detection with FCOS detectorÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>.</span></span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2203.09417" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2203.09418" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2203.09418">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2203.09418" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2203.09420" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Mar 11 10:15:22 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
