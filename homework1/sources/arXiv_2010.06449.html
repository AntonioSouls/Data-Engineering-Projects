<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2010.06449] A review of 3D human pose estimation algorithms for markerless motion capture</title><meta property="og:description" content="Human pose estimation is a very active research field, stimulated by its important applications in robotics, entertainment or health and sports sciences, among others. Advances in convolutional networks triggered noticâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A review of 3D human pose estimation algorithms for markerless motion capture">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="A review of 3D human pose estimation algorithms for markerless motion capture">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2010.06449">

<!--Generated on Mon Feb 26 19:49:41 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_fleqn ltx_pruned_first">
<h1 class="ltx_title ltx_title_document">A review of 3D human pose estimation algorithms for markerless motion capture</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yann <span id="id1.1.id1" class="ltx_text" style="color:#FF0000;">Desmarais</span>
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Denis <span id="id2.1.id1" class="ltx_text" style="color:#FF0000;">Mottet</span>
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Pierre <span id="id3.1.id1" class="ltx_text" style="color:#FF0000;">Slangen</span>
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Philippe <span id="id4.1.id1" class="ltx_text" style="color:#FF0000;">Montesinos</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:philippe.montesinos@mines-ales.fr">philippe.montesinos@mines-ales.fr</a>
</span>
<span class="ltx_contact ltx_role_address"> EuroMov Digital Health in Motion, Univ Montpellier, IMT Mines Ales, 30100 Ales, France
</span>
<span class="ltx_contact ltx_role_address"> EuroMov Digital Health in Motion, Univ Montpellier, IMT Mines Ales, 34090 Montpellier, France
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p">Human pose estimation is a very active research field, stimulated by its important applications in robotics, entertainment or health and sports sciences, among others. Advances in convolutional networks triggered noticeable improvements in 2D pose estimation, leading modern 3D markerless motion capture techniques to an average error per joint of 20 mm. However, with the proliferation of methods, it is becoming increasingly difficult to make an informed choice.
Here, we review the leading human pose estimation methods of the past five years, focusing on metrics, benchmarks and method structures. We propose a taxonomy based on accuracy, speed and robustness that we use to classify de methods and derive directions for future research.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>
<span id="id6.id1" class="ltx_ERROR undefined">\KWD</span>3D Human Pose Estimation, Convolutional Neural Networks, Survey

</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_journal"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">journal: </span>Computer Vision and Image Understanding</span></span></span>
<div id="p2" class="ltx_para">
<p id="p2.1" class="ltx_p"><span id="p2.1.1" class="ltx_text ltx_font_bold">This article is under consideration at Computer Vision and Image Understanding.</span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Human Pose Estimation is the extraction of body configurations in images or videos. Typically, it is the inference of joint coordinates and the reconstruction of a human skeletal representation. In the last few years, 2D pose estimation reached detection rate above 90% on all different human joints <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Newell etÂ al.</span> (<a href="#bib.bib54" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2016</span></a>)</cite>. This progress has been possible in great part because of the success of convolutional neural networks (CNN) and the appearance of accessible large scale datasets (<cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Sigal etÂ al.</span> (<a href="#bib.bib64" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2010</span></a>)</cite>; <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Ionescu etÂ al.</span> (<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2014</span></a>)</cite>). However, it is only recently that these new architectures have been deployed to solve a similar problem in 3D. The challenge for these new 3D markerless pose estimation methods is to be competitive against classical techniques and marker-based motion capture systems. The ultimate goal would be a complete and accurate 3D reconstruction of an individualâ€™s motion from simple monocular images with tolerance to severe occlusion. As this ideal is unrealistic, results on similar tasks seem to indicate that it is possible to reach some of these conditions even if all are not fulfilled.
<br class="ltx_break"></p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Traditionally, commercial motion capture systems track small reflective markers placed on the surface of subjects. While precise, traditional motion capture is heavily constrained by complex sensors and acquisition environment. In 2010, Microsoft released the Kinect sensor that captured human pose from RGB and depth images. However, it was mostly aimed at entertainment usage and was not suitable for outdoor acquisition.
<br class="ltx_break"></p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">However, pose and person detection algorithms based only on image features have existed since a long time. The former paradigm was the fitting of human defined features to complex part-based human models (representations of the silhouette with cylinders, stick-figures, meshes, cones or boxes). While some modern techniques still use that approach, the feature extraction part of the process is now realized using convolutional neural networks.
<br class="ltx_break"></p>
</div>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Other Surveys</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Bray (<a href="#bib.bib4" title="" class="ltx_ref">2000</a>)</cite> reviews optical markerless methods with a taxonomy based on commonly performed subtasks: Initialization, Tracking, Pose Estimation and Recognition (Fig.<a href="#S1.F1" title="Fig. 1 â€£ 1.1 Other Surveys â€£ 1 Introduction â€£ A review of 3D human pose estimation algorithms for markerless motion capture" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). With this classification, they describe the different ways to extract visual features for the pose estimation process and then how to track them between frames.
<br class="ltx_break"></p>
</div>
<div id="S1.SS1.p2" class="ltx_para">
<p id="S1.SS1.p2.1" class="ltx_p">The survey of <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Moeslund and Granum</span> (<a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2001</span></a>)</cite> explores twenty years of vision-based human pose estimation techniques from 1980 to the early 2000s, including marker-based and markerless methods. At that time, many assumptions were taken to facilitate the process of extracting the human silhouette: most methods functioned indoor with non-shifting lighting and nearly half of them used uniform static backgrounds. This survey provides a good history of the different families of pose estimation. The authors also carefully describe the different degrees of performances needed for applications using human pose estimation and how to quantify them. However, their functionality-based taxonomy is no longer adapted for todayâ€™s techniques, because modern methods mostly do not use an initialization step and perform pose estimation and tracking at the same time. Finally, action recognition is nowadays a computer vision task with its own community.
<br class="ltx_break"></p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2010.06449/assets/mocap.png" id="S1.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="180" height="169" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Fig. 1: </span>Classical motion capture system </figcaption>
</figure>
<div id="S1.SS1.p3" class="ltx_para">
<p id="S1.SS1.p3.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Sarafinos etÂ al.</span> (<a href="#bib.bib62" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2016</span></a>)</cite> wrote a review of more recent methods with an emphasis on input data. They suggest a taxonomy that divides pose estimation between monocular versus multi-view techniques and between static image versus video inputs. This distinction is still present in our review, but our analysis is more oriented towards the family of method employed (learning, model-based etc.). They also evaluate part-based and learning-based methods on modern benchmarks. They also propose a synthetic dataset (SynPose300) to evaluate robustness (erroneous initialization, viewing distance, difficult poses or actions). The focus is made on methods from 2008 until 2016. We chose to start our review from 2017, to present recent evolutions in 3D pose estimation.
<br class="ltx_break"></p>
</div>
<div id="S1.SS1.p4" class="ltx_para">
<p id="S1.SS1.p4.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Colyer etÂ al.</span> (<a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite> examined the historical methods used in biomechanical studies of human motion. The authors insist that training and validating markerless methods with marker-based ones is not completely correct, because the reflecting markers can modify the results (helping or degrading the results otherwise obtainable in an â€in-the-wildâ€ context). They also state that marker-based methods are not providing the accuracy of measurement to characterize real human movement for some sport science and biomechanical experiments. However, more precise methods are invasive and impractical, therefore most research is still conducted with optoelectronic commercial systems (which also have some inaccuracies). Furthermore, they compare the accuracy of markerless methods in the HumanEva benchmark <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Sigal etÂ al.</span> (<a href="#bib.bib64" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2010</span></a>)</cite> stating that the precision required for most analysis in sport science is not yet achieved with the current markerless algorithms. While giving a good overview of the field, this survey does not discuss more recent methods (<cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Pavllo etÂ al.</span> (<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite>; <cite class="ltx_cite ltx_citemacro_cite">Cheng etÂ al. (<a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite>) that produce results with five times better accuracy of their best reviewed methods. It also gathers results from an older dataset and not on the more recent ones such as Human3.6M that contains more images with better resolution.
<br class="ltx_break"></p>
</div>
<div id="S1.SS1.p5" class="ltx_para">
<p id="S1.SS1.p5.1" class="ltx_p">Finally, <cite class="ltx_cite ltx_citemacro_cite">Chen etÂ al. (<a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite> detailed recent monocular 2D and 3D pose estimation techniques that are based on deep learning. They explain in detail the different categories of methods that currently exist and the evaluation protocols and metrics for this task. More than twenty methods are reviewed from 2014 and forward. Our review differs as it focuses on 3D pose estimation, but also considers multi-view settings as well as techniques employing different sensors such as inertial measurement units (IMU).</p>
</div>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2 </span>Proposed Approach</h3>

<div id="S1.SS2.p1" class="ltx_para">
<p id="S1.SS2.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Moeslund and Granum</span> (<a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2001</span></a>)</cite> adopts three main criteria to evaluate pose estimation systemsâ€‰:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S1.I1.ix1.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S1.I1.ix1.1.1.m1.1b"><mo id="S1.I1.ix1.1.1.m1.1.1" xref="S1.I1.ix1.1.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S1.I1.ix1.1.1.m1.1c"><ci id="S1.I1.ix1.1.1.m1.1.1.cmml" xref="S1.I1.ix1.1.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.ix1.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S1.I1.ix1.p1" class="ltx_para">
<p id="S1.I1.ix1.p1.1" class="ltx_p">Accuracy</p>
</div>
</li>
<li id="S1.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S1.I1.ix2.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S1.I1.ix2.1.1.m1.1b"><mo id="S1.I1.ix2.1.1.m1.1.1" xref="S1.I1.ix2.1.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S1.I1.ix2.1.1.m1.1c"><ci id="S1.I1.ix2.1.1.m1.1.1.cmml" xref="S1.I1.ix2.1.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.ix2.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S1.I1.ix2.p1" class="ltx_para">
<p id="S1.I1.ix2.p1.1" class="ltx_p">Speed</p>
</div>
</li>
<li id="S1.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S1.I1.ix3.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S1.I1.ix3.1.1.m1.1b"><mo id="S1.I1.ix3.1.1.m1.1.1" xref="S1.I1.ix3.1.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S1.I1.ix3.1.1.m1.1c"><ci id="S1.I1.ix3.1.1.m1.1.1.cmml" xref="S1.I1.ix3.1.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.ix3.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S1.I1.ix3.p1" class="ltx_para">
<p id="S1.I1.ix3.p1.1" class="ltx_p">Robustness</p>
</div>
</li>
</ul>
</div>
<div id="S1.SS2.p2" class="ltx_para">
<p id="S1.SS2.p2.1" class="ltx_p">The relevance to different fields (surveillance, control or analysis) is then studied. Despite the needs of these domains of application having evolved, this analysis is still relevant. In the present review, our main objective is to guide the choices of developers, engineers and researchers who want to build upon the reviewed algorithms. We compare recently published academic methods with regard to their relevance to different application fields. First, we describe the metrics and benchmarks that are commonly used for method evaluation in section <a href="#S2" title="2 Methods Evaluation â€£ A review of 3D human pose estimation algorithms for markerless motion capture" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Then, we detail and explain the families of architectures used for human pose estimation in section <a href="#S3" title="3 Architectures for Human Pose Estimation â€£ A review of 3D human pose estimation algorithms for markerless motion capture" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Next, we present the current state-of-the-art techniques for 3D markerless pose estimation with an emphasis on carefully selected articles in <a href="#S4" title="4 Methods Review â€£ A review of 3D human pose estimation algorithms for markerless motion capture" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Finally an overall analysis of accuracy, robustness and speed performance indices is available in section <a href="#S5" title="5 Analysis and discussion â€£ A review of 3D human pose estimation algorithms for markerless motion capture" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methods Evaluation</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">This section describes how to evaluate markerless pose estimation methods in 3D. To compare these methods, we use evaluations provided across multiple benchmarks datasets. The designers of these datasets often recommend different metrics. We start by describing how these metrics are computed and discuss their relevance in different contexts. For each of these benchmarks the quantity of images, the environments and acquisition modalities are also detailed.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Metrics</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Several metrics measure the accuracy of pose estimation algorithms in 3D. Some are processing the average error, other a detection rate with a predefined threshold and finally some uses perceptual or structural criteria. In this subsection, these metrics are described and their strengths and fail cases are discussed in different contexts.
<br class="ltx_break"></p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p"><span id="S2.SS1.p2.1.1" class="ltx_text ltx_font_bold">MPJPE</span>: Mean Per Joint Position Error. It is one of the most frequently used metrics found in the literature. It is also sometimes referred to as mean reconstruction error or 3D error. MPJPE is the mean of the Euclidean distances between the estimated coordinate and ground truth coordinate over each jointâ€‰:
<br class="ltx_break"></p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S2.E1.m1.5" class="ltx_Math" alttext="MPJPE(x,\hat{x})=\frac{1}{N}\sum_{i=1}^{N}\left\|m_{\hat{x}}(i)-m_{x}(i)\right\|" display="block"><semantics id="S2.E1.m1.5a"><mrow id="S2.E1.m1.5.5" xref="S2.E1.m1.5.5.cmml"><mrow id="S2.E1.m1.5.5.3" xref="S2.E1.m1.5.5.3.cmml"><mi id="S2.E1.m1.5.5.3.2" xref="S2.E1.m1.5.5.3.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.5.5.3.1" xref="S2.E1.m1.5.5.3.1.cmml">â€‹</mo><mi id="S2.E1.m1.5.5.3.3" xref="S2.E1.m1.5.5.3.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.5.5.3.1a" xref="S2.E1.m1.5.5.3.1.cmml">â€‹</mo><mi id="S2.E1.m1.5.5.3.4" xref="S2.E1.m1.5.5.3.4.cmml">J</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.5.5.3.1b" xref="S2.E1.m1.5.5.3.1.cmml">â€‹</mo><mi id="S2.E1.m1.5.5.3.5" xref="S2.E1.m1.5.5.3.5.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.5.5.3.1c" xref="S2.E1.m1.5.5.3.1.cmml">â€‹</mo><mi id="S2.E1.m1.5.5.3.6" xref="S2.E1.m1.5.5.3.6.cmml">E</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.5.5.3.1d" xref="S2.E1.m1.5.5.3.1.cmml">â€‹</mo><mrow id="S2.E1.m1.5.5.3.7.2" xref="S2.E1.m1.5.5.3.7.1.cmml"><mo stretchy="false" id="S2.E1.m1.5.5.3.7.2.1" xref="S2.E1.m1.5.5.3.7.1.cmml">(</mo><mi id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">x</mi><mo id="S2.E1.m1.5.5.3.7.2.2" xref="S2.E1.m1.5.5.3.7.1.cmml">,</mo><mover accent="true" id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml"><mi id="S2.E1.m1.2.2.2" xref="S2.E1.m1.2.2.2.cmml">x</mi><mo id="S2.E1.m1.2.2.1" xref="S2.E1.m1.2.2.1.cmml">^</mo></mover><mo stretchy="false" id="S2.E1.m1.5.5.3.7.2.3" xref="S2.E1.m1.5.5.3.7.1.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.5.5.2" xref="S2.E1.m1.5.5.2.cmml">=</mo><mrow id="S2.E1.m1.5.5.1" xref="S2.E1.m1.5.5.1.cmml"><mfrac id="S2.E1.m1.5.5.1.3" xref="S2.E1.m1.5.5.1.3.cmml"><mn id="S2.E1.m1.5.5.1.3.2" xref="S2.E1.m1.5.5.1.3.2.cmml">1</mn><mi id="S2.E1.m1.5.5.1.3.3" xref="S2.E1.m1.5.5.1.3.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S2.E1.m1.5.5.1.2" xref="S2.E1.m1.5.5.1.2.cmml">â€‹</mo><mrow id="S2.E1.m1.5.5.1.1" xref="S2.E1.m1.5.5.1.1.cmml"><munderover id="S2.E1.m1.5.5.1.1.2" xref="S2.E1.m1.5.5.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S2.E1.m1.5.5.1.1.2.2.2" xref="S2.E1.m1.5.5.1.1.2.2.2.cmml">âˆ‘</mo><mrow id="S2.E1.m1.5.5.1.1.2.2.3" xref="S2.E1.m1.5.5.1.1.2.2.3.cmml"><mi id="S2.E1.m1.5.5.1.1.2.2.3.2" xref="S2.E1.m1.5.5.1.1.2.2.3.2.cmml">i</mi><mo id="S2.E1.m1.5.5.1.1.2.2.3.1" xref="S2.E1.m1.5.5.1.1.2.2.3.1.cmml">=</mo><mn id="S2.E1.m1.5.5.1.1.2.2.3.3" xref="S2.E1.m1.5.5.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S2.E1.m1.5.5.1.1.2.3" xref="S2.E1.m1.5.5.1.1.2.3.cmml">N</mi></munderover><mrow id="S2.E1.m1.5.5.1.1.1.1" xref="S2.E1.m1.5.5.1.1.1.2.cmml"><mo id="S2.E1.m1.5.5.1.1.1.1.2" xref="S2.E1.m1.5.5.1.1.1.2.1.cmml">â€–</mo><mrow id="S2.E1.m1.5.5.1.1.1.1.1" xref="S2.E1.m1.5.5.1.1.1.1.1.cmml"><mrow id="S2.E1.m1.5.5.1.1.1.1.1.2" xref="S2.E1.m1.5.5.1.1.1.1.1.2.cmml"><msub id="S2.E1.m1.5.5.1.1.1.1.1.2.2" xref="S2.E1.m1.5.5.1.1.1.1.1.2.2.cmml"><mi id="S2.E1.m1.5.5.1.1.1.1.1.2.2.2" xref="S2.E1.m1.5.5.1.1.1.1.1.2.2.2.cmml">m</mi><mover accent="true" id="S2.E1.m1.5.5.1.1.1.1.1.2.2.3" xref="S2.E1.m1.5.5.1.1.1.1.1.2.2.3.cmml"><mi id="S2.E1.m1.5.5.1.1.1.1.1.2.2.3.2" xref="S2.E1.m1.5.5.1.1.1.1.1.2.2.3.2.cmml">x</mi><mo id="S2.E1.m1.5.5.1.1.1.1.1.2.2.3.1" xref="S2.E1.m1.5.5.1.1.1.1.1.2.2.3.1.cmml">^</mo></mover></msub><mo lspace="0em" rspace="0em" id="S2.E1.m1.5.5.1.1.1.1.1.2.1" xref="S2.E1.m1.5.5.1.1.1.1.1.2.1.cmml">â€‹</mo><mrow id="S2.E1.m1.5.5.1.1.1.1.1.2.3.2" xref="S2.E1.m1.5.5.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.E1.m1.5.5.1.1.1.1.1.2.3.2.1" xref="S2.E1.m1.5.5.1.1.1.1.1.2.cmml">(</mo><mi id="S2.E1.m1.3.3" xref="S2.E1.m1.3.3.cmml">i</mi><mo stretchy="false" id="S2.E1.m1.5.5.1.1.1.1.1.2.3.2.2" xref="S2.E1.m1.5.5.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.5.5.1.1.1.1.1.1" xref="S2.E1.m1.5.5.1.1.1.1.1.1.cmml">âˆ’</mo><mrow id="S2.E1.m1.5.5.1.1.1.1.1.3" xref="S2.E1.m1.5.5.1.1.1.1.1.3.cmml"><msub id="S2.E1.m1.5.5.1.1.1.1.1.3.2" xref="S2.E1.m1.5.5.1.1.1.1.1.3.2.cmml"><mi id="S2.E1.m1.5.5.1.1.1.1.1.3.2.2" xref="S2.E1.m1.5.5.1.1.1.1.1.3.2.2.cmml">m</mi><mi id="S2.E1.m1.5.5.1.1.1.1.1.3.2.3" xref="S2.E1.m1.5.5.1.1.1.1.1.3.2.3.cmml">x</mi></msub><mo lspace="0em" rspace="0em" id="S2.E1.m1.5.5.1.1.1.1.1.3.1" xref="S2.E1.m1.5.5.1.1.1.1.1.3.1.cmml">â€‹</mo><mrow id="S2.E1.m1.5.5.1.1.1.1.1.3.3.2" xref="S2.E1.m1.5.5.1.1.1.1.1.3.cmml"><mo stretchy="false" id="S2.E1.m1.5.5.1.1.1.1.1.3.3.2.1" xref="S2.E1.m1.5.5.1.1.1.1.1.3.cmml">(</mo><mi id="S2.E1.m1.4.4" xref="S2.E1.m1.4.4.cmml">i</mi><mo stretchy="false" id="S2.E1.m1.5.5.1.1.1.1.1.3.3.2.2" xref="S2.E1.m1.5.5.1.1.1.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E1.m1.5.5.1.1.1.1.3" xref="S2.E1.m1.5.5.1.1.1.2.1.cmml">â€–</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.5b"><apply id="S2.E1.m1.5.5.cmml" xref="S2.E1.m1.5.5"><eq id="S2.E1.m1.5.5.2.cmml" xref="S2.E1.m1.5.5.2"></eq><apply id="S2.E1.m1.5.5.3.cmml" xref="S2.E1.m1.5.5.3"><times id="S2.E1.m1.5.5.3.1.cmml" xref="S2.E1.m1.5.5.3.1"></times><ci id="S2.E1.m1.5.5.3.2.cmml" xref="S2.E1.m1.5.5.3.2">ğ‘€</ci><ci id="S2.E1.m1.5.5.3.3.cmml" xref="S2.E1.m1.5.5.3.3">ğ‘ƒ</ci><ci id="S2.E1.m1.5.5.3.4.cmml" xref="S2.E1.m1.5.5.3.4">ğ½</ci><ci id="S2.E1.m1.5.5.3.5.cmml" xref="S2.E1.m1.5.5.3.5">ğ‘ƒ</ci><ci id="S2.E1.m1.5.5.3.6.cmml" xref="S2.E1.m1.5.5.3.6">ğ¸</ci><interval closure="open" id="S2.E1.m1.5.5.3.7.1.cmml" xref="S2.E1.m1.5.5.3.7.2"><ci id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">ğ‘¥</ci><apply id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2"><ci id="S2.E1.m1.2.2.1.cmml" xref="S2.E1.m1.2.2.1">^</ci><ci id="S2.E1.m1.2.2.2.cmml" xref="S2.E1.m1.2.2.2">ğ‘¥</ci></apply></interval></apply><apply id="S2.E1.m1.5.5.1.cmml" xref="S2.E1.m1.5.5.1"><times id="S2.E1.m1.5.5.1.2.cmml" xref="S2.E1.m1.5.5.1.2"></times><apply id="S2.E1.m1.5.5.1.3.cmml" xref="S2.E1.m1.5.5.1.3"><divide id="S2.E1.m1.5.5.1.3.1.cmml" xref="S2.E1.m1.5.5.1.3"></divide><cn type="integer" id="S2.E1.m1.5.5.1.3.2.cmml" xref="S2.E1.m1.5.5.1.3.2">1</cn><ci id="S2.E1.m1.5.5.1.3.3.cmml" xref="S2.E1.m1.5.5.1.3.3">ğ‘</ci></apply><apply id="S2.E1.m1.5.5.1.1.cmml" xref="S2.E1.m1.5.5.1.1"><apply id="S2.E1.m1.5.5.1.1.2.cmml" xref="S2.E1.m1.5.5.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.5.5.1.1.2.1.cmml" xref="S2.E1.m1.5.5.1.1.2">superscript</csymbol><apply id="S2.E1.m1.5.5.1.1.2.2.cmml" xref="S2.E1.m1.5.5.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.5.5.1.1.2.2.1.cmml" xref="S2.E1.m1.5.5.1.1.2">subscript</csymbol><sum id="S2.E1.m1.5.5.1.1.2.2.2.cmml" xref="S2.E1.m1.5.5.1.1.2.2.2"></sum><apply id="S2.E1.m1.5.5.1.1.2.2.3.cmml" xref="S2.E1.m1.5.5.1.1.2.2.3"><eq id="S2.E1.m1.5.5.1.1.2.2.3.1.cmml" xref="S2.E1.m1.5.5.1.1.2.2.3.1"></eq><ci id="S2.E1.m1.5.5.1.1.2.2.3.2.cmml" xref="S2.E1.m1.5.5.1.1.2.2.3.2">ğ‘–</ci><cn type="integer" id="S2.E1.m1.5.5.1.1.2.2.3.3.cmml" xref="S2.E1.m1.5.5.1.1.2.2.3.3">1</cn></apply></apply><ci id="S2.E1.m1.5.5.1.1.2.3.cmml" xref="S2.E1.m1.5.5.1.1.2.3">ğ‘</ci></apply><apply id="S2.E1.m1.5.5.1.1.1.2.cmml" xref="S2.E1.m1.5.5.1.1.1.1"><csymbol cd="latexml" id="S2.E1.m1.5.5.1.1.1.2.1.cmml" xref="S2.E1.m1.5.5.1.1.1.1.2">norm</csymbol><apply id="S2.E1.m1.5.5.1.1.1.1.1.cmml" xref="S2.E1.m1.5.5.1.1.1.1.1"><minus id="S2.E1.m1.5.5.1.1.1.1.1.1.cmml" xref="S2.E1.m1.5.5.1.1.1.1.1.1"></minus><apply id="S2.E1.m1.5.5.1.1.1.1.1.2.cmml" xref="S2.E1.m1.5.5.1.1.1.1.1.2"><times id="S2.E1.m1.5.5.1.1.1.1.1.2.1.cmml" xref="S2.E1.m1.5.5.1.1.1.1.1.2.1"></times><apply id="S2.E1.m1.5.5.1.1.1.1.1.2.2.cmml" xref="S2.E1.m1.5.5.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.5.5.1.1.1.1.1.2.2.1.cmml" xref="S2.E1.m1.5.5.1.1.1.1.1.2.2">subscript</csymbol><ci id="S2.E1.m1.5.5.1.1.1.1.1.2.2.2.cmml" xref="S2.E1.m1.5.5.1.1.1.1.1.2.2.2">ğ‘š</ci><apply id="S2.E1.m1.5.5.1.1.1.1.1.2.2.3.cmml" xref="S2.E1.m1.5.5.1.1.1.1.1.2.2.3"><ci id="S2.E1.m1.5.5.1.1.1.1.1.2.2.3.1.cmml" xref="S2.E1.m1.5.5.1.1.1.1.1.2.2.3.1">^</ci><ci id="S2.E1.m1.5.5.1.1.1.1.1.2.2.3.2.cmml" xref="S2.E1.m1.5.5.1.1.1.1.1.2.2.3.2">ğ‘¥</ci></apply></apply><ci id="S2.E1.m1.3.3.cmml" xref="S2.E1.m1.3.3">ğ‘–</ci></apply><apply id="S2.E1.m1.5.5.1.1.1.1.1.3.cmml" xref="S2.E1.m1.5.5.1.1.1.1.1.3"><times id="S2.E1.m1.5.5.1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.5.5.1.1.1.1.1.3.1"></times><apply id="S2.E1.m1.5.5.1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.5.5.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E1.m1.5.5.1.1.1.1.1.3.2.1.cmml" xref="S2.E1.m1.5.5.1.1.1.1.1.3.2">subscript</csymbol><ci id="S2.E1.m1.5.5.1.1.1.1.1.3.2.2.cmml" xref="S2.E1.m1.5.5.1.1.1.1.1.3.2.2">ğ‘š</ci><ci id="S2.E1.m1.5.5.1.1.1.1.1.3.2.3.cmml" xref="S2.E1.m1.5.5.1.1.1.1.1.3.2.3">ğ‘¥</ci></apply><ci id="S2.E1.m1.4.4.cmml" xref="S2.E1.m1.4.4">ğ‘–</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.5c">MPJPE(x,\hat{x})=\frac{1}{N}\sum_{i=1}^{N}\left\|m_{\hat{x}}(i)-m_{x}(i)\right\|</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<br class="ltx_break">
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.2" class="ltx_p">N represents the number of processed joints, <math id="S2.SS1.p4.1.m1.1" class="ltx_Math" alttext="m_{\hat{x}}(i)" display="inline"><semantics id="S2.SS1.p4.1.m1.1a"><mrow id="S2.SS1.p4.1.m1.1.2" xref="S2.SS1.p4.1.m1.1.2.cmml"><msub id="S2.SS1.p4.1.m1.1.2.2" xref="S2.SS1.p4.1.m1.1.2.2.cmml"><mi id="S2.SS1.p4.1.m1.1.2.2.2" xref="S2.SS1.p4.1.m1.1.2.2.2.cmml">m</mi><mover accent="true" id="S2.SS1.p4.1.m1.1.2.2.3" xref="S2.SS1.p4.1.m1.1.2.2.3.cmml"><mi id="S2.SS1.p4.1.m1.1.2.2.3.2" xref="S2.SS1.p4.1.m1.1.2.2.3.2.cmml">x</mi><mo id="S2.SS1.p4.1.m1.1.2.2.3.1" xref="S2.SS1.p4.1.m1.1.2.2.3.1.cmml">^</mo></mover></msub><mo lspace="0em" rspace="0em" id="S2.SS1.p4.1.m1.1.2.1" xref="S2.SS1.p4.1.m1.1.2.1.cmml">â€‹</mo><mrow id="S2.SS1.p4.1.m1.1.2.3.2" xref="S2.SS1.p4.1.m1.1.2.cmml"><mo stretchy="false" id="S2.SS1.p4.1.m1.1.2.3.2.1" xref="S2.SS1.p4.1.m1.1.2.cmml">(</mo><mi id="S2.SS1.p4.1.m1.1.1" xref="S2.SS1.p4.1.m1.1.1.cmml">i</mi><mo stretchy="false" id="S2.SS1.p4.1.m1.1.2.3.2.2" xref="S2.SS1.p4.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.1.m1.1b"><apply id="S2.SS1.p4.1.m1.1.2.cmml" xref="S2.SS1.p4.1.m1.1.2"><times id="S2.SS1.p4.1.m1.1.2.1.cmml" xref="S2.SS1.p4.1.m1.1.2.1"></times><apply id="S2.SS1.p4.1.m1.1.2.2.cmml" xref="S2.SS1.p4.1.m1.1.2.2"><csymbol cd="ambiguous" id="S2.SS1.p4.1.m1.1.2.2.1.cmml" xref="S2.SS1.p4.1.m1.1.2.2">subscript</csymbol><ci id="S2.SS1.p4.1.m1.1.2.2.2.cmml" xref="S2.SS1.p4.1.m1.1.2.2.2">ğ‘š</ci><apply id="S2.SS1.p4.1.m1.1.2.2.3.cmml" xref="S2.SS1.p4.1.m1.1.2.2.3"><ci id="S2.SS1.p4.1.m1.1.2.2.3.1.cmml" xref="S2.SS1.p4.1.m1.1.2.2.3.1">^</ci><ci id="S2.SS1.p4.1.m1.1.2.2.3.2.cmml" xref="S2.SS1.p4.1.m1.1.2.2.3.2">ğ‘¥</ci></apply></apply><ci id="S2.SS1.p4.1.m1.1.1.cmml" xref="S2.SS1.p4.1.m1.1.1">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.1.m1.1c">m_{\hat{x}}(i)</annotation></semantics></math> is the function estimating the ith joint coordinates and <math id="S2.SS1.p4.2.m2.1" class="ltx_Math" alttext="m_{x}" display="inline"><semantics id="S2.SS1.p4.2.m2.1a"><msub id="S2.SS1.p4.2.m2.1.1" xref="S2.SS1.p4.2.m2.1.1.cmml"><mi id="S2.SS1.p4.2.m2.1.1.2" xref="S2.SS1.p4.2.m2.1.1.2.cmml">m</mi><mi id="S2.SS1.p4.2.m2.1.1.3" xref="S2.SS1.p4.2.m2.1.1.3.cmml">x</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.2.m2.1b"><apply id="S2.SS1.p4.2.m2.1.1.cmml" xref="S2.SS1.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p4.2.m2.1.1.1.cmml" xref="S2.SS1.p4.2.m2.1.1">subscript</csymbol><ci id="S2.SS1.p4.2.m2.1.1.2.cmml" xref="S2.SS1.p4.2.m2.1.1.2">ğ‘š</ci><ci id="S2.SS1.p4.2.m2.1.1.3.cmml" xref="S2.SS1.p4.2.m2.1.1.3">ğ‘¥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.2.m2.1c">m_{x}</annotation></semantics></math> is the ground truth position of the joint. Equation <a href="#S2.E1" title="In 2.1 Metrics â€£ 2 Methods Evaluation â€£ A review of 3D human pose estimation algorithms for markerless motion capture" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> is the measurement of MPJPE for one frame and one skeleton. To generalize for video frame the average of each MPJPEâ€™s frame of the sequence is calculated.
<br class="ltx_break"></p>
</div>
<div id="S2.SS1.p5" class="ltx_para">
<p id="S2.SS1.p5.1" class="ltx_p">MPJPE is a good baseline metric that can be used to evaluate a wide variety of methods as long as they want to estimate coordinate position and overall skeleton structure. It can also be adapted to evaluate methods that do not estimate the same number of key points as the number of markers used in common datasets and methods estimating relative poses instead of absolute 3D position <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Sigal etÂ al.</span> (<a href="#bib.bib64" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2010</span></a>)</cite>. This is performed by Procrustes alignment (adjustment to the ground truth poses) using a chosen root joint such as the pelvis. It can sometimes be referred as N-MPJPE or P-MPJPE depending if the alignment is by scale only or also by rotation and translation. The main drawbacks of such metrics, identified by <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Ionescu etÂ al.</span> (<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2014</span></a>)</cite>, is their low robustness to outlier errors and the fact that they can be influenced by perceptually irrelevant variations.
<br class="ltx_break"></p>
</div>
<div id="S2.SS1.p6" class="ltx_para">
<p id="S2.SS1.p6.1" class="ltx_p"><span id="S2.SS1.p6.1.1" class="ltx_text ltx_font_bold">MPJVE</span>: Mean Per Joint Velocity Error. Introduced by <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Pavllo etÂ al.</span> (<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite>, this metric can be used when a pose sequence is extracted from a video. Here, the absolute position of joints obtained from the MPJPE is insufficient. For this reason, the author use â€the MPJPE of the first derivative of the 3D pose sequencesâ€ to â€measure the smoothness of predictions over timeâ€. This technique is useful to compare estimation models that are using temporal data.
<br class="ltx_break"></p>
</div>
<div id="S2.SS1.p7" class="ltx_para">
<p id="S2.SS1.p7.1" class="ltx_p"><span id="S2.SS1.p7.1.1" class="ltx_text ltx_font_bold">Angular Metrics</span>: Another approach would be to measure angle errors of joint segments. Ionescu et al. suggest the Mean Per Joint Angle Error (MPJAE) <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Ionescu etÂ al.</span> (<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2014</span></a>)</cite> which is also sometimes referred as Mean Angular Error <cite class="ltx_cite ltx_citemacro_cite">Agarwal and Triggs (<a href="#bib.bib1" title="" class="ltx_ref">2006</a>)</cite>:</p>
</div>
<div id="S2.SS1.p8" class="ltx_para">
<table id="S2.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S2.Ex1.m1.5" class="ltx_Math" alttext="MPJAE(x,\hat{x})=\frac{1}{3N}\sum_{i=1}^{3N}|(m_{\hat{x}}(i)-m_{x}(i)mod\pm 180)|" display="block"><semantics id="S2.Ex1.m1.5a"><mrow id="S2.Ex1.m1.5.5" xref="S2.Ex1.m1.5.5.cmml"><mrow id="S2.Ex1.m1.5.5.3" xref="S2.Ex1.m1.5.5.3.cmml"><mi id="S2.Ex1.m1.5.5.3.2" xref="S2.Ex1.m1.5.5.3.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.5.5.3.1" xref="S2.Ex1.m1.5.5.3.1.cmml">â€‹</mo><mi id="S2.Ex1.m1.5.5.3.3" xref="S2.Ex1.m1.5.5.3.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.5.5.3.1a" xref="S2.Ex1.m1.5.5.3.1.cmml">â€‹</mo><mi id="S2.Ex1.m1.5.5.3.4" xref="S2.Ex1.m1.5.5.3.4.cmml">J</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.5.5.3.1b" xref="S2.Ex1.m1.5.5.3.1.cmml">â€‹</mo><mi id="S2.Ex1.m1.5.5.3.5" xref="S2.Ex1.m1.5.5.3.5.cmml">A</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.5.5.3.1c" xref="S2.Ex1.m1.5.5.3.1.cmml">â€‹</mo><mi id="S2.Ex1.m1.5.5.3.6" xref="S2.Ex1.m1.5.5.3.6.cmml">E</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.5.5.3.1d" xref="S2.Ex1.m1.5.5.3.1.cmml">â€‹</mo><mrow id="S2.Ex1.m1.5.5.3.7.2" xref="S2.Ex1.m1.5.5.3.7.1.cmml"><mo stretchy="false" id="S2.Ex1.m1.5.5.3.7.2.1" xref="S2.Ex1.m1.5.5.3.7.1.cmml">(</mo><mi id="S2.Ex1.m1.1.1" xref="S2.Ex1.m1.1.1.cmml">x</mi><mo id="S2.Ex1.m1.5.5.3.7.2.2" xref="S2.Ex1.m1.5.5.3.7.1.cmml">,</mo><mover accent="true" id="S2.Ex1.m1.2.2" xref="S2.Ex1.m1.2.2.cmml"><mi id="S2.Ex1.m1.2.2.2" xref="S2.Ex1.m1.2.2.2.cmml">x</mi><mo id="S2.Ex1.m1.2.2.1" xref="S2.Ex1.m1.2.2.1.cmml">^</mo></mover><mo stretchy="false" id="S2.Ex1.m1.5.5.3.7.2.3" xref="S2.Ex1.m1.5.5.3.7.1.cmml">)</mo></mrow></mrow><mo id="S2.Ex1.m1.5.5.2" xref="S2.Ex1.m1.5.5.2.cmml">=</mo><mrow id="S2.Ex1.m1.5.5.1" xref="S2.Ex1.m1.5.5.1.cmml"><mfrac id="S2.Ex1.m1.5.5.1.3" xref="S2.Ex1.m1.5.5.1.3.cmml"><mn id="S2.Ex1.m1.5.5.1.3.2" xref="S2.Ex1.m1.5.5.1.3.2.cmml">1</mn><mrow id="S2.Ex1.m1.5.5.1.3.3" xref="S2.Ex1.m1.5.5.1.3.3.cmml"><mn id="S2.Ex1.m1.5.5.1.3.3.2" xref="S2.Ex1.m1.5.5.1.3.3.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.5.5.1.3.3.1" xref="S2.Ex1.m1.5.5.1.3.3.1.cmml">â€‹</mo><mi id="S2.Ex1.m1.5.5.1.3.3.3" xref="S2.Ex1.m1.5.5.1.3.3.3.cmml">N</mi></mrow></mfrac><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.5.5.1.2" xref="S2.Ex1.m1.5.5.1.2.cmml">â€‹</mo><mrow id="S2.Ex1.m1.5.5.1.1" xref="S2.Ex1.m1.5.5.1.1.cmml"><munderover id="S2.Ex1.m1.5.5.1.1.2" xref="S2.Ex1.m1.5.5.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S2.Ex1.m1.5.5.1.1.2.2.2" xref="S2.Ex1.m1.5.5.1.1.2.2.2.cmml">âˆ‘</mo><mrow id="S2.Ex1.m1.5.5.1.1.2.2.3" xref="S2.Ex1.m1.5.5.1.1.2.2.3.cmml"><mi id="S2.Ex1.m1.5.5.1.1.2.2.3.2" xref="S2.Ex1.m1.5.5.1.1.2.2.3.2.cmml">i</mi><mo id="S2.Ex1.m1.5.5.1.1.2.2.3.1" xref="S2.Ex1.m1.5.5.1.1.2.2.3.1.cmml">=</mo><mn id="S2.Ex1.m1.5.5.1.1.2.2.3.3" xref="S2.Ex1.m1.5.5.1.1.2.2.3.3.cmml">1</mn></mrow><mrow id="S2.Ex1.m1.5.5.1.1.2.3" xref="S2.Ex1.m1.5.5.1.1.2.3.cmml"><mn id="S2.Ex1.m1.5.5.1.1.2.3.2" xref="S2.Ex1.m1.5.5.1.1.2.3.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.5.5.1.1.2.3.1" xref="S2.Ex1.m1.5.5.1.1.2.3.1.cmml">â€‹</mo><mi id="S2.Ex1.m1.5.5.1.1.2.3.3" xref="S2.Ex1.m1.5.5.1.1.2.3.3.cmml">N</mi></mrow></munderover><mrow id="S2.Ex1.m1.5.5.1.1.1.1" xref="S2.Ex1.m1.5.5.1.1.1.2.cmml"><mo stretchy="false" id="S2.Ex1.m1.5.5.1.1.1.1.2" xref="S2.Ex1.m1.5.5.1.1.1.2.1.cmml">|</mo><mrow id="S2.Ex1.m1.5.5.1.1.1.1.1.1" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.Ex1.m1.5.5.1.1.1.1.1.1.2" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.cmml"><mrow id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.cmml"><mrow id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.2" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.2.cmml"><msub id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.2.2" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.2.2.2" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.2.2.2.cmml">m</mi><mover accent="true" id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.2.2.3" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.2.2.3.cmml"><mi id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.2.2.3.2" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.2.2.3.2.cmml">x</mi><mo id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.2.2.3.1" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.2.2.3.1.cmml">^</mo></mover></msub><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.2.1" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.2.1.cmml">â€‹</mo><mrow id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.2.3.2" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.2.cmml"><mo stretchy="false" id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.2.3.2.1" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.2.cmml">(</mo><mi id="S2.Ex1.m1.3.3" xref="S2.Ex1.m1.3.3.cmml">i</mi><mo stretchy="false" id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.2.3.2.2" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.2.cmml">)</mo></mrow></mrow><mo id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.1" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.1.cmml">âˆ’</mo><mrow id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3.cmml"><msub id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3.2" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3.2.cmml"><mi id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3.2.2" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3.2.2.cmml">m</mi><mi id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3.2.3" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3.2.3.cmml">x</mi></msub><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3.1" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3.1.cmml">â€‹</mo><mrow id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3.3.2" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3.cmml"><mo stretchy="false" id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3.3.2.1" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3.cmml">(</mo><mi id="S2.Ex1.m1.4.4" xref="S2.Ex1.m1.4.4.cmml">i</mi><mo stretchy="false" id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3.3.2.2" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3.1a" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3.1.cmml">â€‹</mo><mi id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3.4" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3.4.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3.1b" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3.1.cmml">â€‹</mo><mi id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3.5" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3.5.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3.1c" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3.1.cmml">â€‹</mo><mi id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3.6" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3.6.cmml">d</mi></mrow></mrow><mo id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.1" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.1.cmml">Â±</mo><mn id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.3" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.3.cmml">180</mn></mrow><mo stretchy="false" id="S2.Ex1.m1.5.5.1.1.1.1.1.1.3" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo stretchy="false" id="S2.Ex1.m1.5.5.1.1.1.1.3" xref="S2.Ex1.m1.5.5.1.1.1.2.1.cmml">|</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex1.m1.5b"><apply id="S2.Ex1.m1.5.5.cmml" xref="S2.Ex1.m1.5.5"><eq id="S2.Ex1.m1.5.5.2.cmml" xref="S2.Ex1.m1.5.5.2"></eq><apply id="S2.Ex1.m1.5.5.3.cmml" xref="S2.Ex1.m1.5.5.3"><times id="S2.Ex1.m1.5.5.3.1.cmml" xref="S2.Ex1.m1.5.5.3.1"></times><ci id="S2.Ex1.m1.5.5.3.2.cmml" xref="S2.Ex1.m1.5.5.3.2">ğ‘€</ci><ci id="S2.Ex1.m1.5.5.3.3.cmml" xref="S2.Ex1.m1.5.5.3.3">ğ‘ƒ</ci><ci id="S2.Ex1.m1.5.5.3.4.cmml" xref="S2.Ex1.m1.5.5.3.4">ğ½</ci><ci id="S2.Ex1.m1.5.5.3.5.cmml" xref="S2.Ex1.m1.5.5.3.5">ğ´</ci><ci id="S2.Ex1.m1.5.5.3.6.cmml" xref="S2.Ex1.m1.5.5.3.6">ğ¸</ci><interval closure="open" id="S2.Ex1.m1.5.5.3.7.1.cmml" xref="S2.Ex1.m1.5.5.3.7.2"><ci id="S2.Ex1.m1.1.1.cmml" xref="S2.Ex1.m1.1.1">ğ‘¥</ci><apply id="S2.Ex1.m1.2.2.cmml" xref="S2.Ex1.m1.2.2"><ci id="S2.Ex1.m1.2.2.1.cmml" xref="S2.Ex1.m1.2.2.1">^</ci><ci id="S2.Ex1.m1.2.2.2.cmml" xref="S2.Ex1.m1.2.2.2">ğ‘¥</ci></apply></interval></apply><apply id="S2.Ex1.m1.5.5.1.cmml" xref="S2.Ex1.m1.5.5.1"><times id="S2.Ex1.m1.5.5.1.2.cmml" xref="S2.Ex1.m1.5.5.1.2"></times><apply id="S2.Ex1.m1.5.5.1.3.cmml" xref="S2.Ex1.m1.5.5.1.3"><divide id="S2.Ex1.m1.5.5.1.3.1.cmml" xref="S2.Ex1.m1.5.5.1.3"></divide><cn type="integer" id="S2.Ex1.m1.5.5.1.3.2.cmml" xref="S2.Ex1.m1.5.5.1.3.2">1</cn><apply id="S2.Ex1.m1.5.5.1.3.3.cmml" xref="S2.Ex1.m1.5.5.1.3.3"><times id="S2.Ex1.m1.5.5.1.3.3.1.cmml" xref="S2.Ex1.m1.5.5.1.3.3.1"></times><cn type="integer" id="S2.Ex1.m1.5.5.1.3.3.2.cmml" xref="S2.Ex1.m1.5.5.1.3.3.2">3</cn><ci id="S2.Ex1.m1.5.5.1.3.3.3.cmml" xref="S2.Ex1.m1.5.5.1.3.3.3">ğ‘</ci></apply></apply><apply id="S2.Ex1.m1.5.5.1.1.cmml" xref="S2.Ex1.m1.5.5.1.1"><apply id="S2.Ex1.m1.5.5.1.1.2.cmml" xref="S2.Ex1.m1.5.5.1.1.2"><csymbol cd="ambiguous" id="S2.Ex1.m1.5.5.1.1.2.1.cmml" xref="S2.Ex1.m1.5.5.1.1.2">superscript</csymbol><apply id="S2.Ex1.m1.5.5.1.1.2.2.cmml" xref="S2.Ex1.m1.5.5.1.1.2"><csymbol cd="ambiguous" id="S2.Ex1.m1.5.5.1.1.2.2.1.cmml" xref="S2.Ex1.m1.5.5.1.1.2">subscript</csymbol><sum id="S2.Ex1.m1.5.5.1.1.2.2.2.cmml" xref="S2.Ex1.m1.5.5.1.1.2.2.2"></sum><apply id="S2.Ex1.m1.5.5.1.1.2.2.3.cmml" xref="S2.Ex1.m1.5.5.1.1.2.2.3"><eq id="S2.Ex1.m1.5.5.1.1.2.2.3.1.cmml" xref="S2.Ex1.m1.5.5.1.1.2.2.3.1"></eq><ci id="S2.Ex1.m1.5.5.1.1.2.2.3.2.cmml" xref="S2.Ex1.m1.5.5.1.1.2.2.3.2">ğ‘–</ci><cn type="integer" id="S2.Ex1.m1.5.5.1.1.2.2.3.3.cmml" xref="S2.Ex1.m1.5.5.1.1.2.2.3.3">1</cn></apply></apply><apply id="S2.Ex1.m1.5.5.1.1.2.3.cmml" xref="S2.Ex1.m1.5.5.1.1.2.3"><times id="S2.Ex1.m1.5.5.1.1.2.3.1.cmml" xref="S2.Ex1.m1.5.5.1.1.2.3.1"></times><cn type="integer" id="S2.Ex1.m1.5.5.1.1.2.3.2.cmml" xref="S2.Ex1.m1.5.5.1.1.2.3.2">3</cn><ci id="S2.Ex1.m1.5.5.1.1.2.3.3.cmml" xref="S2.Ex1.m1.5.5.1.1.2.3.3">ğ‘</ci></apply></apply><apply id="S2.Ex1.m1.5.5.1.1.1.2.cmml" xref="S2.Ex1.m1.5.5.1.1.1.1"><abs id="S2.Ex1.m1.5.5.1.1.1.2.1.cmml" xref="S2.Ex1.m1.5.5.1.1.1.1.2"></abs><apply id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.cmml" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.1">plus-or-minus</csymbol><apply id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.cmml" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2"><minus id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.1.cmml" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.1"></minus><apply id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.2.cmml" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.2"><times id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.2.1.cmml" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.2.1"></times><apply id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.2.2.cmml" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.2.2.2">ğ‘š</ci><apply id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.2.2.3"><ci id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.2.2.3.1.cmml" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.2.2.3.1">^</ci><ci id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.2.2.3.2.cmml" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.2.2.3.2">ğ‘¥</ci></apply></apply><ci id="S2.Ex1.m1.3.3.cmml" xref="S2.Ex1.m1.3.3">ğ‘–</ci></apply><apply id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3.cmml" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3"><times id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3.1.cmml" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3.1"></times><apply id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3.2.cmml" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3.2"><csymbol cd="ambiguous" id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3.2.1.cmml" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3.2">subscript</csymbol><ci id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3.2.2.cmml" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3.2.2">ğ‘š</ci><ci id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3.2.3.cmml" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3.2.3">ğ‘¥</ci></apply><ci id="S2.Ex1.m1.4.4.cmml" xref="S2.Ex1.m1.4.4">ğ‘–</ci><ci id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3.4.cmml" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3.4">ğ‘š</ci><ci id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3.5.cmml" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3.5">ğ‘œ</ci><ci id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3.6.cmml" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.2.3.6">ğ‘‘</ci></apply></apply><cn type="integer" id="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.3.cmml" xref="S2.Ex1.m1.5.5.1.1.1.1.1.1.1.3">180</cn></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex1.m1.5c">MPJAE(x,\hat{x})=\frac{1}{3N}\sum_{i=1}^{3N}|(m_{\hat{x}}(i)-m_{x}(i)mod\pm 180)|</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
</tr></tbody>
</table>
<br class="ltx_break">
</div>
<div id="S2.SS1.p9" class="ltx_para">
<p id="S2.SS1.p9.2" class="ltx_p">Here <math id="S2.SS1.p9.1.m1.1" class="ltx_Math" alttext="m_{x}" display="inline"><semantics id="S2.SS1.p9.1.m1.1a"><msub id="S2.SS1.p9.1.m1.1.1" xref="S2.SS1.p9.1.m1.1.1.cmml"><mi id="S2.SS1.p9.1.m1.1.1.2" xref="S2.SS1.p9.1.m1.1.1.2.cmml">m</mi><mi id="S2.SS1.p9.1.m1.1.1.3" xref="S2.SS1.p9.1.m1.1.1.3.cmml">x</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p9.1.m1.1b"><apply id="S2.SS1.p9.1.m1.1.1.cmml" xref="S2.SS1.p9.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p9.1.m1.1.1.1.cmml" xref="S2.SS1.p9.1.m1.1.1">subscript</csymbol><ci id="S2.SS1.p9.1.m1.1.1.2.cmml" xref="S2.SS1.p9.1.m1.1.1.2">ğ‘š</ci><ci id="S2.SS1.p9.1.m1.1.1.3.cmml" xref="S2.SS1.p9.1.m1.1.1.3">ğ‘¥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p9.1.m1.1c">m_{x}</annotation></semantics></math> and <math id="S2.SS1.p9.2.m2.1" class="ltx_Math" alttext="m_{\hat{x}}(i)" display="inline"><semantics id="S2.SS1.p9.2.m2.1a"><mrow id="S2.SS1.p9.2.m2.1.2" xref="S2.SS1.p9.2.m2.1.2.cmml"><msub id="S2.SS1.p9.2.m2.1.2.2" xref="S2.SS1.p9.2.m2.1.2.2.cmml"><mi id="S2.SS1.p9.2.m2.1.2.2.2" xref="S2.SS1.p9.2.m2.1.2.2.2.cmml">m</mi><mover accent="true" id="S2.SS1.p9.2.m2.1.2.2.3" xref="S2.SS1.p9.2.m2.1.2.2.3.cmml"><mi id="S2.SS1.p9.2.m2.1.2.2.3.2" xref="S2.SS1.p9.2.m2.1.2.2.3.2.cmml">x</mi><mo id="S2.SS1.p9.2.m2.1.2.2.3.1" xref="S2.SS1.p9.2.m2.1.2.2.3.1.cmml">^</mo></mover></msub><mo lspace="0em" rspace="0em" id="S2.SS1.p9.2.m2.1.2.1" xref="S2.SS1.p9.2.m2.1.2.1.cmml">â€‹</mo><mrow id="S2.SS1.p9.2.m2.1.2.3.2" xref="S2.SS1.p9.2.m2.1.2.cmml"><mo stretchy="false" id="S2.SS1.p9.2.m2.1.2.3.2.1" xref="S2.SS1.p9.2.m2.1.2.cmml">(</mo><mi id="S2.SS1.p9.2.m2.1.1" xref="S2.SS1.p9.2.m2.1.1.cmml">i</mi><mo stretchy="false" id="S2.SS1.p9.2.m2.1.2.3.2.2" xref="S2.SS1.p9.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p9.2.m2.1b"><apply id="S2.SS1.p9.2.m2.1.2.cmml" xref="S2.SS1.p9.2.m2.1.2"><times id="S2.SS1.p9.2.m2.1.2.1.cmml" xref="S2.SS1.p9.2.m2.1.2.1"></times><apply id="S2.SS1.p9.2.m2.1.2.2.cmml" xref="S2.SS1.p9.2.m2.1.2.2"><csymbol cd="ambiguous" id="S2.SS1.p9.2.m2.1.2.2.1.cmml" xref="S2.SS1.p9.2.m2.1.2.2">subscript</csymbol><ci id="S2.SS1.p9.2.m2.1.2.2.2.cmml" xref="S2.SS1.p9.2.m2.1.2.2.2">ğ‘š</ci><apply id="S2.SS1.p9.2.m2.1.2.2.3.cmml" xref="S2.SS1.p9.2.m2.1.2.2.3"><ci id="S2.SS1.p9.2.m2.1.2.2.3.1.cmml" xref="S2.SS1.p9.2.m2.1.2.2.3.1">^</ci><ci id="S2.SS1.p9.2.m2.1.2.2.3.2.cmml" xref="S2.SS1.p9.2.m2.1.2.2.3.2">ğ‘¥</ci></apply></apply><ci id="S2.SS1.p9.2.m2.1.1.cmml" xref="S2.SS1.p9.2.m2.1.1">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p9.2.m2.1c">m_{\hat{x}}(i)</annotation></semantics></math> refer to 3D angles for ground truth and prediction, respectively.</p>
</div>
<div id="S2.SS1.p10" class="ltx_para">
<p id="S2.SS1.p10.1" class="ltx_p">These metrics can be used when the main analysis is performed on angles between two specific limbs rather than the whole body such as for rehabilitation, or sport motion. However, authors of <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Ionescu etÂ al.</span> (<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2014</span></a>)</cite> report that this metric can have little perceptual meaning. It can also be difficult to interpret because angles are calculated locally and joints that are dependent from a faulty predicted one can yield no errors despite a globally misaligned skeleton. As a result, these metrics are less used in recent computer vision publications.
<br class="ltx_break"></p>
</div>
<div id="S2.SS1.p11" class="ltx_para">
<p id="S2.SS1.p11.1" class="ltx_p"><span id="S2.SS1.p11.1.1" class="ltx_text ltx_font_bold">Thresholds Metrics</span>: A common approach in 2D Human Pose Estimation and other detection tasks is to define a threshold where a key point is correctly detected. Then, statistics of correctly predicted joints over a set of images can be computed. The percentage of correct parts (PCP) uses half the size of the ground truth segment to determine if a prediction over a limb segment is correct. A 3D version of PCP can also be adapted. A limb is correctly estimated if the following expression is respected:</p>
</div>
<div id="S2.SS1.p12" class="ltx_para">
<table id="S2.Ex2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S2.Ex2.m1.3" class="ltx_Math" alttext="\frac{\left\|\alpha-\hat{\alpha}\right\|+\left\|\omega-\hat{\omega}\right\|}{2}\leq\theta\left\|\alpha-\omega\right\|" display="block"><semantics id="S2.Ex2.m1.3a"><mrow id="S2.Ex2.m1.3.3" xref="S2.Ex2.m1.3.3.cmml"><mfrac id="S2.Ex2.m1.2.2" xref="S2.Ex2.m1.2.2.cmml"><mrow id="S2.Ex2.m1.2.2.2" xref="S2.Ex2.m1.2.2.2.cmml"><mrow id="S2.Ex2.m1.1.1.1.1.1" xref="S2.Ex2.m1.1.1.1.1.2.cmml"><mo id="S2.Ex2.m1.1.1.1.1.1.2" xref="S2.Ex2.m1.1.1.1.1.2.1.cmml">â€–</mo><mrow id="S2.Ex2.m1.1.1.1.1.1.1" xref="S2.Ex2.m1.1.1.1.1.1.1.cmml"><mi id="S2.Ex2.m1.1.1.1.1.1.1.2" xref="S2.Ex2.m1.1.1.1.1.1.1.2.cmml">Î±</mi><mo id="S2.Ex2.m1.1.1.1.1.1.1.1" xref="S2.Ex2.m1.1.1.1.1.1.1.1.cmml">âˆ’</mo><mover accent="true" id="S2.Ex2.m1.1.1.1.1.1.1.3" xref="S2.Ex2.m1.1.1.1.1.1.1.3.cmml"><mi id="S2.Ex2.m1.1.1.1.1.1.1.3.2" xref="S2.Ex2.m1.1.1.1.1.1.1.3.2.cmml">Î±</mi><mo id="S2.Ex2.m1.1.1.1.1.1.1.3.1" xref="S2.Ex2.m1.1.1.1.1.1.1.3.1.cmml">^</mo></mover></mrow><mo id="S2.Ex2.m1.1.1.1.1.1.3" xref="S2.Ex2.m1.1.1.1.1.2.1.cmml">â€–</mo></mrow><mo id="S2.Ex2.m1.2.2.2.3" xref="S2.Ex2.m1.2.2.2.3.cmml">+</mo><mrow id="S2.Ex2.m1.2.2.2.2.1" xref="S2.Ex2.m1.2.2.2.2.2.cmml"><mo id="S2.Ex2.m1.2.2.2.2.1.2" xref="S2.Ex2.m1.2.2.2.2.2.1.cmml">â€–</mo><mrow id="S2.Ex2.m1.2.2.2.2.1.1" xref="S2.Ex2.m1.2.2.2.2.1.1.cmml"><mi id="S2.Ex2.m1.2.2.2.2.1.1.2" xref="S2.Ex2.m1.2.2.2.2.1.1.2.cmml">Ï‰</mi><mo id="S2.Ex2.m1.2.2.2.2.1.1.1" xref="S2.Ex2.m1.2.2.2.2.1.1.1.cmml">âˆ’</mo><mover accent="true" id="S2.Ex2.m1.2.2.2.2.1.1.3" xref="S2.Ex2.m1.2.2.2.2.1.1.3.cmml"><mi id="S2.Ex2.m1.2.2.2.2.1.1.3.2" xref="S2.Ex2.m1.2.2.2.2.1.1.3.2.cmml">Ï‰</mi><mo id="S2.Ex2.m1.2.2.2.2.1.1.3.1" xref="S2.Ex2.m1.2.2.2.2.1.1.3.1.cmml">^</mo></mover></mrow><mo id="S2.Ex2.m1.2.2.2.2.1.3" xref="S2.Ex2.m1.2.2.2.2.2.1.cmml">â€–</mo></mrow></mrow><mn id="S2.Ex2.m1.2.2.4" xref="S2.Ex2.m1.2.2.4.cmml">2</mn></mfrac><mo id="S2.Ex2.m1.3.3.2" xref="S2.Ex2.m1.3.3.2.cmml">â‰¤</mo><mrow id="S2.Ex2.m1.3.3.1" xref="S2.Ex2.m1.3.3.1.cmml"><mi id="S2.Ex2.m1.3.3.1.3" xref="S2.Ex2.m1.3.3.1.3.cmml">Î¸</mi><mo lspace="0em" rspace="0em" id="S2.Ex2.m1.3.3.1.2" xref="S2.Ex2.m1.3.3.1.2.cmml">â€‹</mo><mrow id="S2.Ex2.m1.3.3.1.1.1" xref="S2.Ex2.m1.3.3.1.1.2.cmml"><mo id="S2.Ex2.m1.3.3.1.1.1.2" xref="S2.Ex2.m1.3.3.1.1.2.1.cmml">â€–</mo><mrow id="S2.Ex2.m1.3.3.1.1.1.1" xref="S2.Ex2.m1.3.3.1.1.1.1.cmml"><mi id="S2.Ex2.m1.3.3.1.1.1.1.2" xref="S2.Ex2.m1.3.3.1.1.1.1.2.cmml">Î±</mi><mo id="S2.Ex2.m1.3.3.1.1.1.1.1" xref="S2.Ex2.m1.3.3.1.1.1.1.1.cmml">âˆ’</mo><mi id="S2.Ex2.m1.3.3.1.1.1.1.3" xref="S2.Ex2.m1.3.3.1.1.1.1.3.cmml">Ï‰</mi></mrow><mo id="S2.Ex2.m1.3.3.1.1.1.3" xref="S2.Ex2.m1.3.3.1.1.2.1.cmml">â€–</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex2.m1.3b"><apply id="S2.Ex2.m1.3.3.cmml" xref="S2.Ex2.m1.3.3"><leq id="S2.Ex2.m1.3.3.2.cmml" xref="S2.Ex2.m1.3.3.2"></leq><apply id="S2.Ex2.m1.2.2.cmml" xref="S2.Ex2.m1.2.2"><divide id="S2.Ex2.m1.2.2.3.cmml" xref="S2.Ex2.m1.2.2"></divide><apply id="S2.Ex2.m1.2.2.2.cmml" xref="S2.Ex2.m1.2.2.2"><plus id="S2.Ex2.m1.2.2.2.3.cmml" xref="S2.Ex2.m1.2.2.2.3"></plus><apply id="S2.Ex2.m1.1.1.1.1.2.cmml" xref="S2.Ex2.m1.1.1.1.1.1"><csymbol cd="latexml" id="S2.Ex2.m1.1.1.1.1.2.1.cmml" xref="S2.Ex2.m1.1.1.1.1.1.2">norm</csymbol><apply id="S2.Ex2.m1.1.1.1.1.1.1.cmml" xref="S2.Ex2.m1.1.1.1.1.1.1"><minus id="S2.Ex2.m1.1.1.1.1.1.1.1.cmml" xref="S2.Ex2.m1.1.1.1.1.1.1.1"></minus><ci id="S2.Ex2.m1.1.1.1.1.1.1.2.cmml" xref="S2.Ex2.m1.1.1.1.1.1.1.2">ğ›¼</ci><apply id="S2.Ex2.m1.1.1.1.1.1.1.3.cmml" xref="S2.Ex2.m1.1.1.1.1.1.1.3"><ci id="S2.Ex2.m1.1.1.1.1.1.1.3.1.cmml" xref="S2.Ex2.m1.1.1.1.1.1.1.3.1">^</ci><ci id="S2.Ex2.m1.1.1.1.1.1.1.3.2.cmml" xref="S2.Ex2.m1.1.1.1.1.1.1.3.2">ğ›¼</ci></apply></apply></apply><apply id="S2.Ex2.m1.2.2.2.2.2.cmml" xref="S2.Ex2.m1.2.2.2.2.1"><csymbol cd="latexml" id="S2.Ex2.m1.2.2.2.2.2.1.cmml" xref="S2.Ex2.m1.2.2.2.2.1.2">norm</csymbol><apply id="S2.Ex2.m1.2.2.2.2.1.1.cmml" xref="S2.Ex2.m1.2.2.2.2.1.1"><minus id="S2.Ex2.m1.2.2.2.2.1.1.1.cmml" xref="S2.Ex2.m1.2.2.2.2.1.1.1"></minus><ci id="S2.Ex2.m1.2.2.2.2.1.1.2.cmml" xref="S2.Ex2.m1.2.2.2.2.1.1.2">ğœ”</ci><apply id="S2.Ex2.m1.2.2.2.2.1.1.3.cmml" xref="S2.Ex2.m1.2.2.2.2.1.1.3"><ci id="S2.Ex2.m1.2.2.2.2.1.1.3.1.cmml" xref="S2.Ex2.m1.2.2.2.2.1.1.3.1">^</ci><ci id="S2.Ex2.m1.2.2.2.2.1.1.3.2.cmml" xref="S2.Ex2.m1.2.2.2.2.1.1.3.2">ğœ”</ci></apply></apply></apply></apply><cn type="integer" id="S2.Ex2.m1.2.2.4.cmml" xref="S2.Ex2.m1.2.2.4">2</cn></apply><apply id="S2.Ex2.m1.3.3.1.cmml" xref="S2.Ex2.m1.3.3.1"><times id="S2.Ex2.m1.3.3.1.2.cmml" xref="S2.Ex2.m1.3.3.1.2"></times><ci id="S2.Ex2.m1.3.3.1.3.cmml" xref="S2.Ex2.m1.3.3.1.3">ğœƒ</ci><apply id="S2.Ex2.m1.3.3.1.1.2.cmml" xref="S2.Ex2.m1.3.3.1.1.1"><csymbol cd="latexml" id="S2.Ex2.m1.3.3.1.1.2.1.cmml" xref="S2.Ex2.m1.3.3.1.1.1.2">norm</csymbol><apply id="S2.Ex2.m1.3.3.1.1.1.1.cmml" xref="S2.Ex2.m1.3.3.1.1.1.1"><minus id="S2.Ex2.m1.3.3.1.1.1.1.1.cmml" xref="S2.Ex2.m1.3.3.1.1.1.1.1"></minus><ci id="S2.Ex2.m1.3.3.1.1.1.1.2.cmml" xref="S2.Ex2.m1.3.3.1.1.1.1.2">ğ›¼</ci><ci id="S2.Ex2.m1.3.3.1.1.1.1.3.cmml" xref="S2.Ex2.m1.3.3.1.1.1.1.3">ğœ”</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex2.m1.3c">\frac{\left\|\alpha-\hat{\alpha}\right\|+\left\|\omega-\hat{\omega}\right\|}{2}\leq\theta\left\|\alpha-\omega\right\|</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS1.p13" class="ltx_para">
<p id="S2.SS1.p13.5" class="ltx_p"><math id="S2.SS1.p13.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S2.SS1.p13.1.m1.1a"><mi id="S2.SS1.p13.1.m1.1.1" xref="S2.SS1.p13.1.m1.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p13.1.m1.1b"><ci id="S2.SS1.p13.1.m1.1.1.cmml" xref="S2.SS1.p13.1.m1.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p13.1.m1.1c">\alpha</annotation></semantics></math> and <math id="S2.SS1.p13.2.m2.1" class="ltx_Math" alttext="\omega" display="inline"><semantics id="S2.SS1.p13.2.m2.1a"><mi id="S2.SS1.p13.2.m2.1.1" xref="S2.SS1.p13.2.m2.1.1.cmml">Ï‰</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p13.2.m2.1b"><ci id="S2.SS1.p13.2.m2.1.1.cmml" xref="S2.SS1.p13.2.m2.1.1">ğœ”</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p13.2.m2.1c">\omega</annotation></semantics></math> are the two measured coordinates of the extremities of the limb and <math id="S2.SS1.p13.3.m3.1" class="ltx_Math" alttext="\hat{\alpha}" display="inline"><semantics id="S2.SS1.p13.3.m3.1a"><mover accent="true" id="S2.SS1.p13.3.m3.1.1" xref="S2.SS1.p13.3.m3.1.1.cmml"><mi id="S2.SS1.p13.3.m3.1.1.2" xref="S2.SS1.p13.3.m3.1.1.2.cmml">Î±</mi><mo id="S2.SS1.p13.3.m3.1.1.1" xref="S2.SS1.p13.3.m3.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S2.SS1.p13.3.m3.1b"><apply id="S2.SS1.p13.3.m3.1.1.cmml" xref="S2.SS1.p13.3.m3.1.1"><ci id="S2.SS1.p13.3.m3.1.1.1.cmml" xref="S2.SS1.p13.3.m3.1.1.1">^</ci><ci id="S2.SS1.p13.3.m3.1.1.2.cmml" xref="S2.SS1.p13.3.m3.1.1.2">ğ›¼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p13.3.m3.1c">\hat{\alpha}</annotation></semantics></math> and <math id="S2.SS1.p13.4.m4.1" class="ltx_Math" alttext="\hat{\omega}" display="inline"><semantics id="S2.SS1.p13.4.m4.1a"><mover accent="true" id="S2.SS1.p13.4.m4.1.1" xref="S2.SS1.p13.4.m4.1.1.cmml"><mi id="S2.SS1.p13.4.m4.1.1.2" xref="S2.SS1.p13.4.m4.1.1.2.cmml">Ï‰</mi><mo id="S2.SS1.p13.4.m4.1.1.1" xref="S2.SS1.p13.4.m4.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S2.SS1.p13.4.m4.1b"><apply id="S2.SS1.p13.4.m4.1.1.cmml" xref="S2.SS1.p13.4.m4.1.1"><ci id="S2.SS1.p13.4.m4.1.1.1.cmml" xref="S2.SS1.p13.4.m4.1.1.1">^</ci><ci id="S2.SS1.p13.4.m4.1.1.2.cmml" xref="S2.SS1.p13.4.m4.1.1.2">ğœ”</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p13.4.m4.1c">\hat{\omega}</annotation></semantics></math> their predictions. <math id="S2.SS1.p13.5.m5.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S2.SS1.p13.5.m5.1a"><mi id="S2.SS1.p13.5.m5.1.1" xref="S2.SS1.p13.5.m5.1.1.cmml">Î¸</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p13.5.m5.1b"><ci id="S2.SS1.p13.5.m5.1.1.cmml" xref="S2.SS1.p13.5.m5.1.1">ğœƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p13.5.m5.1c">\theta</annotation></semantics></math> is a chosen parameter to control the accuracy requirement for the threshold (commonly 0.5). This metric was used to evaluate model-based pose estimation using pictorial structure such as <cite class="ltx_cite ltx_citemacro_cite">Burenius etÂ al. (<a href="#bib.bib5" title="" class="ltx_ref">2013</a>)</cite>. The issue with this metric is that a shorter limb will be less likely to be considered detected as the threshold decreases.
<br class="ltx_break"></p>
</div>
<div id="S2.SS1.p14" class="ltx_para">
<p id="S2.SS1.p14.1" class="ltx_p">Another threshold metric used in 2D pose estimation is the Percentage of Correct Keypoints (PCK). This metric does not have the issue of shorter limbs harder to detect as it uses a subject specific threshold for each individual joint instead of limbs. It is calculated with a portion of a fixed limb length (eg: 0.5 times the head bone length, often referred as PCKh@0.5). In this way, the metric is self-adapting to subjects with different proportions, without bias on specific size of the limbs of an individual. <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Mehta etÂ al.</span> (<a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2016</span></a>)</cite> propose a 3D version of the PCK used as the main evaluation metric for the MPI-INF-3DHP benchmark. A joint is considered detected with this condition:</p>
</div>
<div id="S2.SS1.p15" class="ltx_para">
<table id="S2.Ex3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S2.Ex3.m1.2" class="ltx_Math" alttext="\left\|\alpha-\hat{\alpha}\right\|\leq\theta\left\|k-h\right\|" display="block"><semantics id="S2.Ex3.m1.2a"><mrow id="S2.Ex3.m1.2.2" xref="S2.Ex3.m1.2.2.cmml"><mrow id="S2.Ex3.m1.1.1.1.1" xref="S2.Ex3.m1.1.1.1.2.cmml"><mo id="S2.Ex3.m1.1.1.1.1.2" xref="S2.Ex3.m1.1.1.1.2.1.cmml">â€–</mo><mrow id="S2.Ex3.m1.1.1.1.1.1" xref="S2.Ex3.m1.1.1.1.1.1.cmml"><mi id="S2.Ex3.m1.1.1.1.1.1.2" xref="S2.Ex3.m1.1.1.1.1.1.2.cmml">Î±</mi><mo id="S2.Ex3.m1.1.1.1.1.1.1" xref="S2.Ex3.m1.1.1.1.1.1.1.cmml">âˆ’</mo><mover accent="true" id="S2.Ex3.m1.1.1.1.1.1.3" xref="S2.Ex3.m1.1.1.1.1.1.3.cmml"><mi id="S2.Ex3.m1.1.1.1.1.1.3.2" xref="S2.Ex3.m1.1.1.1.1.1.3.2.cmml">Î±</mi><mo id="S2.Ex3.m1.1.1.1.1.1.3.1" xref="S2.Ex3.m1.1.1.1.1.1.3.1.cmml">^</mo></mover></mrow><mo id="S2.Ex3.m1.1.1.1.1.3" xref="S2.Ex3.m1.1.1.1.2.1.cmml">â€–</mo></mrow><mo id="S2.Ex3.m1.2.2.3" xref="S2.Ex3.m1.2.2.3.cmml">â‰¤</mo><mrow id="S2.Ex3.m1.2.2.2" xref="S2.Ex3.m1.2.2.2.cmml"><mi id="S2.Ex3.m1.2.2.2.3" xref="S2.Ex3.m1.2.2.2.3.cmml">Î¸</mi><mo lspace="0em" rspace="0em" id="S2.Ex3.m1.2.2.2.2" xref="S2.Ex3.m1.2.2.2.2.cmml">â€‹</mo><mrow id="S2.Ex3.m1.2.2.2.1.1" xref="S2.Ex3.m1.2.2.2.1.2.cmml"><mo id="S2.Ex3.m1.2.2.2.1.1.2" xref="S2.Ex3.m1.2.2.2.1.2.1.cmml">â€–</mo><mrow id="S2.Ex3.m1.2.2.2.1.1.1" xref="S2.Ex3.m1.2.2.2.1.1.1.cmml"><mi id="S2.Ex3.m1.2.2.2.1.1.1.2" xref="S2.Ex3.m1.2.2.2.1.1.1.2.cmml">k</mi><mo id="S2.Ex3.m1.2.2.2.1.1.1.1" xref="S2.Ex3.m1.2.2.2.1.1.1.1.cmml">âˆ’</mo><mi id="S2.Ex3.m1.2.2.2.1.1.1.3" xref="S2.Ex3.m1.2.2.2.1.1.1.3.cmml">h</mi></mrow><mo id="S2.Ex3.m1.2.2.2.1.1.3" xref="S2.Ex3.m1.2.2.2.1.2.1.cmml">â€–</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex3.m1.2b"><apply id="S2.Ex3.m1.2.2.cmml" xref="S2.Ex3.m1.2.2"><leq id="S2.Ex3.m1.2.2.3.cmml" xref="S2.Ex3.m1.2.2.3"></leq><apply id="S2.Ex3.m1.1.1.1.2.cmml" xref="S2.Ex3.m1.1.1.1.1"><csymbol cd="latexml" id="S2.Ex3.m1.1.1.1.2.1.cmml" xref="S2.Ex3.m1.1.1.1.1.2">norm</csymbol><apply id="S2.Ex3.m1.1.1.1.1.1.cmml" xref="S2.Ex3.m1.1.1.1.1.1"><minus id="S2.Ex3.m1.1.1.1.1.1.1.cmml" xref="S2.Ex3.m1.1.1.1.1.1.1"></minus><ci id="S2.Ex3.m1.1.1.1.1.1.2.cmml" xref="S2.Ex3.m1.1.1.1.1.1.2">ğ›¼</ci><apply id="S2.Ex3.m1.1.1.1.1.1.3.cmml" xref="S2.Ex3.m1.1.1.1.1.1.3"><ci id="S2.Ex3.m1.1.1.1.1.1.3.1.cmml" xref="S2.Ex3.m1.1.1.1.1.1.3.1">^</ci><ci id="S2.Ex3.m1.1.1.1.1.1.3.2.cmml" xref="S2.Ex3.m1.1.1.1.1.1.3.2">ğ›¼</ci></apply></apply></apply><apply id="S2.Ex3.m1.2.2.2.cmml" xref="S2.Ex3.m1.2.2.2"><times id="S2.Ex3.m1.2.2.2.2.cmml" xref="S2.Ex3.m1.2.2.2.2"></times><ci id="S2.Ex3.m1.2.2.2.3.cmml" xref="S2.Ex3.m1.2.2.2.3">ğœƒ</ci><apply id="S2.Ex3.m1.2.2.2.1.2.cmml" xref="S2.Ex3.m1.2.2.2.1.1"><csymbol cd="latexml" id="S2.Ex3.m1.2.2.2.1.2.1.cmml" xref="S2.Ex3.m1.2.2.2.1.1.2">norm</csymbol><apply id="S2.Ex3.m1.2.2.2.1.1.1.cmml" xref="S2.Ex3.m1.2.2.2.1.1.1"><minus id="S2.Ex3.m1.2.2.2.1.1.1.1.cmml" xref="S2.Ex3.m1.2.2.2.1.1.1.1"></minus><ci id="S2.Ex3.m1.2.2.2.1.1.1.2.cmml" xref="S2.Ex3.m1.2.2.2.1.1.1.2">ğ‘˜</ci><ci id="S2.Ex3.m1.2.2.2.1.1.1.3.cmml" xref="S2.Ex3.m1.2.2.2.1.1.1.3">â„</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex3.m1.2c">\left\|\alpha-\hat{\alpha}\right\|\leq\theta\left\|k-h\right\|</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS1.p16" class="ltx_para">
<p id="S2.SS1.p16.3" class="ltx_p"><math id="S2.SS1.p16.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S2.SS1.p16.1.m1.1a"><mi id="S2.SS1.p16.1.m1.1.1" xref="S2.SS1.p16.1.m1.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p16.1.m1.1b"><ci id="S2.SS1.p16.1.m1.1.1.cmml" xref="S2.SS1.p16.1.m1.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p16.1.m1.1c">\alpha</annotation></semantics></math> and <math id="S2.SS1.p16.2.m2.1" class="ltx_Math" alttext="\hat{\alpha}" display="inline"><semantics id="S2.SS1.p16.2.m2.1a"><mover accent="true" id="S2.SS1.p16.2.m2.1.1" xref="S2.SS1.p16.2.m2.1.1.cmml"><mi id="S2.SS1.p16.2.m2.1.1.2" xref="S2.SS1.p16.2.m2.1.1.2.cmml">Î±</mi><mo id="S2.SS1.p16.2.m2.1.1.1" xref="S2.SS1.p16.2.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S2.SS1.p16.2.m2.1b"><apply id="S2.SS1.p16.2.m2.1.1.cmml" xref="S2.SS1.p16.2.m2.1.1"><ci id="S2.SS1.p16.2.m2.1.1.1.cmml" xref="S2.SS1.p16.2.m2.1.1.1">^</ci><ci id="S2.SS1.p16.2.m2.1.1.2.cmml" xref="S2.SS1.p16.2.m2.1.1.2">ğ›¼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p16.2.m2.1c">\hat{\alpha}</annotation></semantics></math> are the target joint and its prediction. <math id="S2.SS1.p16.3.m3.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S2.SS1.p16.3.m3.1a"><mi id="S2.SS1.p16.3.m3.1.1" xref="S2.SS1.p16.3.m3.1.1.cmml">Î¸</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p16.3.m3.1b"><ci id="S2.SS1.p16.3.m3.1.1.cmml" xref="S2.SS1.p16.3.m3.1.1">ğœƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p16.3.m3.1c">\theta</annotation></semantics></math> is a parameter controlling the fraction of a reference limb length, <span id="S2.SS1.p16.3.1" class="ltx_text ltx_font_italic">k</span> and <span id="S2.SS1.p16.3.2" class="ltx_text ltx_font_italic">h</span> are the coordinates of the extremities of this limb (head, torsoâ€¦). Another solution is to choose a fixed threshold of 150 mm, which loses the specificity of the subject.
<br class="ltx_break"></p>
</div>
<div id="S2.SS1.p17" class="ltx_para">
<p id="S2.SS1.p17.1" class="ltx_p">Using these threshold-based metrics is justified when comparing methods that could have a good overall accuracy, but produce errors in specific scenario (for singular joints or skeletons). However, this is done at the price of losing sensitivity that could be relevant when analyzing precise local coordinates (at the millimeter scale), as in biomechanical applications.
<br class="ltx_break"></p>
</div>
<figure id="S2.T1" class="ltx_table">
<div id="S2.T1.3" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:223.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-254.8pt,131.3pt) scale(0.459748239008409,0.459748239008409) ;">
<table id="S2.T1.3.3" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T1.3.3.4.1" class="ltx_tr">
<td id="S2.T1.3.3.4.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Dataset</td>
<td id="S2.T1.3.3.4.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">#Frame</td>
<td id="S2.T1.3.3.4.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">#Subject</td>
<td id="S2.T1.3.3.4.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">#View</td>
<td id="S2.T1.3.3.4.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Resolution</td>
<td id="S2.T1.3.3.4.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Frequency</td>
<td id="S2.T1.3.3.4.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Depth</td>
<td id="S2.T1.3.3.4.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">IMU</td>
<td id="S2.T1.3.3.4.1.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Context and</td>
</tr>
<tr id="S2.T1.3.3.5.2" class="ltx_tr">
<td id="S2.T1.3.3.5.2.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S2.T1.3.3.5.2.2" class="ltx_td ltx_align_center ltx_border_r">#Video Sequence</td>
<td id="S2.T1.3.3.5.2.3" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.5.2.4" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.5.2.5" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.5.2.6" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.5.2.7" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.5.2.8" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.5.2.9" class="ltx_td ltx_align_center ltx_border_r">Main Characteristics</td>
</tr>
<tr id="S2.T1.3.3.6.3" class="ltx_tr">
<td id="S2.T1.3.3.6.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Human3.6M: <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Ionescu etÂ al.</span> (<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2014</span></a>)</cite>
</td>
<td id="S2.T1.3.3.6.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.6 millions</td>
<td id="S2.T1.3.3.6.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11</td>
<td id="S2.T1.3.3.6.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4</td>
<td id="S2.T1.3.3.6.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1000x1000</td>
<td id="S2.T1.3.3.6.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">50Hz</td>
<td id="S2.T1.3.3.6.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Yes</td>
<td id="S2.T1.3.3.6.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">No</td>
<td id="S2.T1.3.3.6.3.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Lab environnement</td>
</tr>
<tr id="S2.T1.3.3.7.4" class="ltx_tr">
<td id="S2.T1.3.3.7.4.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S2.T1.3.3.7.4.2" class="ltx_td ltx_align_center ltx_border_r">1 376</td>
<td id="S2.T1.3.3.7.4.3" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.7.4.4" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.7.4.5" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.7.4.6" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.7.4.7" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.7.4.8" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.7.4.9" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S2.T1.3.3.8.5" class="ltx_tr">
<td id="S2.T1.3.3.8.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">HumanEva: <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Sigal etÂ al.</span> (<a href="#bib.bib64" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2010</span></a>)</cite>
</td>
<td id="S2.T1.3.3.8.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">80 000</td>
<td id="S2.T1.3.3.8.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4</td>
<td id="S2.T1.3.3.8.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">7</td>
<td id="S2.T1.3.3.8.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">660x500</td>
<td id="S2.T1.3.3.8.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">60Hz</td>
<td id="S2.T1.3.3.8.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">No</td>
<td id="S2.T1.3.3.8.5.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">No</td>
<td id="S2.T1.3.3.8.5.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Lab environnement</td>
</tr>
<tr id="S2.T1.3.3.9.6" class="ltx_tr">
<td id="S2.T1.3.3.9.6.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S2.T1.3.3.9.6.2" class="ltx_td ltx_align_center ltx_border_r">56</td>
<td id="S2.T1.3.3.9.6.3" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.9.6.4" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.9.6.5" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.9.6.6" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.9.6.7" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.9.6.8" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.9.6.9" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S2.T1.3.3.10.7" class="ltx_tr">
<td id="S2.T1.3.3.10.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Total Capture: <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Trumble etÂ al.</span> (<a href="#bib.bib71" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2017</span></a>)</cite>
</td>
<td id="S2.T1.3.3.10.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.9 millions</td>
<td id="S2.T1.3.3.10.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5</td>
<td id="S2.T1.3.3.10.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8</td>
<td id="S2.T1.3.3.10.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1920x1080</td>
<td id="S2.T1.3.3.10.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">60Hz</td>
<td id="S2.T1.3.3.10.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">No</td>
<td id="S2.T1.3.3.10.7.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Yes</td>
<td id="S2.T1.3.3.10.7.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Lab environnement</td>
</tr>
<tr id="S2.T1.3.3.11.8" class="ltx_tr">
<td id="S2.T1.3.3.11.8.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S2.T1.3.3.11.8.2" class="ltx_td ltx_align_center ltx_border_r">N/A</td>
<td id="S2.T1.3.3.11.8.3" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.11.8.4" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.11.8.5" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.11.8.6" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.11.8.7" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.11.8.8" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.11.8.9" class="ltx_td ltx_align_center ltx_border_r">Inertial Measurement Units</td>
</tr>
<tr id="S2.T1.3.3.12.9" class="ltx_tr">
<td id="S2.T1.3.3.12.9.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t"></td>
<td id="S2.T1.3.3.12.9.2" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S2.T1.3.3.12.9.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S2.T1.3.3.12.9.4" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S2.T1.3.3.12.9.5" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S2.T1.3.3.12.9.6" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S2.T1.3.3.12.9.7" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S2.T1.3.3.12.9.8" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S2.T1.3.3.12.9.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">â€In the wildâ€ &amp; Lab</td>
</tr>
<tr id="S2.T1.3.3.13.10" class="ltx_tr">
<td id="S2.T1.3.3.13.10.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">MPI-INF-3DHP: <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Mehta etÂ al.</span> (<a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2016</span></a>)</cite>
</td>
<td id="S2.T1.3.3.13.10.2" class="ltx_td ltx_align_center ltx_border_r">1.3 millions</td>
<td id="S2.T1.3.3.13.10.3" class="ltx_td ltx_align_center ltx_border_r">8</td>
<td id="S2.T1.3.3.13.10.4" class="ltx_td ltx_align_center ltx_border_r">14</td>
<td id="S2.T1.3.3.13.10.5" class="ltx_td ltx_align_center ltx_border_r">N/A</td>
<td id="S2.T1.3.3.13.10.6" class="ltx_td ltx_align_center ltx_border_r">N/A</td>
<td id="S2.T1.3.3.13.10.7" class="ltx_td ltx_align_center ltx_border_r">No</td>
<td id="S2.T1.3.3.13.10.8" class="ltx_td ltx_align_center ltx_border_r">No</td>
<td id="S2.T1.3.3.13.10.9" class="ltx_td ltx_align_center ltx_border_r">outdoor/indoor green screens.</td>
</tr>
<tr id="S2.T1.3.3.14.11" class="ltx_tr">
<td id="S2.T1.3.3.14.11.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S2.T1.3.3.14.11.2" class="ltx_td ltx_align_center ltx_border_r">64</td>
<td id="S2.T1.3.3.14.11.3" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.14.11.4" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.14.11.5" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.14.11.6" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.14.11.7" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.14.11.8" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.14.11.9" class="ltx_td ltx_align_center ltx_border_r">Markerless ground truthes</td>
</tr>
<tr id="S2.T1.3.3.15.12" class="ltx_tr">
<td id="S2.T1.3.3.15.12.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t"></td>
<td id="S2.T1.3.3.15.12.2" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S2.T1.3.3.15.12.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S2.T1.3.3.15.12.4" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S2.T1.3.3.15.12.5" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S2.T1.3.3.15.12.6" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S2.T1.3.3.15.12.7" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S2.T1.3.3.15.12.8" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S2.T1.3.3.15.12.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Multi-person, â€In the wildâ€</td>
</tr>
<tr id="S2.T1.3.3.16.13" class="ltx_tr">
<td id="S2.T1.3.3.16.13.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">MuPoTS-3D (2018): <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Mehta etÂ al.</span> (<a href="#bib.bib49" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite>
</td>
<td id="S2.T1.3.3.16.13.2" class="ltx_td ltx_align_center ltx_border_r">8 000</td>
<td id="S2.T1.3.3.16.13.3" class="ltx_td ltx_align_center ltx_border_r">8</td>
<td id="S2.T1.3.3.16.13.4" class="ltx_td ltx_align_center ltx_border_r">1</td>
<td id="S2.T1.3.3.16.13.5" class="ltx_td ltx_align_center ltx_border_r">2048x2048</td>
<td id="S2.T1.3.3.16.13.6" class="ltx_td ltx_align_center ltx_border_r">30Hz - 60Hz</td>
<td id="S2.T1.3.3.16.13.7" class="ltx_td ltx_align_center ltx_border_r">No</td>
<td id="S2.T1.3.3.16.13.8" class="ltx_td ltx_align_center ltx_border_r">No</td>
<td id="S2.T1.3.3.16.13.9" class="ltx_td ltx_align_center ltx_border_r">indoor/outdoor scenes.</td>
</tr>
<tr id="S2.T1.3.3.17.14" class="ltx_tr">
<td id="S2.T1.3.3.17.14.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S2.T1.3.3.17.14.2" class="ltx_td ltx_align_center ltx_border_r">20</td>
<td id="S2.T1.3.3.17.14.3" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.17.14.4" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.17.14.5" class="ltx_td ltx_align_center ltx_border_r">1920x1080</td>
<td id="S2.T1.3.3.17.14.6" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.17.14.7" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.17.14.8" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.17.14.9" class="ltx_td ltx_align_center ltx_border_r">Markerless ground truthes</td>
</tr>
<tr id="S2.T1.3.3.18.15" class="ltx_tr">
<td id="S2.T1.3.3.18.15.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t"></td>
<td id="S2.T1.3.3.18.15.2" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S2.T1.3.3.18.15.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S2.T1.3.3.18.15.4" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S2.T1.3.3.18.15.5" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S2.T1.3.3.18.15.6" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S2.T1.3.3.18.15.7" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S2.T1.3.3.18.15.8" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S2.T1.3.3.18.15.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">â€In the wildâ€ outdoor</td>
</tr>
<tr id="S2.T1.1.1.1" class="ltx_tr">
<td id="S2.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">3DPW <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">von Marcard etÂ al.</span> (<a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite>
</td>
<td id="S2.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S2.T1.1.1.1.1.m1.1" class="ltx_Math" alttext="&gt;50000" display="inline"><semantics id="S2.T1.1.1.1.1.m1.1a"><mrow id="S2.T1.1.1.1.1.m1.1.1" xref="S2.T1.1.1.1.1.m1.1.1.cmml"><mi id="S2.T1.1.1.1.1.m1.1.1.2" xref="S2.T1.1.1.1.1.m1.1.1.2.cmml"></mi><mo id="S2.T1.1.1.1.1.m1.1.1.1" xref="S2.T1.1.1.1.1.m1.1.1.1.cmml">&gt;</mo><mn id="S2.T1.1.1.1.1.m1.1.1.3" xref="S2.T1.1.1.1.1.m1.1.1.3.cmml">50000</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.1.m1.1b"><apply id="S2.T1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.1.m1.1.1"><gt id="S2.T1.1.1.1.1.m1.1.1.1.cmml" xref="S2.T1.1.1.1.1.m1.1.1.1"></gt><csymbol cd="latexml" id="S2.T1.1.1.1.1.m1.1.1.2.cmml" xref="S2.T1.1.1.1.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S2.T1.1.1.1.1.m1.1.1.3.cmml" xref="S2.T1.1.1.1.1.m1.1.1.3">50000</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.1.m1.1c">&gt;50000</annotation></semantics></math></td>
<td id="S2.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r">7</td>
<td id="S2.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r">1</td>
<td id="S2.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r">N/A</td>
<td id="S2.T1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r">30Hz</td>
<td id="S2.T1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_r">No</td>
<td id="S2.T1.1.1.1.8" class="ltx_td ltx_align_center ltx_border_r">Yes</td>
<td id="S2.T1.1.1.1.9" class="ltx_td ltx_align_center ltx_border_r">single moving camera &amp; IMUs</td>
</tr>
<tr id="S2.T1.3.3.19.16" class="ltx_tr">
<td id="S2.T1.3.3.19.16.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S2.T1.3.3.19.16.2" class="ltx_td ltx_align_center ltx_border_r">60</td>
<td id="S2.T1.3.3.19.16.3" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.19.16.4" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.19.16.5" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.19.16.6" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.19.16.7" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.19.16.8" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.19.16.9" class="ltx_td ltx_align_center ltx_border_r">Up to two subjects</td>
</tr>
<tr id="S2.T1.3.3.20.17" class="ltx_tr">
<td id="S2.T1.3.3.20.17.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Carnegie Mellon <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib72" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">University</span> </a></cite> Mocap</td>
<td id="S2.T1.3.3.20.17.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N/A</td>
<td id="S2.T1.3.3.20.17.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">109</td>
<td id="S2.T1.3.3.20.17.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1</td>
<td id="S2.T1.3.3.20.17.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">352x240</td>
<td id="S2.T1.3.3.20.17.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">30Hz</td>
<td id="S2.T1.3.3.20.17.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">No</td>
<td id="S2.T1.3.3.20.17.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">No</td>
<td id="S2.T1.3.3.20.17.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Indoor environment</td>
</tr>
<tr id="S2.T1.3.3.21.18" class="ltx_tr">
<td id="S2.T1.3.3.21.18.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S2.T1.3.3.21.18.2" class="ltx_td ltx_align_center ltx_border_r">2 605</td>
<td id="S2.T1.3.3.21.18.3" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.21.18.4" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.21.18.5" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.21.18.6" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.21.18.7" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.21.18.8" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.21.18.9" class="ltx_td ltx_align_center ltx_border_r">Various actions and subjects</td>
</tr>
<tr id="S2.T1.2.2.2" class="ltx_tr">
<td id="S2.T1.2.2.2.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">CMU-MMAC: <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">DeÂ la Torre etÂ al.</span> (<a href="#bib.bib69" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2008</span></a>)</cite>
</td>
<td id="S2.T1.2.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<math id="S2.T1.2.2.2.1.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S2.T1.2.2.2.1.m1.1a"><mo id="S2.T1.2.2.2.1.m1.1.1" xref="S2.T1.2.2.2.1.m1.1.1.cmml">â‰ˆ</mo><annotation-xml encoding="MathML-Content" id="S2.T1.2.2.2.1.m1.1b"><approx id="S2.T1.2.2.2.1.m1.1.1.cmml" xref="S2.T1.2.2.2.1.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.2.2.2.1.m1.1c">\approx</annotation></semantics></math>450 000</td>
<td id="S2.T1.2.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">25</td>
<td id="S2.T1.2.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5</td>
<td id="S2.T1.2.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1024x768 (x3)</td>
<td id="S2.T1.2.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">30Hz (x3)</td>
<td id="S2.T1.2.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">No</td>
<td id="S2.T1.2.2.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Yes</td>
<td id="S2.T1.2.2.2.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Lab Environnement</td>
</tr>
<tr id="S2.T1.3.3.22.19" class="ltx_tr">
<td id="S2.T1.3.3.22.19.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S2.T1.3.3.22.19.2" class="ltx_td ltx_align_center ltx_border_r">N/A</td>
<td id="S2.T1.3.3.22.19.3" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.22.19.4" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.22.19.5" class="ltx_td ltx_align_center ltx_border_r">640x480 (x2)</td>
<td id="S2.T1.3.3.22.19.6" class="ltx_td ltx_align_center ltx_border_r">60Hz (x2)</td>
<td id="S2.T1.3.3.22.19.7" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.22.19.8" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.22.19.9" class="ltx_td ltx_align_center ltx_border_r">Subjects cooking 5 recipes</td>
</tr>
<tr id="S2.T1.3.3.23.20" class="ltx_tr">
<td id="S2.T1.3.3.23.20.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">TNT15: <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">von Marcard etÂ al.</span> (<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2016</span></a>)</cite>
</td>
<td id="S2.T1.3.3.23.20.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">13 000</td>
<td id="S2.T1.3.3.23.20.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4</td>
<td id="S2.T1.3.3.23.20.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8</td>
<td id="S2.T1.3.3.23.20.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">800x600</td>
<td id="S2.T1.3.3.23.20.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">50Hz</td>
<td id="S2.T1.3.3.23.20.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">No</td>
<td id="S2.T1.3.3.23.20.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Yes</td>
<td id="S2.T1.3.3.23.20.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Office environment</td>
</tr>
<tr id="S2.T1.3.3.24.21" class="ltx_tr">
<td id="S2.T1.3.3.24.21.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S2.T1.3.3.24.21.2" class="ltx_td ltx_align_center ltx_border_r">20</td>
<td id="S2.T1.3.3.24.21.3" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.24.21.4" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.24.21.5" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.24.21.6" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.24.21.7" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.24.21.8" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.24.21.9" class="ltx_td ltx_align_center ltx_border_r">No marker-based labeling, only IMU</td>
</tr>
<tr id="S2.T1.3.3.25.22" class="ltx_tr">
<td id="S2.T1.3.3.25.22.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">AMASS: <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Mahmood etÂ al.</span> (<a href="#bib.bib39" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite>
</td>
<td id="S2.T1.3.3.25.22.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N/A</td>
<td id="S2.T1.3.3.25.22.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">346</td>
<td id="S2.T1.3.3.25.22.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">variable</td>
<td id="S2.T1.3.3.25.22.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">variable</td>
<td id="S2.T1.3.3.25.22.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">variable</td>
<td id="S2.T1.3.3.25.22.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">No</td>
<td id="S2.T1.3.3.25.22.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">No</td>
<td id="S2.T1.3.3.25.22.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Unified parametrization of 15 datasets</td>
</tr>
<tr id="S2.T1.3.3.3" class="ltx_tr">
<td id="S2.T1.3.3.3.2" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S2.T1.3.3.3.1" class="ltx_td ltx_align_center ltx_border_r">(<math id="S2.T1.3.3.3.1.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S2.T1.3.3.3.1.m1.1a"><mo id="S2.T1.3.3.3.1.m1.1.1" xref="S2.T1.3.3.3.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.3.3.3.1.m1.1b"><gt id="S2.T1.3.3.3.1.m1.1.1.cmml" xref="S2.T1.3.3.3.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.3.3.3.1.m1.1c">&gt;</annotation></semantics></math>40 hours)</td>
<td id="S2.T1.3.3.3.3" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.3.4" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.3.5" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.3.6" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.3.7" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.3.8" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.3.3.3.9" class="ltx_td ltx_align_center ltx_border_r">Mesh body models</td>
</tr>
<tr id="S2.T1.3.3.26.23" class="ltx_tr">
<td id="S2.T1.3.3.26.23.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">MoVi: <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Ghorbani etÂ al.</span> (<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite>
</td>
<td id="S2.T1.3.3.26.23.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N/A</td>
<td id="S2.T1.3.3.26.23.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">90</td>
<td id="S2.T1.3.3.26.23.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4</td>
<td id="S2.T1.3.3.26.23.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">800x600</td>
<td id="S2.T1.3.3.26.23.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">30Hz</td>
<td id="S2.T1.3.3.26.23.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">No</td>
<td id="S2.T1.3.3.26.23.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Yes</td>
<td id="S2.T1.3.3.26.23.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Synchronized MoCap</td>
</tr>
<tr id="S2.T1.3.3.27.24" class="ltx_tr">
<td id="S2.T1.3.3.27.24.1" class="ltx_td ltx_border_b ltx_border_l ltx_border_r"></td>
<td id="S2.T1.3.3.27.24.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">(17 hours)</td>
<td id="S2.T1.3.3.27.24.3" class="ltx_td ltx_border_b ltx_border_r"></td>
<td id="S2.T1.3.3.27.24.4" class="ltx_td ltx_border_b ltx_border_r"></td>
<td id="S2.T1.3.3.27.24.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">1920x1080</td>
<td id="S2.T1.3.3.27.24.6" class="ltx_td ltx_border_b ltx_border_r"></td>
<td id="S2.T1.3.3.27.24.7" class="ltx_td ltx_border_b ltx_border_r"></td>
<td id="S2.T1.3.3.27.24.8" class="ltx_td ltx_border_b ltx_border_r"></td>
<td id="S2.T1.3.3.27.24.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">shape, video and IMU data</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Popular datasets used to compare, train and test human pose estimation models. Video frames, the number of subjects and actions give an indication about the dataset diversity and the number of pose configurations. The number of views from RGB cameras, the resolution and acquisition frequency of cameras assess the quality and quantity of exploitable video information. Inertial Measurement Units (IMU) are sometimes used to refine results from the motion capture or single-image detection. If not specified, the motion capture method is marker-based.</figcaption>
</figure>
<div id="S2.SS1.p18" class="ltx_para">
<p id="S2.SS1.p18.1" class="ltx_p"><span id="S2.SS1.p18.1.1" class="ltx_text ltx_font_bold">Volume &amp; Surface Based Metrics</span>â€‰: Some techniques for human pose estimation need a measurement over surfaces. This type of metric can be found in dense pose estimation Densepose (<cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">GÃ¼ler etÂ al.</span> (<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite>). This task aims at recovering the surface of the whole human body, not just a few joint key points. Geodesic distance-based metrics are often used in this context. An example would be the Geodesic Point Similarity described in <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">GÃ¼ler etÂ al.</span> (<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite>:</p>
</div>
<div id="S2.SS1.p19" class="ltx_para">
<table id="S2.Ex4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S2.Ex4.m1.5" class="ltx_Math" alttext="GPS(j)=\frac{1}{|P_{j}|}\sum_{p\in P_{j}}\exp\left(\frac{-g(i_{p},\hat{i}_{p})^{2}}{2k^{2}}\right)" display="block"><semantics id="S2.Ex4.m1.5a"><mrow id="S2.Ex4.m1.5.6" xref="S2.Ex4.m1.5.6.cmml"><mrow id="S2.Ex4.m1.5.6.2" xref="S2.Ex4.m1.5.6.2.cmml"><mi id="S2.Ex4.m1.5.6.2.2" xref="S2.Ex4.m1.5.6.2.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S2.Ex4.m1.5.6.2.1" xref="S2.Ex4.m1.5.6.2.1.cmml">â€‹</mo><mi id="S2.Ex4.m1.5.6.2.3" xref="S2.Ex4.m1.5.6.2.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.Ex4.m1.5.6.2.1a" xref="S2.Ex4.m1.5.6.2.1.cmml">â€‹</mo><mi id="S2.Ex4.m1.5.6.2.4" xref="S2.Ex4.m1.5.6.2.4.cmml">S</mi><mo lspace="0em" rspace="0em" id="S2.Ex4.m1.5.6.2.1b" xref="S2.Ex4.m1.5.6.2.1.cmml">â€‹</mo><mrow id="S2.Ex4.m1.5.6.2.5.2" xref="S2.Ex4.m1.5.6.2.cmml"><mo stretchy="false" id="S2.Ex4.m1.5.6.2.5.2.1" xref="S2.Ex4.m1.5.6.2.cmml">(</mo><mi id="S2.Ex4.m1.4.4" xref="S2.Ex4.m1.4.4.cmml">j</mi><mo stretchy="false" id="S2.Ex4.m1.5.6.2.5.2.2" xref="S2.Ex4.m1.5.6.2.cmml">)</mo></mrow></mrow><mo id="S2.Ex4.m1.5.6.1" xref="S2.Ex4.m1.5.6.1.cmml">=</mo><mrow id="S2.Ex4.m1.5.6.3" xref="S2.Ex4.m1.5.6.3.cmml"><mfrac id="S2.Ex4.m1.1.1" xref="S2.Ex4.m1.1.1.cmml"><mn id="S2.Ex4.m1.1.1.3" xref="S2.Ex4.m1.1.1.3.cmml">1</mn><mrow id="S2.Ex4.m1.1.1.1.1" xref="S2.Ex4.m1.1.1.1.2.cmml"><mo stretchy="false" id="S2.Ex4.m1.1.1.1.1.2" xref="S2.Ex4.m1.1.1.1.2.1.cmml">|</mo><msub id="S2.Ex4.m1.1.1.1.1.1" xref="S2.Ex4.m1.1.1.1.1.1.cmml"><mi id="S2.Ex4.m1.1.1.1.1.1.2" xref="S2.Ex4.m1.1.1.1.1.1.2.cmml">P</mi><mi id="S2.Ex4.m1.1.1.1.1.1.3" xref="S2.Ex4.m1.1.1.1.1.1.3.cmml">j</mi></msub><mo stretchy="false" id="S2.Ex4.m1.1.1.1.1.3" xref="S2.Ex4.m1.1.1.1.2.1.cmml">|</mo></mrow></mfrac><mo lspace="0em" rspace="0em" id="S2.Ex4.m1.5.6.3.1" xref="S2.Ex4.m1.5.6.3.1.cmml">â€‹</mo><mrow id="S2.Ex4.m1.5.6.3.2" xref="S2.Ex4.m1.5.6.3.2.cmml"><munder id="S2.Ex4.m1.5.6.3.2.1" xref="S2.Ex4.m1.5.6.3.2.1.cmml"><mo movablelimits="false" id="S2.Ex4.m1.5.6.3.2.1.2" xref="S2.Ex4.m1.5.6.3.2.1.2.cmml">âˆ‘</mo><mrow id="S2.Ex4.m1.5.6.3.2.1.3" xref="S2.Ex4.m1.5.6.3.2.1.3.cmml"><mi id="S2.Ex4.m1.5.6.3.2.1.3.2" xref="S2.Ex4.m1.5.6.3.2.1.3.2.cmml">p</mi><mo id="S2.Ex4.m1.5.6.3.2.1.3.1" xref="S2.Ex4.m1.5.6.3.2.1.3.1.cmml">âˆˆ</mo><msub id="S2.Ex4.m1.5.6.3.2.1.3.3" xref="S2.Ex4.m1.5.6.3.2.1.3.3.cmml"><mi id="S2.Ex4.m1.5.6.3.2.1.3.3.2" xref="S2.Ex4.m1.5.6.3.2.1.3.3.2.cmml">P</mi><mi id="S2.Ex4.m1.5.6.3.2.1.3.3.3" xref="S2.Ex4.m1.5.6.3.2.1.3.3.3.cmml">j</mi></msub></mrow></munder><mrow id="S2.Ex4.m1.5.6.3.2.2.2" xref="S2.Ex4.m1.5.6.3.2.2.1.cmml"><mi id="S2.Ex4.m1.5.5" xref="S2.Ex4.m1.5.5.cmml">exp</mi><mo id="S2.Ex4.m1.5.6.3.2.2.2a" xref="S2.Ex4.m1.5.6.3.2.2.1.cmml">â¡</mo><mrow id="S2.Ex4.m1.5.6.3.2.2.2.1" xref="S2.Ex4.m1.5.6.3.2.2.1.cmml"><mo id="S2.Ex4.m1.5.6.3.2.2.2.1.1" xref="S2.Ex4.m1.5.6.3.2.2.1.cmml">(</mo><mfrac id="S2.Ex4.m1.3.3" xref="S2.Ex4.m1.3.3.cmml"><mrow id="S2.Ex4.m1.3.3.2" xref="S2.Ex4.m1.3.3.2.cmml"><mo id="S2.Ex4.m1.3.3.2a" xref="S2.Ex4.m1.3.3.2.cmml">âˆ’</mo><mrow id="S2.Ex4.m1.3.3.2.2" xref="S2.Ex4.m1.3.3.2.2.cmml"><mi id="S2.Ex4.m1.3.3.2.2.4" xref="S2.Ex4.m1.3.3.2.2.4.cmml">g</mi><mo lspace="0em" rspace="0em" id="S2.Ex4.m1.3.3.2.2.3" xref="S2.Ex4.m1.3.3.2.2.3.cmml">â€‹</mo><msup id="S2.Ex4.m1.3.3.2.2.2" xref="S2.Ex4.m1.3.3.2.2.2.cmml"><mrow id="S2.Ex4.m1.3.3.2.2.2.2.2" xref="S2.Ex4.m1.3.3.2.2.2.2.3.cmml"><mo stretchy="false" id="S2.Ex4.m1.3.3.2.2.2.2.2.3" xref="S2.Ex4.m1.3.3.2.2.2.2.3.cmml">(</mo><msub id="S2.Ex4.m1.2.2.1.1.1.1.1.1" xref="S2.Ex4.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S2.Ex4.m1.2.2.1.1.1.1.1.1.2" xref="S2.Ex4.m1.2.2.1.1.1.1.1.1.2.cmml">i</mi><mi id="S2.Ex4.m1.2.2.1.1.1.1.1.1.3" xref="S2.Ex4.m1.2.2.1.1.1.1.1.1.3.cmml">p</mi></msub><mo id="S2.Ex4.m1.3.3.2.2.2.2.2.4" xref="S2.Ex4.m1.3.3.2.2.2.2.3.cmml">,</mo><msub id="S2.Ex4.m1.3.3.2.2.2.2.2.2" xref="S2.Ex4.m1.3.3.2.2.2.2.2.2.cmml"><mover accent="true" id="S2.Ex4.m1.3.3.2.2.2.2.2.2.2" xref="S2.Ex4.m1.3.3.2.2.2.2.2.2.2.cmml"><mi id="S2.Ex4.m1.3.3.2.2.2.2.2.2.2.2" xref="S2.Ex4.m1.3.3.2.2.2.2.2.2.2.2.cmml">i</mi><mo id="S2.Ex4.m1.3.3.2.2.2.2.2.2.2.1" xref="S2.Ex4.m1.3.3.2.2.2.2.2.2.2.1.cmml">^</mo></mover><mi id="S2.Ex4.m1.3.3.2.2.2.2.2.2.3" xref="S2.Ex4.m1.3.3.2.2.2.2.2.2.3.cmml">p</mi></msub><mo stretchy="false" id="S2.Ex4.m1.3.3.2.2.2.2.2.5" xref="S2.Ex4.m1.3.3.2.2.2.2.3.cmml">)</mo></mrow><mn id="S2.Ex4.m1.3.3.2.2.2.4" xref="S2.Ex4.m1.3.3.2.2.2.4.cmml">2</mn></msup></mrow></mrow><mrow id="S2.Ex4.m1.3.3.4" xref="S2.Ex4.m1.3.3.4.cmml"><mn id="S2.Ex4.m1.3.3.4.2" xref="S2.Ex4.m1.3.3.4.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S2.Ex4.m1.3.3.4.1" xref="S2.Ex4.m1.3.3.4.1.cmml">â€‹</mo><msup id="S2.Ex4.m1.3.3.4.3" xref="S2.Ex4.m1.3.3.4.3.cmml"><mi id="S2.Ex4.m1.3.3.4.3.2" xref="S2.Ex4.m1.3.3.4.3.2.cmml">k</mi><mn id="S2.Ex4.m1.3.3.4.3.3" xref="S2.Ex4.m1.3.3.4.3.3.cmml">2</mn></msup></mrow></mfrac><mo id="S2.Ex4.m1.5.6.3.2.2.2.1.2" xref="S2.Ex4.m1.5.6.3.2.2.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex4.m1.5b"><apply id="S2.Ex4.m1.5.6.cmml" xref="S2.Ex4.m1.5.6"><eq id="S2.Ex4.m1.5.6.1.cmml" xref="S2.Ex4.m1.5.6.1"></eq><apply id="S2.Ex4.m1.5.6.2.cmml" xref="S2.Ex4.m1.5.6.2"><times id="S2.Ex4.m1.5.6.2.1.cmml" xref="S2.Ex4.m1.5.6.2.1"></times><ci id="S2.Ex4.m1.5.6.2.2.cmml" xref="S2.Ex4.m1.5.6.2.2">ğº</ci><ci id="S2.Ex4.m1.5.6.2.3.cmml" xref="S2.Ex4.m1.5.6.2.3">ğ‘ƒ</ci><ci id="S2.Ex4.m1.5.6.2.4.cmml" xref="S2.Ex4.m1.5.6.2.4">ğ‘†</ci><ci id="S2.Ex4.m1.4.4.cmml" xref="S2.Ex4.m1.4.4">ğ‘—</ci></apply><apply id="S2.Ex4.m1.5.6.3.cmml" xref="S2.Ex4.m1.5.6.3"><times id="S2.Ex4.m1.5.6.3.1.cmml" xref="S2.Ex4.m1.5.6.3.1"></times><apply id="S2.Ex4.m1.1.1.cmml" xref="S2.Ex4.m1.1.1"><divide id="S2.Ex4.m1.1.1.2.cmml" xref="S2.Ex4.m1.1.1"></divide><cn type="integer" id="S2.Ex4.m1.1.1.3.cmml" xref="S2.Ex4.m1.1.1.3">1</cn><apply id="S2.Ex4.m1.1.1.1.2.cmml" xref="S2.Ex4.m1.1.1.1.1"><abs id="S2.Ex4.m1.1.1.1.2.1.cmml" xref="S2.Ex4.m1.1.1.1.1.2"></abs><apply id="S2.Ex4.m1.1.1.1.1.1.cmml" xref="S2.Ex4.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.Ex4.m1.1.1.1.1.1.1.cmml" xref="S2.Ex4.m1.1.1.1.1.1">subscript</csymbol><ci id="S2.Ex4.m1.1.1.1.1.1.2.cmml" xref="S2.Ex4.m1.1.1.1.1.1.2">ğ‘ƒ</ci><ci id="S2.Ex4.m1.1.1.1.1.1.3.cmml" xref="S2.Ex4.m1.1.1.1.1.1.3">ğ‘—</ci></apply></apply></apply><apply id="S2.Ex4.m1.5.6.3.2.cmml" xref="S2.Ex4.m1.5.6.3.2"><apply id="S2.Ex4.m1.5.6.3.2.1.cmml" xref="S2.Ex4.m1.5.6.3.2.1"><csymbol cd="ambiguous" id="S2.Ex4.m1.5.6.3.2.1.1.cmml" xref="S2.Ex4.m1.5.6.3.2.1">subscript</csymbol><sum id="S2.Ex4.m1.5.6.3.2.1.2.cmml" xref="S2.Ex4.m1.5.6.3.2.1.2"></sum><apply id="S2.Ex4.m1.5.6.3.2.1.3.cmml" xref="S2.Ex4.m1.5.6.3.2.1.3"><in id="S2.Ex4.m1.5.6.3.2.1.3.1.cmml" xref="S2.Ex4.m1.5.6.3.2.1.3.1"></in><ci id="S2.Ex4.m1.5.6.3.2.1.3.2.cmml" xref="S2.Ex4.m1.5.6.3.2.1.3.2">ğ‘</ci><apply id="S2.Ex4.m1.5.6.3.2.1.3.3.cmml" xref="S2.Ex4.m1.5.6.3.2.1.3.3"><csymbol cd="ambiguous" id="S2.Ex4.m1.5.6.3.2.1.3.3.1.cmml" xref="S2.Ex4.m1.5.6.3.2.1.3.3">subscript</csymbol><ci id="S2.Ex4.m1.5.6.3.2.1.3.3.2.cmml" xref="S2.Ex4.m1.5.6.3.2.1.3.3.2">ğ‘ƒ</ci><ci id="S2.Ex4.m1.5.6.3.2.1.3.3.3.cmml" xref="S2.Ex4.m1.5.6.3.2.1.3.3.3">ğ‘—</ci></apply></apply></apply><apply id="S2.Ex4.m1.5.6.3.2.2.1.cmml" xref="S2.Ex4.m1.5.6.3.2.2.2"><exp id="S2.Ex4.m1.5.5.cmml" xref="S2.Ex4.m1.5.5"></exp><apply id="S2.Ex4.m1.3.3.cmml" xref="S2.Ex4.m1.3.3"><divide id="S2.Ex4.m1.3.3.3.cmml" xref="S2.Ex4.m1.3.3"></divide><apply id="S2.Ex4.m1.3.3.2.cmml" xref="S2.Ex4.m1.3.3.2"><minus id="S2.Ex4.m1.3.3.2.3.cmml" xref="S2.Ex4.m1.3.3.2"></minus><apply id="S2.Ex4.m1.3.3.2.2.cmml" xref="S2.Ex4.m1.3.3.2.2"><times id="S2.Ex4.m1.3.3.2.2.3.cmml" xref="S2.Ex4.m1.3.3.2.2.3"></times><ci id="S2.Ex4.m1.3.3.2.2.4.cmml" xref="S2.Ex4.m1.3.3.2.2.4">ğ‘”</ci><apply id="S2.Ex4.m1.3.3.2.2.2.cmml" xref="S2.Ex4.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.Ex4.m1.3.3.2.2.2.3.cmml" xref="S2.Ex4.m1.3.3.2.2.2">superscript</csymbol><interval closure="open" id="S2.Ex4.m1.3.3.2.2.2.2.3.cmml" xref="S2.Ex4.m1.3.3.2.2.2.2.2"><apply id="S2.Ex4.m1.2.2.1.1.1.1.1.1.cmml" xref="S2.Ex4.m1.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.Ex4.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S2.Ex4.m1.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S2.Ex4.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S2.Ex4.m1.2.2.1.1.1.1.1.1.2">ğ‘–</ci><ci id="S2.Ex4.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S2.Ex4.m1.2.2.1.1.1.1.1.1.3">ğ‘</ci></apply><apply id="S2.Ex4.m1.3.3.2.2.2.2.2.2.cmml" xref="S2.Ex4.m1.3.3.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.Ex4.m1.3.3.2.2.2.2.2.2.1.cmml" xref="S2.Ex4.m1.3.3.2.2.2.2.2.2">subscript</csymbol><apply id="S2.Ex4.m1.3.3.2.2.2.2.2.2.2.cmml" xref="S2.Ex4.m1.3.3.2.2.2.2.2.2.2"><ci id="S2.Ex4.m1.3.3.2.2.2.2.2.2.2.1.cmml" xref="S2.Ex4.m1.3.3.2.2.2.2.2.2.2.1">^</ci><ci id="S2.Ex4.m1.3.3.2.2.2.2.2.2.2.2.cmml" xref="S2.Ex4.m1.3.3.2.2.2.2.2.2.2.2">ğ‘–</ci></apply><ci id="S2.Ex4.m1.3.3.2.2.2.2.2.2.3.cmml" xref="S2.Ex4.m1.3.3.2.2.2.2.2.2.3">ğ‘</ci></apply></interval><cn type="integer" id="S2.Ex4.m1.3.3.2.2.2.4.cmml" xref="S2.Ex4.m1.3.3.2.2.2.4">2</cn></apply></apply></apply><apply id="S2.Ex4.m1.3.3.4.cmml" xref="S2.Ex4.m1.3.3.4"><times id="S2.Ex4.m1.3.3.4.1.cmml" xref="S2.Ex4.m1.3.3.4.1"></times><cn type="integer" id="S2.Ex4.m1.3.3.4.2.cmml" xref="S2.Ex4.m1.3.3.4.2">2</cn><apply id="S2.Ex4.m1.3.3.4.3.cmml" xref="S2.Ex4.m1.3.3.4.3"><csymbol cd="ambiguous" id="S2.Ex4.m1.3.3.4.3.1.cmml" xref="S2.Ex4.m1.3.3.4.3">superscript</csymbol><ci id="S2.Ex4.m1.3.3.4.3.2.cmml" xref="S2.Ex4.m1.3.3.4.3.2">ğ‘˜</ci><cn type="integer" id="S2.Ex4.m1.3.3.4.3.3.cmml" xref="S2.Ex4.m1.3.3.4.3.3">2</cn></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex4.m1.5c">GPS(j)=\frac{1}{|P_{j}|}\sum_{p\in P_{j}}\exp\left(\frac{-g(i_{p},\hat{i}_{p})^{2}}{2k^{2}}\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
</tr></tbody>
</table>
<br class="ltx_break">
</div>
<div id="S2.SS1.p20" class="ltx_para">
<p id="S2.SS1.p20.2" class="ltx_p"><math id="S2.SS1.p20.1.m1.1" class="ltx_Math" alttext="P_{j}" display="inline"><semantics id="S2.SS1.p20.1.m1.1a"><msub id="S2.SS1.p20.1.m1.1.1" xref="S2.SS1.p20.1.m1.1.1.cmml"><mi id="S2.SS1.p20.1.m1.1.1.2" xref="S2.SS1.p20.1.m1.1.1.2.cmml">P</mi><mi id="S2.SS1.p20.1.m1.1.1.3" xref="S2.SS1.p20.1.m1.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p20.1.m1.1b"><apply id="S2.SS1.p20.1.m1.1.1.cmml" xref="S2.SS1.p20.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p20.1.m1.1.1.1.cmml" xref="S2.SS1.p20.1.m1.1.1">subscript</csymbol><ci id="S2.SS1.p20.1.m1.1.1.2.cmml" xref="S2.SS1.p20.1.m1.1.1.2">ğ‘ƒ</ci><ci id="S2.SS1.p20.1.m1.1.1.3.cmml" xref="S2.SS1.p20.1.m1.1.1.3">ğ‘—</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p20.1.m1.1c">P_{j}</annotation></semantics></math> is a set of points representing the body surface of the jth one person. <math id="S2.SS1.p20.2.m2.2" class="ltx_Math" alttext="g(i_{p},\hat{i}_{p})" display="inline"><semantics id="S2.SS1.p20.2.m2.2a"><mrow id="S2.SS1.p20.2.m2.2.2" xref="S2.SS1.p20.2.m2.2.2.cmml"><mi id="S2.SS1.p20.2.m2.2.2.4" xref="S2.SS1.p20.2.m2.2.2.4.cmml">g</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p20.2.m2.2.2.3" xref="S2.SS1.p20.2.m2.2.2.3.cmml">â€‹</mo><mrow id="S2.SS1.p20.2.m2.2.2.2.2" xref="S2.SS1.p20.2.m2.2.2.2.3.cmml"><mo stretchy="false" id="S2.SS1.p20.2.m2.2.2.2.2.3" xref="S2.SS1.p20.2.m2.2.2.2.3.cmml">(</mo><msub id="S2.SS1.p20.2.m2.1.1.1.1.1" xref="S2.SS1.p20.2.m2.1.1.1.1.1.cmml"><mi id="S2.SS1.p20.2.m2.1.1.1.1.1.2" xref="S2.SS1.p20.2.m2.1.1.1.1.1.2.cmml">i</mi><mi id="S2.SS1.p20.2.m2.1.1.1.1.1.3" xref="S2.SS1.p20.2.m2.1.1.1.1.1.3.cmml">p</mi></msub><mo id="S2.SS1.p20.2.m2.2.2.2.2.4" xref="S2.SS1.p20.2.m2.2.2.2.3.cmml">,</mo><msub id="S2.SS1.p20.2.m2.2.2.2.2.2" xref="S2.SS1.p20.2.m2.2.2.2.2.2.cmml"><mover accent="true" id="S2.SS1.p20.2.m2.2.2.2.2.2.2" xref="S2.SS1.p20.2.m2.2.2.2.2.2.2.cmml"><mi id="S2.SS1.p20.2.m2.2.2.2.2.2.2.2" xref="S2.SS1.p20.2.m2.2.2.2.2.2.2.2.cmml">i</mi><mo id="S2.SS1.p20.2.m2.2.2.2.2.2.2.1" xref="S2.SS1.p20.2.m2.2.2.2.2.2.2.1.cmml">^</mo></mover><mi id="S2.SS1.p20.2.m2.2.2.2.2.2.3" xref="S2.SS1.p20.2.m2.2.2.2.2.2.3.cmml">p</mi></msub><mo stretchy="false" id="S2.SS1.p20.2.m2.2.2.2.2.5" xref="S2.SS1.p20.2.m2.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p20.2.m2.2b"><apply id="S2.SS1.p20.2.m2.2.2.cmml" xref="S2.SS1.p20.2.m2.2.2"><times id="S2.SS1.p20.2.m2.2.2.3.cmml" xref="S2.SS1.p20.2.m2.2.2.3"></times><ci id="S2.SS1.p20.2.m2.2.2.4.cmml" xref="S2.SS1.p20.2.m2.2.2.4">ğ‘”</ci><interval closure="open" id="S2.SS1.p20.2.m2.2.2.2.3.cmml" xref="S2.SS1.p20.2.m2.2.2.2.2"><apply id="S2.SS1.p20.2.m2.1.1.1.1.1.cmml" xref="S2.SS1.p20.2.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p20.2.m2.1.1.1.1.1.1.cmml" xref="S2.SS1.p20.2.m2.1.1.1.1.1">subscript</csymbol><ci id="S2.SS1.p20.2.m2.1.1.1.1.1.2.cmml" xref="S2.SS1.p20.2.m2.1.1.1.1.1.2">ğ‘–</ci><ci id="S2.SS1.p20.2.m2.1.1.1.1.1.3.cmml" xref="S2.SS1.p20.2.m2.1.1.1.1.1.3">ğ‘</ci></apply><apply id="S2.SS1.p20.2.m2.2.2.2.2.2.cmml" xref="S2.SS1.p20.2.m2.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p20.2.m2.2.2.2.2.2.1.cmml" xref="S2.SS1.p20.2.m2.2.2.2.2.2">subscript</csymbol><apply id="S2.SS1.p20.2.m2.2.2.2.2.2.2.cmml" xref="S2.SS1.p20.2.m2.2.2.2.2.2.2"><ci id="S2.SS1.p20.2.m2.2.2.2.2.2.2.1.cmml" xref="S2.SS1.p20.2.m2.2.2.2.2.2.2.1">^</ci><ci id="S2.SS1.p20.2.m2.2.2.2.2.2.2.2.cmml" xref="S2.SS1.p20.2.m2.2.2.2.2.2.2.2">ğ‘–</ci></apply><ci id="S2.SS1.p20.2.m2.2.2.2.2.2.3.cmml" xref="S2.SS1.p20.2.m2.2.2.2.2.2.3">ğ‘</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p20.2.m2.2c">g(i_{p},\hat{i}_{p})</annotation></semantics></math> is the geodesic distance calculated between the estimated point and the ground truth one. A GPS score of 0.5 indicates that this distance is equal to half a predefined distance adjustable with the k parameter (often setup to be a fraction of a joint segment).
<br class="ltx_break"></p>
</div>
<div id="S2.SS1.p21" class="ltx_para">
<p id="S2.SS1.p21.1" class="ltx_p">3D human shape tracking is another variant of the task that reconstruct and track the human body volume frame by frame in a video. A common approach uses the iterative closest point (ICP) algorithm to fit image data to a model. <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Huang etÂ al.</span> (<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite> suggest using random forests and nearest-neighbor matching with two volumetric features based on voxels and centroÃ¯dal Voronoi tessellation instead of ICP.</p>
</div>
<div id="S2.SS1.p22" class="ltx_para">
<p id="S2.SS1.p22.1" class="ltx_p">Lastly, another popular family of methods is using multi-view data and shape-from-silhouette techniques to create volumetric representations of the human body. This shape can then be useful for joint location prediction. These methods produce probabilistic visual hulls (PVH) (<cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Grauman etÂ al.</span> (<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2003</span></a>)</cite>) in voxel grids. Even though human body shape estimation is not human pose estimation, it is a close task that can be used at different stages of a modular motion capture system. With multi-view datasets it is easy to obtain ground truths PVH that can then be used to evaluate 3D reconstructed volumes (e.g. <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Trumble etÂ al.</span> (<a href="#bib.bib71" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2017</span></a>)</cite>). Here Means Squared Error can be calculated from the voxel grid.
<br class="ltx_break"></p>
</div>
<div id="S2.SS1.p23" class="ltx_para">
<p id="S2.SS1.p23.1" class="ltx_p">Each of these metrics can be used in specific variations or edge cases of the task. For â€classicalâ€ human pose estimation, MPJPE seems more popular as it is simple and no extra parameters intervene in its computation. However, some published articles (<cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Ionescu etÂ al.</span> (<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2014</span></a>)</cite>; <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Mehta etÂ al.</span> (<a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2016</span></a>)</cite>) claim that threshold metrics are better at identifying errors in specific joints and less prone to penalize perceptually irrelevant errors. Finally, to evaluate methods that process videos and produce 3D pose sequences, the MPJVE is a good alternative to highlight techniques that produce more realistic human motions.
<br class="ltx_break"></p>
</div>
<div id="S2.SS1.p24" class="ltx_para">
<p id="S2.SS1.p24.1" class="ltx_p">Furthermore, the metrics described above only express physical accuracy in multiple ways, with threshold-based ones sometimes introducing perceptual parameters. However, depending on the use case, it might be pertinent to take into consideration more complex perceptual metrics <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Marinoiu etÂ al.</span> (<a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2016</span></a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Marinoiu etÂ al.</span> (<a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2013</span></a>)</cite> or structural metrics <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Kocabas etÂ al.</span> (<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019b</span></a>)</cite>. They can help when purely positional information produce the same error score for two different predicted poses. Significant work has been produced on the way human are perceiving what is a valid and realistic human body configuration. These metrics can be useful in fields that are not concerned about the biological and physical constraints, but more about pose semantic.
<br class="ltx_break"></p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Commonly used Benchmarks</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Collecting accurate data for human pose estimation is a long and complex process that is driven by progress in acquisition technologies. Moreover, several specific choices are needed concerning the sensor modality, quantity and the acquisition protocol.
<br class="ltx_break"></p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">The complexity of this task explains why it has taken time for the scientific community to create large benchmarks: today many variations exist between monocular versus multi-view, laboratory controlled versus in-the-wild environments (see Table <a href="#S2.T1" title="Table 1 â€£ 2.1 Metrics â€£ 2 Methods Evaluation â€£ A review of 3D human pose estimation algorithms for markerless motion capture" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) etc. With new commercial solutions (ie: Theia, The Captury) starting to produce results similar to traditional motion capture <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Kanko etÂ al.</span> (<a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite>, some benchmarks are also starting to use markerless labeled ground truths. They as the advantage of easily providing in-the-wild images. However, using this kind of data as ground truth can be questioned as it is itself obtained using methods that are not always available and transparent. Despite this diversity, there are still only a few openly accessible academic benchmarks containing more than millions of images.
<br class="ltx_break"></p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">Datasets have an important role as they are used to validate and test algorithms, but also to train and fine-tune deep learning models. Providing large, high-quality datasets with excellent labels and a wide variety of poses is a major challenge. Three references historically best meet these criteria: HumanEva I and II <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Sigal etÂ al.</span> (<a href="#bib.bib64" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2010</span></a>)</cite>, Human3.6M <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Ionescu etÂ al.</span> (<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2014</span></a>)</cite> and Total Capture <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Trumble etÂ al.</span> (<a href="#bib.bib71" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2017</span></a>)</cite>. They contain video sequences with multiple view angles associated with ground-truth joint coordinates. However, these large-scale references are mainly captured in controlled laboratory environments with marker-based systems, hence with a limited variety of backgrounds, poses and subjects. 
<br class="ltx_break"></p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">Other benchmarks focus on pose and subject diversity, or on in-the-wild environment acquisition. While interesting to experiment with for human pose estimation or its subtasks, they are not providing the same quantity and variation to conduct large-scale evaluations. However, an noticeable exception is MPI-INF-3DHP <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Mehta etÂ al.</span> (<a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2016</span></a>)</cite>, which is more and more used to benchmark algorithms as it proposes a high variety of contexts and subjects (using â€green screenâ€ and outdoor acquisition) with a significant amount of data.
<br class="ltx_break"></p>
</div>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.1" class="ltx_p">Marker-based motion capture is the easiest way to obtain something close to ground-truth data. However, old datasets have a low image resolution and some have inaccurate annotation for some subjects (reported by <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Iskakov etÂ al.</span> (<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite>). Additionally, <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Colyer etÂ al.</span> (<a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite> noted errors of about 10mm or 10<sup id="S2.SS2.p5.1.1" class="ltx_sup"><span id="S2.SS2.p5.1.1.1" class="ltx_text ltx_font_italic">âˆ˜</span></sup> compared to intrusive methods closer to the real human anatomy (ie: intra-cortical bone pins). This is because the key points are reconstructed from groups of markers placed on the subjectsâ€™ skin or clothing (i.e., soft surfaces).
<br class="ltx_break"></p>
</div>
<div id="S2.SS2.p6" class="ltx_para">
<p id="S2.SS2.p6.1" class="ltx_p">Some datasets propose 3D mesh-based models of the human body (<cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Mahmood etÂ al.</span> (<a href="#bib.bib39" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite>; <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Ghorbani etÂ al.</span> (<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite>). These representations can be used as learning targets for estimation algorithms that care about richer information than skeletal representation of the poses and joint positions. They are also useful for applications that need fully-rigged models such as animation. The AMASS dataset <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Mahmood etÂ al.</span> (<a href="#bib.bib39" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite> unifies several motion capture datasets by providing the 3D representation of subjects. This is done by computing body poses using the SMPL <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Loper etÂ al.</span> (<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2015</span></a>)</cite> model with their regression method (MoSh++). Furthermore, <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Ghorbani etÂ al.</span> (<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite> provide a new dataset with motion capture (MoCap) and inertial motion unit data, added to AMASS.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Architectures for Human Pose Estimation</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, the main 3D pose estimation families of methods will be described. They can be classified as methods using human body models, learning algorithms or geometric information. In the case of a neural network learning approach, backbone networks are employed and new loss functions are created. Table <a href="#S3.T2" title="Table 2 â€£ 3 Architectures for Human Pose Estimation â€£ A review of 3D human pose estimation algorithms for markerless motion capture" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> is the complete taxonomy of all discussed methods according to these criteria. In the second part of this section, we summarize the most commonly used architectures for 2D and 3D pose estimation.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<div id="S3.T2.29.29" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:279.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-467.1pt,301.2pt) scale(0.317014988336995,0.317014988336995) ;">
<table id="S3.T2.29.29.29" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.29.29.29.30.1" class="ltx_tr">
<th id="S3.T2.29.29.29.30.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" colspan="3"></th>
<td id="S3.T2.29.29.29.30.1.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_t" colspan="3">Human Body Models</td>
<td id="S3.T2.29.29.29.30.1.3" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="6">Neural Networks</td>
</tr>
<tr id="S3.T2.29.29.29.31.2" class="ltx_tr">
<th id="S3.T2.29.29.29.31.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Method</th>
<td id="S3.T2.29.29.29.31.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Proxy Representation</td>
<td id="S3.T2.29.29.29.31.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Losses</td>
<td id="S3.T2.29.29.29.31.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Kinematic</td>
<td id="S3.T2.29.29.29.31.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Skeleton</td>
<td id="S3.T2.29.29.29.31.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Mesh</td>
<td id="S3.T2.29.29.29.31.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Backbone</td>
<td id="S3.T2.29.29.29.31.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">GAN</td>
<td id="S3.T2.29.29.29.31.2.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">RNN</td>
<td id="S3.T2.29.29.29.31.2.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">TCN</td>
<td id="S3.T2.29.29.29.31.2.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Attention</td>
<td id="S3.T2.29.29.29.31.2.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">GCNN</td>
</tr>
<tr id="S3.T2.29.29.29.32.3" class="ltx_tr">
<th id="S3.T2.29.29.29.32.3.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<td id="S3.T2.29.29.29.32.3.2" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.32.3.3" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.32.3.4" class="ltx_td ltx_align_center ltx_border_r">Chains</td>
<td id="S3.T2.29.29.29.32.3.5" class="ltx_td ltx_align_center ltx_border_r">(e.g. PSM)</td>
<td id="S3.T2.29.29.29.32.3.6" class="ltx_td ltx_align_center ltx_border_r">(e.g. SMPL)</td>
<td id="S3.T2.29.29.29.32.3.7" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.32.3.8" class="ltx_td ltx_align_center ltx_border_r">Adv.lea.</td>
<td id="S3.T2.29.29.29.32.3.9" class="ltx_td ltx_align_center ltx_border_r">LSTM</td>
<td id="S3.T2.29.29.29.32.3.10" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.32.3.11" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.32.3.12" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S3.T2.29.29.29.33.4" class="ltx_tr">
<th id="S3.T2.29.29.29.33.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Pavlakos etÂ al.</span> (<a href="#bib.bib57" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2017</span></a>)</cite></th>
<td id="S3.T2.29.29.29.33.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2D Heatmaps</td>
<td id="S3.T2.29.29.29.33.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">L2</td>
<td id="S3.T2.29.29.29.33.4.4" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.33.4.5" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.33.4.6" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.33.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">SHNet</td>
<td id="S3.T2.29.29.29.33.4.8" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.33.4.9" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.33.4.10" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.33.4.11" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.33.4.12" class="ltx_td ltx_border_r ltx_border_t"></td>
</tr>
<tr id="S3.T2.1.1.1.1" class="ltx_tr">
<th id="S3.T2.1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Mehta etÂ al.</span> (<a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2017</span></a>)</cite></th>
<td id="S3.T2.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2D heatmaps,</td>
<td id="S3.T2.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">L2</td>
<td id="S3.T2.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T2.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.T2.1.1.1.1.1.m1.1a"><mo id="S3.T2.1.1.1.1.1.m1.1.1" xref="S3.T2.1.1.1.1.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S3.T2.1.1.1.1.1.m1.1b"><ci id="S3.T2.1.1.1.1.1.m1.1.1.cmml" xref="S3.T2.1.1.1.1.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.1.1.1.1.1.m1.1c">\bullet</annotation></semantics></math></td>
<td id="S3.T2.1.1.1.1.5" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.1.1.1.1.6" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Resnet50</td>
<td id="S3.T2.1.1.1.1.8" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.1.1.1.1.9" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.1.1.1.1.10" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.1.1.1.1.11" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.1.1.1.1.12" class="ltx_td ltx_border_r ltx_border_t"></td>
</tr>
<tr id="S3.T2.29.29.29.34.5" class="ltx_tr">
<th id="S3.T2.29.29.29.34.5.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<td id="S3.T2.29.29.29.34.5.2" class="ltx_td ltx_align_center ltx_border_r">â€Location mapsâ€</td>
<td id="S3.T2.29.29.29.34.5.3" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.34.5.4" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.34.5.5" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.34.5.6" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.34.5.7" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.34.5.8" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.34.5.9" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.34.5.10" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.34.5.11" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.34.5.12" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S3.T2.29.29.29.35.6" class="ltx_tr">
<th id="S3.T2.29.29.29.35.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Zhou etÂ al.</span> (<a href="#bib.bib81" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2017</span></a>)</cite></th>
<td id="S3.T2.29.29.29.35.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2D heatmaps</td>
<td id="S3.T2.29.29.29.35.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">L2,</td>
<td id="S3.T2.29.29.29.35.6.4" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.35.6.5" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.35.6.6" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.35.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">SHNet</td>
<td id="S3.T2.29.29.29.35.6.8" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.35.6.9" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.35.6.10" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.35.6.11" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.35.6.12" class="ltx_td ltx_border_r ltx_border_t"></td>
</tr>
<tr id="S3.T2.29.29.29.36.7" class="ltx_tr">
<th id="S3.T2.29.29.29.36.7.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<td id="S3.T2.29.29.29.36.7.2" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.36.7.3" class="ltx_td ltx_align_center ltx_border_r">â€geometric lossâ€</td>
<td id="S3.T2.29.29.29.36.7.4" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.36.7.5" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.36.7.6" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.36.7.7" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.36.7.8" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.36.7.9" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.36.7.10" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.36.7.11" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.36.7.12" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S3.T2.29.29.29.37.8" class="ltx_tr">
<th id="S3.T2.29.29.29.37.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Martinez etÂ al.</span> (<a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2017</span></a>)</cite></th>
<td id="S3.T2.29.29.29.37.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2D pose</td>
<td id="S3.T2.29.29.29.37.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">L2</td>
<td id="S3.T2.29.29.29.37.8.4" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.37.8.5" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.37.8.6" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.37.8.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">SHNet (2D)</td>
<td id="S3.T2.29.29.29.37.8.8" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.37.8.9" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.37.8.10" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.37.8.11" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.37.8.12" class="ltx_td ltx_border_r ltx_border_t"></td>
</tr>
<tr id="S3.T2.29.29.29.38.9" class="ltx_tr">
<th id="S3.T2.29.29.29.38.9.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<td id="S3.T2.29.29.29.38.9.2" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.38.9.3" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.38.9.4" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.38.9.5" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.38.9.6" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.38.9.7" class="ltx_td ltx_align_center ltx_border_r">+ MLP</td>
<td id="S3.T2.29.29.29.38.9.8" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.38.9.9" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.38.9.10" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.38.9.11" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.38.9.12" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S3.T2.29.29.29.39.10" class="ltx_tr">
<th id="S3.T2.29.29.29.39.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Sun etÂ al.</span> (<a href="#bib.bib67" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite></th>
<td id="S3.T2.29.29.29.39.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2D/3D heatmaps</td>
<td id="S3.T2.29.29.29.39.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">any heatmap losses</td>
<td id="S3.T2.29.29.29.39.10.4" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.39.10.5" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.39.10.6" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.39.10.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Resnet models</td>
<td id="S3.T2.29.29.29.39.10.8" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.39.10.9" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.39.10.10" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.39.10.11" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.39.10.12" class="ltx_td ltx_border_r ltx_border_t"></td>
</tr>
<tr id="S3.T2.29.29.29.40.11" class="ltx_tr">
<th id="S3.T2.29.29.29.40.11.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<td id="S3.T2.29.29.29.40.11.2" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.40.11.3" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.40.11.4" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.40.11.5" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.40.11.6" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.40.11.7" class="ltx_td ltx_align_center ltx_border_r">and SHNet tested</td>
<td id="S3.T2.29.29.29.40.11.8" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.40.11.9" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.40.11.10" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.40.11.11" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.40.11.12" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S3.T2.2.2.2.2" class="ltx_tr">
<th id="S3.T2.2.2.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Omran etÂ al.</span> (<a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite></th>
<td id="S3.T2.2.2.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">part segmentation map</td>
<td id="S3.T2.2.2.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3D and 2D joint loss (L2)</td>
<td id="S3.T2.2.2.2.2.5" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.2.2.2.2.6" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.2.2.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T2.2.2.2.2.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.T2.2.2.2.2.1.m1.1a"><mo id="S3.T2.2.2.2.2.1.m1.1.1" xref="S3.T2.2.2.2.2.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S3.T2.2.2.2.2.1.m1.1b"><ci id="S3.T2.2.2.2.2.1.m1.1.1.cmml" xref="S3.T2.2.2.2.2.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.2.2.2.2.1.m1.1c">\bullet</annotation></semantics></math></td>
<td id="S3.T2.2.2.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">RefineNet <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Lin etÂ al.</span> (<a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2016</span></a>)</cite>
</td>
<td id="S3.T2.2.2.2.2.8" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.2.2.2.2.9" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.2.2.2.2.10" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.2.2.2.2.11" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.2.2.2.2.12" class="ltx_td ltx_border_r ltx_border_t"></td>
</tr>
<tr id="S3.T2.29.29.29.41.12" class="ltx_tr">
<th id="S3.T2.29.29.29.41.12.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<td id="S3.T2.29.29.29.41.12.2" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.41.12.3" class="ltx_td ltx_align_center ltx_border_r">3D latent parameter loss (L1)</td>
<td id="S3.T2.29.29.29.41.12.4" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.41.12.5" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.41.12.6" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.41.12.7" class="ltx_td ltx_align_center ltx_border_r">+ Resnet50</td>
<td id="S3.T2.29.29.29.41.12.8" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.41.12.9" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.41.12.10" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.41.12.11" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.41.12.12" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S3.T2.3.3.3.3" class="ltx_tr">
<th id="S3.T2.3.3.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Mehta etÂ al.</span> (<a href="#bib.bib49" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite></th>
<td id="S3.T2.3.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">â€Occlusion Robust Pose Mapsâ€</td>
<td id="S3.T2.3.3.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">L2</td>
<td id="S3.T2.3.3.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T2.3.3.3.3.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.T2.3.3.3.3.1.m1.1a"><mo id="S3.T2.3.3.3.3.1.m1.1.1" xref="S3.T2.3.3.3.3.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S3.T2.3.3.3.3.1.m1.1b"><ci id="S3.T2.3.3.3.3.1.m1.1.1.cmml" xref="S3.T2.3.3.3.3.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.3.3.3.3.1.m1.1c">\bullet</annotation></semantics></math></td>
<td id="S3.T2.3.3.3.3.5" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.3.3.3.3.6" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.3.3.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">ResNet50</td>
<td id="S3.T2.3.3.3.3.8" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.3.3.3.3.9" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.3.3.3.3.10" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.3.3.3.3.11" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.3.3.3.3.12" class="ltx_td ltx_border_r ltx_border_t"></td>
</tr>
<tr id="S3.T2.29.29.29.42.13" class="ltx_tr">
<th id="S3.T2.29.29.29.42.13.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<td id="S3.T2.29.29.29.42.13.2" class="ltx_td ltx_align_center ltx_border_r">Part affinity fields</td>
<td id="S3.T2.29.29.29.42.13.3" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.42.13.4" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.42.13.5" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.42.13.6" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.42.13.7" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.42.13.8" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.42.13.9" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.42.13.10" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.42.13.11" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.42.13.12" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S3.T2.4.4.4.4" class="ltx_tr">
<th id="S3.T2.4.4.4.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Kolotouros etÂ al.</span> (<a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite></th>
<td id="S3.T2.4.4.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N/A</td>
<td id="S3.T2.4.4.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">L2</td>
<td id="S3.T2.4.4.4.4.5" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.4.4.4.4.6" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.4.4.4.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T2.4.4.4.4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.T2.4.4.4.4.1.m1.1a"><mo id="S3.T2.4.4.4.4.1.m1.1.1" xref="S3.T2.4.4.4.4.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S3.T2.4.4.4.4.1.m1.1b"><ci id="S3.T2.4.4.4.4.1.m1.1.1.cmml" xref="S3.T2.4.4.4.4.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.4.4.4.4.1.m1.1c">\bullet</annotation></semantics></math></td>
<td id="S3.T2.4.4.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">ResNet50</td>
<td id="S3.T2.4.4.4.4.8" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.4.4.4.4.9" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.4.4.4.4.10" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.4.4.4.4.11" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.4.4.4.4.12" class="ltx_td ltx_border_r ltx_border_t"></td>
</tr>
<tr id="S3.T2.6.6.6.6" class="ltx_tr">
<th id="S3.T2.6.6.6.6.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Wandt and Rosenhahn</span> (<a href="#bib.bib73" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite></th>
<td id="S3.T2.6.6.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N/A</td>
<td id="S3.T2.6.6.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">â€Reprojection lossâ€</td>
<td id="S3.T2.5.5.5.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T2.5.5.5.5.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.T2.5.5.5.5.1.m1.1a"><mo id="S3.T2.5.5.5.5.1.m1.1.1" xref="S3.T2.5.5.5.5.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S3.T2.5.5.5.5.1.m1.1b"><ci id="S3.T2.5.5.5.5.1.m1.1.1.cmml" xref="S3.T2.5.5.5.5.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.5.5.5.5.1.m1.1c">\bullet</annotation></semantics></math></td>
<td id="S3.T2.6.6.6.6.6" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.6.6.6.6.7" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.6.6.6.6.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">SHNet (2D)</td>
<td id="S3.T2.6.6.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T2.6.6.6.6.2.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.T2.6.6.6.6.2.m1.1a"><mo id="S3.T2.6.6.6.6.2.m1.1.1" xref="S3.T2.6.6.6.6.2.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S3.T2.6.6.6.6.2.m1.1b"><ci id="S3.T2.6.6.6.6.2.m1.1.1.cmml" xref="S3.T2.6.6.6.6.2.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.6.6.6.6.2.m1.1c">\bullet</annotation></semantics></math></td>
<td id="S3.T2.6.6.6.6.9" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.6.6.6.6.10" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.6.6.6.6.11" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.6.6.6.6.12" class="ltx_td ltx_border_r ltx_border_t"></td>
</tr>
<tr id="S3.T2.29.29.29.43.14" class="ltx_tr">
<th id="S3.T2.29.29.29.43.14.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<td id="S3.T2.29.29.29.43.14.2" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.43.14.3" class="ltx_td ltx_align_center ltx_border_r">Wasserstein loss, Camera loss</td>
<td id="S3.T2.29.29.29.43.14.4" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.43.14.5" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.43.14.6" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.43.14.7" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.43.14.8" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.43.14.9" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.43.14.10" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.43.14.11" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.43.14.12" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S3.T2.8.8.8.8" class="ltx_tr">
<th id="S3.T2.8.8.8.8.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Xu etÂ al.</span> (<a href="#bib.bib76" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite></th>
<td id="S3.T2.8.8.8.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">pixel-to-surface maps</td>
<td id="S3.T2.8.8.8.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">â€render-and-compare lossâ€</td>
<td id="S3.T2.8.8.8.8.6" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.8.8.8.8.7" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.7.7.7.7.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T2.7.7.7.7.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.T2.7.7.7.7.1.m1.1a"><mo id="S3.T2.7.7.7.7.1.m1.1.1" xref="S3.T2.7.7.7.7.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S3.T2.7.7.7.7.1.m1.1b"><ci id="S3.T2.7.7.7.7.1.m1.1.1.cmml" xref="S3.T2.7.7.7.7.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.7.7.7.7.1.m1.1c">\bullet</annotation></semantics></math></td>
<td id="S3.T2.8.8.8.8.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Resnet50</td>
<td id="S3.T2.8.8.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T2.8.8.8.8.2.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.T2.8.8.8.8.2.m1.1a"><mo id="S3.T2.8.8.8.8.2.m1.1.1" xref="S3.T2.8.8.8.8.2.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S3.T2.8.8.8.8.2.m1.1b"><ci id="S3.T2.8.8.8.8.2.m1.1.1.cmml" xref="S3.T2.8.8.8.8.2.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.8.8.8.8.2.m1.1c">\bullet</annotation></semantics></math></td>
<td id="S3.T2.8.8.8.8.9" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.8.8.8.8.10" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.8.8.8.8.11" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.8.8.8.8.12" class="ltx_td ltx_border_r ltx_border_t"></td>
</tr>
<tr id="S3.T2.29.29.29.44.15" class="ltx_tr">
<th id="S3.T2.29.29.29.44.15.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<td id="S3.T2.29.29.29.44.15.2" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.44.15.3" class="ltx_td ltx_align_center ltx_border_r">reconstruction loss (L2), parameter loss (L2)</td>
<td id="S3.T2.29.29.29.44.15.4" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.44.15.5" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.44.15.6" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.44.15.7" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.44.15.8" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.44.15.9" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.44.15.10" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.44.15.11" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.44.15.12" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S3.T2.29.29.29.45.16" class="ltx_tr">
<th id="S3.T2.29.29.29.45.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Kocabas etÂ al.</span> (<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019b</span></a>)</cite></th>
<td id="S3.T2.29.29.29.45.16.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2D pose</td>
<td id="S3.T2.29.29.29.45.16.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">smooth L1</td>
<td id="S3.T2.29.29.29.45.16.4" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.45.16.5" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.45.16.6" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.45.16.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Resnet50</td>
<td id="S3.T2.29.29.29.45.16.8" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.45.16.9" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.45.16.10" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.45.16.11" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.45.16.12" class="ltx_td ltx_border_r ltx_border_t"></td>
</tr>
<tr id="S3.T2.29.29.29.46.17" class="ltx_tr">
<th id="S3.T2.29.29.29.46.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Mathis etÂ al.</span> (<a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite></th>
<td id="S3.T2.29.29.29.46.17.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N/A</td>
<td id="S3.T2.29.29.29.46.17.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">L2</td>
<td id="S3.T2.29.29.29.46.17.4" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.46.17.5" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.46.17.6" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.46.17.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Resnet50</td>
<td id="S3.T2.29.29.29.46.17.8" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.46.17.9" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.46.17.10" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.46.17.11" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.46.17.12" class="ltx_td ltx_border_r ltx_border_t"></td>
</tr>
<tr id="S3.T2.29.29.29.47.18" class="ltx_tr">
<th id="S3.T2.29.29.29.47.18.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<td id="S3.T2.29.29.29.47.18.2" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.47.18.3" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.47.18.4" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.47.18.5" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.47.18.6" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.47.18.7" class="ltx_td ltx_align_center ltx_border_r">Resnet101 tested</td>
<td id="S3.T2.29.29.29.47.18.8" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.47.18.9" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.47.18.10" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.47.18.11" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.47.18.12" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S3.T2.9.9.9.9" class="ltx_tr">
<th id="S3.T2.9.9.9.9.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Mehta etÂ al.</span> (<a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite></th>
<td id="S3.T2.9.9.9.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">â€3D pose encodingâ€</td>
<td id="S3.T2.9.9.9.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">smooth L1</td>
<td id="S3.T2.9.9.9.9.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T2.9.9.9.9.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.T2.9.9.9.9.1.m1.1a"><mo id="S3.T2.9.9.9.9.1.m1.1.1" xref="S3.T2.9.9.9.9.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S3.T2.9.9.9.9.1.m1.1b"><ci id="S3.T2.9.9.9.9.1.m1.1.1.cmml" xref="S3.T2.9.9.9.9.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.9.9.9.9.1.m1.1c">\bullet</annotation></semantics></math></td>
<td id="S3.T2.9.9.9.9.5" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.9.9.9.9.6" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.9.9.9.9.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">â€SelecSLS Netâ€</td>
<td id="S3.T2.9.9.9.9.8" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.9.9.9.9.9" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.9.9.9.9.10" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.9.9.9.9.11" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.9.9.9.9.12" class="ltx_td ltx_border_r ltx_border_t"></td>
</tr>
<tr id="S3.T2.29.29.29.48.19" class="ltx_tr">
<th id="S3.T2.29.29.29.48.19.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<td id="S3.T2.29.29.29.48.19.2" class="ltx_td ltx_align_center ltx_border_r">Part affinity fields</td>
<td id="S3.T2.29.29.29.48.19.3" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.48.19.4" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.48.19.5" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.48.19.6" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.48.19.7" class="ltx_td ltx_align_center ltx_border_r">Fully connected</td>
<td id="S3.T2.29.29.29.48.19.8" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.48.19.9" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.48.19.10" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.48.19.11" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.48.19.12" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S3.T2.10.10.10.10" class="ltx_tr">
<th id="S3.T2.10.10.10.10.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Hossain and Little</span> (<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite></th>
<td id="S3.T2.10.10.10.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2D pose sequence</td>
<td id="S3.T2.10.10.10.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">L2</td>
<td id="S3.T2.10.10.10.10.5" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.10.10.10.10.6" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.10.10.10.10.7" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.10.10.10.10.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">SHNet (2D)</td>
<td id="S3.T2.10.10.10.10.9" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.10.10.10.10.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T2.10.10.10.10.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.T2.10.10.10.10.1.m1.1a"><mo id="S3.T2.10.10.10.10.1.m1.1.1" xref="S3.T2.10.10.10.10.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S3.T2.10.10.10.10.1.m1.1b"><ci id="S3.T2.10.10.10.10.1.m1.1.1.cmml" xref="S3.T2.10.10.10.10.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.10.10.10.10.1.m1.1c">\bullet</annotation></semantics></math></td>
<td id="S3.T2.10.10.10.10.10" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.10.10.10.10.11" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.10.10.10.10.12" class="ltx_td ltx_border_r ltx_border_t"></td>
</tr>
<tr id="S3.T2.29.29.29.49.20" class="ltx_tr">
<th id="S3.T2.29.29.29.49.20.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<td id="S3.T2.29.29.29.49.20.2" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.49.20.3" class="ltx_td ltx_align_center ltx_border_r">derivative loss on joint sets</td>
<td id="S3.T2.29.29.29.49.20.4" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.49.20.5" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.49.20.6" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.49.20.7" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.49.20.8" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.49.20.9" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.49.20.10" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.49.20.11" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.49.20.12" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S3.T2.11.11.11.11" class="ltx_tr">
<th id="S3.T2.11.11.11.11.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">Cai etÂ al. (<a href="#bib.bib6" title="" class="ltx_ref">2019</a>)</cite></th>
<td id="S3.T2.11.11.11.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2D pose, ST-graph</td>
<td id="S3.T2.11.11.11.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">L2, â€symmetry lossâ€</td>
<td id="S3.T2.11.11.11.11.5" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.11.11.11.11.6" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.11.11.11.11.7" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.11.11.11.11.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">CPN</td>
<td id="S3.T2.11.11.11.11.9" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.11.11.11.11.10" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.11.11.11.11.11" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.11.11.11.11.12" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.11.11.11.11.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T2.11.11.11.11.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.T2.11.11.11.11.1.m1.1a"><mo id="S3.T2.11.11.11.11.1.m1.1.1" xref="S3.T2.11.11.11.11.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S3.T2.11.11.11.11.1.m1.1b"><ci id="S3.T2.11.11.11.11.1.m1.1.1.cmml" xref="S3.T2.11.11.11.11.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.11.11.11.11.1.m1.1c">\bullet</annotation></semantics></math></td>
</tr>
<tr id="S3.T2.29.29.29.50.21" class="ltx_tr">
<th id="S3.T2.29.29.29.50.21.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<td id="S3.T2.29.29.29.50.21.2" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.50.21.3" class="ltx_td ltx_align_center ltx_border_r">derivative loss on joint sets</td>
<td id="S3.T2.29.29.29.50.21.4" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.50.21.5" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.50.21.6" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.50.21.7" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.50.21.8" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.50.21.9" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.50.21.10" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.50.21.11" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.50.21.12" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S3.T2.12.12.12.12" class="ltx_tr">
<th id="S3.T2.12.12.12.12.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Pavllo etÂ al.</span> (<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite></th>
<td id="S3.T2.12.12.12.12.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2D pose</td>
<td id="S3.T2.12.12.12.12.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">trajectory and pose loss</td>
<td id="S3.T2.12.12.12.12.5" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.12.12.12.12.6" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.12.12.12.12.7" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.12.12.12.12.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">SHNet, CPN</td>
<td id="S3.T2.12.12.12.12.9" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.12.12.12.12.10" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.12.12.12.12.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T2.12.12.12.12.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.T2.12.12.12.12.1.m1.1a"><mo id="S3.T2.12.12.12.12.1.m1.1.1" xref="S3.T2.12.12.12.12.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S3.T2.12.12.12.12.1.m1.1b"><ci id="S3.T2.12.12.12.12.1.m1.1.1.cmml" xref="S3.T2.12.12.12.12.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.12.12.12.12.1.m1.1c">\bullet</annotation></semantics></math></td>
<td id="S3.T2.12.12.12.12.11" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.12.12.12.12.12" class="ltx_td ltx_border_r ltx_border_t"></td>
</tr>
<tr id="S3.T2.29.29.29.51.22" class="ltx_tr">
<th id="S3.T2.29.29.29.51.22.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<td id="S3.T2.29.29.29.51.22.2" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.51.22.3" class="ltx_td ltx_align_center ltx_border_r">bone length L2 loss, 2D projection loss</td>
<td id="S3.T2.29.29.29.51.22.4" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.51.22.5" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.51.22.6" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.51.22.7" class="ltx_td ltx_align_center ltx_border_r">and Mask-RCNN tested (2D)</td>
<td id="S3.T2.29.29.29.51.22.8" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.51.22.9" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.51.22.10" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.51.22.11" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.51.22.12" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S3.T2.15.15.15.15" class="ltx_tr">
<th id="S3.T2.15.15.15.15.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Cheng etÂ al.</span> (<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite></th>
<td id="S3.T2.15.15.15.15.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2D pose</td>
<td id="S3.T2.15.15.15.15.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">L2, 2D projection loss</td>
<td id="S3.T2.15.15.15.15.7" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.13.13.13.13.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T2.13.13.13.13.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.T2.13.13.13.13.1.m1.1a"><mo id="S3.T2.13.13.13.13.1.m1.1.1" xref="S3.T2.13.13.13.13.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S3.T2.13.13.13.13.1.m1.1b"><ci id="S3.T2.13.13.13.13.1.m1.1.1.cmml" xref="S3.T2.13.13.13.13.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.13.13.13.13.1.m1.1c">\bullet</annotation></semantics></math></td>
<td id="S3.T2.15.15.15.15.8" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.15.15.15.15.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">SHNet (2D)</td>
<td id="S3.T2.14.14.14.14.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T2.14.14.14.14.2.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.T2.14.14.14.14.2.m1.1a"><mo id="S3.T2.14.14.14.14.2.m1.1.1" xref="S3.T2.14.14.14.14.2.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S3.T2.14.14.14.14.2.m1.1b"><ci id="S3.T2.14.14.14.14.2.m1.1.1.cmml" xref="S3.T2.14.14.14.14.2.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.14.14.14.14.2.m1.1c">\bullet</annotation></semantics></math></td>
<td id="S3.T2.15.15.15.15.10" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.15.15.15.15.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T2.15.15.15.15.3.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.T2.15.15.15.15.3.m1.1a"><mo id="S3.T2.15.15.15.15.3.m1.1.1" xref="S3.T2.15.15.15.15.3.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S3.T2.15.15.15.15.3.m1.1b"><ci id="S3.T2.15.15.15.15.3.m1.1.1.cmml" xref="S3.T2.15.15.15.15.3.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.15.15.15.15.3.m1.1c">\bullet</annotation></semantics></math></td>
<td id="S3.T2.15.15.15.15.11" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.15.15.15.15.12" class="ltx_td ltx_border_r ltx_border_t"></td>
</tr>
<tr id="S3.T2.18.18.18.18" class="ltx_tr">
<th id="S3.T2.18.18.18.18.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">Cheng etÂ al. (<a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite></th>
<td id="S3.T2.18.18.18.18.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2D heatmaps</td>
<td id="S3.T2.18.18.18.18.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">L2, â€multi-view lossâ€</td>
<td id="S3.T2.16.16.16.16.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T2.16.16.16.16.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.T2.16.16.16.16.1.m1.1a"><mo id="S3.T2.16.16.16.16.1.m1.1.1" xref="S3.T2.16.16.16.16.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S3.T2.16.16.16.16.1.m1.1b"><ci id="S3.T2.16.16.16.16.1.m1.1.1.cmml" xref="S3.T2.16.16.16.16.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.16.16.16.16.1.m1.1c">\bullet</annotation></semantics></math></td>
<td id="S3.T2.18.18.18.18.7" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.18.18.18.18.8" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.18.18.18.18.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">HRNet (2D)</td>
<td id="S3.T2.17.17.17.17.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T2.17.17.17.17.2.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.T2.17.17.17.17.2.m1.1a"><mo id="S3.T2.17.17.17.17.2.m1.1.1" xref="S3.T2.17.17.17.17.2.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S3.T2.17.17.17.17.2.m1.1b"><ci id="S3.T2.17.17.17.17.2.m1.1.1.cmml" xref="S3.T2.17.17.17.17.2.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.17.17.17.17.2.m1.1c">\bullet</annotation></semantics></math></td>
<td id="S3.T2.18.18.18.18.10" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.18.18.18.18.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T2.18.18.18.18.3.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.T2.18.18.18.18.3.m1.1a"><mo id="S3.T2.18.18.18.18.3.m1.1.1" xref="S3.T2.18.18.18.18.3.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S3.T2.18.18.18.18.3.m1.1b"><ci id="S3.T2.18.18.18.18.3.m1.1.1.cmml" xref="S3.T2.18.18.18.18.3.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.18.18.18.18.3.m1.1c">\bullet</annotation></semantics></math></td>
<td id="S3.T2.18.18.18.18.11" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.18.18.18.18.12" class="ltx_td ltx_border_r ltx_border_t"></td>
</tr>
<tr id="S3.T2.29.29.29.52.23" class="ltx_tr">
<th id="S3.T2.29.29.29.52.23.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<td id="S3.T2.29.29.29.52.23.2" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.52.23.3" class="ltx_td ltx_align_center ltx_border_r">2D projection loss</td>
<td id="S3.T2.29.29.29.52.23.4" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.52.23.5" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.52.23.6" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.52.23.7" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.52.23.8" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.52.23.9" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.52.23.10" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.52.23.11" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.52.23.12" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S3.T2.20.20.20.20" class="ltx_tr">
<th id="S3.T2.20.20.20.20.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Liu etÂ al.</span> (<a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite></th>
<td id="S3.T2.20.20.20.20.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N/A</td>
<td id="S3.T2.20.20.20.20.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N/A</td>
<td id="S3.T2.20.20.20.20.6" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.20.20.20.20.7" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.20.20.20.20.8" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.20.20.20.20.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">SHNet and CPN</td>
<td id="S3.T2.20.20.20.20.10" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.20.20.20.20.11" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.19.19.19.19.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T2.19.19.19.19.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.T2.19.19.19.19.1.m1.1a"><mo id="S3.T2.19.19.19.19.1.m1.1.1" xref="S3.T2.19.19.19.19.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S3.T2.19.19.19.19.1.m1.1b"><ci id="S3.T2.19.19.19.19.1.m1.1.1.cmml" xref="S3.T2.19.19.19.19.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.19.19.19.19.1.m1.1c">\bullet</annotation></semantics></math></td>
<td id="S3.T2.20.20.20.20.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T2.20.20.20.20.2.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.T2.20.20.20.20.2.m1.1a"><mo id="S3.T2.20.20.20.20.2.m1.1.1" xref="S3.T2.20.20.20.20.2.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S3.T2.20.20.20.20.2.m1.1b"><ci id="S3.T2.20.20.20.20.2.m1.1.1.cmml" xref="S3.T2.20.20.20.20.2.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.20.20.20.20.2.m1.1c">\bullet</annotation></semantics></math></td>
<td id="S3.T2.20.20.20.20.12" class="ltx_td ltx_border_r ltx_border_t"></td>
</tr>
<tr id="S3.T2.29.29.29.53.24" class="ltx_tr">
<th id="S3.T2.29.29.29.53.24.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<td id="S3.T2.29.29.29.53.24.2" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.53.24.3" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.53.24.4" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.53.24.5" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.53.24.6" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.53.24.7" class="ltx_td ltx_align_center ltx_border_r">tested (2D)</td>
<td id="S3.T2.29.29.29.53.24.8" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.53.24.9" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.53.24.10" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.53.24.11" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.53.24.12" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S3.T2.21.21.21.21" class="ltx_tr">
<th id="S3.T2.21.21.21.21.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Wang etÂ al.</span> (<a href="#bib.bib74" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite></th>
<td id="S3.T2.21.21.21.21.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2D pose, ST-graph</td>
<td id="S3.T2.21.21.21.21.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">L2, â€motion lossâ€</td>
<td id="S3.T2.21.21.21.21.5" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.21.21.21.21.6" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.21.21.21.21.7" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.21.21.21.21.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">CPN and HRNet</td>
<td id="S3.T2.21.21.21.21.9" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.21.21.21.21.10" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.21.21.21.21.11" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.21.21.21.21.12" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.21.21.21.21.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T2.21.21.21.21.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.T2.21.21.21.21.1.m1.1a"><mo id="S3.T2.21.21.21.21.1.m1.1.1" xref="S3.T2.21.21.21.21.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S3.T2.21.21.21.21.1.m1.1b"><ci id="S3.T2.21.21.21.21.1.m1.1.1.cmml" xref="S3.T2.21.21.21.21.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.21.21.21.21.1.m1.1c">\bullet</annotation></semantics></math></td>
</tr>
<tr id="S3.T2.29.29.29.54.25" class="ltx_tr">
<th id="S3.T2.29.29.29.54.25.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<td id="S3.T2.29.29.29.54.25.2" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.54.25.3" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.54.25.4" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.54.25.5" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.54.25.6" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.54.25.7" class="ltx_td ltx_align_center ltx_border_r">tested (2D)</td>
<td id="S3.T2.29.29.29.54.25.8" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.54.25.9" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.54.25.10" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.54.25.11" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.54.25.12" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S3.T2.22.22.22.22" class="ltx_tr">
<th id="S3.T2.22.22.22.22.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Qiu etÂ al.</span> (<a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite></th>
<td id="S3.T2.22.22.22.22.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2D heatmaps</td>
<td id="S3.T2.22.22.22.22.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">L2</td>
<td id="S3.T2.22.22.22.22.5" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.22.22.22.22.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T2.22.22.22.22.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.T2.22.22.22.22.1.m1.1a"><mo id="S3.T2.22.22.22.22.1.m1.1.1" xref="S3.T2.22.22.22.22.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S3.T2.22.22.22.22.1.m1.1b"><ci id="S3.T2.22.22.22.22.1.m1.1.1.cmml" xref="S3.T2.22.22.22.22.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.22.22.22.22.1.m1.1c">\bullet</annotation></semantics></math></td>
<td id="S3.T2.22.22.22.22.6" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.22.22.22.22.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">SimpleNet</td>
<td id="S3.T2.22.22.22.22.8" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.22.22.22.22.9" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.22.22.22.22.10" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.22.22.22.22.11" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.22.22.22.22.12" class="ltx_td ltx_border_r ltx_border_t"></td>
</tr>
<tr id="S3.T2.29.29.29.55.26" class="ltx_tr">
<th id="S3.T2.29.29.29.55.26.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Iskakov etÂ al.</span> (<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite></th>
<td id="S3.T2.29.29.29.55.26.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2D heatmaps</td>
<td id="S3.T2.29.29.29.55.26.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">soft L2, L1 regularized</td>
<td id="S3.T2.29.29.29.55.26.4" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.55.26.5" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.55.26.6" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.55.26.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">SimpleNet</td>
<td id="S3.T2.29.29.29.55.26.8" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.55.26.9" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.55.26.10" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.55.26.11" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.55.26.12" class="ltx_td ltx_border_r ltx_border_t"></td>
</tr>
<tr id="S3.T2.23.23.23.23" class="ltx_tr">
<th id="S3.T2.23.23.23.23.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">He etÂ al.</span> (<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite></th>
<td id="S3.T2.23.23.23.23.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2D pose</td>
<td id="S3.T2.23.23.23.23.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">L2</td>
<td id="S3.T2.23.23.23.23.5" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.23.23.23.23.6" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.23.23.23.23.7" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.23.23.23.23.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">SimpleNet</td>
<td id="S3.T2.23.23.23.23.9" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.23.23.23.23.10" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.23.23.23.23.11" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.23.23.23.23.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T2.23.23.23.23.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.T2.23.23.23.23.1.m1.1a"><mo id="S3.T2.23.23.23.23.1.m1.1.1" xref="S3.T2.23.23.23.23.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S3.T2.23.23.23.23.1.m1.1b"><ci id="S3.T2.23.23.23.23.1.m1.1.1.cmml" xref="S3.T2.23.23.23.23.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.23.23.23.23.1.m1.1c">\bullet</annotation></semantics></math></td>
<td id="S3.T2.23.23.23.23.12" class="ltx_td ltx_border_r ltx_border_t"></td>
</tr>
<tr id="S3.T2.25.25.25.25" class="ltx_tr">
<th id="S3.T2.25.25.25.25.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">von Marcard etÂ al.</span> (<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2016</span></a>)</cite></th>
<td id="S3.T2.25.25.25.25.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">multi-view silhouettes</td>
<td id="S3.T2.25.25.25.25.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N/A</td>
<td id="S3.T2.24.24.24.24.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T2.24.24.24.24.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.T2.24.24.24.24.1.m1.1a"><mo id="S3.T2.24.24.24.24.1.m1.1.1" xref="S3.T2.24.24.24.24.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S3.T2.24.24.24.24.1.m1.1b"><ci id="S3.T2.24.24.24.24.1.m1.1.1.cmml" xref="S3.T2.24.24.24.24.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.24.24.24.24.1.m1.1c">\bullet</annotation></semantics></math></td>
<td id="S3.T2.25.25.25.25.6" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.25.25.25.25.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T2.25.25.25.25.2.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.T2.25.25.25.25.2.m1.1a"><mo id="S3.T2.25.25.25.25.2.m1.1.1" xref="S3.T2.25.25.25.25.2.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S3.T2.25.25.25.25.2.m1.1b"><ci id="S3.T2.25.25.25.25.2.m1.1.1.cmml" xref="S3.T2.25.25.25.25.2.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.25.25.25.25.2.m1.1c">\bullet</annotation></semantics></math></td>
<td id="S3.T2.25.25.25.25.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N/A</td>
<td id="S3.T2.25.25.25.25.8" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.25.25.25.25.9" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.25.25.25.25.10" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.25.25.25.25.11" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.25.25.25.25.12" class="ltx_td ltx_border_r ltx_border_t"></td>
</tr>
<tr id="S3.T2.29.29.29.56.27" class="ltx_tr">
<th id="S3.T2.29.29.29.56.27.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<td id="S3.T2.29.29.29.56.27.2" class="ltx_td ltx_align_center ltx_border_r">IMU orientations</td>
<td id="S3.T2.29.29.29.56.27.3" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.56.27.4" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.56.27.5" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.56.27.6" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.56.27.7" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.56.27.8" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.56.27.9" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.56.27.10" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.56.27.11" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.56.27.12" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S3.T2.26.26.26.26" class="ltx_tr">
<th id="S3.T2.26.26.26.26.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Trumble etÂ al.</span> (<a href="#bib.bib71" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2017</span></a>)</cite></th>
<td id="S3.T2.26.26.26.26.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">PVH, IMU orientations</td>
<td id="S3.T2.26.26.26.26.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">L2</td>
<td id="S3.T2.26.26.26.26.5" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.26.26.26.26.6" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.26.26.26.26.7" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.26.26.26.26.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Classical</td>
<td id="S3.T2.26.26.26.26.9" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.26.26.26.26.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T2.26.26.26.26.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.T2.26.26.26.26.1.m1.1a"><mo id="S3.T2.26.26.26.26.1.m1.1.1" xref="S3.T2.26.26.26.26.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S3.T2.26.26.26.26.1.m1.1b"><ci id="S3.T2.26.26.26.26.1.m1.1.1.cmml" xref="S3.T2.26.26.26.26.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.26.26.26.26.1.m1.1c">\bullet</annotation></semantics></math></td>
<td id="S3.T2.26.26.26.26.10" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.26.26.26.26.11" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.26.26.26.26.12" class="ltx_td ltx_border_r ltx_border_t"></td>
</tr>
<tr id="S3.T2.29.29.29.57.28" class="ltx_tr">
<th id="S3.T2.29.29.29.57.28.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<td id="S3.T2.29.29.29.57.28.2" class="ltx_td ltx_align_center ltx_border_r">then 2D coordinates</td>
<td id="S3.T2.29.29.29.57.28.3" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.57.28.4" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.57.28.5" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.57.28.6" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.57.28.7" class="ltx_td ltx_align_center ltx_border_r">3D CNN</td>
<td id="S3.T2.29.29.29.57.28.8" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.57.28.9" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.57.28.10" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.57.28.11" class="ltx_td ltx_border_r"></td>
<td id="S3.T2.29.29.29.57.28.12" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S3.T2.28.28.28.28" class="ltx_tr">
<th id="S3.T2.28.28.28.28.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">von Marcard etÂ al.</span> (<a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite></th>
<td id="S3.T2.28.28.28.28.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2D pose, IMU orientations</td>
<td id="S3.T2.28.28.28.28.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N/A</td>
<td id="S3.T2.27.27.27.27.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T2.27.27.27.27.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.T2.27.27.27.27.1.m1.1a"><mo id="S3.T2.27.27.27.27.1.m1.1.1" xref="S3.T2.27.27.27.27.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S3.T2.27.27.27.27.1.m1.1b"><ci id="S3.T2.27.27.27.27.1.m1.1.1.cmml" xref="S3.T2.27.27.27.27.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.27.27.27.27.1.m1.1c">\bullet</annotation></semantics></math></td>
<td id="S3.T2.28.28.28.28.6" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.28.28.28.28.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T2.28.28.28.28.2.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.T2.28.28.28.28.2.m1.1a"><mo id="S3.T2.28.28.28.28.2.m1.1.1" xref="S3.T2.28.28.28.28.2.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S3.T2.28.28.28.28.2.m1.1b"><ci id="S3.T2.28.28.28.28.2.m1.1.1.cmml" xref="S3.T2.28.28.28.28.2.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.28.28.28.28.2.m1.1c">\bullet</annotation></semantics></math></td>
<td id="S3.T2.28.28.28.28.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<cite class="ltx_cite ltx_citemacro_cite">Cao etÂ al. (<a href="#bib.bib7" title="" class="ltx_ref">2017</a>)</cite> (multi-person)</td>
<td id="S3.T2.28.28.28.28.8" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.28.28.28.28.9" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.28.28.28.28.10" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.28.28.28.28.11" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.28.28.28.28.12" class="ltx_td ltx_border_r ltx_border_t"></td>
</tr>
<tr id="S3.T2.29.29.29.58.29" class="ltx_tr">
<th id="S3.T2.29.29.29.58.29.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Huang etÂ al.</span> (<a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite></th>
<td id="S3.T2.29.29.29.58.29.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">â€multi-channel volumeâ€</td>
<td id="S3.T2.29.29.29.58.29.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">L2</td>
<td id="S3.T2.29.29.29.58.29.4" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.58.29.5" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.58.29.6" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.58.29.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">SHNet (3D Conv)</td>
<td id="S3.T2.29.29.29.58.29.8" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.58.29.9" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.58.29.10" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.58.29.11" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.58.29.12" class="ltx_td ltx_border_r ltx_border_t"></td>
</tr>
<tr id="S3.T2.29.29.29.29" class="ltx_tr">
<th id="S3.T2.29.29.29.29.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Zhang etÂ al.</span> (<a href="#bib.bib80" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite></th>
<td id="S3.T2.29.29.29.29.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">2D heatmaps</td>
<td id="S3.T2.29.29.29.29.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">N/A</td>
<td id="S3.T2.29.29.29.29.5" class="ltx_td ltx_border_b ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.29.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><math id="S3.T2.29.29.29.29.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.T2.29.29.29.29.1.m1.1a"><mo id="S3.T2.29.29.29.29.1.m1.1.1" xref="S3.T2.29.29.29.29.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S3.T2.29.29.29.29.1.m1.1b"><ci id="S3.T2.29.29.29.29.1.m1.1.1.cmml" xref="S3.T2.29.29.29.29.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.29.29.29.29.1.m1.1c">\bullet</annotation></semantics></math></td>
<td id="S3.T2.29.29.29.29.6" class="ltx_td ltx_border_b ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.29.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">SimpleNet</td>
<td id="S3.T2.29.29.29.29.8" class="ltx_td ltx_border_b ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.29.9" class="ltx_td ltx_border_b ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.29.10" class="ltx_td ltx_border_b ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.29.11" class="ltx_td ltx_border_b ltx_border_r ltx_border_t"></td>
<td id="S3.T2.29.29.29.29.12" class="ltx_td ltx_border_b ltx_border_r ltx_border_t"></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>The taxonomy of reviewed methods. In case of multiple stages we indicate intermediate representations. For learning methods, loss functions and backbone architecture are indicated when they are present (for many two-stage methods these backbones concern only the first stage of 2D detection). Backbones are referred to as <span id="S3.T2.34.1" class="ltx_text ltx_font_italic">SHNet</span>: Stacked Hourglass (<cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Newell etÂ al.</span> (<a href="#bib.bib54" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2016</span></a>)</cite>), <span id="S3.T2.35.2" class="ltx_text ltx_font_italic">CPN</span>: Cascaded Pyramid (<cite class="ltx_cite ltx_citemacro_cite">Chen etÂ al. (<a href="#bib.bib9" title="" class="ltx_ref">2018</a>)</cite>), <span id="S3.T2.36.3" class="ltx_text ltx_font_italic">HRNet</span>: High Resolution Network (<cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Sun etÂ al.</span> (<a href="#bib.bib65" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite>) and <span id="S3.T2.37.4" class="ltx_text ltx_font_italic">SimpleNet</span>: Simple Baselines (<cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Xiao etÂ al.</span> (<a href="#bib.bib75" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite>).</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>3D Pose Estimation Taxonomy</h3>

<section id="S3.SS1.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Human Body Models</h4>

<div id="S3.SS1.SSSx1.p1" class="ltx_para">
<p id="S3.SS1.SSSx1.p1.1" class="ltx_p">Historically, pose estimation algorithms were relying on part-based or <span id="S3.SS1.SSSx1.p1.1.1" class="ltx_text ltx_font_bold">skeleton models</span> of the human body. Each node represented a joint and vertices limb length and orientation. One example of this kind of approach is the Pictorial Structure Model (PSM) introduced by <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Fischler and Elschlager</span> (<a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">1973</span></a>)</cite> and used for pose estimation in <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Felzenszwalb and Huttenlocher</span> (<a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2005</span></a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Marinoiu etÂ al.</span> (<a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2013</span></a>)</cite> and <cite class="ltx_cite ltx_citemacro_cite">Belagiannis etÂ al. (<a href="#bib.bib3" title="" class="ltx_ref">2014</a>)</cite>. Originally, this model is devised as the minimization of an energy function. The cost function combines an error term for joint location error and a penalty for segment length deformations (i.e. not corresponding to limb size).
<br class="ltx_break"></p>
</div>
<div id="S3.SS1.SSSx1.p2" class="ltx_para">
<p id="S3.SS1.SSSx1.p2.1" class="ltx_p">Several methods adopt <span id="S3.SS1.SSSx1.p2.1.1" class="ltx_text ltx_font_bold">kinematic</span> based human skeleton, where each linked joint pair is represented as a vector. With this technique, angular and length constraints can be applied to detect poses. Kinematic Chain Space proposed by <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Wandt and Rosenhahn</span> (<a href="#bib.bib73" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite> is an example of such techniques.
<br class="ltx_break"></p>
</div>
<div id="S3.SS1.SSSx1.p3" class="ltx_para">
<p id="S3.SS1.SSSx1.p3.1" class="ltx_p"><span id="S3.SS1.SSSx1.p3.1.1" class="ltx_text ltx_font_bold">Mesh models</span> consists of complete reconstruction of the human body surface. These models offer a richer information than skeleton-based models and can be used to infer the spatial representation of a subject in a virtual scene or to render captured pose into fully-rigged meshes for animation. SMPL <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Loper etÂ al.</span> (<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2015</span></a>)</cite> is the most frequently used human mesh model for pose estimation. The mesh is learned from numerous 3D body scans and can be adapted to a set of pose and shape parameters produced by pose estimation algorithms.</p>
</div>
</section>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Geometric Information</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">When several cameras are available, multi-view geometry is frequently used for 3D pose estimation. One way to infer joint coordinates in three dimensions is to use <span id="S3.SS1.SSS1.p1.1.1" class="ltx_text ltx_font_bold">triangulation</span> with their 2D image coordinates in each view. Depending on the calibration and availability of camera extrinsic and intrinsic parameters, different reconstruction schemes are possible.
<br class="ltx_break"></p>
</div>
<div id="S3.SS1.SSS1.p2" class="ltx_para">
<p id="S3.SS1.SSS1.p2.1" class="ltx_p">Another approach consists of <span id="S3.SS1.SSS1.p2.1.1" class="ltx_text ltx_font_bold">fusing features</span> from different views along epipolar lines before inferring poses. The epipolar line is the image in one camera of a ray passing through the optical center of the other camera and a point in the scene. Considering a point in the first view, its corresponding point in the second view is guaranteed to lie on the epipolar line in the other image. Using this prior information about the different views, the 2D pose can be refined or multi-view features can be merged before the 3D pose itself is estimated.
<br class="ltx_break"></p>
</div>
<div id="S3.SS1.SSS1.p3" class="ltx_para">
<p id="S3.SS1.SSS1.p3.1" class="ltx_p">Finally, other methods use shape-from-silhouette reconstruction to obtain the whole body shape before joint detection. These techniques first segment human shapes from each view and then reconstruct their volume (an example of these methods are probabilistic visual hulls from <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Grauman etÂ al.</span> (<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2003</span></a>)</cite>). These volumes can then be used as intermediate features.</p>
</div>
</section>
<section id="S3.SS1.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Learning Approaches</h4>

<div id="S3.SS1.SSSx2.p1" class="ltx_para">
<p id="S3.SS1.SSSx2.p1.1" class="ltx_p">Since 2014 with <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Toshev and Szegedy</span> (<a href="#bib.bib70" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2014</span></a>)</cite> and <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Tompson etÂ al.</span> (<a href="#bib.bib68" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2014</span></a>)</cite> convolutional neural networks were extensively used for human pose estimation. However, new architectures and training reformulations of their different building blocks are presented each year. These variants sometimes contribute to the improvement of the state-of-the-art for 3D pose estimation. Here, we review the most commonly used families of networks for this task.
<br class="ltx_break"></p>
</div>
<div id="S3.SS1.SSSx2.p2" class="ltx_para">
<p id="S3.SS1.SSSx2.p2.1" class="ltx_p">The first thing to consider is the design choices regarding the convolution operations. For 3D pose estimation, many variants are employed according to the data representation that is used or the hypothesis of the authors. The choices range from <span id="S3.SS1.SSSx2.p2.1.1" class="ltx_text ltx_font_bold">2D convolutions</span> on image data to convolutions on spatial-temporal graph representing a subject motion. Most monocular methods commonly use classic convolutions. For video sequences, multi-view or multimodal setups, richer information have inspired different techniques. Volumetric intermediate features computed from multi-view can be fed to <span id="S3.SS1.SSSx2.p2.1.2" class="ltx_text ltx_font_bold">3D convolution</span> networks to refine or estimate the pose. <span id="S3.SS1.SSSx2.p2.1.3" class="ltx_text ltx_font_bold">Temporal convolutions</span> can be used to reason on past and future frames and help better characterization of the pose.
<br class="ltx_break"></p>
</div>
<div id="S3.SS1.SSSx2.p3" class="ltx_para">
<p id="S3.SS1.SSSx2.p3.1" class="ltx_p">Sometimes, the location of the pose and the trajectory in time are encoded by spatial-temporal graph that can be computed with <span id="S3.SS1.SSSx2.p3.1.1" class="ltx_text ltx_font_bold">Graph Neural Network</span> (GNN). A spectral convolution can then be applied in the Fourrier domain using the eigenvalues of the Laplacian graph <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Kipf and Welling</span> (<a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2017</span></a>)</cite>.
<br class="ltx_break"></p>
</div>
<div id="S3.SS1.SSSx2.p4" class="ltx_para">
<p id="S3.SS1.SSSx2.p4.1" class="ltx_p"><span id="S3.SS1.SSSx2.p4.1.1" class="ltx_text ltx_font_bold">Recurrent Neural Networks</span> (RNN) are another way to process pose sequences. More specifically <span id="S3.SS1.SSSx2.p4.1.2" class="ltx_text ltx_font_bold">Long Short-Term Memory</span> (LSTM) architectures have been used with success on 2D joint sequences <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Hossain and Little</span> (<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite> and other modalities <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Trumble etÂ al.</span> (<a href="#bib.bib71" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2017</span></a>)</cite>. This technique showed success in text translation and other tasks to process sequences with long-term dependencies. From one sequence, LSTM can output another one keeping information about previous inputs passed successively (sequence-to-sequence model). LSTMs are distinguished from RNNs by the state of their cells that is updated by different linear operations called â€gates.â€ These operations select and update information that are useful to remember (this website from <a target="_blank" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" title="" class="ltx_ref ltx_href">Christopher</a> <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib55" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">Olah</span> </a></cite> details LSTM functioning).
<br class="ltx_break"></p>
</div>
<div id="S3.SS1.SSSx2.p5" class="ltx_para">
<p id="S3.SS1.SSSx2.p5.1" class="ltx_p"><span id="S3.SS1.SSSx2.p5.1.1" class="ltx_text ltx_font_bold">Attention mechanism</span> aims to focus networks on the most important information for pose estimation in the input data or intermediate features. <cite class="ltx_cite ltx_citemacro_cite">Cai etÂ al. (<a href="#bib.bib6" title="" class="ltx_ref">2019</a>)</cite> use attention to select the frames that contribute the most to the estimation, whereas <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">He etÂ al.</span> (<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite> describe an â€Epipolar Transformerâ€ module taking advantage of multi-view to focus on learning across epipolar lines. Features in the paired image along the epipolar lines corresponding to the joint points are used.
<br class="ltx_break"></p>
</div>
<div id="S3.SS1.SSSx2.p6" class="ltx_para">
<p id="S3.SS1.SSSx2.p6.1" class="ltx_p">Finally, another interesting line of research concerns generative adversarial networks and <span id="S3.SS1.SSSx2.p6.1.1" class="ltx_text ltx_font_bold">adversarial learning</span> (<cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Goodfellow etÂ al.</span> (<a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2014</span></a>)</cite>). It has mainly been used for 3D pose estimation in two ways: unsupervised mapping of 2D to 3D poses <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Kudo etÂ al.</span> (<a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite> and more frequently as a pose validation module (<cite class="ltx_cite ltx_citemacro_cite">Cheng etÂ al. (<a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite>; <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Wandt and Rosenhahn</span> (<a href="#bib.bib73" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite>; <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Kocabas etÂ al.</span> (<a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019a</span></a>)</cite>). In this case, a discriminator network improves pose consistency by being trained to recognize generated poses from the ones directly extracted from ground-truths. An adversarial loss component is then propagated to the generator network which estimates 3D human poses from visual information or 2D poses. Thus, poses that are inconsistent with known configurations are penalized.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Backbone Architectures</h3>

<figure id="S3.F2" class="ltx_figure"><img src="/html/2010.06449/assets/CNN_Architectures_used_for_2D_HPE.png" id="S3.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="367" height="279" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Fig. 2: </span> Backbone architectures used for 2D pose estimation. (a) Residual blocks are the main characteristic of ResNet variants (Resnet101, Resnet50, Resnet152) <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">He etÂ al.</span> (<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2015</span></a>)</cite>. They are also present in more â€human pose estimationâ€ specialized models: (b) Stacked Hourglass networks <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Newell etÂ al.</span> (<a href="#bib.bib54" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2016</span></a>)</cite> and (c) cascaded pyramid networks <cite class="ltx_cite ltx_citemacro_cite">Chen etÂ al. (<a href="#bib.bib9" title="" class="ltx_ref">2018</a>)</cite>. The simple baseline (d) network proposed by <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Xiao etÂ al.</span> (<a href="#bib.bib75" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite> uses transposed convolutions to recover the higher input resolution. (e) Higher resolution network (HRNet) <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Sun etÂ al.</span> (<a href="#bib.bib65" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite> processes high and low resolutions features in parallel within sub-networks that share information.</figcaption>
</figure>
<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Before reviewing state-of-the-art algorithms it is important to consider the backbone architectures that are commonly used for 2D or 3D pose estimation. They are extensively used in â€top-downâ€ approaches before any computation reducing the problem to a mapping of 2D to 3D coordinates. They reason from low-level joint coordinates to infer high-level information about the human skeleton (see Fig. <a href="#S4.F3" title="Fig. 3 â€£ 4.1 Monocular Images â€£ 4 Methods Review â€£ A review of 3D human pose estimation algorithms for markerless motion capture" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). In opposition, â€bottom-upâ€ techniques reason on human body models and extract features from the images after fitting them to the data. These backbone architectures are also employed directly for 3D pose estimation. For example, <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Huang etÂ al.</span> (<a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite> use Hourglass Networks including 3D convolution on volumetric representations. Space is missing to explain the backbones architectures methods in details, but the reader can refer to <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Xiao etÂ al.</span> (<a href="#bib.bib75" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Newell etÂ al.</span> (<a href="#bib.bib54" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2016</span></a>)</cite> or <cite class="ltx_cite ltx_citemacro_cite">Chen etÂ al. (<a href="#bib.bib9" title="" class="ltx_ref">2018</a>)</cite>. In the present paper, we limit ourselves to the general principles of commonly used backbone architectures and provide an overview of their structure and performance.
<br class="ltx_break"></p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Two commonly used architectures are hourglass networks <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Newell etÂ al.</span> (<a href="#bib.bib54" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2016</span></a>)</cite> and cascaded pyramid networks <cite class="ltx_cite ltx_citemacro_cite">Chen etÂ al. (<a href="#bib.bib9" title="" class="ltx_ref">2018</a>)</cite> (Fig. <a href="#S3.F2" title="Fig. 2 â€£ 3.2 Backbone Architectures â€£ 3 Architectures for Human Pose Estimation â€£ A review of 3D human pose estimation algorithms for markerless motion capture" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, (b) and (c)). They both compute image features at different resolution level with the key idea to encompass global and local discriminant features. Hourglass networks are composed of stacked hourglass modulesâ€‰: the first part of the module is reducing the resolution as it passes through convolutional layers. The second part up-samples the features while summing them with corresponding ones of the same dimension from the previous stage. Intermediate supervision is conducted at the end of each module. Cascaded pyramid networks are a two-step architecture predicting poses from a feature pyramid network. It then refine the results for hard-to-predict points. The pyramid network fuses the features at different resolutions to produce joint position heat maps. In the second stage, the refinement process is using intermediate features from the pyramid at different levels. They are up-sampled and summed before going through a final convolutional block. This process is done only for â€difficult to predict key pointsâ€ (chosen with the loss from the pyramid network). These two architectures are using residual connection modules <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">He etÂ al.</span> (<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2015</span></a>)</cite> as â€building blocksâ€. Based on skip connection between convolutional layers, this technique is widely used in many computer vision tasks for feature extraction. Some 2D pose estimation systems are directly using one of the original variants of this network (ie: Resnet50, Resnet101 etc).
<br class="ltx_break"></p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">High resolution networks <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Sun etÂ al.</span> (<a href="#bib.bib65" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite> are considered as another architecture exploiting different resolutions of the same image. Here, it processes resolutions in parallel with convolution layers sharing weights through â€exchange blocks.â€
<br class="ltx_break"></p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">The efficient 2D multi-person detector from <cite class="ltx_cite ltx_citemacro_cite">Cao etÂ al. (<a href="#bib.bib7" title="" class="ltx_ref">2017</a>)</cite> is sometime used. It uses â€part affinity fieldsâ€ which is â€a non-parametric representation of relationship between body partsâ€. From these features and joint localization confidence maps, human poses are predicted with correct associations to multiple subjects. Another asset of this method is that it runs in real-time.
<br class="ltx_break"></p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p">Finally, <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Xiao etÂ al.</span> (<a href="#bib.bib75" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite> propose a baseline method also using Resnet as its base. This technique is using deconvolution layers to produce heat maps from deep image features, without a specific procedure for difficult-to-predict points. Despite its simplicity, this model achieves competitive results at an efficient computational cost. It is therefore used for comparison evaluations but also sometimes as a backbone 2D detector.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Methods Review</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">This section describes and compares the top performing methods for vision based 3D markerless pose estimation (see Tables <a href="#S4.T3" title="Table 3 â€£ 4.1 Monocular Images â€£ 4 Methods Review â€£ A review of 3D human pose estimation algorithms for markerless motion capture" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, <a href="#S4.T4" title="Table 4 â€£ 4.2 Temporal and Monocular Images â€£ 4 Methods Review â€£ A review of 3D human pose estimation algorithms for markerless motion capture" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and <a href="#S4.T5" title="Table 5 â€£ 4.3 Multi-view â€£ 4 Methods Review â€£ A review of 3D human pose estimation algorithms for markerless motion capture" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>). The selection process was as the followsâ€‰:
<br class="ltx_break"></p>
</div>
<div id="S4.p2" class="ltx_para">
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">Top methods from the state-of-the-art on each most popular benchmark</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">Most cited methods in computer vision and fields of application for 3D human pose estimation (biomechanics, robotics, sport sciences, human-machine interaction etc.)</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">Recent methods reporting interesting results or original approaches that can further advance research</p>
</div>
</li>
</ul>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">The first observation regarding the evolution of accuracy in the state-of-the-art is that it is rapidly improving. This task is getting attention and, every year new, records are reached. The average error dropped from about 100mm to less than 20mm within 10 years. It is yet to be determined whether accuracy is still going to increase in the future. However, given the diversity of approaches and modalities, it can be argued that, to date, there is no consensus on the best method to use.
<br class="ltx_break"></p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">In this review, we focus on the best performing markerless techniques currently available, regardless of the sensors or algorithms they use. Methods using monocular images and video sequences are first presented. Second, we present methods using multiple views. Finally, we include methods using other modalities as a pre-processing step, or during the prediction, specifically inertial measurement units. An overall analysis of the performance indices of the different families of methods will be conducted in the following sections.
<br class="ltx_break"></p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Monocular Images</h3>

<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Method</th>
<th id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Human3.6M</th>
<th id="S4.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">MPI-INF-3DHP</th>
<th id="S4.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">HumanEva</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.2.1" class="ltx_tr">
<th id="S4.T3.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Pavlakos etÂ al.</span> (<a href="#bib.bib57" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2017</span></a>)</cite></th>
<td id="S4.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">71.90</td>
<td id="S4.T3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">-</td>
<td id="S4.T3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T3.1.2.1.4.1" class="ltx_text ltx_font_bold">24.3</span></td>
</tr>
<tr id="S4.T3.1.3.2" class="ltx_tr">
<th id="S4.T3.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Mehta etÂ al.</span> (<a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2017</span></a>)</cite></th>
<td id="S4.T3.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">80.5*</td>
<td id="S4.T3.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">76.6</td>
<td id="S4.T3.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
</tr>
<tr id="S4.T3.1.4.3" class="ltx_tr">
<th id="S4.T3.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Zhou etÂ al.</span> (<a href="#bib.bib81" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2017</span></a>)</cite></th>
<td id="S4.T3.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">64.9</td>
<td id="S4.T3.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r">69.2</td>
<td id="S4.T3.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
</tr>
<tr id="S4.T3.1.5.4" class="ltx_tr">
<th id="S4.T3.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Martinez etÂ al.</span> (<a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2017</span></a>)</cite></th>
<td id="S4.T3.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r">62.9/47.7*</td>
<td id="S4.T3.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T3.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r">24.6</td>
</tr>
<tr id="S4.T3.1.6.5" class="ltx_tr">
<th id="S4.T3.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Sun etÂ al.</span> (<a href="#bib.bib67" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite></th>
<td id="S4.T3.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r">64.1/49.6+/40.6*+</td>
<td id="S4.T3.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T3.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
</tr>
<tr id="S4.T3.1.7.6" class="ltx_tr">
<th id="S4.T3.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Omran etÂ al.</span> (<a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite></th>
<td id="S4.T3.1.7.6.2" class="ltx_td ltx_align_center ltx_border_r">59.9*</td>
<td id="S4.T3.1.7.6.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T3.1.7.6.4" class="ltx_td ltx_align_center ltx_border_r">64.0*</td>
</tr>
<tr id="S4.T3.1.8.7" class="ltx_tr">
<th id="S4.T3.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Mehta etÂ al.</span> (<a href="#bib.bib49" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite></th>
<td id="S4.T3.1.8.7.2" class="ltx_td ltx_align_center ltx_border_r">69.9</td>
<td id="S4.T3.1.8.7.3" class="ltx_td ltx_align_center ltx_border_r">74.1</td>
<td id="S4.T3.1.8.7.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
</tr>
<tr id="S4.T3.1.9.8" class="ltx_tr">
<th id="S4.T3.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Kolotouros etÂ al.</span> (<a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite></th>
<td id="S4.T3.1.9.8.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.1.9.8.2.1" class="ltx_text ltx_font_bold">41.1</span></td>
<td id="S4.T3.1.9.8.3" class="ltx_td ltx_align_center ltx_border_r">76.4</td>
<td id="S4.T3.1.9.8.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
</tr>
<tr id="S4.T3.1.10.9" class="ltx_tr">
<th id="S4.T3.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Wandt and Rosenhahn</span> (<a href="#bib.bib73" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite></th>
<td id="S4.T3.1.10.9.2" class="ltx_td ltx_align_center ltx_border_r">50.9 / 38.2*</td>
<td id="S4.T3.1.10.9.3" class="ltx_td ltx_align_center ltx_border_r">82.5</td>
<td id="S4.T3.1.10.9.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
</tr>
<tr id="S4.T3.1.11.10" class="ltx_tr">
<th id="S4.T3.1.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Xu etÂ al.</span> (<a href="#bib.bib76" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite></th>
<td id="S4.T3.1.11.10.2" class="ltx_td ltx_align_center ltx_border_r">82.4 / 53.9* /48.0*+</td>
<td id="S4.T3.1.11.10.3" class="ltx_td ltx_align_center ltx_border_r">73.1 / 76.9+ / 89.0*+</td>
<td id="S4.T3.1.11.10.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
</tr>
<tr id="S4.T3.1.12.11" class="ltx_tr">
<th id="S4.T3.1.12.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Kocabas etÂ al.</span> (<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019b</span></a>)</cite></th>
<td id="S4.T3.1.12.11.2" class="ltx_td ltx_align_center ltx_border_r">51.83+/45.04*+</td>
<td id="S4.T3.1.12.11.3" class="ltx_td ltx_align_center ltx_border_r">77.5</td>
<td id="S4.T3.1.12.11.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
</tr>
<tr id="S4.T3.1.13.12" class="ltx_tr">
<th id="S4.T3.1.13.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">
<cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Mathis etÂ al.</span> (<a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite> (DeepLabCut)</th>
<td id="S4.T3.1.13.12.2" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T3.1.13.12.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T3.1.13.12.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
</tr>
<tr id="S4.T3.1.14.13" class="ltx_tr">
<th id="S4.T3.1.14.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Mehta etÂ al.</span> (<a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite></th>
<td id="S4.T3.1.14.13.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">63.6</td>
<td id="S4.T3.1.14.13.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T3.1.14.13.3.1" class="ltx_text ltx_font_bold">82.8</span></td>
<td id="S4.T3.1.14.13.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">-</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Accuracy comparison from several state-of-the-art monocular methods. Human3.6M and HumanEva results reported in absolute MPJPE (lower is better); Results from MPI-INF-3DHP are reported in 3DPCK (higher is better). Techniques with the annotation with + are using extra-training data to obtain the result; the others use the benchmarkâ€™s recommended protocols. The * annotation indicates results published with procrustes alignment to ground truth poses before evaluation.</figcaption>
</figure>
<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Pavlakos etÂ al.</span> (<a href="#bib.bib57" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2017</span></a>)</cite> present one of the first methods to propose a one-stage end-to-end convolutional neural network to predict 3D human pose from a single RGB image. They do so by focusing on the 3D nature of the task. Their architecture uses stacked hourglass modules <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Newell etÂ al.</span> (<a href="#bib.bib54" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2016</span></a>)</cite> (see 2D architectures <a href="#S3.F2" title="Fig. 2 â€£ 3.2 Backbone Architectures â€£ 3 Architectures for Human Pose Estimation â€£ A review of 3D human pose estimation algorithms for markerless motion capture" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) that outputs volumes of voxel probability for each joint in a 3D discretized space around the target. They also propose a new intermediate supervision method inspired from success in 2D human pose estimation. This original method does not use a 2D joint estimation step. Instead, they employ a coarse-to-fine approach that leverage 2D heatmaps as ground truths for intermediate supervision, and then fuse them with image features as an output of the next modules. Further down, the network reconstructs 3D voxel maps. The supervision is also done using 3D Gaussian around the given 3D coordinates ground truths. This deep network provides accurate results (72mm) for a monocular method using purely 3D data representation. However, more recent methods showing better results use two-stages top-down architectures including 2D prediction as a first step for 3D detection. This suggests that image features are not rich enough for direct 3D inferences.
<br class="ltx_break"></p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">VNect from <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Mehta etÂ al.</span> (<a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2017</span></a>)</cite> is a framework for 3D root-relative human pose estimation in real-time. It consists of a similar to Resnet50 architecture that generates 2D heat maps for each joint as well as newly introduced â€location maps.â€ These location maps predict the relative X, Y, and Z positions of an articulation relative to their root joint. For each joint, the location is processed from the peek of the 2D heat maps and the root-relative coordinate is read in the location maps. Then, a kinematic model of the human skeleton is fitted to the predicted poses, to improve temporal consistency and reduce jitter. The strengths of this method are that it works in real-time and can be used in different outdoor contexts. However, the authors list several limitations to their approach, mainly that depth estimation errors might lead to erroneous 3D predictions. The method is also only capable of relative pose estimation and therefore requires accurate detection of a root joint (pelvis).
<br class="ltx_break"></p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Mehta etÂ al.</span> (<a href="#bib.bib49" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite> following Vnect, this technique also uses location maps but modify them to become â€Occlusion-Robust Pose-Mapsâ€ (ORPM) that infers 3D pose from multiple subjects. These ORPMs are similar to location maps but also contain structural information about the pose. For each joint, the location in the 3D maps is stored at the position of the joint but also along a predefined set of joints (dividing the body into two arms and legs) and root joints (pelvis and neck). They also employ part affinity fields <cite class="ltx_cite ltx_citemacro_cite">Cao etÂ al. (<a href="#bib.bib7" title="" class="ltx_ref">2017</a>)</cite>, which are often used in 2D multi-person pose estimation. They represent â€2D vector fields pointing from a joint [â€¦] to its parents.â€ Thus, starting from valid root joints, all joints can be inferred following the kinematic chain of the human body. The outstanding feature of this method is that it perform multi-person detection and in a single-shot manner without intermediate use of off-the-shelf 2D pose estimators or person detector. It also introduces two new dataset for training and evaluation: MuCo-3DHP (large-scale multi-person with occlusion dataset, composed from MPI-INF-3D images) and MuPoTS-3D (In-the-wild multi-person dataset, filmed in various environments with markerless ground truths).</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Mehta etÂ al.</span> (<a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite> More recently, in continuation of the previous two methods, the Xnect framework was presented. It is athree-stages method that combines a convolutional network for feature extraction, a fully connected network for pose estimation and a fit of the previous results to a kinematic model to refine pose consistency. The first stageextracts 2D and subject association through part affinity fields (similar to <cite class="ltx_cite ltx_citemacro_cite">Cao etÂ al. (<a href="#bib.bib7" title="" class="ltx_ref">2017</a>)</cite>) as well as 3D pose encoding in the same way as the previous methods. The key difference from <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Mehta etÂ al.</span> (<a href="#bib.bib49" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite> is that each joint only encodes information about its position relative to the parent joint and the position of its children. For this stage, a new backbone module is presented for network architectures to reduce computational costs: SelecSLS. It consists of successive 3 by 3 and 1 by 1 convolutions with inter- and intra-module skip connections. It achieves similar accuracy to Resnet50 while being 1.4 faster at inference. The second stage consists of 3D pose detection, for each subject in parallel, through fully connected networks trained on the MuCo-3DHP dataset incorporating examples with severe occlusions. Finally, kinematic skeleton fitting is performed using the minimization of an energy term consisting of position (through inverse kinematics for 3D features and 2D re-projection), orientation and temporal consistency. Clearly, this is a complex framework that involves many steps, as well as re-tracking of each subject before the last stage. However, the framework is robust to occlusions and adapted to multi-person. It is also computationally efficient since it can run in real-time at 30 fps on generic hardware configurations. However, it appears that the last stage of the framework decreases the overall accuracy while performing better on certain joints and producing a smoother orientation estimate.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Zhou etÂ al.</span> (<a href="#bib.bib81" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2017</span></a>)</cite> published a two-stage method using the hourglass network architecture of <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Newell etÂ al.</span> (<a href="#bib.bib54" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2016</span></a>)</cite> for 2D heat map generation, then regress depth for each joint. In addition, they apply a weakly-supervised process to exploit images that have only been labeled with 2D ground truths. The depth prediction is realized from features at different resolutions, which are extracted from several levels of the Hourglass network. The weakly-supervised training is applied using 3D and 2D labeled data. Euclidean loss is applied to predictions on images with 3D ground truths, whereas a geometric loss is used when only 2D labels are available. This lossadds constraints from the average limb length ratios among predefined bone-groups. The main contribution of this work is the weakly-supervised technique: the authors evaluated whether the contribution of 2D data improved results on the 2D portion of the framework or on the whole 3D estimation task. They state that at PCKh@0.5 (standard metric for 2D pose estimation see <a href="#S2.SS1" title="2.1 Metrics â€£ 2 Methods Evaluation â€£ A review of 3D human pose estimation algorithms for markerless motion capture" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>), the results are similar for 2D human pose estimation when 2D data is not used, but depth estimation is greatly improved. However, an analysis with a threshold smaller than 0.5 might be interesting, as a small increase in 2D accuracy is still observed. Nevertheless, their work confirms that using 2D detectors as part of the 3D detection task is possible and yields accurate results (<math id="S4.SS1.p5.1.m1.1" class="ltx_Math" alttext="\simeq" display="inline"><semantics id="S4.SS1.p5.1.m1.1a"><mo id="S4.SS1.p5.1.m1.1.1" xref="S4.SS1.p5.1.m1.1.1.cmml">â‰ƒ</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.1.m1.1b"><csymbol cd="latexml" id="S4.SS1.p5.1.m1.1.1.cmml" xref="S4.SS1.p5.1.m1.1.1">similar-to-or-equals</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.1.m1.1c">\simeq</annotation></semantics></math>65mm MPJPE).
<br class="ltx_break"></p>
</div>
<div id="S4.SS1.p6" class="ltx_para">
<p id="S4.SS1.p6.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Martinez etÂ al.</span> (<a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2017</span></a>)</cite> presented simple baseline for 3D human pose estimation. This method differs from others in that it does not use image data or intermediate feature maps (ie. joint location heat maps) nor does it use optimization steps with model fitting. Instead, it infers 3D coordinates from 2D coordinates obtained with a state-of-the-art 2D human pose estimation architecture. Despite this simple design, it produces accurate results comparable to and sometimes better than some more complex contemporary techniques. The design choices for the model are the following: a simple 2-layer CNN with batch normalization, ReLU activation, and dropout. It takes as input the 2D predictions of the Hourglass Network <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Newell etÂ al.</span> (<a href="#bib.bib54" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2016</span></a>)</cite>. The results obtained on Human3.6M reach 62mm average error on single images. This method is also one of the first to propose the direct lifting of 2D coordinates from efficient 2D detectors to 3D. The high accuracy obtained with such a simple approach without image data as an input lead the authors to hypothesize that the visual features used in contemporary methods were either not useful to 3D human pose estimation or still under-exploited. The latter hypothesis tends to be confirmed with new methods achieving increasing accuracy with clever exploitation of temporal features (<cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Hossain and Little</span> (<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite>; <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Pavllo etÂ al.</span> (<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite>; <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Cheng etÂ al.</span> (<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite>; <cite class="ltx_cite ltx_citemacro_cite">Cheng etÂ al. (<a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite>) or combination of direct regression and model fitting.Because it is simple, fast and is driven by the increasing performances of 2D detectors, this method and the two-stage technique inspired many recent studies.
<br class="ltx_break"></p>
</div>
<div id="S4.SS1.p7" class="ltx_para">
<p id="S4.SS1.p7.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Sun etÂ al.</span> (<a href="#bib.bib67" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite> introduced â€integral regressionâ€ to extract 3D coordinates from 2D confidence heat maps. It is a function similar to the soft-argmax function commonly used in classification to normalize outputs. Here, it is used to switch from 2D pixel maps to differentiable coordinates, allowing direct regression within an end-to-end network. Their article describes extensive ablation studies on different training architectures losses and backbone (hourglass and residual networks <a href="#S3.F2" title="Fig. 2 â€£ 3.2 Backbone Architectures â€£ 3 Architectures for Human Pose Estimation â€£ A review of 3D human pose estimation algorithms for markerless motion capture" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) and presents good results for both 2D and 3D human pose estimation. The authors also adopt a training strategy <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Sun etÂ al.</span> (<a href="#bib.bib66" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2017</span></a>)</cite> allowing the usage of 2D labeled data with separate supervision for the xy coordinates and for the depth, achieving even higher accuracy on Human3.6M. Many approaches are now using 2D heat maps with the soft-argmax regression.</p>
</div>
<div id="S4.SS1.p8" class="ltx_para">
<p id="S4.SS1.p8.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Omran etÂ al.</span> (<a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite> present the Neural Body Fitting approach that combines part segmentation with a convolutional neural network and a parameterized human body model (SMPL <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Loper etÂ al.</span> (<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2015</span></a>)</cite>). The first stage predicts a part segmentation map using the RefineNet <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Lin etÂ al.</span> (<a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2016</span></a>)</cite> CNN architecture. In the second stage, these part masks are directly fed to a ResNet-50 network that estimates the pose and shape parameter of the SMPL model. The whole process is fully differentiable and can also be trained with 2D data in a weakly supervised manner. The authors demonstrate this by re-projecting the joint coordinate on 2D images and find that, with only 20% of the training data with 3D ground truths, the same accuracy is achieved as with complete annotations. This method is one of the first that describes the training of CNNs followed with the fitting to a parameterized human body model in a single integrated framework. The other characteristic that differentiates this method from others is that it uses the intermediate feature of the part segmentation map. According to the author analysis, using a 24-part segmentation as an input led to significantly better results than direct image data or joint coordinates. This result is particularly interesting and should be considered when designing architectures that want to reason on the 3D structure of the human body.</p>
</div>
<div id="S4.SS1.p9" class="ltx_para">
<p id="S4.SS1.p9.1" class="ltx_p">Similarly, <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Kolotouros etÂ al.</span> (<a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite> designed a system with joint usage of a CNN for direct key point regression and an iterative model optimization technique using a human volumetric model (SMPL <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Loper etÂ al.</span> (<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2015</span></a>)</cite>). The neural network produces good initialization for the iterative fitting of the human model. Then, once the shape model position is refined, it is used to calculate a loss from the initial prediction, which increases the accuracy of the network. The originality of the method lies in using the best of the two techniques. Direct regression allows fast initialization without a priori knowledge directly on the image data; iterative optimization from a human model produces a better shape for the image fitting. The two parts of the system complement and improve each other during each training cycle. The results obtained using this method on Human3.6M are the most accurate of monocular non temporal methods (on a single isolated RGB image). The average error is 41.1mm, which is close to the accuracy of temporal methods. These results show that with simple input data and a suitable method, high accuracy can be achieved. The question is: Could this direct regression/model fitting method using only visual features be suplemented with temporal data to reach a higher accuracy score?</p>
</div>
<div id="S4.SS1.p10" class="ltx_para">
<p id="S4.SS1.p10.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Wandt and Rosenhahn</span> (<a href="#bib.bib73" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite> uses the kinematic chain space to represent human 3D pose within their discriminator (or â€criticâ€) network. To project pose coordinates in the kinematic chain space, limbs (i.e., edges between detected joints) are described as directional vectors. It is then possible to map this representation back to point coordinates. The kinematic chain representation contains information about limb length, angles and body symmetry, while being easy to compute. Later, <cite class="ltx_cite ltx_citemacro_cite">Cheng etÂ al. (<a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite> used a temporal version of the kinematic chain space that is reporting the angle and length modification across frames in a video.
<br class="ltx_break"></p>
</div>
<div id="S4.SS1.p11" class="ltx_para">
<p id="S4.SS1.p11.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Xu etÂ al.</span> (<a href="#bib.bib76" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite> proposed the DenseRaC framework that converts pixel-to-surface maps of the human body (IUV) into parameters for statistical human body model. Similarly to <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Omran etÂ al.</span> (<a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite>, this framework uses intermediate features but, instead of part segmentation, DenseRaC uses pixel-to-3D surface maps. In the first stage of the pipeline, these maps are computed using the Densepose-RCNN architecture (a network that is trained on Densepose-COCO, a dataset of manually annotated human body surface <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">GÃ¼ler etÂ al.</span> (<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite>). To improve training, the authors present a large-scale dataset of synthetic human poses that can also easily produce IUVs. In the second stage, the IUVs are fed to a regression network that estimates the human body shape and pose parameters ( similar to <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Loper etÂ al.</span> (<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2015</span></a>)</cite>) as well as the camera parameters. Once reconstructed, the 3D mesh is re-projected using a differentiable renderer and rasterized in an IUV similar to those produced in the first step. Then, the computation of an adversarial loss with a discriminator network helps to eliminate impossible configurations.</p>
</div>
<div id="S4.SS1.p12" class="ltx_para">
<p id="S4.SS1.p12.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Kocabas etÂ al.</span> (<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019b</span></a>)</cite> takes advantage of the multi-view setting that each major motion capture dataset provides. the authors propose to infer geometry from matching 2D detection in each view and then deduce the 3D coordinates. This technique renders self-supervision possible with completely unlabeled data. Unlike many multi-view techniques (<cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Iskakov etÂ al.</span> (<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite>; <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">He etÂ al.</span> (<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite>) this method does not use camera parameters. Itâ€™s important to note that the training part of the network uses a multi-view setting, but during prediction it becomes a monocular method. The architecture is composed of two networks using ResNet50 and a deconvolution layer as their backbone (see 2D architectures <a href="#S3.F2" title="Fig. 2 â€£ 3.2 Backbone Architectures â€£ 3 Architectures for Human Pose Estimation â€£ A review of 3D human pose estimation algorithms for markerless motion capture" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). Both produce spatial heat maps for each joint in each view. The difference is that, in one case, they are converted in 2D coordinates and in 3D in the other (both with a soft argmax). The 3D network is the one that will be used for inferences, and that has learnable weights. The 2D network is frozen and will be used for the self-supervision.Although the cameras are not calibrated, the authors advise using detected the joint key points in each view to obtain the camera parameters (using RANSAC and SVD). Then, triangulation can be performed to obtain the 3D coordinates, which are then used to supervise the 3D network with a smooth absolute loss. The accuracy score of the method is the current best when few or no labeled data are available (70mm average MPJPE on H3.6M and 64.7 3DPCK on MPI-INF-3DHP). This learning scheme is promising and allows for high accuracy by requiring only raw data (discounting pre-trained 2D detector), provided that multi-view images are accessible.
<br class="ltx_break"></p>
</div>
<div id="S4.SS1.p13" class="ltx_para">
<p id="S4.SS1.p13.1" class="ltx_p">Finally, it is important to consider the methods being used in several research fields. The main one, DeepLabCut <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Mathis etÂ al.</span> (<a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Mathis and Mathis</span> (<a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Nath etÂ al.</span> (<a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite> is a generic markerless keypoint-tracking framework developed with the goal to obtain accuracy results similar to human annotation. Its original application target was the video tracking of predefined keypoints for different species. DeepLabCut achieves good results with little training data, which has made it popular and it is now cited in numerous articles in neuroscience and human movement research. For human gait analysis <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Moro etÂ al.</span> (<a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite>FIKER2020108775, its accuracy is similar to marker-based motion capture. Although DeepLabCut is not in the strict sense a HPE model, it can be used as one. In fact, it is based on an HPE architecture: It consists of the first layers of the DeeperCut network DBLP:journals/corr/InsafutdinovPAA16, which is a multi-person 2D pose estimation model used here for feature extraction, followed by a deconvolution layer. This model and its inspiration do not directly fall within the scope of 3D human pose estimation. Yet, DeepLabCut has been used in a multi-view calibrated cameras context with simple triangulation (<cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Nath etÂ al.</span> (<a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite>; <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Sheshadri etÂ al.</span> (<a href="#bib.bib63" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite>). As this method seems popular in different fields for its flexibility and performance in a wide variety of contexts, it is important to note that more recent results on specific 2D keypoint detection have outperformed DeeperCut <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Newell etÂ al.</span> (<a href="#bib.bib54" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2016</span></a>)</cite>. The main appeal for this method is the definition of personalized labels and its powerful generalization capability.
<br class="ltx_break"></p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2010.06449/assets/3D_Markerless_HPE_Systems.png" id="S4.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="371" height="349" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Fig. 3: </span>Overview of the different levels of 3D markerless human pose estimation. <span id="S4.F3.6.1" class="ltx_text ltx_font_bold">A</span>: Monocular approaches, commonly used 2D pose estimation backbone architectures are described in <a href="#S3.F2" title="Fig. 2 â€£ 3.2 Backbone Architectures â€£ 3 Architectures for Human Pose Estimation â€£ A review of 3D human pose estimation algorithms for markerless motion capture" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. <span id="S4.F3.7.2" class="ltx_text ltx_font_bold">B</span>: 3D feature exploitation and multi-view 2D detection as an input for 3D detectors. <span id="S4.F3.8.3" class="ltx_text ltx_font_bold">C</span>: The different families of 3D pose estimation. <span id="S4.F3.9.4" class="ltx_text ltx_font_bold">D</span>: examples of learning approaches applied to human pose estimation.</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Temporal and Monocular Images</h3>

<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.1.1.1" class="ltx_tr">
<th id="S4.T4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Method</th>
<th id="S4.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Human3.6M</th>
<th id="S4.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">MPI-INF-3DHP</th>
<th id="S4.T4.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">HumanEva</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.1.2.1" class="ltx_tr">
<th id="S4.T4.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Hossain and Little</span> (<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite></th>
<td id="S4.T4.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">58.5</td>
<td id="S4.T4.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">-</td>
<td id="S4.T4.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">22.0</td>
</tr>
<tr id="S4.T4.1.3.2" class="ltx_tr">
<th id="S4.T4.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite">Cai etÂ al. (<a href="#bib.bib6" title="" class="ltx_ref">2019</a>)</cite></th>
<td id="S4.T4.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">48.8 / 39.0*</td>
<td id="S4.T4.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T4.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
</tr>
<tr id="S4.T4.1.4.3" class="ltx_tr">
<th id="S4.T4.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Pavllo etÂ al.</span> (<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite></th>
<td id="S4.T4.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">46.8 / 36.5*</td>
<td id="S4.T4.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T4.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r">23.1/15.8*</td>
</tr>
<tr id="S4.T4.1.5.4" class="ltx_tr">
<th id="S4.T4.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Cheng etÂ al.</span> (<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite></th>
<td id="S4.T4.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r">42.9/32.8*</td>
<td id="S4.T4.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T4.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r">14.3*</td>
</tr>
<tr id="S4.T4.1.6.5" class="ltx_tr">
<th id="S4.T4.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite">Cheng etÂ al. (<a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite></th>
<td id="S4.T4.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T4.1.6.5.2.1" class="ltx_text ltx_font_bold">40.1</span></td>
<td id="S4.T4.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T4.1.6.5.3.1" class="ltx_text ltx_font_bold">84.1</span></td>
<td id="S4.T4.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T4.1.6.5.4.1" class="ltx_text ltx_font_bold">13.5*</span></td>
</tr>
<tr id="S4.T4.1.7.6" class="ltx_tr">
<th id="S4.T4.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Liu etÂ al.</span> (<a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite></th>
<td id="S4.T4.1.7.6.2" class="ltx_td ltx_align_center ltx_border_r">45.1 / 35.6*</td>
<td id="S4.T4.1.7.6.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T4.1.7.6.4" class="ltx_td ltx_align_center ltx_border_r">15.4*</td>
</tr>
<tr id="S4.T4.1.8.7" class="ltx_tr">
<th id="S4.T4.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Wang etÂ al.</span> (<a href="#bib.bib74" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite></th>
<td id="S4.T4.1.8.7.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">42.6 / 32.7*</td>
<td id="S4.T4.1.8.7.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">86.9(2d GT)</td>
<td id="S4.T4.1.8.7.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">-</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Accuracy comparison from several state-of-the-art monocular temporal methods. Human3.6M and HumanEva results reported in absolute MPJPE (lower is better); Results from MPI-INF-3DHP are reported in 3DPCK (higher is better). Techniques with the annotation with + are using extra-training data to obtain the result; the others use the benchmarkâ€™s recommended protocols. The * annotation indicates results published with procrustes alignment to ground truth poses before evaluation. </figcaption>
</figure>
<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Hossain and Little</span> (<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite> used the sequence-to-sequence architecture that was initially applied in machine translation tasks for human pose estimation. The main idea is to use long-term temporal context to predict a new sequence, as for the translation of text into another language. Here, 2D coordinate sequences (predicted with the Hourglass Network architecture) are used to predict the 3D ones using Long Short-Term Memory units (LSTM). The first layer of the network encodes the sequences into a hidden space and the last layers decodes them into a 3D dimensional sequence using residual connections. Additionally, important choices are made for the training. First, the first derivative of the 3D coordinates is included in the loss to mitigate the effect of errors during the 2D prediction stage by enforcing temporal consistency. Second, the training combines layer normalization, recurrent dropout and finally the weighting of joint sets, to force the network to better predict challenging parts. To date, the method is one of the most accurate among methods using temporal data. Notably, an evaluation by the authors shows that the optimal sequence length for their model is 5 frames. Beyond this point, the accuracy slowly decreases, suggesting that either the long-term temporal context does not provide good information or the model does not properly exploit past image data.One problem with this architecture is the fixed length of the temporal data input, which was later solved using temporal convolutional networks (TCNs) <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Pavllo etÂ al.</span> (<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Cheng etÂ al. (<a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite>. This paper further details and analyzes other useful techniques applied to temporal pose data, such as the important contribution of residual connections.
<br class="ltx_break"></p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Cai etÂ al. (<a href="#bib.bib6" title="" class="ltx_ref">2019</a>)</cite> also uses temporal dependency within the pose sequence that composes the subject motion. However, in addition to time constraints, joint position constraints are also enforced using a graph structure. Edges are present to model spatial continuity and symmetry, but also connections with the same joint in future and past frames of the video. These graphs are computed from 2D skeletons that are then passed to a graph convolutional network. Depending on the type of neighboring (various spatial constraints and temporal connections), the authors propose a different type of convolution. Then, a local-to-global architecture is used similarly to <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Newell etÂ al.</span> (<a href="#bib.bib54" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2016</span></a>)</cite> to obtain 3D predictions. The graph structure is pooled and upsampled using lower level features from the previous layers in a hierarchical manner. Then, the method produces a 3D pose from a pose refinement module consisting of a fully connected layer. The authors employ a loss strategy combining different terms to enforce joint locations, limb symmetry and temporal smoothness. The results obtained on different benchmarks show a better accuracy than methods based purely on temporal sequences. However, the method is likely not suitable for sequences shorter than 7 frames on most actions. It is also important to note that the effect of the chosen 2D detector is not tested.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">As mentioned previously <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Pavllo etÂ al.</span> (<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite>, the technique consists of a monocular method that leverages a fully convolutional network to process temporal data. The main advantage is that, compared to RNN and LSTM, they are more computationally efficient and do not need a fixed input size.In this paper, the authors present an architecture based on time-dilated convolutions that also apply semi-supervision to add non-labeled data to the training. The dilated convolutions capture the long-term dependencies, but also need less training parameters and have a better computation speed than sequence-to-sequence models <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Hossain and Little</span> (<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite>. The authors present a comparison showing that about the same number of parameters and FLOPs are needed to yield better accuracy. The semi-supervised process they used is called â€back-projectionâ€: an encoder-decoder architecture encodes a 2D joint prediction into 3D pose and decodes by projecting it back in 2D. The error is then computed between the original and back-projected 2D predictions.This method has a 10% increase in accuracy on two of the main benchmarks, leading to an average error of less than 50mm for monocular methods.The methods that follow this paper, based on video sequences, still use the Temporal Convolutional Networks (TCN) architecture (<cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Cheng etÂ al.</span> (<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Cheng etÂ al. (<a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Liu etÂ al.</span> (<a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite>). Another interesting result is the impact of the 2D detector used. The tests conducted show that the CPN and the Mask-RCNN 2D detectors give better results in the end with their model. The authors think that â€itâ€™s due to the higher heatmap resolution, stronger feature combinationâ€ from these detectors.
<br class="ltx_break"></p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Cheng etÂ al.</span> (<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite> and <cite class="ltx_cite ltx_citemacro_cite">Cheng etÂ al. (<a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite> also use temporal convolutional networks with a specific occlusion training to improve prediction on challenging images.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Cheng etÂ al.</span> (<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite>, they describe an occlusion-aware network where a â€cylinder man modelâ€ is produces occlusion labels. Their architecture is composed of an end-to-end trainable human detector, a 2D pose estimator, a 3D pose estimator and finally a pose discriminator. The two pose estimation algorithms are respectively a 2D and a 3D temporal convolutional networks. They also use 2D-only data during training using a re-projection loss such as in <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Pavllo etÂ al.</span> (<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite>. The occlusion model intervenes at this stage where self-occluded points (computed with the cylinder model) are not taken into account as they are deemed unreliable.
<br class="ltx_break"></p>
</div>
<div id="S4.SS2.p6" class="ltx_para">
<p id="S4.SS2.p6.1" class="ltx_p">Building on the previous work, <cite class="ltx_cite ltx_citemacro_cite">Cheng etÂ al. (<a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite> describe an end-to-end trainable model with several modules that reconstruct 3D joints from monocular videos. 2D confidence heat maps are estimated and used as a feature for 3D prediction. They use a multi-scale convolutional network (HRNet) that fuses spatial features <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Sun etÂ al.</span> (<a href="#bib.bib65" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite>. Thus, the main characteristics of their method are the followingâ€‰:
<br class="ltx_break"></p>
</div>
<div id="S4.SS2.p7" class="ltx_para">
<ul id="S4.I2" class="ltx_itemize">
<li id="S4.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I2.i1.p1" class="ltx_para">
<p id="S4.I2.i1.p1.1" class="ltx_p">Learning of a multi-scale embedding obtained from those heat maps. 3D poses are then predicted with the embedding using a temporal convolutional network (TCN) <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Pavllo etÂ al.</span> (<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite>.</p>
</div>
</li>
<li id="S4.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I2.i2.p1" class="ltx_para">
<p id="S4.I2.i2.p1.1" class="ltx_p">Validation of the pose sequences with a discriminant model based on Spatio-Temporal Kinematic Chains (which enforces limbs angular and length constraints).</p>
</div>
</li>
<li id="S4.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S4.I2.i3.p1" class="ltx_para">
<p id="S4.I2.i3.p1.1" class="ltx_p">Data augmentation using synthetic occlusion at different levels during TCN training.</p>
</div>
</li>
</ul>
</div>
<div id="S4.SS2.p8" class="ltx_para">
<p id="S4.SS2.p8.1" class="ltx_p">Semi-supervised learning is used with the goal of including strictly 2D labeled data during the training process as in <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Pavllo etÂ al.</span> (<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite>. Finally, and only for the training stage, multi-view from Human3.6M dataset is used to enforce a good skeleton orientation prediction. To do so, the authors use a loss function comparing each inference from two pairs of different views at the same time in the video (after applying a camera rotation known from calibration data). Among the methods reviewed here, this method achieves the overall best results on HumanEVA and MPI-INF-3DHP. It also has the best results for a monocular method on Human3.6M (40mm MPJPE). However, it is a complex method with many submodules and parameters reducing errors from occlusion. It also exploits and expands the pose discriminator based on the kinematic chain space from <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Wandt and Rosenhahn</span> (<a href="#bib.bib73" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite>, using it with temporal data. The authors address the contribution of each module to the final accuracy. However, they do so by adding the modules one at a time to the backbone, which does not provide any insight into whether one module improves or compensates for the errors or performance of another. Further cross-comparison could help determine which module has a greater impact and on which data or context.
<br class="ltx_break"></p>
</div>
<div id="S4.SS2.p9" class="ltx_para">
<p id="S4.SS2.p9.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Liu etÂ al.</span> (<a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite> Added an attention mechanism to the temporal approach for extracting 3D pose from video. Similarly to <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Pavllo etÂ al.</span> (<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite>, temporal dilated convolution extracts information in the 2D poses sequence. The added attention mechanism selects the frames and tensor outputs that are the most useful for the detection. The temporal attention modules are computed from the distribution of the tensors at each time step. The kernel attention modules are computed from the distribution of the channel outputs of each layer. Both attention modules are propagated within an attention matrix to the following layer. On top of this architecture, multi-scale dilation convolutions (see dilation convolution in the previous described method <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Pavllo etÂ al.</span> (<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite>) with increasing receptive fields are employed to reduce the vanishing gradient issues. This method shows progress over the state-of-the-art methods. An ablation study also shows how attention modules associated with the multi-scale convolution strategy lead to better results, especially on difficult frames (fast motions or partially occluded subjects).
<br class="ltx_break"></p>
</div>
<div id="S4.SS2.p10" class="ltx_para">
<p id="S4.SS2.p10.1" class="ltx_p">The two main contributions of <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Wang etÂ al.</span> (<a href="#bib.bib74" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite> are: <span id="S4.SS2.p10.1.1" class="ltx_text ltx_font_italic">Motion loss</span>, a new loss function based on keypoint trajectory and <span id="S4.SS2.p10.1.2" class="ltx_text ltx_font_italic">U-shaped Graph Convolutional Network</span> (UGCN), a network architecture. Motion loss is based on coordinate vectors. To make it differentiable, so it can be used in a learning architecture, any motion sequence needs to be encoded using a differentiable operator. The authors empirically chose the scalar product (among other tested). The final loss is composed of this motion loss computed with â€pairwise motion encodingâ€ and an absolute position reconstruction loss. UGCN uses spatial temporal graph to represent motions <cite class="ltx_cite ltx_citemacro_cite">Cai etÂ al. (<a href="#bib.bib6" title="" class="ltx_ref">2019</a>)</cite>. Then, spatial convolutions are applied on each skeleton for each frame before temporal convolutions are applied to the temporal dimension of each joint in the graph. The network architecture is similar to successful ones in semantic segmentation (e.g. <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Ronneberger etÂ al.</span> (<a href="#bib.bib61" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2015</span></a>)</cite>. It consists in three stages: downsampling, upsampling and merging. This architecture captures features at different scales, which implies that UGCN explores temporal and spatial information at different scales. This architecture was tested adding upsampling and downsampling one by one, showing increasing accuracy. The addition of motion loss also drastically improves the results on two benchmarks (Human3.6M and MPI-INF3DHP), improving on the state-of-the-art results.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Multi-view</h3>

<figure id="S4.T5" class="ltx_table">
<table id="S4.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.1.1.1" class="ltx_tr">
<th id="S4.T5.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Method</th>
<th id="S4.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Human3.6M</th>
<th id="S4.T5.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Total Capture</th>
<th id="S4.T5.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Input</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.1.2.1" class="ltx_tr">
<th id="S4.T5.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Qiu etÂ al.</span> (<a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite></th>
<td id="S4.T5.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">31.17 / 26.21+</td>
<td id="S4.T5.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">29</td>
<td id="S4.T5.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Multi-view</td>
</tr>
<tr id="S4.T5.1.3.2" class="ltx_tr">
<th id="S4.T5.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Iskakov etÂ al.</span> (<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite></th>
<td id="S4.T5.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">
<span id="S4.T5.1.3.2.2.1" class="ltx_text ltx_font_bold">17.7+</span>/20.80*+</td>
<td id="S4.T5.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T5.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r">Multi-view</td>
</tr>
<tr id="S4.T5.1.4.3" class="ltx_tr">
<th id="S4.T5.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">He etÂ al.</span> (<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite></th>
<td id="S4.T5.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">26.9/19.0+</td>
<td id="S4.T5.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T5.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r">Multi-view</td>
</tr>
<tr id="S4.T5.1.5.4" class="ltx_tr">
<th id="S4.T5.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">von Marcard etÂ al.</span> (<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2016</span></a>)</cite></th>
<td id="S4.T5.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T5.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T5.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r">Multi-view, IMU</td>
</tr>
<tr id="S4.T5.1.6.5" class="ltx_tr">
<th id="S4.T5.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Trumble etÂ al.</span> (<a href="#bib.bib71" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2017</span></a>)</cite></th>
<td id="S4.T5.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r">87.3</td>
<td id="S4.T5.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r">77.0</td>
<td id="S4.T5.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r">Multi-view, Temporal, IMU</td>
</tr>
<tr id="S4.T5.1.7.6" class="ltx_tr">
<th id="S4.T5.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">von Marcard etÂ al.</span> (<a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite></th>
<td id="S4.T5.1.7.6.2" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T5.1.7.6.3" class="ltx_td ltx_align_center ltx_border_r">26.0</td>
<td id="S4.T5.1.7.6.4" class="ltx_td ltx_align_center ltx_border_r">Monocular, IMU</td>
</tr>
<tr id="S4.T5.1.8.7" class="ltx_tr">
<th id="S4.T5.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Huang etÂ al.</span> (<a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite></th>
<td id="S4.T5.1.8.7.2" class="ltx_td ltx_align_center ltx_border_r">37.5/13.4*</td>
<td id="S4.T5.1.8.7.3" class="ltx_td ltx_align_center ltx_border_r">28.9</td>
<td id="S4.T5.1.8.7.4" class="ltx_td ltx_align_center ltx_border_r">Multi-view, IMU</td>
</tr>
<tr id="S4.T5.1.9.8" class="ltx_tr">
<th id="S4.T5.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Zhang etÂ al.</span> (<a href="#bib.bib80" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite></th>
<td id="S4.T5.1.9.8.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">-</td>
<td id="S4.T5.1.9.8.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T5.1.9.8.3.1" class="ltx_text ltx_font_bold">24.6</span></td>
<td id="S4.T5.1.9.8.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">Multi-view, IMU</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>Accuracy comparison from several state-of-the-art multi-view and multimodal methods. Human3.6M and TotalCapture results reported in absolute MPJPE (lower is better). Techniques with the annotation with + are using extra-training data to obtain the result; the others use the benchmarkâ€™s recommended protocols. The * annotation indicates results published with procrustes alignment to ground truth poses before evaluation.</figcaption>
</figure>
<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Qiu etÂ al.</span> (<a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite> uses a two-stage prediction process similar to <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Zhang etÂ al.</span> (<a href="#bib.bib80" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite> without taking IMU data as inputs. Instead, they opt to fuse data from multi-view images using projective geometry constraints. This process is done through a convolutional layer that merges pixel data from each view along the epipolar lines using weighted matrices. After this step, fused 2D joints heat maps are generated and 3D pose is inferred through the Pictorial Structure Model with a skeleton model. A recursive variation of this model is used to reduce quantization errors and complexity using a divide and conquer scheme for space discretization. Comparing results from methods providing absolute coordinates, the cross view approach improved the state-of-the-art results at the time and further improves them using additional 2D data during training. It also gives competitive results on the TotalCapture dataset without pre-training and without using the IMU data. It can also be used in different camera setup using pseudo-labeling from 2D pose estimator. The method can be applied to 3D human pose estimation in new contexts without the need of training with 3D ground truths. This method illustrates very well the main approach that consists of using improved results in 2D HPE and translating them into 3D. Here the refinement is done by adding multi-view to improve 2D predictions.
<br class="ltx_break"></p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Iskakov etÂ al.</span> (<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite> present a learnable way to triangulate human poses. This article proposes two geometric methods for triangulating 3D joint coordinates from multiple view joint heat maps. The first one is an algebraic method based on solving a system of vector equations with 3D coordinates. The second method is a triangulation from volumetric aggregation of re-projections of the 2D heat maps in a voxel grid. Both are weighting the information coming from different views with learnable coefficients. In the volumetric approach, each heat map corresponding to a joint from each different view is sampled into a voxel cube. These volumetric maps (for a specific joints) are then aggregated with a weighting of the impact from different views and fed into a 3D CNN that refines them. The final step is a soft-argmax operation on the resulting 3D heatmaps yielding computable 3D coordinates. Each of these steps is differentiable and the weights of the different convolutional layers at each stage are updated using an absolute loss. The results on Human3.6M for multiple view input are the best ones to date with a 17.7mm MPJPE accuracy (i.e., with the volumetric method and softmax aggregation during the learning stage as described above). On limit of this method is that it needs a correct cropped volume around the human skeleton to work well. Consequently, at least two camera must detect the pelvis joint. The given absolute MPJPE score is also calculated with the removal of several actions due to annotation errors on Human3.6M.Nevertheless, this method reaches the best accuracy for relative pose estimation among all the methods we reviewed.
<br class="ltx_break"></p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">Epipolar Transformers from <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">He etÂ al.</span> (<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite> are modules using the attention mechanism as well as knowledge of epipolar geometry. The authors noted that most 2D detection of keypoints did not use 3D features at all: this was the motivation for their â€Epipolar Transformerâ€. The goal is to fuse intermediate features from multiple views during 2D inference using projective geometry constraints <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Qiu etÂ al.</span> (<a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite>. First, from a detected point in the source view, the module samples all points on the corresponding epipolar line in another view. Then, features across this line are fused according to a computed weighted similarity with the source point. In the end, the obtained feature maps have the same size as the input, which makes the module compatible with any two-stage multi-view system. Any triangulation algorithm can then be applied. On Human3.6M, the authors computed the 3D coordinates with the recursive pictorial structure model of <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Qiu etÂ al.</span> (<a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite> using their epipolar transformer: they obtained the best MPJPE score across state-of-the-art, and they did so without using external training data. They also compare their results with pre-training on MPII 2D dataset and get results close to the best methods (19mm against 17.7 from <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Iskakov etÂ al.</span> (<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite>) with 10% less parameters and computation operations. The main limitation of the method is that it only works on a fully calibrated multi-camera system, because it needs camera parameters to compute epipolar lines.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Multimodal approaches</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">von Marcard etÂ al.</span> (<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2016</span></a>)</cite> propose one of the first method combining multi-view video and IMU data for pose estimation. The authors claim that their method is less intrusive than marker-based MoCap, as only a few sensors are placed on the subjects. They use a representation of human body constraints based on kinematic chains. Using the silhouettes from multi-view extracted with background subtraction and limb orientation from IMU, they minimize a hybrid energy term obtained from orientation and contour consistency with a human mesh model (<cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Loper etÂ al.</span> (<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2015</span></a>)</cite>). They provide a thorough analysis of their method (with the TNT15 dataset presented in the same paper and some metrics on HumanEva) showing that video and orientation sensor complement each other. The idea is that IMUs accurately measure joint angles but tend to drift during the experiment, whereas video is better suited to obtain positional information.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Trumble etÂ al.</span> (<a href="#bib.bib71" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2017</span></a>)</cite> present the first large-scale motion capture dataset that also contains IMU data. The paper describes a 3D pose estimation technique fusing 3D data from multi-view and limb orientation from IMU, while maintaining the temporal context using a Long Short-Term Memory layer over the five past frames. A PVH <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Grauman etÂ al.</span> (<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2003</span></a>)</cite> is computed from the multi-view video and inputted to a 3D convolutional network with 26 3D joint coordinates as an output. In parallel, kinematic solving provides the same joint coordinates from IMU data. The vectors from both sources are then passed through the LSTM layer and merged into an embedding representing the 3D pose. This way, the authors show that it is possible to learn a â€mapping between the predicted joint estimates of the two data sources and the actual joint locationsâ€. They evaluate their method on the newly presented TotalCapture dataset. They also evaluate the multi-view part of their pipeline without IMU on Human3.6M.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p">Following their work with inertial units, <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">von Marcard etÂ al.</span> (<a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite> present a monocular method with a moving camera in-the-wild and multiple subjects wearing IMUs. The method uses a 2D multi-person pose estimation algorithm <cite class="ltx_cite ltx_citemacro_cite">Cao etÂ al. (<a href="#bib.bib7" title="" class="ltx_ref">2017</a>)</cite> and a module that fits the SMPL <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Loper etÂ al.</span> (<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2015</span></a>)</cite> model to IMU data. Then, each 2D skeleton is paired with 3D pose and shape using graph optimization. Next, with the obtained associations model parameters, camera pose and orientation are optimized and fed back for further iterations. When evaluated on the TotalCapture dataset, the method outperforms previous ones by 44mm. This technique allows for the capture of a new dataset in outdoor environments and without marker ground-truths: the 3D Poses in the Wild Dataset.
<br class="ltx_break"></p>
</div>
<div id="S4.SS4.p4" class="ltx_para">
<p id="S4.SS4.p4.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Huang etÂ al.</span> (<a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite> developed an end-to-end trainable 3D convolutional network witha refinement module based on IMU data. The main idea is to process primitive 3D data from multi-view frames without any transformation. A multi-channel volume, constructed from the segmented human silhouettes and camera parameters, is used as input for the network. 3D voxel confidence heat maps are computed at this stage and can already be used for human pose estimation prediction. The refinement stage merges the volumes constructed from IMU and those constructed from heat maps, as well as the multi-channel volumes. IMU volumes are processed into a â€bone cylinderâ€ from quaternion orientation and the previously predicted joint position. Then, all this 3D information is fed into another 3D convolutional network. The architecture uses hourglass networks and residual network modules (see 2D architectures <a href="#S3.F2" title="Fig. 2 â€£ 3.2 Backbone Architectures â€£ 3 Architectures for Human Pose Estimation â€£ A review of 3D human pose estimation algorithms for markerless motion capture" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) and 3D soft-argmax to extract coordinates. MSE loss is computed at the different levels of the architecture. By randomly stopping information from a camera, the method allows for data augmentation during training, which significantly improves performance on partially captured images. With the vision-only module on a dataset without IMU, the method obtains good overall accuracy. The authors claim that their model can be used in a real-time system because it does not use time sequences. However, they do not provide any performance or speed evaluation. They also point out that their technique does not use a complex human model and is therefore more likely to generalize to new subjects. However, this depends on the performance of the human shape segmentation algorithm at the preprocessing step.
<br class="ltx_break"></p>
</div>
<div id="S4.SS4.p5" class="ltx_para">
<p id="S4.SS4.p5.1" class="ltx_p">Similarly, <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Zhang etÂ al.</span> (<a href="#bib.bib80" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite> fuse IMU with multi-view image data, but this time using a human skeleton model. The approach is to refine 2D joint confidence heat maps with IMU data to then use a part-based model. First, as in <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Qiu etÂ al.</span> (<a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite>, they extract 2D joint heat maps. However, the authors use a simple baseline method <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Xiao etÂ al.</span> (<a href="#bib.bib75" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite> before merging the heat maps information and the IMU orientation to geometrically estimate the correct coordinates. They call this process Orientation Regularized Network (ORN). This technique can be used with any heat map-based prediction method and they use it to train an end-to-end network that produces more accurate results than state-of-the-art 2D methods not exploiting IMU data. The second part of their work consists of a variant of the Pictorial Structure Model (PSM) (<cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Felzenszwalb and Huttenlocher</span> (<a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2005</span></a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Belagiannis etÂ al. (<a href="#bib.bib3" title="" class="ltx_ref">2014</a>)</cite>) which is often used for 3D and 2D HPE. This family of method calculates the most probable human pose within all possible ones in a discrete space. Typically, PSM-type methods uses limb length as the primary constraint. Here, the authors exploit the IMU data to also apply a limb orientation constraint, which improves accuracy. This framework includes a module that can deeply improve results from 2D estimators with inertial data. Note that the 3D part of this system does not use CNN, unlike many other current methods, although it gives excellent accuracy. The authors also use synthetic IMU data to predict 3D pose on the Human3.6M dataset to conclude that their use improves the results by an average of 10mm.
<br class="ltx_break"></p>
</div>
<div id="S4.SS4.p6" class="ltx_para">
<p id="S4.SS4.p6.1" class="ltx_p">Finally, recent works using a single depth sensor <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Yu etÂ al.</span> (<a href="#bib.bib77" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2017</span></a>)</cite> and <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Yu etÂ al.</span> (<a href="#bib.bib78" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite> have shown good results for real-time motion capture. The first one uses the iterative closest point algorithm to reconstruct body shapes and the second optimize the SMPL body model. However, these methods are not tested for joint localization error on a pose estimation data set.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Analysis and discussion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">As mentioned in the introduction, this article is structured around the specifications and requirements, which vary according to the fields of application. Today 3D pose estimation is employed in many fieldsâ€‰:
<br class="ltx_break"></p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text ltx_font_italic">Human Computer Interface:</span> There is an increasing number of applications using human pose and gesture to interact with computers. 3D HPE is essential to help robots and machines better understand and respond to human motions.
<br class="ltx_break"></p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p"><span id="S5.p3.1.1" class="ltx_text ltx_font_italic">Security:</span> The classical applicationis to track people in the indoor and outdoor environment to ensure they do not commit theft or infractions. 
<br class="ltx_break"></p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p"><span id="S5.p4.1.1" class="ltx_text ltx_font_italic">Motion Analysis:</span> This is a broad field that comprises sport and performance analysis, medical study, pose semantics or the study of inter-human interactions.
<br class="ltx_break"></p>
</div>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p"><span id="S5.p5.1.1" class="ltx_text ltx_font_italic">Entertainment:</span> Pose estimation can be used for avatar control (e.g., Kinect) or VR and AR refinement and for avatar animation in games and movies.
<br class="ltx_break"></p>
</div>
<div id="S5.p6" class="ltx_para">
<p id="S5.p6.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Moeslund and Granum</span> (<a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2001</span></a>)</cite> classify them into three categoriesâ€‰: surveillance, analysis and control. Each category requires different performance regarding accuracy, speed or robustness. However, within the same category, there are variations on these criteria. For example, as the previously mentioned authors state, a control application can be constrained to highly controlled environment (avatar control) or within a generic outdoor scene with varying conditions.
<br class="ltx_break"></p>
</div>
<div id="S5.p7" class="ltx_para">
<p id="S5.p7.1" class="ltx_p">For this reason, our review describes each criterion separately and explains in which use case it performs the best. Tables <a href="#S5.T6" title="Table 6 â€£ 5.1 Accuracy â€£ 5 Analysis and discussion â€£ A review of 3D human pose estimation algorithms for markerless motion capture" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, <a href="#S5.T7" title="Table 7 â€£ 5.2 Robustness â€£ 5 Analysis and discussion â€£ A review of 3D human pose estimation algorithms for markerless motion capture" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> and <a href="#S5.T10" title="Table 10 â€£ 5.3 Speed â€£ 5 Analysis and discussion â€£ A review of 3D human pose estimation algorithms for markerless motion capture" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> classify these methods with accuracy reported in MPJPE. The level of robustness corresponds to the number of assumptions or constraints necessary for correct detection. Lastly, to express the speed, we report whether the model can run in real-time. Each of these tables allow to cross-compare the different methods best suited to the most needed specifications.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Accuracy</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Accuracy is the main criterion used to evaluate markerless human pose estimation methods within the computer vision community. However, it has some limitations that are important to keep in mind. First, most benchmarks compare markerless vision-based methods to results obtained from marker-based optoelectronic systems that are themselves not free from errors. As reported in Section <a href="#S2.SS2" title="2.2 Commonly used Benchmarks â€£ 2 Methods Evaluation â€£ A review of 3D human pose estimation algorithms for markerless motion capture" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>, some old motion capture datasets contain videos with low resolution and inaccurate annotations.
<br class="ltx_break"></p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">The second limit in accuracy comparison is that, in many cases, it is not the local error rate that is important to the application but the semantics of the pose as whole (See <a href="#S2.SS1" title="2.1 Metrics â€£ 2 Methods Evaluation â€£ A review of 3D human pose estimation algorithms for markerless motion capture" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>). This can be the case for surveillance systems or human-machine interface for example. When the accuracy requirement (at least with conventional metrics) is low to minimal, see the two other criteria discussed in this review.
<br class="ltx_break"></p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">However, many other fields of research need high accuracy , such as medical science or biology. Typically, the accuracy prerequisite can also be different depending on the analysis. Sometimes, the highest accuracy possible is required regardless of the acquisition and computation procedure complexity (e.g., for research in biomechanics). However, many studies are conducted from human labeled videos with much simpler setups. We propose a solution based on machine vision for each of these scenarios.</p>
</div>
<figure id="S5.T6" class="ltx_table">
<div id="S5.T6.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:65.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-159.6pt,48.2pt) scale(0.404495561678609,0.404495561678609) ;">
<table id="S5.T6.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T6.1.1.1.1" class="ltx_tr">
<th id="S5.T6.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" colspan="5">Accuracy</th>
</tr>
<tr id="S5.T6.1.1.2.2" class="ltx_tr">
<th id="S5.T6.1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">Method</th>
<th id="S5.T6.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Type</th>
<th id="S5.T6.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Accuracy</th>
<th id="S5.T6.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Robustness Level</th>
<th id="S5.T6.1.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Real-time</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T6.1.1.3.1" class="ltx_tr">
<th id="S5.T6.1.1.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Kolotouros etÂ al.</span> (<a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite></th>
<td id="S5.T6.1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Monocular</td>
<td id="S5.T6.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.1.3.1.3.1" class="ltx_text ltx_font_bold">41.1</span></td>
<td id="S5.T6.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">average</td>
<td id="S5.T6.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">âœ—</td>
</tr>
<tr id="S5.T6.1.1.4.2" class="ltx_tr">
<th id="S5.T6.1.1.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Wandt and Rosenhahn</span> (<a href="#bib.bib73" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite></th>
<td id="S5.T6.1.1.4.2.2" class="ltx_td ltx_align_center ltx_border_r">Monocular</td>
<td id="S5.T6.1.1.4.2.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T6.1.1.4.2.3.1" class="ltx_text ltx_framed ltx_framed_underline">50.9</span></td>
<td id="S5.T6.1.1.4.2.4" class="ltx_td ltx_align_center ltx_border_r">average</td>
<td id="S5.T6.1.1.4.2.5" class="ltx_td ltx_align_center">âœ“</td>
</tr>
<tr id="S5.T6.1.1.5.3" class="ltx_tr">
<th id="S5.T6.1.1.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Iskakov etÂ al.</span> (<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite></th>
<td id="S5.T6.1.1.5.3.2" class="ltx_td ltx_align_center ltx_border_r">Multi-view</td>
<td id="S5.T6.1.1.5.3.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T6.1.1.5.3.3.1" class="ltx_text ltx_font_bold">17.7</span></td>
<td id="S5.T6.1.1.5.3.4" class="ltx_td ltx_align_center ltx_border_r">average</td>
<td id="S5.T6.1.1.5.3.5" class="ltx_td ltx_align_center">âœ—</td>
</tr>
<tr id="S5.T6.1.1.6.4" class="ltx_tr">
<th id="S5.T6.1.1.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">He etÂ al.</span> (<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite></th>
<td id="S5.T6.1.1.6.4.2" class="ltx_td ltx_align_center ltx_border_r">Multi-view</td>
<td id="S5.T6.1.1.6.4.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T6.1.1.6.4.3.1" class="ltx_text ltx_framed ltx_framed_underline">26.9</span></td>
<td id="S5.T6.1.1.6.4.4" class="ltx_td ltx_align_center ltx_border_r">low</td>
<td id="S5.T6.1.1.6.4.5" class="ltx_td ltx_align_center">âœ—</td>
</tr>
<tr id="S5.T6.1.1.7.5" class="ltx_tr">
<th id="S5.T6.1.1.7.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite">Cheng etÂ al. (<a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite></th>
<td id="S5.T6.1.1.7.5.2" class="ltx_td ltx_align_center ltx_border_r">Temporal</td>
<td id="S5.T6.1.1.7.5.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T6.1.1.7.5.3.1" class="ltx_text ltx_font_bold">40.1</span></td>
<td id="S5.T6.1.1.7.5.4" class="ltx_td ltx_align_center ltx_border_r">average</td>
<td id="S5.T6.1.1.7.5.5" class="ltx_td ltx_align_center">âœ—</td>
</tr>
<tr id="S5.T6.1.1.8.6" class="ltx_tr">
<th id="S5.T6.1.1.8.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Wang etÂ al.</span> (<a href="#bib.bib74" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite></th>
<td id="S5.T6.1.1.8.6.2" class="ltx_td ltx_align_center ltx_border_r">Temporal</td>
<td id="S5.T6.1.1.8.6.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T6.1.1.8.6.3.1" class="ltx_text ltx_framed ltx_framed_underline">42.6</span></td>
<td id="S5.T6.1.1.8.6.4" class="ltx_td ltx_align_center ltx_border_r">average</td>
<td id="S5.T6.1.1.8.6.5" class="ltx_td ltx_align_center">âœ—</td>
</tr>
<tr id="S5.T6.1.1.9.7" class="ltx_tr">
<th id="S5.T6.1.1.9.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Zhang etÂ al.</span> (<a href="#bib.bib80" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite></th>
<td id="S5.T6.1.1.9.7.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">Multimodal</td>
<td id="S5.T6.1.1.9.7.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S5.T6.1.1.9.7.3.1" class="ltx_text ltx_font_italic">24.6*</span></td>
<td id="S5.T6.1.1.9.7.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">low</td>
<td id="S5.T6.1.1.9.7.5" class="ltx_td ltx_align_center ltx_border_b">âœ—</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Best accuracy methods. Methods reported with performance criteria of current real-life applications that use 3D pose estimation for four setups (monocular, temporal, multi-view). Accuracy is reported in MPJPE on Human3.6M dataset bold for best and underlined for second best. The best multi-modal approaches using IMU (marked with *) are evaluated with MPJPE on TotalCapture for comparison.</figcaption>
</figure>
<section id="S5.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.1 </span>Highest accuracy methods</h4>

<div id="S5.SS1.SSS1.p1" class="ltx_para">
<p id="S5.SS1.SSS1.p1.1" class="ltx_p">When looking at results of contemporary methods, there is an average gap of 10mm error between monocular and multi-view methods. When multi-view methods are used properly, the combination of geometric knowledge of the scene and learning optimizations can results in errors as low as 20 mm. However, the error is higher in a general context (which is also true for monocular methods). For example, <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Iskakov etÂ al.</span> (<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite> obtained 34mm MPJPE on a different dataset than the one used for training.
<br class="ltx_break"></p>
</div>
<div id="S5.SS1.SSS1.p2" class="ltx_para">
<p id="S5.SS1.SSS1.p2.1" class="ltx_p">IMU sensors are also a good way to improve multi-view detection. However, for now, the results seem close to those obtained with image data alone, with <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Zhang etÂ al.</span> (<a href="#bib.bib80" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite> obtaining the most accurate results. Future evaluations on the TotalCapture dataset <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Trumble etÂ al.</span> (<a href="#bib.bib71" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2017</span></a>)</cite> with methods that do not use inertial information could help with the comparison.
<br class="ltx_break"></p>
</div>
<div id="S5.SS1.SSS1.p3" class="ltx_para">
<p id="S5.SS1.SSS1.p3.1" class="ltx_p">The strength and weakness of the multi-view methods comes from the fact that they are based on camera parameters. This helps to the generalizability of the system as it can adapt to new camera views <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">He etÂ al.</span> (<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite>, but also makes the process more complex as it requires calibration. A solution can be the one suggested by <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Kocabas etÂ al.</span> (<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019b</span></a>)</cite>, with a system that computes camera parameters on-the-fly with the detected joint as calibration targets. Multiple images hold much more information than a single one and well-known stereo vision properties are applicable.It turns out that most research has been concentrated on monocular images and 2D estimation, so many other avenues remain unexplored, such as the simultaneous exploitation of temporal and multi-view data.
<br class="ltx_break"></p>
</div>
<div id="S5.SS1.SSS1.p4" class="ltx_para">
<p id="S5.SS1.SSS1.p4.1" class="ltx_p">Monocular methods based on video sequences that can be processed offline such as <cite class="ltx_cite ltx_citemacro_cite">Cheng etÂ al. (<a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite> and <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Pavllo etÂ al.</span> (<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite> have merit. However, their accuracy is not less than 40 mm MPJPE on Human3.6M, and their ability to generalize is also unproven, as no comparative analysis is yet available for them.</p>
</div>
</section>
<section id="S5.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.2 </span>Simplest but accurate methods</h4>

<div id="S5.SS1.SSS2.p1" class="ltx_para">
<p id="S5.SS1.SSS2.p1.1" class="ltx_p">Monocular methods are the simplest, as they can be fed a simple video input. In many cases, they are a clear improvement over handcrafted annotation, as they have similar accuracy but provide richer 3D information with less input data. When video data is available, it is judicious to favor methods based on temporal data that provide better results (<cite class="ltx_cite ltx_citemacro_cite">Cheng etÂ al. (<a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite>; <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Wang etÂ al.</span> (<a href="#bib.bib74" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite> or <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Pavllo etÂ al.</span> (<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite>). <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Kolotouros etÂ al.</span> (<a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite> also propose a method based on single images that is competitive with image sequence techniques (41.1mm MPJPE on Human3.6M). This indicates that their approach of jointly optimizing a mesh model and training a neural network is an effective way to solve monocular pose estimation problems.
<br class="ltx_break"></p>
</div>
<div id="S5.SS1.SSS2.p2" class="ltx_para">
<p id="S5.SS1.SSS2.p2.1" class="ltx_p">Although <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Wandt and Rosenhahn</span> (<a href="#bib.bib73" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite> does not achieve the typical 40mm average error, it does so at a low computational cost. Thus, it is a good compromise between accuracy and speed for real-time pose estimation.</p>
</div>
<div id="S5.SS1.SSS2.p3" class="ltx_para">
<p id="S5.SS1.SSS2.p3.1" class="ltx_p">In many cases, another important factor is the flexibility of the method. Yet most algorithms were not designed with this in mind, as they are limited to the labels in the training data. For the study of human motion, it is common to have to define custom key points to track limbs, segments or joints. This problem can be solved using DeepLabCut <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Mathis etÂ al.</span> (<a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite> or other methods that are not human-specific <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Zhang and Park</span> (<a href="#bib.bib79" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite>. Obviously, these methods still require ground-truth examples for each new key point, but on a much smaller scale. Another possibility is to refine existing models with new custom labels.</p>
</div>
</section>
<section id="S5.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.3 </span>Overall conclusion on accuracy</h4>

<div id="S5.SS1.SSS3.p1" class="ltx_para">
<p id="S5.SS1.SSS3.p1.1" class="ltx_p">The table <a href="#S5.T6" title="Table 6 â€£ 5.1 Accuracy â€£ 5 Analysis and discussion â€£ A review of 3D human pose estimation algorithms for markerless motion capture" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows that the most accurate methods have a medium to low level of robustness and that few of them work in real time. The explanation comes from the fact that they are often complex methods that tackle issues such as occlusion with specific modules that increase the overall computational cost. As explained above, the best methods are naturally multi-view techniques that exploit geometric constraints and therefore need calibrations.
<br class="ltx_break">Currently, the architectures that achieve the best accuracy are two-stage top-down algorithms. They achieve the best results on a variety of benchmarks in monocular image, monocular video, and multi-view configurations. They often build on the success of 2D pose estimation, which is a nearly solved problem (i.e., it achieves average PCK scores above 90%). Many different approaches are effective: direct regression from 2D to 3D, initialization of human parametric models, exploitation of temporal sequences, occlusion-aware modules, generative models, volumetric input representation, multi-view triangulation, to name the most important (See <a href="#S4" title="4 Methods Review â€£ A review of 3D human pose estimation algorithms for markerless motion capture" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and Fig. <a href="#S4.F3" title="Fig. 3 â€£ 4.1 Monocular Images â€£ 4 Methods Review â€£ A review of 3D human pose estimation algorithms for markerless motion capture" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). An interesting point is that, logically, video sequence methods perform better for activities with temporal coherence, such as walking or running, while monocular methods best detect complex static poses better.
<br class="ltx_break"></p>
</div>
<div id="S5.SS1.SSS3.p2" class="ltx_para">
<p id="S5.SS1.SSS3.p2.1" class="ltx_p">Another strong distinction can be made between model-based and model-free approaches. While the state of the art in 2D detection does not use human models, 3D detection successfully does. They are either based on Pictorial Structure <cite class="ltx_cite ltx_citemacro_cite">Belagiannis etÂ al. (<a href="#bib.bib3" title="" class="ltx_ref">2014</a>)</cite> or on a 3D mesh model like SMPL <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Loper etÂ al.</span> (<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2015</span></a>)</cite> or SCAPE <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib2" title="" class="ltx_ref">Anguelov etÂ al. </a></cite>. Recent successful propositions also use 2D detection from CNN as initialization for their models. However model-free techniques perform as well or even better in some cases. The detection error for the 3D human pose estimation task has decreased by nearly 70mm over the last decade, mainly due to convolutional networks in different architectures. Interestingly, these improvements were mainly due to better 2D modules, which may indicate that research into the 3D nature of the problem is a fruitful avenue.</p>
</div>
</section>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Robustness</h3>

<figure id="S5.T7" class="ltx_table">
<div id="S5.T7.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:66.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-157.0pt,47.9pt) scale(0.408518075370703,0.408518075370703) ;">
<table id="S5.T7.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T7.1.1.1.1" class="ltx_tr">
<th id="S5.T7.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" colspan="5">Robustness</th>
</tr>
<tr id="S5.T7.1.1.2.2" class="ltx_tr">
<th id="S5.T7.1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">Method</th>
<th id="S5.T7.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Type</th>
<th id="S5.T7.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Accuracy</th>
<th id="S5.T7.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Robustness Level</th>
<th id="S5.T7.1.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Real-time</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T7.1.1.3.1" class="ltx_tr">
<th id="S5.T7.1.1.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Mehta etÂ al.</span> (<a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite></th>
<td id="S5.T7.1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Monocular</td>
<td id="S5.T7.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">63.6</td>
<td id="S5.T7.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">high</td>
<td id="S5.T7.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">âœ“</td>
</tr>
<tr id="S5.T7.1.1.4.2" class="ltx_tr">
<th id="S5.T7.1.1.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Xu etÂ al.</span> (<a href="#bib.bib76" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite></th>
<td id="S5.T7.1.1.4.2.2" class="ltx_td ltx_align_center ltx_border_r">Monocular</td>
<td id="S5.T7.1.1.4.2.3" class="ltx_td ltx_align_center ltx_border_r">82.4</td>
<td id="S5.T7.1.1.4.2.4" class="ltx_td ltx_align_center ltx_border_r">high</td>
<td id="S5.T7.1.1.4.2.5" class="ltx_td ltx_align_center">âœ“</td>
</tr>
<tr id="S5.T7.1.1.5.3" class="ltx_tr">
<th id="S5.T7.1.1.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Hossain and Little</span> (<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite></th>
<td id="S5.T7.1.1.5.3.2" class="ltx_td ltx_align_center ltx_border_r">Temporal</td>
<td id="S5.T7.1.1.5.3.3" class="ltx_td ltx_align_center ltx_border_r">58.5</td>
<td id="S5.T7.1.1.5.3.4" class="ltx_td ltx_align_center ltx_border_r">high</td>
<td id="S5.T7.1.1.5.3.5" class="ltx_td ltx_align_center">âœ“</td>
</tr>
<tr id="S5.T7.1.1.6.4" class="ltx_tr">
<th id="S5.T7.1.1.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite">Cheng etÂ al. (<a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite></th>
<td id="S5.T7.1.1.6.4.2" class="ltx_td ltx_align_center ltx_border_r">Temporal</td>
<td id="S5.T7.1.1.6.4.3" class="ltx_td ltx_align_center ltx_border_r">40.1</td>
<td id="S5.T7.1.1.6.4.4" class="ltx_td ltx_align_center ltx_border_r">average</td>
<td id="S5.T7.1.1.6.4.5" class="ltx_td ltx_align_center">âœ—</td>
</tr>
<tr id="S5.T7.1.1.7.5" class="ltx_tr">
<th id="S5.T7.1.1.7.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Liu etÂ al.</span> (<a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite></th>
<td id="S5.T7.1.1.7.5.2" class="ltx_td ltx_align_center ltx_border_r">Temporal</td>
<td id="S5.T7.1.1.7.5.3" class="ltx_td ltx_align_center ltx_border_r">45.1</td>
<td id="S5.T7.1.1.7.5.4" class="ltx_td ltx_align_center ltx_border_r">average</td>
<td id="S5.T7.1.1.7.5.5" class="ltx_td ltx_align_center">âœ—</td>
</tr>
<tr id="S5.T7.1.1.8.6" class="ltx_tr">
<th id="S5.T7.1.1.8.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Iskakov etÂ al.</span> (<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite></th>
<td id="S5.T7.1.1.8.6.2" class="ltx_td ltx_align_center ltx_border_r">Multi-view</td>
<td id="S5.T7.1.1.8.6.3" class="ltx_td ltx_align_center ltx_border_r">17.7</td>
<td id="S5.T7.1.1.8.6.4" class="ltx_td ltx_align_center ltx_border_r">average</td>
<td id="S5.T7.1.1.8.6.5" class="ltx_td ltx_align_center">âœ—</td>
</tr>
<tr id="S5.T7.1.1.9.7" class="ltx_tr">
<th id="S5.T7.1.1.9.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">von Marcard etÂ al.</span> (<a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite></th>
<td id="S5.T7.1.1.9.7.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">Multimodal</td>
<td id="S5.T7.1.1.9.7.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">26.0*</td>
<td id="S5.T7.1.1.9.7.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">high</td>
<td id="S5.T7.1.1.9.7.5" class="ltx_td ltx_align_center ltx_border_b">âœ—</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Most robust methods. Robustness level is defined as follows: 1 - 3 assumptions: high level, 3 - 4 assumptions: average level and 5 or more: low level.</figcaption>
</figure>
<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Robustness is usually assessed through changes in the accuracy during cross-dataset evaluation. <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Moeslund and Granum</span> (<a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2001</span></a>)</cite> propose to express robustness as the number of assumptions required for a motion capture configuration to be operational. They define twenty assumptions relative to the acquisition protocol or environment and the subjectâ€™s appearance. Some of them have been overcome by all current method (subject wearing specific clothes or static monochrome backgrounds), but others are still very much debated (occlusions, single person or no camera motion). Below are assumptions that remain in the state of the art:</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p"><span id="S5.SS2.p2.1.1" class="ltx_text ltx_font_italic">Motion Constraintsâ€‰:</span> No camera motion, no fast motion of the subject, no occlusion (no severe occlusion, no auto-occlusion). For temporal methods, the number of frames and the acquisition frequency may also be a limitation.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p"><span id="S5.SS2.p3.1.1" class="ltx_text ltx_font_italic">Environment Constraints:</span> Extra hardware (IMU, Laser Scans), Multiple cameras (with or without camera parameters), etc.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p"><span id="S5.SS2.p4.1.1" class="ltx_text ltx_font_italic">Subject Constraints: </span> A single person, a known first pose, a motion parallel to the camera plane (for a model-based approach)</p>
</div>
<figure id="S5.T8" class="ltx_table">
<table id="S5.T8.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T8.1.1.1" class="ltx_tr">
<th id="S5.T8.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Method</th>
<th id="S5.T8.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">#Frames</th>
<th id="S5.T8.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Causal</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T8.1.2.1" class="ltx_tr">
<td id="S5.T8.1.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Hossain and Little</span> (<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite></td>
<td id="S5.T8.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5</td>
<td id="S5.T8.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">âœ“</td>
</tr>
<tr id="S5.T8.1.3.2" class="ltx_tr">
<td id="S5.T8.1.3.2.1" class="ltx_td ltx_align_center ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite">Cai etÂ al. (<a href="#bib.bib6" title="" class="ltx_ref">2019</a>)</cite></td>
<td id="S5.T8.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">7</td>
<td id="S5.T8.1.3.2.3" class="ltx_td ltx_align_center">âœ—</td>
</tr>
<tr id="S5.T8.1.4.3" class="ltx_tr">
<td id="S5.T8.1.4.3.1" class="ltx_td ltx_align_center ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Pavllo etÂ al.</span> (<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite></td>
<td id="S5.T8.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">243</td>
<td id="S5.T8.1.4.3.3" class="ltx_td ltx_align_center">âœ“</td>
</tr>
<tr id="S5.T8.1.5.4" class="ltx_tr">
<td id="S5.T8.1.5.4.1" class="ltx_td ltx_align_center ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Cheng etÂ al.</span> (<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite></td>
<td id="S5.T8.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r">256</td>
<td id="S5.T8.1.5.4.3" class="ltx_td ltx_align_center">âœ—</td>
</tr>
<tr id="S5.T8.1.6.5" class="ltx_tr">
<td id="S5.T8.1.6.5.1" class="ltx_td ltx_align_center ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite">Cheng etÂ al. (<a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite></td>
<td id="S5.T8.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r">128</td>
<td id="S5.T8.1.6.5.3" class="ltx_td ltx_align_center">âœ—</td>
</tr>
<tr id="S5.T8.1.7.6" class="ltx_tr">
<td id="S5.T8.1.7.6.1" class="ltx_td ltx_align_center ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Liu etÂ al.</span> (<a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite></td>
<td id="S5.T8.1.7.6.2" class="ltx_td ltx_align_center ltx_border_r">243</td>
<td id="S5.T8.1.7.6.3" class="ltx_td ltx_align_center">âœ“</td>
</tr>
<tr id="S5.T8.1.8.7" class="ltx_tr">
<td id="S5.T8.1.8.7.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Wang etÂ al.</span> (<a href="#bib.bib74" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite></td>
<td id="S5.T8.1.8.7.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">96</td>
<td id="S5.T8.1.8.7.3" class="ltx_td ltx_align_center ltx_border_b">âœ—</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Monocular temporal models assumptions. Needed number of frames to obtain the optimal accuracy and whether the system can be adapted to only use past frames (for real-time use)</figcaption>
</figure>
<section id="S5.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.1 </span>Background, Lighting and Clothes</h4>

<div id="S5.SS2.SSS1.p1" class="ltx_para">
<p id="S5.SS2.SSS1.p1.1" class="ltx_p">Most appearance constraints are no longer required, because convolutional networks do much better at identifying meaningful visual invariants than former hand-crafted feature extractors. Yet, it is difficult to evaluate generalization to in-the-wild situations, mainly because large motion capture datasets are still captured in indoor studios. Data augmentation can provide new scenes with background extraction or change the lighting. With the release of new commercial markerless solutions, new benchmarks such as MPI-INF-3DHP <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Mehta etÂ al.</span> (<a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2016</span></a>)</cite> also include real outdoor data.</p>
</div>
</section>
<section id="S5.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.2 </span>Special Hardware &amp; Calibrated Systems</h4>

<div id="S5.SS2.SSS2.p1" class="ltx_para">
<p id="S5.SS2.SSS2.p1.1" class="ltx_p">Two constraints are still relevant: first, the need for specific hardware, second, the need for calibration in multi-view configurations. The two most commonly used extra hardware added for pose estimation are inertial motion units and depth sensors such as infrared or time-of-flight cameras. Inertial motion unit provide additional information on the limbs orientation but suffer from drift in their results after a short usage. They are also more practical than reflecting markers and motion capture suits but are still intrusive for the subject. Different depth sensors have also been used to infer the depth of joints directly or to construct more complex features as a pre-processing step of pose estimation. Less frequently some methods use the 3D scan of each subjects that is captured to fit shapes parameter of human body models. Logically, while most research is conducted on monocular methods, they are always outperformed by 10 to 20 mm MPJPE by multi-view techniques. In multi-view systems, the calibration step is a frequent requirement. Multi-view methods that do not use or use partial calibration need more views to achieve acceptable accuracy, while others can produce good results with fewer cameras but need extrinsic parameters (see table <a href="#S5.T9" title="Table 9 â€£ 5.2.2 Special Hardware &amp; Calibrated Systems â€£ 5.2 Robustness â€£ 5 Analysis and discussion â€£ A review of 3D human pose estimation algorithms for markerless motion capture" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>).</p>
</div>
<figure id="S5.T9" class="ltx_table">
<div id="S5.T9.1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:72.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-135.0pt,44.9pt) scale(0.445295828957583,0.445295828957583) ;">
<table id="S5.T9.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T9.1.1.1.2.1" class="ltx_tr">
<th id="S5.T9.1.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Method</th>
<td id="S5.T9.1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Calibration</td>
<td id="S5.T9.1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Additional</td>
<td id="S5.T9.1.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">#Views</td>
</tr>
<tr id="S5.T9.1.1.1.1" class="ltx_tr">
<th id="S5.T9.1.1.1.1.2" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<td id="S5.T9.1.1.1.1.3" class="ltx_td ltx_border_r"></td>
<td id="S5.T9.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r">Hardware</td>
<td id="S5.T9.1.1.1.1.1" class="ltx_td ltx_align_center">
<math id="S5.T9.1.1.1.1.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S5.T9.1.1.1.1.1.m1.1a"><mo id="S5.T9.1.1.1.1.1.m1.1.1" xref="S5.T9.1.1.1.1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S5.T9.1.1.1.1.1.m1.1b"><lt id="S5.T9.1.1.1.1.1.m1.1.1.cmml" xref="S5.T9.1.1.1.1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S5.T9.1.1.1.1.1.m1.1c">&lt;</annotation></semantics></math> 40 MPJPE</td>
</tr>
<tr id="S5.T9.1.1.1.3.2" class="ltx_tr">
<th id="S5.T9.1.1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Trumble etÂ al.</span> (<a href="#bib.bib71" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2017</span></a>)</cite></th>
<td id="S5.T9.1.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">âœ“</td>
<td id="S5.T9.1.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">âœ“</td>
<td id="S5.T9.1.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_t">4</td>
</tr>
<tr id="S5.T9.1.1.1.4.3" class="ltx_tr">
<th id="S5.T9.1.1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Qiu etÂ al.</span> (<a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite></th>
<td id="S5.T9.1.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">âœ“</td>
<td id="S5.T9.1.1.1.4.3.3" class="ltx_td ltx_border_r"></td>
<td id="S5.T9.1.1.1.4.3.4" class="ltx_td ltx_align_center">4</td>
</tr>
<tr id="S5.T9.1.1.1.5.4" class="ltx_tr">
<th id="S5.T9.1.1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Kocabas etÂ al.</span> (<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019b</span></a>)</cite></th>
<td id="S5.T9.1.1.1.5.4.2" class="ltx_td ltx_border_r"></td>
<td id="S5.T9.1.1.1.5.4.3" class="ltx_td ltx_border_r"></td>
<td id="S5.T9.1.1.1.5.4.4" class="ltx_td ltx_align_center">4</td>
</tr>
<tr id="S5.T9.1.1.1.6.5" class="ltx_tr">
<th id="S5.T9.1.1.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Iskakov etÂ al.</span> (<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite></th>
<td id="S5.T9.1.1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r">âœ“</td>
<td id="S5.T9.1.1.1.6.5.3" class="ltx_td ltx_border_r"></td>
<td id="S5.T9.1.1.1.6.5.4" class="ltx_td ltx_align_center">2</td>
</tr>
<tr id="S5.T9.1.1.1.7.6" class="ltx_tr">
<th id="S5.T9.1.1.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">He etÂ al.</span> (<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite></th>
<td id="S5.T9.1.1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_r">âœ“</td>
<td id="S5.T9.1.1.1.7.6.3" class="ltx_td ltx_border_r"></td>
<td id="S5.T9.1.1.1.7.6.4" class="ltx_td ltx_align_center">3</td>
</tr>
<tr id="S5.T9.1.1.1.8.7" class="ltx_tr">
<th id="S5.T9.1.1.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Huang etÂ al.</span> (<a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite></th>
<td id="S5.T9.1.1.1.8.7.2" class="ltx_td ltx_align_center ltx_border_r">âœ“</td>
<td id="S5.T9.1.1.1.8.7.3" class="ltx_td ltx_align_center ltx_border_r">âœ“</td>
<td id="S5.T9.1.1.1.8.7.4" class="ltx_td ltx_align_center">4(8)</td>
</tr>
<tr id="S5.T9.1.1.1.9.8" class="ltx_tr">
<th id="S5.T9.1.1.1.9.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Zhang etÂ al.</span> (<a href="#bib.bib80" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite></th>
<td id="S5.T9.1.1.1.9.8.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">âœ“</td>
<td id="S5.T9.1.1.1.9.8.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">âœ“</td>
<td id="S5.T9.1.1.1.9.8.4" class="ltx_td ltx_align_center ltx_border_b">4(8)</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9: </span>Multi-view models hardware-related assumptions. The number of camera view used to achieve less than the baseline 40mm MPJPE error (best results from monocular methods) on Human3.6M is also shown (and on TotalCapture under parenthesis).</figcaption>
</figure>
</section>
<section id="S5.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.3 </span>Single Person vs Multi-Person</h4>

<div id="S5.SS2.SSS3.p1" class="ltx_para">
<p id="S5.SS2.SSS3.p1.1" class="ltx_p">The strong assumptions that are still used for many 3D human pose estimation frameworks are related to camera or subject motion and acquisition protocol. One of them is the limitation to one person in the image. As top-down approaches are the most popular, many methods assume or even take as an input a single person. For this reason, some authors recommend the use of off-the-shelf person detectors to crop to the area of the image containing the individual subjects. In this way, the pose estimation algorithms can be applied to each area individually. However, this idea needs to be adapted in multi-view and temporal settings to track each different subject. The main limitation is when subject parts are overlapping in the image, hence indistinguishable by the person detectors. Some research addressed this issue in 2D, but it is still an open problem for 3D with few specific methods <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Mehta etÂ al.</span> (<a href="#bib.bib49" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite> <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Mehta etÂ al.</span> (<a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite>.</p>
</div>
</section>
<section id="S5.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.4 </span>Motion Restriction</h4>

<div id="S5.SS2.SSS4.p1" class="ltx_para">
<p id="S5.SS2.SSS4.p1.1" class="ltx_p">Former markerless motion tracking systems were sometimes constrained to slow motion of only few limbs to perform good detection. It is less and less the case, but there is still some difficulty in predicting quick motions (e.g., in sports video). <cite class="ltx_cite ltx_citemacro_cite">Cheng etÂ al. (<a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite> suggest that temporal and spatial data at different resolutions can be a solution to this issue. A new assumption that can be added for methods based on temporal dataâ€‰: if the video footage is not long enough to provide sufficient information, this can be an issue for real-time inference and even for accuracy. Additionally, temporal methods often use information in the future frame, which is not suitable for real-time. Table <a href="#S5.T8" title="Table 8 â€£ 5.2 Robustness â€£ 5 Analysis and discussion â€£ A review of 3D human pose estimation algorithms for markerless motion capture" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> show that these methods perform best with varying temporal receptive fields. Some methods only need a few past and future images, while the accuracy of others saturates at more than 200. These methods can be adapted to shorter video clips and to real-time application using only past frames, but at the price of a decrease in accuracy. Another strong constraint is the use of video from moving cameras, but this is rarely addressed <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">von Marcard etÂ al.</span> (<a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite>. Many applications can work with the assumption of fixed cameras, but there is a substantial amount of video data produced with moving camera coordinate systems (e.g., in outdoor sports).</p>
</div>
</section>
<section id="S5.SS2.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.5 </span>Occlusions</h4>

<div id="S5.SS2.SSS5.p1" class="ltx_para">
<p id="S5.SS2.SSS5.p1.1" class="ltx_p">Recent solutions predict poses even in the presence of small occlusion, but this remains one of the main challenges in a monocular approach. In most real-world or multi-person scenarios, this needs to be addressed. To this end, many optimizations at training time are employed: data augmentation or occlusion-specific modules (<cite class="ltx_cite ltx_citemacro_cite">Cheng etÂ al. (<a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite>; <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Cheng etÂ al.</span> (<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite>; <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Huang etÂ al.</span> (<a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite>). Another solution could be to consider the 3D scene around the subject. <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Hassan etÂ al.</span> (<a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite> show that combining 3D reconstruction of indoor scene and volumetric models of the human body can help overcome the occlusion issues. Their â€Proximal Relationships with Object eXclusionâ€ method enforces physical constraints such as contacts with a static environment.</p>
</div>
</section>
<section id="S5.SS2.SSS6" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.6 </span>Generalization</h4>

<div id="S5.SS2.SSS6.p1" class="ltx_para">
<p id="S5.SS2.SSS6.p1.1" class="ltx_p">Although neural network models are data-oriented algorithms, few analyses are performed on generalization to new contexts and in-the-wild situations. Protocols for benchmarks address generalization to different subjects, but the background scene and the actions rarely change. Multi-view models generalize best, with good performance in cross-validation between data sets, likely due to the geometric information provided by the camera projection matrices they often use. Yet, new benchmarks are necessary to properly assess generalization. Future research will likely provide carefully designed naturalistic datasets with new labeling solutions, or datasets augmented with new image processing techniques, and possibly benchmarks composed of synthetic poses, to challenge future methods.</p>
</div>
</section>
<section id="S5.SS2.SSS7" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.7 </span>Overall Conclusion on Robustness</h4>

<div id="S5.SS2.SSS7.p1" class="ltx_para">
<p id="S5.SS2.SSS7.p1.1" class="ltx_p">Table <a href="#S5.T7" title="Table 7 â€£ 5.2 Robustness â€£ 5 Analysis and discussion â€£ A review of 3D human pose estimation algorithms for markerless motion capture" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows the less constrained methods and their performance. Robustness relates to the number of assumptions (the fewer the better). For monocular techniques, the multi-person methods trained on complex datasets containing severe occlusions logically impose fewer constraints. For temporal techniques, the ones that do not need future frames for inference perform best. <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Hossain and Little</span> (<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite> achieves maximum accuracy with the fewest images <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Liu etÂ al.</span> (<a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite>. <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">cheng203d</span></cite> address fast motion and occlusions but require a higher acquisition frequency and a wider temporal receptive field to produce better results. The most robust multi-view method is <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Iskakov etÂ al.</span> (<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite> because it does not require any special hardware and can work with only two cameras while achieving acceptable accuracy (see table <a href="#S5.T9" title="Table 9 â€£ 5.2.2 Special Hardware &amp; Calibrated Systems â€£ 5.2 Robustness â€£ 5 Analysis and discussion â€£ A review of 3D human pose estimation algorithms for markerless motion capture" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>). Finally, <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">von Marcard etÂ al.</span> (<a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite> can perform multi-person pose estimation in the wild with mobile cameras. Yet, the low constraints in terms of environment and subject come at the cost of high constraints in terms of additional hardware (IMU and scans).</p>
</div>
<div id="S5.SS2.SSS7.p2" class="ltx_para">
<p id="S5.SS2.SSS7.p2.1" class="ltx_p">Throughout this literature review, we have found that achieving the highest accuracy is the primary concern of novel methods. We also found that system robustness is regularly overlooked when evaluating algorithms and that the complexity of the novel technique is rarely considered, especially for fully engineered systems. Our impression is that the general emphasis on highest accuracy is somewhat misleading, and that the search for â€good enoughâ€ systems depending on the application domain might be at least as important. For example, very high accuracy is mostly irrelevant for surveillance, but robustness and real time operation matters. For healthcare professionals monitoring patient recovery, robustness is paramount, provided that accuracy is enough (i.e., comparable to that of the human eye) but real-time operation only desirable.
<br class="ltx_break"></p>
</div>
</section>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Speed</h3>

<figure id="S5.T10" class="ltx_table">
<div id="S5.T10.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:82.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-174.9pt,66.7pt) scale(0.38268026305952,0.38268026305952) ;">
<table id="S5.T10.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T10.1.1.1.1" class="ltx_tr">
<th id="S5.T10.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" colspan="5">Speed</th>
</tr>
<tr id="S5.T10.1.1.2.2" class="ltx_tr">
<th id="S5.T10.1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">Method</th>
<th id="S5.T10.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Accuracy</th>
<th id="S5.T10.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Robustness Level</th>
<th id="S5.T10.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Speed</th>
<th id="S5.T10.1.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">GPU used</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T10.1.1.3.1" class="ltx_tr">
<th id="S5.T10.1.1.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Mehta etÂ al.</span> (<a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2017</span></a>)</cite></th>
<td id="S5.T10.1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">80.5</td>
<td id="S5.T10.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">average</td>
<td id="S5.T10.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">30fps</td>
<td id="S5.T10.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">N/A</td>
</tr>
<tr id="S5.T10.1.1.4.2" class="ltx_tr">
<th id="S5.T10.1.1.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Wandt and Rosenhahn</span> (<a href="#bib.bib73" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite></th>
<td id="S5.T10.1.1.4.2.2" class="ltx_td ltx_align_center ltx_border_r">50.9</td>
<td id="S5.T10.1.1.4.2.3" class="ltx_td ltx_align_center ltx_border_r">average</td>
<td id="S5.T10.1.1.4.2.4" class="ltx_td ltx_align_center ltx_border_r">20 000fps</td>
<td id="S5.T10.1.1.4.2.5" class="ltx_td ltx_align_center">Nvidia Titan X</td>
</tr>
<tr id="S5.T10.1.1.5.3" class="ltx_tr">
<th id="S5.T10.1.1.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Xu etÂ al.</span> (<a href="#bib.bib76" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite></th>
<td id="S5.T10.1.1.5.3.2" class="ltx_td ltx_align_center ltx_border_r">82.4</td>
<td id="S5.T10.1.1.5.3.3" class="ltx_td ltx_align_center ltx_border_r">high</td>
<td id="S5.T10.1.1.5.3.4" class="ltx_td ltx_align_center ltx_border_r">120fps</td>
<td id="S5.T10.1.1.5.3.5" class="ltx_td ltx_align_center">N/A</td>
</tr>
<tr id="S5.T10.1.1.6.4" class="ltx_tr">
<th id="S5.T10.1.1.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Mathis etÂ al.</span> (<a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite></th>
<td id="S5.T10.1.1.6.4.2" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S5.T10.1.1.6.4.3" class="ltx_td ltx_align_center ltx_border_r">average</td>
<td id="S5.T10.1.1.6.4.4" class="ltx_td ltx_align_center ltx_border_r">30fps</td>
<td id="S5.T10.1.1.6.4.5" class="ltx_td ltx_align_center">Nvidia 1080Ti</td>
</tr>
<tr id="S5.T10.1.1.7.5" class="ltx_tr">
<th id="S5.T10.1.1.7.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Mehta etÂ al.</span> (<a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite></th>
<td id="S5.T10.1.1.7.5.2" class="ltx_td ltx_align_center ltx_border_r">63.6</td>
<td id="S5.T10.1.1.7.5.3" class="ltx_td ltx_align_center ltx_border_r">high</td>
<td id="S5.T10.1.1.7.5.4" class="ltx_td ltx_align_center ltx_border_r">30fps</td>
<td id="S5.T10.1.1.7.5.5" class="ltx_td ltx_align_center">N/A</td>
</tr>
<tr id="S5.T10.1.1.8.6" class="ltx_tr">
<th id="S5.T10.1.1.8.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Hossain and Little</span> (<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite></th>
<td id="S5.T10.1.1.8.6.2" class="ltx_td ltx_align_center ltx_border_r">58.5</td>
<td id="S5.T10.1.1.8.6.3" class="ltx_td ltx_align_center ltx_border_r">high</td>
<td id="S5.T10.1.1.8.6.4" class="ltx_td ltx_align_center ltx_border_r">300fps</td>
<td id="S5.T10.1.1.8.6.5" class="ltx_td ltx_align_center">Nvidia Titan X</td>
</tr>
<tr id="S5.T10.1.1.9.7" class="ltx_tr">
<th id="S5.T10.1.1.9.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Liu etÂ al.</span> (<a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite></th>
<td id="S5.T10.1.1.9.7.2" class="ltx_td ltx_align_center ltx_border_r">45.1</td>
<td id="S5.T10.1.1.9.7.3" class="ltx_td ltx_align_center ltx_border_r">average</td>
<td id="S5.T10.1.1.9.7.4" class="ltx_td ltx_align_center ltx_border_r">3000fps</td>
<td id="S5.T10.1.1.9.7.5" class="ltx_td ltx_align_center">Nvidia Titan RTX</td>
</tr>
<tr id="S5.T10.1.1.10.8" class="ltx_tr">
<th id="S5.T10.1.1.10.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Pavllo etÂ al.</span> (<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite></th>
<td id="S5.T10.1.1.10.8.2" class="ltx_td ltx_align_center ltx_border_r">46.8</td>
<td id="S5.T10.1.1.10.8.3" class="ltx_td ltx_align_center ltx_border_r">average</td>
<td id="S5.T10.1.1.10.8.4" class="ltx_td ltx_align_center ltx_border_r">150 000fps</td>
<td id="S5.T10.1.1.10.8.5" class="ltx_td ltx_align_center">Nvidia GP100</td>
</tr>
<tr id="S5.T10.1.1.11.9" class="ltx_tr">
<th id="S5.T10.1.1.11.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Trumble etÂ al.</span> (<a href="#bib.bib71" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2017</span></a>)</cite></th>
<td id="S5.T10.1.1.11.9.2" class="ltx_td ltx_align_center ltx_border_r">87.3</td>
<td id="S5.T10.1.1.11.9.3" class="ltx_td ltx_align_center ltx_border_r">low</td>
<td id="S5.T10.1.1.11.9.4" class="ltx_td ltx_align_center ltx_border_r">25fps</td>
<td id="S5.T10.1.1.11.9.5" class="ltx_td ltx_align_center">N/A</td>
</tr>
<tr id="S5.T10.1.1.12.10" class="ltx_tr">
<th id="S5.T10.1.1.12.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Huang etÂ al.</span> (<a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite></th>
<td id="S5.T10.1.1.12.10.2" class="ltx_td ltx_align_center ltx_border_r">37.5</td>
<td id="S5.T10.1.1.12.10.3" class="ltx_td ltx_align_center ltx_border_r">low</td>
<td id="S5.T10.1.1.12.10.4" class="ltx_td ltx_align_center ltx_border_r">25fps</td>
<td id="S5.T10.1.1.12.10.5" class="ltx_td ltx_align_center">Nvidia 1080Ti</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 10: </span>Real-time methods. Along with the other criteria for comparison, the speed (in frames inferred per second) and the graphical card used are reported. Note that methods such as <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Hossain and Little</span> (<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite> and <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Wandt and Rosenhahn</span> (<a href="#bib.bib73" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite> are not considering the speed of the 2D detector in the first stage of their techniques when reporting fps.</figcaption>
</figure>
<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">It is easier to evaluate the performance in terms of speed by simply observing the complexity or the number of single operations needed for any one method (using floating-point operation <span id="S5.SS3.p1.1.1" class="ltx_text ltx_font_italic">â€FLOPsâ€</span>). When possible, knowing at what frame rate an inference can be made is also interesting for real-time application such as monitoring, surveillance or virtual reality. In these cases, the main constraint is the whole system latency, which should not exceed specified limits.</p>
</div>
<section id="S5.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.1 </span>Real-time</h4>

<div id="S5.SS3.SSS1.p1" class="ltx_para">
<p id="S5.SS3.SSS1.p1.1" class="ltx_p">Not all methods are complete motion capture systems such as <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Huang etÂ al.</span> (<a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite> or <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Mehta etÂ al.</span> (<a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2017</span></a>)</cite> that report 25 and 30 frames per second for detection. Some only provide the inference timeper frame, without testing in a real life acquisition scenario such as <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Hossain and Little</span> (<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite> and <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Martinez etÂ al.</span> (<a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2017</span></a>)</cite> who each report 3ms per frame. The deepest architectures are often not suitable for real-time applications, as a forward pass in the network takes too long. To gain insight into the complexity of these models, one can look at the number of parameters that they learn.</p>
</div>
</section>
<section id="S5.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.2 </span>Training Time</h4>

<div id="S5.SS3.SSS2.p1" class="ltx_para">
<p id="S5.SS3.SSS2.p1.1" class="ltx_p">Most of the reviewed methods can be trained to adapt to new challenging contexts or refined on new data. It is also important to have an idea of the training time when an application might consider online training. The number of parameters is a good indication of the depth of the architecture (see Table <a href="#S5.T11" title="Table 11 â€£ 5.3.2 Training Time â€£ 5.3 Speed â€£ 5 Analysis and discussion â€£ A review of 3D human pose estimation algorithms for markerless motion capture" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>), which can also be calculated using backbone 2D detection networks in many cases.</p>
</div>
<figure id="S5.T11" class="ltx_table">
<table id="S5.T11.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T11.1.1.1" class="ltx_tr">
<th id="S5.T11.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">Method</th>
<th id="S5.T11.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">#Parameters</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T11.1.2.1" class="ltx_tr">
<th id="S5.T11.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Martinez etÂ al.</span> (<a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2017</span></a>)</cite></th>
<td id="S5.T11.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">4-5M</td>
</tr>
<tr id="S5.T11.1.3.2" class="ltx_tr">
<th id="S5.T11.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">
<cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Sun etÂ al.</span> (<a href="#bib.bib67" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite> HG/Res50/Res101</th>
<td id="S5.T11.1.3.2.2" class="ltx_td ltx_align_center">26M/26M/45M</td>
</tr>
<tr id="S5.T11.1.4.3" class="ltx_tr">
<th id="S5.T11.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Pavllo etÂ al.</span> (<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite></th>
<td id="S5.T11.1.4.3.2" class="ltx_td ltx_align_center">16.95M</td>
</tr>
<tr id="S5.T11.1.5.4" class="ltx_tr">
<th id="S5.T11.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Qiu etÂ al.</span> (<a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite></th>
<td id="S5.T11.1.5.4.2" class="ltx_td ltx_align_center">560M</td>
</tr>
<tr id="S5.T11.1.6.5" class="ltx_tr">
<th id="S5.T11.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">
<cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Iskakov etÂ al.</span> (<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite> alg.</th>
<td id="S5.T11.1.6.5.2" class="ltx_td ltx_align_center">80M</td>
</tr>
<tr id="S5.T11.1.7.6" class="ltx_tr">
<th id="S5.T11.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">
<cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Iskakov etÂ al.</span> (<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite> vol.</th>
<td id="S5.T11.1.7.6.2" class="ltx_td ltx_align_center">81M</td>
</tr>
<tr id="S5.T11.1.8.7" class="ltx_tr">
<th id="S5.T11.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">He etÂ al.</span> (<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite></th>
<td id="S5.T11.1.8.7.2" class="ltx_td ltx_align_center ltx_border_b">69M</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 11: </span>Number of reported parameters of different reviewed techniques.</figcaption>
</figure>
</section>
<section id="S5.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.3 </span>Overall Conclusion on Speed</h4>

<div id="S5.SS3.SSS3.p1" class="ltx_para">
<p id="S5.SS3.SSS3.p1.1" class="ltx_p">In the speed part of table <a href="#S5.T10" title="Table 10 â€£ 5.3 Speed â€£ 5 Analysis and discussion â€£ A review of 3D human pose estimation algorithms for markerless motion capture" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>, we report all methods that can run in real time. The inference speed is in frames per second and the GPU used is also specified. Robustness is variable in this collection of methods, and the accuracy seems a bit below average for the fastest methods. The most accurate is <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Huang etÂ al.</span> (<a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite>, which achieves 37.5 MPJPE on Human3.6M without using its IMU component (it also performs well on TotalCapture with the addition of IMU data).</p>
</div>
</section>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Recommendations for users</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">After this analysis according to the performance criteria, here are our suggestions for the different types of applicationâ€‰:
<br class="ltx_break"></p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p"><span id="S5.SS4.p2.1.1" class="ltx_text ltx_font_italic">Human Computer Interface</span>: for this category of applications, the priority is good accuracy and real-time performance. Robustness depends on whether the system operates in a pre-defined environment or â€in-the-wildâ€. With these specifications, temporal methods that can run in real-time such as <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Pavllo etÂ al.</span> (<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite> or <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Liu etÂ al.</span> (<a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite> correspond best. <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Hossain and Little</span> (<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite> requires fewer frames to be computed but its accuracy is lower. Other more robust methods could be multi-person ones, such as <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Mehta etÂ al.</span> (<a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite>, but control applications usually only interact with one subject. Finally, <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Huang etÂ al.</span> (<a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite> is the most accurate method running in real-time, but it requires multiple views to become robust.
<br class="ltx_break"></p>
</div>
<div id="S5.SS4.p3" class="ltx_para">
<p id="S5.SS4.p3.1" class="ltx_p"><span id="S5.SS4.p3.1.1" class="ltx_text ltx_font_italic">Security</span>: Traditional monitoring systems generally need to be more robust to operate in varied and changing real-world environments and subjects. Speed is also important because infractions need to be identified in real-time. Finally, the higher precision is less important because it is the whole semantics of the movement that is important to identify the subjectâ€™s actions. If you are looking for methods running in real-time with the highest robustness, consider monocular (<cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Xu etÂ al.</span> (<a href="#bib.bib76" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite>) and temporal (<cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Hossain and Little</span> (<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite>) methods. The methods proposed by <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Mehta etÂ al.</span> (<a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite> allows for an even greater robustness with multi-person detection in real-time, if necessary. Less robust methods can also be considered for higher accuracy (e.g. <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Pavllo etÂ al.</span> (<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite> or <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Liu etÂ al.</span> (<a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite>) or speed (e.g. <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Wandt and Rosenhahn</span> (<a href="#bib.bib73" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite>).
<br class="ltx_break"></p>
</div>
<div id="S5.SS4.p4" class="ltx_para">
<p id="S5.SS4.p4.1" class="ltx_p"><span id="S5.SS4.p4.1.1" class="ltx_text ltx_font_italic">Motion Analysis</span>: these applications typically run in lab-controlled environment and computation is performed offline. Accuracy is the most important criteria with less consideration for real-time or high robustness. However, human performance captured in â€real-lifeâ€ scenarios and the need of less intrusive techniques (e.g., medical diagnosis or rehabilitation) demands better robustness than classical marker-based methods (<cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Colyer etÂ al.</span> (<a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2018</span></a>)</cite>). Multi-view configurations now produce results with an average error per joint of less than 30 mm (<cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Iskakov etÂ al.</span> (<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">He etÂ al.</span> (<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2015</span></a>)</cite>). When multiple cameras or calibration are not possible, monocular temporal methods such as <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Kolotouros etÂ al.</span> (<a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite> or <cite class="ltx_cite ltx_citemacro_cite">Cheng etÂ al. (<a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite> can be considered. Finally, if simplicity and usability are required, easy-to-use and flexible systems based on good 2D detectors such as <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Mathis and Mathis</span> (<a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite> or <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Martinez etÂ al.</span> (<a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2017</span></a>)</cite> produce efficient results.
<br class="ltx_break"></p>
</div>
<div id="S5.SS4.p5" class="ltx_para">
<p id="S5.SS4.p5.1" class="ltx_p"><span id="S5.SS4.p5.1.1" class="ltx_text ltx_font_italic">Entertainment</span>: Motion capture for animation or VR is usually done in a controlled environment, so robustness is not the main concern. Accuracy is important because the generated poses and movements must be realistic (see <a href="#S2.SS1" title="2.1 Metrics â€£ 2 Methods Evaluation â€£ A review of 3D human pose estimation algorithms for markerless motion capture" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a> for structural and perceptual metrics). Finally, speed is less important for offline processing that generates avatar or animated characters, whereas real-time is needed for controls in video games or VR. In the first case, multi-view markerless systems can replace classical motion capture systems at a lower cost (e.g. <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Iskakov etÂ al.</span> (<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite> or <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">He etÂ al.</span> (<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2020</span></a>)</cite>). IIn the second case, real-time operation is possible with <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Huang etÂ al.</span> (<a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2019</span></a>)</cite>, which also provides high accuracy in multi-view configurations (and even higher if adding IMUs to the subject is feasible).</p>
</div>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Challenge for future research</h3>

<div id="S5.SS5.p1" class="ltx_para">
<p id="S5.SS5.p1.1" class="ltx_p">Many problems concerning the estimation of the human pose in 3D have still not been solved. The most discussed is that the accuracy is still insufficient for some applications (e.g., in motion analysis). Currently, many different monocular and temporal approaches achieve an average joint position error of about 40 mm, while multi-view approaches achieve about 20-30 mm. This leads to important challenges:</p>
</div>
<div id="S5.SS5.p2" class="ltx_para">
<p id="S5.SS5.p2.1" class="ltx_p">For <span id="S5.SS5.p2.1.1" class="ltx_text ltx_font_italic">monocular</span> techniques, the obstacle to their widespread use is the consistency of their detection over space and time. Difference in accuracy among joints needs to be addressed and temporal consistency (reducing jitter) of motions needs to be better enforced. Reaching 90% of joints accurately detected, as 2D pose estimators do, is a goal for the coming years (the average 3DPCK reaches about 85% with evaluations on MPI-INF-3DHP).</p>
</div>
<div id="S5.SS5.p3" class="ltx_para">
<p id="S5.SS5.p3.1" class="ltx_p">For <span id="S5.SS5.p3.1.1" class="ltx_text ltx_font_italic">multi-view</span> setups, the issue is that they present results close to those of the best monocular methods despite the access to epipolar geometry and 3D information. The combination of recent advances in deep learning (e.g., recurrent network, attention, etc.) and strong prior information about scene structures from camera calibration could take this a step further. Another avenue for applications in more controlled environments could be the use of other modalities such as IMU or depth sensors. Multi-modal information fusion could then lead to even better results.</p>
</div>
<div id="S5.SS5.p4" class="ltx_para">
<p id="S5.SS5.p4.1" class="ltx_p"><span id="S5.SS5.p4.1.1" class="ltx_text ltx_font_italic">Real-time</span> performances are achieved by many methods, but using powerful graphical cards. Porting real-time estimation to the average commercial equipment remains a challenge. Currently, most proposals rely on multi-stage computations, but future research could draw on single-shot object detectors (e.g. <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Liu etÂ al.</span> (<a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2016</span></a>)</cite> (SSD), <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Redmon etÂ al.</span> (<a href="#bib.bib60" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">2016</span></a>)</cite> (YOLO)) to produce reasonable results faster. Similarly, research on real-time 3D human pose estimation with multi-view cameras is developing. Virtual reality could benefit from such systems for gesture-based control.</p>
</div>
<div id="S5.SS5.p5" class="ltx_para">
<p id="S5.SS5.p5.1" class="ltx_p">Finally, there are strong assumptions about occlusions and multi-person configurations that do not yet allow pose estimation to be applied to any video or image. Complex in-the-wild data sets, sometimes including difficult poses (generated or captured by motion), are beginning to emerge, which suggests that these are promising research questions.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Human pose estimation has been one of the focal points of computer vision in recent years. Deep learning has improved the results by a significant amount. However, the most accurate techniques use various architectures (temporal convolutional networks, 3D human body models or learnable triangulation) depending on the input data (single images, videos sequence or multi-view images). What these methods have in common is the use of 2D detection as an intermediate step. However, the diversity of approaches in the new state-of-the art methods demonstrates that a consensus has not yet been established.
<br class="ltx_break"></p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Improved results in the near future will require richer datasets, computational parsimony, and ease of use. Access to new and richer datasets, including a wide variety of poses, movements, and contexts, is essential for robustness. This could be facilitated by new learning processes working with partially annotated data based on monocular videos, but also by commercial markerless tracking tools, which could feed these repositories with better images in â€natural conditionsâ€, without markers and in an outdoor environment. Parsimony in terms of computational cost paves the way for real-time operation, less expensive hardware architectures, and a welcome reduction in power consumption. Ease of use, as was the case with DeepLabCut, is a key to widespread adoption, which in turn challenges the algorithms to solve new questions and use cases.
<br class="ltx_break"></p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">Finally, we believe that important future developments could come from new ways of approaching the full richness of human pose estimation, by reasoning directly on the 3D nature of the problem (using voxel map representations, multi-channel volumesâ€¦), by employing less constrained multi-view approaches that can function with fewer cameras, or by transposing temporal methods (transformers architecture or temporal convolutional networks) from the monocular to the multi-view.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This project was supported by the LabEx NUMEV (ANR-10-LABX-0020) within the I-SITE MUSE.
This research was partially supported by the HUT project co-financed by the European Regional Development Fund (ERDF) and the Occitanie Region.
The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agarwal and Triggs (2006)</span>
<span class="ltx_bibblock">
Agarwal, A., Triggs, B.,
2006.

</span>
<span class="ltx_bibblock">Recovering 3D human pose from monocular images.

</span>
<span class="ltx_bibblock">IEEE Transactions on Pattern Analysis and Machine
Intelligence 28, 44â€“58.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="http://dx.doi.org/10.1109/TPAMI.2006.21" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1109/TPAMI.2006.21</span></a>. conference
Name: IEEE Transactions on Pattern Analysis and Machine Intelligence.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(2)</span>
<span class="ltx_bibblock">
Anguelov, D., Srinivasan, P.,
Koller, D., Thrun, S.,
Davis, J., Rodgers, J., .

</span>
<span class="ltx_bibblock">SCAPE: Shape Completion and Animation of
People , 9.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Belagiannis etÂ al. (2014)</span>
<span class="ltx_bibblock">
Belagiannis, V., Amin, S.,
Andriluka, M., Schiele, B.,
Navab, N., Ilic, S.,
2014.

</span>
<span class="ltx_bibblock">3D Pictorial Structures for Multiple Human
Pose Estimation, in: 2014 IEEE Conference on
Computer Vision and Pattern Recognition, IEEE,
Columbus, OH, USA. pp. 1669â€“1676.

</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6909612" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6909612</a>,
doi:<a target="_blank" href="http://dx.doi.org/10.1109/CVPR.2014.216" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1109/CVPR.2014.216</span></a>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bray (2000)</span>
<span class="ltx_bibblock">
Bray, J., 2000.

</span>
<span class="ltx_bibblock">Markerless Based Human Motion Capture : A
Survey.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Burenius etÂ al. (2013)</span>
<span class="ltx_bibblock">
Burenius, M., Sullivan, J.,
Carlsson, S., 2013.

</span>
<span class="ltx_bibblock">3d pictorial structures for multiple view articulated
pose estimation, in: 2013 IEEE Conference on Computer
Vision and Pattern Recognition, pp. 3618â€“3625.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai etÂ al. (2019)</span>
<span class="ltx_bibblock">
Cai, Y., Ge, L., Liu, J.,
Cai, J., Cham, T.J.,
Yuan, J., Thalmann, N.M.,
2019.

</span>
<span class="ltx_bibblock">Exploiting Spatial-Temporal Relationships for
3D Pose Estimation via Graph Convolutional Networks, in:
2019 IEEE/CVF International Conference on
Computer Vision (ICCV), IEEE,
Seoul, Korea (South). pp. 2272â€“2281.

</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="https://ieeexplore.ieee.org/document/9009459/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ieeexplore.ieee.org/document/9009459/</a>,
doi:<a target="_blank" href="http://dx.doi.org/10.1109/ICCV.2019.00236" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1109/ICCV.2019.00236</span></a>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao etÂ al. (2017)</span>
<span class="ltx_bibblock">
Cao, Z., Simon, T., Wei,
S.E., Sheikh, Y., 2017.

</span>
<span class="ltx_bibblock">Realtime Multi-person 2D Pose Estimation
Using Part Affinity Fields, in: 2017 IEEE
Conference on Computer Vision and Pattern Recognition (CVPR),
pp. 1302â€“1310.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="http://dx.doi.org/10.1109/CVPR.2017.143" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1109/CVPR.2017.143</span></a>. iSSN:
1063-6919.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2020)</span>
<span class="ltx_bibblock">
Chen, Y., Tian, Y., He,
M., 2020.

</span>
<span class="ltx_bibblock">Monocular Human Pose Estimation: A Survey
of Deep Learning-based Methods.

</span>
<span class="ltx_bibblock">Computer Vision and Image Understanding
192, 102897.

</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="http://arxiv.org/abs/2006.01423" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2006.01423</a>,
doi:<a target="_blank" href="http://dx.doi.org/10.1016/j.cviu.2019.102897" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1016/j.cviu.2019.102897</span></a>. arXiv:
2006.01423.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2018)</span>
<span class="ltx_bibblock">
Chen, Y., Wang, Z., Peng,
Y., Zhang, Z., Yu, G.,
Sun, J., 2018.

</span>
<span class="ltx_bibblock">Cascaded Pyramid Network for Multi-Person
Pose Estimation.

</span>
<span class="ltx_bibblock">arXiv:1711.07319 [cs] URL: <a target="_blank" href="http://arxiv.org/abs/1711.07319" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1711.07319</a>. arXiv: 1711.07319.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng etÂ al. (2020)</span>
<span class="ltx_bibblock">
Cheng, Y., Yang, B., Wang,
B., Tan, R.T., 2020.

</span>
<span class="ltx_bibblock">3d human pose estimation using spatio-temporal
networks with explicit occlusion training.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2004.11822" title="" class="ltx_ref ltx_href ltx_font_typewriter">arXiv:2004.11822</a><span id="bib.bib10.1.1" class="ltx_text ltx_font_typewriter">.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib11.5.5.1" class="ltx_text ltx_font_typewriter">Cheng etÂ al. (2019)</span></span>
<span class="ltx_bibblock"><span id="bib.bib11.7.1" class="ltx_text ltx_font_typewriter">
Cheng, Y., Yang, B., Wang,
B., Yan, W., Tan, R.T.,
2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.8.1" class="ltx_text ltx_font_typewriter">Occlusion-Aware Networks for 3D Human Pose
Estimation in Video, pp. 723--732.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.9.1" class="ltx_text ltx_font_typewriter">URL: </span><a target="_blank" href="http://openaccess.thecvf.com/content_ICCV_2019/html/Cheng_Occlusion-Aware_Networks_for_3D_Human_Pose_Estimation_in_Video_ICCV_2019_paper.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://openaccess.thecvf.com/content_ICCV_2019/html/Cheng_Occlusion-Aware_Networks_for_3D_Human_Pose_Estimation_in_Video_ICCV_2019_paper.html</a><span id="bib.bib11.10.2" class="ltx_text ltx_font_typewriter">.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib12.5.5.1" class="ltx_text ltx_font_typewriter">Colyer etÂ al. (2018)</span></span>
<span class="ltx_bibblock"><span id="bib.bib12.7.1" class="ltx_text ltx_font_typewriter">
Colyer, S.L., Evans, M.,
Cosker, D.P., Salo, A.I.T.,
2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.8.1" class="ltx_text ltx_font_typewriter">A Review of the Evolution of Vision-Based
Motion Analysis and the Integration of Advanced Computer Vision
Methods Towards Developing a Markerless System.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.9.1" class="ltx_text ltx_font_typewriter">Sports Medicine - Open 4.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.10.1" class="ltx_text ltx_font_typewriter">URL: </span><a target="_blank" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5986692/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5986692/</a><span id="bib.bib12.11.2" class="ltx_text ltx_font_typewriter">,
doi:</span><a target="_blank" href="http://dx.doi.org/10.1186/s40798-018-0139-y" title="" class="ltx_ref ltx_href ltx_font_typewriter"><span class="ltx_ref ltx_nolink ltx_path">10.1186/s40798-018-0139-y</span></a><span id="bib.bib12.12.3" class="ltx_text ltx_font_typewriter">.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib13.4.4.1" class="ltx_text ltx_font_typewriter">Felzenszwalb and Huttenlocher (2005)</span></span>
<span class="ltx_bibblock"><span id="bib.bib13.6.1" class="ltx_text ltx_font_typewriter">
Felzenszwalb, P.F., Huttenlocher, D.P.,
2005.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.7.1" class="ltx_text ltx_font_typewriter">Pictorial Structures for Object Recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.8.1" class="ltx_text ltx_font_typewriter">International Journal of Computer Vision
61, 55--79.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.9.1" class="ltx_text ltx_font_typewriter">URL: </span><a target="_blank" href="https://doi.org/10.1023/B:VISI.0000042934.15159.49" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1023/B:VISI.0000042934.15159.49</a><span id="bib.bib13.10.2" class="ltx_text ltx_font_typewriter">,
doi:</span><a target="_blank" href="http://dx.doi.org/10.1023/B:VISI.0000042934.15159.49" title="" class="ltx_ref ltx_href ltx_font_typewriter"><span class="ltx_ref ltx_nolink ltx_path">10.1023/B:VISI.0000042934.15159.49</span></a><span id="bib.bib13.11.3" class="ltx_text ltx_font_typewriter">.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib14.5.5.1" class="ltx_text ltx_font_typewriter">Fiker etÂ al. (2020)</span></span>
<span class="ltx_bibblock"><span id="bib.bib14.7.1" class="ltx_text ltx_font_typewriter">
Fiker, R., Kim, L.H.,
Molina, L.A., Chomiak, T.,
Whelan, P.J., 2020.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.8.1" class="ltx_text ltx_font_typewriter">Visual deep lab cut: A user-friendly approach to gait
analysis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.9.1" class="ltx_text ltx_font_typewriter">Journal of Neuroscience Methods ,
108775URL: </span><a target="_blank" href="http://www.sciencedirect.com/science/article/pii/S0165027020301989" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.sciencedirect.com/science/article/pii/S0165027020301989</a><span id="bib.bib14.10.2" class="ltx_text ltx_font_typewriter">,
doi:</span><a target="_blank" href="http://dx.doi.org/https://doi.org/10.1016/j.jneumeth.2020.108775" title="" class="ltx_ref ltx_href ltx_font_typewriter"><span class="ltx_ref ltx_nolink ltx_path">https://doi.org/10.1016/j.jneumeth.2020.108775</span></a><span id="bib.bib14.11.3" class="ltx_text ltx_font_typewriter">.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib15.4.4.1" class="ltx_text ltx_font_typewriter">Fischler and Elschlager (1973)</span></span>
<span class="ltx_bibblock"><span id="bib.bib15.6.1" class="ltx_text ltx_font_typewriter">
Fischler, M., Elschlager, R.,
1973.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.7.1" class="ltx_text ltx_font_typewriter">The Representation and Matching of Pictorial
Structures.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.8.1" class="ltx_text ltx_font_typewriter">IEEE Transactions on Computers
C-22, 67--92.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.9.1" class="ltx_text ltx_font_typewriter">URL: </span><a target="_blank" href="http://ieeexplore.ieee.org/document/1672195/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://ieeexplore.ieee.org/document/1672195/</a><span id="bib.bib15.10.2" class="ltx_text ltx_font_typewriter">,
doi:</span><a target="_blank" href="http://dx.doi.org/10.1109/T-C.1973.223602" title="" class="ltx_ref ltx_href ltx_font_typewriter"><span class="ltx_ref ltx_nolink ltx_path">10.1109/T-C.1973.223602</span></a><span id="bib.bib15.11.3" class="ltx_text ltx_font_typewriter">.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib16.5.5.1" class="ltx_text ltx_font_typewriter">Ghorbani etÂ al. (2020)</span></span>
<span class="ltx_bibblock"><span id="bib.bib16.7.1" class="ltx_text ltx_font_typewriter">
Ghorbani, S., Mahdaviani, K.,
Thaler, A., Kording, K.,
Cook, D.J., Blohm, G.,
Troje, N.F., 2020.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.8.1" class="ltx_text ltx_font_typewriter">MoVi: A Large Multipurpose Motion and
Video Dataset.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.9.1" class="ltx_text ltx_font_typewriter">arXiv:2003.01888 [cs, eess] URL: </span><a target="_blank" href="http://arxiv.org/abs/2003.01888" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2003.01888</a><span id="bib.bib16.10.2" class="ltx_text ltx_font_typewriter">. arXiv: 2003.01888.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib17.5.5.1" class="ltx_text ltx_font_typewriter">Goodfellow etÂ al. (2014)</span></span>
<span class="ltx_bibblock"><span id="bib.bib17.7.1" class="ltx_text ltx_font_typewriter">
Goodfellow, I.J., Pouget-Abadie, J.,
Mirza, M., Xu, B.,
Warde-Farley, D., Ozair, S.,
Courville, A., Bengio, Y.,
2014.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.8.1" class="ltx_text ltx_font_typewriter">Generative Adversarial Networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.9.1" class="ltx_text ltx_font_typewriter">arXiv:1406.2661 [cs, stat] URL: </span><a target="_blank" href="http://arxiv.org/abs/1406.2661" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1406.2661</a><span id="bib.bib17.10.2" class="ltx_text ltx_font_typewriter">. arXiv: 1406.2661.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib18.5.5.1" class="ltx_text ltx_font_typewriter">Grauman etÂ al. (2003)</span></span>
<span class="ltx_bibblock"><span id="bib.bib18.7.1" class="ltx_text ltx_font_typewriter">
Grauman, K., Shakhnarovich, G.,
Darrell, T., 2003.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.8.1" class="ltx_text ltx_font_typewriter">A Bayesian approach to image-based visual hull
reconstruction, in: 2003 IEEE Computer Society
Conference on Computer Vision and Pattern Recognition, 2003.
Proceedings., IEEE Comput. Soc,
Madison, WI, USA. pp. I--187--I--194.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.9.1" class="ltx_text ltx_font_typewriter">URL: </span><a target="_blank" href="http://ieeexplore.ieee.org/document/1211353/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://ieeexplore.ieee.org/document/1211353/</a><span id="bib.bib18.10.2" class="ltx_text ltx_font_typewriter">,
doi:</span><a target="_blank" href="http://dx.doi.org/10.1109/CVPR.2003.1211353" title="" class="ltx_ref ltx_href ltx_font_typewriter"><span class="ltx_ref ltx_nolink ltx_path">10.1109/CVPR.2003.1211353</span></a><span id="bib.bib18.11.3" class="ltx_text ltx_font_typewriter">.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib19.5.5.1" class="ltx_text ltx_font_typewriter">GÃ¼ler etÂ al. (2018)</span></span>
<span class="ltx_bibblock"><span id="bib.bib19.7.1" class="ltx_text ltx_font_typewriter">
GÃ¼ler, R.A., Neverova, N.,
Kokkinos, I., 2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.8.1" class="ltx_text ltx_font_typewriter">DensePose: Dense Human Pose Estimation In
The Wild.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.9.1" class="ltx_text ltx_font_typewriter">arXiv:1802.00434 [cs] URL: </span><a target="_blank" href="http://arxiv.org/abs/1802.00434" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1802.00434</a><span id="bib.bib19.10.2" class="ltx_text ltx_font_typewriter">. arXiv: 1802.00434.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib20.5.5.1" class="ltx_text ltx_font_typewriter">Hassan etÂ al. (2019)</span></span>
<span class="ltx_bibblock"><span id="bib.bib20.7.1" class="ltx_text ltx_font_typewriter">
Hassan, M., Choutas, V.,
Tzionas, D., Black, M.,
2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.8.1" class="ltx_text ltx_font_typewriter">Resolving 3D Human Pose Ambiguities With
3D Scene Constraints, in: 2019 IEEE/CVF
International Conference on Computer Vision (ICCV),
IEEE, Seoul, Korea (South). pp.
2282--2292.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.9.1" class="ltx_text ltx_font_typewriter">URL: </span><a target="_blank" href="https://ieeexplore.ieee.org/document/9010321/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ieeexplore.ieee.org/document/9010321/</a><span id="bib.bib20.10.2" class="ltx_text ltx_font_typewriter">,
doi:</span><a target="_blank" href="http://dx.doi.org/10.1109/ICCV.2019.00237" title="" class="ltx_ref ltx_href ltx_font_typewriter"><span class="ltx_ref ltx_nolink ltx_path">10.1109/ICCV.2019.00237</span></a><span id="bib.bib20.11.3" class="ltx_text ltx_font_typewriter">.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib21.5.5.1" class="ltx_text ltx_font_typewriter">He etÂ al. (2015)</span></span>
<span class="ltx_bibblock"><span id="bib.bib21.7.1" class="ltx_text ltx_font_typewriter">
He, K., Zhang, X., Ren,
S., Sun, J., 2015.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.8.1" class="ltx_text ltx_font_typewriter">Deep Residual Learning for Image
Recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.9.1" class="ltx_text ltx_font_typewriter">arXiv:1512.03385 [cs] URL: </span><a target="_blank" href="http://arxiv.org/abs/1512.03385" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1512.03385</a><span id="bib.bib21.10.2" class="ltx_text ltx_font_typewriter">. arXiv: 1512.03385.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib22.5.5.1" class="ltx_text ltx_font_typewriter">He etÂ al. (2020)</span></span>
<span class="ltx_bibblock"><span id="bib.bib22.7.1" class="ltx_text ltx_font_typewriter">
He, Y., Yan, R.,
Fragkiadaki, K., Yu, S.I.,
2020.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.8.1" class="ltx_text ltx_font_typewriter">Epipolar Transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.9.1" class="ltx_text ltx_font_typewriter">arXiv:2005.04551 [cs] URL: </span><a target="_blank" href="http://arxiv.org/abs/2005.04551" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2005.04551</a><span id="bib.bib22.10.2" class="ltx_text ltx_font_typewriter">. arXiv: 2005.04551.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib23.4.4.1" class="ltx_text ltx_font_typewriter">Hossain and Little (2018)</span></span>
<span class="ltx_bibblock"><span id="bib.bib23.6.1" class="ltx_text ltx_font_typewriter">
Hossain, M.R.I., Little, J.J.,
2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.7.1" class="ltx_text ltx_font_typewriter">Exploiting temporal information for 3D pose
estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.8.1" class="ltx_text ltx_font_typewriter">arXiv:1711.08585 [cs] 11214,
69--86.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.9.1" class="ltx_text ltx_font_typewriter">URL: </span><a target="_blank" href="http://arxiv.org/abs/1711.08585" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1711.08585</a><span id="bib.bib23.10.2" class="ltx_text ltx_font_typewriter">,
doi:</span><a target="_blank" href="http://dx.doi.org/10.1007/978-3-030-01249-6_5" title="" class="ltx_ref ltx_href ltx_font_typewriter"><span class="ltx_ref ltx_nolink ltx_path">10.1007/978-3-030-01249-6_5</span></a><span id="bib.bib23.11.3" class="ltx_text ltx_font_typewriter">. arXiv:
1711.08585.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib24.5.5.1" class="ltx_text ltx_font_typewriter">Huang etÂ al. (2018)</span></span>
<span class="ltx_bibblock"><span id="bib.bib24.7.1" class="ltx_text ltx_font_typewriter">
Huang, C.H., Allain, B.,
Boyer, E., Franco, J.S.,
Tombari, F., Navab, N.,
Ilic, S., 2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.8.1" class="ltx_text ltx_font_typewriter">Tracking-by-Detection of 3D Human Shapes: from
Surfaces to Volumes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.9.1" class="ltx_text ltx_font_typewriter">IEEE Transactions on Pattern Analysis and Machine
Intelligence 40, 1994--2008.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.10.1" class="ltx_text ltx_font_typewriter">URL: </span><a target="_blank" href="https://hal.inria.fr/hal-01588272" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://hal.inria.fr/hal-01588272</a><span id="bib.bib24.11.2" class="ltx_text ltx_font_typewriter">,
doi:</span><a target="_blank" href="http://dx.doi.org/10.1109/TPAMI.2017.2740308" title="" class="ltx_ref ltx_href ltx_font_typewriter"><span class="ltx_ref ltx_nolink ltx_path">10.1109/TPAMI.2017.2740308</span></a><span id="bib.bib24.12.3" class="ltx_text ltx_font_typewriter">.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib25.5.5.1" class="ltx_text ltx_font_typewriter">Huang etÂ al. (2019)</span></span>
<span class="ltx_bibblock"><span id="bib.bib25.7.1" class="ltx_text ltx_font_typewriter">
Huang, F., Zeng, A., Liu,
M., Lai, Q., Xu, Q.,
2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.8.1" class="ltx_text ltx_font_typewriter">DeepFuse: An IMU-Aware Network for
Real-Time 3D Human Pose Estimation from Multi-View Image.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.9.1" class="ltx_text ltx_font_typewriter">arXiv:1912.04071 [cs] URL: </span><a target="_blank" href="http://arxiv.org/abs/1912.04071" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1912.04071</a><span id="bib.bib25.10.2" class="ltx_text ltx_font_typewriter">. arXiv: 1912.04071.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib26.5.5.1" class="ltx_text ltx_font_typewriter">Insafutdinov etÂ al. (2016)</span></span>
<span class="ltx_bibblock"><span id="bib.bib26.7.1" class="ltx_text ltx_font_typewriter">
Insafutdinov, E., Pishchulin, L.,
Andres, B., Andriluka, M.,
Schiele, B., 2016.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.8.1" class="ltx_text ltx_font_typewriter">Deepercut: A deeper, stronger, and faster
multi-person pose estimation model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.9.1" class="ltx_text ltx_font_typewriter">CoRR abs/1605.03170.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.10.1" class="ltx_text ltx_font_typewriter">URL: </span><a target="_blank" href="http://arxiv.org/abs/1605.03170" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1605.03170</a><span id="bib.bib26.11.2" class="ltx_text ltx_font_typewriter">,
</span><a target="_blank" href="http://arxiv.org/abs/1605.03170" title="" class="ltx_ref ltx_href ltx_font_typewriter">arXiv:1605.03170</a><span id="bib.bib26.12.3" class="ltx_text ltx_font_typewriter">.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib27.5.5.1" class="ltx_text ltx_font_typewriter">Ionescu etÂ al. (2014)</span></span>
<span class="ltx_bibblock"><span id="bib.bib27.7.1" class="ltx_text ltx_font_typewriter">
Ionescu, C., Papava, D.,
Olaru, V., Sminchisescu, C.,
2014.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.8.1" class="ltx_text ltx_font_typewriter">Human3.6M: Large Scale Datasets and
Predictive Methods for 3D Human Sensing in Natural
Environments.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.9.1" class="ltx_text ltx_font_typewriter">IEEE Transactions on Pattern Analysis and Machine
Intelligence 36, 1325--1339.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.10.1" class="ltx_text ltx_font_typewriter">URL: </span><a target="_blank" href="http://ieeexplore.ieee.org/document/6682899/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://ieeexplore.ieee.org/document/6682899/</a><span id="bib.bib27.11.2" class="ltx_text ltx_font_typewriter">,
doi:</span><a target="_blank" href="http://dx.doi.org/10.1109/TPAMI.2013.248" title="" class="ltx_ref ltx_href ltx_font_typewriter"><span class="ltx_ref ltx_nolink ltx_path">10.1109/TPAMI.2013.248</span></a><span id="bib.bib27.12.3" class="ltx_text ltx_font_typewriter">.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib28.5.5.1" class="ltx_text ltx_font_typewriter">Iskakov etÂ al. (2019)</span></span>
<span class="ltx_bibblock"><span id="bib.bib28.7.1" class="ltx_text ltx_font_typewriter">
Iskakov, K., Burkov, E.,
Lempitsky, V., Malkov, Y.,
2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.8.1" class="ltx_text ltx_font_typewriter">Learnable Triangulation of Human Pose.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.9.1" class="ltx_text ltx_font_typewriter">arXiv:1905.05754 [cs] URL: </span><a target="_blank" href="http://arxiv.org/abs/1905.05754" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1905.05754</a><span id="bib.bib28.10.2" class="ltx_text ltx_font_typewriter">. arXiv: 1905.05754.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib29.5.5.1" class="ltx_text ltx_font_typewriter">Kanko etÂ al. (2020)</span></span>
<span class="ltx_bibblock"><span id="bib.bib29.7.1" class="ltx_text ltx_font_typewriter">
Kanko, R., Strutzenberger, G.,
Brown, M., Selbie, S.,
Deluzio, K., 2020.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.8.1" class="ltx_text ltx_font_typewriter">Assessment of spatiotemporal gait parameters using a
deep learning algorithm-based markerless motion capture system.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.9.1" class="ltx_text ltx_font_typewriter">preprint. engrXiv.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.10.1" class="ltx_text ltx_font_typewriter">URL: </span><a target="_blank" href="https://osf.io/j4rbg" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://osf.io/j4rbg</a><span id="bib.bib29.11.2" class="ltx_text ltx_font_typewriter">,
doi:</span><a target="_blank" href="http://dx.doi.org/10.31224/osf.io/j4rbg" title="" class="ltx_ref ltx_href ltx_font_typewriter"><span class="ltx_ref ltx_nolink ltx_path">10.31224/osf.io/j4rbg</span></a><span id="bib.bib29.12.3" class="ltx_text ltx_font_typewriter">.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib30.4.4.1" class="ltx_text ltx_font_typewriter">Kipf and Welling (2017)</span></span>
<span class="ltx_bibblock"><span id="bib.bib30.6.1" class="ltx_text ltx_font_typewriter">
Kipf, T.N., Welling, M.,
2017.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.7.1" class="ltx_text ltx_font_typewriter">Semi-Supervised Classification with Graph
Convolutional Networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.8.1" class="ltx_text ltx_font_typewriter">arXiv:1609.02907 [cs, stat] URL: </span><a target="_blank" href="http://arxiv.org/abs/1609.02907" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1609.02907</a><span id="bib.bib30.9.2" class="ltx_text ltx_font_typewriter">. arXiv: 1609.02907.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib31.5.5.1" class="ltx_text ltx_font_typewriter">Kocabas etÂ al. (2019a)</span></span>
<span class="ltx_bibblock"><span id="bib.bib31.7.1" class="ltx_text ltx_font_typewriter">
Kocabas, M., Athanasiou, N.,
Black, M.J., 2019a.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.8.1" class="ltx_text ltx_font_typewriter">VIBE: Video Inference for Human Body Pose
and Shape Estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.9.1" class="ltx_text ltx_font_typewriter">arXiv:1912.05656 [cs] URL: </span><a target="_blank" href="http://arxiv.org/abs/1912.05656" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1912.05656</a><span id="bib.bib31.10.2" class="ltx_text ltx_font_typewriter">. arXiv: 1912.05656.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib32.5.5.1" class="ltx_text ltx_font_typewriter">Kocabas etÂ al. (2019b)</span></span>
<span class="ltx_bibblock"><span id="bib.bib32.7.1" class="ltx_text ltx_font_typewriter">
Kocabas, M., Karagoz, S.,
Akbas, E., 2019b.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.8.1" class="ltx_text ltx_font_typewriter">Self-Supervised Learning of 3D Human Pose
using Multi-view Geometry.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.9.1" class="ltx_text ltx_font_typewriter">arXiv:1903.02330 [cs] URL: </span><a target="_blank" href="http://arxiv.org/abs/1903.02330" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1903.02330</a><span id="bib.bib32.10.2" class="ltx_text ltx_font_typewriter">. arXiv: 1903.02330.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib33.5.5.1" class="ltx_text ltx_font_typewriter">Kolotouros etÂ al. (2019)</span></span>
<span class="ltx_bibblock"><span id="bib.bib33.7.1" class="ltx_text ltx_font_typewriter">
Kolotouros, N., Pavlakos, G.,
Black, M.J., Daniilidis, K.,
2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.8.1" class="ltx_text ltx_font_typewriter">Learning to Reconstruct 3D Human Pose and
Shape via Model-fitting in the Loop.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.9.1" class="ltx_text ltx_font_typewriter">arXiv:1909.12828 [cs] URL: </span><a target="_blank" href="http://arxiv.org/abs/1909.12828" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1909.12828</a><span id="bib.bib33.10.2" class="ltx_text ltx_font_typewriter">. arXiv: 1909.12828.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib34.5.5.1" class="ltx_text ltx_font_typewriter">Kudo etÂ al. (2018)</span></span>
<span class="ltx_bibblock"><span id="bib.bib34.7.1" class="ltx_text ltx_font_typewriter">
Kudo, Y., Ogaki, K.,
Matsui, Y., Odagiri, Y.,
2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.8.1" class="ltx_text ltx_font_typewriter">Unsupervised Adversarial Learning of 3D Human
Pose from 2D Joint Locations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.9.1" class="ltx_text ltx_font_typewriter">arXiv:1803.08244 [cs] URL: </span><a target="_blank" href="http://arxiv.org/abs/1803.08244" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1803.08244</a><span id="bib.bib34.10.2" class="ltx_text ltx_font_typewriter">. arXiv: 1803.08244.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib35.5.5.1" class="ltx_text ltx_font_typewriter">Lin etÂ al. (2016)</span></span>
<span class="ltx_bibblock"><span id="bib.bib35.7.1" class="ltx_text ltx_font_typewriter">
Lin, G., Milan, A., Shen,
C., Reid, I., 2016.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.8.1" class="ltx_text ltx_font_typewriter">RefineNet: Multi-Path Refinement Networks
for High-Resolution Semantic Segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.9.1" class="ltx_text ltx_font_typewriter">arXiv:1611.06612 [cs] URL: </span><a target="_blank" href="http://arxiv.org/abs/1611.06612" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1611.06612</a><span id="bib.bib35.10.2" class="ltx_text ltx_font_typewriter">. arXiv: 1611.06612.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib36.5.5.1" class="ltx_text ltx_font_typewriter">Liu etÂ al. (2020)</span></span>
<span class="ltx_bibblock"><span id="bib.bib36.7.1" class="ltx_text ltx_font_typewriter">
Liu, R., Shen, J., Wang,
H., Chen, C., Cheung, S.c.,
Asari, V., 2020.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.8.1" class="ltx_text ltx_font_typewriter">Attention Mechanism Exploits Temporal
Contexts: Real-Time 3D Human Pose Reconstruction, in:
2020 IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), IEEE,
Seattle, WA, USA. pp. 5063--5072.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.9.1" class="ltx_text ltx_font_typewriter">URL: </span><a target="_blank" href="https://ieeexplore.ieee.org/document/9156272/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ieeexplore.ieee.org/document/9156272/</a><span id="bib.bib36.10.2" class="ltx_text ltx_font_typewriter">,
doi:</span><a target="_blank" href="http://dx.doi.org/10.1109/CVPR42600.2020.00511" title="" class="ltx_ref ltx_href ltx_font_typewriter"><span class="ltx_ref ltx_nolink ltx_path">10.1109/CVPR42600.2020.00511</span></a><span id="bib.bib36.11.3" class="ltx_text ltx_font_typewriter">.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib37.5.5.1" class="ltx_text ltx_font_typewriter">Liu etÂ al. (2016)</span></span>
<span class="ltx_bibblock"><span id="bib.bib37.7.1" class="ltx_text ltx_font_typewriter">
Liu, W., Anguelov, D.,
Erhan, D., Szegedy, C.,
Reed, S., Fu, C.Y.,
Berg, A.C., 2016.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.8.1" class="ltx_text ltx_font_typewriter">SSD: Single Shot MultiBox Detector.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.9.1" class="ltx_text ltx_font_typewriter">arXiv:1512.02325 [cs] 9905,
21--37.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.10.1" class="ltx_text ltx_font_typewriter">URL: </span><a target="_blank" href="http://arxiv.org/abs/1512.02325" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1512.02325</a><span id="bib.bib37.11.2" class="ltx_text ltx_font_typewriter">,
doi:</span><a target="_blank" href="http://dx.doi.org/10.1007/978-3-319-46448-0_2" title="" class="ltx_ref ltx_href ltx_font_typewriter"><span class="ltx_ref ltx_nolink ltx_path">10.1007/978-3-319-46448-0_2</span></a><span id="bib.bib37.12.3" class="ltx_text ltx_font_typewriter">. arXiv:
1512.02325.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib38.5.5.1" class="ltx_text ltx_font_typewriter">Loper etÂ al. (2015)</span></span>
<span class="ltx_bibblock"><span id="bib.bib38.7.1" class="ltx_text ltx_font_typewriter">
Loper, M., Mahmood, N.,
Romero, J., Pons-Moll, G.,
Black, M.J., 2015.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.8.1" class="ltx_text ltx_font_typewriter">SMPL: a skinned multi-person linear model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.9.1" class="ltx_text ltx_font_typewriter">ACM Transactions on Graphics 34,
1--16.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.10.1" class="ltx_text ltx_font_typewriter">URL: </span><a target="_blank" href="https://dl.acm.org/doi/10.1145/2816795.2818013" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://dl.acm.org/doi/10.1145/2816795.2818013</a><span id="bib.bib38.11.2" class="ltx_text ltx_font_typewriter">,
doi:</span><a target="_blank" href="http://dx.doi.org/10.1145/2816795.2818013" title="" class="ltx_ref ltx_href ltx_font_typewriter"><span class="ltx_ref ltx_nolink ltx_path">10.1145/2816795.2818013</span></a><span id="bib.bib38.12.3" class="ltx_text ltx_font_typewriter">.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib39.5.5.1" class="ltx_text ltx_font_typewriter">Mahmood etÂ al. (2019)</span></span>
<span class="ltx_bibblock"><span id="bib.bib39.7.1" class="ltx_text ltx_font_typewriter">
Mahmood, N., Ghorbani, N.,
Troje, N.F., Pons-Moll, G.,
Black, M.J., 2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.8.1" class="ltx_text ltx_font_typewriter">Amass: Archive of motion capture as surface shapes,
in: The IEEE International Conference on Computer Vision
(ICCV).
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.9.1" class="ltx_text ltx_font_typewriter">URL: </span><a target="_blank" href="https://amass.is.tue.mpg.de" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://amass.is.tue.mpg.de</a><span id="bib.bib39.10.2" class="ltx_text ltx_font_typewriter">.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib40.5.5.1" class="ltx_text ltx_font_typewriter">von Marcard etÂ al. (2018)</span></span>
<span class="ltx_bibblock"><span id="bib.bib40.7.1" class="ltx_text ltx_font_typewriter">
von Marcard, T., Henschel, R.,
Black, M.J., Rosenhahn, B.,
Pons-Moll, G., 2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.8.1" class="ltx_text ltx_font_typewriter">Recovering Accurate 3D Human Pose in the
Wild Using IMUs and a Moving Camera, in: Ferrari,
V., Hebert, M., Sminchisescu, C.,
Weiss, Y. (Eds.), Computer Vision â€“
ECCV 2018. Springer International Publishing,
Cham. volume 11214, pp.
614--631.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.9.1" class="ltx_text ltx_font_typewriter">URL: </span><a target="_blank" href="http://link.springer.com/10.1007/978-3-030-01249-6_37" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://link.springer.com/10.1007/978-3-030-01249-6_37</a><span id="bib.bib40.10.2" class="ltx_text ltx_font_typewriter">,
doi:</span><a target="_blank" href="http://dx.doi.org/10.1007/978-3-030-01249-6_37" title="" class="ltx_ref ltx_href ltx_font_typewriter"><span class="ltx_ref ltx_nolink ltx_path">10.1007/978-3-030-01249-6_37</span></a><span id="bib.bib40.11.3" class="ltx_text ltx_font_typewriter">. series Title:
Lecture Notes in Computer Science.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib41.5.5.1" class="ltx_text ltx_font_typewriter">von Marcard etÂ al. (2016)</span></span>
<span class="ltx_bibblock"><span id="bib.bib41.7.1" class="ltx_text ltx_font_typewriter">
von Marcard, T., Pons-Moll, G.,
Rosenhahn, B., 2016.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.8.1" class="ltx_text ltx_font_typewriter">Human pose estimation from video and imus.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.9.1" class="ltx_text ltx_font_typewriter">Transactions on Pattern Analysis and Machine
Intelligence 38, 1533--1547.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.10.1" class="ltx_text ltx_font_typewriter">URL: </span><a target="_blank" href="http://dx.doi.org/10.1109/TPAMI.2016.2522398" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://dx.doi.org/10.1109/TPAMI.2016.2522398</a><span id="bib.bib41.11.2" class="ltx_text ltx_font_typewriter">,
doi:</span><a target="_blank" href="http://dx.doi.org/10.1109/TPAMI.2016.2522398" title="" class="ltx_ref ltx_href ltx_font_typewriter"><span class="ltx_ref ltx_nolink ltx_path">10.1109/TPAMI.2016.2522398</span></a><span id="bib.bib41.12.3" class="ltx_text ltx_font_typewriter">.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib42.5.5.1" class="ltx_text ltx_font_typewriter">Marinoiu etÂ al. (2013)</span></span>
<span class="ltx_bibblock"><span id="bib.bib42.7.1" class="ltx_text ltx_font_typewriter">
Marinoiu, E., Papava, D.,
Sminchisescu, C., 2013.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.8.1" class="ltx_text ltx_font_typewriter">Pictorial human spaces: How well do humans perceive a
3d articulated pose?, in: 2013 IEEE International
Conference on Computer Vision, pp. 1289--1296.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib43.5.5.1" class="ltx_text ltx_font_typewriter">Marinoiu etÂ al. (2016)</span></span>
<span class="ltx_bibblock"><span id="bib.bib43.7.1" class="ltx_text ltx_font_typewriter">
Marinoiu, E., Papava, D.,
Sminchisescu, C., 2016.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.8.1" class="ltx_text ltx_font_typewriter">Pictorial human spaces: A computational study on the
human perception of 3d articulated poses.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.9.1" class="ltx_text ltx_font_typewriter">International Journal of Computer Vision
119, 194â€“215.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.10.1" class="ltx_text ltx_font_typewriter">URL: </span><a target="_blank" href="http://dx.doi.org/10.1007/s11263-016-0888-3" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://dx.doi.org/10.1007/s11263-016-0888-3</a><span id="bib.bib43.11.2" class="ltx_text ltx_font_typewriter">,
doi:</span><a target="_blank" href="http://dx.doi.org/10.1007/s11263-016-0888-3" title="" class="ltx_ref ltx_href ltx_font_typewriter"><span class="ltx_ref ltx_nolink ltx_path">10.1007/s11263-016-0888-3</span></a><span id="bib.bib43.12.3" class="ltx_text ltx_font_typewriter">.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib44.5.5.1" class="ltx_text ltx_font_typewriter">Martinez etÂ al. (2017)</span></span>
<span class="ltx_bibblock"><span id="bib.bib44.7.1" class="ltx_text ltx_font_typewriter">
Martinez, J., Hossain, R.,
Romero, J., Little, J.J.,
2017.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.8.1" class="ltx_text ltx_font_typewriter">A simple yet effective baseline for 3d human pose
estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.9.1" class="ltx_text ltx_font_typewriter">arXiv:1705.03098 [cs] URL: </span><a target="_blank" href="http://arxiv.org/abs/1705.03098" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1705.03098</a><span id="bib.bib44.10.2" class="ltx_text ltx_font_typewriter">. arXiv: 1705.03098.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib45.5.5.1" class="ltx_text ltx_font_typewriter">Mathis etÂ al. (2018)</span></span>
<span class="ltx_bibblock"><span id="bib.bib45.7.1" class="ltx_text ltx_font_typewriter">
Mathis, A., Mamidanna, P.,
Cury, K.M., Abe, T.,
Murthy, V.N., Mathis, M.W.,
Bethge, M., 2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.8.1" class="ltx_text ltx_font_typewriter">DeepLabCut: markerless pose estimation of
user-defined body parts with deep learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.9.1" class="ltx_text ltx_font_typewriter">Nature Neuroscience 21,
1281--1289.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.10.1" class="ltx_text ltx_font_typewriter">URL: </span><a target="_blank" href="https://www.nature.com/articles/s41593-018-0209-y" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.nature.com/articles/s41593-018-0209-y</a><span id="bib.bib45.11.2" class="ltx_text ltx_font_typewriter">,
doi:</span><a target="_blank" href="http://dx.doi.org/10.1038/s41593-018-0209-y" title="" class="ltx_ref ltx_href ltx_font_typewriter"><span class="ltx_ref ltx_nolink ltx_path">10.1038/s41593-018-0209-y</span></a><span id="bib.bib45.12.3" class="ltx_text ltx_font_typewriter">. number: 9
Publisher: Nature Publishing Group.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib46.4.4.1" class="ltx_text ltx_font_typewriter">Mathis and Mathis (2019)</span></span>
<span class="ltx_bibblock"><span id="bib.bib46.6.1" class="ltx_text ltx_font_typewriter">
Mathis, M.W., Mathis, A.,
2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.7.1" class="ltx_text ltx_font_typewriter">Deep learning tools for the measurement of animal
behavior in neuroscience.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.8.1" class="ltx_text ltx_font_typewriter">arXiv:1909.13868 [cs, q-bio] URL: </span><a target="_blank" href="http://arxiv.org/abs/1909.13868" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1909.13868</a><span id="bib.bib46.9.2" class="ltx_text ltx_font_typewriter">. arXiv: 1909.13868.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib47.5.5.1" class="ltx_text ltx_font_typewriter">Mehta etÂ al. (2016)</span></span>
<span class="ltx_bibblock"><span id="bib.bib47.7.1" class="ltx_text ltx_font_typewriter">
Mehta, D., Rhodin, H.,
Casas, D., Fua, P.,
Sotnychenko, O., Xu, W.,
Theobalt, C., 2016.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.8.1" class="ltx_text ltx_font_typewriter">[1611.09813] Monocular 3D Human Pose
Estimation In The Wild Using Improved CNN Supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.9.1" class="ltx_text ltx_font_typewriter">URL: </span><a target="_blank" href="https://arxiv.org/abs/1611.09813" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/1611.09813</a><span id="bib.bib47.10.2" class="ltx_text ltx_font_typewriter">.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib48.5.5.1" class="ltx_text ltx_font_typewriter">Mehta etÂ al. (2020)</span></span>
<span class="ltx_bibblock"><span id="bib.bib48.7.1" class="ltx_text ltx_font_typewriter">
Mehta, D., Sotnychenko, O.,
Mueller, F., Xu, W.,
Elgharib, M., Fua, P.,
Seidel, H.P., Rhodin, H.,
Pons-Moll, G., Theobalt, C.,
2020.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.8.1" class="ltx_text ltx_font_typewriter">XNect: Real-time Multi-Person 3D Motion
Capture with a Single RGB Camera.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.9.1" class="ltx_text ltx_font_typewriter">ACM Transactions on Graphics 39.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.10.1" class="ltx_text ltx_font_typewriter">URL: </span><a target="_blank" href="http://arxiv.org/abs/1907.00837" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1907.00837</a><span id="bib.bib48.11.2" class="ltx_text ltx_font_typewriter">,
doi:</span><a target="_blank" href="http://dx.doi.org/10.1145/3386569.3392410" title="" class="ltx_ref ltx_href ltx_font_typewriter"><span class="ltx_ref ltx_nolink ltx_path">10.1145/3386569.3392410</span></a><span id="bib.bib48.12.3" class="ltx_text ltx_font_typewriter">. arXiv: 1907.00837.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib49.5.5.1" class="ltx_text ltx_font_typewriter">Mehta etÂ al. (2018)</span></span>
<span class="ltx_bibblock"><span id="bib.bib49.7.1" class="ltx_text ltx_font_typewriter">
Mehta, D., Sotnychenko, O.,
Mueller, F., Xu, W.,
Sridhar, S., Pons-Moll, G.,
Theobalt, C., 2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.8.1" class="ltx_text ltx_font_typewriter">Single-Shot Multi-Person 3D Pose
Estimation From Monocular RGB.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.9.1" class="ltx_text ltx_font_typewriter">arXiv:1712.03453 [cs] URL: </span><a target="_blank" href="http://arxiv.org/abs/1712.03453" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1712.03453</a><span id="bib.bib49.10.2" class="ltx_text ltx_font_typewriter">. arXiv: 1712.03453.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib50.5.5.1" class="ltx_text ltx_font_typewriter">Mehta etÂ al. (2017)</span></span>
<span class="ltx_bibblock"><span id="bib.bib50.7.1" class="ltx_text ltx_font_typewriter">
Mehta, D., Sridhar, S.,
Sotnychenko, O., Rhodin, H.,
Shafiei, M., Seidel, H.P.,
Xu, W., Casas, D.,
Theobalt, C., 2017.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.8.1" class="ltx_text ltx_font_typewriter">VNect: real-time 3D human pose estimation with a
single RGB camera.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.9.1" class="ltx_text ltx_font_typewriter">ACM Transactions on Graphics 36,
1--14.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.10.1" class="ltx_text ltx_font_typewriter">URL: </span><a target="_blank" href="http://dl.acm.org/citation.cfm?doid=3072959.3073596" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://dl.acm.org/citation.cfm?doid=3072959.3073596</a><span id="bib.bib50.11.2" class="ltx_text ltx_font_typewriter">,
doi:</span><a target="_blank" href="http://dx.doi.org/10.1145/3072959.3073596" title="" class="ltx_ref ltx_href ltx_font_typewriter"><span class="ltx_ref ltx_nolink ltx_path">10.1145/3072959.3073596</span></a><span id="bib.bib50.12.3" class="ltx_text ltx_font_typewriter">.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib51.4.4.1" class="ltx_text ltx_font_typewriter">Moeslund and Granum (2001)</span></span>
<span class="ltx_bibblock"><span id="bib.bib51.6.1" class="ltx_text ltx_font_typewriter">
Moeslund, T.B., Granum, E.,
2001.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.7.1" class="ltx_text ltx_font_typewriter">A Survey of Computer Vision-Based Human
Motion Capture.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.8.1" class="ltx_text ltx_font_typewriter">Computer Vision and Image Understanding
81, 231--268.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.9.1" class="ltx_text ltx_font_typewriter">URL: </span><a target="_blank" href="http://www.sciencedirect.com/science/article/pii/S107731420090897X" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.sciencedirect.com/science/article/pii/S107731420090897X</a><span id="bib.bib51.10.2" class="ltx_text ltx_font_typewriter">,
doi:</span><a target="_blank" href="http://dx.doi.org/10.1006/cviu.2000.0897" title="" class="ltx_ref ltx_href ltx_font_typewriter"><span class="ltx_ref ltx_nolink ltx_path">10.1006/cviu.2000.0897</span></a><span id="bib.bib51.11.3" class="ltx_text ltx_font_typewriter">.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib52.5.5.1" class="ltx_text ltx_font_typewriter">Moro etÂ al. (2020)</span></span>
<span class="ltx_bibblock"><span id="bib.bib52.7.1" class="ltx_text ltx_font_typewriter">
Moro, M., Marchesi, G.,
Odone, F., Casadio, M.,
2020.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.8.1" class="ltx_text ltx_font_typewriter">Markerless gait analysis in stroke survivors based on
computer vision and deep learning: A pilot study, in:
Proceedings of the 35th Annual ACM Symposium on Applied
Computing, Association for Computing Machinery,
New York, NY, USA. p. 2097â€“2104.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.9.1" class="ltx_text ltx_font_typewriter">URL: </span><a target="_blank" href="https://doi.org/10.1145/3341105.3373963" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3341105.3373963</a><span id="bib.bib52.10.2" class="ltx_text ltx_font_typewriter">,
doi:</span><a target="_blank" href="http://dx.doi.org/10.1145/3341105.3373963" title="" class="ltx_ref ltx_href ltx_font_typewriter"><span class="ltx_ref ltx_nolink ltx_path">10.1145/3341105.3373963</span></a><span id="bib.bib52.11.3" class="ltx_text ltx_font_typewriter">.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib53.5.5.1" class="ltx_text ltx_font_typewriter">Nath etÂ al. (2018)</span></span>
<span class="ltx_bibblock"><span id="bib.bib53.7.1" class="ltx_text ltx_font_typewriter">
Nath, T., Mathis, A.,
Chen, A.C., Patel, A.,
Bethge, M., Mathis, M.W.,
2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.8.1" class="ltx_text ltx_font_typewriter">Using DeepLabCut for 3D markerless pose
estimation across species and behaviors.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.9.1" class="ltx_text ltx_font_typewriter">preprint. Neuroscience.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.10.1" class="ltx_text ltx_font_typewriter">URL: </span><a target="_blank" href="http://biorxiv.org/lookup/doi/10.1101/476531" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://biorxiv.org/lookup/doi/10.1101/476531</a><span id="bib.bib53.11.2" class="ltx_text ltx_font_typewriter">,
doi:</span><a target="_blank" href="http://dx.doi.org/10.1101/476531" title="" class="ltx_ref ltx_href ltx_font_typewriter"><span class="ltx_ref ltx_nolink ltx_path">10.1101/476531</span></a><span id="bib.bib53.12.3" class="ltx_text ltx_font_typewriter">.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib54.5.5.1" class="ltx_text ltx_font_typewriter">Newell etÂ al. (2016)</span></span>
<span class="ltx_bibblock"><span id="bib.bib54.7.1" class="ltx_text ltx_font_typewriter">
Newell, A., Yang, K.,
Deng, J., 2016.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.8.1" class="ltx_text ltx_font_typewriter">Stacked Hourglass Networks for Human Pose
Estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.9.1" class="ltx_text ltx_font_typewriter">arXiv:1603.06937 [cs] URL: </span><a target="_blank" href="http://arxiv.org/abs/1603.06937" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1603.06937</a><span id="bib.bib54.10.2" class="ltx_text ltx_font_typewriter">. arXiv: 1603.06937.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib55.3.3.1" class="ltx_text ltx_font_typewriter">(55)</span></span>
<span class="ltx_bibblock"><span id="bib.bib55.5.1" class="ltx_text ltx_font_typewriter">
Olah, C., .
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.6.1" class="ltx_text ltx_font_typewriter">Understanding LSTM Networks -- colahâ€™s blog.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.7.1" class="ltx_text ltx_font_typewriter">URL: </span><a target="_blank" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a><span id="bib.bib55.8.2" class="ltx_text ltx_font_typewriter">.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib56.5.5.1" class="ltx_text ltx_font_typewriter">Omran etÂ al. (2018)</span></span>
<span class="ltx_bibblock"><span id="bib.bib56.7.1" class="ltx_text ltx_font_typewriter">
Omran, M., Lassner, C.,
Pons-Moll, G., Gehler, P.V.,
Schiele, B., 2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.8.1" class="ltx_text ltx_font_typewriter">Neural Body Fitting: Unifying Deep Learning
and Model-Based Human Pose and Shape Estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.9.1" class="ltx_text ltx_font_typewriter">arXiv:1808.05942 [cs] URL: </span><a target="_blank" href="http://arxiv.org/abs/1808.05942" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1808.05942</a><span id="bib.bib56.10.2" class="ltx_text ltx_font_typewriter">. arXiv: 1808.05942.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib57.5.5.1" class="ltx_text ltx_font_typewriter">Pavlakos etÂ al. (2017)</span></span>
<span class="ltx_bibblock"><span id="bib.bib57.7.1" class="ltx_text ltx_font_typewriter">
Pavlakos, G., Zhou, X.,
Derpanis, K.G., Daniilidis, K.,
2017.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.8.1" class="ltx_text ltx_font_typewriter">Coarse-to-Fine Volumetric Prediction for
Single-Image 3D Human Pose.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.9.1" class="ltx_text ltx_font_typewriter">arXiv:1611.07828 [cs] URL: </span><a target="_blank" href="http://arxiv.org/abs/1611.07828" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1611.07828</a><span id="bib.bib57.10.2" class="ltx_text ltx_font_typewriter">. arXiv: 1611.07828.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib58.5.5.1" class="ltx_text ltx_font_typewriter">Pavllo etÂ al. (2019)</span></span>
<span class="ltx_bibblock"><span id="bib.bib58.7.1" class="ltx_text ltx_font_typewriter">
Pavllo, D., Feichtenhofer, C.,
Grangier, D., Auli, M.,
2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.8.1" class="ltx_text ltx_font_typewriter">3D Human Pose Estimation in Video With
Temporal Convolutions and Semi-Supervised Training, in:
2019 IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), IEEE,
Long Beach, CA, USA. pp. 7745--7754.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.9.1" class="ltx_text ltx_font_typewriter">URL: </span><a target="_blank" href="https://ieeexplore.ieee.org/document/8954163/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ieeexplore.ieee.org/document/8954163/</a><span id="bib.bib58.10.2" class="ltx_text ltx_font_typewriter">,
doi:</span><a target="_blank" href="http://dx.doi.org/10.1109/CVPR.2019.00794" title="" class="ltx_ref ltx_href ltx_font_typewriter"><span class="ltx_ref ltx_nolink ltx_path">10.1109/CVPR.2019.00794</span></a><span id="bib.bib58.11.3" class="ltx_text ltx_font_typewriter">.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib59.5.5.1" class="ltx_text ltx_font_typewriter">Qiu etÂ al. (2019)</span></span>
<span class="ltx_bibblock"><span id="bib.bib59.7.1" class="ltx_text ltx_font_typewriter">
Qiu, H., Wang, C., Wang,
J., Wang, N., Zeng, W.,
2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.8.1" class="ltx_text ltx_font_typewriter">Cross View Fusion for 3D Human Pose
Estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.9.1" class="ltx_text ltx_font_typewriter">arXiv:1909.01203 [cs] URL: </span><a target="_blank" href="http://arxiv.org/abs/1909.01203" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1909.01203</a><span id="bib.bib59.10.2" class="ltx_text ltx_font_typewriter">. arXiv: 1909.01203.
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib60.5.5.1" class="ltx_text ltx_font_typewriter">Redmon etÂ al. (2016)</span></span>
<span class="ltx_bibblock"><span id="bib.bib60.7.1" class="ltx_text ltx_font_typewriter">
Redmon, J., Divvala, S.,
Girshick, R., Farhadi, A.,
2016.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.8.1" class="ltx_text ltx_font_typewriter">You Only Look Once: Unified, Real-Time
Object Detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.9.1" class="ltx_text ltx_font_typewriter">arXiv:1506.02640 [cs] URL: </span><a target="_blank" href="http://arxiv.org/abs/1506.02640" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1506.02640</a><span id="bib.bib60.10.2" class="ltx_text ltx_font_typewriter">. arXiv: 1506.02640.
</span>
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib61.5.5.1" class="ltx_text ltx_font_typewriter">Ronneberger etÂ al. (2015)</span></span>
<span class="ltx_bibblock"><span id="bib.bib61.7.1" class="ltx_text ltx_font_typewriter">
Ronneberger, O., Fischer, P.,
Brox, T., 2015.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.8.1" class="ltx_text ltx_font_typewriter">U-Net: Convolutional Networks for Biomedical
Image Segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.9.1" class="ltx_text ltx_font_typewriter">arXiv:1505.04597 [cs] URL: </span><a target="_blank" href="http://arxiv.org/abs/1505.04597" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1505.04597</a><span id="bib.bib61.10.2" class="ltx_text ltx_font_typewriter">. arXiv: 1505.04597.
</span>
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib62.5.5.1" class="ltx_text ltx_font_typewriter">Sarafinos etÂ al. (2016)</span></span>
<span class="ltx_bibblock"><span id="bib.bib62.7.1" class="ltx_text ltx_font_typewriter">
Sarafinos, Boteanu,
Ionescu, Kakadiaris,
2016.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.8.1" class="ltx_text ltx_font_typewriter">(PDF) 3D Human Pose Estimation: A
Review of the Literature and Analysis of Covariates.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.9.1" class="ltx_text ltx_font_typewriter">URL: </span><a target="_blank" href="https://www.researchgate.net/publication/307905073_3D_Human_Pose_Estimation_A_Review_of_the_Literature_and_Analysis_of_Covariates" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.researchgate.net/publication/307905073_3D_Human_Pose_Estimation_A_Review_of_the_Literature_and_Analysis_of_Covariates</a><span id="bib.bib62.10.2" class="ltx_text ltx_font_typewriter">.
library Catalog: www.researchgate.net.
</span>
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib63.5.5.1" class="ltx_text ltx_font_typewriter">Sheshadri etÂ al. (2020)</span></span>
<span class="ltx_bibblock"><span id="bib.bib63.7.1" class="ltx_text ltx_font_typewriter">
Sheshadri, S., Dann, B.,
Hueser, T., Scherberger, H.,
2020.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.8.1" class="ltx_text ltx_font_typewriter">3D reconstruction toolbox for behavior tracked with
multiple cameras.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.9.1" class="ltx_text ltx_font_typewriter">Journal of Open Source Software
5, 1849.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.10.1" class="ltx_text ltx_font_typewriter">URL: </span><a target="_blank" href="https://joss.theoj.org/papers/10.21105/joss.01849" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://joss.theoj.org/papers/10.21105/joss.01849</a><span id="bib.bib63.11.2" class="ltx_text ltx_font_typewriter">,
doi:</span><a target="_blank" href="http://dx.doi.org/10.21105/joss.01849" title="" class="ltx_ref ltx_href ltx_font_typewriter"><span class="ltx_ref ltx_nolink ltx_path">10.21105/joss.01849</span></a><span id="bib.bib63.12.3" class="ltx_text ltx_font_typewriter">.
</span>
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib64.5.5.1" class="ltx_text ltx_font_typewriter">Sigal etÂ al. (2010)</span></span>
<span class="ltx_bibblock"><span id="bib.bib64.7.1" class="ltx_text ltx_font_typewriter">
Sigal, L., Balan, A.O.,
Black, M.J., 2010.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.8.1" class="ltx_text ltx_font_typewriter">HumanEva: Synchronized Video and Motion
Capture Dataset and Baseline Algorithm for Evaluation of
Articulated Human Motion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.9.1" class="ltx_text ltx_font_typewriter">International Journal of Computer Vision
87, 4--27.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.10.1" class="ltx_text ltx_font_typewriter">URL: </span><a target="_blank" href="http://link.springer.com/10.1007/s11263-009-0273-6" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://link.springer.com/10.1007/s11263-009-0273-6</a><span id="bib.bib64.11.2" class="ltx_text ltx_font_typewriter">,
doi:</span><a target="_blank" href="http://dx.doi.org/10.1007/s11263-009-0273-6" title="" class="ltx_ref ltx_href ltx_font_typewriter"><span class="ltx_ref ltx_nolink ltx_path">10.1007/s11263-009-0273-6</span></a><span id="bib.bib64.12.3" class="ltx_text ltx_font_typewriter">.
</span>
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib65.5.5.1" class="ltx_text ltx_font_typewriter">Sun etÂ al. (2019)</span></span>
<span class="ltx_bibblock"><span id="bib.bib65.7.1" class="ltx_text ltx_font_typewriter">
Sun, K., Xiao, B., Liu,
D., Wang, J., 2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.8.1" class="ltx_text ltx_font_typewriter">Deep high-resolution representation learning for
human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1902.09212" title="" class="ltx_ref ltx_href ltx_font_typewriter">arXiv:1902.09212</a><span id="bib.bib65.9.1" class="ltx_text ltx_font_typewriter">.
</span>
</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib66.5.5.1" class="ltx_text ltx_font_typewriter">Sun etÂ al. (2017)</span></span>
<span class="ltx_bibblock"><span id="bib.bib66.7.1" class="ltx_text ltx_font_typewriter">
Sun, X., Shang, J., Liang,
S., Wei, Y., 2017.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.8.1" class="ltx_text ltx_font_typewriter">Compositional Human Pose Regression.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.9.1" class="ltx_text ltx_font_typewriter">arXiv:1704.00159 [cs] URL: </span><a target="_blank" href="http://arxiv.org/abs/1704.00159" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1704.00159</a><span id="bib.bib66.10.2" class="ltx_text ltx_font_typewriter">. arXiv: 1704.00159.
</span>
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib67.5.5.1" class="ltx_text ltx_font_typewriter">Sun etÂ al. (2018)</span></span>
<span class="ltx_bibblock"><span id="bib.bib67.7.1" class="ltx_text ltx_font_typewriter">
Sun, X., Xiao, B., Wei,
F., Liang, S., Wei, Y.,
2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.8.1" class="ltx_text ltx_font_typewriter">Integral Human Pose Regression.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.9.1" class="ltx_text ltx_font_typewriter">arXiv:1711.08229 [cs] URL: </span><a target="_blank" href="http://arxiv.org/abs/1711.08229" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1711.08229</a><span id="bib.bib67.10.2" class="ltx_text ltx_font_typewriter">. arXiv: 1711.08229.
</span>
</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib68.5.5.1" class="ltx_text ltx_font_typewriter">Tompson etÂ al. (2014)</span></span>
<span class="ltx_bibblock"><span id="bib.bib68.7.1" class="ltx_text ltx_font_typewriter">
Tompson, J., Jain, A.,
LeCun, Y., Bregler, C.,
2014.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.8.1" class="ltx_text ltx_font_typewriter">Joint Training of a Convolutional Network and a
Graphical Model for Human Pose Estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.9.1" class="ltx_text ltx_font_typewriter">arXiv:1406.2984 [cs] URL: </span><a target="_blank" href="http://arxiv.org/abs/1406.2984" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1406.2984</a><span id="bib.bib68.10.2" class="ltx_text ltx_font_typewriter">. arXiv: 1406.2984.
</span>
</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib69.5.5.1" class="ltx_text ltx_font_typewriter">DeÂ la Torre etÂ al. (2008)</span></span>
<span class="ltx_bibblock"><span id="bib.bib69.7.1" class="ltx_text ltx_font_typewriter">
DeÂ la Torre, F., Hodgins, J.,
Bargteil, A., Martin, X.,
Macey, J., Collado, A.,
Beltran, P., 2008.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.8.1" class="ltx_text ltx_font_typewriter">Guide to the Carnegie Mellon University
Multimodal Activity (CMU-MMAC) Database.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.9.1" class="ltx_text ltx_font_typewriter">URL: </span><a target="_blank" href="https://www.ri.cmu.edu/publications/guide-to-the-carnegie-mellon-university-multimodal-activity-cmu-mmac-database/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.ri.cmu.edu/publications/guide-to-the-carnegie-mellon-university-multimodal-activity-cmu-mmac-database/</a><span id="bib.bib69.10.2" class="ltx_text ltx_font_typewriter">.
library Catalog: www.ri.cmu.edu.
</span>
</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib70.4.4.1" class="ltx_text ltx_font_typewriter">Toshev and Szegedy (2014)</span></span>
<span class="ltx_bibblock"><span id="bib.bib70.6.1" class="ltx_text ltx_font_typewriter">
Toshev, A., Szegedy, C.,
2014.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.7.1" class="ltx_text ltx_font_typewriter">DeepPose: Human Pose Estimation via Deep
Neural Networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.8.1" class="ltx_text ltx_font_typewriter">2014 IEEE Conference on Computer Vision and Pattern
Recognition , 1653--1660URL: </span><a target="_blank" href="http://arxiv.org/abs/1312.4659" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1312.4659</a><span id="bib.bib70.9.2" class="ltx_text ltx_font_typewriter">, doi:</span><a target="_blank" href="http://dx.doi.org/10.1109/CVPR.2014.214" title="" class="ltx_ref ltx_href ltx_font_typewriter"><span class="ltx_ref ltx_nolink ltx_path">10.1109/CVPR.2014.214</span></a><span id="bib.bib70.10.3" class="ltx_text ltx_font_typewriter">.
arXiv: 1312.4659.
</span>
</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib71.5.5.1" class="ltx_text ltx_font_typewriter">Trumble etÂ al. (2017)</span></span>
<span class="ltx_bibblock"><span id="bib.bib71.7.1" class="ltx_text ltx_font_typewriter">
Trumble, M., Gilbert, A.,
Malleson, C., Hilton, A.,
Collomosse, J., 2017.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.8.1" class="ltx_text ltx_font_typewriter">Total Capture: 3D Human Pose Estimation
Fusing Video and Inertial Sensors, in:
Procedings of the British Machine Vision
Conference 2017, British Machine Vision Association,
London, UK. p.Â 14.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.9.1" class="ltx_text ltx_font_typewriter">URL: </span><a target="_blank" href="http://www.bmva.org/bmvc/2017/papers/paper014/index.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.bmva.org/bmvc/2017/papers/paper014/index.html</a><span id="bib.bib71.10.2" class="ltx_text ltx_font_typewriter">,
doi:</span><a target="_blank" href="http://dx.doi.org/10.5244/C.31.14" title="" class="ltx_ref ltx_href ltx_font_typewriter"><span class="ltx_ref ltx_nolink ltx_path">10.5244/C.31.14</span></a><span id="bib.bib71.11.3" class="ltx_text ltx_font_typewriter">.
</span>
</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib72.3.3.1" class="ltx_text ltx_font_typewriter">(72)</span></span>
<span class="ltx_bibblock"><span id="bib.bib72.5.1" class="ltx_text ltx_font_typewriter">
University, C.M., .
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.6.1" class="ltx_text ltx_font_typewriter">Graphic lab motion capture database.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="http://mocap.cs.cmu.edu/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://mocap.cs.cmu.edu/</a><span id="bib.bib72.7.1" class="ltx_text ltx_font_typewriter">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.8.1" class="ltx_text ltx_font_typewriter">URL: </span><a target="_blank" href="http://mocap.cs.cmu.edu/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://mocap.cs.cmu.edu/</a><span id="bib.bib72.9.2" class="ltx_text ltx_font_typewriter">. accessed:
2020-04-29.
</span>
</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib73.4.4.1" class="ltx_text ltx_font_typewriter">Wandt and Rosenhahn (2019)</span></span>
<span class="ltx_bibblock"><span id="bib.bib73.6.1" class="ltx_text ltx_font_typewriter">
Wandt, B., Rosenhahn, B.,
2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.7.1" class="ltx_text ltx_font_typewriter">RepNet: Weakly Supervised Training of an
Adversarial Reprojection Network for 3D Human Pose Estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.8.1" class="ltx_text ltx_font_typewriter">arXiv:1902.09868 [cs] URL: </span><a target="_blank" href="http://arxiv.org/abs/1902.09868" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1902.09868</a><span id="bib.bib73.9.2" class="ltx_text ltx_font_typewriter">. arXiv: 1902.09868.
</span>
</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib74.5.5.1" class="ltx_text ltx_font_typewriter">Wang etÂ al. (2020)</span></span>
<span class="ltx_bibblock"><span id="bib.bib74.7.1" class="ltx_text ltx_font_typewriter">
Wang, J., Yan, S., Xiong,
Y., Lin, D., 2020.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib74.8.1" class="ltx_text ltx_font_typewriter">Motion Guided 3D Pose Estimation from
Videos, in: Vedaldi, A., Bischof, H.,
Brox, T., Frahm, J.M. (Eds.),
Computer Vision â€“ ECCV 2020.
Springer International Publishing,
Cham. volume 12358, pp.
764--780.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib74.9.1" class="ltx_text ltx_font_typewriter">URL: </span><a target="_blank" href="http://link.springer.com/10.1007/978-3-030-58601-0_45" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://link.springer.com/10.1007/978-3-030-58601-0_45</a><span id="bib.bib74.10.2" class="ltx_text ltx_font_typewriter">,
doi:</span><a target="_blank" href="http://dx.doi.org/10.1007/978-3-030-58601-0_45" title="" class="ltx_ref ltx_href ltx_font_typewriter"><span class="ltx_ref ltx_nolink ltx_path">10.1007/978-3-030-58601-0_45</span></a><span id="bib.bib74.11.3" class="ltx_text ltx_font_typewriter">. series Title:
Lecture Notes in Computer Science.
</span>
</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib75.5.5.1" class="ltx_text ltx_font_typewriter">Xiao etÂ al. (2018)</span></span>
<span class="ltx_bibblock"><span id="bib.bib75.7.1" class="ltx_text ltx_font_typewriter">
Xiao, B., Wu, H., Wei,
Y., 2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib75.8.1" class="ltx_text ltx_font_typewriter">Simple baselines for human pose estimation and
tracking.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1804.06208" title="" class="ltx_ref ltx_href ltx_font_typewriter">arXiv:1804.06208</a><span id="bib.bib75.9.1" class="ltx_text ltx_font_typewriter">.
</span>
</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib76.5.5.1" class="ltx_text ltx_font_typewriter">Xu etÂ al. (2019)</span></span>
<span class="ltx_bibblock"><span id="bib.bib76.7.1" class="ltx_text ltx_font_typewriter">
Xu, Y., Zhu, S.C., Tung,
T., 2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib76.8.1" class="ltx_text ltx_font_typewriter">DenseRaC: Joint 3D Pose and Shape
Estimation by Dense Render-and-Compare.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib76.9.1" class="ltx_text ltx_font_typewriter">arXiv:1910.00116 [cs, eess] URL: </span><a target="_blank" href="http://arxiv.org/abs/1910.00116" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1910.00116</a><span id="bib.bib76.10.2" class="ltx_text ltx_font_typewriter">. arXiv: 1910.00116.
</span>
</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib77.5.5.1" class="ltx_text ltx_font_typewriter">Yu etÂ al. (2017)</span></span>
<span class="ltx_bibblock"><span id="bib.bib77.7.1" class="ltx_text ltx_font_typewriter">
Yu, T., Guo, K., Xu, F.,
Dong, Y., Su, Z., Zhao,
J., Li, J., Dai, Q.,
Liu, Y., 2017.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib77.8.1" class="ltx_text ltx_font_typewriter">BodyFusion: Real-Time Capture of Human
Motion and Surface Geometry Using a Single Depth Camera, in:
2017 IEEE International Conference on Computer
Vision (ICCV), IEEE, Venice. pp.
910--919.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib77.9.1" class="ltx_text ltx_font_typewriter">URL: </span><a target="_blank" href="http://ieeexplore.ieee.org/document/8237366/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://ieeexplore.ieee.org/document/8237366/</a><span id="bib.bib77.10.2" class="ltx_text ltx_font_typewriter">,
doi:</span><a target="_blank" href="http://dx.doi.org/10.1109/ICCV.2017.104" title="" class="ltx_ref ltx_href ltx_font_typewriter"><span class="ltx_ref ltx_nolink ltx_path">10.1109/ICCV.2017.104</span></a><span id="bib.bib77.11.3" class="ltx_text ltx_font_typewriter">.
</span>
</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib78.5.5.1" class="ltx_text ltx_font_typewriter">Yu etÂ al. (2018)</span></span>
<span class="ltx_bibblock"><span id="bib.bib78.7.1" class="ltx_text ltx_font_typewriter">
Yu, T., Zheng, Z., Guo,
K., Zhao, J., Dai, Q.,
Li, H., Pons-Moll, G.,
Liu, Y., 2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib78.8.1" class="ltx_text ltx_font_typewriter">DoubleFusion: Real-time Capture of Human
Performances with Inner Body Shapes from a Single Depth
Sensor.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib78.9.1" class="ltx_text ltx_font_typewriter">arXiv:1804.06023 [cs] URL: </span><a target="_blank" href="http://arxiv.org/abs/1804.06023" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1804.06023</a><span id="bib.bib78.10.2" class="ltx_text ltx_font_typewriter">. arXiv: 1804.06023.
</span>
</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib79.4.4.1" class="ltx_text ltx_font_typewriter">Zhang and Park (2020)</span></span>
<span class="ltx_bibblock"><span id="bib.bib79.6.1" class="ltx_text ltx_font_typewriter">
Zhang, Y., Park, H.S.,
2020.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib79.7.1" class="ltx_text ltx_font_typewriter">Multiview Supervision By Registration, pp.
420--428.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib79.8.1" class="ltx_text ltx_font_typewriter">URL: </span><a target="_blank" href="http://openaccess.thecvf.com/content_WACV_2020/html/Zhang_Multiview_Supervision_By_Registration_WACV_2020_paper.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://openaccess.thecvf.com/content_WACV_2020/html/Zhang_Multiview_Supervision_By_Registration_WACV_2020_paper.html</a><span id="bib.bib79.9.2" class="ltx_text ltx_font_typewriter">.
</span>
</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib80.5.5.1" class="ltx_text ltx_font_typewriter">Zhang etÂ al. (2020)</span></span>
<span class="ltx_bibblock"><span id="bib.bib80.7.1" class="ltx_text ltx_font_typewriter">
Zhang, Z., Wang, C., Qin,
W., Zeng, W., 2020.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib80.8.1" class="ltx_text ltx_font_typewriter">Fusing wearable imus with multi-view images for human
pose estimation: A geometric approach.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2003.11163" title="" class="ltx_ref ltx_href ltx_font_typewriter">arXiv:2003.11163</a><span id="bib.bib80.9.1" class="ltx_text ltx_font_typewriter">.
</span>
</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib81.5.5.1" class="ltx_text ltx_font_typewriter">Zhou etÂ al. (2017)</span></span>
<span class="ltx_bibblock"><span id="bib.bib81.7.1" class="ltx_text ltx_font_typewriter">
Zhou, X., Huang, Q., Sun,
X., Xue, X., Wei, Y.,
2017.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib81.8.1" class="ltx_text ltx_font_typewriter">Towards 3D Human Pose Estimation in the
Wild: a Weakly-supervised Approach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib81.9.1" class="ltx_text ltx_font_typewriter">arXiv:1704.02447 [cs] URL: </span><a target="_blank" href="http://arxiv.org/abs/1704.02447" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1704.02447</a><span id="bib.bib81.10.2" class="ltx_text ltx_font_typewriter">. arXiv: 1704.02447.
</span>
</span>
</li>
</ul>
</section>
<div id="p3" class="ltx_para">
<p id="p3.1" class="ltx_p"></p>
</div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2010.06448" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2010.06449" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2010.06449">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2010.06449" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2010.06450" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Feb 26 19:49:41 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
