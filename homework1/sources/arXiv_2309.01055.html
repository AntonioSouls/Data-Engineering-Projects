<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2309.01055] Integration of Vision-based Object Detection and Grasping for Articulated Manipulator in Lunar Conditions</title><meta property="og:description" content="The integration of vision-based frameworks to achieve lunar robot applications faces numerous challenges such as terrain configuration or extreme lighting conditions. This paper presents a generic task pipeline using o…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Integration of Vision-based Object Detection and Grasping for Articulated Manipulator in Lunar Conditions">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Integration of Vision-based Object Detection and Grasping for Articulated Manipulator in Lunar Conditions">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2309.01055">

<!--Generated on Wed Feb 28 08:25:58 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:173%;">Integration of Vision-based Object Detection and Grasping for Articulated Manipulator in Lunar Conditions
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Camille Boucher<sup id="id8.8.id1" class="ltx_sup"><span id="id8.8.id1.1" class="ltx_text ltx_font_italic">∗1,2</span></sup>, Gustavo H. Diaz<sup id="id9.9.id2" class="ltx_sup"><span id="id9.9.id2.1" class="ltx_text ltx_font_italic">2</span></sup>, Shreya Santra<sup id="id10.10.id3" class="ltx_sup"><span id="id10.10.id3.1" class="ltx_text ltx_font_italic">2</span></sup>, Kentaro Uno<sup id="id11.11.id4" class="ltx_sup"><span id="id11.11.id4.1" class="ltx_text ltx_font_italic">2</span></sup>, and Kazuya Yoshida<sup id="id12.12.id5" class="ltx_sup"><span id="id12.12.id5.1" class="ltx_text ltx_font_italic">2</span></sup>
</span><span class="ltx_author_notes">*This work is part of the Moonshot Goal 3 supported by the Japan Science and Technology Agency<sup id="id13.13.id1" class="ltx_sup"><span id="id13.13.id1.1" class="ltx_text ltx_font_italic">1</span></sup>C. Boucher is with IMT Atlantique, Brest 29238, France
<span id="id14.14.id2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">camille.boucher@imt-atlantique.net</span><sup id="id15.15.id1" class="ltx_sup"><span id="id15.15.id1.1" class="ltx_text ltx_font_italic">2</span></sup>G.H. Diaz, S. Santra, K. Uno, and K. Yoshida are with Space Robotics Lab. in the Department of Aerospace Engineering, Graduate School of Engineering, Tohoku University,
Sendai 980–8579, Japan
<span id="id16.16.id2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">diaz.huenupan.gustavo.hernan.p3@dc.tohoku.ac.jp</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id17.id1" class="ltx_p">The integration of vision-based frameworks to achieve lunar robot applications faces numerous challenges such as terrain configuration or extreme lighting conditions. This paper presents a generic task pipeline using object detection, instance segmentation and grasp detection, that can be used for various applications by using the results of these vision-based systems in a different way. We achieve a rock stacking task on a non-flat surface in difficult lighting conditions with a very good success rate of 92%. Eventually, we present an experiment to assemble 3D printed robot components to initiate more complex tasks in the future.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The ability to observe and understand the environment has been expanded to robotic systems with artificial intelligence and machine learning has demonstrated its use to achieve impressive outcomes in various fields, including image and data processing for robotic application.
Imitating the human ability to detect and grasp any sort of object has posed challenges for applications such as transporting large objects, construction and automation. The combination of machine vision and robotics to replicate such type of grasping requires precise target detection, localization and manipulation.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">This paper aims to tackle this challenge in lunar conditions with limited lighting conditions, considering various craters, environment changes and irregular objects like in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Integration of Vision-based Object Detection and Grasping for Articulated Manipulator in Lunar Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. A lot of unpredictable situations can occur in a lunar mission without any possibility of human assistance. The robots must achieve missions of exploration, scientific experiments, construction, etc., using accurate and robust neural networks. We aim to demonstrate that a generic software architecture using the vision-based neural networks YOLOv8 (You Only Look Once) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and GPD (Grasp Pose Detection) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, as shown in Fig. <a href="#S3.F2" title="Figure 2 ‣ III System Overview ‣ Integration of Vision-based Object Detection and Grasping for Articulated Manipulator in Lunar Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, can be used to achieve numerous applications like rock stacking or autonomous robot assembling, and this by just modifying the dataset and the manipulation pipeline. The goal is, therefore, not to improve the existing neural networks but to integrate and use them efficiently to perform the aforementioned tasks.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2309.01055/assets/figures/system_overview/moon_stacking.jpg" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="783" height="891" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Robot-xArm7, gripper with camera and second camera fixed at the base, stacking rocks in a moon-like environment.</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">We will first establish state-of-the-art on the different elements of the generic pipeline: object detection, instance segmentation and grasp detection.
A brief overview of our system used for this paper will be done in the second part.
Then, we will present our vision-based frameworks: YOLOv8 used on custom datasets and GPD.
Subsequently, we will explain the ROS integration, and two applications - rock stacking and robot assembling - by just changing the way of using the results from the vision-based system.
Eventually, our experiments will be presented, followed by an analysis and the outline for the future.
</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">state of the art </span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Object Detection</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">In visual object recognition, the use of Convolutional Neural Network (CNN) has led to new challenges. The detectors can be classified into two categories: two-stage or regional-proposal-based algorithms and single-stage ones. One-stage frameworks have the advantage to process the entire image in a single pass, making it more computationally efficient and better suited for real-time detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">In 2015, J. Redmon <em id="S2.SS1.p2.1.1" class="ltx_emph ltx_font_italic">et al.</em> presented a new one-shot framework YOLO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.
J.Terven <em id="S2.SS1.p2.1.2" class="ltx_emph ltx_font_italic">et al.</em> analyzed the YOLO’s evolution, examining the innovations and contributions in each iteration from the original YOLO to the new version YOLOv8 in January 2023 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. The first version performed faster than any existing object detector but the localization error was larger compared with state-of-the-art methods such as region-based Fast R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Through the years, YOLO has evolved to stand out as state-of-the-art object detection in a real-time framework with its remarkable balance of speed and accuracy. It has then been used in numerous fields such as autonomous vehicles with object tracking, like pedestrians <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and other obstacles <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, surveillance and security fields <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> or medical field with cancer detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. D. Reis <em id="S2.SS1.p2.1.3" class="ltx_emph ltx_font_italic">et al.</em> demonstrated the use of the latest version YOLOv8 for detecting flying objects in real time in a challenging environment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Instance Segmentation</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Along with the object detection challenge, the semantic segmentation and the instance segmentation are also very discussed problems. While object detection aims to classify and give the location, the goal of semantic segmentation is to label every pixel into a class according to the region within which it is enclosed.
A.M. Hafiz <em id="S2.SS2.p1.1.1" class="ltx_emph ltx_font_italic">et al.</em> defined the instance segmentation problem as the task of providing simultaneous solutions to object detection as well as semantic segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. They reviewed in 2020 the evolution of instance segmentation up to Mask R‑CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, YOLACT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and TensorMask <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. As for the object detection the one-shot models are said to be faster than the two-stage ones, and therefore more suitable for real time utilization.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.4.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.5.2" class="ltx_text ltx_font_italic">Grasping Detection</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">To allow robots to achieve various tasks and reproduce human behaviours, the challenge of reliably grasping and handling objects, like household items, mechanical parts or packages, is extremely important.
The research on robotic systems for manipulation tasks has mainly focused on human-robot interaction, and at first, systems were lacking in the autonomous part of grasping and placing an unknown object in an unstructured environment.
Mahler <em id="S2.SS3.p1.1.1" class="ltx_emph ltx_font_italic">et al.</em> proposed DexNet, a grasp system, with a 93% grasp success rate, which takes depth images as input and gives grasps in the plane as output, i.e. with a single degree of orientation freedom around the gravity axis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.
Morrison <em id="S2.SS3.p1.1.2" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> and Viereck <em id="S2.SS3.p1.1.3" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> studied the problem of grasping dynamically moving objects and proposed a closed loop system with a grasp success rate of 83% and 88.9%.
Gualtieri <em id="S2.SS3.p1.1.4" class="ltx_emph ltx_font_italic">et al.</em> proposed GPD framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> that takes point cloud data as input and produces 6-DOF grasp poses as output. Their system incorporates a new method for generating grasp hypotheses that, relative to prior methods, does not require a precise segmentation of the object to be grasped and can generate hypotheses on any visible surface.
Their system gives really good results, especially for dense environments with a grasp success rate of 89%. In the final step of their work, they also discussed the idea of combining object and grasp detection. They made experiments on household objects, but only evaluating the accuracy of the object detection on the grasped objects, the grasping strategy was not based on the object detection results such as proposed here.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">System Overview</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Our robotic system is comprised of an articulated arm xArm7 (7-DOF) from UFactory.
It is equipped with a parallel gripper with two fingers; the robotic arm is fixed on a table next to the sandbox. The vision system is made up of two Intel RealSense cameras d435 which retrieve RGB-D (color and depth image).
To recreate lunar-like conditions with uneven surfaces we use sand and an artificial light source as shown in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Integration of Vision-based Object Detection and Grasping for Articulated Manipulator in Lunar Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
For the manipulations we use various irregular objects such as polystyrene rocks and 3D printed robot components like head, body, joint, etc. Fig. <a href="#S4.F4" title="Figure 4 ‣ IV-B Custom dataset and YOLOv8 training ‣ IV Vision-based frameworks ‣ Integration of Vision-based Object Detection and Grasping for Articulated Manipulator in Lunar Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. These objects are challenging because of their irregularities in shape, size, color and weight.
<br class="ltx_break">The computer used is equipped with CPU Intel 19.13900KF 24 cores and GPU NVIDIA GeForce RTX 4090/24 GB.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">The software system shown in Fig. <a href="#S3.F2" title="Figure 2 ‣ III System Overview ‣ Integration of Vision-based Object Detection and Grasping for Articulated Manipulator in Lunar Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> is comprised of some low-level and medium-level packages, like MoveIt, for the controls, motion planning, etc. and a high level with various applications like object detection or robot assembling.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2309.01055/assets/x1.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="192" height="102" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Generic software architecture based on vision-based frameworks.</figcaption>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Vision-based frameworks </span>
</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">YOLOv8</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">To perform object detection and instance segmentation, we choose YOLOv8 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, whose new architecture is well resumed by J.Terven <em id="S4.SS1.p1.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Solawetz <em id="S4.SS1.p1.1.2" class="ltx_emph ltx_font_italic">et al.</em> explained the improvements from the previous versions such as the anchor free detection and the new convolutions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>.
This version has a high mean Average Precision (mAP) (respectively 50.2 and 53.9 mAP50-95 for its medium model and larger one) while maintaining a lower inference speed on the COCO (Common Objects in Context) dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. Another positive highlight is that YOLOv8 can be used both with a command line interface and with a PIP package, which is very useful for ROS integration and for all the tasks like training, validation, prediction, etc.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Custom dataset and YOLOv8 training </span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">In order for our object detection results to be applicable in lunar robotic applications, it needs to perform efficiently in a challenging environment with shadows, high exposure, occlusion, and on miscellaneous objects such as robot parts, screws, bolts, various types of rocks, etc. The construction and the training of a custom dataset, considering the identified complexities, are as important as the model choice to achieve highly accurate results.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">To better highlight the importance of a custom dataset, especially considering the lighting in a lunar scenario, we compare the mean Average Precision between two models. The first one is YOLOv8m, YOLOv8 medium model, trained on coco128 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, a sub-dataset of 128 images from the COCO dataset. The second is YOLOv8m trained on a custom dataset. We add to the coco128 images 62 new pictures of 6 objects (bottle, laptop, mouse, scissors, spoon and fork) in more complex lighting conditions than in the original dataset. Examples images from these datasets are shown in Fig. <a href="#S4.F3" title="Figure 3 ‣ IV-B Custom dataset and YOLOv8 training ‣ IV Vision-based frameworks ‣ Integration of Vision-based Object Detection and Grasping for Articulated Manipulator in Lunar Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> where three objects are shown (scissor, mouse and bottle). For the validation, we ensure to use different objects than the ones used for the training and different lighting conditions. We also make different validation sets by adding several augmentation steps which degrade image quality. We can see in Table 1 that even with the more complex validation set (brightness, exposure and 10% of noise) the model trained with the custom dataset outperforms the original one. Therefore, during the construction of our datasets, we take particular care to include a wide variety of images in different lighting and exposure conditions, occluded and cropped objects, different colors, sizes and shapes, etc.</p>
</div>
<figure id="S4.T1" class="ltx_table">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Object detection accuracy of YOLOv8 models trained with different datasets, on different validation sets.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_rr ltx_border_t">Training dataset</th>
<th id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t">Validation set</th>
<th id="S4.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t">mAP50</th>
<th id="S4.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">mAP50-95</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.2.1" class="ltx_tr">
<td id="S4.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_rr ltx_border_t">COCO128</td>
<td id="S4.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">normal</td>
<td id="S4.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">23.3</td>
<td id="S4.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">22.9</td>
</tr>
<tr id="S4.T1.1.3.2" class="ltx_tr">
<td id="S4.T1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_rr ltx_border_t">Custom</td>
<td id="S4.T1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">b/e*</td>
<td id="S4.T1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">61.4</td>
<td id="S4.T1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">52.3</td>
</tr>
<tr id="S4.T1.1.4.3" class="ltx_tr">
<td id="S4.T1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_rr ltx_border_t">Custom</td>
<td id="S4.T1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">b/e* - noise 5%</td>
<td id="S4.T1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">40.4</td>
<td id="S4.T1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">31.0</td>
</tr>
<tr id="S4.T1.1.5.4" class="ltx_tr">
<td id="S4.T1.1.5.4.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_rr ltx_border_t">Custom</td>
<td id="S4.T1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr ltx_border_t">b/e* - noise 10%</td>
<td id="S4.T1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr ltx_border_t">36.3</td>
<td id="S4.T1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">28.1</td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.T1.2" class="ltx_p ltx_figure_panel">* augmentation step on the validation set: brightness and exposure +/- 25%</p>
</div>
</div>
</figure>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2309.01055/assets/x2.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="158" height="84" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Object detection on bottles, mouses and scissors in difficult lighting.</figcaption>
</figure>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2309.01055/assets/figures/yolo/custom_datasets_prediction.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="333" height="214" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Custom datasets and YOLOv8 predictions (a) moonrock segmentation (b) moonrock segmentation, mask deformed by strong exposure (c) robot components detection.</figcaption>
</figure>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">For the different applications, we constructed two main datasets, each with different challenges.
First, polystyrene-made imitated rocks with the main challenges being lighting conditions and uneven surfaces, Fig. <a href="#S4.F4" title="Figure 4 ‣ IV-B Custom dataset and YOLOv8 training ‣ IV Vision-based frameworks ‣ Integration of Vision-based Object Detection and Grasping for Articulated Manipulator in Lunar Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> (a) and (b). During the dataset creation, the sand, the lighting and the exposure conditions were modified. We also add different augmentation steps: +/- 25% of brightness and exposure and 5% of noise. The augmentation enables the dataset to be artificially enlarged using label-preserving transformation to reduce over-fitting on image data. We include real rocks in the validation to get more reliable accuracy results. Instance segmentation will be performed on this dataset, the validation results will be presented in part VI.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">The second dataset is 3D printed robot components - head, body, joint, legs, etc. Fig. <a href="#S4.F4" title="Figure 4 ‣ IV-B Custom dataset and YOLOv8 training ‣ IV Vision-based frameworks ‣ Integration of Vision-based Object Detection and Grasping for Articulated Manipulator in Lunar Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> (c), with two main challenges. The first one is low inter-class variance: components which look very similar to each other compared to the rest of the labels, for example, the difference between a joint and a body joint consists in being attached or not to a robot body. The second challenge has overlapping classes and occlusions. Using the YOLOv8 results we are able to recognize the robots components and determine their state: if the algorithm detects a body, it will need to get its associated joints, furthermore if a body joint is detected it needs to be classified as available or not (another part already attached to or not), and likewise.
The labelling rules are very important and need to be defined before the annotations; in this dataset , for example, we decide to define a leg as a joint plus a foot and a body and the main body part plus its body joints as we can see in Fig. <a href="#S4.F4" title="Figure 4 ‣ IV-B Custom dataset and YOLOv8 training ‣ IV Vision-based frameworks ‣ Integration of Vision-based Object Detection and Grasping for Articulated Manipulator in Lunar Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.1" class="ltx_p">After creating the datasets we train them on YOLOv8 models. The robot dataset is composed of 528 images with 12 classes; which are split into 90% as the training set and 10% as the validation set. We train on different epochs to detect the moment where the model stops improving and begins overfitting. For this model it is around 50 epochs (about 3 times the number of classes). We also decide to keep the original training hyper-parameters, since the dataset is not very large, we do not want to over-fit the model by tuning the parameters. The hyper-parameters are: batch size of 16, AdamW as the optimizer, momentum of 0.937, weight decay of 0.0005 and learning rate of 0.000667.</p>
</div>
<div id="S4.SS2.p6" class="ltx_para">
<p id="S4.SS2.p6.1" class="ltx_p">We perform the training of the small, medium and large YOLOv8 models and then evaluate to determine an optimal trade-off between inference speed and mAP50-95.
We can see in Table II a noticeable mAP50-95 increase between the small and the medium model but not a significant improvement between the medium and the large. On the validation set all the models have an average total speed (pre-process + inference + post-process) under 10 milliseconds/image, which fit perfectly with the detection in real time. Regarding the results, we decide to choose the medium model YOLOv8m. We test on different SDR videos and we observe a total speed of 0.71 + 7.34 + 0.89 = 8.94 milliseconds, more than 60 fps (frames per second) which is consistent with real-time use.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span>YOLOv8 models training on robot dataset results.</figcaption>
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_rr ltx_border_t">Model</th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t">Parameters</th>
<th id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t">Layers</th>
<th id="S4.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t">mAP50-95</th>
<th id="S4.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Speed (ms)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.2.1" class="ltx_tr">
<td id="S4.T2.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_rr ltx_border_t">Small</td>
<td id="S4.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">11139857</td>
<td id="S4.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">225</td>
<td id="S4.T2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">81.9</td>
<td id="S4.T2.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.8</td>
</tr>
<tr id="S4.T2.1.3.2" class="ltx_tr">
<td id="S4.T2.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_rr ltx_border_t">Medium</td>
<td id="S4.T2.1.3.2.2" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">25862689</td>
<td id="S4.T2.1.3.2.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">295</td>
<td id="S4.T2.1.3.2.4" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">84.3</td>
<td id="S4.T2.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.4</td>
</tr>
<tr id="S4.T2.1.4.3" class="ltx_tr">
<td id="S4.T2.1.4.3.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_rr ltx_border_t">Large</td>
<td id="S4.T2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr ltx_border_t">43638321</td>
<td id="S4.T2.1.4.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr ltx_border_t">365</td>
<td id="S4.T2.1.4.3.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr ltx_border_t">84.5</td>
<td id="S4.T2.1.4.3.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">8.5</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.4.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.5.2" class="ltx_text ltx_font_italic">GPD configuration and tuning</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">For the object manipulations, the results obtained with YOLOv8 are not enough and we need to improve the grasping strategy.
We integrate GPD package for the grasp detection for several reasons; firstly, because it can be easily integrated to ROS with a package in C++ and Python. Moreover, since GPD operates with point cloud input, we can easily manipulate this point cloud using the results obtained from object detection. In addition, since GPD is not limited to detecting planar grasps, it can more easily generate side grasps, which can be needed for some rocks or robots components, it then better ensures the autonomous grasping in any kind of situation.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">The GPD library allows configuring several parameters related to i) the geometry of the gripper and grasp descriptor, ii) pre-processing of the point cloud, iii) grasp candidates generation, iv) filters and selection.
<br class="ltx_break">For the hand geometry, we first test with the actual dimensions of our UFactory gripper, however, we find that we can get more successful grasps using smaller dimensions, according to the size of the objects.
This also reduces the computation time, since the grasp descriptor is based on the volume inside the gripper.
<br class="ltx_break">For ii), we set the workspace parameters to match the field of view of the point cloud from the <span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_italic">pre-grasp position</span> in order to generate candidates only around the observed object of interest. It is also necessary to set the <span id="S4.SS3.p2.1.2" class="ltx_text ltx_font_italic">sample_above_plane</span> to filter out candidates on the table plane.
<br class="ltx_break">For iii), the first important parameter is the <span id="S4.SS3.p2.1.3" class="ltx_text ltx_font_italic">hand_axes</span> to define the main axis of the generated candidates. We select a vertical orientation that facilitates the actual planning and execution trajectory. Second is the <span id="S4.SS3.p2.1.4" class="ltx_text ltx_font_italic">number of orientation</span> and <span id="S4.SS3.p2.1.5" class="ltx_text ltx_font_italic">number of samples</span> generated around the selected axis, we find that 5 orientations and 100 samples are sufficient to find valid grasp candidates in real-time.
<br class="ltx_break">For iv), we enable the <span id="S4.SS3.p2.1.6" class="ltx_text ltx_font_italic">filter by approach direction</span> in the z axis, again to facilitate the planning and execution; setting the number of selected grasps to 20 is also enough to ensure a real-time selection of valid candidates.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Integration on xArm7</span>
</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.4.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.5.2" class="ltx_text ltx_font_italic">Rock stacking in a moon-like environment</span>
</h3>

<figure id="S5.F5" class="ltx_figure"><img src="/html/2309.01055/assets/x3.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="175" height="55" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Rock stacking and Assembly tasks vision-based frameworks.</figcaption>
</figure>
<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">In this part, we will focus on the use of the vision-based frameworks results for the stacking rocks task. For lunar exploration, we want robots being able to autonomously recognize interesting rocks, pick and place them for analysis. We also want the robots to achieve construction tasks. Therefore, our first application is to autonomously stack small and medium rocks in these lunar conditions.
We use our vision-based general framework shown in Fig. <a href="#S3.F2" title="Figure 2 ‣ III System Overview ‣ Integration of Vision-based Object Detection and Grasping for Articulated Manipulator in Lunar Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> for specific sub-tasks as described in Fig. <a href="#S5.F5" title="Figure 5 ‣ V-A Rock stacking in a moon-like environment ‣ V Integration on xArm7 ‣ Integration of Vision-based Object Detection and Grasping for Articulated Manipulator in Lunar Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">The first step is to classify all the detected rocks by size to stack them in decreasing order; the sorting is done using the area of the object’s mask given by the instance segmentation.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">In the next, we move from pure detection to real application. Indeed, the instance segmentation gives results in pixels but the xArm moves in the real world. To obtain usable data, we deproject the pixel results to point coordinates in millimeters (mm) using the depth information and the intrinsic parameters of the RealSense camera. The intrinsic matrix K contains the focal lengths and the principal point. We eventually transform these coordinates from the camera frame to the xArm’s frame using TF ROS package to retrieve the final coordinates in the robot’s workspace Fig. <a href="#S5.F6" title="Figure 6 ‣ V-A Rock stacking in a moon-like environment ‣ V Integration on xArm7 ‣ Integration of Vision-based Object Detection and Grasping for Articulated Manipulator in Lunar Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<figure id="S5.F6" class="ltx_figure"><img src="/html/2309.01055/assets/x4.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="183" height="95" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Scheme of the <span id="S5.F6.2.1" class="ltx_text ltx_font_italic">pixel to point</span> process.</figcaption>
</figure>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p">Now that we can deproject specific points from pixel to coordinates we can grasp the rock. To perform a better grasping, with oriented rocks for instance, we will use a specific package GPD described in the following subsection.</p>
</div>
<div id="S5.SS1.p5" class="ltx_para">
<p id="S5.SS1.p5.1" class="ltx_p">The final step is to actually stack the rock. After grasping, a new instance segmentation is performed from the <span id="S5.SS1.p5.1.1" class="ltx_text ltx_font_italic">eye on base</span> camera’s frames and the deprojection process can be repeated to calculate the height. The xArm is sent to the determined final position, with an accurate <span id="S5.SS1.p5.1.2" class="ltx_text ltx_font_italic">z</span> value given the rock height and the depth of the stacking point.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.4.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.5.2" class="ltx_text ltx_font_italic">Modular robot model assembly task</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">The goal of this task is to assemble the prototype of our modular robot model composed by modular components as shown in Fig. <a href="#S4.F4" title="Figure 4 ‣ IV-B Custom dataset and YOLOv8 training ‣ IV Vision-based frameworks ‣ Integration of Vision-based Object Detection and Grasping for Articulated Manipulator in Lunar Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>(c). This prototype was selected to test our algorithms to demonstrate the challenging task of grasping. In order to implement the assembly using our general framework integrating GPD and YOLOv8 for tasks planning, we need to implement the specific sequence and use the specific modules described in Fig. <a href="#S5.F5" title="Figure 5 ‣ V-A Rock stacking in a moon-like environment ‣ V Integration on xArm7 ‣ Integration of Vision-based Object Detection and Grasping for Articulated Manipulator in Lunar Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">The main steps for this task are the <span id="S5.SS2.p2.1.1" class="ltx_text ltx_font_italic">Get object pose</span> that implements the call to the <span id="S5.SS2.p2.1.2" class="ltx_text ltx_font_italic">Object2workspacePose</span> class to retrieve the object pose in the workspace, Fig. <a href="#S5.F7" title="Figure 7 ‣ V-B Modular robot model assembly task ‣ V Integration on xArm7 ‣ Integration of Vision-based Object Detection and Grasping for Articulated Manipulator in Lunar Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> a). The <span id="S5.SS2.p2.1.3" class="ltx_text ltx_font_italic">Grasping sequence</span> that moves the <span id="S5.SS2.p2.1.4" class="ltx_text ltx_font_italic">eye in hand</span> camera close to the object, centering the <span id="S5.SS2.p2.1.5" class="ltx_text ltx_font_italic">pointcloud</span> on the object and allowing GPD library to generate the grasp candidates around the object of interest, Fig. <a href="#S5.F7" title="Figure 7 ‣ V-B Modular robot model assembly task ‣ V Integration on xArm7 ‣ Integration of Vision-based Object Detection and Grasping for Articulated Manipulator in Lunar Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> b). Once a valid grasp pose has been received and the actual grasping sequence has finished, we move the part to a <span id="S5.SS2.p2.1.6" class="ltx_text ltx_font_italic">pre-assembling</span> position to detect the joint assembling point position and calculate the displacement to the target body joint and assemble the part, Fig. <a href="#S5.F7" title="Figure 7 ‣ V-B Modular robot model assembly task ‣ V Integration on xArm7 ‣ Integration of Vision-based Object Detection and Grasping for Articulated Manipulator in Lunar Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> c).</p>
</div>
<figure id="S5.F7" class="ltx_figure"><img src="/html/2309.01055/assets/x5.png" id="S5.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="192" height="68" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Main steps for the robot assembly task.</figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Experiments</span>
</h2>

<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS1.4.1.1" class="ltx_text">VI-A</span> </span><span id="S6.SS1.5.2" class="ltx_text ltx_font_italic">Rock stacking</span>
</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">The first step in validating the algorithm is the rock detection. We evaluate the mAP of yolov8m model trained on our <span id="S6.SS1.p1.1.1" class="ltx_text ltx_font_italic">moonrock dataset</span> Table III. We provide different validation sets by adding augmentation steps such as brightness, exposure and noise. On the non-modified set, the model obtained very good results with 74.0 mAP50-95 for both the box and the mask. On the worst set, we still observed acceptable accuracy with 46.7 and 45.8 mAP50-95. The training dataset is only made with the imitated rocks, so we tested the transposition to real rocks and the model successfully performed 69.5 mAP50-95 on mask segmentation, compared to 77.0 with fake rocks. Finally, we can notice a small decrease in the accuracy of small rocks compared to big ones.</p>
</div>
<figure id="S6.T3" class="ltx_table">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE III: </span>YOLOv8 Moonrock model object detection accuracy.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S6.T3.1" class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T3.1.1.1" class="ltx_tr">
<td id="S6.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_rr ltx_border_t">Validation Set</td>
<td id="S6.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">mAP50-95 box</td>
<td id="S6.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">mAP50-95 mask</td>
</tr>
<tr id="S6.T3.1.2.2" class="ltx_tr">
<td id="S6.T3.1.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_rr ltx_border_t">normal</td>
<td id="S6.T3.1.2.2.2" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">74.3</td>
<td id="S6.T3.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">74.0</td>
</tr>
<tr id="S6.T3.1.3.3" class="ltx_tr">
<td id="S6.T3.1.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_rr ltx_border_t">b/e*</td>
<td id="S6.T3.1.3.3.2" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">73.7</td>
<td id="S6.T3.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">72.7</td>
</tr>
<tr id="S6.T3.1.4.4" class="ltx_tr">
<td id="S6.T3.1.4.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_rr ltx_border_t">noise 5</td>
<td id="S6.T3.1.4.4.2" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">46.9</td>
<td id="S6.T3.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">46.1</td>
</tr>
<tr id="S6.T3.1.5.5" class="ltx_tr">
<td id="S6.T3.1.5.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_rr ltx_border_t">b/e* - noise 5</td>
<td id="S6.T3.1.5.5.2" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">46.7</td>
<td id="S6.T3.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">45.8</td>
</tr>
<tr id="S6.T3.1.6.6" class="ltx_tr">
<td id="S6.T3.1.6.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_rr ltx_border_tt">small rocks</td>
<td id="S6.T3.1.6.6.2" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt">73.8</td>
<td id="S6.T3.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">72.3</td>
</tr>
<tr id="S6.T3.1.7.7" class="ltx_tr">
<td id="S6.T3.1.7.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_rr ltx_border_t">big rocks</td>
<td id="S6.T3.1.7.7.2" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">75.0</td>
<td id="S6.T3.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">74.9</td>
</tr>
<tr id="S6.T3.1.8.8" class="ltx_tr">
<td id="S6.T3.1.8.8.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_rr ltx_border_t">fake rocks</td>
<td id="S6.T3.1.8.8.2" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">76.7</td>
<td id="S6.T3.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">76.6</td>
</tr>
<tr id="S6.T3.1.9.9" class="ltx_tr">
<td id="S6.T3.1.9.9.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_rr ltx_border_t">real rocks</td>
<td id="S6.T3.1.9.9.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr ltx_border_t">71.1</td>
<td id="S6.T3.1.9.9.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">69.5</td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S6.T3.2" class="ltx_p ltx_figure_panel">*augmentation step: brightness +/- 25% and exposure +/- 20%</p>
</div>
</div>
</figure>
<div id="S6.SS1.p2" class="ltx_para">
<p id="S6.SS1.p2.1" class="ltx_p">To evaluate the algorithm of rock stacking, we perform 50 tests. In the grasping strategy, the rocks need to be sorted by size first. We evaluate the size classification success rate of 96%; the mask area sorting works very well, even on rocks with only 1 cm of difference in length. The two failures are because of very high exposure on a rock which induces a small error on the mask as show in Fig. <a href="#S4.F4" title="Figure 4 ‣ IV-B Custom dataset and YOLOv8 training ‣ IV Vision-based frameworks ‣ Integration of Vision-based Object Detection and Grasping for Articulated Manipulator in Lunar Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>(b).</p>
</div>
<div id="S6.SS1.p3" class="ltx_para">
<p id="S6.SS1.p3.1" class="ltx_p">The use of instance segmentation also results in efficient height estimation, with very good accuracy and only 4% of relative error.</p>
</div>
<div id="S6.SS1.p4" class="ltx_para">
<p id="S6.SS1.p4.1" class="ltx_p">The rock-stacking task has an overall success rate of 92 %. The grasping failed twice, and twice the rock is grasped at the extreme end, so when it is stacked, the rocks’ center of mass are shifted and the rock topples over. We measured an average alignment error (distance from bottom rock’s center to the top one) of 25 mm. To correct this error, we should use the second camera to get a feedback of the grasp and maybe track the rock while it is stacking up.</p>
</div>
<div id="S6.SS1.p5" class="ltx_para">
<p id="S6.SS1.p5.1" class="ltx_p">The rock-stacking task in a moon-like environment faces several challenges. The first is to provide highly accurate results in object detection and instance segmentation in difficult light conditions (strong light variations, shadow occlusions or exposure and brightness); we show that constructing a custom dataset and training the YOLOv8 model on it can overcome these difficulties. Then, we manipulate irregular rocks so we perform instance segmentation to sort the masks’ area to get an accurate size classification and we integrate GPD package in the grasping strategy to autonomously grasp almost any kind of rock . Finally, we tackle the non flat surface challenge, making the rocks’ height calculation difficult, by introducing a second camera to perform instance segmentation.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS2.4.1.1" class="ltx_text">VI-B</span> </span><span id="S6.SS2.5.2" class="ltx_text ltx_font_italic">Robot assembling</span>
</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">For this task we aim to assemble the robot model shown in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Integration of Vision-based Object Detection and Grasping for Articulated Manipulator in Lunar Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, that consist of the parts shown in Fig. <a href="#S4.F4" title="Figure 4 ‣ IV-B Custom dataset and YOLOv8 training ‣ IV Vision-based frameworks ‣ Integration of Vision-based Object Detection and Grasping for Articulated Manipulator in Lunar Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>c). The success of the assembling depends mainly on three factors, associated to the main steps presented on Fig. <a href="#S5.F7" title="Figure 7 ‣ V-B Modular robot model assembly task ‣ V Integration on xArm7 ‣ Integration of Vision-based Object Detection and Grasping for Articulated Manipulator in Lunar Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, i) the accuracy and stability of the object pose detection -Table <a href="#S6.T4" title="TABLE IV ‣ VI-B Robot assembling ‣ VI Experiments ‣ Integration of Vision-based Object Detection and Grasping for Articulated Manipulator in Lunar Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>-, ii) the success of the selected grasp -Table <a href="#S6.T5" title="TABLE V ‣ VI-B Robot assembling ‣ VI Experiments ‣ Integration of Vision-based Object Detection and Grasping for Articulated Manipulator in Lunar Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>- and iii) the visibility of the grasped joint -Table <a href="#S6.T5" title="TABLE V ‣ VI-B Robot assembling ‣ VI Experiments ‣ Integration of Vision-based Object Detection and Grasping for Articulated Manipulator in Lunar Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>-, which depends on the actual grasped orientation. We evaluate these factors separately for the head and the leg objects. For i) we put the objects in a fixed position and recorded the position for 144871 samples, and calculated the standard deviation for every coordinate as shown in Table <a href="#S6.T4" title="TABLE IV ‣ VI-B Robot assembling ‣ VI Experiments ‣ Integration of Vision-based Object Detection and Grasping for Articulated Manipulator in Lunar Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>. We can see that for the head and legs, the maximum error is 0.37 mm, which is pretty accurate to define the grasp trajectory. However, for the body joints the maximum error is 36.16 mm; this is due to the small size of the joints and the noise in the depth frame used for the de-projection of the pose.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.1" class="ltx_p">To evaluate the accuracy of the selected grasps, we repeat the grasping sequence from different initial positions of the objects, the results are shown in Table <a href="#S6.T5" title="TABLE V ‣ VI-B Robot assembling ‣ VI Experiments ‣ Integration of Vision-based Object Detection and Grasping for Articulated Manipulator in Lunar Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>. The success in this step is very dependent on the selected grasp, which due to the GPD implementation is inherently stochastic. More fine tuning of the parameters can be done for a specific object but it will affect the performance of other objects.</p>
</div>
<div id="S6.SS2.p3" class="ltx_para">
<p id="S6.SS2.p3.1" class="ltx_p">For the evaluation of the visibility of the grasped joint, after a successful grasp of the object, we send the <span id="S6.SS2.p3.1.1" class="ltx_text ltx_font_italic">eef</span> to the fixed <span id="S6.SS2.p3.1.2" class="ltx_text ltx_font_italic">pre-assembly</span> position and calculate the success ratio of pose detection, the results are shown in Table <a href="#S6.T5" title="TABLE V ‣ VI-B Robot assembling ‣ VI Experiments ‣ Integration of Vision-based Object Detection and Grasping for Articulated Manipulator in Lunar Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>.</p>
</div>
<div id="S6.SS2.p4" class="ltx_para">
<p id="S6.SS2.p4.1" class="ltx_p">The main challenge of this task is to be able to detect and manipulate small objects, as well as detecting the target positions for assembling. We approach this problem using a real-time system that allows us to calculate several validity checks to improve the success ratios of the assembly pipeline. Another challenge for this experiment is the non-optimal trajectories generated by MoveIt for some cases, which we solve by planning directly in the joint space for those particular cases. We partially tackle occlusion problems by having two cameras, however, there are still limitations particularly in assembling the legs. We plan to improve this by using a camera on a second arm in the near future. The final critical issue is the limitation of YOLOv8 to provide non-oriented bounding boxes, which is required for a more precise assembly. We plan to do more post-processing of the YOLOv8 results in order to estimate the angle.
<br class="ltx_break">Even though we use a very simple and small robot model, we achieved high success rates in the assembly process, so for the future assembly tasks of real robots and bigger parts we expect to improve our results.</p>
</div>
<figure id="S6.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>Object pose estimation accuracy (144871 samples).</figcaption>
<table id="S6.T4.3.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T4.3.3.4.1" class="ltx_tr">
<th id="S6.T4.3.3.4.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S6.T4.3.3.4.1.1.1" class="ltx_text">Object</span></th>
<th id="S6.T4.3.3.4.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3">Standard deviation [mm]</th>
</tr>
<tr id="S6.T4.3.3.3" class="ltx_tr">
<th id="S6.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><math id="S6.T4.1.1.1.1.m1.1" class="ltx_Math" alttext="\sigma_{x}" display="inline"><semantics id="S6.T4.1.1.1.1.m1.1a"><msub id="S6.T4.1.1.1.1.m1.1.1" xref="S6.T4.1.1.1.1.m1.1.1.cmml"><mi id="S6.T4.1.1.1.1.m1.1.1.2" xref="S6.T4.1.1.1.1.m1.1.1.2.cmml">σ</mi><mi id="S6.T4.1.1.1.1.m1.1.1.3" xref="S6.T4.1.1.1.1.m1.1.1.3.cmml">x</mi></msub><annotation-xml encoding="MathML-Content" id="S6.T4.1.1.1.1.m1.1b"><apply id="S6.T4.1.1.1.1.m1.1.1.cmml" xref="S6.T4.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S6.T4.1.1.1.1.m1.1.1.1.cmml" xref="S6.T4.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S6.T4.1.1.1.1.m1.1.1.2.cmml" xref="S6.T4.1.1.1.1.m1.1.1.2">𝜎</ci><ci id="S6.T4.1.1.1.1.m1.1.1.3.cmml" xref="S6.T4.1.1.1.1.m1.1.1.3">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.1.1.1.1.m1.1c">\sigma_{x}</annotation></semantics></math></th>
<th id="S6.T4.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><math id="S6.T4.2.2.2.2.m1.1" class="ltx_Math" alttext="\sigma_{y}" display="inline"><semantics id="S6.T4.2.2.2.2.m1.1a"><msub id="S6.T4.2.2.2.2.m1.1.1" xref="S6.T4.2.2.2.2.m1.1.1.cmml"><mi id="S6.T4.2.2.2.2.m1.1.1.2" xref="S6.T4.2.2.2.2.m1.1.1.2.cmml">σ</mi><mi id="S6.T4.2.2.2.2.m1.1.1.3" xref="S6.T4.2.2.2.2.m1.1.1.3.cmml">y</mi></msub><annotation-xml encoding="MathML-Content" id="S6.T4.2.2.2.2.m1.1b"><apply id="S6.T4.2.2.2.2.m1.1.1.cmml" xref="S6.T4.2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S6.T4.2.2.2.2.m1.1.1.1.cmml" xref="S6.T4.2.2.2.2.m1.1.1">subscript</csymbol><ci id="S6.T4.2.2.2.2.m1.1.1.2.cmml" xref="S6.T4.2.2.2.2.m1.1.1.2">𝜎</ci><ci id="S6.T4.2.2.2.2.m1.1.1.3.cmml" xref="S6.T4.2.2.2.2.m1.1.1.3">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.2.2.2.2.m1.1c">\sigma_{y}</annotation></semantics></math></th>
<th id="S6.T4.3.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><math id="S6.T4.3.3.3.3.m1.1" class="ltx_Math" alttext="\sigma_{z}" display="inline"><semantics id="S6.T4.3.3.3.3.m1.1a"><msub id="S6.T4.3.3.3.3.m1.1.1" xref="S6.T4.3.3.3.3.m1.1.1.cmml"><mi id="S6.T4.3.3.3.3.m1.1.1.2" xref="S6.T4.3.3.3.3.m1.1.1.2.cmml">σ</mi><mi id="S6.T4.3.3.3.3.m1.1.1.3" xref="S6.T4.3.3.3.3.m1.1.1.3.cmml">z</mi></msub><annotation-xml encoding="MathML-Content" id="S6.T4.3.3.3.3.m1.1b"><apply id="S6.T4.3.3.3.3.m1.1.1.cmml" xref="S6.T4.3.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S6.T4.3.3.3.3.m1.1.1.1.cmml" xref="S6.T4.3.3.3.3.m1.1.1">subscript</csymbol><ci id="S6.T4.3.3.3.3.m1.1.1.2.cmml" xref="S6.T4.3.3.3.3.m1.1.1.2">𝜎</ci><ci id="S6.T4.3.3.3.3.m1.1.1.3.cmml" xref="S6.T4.3.3.3.3.m1.1.1.3">𝑧</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.3.3.3.3.m1.1c">\sigma_{z}</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T4.3.3.5.1" class="ltx_tr">
<td id="S6.T4.3.3.5.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">head</td>
<td id="S6.T4.3.3.5.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.37</td>
<td id="S6.T4.3.3.5.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.21</td>
<td id="S6.T4.3.3.5.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.34</td>
</tr>
<tr id="S6.T4.3.3.6.2" class="ltx_tr">
<td id="S6.T4.3.3.6.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">leg</td>
<td id="S6.T4.3.3.6.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.35</td>
<td id="S6.T4.3.3.6.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.13</td>
<td id="S6.T4.3.3.6.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.33</td>
</tr>
<tr id="S6.T4.3.3.7.3" class="ltx_tr">
<td id="S6.T4.3.3.7.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">body joint L</td>
<td id="S6.T4.3.3.7.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6.48</td>
<td id="S6.T4.3.3.7.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">36.16</td>
<td id="S6.T4.3.3.7.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.18</td>
</tr>
<tr id="S6.T4.3.3.8.4" class="ltx_tr">
<td id="S6.T4.3.3.8.4.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">body joint U</td>
<td id="S6.T4.3.3.8.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">6.99</td>
<td id="S6.T4.3.3.8.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">19.45</td>
<td id="S6.T4.3.3.8.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">28.31</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S6.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE V: </span>Grasp success and Grasped joint position detection success.</figcaption>
<table id="S6.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T5.1.1.1" class="ltx_tr">
<th id="S6.T5.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Object</th>
<th id="S6.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">Attempts</th>
<th id="S6.T5.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Grasp success</th>
<th id="S6.T5.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Grasped joint detection</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T5.1.2.1" class="ltx_tr">
<th id="S6.T5.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">head</th>
<th id="S6.T5.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">14</th>
<td id="S6.T5.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">92.8%</td>
<td id="S6.T5.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">95.3%</td>
</tr>
<tr id="S6.T5.1.3.2" class="ltx_tr">
<th id="S6.T5.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">leg</th>
<th id="S6.T5.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">42</th>
<td id="S6.T5.1.3.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">76.1%</td>
<td id="S6.T5.1.3.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">86.2%</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">This paper is the first milestone for the integration of our vision-based systems on robotic manipulators for lunar applications. The explanations of our software framework, its integration for xArm7 and our experiments demonstrate how an integration of the same vision-based software can be used for various robotic applications. The results of these vision-based frameworks can be used in many other ways to improve the performance. For instance, using real-time instance segmentation for tracking the pose of objects for a better manipulation, like stacking or assembling. The next achievement is to autonomously assemble a full-scale modular robot. In addition to the presented software, additional features can be introduced such as a second arm, assembly sequence planning as well as communication between several robots.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"> G. Jocher, A. Chaurasia, S. Waxmann, Laughing. “Home”.
Ultralytics YOLOv8 Docs. https://docs.ultralytics.com/ (accessed July 28, 2023).

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"> A. ten Pas, M. Gualtieri, K. Saenko, and R. Platt, “Grasp pose detection in point clouds” in The International Journal of Robotics Research, 36(13-14):1455–1473, 2017.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"> L. Liu, W. Ouyang, X. Wang, P. Fieguth, J. Chen, X. Liu, M. Pietikäinen, “Deep Learning for Generic Object Detection: A Survey”, arXiv:1809.02165, 2019.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"> J. Redmon, S. Divvala, R.Girshick, A. Farhadi,“You Only Look Once: Unified, Real-Time Object Detection”, 2016.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"> J. Terven, D. Cordova-Esparza,“A Comprehensive Review of YOLO: From YOLOv1 and Beyond”, arXiv:2304.005, 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"> R. Girshick, “Fast r-cnn” in Proceedings of the IEEE international conference on computer vision, pp. 1440–1448, 2015.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"> W. Lan, J. Dang, Y. Wang, and S. Wang, “Pedestrian detection based on yolo network model,” in 2018 IEEE international conference on mechatronics and automation, pp. 1547–1551, IEEE, 2018.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"> N. M. A. A. Dazlee, S. A. Khalil, S. Abdul-Rahman, and S. Mutalib, “Object detection for autonomous vehicles with sensor-based technology using yolo,” International Journal of Intelligent Systems and Applications in Engineering, vol. 10, no. 1, pp. 129–134, 2022.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"> A. H. Ashraf, M. Imran, A. M. Qahtani, A. Alsufyani, O. Almutiry, A. Mahmood, M. Attique, and M. Habib, “Weapons detection for security and video surveillance using cnn and yolo-v5s,” CMC-Comput. Mater. Contin, vol. 70, pp. 2761–2775, 2022.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"> Y. Nie, P. Sommella, M. O’Nils, C. Liguori, and J. Lundgren, “Automatic detection of melanoma with yolo deep convolutional neural networks,” in E-Health and Bioengineering Conference (EHB), pp. 1–4, IEEE, 2019.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"> D.Reis, J. Kupec, J. Hong and A Daoudi, “Real-Time Flying Object Detection with YOLOv8”, arXiv:2305.09972, 2023.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"> A.M. Hafiz, G.M. Bhat, “A survey on instance segmentation: state of the art” Int J Multimed Info Retr 9, 171–189, 2020.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"> K. He, G. Gkioxari, P. Dollar, R. Girshick, “Mask R-CNN” in IEEE Trans Pattern Anal Mach Intell, 2018.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"> D. Bolya, C. Zhou, F. Xiao, Y.J. Lee, “YOLACT: real-time instance segmentation”, arXiv preprint arXiv:1904.02689, 2019.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"> X. Chen, R. Girshick, K. He, P. Dollár, “TensorMask: a foundation for dense object segmentation”, arXiv preprint arXiv:1903.12174, 2019.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"> J. Mahler, J. Liang, S. Niyaz, M. Laskey, R. Doan, X. Liu, J. A. Ojea, and K. Goldberg. “Dex-net 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp metrics”, arXiv preprint arXiv:1703.09312, 2017.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"> D. Morrison, P. Corke, and J. Leitner, “Closing the loop for robotic grasping: A real-time, generative grasp synthesis approach”, arXiv preprint 1804.05172, 2018.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"> U. Viereck, A. Pas, K. Saenko, and R. Platt,“Learning a visuomotor controller for real world robotic grasping using simulated depth images” In Conference on Robot Learning (CoRL), 2017.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"> M. Gualtieri, A. ten Pas, K. Saenko, and R. Platt, “High precision grasp pose detection in dense clutter” In 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 598–605, 2016.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"> J. Solawetz and Francesco. “What is YOLOv8? the ultimate guide”, https://blog.roboflow.com/whats-new-in-yolov8/, 2023.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"> T.-Y. Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays, P. Perona, D. Ramanan, C. L. Zitnick, P. Dollár. “Microsoft COCO: Common Objects in Context”, 2015.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"> Team Roboflow. “COCO 128 Dataset”. Roboflow.
<br class="ltx_break">https://universe.roboflow.com/team-roboflow/coco-128 (accessed July 28, 2023).

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2309.01054" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2309.01055" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2309.01055">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2309.01055" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2309.01056" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 08:25:58 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
