<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2305.04925] PillarNeXt: Rethinking Network Designs for 3D Object Detection in LiDAR Point Clouds</title><meta property="og:description" content="In order to deal with the sparse and unstructured raw point clouds, LiDAR based 3D object detection research mostly focuses on designing dedicated local point aggregators for fine-grained geometrical modeling. In this …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="PillarNeXt: Rethinking Network Designs for 3D Object Detection in LiDAR Point Clouds">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="PillarNeXt: Rethinking Network Designs for 3D Object Detection in LiDAR Point Clouds">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2305.04925">

<!--Generated on Thu Feb 29 08:21:46 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">PillarNeXt: Rethinking Network Designs for 3D Object Detection
<br class="ltx_break">in LiDAR Point Clouds</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jinyu Li   Chenxu Luo   Xiaodong Yang
<br class="ltx_break">QCraft
<br class="ltx_break">
</span><span class="ltx_author_notes">Corresponding author <span id="id1.1.id1" class="ltx_text ltx_font_typewriter">xiaodong@qcraft.ai</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">In order to deal with the sparse and unstructured raw point clouds, LiDAR based 3D object detection research mostly focuses on designing dedicated local point aggregators for fine-grained geometrical modeling. In this paper, we revisit the local point aggregators from the perspective of allocating computational resources. We find that the simplest pillar based models perform surprisingly well considering both accuracy and latency. Additionally, we show that minimal adaptions from the success of 2D object detection, such as enlarging receptive field, significantly boost the performance. Extensive experiments reveal that our pillar based networks with modernized designs in terms of architecture and training render the state-of-the-art performance on the two popular benchmarks: Waymo Open Dataset and nuScenes. Our results challenge the common intuition that the detailed geometry modeling is essential to achieve high performance for 3D object detection.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">3D object detection in LiDAR point clouds is an essential task in an autonomous driving system, as it provides crucial information for subsequent onboard modules, ranging from perception <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, prediction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> to planning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. There have been extensive research efforts on developing sophisticated networks that are specifically designed to cope with point clouds in this field <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2305.04925/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="265" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">Overview of pillar, voxel and multi-view fusion (MVF) based 3D object detection networks under different GFLOPs. The dash lines denote the enhanced versions of corresponding models (+). We report the L2 BEV and 3D APH of vehicle on the validation set of Waymo Open Dataset.</span></figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Due to the sparse and irregular nature of point clouds, most existing works adopt the grid based methods, which convert point clouds into regular grids, such as pillar <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, voxel <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> and range view <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, such that regular operators can be applied.
However, it is a common belief that the grid based methods (especially for pillar) inevitably induce information loss, leading to inferior results, in particular for small objects (e.g., pedestrians). Recent research <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> proposes the hybrid design for fine-grained geometrical modeling to combine the point and gird based representations. We name all the above operators as local point aggregators since they aim to aggregate point features in a certain neighborhood. We observe that the current mainstream of 3D object detection is to develop more specialized operators for point clouds, while leaving the network architecture almost unexplored. Most existing works are still built upon the very original architectures of SECOND <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> or PointPillars <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, which lacks of modernized designs.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Meanwhile, in a closely related area, 2D object detection in images has achieved remarkable progress, which can be largely attributed to the advances in architecture and training. Among them, the powerful backbones (e.g., ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and Swin Transformers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>), the effective necks (e.g., BiFPN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> and YOLOF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>), and the improved training (e.g., bag of freebies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>) are in particular notable. Therefore, the research focus of 2D object detection is largely different from that of 3D object detection.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In light of the aforementioned observations, we rethink what should be the focus for 3D object detection in LiDAR point clouds. Specifically, we revisit the two fundamental issues in designing a 3D object detection model: the local point aggregator and the network architecture.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">First, we compare local point aggregators from a new perspective, i.e., the computational budget. A fine-grained aggregator usually demands more extensive computing resources than the coarse one. For instance, the voxel based aggregator employs 3D convolutions, which require more network parameters and run much slower than 2D convolutions. This raises a question of how to effectively allocate the computational budget or network capacity. Should we spend the resources on fine-grained structures or assign them to coarse grids? Surprisingly, as shown in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ PillarNeXt: Rethinking Network Designs for 3D Object Detection in LiDAR Point Clouds" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, when training under a comparable budget and with an enhanced strategy,
a simpler pillar based model can achieve superior or on par performance with the voxel based model, even for small objects such as pedestrians, while significantly outperform the multi-view fusion based model. This challenges the actual performance gain and the necessity of fine-grained local 3D structures. Our finding is also consistent with the recent works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> in general point cloud analysis, demonstrating that different local point aggregators perform similarly under strong networks.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Second, for the network architecture, we do not aim to propose any domain specific designs for point clouds, instead, we make minimal adaptations from the success of 2D object detection and show that they outperform most of existing networks with specialized architectures for point clouds. For instance, one key finding is that enlarging receptive field properly brings significant improvement. Unlike previous works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> relying on multi-scale feature fusion, we show that a single scale at final stage with sufficient receptive field obtains better performance. Such promising results suggest that 3D object detection can inherit the successful practices well developed in 2D domain.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">Levaraging on the findings above, we propose a pillar based network, dubbed as <span id="S1.p7.1.1" class="ltx_text ltx_font_bold">PillarNeXt</span>, which leads to the state-of-the-art results on two popular benchmarks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Our approach is simple yet effective, and enjoys strong scalability and generalizability. We develop a series of networks with different trade-offs between detection accuracy and inference latency through tuning the number of network parameters,
which can be used for both on-board <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and off-board <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> applications in autonomous driving.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">Our main contributions can be summarized as follows. (1) To our knowledge, this is the first work that compares different local point aggregators (pillar, voxel and multi-view fusion) from the perspective of computational budget allocation. Our findings challenge the common belief by showing that pillar can achieve comparable 3D mAP and better bird’s eye view (BEV) mAP compared to voxel, and substantially outperform multi-view fusion in both 3D and BEV mAP. (2) Inspired by the success of 2D object detection, we find that enlarging receptive field is crucial for 3D object detection. With minimal adaptions, our detectors outperform existing methods with sophisticated designs for point clouds. (3) Our networks with appropriate training achieve superior results on two large-scale benchmarks. We hope our models and related findings can serve as a strong and scalable baseline for future research in this community. Our code and model will be made available at <a target="_blank" href="https://github.com/qcraftai/pillarnext" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/qcraftai/pillarnext</a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">LiDAR based 3D Object Detection.</span>
Existing methods can be roughly categorized into point, grid and hybrid based representations, according to the local point aggregators. As a point based method, PointRCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> generates proposals using <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> and then refines each proposal by RoI pooling. However, conducting neighboring point aggregation in such methods is extremely expensive, so they are not feasible to handle large-scale point clouds in autonomous driving. On the other hand, the grid based methods discretize point clouds into structured grids, where 2D or 3D convolutions can be applied. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> VoxelNet partitions a 3D space into voxels and aggregates point features inside each voxel, then dense 3D convolutions are used for context modeling. SECOND <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> improves the efficiency by introducing sparse 3D convolutions. PointPillars <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> organizes point clouds as vertical columns and adopts 2D convolutions. Another grid based representation is the range view <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> that can be also efficiently processed by 2D convolutions. The multi-view fusion methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> take advantage of both pillar/voxel and range view based representations.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">In spite of efficiency, it is commonly and intuitively believed that the grid based methods induce fine-grained information loss. Therefore, the hybrid methods are proposed to incorporate point features into grid representations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. In this work, we instead focus on the basic network architecture and associated training, and show that the fine-grained local geometrical modeling is overestimated.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Feature Fusion and Receptive Field.</span>
The multi-scale feature fusion starts from feature pyramid network (FPN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> that aggregates hierarchical features in a top-down manner. It is widely used in 2D object detection to combine high-level semantics with low-level spatial cues. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, PANet further points out the bottom-up fusion is also important. Both of them perform fusion by adding up feature maps directly. BiFPN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> shows that features from different scales contribute unequally, and adopts learnable weights to adjust the importance. As an interrelated factor of feature fusion, receptive field is also broadly studied and verified in 2D detection and segmentation. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, the atrous spatial pyramid pooling (ASPP) is proposed to sample features with multiple effective receptive fields. TridentNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> applies three convolutions with different dilation factors to make receptive field range to match with object scale range. A similar strategy is also introduced in YOLOF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> that employs a dilated residual block for enlarging receptive field and meanwhile keeping original receptive field.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Although these techniques regarding feature fusion and receptive field have been extensively adopted in 2D domain, they are hardly discussed in 3D domain. Most existing networks in this field still follow or modify based on the architecture of VoxelNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>. In this work, we aim to integrate the up-to-date designs into 3D object detection networks.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2305.04925/assets/arch.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="515" height="278" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S2.F2.3.2" class="ltx_text" style="font-size:90%;">A schematic overview of the network architecture of the proposed PillarNeXt. Our model takes raw point clouds as input and relies on a simple pillar encoder, which consists of MLPs and max pooling to convert point clouds into a pseudo image. We then apply ResNet-18 with sparse convolutions as the backbone, and adopt ASPP based neck to enlarge receptive field. After that, we upsample the feature maps to yield more detailed representations, and use the center based multi-group head to produce the detection output.</span></figcaption>
</figure>
<div id="S2.p5" class="ltx_para ltx_noindent">
<p id="S2.p5.1" class="ltx_p"><span id="S2.p5.1.1" class="ltx_text ltx_font_bold">Model Scaling.</span>
This aspect has been well studied in image domain, including classification, detection and segmentation tasks. It is observed that jointly increasing the depth, width and resolution improves the accuracy. EfficientNet in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> proposes a compound scaling rule for classification, and this rule is later on extended to object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. It is a general consensus that the model capacity affects the model performance. Therefore, the comparison of different methods should be under the consideration of model capacity in order to receive sound conclusions.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">There is only one work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> that studies model scaling in 3D object detection, to the best of our knowledge. It scales the depth, width and resolution of SECOND <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> to find the importance of each component. It however only focuses on a single type of model, while we systematically compare different local point aggregators under similar computational budgets across a wide range of model scales.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Network Architecture Overview</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We focus on the grid based models due to the runtime efficiency and proximity to 2D object detection.
Typically, a grid based network is composed of (i) a grid encoder to convert raw point clouds into structured feature maps, (ii) a backbone for general feature extraction, (iii) a neck for multi-scale feature fusion, and (iv) a detection head for the task-specific output. Existing networks often couple all these components together. In this section, we have them decoupled and review each part briefly.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Grid Encoder</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">A grid encoder is used to discretize point clouds into structured grids, and then convert and aggregate the points within each grid into the preliminary feature representation. In this work, we target at the following three grid encoders.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Pillar:</span> A pillar based grid encoder arranges points in vertical columns, and applies multilayer perceptrons (MLPs) followed by max pooling to extract pillar features, which are represented as a pseudo image <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Voxel:</span> Similar to pillar, the voxel based grid encoder organizes points in voxels and obtains corresponding features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>. Compared with pillar, the voxel encoder preserves details along the height dimension.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Multi-View Fusion (MVF):</span> A MVF based grid encoder combines the pillar/voxel and range view based representations. Here we follow <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> to incorporate a pillar encoder with a cylindrical view based encoder that groups points in the cylindrical coordinates.</p>
</div>
</li>
</ul>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.2.1.1" class="ltx_tr">
<td id="S3.T1.2.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S3.T1.2.1.1.1.1" class="ltx_text">Model</span></td>
<td id="S3.T1.2.1.1.2" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S3.T1.2.1.1.2.1" class="ltx_text">Channels</span></td>
<td id="S3.T1.2.1.1.3" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S3.T1.2.1.1.3.1" class="ltx_text">#Params (M)</span></td>
<td id="S3.T1.2.1.1.4" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S3.T1.2.1.1.4.1" class="ltx_text">FLOPs (G)</span></td>
<td id="S3.T1.2.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S3.T1.2.1.1.5.1" class="ltx_text">Latency (ms)</span></td>
<td id="S3.T1.2.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">Vehicle</td>
<td id="S3.T1.2.1.1.7" class="ltx_td ltx_align_center ltx_border_t" colspan="2">Pedestrian</td>
</tr>
<tr id="S3.T1.2.2.2" class="ltx_tr">
<td id="S3.T1.2.2.2.1" class="ltx_td ltx_align_center ltx_border_t">3D</td>
<td id="S3.T1.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">BEV</td>
<td id="S3.T1.2.2.2.3" class="ltx_td ltx_align_center ltx_border_t">3D</td>
<td id="S3.T1.2.2.2.4" class="ltx_td ltx_align_center ltx_border_t">BEV</td>
</tr>
<tr id="S3.T1.2.3.3" class="ltx_tr">
<td id="S3.T1.2.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Pillar-T</td>
<td id="S3.T1.2.3.3.2" class="ltx_td ltx_align_left ltx_border_t">[32, 64, 128, 128]</td>
<td id="S3.T1.2.3.3.3" class="ltx_td ltx_align_center ltx_border_t">1.65</td>
<td id="S3.T1.2.3.3.4" class="ltx_td ltx_align_center ltx_border_t">70</td>
<td id="S3.T1.2.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">52</td>
<td id="S3.T1.2.3.3.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.2.3.3.6.1" class="ltx_text ltx_font_bold">62.03</span></td>
<td id="S3.T1.2.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T1.2.3.3.7.1" class="ltx_text ltx_font_bold">82.26</span></td>
<td id="S3.T1.2.3.3.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.2.3.3.8.1" class="ltx_text ltx_font_bold">67.63</span></td>
<td id="S3.T1.2.3.3.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.2.3.3.9.1" class="ltx_text ltx_font_bold">75.76</span></td>
</tr>
<tr id="S3.T1.2.4.4" class="ltx_tr">
<td id="S3.T1.2.4.4.1" class="ltx_td ltx_align_center ltx_border_r">MVF-T</td>
<td id="S3.T1.2.4.4.2" class="ltx_td ltx_align_left">[32, 64, 128, 128]</td>
<td id="S3.T1.2.4.4.3" class="ltx_td ltx_align_center">3.44</td>
<td id="S3.T1.2.4.4.4" class="ltx_td ltx_align_center">78</td>
<td id="S3.T1.2.4.4.5" class="ltx_td ltx_align_center ltx_border_r">137</td>
<td id="S3.T1.2.4.4.6" class="ltx_td ltx_align_center">59.16</td>
<td id="S3.T1.2.4.4.7" class="ltx_td ltx_align_center ltx_border_r">81.33</td>
<td id="S3.T1.2.4.4.8" class="ltx_td ltx_align_center">64.10</td>
<td id="S3.T1.2.4.4.9" class="ltx_td ltx_align_center">73.42</td>
</tr>
<tr id="S3.T1.2.5.5" class="ltx_tr">
<td id="S3.T1.2.5.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Pillar-S</td>
<td id="S3.T1.2.5.5.2" class="ltx_td ltx_align_left ltx_border_t">[42, 84, 168, 168]</td>
<td id="S3.T1.2.5.5.3" class="ltx_td ltx_align_center ltx_border_t">2.83</td>
<td id="S3.T1.2.5.5.4" class="ltx_td ltx_align_center ltx_border_t">121</td>
<td id="S3.T1.2.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">79</td>
<td id="S3.T1.2.5.5.6" class="ltx_td ltx_align_center ltx_border_t">63.18</td>
<td id="S3.T1.2.5.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T1.2.5.5.7.1" class="ltx_text ltx_font_bold">83.22</span></td>
<td id="S3.T1.2.5.5.8" class="ltx_td ltx_align_center ltx_border_t">68.12</td>
<td id="S3.T1.2.5.5.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.2.5.5.9.1" class="ltx_text ltx_font_bold">76.37</span></td>
</tr>
<tr id="S3.T1.2.6.6" class="ltx_tr">
<td id="S3.T1.2.6.6.1" class="ltx_td ltx_align_center ltx_border_r">Voxel-S</td>
<td id="S3.T1.2.6.6.2" class="ltx_td ltx_align_left">[12, 24, 48, 96]</td>
<td id="S3.T1.2.6.6.3" class="ltx_td ltx_align_center">1.53</td>
<td id="S3.T1.2.6.6.4" class="ltx_td ltx_align_center">121</td>
<td id="S3.T1.2.6.6.5" class="ltx_td ltx_align_center ltx_border_r">169</td>
<td id="S3.T1.2.6.6.6" class="ltx_td ltx_align_center"><span id="S3.T1.2.6.6.6.1" class="ltx_text ltx_font_bold">64.67</span></td>
<td id="S3.T1.2.6.6.7" class="ltx_td ltx_align_center ltx_border_r">82.45</td>
<td id="S3.T1.2.6.6.8" class="ltx_td ltx_align_center"><span id="S3.T1.2.6.6.8.1" class="ltx_text ltx_font_bold">69.10</span></td>
<td id="S3.T1.2.6.6.9" class="ltx_td ltx_align_center">76.29</td>
</tr>
<tr id="S3.T1.2.7.7" class="ltx_tr">
<td id="S3.T1.2.7.7.1" class="ltx_td ltx_align_center ltx_border_r">MVF-S</td>
<td id="S3.T1.2.7.7.2" class="ltx_td ltx_align_left">[44, 88, 176, 176]</td>
<td id="S3.T1.2.7.7.3" class="ltx_td ltx_align_center">6.38</td>
<td id="S3.T1.2.7.7.4" class="ltx_td ltx_align_center">148</td>
<td id="S3.T1.2.7.7.5" class="ltx_td ltx_align_center ltx_border_r">186</td>
<td id="S3.T1.2.7.7.6" class="ltx_td ltx_align_center">61.06</td>
<td id="S3.T1.2.7.7.7" class="ltx_td ltx_align_center ltx_border_r">82.51</td>
<td id="S3.T1.2.7.7.8" class="ltx_td ltx_align_center">65.15</td>
<td id="S3.T1.2.7.7.9" class="ltx_td ltx_align_center">74.24</td>
</tr>
<tr id="S3.T1.2.8.8" class="ltx_tr">
<td id="S3.T1.2.8.8.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Pillar-B</td>
<td id="S3.T1.2.8.8.2" class="ltx_td ltx_align_left ltx_border_t">[64, 128, 256, 256]</td>
<td id="S3.T1.2.8.8.3" class="ltx_td ltx_align_center ltx_border_t">6.53</td>
<td id="S3.T1.2.8.8.4" class="ltx_td ltx_align_center ltx_border_t">281</td>
<td id="S3.T1.2.8.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">103</td>
<td id="S3.T1.2.8.8.6" class="ltx_td ltx_align_center ltx_border_t">64.83</td>
<td id="S3.T1.2.8.8.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T1.2.8.8.7.1" class="ltx_text ltx_font_bold">84.37</span></td>
<td id="S3.T1.2.8.8.8" class="ltx_td ltx_align_center ltx_border_t">69.04</td>
<td id="S3.T1.2.8.8.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.2.8.8.9.1" class="ltx_text ltx_font_bold">76.96</span></td>
</tr>
<tr id="S3.T1.2.9.9" class="ltx_tr">
<td id="S3.T1.2.9.9.1" class="ltx_td ltx_align_center ltx_border_r">Voxel-B</td>
<td id="S3.T1.2.9.9.2" class="ltx_td ltx_align_left">[18, 36, 72, 144]</td>
<td id="S3.T1.2.9.9.3" class="ltx_td ltx_align_center">3.42</td>
<td id="S3.T1.2.9.9.4" class="ltx_td ltx_align_center">272</td>
<td id="S3.T1.2.9.9.5" class="ltx_td ltx_align_center ltx_border_r">226</td>
<td id="S3.T1.2.9.9.6" class="ltx_td ltx_align_center"><span id="S3.T1.2.9.9.6.1" class="ltx_text ltx_font_bold">66.00</span></td>
<td id="S3.T1.2.9.9.7" class="ltx_td ltx_align_center ltx_border_r">83.32</td>
<td id="S3.T1.2.9.9.8" class="ltx_td ltx_align_center"><span id="S3.T1.2.9.9.8.1" class="ltx_text ltx_font_bold">69.45</span></td>
<td id="S3.T1.2.9.9.9" class="ltx_td ltx_align_center">76.38</td>
</tr>
<tr id="S3.T1.2.10.10" class="ltx_tr">
<td id="S3.T1.2.10.10.1" class="ltx_td ltx_align_center ltx_border_r">MVF-B</td>
<td id="S3.T1.2.10.10.2" class="ltx_td ltx_align_left">[68, 136, 272, 272]</td>
<td id="S3.T1.2.10.10.3" class="ltx_td ltx_align_center">15.02</td>
<td id="S3.T1.2.10.10.4" class="ltx_td ltx_align_center">353</td>
<td id="S3.T1.2.10.10.5" class="ltx_td ltx_align_center ltx_border_r">291</td>
<td id="S3.T1.2.10.10.6" class="ltx_td ltx_align_center">62.15</td>
<td id="S3.T1.2.10.10.7" class="ltx_td ltx_align_center ltx_border_r">83.22</td>
<td id="S3.T1.2.10.10.8" class="ltx_td ltx_align_center">66.12</td>
<td id="S3.T1.2.10.10.9" class="ltx_td ltx_align_center">75.00</td>
</tr>
<tr id="S3.T1.2.11.11" class="ltx_tr">
<td id="S3.T1.2.11.11.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Pillar-L</td>
<td id="S3.T1.2.11.11.2" class="ltx_td ltx_align_left ltx_border_t">[96, 192, 384, 384]</td>
<td id="S3.T1.2.11.11.3" class="ltx_td ltx_align_center ltx_border_t">14.63</td>
<td id="S3.T1.2.11.11.4" class="ltx_td ltx_align_center ltx_border_t">632</td>
<td id="S3.T1.2.11.11.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">194</td>
<td id="S3.T1.2.11.11.6" class="ltx_td ltx_align_center ltx_border_t">65.86</td>
<td id="S3.T1.2.11.11.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T1.2.11.11.7.1" class="ltx_text ltx_font_bold">84.98</span></td>
<td id="S3.T1.2.11.11.8" class="ltx_td ltx_align_center ltx_border_t">68.42</td>
<td id="S3.T1.2.11.11.9" class="ltx_td ltx_align_center ltx_border_t">76.67</td>
</tr>
<tr id="S3.T1.2.12.12" class="ltx_tr">
<td id="S3.T1.2.12.12.1" class="ltx_td ltx_align_center ltx_border_r">Voxel-L</td>
<td id="S3.T1.2.12.12.2" class="ltx_td ltx_align_left">[28, 56, 112, 224]</td>
<td id="S3.T1.2.12.12.3" class="ltx_td ltx_align_center">8.27</td>
<td id="S3.T1.2.12.12.4" class="ltx_td ltx_align_center">660</td>
<td id="S3.T1.2.12.12.5" class="ltx_td ltx_align_center ltx_border_r">299</td>
<td id="S3.T1.2.12.12.6" class="ltx_td ltx_align_center"><span id="S3.T1.2.12.12.6.1" class="ltx_text ltx_font_bold">67.35</span></td>
<td id="S3.T1.2.12.12.7" class="ltx_td ltx_align_center ltx_border_r">84.17</td>
<td id="S3.T1.2.12.12.8" class="ltx_td ltx_align_center"><span id="S3.T1.2.12.12.8.1" class="ltx_text ltx_font_bold">70.47</span></td>
<td id="S3.T1.2.12.12.9" class="ltx_td ltx_align_center"><span id="S3.T1.2.12.12.9.1" class="ltx_text ltx_font_bold">77.44</span></td>
</tr>
<tr id="S3.T1.2.13.13" class="ltx_tr">
<td id="S3.T1.2.13.13.1" class="ltx_td ltx_align_center ltx_border_r">MVF-L</td>
<td id="S3.T1.2.13.13.2" class="ltx_td ltx_align_left">[96, 192, 384, 384]</td>
<td id="S3.T1.2.13.13.3" class="ltx_td ltx_align_center">29.67</td>
<td id="S3.T1.2.13.13.4" class="ltx_td ltx_align_center">704</td>
<td id="S3.T1.2.13.13.5" class="ltx_td ltx_align_center ltx_border_r">390</td>
<td id="S3.T1.2.13.13.6" class="ltx_td ltx_align_center">63.32</td>
<td id="S3.T1.2.13.13.7" class="ltx_td ltx_align_center ltx_border_r">83.79</td>
<td id="S3.T1.2.13.13.8" class="ltx_td ltx_align_center">66.87</td>
<td id="S3.T1.2.13.13.9" class="ltx_td ltx_align_center">75.34</td>
</tr>
<tr id="S3.T1.2.14.14" class="ltx_tr" style="background-color:#EFEFEF;">
<td id="S3.T1.2.14.14.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T1.2.14.14.1.1" class="ltx_text" style="background-color:#EFEFEF;">Pillar-S+</span></td>
<td id="S3.T1.2.14.14.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T1.2.14.14.2.1" class="ltx_text" style="background-color:#EFEFEF;">[42, 84, 168, 168]</span></td>
<td id="S3.T1.2.14.14.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.2.14.14.3.1" class="ltx_text" style="background-color:#EFEFEF;">2.83</span></td>
<td id="S3.T1.2.14.14.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.2.14.14.4.1" class="ltx_text" style="background-color:#EFEFEF;">121</span></td>
<td id="S3.T1.2.14.14.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T1.2.14.14.5.1" class="ltx_text" style="background-color:#EFEFEF;">79</span></td>
<td id="S3.T1.2.14.14.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.2.14.14.6.1" class="ltx_text ltx_font_bold" style="background-color:#EFEFEF;">70.64</span></td>
<td id="S3.T1.2.14.14.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T1.2.14.14.7.1" class="ltx_text ltx_font_bold" style="background-color:#EFEFEF;">87.86</span></td>
<td id="S3.T1.2.14.14.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.2.14.14.8.1" class="ltx_text" style="background-color:#EFEFEF;">73.48</span></td>
<td id="S3.T1.2.14.14.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.2.14.14.9.1" class="ltx_text" style="background-color:#EFEFEF;">79.95</span></td>
</tr>
<tr id="S3.T1.2.15.15" class="ltx_tr" style="background-color:#EFEFEF;">
<td id="S3.T1.2.15.15.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T1.2.15.15.1.1" class="ltx_text" style="background-color:#EFEFEF;">Voxel-S+</span></td>
<td id="S3.T1.2.15.15.2" class="ltx_td ltx_align_left"><span id="S3.T1.2.15.15.2.1" class="ltx_text" style="background-color:#EFEFEF;">[12, 24, 48, 96]</span></td>
<td id="S3.T1.2.15.15.3" class="ltx_td ltx_align_center"><span id="S3.T1.2.15.15.3.1" class="ltx_text" style="background-color:#EFEFEF;">1.53</span></td>
<td id="S3.T1.2.15.15.4" class="ltx_td ltx_align_center"><span id="S3.T1.2.15.15.4.1" class="ltx_text" style="background-color:#EFEFEF;">121</span></td>
<td id="S3.T1.2.15.15.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T1.2.15.15.5.1" class="ltx_text" style="background-color:#EFEFEF;">169</span></td>
<td id="S3.T1.2.15.15.6" class="ltx_td ltx_align_center"><span id="S3.T1.2.15.15.6.1" class="ltx_text" style="background-color:#EFEFEF;">70.61</span></td>
<td id="S3.T1.2.15.15.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T1.2.15.15.7.1" class="ltx_text" style="background-color:#EFEFEF;">86.83</span></td>
<td id="S3.T1.2.15.15.8" class="ltx_td ltx_align_center"><span id="S3.T1.2.15.15.8.1" class="ltx_text ltx_font_bold" style="background-color:#EFEFEF;">74.26</span></td>
<td id="S3.T1.2.15.15.9" class="ltx_td ltx_align_center"><span id="S3.T1.2.15.15.9.1" class="ltx_text ltx_font_bold" style="background-color:#EFEFEF;">80.37</span></td>
</tr>
<tr id="S3.T1.2.16.16" class="ltx_tr" style="background-color:#EFEFEF;">
<td id="S3.T1.2.16.16.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T1.2.16.16.1.1" class="ltx_text" style="background-color:#EFEFEF;">MVF-S+</span></td>
<td id="S3.T1.2.16.16.2" class="ltx_td ltx_align_left"><span id="S3.T1.2.16.16.2.1" class="ltx_text" style="background-color:#EFEFEF;">[44, 88, 176, 176]</span></td>
<td id="S3.T1.2.16.16.3" class="ltx_td ltx_align_center"><span id="S3.T1.2.16.16.3.1" class="ltx_text" style="background-color:#EFEFEF;">6.38</span></td>
<td id="S3.T1.2.16.16.4" class="ltx_td ltx_align_center"><span id="S3.T1.2.16.16.4.1" class="ltx_text" style="background-color:#EFEFEF;">148</span></td>
<td id="S3.T1.2.16.16.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T1.2.16.16.5.1" class="ltx_text" style="background-color:#EFEFEF;">186</span></td>
<td id="S3.T1.2.16.16.6" class="ltx_td ltx_align_center"><span id="S3.T1.2.16.16.6.1" class="ltx_text" style="background-color:#EFEFEF;">67.46</span></td>
<td id="S3.T1.2.16.16.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T1.2.16.16.7.1" class="ltx_text" style="background-color:#EFEFEF;">86.52</span></td>
<td id="S3.T1.2.16.16.8" class="ltx_td ltx_align_center"><span id="S3.T1.2.16.16.8.1" class="ltx_text" style="background-color:#EFEFEF;">70.87</span></td>
<td id="S3.T1.2.16.16.9" class="ltx_td ltx_align_center"><span id="S3.T1.2.16.16.9.1" class="ltx_text" style="background-color:#EFEFEF;">78.18</span></td>
</tr>
<tr id="S3.T1.2.17.17" class="ltx_tr" style="background-color:#EFEFEF;">
<td id="S3.T1.2.17.17.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T1.2.17.17.1.1" class="ltx_text" style="background-color:#EFEFEF;">Pillar-B+</span></td>
<td id="S3.T1.2.17.17.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T1.2.17.17.2.1" class="ltx_text" style="background-color:#EFEFEF;">[64, 128, 256, 256]</span></td>
<td id="S3.T1.2.17.17.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.2.17.17.3.1" class="ltx_text" style="background-color:#EFEFEF;">6.53</span></td>
<td id="S3.T1.2.17.17.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.2.17.17.4.1" class="ltx_text" style="background-color:#EFEFEF;">281</span></td>
<td id="S3.T1.2.17.17.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T1.2.17.17.5.1" class="ltx_text" style="background-color:#EFEFEF;">103</span></td>
<td id="S3.T1.2.17.17.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.2.17.17.6.1" class="ltx_text ltx_font_bold" style="background-color:#EFEFEF;">71.37</span></td>
<td id="S3.T1.2.17.17.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T1.2.17.17.7.1" class="ltx_text ltx_font_bold" style="background-color:#EFEFEF;">88.13</span></td>
<td id="S3.T1.2.17.17.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.2.17.17.8.1" class="ltx_text" style="background-color:#EFEFEF;">73.93</span></td>
<td id="S3.T1.2.17.17.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.2.17.17.9.1" class="ltx_text" style="background-color:#EFEFEF;">80.28</span></td>
</tr>
<tr id="S3.T1.2.18.18" class="ltx_tr" style="background-color:#EFEFEF;">
<td id="S3.T1.2.18.18.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T1.2.18.18.1.1" class="ltx_text" style="background-color:#EFEFEF;">Voxel-B+</span></td>
<td id="S3.T1.2.18.18.2" class="ltx_td ltx_align_left"><span id="S3.T1.2.18.18.2.1" class="ltx_text" style="background-color:#EFEFEF;">[18, 36, 72, 144]</span></td>
<td id="S3.T1.2.18.18.3" class="ltx_td ltx_align_center"><span id="S3.T1.2.18.18.3.1" class="ltx_text" style="background-color:#EFEFEF;">3.42</span></td>
<td id="S3.T1.2.18.18.4" class="ltx_td ltx_align_center"><span id="S3.T1.2.18.18.4.1" class="ltx_text" style="background-color:#EFEFEF;">272</span></td>
<td id="S3.T1.2.18.18.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T1.2.18.18.5.1" class="ltx_text" style="background-color:#EFEFEF;">226</span></td>
<td id="S3.T1.2.18.18.6" class="ltx_td ltx_align_center"><span id="S3.T1.2.18.18.6.1" class="ltx_text" style="background-color:#EFEFEF;">71.36</span></td>
<td id="S3.T1.2.18.18.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T1.2.18.18.7.1" class="ltx_text" style="background-color:#EFEFEF;">87.33</span></td>
<td id="S3.T1.2.18.18.8" class="ltx_td ltx_align_center"><span id="S3.T1.2.18.18.8.1" class="ltx_text ltx_font_bold" style="background-color:#EFEFEF;">74.76</span></td>
<td id="S3.T1.2.18.18.9" class="ltx_td ltx_align_center"><span id="S3.T1.2.18.18.9.1" class="ltx_text ltx_font_bold" style="background-color:#EFEFEF;">80.74</span></td>
</tr>
<tr id="S3.T1.2.19.19" class="ltx_tr" style="background-color:#EFEFEF;">
<td id="S3.T1.2.19.19.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S3.T1.2.19.19.1.1" class="ltx_text" style="background-color:#EFEFEF;">MVF-B+</span></td>
<td id="S3.T1.2.19.19.2" class="ltx_td ltx_align_left ltx_border_b"><span id="S3.T1.2.19.19.2.1" class="ltx_text" style="background-color:#EFEFEF;">[68, 136, 272, 272]</span></td>
<td id="S3.T1.2.19.19.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S3.T1.2.19.19.3.1" class="ltx_text" style="background-color:#EFEFEF;">15.02</span></td>
<td id="S3.T1.2.19.19.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S3.T1.2.19.19.4.1" class="ltx_text" style="background-color:#EFEFEF;">353</span></td>
<td id="S3.T1.2.19.19.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S3.T1.2.19.19.5.1" class="ltx_text" style="background-color:#EFEFEF;">291</span></td>
<td id="S3.T1.2.19.19.6" class="ltx_td ltx_align_center ltx_border_b"><span id="S3.T1.2.19.19.6.1" class="ltx_text" style="background-color:#EFEFEF;">68.19</span></td>
<td id="S3.T1.2.19.19.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S3.T1.2.19.19.7.1" class="ltx_text" style="background-color:#EFEFEF;">86.82</span></td>
<td id="S3.T1.2.19.19.8" class="ltx_td ltx_align_center ltx_border_b"><span id="S3.T1.2.19.19.8.1" class="ltx_text" style="background-color:#EFEFEF;">71.42</span></td>
<td id="S3.T1.2.19.19.9" class="ltx_td ltx_align_center ltx_border_b"><span id="S3.T1.2.19.19.9.1" class="ltx_text" style="background-color:#EFEFEF;">78.51</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.3.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S3.T1.4.2" class="ltx_text" style="font-size:90%;">Comparison of the pillar, voxel and MVF based networks with model scales from tiny, small, base to large. We report the L2 3D and BEV APH on vehicle and pedestrian on the validation set of WOD. Groups 1 and 2 correspond to the regular and enhanced versions.</span></figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Backbone and Neck</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">A backbone performs further feature abstraction based on the preliminary features extracted by the grid encoder. For fair comparisons, we utilize ResNet-18 as the backbone, since it is commonly used in the previous works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. Specifically, we make use of sparse 2D convolutions in the backbone with the pillar or MVF based encoder, and sparse 3D convolutions in the backbone with the voxel based encoder. A neck can be then utilized to aggregate features from the backbone for enlarging receptive field and fusing multi-scale context. However, how to design an effective neck has not been well explored for object detection in point clouds compared to in images. We aim to close this gap by integrating the advanced neck designs from 2D object detection, such as BiFPN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> using improved multi-level feature fusion or ASPP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> using convolutions with multiple dilated rates on a single feature level, into the model architectures of 3D object detection.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Detection Head</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">In the pioneering works of SECOND <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> and PointPillars <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, the anchor based detection head is employed to pre-define the axis-aligned anchors at each location on the input feature maps to head. CenterPoint <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> instead represents each object by its center point, and predicts a centerness heatmap where the regression of bounding box is realized at each center location.
Due to its simplicity and superior performance, we adopt the center based detection head in all our networks. We show a set of simple modifications in head, such as feature upsampling, multi-grouping and IoU branch, improve the performance notably.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we start from introducing the experimental setup including datasets and implementation details. We then perform comprehensive network design studies on each component in a 3D object detection model. In the end, we present extensive comparisons with the state-of-the-art methods on the two popular benchmarks.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Setup</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We conduct experiments on two large-scale autonomous driving benchmarks: Waymo Open Dataset (WOD) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> and nuScenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">WOD</span> consists of 798 sequences (160K frames) for training and 202 sequences (40K frames) for validation, which are captured with 5 LiDARs at 10Hz. Following the official protocol, we use the average precision (AP) and average precision weighted by heading (APH) as the evaluation metrics. We break down the performance into two difficulty levels, L1 and L2, where the former evaluates objects with at least 5 points and the latter covers all objects with at least one point. We set the IoU thresholds for vehicle, pedestrian and cyclist to 0.7, 0.5 and 0.5. In addition to 3D AP/APH, we also report the results under BEV. <span id="S4.SS1.p1.1.2" class="ltx_text ltx_font_bold">nuScenes</span> contains 1,000 scenes of roughly 20 seconds each, captured by a 32-beam LiDAR at 20Hz. Annotations are available on keyframes at 2Hz. We follow the official evaluation metrics by averaging over 10 classes under mean average precision (mAP) and nuScenes detection score (NDS), and the latter is a weighted average of mAP as well as ATE, ASE, AOE, AVE and AAE that are used to respectively measure the translation, scale, orientation, velocity and attribute related errors.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.2.1.1" class="ltx_tr">
<th id="S4.T2.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T2.2.1.1.1.1" class="ltx_text">Method</span></th>
<th id="S4.T2.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2">Vehicle L1</th>
<th id="S4.T2.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2">Vehicle L2</th>
<th id="S4.T2.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2">Pedestrian L1</th>
<th id="S4.T2.2.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2">Pedestrian L2</th>
</tr>
<tr id="S4.T2.2.2.2" class="ltx_tr">
<th id="S4.T2.2.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AP</th>
<th id="S4.T2.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">APH</th>
<th id="S4.T2.2.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AP</th>
<th id="S4.T2.2.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">APH</th>
<th id="S4.T2.2.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AP</th>
<th id="S4.T2.2.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">APH</th>
<th id="S4.T2.2.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AP</th>
<th id="S4.T2.2.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">APH</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.2.3.1" class="ltx_tr">
<th id="S4.T2.2.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Neck of PillarNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>
</th>
<td id="S4.T2.2.3.1.2" class="ltx_td ltx_align_center ltx_border_t">91.39</td>
<td id="S4.T2.2.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">90.58</td>
<td id="S4.T2.2.3.1.4" class="ltx_td ltx_align_center ltx_border_t">84.54</td>
<td id="S4.T2.2.3.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">83.72</td>
<td id="S4.T2.2.3.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.3.1.6.1" class="ltx_text ltx_font_bold">87.90</span></td>
<td id="S4.T2.2.3.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.3.1.7.1" class="ltx_text ltx_font_bold">83.02</span></td>
<td id="S4.T2.2.3.1.8" class="ltx_td ltx_align_center ltx_border_t">81.93</td>
<td id="S4.T2.2.3.1.9" class="ltx_td ltx_align_center ltx_border_t">77.20</td>
</tr>
<tr id="S4.T2.2.4.2" class="ltx_tr">
<th id="S4.T2.2.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">FPN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>
</th>
<td id="S4.T2.2.4.2.2" class="ltx_td ltx_align_center">92.17</td>
<td id="S4.T2.2.4.2.3" class="ltx_td ltx_align_center ltx_border_r">91.35</td>
<td id="S4.T2.2.4.2.4" class="ltx_td ltx_align_center">85.96</td>
<td id="S4.T2.2.4.2.5" class="ltx_td ltx_align_center ltx_border_r">85.13</td>
<td id="S4.T2.2.4.2.6" class="ltx_td ltx_align_center">87.88</td>
<td id="S4.T2.2.4.2.7" class="ltx_td ltx_align_center ltx_border_r">82.91</td>
<td id="S4.T2.2.4.2.8" class="ltx_td ltx_align_center">82.05</td>
<td id="S4.T2.2.4.2.9" class="ltx_td ltx_align_center">77.23</td>
</tr>
<tr id="S4.T2.2.5.3" class="ltx_tr">
<th id="S4.T2.2.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">BiFPN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>
</th>
<td id="S4.T2.2.5.3.2" class="ltx_td ltx_align_center">92.71</td>
<td id="S4.T2.2.5.3.3" class="ltx_td ltx_align_center ltx_border_r">91.90</td>
<td id="S4.T2.2.5.3.4" class="ltx_td ltx_align_center">86.92</td>
<td id="S4.T2.2.5.3.5" class="ltx_td ltx_align_center ltx_border_r">86.09</td>
<td id="S4.T2.2.5.3.6" class="ltx_td ltx_align_center">87.86</td>
<td id="S4.T2.2.5.3.7" class="ltx_td ltx_align_center ltx_border_r">82.88</td>
<td id="S4.T2.2.5.3.8" class="ltx_td ltx_align_center">82.05</td>
<td id="S4.T2.2.5.3.9" class="ltx_td ltx_align_center">77.23</td>
</tr>
<tr id="S4.T2.2.6.4" class="ltx_tr">
<th id="S4.T2.2.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Plain</th>
<td id="S4.T2.2.6.4.2" class="ltx_td ltx_align_center ltx_border_t">91.01</td>
<td id="S4.T2.2.6.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">90.19</td>
<td id="S4.T2.2.6.4.4" class="ltx_td ltx_align_center ltx_border_t">83.86</td>
<td id="S4.T2.2.6.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">83.04</td>
<td id="S4.T2.2.6.4.6" class="ltx_td ltx_align_center ltx_border_t">87.59</td>
<td id="S4.T2.2.6.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">82.61</td>
<td id="S4.T2.2.6.4.8" class="ltx_td ltx_align_center ltx_border_t">81.52</td>
<td id="S4.T2.2.6.4.9" class="ltx_td ltx_align_center ltx_border_t">76.71</td>
</tr>
<tr id="S4.T2.2.7.5" class="ltx_tr">
<th id="S4.T2.2.7.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Dilated Block <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</th>
<td id="S4.T2.2.7.5.2" class="ltx_td ltx_align_center">92.70</td>
<td id="S4.T2.2.7.5.3" class="ltx_td ltx_align_center ltx_border_r">91.90</td>
<td id="S4.T2.2.7.5.4" class="ltx_td ltx_align_center">86.61</td>
<td id="S4.T2.2.7.5.5" class="ltx_td ltx_align_center ltx_border_r">85.79</td>
<td id="S4.T2.2.7.5.6" class="ltx_td ltx_align_center">87.84</td>
<td id="S4.T2.2.7.5.7" class="ltx_td ltx_align_center ltx_border_r">82.91</td>
<td id="S4.T2.2.7.5.8" class="ltx_td ltx_align_center"><span id="S4.T2.2.7.5.8.1" class="ltx_text ltx_font_bold">82.09</span></td>
<td id="S4.T2.2.7.5.9" class="ltx_td ltx_align_center"><span id="S4.T2.2.7.5.9.1" class="ltx_text ltx_font_bold">77.29</span></td>
</tr>
<tr id="S4.T2.2.8.6" class="ltx_tr">
<th id="S4.T2.2.8.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">ASPP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>
</th>
<td id="S4.T2.2.8.6.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.2.8.6.2.1" class="ltx_text ltx_font_bold">92.77</span></td>
<td id="S4.T2.2.8.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T2.2.8.6.3.1" class="ltx_text ltx_font_bold">91.94</span></td>
<td id="S4.T2.2.8.6.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.2.8.6.4.1" class="ltx_text ltx_font_bold">86.99</span></td>
<td id="S4.T2.2.8.6.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T2.2.8.6.5.1" class="ltx_text ltx_font_bold">86.14</span></td>
<td id="S4.T2.2.8.6.6" class="ltx_td ltx_align_center ltx_border_b">87.74</td>
<td id="S4.T2.2.8.6.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">82.85</td>
<td id="S4.T2.2.8.6.8" class="ltx_td ltx_align_center ltx_border_b">82.00</td>
<td id="S4.T2.2.8.6.9" class="ltx_td ltx_align_center ltx_border_b">77.26</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.3.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.4.2" class="ltx_text" style="font-size:90%;">Comparison of different neck modules integrated in our networks. Groups 1 and 2 correspond to the multi-scale and sing-scale necks, respectively. We report the L1 and L2 BEV AP and APH for vehicle and pedestrian on the validation set of WOD.</span></figcaption>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">We implement our networks in PyTorch. All models are trained by using AdamW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> as the optimizer and under the one-cycle <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> learning rate schedule.
For <span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">WOD</span>, the detection range is set to [-76.8m, 76.8m] horizontally and [-2m, 4m] vertically. Our pillar size is 0.075m in x/y-axis (and 0.15m in z-axis for voxel based).
For MVF based models, we keep the same pillar size and use [1.8<sup id="S4.SS1.p2.1.2" class="ltx_sup"><span id="S4.SS1.p2.1.2.1" class="ltx_text ltx_font_italic">∘</span></sup>, 0.2m] for yaw and z-axis in the cylindrical view. We train each model for 12 epochs and take 3 frames as input unless otherwise specified. For inference, we use the non-maximum suppression (NMS) thresholds of 0.7, 0.2 and 0.25 for vehicle, pedestrian and cyclist. As for <span id="S4.SS1.p2.1.3" class="ltx_text ltx_font_bold">nuScenes</span>, we take 10 frames as input, and set detection range to [-50.4m, 50.4m] horizontally and [-5m, 3m] vertically (0.2m in z-axis for voxel based). For inference, we use NMS threshold of 0.2 for all classes. Other settings are the same as WOD. We report the inference latency on a single NVIDIA TITAN RTX GPU.
More implementation details can be found in Appendix.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2305.04925/assets/x2.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="368" height="356" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S4.F3.3.2" class="ltx_text" style="font-size:90%;">Comparison of the learning behaviors between the pillar and voxel based models. We report the L2 3D and BEV APH of vehicle on the validation set on WOD.</span></figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Network Design Study</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We perform extensive studies to analyze and understand the contribution of each individual network design. We first evaluate the impact of grid encoder, and demonstrate the importance of neck module, then investigate the effect of resolution, finally summarize the components one by one to show the improvement roadmap.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.2.2" class="ltx_tr">
<th id="S4.T3.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="S4.T3.2.2.3.1" class="ltx_text" style="font-size:90%;">In Size</span></th>
<th id="S4.T3.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.7pt;padding-right:3.7pt;">
<span id="S4.T3.1.1.1.1" class="ltx_text" style="font-size:90%;">Backbone </span><math id="S4.T3.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T3.1.1.1.m1.1a"><mo mathsize="90%" stretchy="false" id="S4.T3.1.1.1.m1.1.1" xref="S4.T3.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T3.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.7pt;padding-right:3.7pt;">
<span id="S4.T3.2.2.2.1" class="ltx_text" style="font-size:90%;">Head </span><math id="S4.T3.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.2.2.2.m1.1a"><mo mathsize="90%" stretchy="false" id="S4.T3.2.2.2.m1.1.1" xref="S4.T3.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.m1.1b"><ci id="S4.T3.2.2.2.m1.1.1.cmml" xref="S4.T3.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T3.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="S4.T3.2.2.4.1" class="ltx_text" style="font-size:90%;">Out Size</span></th>
<th id="S4.T3.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="S4.T3.2.2.5.1" class="ltx_text" style="font-size:90%;">Veh</span></th>
<th id="S4.T3.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="S4.T3.2.2.6.1" class="ltx_text" style="font-size:90%;">Ped</span></th>
<th id="S4.T3.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="S4.T3.2.2.7.1" class="ltx_text" style="font-size:90%;">Latency</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.2.3.1" class="ltx_tr">
<td id="S4.T3.2.3.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="S4.T3.2.3.1.1.1" class="ltx_text" style="font-size:90%;">0.3</span></td>
<td id="S4.T3.2.3.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="S4.T3.2.3.1.2.1" class="ltx_text" style="font-size:90%;">1</span></td>
<td id="S4.T3.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="S4.T3.2.3.1.3.1" class="ltx_text" style="font-size:90%;">1</span></td>
<td id="S4.T3.2.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="S4.T3.2.3.1.4.1" class="ltx_text" style="font-size:90%;">0.3</span></td>
<td id="S4.T3.2.3.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="S4.T3.2.3.1.5.1" class="ltx_text" style="font-size:90%;">65.0</span></td>
<td id="S4.T3.2.3.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="S4.T3.2.3.1.6.1" class="ltx_text" style="font-size:90%;">67.2</span></td>
<td id="S4.T3.2.3.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="S4.T3.2.3.1.7.1" class="ltx_text" style="font-size:90%;">255</span></td>
</tr>
<tr id="S4.T3.2.4.2" class="ltx_tr">
<td id="S4.T3.2.4.2.1" class="ltx_td ltx_align_left" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="S4.T3.2.4.2.1.1" class="ltx_text" style="font-size:90%;">0.075</span></td>
<td id="S4.T3.2.4.2.2" class="ltx_td ltx_align_center" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="S4.T3.2.4.2.2.1" class="ltx_text" style="font-size:90%;">8</span></td>
<td id="S4.T3.2.4.2.3" class="ltx_td ltx_align_center" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="S4.T3.2.4.2.3.1" class="ltx_text" style="font-size:90%;">1</span></td>
<td id="S4.T3.2.4.2.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="S4.T3.2.4.2.4.1" class="ltx_text" style="font-size:90%;">0.6</span></td>
<td id="S4.T3.2.4.2.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="S4.T3.2.4.2.5.1" class="ltx_text" style="font-size:90%;">62.8</span></td>
<td id="S4.T3.2.4.2.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="S4.T3.2.4.2.6.1" class="ltx_text" style="font-size:90%;">66.6</span></td>
<td id="S4.T3.2.4.2.7" class="ltx_td ltx_align_center" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="S4.T3.2.4.2.7.1" class="ltx_text" style="font-size:90%;">131</span></td>
</tr>
<tr id="S4.T3.2.5.3" class="ltx_tr">
<td id="S4.T3.2.5.3.1" class="ltx_td ltx_align_left ltx_border_b" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="S4.T3.2.5.3.1.1" class="ltx_text" style="font-size:90%;">0.075</span></td>
<td id="S4.T3.2.5.3.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="S4.T3.2.5.3.2.1" class="ltx_text" style="font-size:90%;">8</span></td>
<td id="S4.T3.2.5.3.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="S4.T3.2.5.3.3.1" class="ltx_text" style="font-size:90%;">2</span></td>
<td id="S4.T3.2.5.3.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="S4.T3.2.5.3.4.1" class="ltx_text" style="font-size:90%;">0.3</span></td>
<td id="S4.T3.2.5.3.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="S4.T3.2.5.3.5.1" class="ltx_text" style="font-size:90%;">64.8</span></td>
<td id="S4.T3.2.5.3.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="S4.T3.2.5.3.6.1" class="ltx_text" style="font-size:90%;">69.0</span></td>
<td id="S4.T3.2.5.3.7" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="S4.T3.2.5.3.7.1" class="ltx_text" style="font-size:90%;">173</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparison of different resolutions. We adopt the pillar size (m) to represent the resolutions of input grids and output features (consumed by head). We evaluate the overall downsampling rate in backbone and the upsampling rate in head. We report the L2 3D APH and latency (ms) on the validation set of WOD.</figcaption>
</figure>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Study of Grid Encoders</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">We begin with evaluating the three representative grid encoders as introduced in Section <a href="#S3.SS1" title="3.1 Grid Encoder ‣ 3 Network Architecture Overview ‣ PillarNeXt: Rethinking Network Designs for 3D Object Detection in LiDAR Point Clouds" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>, i.e., pillar, voxel and MVF. Although the three encoders have been proposed for a long time, they have never been fairly compared under the same network architecture and grid resolution. Here, we experiment with a sparse ResNet-18 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> as the backbone for its effectiveness and efficiency. We scale the width of the backbone and obtain a series of networks ranging from tiny, small, base to large, namely Pillar/Voxel/MVF-T/S/B/L. Table <a href="#S3.T1" title="Table 1 ‣ 3.1 Grid Encoder ‣ 3 Network Architecture Overview ‣ PillarNeXt: Rethinking Network Designs for 3D Object Detection in LiDAR Point Clouds" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> lists the channel and parameter numbers, FLOPs, and latency of each model. Note there is no Voxel-T as FLOPs of voxel based models are much higher and the smallest one starts from a similar computational cost as Pillar-S.</p>
</div>
<div id="S4.SS2.SSS1.p2" class="ltx_para">
<p id="S4.SS2.SSS1.p2.1" class="ltx_p">For the first group in Table <a href="#S3.T1" title="Table 1 ‣ 3.1 Grid Encoder ‣ 3 Network Architecture Overview ‣ PillarNeXt: Rethinking Network Designs for 3D Object Detection in LiDAR Point Clouds" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we compare the three grid encoders with different model scales under the regular training schedule (i.e., 12 epochs), which is commonly adopted. As can be seen in the table, under BEV APH, the pillar encoder performs favorably on vehicle and is comparable on pedestrian, while with remarkably lower latency. This conclusion is further supported by the per-class comparison on nuScenes in Table <a href="#S4.T8" title="Table 8 ‣ 4.3 Comparison with State-of-the-Art on WOD ‣ 4 Experiments ‣ PillarNeXt: Rethinking Network Designs for 3D Object Detection in LiDAR Point Clouds" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. Note pedestrians usually take only a few pillars in the perspective of BEV, nevertheless, the pillar encoder is sufficient to achieve superior or on-par performance, including for the small objects.</p>
</div>
<div id="S4.SS2.SSS1.p3" class="ltx_para">
<p id="S4.SS2.SSS1.p3.1" class="ltx_p">However, the pillar encoder still lacks behind under 3D APH. To further study the reasons for this gap, we enhance the models by extending the training schedule to 36 epochs with an extra IoU regression loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, and incorporating an IoU score branch <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> in the multi-group detection head (one for vehicle and the other for pedestrian and cyclist, as illustrated in Figure <a href="#S2.F2" title="Figure 2 ‣ 2 Related Work ‣ PillarNeXt: Rethinking Network Designs for 3D Object Detection in LiDAR Point Clouds" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).
We call the models trained under this enhanced strategy as Pillar/Voxel/MVF+. As compared in the second group of Table <a href="#S3.T1" title="Table 1 ‣ 3.1 Grid Encoder ‣ 3 Network Architecture Overview ‣ PillarNeXt: Rethinking Network Designs for 3D Object Detection in LiDAR Point Clouds" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the pillar models achieve comparable or even better results than the voxel models on vehicle under 3D APH, while running considerably faster. We hypothesize that without explicit height modeling, the pillar based networks require refined designs such as longer training to be able to fully converge. This challenges the common belief that the pillar encoder loses height information, and suggests that the fine-grained local geometrical modeling may not be necessary.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2305.04925/assets/roadmap-3.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="339" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.3.2" class="ltx_text" style="font-size:90%;">Improvement by each individual component to illustrate the performance boosting roadmap. We report the L2 3D and BEV APH of vehicle and pedestrian on the validation set of WOD.</span></figcaption>
</figure>
<div id="S4.SS2.SSS1.p4" class="ltx_para">
<p id="S4.SS2.SSS1.p4.1" class="ltx_p">This counter-intuitive result motivates us to rethink how to efficiently and effectively allocate the computational resources for a 3D object detection network. The pillar based models allocate resources only in the BEV space, while the voxel and MVF based methods also spend computations along the height dimension. When comparing these methods, previous works fail to take into account the computational budget. In our experiments, we show that under similar FLOPs, allocating computations to the height dimension is not beneficial. We conclude that investing all FLOPs in the BEV space is not only more efficient but also more effective, as shown in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ PillarNeXt: Rethinking Network Designs for 3D Object Detection in LiDAR Point Clouds" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and Tables <a href="#S3.T1" title="Table 1 ‣ 3.1 Grid Encoder ‣ 3 Network Architecture Overview ‣ PillarNeXt: Rethinking Network Designs for 3D Object Detection in LiDAR Point Clouds" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and <a href="#S4.T8" title="Table 8 ‣ 4.3 Comparison with State-of-the-Art on WOD ‣ 4 Experiments ‣ PillarNeXt: Rethinking Network Designs for 3D Object Detection in LiDAR Point Clouds" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
<div id="S4.SS2.SSS1.p5" class="ltx_para">
<p id="S4.SS2.SSS1.p5.1" class="ltx_p">It also reveals that training matters. Most previous works usually employ the regular or short training schedule for comparison, which could result in different conclusions. We demonstrate the learning behaviors of pillar and voxel based networks in Figure <a href="#S4.F3" title="Figure 3 ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ PillarNeXt: Rethinking Network Designs for 3D Object Detection in LiDAR Point Clouds" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Interestingly, the pillar model is found to converge comparably to the voxel model in BEV APH. While for 3D APH, the pillar model converges much slower than the voxel model. However, this gap diminishes when training continues for sufficient epochs, suggesting that the performance gap in 3D APH between pillar and voxel reported by the previous methods are partially caused by their different convergence rates, instead of the more fine-grained geometrical modeling in voxel.</p>
</div>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Study of Neck Modules</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">In the previous study, we conclude that the different local point aggregators may not be essential to the final results. In the following, we show that a simple upgrade on the network architecture improves the performance greatly. In particular, we focus on the neck module design, which has not been well explored in 3D object detection.</p>
</div>
<div id="S4.SS2.SSS2.p2" class="ltx_para">
<p id="S4.SS2.SSS2.p2.1" class="ltx_p">Most current networks in the field rely on the multi-scale fusion as used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, which upsample feature maps from different stages to the same resolution and then concatenate them. How to design a neck module to conduct more effective feature aggregation has been extensively researched in 2D object detection, but most advanced techniques have not been adopted in 3D object detection. We first integrate with the two popular designs, i.e., FPN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> and BiFPN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>. As shown in the first group of Table <a href="#S4.T2" title="Table 2 ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ PillarNeXt: Rethinking Network Designs for 3D Object Detection in LiDAR Point Clouds" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we observe up to 2.38% improvement on vehicle over the neck developed in the most recent work PillarNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>.</p>
</div>
<div id="S4.SS2.SSS2.p3" class="ltx_para">
<p id="S4.SS2.SSS2.p3.1" class="ltx_p">One main goal of using multi-scale features is to deal with large variations of object scales. However, 3D objects in the BEV space do not suffer from such a problem. This motivates us to rethink whether the multi-scale representation is required for 3D object detection. We therefore investigate three single-scale neck modules. The baseline is a plain neck using a residual block without downsampling or upsampling, which gets inferior performance due to the limited receptive field. In YOLOF<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, it is argued that 2D object detector performs better when the receptive field matches with the object size. Inspired by this observation, we apply the dilated blocks as in YOLOF to enlarge the receptive field and yield better performance on vehicle. We also integrate with the ASPP block <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and obtain up to 2.45% improvement on vehicle compared with the neck used in PillarNet.
All the above designs achieve comparable performance on pedestrian. These comparisons collectively imply that the multi-scale features may not be necessary, instead, enlarging receptive field plays the key role.</p>
</div>
<div id="S4.SS2.SSS2.p4" class="ltx_para">
<p id="S4.SS2.SSS2.p4.1" class="ltx_p">This study demonstrates that simply adapting the neck modules from 2D object detection brings non-trivial improvements to 3D object detection, which is encouraging to explore more successful practices in the image domain to upgrade the network designs for point clouds.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T4.1.2.1" class="ltx_tr">
<th id="S4.T4.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;" rowspan="2"><span id="S4.T4.1.2.1.1.1" class="ltx_text">Method</span></th>
<th id="S4.T4.1.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;" rowspan="2"><span id="S4.T4.1.2.1.2.1" class="ltx_text">Frames</span></th>
<td id="S4.T4.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;" colspan="2">Vehicle L1</td>
<td id="S4.T4.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;" colspan="2">Vehicle L2</td>
<td id="S4.T4.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;" colspan="2">Pedestrian L1</td>
<td id="S4.T4.1.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;" colspan="2">Pedestrian L2</td>
<td id="S4.T4.1.2.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;" colspan="2">Cyclist L1</td>
<td id="S4.T4.1.2.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;" colspan="2">Cyclist L2</td>
</tr>
<tr id="S4.T4.1.3.2" class="ltx_tr">
<td id="S4.T4.1.3.2.1" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">AP</td>
<td id="S4.T4.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">APH</td>
<td id="S4.T4.1.3.2.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">AP</td>
<td id="S4.T4.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">APH</td>
<td id="S4.T4.1.3.2.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">AP</td>
<td id="S4.T4.1.3.2.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">APH</td>
<td id="S4.T4.1.3.2.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">AP</td>
<td id="S4.T4.1.3.2.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">APH</td>
<td id="S4.T4.1.3.2.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">AP</td>
<td id="S4.T4.1.3.2.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">APH</td>
<td id="S4.T4.1.3.2.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">AP</td>
<td id="S4.T4.1.3.2.12" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">APH</td>
</tr>
<tr id="S4.T4.1.4.3" class="ltx_tr">
<th id="S4.T4.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">SST-TS* <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>
</th>
<th id="S4.T4.1.4.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">1</th>
<td id="S4.T4.1.4.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">76.22</td>
<td id="S4.T4.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">75.79</td>
<td id="S4.T4.1.4.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">68.04</td>
<td id="S4.T4.1.4.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">67.64</td>
<td id="S4.T4.1.4.3.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">81.39</td>
<td id="S4.T4.1.4.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">74.05</td>
<td id="S4.T4.1.4.3.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">72.82</td>
<td id="S4.T4.1.4.3.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">65.93</td>
<td id="S4.T4.1.4.3.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T4.1.4.3.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T4.1.4.3.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T4.1.4.3.14" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
</tr>
<tr id="S4.T4.1.5.4" class="ltx_tr">
<th id="S4.T4.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">SWFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>
</th>
<th id="S4.T4.1.5.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">1</th>
<td id="S4.T4.1.5.4.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">77.8</td>
<td id="S4.T4.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">77.3</td>
<td id="S4.T4.1.5.4.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">69.2</td>
<td id="S4.T4.1.5.4.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">68.8</td>
<td id="S4.T4.1.5.4.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">80.9</td>
<td id="S4.T4.1.5.4.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">72.7</td>
<td id="S4.T4.1.5.4.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">72.5</td>
<td id="S4.T4.1.5.4.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">64.9</td>
<td id="S4.T4.1.5.4.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T4.1.5.4.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T4.1.5.4.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T4.1.5.4.14" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
</tr>
<tr id="S4.T4.1.6.5" class="ltx_tr">
<th id="S4.T4.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">PillarNet-18 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>
</th>
<th id="S4.T4.1.6.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">1</th>
<td id="S4.T4.1.6.5.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">78.24</td>
<td id="S4.T4.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">77.73</td>
<td id="S4.T4.1.6.5.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">70.40</td>
<td id="S4.T4.1.6.5.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">69.92</td>
<td id="S4.T4.1.6.5.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">79.80</td>
<td id="S4.T4.1.6.5.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">72.59</td>
<td id="S4.T4.1.6.5.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">71.57</td>
<td id="S4.T4.1.6.5.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">64.90</td>
<td id="S4.T4.1.6.5.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">70.40</td>
<td id="S4.T4.1.6.5.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">69.29</td>
<td id="S4.T4.1.6.5.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">67.75</td>
<td id="S4.T4.1.6.5.14" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">66.68</td>
</tr>
<tr id="S4.T4.1.7.6" class="ltx_tr">
<th id="S4.T4.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">AFDetV2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>
</th>
<th id="S4.T4.1.7.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">1</th>
<td id="S4.T4.1.7.6.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">77.64</td>
<td id="S4.T4.1.7.6.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">77.14</td>
<td id="S4.T4.1.7.6.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">69.68</td>
<td id="S4.T4.1.7.6.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">69.22</td>
<td id="S4.T4.1.7.6.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">80.19</td>
<td id="S4.T4.1.7.6.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">74.62</td>
<td id="S4.T4.1.7.6.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">72.16</td>
<td id="S4.T4.1.7.6.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">66.95</td>
<td id="S4.T4.1.7.6.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">73.72</td>
<td id="S4.T4.1.7.6.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">72.74</td>
<td id="S4.T4.1.7.6.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">71.06</td>
<td id="S4.T4.1.7.6.14" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">70.12</td>
</tr>
<tr id="S4.T4.1.8.7" class="ltx_tr">
<th id="S4.T4.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">PV-RCNN++* <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>
</th>
<th id="S4.T4.1.8.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">1</th>
<td id="S4.T4.1.8.7.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">79.25</td>
<td id="S4.T4.1.8.7.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">78.78</td>
<td id="S4.T4.1.8.7.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">70.61</td>
<td id="S4.T4.1.8.7.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">70.18</td>
<td id="S4.T4.1.8.7.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">81.83</td>
<td id="S4.T4.1.8.7.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">76.28</td>
<td id="S4.T4.1.8.7.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">73.17</td>
<td id="S4.T4.1.8.7.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">68.00</td>
<td id="S4.T4.1.8.7.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">73.72</td>
<td id="S4.T4.1.8.7.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">72.66</td>
<td id="S4.T4.1.8.7.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">71.21</td>
<td id="S4.T4.1.8.7.14" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">70.19</td>
</tr>
<tr id="S4.T4.1.9.8" class="ltx_tr">
<th id="S4.T4.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">PillarNeXt-B</th>
<th id="S4.T4.1.9.8.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">1</th>
<td id="S4.T4.1.9.8.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">78.40</td>
<td id="S4.T4.1.9.8.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">77.90</td>
<td id="S4.T4.1.9.8.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">70.27</td>
<td id="S4.T4.1.9.8.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">69.81</td>
<td id="S4.T4.1.9.8.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">82.53</td>
<td id="S4.T4.1.9.8.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">77.14</td>
<td id="S4.T4.1.9.8.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">74.90</td>
<td id="S4.T4.1.9.8.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">69.80</td>
<td id="S4.T4.1.9.8.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">73.21</td>
<td id="S4.T4.1.9.8.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">72.20</td>
<td id="S4.T4.1.9.8.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">70.58</td>
<td id="S4.T4.1.9.8.14" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">69.62</td>
</tr>
<tr id="S4.T4.1.10.9" class="ltx_tr">
<th id="S4.T4.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">PillarNet-18 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>
</th>
<th id="S4.T4.1.10.9.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">2</th>
<td id="S4.T4.1.10.9.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">79.59</td>
<td id="S4.T4.1.10.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">79.06</td>
<td id="S4.T4.1.10.9.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">71.56</td>
<td id="S4.T4.1.10.9.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">71.08</td>
<td id="S4.T4.1.10.9.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">82.11</td>
<td id="S4.T4.1.10.9.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">78.82</td>
<td id="S4.T4.1.10.9.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">74.49</td>
<td id="S4.T4.1.10.9.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">71.35</td>
<td id="S4.T4.1.10.9.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">70.41</td>
<td id="S4.T4.1.10.9.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">69.57</td>
<td id="S4.T4.1.10.9.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">68.27</td>
<td id="S4.T4.1.10.9.14" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">67.46</td>
</tr>
<tr id="S4.T4.1.11.10" class="ltx_tr">
<th id="S4.T4.1.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">PillarNet-34 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>
</th>
<th id="S4.T4.1.11.10.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">2</th>
<td id="S4.T4.1.11.10.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">79.98</td>
<td id="S4.T4.1.11.10.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">79.47</td>
<td id="S4.T4.1.11.10.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">72.00</td>
<td id="S4.T4.1.11.10.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">71.53</td>
<td id="S4.T4.1.11.10.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">82.52</td>
<td id="S4.T4.1.11.10.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">79.33</td>
<td id="S4.T4.1.11.10.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">75.00</td>
<td id="S4.T4.1.11.10.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">71.95</td>
<td id="S4.T4.1.11.10.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">70.51</td>
<td id="S4.T4.1.11.10.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">69.69</td>
<td id="S4.T4.1.11.10.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">68.38</td>
<td id="S4.T4.1.11.10.14" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">67.58</td>
</tr>
<tr id="S4.T4.1.12.11" class="ltx_tr">
<th id="S4.T4.1.12.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">PV-RCNN++* <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>
</th>
<th id="S4.T4.1.12.11.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">2</th>
<td id="S4.T4.1.12.11.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">80.17</td>
<td id="S4.T4.1.12.11.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">79.70</td>
<td id="S4.T4.1.12.11.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">72.14</td>
<td id="S4.T4.1.12.11.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">71.70</td>
<td id="S4.T4.1.12.11.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">83.48</td>
<td id="S4.T4.1.12.11.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">80.42</td>
<td id="S4.T4.1.12.11.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">75.54</td>
<td id="S4.T4.1.12.11.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">72.61</td>
<td id="S4.T4.1.12.11.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">74.63</td>
<td id="S4.T4.1.12.11.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">73.75</td>
<td id="S4.T4.1.12.11.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">72.35</td>
<td id="S4.T4.1.12.11.14" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">71.50</td>
</tr>
<tr id="S4.T4.1.13.12" class="ltx_tr">
<th id="S4.T4.1.13.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">RSN* <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>
</th>
<th id="S4.T4.1.13.12.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">3</th>
<td id="S4.T4.1.13.12.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">78.4</td>
<td id="S4.T4.1.13.12.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">78.1</td>
<td id="S4.T4.1.13.12.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">69.5</td>
<td id="S4.T4.1.13.12.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">69.1</td>
<td id="S4.T4.1.13.12.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">79.4</td>
<td id="S4.T4.1.13.12.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">76.2</td>
<td id="S4.T4.1.13.12.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">69.9</td>
<td id="S4.T4.1.13.12.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">67.0</td>
<td id="S4.T4.1.13.12.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T4.1.13.12.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T4.1.13.12.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T4.1.13.12.14" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
</tr>
<tr id="S4.T4.1.14.13" class="ltx_tr">
<th id="S4.T4.1.14.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">SST-TS* <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>
</th>
<th id="S4.T4.1.14.13.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">3</th>
<td id="S4.T4.1.14.13.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">78.66</td>
<td id="S4.T4.1.14.13.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">78.21</td>
<td id="S4.T4.1.14.13.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">69.98</td>
<td id="S4.T4.1.14.13.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">69.57</td>
<td id="S4.T4.1.14.13.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">83.81</td>
<td id="S4.T4.1.14.13.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">80.14</td>
<td id="S4.T4.1.14.13.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">75.94</td>
<td id="S4.T4.1.14.13.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">72.37</td>
<td id="S4.T4.1.14.13.11" class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td id="S4.T4.1.14.13.12" class="ltx_td ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td id="S4.T4.1.14.13.13" class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td id="S4.T4.1.14.13.14" class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
</tr>
<tr id="S4.T4.1.15.14" class="ltx_tr">
<th id="S4.T4.1.15.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">SWFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>
</th>
<th id="S4.T4.1.15.14.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">3</th>
<td id="S4.T4.1.15.14.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">79.4</td>
<td id="S4.T4.1.15.14.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">78.9</td>
<td id="S4.T4.1.15.14.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">71.1</td>
<td id="S4.T4.1.15.14.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">70.6</td>
<td id="S4.T4.1.15.14.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">82.9</td>
<td id="S4.T4.1.15.14.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">79.0</td>
<td id="S4.T4.1.15.14.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">74.8</td>
<td id="S4.T4.1.15.14.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">71.1</td>
<td id="S4.T4.1.15.14.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T4.1.15.14.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T4.1.15.14.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T4.1.15.14.14" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
</tr>
<tr id="S4.T4.1.16.15" class="ltx_tr">
<th id="S4.T4.1.16.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">PillarNeXt-B</th>
<th id="S4.T4.1.16.15.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">3</th>
<td id="S4.T4.1.16.15.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T4.1.16.15.3.1" class="ltx_text ltx_font_bold">80.58</span></td>
<td id="S4.T4.1.16.15.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T4.1.16.15.4.1" class="ltx_text ltx_font_bold">80.08</span></td>
<td id="S4.T4.1.16.15.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T4.1.16.15.5.1" class="ltx_text ltx_font_bold">72.89</span></td>
<td id="S4.T4.1.16.15.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T4.1.16.15.6.1" class="ltx_text ltx_font_bold">72.42</span></td>
<td id="S4.T4.1.16.15.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T4.1.16.15.7.1" class="ltx_text ltx_font_bold">85.04</span></td>
<td id="S4.T4.1.16.15.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T4.1.16.15.8.1" class="ltx_text ltx_font_bold">82.11</span></td>
<td id="S4.T4.1.16.15.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T4.1.16.15.9.1" class="ltx_text ltx_font_bold">78.04</span></td>
<td id="S4.T4.1.16.15.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T4.1.16.15.10.1" class="ltx_text ltx_font_bold">75.19</span></td>
<td id="S4.T4.1.16.15.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T4.1.16.15.11.1" class="ltx_text ltx_font_bold">78.92</span></td>
<td id="S4.T4.1.16.15.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T4.1.16.15.12.1" class="ltx_text ltx_font_bold">77.94</span></td>
<td id="S4.T4.1.16.15.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T4.1.16.15.13.1" class="ltx_text ltx_font_bold">76.71</span></td>
<td id="S4.T4.1.16.15.14" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T4.1.16.15.14.1" class="ltx_text ltx_font_bold">75.74</span></td>
</tr>
<tr id="S4.T4.1.17.16" class="ltx_tr">
<th id="S4.T4.1.17.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">CenterFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</th>
<th id="S4.T4.1.17.16.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">8</th>
<td id="S4.T4.1.17.16.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">78.8</td>
<td id="S4.T4.1.17.16.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">78.3</td>
<td id="S4.T4.1.17.16.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">74.3</td>
<td id="S4.T4.1.17.16.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">73.8</td>
<td id="S4.T4.1.17.16.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">82.1</td>
<td id="S4.T4.1.17.16.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">79.3</td>
<td id="S4.T4.1.17.16.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">77.8</td>
<td id="S4.T4.1.17.16.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">75.0</td>
<td id="S4.T4.1.17.16.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">75.2</td>
<td id="S4.T4.1.17.16.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">74.4</td>
<td id="S4.T4.1.17.16.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">73.2</td>
<td id="S4.T4.1.17.16.14" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">72.3</td>
</tr>
<tr id="S4.T4.1.18.17" class="ltx_tr">
<th id="S4.T4.1.18.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">MPPNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>
</th>
<th id="S4.T4.1.18.17.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">16</th>
<td id="S4.T4.1.18.17.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">82.74</td>
<td id="S4.T4.1.18.17.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">82.28</td>
<td id="S4.T4.1.18.17.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">75.41</td>
<td id="S4.T4.1.18.17.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">74.96</td>
<td id="S4.T4.1.18.17.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">84.69</td>
<td id="S4.T4.1.18.17.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">82.25</td>
<td id="S4.T4.1.18.17.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">77.43</td>
<td id="S4.T4.1.18.17.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">75.06</td>
<td id="S4.T4.1.18.17.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">77.28</td>
<td id="S4.T4.1.18.17.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">76.66</td>
<td id="S4.T4.1.18.17.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">75.13</td>
<td id="S4.T4.1.18.17.14" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">74.52</td>
</tr>
<tr id="S4.T4.1.1" class="ltx_tr">
<th id="S4.T4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">3DAL<sup id="S4.T4.1.1.1.1" class="ltx_sup">†</sup> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>
</th>
<th id="S4.T4.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">ALL</th>
<td id="S4.T4.1.1.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.0pt;padding-right:4.0pt;">84.50</td>
<td id="S4.T4.1.1.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T4.1.1.5" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T4.1.1.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T4.1.1.7" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.0pt;padding-right:4.0pt;">82.88</td>
<td id="S4.T4.1.1.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T4.1.1.9" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T4.1.1.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T4.1.1.11" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T4.1.1.12" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T4.1.1.13" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T4.1.1.14" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.6.2.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S4.T4.3.1" class="ltx_text" style="font-size:90%;">Comparison of PillarNeXt-B and the state-of-the-art methods under the 3D metrics on the validation set of WOD. * denotes the two-stage models and <sup id="S4.T4.3.1.1" class="ltx_sup">†</sup> indicates the off-board method.</span></figcaption>
</figure>
<figure id="S4.T5" class="ltx_table">
<table id="S4.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T5.1.2.1" class="ltx_tr">
<th id="S4.T5.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;" rowspan="2"><span id="S4.T5.1.2.1.1.1" class="ltx_text">Method</span></th>
<th id="S4.T5.1.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;" rowspan="2"><span id="S4.T5.1.2.1.2.1" class="ltx_text">Frames</span></th>
<td id="S4.T5.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;" colspan="2">Vehicle L1</td>
<td id="S4.T5.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;" colspan="2">Vehicle L2</td>
<td id="S4.T5.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;" colspan="2">Pedestrian L1</td>
<td id="S4.T5.1.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;" colspan="2">Pedestrian L2</td>
<td id="S4.T5.1.2.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;" colspan="2">Cyclist L1</td>
<td id="S4.T5.1.2.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;" colspan="2">Cyclist L2</td>
</tr>
<tr id="S4.T5.1.3.2" class="ltx_tr">
<td id="S4.T5.1.3.2.1" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">AP</td>
<td id="S4.T5.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">APH</td>
<td id="S4.T5.1.3.2.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">AP</td>
<td id="S4.T5.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">APH</td>
<td id="S4.T5.1.3.2.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">AP</td>
<td id="S4.T5.1.3.2.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">APH</td>
<td id="S4.T5.1.3.2.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">AP</td>
<td id="S4.T5.1.3.2.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">APH</td>
<td id="S4.T5.1.3.2.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">AP</td>
<td id="S4.T5.1.3.2.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">APH</td>
<td id="S4.T5.1.3.2.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">AP</td>
<td id="S4.T5.1.3.2.12" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">APH</td>
</tr>
<tr id="S4.T5.1.4.3" class="ltx_tr">
<th id="S4.T5.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">PV-RCNN++* <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>
</th>
<th id="S4.T5.1.4.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">1</th>
<td id="S4.T5.1.4.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">91.57</td>
<td id="S4.T5.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T5.1.4.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T5.1.4.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T5.1.4.3.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">85.43</td>
<td id="S4.T5.1.4.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T5.1.4.3.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T5.1.4.3.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T5.1.4.3.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">75.94</td>
<td id="S4.T5.1.4.3.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T5.1.4.3.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T5.1.4.3.14" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
</tr>
<tr id="S4.T5.1.5.4" class="ltx_tr">
<th id="S4.T5.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">PillarNeXt-B</th>
<th id="S4.T5.1.5.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">1</th>
<td id="S4.T5.1.5.4.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">93.30</td>
<td id="S4.T5.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">92.60</td>
<td id="S4.T5.1.5.4.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">87.26</td>
<td id="S4.T5.1.5.4.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">86.53</td>
<td id="S4.T5.1.5.4.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">88.19</td>
<td id="S4.T5.1.5.4.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">82.13</td>
<td id="S4.T5.1.5.4.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">81.77</td>
<td id="S4.T5.1.5.4.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">75.82</td>
<td id="S4.T5.1.5.4.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">75.67</td>
<td id="S4.T5.1.5.4.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">74.61</td>
<td id="S4.T5.1.5.4.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">72.97</td>
<td id="S4.T5.1.5.4.14" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">71.95</td>
</tr>
<tr id="S4.T5.1.6.5" class="ltx_tr">
<th id="S4.T5.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">SWFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>
</th>
<th id="S4.T5.1.6.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">3</th>
<td id="S4.T5.1.6.5.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">92.60</td>
<td id="S4.T5.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T5.1.6.5.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T5.1.6.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T5.1.6.5.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">87.50</td>
<td id="S4.T5.1.6.5.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T5.1.6.5.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T5.1.6.5.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T5.1.6.5.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T5.1.6.5.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T5.1.6.5.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T5.1.6.5.14" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
</tr>
<tr id="S4.T5.1.7.6" class="ltx_tr">
<th id="S4.T5.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">PillarNeXt-B</th>
<th id="S4.T5.1.7.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">3</th>
<td id="S4.T5.1.7.6.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T5.1.7.6.3.1" class="ltx_text ltx_font_bold">94.41</span></td>
<td id="S4.T5.1.7.6.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">93.73</td>
<td id="S4.T5.1.7.6.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">89.36</td>
<td id="S4.T5.1.7.6.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">88.66</td>
<td id="S4.T5.1.7.6.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T5.1.7.6.7.1" class="ltx_text ltx_font_bold">90.20</span></td>
<td id="S4.T5.1.7.6.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">86.94</td>
<td id="S4.T5.1.7.6.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">84.66</td>
<td id="S4.T5.1.7.6.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">81.36</td>
<td id="S4.T5.1.7.6.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">81.35</td>
<td id="S4.T5.1.7.6.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">80.32</td>
<td id="S4.T5.1.7.6.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">79.23</td>
<td id="S4.T5.1.7.6.14" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">78.22</td>
</tr>
<tr id="S4.T5.1.1" class="ltx_tr">
<th id="S4.T5.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">3DAL<sup id="S4.T5.1.1.1.1" class="ltx_sup">†</sup> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>
</th>
<th id="S4.T5.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">ALL</th>
<td id="S4.T5.1.1.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">93.30</td>
<td id="S4.T5.1.1.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T5.1.1.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T5.1.1.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T5.1.1.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">86.32</td>
<td id="S4.T5.1.1.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T5.1.1.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T5.1.1.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T5.1.1.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T5.1.1.12" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T5.1.1.13" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="S4.T5.1.1.14" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T5.6.2.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="S4.T5.3.1" class="ltx_text" style="font-size:90%;">Comparison of PillarNeXt-B and the state-of-the-art methods under the BEV metrics on the validation set of WOD. * denotes the two-stage models and <sup id="S4.T5.3.1.1" class="ltx_sup">†</sup> indicates the off-board method.</span></figcaption>
</figure>
</section>
<section id="S4.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3 </span>Study of Resolutions</h4>

<div id="S4.SS2.SSS3.p1" class="ltx_para">
<p id="S4.SS2.SSS3.p1.1" class="ltx_p">Intuitively, a smaller grid size retains more fine-grained information but requires a higher computational cost. Downsampling can effectively reduce the cost but degrade the performance. We experiment with different grid and feature resolutions by changing grid sizes and feature sampling rates in backbone and head. As shown in Table <a href="#S4.T3" title="Table 3 ‣ 4.2 Network Design Study ‣ 4 Experiments ‣ PillarNeXt: Rethinking Network Designs for 3D Object Detection in LiDAR Point Clouds" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, if the output feature resolution is fixed (0.3), using a large grid size (0.075 to 0.3) does not affect the performance of large objects such as vehicles, but deteriorates the accuracy of small objects like pedestrians. Downsampling the output feature resolution (0.3 to 0.6) impairs the performance of both categories. However, if simply providing an upsampling layer in the detection head, we obtain significant improvement, especially for the small objects. This suggests that the fine-grained information may have already been encoded in the downsampled feature maps, and a simple upsampling layer in head can effectively recover the details.</p>
</div>
</section>
<section id="S4.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.4 </span>Summary</h4>

<div id="S4.SS2.SSS4.p1" class="ltx_para">
<p id="S4.SS2.SSS4.p1.1" class="ltx_p">We provide the improvement of each component one by one to elucidate the boosting roadmap in Figure <a href="#S4.F4" title="Figure 4 ‣ 4.2.1 Study of Grid Encoders ‣ 4.2 Network Design Study ‣ 4 Experiments ‣ PillarNeXt: Rethinking Network Designs for 3D Object Detection in LiDAR Point Clouds" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. As compared in this figure, one can see that the model scaling (e.g., tiny to base), the enhanced network neck and head (e.g., ASPP based neck and simple modifications in head), and the appropriate training (e.g., sufficient training epochs and data augmentation), produce tremendous improvements over the original baseline model. In the following experiments, we utilize Pillar-B with above improvements as the default setting for our proposed network PillarNeXt-B. It is extensively compared to the state-of-the-art methods that are specifically developed for point clouds. We illustrate the overall architecture of PillarNeXt in Figure <a href="#S2.F2" title="Figure 2 ‣ 2 Related Work ‣ PillarNeXt: Rethinking Network Designs for 3D Object Detection in LiDAR Point Clouds" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Comparison with State-of-the-Art on WOD</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We compare PillarNeXt-B with the published results on the validation set of WOD. As a common practice, we list the methods of using single and multiple frames separately. For completeness, we also compare to the methods with long-term temporal modeling. Our model is trained for 36 epochs with the faded copy-and-paste data augmentation.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">As compared in Table <a href="#S4.T4" title="Table 4 ‣ 4.2.2 Study of Neck Modules ‣ 4.2 Network Design Study ‣ 4 Experiments ‣ PillarNeXt: Rethinking Network Designs for 3D Object Detection in LiDAR Point Clouds" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, our single-stage model outperforms many two-stage methods. It is also worth noticing that our pillar based approach without explicit temporal modeling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> even achieves better results for small objects such as pedestrians than the methods with complex temporal modeling and fine-grained geometrical modeling. This clearly verifies the importance of network designs in terms of basic architecture and appropriate training.</p>
</div>
<figure id="S4.T6" class="ltx_table">
<table id="S4.T6.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T6.1.2.1" class="ltx_tr">
<th id="S4.T6.1.2.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;" rowspan="2"><span id="S4.T6.1.2.1.1.1" class="ltx_text">Method</span></th>
<th id="S4.T6.1.2.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;" rowspan="2"><span id="S4.T6.1.2.1.2.1" class="ltx_text">Frames</span></th>
<th id="S4.T6.1.2.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;" colspan="2">All L2</th>
<th id="S4.T6.1.2.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;" colspan="2">Vehicle L1</th>
<th id="S4.T6.1.2.1.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;" colspan="2">Vehicle L2</th>
<th id="S4.T6.1.2.1.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;" colspan="2">Pedestrian L1</th>
<th id="S4.T6.1.2.1.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;" colspan="2">Pedestrian L2</th>
<th id="S4.T6.1.2.1.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;" colspan="2">Cyclist L1</th>
<th id="S4.T6.1.2.1.9" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;" colspan="2">Cyclist L2</th>
</tr>
<tr id="S4.T6.1.3.2" class="ltx_tr">
<th id="S4.T6.1.3.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column" style="padding-left:1.5pt;padding-right:1.5pt;">mAP</th>
<th id="S4.T6.1.3.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">mAPH</th>
<th id="S4.T6.1.3.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column" style="padding-left:1.5pt;padding-right:1.5pt;">AP</th>
<th id="S4.T6.1.3.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">APH</th>
<th id="S4.T6.1.3.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column" style="padding-left:1.5pt;padding-right:1.5pt;">AP</th>
<th id="S4.T6.1.3.2.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">APH</th>
<th id="S4.T6.1.3.2.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column" style="padding-left:1.5pt;padding-right:1.5pt;">AP</th>
<th id="S4.T6.1.3.2.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">APH</th>
<th id="S4.T6.1.3.2.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column" style="padding-left:1.5pt;padding-right:1.5pt;">AP</th>
<th id="S4.T6.1.3.2.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">APH</th>
<th id="S4.T6.1.3.2.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column" style="padding-left:1.5pt;padding-right:1.5pt;">AP</th>
<th id="S4.T6.1.3.2.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">APH</th>
<th id="S4.T6.1.3.2.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column" style="padding-left:1.5pt;padding-right:1.5pt;">AP</th>
<th id="S4.T6.1.3.2.14" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column" style="padding-left:1.5pt;padding-right:1.5pt;">APH</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T6.1.4.1" class="ltx_tr">
<th id="S4.T6.1.4.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">SWFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>
</th>
<th id="S4.T6.1.4.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">3</th>
<td id="S4.T6.1.4.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">-</td>
<td id="S4.T6.1.4.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">-</td>
<td id="S4.T6.1.4.1.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">82.89</td>
<td id="S4.T6.1.4.1.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">82.49</td>
<td id="S4.T6.1.4.1.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">75.02</td>
<td id="S4.T6.1.4.1.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">74.65</td>
<td id="S4.T6.1.4.1.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">82.13</td>
<td id="S4.T6.1.4.1.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">78.13</td>
<td id="S4.T6.1.4.1.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">75.87</td>
<td id="S4.T6.1.4.1.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">72.07</td>
<td id="S4.T6.1.4.1.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">-</td>
<td id="S4.T6.1.4.1.14" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">-</td>
<td id="S4.T6.1.4.1.15" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">-</td>
<td id="S4.T6.1.4.1.16" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">-</td>
</tr>
<tr id="S4.T6.1.1" class="ltx_tr">
<th id="S4.T6.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">PillarNet-34<sup id="S4.T6.1.1.1.1" class="ltx_sup">†</sup> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>
</th>
<th id="S4.T6.1.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">3</th>
<td id="S4.T6.1.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">73.98</td>
<td id="S4.T6.1.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">72.48</td>
<td id="S4.T6.1.1.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">83.23</td>
<td id="S4.T6.1.1.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">82.80</td>
<td id="S4.T6.1.1.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">76.09</td>
<td id="S4.T6.1.1.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">75.69</td>
<td id="S4.T6.1.1.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">82.38</td>
<td id="S4.T6.1.1.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">79.02</td>
<td id="S4.T6.1.1.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">76.66</td>
<td id="S4.T6.1.1.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">73.46</td>
<td id="S4.T6.1.1.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">71.44</td>
<td id="S4.T6.1.1.14" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">70.51</td>
<td id="S4.T6.1.1.15" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">69.20</td>
<td id="S4.T6.1.1.16" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">68.29</td>
</tr>
<tr id="S4.T6.1.5.2" class="ltx_tr">
<th id="S4.T6.1.5.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">CenterPoint++<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>
</th>
<th id="S4.T6.1.5.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">3</th>
<td id="S4.T6.1.5.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">74.20</td>
<td id="S4.T6.1.5.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">72.80</td>
<td id="S4.T6.1.5.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">82.80</td>
<td id="S4.T6.1.5.2.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">82.30</td>
<td id="S4.T6.1.5.2.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">75.50</td>
<td id="S4.T6.1.5.2.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">75.10</td>
<td id="S4.T6.1.5.2.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">81.00</td>
<td id="S4.T6.1.5.2.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">78.20</td>
<td id="S4.T6.1.5.2.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">75.10</td>
<td id="S4.T6.1.5.2.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">72.40</td>
<td id="S4.T6.1.5.2.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">74.40</td>
<td id="S4.T6.1.5.2.14" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">73.30</td>
<td id="S4.T6.1.5.2.15" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">72.00</td>
<td id="S4.T6.1.5.2.16" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">71.00</td>
</tr>
<tr id="S4.T6.1.6.3" class="ltx_tr">
<th id="S4.T6.1.6.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">AFDetV2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>
</th>
<th id="S4.T6.1.6.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">2</th>
<td id="S4.T6.1.6.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">74.60</td>
<td id="S4.T6.1.6.3.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">73.12</td>
<td id="S4.T6.1.6.3.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">81.65</td>
<td id="S4.T6.1.6.3.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">81.22</td>
<td id="S4.T6.1.6.3.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">74.30</td>
<td id="S4.T6.1.6.3.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">73.89</td>
<td id="S4.T6.1.6.3.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">81.26</td>
<td id="S4.T6.1.6.3.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">78.05</td>
<td id="S4.T6.1.6.3.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">75.47</td>
<td id="S4.T6.1.6.3.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">72.41</td>
<td id="S4.T6.1.6.3.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">76.41</td>
<td id="S4.T6.1.6.3.14" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">75.37</td>
<td id="S4.T6.1.6.3.15" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">74.05</td>
<td id="S4.T6.1.6.3.16" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">73.04</td>
</tr>
<tr id="S4.T6.1.7.4" class="ltx_tr">
<th id="S4.T6.1.7.4.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">PV-RCNN++* <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>
</th>
<th id="S4.T6.1.7.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">2</th>
<td id="S4.T6.1.7.4.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">75.00</td>
<td id="S4.T6.1.7.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">73.52</td>
<td id="S4.T6.1.7.4.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">83.74</td>
<td id="S4.T6.1.7.4.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">83.32</td>
<td id="S4.T6.1.7.4.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">76.31</td>
<td id="S4.T6.1.7.4.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">75.92</td>
<td id="S4.T6.1.7.4.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">82.60</td>
<td id="S4.T6.1.7.4.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">79.38</td>
<td id="S4.T6.1.7.4.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">76.63</td>
<td id="S4.T6.1.7.4.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">73.55</td>
<td id="S4.T6.1.7.4.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">74.44</td>
<td id="S4.T6.1.7.4.14" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">73.43</td>
<td id="S4.T6.1.7.4.15" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">72.06</td>
<td id="S4.T6.1.7.4.16" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">71.09</td>
</tr>
<tr id="S4.T6.1.8.5" class="ltx_tr">
<th id="S4.T6.1.8.5.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">PillarNeXt-B</th>
<th id="S4.T6.1.8.5.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">3</th>
<td id="S4.T6.1.8.5.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b" style="padding-left:1.5pt;padding-right:1.5pt;"><span id="S4.T6.1.8.5.3.1" class="ltx_text ltx_font_bold">75.53</span></td>
<td id="S4.T6.1.8.5.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;"><span id="S4.T6.1.8.5.4.1" class="ltx_text ltx_font_bold">74.10</span></td>
<td id="S4.T6.1.8.5.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b" style="padding-left:1.5pt;padding-right:1.5pt;">83.28</td>
<td id="S4.T6.1.8.5.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">82.83</td>
<td id="S4.T6.1.8.5.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b" style="padding-left:1.5pt;padding-right:1.5pt;">76.18</td>
<td id="S4.T6.1.8.5.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">75.76</td>
<td id="S4.T6.1.8.5.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b" style="padding-left:1.5pt;padding-right:1.5pt;">84.40</td>
<td id="S4.T6.1.8.5.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">81.44</td>
<td id="S4.T6.1.8.5.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b" style="padding-left:1.5pt;padding-right:1.5pt;">78.84</td>
<td id="S4.T6.1.8.5.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">75.98</td>
<td id="S4.T6.1.8.5.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b" style="padding-left:1.5pt;padding-right:1.5pt;">73.77</td>
<td id="S4.T6.1.8.5.14" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r" style="padding-left:1.5pt;padding-right:1.5pt;">72.73</td>
<td id="S4.T6.1.8.5.15" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b" style="padding-left:1.5pt;padding-right:1.5pt;">71.56</td>
<td id="S4.T6.1.8.5.16" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b" style="padding-left:1.5pt;padding-right:1.5pt;">70.55</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T6.6.2.1" class="ltx_text" style="font-size:90%;">Table 6</span>: </span><span id="S4.T6.3.1" class="ltx_text" style="font-size:90%;">Comparison of PillarNeXt-B and the state-of-the-art methods under the 3D metrics on the test set of WOD. * denotes the two-stage model and <sup id="S4.T6.3.1.1" class="ltx_sup">†</sup> indicates using test-time augmentations.</span></figcaption>
</figure>
<figure id="S4.T7" class="ltx_table">
<table id="S4.T7.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T7.5.5" class="ltx_tr">
<th id="S4.T7.5.5.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Method</th>
<th id="S4.T7.5.5.7" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Encoder</th>
<th id="S4.T7.5.5.8" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Grid Size</th>
<th id="S4.T7.5.5.9" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">NDS</th>
<th id="S4.T7.5.5.10" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">mAP</th>
<th id="S4.T7.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">mATE<math id="S4.T7.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.1.1.1.m1.1a"><mo stretchy="false" id="S4.T7.1.1.1.m1.1.1" xref="S4.T7.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.1.1.1.m1.1b"><ci id="S4.T7.1.1.1.m1.1.1.cmml" xref="S4.T7.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T7.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">mASE<math id="S4.T7.2.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.2.2.2.m1.1a"><mo stretchy="false" id="S4.T7.2.2.2.m1.1.1" xref="S4.T7.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.2.2.2.m1.1b"><ci id="S4.T7.2.2.2.m1.1.1.cmml" xref="S4.T7.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.2.2.2.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T7.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">mAOE<math id="S4.T7.3.3.3.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.3.3.3.m1.1a"><mo stretchy="false" id="S4.T7.3.3.3.m1.1.1" xref="S4.T7.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.3.3.3.m1.1b"><ci id="S4.T7.3.3.3.m1.1.1.cmml" xref="S4.T7.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.3.3.3.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T7.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">mAVE<math id="S4.T7.4.4.4.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.4.4.4.m1.1a"><mo stretchy="false" id="S4.T7.4.4.4.m1.1.1" xref="S4.T7.4.4.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.4.4.4.m1.1b"><ci id="S4.T7.4.4.4.m1.1.1.cmml" xref="S4.T7.4.4.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.4.4.4.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T7.5.5.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">mAAE<math id="S4.T7.5.5.5.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.5.5.5.m1.1a"><mo stretchy="false" id="S4.T7.5.5.5.m1.1.1" xref="S4.T7.5.5.5.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.5.5.5.m1.1b"><ci id="S4.T7.5.5.5.m1.1.1.cmml" xref="S4.T7.5.5.5.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.5.5.5.m1.1c">\downarrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T7.5.6.1" class="ltx_tr">
<td id="S4.T7.5.6.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">CenterPoint <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>
</td>
<td id="S4.T7.5.6.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">V</td>
<td id="S4.T7.5.6.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.075</td>
<td id="S4.T7.5.6.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">66.8</td>
<td id="S4.T7.5.6.1.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">59.6</td>
<td id="S4.T7.5.6.1.6" class="ltx_td ltx_align_center ltx_border_t">0.292</td>
<td id="S4.T7.5.6.1.7" class="ltx_td ltx_align_center ltx_border_t">0.255</td>
<td id="S4.T7.5.6.1.8" class="ltx_td ltx_align_center ltx_border_t">0.302</td>
<td id="S4.T7.5.6.1.9" class="ltx_td ltx_align_center ltx_border_t">0.259</td>
<td id="S4.T7.5.6.1.10" class="ltx_td ltx_align_center ltx_border_t">0.193</td>
</tr>
<tr id="S4.T7.5.7.2" class="ltx_tr">
<td id="S4.T7.5.7.2.1" class="ltx_td ltx_align_left ltx_border_r">OHS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</td>
<td id="S4.T7.5.7.2.2" class="ltx_td ltx_align_left ltx_border_r">V</td>
<td id="S4.T7.5.7.2.3" class="ltx_td ltx_align_left ltx_border_r">0.1</td>
<td id="S4.T7.5.7.2.4" class="ltx_td ltx_align_left ltx_border_r">66.0</td>
<td id="S4.T7.5.7.2.5" class="ltx_td ltx_align_left ltx_border_r">59.5</td>
<td id="S4.T7.5.7.2.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.5.7.2.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.5.7.2.8" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.5.7.2.9" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.5.7.2.10" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T7.5.8.3" class="ltx_tr">
<td id="S4.T7.5.8.3.1" class="ltx_td ltx_align_left ltx_border_r">PillarNet-18 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>
</td>
<td id="S4.T7.5.8.3.2" class="ltx_td ltx_align_left ltx_border_r">P</td>
<td id="S4.T7.5.8.3.3" class="ltx_td ltx_align_left ltx_border_r">0.075</td>
<td id="S4.T7.5.8.3.4" class="ltx_td ltx_align_left ltx_border_r">67.4</td>
<td id="S4.T7.5.8.3.5" class="ltx_td ltx_align_left ltx_border_r">59.9</td>
<td id="S4.T7.5.8.3.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.5.8.3.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.5.8.3.8" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.5.8.3.9" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.5.8.3.10" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T7.5.9.4" class="ltx_tr">
<td id="S4.T7.5.9.4.1" class="ltx_td ltx_align_left ltx_border_r">Transfusion-L <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
</td>
<td id="S4.T7.5.9.4.2" class="ltx_td ltx_align_left ltx_border_r">V</td>
<td id="S4.T7.5.9.4.3" class="ltx_td ltx_align_left ltx_border_r">0.075</td>
<td id="S4.T7.5.9.4.4" class="ltx_td ltx_align_left ltx_border_r">66.8</td>
<td id="S4.T7.5.9.4.5" class="ltx_td ltx_align_left ltx_border_r">60.0</td>
<td id="S4.T7.5.9.4.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.5.9.4.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.5.9.4.8" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.5.9.4.9" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.5.9.4.10" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T7.5.10.5" class="ltx_tr">
<td id="S4.T7.5.10.5.1" class="ltx_td ltx_align_left ltx_border_r">UVTR-L <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>
</td>
<td id="S4.T7.5.10.5.2" class="ltx_td ltx_align_left ltx_border_r">V</td>
<td id="S4.T7.5.10.5.3" class="ltx_td ltx_align_left ltx_border_r">0.075</td>
<td id="S4.T7.5.10.5.4" class="ltx_td ltx_align_left ltx_border_r">67.7</td>
<td id="S4.T7.5.10.5.5" class="ltx_td ltx_align_left ltx_border_r">60.9</td>
<td id="S4.T7.5.10.5.6" class="ltx_td ltx_align_center">0.334</td>
<td id="S4.T7.5.10.5.7" class="ltx_td ltx_align_center">0.257</td>
<td id="S4.T7.5.10.5.8" class="ltx_td ltx_align_center">0.300</td>
<td id="S4.T7.5.10.5.9" class="ltx_td ltx_align_center">0.204</td>
<td id="S4.T7.5.10.5.10" class="ltx_td ltx_align_center">0.182</td>
</tr>
<tr id="S4.T7.5.11.6" class="ltx_tr">
<td id="S4.T7.5.11.6.1" class="ltx_td ltx_align_left ltx_border_r">VISTA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>
</td>
<td id="S4.T7.5.11.6.2" class="ltx_td ltx_align_left ltx_border_r">V+R</td>
<td id="S4.T7.5.11.6.3" class="ltx_td ltx_align_left ltx_border_r">0.1</td>
<td id="S4.T7.5.11.6.4" class="ltx_td ltx_align_left ltx_border_r">68.1</td>
<td id="S4.T7.5.11.6.5" class="ltx_td ltx_align_left ltx_border_r">60.8</td>
<td id="S4.T7.5.11.6.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.5.11.6.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.5.11.6.8" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.5.11.6.9" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.5.11.6.10" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T7.5.12.7" class="ltx_tr">
<td id="S4.T7.5.12.7.1" class="ltx_td ltx_align_left ltx_border_r">PillarNeXt-B</td>
<td id="S4.T7.5.12.7.2" class="ltx_td ltx_align_left ltx_border_r">P</td>
<td id="S4.T7.5.12.7.3" class="ltx_td ltx_align_left ltx_border_r">0.075</td>
<td id="S4.T7.5.12.7.4" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T7.5.12.7.4.1" class="ltx_text ltx_font_bold">68.8</span></td>
<td id="S4.T7.5.12.7.5" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T7.5.12.7.5.1" class="ltx_text ltx_font_bold">62.5</span></td>
<td id="S4.T7.5.12.7.6" class="ltx_td ltx_align_center">0.278</td>
<td id="S4.T7.5.12.7.7" class="ltx_td ltx_align_center">0.251</td>
<td id="S4.T7.5.12.7.8" class="ltx_td ltx_align_center">0.269</td>
<td id="S4.T7.5.12.7.9" class="ltx_td ltx_align_center">0.248</td>
<td id="S4.T7.5.12.7.10" class="ltx_td ltx_align_center">0.201</td>
</tr>
<tr id="S4.T7.5.13.8" class="ltx_tr">
<td id="S4.T7.5.13.8.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">Our Voxel-B</td>
<td id="S4.T7.5.13.8.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">V</td>
<td id="S4.T7.5.13.8.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">0.075</td>
<td id="S4.T7.5.13.8.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">68.2</td>
<td id="S4.T7.5.13.8.5" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">62.4</td>
<td id="S4.T7.5.13.8.6" class="ltx_td ltx_align_center ltx_border_b">0.278</td>
<td id="S4.T7.5.13.8.7" class="ltx_td ltx_align_center ltx_border_b">0.250</td>
<td id="S4.T7.5.13.8.8" class="ltx_td ltx_align_center ltx_border_b">0.308</td>
<td id="S4.T7.5.13.8.9" class="ltx_td ltx_align_center ltx_border_b">0.263</td>
<td id="S4.T7.5.13.8.10" class="ltx_td ltx_align_center ltx_border_b">0.198</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T7.7.1.1" class="ltx_text" style="font-size:90%;">Table 7</span>: </span><span id="S4.T7.8.2" class="ltx_text" style="font-size:90%;">Comparison of PillarNeXt-B and the state-of-the-art methods on the validation set of nuScenes. P/V/R denotes the pillar, voxel and range view based grid encoder, respectively. Most leading methods adopt the voxel based representations.</span></figcaption>
</figure>
<figure id="S4.T8" class="ltx_table">
<table id="S4.T8.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T8.2.1.1" class="ltx_tr">
<th id="S4.T8.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">Method</th>
<th id="S4.T8.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Car</th>
<th id="S4.T8.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Truck</th>
<th id="S4.T8.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Bus</th>
<th id="S4.T8.2.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Trailer</th>
<th id="S4.T8.2.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">CV</th>
<th id="S4.T8.2.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Ped</th>
<th id="S4.T8.2.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Motor</th>
<th id="S4.T8.2.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Bicycle</th>
<th id="S4.T8.2.1.1.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">TC</th>
<th id="S4.T8.2.1.1.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Barrier</th>
<th id="S4.T8.2.1.1.12" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">mAP</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T8.2.2.1" class="ltx_tr">
<th id="S4.T8.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">PillarNeXt-B</th>
<td id="S4.T8.2.2.1.2" class="ltx_td ltx_align_center ltx_border_t">84.8</td>
<td id="S4.T8.2.2.1.3" class="ltx_td ltx_align_center ltx_border_t">58.6</td>
<td id="S4.T8.2.2.1.4" class="ltx_td ltx_align_center ltx_border_t">66.5</td>
<td id="S4.T8.2.2.1.5" class="ltx_td ltx_align_center ltx_border_t">35.3</td>
<td id="S4.T8.2.2.1.6" class="ltx_td ltx_align_center ltx_border_t">21.4</td>
<td id="S4.T8.2.2.1.7" class="ltx_td ltx_align_center ltx_border_t">87.2</td>
<td id="S4.T8.2.2.1.8" class="ltx_td ltx_align_center ltx_border_t">68.0</td>
<td id="S4.T8.2.2.1.9" class="ltx_td ltx_align_center ltx_border_t">56.4</td>
<td id="S4.T8.2.2.1.10" class="ltx_td ltx_align_center ltx_border_t">77.0</td>
<td id="S4.T8.2.2.1.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">69.8</td>
<td id="S4.T8.2.2.1.12" class="ltx_td ltx_align_center ltx_border_t">62.5</td>
</tr>
<tr id="S4.T8.2.3.2" class="ltx_tr">
<th id="S4.T8.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r">Our Voxel-B</th>
<td id="S4.T8.2.3.2.2" class="ltx_td ltx_align_center ltx_border_b">84.3</td>
<td id="S4.T8.2.3.2.3" class="ltx_td ltx_align_center ltx_border_b">58.3</td>
<td id="S4.T8.2.3.2.4" class="ltx_td ltx_align_center ltx_border_b">69.3</td>
<td id="S4.T8.2.3.2.5" class="ltx_td ltx_align_center ltx_border_b">37.1</td>
<td id="S4.T8.2.3.2.6" class="ltx_td ltx_align_center ltx_border_b">21.4</td>
<td id="S4.T8.2.3.2.7" class="ltx_td ltx_align_center ltx_border_b">87.4</td>
<td id="S4.T8.2.3.2.8" class="ltx_td ltx_align_center ltx_border_b">67.6</td>
<td id="S4.T8.2.3.2.9" class="ltx_td ltx_align_center ltx_border_b">54.7</td>
<td id="S4.T8.2.3.2.10" class="ltx_td ltx_align_center ltx_border_b">75.0</td>
<td id="S4.T8.2.3.2.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">69.2</td>
<td id="S4.T8.2.3.2.12" class="ltx_td ltx_align_center ltx_border_b">62.4</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T8.3.1.1" class="ltx_text" style="font-size:90%;">Table 8</span>: </span><span id="S4.T8.4.2" class="ltx_text" style="font-size:90%;">Comparison of our proposed pillar and voxel based models under per-class AP and mAP on the validation set of nuScenes. Abbreviations are construction vehicle (CV), pedestrian (Ped), motorcycle (Motor), and traffic cone (TC).</span></figcaption>
</figure>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">In addition to 3D results, we also report BEV metrics in Table <a href="#S4.T5" title="Table 5 ‣ 4.2.2 Study of Neck Modules ‣ 4.2 Network Design Study ‣ 4 Experiments ‣ PillarNeXt: Rethinking Network Designs for 3D Object Detection in LiDAR Point Clouds" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. BEV representation is widely used in autonomous driving as the downstream tasks are naturally carried out in the space of BEV. Interestingly, our single-frame model already outperforms many multi-frame methods. As compared with the off-board method 3DAL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, which takes the whole sequence (around 200 frames) for refinement, our 3-frame model achieves better performance. This again validates the efficacy of our succinct single-stage network.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p">In Table <a href="#S4.T6" title="Table 6 ‣ 4.3 Comparison with State-of-the-Art on WOD ‣ 4 Experiments ‣ PillarNeXt: Rethinking Network Designs for 3D Object Detection in LiDAR Point Clouds" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, we further demonstrate our results on the test set of WOD to evaluate the generalization of our approach. We do not use any test-time augmentation or model ensembling. PillarNeXt-B without bells and whistles is also found to outperform the state-of-the-art methods.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Comparison with State-of-the-Art on nuScenes</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">We in the end compare PillarNeXt-B with the state-of-the-art methods on nuScenes. Our model is trained for 20 epochs with the commonly used re-sampling CBGS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> and the faded copy-and-paste data augmentation.
We report the results on the validation set in Table <a href="#S4.T7" title="Table 7 ‣ 4.3 Comparison with State-of-the-Art on WOD ‣ 4 Experiments ‣ PillarNeXt: Rethinking Network Designs for 3D Object Detection in LiDAR Point Clouds" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. Here we use a simpler model by removing the IoU score branch in the detection head. Our approach achieves the superior performance of 68.8% NDS and 62.5% mAP, which show the exceptional generalizability of the proposed model across different datasets. It is noteworthy that apart from PillarNet-18, all high-performing methods are voxel based. Our pillar based model outperforms the leading voxel and multi-view based methods by a large margin in mAP. For further analysis, we also compare with our voxel based model (Voxel-B) under exactly the same setting. PillarNeXt-B obtains higher NDS and comparable mAP. In particular, for the per-class performance in Table <a href="#S4.T8" title="Table 8 ‣ 4.3 Comparison with State-of-the-Art on WOD ‣ 4 Experiments ‣ PillarNeXt: Rethinking Network Designs for 3D Object Detection in LiDAR Point Clouds" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, PillarNeXt-B achieves on par or superior results compared to Voxel-B in pedestrians and traffic cones. This further verifies that the pillar based model can be highly effective in accurately detecting small objects.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we challenge the common belief that a high-performing 3D object detection model requires the fine-grained local geometrical modeling. We systematically study three local point aggregators, and find that the simplest pillar encoder with enhanced strategy performs the best in both accuracy and latency. We demonstrate that enlarging receptive field and manipulating resolutions play the key roles. We hope our findings can serve as a solid baseline and encourage the research community to rethink what should be focused on for LiDAR based 3D object detection.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Xuyang Bai, Zeyu Hu, Xinge Zhu, Qingqiu Huang, Yilun Chen, Hongbo Fu, and
Chiew-Lan Tai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">TransFusion: Robust lidar-camera fusion for 3D object detection
with transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Liao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">YOLOv4: Optimal speed and accuracy of object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv:2004.10934</span><span id="bib.bib2.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Eli Bronstein, Mark Palatucci, Dominik Notz, Brandyn White, Alex Kuefler, Yiren
Lu, Supratik Paul, Payam Nikdel, Paul Mougin, Hongge Chen, Justin Fu, Austin
Abrams, Punit Shah, Evan Racah, Benjamin Frenkel, Shimon Whiteson, and
Dragomir Anguelov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Hierarchical model-based imitation learning for planning in
autonomous driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IROS</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong,
Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">nuScenes: A multimodal dataset for autonomous driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and
Alan L. Yuille.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">DeepLab: Semantic image segmentation with deep convolutional nets,
atrous convolution, and fully connected CRFs.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">TPAMI</span><span id="bib.bib5.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Qi Chen, Lin Sun, Zhixin Wang, Kui Jia, and Alan Yuille.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Object as hotspots: An anchor-free 3D object detection approach via
firing of hotspots.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Qiang Chen, Yingming Wang, Tong Yang, Xiangyu Zhang, Jian Cheng, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">You only look one-level feature.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Xuesong Chen, Shaoshuai Shi, Benjin Zhu, Ka Chun Cheung, Hang Xu, and Hongsheng
Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">MPPNet: Multi-frame feature intertwining with proxy points for 3D
temporal object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Shengheng Deng, Zhihao Liang, Lin Sun, and Kui Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">VISTA: Boosting 3D object detection via dual cross-view spatial
attention.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Piotr Dollár, Mannat Singh, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Fast and accurate model scaling.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Lue Fan, Ziqi Pang, Tianyuan Zhang, Yu-Xiong Wang, Hang Zhao, Feng Wang, Naiyan
Wang, and Zhaoxiang Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Embracing single stride 3D object detector with sparse transformer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Yihan Hu, Zhuangzhuang Ding, Runzhou Ge, Wenxin Shao, Li Huang, Kun Li, and
Qiang Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">AFDetV2: Rethinking the necessity of the second stage for object
detection from point clouds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">AAAI</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Alex Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar
Beijbom.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">PointPillars: Fast encoders for object detection from point clouds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Yanwei Li, Yilun Chen, Xiaojuan Qi, Zeming Li, Jian Sun, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Unifying voxel-based representation with transformer for 3D object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Yanghao Li, Yuntao Chen, Naiyan Wang, and Zhaoxiang Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Scale-aware trident networks for object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan,
and Serge Belongie.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Feature pyramid networks for object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Path aggregation network for instance segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Ze Liu, Han Hu, Yue Cao, Zheng Zhang, and Xin Tong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">A closer look at local aggregation operators in point cloud analysis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
Baining Guo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Swin transformer: Hierarchical vision transformer using shifted
windows.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Zhijian Liu, Haotian Tang, Shengyu Zhao, Kevin Shao, and Song Han.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">PVNAS: 3D neural architecture search with point-voxel
convolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">TPAMI</span><span id="bib.bib21.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Ilya Loshchilov and Frank Hutter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Decoupled weight decay regularization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Yiren Lu, Justin Fu, George Tucker, Xinlei Pan, Eli Bronstein, Becca Roelofs,
Benjamin Sapp, Brandyn White, Aleksandra Faust, Shimon Whiteson, Dragomir
Anguelov, and Sergey Levine.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Imitation is not enough: Robustifying imitation with reinforcement
learning for challenging driving scenarios.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Chenxu Luo, Xiaodong Yang, and Alan Yuille.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Exploring simple 3d multi-object tracking for autonomous driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Chenxu Luo, Xiaodong Yang, and Alan Yuille.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Self-supervised pillar motion learning for autonomous driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Gregory Meyer, Ankit Laddha, Eric Kee, Carlos Vallespi-Gonzalez, and Carl
Wellington.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">LaserNet: An efficient probabilistic 3D object detector for
autonomous driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Nigamaa Nayakanti, Rami Al-Rfou, Aurick Zhou, Kratarth Goel, Khaled Refaat, and
Benjamin Sapp.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Wayformer: Motion forecasting via simple and efficient attention
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv:2207.05844</span><span id="bib.bib27.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Charles Qi, Li Yi, Hao Su, and Leonidas J Guibas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">PointNet++: Deep hierarchical feature learning on point sets in a
metric space.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib28.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Charles Qi, Yin Zhou, Mahyar Najibi, Pei Sun, Khoa Vo, Boyang Deng, and
Dragomir Anguelov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Offboard 3D object detection from point cloud sequences.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai, Hasan Abed Al Kader Hammoud,
Mohamed Elhoseiny, and Bernard Ghanem.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">PointNeXt: Revisiting PointNet++ with improved training and
scaling strategies.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Guangsheng Shi, Ruifeng Li, and Chao Ma.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">PillarNet: High-performance pillar-based 3D object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Shaoshuai Shi, Li Jiang, Jiajun Deng, Zhe Wang, Chaoxu Guo, Jianping Shi,
Xiaogang Wang, and Hongsheng Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">PV-RCNN++: Point-voxel feature set abstraction with local vector
representation for 3D object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IJCV</span><span id="bib.bib32.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">PointRCNN: 3D object proposal generation and detection from point
cloud.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Leslie Smith and Nicholay Topin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Super-convergence: Very fast training of neural networks using large
learning rates.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv:1708.07120</span><span id="bib.bib34.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai
Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, Vijay
Vasudevan, Wei Han, Jiquan Ngiam, Hang Zhao, Aleksei Timofeev, Scott
Ettinger, Maxim Krivokon, Amy Gao, Aditya Joshi, Yu Zhang, Jonathon Shlens,
Zhifeng Chen, and Dragomir Anguelov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Scalability in perception for autonomous driving: Waymo open dataset.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Pei Sun, Mingxing Tan, Weiyue Wang, Chenxi Liu, Fei Xia, Zhaoqi Leng, and
Dragomir Anguelov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">SWFormer: Sparse window transformer for 3D object detection in
point clouds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Pei Sun, Weiyue Wang, Yuning Chai, Gamaleldin Elsayed, Alex Bewley, Xiao Zhang,
Cristian Sminchisescu, and Dragomir Anguelov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">RSN: Range sparse net for efficient, accurate lidar 3D object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib37.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib37.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Mingxing Tan and Quoc Le.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">EfficientNet: Rethinking model scaling for convolutional neural
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICML</span><span id="bib.bib38.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Mingxing Tan, Ruoming Pang, and Quoc V Le.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">EfficientDet: Scalable and efficient object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib39.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib39.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Xiaofang Wang and Kris Kitani.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Cost-aware comparison of lidar-based 3D object detectors.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib40.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICRA</span><span id="bib.bib40.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Xishun Wang, Tong Su, Fang Da, and Xiaodong Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">ProphNet: Efficient agent-centric motion forecasting with
anchor-informed proposals.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib41.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib41.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Yue Wang, Alireza Fathi, Abhijit Kundu, David A Ross, Caroline Pantofaru, Tom
Funkhouser, and Justin Solomon.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Pillar-based object detection for autonomous driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib42.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib42.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Yan Yan, Yuxing Mao, and Bo Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">SECOND: Sparsely embedded convolutional detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Sensors</span><span id="bib.bib43.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Xiaodong Yang, Pavlo Molchanov, and Jan Kautz.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Making convolutional networks recurrent for visual sequence learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib44.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib44.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Tianwei Yin, Xingyi Zhou, and Philipp Krahenbuhl.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">Center-based 3D object detection and tracking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib45.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib45.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Zhi Zhang, Tong He, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">Bag of freebies for training object detection neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv:1902.04103</span><span id="bib.bib46.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Yin Zhou, Pei Sun, Yu Zhang, Dragomir Anguelov, Jiyang Gao, Tom Ouyang, James
Guo, Jiquan Ngiam, and Vijay Vasudevan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">End-to-end multi-view fusion for 3D object detection in LiDAR
point clouds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib47.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRL</span><span id="bib.bib47.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
Yin Zhou and Oncel Tuzel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">VoxelNet: End-to-end learning for point cloud based 3D object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib48.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib48.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
Zixiang Zhou, Xiangchen Zhao, Yu Wang, Panqu Wang, and Hassan Foroosh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">CenterFormer: Center-based transformer for 3D object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib49.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib49.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
Benjin Zhu, Zhengkai Jiang, Xiangxin Zhou, Zeming Li, and Gang Yu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">Class-balanced grouping and sampling for point cloud 3D object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv:1908.09492</span><span id="bib.bib50.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="Ax1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">Appendix</h2>

</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>More Implementation Details</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.3" class="ltx_p">We use 0.01 weight decay in the optimizer AdamW, and set the one-cycle scheduled learning rate to 0.001 when the batch size is 16. Our basic augmentations include random flipping, random rotation within the range of <math id="A1.p1.1.m1.2" class="ltx_Math" alttext="[-\frac{\pi}{4},\frac{\pi}{4}]" display="inline"><semantics id="A1.p1.1.m1.2a"><mrow id="A1.p1.1.m1.2.2.1" xref="A1.p1.1.m1.2.2.2.cmml"><mo stretchy="false" id="A1.p1.1.m1.2.2.1.2" xref="A1.p1.1.m1.2.2.2.cmml">[</mo><mrow id="A1.p1.1.m1.2.2.1.1" xref="A1.p1.1.m1.2.2.1.1.cmml"><mo id="A1.p1.1.m1.2.2.1.1a" xref="A1.p1.1.m1.2.2.1.1.cmml">−</mo><mfrac id="A1.p1.1.m1.2.2.1.1.2" xref="A1.p1.1.m1.2.2.1.1.2.cmml"><mi id="A1.p1.1.m1.2.2.1.1.2.2" xref="A1.p1.1.m1.2.2.1.1.2.2.cmml">π</mi><mn id="A1.p1.1.m1.2.2.1.1.2.3" xref="A1.p1.1.m1.2.2.1.1.2.3.cmml">4</mn></mfrac></mrow><mo id="A1.p1.1.m1.2.2.1.3" xref="A1.p1.1.m1.2.2.2.cmml">,</mo><mfrac id="A1.p1.1.m1.1.1" xref="A1.p1.1.m1.1.1.cmml"><mi id="A1.p1.1.m1.1.1.2" xref="A1.p1.1.m1.1.1.2.cmml">π</mi><mn id="A1.p1.1.m1.1.1.3" xref="A1.p1.1.m1.1.1.3.cmml">4</mn></mfrac><mo stretchy="false" id="A1.p1.1.m1.2.2.1.4" xref="A1.p1.1.m1.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.p1.1.m1.2b"><interval closure="closed" id="A1.p1.1.m1.2.2.2.cmml" xref="A1.p1.1.m1.2.2.1"><apply id="A1.p1.1.m1.2.2.1.1.cmml" xref="A1.p1.1.m1.2.2.1.1"><minus id="A1.p1.1.m1.2.2.1.1.1.cmml" xref="A1.p1.1.m1.2.2.1.1"></minus><apply id="A1.p1.1.m1.2.2.1.1.2.cmml" xref="A1.p1.1.m1.2.2.1.1.2"><divide id="A1.p1.1.m1.2.2.1.1.2.1.cmml" xref="A1.p1.1.m1.2.2.1.1.2"></divide><ci id="A1.p1.1.m1.2.2.1.1.2.2.cmml" xref="A1.p1.1.m1.2.2.1.1.2.2">𝜋</ci><cn type="integer" id="A1.p1.1.m1.2.2.1.1.2.3.cmml" xref="A1.p1.1.m1.2.2.1.1.2.3">4</cn></apply></apply><apply id="A1.p1.1.m1.1.1.cmml" xref="A1.p1.1.m1.1.1"><divide id="A1.p1.1.m1.1.1.1.cmml" xref="A1.p1.1.m1.1.1"></divide><ci id="A1.p1.1.m1.1.1.2.cmml" xref="A1.p1.1.m1.1.1.2">𝜋</ci><cn type="integer" id="A1.p1.1.m1.1.1.3.cmml" xref="A1.p1.1.m1.1.1.3">4</cn></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.1.m1.2c">[-\frac{\pi}{4},\frac{\pi}{4}]</annotation></semantics></math>, random scaling between <math id="A1.p1.2.m2.2" class="ltx_Math" alttext="[0.9,1.1]" display="inline"><semantics id="A1.p1.2.m2.2a"><mrow id="A1.p1.2.m2.2.3.2" xref="A1.p1.2.m2.2.3.1.cmml"><mo stretchy="false" id="A1.p1.2.m2.2.3.2.1" xref="A1.p1.2.m2.2.3.1.cmml">[</mo><mn id="A1.p1.2.m2.1.1" xref="A1.p1.2.m2.1.1.cmml">0.9</mn><mo id="A1.p1.2.m2.2.3.2.2" xref="A1.p1.2.m2.2.3.1.cmml">,</mo><mn id="A1.p1.2.m2.2.2" xref="A1.p1.2.m2.2.2.cmml">1.1</mn><mo stretchy="false" id="A1.p1.2.m2.2.3.2.3" xref="A1.p1.2.m2.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.p1.2.m2.2b"><interval closure="closed" id="A1.p1.2.m2.2.3.1.cmml" xref="A1.p1.2.m2.2.3.2"><cn type="float" id="A1.p1.2.m2.1.1.cmml" xref="A1.p1.2.m2.1.1">0.9</cn><cn type="float" id="A1.p1.2.m2.2.2.cmml" xref="A1.p1.2.m2.2.2">1.1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.2.m2.2c">[0.9,1.1]</annotation></semantics></math>, and random translation with a noise factor of <math id="A1.p1.3.m3.1" class="ltx_Math" alttext="0.5" display="inline"><semantics id="A1.p1.3.m3.1a"><mn id="A1.p1.3.m3.1.1" xref="A1.p1.3.m3.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="A1.p1.3.m3.1b"><cn type="float" id="A1.p1.3.m3.1.1.cmml" xref="A1.p1.3.m3.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.3.m3.1c">0.5</annotation></semantics></math>, which are used for all experiments. Table <a href="#A1.T9" title="Table 9 ‣ Appendix A More Implementation Details ‣ PillarNeXt: Rethinking Network Designs for 3D Object Detection in LiDAR Point Clouds" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> shows the detailed architecture and training settings used in various experiments. As can be seen in this table, for the experiments on WOD, “Study” denotes the regular setting in Section <a href="#S4.SS2" title="4.2 Network Design Study ‣ 4 Experiments ‣ PillarNeXt: Rethinking Network Designs for 3D Object Detection in LiDAR Point Clouds" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>, and “Study+” indicates the enhanced version. Pillar/Voxel/MVF+ are trained with such enhanced strategy. We use the full settings with additional faded copy-and-paste augmentation when comparing with the single-frame or multi-frame based state-of-the-art methods.</p>
</div>
<figure id="A1.T9" class="ltx_table">
<table id="A1.T9.22" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T9.22.23.1" class="ltx_tr">
<th id="A1.T9.22.23.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" rowspan="2"></th>
<th id="A1.T9.22.23.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="4">Waymo Open Dataset (WOD)</th>
<th id="A1.T9.22.23.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" rowspan="2"><span id="A1.T9.22.23.1.3.1" class="ltx_text">nuScenes</span></th>
</tr>
<tr id="A1.T9.22.24.2" class="ltx_tr">
<th id="A1.T9.22.24.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Study</th>
<th id="A1.T9.22.24.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Study+</th>
<th id="A1.T9.22.24.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Full (Single-Frame)</th>
<th id="A1.T9.22.24.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Full (Multi-Frame)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T9.5.5" class="ltx_tr">
<th id="A1.T9.5.5.6" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Flip/Scaling/Rotation/Translation</th>
<td id="A1.T9.1.1.1" class="ltx_td ltx_align_center ltx_border_t"><math id="A1.T9.1.1.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A1.T9.1.1.1.m1.1a"><mi mathvariant="normal" id="A1.T9.1.1.1.m1.1.1" xref="A1.T9.1.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A1.T9.1.1.1.m1.1b"><ci id="A1.T9.1.1.1.m1.1.1.cmml" xref="A1.T9.1.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T9.1.1.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="A1.T9.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><math id="A1.T9.2.2.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A1.T9.2.2.2.m1.1a"><mi mathvariant="normal" id="A1.T9.2.2.2.m1.1.1" xref="A1.T9.2.2.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A1.T9.2.2.2.m1.1b"><ci id="A1.T9.2.2.2.m1.1.1.cmml" xref="A1.T9.2.2.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T9.2.2.2.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="A1.T9.3.3.3" class="ltx_td ltx_align_center ltx_border_t"><math id="A1.T9.3.3.3.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A1.T9.3.3.3.m1.1a"><mi mathvariant="normal" id="A1.T9.3.3.3.m1.1.1" xref="A1.T9.3.3.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A1.T9.3.3.3.m1.1b"><ci id="A1.T9.3.3.3.m1.1.1.cmml" xref="A1.T9.3.3.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T9.3.3.3.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="A1.T9.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="A1.T9.4.4.4.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A1.T9.4.4.4.m1.1a"><mi mathvariant="normal" id="A1.T9.4.4.4.m1.1.1" xref="A1.T9.4.4.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A1.T9.4.4.4.m1.1b"><ci id="A1.T9.4.4.4.m1.1.1.cmml" xref="A1.T9.4.4.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T9.4.4.4.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="A1.T9.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="A1.T9.5.5.5.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A1.T9.5.5.5.m1.1a"><mi mathvariant="normal" id="A1.T9.5.5.5.m1.1.1" xref="A1.T9.5.5.5.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A1.T9.5.5.5.m1.1b"><ci id="A1.T9.5.5.5.m1.1.1.cmml" xref="A1.T9.5.5.5.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T9.5.5.5.m1.1c">\checkmark</annotation></semantics></math></td>
</tr>
<tr id="A1.T9.8.8" class="ltx_tr">
<th id="A1.T9.8.8.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Random Drop Frames</th>
<td id="A1.T9.6.6.1" class="ltx_td ltx_align_center"><math id="A1.T9.6.6.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A1.T9.6.6.1.m1.1a"><mi mathvariant="normal" id="A1.T9.6.6.1.m1.1.1" xref="A1.T9.6.6.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A1.T9.6.6.1.m1.1b"><ci id="A1.T9.6.6.1.m1.1.1.cmml" xref="A1.T9.6.6.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T9.6.6.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="A1.T9.7.7.2" class="ltx_td ltx_align_center"><math id="A1.T9.7.7.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A1.T9.7.7.2.m1.1a"><mi mathvariant="normal" id="A1.T9.7.7.2.m1.1.1" xref="A1.T9.7.7.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A1.T9.7.7.2.m1.1b"><ci id="A1.T9.7.7.2.m1.1.1.cmml" xref="A1.T9.7.7.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T9.7.7.2.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="A1.T9.8.8.5" class="ltx_td"></td>
<td id="A1.T9.8.8.3" class="ltx_td ltx_align_center ltx_border_r"><math id="A1.T9.8.8.3.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A1.T9.8.8.3.m1.1a"><mi mathvariant="normal" id="A1.T9.8.8.3.m1.1.1" xref="A1.T9.8.8.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A1.T9.8.8.3.m1.1b"><ci id="A1.T9.8.8.3.m1.1.1.cmml" xref="A1.T9.8.8.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T9.8.8.3.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="A1.T9.8.8.6" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="A1.T9.11.11" class="ltx_tr">
<th id="A1.T9.11.11.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Faded Copy-and-Paste</th>
<td id="A1.T9.11.11.5" class="ltx_td"></td>
<td id="A1.T9.11.11.6" class="ltx_td"></td>
<td id="A1.T9.9.9.1" class="ltx_td ltx_align_center"><math id="A1.T9.9.9.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A1.T9.9.9.1.m1.1a"><mi mathvariant="normal" id="A1.T9.9.9.1.m1.1.1" xref="A1.T9.9.9.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A1.T9.9.9.1.m1.1b"><ci id="A1.T9.9.9.1.m1.1.1.cmml" xref="A1.T9.9.9.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T9.9.9.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="A1.T9.10.10.2" class="ltx_td ltx_align_center ltx_border_r"><math id="A1.T9.10.10.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A1.T9.10.10.2.m1.1a"><mi mathvariant="normal" id="A1.T9.10.10.2.m1.1.1" xref="A1.T9.10.10.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A1.T9.10.10.2.m1.1b"><ci id="A1.T9.10.10.2.m1.1.1.cmml" xref="A1.T9.10.10.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T9.10.10.2.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="A1.T9.11.11.3" class="ltx_td ltx_align_center ltx_border_r"><math id="A1.T9.11.11.3.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A1.T9.11.11.3.m1.1a"><mi mathvariant="normal" id="A1.T9.11.11.3.m1.1.1" xref="A1.T9.11.11.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A1.T9.11.11.3.m1.1b"><ci id="A1.T9.11.11.3.m1.1.1.cmml" xref="A1.T9.11.11.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T9.11.11.3.m1.1c">\checkmark</annotation></semantics></math></td>
</tr>
<tr id="A1.T9.15.15" class="ltx_tr">
<th id="A1.T9.15.15.5" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">
<span id="A1.T9.15.15.5.1" class="ltx_ERROR undefined">\hdashline</span>IoU Regression Loss</th>
<td id="A1.T9.15.15.6" class="ltx_td"></td>
<td id="A1.T9.12.12.1" class="ltx_td ltx_align_center"><math id="A1.T9.12.12.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A1.T9.12.12.1.m1.1a"><mi mathvariant="normal" id="A1.T9.12.12.1.m1.1.1" xref="A1.T9.12.12.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A1.T9.12.12.1.m1.1b"><ci id="A1.T9.12.12.1.m1.1.1.cmml" xref="A1.T9.12.12.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T9.12.12.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="A1.T9.13.13.2" class="ltx_td ltx_align_center"><math id="A1.T9.13.13.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A1.T9.13.13.2.m1.1a"><mi mathvariant="normal" id="A1.T9.13.13.2.m1.1.1" xref="A1.T9.13.13.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A1.T9.13.13.2.m1.1b"><ci id="A1.T9.13.13.2.m1.1.1.cmml" xref="A1.T9.13.13.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T9.13.13.2.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="A1.T9.14.14.3" class="ltx_td ltx_align_center ltx_border_r"><math id="A1.T9.14.14.3.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A1.T9.14.14.3.m1.1a"><mi mathvariant="normal" id="A1.T9.14.14.3.m1.1.1" xref="A1.T9.14.14.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A1.T9.14.14.3.m1.1b"><ci id="A1.T9.14.14.3.m1.1.1.cmml" xref="A1.T9.14.14.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T9.14.14.3.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="A1.T9.15.15.4" class="ltx_td ltx_align_center ltx_border_r"><math id="A1.T9.15.15.4.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A1.T9.15.15.4.m1.1a"><mi mathvariant="normal" id="A1.T9.15.15.4.m1.1.1" xref="A1.T9.15.15.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A1.T9.15.15.4.m1.1b"><ci id="A1.T9.15.15.4.m1.1.1.cmml" xref="A1.T9.15.15.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T9.15.15.4.m1.1c">\checkmark</annotation></semantics></math></td>
</tr>
<tr id="A1.T9.18.18" class="ltx_tr">
<th id="A1.T9.18.18.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">IoU Score Branch</th>
<td id="A1.T9.18.18.5" class="ltx_td"></td>
<td id="A1.T9.16.16.1" class="ltx_td ltx_align_center"><math id="A1.T9.16.16.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A1.T9.16.16.1.m1.1a"><mi mathvariant="normal" id="A1.T9.16.16.1.m1.1.1" xref="A1.T9.16.16.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A1.T9.16.16.1.m1.1b"><ci id="A1.T9.16.16.1.m1.1.1.cmml" xref="A1.T9.16.16.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T9.16.16.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="A1.T9.17.17.2" class="ltx_td ltx_align_center"><math id="A1.T9.17.17.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A1.T9.17.17.2.m1.1a"><mi mathvariant="normal" id="A1.T9.17.17.2.m1.1.1" xref="A1.T9.17.17.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A1.T9.17.17.2.m1.1b"><ci id="A1.T9.17.17.2.m1.1.1.cmml" xref="A1.T9.17.17.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T9.17.17.2.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="A1.T9.18.18.3" class="ltx_td ltx_align_center ltx_border_r"><math id="A1.T9.18.18.3.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A1.T9.18.18.3.m1.1a"><mi mathvariant="normal" id="A1.T9.18.18.3.m1.1.1" xref="A1.T9.18.18.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A1.T9.18.18.3.m1.1b"><ci id="A1.T9.18.18.3.m1.1.1.cmml" xref="A1.T9.18.18.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T9.18.18.3.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="A1.T9.18.18.6" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="A1.T9.22.22" class="ltx_tr">
<th id="A1.T9.22.22.5" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Multi-Group Head</th>
<td id="A1.T9.22.22.6" class="ltx_td"></td>
<td id="A1.T9.19.19.1" class="ltx_td ltx_align_center"><math id="A1.T9.19.19.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A1.T9.19.19.1.m1.1a"><mi mathvariant="normal" id="A1.T9.19.19.1.m1.1.1" xref="A1.T9.19.19.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A1.T9.19.19.1.m1.1b"><ci id="A1.T9.19.19.1.m1.1.1.cmml" xref="A1.T9.19.19.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T9.19.19.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="A1.T9.20.20.2" class="ltx_td ltx_align_center"><math id="A1.T9.20.20.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A1.T9.20.20.2.m1.1a"><mi mathvariant="normal" id="A1.T9.20.20.2.m1.1.1" xref="A1.T9.20.20.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A1.T9.20.20.2.m1.1b"><ci id="A1.T9.20.20.2.m1.1.1.cmml" xref="A1.T9.20.20.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T9.20.20.2.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="A1.T9.21.21.3" class="ltx_td ltx_align_center ltx_border_r"><math id="A1.T9.21.21.3.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A1.T9.21.21.3.m1.1a"><mi mathvariant="normal" id="A1.T9.21.21.3.m1.1.1" xref="A1.T9.21.21.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A1.T9.21.21.3.m1.1b"><ci id="A1.T9.21.21.3.m1.1.1.cmml" xref="A1.T9.21.21.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T9.21.21.3.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="A1.T9.22.22.4" class="ltx_td ltx_align_center ltx_border_r"><math id="A1.T9.22.22.4.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A1.T9.22.22.4.m1.1a"><mi mathvariant="normal" id="A1.T9.22.22.4.m1.1.1" xref="A1.T9.22.22.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A1.T9.22.22.4.m1.1b"><ci id="A1.T9.22.22.4.m1.1.1.cmml" xref="A1.T9.22.22.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T9.22.22.4.m1.1c">\checkmark</annotation></semantics></math></td>
</tr>
<tr id="A1.T9.22.25.1" class="ltx_tr">
<th id="A1.T9.22.25.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r">
<span id="A1.T9.22.25.1.1.1" class="ltx_ERROR undefined">\hdashline</span>Epochs</th>
<td id="A1.T9.22.25.1.2" class="ltx_td ltx_align_center ltx_border_b">12</td>
<td id="A1.T9.22.25.1.3" class="ltx_td ltx_align_center ltx_border_b">36</td>
<td id="A1.T9.22.25.1.4" class="ltx_td ltx_align_center ltx_border_b">36</td>
<td id="A1.T9.22.25.1.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">36</td>
<td id="A1.T9.22.25.1.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">20</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="A1.T9.24.1.1" class="ltx_text" style="font-size:90%;">Table 9</span>: </span><span id="A1.T9.25.2" class="ltx_text" style="font-size:90%;">Details of the experimental settings for different experiments on the two benchmark datasets.</span></figcaption>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2305.04924" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2305.04925" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2305.04925">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2305.04925" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2305.04926" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 08:21:46 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
