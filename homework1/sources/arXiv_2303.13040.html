<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2303.13040] Open-Vocabulary Object Detection using Pseudo Caption Labels</title><meta property="og:description" content="Recent open-vocabulary detection methods aim to detect novel objects by distilling knowledge from vision-language models (VLMs) trained on a vast amount of image-text pairs.
To improve the effectiveness of these method…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Open-Vocabulary Object Detection using Pseudo Caption Labels">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Open-Vocabulary Object Detection using Pseudo Caption Labels">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2303.13040">

<!--Generated on Thu Feb 29 18:38:38 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Open-Vocabulary Object Detection using Pseudo Caption Labels</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Han-Cheol Cho      Won Young Jhoo      Wooyoung Kang      Byungseok Roh
<br class="ltx_break">Multi-modal Understanding Team, KakaoBrain
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{simon.cho, iji.young, edwin.kang, peter.roh}@kakaobrain.com</span>
</span><span class="ltx_author_notes">Corresponding author.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Recent open-vocabulary detection methods aim to detect novel objects by distilling knowledge from vision-language models (VLMs) trained on a vast amount of image-text pairs.
To improve the effectiveness of these methods, researchers have utilized datasets with a large vocabulary that contains a large number of object classes, under the assumption that such data will enable models to extract comprehensive knowledge on the relationships between various objects and better generalize to unseen object classes.
In this study, we argue that more fine-grained labels are necessary to extract richer knowledge about novel objects, including object attributes and relationships, in addition to their names.
To address this challenge, we propose a simple and effective method named Pseudo Caption Labeling (PCL), which utilizes an image captioning model to generate captions that describe object instances from diverse perspectives.
The resulting pseudo caption labels offer dense samples for knowledge distillation.
On the LVIS benchmark, our best model trained on the de-duplicated <em id="id2.id1.1" class="ltx_emph ltx_font_italic">VisualGenome</em> dataset achieves an AP of 34.5 and an APr of 30.6, comparable to the state-of-the-art performance.
PCL’s simplicity and flexibility are other notable features, as it is a straightforward pre-processing technique that can be used with any image captioning model without imposing any restrictions on model architecture or training process.
Code is available at: <a href="TBA" title="" class="ltx_ref ltx_url ltx_font_typewriter">TBA</a>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure"><img src="/html/2303.13040/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="237" height="201" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">System overview. PCL generates pseudo caption labels (shown in blue) by employing an image captioning model. These labels provide detailed information about the main object and its surroundings compared to the manual prompt labels.</span></figcaption>
</figure>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2303.13040/assets/x2.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="528" height="253" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.3.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S1.F2.4.2" class="ltx_text" style="font-size:90%;">An illustration of the distillation points for two object instances, a cat and a dog, in the CLIP text latent space. The blue points represent manual prompt labels, while the orange ones indicate pseudo caption labels. Some of the pseudo caption labels are referring to <span id="S1.F2.4.2.1" class="ltx_text ltx_font_bold">chair</span>, which is a novel object class. It is expected that their embeddings are located near the chair class embedding.</span></figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Open-vocabulary detection (OVD) is a challenging task that aims to identify novel objects not present in the training data.
Recent studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> have shown the potential of leveraging vision-language models (VLMs) trained on a vast amount of image-text pairs to achieve this goal.
Many of them use datasets with a large vocabulary to better distill knowledge from VLMs.
Previous research <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> has utilized multiple datasets such as Object365 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, OpenImages <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, and VisualGenome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> to acquire such data.
Alternatively, another approach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> acquires annotated data through a weakly-supervised manner.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Utilizing a large vocabulary dataset is necessary, but may not be sufficient to extract comprehensive knowledge about novel objects.
VLMs are trained on image-text pairs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, where the accompanying text provides diverse information such as object names, their attributes and relationships, and the surrounding environments in which they exist.
As a result, object names or concepts in VLM’s latent space are associated with many different types of information.
If we can utilize such diverse information, it will be possible to extract rich knowledge about novel objects from VLMs.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To address this problem, we propose a simple and effective method called <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">Pseudo Caption Labeling</span> (PCL).
PCL is a pre-processing technique that utilizes an image-captioning model to transform the class name of an object instance into descriptive captions, as shown in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Open-Vocabulary Object Detection using Pseudo Caption Labels" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
More specifically, we generate these pseudo caption labels by feeding a cropped object image with a 20% margin into the captioning model.
Additionally, the object name is used as the output prefix to ensure that the generated caption labels explicitly mention the target object.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Figure <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Open-Vocabulary Object Detection using Pseudo Caption Labels" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> illustrates how pseudo caption labels enable a model to better distill knowledge about novel objects.
The figure shows three object instances, where two of them are from known classes, cat and dog, and the other is from unseen category, chair.
Although the training dataset does not have object annotations for the chair category, the pseudo caption labels for the cat and dog class objects may describe the novel cateogry object chair such as “dog sitting on a chair” and “cat sleeping on a chair”.
These pseudo caption labels representing the relationship between two objects are likely to be located between the class embeddings of the objects in the VLM’s latent space, allowing the model extract knowledge closer to the novel object class more effectively.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Our best model, which was trained on the de-duplicated VisualGenome (VG) dataset, achieved an AP of 34.5 and an APr of 30.6 on the LVIS benchmark, which are comparable to the previous state-of-the-art methods that used multiple datasets.
Ablation experiments revealed that various design choices we made, such as using multiple captions per object instance and using class name as output prefix, were crucial for achieving such performance.
Another key feature is its simplicity and flexibility, as it is a straightforward pre-processing technique that can be used with any image-captioning model without imposing any restrictions on model architecture or training process.
Given these advantages, we believe that PCL will serve as a building block for a robust baseline in future OVD research.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Vision-Language Pre-training</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Modality-specific pre-training is a widely used approach that improves performance on downstream tasks.
For instance, in computer vision, pre-trained models on the ImageNet classification task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> are commonly used.
In natural language processing area, models are often pre-trained on a large-scale unlabeled text data such as Wikipeidia, BookCorpus <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, and Colossal Clean Crawled Corpus <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Vision-language pre-training (VLP) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> takes this approach one step further by learning a shared latent space where two different modalities are aligned together.
This shared latent space allows for zero-shot generalization, a unique characteristic of VLP.
For example, CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> achieved impressive results on zero-shot image classification on various benchmark datasets.
Most OVD methods, including ours, also rely on the use of VLMs.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Open-Vocabulary Object Detection</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Open-vocabulary detection methods can be classified into three categories.
The first category consists of methods that utilize image-text pairs.
For example, RegionCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> trains a region-level CLIP model on region-concept pairs created from an image-caption dataset.
When fine-tuned on a detection dataset, it outperforms the one using the original CLIP model.
VLDet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> trains a model on a detection dataset and an image-text pair dataset simultaneously.
For an input consisting of an image-text pair, it extracts all nouns from the caption, matches them with region proposals using a bipartite matching algorithm, and uses them as ground-truth labels.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">The methods in the second category leverage both the visual and linguistic latent spaces of VLMs.
For instance, ViLD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> uses an additional loss to minimize the L1 distance between the predicted class embeddings of region proposals and the corresponding CLIP image embeddings.
Object-centric OVD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> introduces inter-embedding relationship loss to ensure that the relationship between two region embeddings is consistent with that of CLIP image embeddings.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">The methods in the last category employ the image encoder of VLMs directly as the detector’s backbone.
OWL-ViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> constructs a detection model by attaching MLP heads on top of the CLIP image encoder.
F-VLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> uses a similar approach to OWL-ViT, but does not train the backbone during training.
These methods can effectively utilize the knowledge in VLM’s visual latent space and typically outperform other approaches.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">Compared to these approaches, PCL is an extension of the most fundamental approach, focusing solely on extracting knowledge from VLM’s linguistic latent space by using only a detection dataset.
We believe that combining our approach with previous methods could lead to enhanced performance, but we’ll leave this for future studies.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we explain three main components of the proposed methods: an image captioning model, a pseudo caption label generation method, and an open-vocabulary detection model.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Image Captioning Model</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.4" class="ltx_p">Given an image <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">I</annotation></semantics></math> and a corresponding caption <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">c</annotation></semantics></math> consisting of <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mi id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">T</annotation></semantics></math> words (<math id="S3.SS1.p1.4.m4.1" class="ltx_math_unparsed" alttext="w_{1},...,w_{T})" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><mrow id="S3.SS1.p1.4.m4.1b"><msub id="S3.SS1.p1.4.m4.1.2"><mi id="S3.SS1.p1.4.m4.1.2.2">w</mi><mn id="S3.SS1.p1.4.m4.1.2.3">1</mn></msub><mo id="S3.SS1.p1.4.m4.1.3">,</mo><mi mathvariant="normal" id="S3.SS1.p1.4.m4.1.1">…</mi><mo id="S3.SS1.p1.4.m4.1.4">,</mo><msub id="S3.SS1.p1.4.m4.1.5"><mi id="S3.SS1.p1.4.m4.1.5.2">w</mi><mi id="S3.SS1.p1.4.m4.1.5.3">T</mi></msub><mo stretchy="false" id="S3.SS1.p1.4.m4.1.6">)</mo></mrow><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">w_{1},...,w_{T})</annotation></semantics></math>, an image captioning model is typically trained by minimizing the negative log-likelihood, as follows:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.2" class="ltx_Math" alttext="\mathcal{L}={-\log p(c|I)}=\sum_{t=0}^{T}{-\log p(w_{t+1}|w_{\leq t},I)}," display="block"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.2.1" xref="S3.E1.m1.2.2.1.1.cmml"><mrow id="S3.E1.m1.2.2.1.1" xref="S3.E1.m1.2.2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.2.2.1.1.4" xref="S3.E1.m1.2.2.1.1.4.cmml">ℒ</mi><mo id="S3.E1.m1.2.2.1.1.5" xref="S3.E1.m1.2.2.1.1.5.cmml">=</mo><mrow id="S3.E1.m1.2.2.1.1.1" xref="S3.E1.m1.2.2.1.1.1.cmml"><mo rspace="0.167em" id="S3.E1.m1.2.2.1.1.1a" xref="S3.E1.m1.2.2.1.1.1.cmml">−</mo><mrow id="S3.E1.m1.2.2.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.cmml"><mrow id="S3.E1.m1.2.2.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.3.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.3.1" xref="S3.E1.m1.2.2.1.1.1.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S3.E1.m1.2.2.1.1.1.1.3a" xref="S3.E1.m1.2.2.1.1.1.1.3.cmml">⁡</mo><mi id="S3.E1.m1.2.2.1.1.1.1.3.2" xref="S3.E1.m1.2.2.1.1.1.1.3.2.cmml">p</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.2.2.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.2.2.1.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.2.cmml">c</mi><mo fence="false" id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml">|</mo><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml">I</mi></mrow><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo rspace="0.111em" id="S3.E1.m1.2.2.1.1.6" xref="S3.E1.m1.2.2.1.1.6.cmml">=</mo><mrow id="S3.E1.m1.2.2.1.1.2" xref="S3.E1.m1.2.2.1.1.2.cmml"><munderover id="S3.E1.m1.2.2.1.1.2.3" xref="S3.E1.m1.2.2.1.1.2.3.cmml"><mo movablelimits="false" rspace="0em" id="S3.E1.m1.2.2.1.1.2.3.2.2" xref="S3.E1.m1.2.2.1.1.2.3.2.2.cmml">∑</mo><mrow id="S3.E1.m1.2.2.1.1.2.3.2.3" xref="S3.E1.m1.2.2.1.1.2.3.2.3.cmml"><mi id="S3.E1.m1.2.2.1.1.2.3.2.3.2" xref="S3.E1.m1.2.2.1.1.2.3.2.3.2.cmml">t</mi><mo id="S3.E1.m1.2.2.1.1.2.3.2.3.1" xref="S3.E1.m1.2.2.1.1.2.3.2.3.1.cmml">=</mo><mn id="S3.E1.m1.2.2.1.1.2.3.2.3.3" xref="S3.E1.m1.2.2.1.1.2.3.2.3.3.cmml">0</mn></mrow><mi id="S3.E1.m1.2.2.1.1.2.3.3" xref="S3.E1.m1.2.2.1.1.2.3.3.cmml">T</mi></munderover><mo lspace="0em" id="S3.E1.m1.2.2.1.1.2.2" xref="S3.E1.m1.2.2.1.1.2.2.cmml">−</mo><mrow id="S3.E1.m1.2.2.1.1.2.1" xref="S3.E1.m1.2.2.1.1.2.1.cmml"><mrow id="S3.E1.m1.2.2.1.1.2.1.3" xref="S3.E1.m1.2.2.1.1.2.1.3.cmml"><mi id="S3.E1.m1.2.2.1.1.2.1.3.1" xref="S3.E1.m1.2.2.1.1.2.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S3.E1.m1.2.2.1.1.2.1.3a" xref="S3.E1.m1.2.2.1.1.2.1.3.cmml">⁡</mo><mi id="S3.E1.m1.2.2.1.1.2.1.3.2" xref="S3.E1.m1.2.2.1.1.2.1.3.2.cmml">p</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.1.1.2.1.2" xref="S3.E1.m1.2.2.1.1.2.1.2.cmml">​</mo><mrow id="S3.E1.m1.2.2.1.1.2.1.1.1" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.1.1.2.1.1.1.2" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.2.2.1.1.2.1.1.1.1" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.cmml"><msub id="S3.E1.m1.2.2.1.1.2.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.3.cmml"><mi id="S3.E1.m1.2.2.1.1.2.1.1.1.1.3.2" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.3.2.cmml">w</mi><mrow id="S3.E1.m1.2.2.1.1.2.1.1.1.1.3.3" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.3.3.cmml"><mi id="S3.E1.m1.2.2.1.1.2.1.1.1.1.3.3.2" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.3.3.2.cmml">t</mi><mo id="S3.E1.m1.2.2.1.1.2.1.1.1.1.3.3.1" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.3.3.1.cmml">+</mo><mn id="S3.E1.m1.2.2.1.1.2.1.1.1.1.3.3.3" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.3.3.3.cmml">1</mn></mrow></msub><mo fence="false" id="S3.E1.m1.2.2.1.1.2.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.2.cmml">|</mo><mrow id="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.2.cmml"><msub id="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.2.cmml">w</mi><mrow id="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.3.2.cmml"></mi><mo id="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.3.1" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.3.1.cmml">≤</mo><mi id="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.3.3.cmml">t</mi></mrow></msub><mo id="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.2.cmml">,</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">I</mi></mrow></mrow><mo stretchy="false" id="S3.E1.m1.2.2.1.1.2.1.1.1.3" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E1.m1.2.2.1.2" xref="S3.E1.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1"><and id="S3.E1.m1.2.2.1.1a.cmml" xref="S3.E1.m1.2.2.1"></and><apply id="S3.E1.m1.2.2.1.1b.cmml" xref="S3.E1.m1.2.2.1"><eq id="S3.E1.m1.2.2.1.1.5.cmml" xref="S3.E1.m1.2.2.1.1.5"></eq><ci id="S3.E1.m1.2.2.1.1.4.cmml" xref="S3.E1.m1.2.2.1.1.4">ℒ</ci><apply id="S3.E1.m1.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1"><minus id="S3.E1.m1.2.2.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1"></minus><apply id="S3.E1.m1.2.2.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1"><times id="S3.E1.m1.2.2.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.2"></times><apply id="S3.E1.m1.2.2.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.3"><log id="S3.E1.m1.2.2.1.1.1.1.3.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.3.1"></log><ci id="S3.E1.m1.2.2.1.1.1.1.3.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.3.2">𝑝</ci></apply><apply id="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1">conditional</csymbol><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.2">𝑐</ci><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3">𝐼</ci></apply></apply></apply></apply><apply id="S3.E1.m1.2.2.1.1c.cmml" xref="S3.E1.m1.2.2.1"><eq id="S3.E1.m1.2.2.1.1.6.cmml" xref="S3.E1.m1.2.2.1.1.6"></eq><share href="#S3.E1.m1.2.2.1.1.1.cmml" id="S3.E1.m1.2.2.1.1d.cmml" xref="S3.E1.m1.2.2.1"></share><apply id="S3.E1.m1.2.2.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.2"><minus id="S3.E1.m1.2.2.1.1.2.2.cmml" xref="S3.E1.m1.2.2.1.1.2.2"></minus><apply id="S3.E1.m1.2.2.1.1.2.3.cmml" xref="S3.E1.m1.2.2.1.1.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.2.3.1.cmml" xref="S3.E1.m1.2.2.1.1.2.3">superscript</csymbol><apply id="S3.E1.m1.2.2.1.1.2.3.2.cmml" xref="S3.E1.m1.2.2.1.1.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.2.3.2.1.cmml" xref="S3.E1.m1.2.2.1.1.2.3">subscript</csymbol><sum id="S3.E1.m1.2.2.1.1.2.3.2.2.cmml" xref="S3.E1.m1.2.2.1.1.2.3.2.2"></sum><apply id="S3.E1.m1.2.2.1.1.2.3.2.3.cmml" xref="S3.E1.m1.2.2.1.1.2.3.2.3"><eq id="S3.E1.m1.2.2.1.1.2.3.2.3.1.cmml" xref="S3.E1.m1.2.2.1.1.2.3.2.3.1"></eq><ci id="S3.E1.m1.2.2.1.1.2.3.2.3.2.cmml" xref="S3.E1.m1.2.2.1.1.2.3.2.3.2">𝑡</ci><cn type="integer" id="S3.E1.m1.2.2.1.1.2.3.2.3.3.cmml" xref="S3.E1.m1.2.2.1.1.2.3.2.3.3">0</cn></apply></apply><ci id="S3.E1.m1.2.2.1.1.2.3.3.cmml" xref="S3.E1.m1.2.2.1.1.2.3.3">𝑇</ci></apply><apply id="S3.E1.m1.2.2.1.1.2.1.cmml" xref="S3.E1.m1.2.2.1.1.2.1"><times id="S3.E1.m1.2.2.1.1.2.1.2.cmml" xref="S3.E1.m1.2.2.1.1.2.1.2"></times><apply id="S3.E1.m1.2.2.1.1.2.1.3.cmml" xref="S3.E1.m1.2.2.1.1.2.1.3"><log id="S3.E1.m1.2.2.1.1.2.1.3.1.cmml" xref="S3.E1.m1.2.2.1.1.2.1.3.1"></log><ci id="S3.E1.m1.2.2.1.1.2.1.3.2.cmml" xref="S3.E1.m1.2.2.1.1.2.1.3.2">𝑝</ci></apply><apply id="S3.E1.m1.2.2.1.1.2.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.2.2.1.1.2.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.2">conditional</csymbol><apply id="S3.E1.m1.2.2.1.1.2.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.2.1.1.1.1.3.1.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.2.1.1.1.1.3.2.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.3.2">𝑤</ci><apply id="S3.E1.m1.2.2.1.1.2.1.1.1.1.3.3.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.3.3"><plus id="S3.E1.m1.2.2.1.1.2.1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.3.3.1"></plus><ci id="S3.E1.m1.2.2.1.1.2.1.1.1.1.3.3.2.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.3.3.2">𝑡</ci><cn type="integer" id="S3.E1.m1.2.2.1.1.2.1.1.1.1.3.3.3.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.3.3.3">1</cn></apply></apply><list id="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1"><apply id="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.2">𝑤</ci><apply id="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.3"><leq id="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.3.1"></leq><csymbol cd="latexml" id="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.3.2">absent</csymbol><ci id="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.3.3">𝑡</ci></apply></apply><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝐼</ci></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">\mathcal{L}={-\log p(c|I)}=\sum_{t=0}^{T}{-\log p(w_{t+1}|w_{\leq t},I)},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p1.6" class="ltx_p">where the BOS and EOS tokens (<math id="S3.SS1.p1.5.m1.1" class="ltx_Math" alttext="w_{0}" display="inline"><semantics id="S3.SS1.p1.5.m1.1a"><msub id="S3.SS1.p1.5.m1.1.1" xref="S3.SS1.p1.5.m1.1.1.cmml"><mi id="S3.SS1.p1.5.m1.1.1.2" xref="S3.SS1.p1.5.m1.1.1.2.cmml">w</mi><mn id="S3.SS1.p1.5.m1.1.1.3" xref="S3.SS1.p1.5.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m1.1b"><apply id="S3.SS1.p1.5.m1.1.1.cmml" xref="S3.SS1.p1.5.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m1.1.1.1.cmml" xref="S3.SS1.p1.5.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.5.m1.1.1.2.cmml" xref="S3.SS1.p1.5.m1.1.1.2">𝑤</ci><cn type="integer" id="S3.SS1.p1.5.m1.1.1.3.cmml" xref="S3.SS1.p1.5.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m1.1c">w_{0}</annotation></semantics></math> and <math id="S3.SS1.p1.6.m2.1" class="ltx_Math" alttext="w_{T+1}" display="inline"><semantics id="S3.SS1.p1.6.m2.1a"><msub id="S3.SS1.p1.6.m2.1.1" xref="S3.SS1.p1.6.m2.1.1.cmml"><mi id="S3.SS1.p1.6.m2.1.1.2" xref="S3.SS1.p1.6.m2.1.1.2.cmml">w</mi><mrow id="S3.SS1.p1.6.m2.1.1.3" xref="S3.SS1.p1.6.m2.1.1.3.cmml"><mi id="S3.SS1.p1.6.m2.1.1.3.2" xref="S3.SS1.p1.6.m2.1.1.3.2.cmml">T</mi><mo id="S3.SS1.p1.6.m2.1.1.3.1" xref="S3.SS1.p1.6.m2.1.1.3.1.cmml">+</mo><mn id="S3.SS1.p1.6.m2.1.1.3.3" xref="S3.SS1.p1.6.m2.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m2.1b"><apply id="S3.SS1.p1.6.m2.1.1.cmml" xref="S3.SS1.p1.6.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m2.1.1.1.cmml" xref="S3.SS1.p1.6.m2.1.1">subscript</csymbol><ci id="S3.SS1.p1.6.m2.1.1.2.cmml" xref="S3.SS1.p1.6.m2.1.1.2">𝑤</ci><apply id="S3.SS1.p1.6.m2.1.1.3.cmml" xref="S3.SS1.p1.6.m2.1.1.3"><plus id="S3.SS1.p1.6.m2.1.1.3.1.cmml" xref="S3.SS1.p1.6.m2.1.1.3.1"></plus><ci id="S3.SS1.p1.6.m2.1.1.3.2.cmml" xref="S3.SS1.p1.6.m2.1.1.3.2">𝑇</ci><cn type="integer" id="S3.SS1.p1.6.m2.1.1.3.3.cmml" xref="S3.SS1.p1.6.m2.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m2.1c">w_{T+1}</annotation></semantics></math>) represent the start and end of a sentence, respectively.
Following the recent image captioning works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, we train the model on a combination of large-scale captioning datasets consisting of Conceptual Captions 3M (CC3M) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, Conceptual Captions 12M (CC12M) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, MSCOCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, and Visual Genome (VG) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.
For VG, we use all three types of annotations that describe regions, attributes, and relationships.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.3" class="ltx_p">While large and diverse datasets can enhance the visual-language understanding of a captioning model, it can also lead to the generation of captions that include irrelevant details, such as background objects and scenery, due to the influence of caption styles in CC3M, CC12M, and MSCOCO.
To address this issue, we introduced a style conditioning mechanism that controls output caption styles.
Formally, Equation <a href="#S3.E1" title="Equation 1 ‣ 3.1 Image Captioning Model ‣ 3 Method ‣ Open-Vocabulary Object Detection using Pseudo Caption Labels" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> can be re-formulated as follows:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.5" class="ltx_Math" alttext="\mathcal{L}={-\log p(c|I,z)}=\sum_{t=0}^{T}{-\log p(w_{t+1}|w_{\leq t},I,z)}," display="block"><semantics id="S3.E2.m1.5a"><mrow id="S3.E2.m1.5.5.1" xref="S3.E2.m1.5.5.1.1.cmml"><mrow id="S3.E2.m1.5.5.1.1" xref="S3.E2.m1.5.5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.5.5.1.1.4" xref="S3.E2.m1.5.5.1.1.4.cmml">ℒ</mi><mo id="S3.E2.m1.5.5.1.1.5" xref="S3.E2.m1.5.5.1.1.5.cmml">=</mo><mrow id="S3.E2.m1.5.5.1.1.1" xref="S3.E2.m1.5.5.1.1.1.cmml"><mo rspace="0.167em" id="S3.E2.m1.5.5.1.1.1a" xref="S3.E2.m1.5.5.1.1.1.cmml">−</mo><mrow id="S3.E2.m1.5.5.1.1.1.1" xref="S3.E2.m1.5.5.1.1.1.1.cmml"><mrow id="S3.E2.m1.5.5.1.1.1.1.3" xref="S3.E2.m1.5.5.1.1.1.1.3.cmml"><mi id="S3.E2.m1.5.5.1.1.1.1.3.1" xref="S3.E2.m1.5.5.1.1.1.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S3.E2.m1.5.5.1.1.1.1.3a" xref="S3.E2.m1.5.5.1.1.1.1.3.cmml">⁡</mo><mi id="S3.E2.m1.5.5.1.1.1.1.3.2" xref="S3.E2.m1.5.5.1.1.1.1.3.2.cmml">p</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.1.1.1.1.2" xref="S3.E2.m1.5.5.1.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m1.5.5.1.1.1.1.1.1" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.5.5.1.1.1.1.1.1.2" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.5.5.1.1.1.1.1.1.1" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.5.5.1.1.1.1.1.1.1.2" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.2.cmml">c</mi><mo fence="false" id="S3.E2.m1.5.5.1.1.1.1.1.1.1.1" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.cmml">|</mo><mrow id="S3.E2.m1.5.5.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.3.1.cmml"><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">I</mi><mo id="S3.E2.m1.5.5.1.1.1.1.1.1.1.3.2.1" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.3.1.cmml">,</mo><mi id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml">z</mi></mrow></mrow><mo stretchy="false" id="S3.E2.m1.5.5.1.1.1.1.1.1.3" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo rspace="0.111em" id="S3.E2.m1.5.5.1.1.6" xref="S3.E2.m1.5.5.1.1.6.cmml">=</mo><mrow id="S3.E2.m1.5.5.1.1.2" xref="S3.E2.m1.5.5.1.1.2.cmml"><munderover id="S3.E2.m1.5.5.1.1.2.3" xref="S3.E2.m1.5.5.1.1.2.3.cmml"><mo movablelimits="false" rspace="0em" id="S3.E2.m1.5.5.1.1.2.3.2.2" xref="S3.E2.m1.5.5.1.1.2.3.2.2.cmml">∑</mo><mrow id="S3.E2.m1.5.5.1.1.2.3.2.3" xref="S3.E2.m1.5.5.1.1.2.3.2.3.cmml"><mi id="S3.E2.m1.5.5.1.1.2.3.2.3.2" xref="S3.E2.m1.5.5.1.1.2.3.2.3.2.cmml">t</mi><mo id="S3.E2.m1.5.5.1.1.2.3.2.3.1" xref="S3.E2.m1.5.5.1.1.2.3.2.3.1.cmml">=</mo><mn id="S3.E2.m1.5.5.1.1.2.3.2.3.3" xref="S3.E2.m1.5.5.1.1.2.3.2.3.3.cmml">0</mn></mrow><mi id="S3.E2.m1.5.5.1.1.2.3.3" xref="S3.E2.m1.5.5.1.1.2.3.3.cmml">T</mi></munderover><mo lspace="0em" id="S3.E2.m1.5.5.1.1.2.2" xref="S3.E2.m1.5.5.1.1.2.2.cmml">−</mo><mrow id="S3.E2.m1.5.5.1.1.2.1" xref="S3.E2.m1.5.5.1.1.2.1.cmml"><mrow id="S3.E2.m1.5.5.1.1.2.1.3" xref="S3.E2.m1.5.5.1.1.2.1.3.cmml"><mi id="S3.E2.m1.5.5.1.1.2.1.3.1" xref="S3.E2.m1.5.5.1.1.2.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S3.E2.m1.5.5.1.1.2.1.3a" xref="S3.E2.m1.5.5.1.1.2.1.3.cmml">⁡</mo><mi id="S3.E2.m1.5.5.1.1.2.1.3.2" xref="S3.E2.m1.5.5.1.1.2.1.3.2.cmml">p</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.1.1.2.1.2" xref="S3.E2.m1.5.5.1.1.2.1.2.cmml">​</mo><mrow id="S3.E2.m1.5.5.1.1.2.1.1.1" xref="S3.E2.m1.5.5.1.1.2.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.5.5.1.1.2.1.1.1.2" xref="S3.E2.m1.5.5.1.1.2.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.5.5.1.1.2.1.1.1.1" xref="S3.E2.m1.5.5.1.1.2.1.1.1.1.cmml"><msub id="S3.E2.m1.5.5.1.1.2.1.1.1.1.3" xref="S3.E2.m1.5.5.1.1.2.1.1.1.1.3.cmml"><mi id="S3.E2.m1.5.5.1.1.2.1.1.1.1.3.2" xref="S3.E2.m1.5.5.1.1.2.1.1.1.1.3.2.cmml">w</mi><mrow id="S3.E2.m1.5.5.1.1.2.1.1.1.1.3.3" xref="S3.E2.m1.5.5.1.1.2.1.1.1.1.3.3.cmml"><mi id="S3.E2.m1.5.5.1.1.2.1.1.1.1.3.3.2" xref="S3.E2.m1.5.5.1.1.2.1.1.1.1.3.3.2.cmml">t</mi><mo id="S3.E2.m1.5.5.1.1.2.1.1.1.1.3.3.1" xref="S3.E2.m1.5.5.1.1.2.1.1.1.1.3.3.1.cmml">+</mo><mn id="S3.E2.m1.5.5.1.1.2.1.1.1.1.3.3.3" xref="S3.E2.m1.5.5.1.1.2.1.1.1.1.3.3.3.cmml">1</mn></mrow></msub><mo fence="false" id="S3.E2.m1.5.5.1.1.2.1.1.1.1.2" xref="S3.E2.m1.5.5.1.1.2.1.1.1.1.2.cmml">|</mo><mrow id="S3.E2.m1.5.5.1.1.2.1.1.1.1.1.1" xref="S3.E2.m1.5.5.1.1.2.1.1.1.1.1.2.cmml"><msub id="S3.E2.m1.5.5.1.1.2.1.1.1.1.1.1.1" xref="S3.E2.m1.5.5.1.1.2.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.5.5.1.1.2.1.1.1.1.1.1.1.2" xref="S3.E2.m1.5.5.1.1.2.1.1.1.1.1.1.1.2.cmml">w</mi><mrow id="S3.E2.m1.5.5.1.1.2.1.1.1.1.1.1.1.3" xref="S3.E2.m1.5.5.1.1.2.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.5.5.1.1.2.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.5.5.1.1.2.1.1.1.1.1.1.1.3.2.cmml"></mi><mo id="S3.E2.m1.5.5.1.1.2.1.1.1.1.1.1.1.3.1" xref="S3.E2.m1.5.5.1.1.2.1.1.1.1.1.1.1.3.1.cmml">≤</mo><mi id="S3.E2.m1.5.5.1.1.2.1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.5.5.1.1.2.1.1.1.1.1.1.1.3.3.cmml">t</mi></mrow></msub><mo id="S3.E2.m1.5.5.1.1.2.1.1.1.1.1.1.2" xref="S3.E2.m1.5.5.1.1.2.1.1.1.1.1.2.cmml">,</mo><mi id="S3.E2.m1.3.3" xref="S3.E2.m1.3.3.cmml">I</mi><mo id="S3.E2.m1.5.5.1.1.2.1.1.1.1.1.1.3" xref="S3.E2.m1.5.5.1.1.2.1.1.1.1.1.2.cmml">,</mo><mi id="S3.E2.m1.4.4" xref="S3.E2.m1.4.4.cmml">z</mi></mrow></mrow><mo stretchy="false" id="S3.E2.m1.5.5.1.1.2.1.1.1.3" xref="S3.E2.m1.5.5.1.1.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E2.m1.5.5.1.2" xref="S3.E2.m1.5.5.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.5b"><apply id="S3.E2.m1.5.5.1.1.cmml" xref="S3.E2.m1.5.5.1"><and id="S3.E2.m1.5.5.1.1a.cmml" xref="S3.E2.m1.5.5.1"></and><apply id="S3.E2.m1.5.5.1.1b.cmml" xref="S3.E2.m1.5.5.1"><eq id="S3.E2.m1.5.5.1.1.5.cmml" xref="S3.E2.m1.5.5.1.1.5"></eq><ci id="S3.E2.m1.5.5.1.1.4.cmml" xref="S3.E2.m1.5.5.1.1.4">ℒ</ci><apply id="S3.E2.m1.5.5.1.1.1.cmml" xref="S3.E2.m1.5.5.1.1.1"><minus id="S3.E2.m1.5.5.1.1.1.2.cmml" xref="S3.E2.m1.5.5.1.1.1"></minus><apply id="S3.E2.m1.5.5.1.1.1.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1"><times id="S3.E2.m1.5.5.1.1.1.1.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1.2"></times><apply id="S3.E2.m1.5.5.1.1.1.1.3.cmml" xref="S3.E2.m1.5.5.1.1.1.1.3"><log id="S3.E2.m1.5.5.1.1.1.1.3.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.3.1"></log><ci id="S3.E2.m1.5.5.1.1.1.1.3.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1.3.2">𝑝</ci></apply><apply id="S3.E2.m1.5.5.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.1">conditional</csymbol><ci id="S3.E2.m1.5.5.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.2">𝑐</ci><list id="S3.E2.m1.5.5.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.3.2"><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">𝐼</ci><ci id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2">𝑧</ci></list></apply></apply></apply></apply><apply id="S3.E2.m1.5.5.1.1c.cmml" xref="S3.E2.m1.5.5.1"><eq id="S3.E2.m1.5.5.1.1.6.cmml" xref="S3.E2.m1.5.5.1.1.6"></eq><share href="#S3.E2.m1.5.5.1.1.1.cmml" id="S3.E2.m1.5.5.1.1d.cmml" xref="S3.E2.m1.5.5.1"></share><apply id="S3.E2.m1.5.5.1.1.2.cmml" xref="S3.E2.m1.5.5.1.1.2"><minus id="S3.E2.m1.5.5.1.1.2.2.cmml" xref="S3.E2.m1.5.5.1.1.2.2"></minus><apply id="S3.E2.m1.5.5.1.1.2.3.cmml" xref="S3.E2.m1.5.5.1.1.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.2.3.1.cmml" xref="S3.E2.m1.5.5.1.1.2.3">superscript</csymbol><apply id="S3.E2.m1.5.5.1.1.2.3.2.cmml" xref="S3.E2.m1.5.5.1.1.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.2.3.2.1.cmml" xref="S3.E2.m1.5.5.1.1.2.3">subscript</csymbol><sum id="S3.E2.m1.5.5.1.1.2.3.2.2.cmml" xref="S3.E2.m1.5.5.1.1.2.3.2.2"></sum><apply id="S3.E2.m1.5.5.1.1.2.3.2.3.cmml" xref="S3.E2.m1.5.5.1.1.2.3.2.3"><eq id="S3.E2.m1.5.5.1.1.2.3.2.3.1.cmml" xref="S3.E2.m1.5.5.1.1.2.3.2.3.1"></eq><ci id="S3.E2.m1.5.5.1.1.2.3.2.3.2.cmml" xref="S3.E2.m1.5.5.1.1.2.3.2.3.2">𝑡</ci><cn type="integer" id="S3.E2.m1.5.5.1.1.2.3.2.3.3.cmml" xref="S3.E2.m1.5.5.1.1.2.3.2.3.3">0</cn></apply></apply><ci id="S3.E2.m1.5.5.1.1.2.3.3.cmml" xref="S3.E2.m1.5.5.1.1.2.3.3">𝑇</ci></apply><apply id="S3.E2.m1.5.5.1.1.2.1.cmml" xref="S3.E2.m1.5.5.1.1.2.1"><times id="S3.E2.m1.5.5.1.1.2.1.2.cmml" xref="S3.E2.m1.5.5.1.1.2.1.2"></times><apply id="S3.E2.m1.5.5.1.1.2.1.3.cmml" xref="S3.E2.m1.5.5.1.1.2.1.3"><log id="S3.E2.m1.5.5.1.1.2.1.3.1.cmml" xref="S3.E2.m1.5.5.1.1.2.1.3.1"></log><ci id="S3.E2.m1.5.5.1.1.2.1.3.2.cmml" xref="S3.E2.m1.5.5.1.1.2.1.3.2">𝑝</ci></apply><apply id="S3.E2.m1.5.5.1.1.2.1.1.1.1.cmml" xref="S3.E2.m1.5.5.1.1.2.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.5.5.1.1.2.1.1.1.1.2.cmml" xref="S3.E2.m1.5.5.1.1.2.1.1.1.1.2">conditional</csymbol><apply id="S3.E2.m1.5.5.1.1.2.1.1.1.1.3.cmml" xref="S3.E2.m1.5.5.1.1.2.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.2.1.1.1.1.3.1.cmml" xref="S3.E2.m1.5.5.1.1.2.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.5.5.1.1.2.1.1.1.1.3.2.cmml" xref="S3.E2.m1.5.5.1.1.2.1.1.1.1.3.2">𝑤</ci><apply id="S3.E2.m1.5.5.1.1.2.1.1.1.1.3.3.cmml" xref="S3.E2.m1.5.5.1.1.2.1.1.1.1.3.3"><plus id="S3.E2.m1.5.5.1.1.2.1.1.1.1.3.3.1.cmml" xref="S3.E2.m1.5.5.1.1.2.1.1.1.1.3.3.1"></plus><ci id="S3.E2.m1.5.5.1.1.2.1.1.1.1.3.3.2.cmml" xref="S3.E2.m1.5.5.1.1.2.1.1.1.1.3.3.2">𝑡</ci><cn type="integer" id="S3.E2.m1.5.5.1.1.2.1.1.1.1.3.3.3.cmml" xref="S3.E2.m1.5.5.1.1.2.1.1.1.1.3.3.3">1</cn></apply></apply><list id="S3.E2.m1.5.5.1.1.2.1.1.1.1.1.2.cmml" xref="S3.E2.m1.5.5.1.1.2.1.1.1.1.1.1"><apply id="S3.E2.m1.5.5.1.1.2.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.5.5.1.1.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.5.5.1.1.2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.5.5.1.1.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.5.5.1.1.2.1.1.1.1.1.1.1.2">𝑤</ci><apply id="S3.E2.m1.5.5.1.1.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.5.5.1.1.2.1.1.1.1.1.1.1.3"><leq id="S3.E2.m1.5.5.1.1.2.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.5.5.1.1.2.1.1.1.1.1.1.1.3.1"></leq><csymbol cd="latexml" id="S3.E2.m1.5.5.1.1.2.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.5.5.1.1.2.1.1.1.1.1.1.1.3.2">absent</csymbol><ci id="S3.E2.m1.5.5.1.1.2.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.5.5.1.1.2.1.1.1.1.1.1.1.3.3">𝑡</ci></apply></apply><ci id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3">𝐼</ci><ci id="S3.E2.m1.4.4.cmml" xref="S3.E2.m1.4.4">𝑧</ci></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.5c">\mathcal{L}={-\log p(c|I,z)}=\sum_{t=0}^{T}{-\log p(w_{t+1}|w_{\leq t},I,z)},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p2.2" class="ltx_p">where the variable <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝑧</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">z</annotation></semantics></math> represents the style conditioning vector that corresponds to the training dataset identities.
To incorporate the style vector, we concatenate the style vector <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">𝑧</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">z</annotation></semantics></math> to the visual feature vector and feed the combined vector to the caption decoder as a prefix context.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">By using style conditioning, our conditional captioning model can reliably produce captions that accurately describe images in a desired format.
We have chosen the VG-region style as the style condition for our experiments, because it results in generated captions that effectively describe the primary objects, their attributes and their relationships with the surrounding environment, all in a brief and succinct manner.
More details about our conditional captioning model are explained in Section <a href="#S4.SS1.SSS1" title="4.1.1 Captioning Model ‣ 4.1 Implementation Details ‣ 4 Experiments ‣ Open-Vocabulary Object Detection using Pseudo Caption Labels" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1.1</span></a>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Pseudo Caption Label Generation</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">To generate pseudo caption labels for each object in the training dataset, we use the captioning model explained earlier.
This is done through a 4-step process:</p>
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">Crop an object image</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">Feed the cropped image to the captioning model</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">Generate captions for the object, utilizing the object name as the output prefix</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p">Collect K distinct captions for the object</p>
</div>
</li>
</ol>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Pseudo caption labels provide a model much diverse information for knowledge distillation compared to manual prompt labels.
Nevertheless, during testing, we still employ manual prompts labels, which can lead to a decline in performance due to the discrepancy of target labels between the training and testing phases.
To mitigate this issue, we also use a small portion of manual prompt labels for training.
We show its impact in the ablation experiments in Section <a href="#S4.SS4" title="4.4 Ablation Studies ‣ 4 Experiments ‣ Open-Vocabulary Object Detection using Pseudo Caption Labels" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Detection Model</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> is a Transformer-based object detection model that solves object detection as a set prediction problem, utilizing a bipartite matching algorithm.
Our detection model is based on Deformable-DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, which improves the convergence speed of DETR using a deformable attention mechanism.
For open-vocabulary detection, we replaced the model’s last classification layer with pre-trained CLIP text embeddings of target classes, following the previous work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.4" class="ltx_p">The model operates similarly to the original Deformable DETR, with the decoder producing class embeddings for object queries.
To compute the class probability of the <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mi id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><ci id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">j</annotation></semantics></math>-th target class <math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="c_{j}" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><msub id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml"><mi id="S3.SS3.p2.2.m2.1.1.2" xref="S3.SS3.p2.2.m2.1.1.2.cmml">c</mi><mi id="S3.SS3.p2.2.m2.1.1.3" xref="S3.SS3.p2.2.m2.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><apply id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p2.2.m2.1.1.2.cmml" xref="S3.SS3.p2.2.m2.1.1.2">𝑐</ci><ci id="S3.SS3.p2.2.m2.1.1.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">c_{j}</annotation></semantics></math> given the <math id="S3.SS3.p2.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS3.p2.3.m3.1a"><mi id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><ci id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">i</annotation></semantics></math>-th object query <math id="S3.SS3.p2.4.m4.1" class="ltx_Math" alttext="o_{i}" display="inline"><semantics id="S3.SS3.p2.4.m4.1a"><msub id="S3.SS3.p2.4.m4.1.1" xref="S3.SS3.p2.4.m4.1.1.cmml"><mi id="S3.SS3.p2.4.m4.1.1.2" xref="S3.SS3.p2.4.m4.1.1.2.cmml">o</mi><mi id="S3.SS3.p2.4.m4.1.1.3" xref="S3.SS3.p2.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.1b"><apply id="S3.SS3.p2.4.m4.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.4.m4.1.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS3.p2.4.m4.1.1.2.cmml" xref="S3.SS3.p2.4.m4.1.1.2">𝑜</ci><ci id="S3.SS3.p2.4.m4.1.1.3.cmml" xref="S3.SS3.p2.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.1c">o_{i}</annotation></semantics></math>, we use the following equation:</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.3" class="ltx_Math" alttext="p(c_{j}|o_{i})=s(\hat{e}_{i},e_{j})=\sigma\left(a\frac{\hat{e}_{i}\cdot e_{j}}{|\hat{e}_{i}||e_{j}|}+b\right)." display="block"><semantics id="S3.E3.m1.3a"><mrow id="S3.E3.m1.3.3.1" xref="S3.E3.m1.3.3.1.1.cmml"><mrow id="S3.E3.m1.3.3.1.1" xref="S3.E3.m1.3.3.1.1.cmml"><mrow id="S3.E3.m1.3.3.1.1.1" xref="S3.E3.m1.3.3.1.1.1.cmml"><mi id="S3.E3.m1.3.3.1.1.1.3" xref="S3.E3.m1.3.3.1.1.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.1.1.1.2" xref="S3.E3.m1.3.3.1.1.1.2.cmml">​</mo><mrow id="S3.E3.m1.3.3.1.1.1.1.1" xref="S3.E3.m1.3.3.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.3.3.1.1.1.1.1.2" xref="S3.E3.m1.3.3.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.3.3.1.1.1.1.1.1" xref="S3.E3.m1.3.3.1.1.1.1.1.1.cmml"><msub id="S3.E3.m1.3.3.1.1.1.1.1.1.2" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2.cmml"><mi id="S3.E3.m1.3.3.1.1.1.1.1.1.2.2" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.cmml">c</mi><mi id="S3.E3.m1.3.3.1.1.1.1.1.1.2.3" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2.3.cmml">j</mi></msub><mo fence="false" id="S3.E3.m1.3.3.1.1.1.1.1.1.1" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.cmml">|</mo><msub id="S3.E3.m1.3.3.1.1.1.1.1.1.3" xref="S3.E3.m1.3.3.1.1.1.1.1.1.3.cmml"><mi id="S3.E3.m1.3.3.1.1.1.1.1.1.3.2" xref="S3.E3.m1.3.3.1.1.1.1.1.1.3.2.cmml">o</mi><mi id="S3.E3.m1.3.3.1.1.1.1.1.1.3.3" xref="S3.E3.m1.3.3.1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo stretchy="false" id="S3.E3.m1.3.3.1.1.1.1.1.3" xref="S3.E3.m1.3.3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.3.3.1.1.6" xref="S3.E3.m1.3.3.1.1.6.cmml">=</mo><mrow id="S3.E3.m1.3.3.1.1.3" xref="S3.E3.m1.3.3.1.1.3.cmml"><mi id="S3.E3.m1.3.3.1.1.3.4" xref="S3.E3.m1.3.3.1.1.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.1.1.3.3" xref="S3.E3.m1.3.3.1.1.3.3.cmml">​</mo><mrow id="S3.E3.m1.3.3.1.1.3.2.2" xref="S3.E3.m1.3.3.1.1.3.2.3.cmml"><mo stretchy="false" id="S3.E3.m1.3.3.1.1.3.2.2.3" xref="S3.E3.m1.3.3.1.1.3.2.3.cmml">(</mo><msub id="S3.E3.m1.3.3.1.1.2.1.1.1" xref="S3.E3.m1.3.3.1.1.2.1.1.1.cmml"><mover accent="true" id="S3.E3.m1.3.3.1.1.2.1.1.1.2" xref="S3.E3.m1.3.3.1.1.2.1.1.1.2.cmml"><mi id="S3.E3.m1.3.3.1.1.2.1.1.1.2.2" xref="S3.E3.m1.3.3.1.1.2.1.1.1.2.2.cmml">e</mi><mo id="S3.E3.m1.3.3.1.1.2.1.1.1.2.1" xref="S3.E3.m1.3.3.1.1.2.1.1.1.2.1.cmml">^</mo></mover><mi id="S3.E3.m1.3.3.1.1.2.1.1.1.3" xref="S3.E3.m1.3.3.1.1.2.1.1.1.3.cmml">i</mi></msub><mo id="S3.E3.m1.3.3.1.1.3.2.2.4" xref="S3.E3.m1.3.3.1.1.3.2.3.cmml">,</mo><msub id="S3.E3.m1.3.3.1.1.3.2.2.2" xref="S3.E3.m1.3.3.1.1.3.2.2.2.cmml"><mi id="S3.E3.m1.3.3.1.1.3.2.2.2.2" xref="S3.E3.m1.3.3.1.1.3.2.2.2.2.cmml">e</mi><mi id="S3.E3.m1.3.3.1.1.3.2.2.2.3" xref="S3.E3.m1.3.3.1.1.3.2.2.2.3.cmml">j</mi></msub><mo stretchy="false" id="S3.E3.m1.3.3.1.1.3.2.2.5" xref="S3.E3.m1.3.3.1.1.3.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.3.3.1.1.7" xref="S3.E3.m1.3.3.1.1.7.cmml">=</mo><mrow id="S3.E3.m1.3.3.1.1.4" xref="S3.E3.m1.3.3.1.1.4.cmml"><mi id="S3.E3.m1.3.3.1.1.4.3" xref="S3.E3.m1.3.3.1.1.4.3.cmml">σ</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.1.1.4.2" xref="S3.E3.m1.3.3.1.1.4.2.cmml">​</mo><mrow id="S3.E3.m1.3.3.1.1.4.1.1" xref="S3.E3.m1.3.3.1.1.4.1.1.1.cmml"><mo id="S3.E3.m1.3.3.1.1.4.1.1.2" xref="S3.E3.m1.3.3.1.1.4.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.3.3.1.1.4.1.1.1" xref="S3.E3.m1.3.3.1.1.4.1.1.1.cmml"><mrow id="S3.E3.m1.3.3.1.1.4.1.1.1.2" xref="S3.E3.m1.3.3.1.1.4.1.1.1.2.cmml"><mi id="S3.E3.m1.3.3.1.1.4.1.1.1.2.2" xref="S3.E3.m1.3.3.1.1.4.1.1.1.2.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.1.1.4.1.1.1.2.1" xref="S3.E3.m1.3.3.1.1.4.1.1.1.2.1.cmml">​</mo><mfrac id="S3.E3.m1.2.2" xref="S3.E3.m1.2.2.cmml"><mrow id="S3.E3.m1.2.2.4" xref="S3.E3.m1.2.2.4.cmml"><msub id="S3.E3.m1.2.2.4.2" xref="S3.E3.m1.2.2.4.2.cmml"><mover accent="true" id="S3.E3.m1.2.2.4.2.2" xref="S3.E3.m1.2.2.4.2.2.cmml"><mi id="S3.E3.m1.2.2.4.2.2.2" xref="S3.E3.m1.2.2.4.2.2.2.cmml">e</mi><mo id="S3.E3.m1.2.2.4.2.2.1" xref="S3.E3.m1.2.2.4.2.2.1.cmml">^</mo></mover><mi id="S3.E3.m1.2.2.4.2.3" xref="S3.E3.m1.2.2.4.2.3.cmml">i</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E3.m1.2.2.4.1" xref="S3.E3.m1.2.2.4.1.cmml">⋅</mo><msub id="S3.E3.m1.2.2.4.3" xref="S3.E3.m1.2.2.4.3.cmml"><mi id="S3.E3.m1.2.2.4.3.2" xref="S3.E3.m1.2.2.4.3.2.cmml">e</mi><mi id="S3.E3.m1.2.2.4.3.3" xref="S3.E3.m1.2.2.4.3.3.cmml">j</mi></msub></mrow><mrow id="S3.E3.m1.2.2.2" xref="S3.E3.m1.2.2.2.cmml"><mrow id="S3.E3.m1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.2.1.cmml">|</mo><msub id="S3.E3.m1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.cmml"><mover accent="true" id="S3.E3.m1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.2.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.2.2" xref="S3.E3.m1.1.1.1.1.1.1.2.2.cmml">e</mi><mo id="S3.E3.m1.1.1.1.1.1.1.2.1" xref="S3.E3.m1.1.1.1.1.1.1.2.1.cmml">^</mo></mover><mi id="S3.E3.m1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.2.1.cmml">|</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.2.3" xref="S3.E3.m1.2.2.2.3.cmml">​</mo><mrow id="S3.E3.m1.2.2.2.2.1" xref="S3.E3.m1.2.2.2.2.2.cmml"><mo stretchy="false" id="S3.E3.m1.2.2.2.2.1.2" xref="S3.E3.m1.2.2.2.2.2.1.cmml">|</mo><msub id="S3.E3.m1.2.2.2.2.1.1" xref="S3.E3.m1.2.2.2.2.1.1.cmml"><mi id="S3.E3.m1.2.2.2.2.1.1.2" xref="S3.E3.m1.2.2.2.2.1.1.2.cmml">e</mi><mi id="S3.E3.m1.2.2.2.2.1.1.3" xref="S3.E3.m1.2.2.2.2.1.1.3.cmml">j</mi></msub><mo stretchy="false" id="S3.E3.m1.2.2.2.2.1.3" xref="S3.E3.m1.2.2.2.2.2.1.cmml">|</mo></mrow></mrow></mfrac></mrow><mo id="S3.E3.m1.3.3.1.1.4.1.1.1.1" xref="S3.E3.m1.3.3.1.1.4.1.1.1.1.cmml">+</mo><mi id="S3.E3.m1.3.3.1.1.4.1.1.1.3" xref="S3.E3.m1.3.3.1.1.4.1.1.1.3.cmml">b</mi></mrow><mo id="S3.E3.m1.3.3.1.1.4.1.1.3" xref="S3.E3.m1.3.3.1.1.4.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S3.E3.m1.3.3.1.2" xref="S3.E3.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.3b"><apply id="S3.E3.m1.3.3.1.1.cmml" xref="S3.E3.m1.3.3.1"><and id="S3.E3.m1.3.3.1.1a.cmml" xref="S3.E3.m1.3.3.1"></and><apply id="S3.E3.m1.3.3.1.1b.cmml" xref="S3.E3.m1.3.3.1"><eq id="S3.E3.m1.3.3.1.1.6.cmml" xref="S3.E3.m1.3.3.1.1.6"></eq><apply id="S3.E3.m1.3.3.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1"><times id="S3.E3.m1.3.3.1.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.1.2"></times><ci id="S3.E3.m1.3.3.1.1.1.3.cmml" xref="S3.E3.m1.3.3.1.1.1.3">𝑝</ci><apply id="S3.E3.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1"><csymbol cd="latexml" id="S3.E3.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1">conditional</csymbol><apply id="S3.E3.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E3.m1.3.3.1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2.2">𝑐</ci><ci id="S3.E3.m1.3.3.1.1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2.3">𝑗</ci></apply><apply id="S3.E3.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.3.3.1.1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.3.2">𝑜</ci><ci id="S3.E3.m1.3.3.1.1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply></apply><apply id="S3.E3.m1.3.3.1.1.3.cmml" xref="S3.E3.m1.3.3.1.1.3"><times id="S3.E3.m1.3.3.1.1.3.3.cmml" xref="S3.E3.m1.3.3.1.1.3.3"></times><ci id="S3.E3.m1.3.3.1.1.3.4.cmml" xref="S3.E3.m1.3.3.1.1.3.4">𝑠</ci><interval closure="open" id="S3.E3.m1.3.3.1.1.3.2.3.cmml" xref="S3.E3.m1.3.3.1.1.3.2.2"><apply id="S3.E3.m1.3.3.1.1.2.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.2.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.2.1.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.2.1.1.1">subscript</csymbol><apply id="S3.E3.m1.3.3.1.1.2.1.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.2.1.1.1.2"><ci id="S3.E3.m1.3.3.1.1.2.1.1.1.2.1.cmml" xref="S3.E3.m1.3.3.1.1.2.1.1.1.2.1">^</ci><ci id="S3.E3.m1.3.3.1.1.2.1.1.1.2.2.cmml" xref="S3.E3.m1.3.3.1.1.2.1.1.1.2.2">𝑒</ci></apply><ci id="S3.E3.m1.3.3.1.1.2.1.1.1.3.cmml" xref="S3.E3.m1.3.3.1.1.2.1.1.1.3">𝑖</ci></apply><apply id="S3.E3.m1.3.3.1.1.3.2.2.2.cmml" xref="S3.E3.m1.3.3.1.1.3.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.3.2.2.2.1.cmml" xref="S3.E3.m1.3.3.1.1.3.2.2.2">subscript</csymbol><ci id="S3.E3.m1.3.3.1.1.3.2.2.2.2.cmml" xref="S3.E3.m1.3.3.1.1.3.2.2.2.2">𝑒</ci><ci id="S3.E3.m1.3.3.1.1.3.2.2.2.3.cmml" xref="S3.E3.m1.3.3.1.1.3.2.2.2.3">𝑗</ci></apply></interval></apply></apply><apply id="S3.E3.m1.3.3.1.1c.cmml" xref="S3.E3.m1.3.3.1"><eq id="S3.E3.m1.3.3.1.1.7.cmml" xref="S3.E3.m1.3.3.1.1.7"></eq><share href="#S3.E3.m1.3.3.1.1.3.cmml" id="S3.E3.m1.3.3.1.1d.cmml" xref="S3.E3.m1.3.3.1"></share><apply id="S3.E3.m1.3.3.1.1.4.cmml" xref="S3.E3.m1.3.3.1.1.4"><times id="S3.E3.m1.3.3.1.1.4.2.cmml" xref="S3.E3.m1.3.3.1.1.4.2"></times><ci id="S3.E3.m1.3.3.1.1.4.3.cmml" xref="S3.E3.m1.3.3.1.1.4.3">𝜎</ci><apply id="S3.E3.m1.3.3.1.1.4.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.4.1.1"><plus id="S3.E3.m1.3.3.1.1.4.1.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.4.1.1.1.1"></plus><apply id="S3.E3.m1.3.3.1.1.4.1.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.4.1.1.1.2"><times id="S3.E3.m1.3.3.1.1.4.1.1.1.2.1.cmml" xref="S3.E3.m1.3.3.1.1.4.1.1.1.2.1"></times><ci id="S3.E3.m1.3.3.1.1.4.1.1.1.2.2.cmml" xref="S3.E3.m1.3.3.1.1.4.1.1.1.2.2">𝑎</ci><apply id="S3.E3.m1.2.2.cmml" xref="S3.E3.m1.2.2"><divide id="S3.E3.m1.2.2.3.cmml" xref="S3.E3.m1.2.2"></divide><apply id="S3.E3.m1.2.2.4.cmml" xref="S3.E3.m1.2.2.4"><ci id="S3.E3.m1.2.2.4.1.cmml" xref="S3.E3.m1.2.2.4.1">⋅</ci><apply id="S3.E3.m1.2.2.4.2.cmml" xref="S3.E3.m1.2.2.4.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.4.2.1.cmml" xref="S3.E3.m1.2.2.4.2">subscript</csymbol><apply id="S3.E3.m1.2.2.4.2.2.cmml" xref="S3.E3.m1.2.2.4.2.2"><ci id="S3.E3.m1.2.2.4.2.2.1.cmml" xref="S3.E3.m1.2.2.4.2.2.1">^</ci><ci id="S3.E3.m1.2.2.4.2.2.2.cmml" xref="S3.E3.m1.2.2.4.2.2.2">𝑒</ci></apply><ci id="S3.E3.m1.2.2.4.2.3.cmml" xref="S3.E3.m1.2.2.4.2.3">𝑖</ci></apply><apply id="S3.E3.m1.2.2.4.3.cmml" xref="S3.E3.m1.2.2.4.3"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.4.3.1.cmml" xref="S3.E3.m1.2.2.4.3">subscript</csymbol><ci id="S3.E3.m1.2.2.4.3.2.cmml" xref="S3.E3.m1.2.2.4.3.2">𝑒</ci><ci id="S3.E3.m1.2.2.4.3.3.cmml" xref="S3.E3.m1.2.2.4.3.3">𝑗</ci></apply></apply><apply id="S3.E3.m1.2.2.2.cmml" xref="S3.E3.m1.2.2.2"><times id="S3.E3.m1.2.2.2.3.cmml" xref="S3.E3.m1.2.2.2.3"></times><apply id="S3.E3.m1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1"><abs id="S3.E3.m1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.2"></abs><apply id="S3.E3.m1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1">subscript</csymbol><apply id="S3.E3.m1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2"><ci id="S3.E3.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.1">^</ci><ci id="S3.E3.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.2">𝑒</ci></apply><ci id="S3.E3.m1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3">𝑖</ci></apply></apply><apply id="S3.E3.m1.2.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2.1"><abs id="S3.E3.m1.2.2.2.2.2.1.cmml" xref="S3.E3.m1.2.2.2.2.1.2"></abs><apply id="S3.E3.m1.2.2.2.2.1.1.cmml" xref="S3.E3.m1.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.2.2.1.1.1.cmml" xref="S3.E3.m1.2.2.2.2.1.1">subscript</csymbol><ci id="S3.E3.m1.2.2.2.2.1.1.2.cmml" xref="S3.E3.m1.2.2.2.2.1.1.2">𝑒</ci><ci id="S3.E3.m1.2.2.2.2.1.1.3.cmml" xref="S3.E3.m1.2.2.2.2.1.1.3">𝑗</ci></apply></apply></apply></apply></apply><ci id="S3.E3.m1.3.3.1.1.4.1.1.1.3.cmml" xref="S3.E3.m1.3.3.1.1.4.1.1.1.3">𝑏</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.3c">p(c_{j}|o_{i})=s(\hat{e}_{i},e_{j})=\sigma\left(a\frac{\hat{e}_{i}\cdot e_{j}}{|\hat{e}_{i}||e_{j}|}+b\right).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p2.12" class="ltx_p">Here, <math id="S3.SS3.p2.5.m1.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S3.SS3.p2.5.m1.1a"><mi id="S3.SS3.p2.5.m1.1.1" xref="S3.SS3.p2.5.m1.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.5.m1.1b"><ci id="S3.SS3.p2.5.m1.1.1.cmml" xref="S3.SS3.p2.5.m1.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m1.1c">s</annotation></semantics></math> denotes the score function, <math id="S3.SS3.p2.6.m2.1" class="ltx_Math" alttext="\hat{e}_{i}" display="inline"><semantics id="S3.SS3.p2.6.m2.1a"><msub id="S3.SS3.p2.6.m2.1.1" xref="S3.SS3.p2.6.m2.1.1.cmml"><mover accent="true" id="S3.SS3.p2.6.m2.1.1.2" xref="S3.SS3.p2.6.m2.1.1.2.cmml"><mi id="S3.SS3.p2.6.m2.1.1.2.2" xref="S3.SS3.p2.6.m2.1.1.2.2.cmml">e</mi><mo id="S3.SS3.p2.6.m2.1.1.2.1" xref="S3.SS3.p2.6.m2.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS3.p2.6.m2.1.1.3" xref="S3.SS3.p2.6.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.6.m2.1b"><apply id="S3.SS3.p2.6.m2.1.1.cmml" xref="S3.SS3.p2.6.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.6.m2.1.1.1.cmml" xref="S3.SS3.p2.6.m2.1.1">subscript</csymbol><apply id="S3.SS3.p2.6.m2.1.1.2.cmml" xref="S3.SS3.p2.6.m2.1.1.2"><ci id="S3.SS3.p2.6.m2.1.1.2.1.cmml" xref="S3.SS3.p2.6.m2.1.1.2.1">^</ci><ci id="S3.SS3.p2.6.m2.1.1.2.2.cmml" xref="S3.SS3.p2.6.m2.1.1.2.2">𝑒</ci></apply><ci id="S3.SS3.p2.6.m2.1.1.3.cmml" xref="S3.SS3.p2.6.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.6.m2.1c">\hat{e}_{i}</annotation></semantics></math> represents the predicted class embedding of the object query <math id="S3.SS3.p2.7.m3.1" class="ltx_Math" alttext="o_{i}" display="inline"><semantics id="S3.SS3.p2.7.m3.1a"><msub id="S3.SS3.p2.7.m3.1.1" xref="S3.SS3.p2.7.m3.1.1.cmml"><mi id="S3.SS3.p2.7.m3.1.1.2" xref="S3.SS3.p2.7.m3.1.1.2.cmml">o</mi><mi id="S3.SS3.p2.7.m3.1.1.3" xref="S3.SS3.p2.7.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.7.m3.1b"><apply id="S3.SS3.p2.7.m3.1.1.cmml" xref="S3.SS3.p2.7.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.7.m3.1.1.1.cmml" xref="S3.SS3.p2.7.m3.1.1">subscript</csymbol><ci id="S3.SS3.p2.7.m3.1.1.2.cmml" xref="S3.SS3.p2.7.m3.1.1.2">𝑜</ci><ci id="S3.SS3.p2.7.m3.1.1.3.cmml" xref="S3.SS3.p2.7.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.7.m3.1c">o_{i}</annotation></semantics></math>, and <math id="S3.SS3.p2.8.m4.1" class="ltx_Math" alttext="e_{j}" display="inline"><semantics id="S3.SS3.p2.8.m4.1a"><msub id="S3.SS3.p2.8.m4.1.1" xref="S3.SS3.p2.8.m4.1.1.cmml"><mi id="S3.SS3.p2.8.m4.1.1.2" xref="S3.SS3.p2.8.m4.1.1.2.cmml">e</mi><mi id="S3.SS3.p2.8.m4.1.1.3" xref="S3.SS3.p2.8.m4.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.8.m4.1b"><apply id="S3.SS3.p2.8.m4.1.1.cmml" xref="S3.SS3.p2.8.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.8.m4.1.1.1.cmml" xref="S3.SS3.p2.8.m4.1.1">subscript</csymbol><ci id="S3.SS3.p2.8.m4.1.1.2.cmml" xref="S3.SS3.p2.8.m4.1.1.2">𝑒</ci><ci id="S3.SS3.p2.8.m4.1.1.3.cmml" xref="S3.SS3.p2.8.m4.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.8.m4.1c">e_{j}</annotation></semantics></math> is the target class embedding generated by applying the CLIP text encoder to one of pseudo caption labels (or manual prompt labels) for the target class <math id="S3.SS3.p2.9.m5.1" class="ltx_Math" alttext="c_{j}" display="inline"><semantics id="S3.SS3.p2.9.m5.1a"><msub id="S3.SS3.p2.9.m5.1.1" xref="S3.SS3.p2.9.m5.1.1.cmml"><mi id="S3.SS3.p2.9.m5.1.1.2" xref="S3.SS3.p2.9.m5.1.1.2.cmml">c</mi><mi id="S3.SS3.p2.9.m5.1.1.3" xref="S3.SS3.p2.9.m5.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.9.m5.1b"><apply id="S3.SS3.p2.9.m5.1.1.cmml" xref="S3.SS3.p2.9.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.9.m5.1.1.1.cmml" xref="S3.SS3.p2.9.m5.1.1">subscript</csymbol><ci id="S3.SS3.p2.9.m5.1.1.2.cmml" xref="S3.SS3.p2.9.m5.1.1.2">𝑐</ci><ci id="S3.SS3.p2.9.m5.1.1.3.cmml" xref="S3.SS3.p2.9.m5.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.9.m5.1c">c_{j}</annotation></semantics></math>.
<math id="S3.SS3.p2.10.m6.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S3.SS3.p2.10.m6.1a"><mi id="S3.SS3.p2.10.m6.1.1" xref="S3.SS3.p2.10.m6.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.10.m6.1b"><ci id="S3.SS3.p2.10.m6.1.1.cmml" xref="S3.SS3.p2.10.m6.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.10.m6.1c">a</annotation></semantics></math> is the logit scale, <math id="S3.SS3.p2.11.m7.1" class="ltx_Math" alttext="b" display="inline"><semantics id="S3.SS3.p2.11.m7.1a"><mi id="S3.SS3.p2.11.m7.1.1" xref="S3.SS3.p2.11.m7.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.11.m7.1b"><ci id="S3.SS3.p2.11.m7.1.1.cmml" xref="S3.SS3.p2.11.m7.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.11.m7.1c">b</annotation></semantics></math> is the logit shift, and <math id="S3.SS3.p2.12.m8.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S3.SS3.p2.12.m8.1a"><mi id="S3.SS3.p2.12.m8.1.1" xref="S3.SS3.p2.12.m8.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.12.m8.1b"><ci id="S3.SS3.p2.12.m8.1.1.cmml" xref="S3.SS3.p2.12.m8.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.12.m8.1c">\sigma</annotation></semantics></math> is the sigmoid function.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.5" class="ltx_p">Then, we calculate the classification loss using Equation <a href="#S3.E4" title="Equation 4 ‣ 3.3 Detection Model ‣ 3 Method ‣ Open-Vocabulary Object Detection using Pseudo Caption Labels" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
In this equation, <math id="S3.SS3.p3.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><mi id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><ci id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">N</annotation></semantics></math> is the number of object queries, <math id="S3.SS3.p3.2.m2.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS3.p3.2.m2.1a"><mi id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><ci id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">M</annotation></semantics></math> is the number of target classes and sampled negative classes, and <math id="S3.SS3.p3.3.m3.2" class="ltx_Math" alttext="y_{i,j}" display="inline"><semantics id="S3.SS3.p3.3.m3.2a"><msub id="S3.SS3.p3.3.m3.2.3" xref="S3.SS3.p3.3.m3.2.3.cmml"><mi id="S3.SS3.p3.3.m3.2.3.2" xref="S3.SS3.p3.3.m3.2.3.2.cmml">y</mi><mrow id="S3.SS3.p3.3.m3.2.2.2.4" xref="S3.SS3.p3.3.m3.2.2.2.3.cmml"><mi id="S3.SS3.p3.3.m3.1.1.1.1" xref="S3.SS3.p3.3.m3.1.1.1.1.cmml">i</mi><mo id="S3.SS3.p3.3.m3.2.2.2.4.1" xref="S3.SS3.p3.3.m3.2.2.2.3.cmml">,</mo><mi id="S3.SS3.p3.3.m3.2.2.2.2" xref="S3.SS3.p3.3.m3.2.2.2.2.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.3.m3.2b"><apply id="S3.SS3.p3.3.m3.2.3.cmml" xref="S3.SS3.p3.3.m3.2.3"><csymbol cd="ambiguous" id="S3.SS3.p3.3.m3.2.3.1.cmml" xref="S3.SS3.p3.3.m3.2.3">subscript</csymbol><ci id="S3.SS3.p3.3.m3.2.3.2.cmml" xref="S3.SS3.p3.3.m3.2.3.2">𝑦</ci><list id="S3.SS3.p3.3.m3.2.2.2.3.cmml" xref="S3.SS3.p3.3.m3.2.2.2.4"><ci id="S3.SS3.p3.3.m3.1.1.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1.1.1">𝑖</ci><ci id="S3.SS3.p3.3.m3.2.2.2.2.cmml" xref="S3.SS3.p3.3.m3.2.2.2.2">𝑗</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.3.m3.2c">y_{i,j}</annotation></semantics></math> equals 1 if the object query <math id="S3.SS3.p3.4.m4.1" class="ltx_Math" alttext="o_{i}" display="inline"><semantics id="S3.SS3.p3.4.m4.1a"><msub id="S3.SS3.p3.4.m4.1.1" xref="S3.SS3.p3.4.m4.1.1.cmml"><mi id="S3.SS3.p3.4.m4.1.1.2" xref="S3.SS3.p3.4.m4.1.1.2.cmml">o</mi><mi id="S3.SS3.p3.4.m4.1.1.3" xref="S3.SS3.p3.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.4.m4.1b"><apply id="S3.SS3.p3.4.m4.1.1.cmml" xref="S3.SS3.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.4.m4.1.1.1.cmml" xref="S3.SS3.p3.4.m4.1.1">subscript</csymbol><ci id="S3.SS3.p3.4.m4.1.1.2.cmml" xref="S3.SS3.p3.4.m4.1.1.2">𝑜</ci><ci id="S3.SS3.p3.4.m4.1.1.3.cmml" xref="S3.SS3.p3.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.4.m4.1c">o_{i}</annotation></semantics></math> and the target class <math id="S3.SS3.p3.5.m5.1" class="ltx_Math" alttext="c_{j}" display="inline"><semantics id="S3.SS3.p3.5.m5.1a"><msub id="S3.SS3.p3.5.m5.1.1" xref="S3.SS3.p3.5.m5.1.1.cmml"><mi id="S3.SS3.p3.5.m5.1.1.2" xref="S3.SS3.p3.5.m5.1.1.2.cmml">c</mi><mi id="S3.SS3.p3.5.m5.1.1.3" xref="S3.SS3.p3.5.m5.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.5.m5.1b"><apply id="S3.SS3.p3.5.m5.1.1.cmml" xref="S3.SS3.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.5.m5.1.1.1.cmml" xref="S3.SS3.p3.5.m5.1.1">subscript</csymbol><ci id="S3.SS3.p3.5.m5.1.1.2.cmml" xref="S3.SS3.p3.5.m5.1.1.2">𝑐</ci><ci id="S3.SS3.p3.5.m5.1.1.3.cmml" xref="S3.SS3.p3.5.m5.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.5.m5.1c">c_{j}</annotation></semantics></math> form a positive pair obtained via a bipartite matching, and 0 otherwise.</p>
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.4" class="ltx_Math" alttext="\mathcal{L}_{class}=\frac{1}{N}\sum^{N}_{i=1}\sum^{M}_{j=1}{BCE(p(c_{j}|o_{i}),y_{i,j})}" display="block"><semantics id="S3.E4.m1.4a"><mrow id="S3.E4.m1.4.4" xref="S3.E4.m1.4.4.cmml"><msub id="S3.E4.m1.4.4.4" xref="S3.E4.m1.4.4.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.4.4.4.2" xref="S3.E4.m1.4.4.4.2.cmml">ℒ</mi><mrow id="S3.E4.m1.4.4.4.3" xref="S3.E4.m1.4.4.4.3.cmml"><mi id="S3.E4.m1.4.4.4.3.2" xref="S3.E4.m1.4.4.4.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.4.4.4.3.1" xref="S3.E4.m1.4.4.4.3.1.cmml">​</mo><mi id="S3.E4.m1.4.4.4.3.3" xref="S3.E4.m1.4.4.4.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.4.4.4.3.1a" xref="S3.E4.m1.4.4.4.3.1.cmml">​</mo><mi id="S3.E4.m1.4.4.4.3.4" xref="S3.E4.m1.4.4.4.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.4.4.4.3.1b" xref="S3.E4.m1.4.4.4.3.1.cmml">​</mo><mi id="S3.E4.m1.4.4.4.3.5" xref="S3.E4.m1.4.4.4.3.5.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.4.4.4.3.1c" xref="S3.E4.m1.4.4.4.3.1.cmml">​</mo><mi id="S3.E4.m1.4.4.4.3.6" xref="S3.E4.m1.4.4.4.3.6.cmml">s</mi></mrow></msub><mo id="S3.E4.m1.4.4.3" xref="S3.E4.m1.4.4.3.cmml">=</mo><mrow id="S3.E4.m1.4.4.2" xref="S3.E4.m1.4.4.2.cmml"><mfrac id="S3.E4.m1.4.4.2.4" xref="S3.E4.m1.4.4.2.4.cmml"><mn id="S3.E4.m1.4.4.2.4.2" xref="S3.E4.m1.4.4.2.4.2.cmml">1</mn><mi id="S3.E4.m1.4.4.2.4.3" xref="S3.E4.m1.4.4.2.4.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S3.E4.m1.4.4.2.3" xref="S3.E4.m1.4.4.2.3.cmml">​</mo><mrow id="S3.E4.m1.4.4.2.2" xref="S3.E4.m1.4.4.2.2.cmml"><munderover id="S3.E4.m1.4.4.2.2.3" xref="S3.E4.m1.4.4.2.2.3.cmml"><mo movablelimits="false" rspace="0em" id="S3.E4.m1.4.4.2.2.3.2.2" xref="S3.E4.m1.4.4.2.2.3.2.2.cmml">∑</mo><mrow id="S3.E4.m1.4.4.2.2.3.3" xref="S3.E4.m1.4.4.2.2.3.3.cmml"><mi id="S3.E4.m1.4.4.2.2.3.3.2" xref="S3.E4.m1.4.4.2.2.3.3.2.cmml">i</mi><mo id="S3.E4.m1.4.4.2.2.3.3.1" xref="S3.E4.m1.4.4.2.2.3.3.1.cmml">=</mo><mn id="S3.E4.m1.4.4.2.2.3.3.3" xref="S3.E4.m1.4.4.2.2.3.3.3.cmml">1</mn></mrow><mi id="S3.E4.m1.4.4.2.2.3.2.3" xref="S3.E4.m1.4.4.2.2.3.2.3.cmml">N</mi></munderover><mrow id="S3.E4.m1.4.4.2.2.2" xref="S3.E4.m1.4.4.2.2.2.cmml"><munderover id="S3.E4.m1.4.4.2.2.2.3" xref="S3.E4.m1.4.4.2.2.2.3.cmml"><mo movablelimits="false" id="S3.E4.m1.4.4.2.2.2.3.2.2" xref="S3.E4.m1.4.4.2.2.2.3.2.2.cmml">∑</mo><mrow id="S3.E4.m1.4.4.2.2.2.3.3" xref="S3.E4.m1.4.4.2.2.2.3.3.cmml"><mi id="S3.E4.m1.4.4.2.2.2.3.3.2" xref="S3.E4.m1.4.4.2.2.2.3.3.2.cmml">j</mi><mo id="S3.E4.m1.4.4.2.2.2.3.3.1" xref="S3.E4.m1.4.4.2.2.2.3.3.1.cmml">=</mo><mn id="S3.E4.m1.4.4.2.2.2.3.3.3" xref="S3.E4.m1.4.4.2.2.2.3.3.3.cmml">1</mn></mrow><mi id="S3.E4.m1.4.4.2.2.2.3.2.3" xref="S3.E4.m1.4.4.2.2.2.3.2.3.cmml">M</mi></munderover><mrow id="S3.E4.m1.4.4.2.2.2.2" xref="S3.E4.m1.4.4.2.2.2.2.cmml"><mi id="S3.E4.m1.4.4.2.2.2.2.4" xref="S3.E4.m1.4.4.2.2.2.2.4.cmml">B</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.4.4.2.2.2.2.3" xref="S3.E4.m1.4.4.2.2.2.2.3.cmml">​</mo><mi id="S3.E4.m1.4.4.2.2.2.2.5" xref="S3.E4.m1.4.4.2.2.2.2.5.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.4.4.2.2.2.2.3a" xref="S3.E4.m1.4.4.2.2.2.2.3.cmml">​</mo><mi id="S3.E4.m1.4.4.2.2.2.2.6" xref="S3.E4.m1.4.4.2.2.2.2.6.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.4.4.2.2.2.2.3b" xref="S3.E4.m1.4.4.2.2.2.2.3.cmml">​</mo><mrow id="S3.E4.m1.4.4.2.2.2.2.2.2" xref="S3.E4.m1.4.4.2.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.E4.m1.4.4.2.2.2.2.2.2.3" xref="S3.E4.m1.4.4.2.2.2.2.2.3.cmml">(</mo><mrow id="S3.E4.m1.3.3.1.1.1.1.1.1.1" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.3.3.1.1.1.1.1.1.1.2" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.2.cmml">c</mi><mi id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.3.cmml">j</mi></msub><mo fence="false" id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml">|</mo><msub id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2.cmml">o</mi><mi id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo stretchy="false" id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E4.m1.4.4.2.2.2.2.2.2.4" xref="S3.E4.m1.4.4.2.2.2.2.2.3.cmml">,</mo><msub id="S3.E4.m1.4.4.2.2.2.2.2.2.2" xref="S3.E4.m1.4.4.2.2.2.2.2.2.2.cmml"><mi id="S3.E4.m1.4.4.2.2.2.2.2.2.2.2" xref="S3.E4.m1.4.4.2.2.2.2.2.2.2.2.cmml">y</mi><mrow id="S3.E4.m1.2.2.2.4" xref="S3.E4.m1.2.2.2.3.cmml"><mi id="S3.E4.m1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml">i</mi><mo id="S3.E4.m1.2.2.2.4.1" xref="S3.E4.m1.2.2.2.3.cmml">,</mo><mi id="S3.E4.m1.2.2.2.2" xref="S3.E4.m1.2.2.2.2.cmml">j</mi></mrow></msub><mo stretchy="false" id="S3.E4.m1.4.4.2.2.2.2.2.2.5" xref="S3.E4.m1.4.4.2.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.4b"><apply id="S3.E4.m1.4.4.cmml" xref="S3.E4.m1.4.4"><eq id="S3.E4.m1.4.4.3.cmml" xref="S3.E4.m1.4.4.3"></eq><apply id="S3.E4.m1.4.4.4.cmml" xref="S3.E4.m1.4.4.4"><csymbol cd="ambiguous" id="S3.E4.m1.4.4.4.1.cmml" xref="S3.E4.m1.4.4.4">subscript</csymbol><ci id="S3.E4.m1.4.4.4.2.cmml" xref="S3.E4.m1.4.4.4.2">ℒ</ci><apply id="S3.E4.m1.4.4.4.3.cmml" xref="S3.E4.m1.4.4.4.3"><times id="S3.E4.m1.4.4.4.3.1.cmml" xref="S3.E4.m1.4.4.4.3.1"></times><ci id="S3.E4.m1.4.4.4.3.2.cmml" xref="S3.E4.m1.4.4.4.3.2">𝑐</ci><ci id="S3.E4.m1.4.4.4.3.3.cmml" xref="S3.E4.m1.4.4.4.3.3">𝑙</ci><ci id="S3.E4.m1.4.4.4.3.4.cmml" xref="S3.E4.m1.4.4.4.3.4">𝑎</ci><ci id="S3.E4.m1.4.4.4.3.5.cmml" xref="S3.E4.m1.4.4.4.3.5">𝑠</ci><ci id="S3.E4.m1.4.4.4.3.6.cmml" xref="S3.E4.m1.4.4.4.3.6">𝑠</ci></apply></apply><apply id="S3.E4.m1.4.4.2.cmml" xref="S3.E4.m1.4.4.2"><times id="S3.E4.m1.4.4.2.3.cmml" xref="S3.E4.m1.4.4.2.3"></times><apply id="S3.E4.m1.4.4.2.4.cmml" xref="S3.E4.m1.4.4.2.4"><divide id="S3.E4.m1.4.4.2.4.1.cmml" xref="S3.E4.m1.4.4.2.4"></divide><cn type="integer" id="S3.E4.m1.4.4.2.4.2.cmml" xref="S3.E4.m1.4.4.2.4.2">1</cn><ci id="S3.E4.m1.4.4.2.4.3.cmml" xref="S3.E4.m1.4.4.2.4.3">𝑁</ci></apply><apply id="S3.E4.m1.4.4.2.2.cmml" xref="S3.E4.m1.4.4.2.2"><apply id="S3.E4.m1.4.4.2.2.3.cmml" xref="S3.E4.m1.4.4.2.2.3"><csymbol cd="ambiguous" id="S3.E4.m1.4.4.2.2.3.1.cmml" xref="S3.E4.m1.4.4.2.2.3">subscript</csymbol><apply id="S3.E4.m1.4.4.2.2.3.2.cmml" xref="S3.E4.m1.4.4.2.2.3"><csymbol cd="ambiguous" id="S3.E4.m1.4.4.2.2.3.2.1.cmml" xref="S3.E4.m1.4.4.2.2.3">superscript</csymbol><sum id="S3.E4.m1.4.4.2.2.3.2.2.cmml" xref="S3.E4.m1.4.4.2.2.3.2.2"></sum><ci id="S3.E4.m1.4.4.2.2.3.2.3.cmml" xref="S3.E4.m1.4.4.2.2.3.2.3">𝑁</ci></apply><apply id="S3.E4.m1.4.4.2.2.3.3.cmml" xref="S3.E4.m1.4.4.2.2.3.3"><eq id="S3.E4.m1.4.4.2.2.3.3.1.cmml" xref="S3.E4.m1.4.4.2.2.3.3.1"></eq><ci id="S3.E4.m1.4.4.2.2.3.3.2.cmml" xref="S3.E4.m1.4.4.2.2.3.3.2">𝑖</ci><cn type="integer" id="S3.E4.m1.4.4.2.2.3.3.3.cmml" xref="S3.E4.m1.4.4.2.2.3.3.3">1</cn></apply></apply><apply id="S3.E4.m1.4.4.2.2.2.cmml" xref="S3.E4.m1.4.4.2.2.2"><apply id="S3.E4.m1.4.4.2.2.2.3.cmml" xref="S3.E4.m1.4.4.2.2.2.3"><csymbol cd="ambiguous" id="S3.E4.m1.4.4.2.2.2.3.1.cmml" xref="S3.E4.m1.4.4.2.2.2.3">subscript</csymbol><apply id="S3.E4.m1.4.4.2.2.2.3.2.cmml" xref="S3.E4.m1.4.4.2.2.2.3"><csymbol cd="ambiguous" id="S3.E4.m1.4.4.2.2.2.3.2.1.cmml" xref="S3.E4.m1.4.4.2.2.2.3">superscript</csymbol><sum id="S3.E4.m1.4.4.2.2.2.3.2.2.cmml" xref="S3.E4.m1.4.4.2.2.2.3.2.2"></sum><ci id="S3.E4.m1.4.4.2.2.2.3.2.3.cmml" xref="S3.E4.m1.4.4.2.2.2.3.2.3">𝑀</ci></apply><apply id="S3.E4.m1.4.4.2.2.2.3.3.cmml" xref="S3.E4.m1.4.4.2.2.2.3.3"><eq id="S3.E4.m1.4.4.2.2.2.3.3.1.cmml" xref="S3.E4.m1.4.4.2.2.2.3.3.1"></eq><ci id="S3.E4.m1.4.4.2.2.2.3.3.2.cmml" xref="S3.E4.m1.4.4.2.2.2.3.3.2">𝑗</ci><cn type="integer" id="S3.E4.m1.4.4.2.2.2.3.3.3.cmml" xref="S3.E4.m1.4.4.2.2.2.3.3.3">1</cn></apply></apply><apply id="S3.E4.m1.4.4.2.2.2.2.cmml" xref="S3.E4.m1.4.4.2.2.2.2"><times id="S3.E4.m1.4.4.2.2.2.2.3.cmml" xref="S3.E4.m1.4.4.2.2.2.2.3"></times><ci id="S3.E4.m1.4.4.2.2.2.2.4.cmml" xref="S3.E4.m1.4.4.2.2.2.2.4">𝐵</ci><ci id="S3.E4.m1.4.4.2.2.2.2.5.cmml" xref="S3.E4.m1.4.4.2.2.2.2.5">𝐶</ci><ci id="S3.E4.m1.4.4.2.2.2.2.6.cmml" xref="S3.E4.m1.4.4.2.2.2.2.6">𝐸</ci><interval closure="open" id="S3.E4.m1.4.4.2.2.2.2.2.3.cmml" xref="S3.E4.m1.4.4.2.2.2.2.2.2"><apply id="S3.E4.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1"><times id="S3.E4.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.2"></times><ci id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3">𝑝</ci><apply id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.1">conditional</csymbol><apply id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.2">𝑐</ci><ci id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.3">𝑗</ci></apply><apply id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2">𝑜</ci><ci id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply></apply><apply id="S3.E4.m1.4.4.2.2.2.2.2.2.2.cmml" xref="S3.E4.m1.4.4.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.4.4.2.2.2.2.2.2.2.1.cmml" xref="S3.E4.m1.4.4.2.2.2.2.2.2.2">subscript</csymbol><ci id="S3.E4.m1.4.4.2.2.2.2.2.2.2.2.cmml" xref="S3.E4.m1.4.4.2.2.2.2.2.2.2.2">𝑦</ci><list id="S3.E4.m1.2.2.2.3.cmml" xref="S3.E4.m1.2.2.2.4"><ci id="S3.E4.m1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1">𝑖</ci><ci id="S3.E4.m1.2.2.2.2.cmml" xref="S3.E4.m1.2.2.2.2">𝑗</ci></list></apply></interval></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.4c">\mathcal{L}_{class}=\frac{1}{N}\sum^{N}_{i=1}\sum^{M}_{j=1}{BCE(p(c_{j}|o_{i}),y_{i,j})}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">All other components are identical to those in the original Deformable DETR.
In addition, it is noteworthy that our approach can be applied to other object detection architectures, including Faster-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we evaluate the proposed method on the LVIS benchmark, demonstrate the impact of our design choices through ablation experiments, and show the PCL model’s ability to comprehend complex queries with qualitative analysis.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Implementation Details</h3>

<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Captioning Model</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.1" class="ltx_p">Our style-conditional captioning model consists of two main modules: an image encoder and a caption decoder. We initialize the image encoder and the caption decoder using pre-trained CLIP ViT-L/14 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> and GPT2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, respectively. The model trained for 10 epochs using widely used captioning datasets, including CC3M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, CC12M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, and MSCOCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. Additionally, we further utilize region, attribute, and relation annotations in VG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> dataset. During the training of our model, we use AdamW optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> with a linear warm-up strategy for the first epoch followed by a learning rate decaying with a cosine schedule. We set a base learning rate as 0.0016 with a total batch size of 2048.</p>
</div>
<div id="S4.SS1.SSS1.p2" class="ltx_para">
<p id="S4.SS1.SSS1.p2.1" class="ltx_p">For style conditioning, we categorize the datasets into four styles: a) <span id="S4.SS1.SSS1.p2.1.1" class="ltx_text ltx_font_italic">Scene description</span>, b) <span id="S4.SS1.SSS1.p2.1.2" class="ltx_text ltx_font_italic">Attribute description</span>, c) <span id="S4.SS1.SSS1.p2.1.3" class="ltx_text ltx_font_italic">Relation description</span>, and d) <span id="S4.SS1.SSS1.p2.1.4" class="ltx_text ltx_font_italic">Region description</span>. We assign CC3M, CC12M, and MSCOCO to the style a) and the attribute, relation, and region annotations of VG are assigned to the styles b), c), and d), respectively.
During the training phase, given an image-text pair, we firstly assign a style to the pair according to its dataset identity (<em id="S4.SS1.SSS1.p2.1.5" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S4.SS1.SSS1.p2.1.6" class="ltx_text"></span>, <span id="S4.SS1.SSS1.p2.1.7" class="ltx_text ltx_font_italic">”Relation description”</span> if the pair comes from relation annotation of VG). Then, we concatenate the CLS token of visual features from the image encoder and the parsed word embeddings of style keywords. After the concatenation, the resulting features are utilized as a prefix sequence of our caption decoder model, <em id="S4.SS1.SSS1.p2.1.8" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.SS1.SSS1.p2.1.9" class="ltx_text"></span>, GPT2.
At the inference time, we use <span id="S4.SS1.SSS1.p2.1.10" class="ltx_text ltx_font_italic">Region description</span> style condition since it is the most favorable to the OVD task.</p>
</div>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Detection Model</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.p1.3" class="ltx_p">Our detection model is a modified Deformable DETR described in Section <a href="#S3.SS3" title="3.3 Detection Model ‣ 3 Method ‣ Open-Vocabulary Object Detection using Pseudo Caption Labels" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>.
The following settings apply to all models and experiments, unless otherwise specified.
To handle a large vocabulary dataset, we increase the number of object queries from 100 to 300.
We also activate iterative box-refinement and two-stage prediction options.
The Swin-T model serves as the backbone.
We train models for 80K steps with an effective batch size of 128 by accumulating gradients for two batches (64<math id="S4.SS1.SSS2.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.SSS2.p1.1.m1.1a"><mo id="S4.SS1.SSS2.p1.1.m1.1.1" xref="S4.SS1.SSS2.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.1.m1.1b"><times id="S4.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.1.m1.1c">\times</annotation></semantics></math>2).
It is approximately 100 epochs for the LVIS base and de-duplicated VG datasets.
The initial learning rate is <math id="S4.SS1.SSS2.p1.2.m2.1" class="ltx_Math" alttext="2\times 10^{-4}" display="inline"><semantics id="S4.SS1.SSS2.p1.2.m2.1a"><mrow id="S4.SS1.SSS2.p1.2.m2.1.1" xref="S4.SS1.SSS2.p1.2.m2.1.1.cmml"><mn id="S4.SS1.SSS2.p1.2.m2.1.1.2" xref="S4.SS1.SSS2.p1.2.m2.1.1.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.SSS2.p1.2.m2.1.1.1" xref="S4.SS1.SSS2.p1.2.m2.1.1.1.cmml">×</mo><msup id="S4.SS1.SSS2.p1.2.m2.1.1.3" xref="S4.SS1.SSS2.p1.2.m2.1.1.3.cmml"><mn id="S4.SS1.SSS2.p1.2.m2.1.1.3.2" xref="S4.SS1.SSS2.p1.2.m2.1.1.3.2.cmml">10</mn><mrow id="S4.SS1.SSS2.p1.2.m2.1.1.3.3" xref="S4.SS1.SSS2.p1.2.m2.1.1.3.3.cmml"><mo id="S4.SS1.SSS2.p1.2.m2.1.1.3.3a" xref="S4.SS1.SSS2.p1.2.m2.1.1.3.3.cmml">−</mo><mn id="S4.SS1.SSS2.p1.2.m2.1.1.3.3.2" xref="S4.SS1.SSS2.p1.2.m2.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.2.m2.1b"><apply id="S4.SS1.SSS2.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1"><times id="S4.SS1.SSS2.p1.2.m2.1.1.1.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1.1"></times><cn type="integer" id="S4.SS1.SSS2.p1.2.m2.1.1.2.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1.2">2</cn><apply id="S4.SS1.SSS2.p1.2.m2.1.1.3.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p1.2.m2.1.1.3.1.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1.3">superscript</csymbol><cn type="integer" id="S4.SS1.SSS2.p1.2.m2.1.1.3.2.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1.3.2">10</cn><apply id="S4.SS1.SSS2.p1.2.m2.1.1.3.3.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1.3.3"><minus id="S4.SS1.SSS2.p1.2.m2.1.1.3.3.1.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1.3.3"></minus><cn type="integer" id="S4.SS1.SSS2.p1.2.m2.1.1.3.3.2.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.2.m2.1c">2\times 10^{-4}</annotation></semantics></math> and decreases to <math id="S4.SS1.SSS2.p1.3.m3.1" class="ltx_Math" alttext="2\times 10^{-5}" display="inline"><semantics id="S4.SS1.SSS2.p1.3.m3.1a"><mrow id="S4.SS1.SSS2.p1.3.m3.1.1" xref="S4.SS1.SSS2.p1.3.m3.1.1.cmml"><mn id="S4.SS1.SSS2.p1.3.m3.1.1.2" xref="S4.SS1.SSS2.p1.3.m3.1.1.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.SSS2.p1.3.m3.1.1.1" xref="S4.SS1.SSS2.p1.3.m3.1.1.1.cmml">×</mo><msup id="S4.SS1.SSS2.p1.3.m3.1.1.3" xref="S4.SS1.SSS2.p1.3.m3.1.1.3.cmml"><mn id="S4.SS1.SSS2.p1.3.m3.1.1.3.2" xref="S4.SS1.SSS2.p1.3.m3.1.1.3.2.cmml">10</mn><mrow id="S4.SS1.SSS2.p1.3.m3.1.1.3.3" xref="S4.SS1.SSS2.p1.3.m3.1.1.3.3.cmml"><mo id="S4.SS1.SSS2.p1.3.m3.1.1.3.3a" xref="S4.SS1.SSS2.p1.3.m3.1.1.3.3.cmml">−</mo><mn id="S4.SS1.SSS2.p1.3.m3.1.1.3.3.2" xref="S4.SS1.SSS2.p1.3.m3.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.3.m3.1b"><apply id="S4.SS1.SSS2.p1.3.m3.1.1.cmml" xref="S4.SS1.SSS2.p1.3.m3.1.1"><times id="S4.SS1.SSS2.p1.3.m3.1.1.1.cmml" xref="S4.SS1.SSS2.p1.3.m3.1.1.1"></times><cn type="integer" id="S4.SS1.SSS2.p1.3.m3.1.1.2.cmml" xref="S4.SS1.SSS2.p1.3.m3.1.1.2">2</cn><apply id="S4.SS1.SSS2.p1.3.m3.1.1.3.cmml" xref="S4.SS1.SSS2.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p1.3.m3.1.1.3.1.cmml" xref="S4.SS1.SSS2.p1.3.m3.1.1.3">superscript</csymbol><cn type="integer" id="S4.SS1.SSS2.p1.3.m3.1.1.3.2.cmml" xref="S4.SS1.SSS2.p1.3.m3.1.1.3.2">10</cn><apply id="S4.SS1.SSS2.p1.3.m3.1.1.3.3.cmml" xref="S4.SS1.SSS2.p1.3.m3.1.1.3.3"><minus id="S4.SS1.SSS2.p1.3.m3.1.1.3.3.1.cmml" xref="S4.SS1.SSS2.p1.3.m3.1.1.3.3"></minus><cn type="integer" id="S4.SS1.SSS2.p1.3.m3.1.1.3.3.2.cmml" xref="S4.SS1.SSS2.p1.3.m3.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.3.m3.1c">2\times 10^{-5}</annotation></semantics></math> after 65K steps.
Additionally, we keep the logit scale and logit shift fixed at 25 and -0.3, respectively, as our preliminary experiments have shown that they not only reduce training time but also does not hurt final performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.
For the baseline models, we use RFS sampling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> with the threshold 0.001, while no sampling methods are used for PCL models.</p>
</div>
<div id="S4.SS1.SSS2.p2" class="ltx_para">
<p id="S4.SS1.SSS2.p2.1" class="ltx_p">Baseline models uses 80 labels per object, which are generated from the prompt templates introduced in the previous work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.
PCL models use 20 labels per object, consisting of 80% (16) pseudo caption labels and 20% (4) manual prompt labels.
Manual prompt labels are created by sampling from the seven best prompt templates <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> to better match their label patterns with those used for testing.
During training, PCL models randomly sample one of 20 labels as the target label, following a commonly used method in previous OVD works.
However, for baseline models, we use the mean embedding of the 80 labels, as it demonstrated slightly higher performance in our preliminary experiments.</p>
</div>
<div id="S4.SS1.SSS2.p3" class="ltx_para">
<p id="S4.SS1.SSS2.p3.1" class="ltx_p">Choosing an appropriate negative class sampling method is crucial when training models on a large vocabulary dataset.
For the baseline model trained on the LVIS base, we samples 50 negative classes based on the square-root class frequency in the training data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>.
For all other models, we use target embeddings collected from all GPUs and a memory bank, which keeps a fixed amount of unique target embeddings during training.
We set the memory bank size to 200 and update 10 oldest entries with new ones every training step.
This approach is not dependent on the number of target classes in the training data, making it much easier to use in conjunction with PCL.
The choice of a negative class sampling method is determined by the model’s performance in preliminary experiments.</p>
</div>
<div id="S4.SS1.SSS2.p4" class="ltx_para">
<p id="S4.SS1.SSS2.p4.1" class="ltx_p">Our models are trained on two datasets: the LVIS base and the de-duplicated VG datasets.
LVIS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> is a large vocabulary dataset, consisting of 1,203 object categories with three class types, frequent, common and rare.
The LVIS base is the official training split for OVD evaluation comprised of only frequent and common type objects.
VG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> is a manually annotated dataset with the largest vocabulary size, including over 80,000 categories.
The de-duplicated VG is its subset built by removing the LVIS validation images and the rare category annotations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.
We selected VG because PCL can take advantage of a larger vocabulary dataset to provide the model with more diverse object-dependent contextual information, leading to significant enhancements in OVD performance.
To encode target labels, we use the CLIP text encoder of the ViT-L/14 model.</p>
</div>
<div id="S4.SS1.SSS2.p5" class="ltx_para">
<p id="S4.SS1.SSS2.p5.1" class="ltx_p">Our evaluation metric is based on box-AP and box-APr, as our detector does not generate segmentation masks.
For evaluation, we use the seven best prompt templates <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> with all category names of the LVIS dataset.
We ensemble the class prediction results over the prompts like previous work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.2.1.1" class="ltx_tr">
<th id="S4.T1.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t">Training Data</th>
<th id="S4.T1.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Method</th>
<th id="S4.T1.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">APr</th>
<th id="S4.T1.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AP</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.2.2.1" class="ltx_tr">
<th id="S4.T1.2.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" rowspan="2"><span id="S4.T1.2.2.1.1.1" class="ltx_text">LVIS base</span></th>
<td id="S4.T1.2.2.1.2" class="ltx_td ltx_align_center ltx_border_tt">Baseline</td>
<td id="S4.T1.2.2.1.3" class="ltx_td ltx_align_center ltx_border_tt">18.7</td>
<td id="S4.T1.2.2.1.4" class="ltx_td ltx_align_center ltx_border_tt">34.5</td>
</tr>
<tr id="S4.T1.2.3.2" class="ltx_tr">
<td id="S4.T1.2.3.2.1" class="ltx_td ltx_align_center">PCL (ours)</td>
<td id="S4.T1.2.3.2.2" class="ltx_td ltx_align_center">20.1 (+1.4)</td>
<td id="S4.T1.2.3.2.3" class="ltx_td ltx_align_center">33.5</td>
</tr>
<tr id="S4.T1.2.4.3" class="ltx_tr">
<th id="S4.T1.2.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" rowspan="2"><span id="S4.T1.2.4.3.1.1" class="ltx_text">VG dedup</span></th>
<td id="S4.T1.2.4.3.2" class="ltx_td ltx_align_center ltx_border_t">Baseline</td>
<td id="S4.T1.2.4.3.3" class="ltx_td ltx_align_center ltx_border_t">21.1</td>
<td id="S4.T1.2.4.3.4" class="ltx_td ltx_align_center ltx_border_t">24.3</td>
</tr>
<tr id="S4.T1.2.5.4" class="ltx_tr">
<td id="S4.T1.2.5.4.1" class="ltx_td ltx_align_center">PCL (ours)</td>
<td id="S4.T1.2.5.4.2" class="ltx_td ltx_align_center">25.7 (+4.6)</td>
<td id="S4.T1.2.5.4.3" class="ltx_td ltx_align_center">29.5</td>
</tr>
<tr id="S4.T1.2.6.5" class="ltx_tr">
<th id="S4.T1.2.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" rowspan="2">
<span id="S4.T1.2.6.5.1.1" class="ltx_ERROR undefined">\hdashline</span><span id="S4.T1.2.6.5.1.2" class="ltx_text">VG all</span>
</th>
<td id="S4.T1.2.6.5.2" class="ltx_td ltx_align_center">Baseline†</td>
<td id="S4.T1.2.6.5.3" class="ltx_td ltx_align_center">25.3</td>
<td id="S4.T1.2.6.5.4" class="ltx_td ltx_align_center">26.6</td>
</tr>
<tr id="S4.T1.2.7.6" class="ltx_tr">
<td id="S4.T1.2.7.6.1" class="ltx_td ltx_align_center ltx_border_b">PCL (ours)†</td>
<td id="S4.T1.2.7.6.2" class="ltx_td ltx_align_center ltx_border_b">28.5</td>
<td id="S4.T1.2.7.6.3" class="ltx_td ltx_align_center ltx_border_b">30.8</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.3.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.4.2" class="ltx_text" style="font-size:90%;">Evaluation results on the LVIS benchmark. PCL consistently improves the novel object detection performance (APr). Notably, the improvement is more significant when the method is applied to the de-duplicated VG dataset, which contains objects over 80K categories. Furthermore, we provide an upper-bound performance, denoted with the †symbol, without de-duplication.</span></figcaption>
</figure>
<figure id="S4.T2" class="ltx_table">
<div id="S4.T2.7.7" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:525.5pt;height:274.5pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-13.8pt,7.2pt) scale(0.95,0.95) ;">
<table id="S4.T2.7.7.7" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.7.7.7.8.1" class="ltx_tr">
<td id="S4.T2.7.7.7.8.1.1" class="ltx_td ltx_align_center ltx_border_t">Method</td>
<td id="S4.T2.7.7.7.8.1.2" class="ltx_td ltx_align_center ltx_border_t">Backbone</td>
<td id="S4.T2.7.7.7.8.1.3" class="ltx_td ltx_align_center ltx_border_t">Params.</td>
<td id="S4.T2.7.7.7.8.1.4" class="ltx_td ltx_align_center ltx_border_t">Detector</td>
<td id="S4.T2.7.7.7.8.1.5" class="ltx_td ltx_align_center ltx_border_t">Detection Data</td>
<td id="S4.T2.7.7.7.8.1.6" class="ltx_td ltx_align_center ltx_border_t">Image Data</td>
<td id="S4.T2.7.7.7.8.1.7" class="ltx_td ltx_align_center ltx_border_t">KD Source</td>
<td id="S4.T2.7.7.7.8.1.8" class="ltx_td ltx_align_center ltx_border_t">APr</td>
<td id="S4.T2.7.7.7.8.1.9" class="ltx_td ltx_align_center ltx_border_t">AP</td>
</tr>
<tr id="S4.T2.7.7.7.9.2" class="ltx_tr">
<td id="S4.T2.7.7.7.9.2.1" class="ltx_td ltx_align_center ltx_border_tt">GLIP</td>
<td id="S4.T2.7.7.7.9.2.2" class="ltx_td ltx_align_center ltx_border_tt">Swin-T</td>
<td id="S4.T2.7.7.7.9.2.3" class="ltx_td ltx_align_center ltx_border_tt">28M</td>
<td id="S4.T2.7.7.7.9.2.4" class="ltx_td ltx_align_center ltx_border_tt">GLIP</td>
<td id="S4.T2.7.7.7.9.2.5" class="ltx_td ltx_align_center ltx_border_tt">O365, GoldG</td>
<td id="S4.T2.7.7.7.9.2.6" class="ltx_td ltx_align_center ltx_border_tt">Cap4M</td>
<td id="S4.T2.7.7.7.9.2.7" class="ltx_td ltx_align_center ltx_border_tt">-</td>
<td id="S4.T2.7.7.7.9.2.8" class="ltx_td ltx_align_center ltx_border_tt">10.1</td>
<td id="S4.T2.7.7.7.9.2.9" class="ltx_td ltx_align_center ltx_border_tt">17.2</td>
</tr>
<tr id="S4.T2.1.1.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1.1.2" class="ltx_td ltx_align_center">ViLD-text</td>
<td id="S4.T2.1.1.1.1.3" class="ltx_td ltx_align_center">RN50</td>
<td id="S4.T2.1.1.1.1.4" class="ltx_td ltx_align_center">26M</td>
<td id="S4.T2.1.1.1.1.5" class="ltx_td ltx_align_center">Mask-RCNN</td>
<td id="S4.T2.1.1.1.1.6" class="ltx_td ltx_align_center">LVIS base</td>
<td id="S4.T2.1.1.1.1.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.1.1.1.1.1" class="ltx_td ltx_align_center">CLIP<sub id="S4.T2.1.1.1.1.1.1" class="ltx_sub"><span id="S4.T2.1.1.1.1.1.1.1" class="ltx_text ltx_font_italic">text</span></sub> ViT-B/32</td>
<td id="S4.T2.1.1.1.1.8" class="ltx_td ltx_align_center">10.6</td>
<td id="S4.T2.1.1.1.1.9" class="ltx_td ltx_align_center">27.9</td>
</tr>
<tr id="S4.T2.7.7.7.10.3" class="ltx_tr">
<td id="S4.T2.7.7.7.10.3.1" class="ltx_td ltx_align_center">ViLD-ens.</td>
<td id="S4.T2.7.7.7.10.3.2" class="ltx_td ltx_align_center">RN50</td>
<td id="S4.T2.7.7.7.10.3.3" class="ltx_td ltx_align_center">26M</td>
<td id="S4.T2.7.7.7.10.3.4" class="ltx_td ltx_align_center">Mask-RCNN</td>
<td id="S4.T2.7.7.7.10.3.5" class="ltx_td ltx_align_center">LVIS base</td>
<td id="S4.T2.7.7.7.10.3.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.7.7.7.10.3.7" class="ltx_td ltx_align_center">CLIP ViT-B/32</td>
<td id="S4.T2.7.7.7.10.3.8" class="ltx_td ltx_align_center">16.7</td>
<td id="S4.T2.7.7.7.10.3.9" class="ltx_td ltx_align_center">27.8</td>
</tr>
<tr id="S4.T2.7.7.7.11.4" class="ltx_tr">
<td id="S4.T2.7.7.7.11.4.1" class="ltx_td ltx_align_center">RegionCLIP</td>
<td id="S4.T2.7.7.7.11.4.2" class="ltx_td ltx_align_center">RN50</td>
<td id="S4.T2.7.7.7.11.4.3" class="ltx_td ltx_align_center">26M</td>
<td id="S4.T2.7.7.7.11.4.4" class="ltx_td ltx_align_center">Faster-RCNN</td>
<td id="S4.T2.7.7.7.11.4.5" class="ltx_td ltx_align_center">LVIS base</td>
<td id="S4.T2.7.7.7.11.4.6" class="ltx_td ltx_align_center">CC3M</td>
<td id="S4.T2.7.7.7.11.4.7" class="ltx_td ltx_align_center">CLIP ViT-B/32</td>
<td id="S4.T2.7.7.7.11.4.8" class="ltx_td ltx_align_center">17.1</td>
<td id="S4.T2.7.7.7.11.4.9" class="ltx_td ltx_align_center">28.2</td>
</tr>
<tr id="S4.T2.2.2.2.2" class="ltx_tr">
<td id="S4.T2.2.2.2.2.2" class="ltx_td ltx_align_center">VLDet</td>
<td id="S4.T2.2.2.2.2.3" class="ltx_td ltx_align_center">RN50</td>
<td id="S4.T2.2.2.2.2.4" class="ltx_td ltx_align_center">26M</td>
<td id="S4.T2.2.2.2.2.5" class="ltx_td ltx_align_center">CenterNetV2</td>
<td id="S4.T2.2.2.2.2.6" class="ltx_td ltx_align_center">LVIS base</td>
<td id="S4.T2.2.2.2.2.7" class="ltx_td ltx_align_center">CC3M</td>
<td id="S4.T2.2.2.2.2.1" class="ltx_td ltx_align_center">CLIP<sub id="S4.T2.2.2.2.2.1.1" class="ltx_sub"><span id="S4.T2.2.2.2.2.1.1.1" class="ltx_text ltx_font_italic">text</span></sub> RN50</td>
<td id="S4.T2.2.2.2.2.8" class="ltx_td ltx_align_center">22.9</td>
<td id="S4.T2.2.2.2.2.9" class="ltx_td ltx_align_center">33.4</td>
</tr>
<tr id="S4.T2.3.3.3.3" class="ltx_tr">
<td id="S4.T2.3.3.3.3.2" class="ltx_td ltx_align_center">PCL (ours)</td>
<td id="S4.T2.3.3.3.3.3" class="ltx_td ltx_align_center">Swin-T</td>
<td id="S4.T2.3.3.3.3.4" class="ltx_td ltx_align_center">28M</td>
<td id="S4.T2.3.3.3.3.5" class="ltx_td ltx_align_center">Def.DETR</td>
<td id="S4.T2.3.3.3.3.6" class="ltx_td ltx_align_center">VG dedup</td>
<td id="S4.T2.3.3.3.3.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.3.3.3.3.1" class="ltx_td ltx_align_center">CLIP<sub id="S4.T2.3.3.3.3.1.1" class="ltx_sub"><span id="S4.T2.3.3.3.3.1.1.1" class="ltx_text ltx_font_italic">text</span></sub> ViT-L/14</td>
<td id="S4.T2.3.3.3.3.8" class="ltx_td ltx_align_center"><span id="S4.T2.3.3.3.3.8.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">25.7</span></td>
<td id="S4.T2.3.3.3.3.9" class="ltx_td ltx_align_center">29.5</td>
</tr>
<tr id="S4.T2.7.7.7.12.5" class="ltx_tr">
<td id="S4.T2.7.7.7.12.5.1" class="ltx_td ltx_align_center ltx_border_t">OWL-ViT</td>
<td id="S4.T2.7.7.7.12.5.2" class="ltx_td ltx_align_center ltx_border_t">ViT-B/16</td>
<td id="S4.T2.7.7.7.12.5.3" class="ltx_td ltx_align_center ltx_border_t">86M</td>
<td id="S4.T2.7.7.7.12.5.4" class="ltx_td ltx_align_center ltx_border_t">MLP</td>
<td id="S4.T2.7.7.7.12.5.5" class="ltx_td ltx_align_center ltx_border_t">O365, VG dedup</td>
<td id="S4.T2.7.7.7.12.5.6" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T2.7.7.7.12.5.7" class="ltx_td ltx_align_center ltx_border_t">CLIP ViT-B/16</td>
<td id="S4.T2.7.7.7.12.5.8" class="ltx_td ltx_align_center ltx_border_t">20.6</td>
<td id="S4.T2.7.7.7.12.5.9" class="ltx_td ltx_align_center ltx_border_t">27.2</td>
</tr>
<tr id="S4.T2.7.7.7.13.6" class="ltx_tr">
<td id="S4.T2.7.7.7.13.6.1" class="ltx_td ltx_align_center">ViLD-ens.</td>
<td id="S4.T2.7.7.7.13.6.2" class="ltx_td ltx_align_center">EN-B7</td>
<td id="S4.T2.7.7.7.13.6.3" class="ltx_td ltx_align_center">67M</td>
<td id="S4.T2.7.7.7.13.6.4" class="ltx_td ltx_align_center">Mask-RCNN</td>
<td id="S4.T2.7.7.7.13.6.5" class="ltx_td ltx_align_center">LVIS base</td>
<td id="S4.T2.7.7.7.13.6.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.7.7.7.13.6.7" class="ltx_td ltx_align_center">CLIP ViT-L/14</td>
<td id="S4.T2.7.7.7.13.6.8" class="ltx_td ltx_align_center">22.0</td>
<td id="S4.T2.7.7.7.13.6.9" class="ltx_td ltx_align_center">32.4</td>
</tr>
<tr id="S4.T2.7.7.7.14.7" class="ltx_tr">
<td id="S4.T2.7.7.7.14.7.1" class="ltx_td ltx_align_center">ViLD-ens.</td>
<td id="S4.T2.7.7.7.14.7.2" class="ltx_td ltx_align_center">EN-B7</td>
<td id="S4.T2.7.7.7.14.7.3" class="ltx_td ltx_align_center">67M</td>
<td id="S4.T2.7.7.7.14.7.4" class="ltx_td ltx_align_center">Mask-RCNN</td>
<td id="S4.T2.7.7.7.14.7.5" class="ltx_td ltx_align_center">LVIS base</td>
<td id="S4.T2.7.7.7.14.7.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.7.7.7.14.7.7" class="ltx_td ltx_align_center">ALIGN</td>
<td id="S4.T2.7.7.7.14.7.8" class="ltx_td ltx_align_center">27.0</td>
<td id="S4.T2.7.7.7.14.7.9" class="ltx_td ltx_align_center">31.8</td>
</tr>
<tr id="S4.T2.4.4.4.4" class="ltx_tr">
<td id="S4.T2.4.4.4.4.2" class="ltx_td ltx_align_center">VLDet</td>
<td id="S4.T2.4.4.4.4.3" class="ltx_td ltx_align_center">Swin-B</td>
<td id="S4.T2.4.4.4.4.4" class="ltx_td ltx_align_center">88M</td>
<td id="S4.T2.4.4.4.4.5" class="ltx_td ltx_align_center">CenterNetV2</td>
<td id="S4.T2.4.4.4.4.6" class="ltx_td ltx_align_center">LVIS base</td>
<td id="S4.T2.4.4.4.4.7" class="ltx_td ltx_align_center">CC3M</td>
<td id="S4.T2.4.4.4.4.1" class="ltx_td ltx_align_center">CLIP<sub id="S4.T2.4.4.4.4.1.1" class="ltx_sub"><span id="S4.T2.4.4.4.4.1.1.1" class="ltx_text ltx_font_italic">text</span></sub> RN50</td>
<td id="S4.T2.4.4.4.4.8" class="ltx_td ltx_align_center"><span id="S4.T2.4.4.4.4.8.1" class="ltx_text ltx_font_bold">29.6</span></td>
<td id="S4.T2.4.4.4.4.9" class="ltx_td ltx_align_center">42.7</td>
</tr>
<tr id="S4.T2.5.5.5.5" class="ltx_tr">
<td id="S4.T2.5.5.5.5.2" class="ltx_td ltx_align_center">PCL (ours)</td>
<td id="S4.T2.5.5.5.5.3" class="ltx_td ltx_align_center">Swin-B</td>
<td id="S4.T2.5.5.5.5.4" class="ltx_td ltx_align_center">88M</td>
<td id="S4.T2.5.5.5.5.5" class="ltx_td ltx_align_center">Def.DETR</td>
<td id="S4.T2.5.5.5.5.6" class="ltx_td ltx_align_center">VG dedup</td>
<td id="S4.T2.5.5.5.5.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.5.5.5.5.1" class="ltx_td ltx_align_center">CLIP<sub id="S4.T2.5.5.5.5.1.1" class="ltx_sub"><span id="S4.T2.5.5.5.5.1.1.1" class="ltx_text ltx_font_italic">text</span></sub> ViT-L/14</td>
<td id="S4.T2.5.5.5.5.8" class="ltx_td ltx_align_center"><span id="S4.T2.5.5.5.5.8.1" class="ltx_text ltx_framed ltx_framed_underline">29.1</span></td>
<td id="S4.T2.5.5.5.5.9" class="ltx_td ltx_align_center">32.9</td>
</tr>
<tr id="S4.T2.7.7.7.15.8" class="ltx_tr">
<td id="S4.T2.7.7.7.15.8.1" class="ltx_td ltx_align_center ltx_border_t">GLIP</td>
<td id="S4.T2.7.7.7.15.8.2" class="ltx_td ltx_align_center ltx_border_t">Swin-L</td>
<td id="S4.T2.7.7.7.15.8.3" class="ltx_td ltx_align_center ltx_border_t">197M</td>
<td id="S4.T2.7.7.7.15.8.4" class="ltx_td ltx_align_center ltx_border_t">GLIP</td>
<td id="S4.T2.7.7.7.15.8.5" class="ltx_td ltx_align_center ltx_border_t">OI, O365, …</td>
<td id="S4.T2.7.7.7.15.8.6" class="ltx_td ltx_align_center ltx_border_t">CC12M, SBU</td>
<td id="S4.T2.7.7.7.15.8.7" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T2.7.7.7.15.8.8" class="ltx_td ltx_align_center ltx_border_t">17.1</td>
<td id="S4.T2.7.7.7.15.8.9" class="ltx_td ltx_align_center ltx_border_t">26.9</td>
</tr>
<tr id="S4.T2.7.7.7.16.9" class="ltx_tr">
<td id="S4.T2.7.7.7.16.9.1" class="ltx_td ltx_align_center">OWL-ViT</td>
<td id="S4.T2.7.7.7.16.9.2" class="ltx_td ltx_align_center">ViT-L/14</td>
<td id="S4.T2.7.7.7.16.9.3" class="ltx_td ltx_align_center">303M</td>
<td id="S4.T2.7.7.7.16.9.4" class="ltx_td ltx_align_center">MLP</td>
<td id="S4.T2.7.7.7.16.9.5" class="ltx_td ltx_align_center">O365, VG dedup</td>
<td id="S4.T2.7.7.7.16.9.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.7.7.7.16.9.7" class="ltx_td ltx_align_center">CLIP ViT-L/14</td>
<td id="S4.T2.7.7.7.16.9.8" class="ltx_td ltx_align_center"><span id="S4.T2.7.7.7.16.9.8.1" class="ltx_text ltx_font_bold">31.2</span></td>
<td id="S4.T2.7.7.7.16.9.9" class="ltx_td ltx_align_center">34.6</td>
</tr>
<tr id="S4.T2.6.6.6.6" class="ltx_tr">
<td id="S4.T2.6.6.6.6.2" class="ltx_td ltx_align_center">PCL (ours)</td>
<td id="S4.T2.6.6.6.6.3" class="ltx_td ltx_align_center">Swin-L</td>
<td id="S4.T2.6.6.6.6.4" class="ltx_td ltx_align_center">197M</td>
<td id="S4.T2.6.6.6.6.5" class="ltx_td ltx_align_center">Def.DETR</td>
<td id="S4.T2.6.6.6.6.6" class="ltx_td ltx_align_center">LVIS base</td>
<td id="S4.T2.6.6.6.6.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.6.6.6.6.1" class="ltx_td ltx_align_center">CLIP<sub id="S4.T2.6.6.6.6.1.1" class="ltx_sub"><span id="S4.T2.6.6.6.6.1.1.1" class="ltx_text ltx_font_italic">text</span></sub> ViT-L/14</td>
<td id="S4.T2.6.6.6.6.8" class="ltx_td ltx_align_center">24.7</td>
<td id="S4.T2.6.6.6.6.9" class="ltx_td ltx_align_center">38.7</td>
</tr>
<tr id="S4.T2.7.7.7.7" class="ltx_tr">
<td id="S4.T2.7.7.7.7.2" class="ltx_td ltx_align_center ltx_border_b">PCL (ours)</td>
<td id="S4.T2.7.7.7.7.3" class="ltx_td ltx_align_center ltx_border_b">Swin-L</td>
<td id="S4.T2.7.7.7.7.4" class="ltx_td ltx_align_center ltx_border_b">197M</td>
<td id="S4.T2.7.7.7.7.5" class="ltx_td ltx_align_center ltx_border_b">Def.DETR</td>
<td id="S4.T2.7.7.7.7.6" class="ltx_td ltx_align_center ltx_border_b">VG dedup</td>
<td id="S4.T2.7.7.7.7.7" class="ltx_td ltx_align_center ltx_border_b">-</td>
<td id="S4.T2.7.7.7.7.1" class="ltx_td ltx_align_center ltx_border_b">CLIP<sub id="S4.T2.7.7.7.7.1.1" class="ltx_sub"><span id="S4.T2.7.7.7.7.1.1.1" class="ltx_text ltx_font_italic">text</span></sub> ViT-L/14</td>
<td id="S4.T2.7.7.7.7.8" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.7.7.7.7.8.1" class="ltx_text ltx_framed ltx_framed_underline">30.6</span></td>
<td id="S4.T2.7.7.7.7.9" class="ltx_td ltx_align_center ltx_border_b">34.5</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.12.2.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.9.1" class="ltx_text" style="font-size:90%;">Comparison with other methods. We categorize the methods into three groups based on backbone size. The first group includes ResNet50 (RN50) and Swin-T, the second group includes EfficientNet-B7 (EN-B7), ViT-B/16, and Swin-B, and the third group includes Swin-L and ViT-L/14. For all detection datasets, we used their de-duplicated versions. GLIP with Swin-L uses OI, O365, VG dedup and ImageNetBoxes as detection data. The KD source column shows the VLMs used by each method, in which the subscript ‘<sub id="S4.T2.9.1.1" class="ltx_sub"><span id="S4.T2.9.1.1.1" class="ltx_text ltx_font_italic">text</span></sub>’ means that the method used only the CLIP text encoder.</span></figcaption>
</figure>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>The Effect of Pseudo Caption Labels</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Table <a href="#S4.T1" title="Table 1 ‣ 4.1.2 Detection Model ‣ 4.1 Implementation Details ‣ 4 Experiments ‣ Open-Vocabulary Object Detection using Pseudo Caption Labels" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> compares the evaluation results of the baseline and PCL models.
The PCL model trained on the LVIS base improves APr from 18.7 to 20.1 compared to the baseline model.
There is a decrease in AP from 34.5 to 33.5, which was anticipated since the model was exposed to manual prompt labels only 20% of the time during training, which have the same patterns as those used during testing.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">On the other hand, the PCL model trained on the de-duplicated VG dataset shows a significant improvement, with an increase in APr from 21.1 to 25.7.
It even outperforms the baseline model trained with all categories (Baseline†).
Although the baseline model was not heavily tuned, it is still an intriguing result.
We speculate that the PCL model was able better generalize to unseen object classes by leveraging more comprehensive knowledge extracted from VLMs using diverse pseudo caption labels generated from object instances over 80,000 categories.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Moreover, it is worth noting that the performance gap between the PCL models with and without de-duplication (25.7 vs. 28.5) suggests that there is still room for improvement in future research.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Comparison with Other Methods</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">In this section, we provide a comprehensive comparison of PCL models with previous work, considering factors such as model sizes, detection datasets, image datasets, and sources of knowledge distillation.
The comparison targets for our model are previous works that have reported box-AP and box-APr, since our model does not produce segmentation results.
Table <a href="#S4.T2" title="Table 2 ‣ 4.1.2 Detection Model ‣ 4.1 Implementation Details ‣ 4 Experiments ‣ Open-Vocabulary Object Detection using Pseudo Caption Labels" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> summarizes the comparison results, grouped by backbone sizes.
The fifth column denotes the detection datasets used to train the models.
O365, GoldG and OI stands for Object365 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, GoldG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, and OpenImages <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> datasets, respectively.
The sixth column shows the image-label or image-text pair datasets used as an additional source of information, including GLIP’s 4 million image-text pair dataset (Cap4M) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, a subset of ImageNet-21K filtered with LVIS categories (ImageNet-L) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, ConceptualCaptions’ 3 million image-text pair dataset (CC3M) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, ConceptualCaptions’ 12 million image-text pair dataset (CC12M) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, and Stony Brook University’s 1 million photograph caption dataset (SBU) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, respectively.
The seventh column shows the image and text encoders used by each method.
GLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> does not use pre-trained encoders.
ViLD-text <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, VLDet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> and PCL methods use CLIP text encoders, while ViLD-ensemble <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, RegionCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, OWL-ViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> use both CLIP image and text encoders.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">Our best model achieved an APr of 30.6, as shown in the bottom row of Table <a href="#S4.T2" title="Table 2 ‣ 4.1.2 Detection Model ‣ 4.1 Implementation Details ‣ 4 Experiments ‣ Open-Vocabulary Object Detection using Pseudo Caption Labels" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, which is comparable to the state-of-the-art method of OWL-ViT (31.2).
This result indicates that by using PCL, our model was able to distill sufficient knowledge to attain the same level of strength as OWL-ViT.
Notably, we observed that the PCL model with a medium size backbone (Swin-B) performed only slightly worse than our best model (29.1 vs. 30.6), whereas OWL-ViT had a noticable performance gap (20.6 vs. 31.2).
This difference can be attributed to the fact that PCL extracts knowledge from the same large CLIP model (CLIP ViT-L/14 text encoder) regardless of the size of its backbone.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">We also compared our best model with the PCL model trained on the LVIS base.
The performance gap (30.6 vs. 24.7) indicates that PCL requires a sufficiently large vocabulary data to have a significant impact.
Thus, a straightforward future research direction is to combine PCL with other OVD methods that expand the vocabulary size of data, such as RegionCLIP and VLDet.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Ablation Studies</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">We conduct experiments to assess the effect of our design choices for PCL.
These include the number of labels assigned per object, the utilization of object class names as the prefix for the captioning model’s output, the application of manual prompt labels, and the use of crop margin.
The experiments are conducted under the same conditions as the main experiment, using the de-duplicated VG dataset and the Swin-T backbone.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.3.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.3.3.4.1" class="ltx_tr">
<th id="S4.T3.3.3.4.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t">Method</th>
<th id="S4.T3.3.3.4.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">APr</th>
<th id="S4.T3.3.3.4.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AP</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.3.3.5.1" class="ltx_tr">
<th id="S4.T3.3.3.5.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">PCL (ours)</th>
<td id="S4.T3.3.3.5.1.2" class="ltx_td ltx_align_center ltx_border_tt">25.7</td>
<td id="S4.T3.3.3.5.1.3" class="ltx_td ltx_align_center ltx_border_tt">29.5</td>
</tr>
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">20 <math id="S4.T3.1.1.1.1.m1.1" class="ltx_Math" alttext="\Rightarrow" display="inline"><semantics id="S4.T3.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T3.1.1.1.1.m1.1.1" xref="S4.T3.1.1.1.1.m1.1.1.cmml">⇒</mo><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1">⇒</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.m1.1c">\Rightarrow</annotation></semantics></math> 10 labels per object</th>
<td id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center">24.3 (-1.4)</td>
<td id="S4.T3.1.1.1.3" class="ltx_td ltx_align_center">29.0 (-0.5)</td>
</tr>
<tr id="S4.T3.2.2.2" class="ltx_tr">
<th id="S4.T3.2.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">20 <math id="S4.T3.2.2.2.1.m1.1" class="ltx_Math" alttext="\Rightarrow" display="inline"><semantics id="S4.T3.2.2.2.1.m1.1a"><mo stretchy="false" id="S4.T3.2.2.2.1.m1.1.1" xref="S4.T3.2.2.2.1.m1.1.1.cmml">⇒</mo><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.1.m1.1b"><ci id="S4.T3.2.2.2.1.m1.1.1.cmml" xref="S4.T3.2.2.2.1.m1.1.1">⇒</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.1.m1.1c">\Rightarrow</annotation></semantics></math> 1 label per object</th>
<td id="S4.T3.2.2.2.2" class="ltx_td ltx_align_center">23.3 (-2.4)</td>
<td id="S4.T3.2.2.2.3" class="ltx_td ltx_align_center">28.1 (-1.4)</td>
</tr>
<tr id="S4.T3.3.3.6.2" class="ltx_tr">
<th id="S4.T3.3.3.6.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">without output prefix</th>
<td id="S4.T3.3.3.6.2.2" class="ltx_td ltx_align_center">23.6 (-2.1)</td>
<td id="S4.T3.3.3.6.2.3" class="ltx_td ltx_align_center">26.3 (-3.2)</td>
</tr>
<tr id="S4.T3.3.3.7.3" class="ltx_tr">
<th id="S4.T3.3.3.7.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">without manual prompts</th>
<td id="S4.T3.3.3.7.3.2" class="ltx_td ltx_align_center">23.7 (-2.0)</td>
<td id="S4.T3.3.3.7.3.3" class="ltx_td ltx_align_center">27.3 (-2.2)</td>
</tr>
<tr id="S4.T3.3.3.3" class="ltx_tr">
<th id="S4.T3.3.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b">20% <math id="S4.T3.3.3.3.1.m1.1" class="ltx_Math" alttext="\Rightarrow" display="inline"><semantics id="S4.T3.3.3.3.1.m1.1a"><mo stretchy="false" id="S4.T3.3.3.3.1.m1.1.1" xref="S4.T3.3.3.3.1.m1.1.1.cmml">⇒</mo><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.1.m1.1b"><ci id="S4.T3.3.3.3.1.m1.1.1.cmml" xref="S4.T3.3.3.3.1.m1.1.1">⇒</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.1.m1.1c">\Rightarrow</annotation></semantics></math> 0% crop margin</th>
<td id="S4.T3.3.3.3.2" class="ltx_td ltx_align_center ltx_border_b">24.1 (-1.6)</td>
<td id="S4.T3.3.3.3.3" class="ltx_td ltx_align_center ltx_border_b">29.1 (-0.4)</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.5.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S4.T3.6.2" class="ltx_text" style="font-size:90%;">Ablation study results. Second and third row shows the effect of the number of labels per object. The following rows describe the effect of using class name output prefix, manual prompts, and the crop margin.</span></figcaption>
</figure>
<section id="S4.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.1 </span>The Impact of the Number of Labels</h4>

<div id="S4.SS4.SSS1.p1" class="ltx_para">
<p id="S4.SS4.SSS1.p1.1" class="ltx_p">Objects can be described from various viewpoints.
In the main experiment, we employed 20 labels per object, out of which 16 were pseudo caption labels, to account for this variability.</p>
</div>
<div id="S4.SS4.SSS1.p2" class="ltx_para">
<p id="S4.SS4.SSS1.p2.1" class="ltx_p">To investigate the effect of the number of labels per object, we compared the performance of PCL models trained with 1, 10 and 20 labels per object.
Specifically, for the PCL models utilizing 10 and 20 labels per object, we used 8 and 16 pseudo caption labels, respectively.
For the model with only 1 label per object, we assigned a pseudo caption label 80% of the time and a manual prompt label 20% of the time.</p>
</div>
<div id="S4.SS4.SSS1.p3" class="ltx_para">
<p id="S4.SS4.SSS1.p3.1" class="ltx_p">As shown in the second row of Table <a href="#S4.T3" title="Table 3 ‣ 4.4 Ablation Studies ‣ 4 Experiments ‣ Open-Vocabulary Object Detection using Pseudo Caption Labels" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the PCL model trained using 10 labels per object experienced a slight drop in APr from 25.7 to 24.3 (-1.4).
When the number of labels per object was reduced to 1, the performance dropped even more significantly from 25.7 to 23.3 (-2.4), as shown in the third row.
These results indicate that rich and diverse descriptions for an object are necessary to extract sufficient knowledge from VLMs.</p>
</div>
</section>
<section id="S4.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.2 </span>The Impact of Using Class Name as Output Prefix</h4>

<figure id="S4.F3" class="ltx_figure"><img src="/html/2303.13040/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="242" height="113" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.3.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S4.F3.4.2" class="ltx_text" style="font-size:90%;">Pseudo caption labels for a photo of a <span id="S4.F3.4.2.1" class="ltx_text ltx_font_bold">bed</span>. The captioning model generated inaccurate captions that referred to incorrect objects, such as pillows or blankets, since the output prefix did not include the object class name.</span></figcaption>
</figure>
<div id="S4.SS4.SSS2.p1" class="ltx_para">
<p id="S4.SS4.SSS2.p1.1" class="ltx_p">To generate pseudo caption labels, we imposed a constraint on the captioning model to output the object class name as the prefix.
This constraint is necessary since the model may generate captions that describe backgrounds or secondary objects located within the bounding box of the main object.
Figure <a href="#S4.F3" title="Figure 3 ‣ 4.4.2 The Impact of Using Class Name as Output Prefix ‣ 4.4 Ablation Studies ‣ 4 Experiments ‣ Open-Vocabulary Object Detection using Pseudo Caption Labels" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates an example where captions describe secondary objects such as pillow, blanket, and curtain instead of the primary object, which is a bed.</p>
</div>
<div id="S4.SS4.SSS2.p2" class="ltx_para">
<p id="S4.SS4.SSS2.p2.1" class="ltx_p">We evaluated the impact of this constraint by training a PCL model with pseudo caption labels generated without the output prefix constraint.
The fourth row in Table <a href="#S4.T3" title="Table 3 ‣ 4.4 Ablation Studies ‣ 4 Experiments ‣ Open-Vocabulary Object Detection using Pseudo Caption Labels" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows that model performance significantly deteriorates in both AP (-3.2) and APr (-2.1) compared to the original PCL model in the first row.
This decline in performance is more pronounced in AP since many pseudo caption labels for known class objects now refer to incorrect object names.</p>
</div>
</section>
<section id="S4.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.3 </span>The Impact of Using Manual Prompt Labels</h4>

<div id="S4.SS4.SSS3.p1" class="ltx_para">
<p id="S4.SS4.SSS3.p1.1" class="ltx_p">To train a PCL model, we utilize a small portion of manual prompt labels in addition to the pseudo caption labels.
It is expected to reduce the discrepancy in target label patterns between the training and testing phases, resulting in improved detection performance for both seen and unseen object classes.</p>
</div>
<div id="S4.SS4.SSS3.p2" class="ltx_para">
<p id="S4.SS4.SSS3.p2.1" class="ltx_p">We can see the impact of using manual prompt labels by comparing two models in the first and fifth rows of Table <a href="#S4.T3" title="Table 3 ‣ 4.4 Ablation Studies ‣ 4 Experiments ‣ Open-Vocabulary Object Detection using Pseudo Caption Labels" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
The PCL model trained without manual prompt labels experienced significant performance drop in both AP (-2.2) and APr (-2.0).
This finding highlights the importance of maintaining label pattern consistency between training and testing for achieving high performance.</p>
</div>
</section>
<section id="S4.SS4.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.4 </span>The Impact of Image Crop Margin</h4>

<div id="S4.SS4.SSS4.p1" class="ltx_para">
<p id="S4.SS4.SSS4.p1.1" class="ltx_p">In addition to describing the characteristics of target objects, good pseudo caption labels also capture information about their surrounding environments.
An example is the label “dog running on the grass” in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Open-Vocabulary Object Detection using Pseudo Caption Labels" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
To allow the captioning model to better incorporate contextual information, we applied a 20% margin to the object’s bounding box area.</p>
</div>
<div id="S4.SS4.SSS4.p2" class="ltx_para">
<p id="S4.SS4.SSS4.p2.1" class="ltx_p">The impact of using the crop margin on the performance of the PCL model is shown in the last row of Table <a href="#S4.T3" title="Table 3 ‣ 4.4 Ablation Studies ‣ 4 Experiments ‣ Open-Vocabulary Object Detection using Pseudo Caption Labels" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
When we crop the object image using the original bounding box, APr experiences a significant drop of -1.6, while AP is less affected.
We believe that the model is able to recognize known category objects based on their characteristics, even when the context is not rich enough.</p>
</div>
</section>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Complex Freeform Text Detection</h3>

<figure id="S4.F4" class="ltx_figure"><img src="/html/2303.13040/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="475" height="350" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.3.2" class="ltx_text" style="font-size:90%;">Comparison of the top-1 detection results of the baseline and PCL models with complex queries. The queries used were “cat wearing a collar with a silver pendant”, “a cup of coffee with a layer of white cream on top”, and “a girl dressed in a blue shirt and black pants”, displayed from the top row. The comparison illustrates that the PCL model is capable of accurately distinguishing the main objects from other information in the queries.</span></figcaption>
</figure>
<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">The results of the LVIS benchmark indicate that PCL significantly enhances novel object detection performance.
This section provides a qualitative analysis demonstrating that PCL models understand the linguistic structure of an input query better than baseline models.</p>
</div>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.1" class="ltx_p">Figure <a href="#S4.F4" title="Figure 4 ‣ 4.5 Complex Freeform Text Detection ‣ 4 Experiments ‣ Open-Vocabulary Object Detection using Pseudo Caption Labels" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> compares the top-1 detection outcomes of the baseline and PCL models (with the Swin-T backbone).
The first row shows the results of the models with the query “cat wearing a collar with a silver pendant”.
The models need to comprehend that the main object to detect is a “cat”, and the “collar” and “pendant” are secondary objects that should be included.
However, the baseline model mistakenly detects only “a silver pendant”, while the PCL model accurately identifies the main object, which is a “cat” wearing “a collar with a silver pendant”.
For the second and third samples, the same results are obtained when using queries “a cup of coffee with a layer of white cream on top” and “a girl dressed in a blue shirt and black pants”.</p>
</div>
<div id="S4.SS5.p3" class="ltx_para">
<p id="S4.SS5.p3.1" class="ltx_p">This result demonstrates the ability of the PCL model to learn the linguistic structure from pseudo caption labels and use that knowledge to comprehend complex queries accurately.
This skill of PCL can potentially be employed for referring expression comprehension.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<figure id="S5.F5" class="ltx_figure"><img src="/html/2303.13040/assets/x5.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="242" height="96" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.3.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S5.F5.4.2" class="ltx_text" style="font-size:90%;">Pseudo caption labels for a photo of a <span id="S5.F5.4.2.1" class="ltx_text ltx_font_bold">cat</span>. Depending on the captioning model’s performance, generated captions may not correctly describe the object as shown in red color.</span></figcaption>
</figure>
<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this study, we presented a simple and effective knowledge distillation method, Pseudo Caption Labeling (PCL), for OVD.
PCL is a simple pre-processing technique that transforms a detection dataset to a region-caption dataset using an image-captioning model, making it possible to use well-established OVD model architectures and training recipes.
Our experiment results also demonstrated its effectiveness by achieving comparable performance to the state-of-the-art OVD methods.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">While our proposed method has shown promising results, we also have identified areas that require further improvement.
Firstly, the captioning model often generates pseudo caption labels with inaccurate contextual information, as shown in Figure <a href="#S5.F5" title="Figure 5 ‣ 5 Conclusion ‣ Open-Vocabulary Object Detection using Pseudo Caption Labels" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
For this specific example, we found that 5 were correct, 7 were incorrect and 4 were ambiguous among 16 pseudo caption labels.
This issue could potentially be addressed by using a more accurate and reliable captioning model.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Secondly, the output prefix constraint may restrict the syntactic diversity of the generated captions.
One potential solution is to sample a large number of captions without this constraint and then select those that contain the target class name as a sub-string.
Alternatively, we could develop a captioning model that explicitly takes object name as an input condition to generate more diverse captions while maintaining accuracy.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">Lastly, a small portion of manual prompt labels were used during training to compensate for the discrepancy of target labels between training and testing phases.
A potential improvement to this approach is to use object-specific prompts at testing time utilizing large language models, as demonstrated in the previous work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Ankan Bansal, Karan Sikka, Gaurav Sharma, Rama Chellappa, and Ajay Divakaran.

</span>
<span class="ltx_bibblock">Zero-shot object detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Proceedings of the European Conference on Computer Vision
(ECCV)</span>, 2018.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Federico Bianchi, Giuseppe Attanasio, Raphael Pisoni, Silvia Terragni, Gabriele
Sarti, and S. Veera Lakshmi.

</span>
<span class="ltx_bibblock">Contrastive language-image pre-training for the italian language.

</span>
<span class="ltx_bibblock">In <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">ArXiv</span>, 2021.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander
Kirillov, and Sergey Zagoruyko.

</span>
<span class="ltx_bibblock">End-to-end object detection with transformers.

</span>
<span class="ltx_bibblock">In <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Proceedings of the European Conference on Computer Vision
(ECCV)</span>, 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut.

</span>
<span class="ltx_bibblock">Conceptual 12m: Pushing web-scale image-text pre-training to
recognize long-tail visual concepts.

</span>
<span class="ltx_bibblock">In <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Proceedings of Computer Vision and Pattern Recognition
(CVPR)</span>, 2021.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.

</span>
<span class="ltx_bibblock">Imagenet: A large-scale hierarchical image database.

</span>
<span class="ltx_bibblock">In <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Proceedings of the Computer Vision and Pattern Recognition
(CVPR)</span>, 2009.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Mingfei Gao, Chen Xing, Juan Carlos Niebles, Junnan Li, Ran Xu, Wenhao Liu, and
Caiming Xiong.

</span>
<span class="ltx_bibblock">Open vocabulary object detection with pseudo bounding-box labels.

</span>
<span class="ltx_bibblock">In <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Proceedings of the European Conference on Computer Vision
(ECCV)</span>, 2021.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui.

</span>
<span class="ltx_bibblock">Open-vocabulary object detection via vision and language knowledge
distillation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Proceedings of International Conference on Learning
Representations (ICLR)</span>, 2021.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Agrim Gupta, Piotr Dollar, and Ross Girshick.

</span>
<span class="ltx_bibblock">Lvis: A dataset for large vocabulary instance segmentation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Proceedings of Computer Vision and Pattern Recognition
(CVPR)</span>, 2019.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V.
Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig.

</span>
<span class="ltx_bibblock">Scaling up visual and vision-language representation learning with
noisy text supervision.

</span>
<span class="ltx_bibblock">In <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Proceedings of International Conference on Machine Learning
(ICML)</span>, 2021.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and
Nicolas Carion.

</span>
<span class="ltx_bibblock">Mdetr-modulated detection for end-to-end multi-modal understanding.

</span>
<span class="ltx_bibblock">In <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Proceedings of International Conference on Computer Vision
(ICCV)</span>, 2021.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Wonjae Kim, Bokyung Son, and Ildoo Kim.

</span>
<span class="ltx_bibblock">Vilt: Vision-and-language transformer without convolution or region
supervision.

</span>
<span class="ltx_bibblock">In <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Proceedings of International Conference on Machine Learning
(ICML)</span>, 2021.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua
Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al.

</span>
<span class="ltx_bibblock">Visual genome: Connecting language and vision using crowdsourced
dense image annotations.

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">International Journal of Computer Vision</span>, 2017.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Weicheng Kuo, Yin Cui, Xiuye Gu, A. J. Piergiovanni, and Anelia Angelova.

</span>
<span class="ltx_bibblock">F-vlm: Open-vocabulary object detection upon frozen vision and
language models.

</span>
<span class="ltx_bibblock">In <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Proceedings of International Conference on Learning
Representations (ICLR)</span>, 2023.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi
Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander
Kolesnikov, et al.

</span>
<span class="ltx_bibblock">The open images dataset v4: Unified image classification, object
detection, and visual relationship detection at scale.

</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">International Journal of Computer Vision</span>, 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.

</span>
<span class="ltx_bibblock">Blip: Bootstrapping language-image pre-training for unified
vision-language understanding and generation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Proceedings of International Conference on Machine Learning
(ICML)</span>, 2022.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li,
Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang,
and Jianfeng Gao.

</span>
<span class="ltx_bibblock">Grounded language-image pre-training.

</span>
<span class="ltx_bibblock">In <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Proceedings of Computer Vision and Pattern Recognition
(CVPR)</span>, 2022.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Chuang Lin, Pei Sun, Yi Jiang, Ping Luo, Lizhen Qu, Gholamreza Haffari, Zehuan
Yuan, and Jianfei Cai.

</span>
<span class="ltx_bibblock">Learning object-language alignments for open-vocabulary object
detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">Proceedings of International Conference on Learning
Representations (ICLR)</span>, 2023.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C Lawrence Zitnick.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context.

</span>
<span class="ltx_bibblock">In <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Proceedings of the European Conference on Computer Vision
(ECCV)</span>, 2014.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter.

</span>
<span class="ltx_bibblock">Decoupled weight decay regularization.

</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1711.05101</span>, 2017.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri,
Yixuan Li, Ashwin Bharambe, and Laurens Van Der Maaten.

</span>
<span class="ltx_bibblock">Exploring the limits of weakly supervised pretraining.

</span>
<span class="ltx_bibblock">In <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Proceedings of the European Conference on Computer Vision
(ECCV)</span>, 2018.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Sachit Menon and Carl Vondrick.

</span>
<span class="ltx_bibblock">Visual classification via description from large language models.

</span>
<span class="ltx_bibblock">In <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Proceedings of International Conference on Learning
Representations (ICLR)</span>, 2023.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Matthias Minderer, Alexey A. Gritsenko, Austin Stone, Maxim Neumann, Dirk
Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa
Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil
Houlsby.

</span>
<span class="ltx_bibblock">Simple open-vocabulary object detection with vision transformers.

</span>
<span class="ltx_bibblock">In <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">Proceedings of the European Conference on Computer Vision
(ECCV)</span>, 2022.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Vicente Ordonez, Girish Kulkarni, and Tamara Berg.

</span>
<span class="ltx_bibblock">Im2text: Describing images using 1 million captioned photographs.

</span>
<span class="ltx_bibblock">In <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">Proceedings of Neural Information Processing Systems
(NeurIPS)</span>, 2011.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
Gretchen Krueger, and Ilya Sutskever.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language
supervision.

</span>
<span class="ltx_bibblock">In <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">Proceedings of International Conference on Machine Learning
(ICML)</span>, 2021.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
<span class="ltx_bibblock">2019.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
Matena, Yanqi Zhou, Wei Li, and Peter J Liu.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text
transformer.

</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">The Journal of Machine Learning Research</span>, 2020.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Hanoona Rasheed, Muhammad Maaz, Muhammad Uzair Khattak, Salman Khan, and
Fahad Shahbaz Khan.

</span>
<span class="ltx_bibblock">Bridging the gap between object and image-level representations for
open-vocabulary detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">Proceedings of Neural Information Processing Systems
(NeurIPS)</span>, 2022.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.

</span>
<span class="ltx_bibblock">Faster r-cnn: Towards real-time object detection with region proposal
networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">Proceedings of the Advances in Neural Information Processing
Systems (NeurIPS)</span>, 2015.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing
Li, and Jian Sun.

</span>
<span class="ltx_bibblock">Objects365: A large-scale, high-quality dataset for object detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">Proceedings of International Conference on Computer Vision
(ICCV)</span>, 2019.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut.

</span>
<span class="ltx_bibblock">Conceptual captions: A cleaned, hypernymed, image alt-text dataset
for automatic image captioning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">Proceedings of the Annual Meeting of the Association for
Computational Linguistics (ACL)</span>, 2018.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan,
Zicheng Liu, Ce Liu, and Lijuan Wang.

</span>
<span class="ltx_bibblock">Git: A generative image-to-text transformer for vision and language.

</span>
<span class="ltx_bibblock">In <span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">ArXiv</span>, 2022.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao.

</span>
<span class="ltx_bibblock">SimVLM: Simple visual language model pretraining with weak
supervision.

</span>
<span class="ltx_bibblock">In <span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">Proceedings of International Conference on Learning
Representations (ICLR)</span>, 2022.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang,
Yejin Choi, and Jianfeng Gao.

</span>
<span class="ltx_bibblock">Vinvl: Revisiting visual representations in vision-language models.

</span>
<span class="ltx_bibblock">In <span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">Proceedings of Computer Vision and Pattern Recognition
(CVPR)</span>, 2021.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel C. F. Codella,
Liunian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, and Jianfeng
Gao.

</span>
<span class="ltx_bibblock">Regionclip: Region-based language-image pretraining.

</span>
<span class="ltx_bibblock">In <span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">Proceedings of Computer Vision and Pattern Recognition
(CVPR)</span>, 2022.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Xingyi Zhou, Rohit Girdhar, Armand Joulin, Phillip Krahenbuhl, and Ishan Misra.

</span>
<span class="ltx_bibblock">Detecting twenty-thousand classes using image-level supervision.

</span>
<span class="ltx_bibblock">In <span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">Proceedings of European Conference on Computer Vision
(ECCV)</span>, 2022.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Xingyi Zhou, Vladlen Koltun, and Philipp Krähenbühl.

</span>
<span class="ltx_bibblock">Probabilistic two-stage detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">ArXiv</span>, 2021.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai.

</span>
<span class="ltx_bibblock">Deformable {detr}: Deformable transformers for end-to-end
object detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">Proceedings of International Conference on Learning
Representations (ICLR)</span>, 2021.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun,
Antonio Torralba, and Sanja Fidler.

</span>
<span class="ltx_bibblock">Aligning books and movies: Towards story-like visual explanations by
watching movies and reading books.

</span>
<span class="ltx_bibblock">In <span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">Proceedings of the International Conference on Computer
Vision (ICCV)</span>, 2015.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2303.13039" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2303.13040" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2303.13040">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2303.13040" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2303.13041" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 18:38:38 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
