<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2306.03537] Real-Time Onboard Object Detection for Augmented Reality: Enhancing Head-Mounted Display with YOLOv8 The study has been supported by funding provided through an unrestricted gift by Meta.</title><meta property="og:description" content="This paper introduces a software architecture for real-time object detection using machine learning (ML) in an augmented reality (AR) environment. Our approach uses the recent state-of-the-art YOLOv8 network that runs …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Real-Time Onboard Object Detection for Augmented Reality: Enhancing Head-Mounted Display with YOLOv8 The study has been supported by funding provided through an unrestricted gift by Meta.">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Real-Time Onboard Object Detection for Augmented Reality: Enhancing Head-Mounted Display with YOLOv8 The study has been supported by funding provided through an unrestricted gift by Meta.">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2306.03537">

<!--Generated on Thu Feb 29 02:13:07 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
augmented reality,  machine learning,  real-time object detection,  edge computing
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Real-Time Onboard Object Detection for Augmented Reality: Enhancing Head-Mounted Display with YOLOv8
<br class="ltx_break"><span id="id1.id1" class="ltx_note ltx_role_thanks"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span>The study has been supported by funding provided through an unrestricted gift by Meta.</span></span></span>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">

Mikołaj Łysakowski1, Kamil Żywanowski1, Adam Banaszczyk1, Michał R. Nowicki12,
<br class="ltx_break">Piotr Skrzypczyński12, Sławomir K. Tadeja3
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">1 Poznań University of Technology, Centre for Artificial Intelligence and Cybersecurity
</span>
<span class="ltx_contact ltx_role_affiliation">2 Poznań University of Technology, Institute of Robotics and Machine Intelligence
</span>
<span class="ltx_contact ltx_role_affiliation">3 University of Cambridge, Department of Engineering, Institute for Manufacturing
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">This paper introduces a software architecture for real-time object detection using machine learning (ML) in an augmented reality (AR) environment. Our approach uses the recent state-of-the-art <span id="id2.id1.1" class="ltx_text ltx_font_italic">YOLOv8</span> network that runs onboard on the <span id="id2.id1.2" class="ltx_text ltx_font_italic">Microsoft HoloLens 2</span> head-mounted display (HMD).
The primary motivation behind this research is to enable the application of advanced ML models for enhanced perception and situational awareness with a wearable, hands-free AR platform.
We show the image processing pipeline for the YOLOv8 model and the techniques used to make it real-time on the resource-limited edge computing platform of the headset. The experimental results demonstrate that our solution achieves real-time processing without needing offloading tasks to the cloud or any other external servers while retaining satisfactory accuracy regarding the usual mAP metric and measured qualitative performance.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
augmented reality, machine learning, real-time object detection, edge computing

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Augmented Reality (AR) technology
belonging to the class of <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">immersive technologies</span> offers an ability to blend digital artifacts and the physical environment by superimposing digital content in the user’s field of view (FoV) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> (Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Real-Time Onboard Object Detection for Augmented Reality: Enhancing Head-Mounted Display with YOLOv8 The study has been supported by funding provided through an unrestricted gift by Meta." class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Presently, popular AR applications running on mobile devices, such as smartphones or tablets,
can be further enhanced with machine learning (ML). Thanks to such an approach, we can include vision-based features for object detection and tracking on video and imagery data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2306.03537/assets/figures/fig0_v2.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="414" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">The proposed onboard object detection with YOLOv8 offers real-time onboard object detection enhancing HoloLens 2 capabilities without a common requirement of WiFi or Internet access to perform server-based computations.</span></figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">However, mobile AR solutions have significant limitations, such as a relatively small FoV confined by screen canvas or needing hand control <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.
The latter narrows down potential scenarios where we can successfully deploy AR, such as manual assembly <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, device repair task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, or the use of AR enhancers by older adults <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.
In such cases, the user’s ability to not only freely move hands but promptly shift the unconstrained FoV or the body posture is crucial for safety concerns and task completion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">These caveats are circumvented by the alternative technology of wearable smart head-mounted display (HMD) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. The AR headsets, such as widely-considered state-of-the-art <span id="S1.p4.1.1" class="ltx_text ltx_font_italic">Microsoft HoloLens 2</span> (HL2) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, offer a hands-free AR experience <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Unfortunately, HL2 and other similar headsets do not offer a satisfactory level of support for ML-based processing that could enhance the user’s ability to interact with the environment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>,
Thus, having onboard, real-time ML models running in the headset’s edge computing platform is crucial for developing new AR application areas.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">We address the problem of real-time object detection on the HL2, including the most recent <span id="S1.p5.1.1" class="ltx_text ltx_font_italic">You Only Look Once</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> YOLOv8 framework.
We focus on defining the steps necessary to achieve a desired frame rate of image processing with the onboard ML model while identifying the constraints of the HL2 computing platform. Overcoming these limitations enables using widely-available ML algorithms on headsets.
We also believe that AR developers can use our work on YOLOv8 for HL2 to create new applications extending the current use cases of this headset.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Our contribution can be summarized as a unique, easy-to-replicate, real-time, onboard object detection pipeline on the HL2 headset.
Following the open-science principle, our code and complete guide on how to run the most recent YOLOv8 network architecture in the resource-limited hardware environment of this HMD is freely available as a GitHub<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/kolaszko/hl2_detection</span></span></span></span> repository.
Furthermore, as a byproduct of our work, we also developed a commented list of limitations of the HL2 as a computing platform for ML. These should be tackled first to broaden the development of modern ML-based applications on this widely used among AR community device <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related work</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The usage of object detection on AR headsets is an item of past and current research explorations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. For example, in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, the authors explore the possibility of using a no longer state-of-the-art object detection approach with a two-stage network to detect and track objects while offloading the processing to the high-end server. We argue that while on the server side, we are not constrained by the computational capabilities of HL2, we cannot use the headset to carry out object-detection tasks without local WiFi or a fast wireless broadband communication (e.g., LTE connection). Off-board processing limits the HL2 headset to only a frame-capturing camera and head-mounted display output than a standalone solution for object detection and tracking.
Presently, the Microsoft Azure Custom Vision library <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> offers a complete high-level solution when considering off-board computations.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">An example application is tomato picking, where farmers rely on inexperienced, part-time workers to harvest the fruits with desired ripeness and blemishes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.
It negatively impacts harvest quality and work efficiency as people need to learn the task.
The presented work shows a complete AR and ML-based solution to this problem that requires powerful servers needing a steady, reliable LTE connection, which might not be available in the field.
Another example concerns a system for supporting visually-impaired people, utilizing object detection in images to provide information about objects in the surroundings using an audio interface to the user <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.
The application directs users to the desired objects in the scene based on audio communication.
This system uses an old YOLOv2 model that is offloaded to the server, thus making it vulnerable to LTE connection stability outdoors, hampering its ability to efficiently determine desired objects’ locations.
Similarly, in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, we are introduced to the concept of using HL2 as a tool to increase drivers’ road condition awareness.
The authors explore the idea of a system that can focus users’ attention on incoming vehicles and support lane detection while extending the view with vehicle speed.
In such a context, on-time, local processing capabilities are crucial to ensure the proper operation of the complete solution.
These examples are only a subset of the existing works concerned with object detection and tracking in headset-based AR that rely on server-side computation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, summarized in a recent review paper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">The alternative to server-side processing includes simpler algorithms that run in real-time on AR headsets.
Such an approach is proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, where a custom-made processing pipeline is used to detect cracks in the constructions using only the onboard computation capabilities of HL2.
Designing custom ML pipelines offers the desired object detection accuracy but requires expert knowledge and is time-consuming,
Hence, another approach is to apply a readily-available software framework, like <span id="S2.p3.1.1" class="ltx_text ltx_font_italic">Vuforia</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> or <span id="S2.p3.1.2" class="ltx_text ltx_font_italic">easyAR</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.
These solutions, however, require a 3D CAD model of the considered objects, which in practice limits detection capabilities to an object instance from a class of rigid objects.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">As a middle ground, it is possible to combine server-side processing with on-device edge processing. Still, the resulting solution has to properly synchronize the processing on both ends, raising even more issues <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">Consequently, we propose a pipeline that allows anybody to achieve real-time object detection and tracking capabilities using the state-of-the-art YOLOv8 network onboard HL2 without needing to be connected to any server.
Furthermore, to achieve our real-time performance, we put a hard limit of 100 ms on end-to-end processing from an image capturing to data visualization as greater latency reduces an immersive experience to users <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.
To that end, our approach offers new advantages to the ones already provided by a standalone AR headset such as HL2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Hardware and Software Frameworks</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The general, high-level processing idea is presented in Fig. <a href="#S3.F2" title="Figure 2 ‣ III Hardware and Software Frameworks ‣ Real-Time Onboard Object Detection for Augmented Reality: Enhancing Head-Mounted Display with YOLOv8 The study has been supported by funding provided through an unrestricted gift by Meta." class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. We start by preparing the YOLOv8 neural network models for HL2. These models can be optionally retrained (fine-tuned) to include different detection classes.
The next step involves exporting the model to the <span id="S3.p1.1.1" class="ltx_text ltx_font_italic">Open Neural Network Exchange</span> (ONNX) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> format. The model is then used by the <span id="S3.p1.1.2" class="ltx_text ltx_font_italic">Barracuda</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> library in the <span id="S3.p1.1.3" class="ltx_text ltx_font_italic">Unity</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> engine to perform object detection on HL2 and to provide visualization of the detected objects. We decided to use the Unity platform as it is among the most widely used software framework in AR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and VR (virtual reality) research <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. We will introduce the used frameworks in more detail in the following sections.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2306.03537/assets/figures/libs_2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="246" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.3.2" class="ltx_text" style="font-size:90%;">The relation between the used hardware and software frameworks. Notice the clear distinction between the offline phase performed outside the AR device (red) and the online operation on HL2 (yellow).</span></figcaption>
</figure>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2306.03537/assets/figures/fig1_v2.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="173" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.3.2" class="ltx_text" style="font-size:90%;">The image processing steps performed on the HL2 to achieve the object detection within the user’s FoV.</span></figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Augmented Reality Equipment: Microsoft HoloLens 2</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">To showcase the possibility of optimizing an ML model performance on an HMD treated as an edge computing platform, we decided to use the <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_italic">HoloLens 2</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.
As a computing platform, this headset is equipped with Snapdragon 850, a high-performance 64-bit ARM LTE system on a chip designed by Qualcomm, and an Adreno 630 graphics processing unit (GPU).
The headset also contains a second-generation custom-built holographic processing unit (HPU) for computations related to sensor information, core algorithm accelerators, and compute nodes enabling onboard image processing without using Snapdragon’s resources.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">HL2 perception system consists of four grayscale cameras used in simultaneous localization and mapping (SLAM), two infrared (IR) cameras for the built-in eye tracking, a time-of-flight depth sensor used in hand tracking and spatial mapping, an inertial measurement unit (IMU) sensors, and frontal RGB camera.
Our work uses a world-facing RGB camera mounted above the user’s eyes in the center of the front headset panel.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">All these features and their widespread application in AR-related research <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, made the HL2 a best-suited candidate for deploying and testing our real-time, onboard object detection with YOLOv8 network architecture.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Object detection: You Only Look Once (YOLO) network</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Object detection is an active research topic with multiple scientific and real-world applications, as previously summarized <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. The YOLO family <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> is commonly used when we need robust object detection capabilities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. YOLO model is based on single-stage object detection with different backbone sizes that can be chosen based on the available processing power <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. Real-time object detection on constrained devices can be powered by YOLO Tiny or YOLO Nano, the smallest models in the YOLO family, which however, limits the performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Despite offering unrivaled results, this approach is still actively explored <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, with research focusing on achieving the best possible score (i.e., mean average precision, mAP) on the available datasets while doing it with the least amount of network parameters <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>.
Currently, from the YOLO family <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, the YOLOv8 network architecture gives the best results. Hence, we will focus on this version in our demo application.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Barracuda library for ML inference</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The neural network part of the detection pipeline is based on the Barracuda <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> library. It is an open-source library developed by Unity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> for utilizing neural networks in the game engine. It supports the most common deep learning layers and provides GPU and CPU inference engines. Cross-framework support for different machine learning libraries is ensured by using an ONNX format to load pretrained neural networks. It enables interoperability between different ML frameworks, providing a standard set of operations used in deep learning.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.4.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.5.2" class="ltx_text ltx_font_italic">Model preparation</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">Each model used in the online operation can be prepared using the same pipeline. We export each model from <span id="S3.SS4.p1.1.1" class="ltx_text ltx_font_italic">PyTorch</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> serialized <code id="S3.SS4.p1.1.2" class="ltx_verbatim ltx_font_typewriter">.pt</code> file to the ONNX format.
Since the current Barracuda version supports ONNX deep learning operations (opset) up to version 9, exporting models with the proper opset flag is crucial. Apart from the export, it is also possible to reduce the model with the ONNX simplification tool. The operation merges redundant operators using constant folding, consequently speeding up inference.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">We successfully tested exporting and deploying the publicly available original YOLOv8 object detection models. Moreover, we can train the YOLOv8 for any custom class with sufficient data while following the guidelines for model fine-tuning to custom datasets.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Object detection pipeline on HL2</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We present the universal pipeline for onboard, real-time YOLO-based object detection for HL2. The processing pipeline used in our evaluation is presented in Fig. <a href="#S3.F3" title="Figure 3 ‣ III Hardware and Software Frameworks ‣ Real-Time Onboard Object Detection for Augmented Reality: Enhancing Head-Mounted Display with YOLOv8 The study has been supported by funding provided through an unrestricted gift by Meta." class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">The processing starts by an image acquisition done with <code id="S4.p2.1.1" class="ltx_verbatim ltx_font_typewriter">HoloLensCameraStream</code> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> package. This plugin enables users to collect RGB camera images in all HL2-supported resolutions and frame rates, along with the current camera position in the world coordinate system. The package provides the functionality essential to calculate a projection from pixel coordinates into 3D world space using extrinsic and projection matrices. The image, current camera-to-world matrix, and projection matrix are constantly updated in a separate thread whenever new data is available.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.4" class="ltx_p">Next, we perform the initial image preprocessing step, which consists of cropping an <math id="S4.p3.1.m1.1" class="ltx_Math" alttext="n\times n" display="inline"><semantics id="S4.p3.1.m1.1a"><mrow id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml"><mi id="S4.p3.1.m1.1.1.2" xref="S4.p3.1.m1.1.1.2.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="S4.p3.1.m1.1.1.1" xref="S4.p3.1.m1.1.1.1.cmml">×</mo><mi id="S4.p3.1.m1.1.1.3" xref="S4.p3.1.m1.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><apply id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1"><times id="S4.p3.1.m1.1.1.1.cmml" xref="S4.p3.1.m1.1.1.1"></times><ci id="S4.p3.1.m1.1.1.2.cmml" xref="S4.p3.1.m1.1.1.2">𝑛</ci><ci id="S4.p3.1.m1.1.1.3.cmml" xref="S4.p3.1.m1.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">n\times n</annotation></semantics></math> image out of the center of the acquired camera image, where <math id="S4.p3.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.p3.2.m2.1a"><mi id="S4.p3.2.m2.1.1" xref="S4.p3.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.p3.2.m2.1b"><ci id="S4.p3.2.m2.1.1.cmml" xref="S4.p3.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.2.m2.1c">n</annotation></semantics></math> is the size of the neural network’s input, and all pixel values are normalized to <math id="S4.p3.3.m3.2" class="ltx_Math" alttext="[0;1]" display="inline"><semantics id="S4.p3.3.m3.2a"><mrow id="S4.p3.3.m3.2.3.2" xref="S4.p3.3.m3.2.3.1.cmml"><mo stretchy="false" id="S4.p3.3.m3.2.3.2.1" xref="S4.p3.3.m3.2.3.1.cmml">[</mo><mn id="S4.p3.3.m3.1.1" xref="S4.p3.3.m3.1.1.cmml">0</mn><mo id="S4.p3.3.m3.2.3.2.2" xref="S4.p3.3.m3.2.3.1.cmml">;</mo><mn id="S4.p3.3.m3.2.2" xref="S4.p3.3.m3.2.2.cmml">1</mn><mo stretchy="false" id="S4.p3.3.m3.2.3.2.3" xref="S4.p3.3.m3.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.3.m3.2b"><list id="S4.p3.3.m3.2.3.1.cmml" xref="S4.p3.3.m3.2.3.2"><cn type="integer" id="S4.p3.3.m3.1.1.cmml" xref="S4.p3.3.m3.1.1">0</cn><cn type="integer" id="S4.p3.3.m3.2.2.cmml" xref="S4.p3.3.m3.2.2">1</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.3.m3.2c">[0;1]</annotation></semantics></math> range. Since YOLOv8 accepts a square input, we have omitted using a rectangular image, simplifying the pre and postprocessing. Finally, the input tensor of size <math id="S4.p3.4.m4.4" class="ltx_Math" alttext="(1,n,n,3)" display="inline"><semantics id="S4.p3.4.m4.4a"><mrow id="S4.p3.4.m4.4.5.2" xref="S4.p3.4.m4.4.5.1.cmml"><mo stretchy="false" id="S4.p3.4.m4.4.5.2.1" xref="S4.p3.4.m4.4.5.1.cmml">(</mo><mn id="S4.p3.4.m4.1.1" xref="S4.p3.4.m4.1.1.cmml">1</mn><mo id="S4.p3.4.m4.4.5.2.2" xref="S4.p3.4.m4.4.5.1.cmml">,</mo><mi id="S4.p3.4.m4.2.2" xref="S4.p3.4.m4.2.2.cmml">n</mi><mo id="S4.p3.4.m4.4.5.2.3" xref="S4.p3.4.m4.4.5.1.cmml">,</mo><mi id="S4.p3.4.m4.3.3" xref="S4.p3.4.m4.3.3.cmml">n</mi><mo id="S4.p3.4.m4.4.5.2.4" xref="S4.p3.4.m4.4.5.1.cmml">,</mo><mn id="S4.p3.4.m4.4.4" xref="S4.p3.4.m4.4.4.cmml">3</mn><mo stretchy="false" id="S4.p3.4.m4.4.5.2.5" xref="S4.p3.4.m4.4.5.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.4.m4.4b"><vector id="S4.p3.4.m4.4.5.1.cmml" xref="S4.p3.4.m4.4.5.2"><cn type="integer" id="S4.p3.4.m4.1.1.cmml" xref="S4.p3.4.m4.1.1">1</cn><ci id="S4.p3.4.m4.2.2.cmml" xref="S4.p3.4.m4.2.2">𝑛</ci><ci id="S4.p3.4.m4.3.3.cmml" xref="S4.p3.4.m4.3.3">𝑛</ci><cn type="integer" id="S4.p3.4.m4.4.4.cmml" xref="S4.p3.4.m4.4.4">3</cn></vector></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.4.m4.4c">(1,n,n,3)</annotation></semantics></math> is created. The image is then passed to the module for image inference with a YOLOv8-based model. The neural network structure and weights are loaded to <code id="S4.p3.4.1" class="ltx_verbatim ltx_font_typewriter">Unity.Barracuda.Model</code> using an ONNX file distributed as an asset inside the application. The procedure of model preparation is described in Sect. <a href="#S3.SS4" title="III-D Model preparation ‣ III Hardware and Software Frameworks ‣ Real-Time Onboard Object Detection for Augmented Reality: Enhancing Head-Mounted Display with YOLOv8 The study has been supported by funding provided through an unrestricted gift by Meta." class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-D</span></span></a>.
Once inference is completed, we have to parse the raw YOLO output detection into final detection, consisting of bounding box, object class and class score. At first, we determine a class of an object by choosing the one with the highest score for every detection in raw output. Next, the detections are filtered by a class score. The threshold can be selected using a precision-recall curve and depends on the requirements of the target application. Elements with scores lower than the given threshold are rejected. The next step is to perform <span id="S4.p3.4.2" class="ltx_text ltx_font_italic">Non-Maximum Suppression</span> (NMS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> to discard overlapping boxes and select the best one.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">The inference step of neural network inference produces a 2D bounding box on the image. We use the time of the original image acquisition from the camera-to-world matrix to project this 2D bounding box into the 3D scene observed by the user in AR.
Based on this implementation, we can adequately annotate the 3D position of the object even if the user is looking in a different direction than when we captured the image for object detection.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Evaluation</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">The proposed processing pipeline with YOLOv8 is evaluated under two key criteria for the final application: (1) processing time and (2) object detection performance. All experiments were performed using the <span id="S5.p1.1.1" class="ltx_text ltx_font_italic">val2017</span> subset of Microsoft COCO dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>. This image data collection is a large-scale object detection, segmentation, and captioning dataset containing 91 categories of ordinary objects. It is a common choice in object detection tasks regarding benchmarking methods and using COCO pretrained models to perform transfer learning and fine-tuning to adapt models to different detection tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. Weights of YOLOv8 models pretrained on COCO are available online.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.2" class="ltx_p">We used <code id="S5.p2.2.1" class="ltx_verbatim ltx_font_typewriter">System.Diagnostics.Stopwatch</code> for processing time measurement with high-resolution performance counter mode on capturing either the whole processing or a selected part of the pipeline.
Each time measurement was repeated <math id="S5.p2.1.m1.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S5.p2.1.m1.1a"><mn id="S5.p2.1.m1.1.1" xref="S5.p2.1.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S5.p2.1.m1.1b"><cn type="integer" id="S5.p2.1.m1.1.1.cmml" xref="S5.p2.1.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.1.m1.1c">100</annotation></semantics></math> times after the model warm-up, i.e., several initial inferences that are necessary for each GPU application to stabilize the processing times and further ensure fair and robust comparisons between different configurations.
The HL2 battery charge level was over <math id="S5.p2.2.m2.1" class="ltx_Math" alttext="50\%" display="inline"><semantics id="S5.p2.2.m2.1a"><mrow id="S5.p2.2.m2.1.1" xref="S5.p2.2.m2.1.1.cmml"><mn id="S5.p2.2.m2.1.1.2" xref="S5.p2.2.m2.1.1.2.cmml">50</mn><mo id="S5.p2.2.m2.1.1.1" xref="S5.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.p2.2.m2.1b"><apply id="S5.p2.2.m2.1.1.cmml" xref="S5.p2.2.m2.1.1"><csymbol cd="latexml" id="S5.p2.2.m2.1.1.1.cmml" xref="S5.p2.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S5.p2.2.m2.1.1.2.cmml" xref="S5.p2.2.m2.1.1.2">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.2.m2.1c">50\%</annotation></semantics></math> for all trials, and the headset was not connected to any other device or power source.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.4.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.5.2" class="ltx_text ltx_font_italic">Measuring the impact of YOLOv8 model size</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">The first design choice we have to make when using the YOLOv8 detector is selecting the network’s size among the available family of architectures.
The YOLOv8 authors publicly share five pretrained networks that can be used out-of-the-box in the desired application, starting from the smallest network to the more extensive networks measured as a number of parameters: (i) nano (YOLOv8n), (ii) small (YOLOv8s), (iii) medium (YOLOv8m), (iv) large (YOLOv8l), and (v) extra large (YOLOv8x). Selecting a smaller network hinders the final performance while simultaneously taking less memory on the device and providing faster inference.
Simultaneously, a larger network offers a better object detection performance.</p>
</div>
<figure id="S5.F4" class="ltx_figure"><img src="/html/2306.03537/assets/figures/model_variant_vs_processing_time.png" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="437" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F4.4.2.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S5.F4.2.1" class="ltx_text" style="font-size:90%;">The single-image, onboard HL2 processing times depending on the chosen YOLOv8 model size for a fixed image size <math id="S5.F4.2.1.m1.1" class="ltx_Math" alttext="224\times 224" display="inline"><semantics id="S5.F4.2.1.m1.1b"><mrow id="S5.F4.2.1.m1.1.1" xref="S5.F4.2.1.m1.1.1.cmml"><mn id="S5.F4.2.1.m1.1.1.2" xref="S5.F4.2.1.m1.1.1.2.cmml">224</mn><mo lspace="0.222em" rspace="0.222em" id="S5.F4.2.1.m1.1.1.1" xref="S5.F4.2.1.m1.1.1.1.cmml">×</mo><mn id="S5.F4.2.1.m1.1.1.3" xref="S5.F4.2.1.m1.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.F4.2.1.m1.1c"><apply id="S5.F4.2.1.m1.1.1.cmml" xref="S5.F4.2.1.m1.1.1"><times id="S5.F4.2.1.m1.1.1.1.cmml" xref="S5.F4.2.1.m1.1.1.1"></times><cn type="integer" id="S5.F4.2.1.m1.1.1.2.cmml" xref="S5.F4.2.1.m1.1.1.2">224</cn><cn type="integer" id="S5.F4.2.1.m1.1.1.3.cmml" xref="S5.F4.2.1.m1.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.2.1.m1.1d">224\times 224</annotation></semantics></math> pixels.</span></figcaption>
</figure>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">Our experiments measured the processing time from the image capture moment to the bounding boxes projected in the 3D view.
The comparison was performed with a usual image size of <math id="S5.SS1.p2.1.m1.1" class="ltx_Math" alttext="224\times 224" display="inline"><semantics id="S5.SS1.p2.1.m1.1a"><mrow id="S5.SS1.p2.1.m1.1.1" xref="S5.SS1.p2.1.m1.1.1.cmml"><mn id="S5.SS1.p2.1.m1.1.1.2" xref="S5.SS1.p2.1.m1.1.1.2.cmml">224</mn><mo lspace="0.222em" rspace="0.222em" id="S5.SS1.p2.1.m1.1.1.1" xref="S5.SS1.p2.1.m1.1.1.1.cmml">×</mo><mn id="S5.SS1.p2.1.m1.1.1.3" xref="S5.SS1.p2.1.m1.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.1b"><apply id="S5.SS1.p2.1.m1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1"><times id="S5.SS1.p2.1.m1.1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1.1"></times><cn type="integer" id="S5.SS1.p2.1.m1.1.1.2.cmml" xref="S5.SS1.p2.1.m1.1.1.2">224</cn><cn type="integer" id="S5.SS1.p2.1.m1.1.1.3.cmml" xref="S5.SS1.p2.1.m1.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.1c">224\times 224</annotation></semantics></math> pixels, and the results are presented in Fig. <a href="#S5.F4" title="Figure 4 ‣ V-A Measuring the impact of YOLOv8 model size ‣ V Evaluation ‣ Real-Time Onboard Object Detection for Augmented Reality: Enhancing Head-Mounted Display with YOLOv8 The study has been supported by funding provided through an unrestricted gift by Meta." class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
These results suggest that for the best user experience in dynamic scenes, only the smallest YOLOv8n can meet the real-time requirements.
Other models can still work in the resource-constrained environment of HL2.
However, they might only be suited for scenarios where real-time performance is not vital for user experience, e.g., when used to classify the object held in hand or when the scene is not dynamic.
In these cases, the object detection will still be able to properly place the object position in the user’s surroundings but will require more time to get these results.</p>
</div>
<figure id="S5.F5" class="ltx_figure"><img src="/html/2306.03537/assets/figures/coco_mAP_model_variants.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.4.2.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S5.F5.2.1" class="ltx_text" style="font-size:90%;">The measured performance of object detection as mAP values depending on the network size for YOLOv8 starting from the smallest network (YOLOv8n) to the largest network (YOLOv8x) for a fixed image size <math id="S5.F5.2.1.m1.1" class="ltx_Math" alttext="224\times 224" display="inline"><semantics id="S5.F5.2.1.m1.1b"><mrow id="S5.F5.2.1.m1.1.1" xref="S5.F5.2.1.m1.1.1.cmml"><mn id="S5.F5.2.1.m1.1.1.2" xref="S5.F5.2.1.m1.1.1.2.cmml">224</mn><mo lspace="0.222em" rspace="0.222em" id="S5.F5.2.1.m1.1.1.1" xref="S5.F5.2.1.m1.1.1.1.cmml">×</mo><mn id="S5.F5.2.1.m1.1.1.3" xref="S5.F5.2.1.m1.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.F5.2.1.m1.1c"><apply id="S5.F5.2.1.m1.1.1.cmml" xref="S5.F5.2.1.m1.1.1"><times id="S5.F5.2.1.m1.1.1.1.cmml" xref="S5.F5.2.1.m1.1.1.1"></times><cn type="integer" id="S5.F5.2.1.m1.1.1.2.cmml" xref="S5.F5.2.1.m1.1.1.2">224</cn><cn type="integer" id="S5.F5.2.1.m1.1.1.3.cmml" xref="S5.F5.2.1.m1.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F5.2.1.m1.1d">224\times 224</annotation></semantics></math> pixels.</span></figcaption>
</figure>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">The usage of network models with a lower number of parameters results in lower performance. The usual metric to quantify performance is mean average precision (mAP), which is the average precision for all object classes measured at a selected threshold <code id="S5.SS1.p3.1.1" class="ltx_verbatim ltx_font_typewriter">A</code>.
<code id="S5.SS1.p3.1.2" class="ltx_verbatim ltx_font_typewriter">mAP@A</code> indicates the performance when at least a <code id="S5.SS1.p3.1.3" class="ltx_verbatim ltx_font_typewriter">A</code><math id="S5.SS1.p3.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S5.SS1.p3.1.m1.1a"><mo id="S5.SS1.p3.1.m1.1.1" xref="S5.SS1.p3.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.1b"><csymbol cd="latexml" id="S5.SS1.p3.1.m1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.1c">\%</annotation></semantics></math> overlap between the bounding box from detection and ground truth bounding box (Intersection over Union – IoU) is required to assume that the object was correctly recognized.
The performance of different detection model configurations is presented in Fig. <a href="#S5.F5" title="Figure 5 ‣ V-A Measuring the impact of YOLOv8 model size ‣ V Evaluation ‣ Real-Time Onboard Object Detection for Augmented Reality: Enhancing Head-Mounted Display with YOLOv8 The study has been supported by funding provided through an unrestricted gift by Meta." class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> for <code id="S5.SS1.p3.1.4" class="ltx_verbatim ltx_font_typewriter">mAP@50</code> and <code id="S5.SS1.p3.1.5" class="ltx_verbatim ltx_font_typewriter">mAP@50-95</code> averaging the performance over a range of IoU thresholds.
The obtained results suggest that a significant drop in performance should be expected when using smaller models.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.4.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.5.2" class="ltx_text ltx_font_italic">Object detection depending on the input image size</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Unfortunately, even with the smallest model, i.e., YOLOv8n, we ought to seek further improvements to achieve real-time performance dictated by the best immersive experience for AR headset users.</p>
</div>
<figure id="S5.F6" class="ltx_figure"><img src="/html/2306.03537/assets/figures/input_size_vs_processing_time.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="451" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S5.F6.3.2" class="ltx_text" style="font-size:90%;">The total processing time for object detection for YOLOv8n model with different input image sizes. The light blue interval shows the standard deviation of the performed measurements.</span></figcaption>
</figure>
<figure id="S5.F7" class="ltx_figure"><img src="/html/2306.03537/assets/figures/coco_mAP_input_sizes.png" id="S5.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S5.F7.3.2" class="ltx_text" style="font-size:90%;">The measured performance of object detection using YOLOv8n model with different input image sizes.</span></figcaption>
</figure>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">Apart from the size of the network, the other possibility is to reduce the input image size as it directly impacts the inference times.
The results we obtained for varying image input sizes are presented in Fig. <a href="#S5.F6" title="Figure 6 ‣ V-B Object detection depending on the input image size ‣ V Evaluation ‣ Real-Time Onboard Object Detection for Augmented Reality: Enhancing Head-Mounted Display with YOLOv8 The study has been supported by funding provided through an unrestricted gift by Meta." class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">The obtained relation between processing times and an input image size shows that processing times scale almost quadratically with the side length of an image (i.e. linearly with the number of pixels).
Based on this observation, we can see that it is possible to obtain object detection results in less than 100 [ms] when using an image size of <math id="S5.SS2.p3.1.m1.1" class="ltx_Math" alttext="160\times 160" display="inline"><semantics id="S5.SS2.p3.1.m1.1a"><mrow id="S5.SS2.p3.1.m1.1.1" xref="S5.SS2.p3.1.m1.1.1.cmml"><mn id="S5.SS2.p3.1.m1.1.1.2" xref="S5.SS2.p3.1.m1.1.1.2.cmml">160</mn><mo lspace="0.222em" rspace="0.222em" id="S5.SS2.p3.1.m1.1.1.1" xref="S5.SS2.p3.1.m1.1.1.1.cmml">×</mo><mn id="S5.SS2.p3.1.m1.1.1.3" xref="S5.SS2.p3.1.m1.1.1.3.cmml">160</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.1.m1.1b"><apply id="S5.SS2.p3.1.m1.1.1.cmml" xref="S5.SS2.p3.1.m1.1.1"><times id="S5.SS2.p3.1.m1.1.1.1.cmml" xref="S5.SS2.p3.1.m1.1.1.1"></times><cn type="integer" id="S5.SS2.p3.1.m1.1.1.2.cmml" xref="S5.SS2.p3.1.m1.1.1.2">160</cn><cn type="integer" id="S5.SS2.p3.1.m1.1.1.3.cmml" xref="S5.SS2.p3.1.m1.1.1.3">160</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.1.m1.1c">160\times 160</annotation></semantics></math> pixels.
Using smaller input image sizes might impact the achieved performance of the algorithm. We show the influence of the input image size on the mAP of the algorithms in Fig. <a href="#S5.F7" title="Figure 7 ‣ V-B Object detection depending on the input image size ‣ V Evaluation ‣ Real-Time Onboard Object Detection for Augmented Reality: Enhancing Head-Mounted Display with YOLOv8 The study has been supported by funding provided through an unrestricted gift by Meta." class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS3.4.1.1" class="ltx_text">V-C</span> </span><span id="S5.SS3.5.2" class="ltx_text ltx_font_italic">Choosing the best model based on processing time budget</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">We might also have a greater processing time budget depending on the application. In these scenarios, we wanted to quantify if using a larger network for inference is more beneficial or, rather, increasing the backbone size to improve the network’s detection performance makes sense.</p>
</div>
<figure id="S5.F8" class="ltx_figure"><img src="/html/2306.03537/assets/figures/coco_mAP_time_model_variants.png" id="S5.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F8.2.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S5.F8.3.2" class="ltx_text" style="font-size:90%;">Comparison of mAP and inference time for different sizes of YOLOv8 models</span></figcaption>
</figure>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">The obtained results suggest that for any application with an inference time budget below 400 [ms], it is beneficial to use YOLOv8n while tuning the image size to fit the budget requirements.
Compared to YOLOv8n at the same processing time, larger networks perform worse as they need to use smaller images.
For processing times thresholds greater than 400 [ms], we should choose YOLOv8s as it offers better performance than YOLOv8n despite smaller input image sizes than YOLOv8n while outperforming all larger backbones in the analyzed processing time interval.
The presented conclusions are drawn based on the obtained performance on all objects in the COCO dataset, which might not hold equally for particular object classes.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS4.4.1.1" class="ltx_text">V-D</span> </span><span id="S5.SS4.5.2" class="ltx_text ltx_font_italic">Model performance analysis for AR applications</span>
</h3>

<figure id="S5.F9" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S5.F9.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2306.03537/assets/figures/use_case/1_0.jpg" id="S5.F9.sf1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="598" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F9.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S5.F9.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2306.03537/assets/figures/use_case/1_5.jpg" id="S5.F9.sf2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="598" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F9.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S5.F9.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2306.03537/assets/figures/use_case/2_0.jpg" id="S5.F9.sf3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="598" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F9.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S5.F9.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2306.03537/assets/figures/use_case/2_5.jpg" id="S5.F9.sf4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="598" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F9.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S5.F9.sf5" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2306.03537/assets/figures/use_case/3_0.jpg" id="S5.F9.sf5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="598" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F9.sf5.2.1.1" class="ltx_text" style="font-size:90%;">(e)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F9.4.2.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="S5.F9.2.1" class="ltx_text" style="font-size:90%;">The performance of the proposed real-time YOLOv8n when using <math id="S5.F9.2.1.m1.1" class="ltx_Math" alttext="160\times 160" display="inline"><semantics id="S5.F9.2.1.m1.1b"><mrow id="S5.F9.2.1.m1.1.1" xref="S5.F9.2.1.m1.1.1.cmml"><mn id="S5.F9.2.1.m1.1.1.2" xref="S5.F9.2.1.m1.1.1.2.cmml">160</mn><mo lspace="0.222em" rspace="0.222em" id="S5.F9.2.1.m1.1.1.1" xref="S5.F9.2.1.m1.1.1.1.cmml">×</mo><mn id="S5.F9.2.1.m1.1.1.3" xref="S5.F9.2.1.m1.1.1.3.cmml">160</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.F9.2.1.m1.1c"><apply id="S5.F9.2.1.m1.1.1.cmml" xref="S5.F9.2.1.m1.1.1"><times id="S5.F9.2.1.m1.1.1.1.cmml" xref="S5.F9.2.1.m1.1.1.1"></times><cn type="integer" id="S5.F9.2.1.m1.1.1.2.cmml" xref="S5.F9.2.1.m1.1.1.2">160</cn><cn type="integer" id="S5.F9.2.1.m1.1.1.3.cmml" xref="S5.F9.2.1.m1.1.1.3">160</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F9.2.1.m1.1d">160\times 160</annotation></semantics></math> input image size. The network detects objects reliably from (a) 1 [m], (b) 1.5 [m], (c) 2.0 [m], (d) and 2.5 [m] distance, with decaying results up to (e) 3.0 [m].</span></figcaption>
</figure>
<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">Flustered by the reduced performance of the YOLOv8n with a small input image size of <math id="S5.SS4.p1.1.m1.1" class="ltx_Math" alttext="160\times 160" display="inline"><semantics id="S5.SS4.p1.1.m1.1a"><mrow id="S5.SS4.p1.1.m1.1.1" xref="S5.SS4.p1.1.m1.1.1.cmml"><mn id="S5.SS4.p1.1.m1.1.1.2" xref="S5.SS4.p1.1.m1.1.1.2.cmml">160</mn><mo lspace="0.222em" rspace="0.222em" id="S5.SS4.p1.1.m1.1.1.1" xref="S5.SS4.p1.1.m1.1.1.1.cmml">×</mo><mn id="S5.SS4.p1.1.m1.1.1.3" xref="S5.SS4.p1.1.m1.1.1.3.cmml">160</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.1.m1.1b"><apply id="S5.SS4.p1.1.m1.1.1.cmml" xref="S5.SS4.p1.1.m1.1.1"><times id="S5.SS4.p1.1.m1.1.1.1.cmml" xref="S5.SS4.p1.1.m1.1.1.1"></times><cn type="integer" id="S5.SS4.p1.1.m1.1.1.2.cmml" xref="S5.SS4.p1.1.m1.1.1.2">160</cn><cn type="integer" id="S5.SS4.p1.1.m1.1.1.3.cmml" xref="S5.SS4.p1.1.m1.1.1.3">160</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.1.m1.1c">160\times 160</annotation></semantics></math>, we conducted a series of real-world test experiments.
We focused on an example object, i.e. a smartphone, detected from 1 [m] up to 4 [m] with an object observed from 20 different viewing angles at each distance as presented in Fig. <a href="#S5.F9" title="Figure 9 ‣ V-D Model performance analysis for AR applications ‣ V Evaluation ‣ Real-Time Onboard Object Detection for Augmented Reality: Enhancing Head-Mounted Display with YOLOv8 The study has been supported by funding provided through an unrestricted gift by Meta." class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.
We selected these distances, arguing that reaching up to 2 [m] is crucial for AR interaction due to the maximum extent of human arms and hand-held tools. We frequently encounter such situations when dealing with shop floor tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> or device repair <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, which require close vicinity, i.e., arms-stretch distance from non-digital asset users are interacting with.</p>
</div>
<figure id="S5.F10" class="ltx_figure"><img src="/html/2306.03537/assets/figures/recall.png" id="S5.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="592" height="444" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F10.4.2.1" class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span id="S5.F10.2.1" class="ltx_text" style="font-size:90%;">Recall depending on the distance to the smartphone for different confidence thresholds based on the real-time YOLOv8n run on <math id="S5.F10.2.1.m1.1" class="ltx_Math" alttext="160\times 160" display="inline"><semantics id="S5.F10.2.1.m1.1b"><mrow id="S5.F10.2.1.m1.1.1" xref="S5.F10.2.1.m1.1.1.cmml"><mn id="S5.F10.2.1.m1.1.1.2" xref="S5.F10.2.1.m1.1.1.2.cmml">160</mn><mo lspace="0.222em" rspace="0.222em" id="S5.F10.2.1.m1.1.1.1" xref="S5.F10.2.1.m1.1.1.1.cmml">×</mo><mn id="S5.F10.2.1.m1.1.1.3" xref="S5.F10.2.1.m1.1.1.3.cmml">160</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.F10.2.1.m1.1c"><apply id="S5.F10.2.1.m1.1.1.cmml" xref="S5.F10.2.1.m1.1.1"><times id="S5.F10.2.1.m1.1.1.1.cmml" xref="S5.F10.2.1.m1.1.1.1"></times><cn type="integer" id="S5.F10.2.1.m1.1.1.2.cmml" xref="S5.F10.2.1.m1.1.1.2">160</cn><cn type="integer" id="S5.F10.2.1.m1.1.1.3.cmml" xref="S5.F10.2.1.m1.1.1.3">160</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F10.2.1.m1.1d">160\times 160</annotation></semantics></math> input image sizes. The obtained results suggest that the proposed real-time configuration of YOLOv8n on HL2 can be sufficient for most AR applications.</span></figcaption>
</figure>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p">We measured the performance at each distance as a recall, i.e., a ratio of the number of cases when the smartphone was properly detected to the number of images.
The results obtained for different confidence thresholds are presented in Fig. <a href="#S5.F10" title="Figure 10 ‣ V-D Model performance analysis for AR applications ‣ V Evaluation ‣ Real-Time Onboard Object Detection for Augmented Reality: Enhancing Head-Mounted Display with YOLOv8 The study has been supported by funding provided through an unrestricted gift by Meta." class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>.
These outcomes suggest that the proposed configuration offering inference results in less than 100 [ms] can still detect all object (i.e. smartphone) instances if we focus on distances closer than 2.5 [m]. We believe that such performance fulfills the requirements for most AR use case scenarios <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Ablation study</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">The goal of the following section is to understand further the limitations of the YOLOv8 with <math id="S6.p1.1.m1.1" class="ltx_Math" alttext="160\times 160" display="inline"><semantics id="S6.p1.1.m1.1a"><mrow id="S6.p1.1.m1.1.1" xref="S6.p1.1.m1.1.1.cmml"><mn id="S6.p1.1.m1.1.1.2" xref="S6.p1.1.m1.1.1.2.cmml">160</mn><mo lspace="0.222em" rspace="0.222em" id="S6.p1.1.m1.1.1.1" xref="S6.p1.1.m1.1.1.1.cmml">×</mo><mn id="S6.p1.1.m1.1.1.3" xref="S6.p1.1.m1.1.1.3.cmml">160</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.p1.1.m1.1b"><apply id="S6.p1.1.m1.1.1.cmml" xref="S6.p1.1.m1.1.1"><times id="S6.p1.1.m1.1.1.1.cmml" xref="S6.p1.1.m1.1.1.1"></times><cn type="integer" id="S6.p1.1.m1.1.1.2.cmml" xref="S6.p1.1.m1.1.1.2">160</cn><cn type="integer" id="S6.p1.1.m1.1.1.3.cmml" xref="S6.p1.1.m1.1.1.3">160</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.1.m1.1c">160\times 160</annotation></semantics></math> input image size and propose a solution that could further speed up the processing capabilities depending on the application.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS1.4.1.1" class="ltx_text">VI-A</span> </span><span id="S6.SS1.5.2" class="ltx_text ltx_font_italic">Processing time analysis</span>
</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">As a first step, we measured the time spent to prepare the data (i.e., preprocessing), make the actual bounding box prediction (i.e., inference), and the time necessary to analyze the obtained predictions (i.e., postprocessing).
The results are summarized in Tab. <a href="#S6.T1" title="TABLE I ‣ VI-A Processing time analysis ‣ VI Ablation study ‣ Real-Time Onboard Object Detection for Augmented Reality: Enhancing Head-Mounted Display with YOLOv8 The study has been supported by funding provided through an unrestricted gift by Meta." class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> and indicate the most time spent performing the inference.
It shows that postprocessing, even though the NMS step is not performed on the GPU, is not a limiting factor.</p>
</div>
<figure id="S6.T1" class="ltx_table">
<table id="S6.T1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T1.4.1.1" class="ltx_tr">
<th id="S6.T1.4.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" style="padding:2.5pt 10.0pt;"></th>
<th id="S6.T1.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding:2.5pt 10.0pt;" colspan="2"><span id="S6.T1.4.1.1.2.1" class="ltx_text ltx_font_bold">processing time</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T1.4.2.1" class="ltx_tr">
<th id="S6.T1.4.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:2.5pt 10.0pt;"><span id="S6.T1.4.2.1.1.1" class="ltx_text ltx_font_bold">operation</span></th>
<td id="S6.T1.4.2.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:2.5pt 10.0pt;"><span id="S6.T1.4.2.1.2.1" class="ltx_text ltx_font_bold">mean [ms]</span></td>
<td id="S6.T1.4.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:2.5pt 10.0pt;"><span id="S6.T1.4.2.1.3.1" class="ltx_text ltx_font_bold">std [ms]</span></td>
</tr>
<tr id="S6.T1.4.3.2" class="ltx_tr">
<th id="S6.T1.4.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:2.5pt 10.0pt;"><span id="S6.T1.4.3.2.1.1" class="ltx_text ltx_font_bold">preprocessing</span></th>
<td id="S6.T1.4.3.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:2.5pt 10.0pt;">1.97</td>
<td id="S6.T1.4.3.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:2.5pt 10.0pt;">1.49</td>
</tr>
<tr id="S6.T1.4.4.3" class="ltx_tr">
<th id="S6.T1.4.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:2.5pt 10.0pt;"><span id="S6.T1.4.4.3.1.1" class="ltx_text ltx_font_bold">inference</span></th>
<td id="S6.T1.4.4.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:2.5pt 10.0pt;">89.84</td>
<td id="S6.T1.4.4.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:2.5pt 10.0pt;">7.45</td>
</tr>
<tr id="S6.T1.4.5.4" class="ltx_tr">
<th id="S6.T1.4.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:2.5pt 10.0pt;"><span id="S6.T1.4.5.4.1.1" class="ltx_text ltx_font_bold">postprocessing</span></th>
<td id="S6.T1.4.5.4.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:2.5pt 10.0pt;">4.06</td>
<td id="S6.T1.4.5.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:2.5pt 10.0pt;">1.17</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S6.T1.5.2.1" class="ltx_text" style="font-size:90%;">TABLE I</span>: </span><span id="S6.T1.2.1" class="ltx_text" style="font-size:90%;">The analysis of the total processing time when using the YOLOv8n model and <math id="S6.T1.2.1.m1.1" class="ltx_Math" alttext="160\times 160" display="inline"><semantics id="S6.T1.2.1.m1.1b"><mrow id="S6.T1.2.1.m1.1.1" xref="S6.T1.2.1.m1.1.1.cmml"><mn id="S6.T1.2.1.m1.1.1.2" xref="S6.T1.2.1.m1.1.1.2.cmml">160</mn><mo lspace="0.222em" rspace="0.222em" id="S6.T1.2.1.m1.1.1.1" xref="S6.T1.2.1.m1.1.1.1.cmml">×</mo><mn id="S6.T1.2.1.m1.1.1.3" xref="S6.T1.2.1.m1.1.1.3.cmml">160</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T1.2.1.m1.1c"><apply id="S6.T1.2.1.m1.1.1.cmml" xref="S6.T1.2.1.m1.1.1"><times id="S6.T1.2.1.m1.1.1.1.cmml" xref="S6.T1.2.1.m1.1.1.1"></times><cn type="integer" id="S6.T1.2.1.m1.1.1.2.cmml" xref="S6.T1.2.1.m1.1.1.2">160</cn><cn type="integer" id="S6.T1.2.1.m1.1.1.3.cmml" xref="S6.T1.2.1.m1.1.1.3">160</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.2.1.m1.1d">160\times 160</annotation></semantics></math> images size input revealing that the most time is spent doing the core neural model inference</span></figcaption>
</figure>
<div id="S6.SS1.p2" class="ltx_para">
<p id="S6.SS1.p2.1" class="ltx_p">Further analysis of the inference time using the Unity profiler tool revealed that the time spent to copy the data for inference is negligible, taking less than <math id="S6.SS1.p2.1.m1.1" class="ltx_Math" alttext="1\%" display="inline"><semantics id="S6.SS1.p2.1.m1.1a"><mrow id="S6.SS1.p2.1.m1.1.1" xref="S6.SS1.p2.1.m1.1.1.cmml"><mn id="S6.SS1.p2.1.m1.1.1.2" xref="S6.SS1.p2.1.m1.1.1.2.cmml">1</mn><mo id="S6.SS1.p2.1.m1.1.1.1" xref="S6.SS1.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.1.m1.1b"><apply id="S6.SS1.p2.1.m1.1.1.cmml" xref="S6.SS1.p2.1.m1.1.1"><csymbol cd="latexml" id="S6.SS1.p2.1.m1.1.1.1.cmml" xref="S6.SS1.p2.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S6.SS1.p2.1.m1.1.1.2.cmml" xref="S6.SS1.p2.1.m1.1.1.2">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.1.m1.1c">1\%</annotation></semantics></math> of the overall inference time.
Therefore, further improvements should be sought in the inference itself.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS2.4.1.1" class="ltx_text">VI-B</span> </span><span id="S6.SS2.5.2" class="ltx_text ltx_font_italic">Testing different model processing backends</span>
</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">The processing time analysis indicates that we should improve the inference time. One possibility is to explore the inference backends available in the Barracuda package <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> The backend choice determines whether the neural network will be run on GPU or CPU and what kind of implementation will be used.
The results received for different backends are presented in Tab. <a href="#S6.T2" title="TABLE II ‣ VI-B Testing different model processing backends ‣ VI Ablation study ‣ Real-Time Onboard Object Detection for Augmented Reality: Enhancing Head-Mounted Display with YOLOv8 The study has been supported by funding provided through an unrestricted gift by Meta." class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>.</p>
</div>
<figure id="S6.T2" class="ltx_table">
<table id="S6.T2.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T2.4.1.1" class="ltx_tr">
<th id="S6.T2.4.1.1.1" class="ltx_td ltx_th ltx_th_row" style="padding:2.5pt 10.0pt;"></th>
<td id="S6.T2.4.1.1.2" class="ltx_td" style="padding:2.5pt 10.0pt;"></td>
<td id="S6.T2.4.1.1.3" class="ltx_td ltx_align_center" style="padding:2.5pt 10.0pt;" colspan="2"><span id="S6.T2.4.1.1.3.1" class="ltx_text ltx_font_bold">inference time</span></td>
</tr>
<tr id="S6.T2.4.2.2" class="ltx_tr">
<th id="S6.T2.4.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding:2.5pt 10.0pt;"><span id="S6.T2.4.2.2.1.1" class="ltx_text ltx_font_bold">device</span></th>
<td id="S6.T2.4.2.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:2.5pt 10.0pt;"><span id="S6.T2.4.2.2.2.1" class="ltx_text ltx_font_bold">backend</span></td>
<td id="S6.T2.4.2.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:2.5pt 10.0pt;"><span id="S6.T2.4.2.2.3.1" class="ltx_text ltx_font_bold">mean [ms]</span></td>
<td id="S6.T2.4.2.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:2.5pt 10.0pt;"><span id="S6.T2.4.2.2.4.1" class="ltx_text ltx_font_bold">std [ms]</span></td>
</tr>
<tr id="S6.T2.4.3.3" class="ltx_tr">
<th id="S6.T2.4.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:2.5pt 10.0pt;" rowspan="3"><span id="S6.T2.4.3.3.1.1" class="ltx_text ltx_font_bold">GPU</span></th>
<td id="S6.T2.4.3.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:2.5pt 10.0pt;"><span id="S6.T2.4.3.3.2.1" class="ltx_text ltx_font_bold">Compute</span></td>
<td id="S6.T2.4.3.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:2.5pt 10.0pt;">103.64</td>
<td id="S6.T2.4.3.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:2.5pt 10.0pt;">6.68</td>
</tr>
<tr id="S6.T2.4.4.4" class="ltx_tr">
<td id="S6.T2.4.4.4.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:2.5pt 10.0pt;"><span id="S6.T2.4.4.4.1.1" class="ltx_text ltx_font_bold">ComputeRef</span></td>
<td id="S6.T2.4.4.4.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:2.5pt 10.0pt;">174.65</td>
<td id="S6.T2.4.4.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:2.5pt 10.0pt;">6.65</td>
</tr>
<tr id="S6.T2.4.5.5" class="ltx_tr">
<td id="S6.T2.4.5.5.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:2.5pt 10.0pt;">
<table id="S6.T2.4.5.5.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S6.T2.4.5.5.1.1.1" class="ltx_tr">
<td id="S6.T2.4.5.5.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:2.5pt 10.0pt;"><span id="S6.T2.4.5.5.1.1.1.1.1" class="ltx_text ltx_font_bold">Compute</span></td>
</tr>
<tr id="S6.T2.4.5.5.1.1.2" class="ltx_tr">
<td id="S6.T2.4.5.5.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:2.5pt 10.0pt;"><span id="S6.T2.4.5.5.1.1.2.1.1" class="ltx_text ltx_font_bold">Precompiled</span></td>
</tr>
</table>
</td>
<td id="S6.T2.4.5.5.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:2.5pt 10.0pt;">89.84</td>
<td id="S6.T2.4.5.5.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:2.5pt 10.0pt;">7.45</td>
</tr>
<tr id="S6.T2.4.6.6" class="ltx_tr">
<th id="S6.T2.4.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:2.5pt 10.0pt;" rowspan="2"><span id="S6.T2.4.6.6.1.1" class="ltx_text ltx_font_bold">CPU</span></th>
<td id="S6.T2.4.6.6.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:2.5pt 10.0pt;"><span id="S6.T2.4.6.6.2.1" class="ltx_text ltx_font_bold">CSharp</span></td>
<td id="S6.T2.4.6.6.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:2.5pt 10.0pt;">344.83</td>
<td id="S6.T2.4.6.6.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:2.5pt 10.0pt;">23.31</td>
</tr>
<tr id="S6.T2.4.7.7" class="ltx_tr">
<td id="S6.T2.4.7.7.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:2.5pt 10.0pt;"><span id="S6.T2.4.7.7.1.1" class="ltx_text ltx_font_bold">CSharpBurst</span></td>
<td id="S6.T2.4.7.7.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:2.5pt 10.0pt;">216.11</td>
<td id="S6.T2.4.7.7.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:2.5pt 10.0pt;">25.62</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S6.T2.5.2.1" class="ltx_text" style="font-size:90%;">TABLE II</span>: </span><span id="S6.T2.2.1" class="ltx_text" style="font-size:90%;">Inference times for different backend selections on HL2 using YOLOv8n and image input size of <math id="S6.T2.2.1.m1.1" class="ltx_Math" alttext="160\times 160" display="inline"><semantics id="S6.T2.2.1.m1.1b"><mrow id="S6.T2.2.1.m1.1.1" xref="S6.T2.2.1.m1.1.1.cmml"><mn id="S6.T2.2.1.m1.1.1.2" xref="S6.T2.2.1.m1.1.1.2.cmml">160</mn><mo lspace="0.222em" rspace="0.222em" id="S6.T2.2.1.m1.1.1.1" xref="S6.T2.2.1.m1.1.1.1.cmml">×</mo><mn id="S6.T2.2.1.m1.1.1.3" xref="S6.T2.2.1.m1.1.1.3.cmml">160</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T2.2.1.m1.1c"><apply id="S6.T2.2.1.m1.1.1.cmml" xref="S6.T2.2.1.m1.1.1"><times id="S6.T2.2.1.m1.1.1.1.cmml" xref="S6.T2.2.1.m1.1.1.1"></times><cn type="integer" id="S6.T2.2.1.m1.1.1.2.cmml" xref="S6.T2.2.1.m1.1.1.2">160</cn><cn type="integer" id="S6.T2.2.1.m1.1.1.3.cmml" xref="S6.T2.2.1.m1.1.1.3">160</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.2.1.m1.1d">160\times 160</annotation></semantics></math></span></figcaption>
</figure>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.1" class="ltx_p">The fastest inference times were obtained for <code id="S6.SS2.p2.1.1" class="ltx_verbatim ltx_font_typewriter">ComputePrecompiled</code> backend, followed by the <code id="S6.SS2.p2.1.2" class="ltx_verbatim ltx_font_typewriter">Compute</code> backend, which did not improve further results. We were unable to execute an inference with remaining <code id="S6.SS2.p2.1.3" class="ltx_verbatim ltx_font_typewriter">CSharpRef</code> and <code id="S6.SS2.p2.1.4" class="ltx_verbatim ltx_font_typewriter">PixelShader</code> backends.</p>
</div>
<div id="S6.SS2.p3" class="ltx_para">
<p id="S6.SS2.p3.1" class="ltx_p">Another way to improve the neural network inference performance is the usage of quantization, either with reduced float precision (FP16) or integer (INT8).
Unfortunately, the Barracuda library does not support the FP16 nor INT8 quantization in its current implementation.
FP16 usually offers a significant speed-up compared to the full float implementations as already proven on other computing platforms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>.</p>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS3.4.1.1" class="ltx_text">VI-C</span> </span><span id="S6.SS3.5.2" class="ltx_text ltx_font_italic">Dealing with non-square image input</span>
</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.3" class="ltx_p">One commonly used approach to dealing with larger images and smaller objects assumes sliding the object detector over the whole image <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>.
Consequently, we wanted to verify if it is beneficial for HL2 to divide the input image into smaller sub-images while increasing the batch size of the passed data input.
Here, another gain is that the view from HL2 is not square, as we have a greater horizontal field of view than the vertical one.
Our experiment compared two approaches: the network with <math id="S6.SS3.p1.1.m1.1" class="ltx_Math" alttext="320\times 320" display="inline"><semantics id="S6.SS3.p1.1.m1.1a"><mrow id="S6.SS3.p1.1.m1.1.1" xref="S6.SS3.p1.1.m1.1.1.cmml"><mn id="S6.SS3.p1.1.m1.1.1.2" xref="S6.SS3.p1.1.m1.1.1.2.cmml">320</mn><mo lspace="0.222em" rspace="0.222em" id="S6.SS3.p1.1.m1.1.1.1" xref="S6.SS3.p1.1.m1.1.1.1.cmml">×</mo><mn id="S6.SS3.p1.1.m1.1.1.3" xref="S6.SS3.p1.1.m1.1.1.3.cmml">320</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.1.m1.1b"><apply id="S6.SS3.p1.1.m1.1.1.cmml" xref="S6.SS3.p1.1.m1.1.1"><times id="S6.SS3.p1.1.m1.1.1.1.cmml" xref="S6.SS3.p1.1.m1.1.1.1"></times><cn type="integer" id="S6.SS3.p1.1.m1.1.1.2.cmml" xref="S6.SS3.p1.1.m1.1.1.2">320</cn><cn type="integer" id="S6.SS3.p1.1.m1.1.1.3.cmml" xref="S6.SS3.p1.1.m1.1.1.3">320</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.1.m1.1c">320\times 320</annotation></semantics></math> input image size and a network input of <math id="S6.SS3.p1.2.m2.1" class="ltx_Math" alttext="320\times 160" display="inline"><semantics id="S6.SS3.p1.2.m2.1a"><mrow id="S6.SS3.p1.2.m2.1.1" xref="S6.SS3.p1.2.m2.1.1.cmml"><mn id="S6.SS3.p1.2.m2.1.1.2" xref="S6.SS3.p1.2.m2.1.1.2.cmml">320</mn><mo lspace="0.222em" rspace="0.222em" id="S6.SS3.p1.2.m2.1.1.1" xref="S6.SS3.p1.2.m2.1.1.1.cmml">×</mo><mn id="S6.SS3.p1.2.m2.1.1.3" xref="S6.SS3.p1.2.m2.1.1.3.cmml">160</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.2.m2.1b"><apply id="S6.SS3.p1.2.m2.1.1.cmml" xref="S6.SS3.p1.2.m2.1.1"><times id="S6.SS3.p1.2.m2.1.1.1.cmml" xref="S6.SS3.p1.2.m2.1.1.1"></times><cn type="integer" id="S6.SS3.p1.2.m2.1.1.2.cmml" xref="S6.SS3.p1.2.m2.1.1.2">320</cn><cn type="integer" id="S6.SS3.p1.2.m2.1.1.3.cmml" xref="S6.SS3.p1.2.m2.1.1.3">160</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.2.m2.1c">320\times 160</annotation></semantics></math> pixels divided into two sub-images, each of size <math id="S6.SS3.p1.3.m3.1" class="ltx_Math" alttext="160\times 160" display="inline"><semantics id="S6.SS3.p1.3.m3.1a"><mrow id="S6.SS3.p1.3.m3.1.1" xref="S6.SS3.p1.3.m3.1.1.cmml"><mn id="S6.SS3.p1.3.m3.1.1.2" xref="S6.SS3.p1.3.m3.1.1.2.cmml">160</mn><mo lspace="0.222em" rspace="0.222em" id="S6.SS3.p1.3.m3.1.1.1" xref="S6.SS3.p1.3.m3.1.1.1.cmml">×</mo><mn id="S6.SS3.p1.3.m3.1.1.3" xref="S6.SS3.p1.3.m3.1.1.3.cmml">160</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.3.m3.1b"><apply id="S6.SS3.p1.3.m3.1.1.cmml" xref="S6.SS3.p1.3.m3.1.1"><times id="S6.SS3.p1.3.m3.1.1.1.cmml" xref="S6.SS3.p1.3.m3.1.1.1"></times><cn type="integer" id="S6.SS3.p1.3.m3.1.1.2.cmml" xref="S6.SS3.p1.3.m3.1.1.2">160</cn><cn type="integer" id="S6.SS3.p1.3.m3.1.1.3.cmml" xref="S6.SS3.p1.3.m3.1.1.3">160</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.3.m3.1c">160\times 160</annotation></semantics></math> pixels, passed as a batch size two for network inference.
The results are summarized in Tab. <a href="#S6.T3" title="TABLE III ‣ VI-C Dealing with non-square image input ‣ VI Ablation study ‣ Real-Time Onboard Object Detection for Augmented Reality: Enhancing Head-Mounted Display with YOLOv8 The study has been supported by funding provided through an unrestricted gift by Meta." class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>.</p>
</div>
<figure id="S6.T3" class="ltx_table">
<table id="S6.T3.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T3.6.1.1" class="ltx_tr">
<th id="S6.T3.6.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" style="padding:2.5pt 10.0pt;"></th>
<th id="S6.T3.6.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding:2.5pt 10.0pt;" colspan="2"><span id="S6.T3.6.1.1.2.1" class="ltx_text ltx_font_bold">detection time</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T3.6.2.1" class="ltx_tr">
<th id="S6.T3.6.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:2.5pt 10.0pt;"><span id="S6.T3.6.2.1.1.1" class="ltx_text ltx_font_bold">image size</span></th>
<td id="S6.T3.6.2.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:2.5pt 10.0pt;"><span id="S6.T3.6.2.1.2.1" class="ltx_text ltx_font_bold">mean [ms]</span></td>
<td id="S6.T3.6.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:2.5pt 10.0pt;"><span id="S6.T3.6.2.1.3.1" class="ltx_text ltx_font_bold">std [ms]</span></td>
</tr>
<tr id="S6.T3.6.3.2" class="ltx_tr">
<th id="S6.T3.6.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:2.5pt 10.0pt;"><span id="S6.T3.6.3.2.1.1" class="ltx_text ltx_font_bold">2 x 160 x 160</span></th>
<td id="S6.T3.6.3.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:2.5pt 10.0pt;">133.64</td>
<td id="S6.T3.6.3.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:2.5pt 10.0pt;">3.94</td>
</tr>
<tr id="S6.T3.6.4.3" class="ltx_tr">
<th id="S6.T3.6.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:2.5pt 10.0pt;"><span id="S6.T3.6.4.3.1.1" class="ltx_text ltx_font_bold">1 x 320 x 320</span></th>
<td id="S6.T3.6.4.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:2.5pt 10.0pt;">236.31</td>
<td id="S6.T3.6.4.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:2.5pt 10.0pt;">4.98</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S6.T3.7.3.1" class="ltx_text" style="font-size:90%;">TABLE III</span>: </span><span id="S6.T3.4.2" class="ltx_text" style="font-size:90%;">Inference time comparison between the YOLOv8 processing the <math id="S6.T3.3.1.m1.1" class="ltx_Math" alttext="320\times 320" display="inline"><semantics id="S6.T3.3.1.m1.1b"><mrow id="S6.T3.3.1.m1.1.1" xref="S6.T3.3.1.m1.1.1.cmml"><mn id="S6.T3.3.1.m1.1.1.2" xref="S6.T3.3.1.m1.1.1.2.cmml">320</mn><mo lspace="0.222em" rspace="0.222em" id="S6.T3.3.1.m1.1.1.1" xref="S6.T3.3.1.m1.1.1.1.cmml">×</mo><mn id="S6.T3.3.1.m1.1.1.3" xref="S6.T3.3.1.m1.1.1.3.cmml">320</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T3.3.1.m1.1c"><apply id="S6.T3.3.1.m1.1.1.cmml" xref="S6.T3.3.1.m1.1.1"><times id="S6.T3.3.1.m1.1.1.1.cmml" xref="S6.T3.3.1.m1.1.1.1"></times><cn type="integer" id="S6.T3.3.1.m1.1.1.2.cmml" xref="S6.T3.3.1.m1.1.1.2">320</cn><cn type="integer" id="S6.T3.3.1.m1.1.1.3.cmml" xref="S6.T3.3.1.m1.1.1.3">320</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.3.1.m1.1d">320\times 320</annotation></semantics></math> input image size compared to the two side-stacked images of <math id="S6.T3.4.2.m2.1" class="ltx_Math" alttext="160\times 160" display="inline"><semantics id="S6.T3.4.2.m2.1b"><mrow id="S6.T3.4.2.m2.1.1" xref="S6.T3.4.2.m2.1.1.cmml"><mn id="S6.T3.4.2.m2.1.1.2" xref="S6.T3.4.2.m2.1.1.2.cmml">160</mn><mo lspace="0.222em" rspace="0.222em" id="S6.T3.4.2.m2.1.1.1" xref="S6.T3.4.2.m2.1.1.1.cmml">×</mo><mn id="S6.T3.4.2.m2.1.1.3" xref="S6.T3.4.2.m2.1.1.3.cmml">160</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T3.4.2.m2.1c"><apply id="S6.T3.4.2.m2.1.1.cmml" xref="S6.T3.4.2.m2.1.1"><times id="S6.T3.4.2.m2.1.1.1.cmml" xref="S6.T3.4.2.m2.1.1.1"></times><cn type="integer" id="S6.T3.4.2.m2.1.1.2.cmml" xref="S6.T3.4.2.m2.1.1.2">160</cn><cn type="integer" id="S6.T3.4.2.m2.1.1.3.cmml" xref="S6.T3.4.2.m2.1.1.3">160</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.4.2.m2.1d">160\times 160</annotation></semantics></math> input images passed as batch size two.</span></figcaption>
</figure>
<div id="S6.SS3.p2" class="ltx_para">
<p id="S6.SS3.p2.1" class="ltx_p">The total processing time for two side-stacked images is almost half the time required to process the square-size single image.
These results confirm that the inference time linearly depends on the number of image pixels and that the proposed batch approach allows the process of non-square image inputs to preserve the natural aspect sizes of objects.
Retraining the model for non-square inputs would require more effort than the proposed sliding window approach.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Conclusions</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">This paper presents the steps required for real-time object detection using the state-of-the-art YOLOv8 network model on the Microsoft HoloLens 2 edge computing platform.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">We believe that the ability to run advanced ML such as YOLOv8 algorithms directly on an HMD will become necessary for the emerging edge-based virtual reality applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> and the <span id="S7.p2.1.1" class="ltx_text ltx_font_italic">Metaverse</span> concept <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>. For a wide range of practical applications, particularly educational <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> and medical <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, the VR and AR technologies need to converge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, providing the users with an ability to seamlessly interact with both real and virtual elements of the surroundings.</p>
</div>
<div id="S7.p3" class="ltx_para">
<p id="S7.p3.1" class="ltx_p">To that end, our experiments show the universal path that can be taken to ensure real-time operation by finding the best trade-off between the neural network model size and the input image size. In the presented case of an AR headset, we are forced to reduce the input image sizes to <math id="S7.p3.1.m1.1" class="ltx_Math" alttext="160\times 160" display="inline"><semantics id="S7.p3.1.m1.1a"><mrow id="S7.p3.1.m1.1.1" xref="S7.p3.1.m1.1.1.cmml"><mn id="S7.p3.1.m1.1.1.2" xref="S7.p3.1.m1.1.1.2.cmml">160</mn><mo lspace="0.222em" rspace="0.222em" id="S7.p3.1.m1.1.1.1" xref="S7.p3.1.m1.1.1.1.cmml">×</mo><mn id="S7.p3.1.m1.1.1.3" xref="S7.p3.1.m1.1.1.3.cmml">160</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.p3.1.m1.1b"><apply id="S7.p3.1.m1.1.1.cmml" xref="S7.p3.1.m1.1.1"><times id="S7.p3.1.m1.1.1.1.cmml" xref="S7.p3.1.m1.1.1.1"></times><cn type="integer" id="S7.p3.1.m1.1.1.2.cmml" xref="S7.p3.1.m1.1.1.2">160</cn><cn type="integer" id="S7.p3.1.m1.1.1.3.cmml" xref="S7.p3.1.m1.1.1.3">160</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.p3.1.m1.1c">160\times 160</annotation></semantics></math> pixels, still obtaining satisfactory results from the perspective of AR applications mainly within the 2.5 [m] range for an object of interest detection. Such distance span is well-aligned with the typical operational range of AR use case scenarios <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
<div id="S7.p4" class="ltx_para">
<p id="S7.p4.1" class="ltx_p">The analysis of processing times (see Tab. <a href="#S6.T1" title="TABLE I ‣ VI-A Processing time analysis ‣ VI Ablation study ‣ Real-Time Onboard Object Detection for Augmented Reality: Enhancing Head-Mounted Display with YOLOv8 The study has been supported by funding provided through an unrestricted gift by Meta." class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>) reveals that further improvements should be sought in the inference itself, as preprocessing, postprocessing, or data copying take little time. Beyond that, to boost the number of possible use cases of the proposed solution, we also show that slicing a view into multiple images processed in a single batch can be a sensible approach when we are dealing with situations where wider FoV is used.</p>
</div>
<div id="S7.p5" class="ltx_para">
<p id="S7.p5.1" class="ltx_p">In our future work, we plan to tackle the missing FP16 support to reduce inference times further. In addition, we will also incorporate tracking into object detection to extend the possible applications of the presented framework.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
T. P. Caudell and D. W. Mizell, “Augmented reality: an application of heads-up display technology to manual manufacturing processes,” 25th Hawaii Int. Conf. on System Sciences, Kauai, HI, USA, 1992, vol. 2, pp. 659–669.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
J. Y. Mambu, E. Anderson, A. Wahyudi, G. Keyeh and B. Dajoh, “Blind Reader: An Object Identification Mobile-based Application for the Blind using Augmented Reality Detection,” 2019 Int. Conf. on Cybernetics and Intelligent System, Denpasar, Indonesia, 2019, pp. 138-141.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
S. K. Tadeja, D. Janik, P. Stachura, M. Tomecki, K. Książczak and K. Walas, “MARS: A Cross-Platform Mobile AR System for Remote Collaborative Instruction and Installation Support using Digital Twins,” 2022 IEEE Conf. on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW), Christchurch, New Zealand, 2022, pp. 373-380.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
S. K. Tadeja, D. Janik, P. Stachura, M. Tomecki and K. Walas, “Design of ARQ: An Augmented Reality System for Assembly Training Enhanced with QR-Tagging and 3D Engineering Asset Model,” 2022 IEEE Conf. on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW), Christchurch, New Zealand, 2022, pp. 466-471.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
L. O. Solari Bozzi, K. Samson, S. Tadeja, S. Pattinson, and T. Bohné, “Towards Augmented Reality Guiding Systems: An Engineering Design of an Immersive System for Complex 3D Printing Repair Process,” 2023 IEEE Conf. on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW), Shanghai, China, 2023, pp. 384-389.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
A. Seifert and A. Schlomann, “The Use of Virtual and Augmented Reality by Older Adults: Potentials and Challenges”, Front. Virtual Real. 2:639718. 2021.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
A. Syberfeldt, M. Holm, O. Danielsson, L. Wang, and R. Brewster, “Support systems on the industrial shop-floors of the future – operators’ perspective on augmented reality”, CIRP, 2016, 44, pp. 108–113.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Microsoft, HoloLens 2, [online] https://www.microsoft.com/en-us/HoloLens/hardware

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
M. A. Latif Sarker and D. Seog Han, “Human-Centric Autonomous Driving Based on a Two-Stage Machine Learning Algorithm,” 2022 27th Asia Pacific Conf. on Communications (APCC), Jeju Island, Korea, Republic of, 2022, pp. 334-3335.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
A. Seeliger, R. Weibel, S. Feuerriegel, “Context-Adaptive Visual Cues for Safe Navigation in Augmented Reality Using Machine Learning”,
Int. Journal of Human–Computer Interaction, 1044-7318, 2022.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
J. Redmon, S. Divvala, R. Girshick and A. Farhadi, “You Only Look Once: Unified, Real-Time Object Detection,” 2016 IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, USA, 2016, pp. 779-788.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
K. Roszyk, K.; Nowicki, M.R.; Skrzypczyński, P. "Adopting the YOLOv4 Architecture for Low-Latency Multispectral Pedestrian Detection in Autonomous Driving", Sensors, 22, 1082, 2022.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
A. Farasin, F. Peciarolo, M. Grangetto, E. Gianaria, P. Garza,
“Real-time object detection and tracking in mixed reality using microsoft HoloLens”, 15th Int. Joint Conf. on Computer Vision, Imaging and Computer Graphics Theory and Applications, 2020, Vol. 4, 165–172.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
R. Goka et al., “Development of Tomato Harvest Support System Using Mixed Reality Head Mounted Display,” 2022 IEEE 4th Global Conf. on Life Sciences and Technologies, Osaka, Japan, 2022, pp. 167–169.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
M. Eckert, M. Blex, C. M. Friedrich, “Object detection featuring 3D audio localization for Microsoft HoloLens”, in Proc. 11th Int. Joint Conf. on Biomedical Eng. Sys. and Techn., 2018, Vol. 5, pp. 555–561.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
R. Anderson, J. Toledo and H. ElAarag, “Feasibility Study on the Utilization of Microsoft HoloLens to Increase Driving Conditions Awareness,” 2019 SoutheastCon, Huntsville, AL, USA, 2019, pp. 1-8.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Microsoft, Azure Custom Vision, [online] <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://learn.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/</span>

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
A. Ibrahim, J. B. Lanier, B. Huynh, J. O’Donovan, T. Höllerer,
“Real-time Object Recognition on the Microsoft HoloLens”, technical report, UC Santa Barbara, 2017.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
H. Bahri, D. Krčmařík, J. Kočí,
“Accurate object detection system on HoloLens using yolo algorithm”,
in 2019 Int. Conf. on Control, Artificial Intelligence, Robotics &amp; Optimization (ICCAIRO), 2018, pp. 219–224.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Y. Ghasemi, H. Jeong, S. H. Choi, K.-B. Park, J. Y. Lee,
“Deep learning-based object detection in augmented reality: A systematic review”,
Computers in Industry, 139, 103661, 2022.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
K. Malek, A. Mohammadkhorasani, F. Moreu,
“Methodology to integrate augmented reality and pattern recognition for crack detection”,
Computer-Aided Civil and Infrastructure Engineering (2022).

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
PTC, Vuforia Engine Library, [online] <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://library.vuforia.com/</span>

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
EasyAR Library, [online] <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.easyar.com/</span>

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Y. Guan, X, Hou, N, Wu, B, Han, and T, Han,
“DeepMix: mobility-aware, lightweight, and hybrid 3D object detection for headsets”, 20th Int. Conf. on Mobile Systems, Applications and Services, 2022.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
K. Chen, T. Li, H.-S. Kim, D. E. Culler, and R. H. Katz. “MARVEL: Enabling Mobile Augmented Reality with Low Energy and Low Latency”. In Proc. of ACM Conference on Embedded
Networked Sensor Systems (SenSys), 2018.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
L. Liu, H. Li, and M. Gruteser. “Edge Assisted Real-time Object
Detection for Mobile Augmented Reality”, 25th Annual Int. Conf. on Mobile Computing and
Networking (MobiCom), 2019.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
ONNX Runtime developers, ONNX Runtime, [online] <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://onnxruntime.ai/</span>

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Unity, Barracuda Library, [online] <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://docs.unity3d.com/</span>

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Unity, Unity Game Engine, [online] <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://unity.com/</span>

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
M. Xu et al., “A Full Dive Into Realizing the Edge-Enabled Metaverse: Visions, Enabling Technologies, and Challenges,” in IEEE Communications Surveys &amp; Tutorials, vol. 25, no. 1, pp. 656–700, 2023.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
R. Padilla, S. L. Netto and E. A. B. da Silva, “A Survey on Performance Metrics for Object-Detection Algorithms,” 2020 Int. Conf. on Systems, Signals and Image Processing, Niteroi, Brazil, 2020, pp. 237-242.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
S. S. A. Zaidi, M. S. Ansari, A. Aslam, N. Kanwal, M. Asghar, B. Lee, “A survey of modern deep learning based object detection models”, Digital Signal Processing, pp. 103514, 2022.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Z. Zou, K. Chen, Z. Shi, Y. Guo and J. Ye, “Object Detection in 20 Years: A Survey,” in Proc. of the IEEE, vol. 111, no. 3, pp. 257-276, March 2023.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
PyTorch Foundation, PyTorch, [online] <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://pytorch.org/</span>

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
EnoxSoftware, HoloLensCameraStream, Unity plugin, [online]
<span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/EnoxSoftware/HoloLensCameraStream</span>

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
D. Lowe, (2004). Distinctive Image Features from Scale-Invariant Keypoints. Int. Journal of Computer Vision. 60. 91.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
L. Tsung-Yi, M. Maire, S. J. Belongie, L. D. Bourdev, R. B. Girshick, J. Hays, et al. (2014). “Microsoft COCO: Common Objects in Context”, CoRR, abs/1405.0312.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
I. Athanasiadis, P. Mousouliotis, and L. Petrou, Loukas, “A Framework of Transfer Learning in Object Detection for Embedded Systems”, arXiv:1811.04863, 2018.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Unity, Unity Barracuda Backend, [online] <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://docs.unity3d.com/Packages/com.unity.barracuda@3.0/api/Unity.Barracuda.WorkerFactory.Type.html</span>

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
N. -M. Ho and W. -F. Wong, “Exploiting half precision arithmetic in Nvidia GPUs,” 2017 IEEE High Performance Extreme Computing Conference (HPEC), Waltham, MA, USA, 2017, pp. 1-7.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
A. Haidar, S. Tomov, J. Dongarra and N. J. Higham, “Harnessing GPU Tensor Cores for Fast FP16 Arithmetic to Speed up Mixed-Precision Iterative Refinement Solvers,” SC18: Int. Conf. for HPC, Networking, Storage and Analysis, Dallas, TX, USA, 2018, pp. 603-613.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
J. Lee, J. Bang and S. -I. Yang, “Object detection with sliding window in images including multiple similar objects,” 2017 Int. Conf. on Information and Communication Technology Convergence (ICTC), Jeju, Korea (South), 2017, pp. 803-806.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
T. Ribeiro, et al., “Virtual Reality Solutions Employing Artificial Intelligence Methods: A Systematic Literature Review”, ACM Comput. Surv. 55, 10, Article 214, 2023.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
M. Igras-Cybulska, D. Hekiert, A. Cybulski, S. Tadeja, et al. (2023). “Towards Multimodal VR Trainer of Voice Emission and Public Speaking: Work-in-Progress,” 2023 IEEE Conf. on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW), Shanghai, China, 2023.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
P. Zikas et al., “MAGES 4.0: Accelerating the World’s Transition to VR Training and Democratizing the Authoring of the Medical Metaverse,” in IEEE CG&amp;A, vol. 43, no. 2, pp. 43-56, 2023.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
K. Li, et al., “When Internet of Things Meets Metaverse: Convergence of Physical and Cyber Worlds,” in IEEE Internet of Things Journal, vol. 10, no. 5, pp. 4148–4173, 2023.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2306.03536" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2306.03537" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2306.03537">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2306.03537" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2306.03538" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 02:13:07 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
