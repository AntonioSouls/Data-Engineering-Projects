<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2112.04981] PE-former: Pose Estimation Transformer</title><meta property="og:description" content="Vision transformer architectures have been demonstrated to work very effectively for image classification tasks. Efforts to solve more challenging vision tasks with transformers rely on convolutional backbones for feat…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="PE-former: Pose Estimation Transformer">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="PE-former: Pose Estimation Transformer">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2112.04981">

<!--Generated on Fri Mar  1 15:41:39 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Vision Transformers Human Pose estimation.">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Institute of Computer Science, FORTH, Heraklion, Crete, Greece </span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Computer Science Department, University of Crete, Greece
<span id="id2.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">email: </span>{padeler,argyros}@ics.forth.gr</span></span></span>
<br class="ltx_break"><a target="_blank" href="https://www.ics.forth.gr/hccv/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.ics.forth.gr/hccv/</a></span></span></span>
<h1 class="ltx_title ltx_title_document">PE-former: Pose Estimation Transformer</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Paschalis Panteleris
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Antonis Argyros
</span><span class="ltx_author_notes">1122</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Vision transformer architectures have been demonstrated to work very effectively for image classification tasks. Efforts to solve more challenging vision tasks with transformers rely on convolutional backbones for feature extraction. In this paper we investigate the use of a pure transformer architecture (i.e., one with no CNN backbone) for the problem of 2D body pose estimation.
We evaluate two ViT architectures on the COCO dataset. We demonstrate that using an encoder-decoder transformer architecture yields state of the art results on this estimation problem.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Vision Transformers Human Pose estimation.
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In recent years, transformers have been gaining ground versus traditional convolutional neural networks in a number of computer vision tasks. The work of Dosovitskiy et al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> demonstrated the use of the pure transformer model for image classification. This was followed by a number of recent papers demonstrating improved performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>,
reduced computational requirements <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>
or both. All of these pure transformer models are applied only to the task of image classification.
For more challenging vision tasks such as 2D human pose estimation, recent methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> use a combination of a convolutional backbone followed by a transformer front-end to achieve similar performance to that of convolutional architectures.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In this work, we investigate the use of a pure transformer-based architecture for the task of 2D human pose estimation. More specifically, we focus on single instance, direct coordinate regression of the human body keypoints. We evaluate an encoder-decoder architecture derived from the work of Carion et al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. In our model we eliminate the need for a convolutional based backbone
(a ResNet in the case of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>) and we replace the encoder part of the transformer with a vision-based <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> transformer encoder.
By foregoing with the need of a convolutional backbone, our model is realized by a much simpler architecture.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">For the encoder part, we investigate the use of two vision transformer architectures:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Deit<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>: ViT architecture trained with distillation, which exhibits
baseline performance for vision transformers.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Xcit<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>: Cross-covariance transformers. Xcit reduces the computational requirements by transposing the way ViT operates, i.e., instead of attending on tokens the transformer attends on the token-channels.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">When comparing to other regression based methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>
which use a combination of transformers and convolutional backbones,
our Deit-based model performs on par with similar sized (in million parameters) models.
Moreover, our best model using an Xcit-based encoder
achieves state of the art results on the COCO dataset.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">We compare our transformer-only architecture with two “baselines”. One that uses
a ResNet50 as the encoder section and another that uses a ViT (encoder+decoder) feature extractor.
The architecture and details of these baselines are discussed in Section <a href="#S3" title="3 Method ‣ PE-former: Pose Estimation Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
We demonstrate experimentally that against these baselines, when everything else is kept the same, the transformer encoder improves performance.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Finally, we further improve the performance of our models using unsupervised pre-training, as proposed
by Caron et al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. The training results of models that use supervised and unsupervised pre-training are presented in Section <a href="#S4" title="4 Experiments ‣ PE-former: Pose Estimation Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">In summary, the contributions of our work<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Code is available on <a target="_blank" href="https://github.com/padeler/PE-former" title="" class="ltx_ref ltx_href">https://github.com/padeler/PE-former</a></span></span></span> are the following:</p>
<ul id="S1.I2" class="ltx_itemize">
<li id="S1.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i1.p1" class="ltx_para">
<p id="S1.I2.i1.p1.1" class="ltx_p">We propose a novel architecture for 2D human pose estimation that is using vision transformers
without the need for a CNN backbone for feature extraction.</p>
</div>
</li>
<li id="S1.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i2.p1" class="ltx_para">
<p id="S1.I2.i2.p1.1" class="ltx_p">We demonstrate that our proposed architecture outperforms the SOTA on public
datasets by as much as <math id="S1.I2.i2.p1.1.m1.1" class="ltx_Math" alttext="4.4\%" display="inline"><semantics id="S1.I2.i2.p1.1.m1.1a"><mrow id="S1.I2.i2.p1.1.m1.1.1" xref="S1.I2.i2.p1.1.m1.1.1.cmml"><mn id="S1.I2.i2.p1.1.m1.1.1.2" xref="S1.I2.i2.p1.1.m1.1.1.2.cmml">4.4</mn><mo id="S1.I2.i2.p1.1.m1.1.1.1" xref="S1.I2.i2.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.I2.i2.p1.1.m1.1b"><apply id="S1.I2.i2.p1.1.m1.1.1.cmml" xref="S1.I2.i2.p1.1.m1.1.1"><csymbol cd="latexml" id="S1.I2.i2.p1.1.m1.1.1.1.cmml" xref="S1.I2.i2.p1.1.m1.1.1.1">percent</csymbol><cn type="float" id="S1.I2.i2.p1.1.m1.1.1.2.cmml" xref="S1.I2.i2.p1.1.m1.1.1.2">4.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.I2.i2.p1.1.m1.1c">4.4\%</annotation></semantics></math> in AP (compared to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>).</p>
</div>
</li>
<li id="S1.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i3.p1" class="ltx_para">
<p id="S1.I2.i3.p1.1" class="ltx_p">We evaluate the performance of different vision transformer encoders for the
task of 2D human pose estimation.</p>
</div>
</li>
<li id="S1.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i4.p1" class="ltx_para">
<p id="S1.I2.i4.p1.1" class="ltx_p">We demonstrate that the unsupervised pretraining of our transformer-based models, improves their performance by <math id="S1.I2.i4.p1.1.m1.1" class="ltx_Math" alttext="0.5\%" display="inline"><semantics id="S1.I2.i4.p1.1.m1.1a"><mrow id="S1.I2.i4.p1.1.m1.1.1" xref="S1.I2.i4.p1.1.m1.1.1.cmml"><mn id="S1.I2.i4.p1.1.m1.1.1.2" xref="S1.I2.i4.p1.1.m1.1.1.2.cmml">0.5</mn><mo id="S1.I2.i4.p1.1.m1.1.1.1" xref="S1.I2.i4.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.I2.i4.p1.1.m1.1b"><apply id="S1.I2.i4.p1.1.m1.1.1.cmml" xref="S1.I2.i4.p1.1.m1.1.1"><csymbol cd="latexml" id="S1.I2.i4.p1.1.m1.1.1.1.cmml" xref="S1.I2.i4.p1.1.m1.1.1.1">percent</csymbol><cn type="float" id="S1.I2.i4.p1.1.m1.1.1.2.cmml" xref="S1.I2.i4.p1.1.m1.1.1.2">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.I2.i4.p1.1.m1.1c">0.5\%</annotation></semantics></math> (for the Xcit model).</p>
</div>
</li>
</ul>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2112.04981/assets/x1.png" id="S1.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="328" height="222" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>

The DETR model adapted for 2D human pose estimation (top) and
our proposed model using Visual transformers (bottom).
In our model, the cropped image of a person (bottom left), is split into <math id="S1.F1.10.9.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S1.F1.10.9.m1.1b"><mi id="S1.F1.10.9.m1.1.1" xref="S1.F1.10.9.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S1.F1.10.9.m1.1c"><ci id="S1.F1.10.9.m1.1.1.cmml" xref="S1.F1.10.9.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.10.9.m1.1d">N</annotation></semantics></math> tokens of <math id="S1.F1.11.10.m2.1" class="ltx_Math" alttext="p\times p" display="inline"><semantics id="S1.F1.11.10.m2.1b"><mrow id="S1.F1.11.10.m2.1.1" xref="S1.F1.11.10.m2.1.1.cmml"><mi id="S1.F1.11.10.m2.1.1.2" xref="S1.F1.11.10.m2.1.1.2.cmml">p</mi><mo lspace="0.222em" rspace="0.222em" id="S1.F1.11.10.m2.1.1.1" xref="S1.F1.11.10.m2.1.1.1.cmml">×</mo><mi id="S1.F1.11.10.m2.1.1.3" xref="S1.F1.11.10.m2.1.1.3.cmml">p</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.F1.11.10.m2.1c"><apply id="S1.F1.11.10.m2.1.1.cmml" xref="S1.F1.11.10.m2.1.1"><times id="S1.F1.11.10.m2.1.1.1.cmml" xref="S1.F1.11.10.m2.1.1.1"></times><ci id="S1.F1.11.10.m2.1.1.2.cmml" xref="S1.F1.11.10.m2.1.1.2">𝑝</ci><ci id="S1.F1.11.10.m2.1.1.3.cmml" xref="S1.F1.11.10.m2.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.11.10.m2.1d">p\times p</annotation></semantics></math> pixels
(typically <math id="S1.F1.12.11.m3.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S1.F1.12.11.m3.1b"><mi id="S1.F1.12.11.m3.1.1" xref="S1.F1.12.11.m3.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S1.F1.12.11.m3.1c"><ci id="S1.F1.12.11.m3.1.1.cmml" xref="S1.F1.12.11.m3.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.12.11.m3.1d">p</annotation></semantics></math> is equal to 8 or 16 pixels). The tokens are processed by the transformer encoder
(can be Deit or Xcit based). The output feature tokens (<math id="S1.F1.13.12.m4.1" class="ltx_Math" alttext="N+1" display="inline"><semantics id="S1.F1.13.12.m4.1b"><mrow id="S1.F1.13.12.m4.1.1" xref="S1.F1.13.12.m4.1.1.cmml"><mi id="S1.F1.13.12.m4.1.1.2" xref="S1.F1.13.12.m4.1.1.2.cmml">N</mi><mo id="S1.F1.13.12.m4.1.1.1" xref="S1.F1.13.12.m4.1.1.1.cmml">+</mo><mn id="S1.F1.13.12.m4.1.1.3" xref="S1.F1.13.12.m4.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.F1.13.12.m4.1c"><apply id="S1.F1.13.12.m4.1.1.cmml" xref="S1.F1.13.12.m4.1.1"><plus id="S1.F1.13.12.m4.1.1.1.cmml" xref="S1.F1.13.12.m4.1.1.1"></plus><ci id="S1.F1.13.12.m4.1.1.2.cmml" xref="S1.F1.13.12.m4.1.1.2">𝑁</ci><cn type="integer" id="S1.F1.13.12.m4.1.1.3.cmml" xref="S1.F1.13.12.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.13.12.m4.1d">N+1</annotation></semantics></math> for the class token) are used
as the memory tokens in the DETR based transformer decoder. The decoder input is
<math id="S1.F1.14.13.m5.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S1.F1.14.13.m5.1b"><mi id="S1.F1.14.13.m5.1.1" xref="S1.F1.14.13.m5.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S1.F1.14.13.m5.1c"><ci id="S1.F1.14.13.m5.1.1.cmml" xref="S1.F1.14.13.m5.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.14.13.m5.1d">M</annotation></semantics></math> joint queries. <math id="S1.F1.15.14.m6.1" class="ltx_Math" alttext="M=100" display="inline"><semantics id="S1.F1.15.14.m6.1b"><mrow id="S1.F1.15.14.m6.1.1" xref="S1.F1.15.14.m6.1.1.cmml"><mi id="S1.F1.15.14.m6.1.1.2" xref="S1.F1.15.14.m6.1.1.2.cmml">M</mi><mo id="S1.F1.15.14.m6.1.1.1" xref="S1.F1.15.14.m6.1.1.1.cmml">=</mo><mn id="S1.F1.15.14.m6.1.1.3" xref="S1.F1.15.14.m6.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.F1.15.14.m6.1c"><apply id="S1.F1.15.14.m6.1.1.cmml" xref="S1.F1.15.14.m6.1.1"><eq id="S1.F1.15.14.m6.1.1.1.cmml" xref="S1.F1.15.14.m6.1.1.1"></eq><ci id="S1.F1.15.14.m6.1.1.2.cmml" xref="S1.F1.15.14.m6.1.1.2">𝑀</ci><cn type="integer" id="S1.F1.15.14.m6.1.1.3.cmml" xref="S1.F1.15.14.m6.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.15.14.m6.1d">M=100</annotation></semantics></math> in our experiments. The Decoder outputs <math id="S1.F1.16.15.m7.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S1.F1.16.15.m7.1b"><mi id="S1.F1.16.15.m7.1.1" xref="S1.F1.16.15.m7.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S1.F1.16.15.m7.1c"><ci id="S1.F1.16.15.m7.1.1.cmml" xref="S1.F1.16.15.m7.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.16.15.m7.1d">M</annotation></semantics></math> prediction tokens which are
processed by a classification and a regression head (FFNs). The output is 2D joint locations (range <math id="S1.F1.17.16.m8.2" class="ltx_Math" alttext="[0,1]" display="inline"><semantics id="S1.F1.17.16.m8.2b"><mrow id="S1.F1.17.16.m8.2.3.2" xref="S1.F1.17.16.m8.2.3.1.cmml"><mo stretchy="false" id="S1.F1.17.16.m8.2.3.2.1" xref="S1.F1.17.16.m8.2.3.1.cmml">[</mo><mn id="S1.F1.17.16.m8.1.1" xref="S1.F1.17.16.m8.1.1.cmml">0</mn><mo id="S1.F1.17.16.m8.2.3.2.2" xref="S1.F1.17.16.m8.2.3.1.cmml">,</mo><mn id="S1.F1.17.16.m8.2.2" xref="S1.F1.17.16.m8.2.2.cmml">1</mn><mo stretchy="false" id="S1.F1.17.16.m8.2.3.2.3" xref="S1.F1.17.16.m8.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.F1.17.16.m8.2c"><interval closure="closed" id="S1.F1.17.16.m8.2.3.1.cmml" xref="S1.F1.17.16.m8.2.3.2"><cn type="integer" id="S1.F1.17.16.m8.1.1.cmml" xref="S1.F1.17.16.m8.1.1">0</cn><cn type="integer" id="S1.F1.17.16.m8.2.2.cmml" xref="S1.F1.17.16.m8.2.2">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.17.16.m8.2d">[0,1]</annotation></semantics></math>) and class predictions.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Vision Transformers</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The success of transformers in Natural Language Processing (NLP) has motivated a lot of researchers to adopt transformers for the solution of vision tasks. While attention-based mechanisms acting on image features such as the ones produced from a CNN, have been around for many years, only recently,
Dosovitskiy et al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> demonstrated successfully with ViT the
use of transformers for image feature extraction, replacing the use of CNNs for
the task of image classification.
While the ViT approach is very promising, it still suffers from issues that arise from the
quadratic complexity of the attention mechanism and translates to heavy computational and memory requirements.
Additionally, the vanilla ViT models and, especially, their larger variants, are very hard to train and require to be trained on huge annotated datasets.
Following ViT, many methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
appeared trying to solve or circumvent these issues while also
maintaining SOTA performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.
In this work, we incorporate two very promising architectures,
DEIT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and Xcit <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> in our models
and evaluate their performance.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Transformers for pose estimation</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Attention-based mechanisms have been applied to tackle demanding vision tasks such as 2D human pose estimation. However, these rely on CNN backbones for feature extraction.
The addition of an attention mechanism enables methods such as the one proposed by Li et al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> to achieve state of the art results, improving on the best CNN based methods such as the HRNet by Sun et al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.
However, Li et al Li et al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> base their model and build on an HRNet CNN backbone. Similarly, methods such as
TFPose by Mao et al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, POET by Stoffl et al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and
Transpose by Yang et al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, build on a robust CNN backbone and
apply the attention mechanism on the extracted image features.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">In this work, similarly to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, we use the transformer-based decoder module
and the bipartite matching technique of Carion et al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. However, instead of relying on a CNN backbone or encoder for image features extraction, we introduce an architecture that directly uses the output features from a vision transformer.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.2" class="ltx_p">As shown in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ PE-former: Pose Estimation Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> (bottom row), our architecture consists of two major components: A Visual Transformer encoder, and a Transformer decoder. The input image is initially converted into tokens following the ViT paradigm.
A position embedding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> is used to help retain the patch-location information.
The tokens and the position embedding are used as input to transformer encoder.
The transformed tokens are used as the memory <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> input of the transformer decoder.
The inputs of the decoder are <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.p1.1.m1.1a"><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">M</annotation></semantics></math> learned queries.
For each query the network produces a joint prediction.
The output tokens from the transformer decoder are passed through two heads. The heads are feed forward neural networks (FFNs) following the architecture of DETR.
The first is a classification head used to predict the joint type (i.e., class) of each query.
The second is a regression head that predicts the normalized coordinates in the range <math id="S3.p1.2.m2.2" class="ltx_Math" alttext="[0,1]" display="inline"><semantics id="S3.p1.2.m2.2a"><mrow id="S3.p1.2.m2.2.3.2" xref="S3.p1.2.m2.2.3.1.cmml"><mo stretchy="false" id="S3.p1.2.m2.2.3.2.1" xref="S3.p1.2.m2.2.3.1.cmml">[</mo><mn id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">0</mn><mo id="S3.p1.2.m2.2.3.2.2" xref="S3.p1.2.m2.2.3.1.cmml">,</mo><mn id="S3.p1.2.m2.2.2" xref="S3.p1.2.m2.2.2.cmml">1</mn><mo stretchy="false" id="S3.p1.2.m2.2.3.2.3" xref="S3.p1.2.m2.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.2b"><interval closure="closed" id="S3.p1.2.m2.2.3.1.cmml" xref="S3.p1.2.m2.2.3.2"><cn type="integer" id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">0</cn><cn type="integer" id="S3.p1.2.m2.2.2.cmml" xref="S3.p1.2.m2.2.2">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.2c">[0,1]</annotation></semantics></math>
of the joint in the input image.
Predictions that do not correspond to joints are mapped to a “non object” class.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">In Section <a href="#S3.SS1" title="3.1 Transformer Encoder ‣ 3 Method ‣ PE-former: Pose Estimation Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a> we discuss the details of our encoder module and
we present the architecture of the various encoders that we examined. In Section <a href="#S3.SS2" title="3.2 Transformer Decoder ‣ 3 Method ‣ PE-former: Pose Estimation Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a> we present the details of the decoder module which is
derived from the DETR decoder. Finally, in Section <a href="#S3.SS3" title="3.3 Training ‣ 3 Method ‣ PE-former: Pose Estimation Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>,
we discuss the training techniques and hyperparameters used for the experiments.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Transformer Encoder</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.9" class="ltx_p">Dosovitskiy et al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> proposed the use of image patches which, in the case of ViT, have a size of <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="16\times 16" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mn id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">16</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.1.m1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">16</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><times id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">16</cn><cn type="integer" id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">16\times 16</annotation></semantics></math>. This approach of splitting the image into patches
is adopted by all the vision transformer methods that appeared since ViT.
In general, an input image of dimensions <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="W\times H" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mrow id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">W</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.2.m2.1.1.1" xref="S3.SS1.p1.2.m2.1.1.1.cmml">×</mo><mi id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">H</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><times id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1"></times><ci id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">𝑊</ci><ci id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3">𝐻</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">W\times H</annotation></semantics></math> pixels, is split into <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="n\times m" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mrow id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.3.m3.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.cmml">×</mo><mi id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><times id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1"></times><ci id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2">𝑛</ci><ci id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">n\times m</annotation></semantics></math> patches, each of which is a square block of <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="p\times p" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><mrow id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml"><mi id="S3.SS1.p1.4.m4.1.1.2" xref="S3.SS1.p1.4.m4.1.1.2.cmml">p</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.4.m4.1.1.1" xref="S3.SS1.p1.4.m4.1.1.1.cmml">×</mo><mi id="S3.SS1.p1.4.m4.1.1.3" xref="S3.SS1.p1.4.m4.1.1.3.cmml">p</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><apply id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1"><times id="S3.SS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1.1"></times><ci id="S3.SS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2">𝑝</ci><ci id="S3.SS1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">p\times p</annotation></semantics></math> pixels.
These patches are flattened into vectors (tokens) and are subsequently passed through the transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> layers. The self-attention mechanism operates on an input matrix <math id="S3.SS1.p1.5.m5.1" class="ltx_Math" alttext="X\in R^{N\times d}" display="inline"><semantics id="S3.SS1.p1.5.m5.1a"><mrow id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml"><mi id="S3.SS1.p1.5.m5.1.1.2" xref="S3.SS1.p1.5.m5.1.1.2.cmml">X</mi><mo id="S3.SS1.p1.5.m5.1.1.1" xref="S3.SS1.p1.5.m5.1.1.1.cmml">∈</mo><msup id="S3.SS1.p1.5.m5.1.1.3" xref="S3.SS1.p1.5.m5.1.1.3.cmml"><mi id="S3.SS1.p1.5.m5.1.1.3.2" xref="S3.SS1.p1.5.m5.1.1.3.2.cmml">R</mi><mrow id="S3.SS1.p1.5.m5.1.1.3.3" xref="S3.SS1.p1.5.m5.1.1.3.3.cmml"><mi id="S3.SS1.p1.5.m5.1.1.3.3.2" xref="S3.SS1.p1.5.m5.1.1.3.3.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.5.m5.1.1.3.3.1" xref="S3.SS1.p1.5.m5.1.1.3.3.1.cmml">×</mo><mi id="S3.SS1.p1.5.m5.1.1.3.3.3" xref="S3.SS1.p1.5.m5.1.1.3.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><apply id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1"><in id="S3.SS1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1.1"></in><ci id="S3.SS1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1.2">𝑋</ci><apply id="S3.SS1.p1.5.m5.1.1.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.3.1.cmml" xref="S3.SS1.p1.5.m5.1.1.3">superscript</csymbol><ci id="S3.SS1.p1.5.m5.1.1.3.2.cmml" xref="S3.SS1.p1.5.m5.1.1.3.2">𝑅</ci><apply id="S3.SS1.p1.5.m5.1.1.3.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3.3"><times id="S3.SS1.p1.5.m5.1.1.3.3.1.cmml" xref="S3.SS1.p1.5.m5.1.1.3.3.1"></times><ci id="S3.SS1.p1.5.m5.1.1.3.3.2.cmml" xref="S3.SS1.p1.5.m5.1.1.3.3.2">𝑁</ci><ci id="S3.SS1.p1.5.m5.1.1.3.3.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3.3.3">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">X\in R^{N\times d}</annotation></semantics></math>, where <math id="S3.SS1.p1.6.m6.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS1.p1.6.m6.1a"><mi id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><ci id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">N</annotation></semantics></math> is the number of tokens. Each token has <math id="S3.SS1.p1.7.m7.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S3.SS1.p1.7.m7.1a"><mi id="S3.SS1.p1.7.m7.1.1" xref="S3.SS1.p1.7.m7.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.1b"><ci id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.1c">d</annotation></semantics></math> dimensions.
The input <math id="S3.SS1.p1.8.m8.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S3.SS1.p1.8.m8.1a"><mi id="S3.SS1.p1.8.m8.1.1" xref="S3.SS1.p1.8.m8.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m8.1b"><ci id="S3.SS1.p1.8.m8.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m8.1c">X</annotation></semantics></math> is linearly projected to queries, keys and values (<math id="S3.SS1.p1.9.m9.3" class="ltx_Math" alttext="Q,K,V" display="inline"><semantics id="S3.SS1.p1.9.m9.3a"><mrow id="S3.SS1.p1.9.m9.3.4.2" xref="S3.SS1.p1.9.m9.3.4.1.cmml"><mi id="S3.SS1.p1.9.m9.1.1" xref="S3.SS1.p1.9.m9.1.1.cmml">Q</mi><mo id="S3.SS1.p1.9.m9.3.4.2.1" xref="S3.SS1.p1.9.m9.3.4.1.cmml">,</mo><mi id="S3.SS1.p1.9.m9.2.2" xref="S3.SS1.p1.9.m9.2.2.cmml">K</mi><mo id="S3.SS1.p1.9.m9.3.4.2.2" xref="S3.SS1.p1.9.m9.3.4.1.cmml">,</mo><mi id="S3.SS1.p1.9.m9.3.3" xref="S3.SS1.p1.9.m9.3.3.cmml">V</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.9.m9.3b"><list id="S3.SS1.p1.9.m9.3.4.1.cmml" xref="S3.SS1.p1.9.m9.3.4.2"><ci id="S3.SS1.p1.9.m9.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1">𝑄</ci><ci id="S3.SS1.p1.9.m9.2.2.cmml" xref="S3.SS1.p1.9.m9.2.2">𝐾</ci><ci id="S3.SS1.p1.9.m9.3.3.cmml" xref="S3.SS1.p1.9.m9.3.3">𝑉</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.9.m9.3c">Q,K,V</annotation></semantics></math>).
Keys and values are used to compute an attention map</p>
<table id="S3.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex1.m1.3" class="ltx_Math" alttext="A(K,Q)=Softmax(QK^{T}/\sqrt{d})." display="block"><semantics id="S3.Ex1.m1.3a"><mrow id="S3.Ex1.m1.3.3.1" xref="S3.Ex1.m1.3.3.1.1.cmml"><mrow id="S3.Ex1.m1.3.3.1.1" xref="S3.Ex1.m1.3.3.1.1.cmml"><mrow id="S3.Ex1.m1.3.3.1.1.3" xref="S3.Ex1.m1.3.3.1.1.3.cmml"><mi id="S3.Ex1.m1.3.3.1.1.3.2" xref="S3.Ex1.m1.3.3.1.1.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.3.3.1.1.3.1" xref="S3.Ex1.m1.3.3.1.1.3.1.cmml">​</mo><mrow id="S3.Ex1.m1.3.3.1.1.3.3.2" xref="S3.Ex1.m1.3.3.1.1.3.3.1.cmml"><mo stretchy="false" id="S3.Ex1.m1.3.3.1.1.3.3.2.1" xref="S3.Ex1.m1.3.3.1.1.3.3.1.cmml">(</mo><mi id="S3.Ex1.m1.1.1" xref="S3.Ex1.m1.1.1.cmml">K</mi><mo id="S3.Ex1.m1.3.3.1.1.3.3.2.2" xref="S3.Ex1.m1.3.3.1.1.3.3.1.cmml">,</mo><mi id="S3.Ex1.m1.2.2" xref="S3.Ex1.m1.2.2.cmml">Q</mi><mo stretchy="false" id="S3.Ex1.m1.3.3.1.1.3.3.2.3" xref="S3.Ex1.m1.3.3.1.1.3.3.1.cmml">)</mo></mrow></mrow><mo id="S3.Ex1.m1.3.3.1.1.2" xref="S3.Ex1.m1.3.3.1.1.2.cmml">=</mo><mrow id="S3.Ex1.m1.3.3.1.1.1" xref="S3.Ex1.m1.3.3.1.1.1.cmml"><mi id="S3.Ex1.m1.3.3.1.1.1.3" xref="S3.Ex1.m1.3.3.1.1.1.3.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.3.3.1.1.1.2" xref="S3.Ex1.m1.3.3.1.1.1.2.cmml">​</mo><mi id="S3.Ex1.m1.3.3.1.1.1.4" xref="S3.Ex1.m1.3.3.1.1.1.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.3.3.1.1.1.2a" xref="S3.Ex1.m1.3.3.1.1.1.2.cmml">​</mo><mi id="S3.Ex1.m1.3.3.1.1.1.5" xref="S3.Ex1.m1.3.3.1.1.1.5.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.3.3.1.1.1.2b" xref="S3.Ex1.m1.3.3.1.1.1.2.cmml">​</mo><mi id="S3.Ex1.m1.3.3.1.1.1.6" xref="S3.Ex1.m1.3.3.1.1.1.6.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.3.3.1.1.1.2c" xref="S3.Ex1.m1.3.3.1.1.1.2.cmml">​</mo><mi id="S3.Ex1.m1.3.3.1.1.1.7" xref="S3.Ex1.m1.3.3.1.1.1.7.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.3.3.1.1.1.2d" xref="S3.Ex1.m1.3.3.1.1.1.2.cmml">​</mo><mi id="S3.Ex1.m1.3.3.1.1.1.8" xref="S3.Ex1.m1.3.3.1.1.1.8.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.3.3.1.1.1.2e" xref="S3.Ex1.m1.3.3.1.1.1.2.cmml">​</mo><mi id="S3.Ex1.m1.3.3.1.1.1.9" xref="S3.Ex1.m1.3.3.1.1.1.9.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.3.3.1.1.1.2f" xref="S3.Ex1.m1.3.3.1.1.1.2.cmml">​</mo><mrow id="S3.Ex1.m1.3.3.1.1.1.1.1" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.Ex1.m1.3.3.1.1.1.1.1.2" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.Ex1.m1.3.3.1.1.1.1.1.1" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.cmml"><mrow id="S3.Ex1.m1.3.3.1.1.1.1.1.1.2" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.2.cmml"><mi id="S3.Ex1.m1.3.3.1.1.1.1.1.1.2.2" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.2.2.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.3.3.1.1.1.1.1.1.2.1" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.2.1.cmml">​</mo><msup id="S3.Ex1.m1.3.3.1.1.1.1.1.1.2.3" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.2.3.cmml"><mi id="S3.Ex1.m1.3.3.1.1.1.1.1.1.2.3.2" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.2.3.2.cmml">K</mi><mi id="S3.Ex1.m1.3.3.1.1.1.1.1.1.2.3.3" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.2.3.3.cmml">T</mi></msup></mrow><mo id="S3.Ex1.m1.3.3.1.1.1.1.1.1.1" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.1.cmml">/</mo><msqrt id="S3.Ex1.m1.3.3.1.1.1.1.1.1.3" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.3.cmml"><mi id="S3.Ex1.m1.3.3.1.1.1.1.1.1.3.2" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.3.2.cmml">d</mi></msqrt></mrow><mo stretchy="false" id="S3.Ex1.m1.3.3.1.1.1.1.1.3" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S3.Ex1.m1.3.3.1.2" xref="S3.Ex1.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.3b"><apply id="S3.Ex1.m1.3.3.1.1.cmml" xref="S3.Ex1.m1.3.3.1"><eq id="S3.Ex1.m1.3.3.1.1.2.cmml" xref="S3.Ex1.m1.3.3.1.1.2"></eq><apply id="S3.Ex1.m1.3.3.1.1.3.cmml" xref="S3.Ex1.m1.3.3.1.1.3"><times id="S3.Ex1.m1.3.3.1.1.3.1.cmml" xref="S3.Ex1.m1.3.3.1.1.3.1"></times><ci id="S3.Ex1.m1.3.3.1.1.3.2.cmml" xref="S3.Ex1.m1.3.3.1.1.3.2">𝐴</ci><interval closure="open" id="S3.Ex1.m1.3.3.1.1.3.3.1.cmml" xref="S3.Ex1.m1.3.3.1.1.3.3.2"><ci id="S3.Ex1.m1.1.1.cmml" xref="S3.Ex1.m1.1.1">𝐾</ci><ci id="S3.Ex1.m1.2.2.cmml" xref="S3.Ex1.m1.2.2">𝑄</ci></interval></apply><apply id="S3.Ex1.m1.3.3.1.1.1.cmml" xref="S3.Ex1.m1.3.3.1.1.1"><times id="S3.Ex1.m1.3.3.1.1.1.2.cmml" xref="S3.Ex1.m1.3.3.1.1.1.2"></times><ci id="S3.Ex1.m1.3.3.1.1.1.3.cmml" xref="S3.Ex1.m1.3.3.1.1.1.3">𝑆</ci><ci id="S3.Ex1.m1.3.3.1.1.1.4.cmml" xref="S3.Ex1.m1.3.3.1.1.1.4">𝑜</ci><ci id="S3.Ex1.m1.3.3.1.1.1.5.cmml" xref="S3.Ex1.m1.3.3.1.1.1.5">𝑓</ci><ci id="S3.Ex1.m1.3.3.1.1.1.6.cmml" xref="S3.Ex1.m1.3.3.1.1.1.6">𝑡</ci><ci id="S3.Ex1.m1.3.3.1.1.1.7.cmml" xref="S3.Ex1.m1.3.3.1.1.1.7">𝑚</ci><ci id="S3.Ex1.m1.3.3.1.1.1.8.cmml" xref="S3.Ex1.m1.3.3.1.1.1.8">𝑎</ci><ci id="S3.Ex1.m1.3.3.1.1.1.9.cmml" xref="S3.Ex1.m1.3.3.1.1.1.9">𝑥</ci><apply id="S3.Ex1.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.1"><divide id="S3.Ex1.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.1"></divide><apply id="S3.Ex1.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.2"><times id="S3.Ex1.m1.3.3.1.1.1.1.1.1.2.1.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.2.1"></times><ci id="S3.Ex1.m1.3.3.1.1.1.1.1.1.2.2.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.2.2">𝑄</ci><apply id="S3.Ex1.m1.3.3.1.1.1.1.1.1.2.3.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.3.3.1.1.1.1.1.1.2.3.1.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.2.3">superscript</csymbol><ci id="S3.Ex1.m1.3.3.1.1.1.1.1.1.2.3.2.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.2.3.2">𝐾</ci><ci id="S3.Ex1.m1.3.3.1.1.1.1.1.1.2.3.3.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.2.3.3">𝑇</ci></apply></apply><apply id="S3.Ex1.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.3"><root id="S3.Ex1.m1.3.3.1.1.1.1.1.1.3a.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.3"></root><ci id="S3.Ex1.m1.3.3.1.1.1.1.1.1.3.2.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.3.2">𝑑</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.3c">A(K,Q)=Softmax(QK^{T}/\sqrt{d}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS1.p1.10" class="ltx_p">The results of the self-attention mechanism is the weighted sum
of the values <math id="S3.SS1.p1.10.m1.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S3.SS1.p1.10.m1.1a"><mi id="S3.SS1.p1.10.m1.1.1" xref="S3.SS1.p1.10.m1.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.10.m1.1b"><ci id="S3.SS1.p1.10.m1.1.1.cmml" xref="S3.SS1.p1.10.m1.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.10.m1.1c">V</annotation></semantics></math> with the attention map:</p>
<table id="S3.Ex2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex2.m1.6" class="ltx_Math" alttext="Attention(Q,K,V)=A(K,Q)V." display="block"><semantics id="S3.Ex2.m1.6a"><mrow id="S3.Ex2.m1.6.6.1" xref="S3.Ex2.m1.6.6.1.1.cmml"><mrow id="S3.Ex2.m1.6.6.1.1" xref="S3.Ex2.m1.6.6.1.1.cmml"><mrow id="S3.Ex2.m1.6.6.1.1.2" xref="S3.Ex2.m1.6.6.1.1.2.cmml"><mi id="S3.Ex2.m1.6.6.1.1.2.2" xref="S3.Ex2.m1.6.6.1.1.2.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.6.6.1.1.2.1" xref="S3.Ex2.m1.6.6.1.1.2.1.cmml">​</mo><mi id="S3.Ex2.m1.6.6.1.1.2.3" xref="S3.Ex2.m1.6.6.1.1.2.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.6.6.1.1.2.1a" xref="S3.Ex2.m1.6.6.1.1.2.1.cmml">​</mo><mi id="S3.Ex2.m1.6.6.1.1.2.4" xref="S3.Ex2.m1.6.6.1.1.2.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.6.6.1.1.2.1b" xref="S3.Ex2.m1.6.6.1.1.2.1.cmml">​</mo><mi id="S3.Ex2.m1.6.6.1.1.2.5" xref="S3.Ex2.m1.6.6.1.1.2.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.6.6.1.1.2.1c" xref="S3.Ex2.m1.6.6.1.1.2.1.cmml">​</mo><mi id="S3.Ex2.m1.6.6.1.1.2.6" xref="S3.Ex2.m1.6.6.1.1.2.6.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.6.6.1.1.2.1d" xref="S3.Ex2.m1.6.6.1.1.2.1.cmml">​</mo><mi id="S3.Ex2.m1.6.6.1.1.2.7" xref="S3.Ex2.m1.6.6.1.1.2.7.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.6.6.1.1.2.1e" xref="S3.Ex2.m1.6.6.1.1.2.1.cmml">​</mo><mi id="S3.Ex2.m1.6.6.1.1.2.8" xref="S3.Ex2.m1.6.6.1.1.2.8.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.6.6.1.1.2.1f" xref="S3.Ex2.m1.6.6.1.1.2.1.cmml">​</mo><mi id="S3.Ex2.m1.6.6.1.1.2.9" xref="S3.Ex2.m1.6.6.1.1.2.9.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.6.6.1.1.2.1g" xref="S3.Ex2.m1.6.6.1.1.2.1.cmml">​</mo><mi id="S3.Ex2.m1.6.6.1.1.2.10" xref="S3.Ex2.m1.6.6.1.1.2.10.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.6.6.1.1.2.1h" xref="S3.Ex2.m1.6.6.1.1.2.1.cmml">​</mo><mrow id="S3.Ex2.m1.6.6.1.1.2.11.2" xref="S3.Ex2.m1.6.6.1.1.2.11.1.cmml"><mo stretchy="false" id="S3.Ex2.m1.6.6.1.1.2.11.2.1" xref="S3.Ex2.m1.6.6.1.1.2.11.1.cmml">(</mo><mi id="S3.Ex2.m1.1.1" xref="S3.Ex2.m1.1.1.cmml">Q</mi><mo id="S3.Ex2.m1.6.6.1.1.2.11.2.2" xref="S3.Ex2.m1.6.6.1.1.2.11.1.cmml">,</mo><mi id="S3.Ex2.m1.2.2" xref="S3.Ex2.m1.2.2.cmml">K</mi><mo id="S3.Ex2.m1.6.6.1.1.2.11.2.3" xref="S3.Ex2.m1.6.6.1.1.2.11.1.cmml">,</mo><mi id="S3.Ex2.m1.3.3" xref="S3.Ex2.m1.3.3.cmml">V</mi><mo stretchy="false" id="S3.Ex2.m1.6.6.1.1.2.11.2.4" xref="S3.Ex2.m1.6.6.1.1.2.11.1.cmml">)</mo></mrow></mrow><mo id="S3.Ex2.m1.6.6.1.1.1" xref="S3.Ex2.m1.6.6.1.1.1.cmml">=</mo><mrow id="S3.Ex2.m1.6.6.1.1.3" xref="S3.Ex2.m1.6.6.1.1.3.cmml"><mi id="S3.Ex2.m1.6.6.1.1.3.2" xref="S3.Ex2.m1.6.6.1.1.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.6.6.1.1.3.1" xref="S3.Ex2.m1.6.6.1.1.3.1.cmml">​</mo><mrow id="S3.Ex2.m1.6.6.1.1.3.3.2" xref="S3.Ex2.m1.6.6.1.1.3.3.1.cmml"><mo stretchy="false" id="S3.Ex2.m1.6.6.1.1.3.3.2.1" xref="S3.Ex2.m1.6.6.1.1.3.3.1.cmml">(</mo><mi id="S3.Ex2.m1.4.4" xref="S3.Ex2.m1.4.4.cmml">K</mi><mo id="S3.Ex2.m1.6.6.1.1.3.3.2.2" xref="S3.Ex2.m1.6.6.1.1.3.3.1.cmml">,</mo><mi id="S3.Ex2.m1.5.5" xref="S3.Ex2.m1.5.5.cmml">Q</mi><mo stretchy="false" id="S3.Ex2.m1.6.6.1.1.3.3.2.3" xref="S3.Ex2.m1.6.6.1.1.3.3.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.6.6.1.1.3.1a" xref="S3.Ex2.m1.6.6.1.1.3.1.cmml">​</mo><mi id="S3.Ex2.m1.6.6.1.1.3.4" xref="S3.Ex2.m1.6.6.1.1.3.4.cmml">V</mi></mrow></mrow><mo lspace="0em" id="S3.Ex2.m1.6.6.1.2" xref="S3.Ex2.m1.6.6.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex2.m1.6b"><apply id="S3.Ex2.m1.6.6.1.1.cmml" xref="S3.Ex2.m1.6.6.1"><eq id="S3.Ex2.m1.6.6.1.1.1.cmml" xref="S3.Ex2.m1.6.6.1.1.1"></eq><apply id="S3.Ex2.m1.6.6.1.1.2.cmml" xref="S3.Ex2.m1.6.6.1.1.2"><times id="S3.Ex2.m1.6.6.1.1.2.1.cmml" xref="S3.Ex2.m1.6.6.1.1.2.1"></times><ci id="S3.Ex2.m1.6.6.1.1.2.2.cmml" xref="S3.Ex2.m1.6.6.1.1.2.2">𝐴</ci><ci id="S3.Ex2.m1.6.6.1.1.2.3.cmml" xref="S3.Ex2.m1.6.6.1.1.2.3">𝑡</ci><ci id="S3.Ex2.m1.6.6.1.1.2.4.cmml" xref="S3.Ex2.m1.6.6.1.1.2.4">𝑡</ci><ci id="S3.Ex2.m1.6.6.1.1.2.5.cmml" xref="S3.Ex2.m1.6.6.1.1.2.5">𝑒</ci><ci id="S3.Ex2.m1.6.6.1.1.2.6.cmml" xref="S3.Ex2.m1.6.6.1.1.2.6">𝑛</ci><ci id="S3.Ex2.m1.6.6.1.1.2.7.cmml" xref="S3.Ex2.m1.6.6.1.1.2.7">𝑡</ci><ci id="S3.Ex2.m1.6.6.1.1.2.8.cmml" xref="S3.Ex2.m1.6.6.1.1.2.8">𝑖</ci><ci id="S3.Ex2.m1.6.6.1.1.2.9.cmml" xref="S3.Ex2.m1.6.6.1.1.2.9">𝑜</ci><ci id="S3.Ex2.m1.6.6.1.1.2.10.cmml" xref="S3.Ex2.m1.6.6.1.1.2.10">𝑛</ci><vector id="S3.Ex2.m1.6.6.1.1.2.11.1.cmml" xref="S3.Ex2.m1.6.6.1.1.2.11.2"><ci id="S3.Ex2.m1.1.1.cmml" xref="S3.Ex2.m1.1.1">𝑄</ci><ci id="S3.Ex2.m1.2.2.cmml" xref="S3.Ex2.m1.2.2">𝐾</ci><ci id="S3.Ex2.m1.3.3.cmml" xref="S3.Ex2.m1.3.3">𝑉</ci></vector></apply><apply id="S3.Ex2.m1.6.6.1.1.3.cmml" xref="S3.Ex2.m1.6.6.1.1.3"><times id="S3.Ex2.m1.6.6.1.1.3.1.cmml" xref="S3.Ex2.m1.6.6.1.1.3.1"></times><ci id="S3.Ex2.m1.6.6.1.1.3.2.cmml" xref="S3.Ex2.m1.6.6.1.1.3.2">𝐴</ci><interval closure="open" id="S3.Ex2.m1.6.6.1.1.3.3.1.cmml" xref="S3.Ex2.m1.6.6.1.1.3.3.2"><ci id="S3.Ex2.m1.4.4.cmml" xref="S3.Ex2.m1.4.4">𝐾</ci><ci id="S3.Ex2.m1.5.5.cmml" xref="S3.Ex2.m1.5.5">𝑄</ci></interval><ci id="S3.Ex2.m1.6.6.1.1.3.4.cmml" xref="S3.Ex2.m1.6.6.1.1.3.4">𝑉</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex2.m1.6c">Attention(Q,K,V)=A(K,Q)V.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS1.p1.12" class="ltx_p">The computational complexity of this original self-attention mechanism scales quadratically with <math id="S3.SS1.p1.11.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS1.p1.11.m1.1a"><mi id="S3.SS1.p1.11.m1.1.1" xref="S3.SS1.p1.11.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.11.m1.1b"><ci id="S3.SS1.p1.11.m1.1.1.cmml" xref="S3.SS1.p1.11.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.11.m1.1c">N</annotation></semantics></math>, due to pairwise interactions between all <math id="S3.SS1.p1.12.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS1.p1.12.m2.1a"><mi id="S3.SS1.p1.12.m2.1.1" xref="S3.SS1.p1.12.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.12.m2.1b"><ci id="S3.SS1.p1.12.m2.1.1.cmml" xref="S3.SS1.p1.12.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.12.m2.1c">N</annotation></semantics></math> elements.
The use of image patches in ViT instead of pixels makes the use of self-attention tractable, but
the quadratic complexity still remains an issue.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">In addition to the tokens created from the image patches, a vision transformer also has
an extra (learned) token. This token, usually called the CLS token, is used to generate
the final prediction when the transformer is trained for a classification task.
The transformed tokens at the output of the encoder have the same number of channels as the input.
In our model, the output tokens of the encoder transformer including the
CLS token are used as the “memory” input of the decoder transformer.</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>DeiT Encoder:</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.2" class="ltx_p">Touvron et al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> proposed a training methodology for the ViT architecture
that enables efficient training of the vision transformer without the use of huge datasets.
The proposed methodology enabled DeiT models to surpass the accuracy of the
ViT baseline, while training just on the ImageNet dataset.
They further improved on that by proposing a distillation strategy that yielded accuracy
similar to that of the best convolutional models.
Our DeiT encoder uses the <em id="S3.SS1.SSS1.p1.2.1" class="ltx_emph ltx_font_italic">DeiT-S</em> model architecture. For our experiments, the encoders are initialized using either the weights provided by the DeiT authors (DeiT-S, trained with distillation at <math id="S3.SS1.SSS1.p1.1.m1.1" class="ltx_Math" alttext="224\times 224" display="inline"><semantics id="S3.SS1.SSS1.p1.1.m1.1a"><mrow id="S3.SS1.SSS1.p1.1.m1.1.1" xref="S3.SS1.SSS1.p1.1.m1.1.1.cmml"><mn id="S3.SS1.SSS1.p1.1.m1.1.1.2" xref="S3.SS1.SSS1.p1.1.m1.1.1.2.cmml">224</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS1.p1.1.m1.1.1.1" xref="S3.SS1.SSS1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS1.SSS1.p1.1.m1.1.1.3" xref="S3.SS1.SSS1.p1.1.m1.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.1.m1.1b"><apply id="S3.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1"><times id="S3.SS1.SSS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.SS1.SSS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1.2">224</cn><cn type="integer" id="S3.SS1.SSS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.1.m1.1c">224\times 224</annotation></semantics></math>) of the weights provided by the Dino authors.
For our experiments we use two versions of the DeiT-S model, both trained with
input sizes of <math id="S3.SS1.SSS1.p1.2.m2.1" class="ltx_Math" alttext="224\times 224" display="inline"><semantics id="S3.SS1.SSS1.p1.2.m2.1a"><mrow id="S3.SS1.SSS1.p1.2.m2.1.1" xref="S3.SS1.SSS1.p1.2.m2.1.1.cmml"><mn id="S3.SS1.SSS1.p1.2.m2.1.1.2" xref="S3.SS1.SSS1.p1.2.m2.1.1.2.cmml">224</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS1.p1.2.m2.1.1.1" xref="S3.SS1.SSS1.p1.2.m2.1.1.1.cmml">×</mo><mn id="S3.SS1.SSS1.p1.2.m2.1.1.3" xref="S3.SS1.SSS1.p1.2.m2.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.2.m2.1b"><apply id="S3.SS1.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1"><times id="S3.SS1.SSS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.1"></times><cn type="integer" id="S3.SS1.SSS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.2">224</cn><cn type="integer" id="S3.SS1.SSS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.2.m2.1c">224\times 224</annotation></semantics></math> pixels:</p>
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">the DeiT-S variant trained with distillation and</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">the DeiT-S trained using the unsupervised methodology of Dino.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Xcit Encoder:</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">El-Nouby et al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> recently proposed the Xcit transformer.
In this approach they transpose the encoder architecture.
Attention happens between the channels of the tokens rather than between
the tokens themselves. The proposed attention mechanism is complemented by
blocks that enable “local patch interations”, enabling the exchange of information
between neighboring tokens on the image plane.</p>
</div>
<div id="S3.SS1.SSS2.p2" class="ltx_para">
<p id="S3.SS1.SSS2.p2.1" class="ltx_p">The Xcit transformer scales linearly with the size of patches since the
memory and computation expensive operation of attention is applied to the channels
which are fixed in size. This characteristic of Xcit enables training with larger input image sizes and more patches. In our experiments we use the <em id="S3.SS1.SSS2.p2.1.1" class="ltx_emph ltx_font_italic">Xcit-Small-12</em> variant of the model
which corresponds roughly to the <em id="S3.SS1.SSS2.p2.1.2" class="ltx_emph ltx_font_italic">DeiT-S</em> variant of the DeiT architecture.
Similar to the DeiT (Section <a href="#S3.SS1.SSS1" title="3.1.1 DeiT Encoder: ‣ 3.1 Transformer Encoder ‣ 3 Method ‣ PE-former: Pose Estimation Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1.1</span></a>) encoder we use pre-trained weights for this
encoder. The weights are obtained with</p>
<ol id="S3.I2" class="ltx_enumerate">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p">supervised training on the ImageNet and</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p">with the unsupervised methodology of Dino.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S3.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.3 </span>Resnet Encoder:</h4>

<div id="S3.SS1.SSS3.p1" class="ltx_para">
<p id="S3.SS1.SSS3.p1.1" class="ltx_p">In order to verify the positive impact of our proposed attention-based encoder
we implemented a third type of encoder, this time using Resnet50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
as the architecture. The 50-layer variant was selected because it has a similar number
of parameters as the examined transformer-based encoders.</p>
</div>
<div id="S3.SS1.SSS3.p2" class="ltx_para">
<p id="S3.SS1.SSS3.p2.1" class="ltx_p">The Resnet encoder replaces the “Visual Transformer Encoder” block
shown in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ PE-former: Pose Estimation Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Following the DETR implementation we use a <math id="S3.SS1.SSS3.p2.1.m1.1" class="ltx_Math" alttext="1\times 1" display="inline"><semantics id="S3.SS1.SSS3.p2.1.m1.1a"><mrow id="S3.SS1.SSS3.p2.1.m1.1.1" xref="S3.SS1.SSS3.p2.1.m1.1.1.cmml"><mn id="S3.SS1.SSS3.p2.1.m1.1.1.2" xref="S3.SS1.SSS3.p2.1.m1.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS3.p2.1.m1.1.1.1" xref="S3.SS1.SSS3.p2.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS1.SSS3.p2.1.m1.1.1.3" xref="S3.SS1.SSS3.p2.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p2.1.m1.1b"><apply id="S3.SS1.SSS3.p2.1.m1.1.1.cmml" xref="S3.SS1.SSS3.p2.1.m1.1.1"><times id="S3.SS1.SSS3.p2.1.m1.1.1.1.cmml" xref="S3.SS1.SSS3.p2.1.m1.1.1.1"></times><cn type="integer" id="S3.SS1.SSS3.p2.1.m1.1.1.2.cmml" xref="S3.SS1.SSS3.p2.1.m1.1.1.2">1</cn><cn type="integer" id="S3.SS1.SSS3.p2.1.m1.1.1.3.cmml" xref="S3.SS1.SSS3.p2.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p2.1.m1.1c">1\times 1</annotation></semantics></math> convolution to project the output features of the Resnet to the number of channels expected by the decoder. Subsequently we unroll each feature channel to a vector. These vectors are
passed as the memory tokens to the decoder.
The Resnet50 encoder is used to experimentally validate the positive effect of
using vision transformers (DEIT or Xcit) versus a conventional convolutional encoder.</p>
</div>
</section>
<section id="S3.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.4 </span>VAB (ViT as Backbone):</h4>

<div id="S3.SS1.SSS4.p1" class="ltx_para">
<p id="S3.SS1.SSS4.p1.1" class="ltx_p">Related work that tackles pose estimation with transformers typically uses a convolutional backbone
to extract image features. This model architecture is shown on the top row of Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ PE-former: Pose Estimation Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The features created by the CNN backbone are flattened and used as input tokens to a transformer front-end. In this model the transformer consists of an encoder and a decoder block.</p>
</div>
<div id="S3.SS1.SSS4.p2" class="ltx_para">
<p id="S3.SS1.SSS4.p2.1" class="ltx_p">For our experiments we replace the CNN backbone for a visual transformer.
The vision transformer outputs feature tokens which are subsequently processed by the
(unchanged) transformer encoder-decoder block. We call this modification VAB for “ViT as Backbone”.
The VAB approach is closer in spirit to contemporary methods for pose estimation
replacing only the CNN backbone with ViT. However it is larger in number of parameters
and requires more computational resources than its CNN-based counterparts.
In our experiments we use the DeiT-S as the visual transformer backbone.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Transformer Decoder</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.2" class="ltx_p">We adapt the decoder block of DETR and use it in our model. A number of learned queries are used to predict joint locations. Predictions that do not correspond to a joint are mapped to the “non-object” class. Following the work of Li et al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> we use <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="M=100" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">M</mi><mo id="S3.SS2.p1.1.m1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><eq id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1"></eq><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">𝑀</ci><cn type="integer" id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">M=100</annotation></semantics></math> queries for all our models. Adding more queries does not improve the result, while having queries equal to the number of expected joints (i.e., <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="17" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mn id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">17</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><cn type="integer" id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">17</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">17</annotation></semantics></math> for the COCO dataset) gives slightly worse results.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.4" class="ltx_p">Bipartite graph matching is used at training time to map the regressed joints to the
ground truth annotations. In contrast to the DETR decoder, our decoder is
configured to regress to 2D keypoints rather than object bounding boxes.
This translates to regressing <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mn id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><cn type="integer" id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">2</annotation></semantics></math> scalar values (<math id="S3.SS2.p2.2.m2.2" class="ltx_Math" alttext="x,y" display="inline"><semantics id="S3.SS2.p2.2.m2.2a"><mrow id="S3.SS2.p2.2.m2.2.3.2" xref="S3.SS2.p2.2.m2.2.3.1.cmml"><mi id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml">x</mi><mo id="S3.SS2.p2.2.m2.2.3.2.1" xref="S3.SS2.p2.2.m2.2.3.1.cmml">,</mo><mi id="S3.SS2.p2.2.m2.2.2" xref="S3.SS2.p2.2.m2.2.2.cmml">y</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.2b"><list id="S3.SS2.p2.2.m2.2.3.1.cmml" xref="S3.SS2.p2.2.m2.2.3.2"><ci id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">𝑥</ci><ci id="S3.SS2.p2.2.m2.2.2.cmml" xref="S3.SS2.p2.2.m2.2.2">𝑦</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.2c">x,y</annotation></semantics></math>) in the range of <math id="S3.SS2.p2.3.m3.2" class="ltx_Math" alttext="[0,1]" display="inline"><semantics id="S3.SS2.p2.3.m3.2a"><mrow id="S3.SS2.p2.3.m3.2.3.2" xref="S3.SS2.p2.3.m3.2.3.1.cmml"><mo stretchy="false" id="S3.SS2.p2.3.m3.2.3.2.1" xref="S3.SS2.p2.3.m3.2.3.1.cmml">[</mo><mn id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml">0</mn><mo id="S3.SS2.p2.3.m3.2.3.2.2" xref="S3.SS2.p2.3.m3.2.3.1.cmml">,</mo><mn id="S3.SS2.p2.3.m3.2.2" xref="S3.SS2.p2.3.m3.2.2.cmml">1</mn><mo stretchy="false" id="S3.SS2.p2.3.m3.2.3.2.3" xref="S3.SS2.p2.3.m3.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.2b"><interval closure="closed" id="S3.SS2.p2.3.m3.2.3.1.cmml" xref="S3.SS2.p2.3.m3.2.3.2"><cn type="integer" id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">0</cn><cn type="integer" id="S3.SS2.p2.3.m3.2.2.cmml" xref="S3.SS2.p2.3.m3.2.2">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.2c">[0,1]</annotation></semantics></math> for each joint. In all our experiments we used a decoder with <math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><mn id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><cn type="integer" id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">6</annotation></semantics></math> layers which achieves a good performance balance. Adding more layers to the decoder gives slightly improved results at a cost of more parameters and higher computational and memory requirements.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Training</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.7" class="ltx_p">We use AdamW to train our models with a weight decay of <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="1e-4" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mrow id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mrow id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml"><mn id="S3.SS3.p1.1.m1.1.1.2.2" xref="S3.SS3.p1.1.m1.1.1.2.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S3.SS3.p1.1.m1.1.1.2.1" xref="S3.SS3.p1.1.m1.1.1.2.1.cmml">​</mo><mi id="S3.SS3.p1.1.m1.1.1.2.3" xref="S3.SS3.p1.1.m1.1.1.2.3.cmml">e</mi></mrow><mo id="S3.SS3.p1.1.m1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.cmml">−</mo><mn id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><minus id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1"></minus><apply id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2"><times id="S3.SS3.p1.1.m1.1.1.2.1.cmml" xref="S3.SS3.p1.1.m1.1.1.2.1"></times><cn type="integer" id="S3.SS3.p1.1.m1.1.1.2.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2.2">1</cn><ci id="S3.SS3.p1.1.m1.1.1.2.3.cmml" xref="S3.SS3.p1.1.m1.1.1.2.3">𝑒</ci></apply><cn type="integer" id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">1e-4</annotation></semantics></math>. For all our experiments we use pretrained encoders while the decoder weights are randomly initialized. The pretrained weights used are noted in each experiment. The learning rate for the encoder is set to <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="1e-5" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mrow id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml"><mrow id="S3.SS3.p1.2.m2.1.1.2" xref="S3.SS3.p1.2.m2.1.1.2.cmml"><mn id="S3.SS3.p1.2.m2.1.1.2.2" xref="S3.SS3.p1.2.m2.1.1.2.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S3.SS3.p1.2.m2.1.1.2.1" xref="S3.SS3.p1.2.m2.1.1.2.1.cmml">​</mo><mi id="S3.SS3.p1.2.m2.1.1.2.3" xref="S3.SS3.p1.2.m2.1.1.2.3.cmml">e</mi></mrow><mo id="S3.SS3.p1.2.m2.1.1.1" xref="S3.SS3.p1.2.m2.1.1.1.cmml">−</mo><mn id="S3.SS3.p1.2.m2.1.1.3" xref="S3.SS3.p1.2.m2.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><apply id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1"><minus id="S3.SS3.p1.2.m2.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1"></minus><apply id="S3.SS3.p1.2.m2.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.2"><times id="S3.SS3.p1.2.m2.1.1.2.1.cmml" xref="S3.SS3.p1.2.m2.1.1.2.1"></times><cn type="integer" id="S3.SS3.p1.2.m2.1.1.2.2.cmml" xref="S3.SS3.p1.2.m2.1.1.2.2">1</cn><ci id="S3.SS3.p1.2.m2.1.1.2.3.cmml" xref="S3.SS3.p1.2.m2.1.1.2.3">𝑒</ci></apply><cn type="integer" id="S3.SS3.p1.2.m2.1.1.3.cmml" xref="S3.SS3.p1.2.m2.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">1e-5</annotation></semantics></math> and for the decoder to <math id="S3.SS3.p1.3.m3.1" class="ltx_Math" alttext="1e-4" display="inline"><semantics id="S3.SS3.p1.3.m3.1a"><mrow id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml"><mrow id="S3.SS3.p1.3.m3.1.1.2" xref="S3.SS3.p1.3.m3.1.1.2.cmml"><mn id="S3.SS3.p1.3.m3.1.1.2.2" xref="S3.SS3.p1.3.m3.1.1.2.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S3.SS3.p1.3.m3.1.1.2.1" xref="S3.SS3.p1.3.m3.1.1.2.1.cmml">​</mo><mi id="S3.SS3.p1.3.m3.1.1.2.3" xref="S3.SS3.p1.3.m3.1.1.2.3.cmml">e</mi></mrow><mo id="S3.SS3.p1.3.m3.1.1.1" xref="S3.SS3.p1.3.m3.1.1.1.cmml">−</mo><mn id="S3.SS3.p1.3.m3.1.1.3" xref="S3.SS3.p1.3.m3.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><apply id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1"><minus id="S3.SS3.p1.3.m3.1.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1.1"></minus><apply id="S3.SS3.p1.3.m3.1.1.2.cmml" xref="S3.SS3.p1.3.m3.1.1.2"><times id="S3.SS3.p1.3.m3.1.1.2.1.cmml" xref="S3.SS3.p1.3.m3.1.1.2.1"></times><cn type="integer" id="S3.SS3.p1.3.m3.1.1.2.2.cmml" xref="S3.SS3.p1.3.m3.1.1.2.2">1</cn><ci id="S3.SS3.p1.3.m3.1.1.2.3.cmml" xref="S3.SS3.p1.3.m3.1.1.2.3">𝑒</ci></apply><cn type="integer" id="S3.SS3.p1.3.m3.1.1.3.cmml" xref="S3.SS3.p1.3.m3.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">1e-4</annotation></semantics></math>. The learning rate drops by a factor of <math id="S3.SS3.p1.4.m4.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S3.SS3.p1.4.m4.1a"><mn id="S3.SS3.p1.4.m4.1.1" xref="S3.SS3.p1.4.m4.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.1b"><cn type="integer" id="S3.SS3.p1.4.m4.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">10</annotation></semantics></math> after the first <math id="S3.SS3.p1.5.m5.1" class="ltx_Math" alttext="50" display="inline"><semantics id="S3.SS3.p1.5.m5.1a"><mn id="S3.SS3.p1.5.m5.1.1" xref="S3.SS3.p1.5.m5.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.5.m5.1b"><cn type="integer" id="S3.SS3.p1.5.m5.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.5.m5.1c">50</annotation></semantics></math> epochs and
the models are trained for a total of <math id="S3.SS3.p1.6.m6.1" class="ltx_Math" alttext="80" display="inline"><semantics id="S3.SS3.p1.6.m6.1a"><mn id="S3.SS3.p1.6.m6.1.1" xref="S3.SS3.p1.6.m6.1.1.cmml">80</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.6.m6.1b"><cn type="integer" id="S3.SS3.p1.6.m6.1.1.cmml" xref="S3.SS3.p1.6.m6.1.1">80</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.6.m6.1c">80</annotation></semantics></math> epochs. For all our models we use a
batch size of <math id="S3.SS3.p1.7.m7.1" class="ltx_Math" alttext="42" display="inline"><semantics id="S3.SS3.p1.7.m7.1a"><mn id="S3.SS3.p1.7.m7.1.1" xref="S3.SS3.p1.7.m7.1.1.cmml">42</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.7.m7.1b"><cn type="integer" id="S3.SS3.p1.7.m7.1.1.cmml" xref="S3.SS3.p1.7.m7.1.1">42</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.7.m7.1c">42</annotation></semantics></math>.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.2" class="ltx_p">For data augmentation we follow the approach of Xiao et al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. According to this we apply a random scaling in the range <math id="S3.SS3.p2.1.m1.2" class="ltx_Math" alttext="[0.7,1.3]" display="inline"><semantics id="S3.SS3.p2.1.m1.2a"><mrow id="S3.SS3.p2.1.m1.2.3.2" xref="S3.SS3.p2.1.m1.2.3.1.cmml"><mo stretchy="false" id="S3.SS3.p2.1.m1.2.3.2.1" xref="S3.SS3.p2.1.m1.2.3.1.cmml">[</mo><mn id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">0.7</mn><mo id="S3.SS3.p2.1.m1.2.3.2.2" xref="S3.SS3.p2.1.m1.2.3.1.cmml">,</mo><mn id="S3.SS3.p2.1.m1.2.2" xref="S3.SS3.p2.1.m1.2.2.cmml">1.3</mn><mo stretchy="false" id="S3.SS3.p2.1.m1.2.3.2.3" xref="S3.SS3.p2.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.2b"><interval closure="closed" id="S3.SS3.p2.1.m1.2.3.1.cmml" xref="S3.SS3.p2.1.m1.2.3.2"><cn type="float" id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">0.7</cn><cn type="float" id="S3.SS3.p2.1.m1.2.2.cmml" xref="S3.SS3.p2.1.m1.2.2">1.3</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.2c">[0.7,1.3]</annotation></semantics></math>, a random rotation in the range (<math id="S3.SS3.p2.2.m2.2" class="ltx_Math" alttext="[-40^{\circ},40^{\circ}]" display="inline"><semantics id="S3.SS3.p2.2.m2.2a"><mrow id="S3.SS3.p2.2.m2.2.2.2" xref="S3.SS3.p2.2.m2.2.2.3.cmml"><mo stretchy="false" id="S3.SS3.p2.2.m2.2.2.2.3" xref="S3.SS3.p2.2.m2.2.2.3.cmml">[</mo><mrow id="S3.SS3.p2.2.m2.1.1.1.1" xref="S3.SS3.p2.2.m2.1.1.1.1.cmml"><mo id="S3.SS3.p2.2.m2.1.1.1.1a" xref="S3.SS3.p2.2.m2.1.1.1.1.cmml">−</mo><msup id="S3.SS3.p2.2.m2.1.1.1.1.2" xref="S3.SS3.p2.2.m2.1.1.1.1.2.cmml"><mn id="S3.SS3.p2.2.m2.1.1.1.1.2.2" xref="S3.SS3.p2.2.m2.1.1.1.1.2.2.cmml">40</mn><mo id="S3.SS3.p2.2.m2.1.1.1.1.2.3" xref="S3.SS3.p2.2.m2.1.1.1.1.2.3.cmml">∘</mo></msup></mrow><mo id="S3.SS3.p2.2.m2.2.2.2.4" xref="S3.SS3.p2.2.m2.2.2.3.cmml">,</mo><msup id="S3.SS3.p2.2.m2.2.2.2.2" xref="S3.SS3.p2.2.m2.2.2.2.2.cmml"><mn id="S3.SS3.p2.2.m2.2.2.2.2.2" xref="S3.SS3.p2.2.m2.2.2.2.2.2.cmml">40</mn><mo id="S3.SS3.p2.2.m2.2.2.2.2.3" xref="S3.SS3.p2.2.m2.2.2.2.2.3.cmml">∘</mo></msup><mo stretchy="false" id="S3.SS3.p2.2.m2.2.2.2.5" xref="S3.SS3.p2.2.m2.2.2.3.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.2b"><interval closure="closed" id="S3.SS3.p2.2.m2.2.2.3.cmml" xref="S3.SS3.p2.2.m2.2.2.2"><apply id="S3.SS3.p2.2.m2.1.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1.1.1"><minus id="S3.SS3.p2.2.m2.1.1.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1.1.1"></minus><apply id="S3.SS3.p2.2.m2.1.1.1.1.2.cmml" xref="S3.SS3.p2.2.m2.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.1.1.1.1.2.1.cmml" xref="S3.SS3.p2.2.m2.1.1.1.1.2">superscript</csymbol><cn type="integer" id="S3.SS3.p2.2.m2.1.1.1.1.2.2.cmml" xref="S3.SS3.p2.2.m2.1.1.1.1.2.2">40</cn><compose id="S3.SS3.p2.2.m2.1.1.1.1.2.3.cmml" xref="S3.SS3.p2.2.m2.1.1.1.1.2.3"></compose></apply></apply><apply id="S3.SS3.p2.2.m2.2.2.2.2.cmml" xref="S3.SS3.p2.2.m2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.2.2.2.2.1.cmml" xref="S3.SS3.p2.2.m2.2.2.2.2">superscript</csymbol><cn type="integer" id="S3.SS3.p2.2.m2.2.2.2.2.2.cmml" xref="S3.SS3.p2.2.m2.2.2.2.2.2">40</cn><compose id="S3.SS3.p2.2.m2.2.2.2.2.3.cmml" xref="S3.SS3.p2.2.m2.2.2.2.2.3"></compose></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.2c">[-40^{\circ},40^{\circ}]</annotation></semantics></math>) and a random flip to the image crop. We found experimentally that the jitter, blur and solarization used by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> for the object detection task were not helpful in our 2D human pose estimation problem, so these operations were not used.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We compare against two other regression based methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> for
single instance 2D human pose estimation. Both of these methods use a convolutional backbone
and an attention mechanism derived from the DETR decoder to regress to the
2D keypoints of a human in the input image crop.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Evaluation methodology</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We train and evaluate our models on the COCO val dataset. We use the detection results from a person detector with AP 50.2 on the COCO Val2017 set. Following standard practice <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> we use flip test and average the results from the original and flipped images. Unless otherwise noted, the methods we compare against use the same evaluation techniques.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Comparison with SOTA</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.2" class="ltx_p">As shown in Table <a href="#S4.T1" title="Table 1 ‣ 4.2 Comparison with SOTA ‣ 4 Experiments ‣ PE-former: Pose Estimation Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, our architecture performs on par or surpasses methods that use CNN backbones for the same task. Using the DEIT encoder requires more memory and CPU resources although the number of parameters is relatively low. Due to the increased requirements, we limit our comparison tests of DEIT to <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="192\times 256" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mn id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">192</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p1.1.m1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><times id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">192</cn><cn type="integer" id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">192\times 256</annotation></semantics></math> resolution for patch sizes of <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="8\times 8" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mrow id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><mn id="S4.SS2.p1.2.m2.1.1.2" xref="S4.SS2.p1.2.m2.1.1.2.cmml">8</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p1.2.m2.1.1.1" xref="S4.SS2.p1.2.m2.1.1.1.cmml">×</mo><mn id="S4.SS2.p1.2.m2.1.1.3" xref="S4.SS2.p1.2.m2.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><times id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1.1"></times><cn type="integer" id="S4.SS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2">8</cn><cn type="integer" id="S4.SS2.p1.2.m2.1.1.3.cmml" xref="S4.SS2.p1.2.m2.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">8\times 8</annotation></semantics></math> pixels.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.4" class="ltx_p">Our DEIT variant scores just <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="0.4\%" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mrow id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml"><mn id="S4.SS2.p2.1.m1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.2.cmml">0.4</mn><mo id="S4.SS2.p2.1.m1.1.1.1" xref="S4.SS2.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><apply id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1"><csymbol cd="latexml" id="S4.SS2.p2.1.m1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.1.1.2">0.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">0.4\%</annotation></semantics></math> lower than TFPose
<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>We contacted the authors of TFPose for additional information to use in our comparison, such as number of parameters and AR scores but got no response</span></span></span>
for input <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="192\times 256" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mrow id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml"><mn id="S4.SS2.p2.2.m2.1.1.2" xref="S4.SS2.p2.2.m2.1.1.2.cmml">192</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p2.2.m2.1.1.1" xref="S4.SS2.p2.2.m2.1.1.1.cmml">×</mo><mn id="S4.SS2.p2.2.m2.1.1.3" xref="S4.SS2.p2.2.m2.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><apply id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1"><times id="S4.SS2.p2.2.m2.1.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1.1"></times><cn type="integer" id="S4.SS2.p2.2.m2.1.1.2.cmml" xref="S4.SS2.p2.2.m2.1.1.2">192</cn><cn type="integer" id="S4.SS2.p2.2.m2.1.1.3.cmml" xref="S4.SS2.p2.2.m2.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">192\times 256</annotation></semantics></math>.
However, it outperforms PRTR @ <math id="S4.SS2.p2.3.m3.1" class="ltx_Math" alttext="288\times 384" display="inline"><semantics id="S4.SS2.p2.3.m3.1a"><mrow id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml"><mn id="S4.SS2.p2.3.m3.1.1.2" xref="S4.SS2.p2.3.m3.1.1.2.cmml">288</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p2.3.m3.1.1.1" xref="S4.SS2.p2.3.m3.1.1.1.cmml">×</mo><mn id="S4.SS2.p2.3.m3.1.1.3" xref="S4.SS2.p2.3.m3.1.1.3.cmml">384</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.1b"><apply id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1"><times id="S4.SS2.p2.3.m3.1.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1.1"></times><cn type="integer" id="S4.SS2.p2.3.m3.1.1.2.cmml" xref="S4.SS2.p2.3.m3.1.1.2">288</cn><cn type="integer" id="S4.SS2.p2.3.m3.1.1.3.cmml" xref="S4.SS2.p2.3.m3.1.1.3">384</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.1c">288\times 384</annotation></semantics></math>, despite the higher input resolution and the
greater number of parameters (<math id="S4.SS2.p2.4.m4.1" class="ltx_Math" alttext="5.1M" display="inline"><semantics id="S4.SS2.p2.4.m4.1a"><mrow id="S4.SS2.p2.4.m4.1.1" xref="S4.SS2.p2.4.m4.1.1.cmml"><mn id="S4.SS2.p2.4.m4.1.1.2" xref="S4.SS2.p2.4.m4.1.1.2.cmml">5.1</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p2.4.m4.1.1.1" xref="S4.SS2.p2.4.m4.1.1.1.cmml">​</mo><mi id="S4.SS2.p2.4.m4.1.1.3" xref="S4.SS2.p2.4.m4.1.1.3.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.4.m4.1b"><apply id="S4.SS2.p2.4.m4.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1"><times id="S4.SS2.p2.4.m4.1.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1.1"></times><cn type="float" id="S4.SS2.p2.4.m4.1.1.2.cmml" xref="S4.SS2.p2.4.m4.1.1.2">5.1</cn><ci id="S4.SS2.p2.4.m4.1.1.3.cmml" xref="S4.SS2.p2.4.m4.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.4.m4.1c">5.1M</annotation></semantics></math>) of the later.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.4" class="ltx_p">Due to its reduced computational and memory requirements, our Xcit variant can be trained
to higher resolutions and, thus, larger number of input tokens. We train networks at <math id="S4.SS2.p3.1.m1.1" class="ltx_Math" alttext="192\times 256" display="inline"><semantics id="S4.SS2.p3.1.m1.1a"><mrow id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml"><mn id="S4.SS2.p3.1.m1.1.1.2" xref="S4.SS2.p3.1.m1.1.1.2.cmml">192</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p3.1.m1.1.1.1" xref="S4.SS2.p3.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS2.p3.1.m1.1.1.3" xref="S4.SS2.p3.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><apply id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1"><times id="S4.SS2.p3.1.m1.1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1.1"></times><cn type="integer" id="S4.SS2.p3.1.m1.1.1.2.cmml" xref="S4.SS2.p3.1.m1.1.1.2">192</cn><cn type="integer" id="S4.SS2.p3.1.m1.1.1.3.cmml" xref="S4.SS2.p3.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">192\times 256</annotation></semantics></math> and <math id="S4.SS2.p3.2.m2.1" class="ltx_Math" alttext="288\times 384" display="inline"><semantics id="S4.SS2.p3.2.m2.1a"><mrow id="S4.SS2.p3.2.m2.1.1" xref="S4.SS2.p3.2.m2.1.1.cmml"><mn id="S4.SS2.p3.2.m2.1.1.2" xref="S4.SS2.p3.2.m2.1.1.2.cmml">288</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p3.2.m2.1.1.1" xref="S4.SS2.p3.2.m2.1.1.1.cmml">×</mo><mn id="S4.SS2.p3.2.m2.1.1.3" xref="S4.SS2.p3.2.m2.1.1.3.cmml">384</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.m2.1b"><apply id="S4.SS2.p3.2.m2.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1"><times id="S4.SS2.p3.2.m2.1.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1.1"></times><cn type="integer" id="S4.SS2.p3.2.m2.1.1.2.cmml" xref="S4.SS2.p3.2.m2.1.1.2">288</cn><cn type="integer" id="S4.SS2.p3.2.m2.1.1.3.cmml" xref="S4.SS2.p3.2.m2.1.1.3">384</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.m2.1c">288\times 384</annotation></semantics></math> input resolutions for the <math id="S4.SS2.p3.3.m3.1" class="ltx_Math" alttext="16\times 16" display="inline"><semantics id="S4.SS2.p3.3.m3.1a"><mrow id="S4.SS2.p3.3.m3.1.1" xref="S4.SS2.p3.3.m3.1.1.cmml"><mn id="S4.SS2.p3.3.m3.1.1.2" xref="S4.SS2.p3.3.m3.1.1.2.cmml">16</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p3.3.m3.1.1.1" xref="S4.SS2.p3.3.m3.1.1.1.cmml">×</mo><mn id="S4.SS2.p3.3.m3.1.1.3" xref="S4.SS2.p3.3.m3.1.1.3.cmml">16</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.3.m3.1b"><apply id="S4.SS2.p3.3.m3.1.1.cmml" xref="S4.SS2.p3.3.m3.1.1"><times id="S4.SS2.p3.3.m3.1.1.1.cmml" xref="S4.SS2.p3.3.m3.1.1.1"></times><cn type="integer" id="S4.SS2.p3.3.m3.1.1.2.cmml" xref="S4.SS2.p3.3.m3.1.1.2">16</cn><cn type="integer" id="S4.SS2.p3.3.m3.1.1.3.cmml" xref="S4.SS2.p3.3.m3.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.3.m3.1c">16\times 16</annotation></semantics></math> and <math id="S4.SS2.p3.4.m4.1" class="ltx_Math" alttext="8\times 8" display="inline"><semantics id="S4.SS2.p3.4.m4.1a"><mrow id="S4.SS2.p3.4.m4.1.1" xref="S4.SS2.p3.4.m4.1.1.cmml"><mn id="S4.SS2.p3.4.m4.1.1.2" xref="S4.SS2.p3.4.m4.1.1.2.cmml">8</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p3.4.m4.1.1.1" xref="S4.SS2.p3.4.m4.1.1.1.cmml">×</mo><mn id="S4.SS2.p3.4.m4.1.1.3" xref="S4.SS2.p3.4.m4.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.4.m4.1b"><apply id="S4.SS2.p3.4.m4.1.1.cmml" xref="S4.SS2.p3.4.m4.1.1"><times id="S4.SS2.p3.4.m4.1.1.1.cmml" xref="S4.SS2.p3.4.m4.1.1.1"></times><cn type="integer" id="S4.SS2.p3.4.m4.1.1.2.cmml" xref="S4.SS2.p3.4.m4.1.1.2">8</cn><cn type="integer" id="S4.SS2.p3.4.m4.1.1.3.cmml" xref="S4.SS2.p3.4.m4.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.4.m4.1c">8\times 8</annotation></semantics></math> patch sizes. Our Xcit based models outperform both the TFPose and PRTR networks at their corresponding resolutions, while having the same number of parameters as PRTR.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.8" class="ltx_p">Overall, patch size of <math id="S4.SS2.p4.1.m1.1" class="ltx_Math" alttext="8\times 8" display="inline"><semantics id="S4.SS2.p4.1.m1.1a"><mrow id="S4.SS2.p4.1.m1.1.1" xref="S4.SS2.p4.1.m1.1.1.cmml"><mn id="S4.SS2.p4.1.m1.1.1.2" xref="S4.SS2.p4.1.m1.1.1.2.cmml">8</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p4.1.m1.1.1.1" xref="S4.SS2.p4.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS2.p4.1.m1.1.1.3" xref="S4.SS2.p4.1.m1.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.1.m1.1b"><apply id="S4.SS2.p4.1.m1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1"><times id="S4.SS2.p4.1.m1.1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1.1"></times><cn type="integer" id="S4.SS2.p4.1.m1.1.1.2.cmml" xref="S4.SS2.p4.1.m1.1.1.2">8</cn><cn type="integer" id="S4.SS2.p4.1.m1.1.1.3.cmml" xref="S4.SS2.p4.1.m1.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.1.m1.1c">8\times 8</annotation></semantics></math> pixels and resolutions of <math id="S4.SS2.p4.2.m2.1" class="ltx_Math" alttext="288\times 384" display="inline"><semantics id="S4.SS2.p4.2.m2.1a"><mrow id="S4.SS2.p4.2.m2.1.1" xref="S4.SS2.p4.2.m2.1.1.cmml"><mn id="S4.SS2.p4.2.m2.1.1.2" xref="S4.SS2.p4.2.m2.1.1.2.cmml">288</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p4.2.m2.1.1.1" xref="S4.SS2.p4.2.m2.1.1.1.cmml">×</mo><mn id="S4.SS2.p4.2.m2.1.1.3" xref="S4.SS2.p4.2.m2.1.1.3.cmml">384</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.2.m2.1b"><apply id="S4.SS2.p4.2.m2.1.1.cmml" xref="S4.SS2.p4.2.m2.1.1"><times id="S4.SS2.p4.2.m2.1.1.1.cmml" xref="S4.SS2.p4.2.m2.1.1.1"></times><cn type="integer" id="S4.SS2.p4.2.m2.1.1.2.cmml" xref="S4.SS2.p4.2.m2.1.1.2">288</cn><cn type="integer" id="S4.SS2.p4.2.m2.1.1.3.cmml" xref="S4.SS2.p4.2.m2.1.1.3">384</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.2.m2.1c">288\times 384</annotation></semantics></math> yield the best performance. In fact, our Xcit-based network (Xcit-dino-p8) at <math id="S4.SS2.p4.3.m3.1" class="ltx_Math" alttext="288\times 384" display="inline"><semantics id="S4.SS2.p4.3.m3.1a"><mrow id="S4.SS2.p4.3.m3.1.1" xref="S4.SS2.p4.3.m3.1.1.cmml"><mn id="S4.SS2.p4.3.m3.1.1.2" xref="S4.SS2.p4.3.m3.1.1.2.cmml">288</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p4.3.m3.1.1.1" xref="S4.SS2.p4.3.m3.1.1.1.cmml">×</mo><mn id="S4.SS2.p4.3.m3.1.1.3" xref="S4.SS2.p4.3.m3.1.1.3.cmml">384</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.3.m3.1b"><apply id="S4.SS2.p4.3.m3.1.1.cmml" xref="S4.SS2.p4.3.m3.1.1"><times id="S4.SS2.p4.3.m3.1.1.1.cmml" xref="S4.SS2.p4.3.m3.1.1.1"></times><cn type="integer" id="S4.SS2.p4.3.m3.1.1.2.cmml" xref="S4.SS2.p4.3.m3.1.1.2">288</cn><cn type="integer" id="S4.SS2.p4.3.m3.1.1.3.cmml" xref="S4.SS2.p4.3.m3.1.1.3">384</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.3.m3.1c">288\times 384</annotation></semantics></math> outperforms the PRTR trained at the same resolution by <math id="S4.SS2.p4.4.m4.1" class="ltx_Math" alttext="4.4\%" display="inline"><semantics id="S4.SS2.p4.4.m4.1a"><mrow id="S4.SS2.p4.4.m4.1.1" xref="S4.SS2.p4.4.m4.1.1.cmml"><mn id="S4.SS2.p4.4.m4.1.1.2" xref="S4.SS2.p4.4.m4.1.1.2.cmml">4.4</mn><mo id="S4.SS2.p4.4.m4.1.1.1" xref="S4.SS2.p4.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.4.m4.1b"><apply id="S4.SS2.p4.4.m4.1.1.cmml" xref="S4.SS2.p4.4.m4.1.1"><csymbol cd="latexml" id="S4.SS2.p4.4.m4.1.1.1.cmml" xref="S4.SS2.p4.4.m4.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.p4.4.m4.1.1.2.cmml" xref="S4.SS2.p4.4.m4.1.1.2">4.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.4.m4.1c">4.4\%</annotation></semantics></math> in AP and <math id="S4.SS2.p4.5.m5.1" class="ltx_Math" alttext="3.4\%" display="inline"><semantics id="S4.SS2.p4.5.m5.1a"><mrow id="S4.SS2.p4.5.m5.1.1" xref="S4.SS2.p4.5.m5.1.1.cmml"><mn id="S4.SS2.p4.5.m5.1.1.2" xref="S4.SS2.p4.5.m5.1.1.2.cmml">3.4</mn><mo id="S4.SS2.p4.5.m5.1.1.1" xref="S4.SS2.p4.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.5.m5.1b"><apply id="S4.SS2.p4.5.m5.1.1.cmml" xref="S4.SS2.p4.5.m5.1.1"><csymbol cd="latexml" id="S4.SS2.p4.5.m5.1.1.1.cmml" xref="S4.SS2.p4.5.m5.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.p4.5.m5.1.1.2.cmml" xref="S4.SS2.p4.5.m5.1.1.2">3.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.5.m5.1c">3.4\%</annotation></semantics></math> in AR and even outperforms PRTR at resolution <math id="S4.SS2.p4.6.m6.1" class="ltx_Math" alttext="384\times 512" display="inline"><semantics id="S4.SS2.p4.6.m6.1a"><mrow id="S4.SS2.p4.6.m6.1.1" xref="S4.SS2.p4.6.m6.1.1.cmml"><mn id="S4.SS2.p4.6.m6.1.1.2" xref="S4.SS2.p4.6.m6.1.1.2.cmml">384</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p4.6.m6.1.1.1" xref="S4.SS2.p4.6.m6.1.1.1.cmml">×</mo><mn id="S4.SS2.p4.6.m6.1.1.3" xref="S4.SS2.p4.6.m6.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.6.m6.1b"><apply id="S4.SS2.p4.6.m6.1.1.cmml" xref="S4.SS2.p4.6.m6.1.1"><times id="S4.SS2.p4.6.m6.1.1.1.cmml" xref="S4.SS2.p4.6.m6.1.1.1"></times><cn type="integer" id="S4.SS2.p4.6.m6.1.1.2.cmml" xref="S4.SS2.p4.6.m6.1.1.2">384</cn><cn type="integer" id="S4.SS2.p4.6.m6.1.1.3.cmml" xref="S4.SS2.p4.6.m6.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.6.m6.1c">384\times 512</annotation></semantics></math> by <math id="S4.SS2.p4.7.m7.1" class="ltx_Math" alttext="1.6\%" display="inline"><semantics id="S4.SS2.p4.7.m7.1a"><mrow id="S4.SS2.p4.7.m7.1.1" xref="S4.SS2.p4.7.m7.1.1.cmml"><mn id="S4.SS2.p4.7.m7.1.1.2" xref="S4.SS2.p4.7.m7.1.1.2.cmml">1.6</mn><mo id="S4.SS2.p4.7.m7.1.1.1" xref="S4.SS2.p4.7.m7.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.7.m7.1b"><apply id="S4.SS2.p4.7.m7.1.1.cmml" xref="S4.SS2.p4.7.m7.1.1"><csymbol cd="latexml" id="S4.SS2.p4.7.m7.1.1.1.cmml" xref="S4.SS2.p4.7.m7.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.p4.7.m7.1.1.2.cmml" xref="S4.SS2.p4.7.m7.1.1.2">1.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.7.m7.1c">1.6\%</annotation></semantics></math> in AP. However, experiments of the Xcit variant at even higher resolutions (<math id="S4.SS2.p4.8.m8.1" class="ltx_Math" alttext="384\times 512" display="inline"><semantics id="S4.SS2.p4.8.m8.1a"><mrow id="S4.SS2.p4.8.m8.1.1" xref="S4.SS2.p4.8.m8.1.1.cmml"><mn id="S4.SS2.p4.8.m8.1.1.2" xref="S4.SS2.p4.8.m8.1.1.2.cmml">384</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p4.8.m8.1.1.1" xref="S4.SS2.p4.8.m8.1.1.1.cmml">×</mo><mn id="S4.SS2.p4.8.m8.1.1.3" xref="S4.SS2.p4.8.m8.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.8.m8.1b"><apply id="S4.SS2.p4.8.m8.1.1.cmml" xref="S4.SS2.p4.8.m8.1.1"><times id="S4.SS2.p4.8.m8.1.1.1.cmml" xref="S4.SS2.p4.8.m8.1.1.1"></times><cn type="integer" id="S4.SS2.p4.8.m8.1.1.2.cmml" xref="S4.SS2.p4.8.m8.1.1.2">384</cn><cn type="integer" id="S4.SS2.p4.8.m8.1.1.3.cmml" xref="S4.SS2.p4.8.m8.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.8.m8.1c">384\times 512</annotation></semantics></math>) did not show any significant improvements.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.3.1.1" class="ltx_tr">
<th id="S4.T1.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_rr ltx_border_t"><span id="S4.T1.3.1.1.1.1" class="ltx_text ltx_font_bold">Method</span></th>
<th id="S4.T1.3.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.3.1.1.2.1" class="ltx_text ltx_font_bold">Input size</span></th>
<th id="S4.T1.3.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.3.1.1.3.1" class="ltx_text ltx_font_bold">#Parameters</span></th>
<th id="S4.T1.3.1.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.3.1.1.4.1" class="ltx_text ltx_font_bold">AP</span></th>
<th id="S4.T1.3.1.1.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.3.1.1.5.1" class="ltx_text ltx_font_bold">AR</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.3.2.1" class="ltx_tr">
<th id="S4.T1.3.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_tt">TFPose</th>
<td id="S4.T1.3.2.1.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">192x256</td>
<td id="S4.T1.3.2.1.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">–</td>
<td id="S4.T1.3.2.1.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">71.0</td>
<td id="S4.T1.3.2.1.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">–</td>
</tr>
<tr id="S4.T1.3.3.2" class="ltx_tr">
<th id="S4.T1.3.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">TFPose</th>
<td id="S4.T1.3.3.2.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">288x384</td>
<td id="S4.T1.3.3.2.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">–</td>
<td id="S4.T1.3.3.2.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">72.4</td>
<td id="S4.T1.3.3.2.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">–</td>
</tr>
<tr id="S4.T1.3.4.3" class="ltx_tr">
<th id="S4.T1.3.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">PRTR</th>
<td id="S4.T1.3.4.3.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">288x384</td>
<td id="S4.T1.3.4.3.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">41.5M</td>
<td id="S4.T1.3.4.3.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">68.2</td>
<td id="S4.T1.3.4.3.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">76</td>
</tr>
<tr id="S4.T1.3.5.4" class="ltx_tr">
<th id="S4.T1.3.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">PRTR</th>
<td id="S4.T1.3.5.4.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">384x512</td>
<td id="S4.T1.3.5.4.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">41.5M</td>
<td id="S4.T1.3.5.4.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">71.0</td>
<td id="S4.T1.3.5.4.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">78</td>
</tr>
<tr id="S4.T1.3.6.5" class="ltx_tr">
<th id="S4.T1.3.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">OURS-Deit-dino-p8</th>
<td id="S4.T1.3.6.5.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">192x256</td>
<td id="S4.T1.3.6.5.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">36.4M</td>
<td id="S4.T1.3.6.5.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">70.6</td>
<td id="S4.T1.3.6.5.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">78.1</td>
</tr>
<tr id="S4.T1.3.7.6" class="ltx_tr">
<th id="S4.T1.3.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">OURS-Xcit-p16</th>
<td id="S4.T1.3.7.6.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">288x384</td>
<td id="S4.T1.3.7.6.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">40.6M</td>
<td id="S4.T1.3.7.6.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">70.2</td>
<td id="S4.T1.3.7.6.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">77.4</td>
</tr>
<tr id="S4.T1.3.8.7" class="ltx_tr">
<th id="S4.T1.3.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">OURS-Xcit-dino-p16</th>
<td id="S4.T1.3.8.7.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">288x384</td>
<td id="S4.T1.3.8.7.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">40.6M</td>
<td id="S4.T1.3.8.7.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">70.7</td>
<td id="S4.T1.3.8.7.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">77.9</td>
</tr>
<tr id="S4.T1.3.9.8" class="ltx_tr">
<th id="S4.T1.3.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">OURS-Xcit-dino-p8</th>
<td id="S4.T1.3.9.8.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">192x256</td>
<td id="S4.T1.3.9.8.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">40.5M</td>
<td id="S4.T1.3.9.8.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">71.6</td>
<td id="S4.T1.3.9.8.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">78.7</td>
</tr>
<tr id="S4.T1.3.10.9" class="ltx_tr">
<th id="S4.T1.3.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_rr ltx_border_t">OURS-Xcit-dino-p8</th>
<td id="S4.T1.3.10.9.2" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t">288x384</td>
<td id="S4.T1.3.10.9.3" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t">40.5M</td>
<td id="S4.T1.3.10.9.4" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t">72.6</td>
<td id="S4.T1.3.10.9.5" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t">79.4</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>A comparison of transformer based methods for 2D body pose estimation with direct regression. Both TFPose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and PRTR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> models use a Resnet50 for backbone. The dataset is COCO2017-Val. Flip test is used on all methods. The decoder depth for all models is <math id="S4.T1.2.m1.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S4.T1.2.m1.1b"><mn id="S4.T1.2.m1.1.1" xref="S4.T1.2.m1.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S4.T1.2.m1.1c"><cn type="integer" id="S4.T1.2.m1.1.1.cmml" xref="S4.T1.2.m1.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.m1.1d">6</annotation></semantics></math>. Note: the code for TFPose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> is not available and the model is not provided by the authors, so the number of parameters is unknown. In our work the “-Dino” suffix denotes the use of unsupervised pretraining, Xcit refers to Xcit-small-12, and Deit refers to Deit-small.</figcaption>
</figure>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.7" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.7.1.1" class="ltx_tr">
<th id="S4.T2.7.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_rr ltx_border_t"><span id="S4.T2.7.1.1.1.1" class="ltx_text ltx_font_bold">Method</span></th>
<th id="S4.T2.7.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T2.7.1.1.2.1" class="ltx_text ltx_font_bold">#Parameters</span></th>
<th id="S4.T2.7.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T2.7.1.1.3.1" class="ltx_text ltx_font_bold">AP</span></th>
<th id="S4.T2.7.1.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T2.7.1.1.4.1" class="ltx_text ltx_font_bold">AR</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.7.2.1" class="ltx_tr">
<th id="S4.T2.7.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_tt">Resnet50-PE-former</th>
<td id="S4.T2.7.2.1.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">39.0M</td>
<td id="S4.T2.7.2.1.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">63.4</td>
<td id="S4.T2.7.2.1.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">72.2</td>
</tr>
<tr id="S4.T2.7.3.2" class="ltx_tr">
<th id="S4.T2.7.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">VAB</th>
<td id="S4.T2.7.3.2.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">47.4M</td>
<td id="S4.T2.7.3.2.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">63.2</td>
<td id="S4.T2.7.3.2.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">72.6</td>
</tr>
<tr id="S4.T2.7.4.3" class="ltx_tr">
<th id="S4.T2.7.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">Deit</th>
<td id="S4.T2.7.4.3.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">36.4M</td>
<td id="S4.T2.7.4.3.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">62.2</td>
<td id="S4.T2.7.4.3.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">71.6</td>
</tr>
<tr id="S4.T2.7.5.4" class="ltx_tr">
<th id="S4.T2.7.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_rr ltx_border_t">Xcit</th>
<td id="S4.T2.7.5.4.2" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t">40.6M</td>
<td id="S4.T2.7.5.4.3" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t">66.2</td>
<td id="S4.T2.7.5.4.4" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t">74.6</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Encoder Comparison. Deit vs Xcit vs resnet50 vs VAB.
For all experiments the patch size is set to <math id="S4.T2.4.m1.1" class="ltx_Math" alttext="16\times 16" display="inline"><semantics id="S4.T2.4.m1.1b"><mrow id="S4.T2.4.m1.1.1" xref="S4.T2.4.m1.1.1.cmml"><mn id="S4.T2.4.m1.1.1.2" xref="S4.T2.4.m1.1.1.2.cmml">16</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T2.4.m1.1.1.1" xref="S4.T2.4.m1.1.1.1.cmml">×</mo><mn id="S4.T2.4.m1.1.1.3" xref="S4.T2.4.m1.1.1.3.cmml">16</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.4.m1.1c"><apply id="S4.T2.4.m1.1.1.cmml" xref="S4.T2.4.m1.1.1"><times id="S4.T2.4.m1.1.1.1.cmml" xref="S4.T2.4.m1.1.1.1"></times><cn type="integer" id="S4.T2.4.m1.1.1.2.cmml" xref="S4.T2.4.m1.1.1.2">16</cn><cn type="integer" id="S4.T2.4.m1.1.1.3.cmml" xref="S4.T2.4.m1.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.m1.1d">16\times 16</annotation></semantics></math> pixels. Input resolution is <math id="S4.T2.5.m2.1" class="ltx_Math" alttext="192\times 256" display="inline"><semantics id="S4.T2.5.m2.1b"><mrow id="S4.T2.5.m2.1.1" xref="S4.T2.5.m2.1.1.cmml"><mn id="S4.T2.5.m2.1.1.2" xref="S4.T2.5.m2.1.1.2.cmml">192</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T2.5.m2.1.1.1" xref="S4.T2.5.m2.1.1.1.cmml">×</mo><mn id="S4.T2.5.m2.1.1.3" xref="S4.T2.5.m2.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.5.m2.1c"><apply id="S4.T2.5.m2.1.1.cmml" xref="S4.T2.5.m2.1.1"><times id="S4.T2.5.m2.1.1.1.cmml" xref="S4.T2.5.m2.1.1.1"></times><cn type="integer" id="S4.T2.5.m2.1.1.2.cmml" xref="S4.T2.5.m2.1.1.2">192</cn><cn type="integer" id="S4.T2.5.m2.1.1.3.cmml" xref="S4.T2.5.m2.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.m2.1d">192\times 256</annotation></semantics></math>.
All Networks are trained and evaluated on the COCO val dataset. Deit performs worse while also having the smallest number of parameters. Xcit is the best performing overall, however it also has <math id="S4.T2.6.m3.1" class="ltx_Math" alttext="10\%" display="inline"><semantics id="S4.T2.6.m3.1b"><mrow id="S4.T2.6.m3.1.1" xref="S4.T2.6.m3.1.1.cmml"><mn id="S4.T2.6.m3.1.1.2" xref="S4.T2.6.m3.1.1.2.cmml">10</mn><mo id="S4.T2.6.m3.1.1.1" xref="S4.T2.6.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.6.m3.1c"><apply id="S4.T2.6.m3.1.1.cmml" xref="S4.T2.6.m3.1.1"><csymbol cd="latexml" id="S4.T2.6.m3.1.1.1.cmml" xref="S4.T2.6.m3.1.1.1">percent</csymbol><cn type="integer" id="S4.T2.6.m3.1.1.2.cmml" xref="S4.T2.6.m3.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.m3.1d">10\%</annotation></semantics></math> more parameters than Deit.</figcaption>
</figure>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.5.1.1" class="ltx_tr">
<th id="S4.T3.5.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_rr ltx_border_t"><span id="S4.T3.5.1.1.1.1" class="ltx_text ltx_font_bold">Method</span></th>
<th id="S4.T3.5.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T3.5.1.1.2.1" class="ltx_text ltx_font_bold"># Parameters</span></th>
<th id="S4.T3.5.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T3.5.1.1.3.1" class="ltx_text ltx_font_bold">AP</span></th>
<th id="S4.T3.5.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T3.5.1.1.4.1" class="ltx_text ltx_font_bold">AR</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.5.2.1" class="ltx_tr">
<th id="S4.T3.5.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_tt">Deit</th>
<td id="S4.T3.5.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">36.4M</td>
<td id="S4.T3.5.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">62.2</td>
<td id="S4.T3.5.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">71.6</td>
</tr>
<tr id="S4.T3.5.3.2" class="ltx_tr">
<th id="S4.T3.5.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">Dino Deit</th>
<td id="S4.T3.5.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">36.4M</td>
<td id="S4.T3.5.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">66.7</td>
<td id="S4.T3.5.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">75.0</td>
</tr>
<tr id="S4.T3.5.4.3" class="ltx_tr">
<th id="S4.T3.5.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">Xcit</th>
<td id="S4.T3.5.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">40.6M</td>
<td id="S4.T3.5.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">66.2</td>
<td id="S4.T3.5.4.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">74.6</td>
</tr>
<tr id="S4.T3.5.5.4" class="ltx_tr">
<th id="S4.T3.5.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">Dino Xcit</th>
<td id="S4.T3.5.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">40.6M</td>
<td id="S4.T3.5.5.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">68.0</td>
<td id="S4.T3.5.5.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">76.1</td>
</tr>
<tr id="S4.T3.5.6.5" class="ltx_tr">
<th id="S4.T3.5.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_tt">Resnet50-PE-former</th>
<td id="S4.T3.5.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">39.0M</td>
<td id="S4.T3.5.6.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">63.4</td>
<td id="S4.T3.5.6.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">72.2</td>
</tr>
<tr id="S4.T3.5.7.6" class="ltx_tr">
<th id="S4.T3.5.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_rr ltx_border_t">Dino Resnet50-PE-former</th>
<td id="S4.T3.5.7.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">39.0M</td>
<td id="S4.T3.5.7.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">61.0</td>
<td id="S4.T3.5.7.6.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">70.1</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparison of Unsupervised Dino weight init.
Patch size is set to <math id="S4.T3.3.m1.1" class="ltx_Math" alttext="16" display="inline"><semantics id="S4.T3.3.m1.1b"><mn id="S4.T3.3.m1.1.1" xref="S4.T3.3.m1.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="S4.T3.3.m1.1c"><cn type="integer" id="S4.T3.3.m1.1.1.cmml" xref="S4.T3.3.m1.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.m1.1d">16</annotation></semantics></math> and input size is <math id="S4.T3.4.m2.1" class="ltx_Math" alttext="192\times 256" display="inline"><semantics id="S4.T3.4.m2.1b"><mrow id="S4.T3.4.m2.1.1" xref="S4.T3.4.m2.1.1.cmml"><mn id="S4.T3.4.m2.1.1.2" xref="S4.T3.4.m2.1.1.2.cmml">192</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T3.4.m2.1.1.1" xref="S4.T3.4.m2.1.1.1.cmml">×</mo><mn id="S4.T3.4.m2.1.1.3" xref="S4.T3.4.m2.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.4.m2.1c"><apply id="S4.T3.4.m2.1.1.cmml" xref="S4.T3.4.m2.1.1"><times id="S4.T3.4.m2.1.1.1.cmml" xref="S4.T3.4.m2.1.1.1"></times><cn type="integer" id="S4.T3.4.m2.1.1.2.cmml" xref="S4.T3.4.m2.1.1.2">192</cn><cn type="integer" id="S4.T3.4.m2.1.1.3.cmml" xref="S4.T3.4.m2.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.m2.1d">192\times 256</annotation></semantics></math>. Dino networks are
identical but initialized with the weights of networks trained with the dino unsupervised methodology. Networks are trained and evaluated on the COCO val dataset. Interestingly, the Resnet50 variant does not show the same improvements as the attention-based encoders with unsupervised pre-training.</figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Transformer vs CNN vs VAB Encoders</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.2" class="ltx_p">We evaluate the performance of a transformer based encoder (Deit, Xcit) versus a
CNN (Resnet50) based encoder and a VAB model. The networks are trained on COCO
and evaluated on the COCO2017-val.
For all the encoders, the imagenet pretrained weights are used. The results are shown in Table <a href="#S4.T2" title="Table 2 ‣ 4.2 Comparison with SOTA ‣ 4 Experiments ‣ PE-former: Pose Estimation Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The networks are trained with an input resolution of <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="192\times 256" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mrow id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><mn id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2.cmml">192</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.p1.1.m1.1.1.1" xref="S4.SS3.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS3.p1.1.m1.1.1.3" xref="S4.SS3.p1.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><times id="S4.SS3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS3.p1.1.m1.1.1.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2">192</cn><cn type="integer" id="S4.SS3.p1.1.m1.1.1.3.cmml" xref="S4.SS3.p1.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">192\times 256</annotation></semantics></math> and patch size of <math id="S4.SS3.p1.2.m2.1" class="ltx_Math" alttext="16\times 16" display="inline"><semantics id="S4.SS3.p1.2.m2.1a"><mrow id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml"><mn id="S4.SS3.p1.2.m2.1.1.2" xref="S4.SS3.p1.2.m2.1.1.2.cmml">16</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.p1.2.m2.1.1.1" xref="S4.SS3.p1.2.m2.1.1.1.cmml">×</mo><mn id="S4.SS3.p1.2.m2.1.1.3" xref="S4.SS3.p1.2.m2.1.1.3.cmml">16</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><apply id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1"><times id="S4.SS3.p1.2.m2.1.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1.1"></times><cn type="integer" id="S4.SS3.p1.2.m2.1.1.2.cmml" xref="S4.SS3.p1.2.m2.1.1.2">16</cn><cn type="integer" id="S4.SS3.p1.2.m2.1.1.3.cmml" xref="S4.SS3.p1.2.m2.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">16\times 16</annotation></semantics></math> pixels.
Apart from replacing the encoders, the rest of the model (decoder) and training hyperparameters are kept the same.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.4" class="ltx_p">As expected, VAB is the largest network with the highest memory and computational requirements.
However, it performs on par with the Resnet50 variant. The Resnet variant despite having almost the same number of parameters as Xcit, requires less memory and computational resources during training and inference. Still, it under-performs compared to Xcit by <math id="S4.SS3.p2.1.m1.1" class="ltx_Math" alttext="2.8\%" display="inline"><semantics id="S4.SS3.p2.1.m1.1a"><mrow id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml"><mn id="S4.SS3.p2.1.m1.1.1.2" xref="S4.SS3.p2.1.m1.1.1.2.cmml">2.8</mn><mo id="S4.SS3.p2.1.m1.1.1.1" xref="S4.SS3.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><apply id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"><csymbol cd="latexml" id="S4.SS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.SS3.p2.1.m1.1.1.2.cmml" xref="S4.SS3.p2.1.m1.1.1.2">2.8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">2.8\%</annotation></semantics></math> in AP and <math id="S4.SS3.p2.2.m2.1" class="ltx_Math" alttext="2.4\%" display="inline"><semantics id="S4.SS3.p2.2.m2.1a"><mrow id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml"><mn id="S4.SS3.p2.2.m2.1.1.2" xref="S4.SS3.p2.2.m2.1.1.2.cmml">2.4</mn><mo id="S4.SS3.p2.2.m2.1.1.1" xref="S4.SS3.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><apply id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1"><csymbol cd="latexml" id="S4.SS3.p2.2.m2.1.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1.1">percent</csymbol><cn type="float" id="S4.SS3.p2.2.m2.1.1.2.cmml" xref="S4.SS3.p2.2.m2.1.1.2">2.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">2.4\%</annotation></semantics></math> in AR.
The Deit variant performs a bit worse (<math id="S4.SS3.p2.3.m3.1" class="ltx_Math" alttext="1.2\%" display="inline"><semantics id="S4.SS3.p2.3.m3.1a"><mrow id="S4.SS3.p2.3.m3.1.1" xref="S4.SS3.p2.3.m3.1.1.cmml"><mn id="S4.SS3.p2.3.m3.1.1.2" xref="S4.SS3.p2.3.m3.1.1.2.cmml">1.2</mn><mo id="S4.SS3.p2.3.m3.1.1.1" xref="S4.SS3.p2.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.3.m3.1b"><apply id="S4.SS3.p2.3.m3.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1"><csymbol cd="latexml" id="S4.SS3.p2.3.m3.1.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1.1">percent</csymbol><cn type="float" id="S4.SS3.p2.3.m3.1.1.2.cmml" xref="S4.SS3.p2.3.m3.1.1.2">1.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.3.m3.1c">1.2\%</annotation></semantics></math> in AP) than Resnet but it is also smaller by <math id="S4.SS3.p2.4.m4.1" class="ltx_Math" alttext="2.6M" display="inline"><semantics id="S4.SS3.p2.4.m4.1a"><mrow id="S4.SS3.p2.4.m4.1.1" xref="S4.SS3.p2.4.m4.1.1.cmml"><mn id="S4.SS3.p2.4.m4.1.1.2" xref="S4.SS3.p2.4.m4.1.1.2.cmml">2.6</mn><mo lspace="0em" rspace="0em" id="S4.SS3.p2.4.m4.1.1.1" xref="S4.SS3.p2.4.m4.1.1.1.cmml">​</mo><mi id="S4.SS3.p2.4.m4.1.1.3" xref="S4.SS3.p2.4.m4.1.1.3.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.4.m4.1b"><apply id="S4.SS3.p2.4.m4.1.1.cmml" xref="S4.SS3.p2.4.m4.1.1"><times id="S4.SS3.p2.4.m4.1.1.1.cmml" xref="S4.SS3.p2.4.m4.1.1.1"></times><cn type="float" id="S4.SS3.p2.4.m4.1.1.2.cmml" xref="S4.SS3.p2.4.m4.1.1.2">2.6</cn><ci id="S4.SS3.p2.4.m4.1.1.3.cmml" xref="S4.SS3.p2.4.m4.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.4.m4.1c">2.6M</annotation></semantics></math> parameters.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.2" class="ltx_p">Xcit exhibits the best overall performance with a lead of <math id="S4.SS3.p3.1.m1.1" class="ltx_Math" alttext="2.8\%" display="inline"><semantics id="S4.SS3.p3.1.m1.1a"><mrow id="S4.SS3.p3.1.m1.1.1" xref="S4.SS3.p3.1.m1.1.1.cmml"><mn id="S4.SS3.p3.1.m1.1.1.2" xref="S4.SS3.p3.1.m1.1.1.2.cmml">2.8</mn><mo id="S4.SS3.p3.1.m1.1.1.1" xref="S4.SS3.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.1b"><apply id="S4.SS3.p3.1.m1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1"><csymbol cd="latexml" id="S4.SS3.p3.1.m1.1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.SS3.p3.1.m1.1.1.2.cmml" xref="S4.SS3.p3.1.m1.1.1.2">2.8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">2.8\%</annotation></semantics></math> in AP and <math id="S4.SS3.p3.2.m2.1" class="ltx_Math" alttext="2.4\%" display="inline"><semantics id="S4.SS3.p3.2.m2.1a"><mrow id="S4.SS3.p3.2.m2.1.1" xref="S4.SS3.p3.2.m2.1.1.cmml"><mn id="S4.SS3.p3.2.m2.1.1.2" xref="S4.SS3.p3.2.m2.1.1.2.cmml">2.4</mn><mo id="S4.SS3.p3.2.m2.1.1.1" xref="S4.SS3.p3.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.2.m2.1b"><apply id="S4.SS3.p3.2.m2.1.1.cmml" xref="S4.SS3.p3.2.m2.1.1"><csymbol cd="latexml" id="S4.SS3.p3.2.m2.1.1.1.cmml" xref="S4.SS3.p3.2.m2.1.1.1">percent</csymbol><cn type="float" id="S4.SS3.p3.2.m2.1.1.2.cmml" xref="S4.SS3.p3.2.m2.1.1.2">2.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.2.m2.1c">2.4\%</annotation></semantics></math> in AR over the Resnet50 variant.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Unsupervised pre-training</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">All the models presented in Section <a href="#S3" title="3 Method ‣ PE-former: Pose Estimation Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> use pre-trained
weights for the encoder part, while the decoder’s weights are randomly initialized.
We use the weights as they are provided by the authors of each architecture
Deit<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://github.com/facebookresearch/deit" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/facebookresearch/deit</a></span></span></span>,
Xcit<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://github.com/facebookresearch/xcit" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/facebookresearch/xcit</a></span></span></span>).
Additionally, we evaluate the use of weights<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://github.com/facebookresearch/dino" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/facebookresearch/dino</a></span></span></span>
created using unsupervised learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.
More specifically, we evaluate using the Deit, Xcit and Resnet50 encoders.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.2" class="ltx_p">Table <a href="#S4.T3" title="Table 3 ‣ 4.2 Comparison with SOTA ‣ 4 Experiments ‣ PE-former: Pose Estimation Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> presents the obtained results.
For the purposes of our evaluation we train two Deit (deit-small) and two Xcit (xcit-small-12) variants. For all variants, input resolution of <math id="S4.SS4.p2.1.m1.1" class="ltx_Math" alttext="192\times 256" display="inline"><semantics id="S4.SS4.p2.1.m1.1a"><mrow id="S4.SS4.p2.1.m1.1.1" xref="S4.SS4.p2.1.m1.1.1.cmml"><mn id="S4.SS4.p2.1.m1.1.1.2" xref="S4.SS4.p2.1.m1.1.1.2.cmml">192</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS4.p2.1.m1.1.1.1" xref="S4.SS4.p2.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS4.p2.1.m1.1.1.3" xref="S4.SS4.p2.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.1.m1.1b"><apply id="S4.SS4.p2.1.m1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1"><times id="S4.SS4.p2.1.m1.1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1.1"></times><cn type="integer" id="S4.SS4.p2.1.m1.1.1.2.cmml" xref="S4.SS4.p2.1.m1.1.1.2">192</cn><cn type="integer" id="S4.SS4.p2.1.m1.1.1.3.cmml" xref="S4.SS4.p2.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.1.m1.1c">192\times 256</annotation></semantics></math> and patch size is set to <math id="S4.SS4.p2.2.m2.1" class="ltx_Math" alttext="16\times 16" display="inline"><semantics id="S4.SS4.p2.2.m2.1a"><mrow id="S4.SS4.p2.2.m2.1.1" xref="S4.SS4.p2.2.m2.1.1.cmml"><mn id="S4.SS4.p2.2.m2.1.1.2" xref="S4.SS4.p2.2.m2.1.1.2.cmml">16</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS4.p2.2.m2.1.1.1" xref="S4.SS4.p2.2.m2.1.1.1.cmml">×</mo><mn id="S4.SS4.p2.2.m2.1.1.3" xref="S4.SS4.p2.2.m2.1.1.3.cmml">16</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.2.m2.1b"><apply id="S4.SS4.p2.2.m2.1.1.cmml" xref="S4.SS4.p2.2.m2.1.1"><times id="S4.SS4.p2.2.m2.1.1.1.cmml" xref="S4.SS4.p2.2.m2.1.1.1"></times><cn type="integer" id="S4.SS4.p2.2.m2.1.1.2.cmml" xref="S4.SS4.p2.2.m2.1.1.2">16</cn><cn type="integer" id="S4.SS4.p2.2.m2.1.1.3.cmml" xref="S4.SS4.p2.2.m2.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.2.m2.1c">16\times 16</annotation></semantics></math>. All hyperparameters are kept the same. We initialize the encoders of the Dino
variants using weights acquired with unsupervised learning on Imagenet. In contrast, the normal variants start with encoders initialized with weights acquired with supervised learning on Imagenet. For both Deit and Xcit we observe significant improvement on the performance.
However, the amount of improvement drops as the overall performance of the network gets higher (see the entries of Table <a href="#S4.T1" title="Table 1 ‣ 4.2 Comparison with SOTA ‣ 4 Experiments ‣ PE-former: Pose Estimation Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> for <em id="S4.SS4.p2.2.1" class="ltx_emph ltx_font_italic">OURS-Xcit-p16</em> and <em id="S4.SS4.p2.2.2" class="ltx_emph ltx_font_italic">OURS-Xcit-dino-p16</em>).</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p">As the authors of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> hypothesise, we believe that this improvement
stems from having the dino-trained model learn a different embedding than the supervised variant.
The model is focusing only on the salient parts of the image (i.e., the person) and is not miss-guided by annotation biases and errors that affect the supervised approach. This hypotheses is further supported by the recent work of He et al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> on unsupervised learning using masked autoencoders.</p>
</div>
<div id="S4.SS4.p4" class="ltx_para">
<p id="S4.SS4.p4.1" class="ltx_p">Interestingly, our CNN based encoder (Resnet50) does not benefit from the unsupervised pretraining. The resnet50-PE-former-dino pretrained with the DINO weights, yields worse results than the supervised
variant (resnet50-PE-former). This result hints to a fundamentally different way the transformer-based methods learn about salient image parts:
When training a CNN, interactions are local on each layer (i.e., depend on the size of the convolutional kernel).
On the other hand, vision transformers such as Xcit and Deit, allow for long-range interactions. Long range interactions enable the exchange of information between areas in the image that are far apart. For DINO the implicit task during training is to identify salient parts in an image. As a consequence long range interactions help the network reason about larger objects (i.e, a person).</p>
</div>
<div id="S4.SS4.p5" class="ltx_para">
<p id="S4.SS4.p5.1" class="ltx_p">For models with the transformer based encoders (deit, xcit), it is possible that further fine tuning such as pre-training on the same resolution as the final network or pre-training on a person dataset (i.e COCO-person) instead of imagenet, could further boost the final gains.
However these tasks are beyond the scope of this work and are left for future work.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.3" class="ltx_p">We presented a novel encoder-decoder architecture using only transformers that achieves SOTA results for the task for single instance 2D human pose estimation. We evaluated two very promising ViT variants as our encoders: Xcit and Deit. We verified the positive impact of our transformer based encoder-decoder by comparing with modified versions of our model with a CNN-encoder and a VAB variant.
Out model using the Xcit based encoder, performs best both in accuracy and resource requirements, outperforming contemporary methods by as mach as <math id="S5.p1.1.m1.1" class="ltx_Math" alttext="4.4\%" display="inline"><semantics id="S5.p1.1.m1.1a"><mrow id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml"><mn id="S5.p1.1.m1.1.1.2" xref="S5.p1.1.m1.1.1.2.cmml">4.4</mn><mo id="S5.p1.1.m1.1.1.1" xref="S5.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><apply id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1"><csymbol cd="latexml" id="S5.p1.1.m1.1.1.1.cmml" xref="S5.p1.1.m1.1.1.1">percent</csymbol><cn type="float" id="S5.p1.1.m1.1.1.2.cmml" xref="S5.p1.1.m1.1.1.2">4.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">4.4\%</annotation></semantics></math> in AP on the COCO-val dataset.
Our Deit based encoder variant is on par with methods using CNN based backbones for patch sizes of <math id="S5.p1.2.m2.1" class="ltx_Math" alttext="8\times 8" display="inline"><semantics id="S5.p1.2.m2.1a"><mrow id="S5.p1.2.m2.1.1" xref="S5.p1.2.m2.1.1.cmml"><mn id="S5.p1.2.m2.1.1.2" xref="S5.p1.2.m2.1.1.2.cmml">8</mn><mo lspace="0.222em" rspace="0.222em" id="S5.p1.2.m2.1.1.1" xref="S5.p1.2.m2.1.1.1.cmml">×</mo><mn id="S5.p1.2.m2.1.1.3" xref="S5.p1.2.m2.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.2.m2.1b"><apply id="S5.p1.2.m2.1.1.cmml" xref="S5.p1.2.m2.1.1"><times id="S5.p1.2.m2.1.1.1.cmml" xref="S5.p1.2.m2.1.1.1"></times><cn type="integer" id="S5.p1.2.m2.1.1.2.cmml" xref="S5.p1.2.m2.1.1.2">8</cn><cn type="integer" id="S5.p1.2.m2.1.1.3.cmml" xref="S5.p1.2.m2.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.2.m2.1c">8\times 8</annotation></semantics></math> pixels.
Furthermore, we demonstrate that using unsupervised pretraining can improve performance, especially for larger patch sizes (i.e., <math id="S5.p1.3.m3.1" class="ltx_Math" alttext="16\times 16" display="inline"><semantics id="S5.p1.3.m3.1a"><mrow id="S5.p1.3.m3.1.1" xref="S5.p1.3.m3.1.1.cmml"><mn id="S5.p1.3.m3.1.1.2" xref="S5.p1.3.m3.1.1.2.cmml">16</mn><mo lspace="0.222em" rspace="0.222em" id="S5.p1.3.m3.1.1.1" xref="S5.p1.3.m3.1.1.1.cmml">×</mo><mn id="S5.p1.3.m3.1.1.3" xref="S5.p1.3.m3.1.1.3.cmml">16</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.3.m3.1b"><apply id="S5.p1.3.m3.1.1.cmml" xref="S5.p1.3.m3.1.1"><times id="S5.p1.3.m3.1.1.1.cmml" xref="S5.p1.3.m3.1.1.1"></times><cn type="integer" id="S5.p1.3.m3.1.1.2.cmml" xref="S5.p1.3.m3.1.1.2">16</cn><cn type="integer" id="S5.p1.3.m3.1.1.3.cmml" xref="S5.p1.3.m3.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.3.m3.1c">16\times 16</annotation></semantics></math>).</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Attention-based models look promising for a range of vision tasks.
It is conceivable that, with further improvements on the architectures or
the training methodologies, these models will dethrone the older CNN-based
architectures both in accuracy and resource requirements.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.:
End-to-end object detection with transformers. In: European Conference on
Computer Vision. pp. 213–229. Springer (2020)

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Caron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., Bojanowski, P.,
Joulin, A.: Emerging properties in self-supervised vision transformers. arXiv
preprint arXiv:2104.14294 (2021)

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.:
An image is worth 16x16 words: Transformers for image recognition at scale.
arXiv preprint arXiv:2010.11929 (2020)

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
El-Nouby, A., Touvron, H., Caron, M., Bojanowski, P., Douze, M., Joulin, A.,
Laptev, I., Neverova, N., Synnaeve, G., Verbeek, J., et al.: Xcit:
Cross-covariance image transformers. arXiv preprint arXiv:2106.09681 (2021)

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
He, K., Chen, X., Xie, S., Li, Y., Dollár, P., Girshick, R.: Masked
autoencoders are scalable vision learners. arXiv preprint arXiv:2111.06377
(2021)

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image
recognition. In: Proceedings of the IEEE conference on computer vision and
pattern recognition. pp. 770–778 (2016)

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Li, K., Wang, S., Zhang, X., Xu, Y., Xu, W., Tu, Z.: Pose recognition with
cascade transformers. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. pp. 1944–1953 (2021)

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Mao, W., Ge, Y., Shen, C., Tian, Z., Wang, X., Wang, Z.: Tfpose: Direct human
pose estimation with transformers. arXiv preprint arXiv:2103.15320 (2021)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Mehta, S., Rastegari, M.: Mobilevit: Light-weight, general-purpose, and
mobile-friendly vision transformer. arXiv preprint arXiv:2110.02178 (2021)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Stoffl, L., Vidal, M., Mathis, A.: End-to-end trainable multi-instance pose
estimation with transformers. arXiv preprint arXiv:2103.12115 (2021)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Sun, K., Xiao, B., Liu, D., Wang, J.: Deep high-resolution representation
learning for human pose estimation. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. pp. 5693–5703 (2019)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., Jégou, H.:
Training data-efficient image transformers &amp; distillation through attention.
In: International Conference on Machine Learning. pp. 10347–10357. PMLR
(2021)

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,
Kaiser, Ł., Polosukhin, I.: Attention is all you need. In: Advances in
neural information processing systems. pp. 5998–6008 (2017)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Wang, S., Li, B.Z., Khabsa, M., Fang, H., Ma, H.: Linformer: Self-attention
with linear complexity. arXiv preprint arXiv:2006.04768 (2020)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Xiao, B., Wu, H., Wei, Y.: Simple baselines for human pose estimation and
tracking. In: European Conference on Computer Vision (ECCV) (2018)

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Xiong, Y., Zeng, Z., Chakraborty, R., Tan, M., Fung, G., Li, Y., Singh, V.:
Nystr<math id="bib.bib16.1.m1.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="bib.bib16.1.m1.1a"><mo id="bib.bib16.1.m1.1.1" xref="bib.bib16.1.m1.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="bib.bib16.1.m1.1b"><ci id="bib.bib16.1.m1.1.1.cmml" xref="bib.bib16.1.m1.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib16.1.m1.1c">\backslash</annotation></semantics></math>” omformer: A nystr<math id="bib.bib16.2.m2.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="bib.bib16.2.m2.1a"><mo id="bib.bib16.2.m2.1.1" xref="bib.bib16.2.m2.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="bib.bib16.2.m2.1b"><ci id="bib.bib16.2.m2.1.1.cmml" xref="bib.bib16.2.m2.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib16.2.m2.1c">\backslash</annotation></semantics></math>” om-based algorithm for
approximating self-attention. arXiv preprint arXiv:2102.03902 (2021)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Yang, S., Quan, Z., Nie, M., Yang, W.: Transpose: Keypoint localization via
transformer. In: Proceedings of the IEEE/CVF International Conference on
Computer Vision. pp. 11802–11812 (2021)

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Yuan, L., Chen, Y., Wang, T., Yu, W., Shi, Y., Jiang, Z., Tay, F.E., Feng, J.,
Yan, S.: Tokens-to-token vit: Training vision transformers from scratch on
imagenet. arXiv preprint arXiv:2101.11986 (2021)

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Zhang, Z., Zhang, H., Zhao, L., Chen, T., Pfister, T.: Aggregating nested
transformers. arXiv preprint arXiv:2105.12723 (2021)

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2112.04980" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2112.04981" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2112.04981">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2112.04981" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2112.04982" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 15:41:39 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
