<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2212.00352] A Dataset with Multibeam Forward-Looking Sonar for Underwater Object Detection</title><meta property="og:description" content="Multibeam forward-looking sonar (MFLS) plays an important role in underwater detection. There are several challenges to the research on underwater object detection with MFLS. Firstly, the research is lack of available …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A Dataset with Multibeam Forward-Looking Sonar for Underwater Object Detection">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="A Dataset with Multibeam Forward-Looking Sonar for Underwater Object Detection">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2212.00352">

<!--Generated on Fri Mar  1 08:27:28 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_fleqn">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\UseRawInputEncoding</span>
</div>
<h1 class="ltx_title ltx_title_document">A Dataset with Multibeam Forward-Looking Sonar for Underwater Object Detection</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kaibing Xie
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Peng Cheng Laboratory, Shenzhen, China
</span>
<span class="ltx_contact ltx_role_affiliation">
</span>
<span class="ltx_contact ltx_role_affiliation">Kaibing Xie(xiekb@pcl.ac.cn), Jian Yang(yangj01@pcl.ac.cn)
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jian Yang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Peng Cheng Laboratory, Shenzhen, China
</span>
<span class="ltx_contact ltx_role_affiliation">
</span>
<span class="ltx_contact ltx_role_affiliation">Kaibing Xie(xiekb@pcl.ac.cn), Jian Yang(yangj01@pcl.ac.cn)
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kang Qiu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Peng Cheng Laboratory, Shenzhen, China
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p"><span id="id1.id1.1" class="ltx_text ltx_font_bold">Multibeam forward-looking sonar (MFLS) plays an important role in underwater detection. There are several challenges to the research on underwater object detection with MFLS. Firstly, the research is lack of available dataset. Secondly, the sonar image, generally processed at pixel level and transformed to sector representation for the visual habits of human beings, is disadvantageous to the research in artificial intelligence (AI) areas. Towards these challenges, we present a novel dataset, the underwater acoustic target detection (UATD) dataset, consisting of over 9000 MFLS images captured using Tritech Gemini 1200ik sonar. Our dataset provides raw data of sonar images with annotation of 10 categories of target objects (cube, cylinder, tyres, etc). The data was collected from lake and shallow water. To verify the practicality of UATD, we apply the dataset to the state-of-the-art detectors and provide corresponding benchmarks for its accuracy and efficiency.</span></p>
</div>
<section id="Sx1" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">Background &amp; Summary</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">Object detection is becoming faster and more accurate with the development of AI technology. This helps underwater robots archive better performance in accident rescue, facilities maintenance, biological investigation and other underwater applications. Onshore AI algorithms are developing rapidly based on rich and high-quality datasets. In order to transfer AI achievements from land to underwater, appropriate underwater datasets are required. There have been several underwater optical datasets, such as Brackish<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> dataset, Segmentation of Underwater IMagery<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> (SUIM) dataset, Detecting Underwater Objects<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> (DUO) dataset, for the research on object detection, semantic segmentation and other AI applications. Due to the scattering and attenuation of the light in water, underwater optical imaging is a difficult task and often gets low quality images. So acoustic sensors are widely used for perceiving the underwater environment. MFLS is portable for underwater robots while providing dynamic real-time image data in high resolution. It is very applicable for scenarios requiring close and detailed inspection.</p>
</div>
<div id="Sx1.p2" class="ltx_para">
<p id="Sx1.p2.1" class="ltx_p">There have been previous works on underwater object detection and related applications with MFLS in AI areas. Haoting Zhang et al. proposed MFLS image target detection models based on You Only Look Once (YOLO) v5 network<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Zhimiao Fan et al. proposed a modified Mask Region Convolutional Neural Network (Mask RCNN) for MFLS image object detection and segmentation<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Longyu Jiang et al. proposed three simple but effective active-learning-based algorithms for MFLS image object detection<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Alan Preciado-Grijalva et al. investigated the potential of three self-supervised learning methods (RotNet, Denoising Autoencoders, and Jigsaw) to learn sonar image representation<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Gustavo Divas Karimanzira et al. proposed an underwater object detection solution with MFLS based on RCNN and deployed the solution on an NVIDIA Jetson TX2<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Neves et al. proposed a novel multi-object detection system using two novel convolutional neural network-based architectures that output object position and rotation from sonar images to support autonomous underwater vehicle (AUV) navigation<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. Xiang Cao et al. proposed an obstacle detection and avoidance algorithm for an AUV with MFLS, using the YOLOv3 network for obstacle detection<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. There are different defects among the datasets used in these researches, such as small sample size, few categories of objects, and virtual image data based on style transfer technology. The most important point is that most of the datasets in the related research are not public.</p>
</div>
<div id="Sx1.p3" class="ltx_para">
<p id="Sx1.p3.1" class="ltx_p">Underwater data collection often comes with high costs of economy, labor and time. Professionals are highly required in operating the MFLS devices and annotating the sonar images while most of the researchers are inexperienced in the MFLS, which results in few public related datasets<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. Developing MFLS-based algorithms requires a large number of sonar images to verify and improve their approaches. For example, object detection methods in deep learning fields require data to train neural networks. Some researchers have applied sonar simulation<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and image translation technology <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> as a solution of lacking data. Since the complexity of acoustic propagation property and the instability of underwater environment, there have always been differences between the generated images and real images. Contrasting to the optical images, the lack of dataset obstructs the development of research on object detection with MFLS images in AI areas. Our proposed dataset aims to improve the above situations.</p>
</div>
<div id="Sx1.p4" class="ltx_para">
<p id="Sx1.p4.1" class="ltx_p">Considering the recognition habits of human vision, MFLS generally provides images processed with filters and pseudo-coloring which may cause the loss of effective data. Based on acoustic propagation characteristics, the MFLS provides the range and azimuth angle information. So the image in sector representation achieves better visual perception. But invalid information is imported to areas beyond the sector in the images. Figure 1 shows the MFLS raw image and the processed images. The images contain the same three target objects: ball, square cage and human body model.</p>
</div>
<figure id="Sx1.F1" class="ltx_figure"><img src="/html/2212.00352/assets/x1.png" id="Sx1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="112" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="Sx1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="Sx1.F1.3.2" class="ltx_text" style="font-size:90%;">MFLS raw and processed images</span></figcaption>
</figure>
<div id="Sx1.p5" class="ltx_para">
<p id="Sx1.p5.1" class="ltx_p">There have been several MFLS datasets providing processed image data. Erin McCann et al. provided a sonar dataset containing 8 fish species for fish classification and fishery assessment<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. Deepak Singh et al. provided a sonar dataset containing typical household marine debris and distractor marine objects in 11 classes for semantic segmentation<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. Matheus M. Dos Santos et al. published dataset ARACATI 2017, which provides optical aerial and acoustic underwater images for cross-view and cross-domain underwater localization<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. Pontoon objects and moving boats were present in the MFLS data of the dataset. Figure 2 shows an example of comparison of the three datasets and our UATD dataset.</p>
</div>
<figure id="Sx1.F2" class="ltx_figure"><img src="/html/2212.00352/assets/x2.png" id="Sx1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="89" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="Sx1.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="Sx1.F2.3.2" class="ltx_text" style="font-size:90%;">Comparison of MFLS datasets</span></figcaption>
</figure>
<div id="Sx1.p6" class="ltx_para">
<p id="Sx1.p6.1" class="ltx_p">Our UATD dataset directly addresses the above two issues. Starting from 2020, we collected the MFLS data in Maoming and Dalian, China. The environment included lake and shallow water. The dataset provides raw data of MFLS images in high resolution with annotation of 10 categories of target objects. A corresponding benchmark of SOTA detectors performed on UATD including efficiency and accuracy indicators was provided. This dataset could promote the research on underwater object detection based on MFLS. Our work supports three consecutive China Underwater Robot Professional Contest (URPC), providing the dataset for the underwater target object detection algorithm competition. UPRC2022 refers to <a target="_blank" href="https://challenge.datacastle.cn/v3/cmptDetail.html?id=680" title="" class="ltx_ref ltx_href">https://challenge.datacastle.cn/v3/cmptDetail.html?id=680</a>.</p>
</div>
</section>
<section id="Sx2" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">Methods</h2>

<section id="Sx2.SS0.SSSx1" class="ltx_subsubsection">
<h3 class="ltx_title ltx_title_subsubsection">Collecting MFLS Data</h3>

<div id="Sx2.SS0.SSSx1.p1" class="ltx_para">
<p id="Sx2.SS0.SSSx1.p1.1" class="ltx_p">Tritech Gemini 1200ik (website: <a target="_blank" href="https://www.tritech.co.uk/product/gemini-1200ik" title="" class="ltx_ref ltx_href">https://www.tritech.co.uk/product/gemini-1200ik</a>) multibeam forward-looking sonar was used for data collection. The sonar operates at two acoustic frequencies, 720kHz for long-range target detection, and 1200kHz for enhanced high-resolution imaging at shorter ranges. Table 1 shows the acoustic specifications of the sonar. The Gemini software development kit providing the raw data of sonar images is available for Windows and Linux operating systems.</p>
</div>
<figure id="Sx2.T1" class="ltx_table">
<table id="Sx2.T1.8" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="Sx2.T1.8.9.1" class="ltx_tr" style="background-color:#FFBFBF;">
<th id="Sx2.T1.8.9.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="Sx2.T1.8.9.1.1.1" class="ltx_text" style="background-color:#FFBFBF;">Acoustic Specifications</span></th>
<td id="Sx2.T1.8.9.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Sx2.T1.8.9.1.2.1" class="ltx_text" style="background-color:#FFBFBF;">Low Frequency Mode</span></td>
<td id="Sx2.T1.8.9.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Sx2.T1.8.9.1.3.1" class="ltx_text" style="background-color:#FFBFBF;">High Frequency Mode</span></td>
</tr>
<tr id="Sx2.T1.8.10.2" class="ltx_tr">
<th id="Sx2.T1.8.10.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Operating frequency</th>
<td id="Sx2.T1.8.10.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">720kHz</td>
<td id="Sx2.T1.8.10.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1200kHz</td>
</tr>
<tr id="Sx2.T1.4.4" class="ltx_tr">
<th id="Sx2.T1.4.4.5" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Angular resolution</th>
<td id="Sx2.T1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.0<sup id="Sx2.T1.2.2.2.1" class="ltx_sup"><span id="Sx2.T1.2.2.2.1.1" class="ltx_text ltx_font_italic">o</span></sup> acoustic, 0.25<sup id="Sx2.T1.2.2.2.2" class="ltx_sup"><span id="Sx2.T1.2.2.2.2.1" class="ltx_text ltx_font_italic">o</span></sup> effective</td>
<td id="Sx2.T1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.6<sup id="Sx2.T1.4.4.4.1" class="ltx_sup"><span id="Sx2.T1.4.4.4.1.1" class="ltx_text ltx_font_italic">o</span></sup> acoustic, 0.12<sup id="Sx2.T1.4.4.4.2" class="ltx_sup"><span id="Sx2.T1.4.4.4.2.1" class="ltx_text ltx_font_italic">o</span></sup> effective</td>
</tr>
<tr id="Sx2.T1.8.11.3" class="ltx_tr">
<th id="Sx2.T1.8.11.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Range</th>
<td id="Sx2.T1.8.11.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.1m-120m</td>
<td id="Sx2.T1.8.11.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.1m-50m</td>
</tr>
<tr id="Sx2.T1.8.12.4" class="ltx_tr">
<th id="Sx2.T1.8.12.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Number of beams</th>
<td id="Sx2.T1.8.12.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">512</td>
<td id="Sx2.T1.8.12.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1024</td>
</tr>
<tr id="Sx2.T1.6.6" class="ltx_tr">
<th id="Sx2.T1.6.6.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Horizontal beamwidth</th>
<td id="Sx2.T1.5.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">120<sup id="Sx2.T1.5.5.1.1" class="ltx_sup"><span id="Sx2.T1.5.5.1.1.1" class="ltx_text ltx_font_italic">o</span></sup>
</td>
<td id="Sx2.T1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">120<sup id="Sx2.T1.6.6.2.1" class="ltx_sup"><span id="Sx2.T1.6.6.2.1.1" class="ltx_text ltx_font_italic">o</span></sup>
</td>
</tr>
<tr id="Sx2.T1.8.8" class="ltx_tr">
<th id="Sx2.T1.8.8.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Vertical beamwidth</th>
<td id="Sx2.T1.7.7.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">20<sup id="Sx2.T1.7.7.1.1" class="ltx_sup"><span id="Sx2.T1.7.7.1.1.1" class="ltx_text ltx_font_italic">o</span></sup>
</td>
<td id="Sx2.T1.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">12<sup id="Sx2.T1.8.8.2.1" class="ltx_sup"><span id="Sx2.T1.8.8.2.1.1" class="ltx_text ltx_font_italic">o</span></sup>
</td>
</tr>
<tr id="Sx2.T1.8.13.5" class="ltx_tr">
<th id="Sx2.T1.8.13.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Range resolution</th>
<td id="Sx2.T1.8.13.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4mm</td>
<td id="Sx2.T1.8.13.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2.4mm</td>
</tr>
<tr id="Sx2.T1.8.14.6" class="ltx_tr">
<th id="Sx2.T1.8.14.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Update rate</th>
<td id="Sx2.T1.8.14.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">5-65Hz(mode and range dependent)</td>
</tr>
<tr id="Sx2.T1.8.15.7" class="ltx_tr">
<th id="Sx2.T1.8.15.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">CHIRP support</th>
<td id="Sx2.T1.8.15.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">Yes</td>
</tr>
<tr id="Sx2.T1.8.16.8" class="ltx_tr">
<th id="Sx2.T1.8.16.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Speed of Sound</th>
<td id="Sx2.T1.8.16.8.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" colspan="2">Integrated Velocity of Sound sensor for accuracy</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="Sx2.T1.10.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="Sx2.T1.11.2" class="ltx_text" style="font-size:90%;">Acoustic specifications of Tritech Gemini 1200ik</span></figcaption>
</figure>
<div id="Sx2.SS0.SSSx1.p2" class="ltx_para">
<p id="Sx2.SS0.SSSx1.p2.1" class="ltx_p">We have designed a mechanical structure for the sonar to collect data, as shown in figure 3(a). The sonar is fixed to a box structure. The box structure is mounted to the end of a metal rod. A connecting piece is installed in the middle of the metal rod to fix the collection equipment to the hull. The connecting piece allows us to adjust the rod to control the depth of sonar to the surface and the tilt angle to the water bottom during the collection. The sonar data collection structure equipped on the boat is shown in figure3(b).</p>
</div>
<figure id="Sx2.F3" class="ltx_figure"><img src="/html/2212.00352/assets/x3.png" id="Sx2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="322" height="150" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="Sx2.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="Sx2.F3.3.2" class="ltx_text" style="font-size:90%;">Data collection equipment</span></figcaption>
</figure>
<div id="Sx2.SS0.SSSx1.p3" class="ltx_para">
<p id="Sx2.SS0.SSSx1.p3.4" class="ltx_p">We performed the experiments in two places: Golden Pebble Beach at Dalian(39.0904292<sup id="Sx2.SS0.SSSx1.p3.4.1" class="ltx_sup"><span id="Sx2.SS0.SSSx1.p3.4.1.1" class="ltx_text ltx_font_italic">o</span></sup>N,122.0071952<sup id="Sx2.SS0.SSSx1.p3.4.2" class="ltx_sup"><span id="Sx2.SS0.SSSx1.p3.4.2.1" class="ltx_text ltx_font_italic">o</span></sup>E) and Haoxin Lake at Maoming(21.7011602<sup id="Sx2.SS0.SSSx1.p3.4.3" class="ltx_sup"><span id="Sx2.SS0.SSSx1.p3.4.3.1" class="ltx_text ltx_font_italic">o</span></sup>N, 110.8641811<sup id="Sx2.SS0.SSSx1.p3.4.4" class="ltx_sup"><span id="Sx2.SS0.SSSx1.p3.4.4.1" class="ltx_text ltx_font_italic">o</span></sup>E). The environments of experiments performed and satellite maps with the experimental areas marked are shown in figure 4. The experimental waters have a minimum depth of 4 meters and a maximum depth of 10 meters at Dalian, and about a depth of 4 meters at Maoming.</p>
</div>
<figure id="Sx2.F4" class="ltx_figure"><img src="/html/2212.00352/assets/x4.png" id="Sx2.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="85" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="Sx2.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="Sx2.F4.3.2" class="ltx_text" style="font-size:90%;">Environment and satellite map of experiments performed</span></figcaption>
</figure>
<figure id="Sx2.F5" class="ltx_figure"><img src="/html/2212.00352/assets/x5.png" id="Sx2.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="190" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="Sx2.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="Sx2.F5.3.2" class="ltx_text" style="font-size:90%;">Objects of the dataset. The scale of the objects is shown below the name of the objects in the figure. The scale is in meter and the representation: L(Length), W(Width), H(Height), R(Radius).</span></figcaption>
</figure>
<div id="Sx2.SS0.SSSx1.p4" class="ltx_para">
<p id="Sx2.SS0.SSSx1.p4.1" class="ltx_p">Ten categories of target objects were selected: cube, ball, cylinder, human body model, tyre, circle cage, square cage, metal bucket, plane model and ROV, as shown in figure 5 with their scales. The objects were tied to a floating ball with a long rope individually so that the rough location of the objects could be distinguished according to the ball floating on the water. As a result, the objects might suspend in water or lay on the bottom.</p>
</div>
<div id="Sx2.SS0.SSSx1.p5" class="ltx_para">
<p id="Sx2.SS0.SSSx1.p5.1" class="ltx_p">After deploying the objects, we drove the boat mounted with sonar and cruised around the selected sites, searching the target objects and recording data by adjusting the sonar direction.</p>
</div>
</section>
<section id="Sx2.SS0.SSSx2" class="ltx_subsubsection">
<h3 class="ltx_title ltx_title_subsubsection">Object annotations</h3>

<div id="Sx2.SS0.SSSx2.p1" class="ltx_para">
<p id="Sx2.SS0.SSSx2.p1.1" class="ltx_p">The shape of the same target may change when the sonar is imaging at different positions and angles, which makes it difficult for the annotator to judge the target category only by experience and intuition when annotating. Therefore, we developed an annotation software for sonar images named forward-looking sonar label tool (OpenSLT). Compared with other annotation tools, OpenSLT has the following two new features: 1) input as image stream. 2) real-time annotation. These features allow us to overcome the above problem as mentioned. OpenSLT can be divided into three modules: toolbar, image display area and annotation display area, as shown in figure 6(a). The tool first receives the raw data of sonar as input and plays in the form of a video stream. The playback speed can be accelerated or slowed down until the target is found. Then annotator can press the pause button and annotate in image display area using mouse. The annotation will be automatically generated and saved locally. This annotation method enables the annotator to continuously track the target object when annotating as shown in figure 6, avoiding the situations where the target object position cannot be confirmed and the target object type cannot be judged when it reappears. With the data protocol provided by Tritech, we extract the sonar working information and the sonar image information from every frame of sonar original data, including working range, frequency, azimuth, elevation, sound speed and image resolution. Then we store these information in a CSV file. OpenSLT loads the CSV file and retains these information during the annotation. In addition, OpenSLT generates file path information in the annotation for batch processing.</p>
</div>
<figure id="Sx2.F6" class="ltx_figure"><img src="/html/2212.00352/assets/x6.png" id="Sx2.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="114" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="Sx2.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="Sx2.F6.3.2" class="ltx_text" style="font-size:90%;">Sequential frame annotation 
<br class="ltx_break">This example shows the moving ROV during the data collection.</span></figcaption>
</figure>
</section>
</section>
<section id="Sx3" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">Data Records</h2>

<div id="Sx3.p1" class="ltx_para">
<p id="Sx3.p1.1" class="ltx_p">UATD dataset is openly available to the public in a figshare repository<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. The dataset contains 9200 image files in BMP format corresponding with the same number of annotation files in XML format, and is divided into three ZIP archives, namely "UATD_Training.zip", "UATD_Test_1.zip", "UATD_Test_2.zip". "UATD-Training" contains 7600 pairs of images and annotations. The remaining two parts contain 800 pairs of images and annotations respectively. Each part consists of two folders, storing the image files and annotation files respectively. The image files are in the folder named "image", and the annotation files are in the folder named "annotation".</p>
</div>
<div id="Sx3.p2" class="ltx_para">
<p id="Sx3.p2.1" class="ltx_p">The class distribution of the objects is shown in figure 7(a). The statistic of collecting ranges and frequencies is shown in figure 7(b). A total of 2900 images have been collected in 720k Hz and 6300 have been in 1200k Hz. The sonar working range distributes from 5 meters to 25 meters during the data collection. Therefore, we count the number of images with the scope of every 1 meter by range. Besides, the images are also counted by working frequency of 720k and 1200k respectively as shown in figure 7(b).</p>
</div>
<figure id="Sx3.F7" class="ltx_figure"><img src="/html/2212.00352/assets/x7.png" id="Sx3.F7.g1" class="ltx_graphics ltx_centering ltx_img_square" width="368" height="426" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="Sx3.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="Sx3.F7.3.2" class="ltx_text" style="font-size:90%;">An overview of the distribution statistics of UATD dataset</span></figcaption>
</figure>
<div id="Sx3.p3" class="ltx_para">
<p id="Sx3.p3.2" class="ltx_p">A pair of UATD data is shown in figure 8 as an example. The echo intensities data is stored in the first channel of the BMP image file. The data in the rest of the two channels of the image is the same as the first channel. The annotation file can be divided into four sections. The "sonar" section provides some basic sonar working information at the moment the corresponding image is collected. As shown in the example: the range is 14.9941m, the azimuth is 120<sup id="Sx3.p3.2.1" class="ltx_sup"><span id="Sx3.p3.2.1.1" class="ltx_text ltx_font_italic">o</span></sup>, the elevation is 12<sup id="Sx3.p3.2.2" class="ltx_sup"><span id="Sx3.p3.2.2.1" class="ltx_text ltx_font_italic">o</span></sup>, the sound speed is 1582.4m/s and the frequency is 1200k Hz. All of these parameters are parsed from the sonar output data stream directly. The "file" section provides some information about the relative paths of the image file and annotation file. The "filename" parameter provides the common filename prefix of a pair of image files and annotation files. The "size" section provides the image information. In this example, the image resolution is 1024x1428 and owns 3 channels. The "object" section provides the category name under "name" tab and bounding box in pixels of the object under the "bndbox" tab.</p>
</div>
<figure id="Sx3.F8" class="ltx_figure"><img src="/html/2212.00352/assets/x8.png" id="Sx3.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="266" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="Sx3.F8.2.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="Sx3.F8.3.2" class="ltx_text" style="font-size:90%;">An example of UATD dataset</span></figcaption>
</figure>
</section>
<section id="Sx4" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">Technical Validation</h2>

<div id="Sx4.p1" class="ltx_para">
<p id="Sx4.p1.1" class="ltx_p">The appearances of the same underwater target object at different imaging angles of the MFLS are generally different, leading to a great challenge to the subsequent labeling work. To address the challenge, we designed three effective methods to ensure the accuracy of sonar image annotations. Firstly, three members of our team were responsible for the annotation and completed the labeling work individually after randomly assigning the collected data. Then, with the <span id="Sx4.p1.1.1" class="ltx_text ltx_font_italic">data playback</span> function in OpenSLT, cross-checking was performed to reduce manual annotation errors. Secondly, we recorded the video stream displaying the processed images for vision habits from Gemini 1200ik synchronously with raw data during the data collection. At the same time, OpenSLT played back the data in the way of the stream to corresponding with the recorded video stream. So it was convenient to detect and track the object by comparing it with the video during annotating the raw data in OpenSLT, avoiding labeling errors caused by losing the target. Finally, we handed over the annotated data to a professional data management company cooperating with us. The professional staff members of the company have checked the data again to ensure correctness.</p>
</div>
<section id="Sx4.SS0.SSSx1" class="ltx_subsubsection">
<h3 class="ltx_title ltx_title_subsubsection">Object detection benchmarks</h3>

<div id="Sx4.SS0.SSSx1.p1" class="ltx_para">
<p id="Sx4.SS0.SSSx1.p1.1" class="ltx_p">A benchmark based on our dataset is given in table 2. Currently, MMdetection<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> is one of the best open-source object detection toolboxes based on PyTorch, which provides a variety of SOTA detectors and is simple to employ. Therefore, the benchmarks are generated by MMdetection(V2.25.0). We choose Faster-RCNN<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> and YOLOv3<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> with various backbones as our object detectors which are the most popular two-stage and one-stage SOTA detectors respectively.</p>
</div>
<figure id="Sx4.T2" class="ltx_table">
<table id="Sx4.T2.10" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="Sx4.T2.10.10" class="ltx_tr" style="background-color:#FFBFBF;">
<td id="Sx4.T2.10.10.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;"><span id="Sx4.T2.10.10.11.1" class="ltx_text" style="background-color:#FFBFBF;">Model</span></td>
<td id="Sx4.T2.10.10.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;"><span id="Sx4.T2.10.10.12.1" class="ltx_text" style="background-color:#FFBFBF;">Backbone</span></td>
<td id="Sx4.T2.10.10.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;"><span id="Sx4.T2.10.10.13.1" class="ltx_text" style="background-color:#FFBFBF;">mAP</span></td>
<td id="Sx4.T2.10.10.14" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;"><span id="Sx4.T2.10.10.14.1" class="ltx_text" style="background-color:#FFBFBF;">mAR</span></td>
<td id="Sx4.T2.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;"><math id="Sx4.T2.1.1.1.m1.1" class="ltx_Math" style="background-color:#FFBFBF;" alttext="AP_{ball}" display="inline"><semantics id="Sx4.T2.1.1.1.m1.1a"><mrow id="Sx4.T2.1.1.1.m1.1.1" xref="Sx4.T2.1.1.1.m1.1.1.cmml"><mi mathbackground="#FFBFBF" id="Sx4.T2.1.1.1.m1.1.1.2" xref="Sx4.T2.1.1.1.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="Sx4.T2.1.1.1.m1.1.1.1" xref="Sx4.T2.1.1.1.m1.1.1.1.cmml">​</mo><msub id="Sx4.T2.1.1.1.m1.1.1.3" xref="Sx4.T2.1.1.1.m1.1.1.3.cmml"><mi mathbackground="#FFBFBF" id="Sx4.T2.1.1.1.m1.1.1.3.2" xref="Sx4.T2.1.1.1.m1.1.1.3.2.cmml">P</mi><mrow id="Sx4.T2.1.1.1.m1.1.1.3.3" xref="Sx4.T2.1.1.1.m1.1.1.3.3.cmml"><mi mathbackground="#FFBFBF" id="Sx4.T2.1.1.1.m1.1.1.3.3.2" xref="Sx4.T2.1.1.1.m1.1.1.3.3.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="Sx4.T2.1.1.1.m1.1.1.3.3.1" xref="Sx4.T2.1.1.1.m1.1.1.3.3.1.cmml">​</mo><mi mathbackground="#FFBFBF" id="Sx4.T2.1.1.1.m1.1.1.3.3.3" xref="Sx4.T2.1.1.1.m1.1.1.3.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="Sx4.T2.1.1.1.m1.1.1.3.3.1a" xref="Sx4.T2.1.1.1.m1.1.1.3.3.1.cmml">​</mo><mi mathbackground="#FFBFBF" id="Sx4.T2.1.1.1.m1.1.1.3.3.4" xref="Sx4.T2.1.1.1.m1.1.1.3.3.4.cmml">l</mi><mo lspace="0em" rspace="0em" id="Sx4.T2.1.1.1.m1.1.1.3.3.1b" xref="Sx4.T2.1.1.1.m1.1.1.3.3.1.cmml">​</mo><mi mathbackground="#FFBFBF" id="Sx4.T2.1.1.1.m1.1.1.3.3.5" xref="Sx4.T2.1.1.1.m1.1.1.3.3.5.cmml">l</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="Sx4.T2.1.1.1.m1.1b"><apply id="Sx4.T2.1.1.1.m1.1.1.cmml" xref="Sx4.T2.1.1.1.m1.1.1"><times id="Sx4.T2.1.1.1.m1.1.1.1.cmml" xref="Sx4.T2.1.1.1.m1.1.1.1"></times><ci id="Sx4.T2.1.1.1.m1.1.1.2.cmml" xref="Sx4.T2.1.1.1.m1.1.1.2">𝐴</ci><apply id="Sx4.T2.1.1.1.m1.1.1.3.cmml" xref="Sx4.T2.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="Sx4.T2.1.1.1.m1.1.1.3.1.cmml" xref="Sx4.T2.1.1.1.m1.1.1.3">subscript</csymbol><ci id="Sx4.T2.1.1.1.m1.1.1.3.2.cmml" xref="Sx4.T2.1.1.1.m1.1.1.3.2">𝑃</ci><apply id="Sx4.T2.1.1.1.m1.1.1.3.3.cmml" xref="Sx4.T2.1.1.1.m1.1.1.3.3"><times id="Sx4.T2.1.1.1.m1.1.1.3.3.1.cmml" xref="Sx4.T2.1.1.1.m1.1.1.3.3.1"></times><ci id="Sx4.T2.1.1.1.m1.1.1.3.3.2.cmml" xref="Sx4.T2.1.1.1.m1.1.1.3.3.2">𝑏</ci><ci id="Sx4.T2.1.1.1.m1.1.1.3.3.3.cmml" xref="Sx4.T2.1.1.1.m1.1.1.3.3.3">𝑎</ci><ci id="Sx4.T2.1.1.1.m1.1.1.3.3.4.cmml" xref="Sx4.T2.1.1.1.m1.1.1.3.3.4">𝑙</ci><ci id="Sx4.T2.1.1.1.m1.1.1.3.3.5.cmml" xref="Sx4.T2.1.1.1.m1.1.1.3.3.5">𝑙</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.T2.1.1.1.m1.1c">AP_{ball}</annotation></semantics></math></td>
<td id="Sx4.T2.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;"><math id="Sx4.T2.2.2.2.m1.1" class="ltx_Math" style="background-color:#FFBFBF;" alttext="AP_{cube}" display="inline"><semantics id="Sx4.T2.2.2.2.m1.1a"><mrow id="Sx4.T2.2.2.2.m1.1.1" xref="Sx4.T2.2.2.2.m1.1.1.cmml"><mi mathbackground="#FFBFBF" id="Sx4.T2.2.2.2.m1.1.1.2" xref="Sx4.T2.2.2.2.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="Sx4.T2.2.2.2.m1.1.1.1" xref="Sx4.T2.2.2.2.m1.1.1.1.cmml">​</mo><msub id="Sx4.T2.2.2.2.m1.1.1.3" xref="Sx4.T2.2.2.2.m1.1.1.3.cmml"><mi mathbackground="#FFBFBF" id="Sx4.T2.2.2.2.m1.1.1.3.2" xref="Sx4.T2.2.2.2.m1.1.1.3.2.cmml">P</mi><mrow id="Sx4.T2.2.2.2.m1.1.1.3.3" xref="Sx4.T2.2.2.2.m1.1.1.3.3.cmml"><mi mathbackground="#FFBFBF" id="Sx4.T2.2.2.2.m1.1.1.3.3.2" xref="Sx4.T2.2.2.2.m1.1.1.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="Sx4.T2.2.2.2.m1.1.1.3.3.1" xref="Sx4.T2.2.2.2.m1.1.1.3.3.1.cmml">​</mo><mi mathbackground="#FFBFBF" id="Sx4.T2.2.2.2.m1.1.1.3.3.3" xref="Sx4.T2.2.2.2.m1.1.1.3.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="Sx4.T2.2.2.2.m1.1.1.3.3.1a" xref="Sx4.T2.2.2.2.m1.1.1.3.3.1.cmml">​</mo><mi mathbackground="#FFBFBF" id="Sx4.T2.2.2.2.m1.1.1.3.3.4" xref="Sx4.T2.2.2.2.m1.1.1.3.3.4.cmml">b</mi><mo lspace="0em" rspace="0em" id="Sx4.T2.2.2.2.m1.1.1.3.3.1b" xref="Sx4.T2.2.2.2.m1.1.1.3.3.1.cmml">​</mo><mi mathbackground="#FFBFBF" id="Sx4.T2.2.2.2.m1.1.1.3.3.5" xref="Sx4.T2.2.2.2.m1.1.1.3.3.5.cmml">e</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="Sx4.T2.2.2.2.m1.1b"><apply id="Sx4.T2.2.2.2.m1.1.1.cmml" xref="Sx4.T2.2.2.2.m1.1.1"><times id="Sx4.T2.2.2.2.m1.1.1.1.cmml" xref="Sx4.T2.2.2.2.m1.1.1.1"></times><ci id="Sx4.T2.2.2.2.m1.1.1.2.cmml" xref="Sx4.T2.2.2.2.m1.1.1.2">𝐴</ci><apply id="Sx4.T2.2.2.2.m1.1.1.3.cmml" xref="Sx4.T2.2.2.2.m1.1.1.3"><csymbol cd="ambiguous" id="Sx4.T2.2.2.2.m1.1.1.3.1.cmml" xref="Sx4.T2.2.2.2.m1.1.1.3">subscript</csymbol><ci id="Sx4.T2.2.2.2.m1.1.1.3.2.cmml" xref="Sx4.T2.2.2.2.m1.1.1.3.2">𝑃</ci><apply id="Sx4.T2.2.2.2.m1.1.1.3.3.cmml" xref="Sx4.T2.2.2.2.m1.1.1.3.3"><times id="Sx4.T2.2.2.2.m1.1.1.3.3.1.cmml" xref="Sx4.T2.2.2.2.m1.1.1.3.3.1"></times><ci id="Sx4.T2.2.2.2.m1.1.1.3.3.2.cmml" xref="Sx4.T2.2.2.2.m1.1.1.3.3.2">𝑐</ci><ci id="Sx4.T2.2.2.2.m1.1.1.3.3.3.cmml" xref="Sx4.T2.2.2.2.m1.1.1.3.3.3">𝑢</ci><ci id="Sx4.T2.2.2.2.m1.1.1.3.3.4.cmml" xref="Sx4.T2.2.2.2.m1.1.1.3.3.4">𝑏</ci><ci id="Sx4.T2.2.2.2.m1.1.1.3.3.5.cmml" xref="Sx4.T2.2.2.2.m1.1.1.3.3.5">𝑒</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.T2.2.2.2.m1.1c">AP_{cube}</annotation></semantics></math></td>
<td id="Sx4.T2.3.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;"><math id="Sx4.T2.3.3.3.m1.1" class="ltx_Math" style="background-color:#FFBFBF;" alttext="AP_{hb}" display="inline"><semantics id="Sx4.T2.3.3.3.m1.1a"><mrow id="Sx4.T2.3.3.3.m1.1.1" xref="Sx4.T2.3.3.3.m1.1.1.cmml"><mi mathbackground="#FFBFBF" id="Sx4.T2.3.3.3.m1.1.1.2" xref="Sx4.T2.3.3.3.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="Sx4.T2.3.3.3.m1.1.1.1" xref="Sx4.T2.3.3.3.m1.1.1.1.cmml">​</mo><msub id="Sx4.T2.3.3.3.m1.1.1.3" xref="Sx4.T2.3.3.3.m1.1.1.3.cmml"><mi mathbackground="#FFBFBF" id="Sx4.T2.3.3.3.m1.1.1.3.2" xref="Sx4.T2.3.3.3.m1.1.1.3.2.cmml">P</mi><mrow id="Sx4.T2.3.3.3.m1.1.1.3.3" xref="Sx4.T2.3.3.3.m1.1.1.3.3.cmml"><mi mathbackground="#FFBFBF" id="Sx4.T2.3.3.3.m1.1.1.3.3.2" xref="Sx4.T2.3.3.3.m1.1.1.3.3.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="Sx4.T2.3.3.3.m1.1.1.3.3.1" xref="Sx4.T2.3.3.3.m1.1.1.3.3.1.cmml">​</mo><mi mathbackground="#FFBFBF" id="Sx4.T2.3.3.3.m1.1.1.3.3.3" xref="Sx4.T2.3.3.3.m1.1.1.3.3.3.cmml">b</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="Sx4.T2.3.3.3.m1.1b"><apply id="Sx4.T2.3.3.3.m1.1.1.cmml" xref="Sx4.T2.3.3.3.m1.1.1"><times id="Sx4.T2.3.3.3.m1.1.1.1.cmml" xref="Sx4.T2.3.3.3.m1.1.1.1"></times><ci id="Sx4.T2.3.3.3.m1.1.1.2.cmml" xref="Sx4.T2.3.3.3.m1.1.1.2">𝐴</ci><apply id="Sx4.T2.3.3.3.m1.1.1.3.cmml" xref="Sx4.T2.3.3.3.m1.1.1.3"><csymbol cd="ambiguous" id="Sx4.T2.3.3.3.m1.1.1.3.1.cmml" xref="Sx4.T2.3.3.3.m1.1.1.3">subscript</csymbol><ci id="Sx4.T2.3.3.3.m1.1.1.3.2.cmml" xref="Sx4.T2.3.3.3.m1.1.1.3.2">𝑃</ci><apply id="Sx4.T2.3.3.3.m1.1.1.3.3.cmml" xref="Sx4.T2.3.3.3.m1.1.1.3.3"><times id="Sx4.T2.3.3.3.m1.1.1.3.3.1.cmml" xref="Sx4.T2.3.3.3.m1.1.1.3.3.1"></times><ci id="Sx4.T2.3.3.3.m1.1.1.3.3.2.cmml" xref="Sx4.T2.3.3.3.m1.1.1.3.3.2">ℎ</ci><ci id="Sx4.T2.3.3.3.m1.1.1.3.3.3.cmml" xref="Sx4.T2.3.3.3.m1.1.1.3.3.3">𝑏</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.T2.3.3.3.m1.1c">AP_{hb}</annotation></semantics></math></td>
<td id="Sx4.T2.4.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;"><math id="Sx4.T2.4.4.4.m1.1" class="ltx_Math" style="background-color:#FFBFBF;" alttext="AP_{tyre}" display="inline"><semantics id="Sx4.T2.4.4.4.m1.1a"><mrow id="Sx4.T2.4.4.4.m1.1.1" xref="Sx4.T2.4.4.4.m1.1.1.cmml"><mi mathbackground="#FFBFBF" id="Sx4.T2.4.4.4.m1.1.1.2" xref="Sx4.T2.4.4.4.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="Sx4.T2.4.4.4.m1.1.1.1" xref="Sx4.T2.4.4.4.m1.1.1.1.cmml">​</mo><msub id="Sx4.T2.4.4.4.m1.1.1.3" xref="Sx4.T2.4.4.4.m1.1.1.3.cmml"><mi mathbackground="#FFBFBF" id="Sx4.T2.4.4.4.m1.1.1.3.2" xref="Sx4.T2.4.4.4.m1.1.1.3.2.cmml">P</mi><mrow id="Sx4.T2.4.4.4.m1.1.1.3.3" xref="Sx4.T2.4.4.4.m1.1.1.3.3.cmml"><mi mathbackground="#FFBFBF" id="Sx4.T2.4.4.4.m1.1.1.3.3.2" xref="Sx4.T2.4.4.4.m1.1.1.3.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="Sx4.T2.4.4.4.m1.1.1.3.3.1" xref="Sx4.T2.4.4.4.m1.1.1.3.3.1.cmml">​</mo><mi mathbackground="#FFBFBF" id="Sx4.T2.4.4.4.m1.1.1.3.3.3" xref="Sx4.T2.4.4.4.m1.1.1.3.3.3.cmml">y</mi><mo lspace="0em" rspace="0em" id="Sx4.T2.4.4.4.m1.1.1.3.3.1a" xref="Sx4.T2.4.4.4.m1.1.1.3.3.1.cmml">​</mo><mi mathbackground="#FFBFBF" id="Sx4.T2.4.4.4.m1.1.1.3.3.4" xref="Sx4.T2.4.4.4.m1.1.1.3.3.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="Sx4.T2.4.4.4.m1.1.1.3.3.1b" xref="Sx4.T2.4.4.4.m1.1.1.3.3.1.cmml">​</mo><mi mathbackground="#FFBFBF" id="Sx4.T2.4.4.4.m1.1.1.3.3.5" xref="Sx4.T2.4.4.4.m1.1.1.3.3.5.cmml">e</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="Sx4.T2.4.4.4.m1.1b"><apply id="Sx4.T2.4.4.4.m1.1.1.cmml" xref="Sx4.T2.4.4.4.m1.1.1"><times id="Sx4.T2.4.4.4.m1.1.1.1.cmml" xref="Sx4.T2.4.4.4.m1.1.1.1"></times><ci id="Sx4.T2.4.4.4.m1.1.1.2.cmml" xref="Sx4.T2.4.4.4.m1.1.1.2">𝐴</ci><apply id="Sx4.T2.4.4.4.m1.1.1.3.cmml" xref="Sx4.T2.4.4.4.m1.1.1.3"><csymbol cd="ambiguous" id="Sx4.T2.4.4.4.m1.1.1.3.1.cmml" xref="Sx4.T2.4.4.4.m1.1.1.3">subscript</csymbol><ci id="Sx4.T2.4.4.4.m1.1.1.3.2.cmml" xref="Sx4.T2.4.4.4.m1.1.1.3.2">𝑃</ci><apply id="Sx4.T2.4.4.4.m1.1.1.3.3.cmml" xref="Sx4.T2.4.4.4.m1.1.1.3.3"><times id="Sx4.T2.4.4.4.m1.1.1.3.3.1.cmml" xref="Sx4.T2.4.4.4.m1.1.1.3.3.1"></times><ci id="Sx4.T2.4.4.4.m1.1.1.3.3.2.cmml" xref="Sx4.T2.4.4.4.m1.1.1.3.3.2">𝑡</ci><ci id="Sx4.T2.4.4.4.m1.1.1.3.3.3.cmml" xref="Sx4.T2.4.4.4.m1.1.1.3.3.3">𝑦</ci><ci id="Sx4.T2.4.4.4.m1.1.1.3.3.4.cmml" xref="Sx4.T2.4.4.4.m1.1.1.3.3.4">𝑟</ci><ci id="Sx4.T2.4.4.4.m1.1.1.3.3.5.cmml" xref="Sx4.T2.4.4.4.m1.1.1.3.3.5">𝑒</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.T2.4.4.4.m1.1c">AP_{tyre}</annotation></semantics></math></td>
<td id="Sx4.T2.5.5.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;"><math id="Sx4.T2.5.5.5.m1.1" class="ltx_Math" style="background-color:#FFBFBF;" alttext="AP_{sc}" display="inline"><semantics id="Sx4.T2.5.5.5.m1.1a"><mrow id="Sx4.T2.5.5.5.m1.1.1" xref="Sx4.T2.5.5.5.m1.1.1.cmml"><mi mathbackground="#FFBFBF" id="Sx4.T2.5.5.5.m1.1.1.2" xref="Sx4.T2.5.5.5.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="Sx4.T2.5.5.5.m1.1.1.1" xref="Sx4.T2.5.5.5.m1.1.1.1.cmml">​</mo><msub id="Sx4.T2.5.5.5.m1.1.1.3" xref="Sx4.T2.5.5.5.m1.1.1.3.cmml"><mi mathbackground="#FFBFBF" id="Sx4.T2.5.5.5.m1.1.1.3.2" xref="Sx4.T2.5.5.5.m1.1.1.3.2.cmml">P</mi><mrow id="Sx4.T2.5.5.5.m1.1.1.3.3" xref="Sx4.T2.5.5.5.m1.1.1.3.3.cmml"><mi mathbackground="#FFBFBF" id="Sx4.T2.5.5.5.m1.1.1.3.3.2" xref="Sx4.T2.5.5.5.m1.1.1.3.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="Sx4.T2.5.5.5.m1.1.1.3.3.1" xref="Sx4.T2.5.5.5.m1.1.1.3.3.1.cmml">​</mo><mi mathbackground="#FFBFBF" id="Sx4.T2.5.5.5.m1.1.1.3.3.3" xref="Sx4.T2.5.5.5.m1.1.1.3.3.3.cmml">c</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="Sx4.T2.5.5.5.m1.1b"><apply id="Sx4.T2.5.5.5.m1.1.1.cmml" xref="Sx4.T2.5.5.5.m1.1.1"><times id="Sx4.T2.5.5.5.m1.1.1.1.cmml" xref="Sx4.T2.5.5.5.m1.1.1.1"></times><ci id="Sx4.T2.5.5.5.m1.1.1.2.cmml" xref="Sx4.T2.5.5.5.m1.1.1.2">𝐴</ci><apply id="Sx4.T2.5.5.5.m1.1.1.3.cmml" xref="Sx4.T2.5.5.5.m1.1.1.3"><csymbol cd="ambiguous" id="Sx4.T2.5.5.5.m1.1.1.3.1.cmml" xref="Sx4.T2.5.5.5.m1.1.1.3">subscript</csymbol><ci id="Sx4.T2.5.5.5.m1.1.1.3.2.cmml" xref="Sx4.T2.5.5.5.m1.1.1.3.2">𝑃</ci><apply id="Sx4.T2.5.5.5.m1.1.1.3.3.cmml" xref="Sx4.T2.5.5.5.m1.1.1.3.3"><times id="Sx4.T2.5.5.5.m1.1.1.3.3.1.cmml" xref="Sx4.T2.5.5.5.m1.1.1.3.3.1"></times><ci id="Sx4.T2.5.5.5.m1.1.1.3.3.2.cmml" xref="Sx4.T2.5.5.5.m1.1.1.3.3.2">𝑠</ci><ci id="Sx4.T2.5.5.5.m1.1.1.3.3.3.cmml" xref="Sx4.T2.5.5.5.m1.1.1.3.3.3">𝑐</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.T2.5.5.5.m1.1c">AP_{sc}</annotation></semantics></math></td>
<td id="Sx4.T2.6.6.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;"><math id="Sx4.T2.6.6.6.m1.1" class="ltx_Math" style="background-color:#FFBFBF;" alttext="AP_{plane}" display="inline"><semantics id="Sx4.T2.6.6.6.m1.1a"><mrow id="Sx4.T2.6.6.6.m1.1.1" xref="Sx4.T2.6.6.6.m1.1.1.cmml"><mi mathbackground="#FFBFBF" id="Sx4.T2.6.6.6.m1.1.1.2" xref="Sx4.T2.6.6.6.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="Sx4.T2.6.6.6.m1.1.1.1" xref="Sx4.T2.6.6.6.m1.1.1.1.cmml">​</mo><msub id="Sx4.T2.6.6.6.m1.1.1.3" xref="Sx4.T2.6.6.6.m1.1.1.3.cmml"><mi mathbackground="#FFBFBF" id="Sx4.T2.6.6.6.m1.1.1.3.2" xref="Sx4.T2.6.6.6.m1.1.1.3.2.cmml">P</mi><mrow id="Sx4.T2.6.6.6.m1.1.1.3.3" xref="Sx4.T2.6.6.6.m1.1.1.3.3.cmml"><mi mathbackground="#FFBFBF" id="Sx4.T2.6.6.6.m1.1.1.3.3.2" xref="Sx4.T2.6.6.6.m1.1.1.3.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="Sx4.T2.6.6.6.m1.1.1.3.3.1" xref="Sx4.T2.6.6.6.m1.1.1.3.3.1.cmml">​</mo><mi mathbackground="#FFBFBF" id="Sx4.T2.6.6.6.m1.1.1.3.3.3" xref="Sx4.T2.6.6.6.m1.1.1.3.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="Sx4.T2.6.6.6.m1.1.1.3.3.1a" xref="Sx4.T2.6.6.6.m1.1.1.3.3.1.cmml">​</mo><mi mathbackground="#FFBFBF" id="Sx4.T2.6.6.6.m1.1.1.3.3.4" xref="Sx4.T2.6.6.6.m1.1.1.3.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="Sx4.T2.6.6.6.m1.1.1.3.3.1b" xref="Sx4.T2.6.6.6.m1.1.1.3.3.1.cmml">​</mo><mi mathbackground="#FFBFBF" id="Sx4.T2.6.6.6.m1.1.1.3.3.5" xref="Sx4.T2.6.6.6.m1.1.1.3.3.5.cmml">n</mi><mo lspace="0em" rspace="0em" id="Sx4.T2.6.6.6.m1.1.1.3.3.1c" xref="Sx4.T2.6.6.6.m1.1.1.3.3.1.cmml">​</mo><mi mathbackground="#FFBFBF" id="Sx4.T2.6.6.6.m1.1.1.3.3.6" xref="Sx4.T2.6.6.6.m1.1.1.3.3.6.cmml">e</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="Sx4.T2.6.6.6.m1.1b"><apply id="Sx4.T2.6.6.6.m1.1.1.cmml" xref="Sx4.T2.6.6.6.m1.1.1"><times id="Sx4.T2.6.6.6.m1.1.1.1.cmml" xref="Sx4.T2.6.6.6.m1.1.1.1"></times><ci id="Sx4.T2.6.6.6.m1.1.1.2.cmml" xref="Sx4.T2.6.6.6.m1.1.1.2">𝐴</ci><apply id="Sx4.T2.6.6.6.m1.1.1.3.cmml" xref="Sx4.T2.6.6.6.m1.1.1.3"><csymbol cd="ambiguous" id="Sx4.T2.6.6.6.m1.1.1.3.1.cmml" xref="Sx4.T2.6.6.6.m1.1.1.3">subscript</csymbol><ci id="Sx4.T2.6.6.6.m1.1.1.3.2.cmml" xref="Sx4.T2.6.6.6.m1.1.1.3.2">𝑃</ci><apply id="Sx4.T2.6.6.6.m1.1.1.3.3.cmml" xref="Sx4.T2.6.6.6.m1.1.1.3.3"><times id="Sx4.T2.6.6.6.m1.1.1.3.3.1.cmml" xref="Sx4.T2.6.6.6.m1.1.1.3.3.1"></times><ci id="Sx4.T2.6.6.6.m1.1.1.3.3.2.cmml" xref="Sx4.T2.6.6.6.m1.1.1.3.3.2">𝑝</ci><ci id="Sx4.T2.6.6.6.m1.1.1.3.3.3.cmml" xref="Sx4.T2.6.6.6.m1.1.1.3.3.3">𝑙</ci><ci id="Sx4.T2.6.6.6.m1.1.1.3.3.4.cmml" xref="Sx4.T2.6.6.6.m1.1.1.3.3.4">𝑎</ci><ci id="Sx4.T2.6.6.6.m1.1.1.3.3.5.cmml" xref="Sx4.T2.6.6.6.m1.1.1.3.3.5">𝑛</ci><ci id="Sx4.T2.6.6.6.m1.1.1.3.3.6.cmml" xref="Sx4.T2.6.6.6.m1.1.1.3.3.6">𝑒</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.T2.6.6.6.m1.1c">AP_{plane}</annotation></semantics></math></td>
<td id="Sx4.T2.7.7.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;"><math id="Sx4.T2.7.7.7.m1.1" class="ltx_Math" style="background-color:#FFBFBF;" alttext="AP_{rov}" display="inline"><semantics id="Sx4.T2.7.7.7.m1.1a"><mrow id="Sx4.T2.7.7.7.m1.1.1" xref="Sx4.T2.7.7.7.m1.1.1.cmml"><mi mathbackground="#FFBFBF" id="Sx4.T2.7.7.7.m1.1.1.2" xref="Sx4.T2.7.7.7.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="Sx4.T2.7.7.7.m1.1.1.1" xref="Sx4.T2.7.7.7.m1.1.1.1.cmml">​</mo><msub id="Sx4.T2.7.7.7.m1.1.1.3" xref="Sx4.T2.7.7.7.m1.1.1.3.cmml"><mi mathbackground="#FFBFBF" id="Sx4.T2.7.7.7.m1.1.1.3.2" xref="Sx4.T2.7.7.7.m1.1.1.3.2.cmml">P</mi><mrow id="Sx4.T2.7.7.7.m1.1.1.3.3" xref="Sx4.T2.7.7.7.m1.1.1.3.3.cmml"><mi mathbackground="#FFBFBF" id="Sx4.T2.7.7.7.m1.1.1.3.3.2" xref="Sx4.T2.7.7.7.m1.1.1.3.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="Sx4.T2.7.7.7.m1.1.1.3.3.1" xref="Sx4.T2.7.7.7.m1.1.1.3.3.1.cmml">​</mo><mi mathbackground="#FFBFBF" id="Sx4.T2.7.7.7.m1.1.1.3.3.3" xref="Sx4.T2.7.7.7.m1.1.1.3.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="Sx4.T2.7.7.7.m1.1.1.3.3.1a" xref="Sx4.T2.7.7.7.m1.1.1.3.3.1.cmml">​</mo><mi mathbackground="#FFBFBF" id="Sx4.T2.7.7.7.m1.1.1.3.3.4" xref="Sx4.T2.7.7.7.m1.1.1.3.3.4.cmml">v</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="Sx4.T2.7.7.7.m1.1b"><apply id="Sx4.T2.7.7.7.m1.1.1.cmml" xref="Sx4.T2.7.7.7.m1.1.1"><times id="Sx4.T2.7.7.7.m1.1.1.1.cmml" xref="Sx4.T2.7.7.7.m1.1.1.1"></times><ci id="Sx4.T2.7.7.7.m1.1.1.2.cmml" xref="Sx4.T2.7.7.7.m1.1.1.2">𝐴</ci><apply id="Sx4.T2.7.7.7.m1.1.1.3.cmml" xref="Sx4.T2.7.7.7.m1.1.1.3"><csymbol cd="ambiguous" id="Sx4.T2.7.7.7.m1.1.1.3.1.cmml" xref="Sx4.T2.7.7.7.m1.1.1.3">subscript</csymbol><ci id="Sx4.T2.7.7.7.m1.1.1.3.2.cmml" xref="Sx4.T2.7.7.7.m1.1.1.3.2">𝑃</ci><apply id="Sx4.T2.7.7.7.m1.1.1.3.3.cmml" xref="Sx4.T2.7.7.7.m1.1.1.3.3"><times id="Sx4.T2.7.7.7.m1.1.1.3.3.1.cmml" xref="Sx4.T2.7.7.7.m1.1.1.3.3.1"></times><ci id="Sx4.T2.7.7.7.m1.1.1.3.3.2.cmml" xref="Sx4.T2.7.7.7.m1.1.1.3.3.2">𝑟</ci><ci id="Sx4.T2.7.7.7.m1.1.1.3.3.3.cmml" xref="Sx4.T2.7.7.7.m1.1.1.3.3.3">𝑜</ci><ci id="Sx4.T2.7.7.7.m1.1.1.3.3.4.cmml" xref="Sx4.T2.7.7.7.m1.1.1.3.3.4">𝑣</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.T2.7.7.7.m1.1c">AP_{rov}</annotation></semantics></math></td>
<td id="Sx4.T2.8.8.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;"><math id="Sx4.T2.8.8.8.m1.1" class="ltx_Math" style="background-color:#FFBFBF;" alttext="AP_{cc}" display="inline"><semantics id="Sx4.T2.8.8.8.m1.1a"><mrow id="Sx4.T2.8.8.8.m1.1.1" xref="Sx4.T2.8.8.8.m1.1.1.cmml"><mi mathbackground="#FFBFBF" id="Sx4.T2.8.8.8.m1.1.1.2" xref="Sx4.T2.8.8.8.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="Sx4.T2.8.8.8.m1.1.1.1" xref="Sx4.T2.8.8.8.m1.1.1.1.cmml">​</mo><msub id="Sx4.T2.8.8.8.m1.1.1.3" xref="Sx4.T2.8.8.8.m1.1.1.3.cmml"><mi mathbackground="#FFBFBF" id="Sx4.T2.8.8.8.m1.1.1.3.2" xref="Sx4.T2.8.8.8.m1.1.1.3.2.cmml">P</mi><mrow id="Sx4.T2.8.8.8.m1.1.1.3.3" xref="Sx4.T2.8.8.8.m1.1.1.3.3.cmml"><mi mathbackground="#FFBFBF" id="Sx4.T2.8.8.8.m1.1.1.3.3.2" xref="Sx4.T2.8.8.8.m1.1.1.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="Sx4.T2.8.8.8.m1.1.1.3.3.1" xref="Sx4.T2.8.8.8.m1.1.1.3.3.1.cmml">​</mo><mi mathbackground="#FFBFBF" id="Sx4.T2.8.8.8.m1.1.1.3.3.3" xref="Sx4.T2.8.8.8.m1.1.1.3.3.3.cmml">c</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="Sx4.T2.8.8.8.m1.1b"><apply id="Sx4.T2.8.8.8.m1.1.1.cmml" xref="Sx4.T2.8.8.8.m1.1.1"><times id="Sx4.T2.8.8.8.m1.1.1.1.cmml" xref="Sx4.T2.8.8.8.m1.1.1.1"></times><ci id="Sx4.T2.8.8.8.m1.1.1.2.cmml" xref="Sx4.T2.8.8.8.m1.1.1.2">𝐴</ci><apply id="Sx4.T2.8.8.8.m1.1.1.3.cmml" xref="Sx4.T2.8.8.8.m1.1.1.3"><csymbol cd="ambiguous" id="Sx4.T2.8.8.8.m1.1.1.3.1.cmml" xref="Sx4.T2.8.8.8.m1.1.1.3">subscript</csymbol><ci id="Sx4.T2.8.8.8.m1.1.1.3.2.cmml" xref="Sx4.T2.8.8.8.m1.1.1.3.2">𝑃</ci><apply id="Sx4.T2.8.8.8.m1.1.1.3.3.cmml" xref="Sx4.T2.8.8.8.m1.1.1.3.3"><times id="Sx4.T2.8.8.8.m1.1.1.3.3.1.cmml" xref="Sx4.T2.8.8.8.m1.1.1.3.3.1"></times><ci id="Sx4.T2.8.8.8.m1.1.1.3.3.2.cmml" xref="Sx4.T2.8.8.8.m1.1.1.3.3.2">𝑐</ci><ci id="Sx4.T2.8.8.8.m1.1.1.3.3.3.cmml" xref="Sx4.T2.8.8.8.m1.1.1.3.3.3">𝑐</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.T2.8.8.8.m1.1c">AP_{cc}</annotation></semantics></math></td>
<td id="Sx4.T2.9.9.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;"><math id="Sx4.T2.9.9.9.m1.1" class="ltx_Math" style="background-color:#FFBFBF;" alttext="AP_{cy}" display="inline"><semantics id="Sx4.T2.9.9.9.m1.1a"><mrow id="Sx4.T2.9.9.9.m1.1.1" xref="Sx4.T2.9.9.9.m1.1.1.cmml"><mi mathbackground="#FFBFBF" id="Sx4.T2.9.9.9.m1.1.1.2" xref="Sx4.T2.9.9.9.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="Sx4.T2.9.9.9.m1.1.1.1" xref="Sx4.T2.9.9.9.m1.1.1.1.cmml">​</mo><msub id="Sx4.T2.9.9.9.m1.1.1.3" xref="Sx4.T2.9.9.9.m1.1.1.3.cmml"><mi mathbackground="#FFBFBF" id="Sx4.T2.9.9.9.m1.1.1.3.2" xref="Sx4.T2.9.9.9.m1.1.1.3.2.cmml">P</mi><mrow id="Sx4.T2.9.9.9.m1.1.1.3.3" xref="Sx4.T2.9.9.9.m1.1.1.3.3.cmml"><mi mathbackground="#FFBFBF" id="Sx4.T2.9.9.9.m1.1.1.3.3.2" xref="Sx4.T2.9.9.9.m1.1.1.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="Sx4.T2.9.9.9.m1.1.1.3.3.1" xref="Sx4.T2.9.9.9.m1.1.1.3.3.1.cmml">​</mo><mi mathbackground="#FFBFBF" id="Sx4.T2.9.9.9.m1.1.1.3.3.3" xref="Sx4.T2.9.9.9.m1.1.1.3.3.3.cmml">y</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="Sx4.T2.9.9.9.m1.1b"><apply id="Sx4.T2.9.9.9.m1.1.1.cmml" xref="Sx4.T2.9.9.9.m1.1.1"><times id="Sx4.T2.9.9.9.m1.1.1.1.cmml" xref="Sx4.T2.9.9.9.m1.1.1.1"></times><ci id="Sx4.T2.9.9.9.m1.1.1.2.cmml" xref="Sx4.T2.9.9.9.m1.1.1.2">𝐴</ci><apply id="Sx4.T2.9.9.9.m1.1.1.3.cmml" xref="Sx4.T2.9.9.9.m1.1.1.3"><csymbol cd="ambiguous" id="Sx4.T2.9.9.9.m1.1.1.3.1.cmml" xref="Sx4.T2.9.9.9.m1.1.1.3">subscript</csymbol><ci id="Sx4.T2.9.9.9.m1.1.1.3.2.cmml" xref="Sx4.T2.9.9.9.m1.1.1.3.2">𝑃</ci><apply id="Sx4.T2.9.9.9.m1.1.1.3.3.cmml" xref="Sx4.T2.9.9.9.m1.1.1.3.3"><times id="Sx4.T2.9.9.9.m1.1.1.3.3.1.cmml" xref="Sx4.T2.9.9.9.m1.1.1.3.3.1"></times><ci id="Sx4.T2.9.9.9.m1.1.1.3.3.2.cmml" xref="Sx4.T2.9.9.9.m1.1.1.3.3.2">𝑐</ci><ci id="Sx4.T2.9.9.9.m1.1.1.3.3.3.cmml" xref="Sx4.T2.9.9.9.m1.1.1.3.3.3">𝑦</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.T2.9.9.9.m1.1c">AP_{cy}</annotation></semantics></math></td>
<td id="Sx4.T2.10.10.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;"><math id="Sx4.T2.10.10.10.m1.1" class="ltx_Math" style="background-color:#FFBFBF;" alttext="AP_{mb}" display="inline"><semantics id="Sx4.T2.10.10.10.m1.1a"><mrow id="Sx4.T2.10.10.10.m1.1.1" xref="Sx4.T2.10.10.10.m1.1.1.cmml"><mi mathbackground="#FFBFBF" id="Sx4.T2.10.10.10.m1.1.1.2" xref="Sx4.T2.10.10.10.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="Sx4.T2.10.10.10.m1.1.1.1" xref="Sx4.T2.10.10.10.m1.1.1.1.cmml">​</mo><msub id="Sx4.T2.10.10.10.m1.1.1.3" xref="Sx4.T2.10.10.10.m1.1.1.3.cmml"><mi mathbackground="#FFBFBF" id="Sx4.T2.10.10.10.m1.1.1.3.2" xref="Sx4.T2.10.10.10.m1.1.1.3.2.cmml">P</mi><mrow id="Sx4.T2.10.10.10.m1.1.1.3.3" xref="Sx4.T2.10.10.10.m1.1.1.3.3.cmml"><mi mathbackground="#FFBFBF" id="Sx4.T2.10.10.10.m1.1.1.3.3.2" xref="Sx4.T2.10.10.10.m1.1.1.3.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="Sx4.T2.10.10.10.m1.1.1.3.3.1" xref="Sx4.T2.10.10.10.m1.1.1.3.3.1.cmml">​</mo><mi mathbackground="#FFBFBF" id="Sx4.T2.10.10.10.m1.1.1.3.3.3" xref="Sx4.T2.10.10.10.m1.1.1.3.3.3.cmml">b</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="Sx4.T2.10.10.10.m1.1b"><apply id="Sx4.T2.10.10.10.m1.1.1.cmml" xref="Sx4.T2.10.10.10.m1.1.1"><times id="Sx4.T2.10.10.10.m1.1.1.1.cmml" xref="Sx4.T2.10.10.10.m1.1.1.1"></times><ci id="Sx4.T2.10.10.10.m1.1.1.2.cmml" xref="Sx4.T2.10.10.10.m1.1.1.2">𝐴</ci><apply id="Sx4.T2.10.10.10.m1.1.1.3.cmml" xref="Sx4.T2.10.10.10.m1.1.1.3"><csymbol cd="ambiguous" id="Sx4.T2.10.10.10.m1.1.1.3.1.cmml" xref="Sx4.T2.10.10.10.m1.1.1.3">subscript</csymbol><ci id="Sx4.T2.10.10.10.m1.1.1.3.2.cmml" xref="Sx4.T2.10.10.10.m1.1.1.3.2">𝑃</ci><apply id="Sx4.T2.10.10.10.m1.1.1.3.3.cmml" xref="Sx4.T2.10.10.10.m1.1.1.3.3"><times id="Sx4.T2.10.10.10.m1.1.1.3.3.1.cmml" xref="Sx4.T2.10.10.10.m1.1.1.3.3.1"></times><ci id="Sx4.T2.10.10.10.m1.1.1.3.3.2.cmml" xref="Sx4.T2.10.10.10.m1.1.1.3.3.2">𝑚</ci><ci id="Sx4.T2.10.10.10.m1.1.1.3.3.3.cmml" xref="Sx4.T2.10.10.10.m1.1.1.3.3.3">𝑏</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.T2.10.10.10.m1.1c">AP_{mb}</annotation></semantics></math></td>
<td id="Sx4.T2.10.10.15" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;"><span id="Sx4.T2.10.10.15.1" class="ltx_text" style="background-color:#FFBFBF;">Params</span></td>
<td id="Sx4.T2.10.10.16" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;"><span id="Sx4.T2.10.10.16.1" class="ltx_text" style="background-color:#FFBFBF;">FLOPs</span></td>
<td id="Sx4.T2.10.10.17" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;"><span id="Sx4.T2.10.10.17.1" class="ltx_text" style="background-color:#FFBFBF;">FPS</span></td>
</tr>
<tr id="Sx4.T2.10.11.1" class="ltx_tr">
<td id="Sx4.T2.10.11.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;" rowspan="3"><span id="Sx4.T2.10.11.1.1.1" class="ltx_text">Faster-RCNN</span></td>
<td id="Sx4.T2.10.11.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;">Resnet-18</td>
<td id="Sx4.T2.10.11.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;"><span id="Sx4.T2.10.11.1.3.1" class="ltx_text" style="color:#FF0000;">0.839</span></td>
<td id="Sx4.T2.10.11.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;"><span id="Sx4.T2.10.11.1.4.1" class="ltx_text" style="color:#FF0000;">0.897</span></td>
<td id="Sx4.T2.10.11.1.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;">0.869</td>
<td id="Sx4.T2.10.11.1.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;">0.717</td>
<td id="Sx4.T2.10.11.1.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;">0.831</td>
<td id="Sx4.T2.10.11.1.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;">0.847</td>
<td id="Sx4.T2.10.11.1.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;">0.547</td>
<td id="Sx4.T2.10.11.1.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;">0.986</td>
<td id="Sx4.T2.10.11.1.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;">0.957</td>
<td id="Sx4.T2.10.11.1.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;">0.666</td>
<td id="Sx4.T2.10.11.1.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;">0.973</td>
<td id="Sx4.T2.10.11.1.14" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;">1.00</td>
<td id="Sx4.T2.10.11.1.15" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;">28.17M</td>
<td id="Sx4.T2.10.11.1.16" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;">49.78G</td>
<td id="Sx4.T2.10.11.1.17" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;">44.1</td>
</tr>
<tr id="Sx4.T2.10.12.2" class="ltx_tr">
<td id="Sx4.T2.10.12.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">Resnet-50</td>
<td id="Sx4.T2.10.12.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">0.829</td>
<td id="Sx4.T2.10.12.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">0.890</td>
<td id="Sx4.T2.10.12.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">0.870</td>
<td id="Sx4.T2.10.12.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">0.686</td>
<td id="Sx4.T2.10.12.2.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">0.878</td>
<td id="Sx4.T2.10.12.2.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">0.889</td>
<td id="Sx4.T2.10.12.2.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">0.621</td>
<td id="Sx4.T2.10.12.2.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">0.973</td>
<td id="Sx4.T2.10.12.2.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">0.969</td>
<td id="Sx4.T2.10.12.2.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">0.538</td>
<td id="Sx4.T2.10.12.2.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">0.872</td>
<td id="Sx4.T2.10.12.2.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">1.000</td>
<td id="Sx4.T2.10.12.2.14" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">41.17M</td>
<td id="Sx4.T2.10.12.2.15" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">63.29G</td>
<td id="Sx4.T2.10.12.2.16" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">32.9</td>
</tr>
<tr id="Sx4.T2.10.13.3" class="ltx_tr">
<td id="Sx4.T2.10.13.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">Resnet-101</td>
<td id="Sx4.T2.10.13.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">0.818</td>
<td id="Sx4.T2.10.13.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">0.877</td>
<td id="Sx4.T2.10.13.3.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">0.865</td>
<td id="Sx4.T2.10.13.3.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">0.697</td>
<td id="Sx4.T2.10.13.3.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">0.913</td>
<td id="Sx4.T2.10.13.3.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">0.840</td>
<td id="Sx4.T2.10.13.3.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">0.572</td>
<td id="Sx4.T2.10.13.3.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">0.967</td>
<td id="Sx4.T2.10.13.3.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">0.974</td>
<td id="Sx4.T2.10.13.3.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">0.491</td>
<td id="Sx4.T2.10.13.3.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">0.944</td>
<td id="Sx4.T2.10.13.3.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">0.912</td>
<td id="Sx4.T2.10.13.3.14" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">60.16M</td>
<td id="Sx4.T2.10.13.3.15" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">82.77G</td>
<td id="Sx4.T2.10.13.3.16" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">26.6</td>
</tr>
<tr id="Sx4.T2.10.14.4" class="ltx_tr">
<td id="Sx4.T2.10.14.4.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;" rowspan="2"><span id="Sx4.T2.10.14.4.1.1" class="ltx_text">YOLOv3</span></td>
<td id="Sx4.T2.10.14.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;">Darknet-53</td>
<td id="Sx4.T2.10.14.4.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;">0.801</td>
<td id="Sx4.T2.10.14.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;">0.880</td>
<td id="Sx4.T2.10.14.4.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;">0.860</td>
<td id="Sx4.T2.10.14.4.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;">0.669</td>
<td id="Sx4.T2.10.14.4.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;">0.782</td>
<td id="Sx4.T2.10.14.4.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;">0.874</td>
<td id="Sx4.T2.10.14.4.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;">0.470</td>
<td id="Sx4.T2.10.14.4.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;">0.988</td>
<td id="Sx4.T2.10.14.4.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;">0.945</td>
<td id="Sx4.T2.10.14.4.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;">0.519</td>
<td id="Sx4.T2.10.14.4.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;">0.906</td>
<td id="Sx4.T2.10.14.4.14" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;">1.000</td>
<td id="Sx4.T2.10.14.4.15" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;">61.57M</td>
<td id="Sx4.T2.10.14.4.16" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;">49.67G</td>
<td id="Sx4.T2.10.14.4.17" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.7pt;padding-right:0.7pt;">49.8</td>
</tr>
<tr id="Sx4.T2.10.15.5" class="ltx_tr">
<td id="Sx4.T2.10.15.5.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">MobilenetV2</td>
<td id="Sx4.T2.10.15.5.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">0.787</td>
<td id="Sx4.T2.10.15.5.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">0.868</td>
<td id="Sx4.T2.10.15.5.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">0.790</td>
<td id="Sx4.T2.10.15.5.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">0.573</td>
<td id="Sx4.T2.10.15.5.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">0.808</td>
<td id="Sx4.T2.10.15.5.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">0.738</td>
<td id="Sx4.T2.10.15.5.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">0.518</td>
<td id="Sx4.T2.10.15.5.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">0.992</td>
<td id="Sx4.T2.10.15.5.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">0.986</td>
<td id="Sx4.T2.10.15.5.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">0.498</td>
<td id="Sx4.T2.10.15.5.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">0.963</td>
<td id="Sx4.T2.10.15.5.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;">1.000</td>
<td id="Sx4.T2.10.15.5.14" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;"><span id="Sx4.T2.10.15.5.14.1" class="ltx_text" style="color:#FF0000;">3.68M</span></td>
<td id="Sx4.T2.10.15.5.15" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;"><span id="Sx4.T2.10.15.5.15.1" class="ltx_text" style="color:#FF0000;">4.22G</span></td>
<td id="Sx4.T2.10.15.5.16" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r" style="padding-left:0.7pt;padding-right:0.7pt;"><span id="Sx4.T2.10.15.5.16.1" class="ltx_text" style="color:#FF0000;">93.4</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="Sx4.T2.27.6.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="Sx4.T2.20.5" class="ltx_text" style="font-size:90%;">Benchmark of Faster-RCNN and YOLOv3 with various backbones performed on UATD. (AP<sub id="Sx4.T2.20.5.1" class="ltx_sub"><span id="Sx4.T2.20.5.1.1" class="ltx_text ltx_font_italic">hb</span></sub>: AP in human body, AP<sub id="Sx4.T2.20.5.2" class="ltx_sub"><span id="Sx4.T2.20.5.2.1" class="ltx_text ltx_font_italic">sc</span></sub>: AP in square cage, AP<sub id="Sx4.T2.20.5.3" class="ltx_sub"><span id="Sx4.T2.20.5.3.1" class="ltx_text ltx_font_italic">cc</span></sub>: AP in circle cage, AP<sub id="Sx4.T2.20.5.4" class="ltx_sub"><span id="Sx4.T2.20.5.4.1" class="ltx_text ltx_font_italic">cy</span></sub>: AP in cylinder, AP<sub id="Sx4.T2.20.5.5" class="ltx_sub"><span id="Sx4.T2.20.5.5.1" class="ltx_text ltx_font_italic">mb</span></sub>: AP in metal bucket)</span></figcaption>
</figure>
<div id="Sx4.SS0.SSSx1.p2" class="ltx_para">
<p id="Sx4.SS0.SSSx1.p2.1" class="ltx_p">The evaluation has considered both accuracy and efficiency. We first adopted the evaluation metric mean average precision (mAP) and mean average recall rate<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> (mAR) to measure the accuracy of detectors on UATD. Then, the efficiency was tested on the local computer and the indicators of FPS, Params and FLOPs were given. More details as below:</p>
</div>
<div id="Sx4.SS0.SSSx1.p3" class="ltx_para">
<p id="Sx4.SS0.SSSx1.p3.1" class="ltx_p"><span id="Sx4.SS0.SSSx1.p3.1.1" class="ltx_text ltx_font_bold">Accuracy metrics:</span></p>
</div>
<div id="Sx4.SS0.SSSx1.p4" class="ltx_para">
<p id="Sx4.SS0.SSSx1.p4.1" class="ltx_p"><span id="Sx4.SS0.SSSx1.p4.1.1" class="ltx_text ltx_font_bold">mAP</span> - Corresponds to the mean AP for intersect of union (IoU) equals to 0.5 on total categories (10 in UATD).</p>
</div>
<div id="Sx4.SS0.SSSx1.p5" class="ltx_para">
<p id="Sx4.SS0.SSSx1.p5.1" class="ltx_p"><span id="Sx4.SS0.SSSx1.p5.1.1" class="ltx_text ltx_font_bold">mAR</span> - Corresponds to the mean recall rate on total categories (10 in UATD).</p>
</div>
<div id="Sx4.SS0.SSSx1.p6" class="ltx_para">
<p id="Sx4.SS0.SSSx1.p6.1" class="ltx_p"><span id="Sx4.SS0.SSSx1.p6.1.1" class="ltx_text ltx_font_bold">AP<sub id="Sx4.SS0.SSSx1.p6.1.1.1" class="ltx_sub"><span id="Sx4.SS0.SSSx1.p6.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">name</span></sub></span> - AP of class (name belongs to the classes in UATD).</p>
</div>
<div id="Sx4.SS0.SSSx1.p7" class="ltx_para">
<p id="Sx4.SS0.SSSx1.p7.1" class="ltx_p"><span id="Sx4.SS0.SSSx1.p7.1.1" class="ltx_text ltx_font_bold">Efficiency metrics:</span></p>
</div>
<div id="Sx4.SS0.SSSx1.p8" class="ltx_para">
<p id="Sx4.SS0.SSSx1.p8.1" class="ltx_p"><span id="Sx4.SS0.SSSx1.p8.1.1" class="ltx_text ltx_font_bold">Params</span> - The parameter size of models.</p>
</div>
<div id="Sx4.SS0.SSSx1.p9" class="ltx_para">
<p id="Sx4.SS0.SSSx1.p9.1" class="ltx_p"><span id="Sx4.SS0.SSSx1.p9.1.1" class="ltx_text ltx_font_bold">FLOPs</span> - Floating-point operations per second with input image size of 512 x 512.</p>
</div>
<div id="Sx4.SS0.SSSx1.p10" class="ltx_para">
<p id="Sx4.SS0.SSSx1.p10.1" class="ltx_p"><span id="Sx4.SS0.SSSx1.p10.1.1" class="ltx_text ltx_font_bold">FPS</span> - Frames per seconds.</p>
</div>
<div id="Sx4.SS0.SSSx1.p11" class="ltx_para">
<p id="Sx4.SS0.SSSx1.p11.1" class="ltx_p">The UATD was trained on the local machine with an NVIDIA GeForce GTX 1080 GPU. The image height of the input sonar image is resized to 512 while the image width is scaled by the original ratio in both training and inference. The pretrained parameters on ImageNet<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> were used to initialize the backbone. During the training period, the initial learning rate was set to 0.0005 and decreased by 0.1 at the 8th and 11th epoch (12 epochs in total) respectively. The warm-up strategy was adopted with a 0.0001 warm-up ratio and increased by linear step in the first 500 iterations. Otherwise, Adam method was employed to optimize the models.</p>
</div>
<div id="Sx4.SS0.SSSx1.p12" class="ltx_para">
<p id="Sx4.SS0.SSSx1.p12.1" class="ltx_p">According to the benchmark, Faster-RCNN with Resnet-18 backbone achieves the best mAP of 83.9% and the best mAR of 89.7%. On the other hand, YOLOv3 with MobilenetV2 backbone has a good performance in efficiency with only 3.68M Params and 4.22G FLOPs, as well as the fastest inference speed of 93.4 FPS tested on the local machine.</p>
</div>
</section>
</section>
<section id="Sx5" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">Code availability</h2>

<div id="Sx5.p1" class="ltx_para">
<p id="Sx5.p1.1" class="ltx_p">UATD dataset is published in a figshare repository<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. Furthermore, the annotation tool OpenSLT is published alongside the dataset, archived as "UATD_OpenSLT.zip". OpenSLT is developed based on Qt 5.9. The tool worked well on Ubuntu 18.04/20.04 environment during our annotation work. In addition, we provide an example (a small dataset with several sonar image files and corresponding CSV files) with the tool for users to test. The file README.md along with the tool plays the role of guidance for the users.</p>
</div>
</section>
<section id="Sx6" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx6.p1" class="ltx_para">
<p id="Sx6.p1.1" class="ltx_p">This work was supported by the National Natural Science Foundation of China (Grant, No.62027826). We thank the Dalian Key Laboratory of Underwater Robot of Dalian University of Technology for their support during the data collection. We thank the support provided by OpenI Community(<a target="_blank" href="https://openi.pcl.ac.cn" title="" class="ltx_ref ltx_href">https://openi.pcl.ac.cn</a>) during data processing.</p>
</div>
</section>
<section id="Sx7" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">Author contributions</h2>

<div id="Sx7.p1" class="ltx_para">
<p id="Sx7.p1.1" class="ltx_p">Kang Qiu, Jian Yang and Kaibing Xie generated the dataset. Jian Yang contributed to the annotation software development and technical validation. Kaibing Xie draft the paper and provided feedback on the manuscript.</p>
</div>
</section>
<section id="Sx8" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">Competing interests</h2>

<div id="Sx8.p1" class="ltx_para">
<p id="Sx8.p1.1" class="ltx_p">The authors declare no competing interests.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography" lang="en">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Pedersen, M., Bruslund Haurum, J.,
Gade, R. &amp; Moeslund, T. B.

</span>
<span class="ltx_bibblock">Detection of marine animals in a new underwater
dataset with varying visibility.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition Workshops</em>,
18–26,
(2019).

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Islam, M. J. <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">et al.</em>

</span>
<span class="ltx_bibblock">Semantic segmentation of underwater imagery: Dataset
and benchmark.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.2.1" class="ltx_emph ltx_font_italic">2020 IEEE/RSJ International Conference
on Intelligent Robots and Systems (IROS)</em>, 1769–1776,
<a target="_blank" href="https://doi.org/10.1109/IROS45743.2020.9340821" title="" class="ltx_ref ltx_url">https://doi.org/10.1109/IROS45743.2020.9340821</a> (2020).

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Liu, C. <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">et al.</em>

</span>
<span class="ltx_bibblock">A dataset and benchmark of underwater object
detection for robot picking.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.2.1" class="ltx_emph ltx_font_italic">2021 IEEE International Conference on
Multimedia &amp; Expo Workshops (ICMEW)</em>, 1–6,
<a target="_blank" href="https://doi.org/10.1109/ICMEW53276.2021.9455997" title="" class="ltx_ref ltx_url">https://doi.org/10.1109/ICMEW53276.2021.9455997</a> (2021).

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Zhang, H., Tian, M., Shao,
G., Cheng, J. &amp; Liu, J.

</span>
<span class="ltx_bibblock">Target detection of forward-looking
sonar image based on improved yolov5.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic"><span id="bib.bib4.1.1.1" class="ltx_ERROR undefined">\JournalTitle</span>IEEE Access</em> <span id="bib.bib4.2.2" class="ltx_text ltx_font_bold">10</span>,
18023–18034,
<a target="_blank" href="https://doi.org/10.1109/ACCESS.2022.3150339" title="" class="ltx_ref ltx_url">https://doi.org/10.1109/ACCESS.2022.3150339</a> (2022).

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Fan, Z., Xia, W., Liu, X.
&amp; Li, H.

</span>
<span class="ltx_bibblock">Detection and segmentation of
underwater objects from forward-looking sonar based on a modified mask
rcnn.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic"><span id="bib.bib5.1.1.1" class="ltx_ERROR undefined">\JournalTitle</span>Signal, Image and Video Processing</em>
<span id="bib.bib5.2.2" class="ltx_text ltx_font_bold">15</span>, 1135–1143,
<a target="_blank" href="https://doi.org/10.1007/s11760-020-01841-x" title="" class="ltx_ref ltx_url">https://doi.org/10.1007/s11760-020-01841-x</a> (2021).

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Jiang, L., Cai, T., Ma,
Q., Xu, F. &amp; Wang, S.

</span>
<span class="ltx_bibblock">Active object detection in sonar
images.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic"><span id="bib.bib6.1.1.1" class="ltx_ERROR undefined">\JournalTitle</span>IEEE Access</em> <span id="bib.bib6.2.2" class="ltx_text ltx_font_bold">8</span>,
102540–102553,
<a target="_blank" href="https://doi.org/10.1109/ACCESS.2020.2999341" title="" class="ltx_ref ltx_url">https://doi.org/10.1109/ACCESS.2020.2999341</a> (2020).

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Preciado-Grijalva, A., Wehbe, B.,
Firvida, M. B. &amp; Valdenegro-Toro, M.

</span>
<span class="ltx_bibblock">Self-supervised learning for sonar image
classification.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">2022 IEEE/CVF Conference on Computer
Vision and Pattern Recognition Workshops (CVPRW)</em>,
1498–1507,
<a target="_blank" href="https://doi.org/10.1109/CVPRW56347.2022.00156" title="" class="ltx_ref ltx_url">https://doi.org/10.1109/CVPRW56347.2022.00156</a> (2022).

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Karimanzira, D., Renkewitz, H.,
Shea, D. &amp; Albiez, J.

</span>
<span class="ltx_bibblock">Object detection in sonar images.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic"><span id="bib.bib8.1.1.1" class="ltx_ERROR undefined">\JournalTitle</span>Electronics</em> <span id="bib.bib8.2.2" class="ltx_text ltx_font_bold">9</span>,
1180, <a target="_blank" href="https://www.mdpi.com/2079-9292/9/7/1180" title="" class="ltx_ref ltx_url">https://www.mdpi.com/2079-9292/9/7/1180</a>
(2020).

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Neves, G., Ruiz, M.,
Fontinele, J. &amp; Oliveira, L.

</span>
<span class="ltx_bibblock">Rotated object detection with
forward-looking sonar in underwater applications.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic"><span id="bib.bib9.1.1.1" class="ltx_ERROR undefined">\JournalTitle</span>Expert Systems with Applications</em>
<span id="bib.bib9.2.2" class="ltx_text ltx_font_bold">140</span>, 112870,
<a target="_blank" href="https://doi.org/10.1016/j.eswa.2019.112870" title="" class="ltx_ref ltx_url">https://doi.org/10.1016/j.eswa.2019.112870</a> (2020).

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Cao, X., Ren, L. &amp; Sun,
C.

</span>
<span class="ltx_bibblock">Research on obstacle detection and
avoidance of autonomous underwater vehicle based on forward-looking sonar.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic"><span id="bib.bib10.1.1.1" class="ltx_ERROR undefined">\JournalTitle</span>IEEE Transactions on Neural Networks and
Learning Systems</em> <a target="_blank" href="https://doi.org/10.1109/TNNLS.2022.3156907" title="" class="ltx_ref ltx_url">https://doi.org/10.1109/TNNLS.2022.3156907</a>
(2022).

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Choi, W.-S. <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">et al.</em>

</span>
<span class="ltx_bibblock">Physics-based modelling and
simulation of multibeam echosounder perception for autonomous underwater
manipulation.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.2.1" class="ltx_emph ltx_font_italic"><span id="bib.bib11.2.1.1" class="ltx_ERROR undefined">\JournalTitle</span>Frontiers in Robotics and AI</em>
<span id="bib.bib11.3.2" class="ltx_text ltx_font_bold">8</span>, <a target="_blank" href="https://doi.org/10.3389/frobt.2021.706646" title="" class="ltx_ref ltx_url">https://doi.org/10.3389/frobt.2021.706646</a>
(2021).

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Cerqueira, R., Trocoli, T.,
Albiez, J. &amp; Oliveira, L.

</span>
<span class="ltx_bibblock">A rasterized ray-tracer pipeline
for real-time, multi-device sonar simulation.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic"><span id="bib.bib12.1.1.1" class="ltx_ERROR undefined">\JournalTitle</span>Graphical Models</em>
<span id="bib.bib12.2.2" class="ltx_text ltx_font_bold">111</span>, 101086,
<a target="_blank" href="https://doi.org/10.1016/j.gmod.2020.101086" title="" class="ltx_ref ltx_url">https://doi.org/10.1016/j.gmod.2020.101086</a> (2020).

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Sung, M., Kim, J., Kim,
J. &amp; Yu, S.-C.

</span>
<span class="ltx_bibblock">Realistic sonar image simulation
using generative adversarial network.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic"><span id="bib.bib13.1.1.1" class="ltx_ERROR undefined">\JournalTitle</span>IFAC-PapersOnLine</em>
<span id="bib.bib13.2.2" class="ltx_text ltx_font_bold">52</span>, 291–296,
<a target="_blank" href="https://doi.org/10.1016/j.ifacol.2019.12.322" title="" class="ltx_ref ltx_url">https://doi.org/10.1016/j.ifacol.2019.12.322</a> (2019).

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Sung, M. <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">et al.</em>

</span>
<span class="ltx_bibblock">Realistic sonar image simulation
using deep learning for underwater object detection.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.2.1" class="ltx_emph ltx_font_italic"><span id="bib.bib14.2.1.1" class="ltx_ERROR undefined">\JournalTitle</span>International Journal of Control, Automation and
Systems</em> <span id="bib.bib14.3.2" class="ltx_text ltx_font_bold">18</span>, 523–534,
<a target="_blank" href="https://doi.org/10.1007/s12555-019-0691-3" title="" class="ltx_ref ltx_url">https://doi.org/10.1007/s12555-019-0691-3</a> (2020).

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Liu, D. <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">et al.</em>

</span>
<span class="ltx_bibblock">Cyclegan-based realistic image
dataset generation for forward-looking sonar.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.2.1" class="ltx_emph ltx_font_italic"><span id="bib.bib15.2.1.1" class="ltx_ERROR undefined">\JournalTitle</span>Advanced Robotics</em>
<span id="bib.bib15.3.2" class="ltx_text ltx_font_bold">35</span>, 242–254,
<a target="_blank" href="https://doi.org/10.1080/01691864.2021.1873845" title="" class="ltx_ref ltx_url">https://doi.org/10.1080/01691864.2021.1873845</a> (2021).

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
McCann, E., Li, L.,
Pangle, K., Johnson, N. &amp;
Eickholt, J.

</span>
<span class="ltx_bibblock">An underwater observation dataset
for fish classification and fishery assessment.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic"><span id="bib.bib16.1.1.1" class="ltx_ERROR undefined">\JournalTitle</span>Scientific data</em> <span id="bib.bib16.2.2" class="ltx_text ltx_font_bold">5</span>,
1–8, <a target="_blank" href="https://doi.org/10.1038/sdata.2018.190" title="" class="ltx_ref ltx_url">https://doi.org/10.1038/sdata.2018.190</a>
(2018).

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Singh, D. &amp; Valdenegro-Toro, M.

</span>
<span class="ltx_bibblock">The marine debris dataset for forward-looking sonar
semantic segmentation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">2021 IEEE/CVF International Conference
on Computer Vision Workshops (ICCVW)</em>, 3734–3742,
<a target="_blank" href="https://doi.org/10.1109/ICCVW54120.2021.00417" title="" class="ltx_ref ltx_url">https://doi.org/10.1109/ICCVW54120.2021.00417</a> (2021).

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Dos Santos, M. M., De Giacomo, G. G.,
Drews-Jr, P. L. &amp; Botelho, S. S.

</span>
<span class="ltx_bibblock">Cross-view and cross-domain
underwater localization based on optical aerial and acoustic underwater
images.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic"><span id="bib.bib18.1.1.1" class="ltx_ERROR undefined">\JournalTitle</span>IEEE Robotics and Automation Letters</em>
<span id="bib.bib18.2.2" class="ltx_text ltx_font_bold">7</span>, 4969–4974,
<a target="_blank" href="https://doi.org/10.1109/LRA.2022.3154482" title="" class="ltx_ref ltx_url">https://doi.org/10.1109/LRA.2022.3154482</a> (2022).

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Yang, J. &amp; Xie, K.

</span>
<span class="ltx_bibblock">Underwater acoustic target detection (UATD)
dataset.

</span>
<span class="ltx_bibblock">Figshare
<a target="_blank" href="https://doi.org/10.6084/m9.figshare.21331143.v3" title="" class="ltx_ref ltx_url">https://doi.org/10.6084/m9.figshare.21331143.v3</a>
(2022).

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Chen, K. <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">et al.</em>

</span>
<span class="ltx_bibblock">Mmdetection: Open mmlab detection toolbox and
benchmark.

</span>
<span class="ltx_bibblock">Preprint at
<a target="_blank" href="https://doi.org/10.48550/arXiv.1906.07155" title="" class="ltx_ref ltx_url">https://doi.org/10.48550/arXiv.1906.07155</a> (2019).

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Ren, S., He, K., Girshick,
R. &amp; Sun, J.

</span>
<span class="ltx_bibblock">Faster r-cnn: Towards real-time
object detection with region proposal networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic"><span id="bib.bib21.1.1.1" class="ltx_ERROR undefined">\JournalTitle</span>IEEE Transactions on Pattern Analysis and
Machine Intelligence</em> <span id="bib.bib21.2.2" class="ltx_text ltx_font_bold">39</span>,
1137–1149, <a target="_blank" href="https://doi.org/10.1109/TPAMI.2016.2577031" title="" class="ltx_ref ltx_url">https://doi.org/10.1109/TPAMI.2016.2577031</a>
(2017).

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Redmon, J. &amp; Farhadi, A.

</span>
<span class="ltx_bibblock">Yolov3: An incremental improvement.

</span>
<span class="ltx_bibblock">Preprint at
<a target="_blank" href="https://doi.org/10.48550/arXiv.1804.02767" title="" class="ltx_ref ltx_url">https://doi.org/10.48550/arXiv.1804.02767</a> (2018).

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Everingham, M., Van Gool, L.,
Williams, C. K., Winn, J. &amp;
Zisserman, A.

</span>
<span class="ltx_bibblock">The pascal visual object classes
(voc) challenge.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic"><span id="bib.bib23.1.1.1" class="ltx_ERROR undefined">\JournalTitle</span>International journal of computer vision</em>
<span id="bib.bib23.2.2" class="ltx_text ltx_font_bold">88</span>, 303–338,
<a target="_blank" href="https://doi.org/10.1007/s11263-009-0275-4" title="" class="ltx_ref ltx_url">https://doi.org/10.1007/s11263-009-0275-4</a> (2010).

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Deng, J. <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">et al.</em>

</span>
<span class="ltx_bibblock">Imagenet: A large-scale hierarchical image database.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.2.1" class="ltx_emph ltx_font_italic">2009 IEEE conference on computer vision
and pattern recognition</em>, 248–255,
<a target="_blank" href="https://doi.org/10.1109/CVPR.2009.5206848" title="" class="ltx_ref ltx_url">https://doi.org/10.1109/CVPR.2009.5206848</a> (2009).

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2212.00351" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2212.00352" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2212.00352">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2212.00352" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2212.00353" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 08:27:28 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
