<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2112.12141] Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving</title><meta property="og:description" content="3D human pose estimation (HPE) in autonomous vehicles (AV) differs from other use cases in many factors, including the 3D resolution and range of data, absence of dense depth maps, failure modes for LiDAR, relative loc…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2112.12141">

<!--Generated on Fri Mar  1 15:59:26 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jingxiao Zheng<sup id="id3.1.id1" class="ltx_sup">1</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xinwei Shi<sup id="id4.1.id1" class="ltx_sup">1</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Alexander Gorban<sup id="id5.1.id1" class="ltx_sup">1</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Junhua Mao<sup id="id6.1.id1" class="ltx_sup">1</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yang Song<sup id="id7.1.id1" class="ltx_sup">1</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Charles R. Qi<sup id="id8.1.id1" class="ltx_sup">1</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ting Liu<sup id="id9.1.id1" class="ltx_sup">2</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Visesh Chari<sup id="id10.1.id1" class="ltx_sup">1</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Andre Cornman<sup id="id11.1.id1" class="ltx_sup">1</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yin Zhou<sup id="id12.1.id1" class="ltx_sup">1</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Congcong Li<sup id="id13.1.id1" class="ltx_sup">1</sup>  Dragomir Anguelov<sup id="id14.2.id2" class="ltx_sup">1</sup>

<br class="ltx_break"><sup id="id15.3.id3" class="ltx_sup">1</sup> Waymo LLC  <sup id="id16.4.id4" class="ltx_sup">2</sup> Google Research
<br class="ltx_break"><span id="id17.5.id5" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{jingxiaozheng, xinweis, gorban, junhuamao, yangsong, rqi}@waymo.com</span>, 
<br class="ltx_break"><span id="id18.6.id6" class="ltx_text ltx_font_typewriter" style="font-size:90%;">liuti@google.com, {visesh, cornman, yinzhou, congcongli, dragomir}@waymo.com</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id19.id1" class="ltx_p">3D human pose estimation (HPE) in autonomous vehicles (AV) differs from other use cases in many factors, including the 3D resolution and range of data, absence of dense depth maps, failure modes for LiDAR, relative location between the camera and LiDAR, and a high bar for estimation accuracy.
Data collected for other use cases (such as virtual reality, gaming, and animation) may therefore not be usable for AV applications. This necessitates the collection and annotation of a large amount of 3D data for HPE in AV, which is time-consuming and expensive.
</p>
<p id="id2.2" class="ltx_p">In this paper, we propose one of the first approaches to alleviate this problem in the AV setting. Specifically, we propose a multi-modal approach which uses 2D labels on RGB images as weak supervision to perform 3D HPE. The proposed multi-modal architecture incorporates LiDAR and camera inputs with an auxiliary segmentation branch. On the Waymo Open Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, our approach achieves a <math id="id1.1.m1.1" class="ltx_Math" alttext="\sim 22\%" display="inline"><semantics id="id1.1.m1.1a"><mrow id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mi id="id1.1.m1.1.1.2" xref="id1.1.m1.1.1.2.cmml"></mi><mo id="id1.1.m1.1.1.1" xref="id1.1.m1.1.1.1.cmml">∼</mo><mrow id="id1.1.m1.1.1.3" xref="id1.1.m1.1.1.3.cmml"><mn id="id1.1.m1.1.1.3.2" xref="id1.1.m1.1.1.3.2.cmml">22</mn><mo id="id1.1.m1.1.1.3.1" xref="id1.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><csymbol cd="latexml" id="id1.1.m1.1.1.1.cmml" xref="id1.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="id1.1.m1.1.1.2.cmml" xref="id1.1.m1.1.1.2">absent</csymbol><apply id="id1.1.m1.1.1.3.cmml" xref="id1.1.m1.1.1.3"><csymbol cd="latexml" id="id1.1.m1.1.1.3.1.cmml" xref="id1.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="id1.1.m1.1.1.3.2.cmml" xref="id1.1.m1.1.1.3.2">22</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">\sim 22\%</annotation></semantics></math> relative improvement over camera-only 2D HPE baseline, and <math id="id2.2.m2.1" class="ltx_Math" alttext="\sim 6\%" display="inline"><semantics id="id2.2.m2.1a"><mrow id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml"><mi id="id2.2.m2.1.1.2" xref="id2.2.m2.1.1.2.cmml"></mi><mo id="id2.2.m2.1.1.1" xref="id2.2.m2.1.1.1.cmml">∼</mo><mrow id="id2.2.m2.1.1.3" xref="id2.2.m2.1.1.3.cmml"><mn id="id2.2.m2.1.1.3.2" xref="id2.2.m2.1.1.3.2.cmml">6</mn><mo id="id2.2.m2.1.1.3.1" xref="id2.2.m2.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><apply id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1"><csymbol cd="latexml" id="id2.2.m2.1.1.1.cmml" xref="id2.2.m2.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="id2.2.m2.1.1.2.cmml" xref="id2.2.m2.1.1.2">absent</csymbol><apply id="id2.2.m2.1.1.3.cmml" xref="id2.2.m2.1.1.3"><csymbol cd="latexml" id="id2.2.m2.1.1.3.1.cmml" xref="id2.2.m2.1.1.3.1">percent</csymbol><cn type="integer" id="id2.2.m2.1.1.3.2.cmml" xref="id2.2.m2.1.1.3.2">6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">\sim 6\%</annotation></semantics></math> improvement over LiDAR-only model. Finally, careful ablation studies and parts based analysis illustrate the advantages of each of our contributions.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">3D Human Pose Estimation (3D HPE) for autonomous vehicles (AV) has received little attention in the academic community relative to other applications like animation, games, virtual reality (VR), or surveillance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> despite its central role in AV.
Arguably, this could be because 3D HPE in AV differs greatly from HPE in other scenarios. For one, AV requires HPE in outdoor environments and in 3D, which is not the case for animation or games which are not outdoor <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> or surveillance which is not necessarily in 3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.
Secondly, sensor characteristics and placements for LiDAR follow different logic compared to other depth sensors like in games or VR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>.
Thirdly, requirements for accuracy, real-time prediction and generalization over a wide variety of scenarios are also different. Animation, surveillance, games and VR have relatively lower bars for accuracy compared to AV where HPE is a critical component for the perception module.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2112.12141/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_img_square" width="461" height="384" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">Different characteristics of RGB-D and Camera+LiDAR sensors. Top row examples are from dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>; bottom row examples are from the Waymo Open Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>.</span></figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Diving deeper into the sensor, LiDAR differs from other depth sensors in several ways. Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> summarizes these differences and gives visual illustrations. Firstly, LiDAR has longer range and larger FOV than RGB-D sensors, and it is more suitable for outdoor scenes. Point clouds from LiDAR are sparser and sweep a wider range of the environment. Secondly, LiDARs and cameras may not be co-located on AV platforms. Accurate registration is needed for correspondence between point clouds and image textures.
Finally, failure cases for LiDAR caused by reflective materials, weather conditions, and dust on sensors differ from other sensor failures due to the difference in the physics of sensing as well as environmental factors. </p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Given the aforementioned differences and the evidence of 3D HPE models not generalizing across different datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> because of dataset bias, we see the need for developing approaches specific to AV that tackle the problem of 3D HPE.
One straightforward way to tackle this problem
would be to collect 3D human pose annotations for a large and diverse dataset of LiDAR point clouds in AV scenarios like the Waymo Open Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. However, the ”in-the-wild” setting of 3D HPE for AV presents serious challenges to annotating training data at this scale, in terms of time, cost and coverage of long tail scenarios.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this paper, we propose an approach to use widely available and easier to get 2D human pose annotations to drive 3D HPE in a weakly-supervised setting. While the weakly-supervised setting is not uncommon for 3D HPE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, using LiDAR in the AV setting requires separate consideration for the reasons mentioned thus far.
Figure <a href="#S2.F2" title="Figure 2 ‣ 2 Related Work ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the idea of the proposed method. While we use PointNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>-inspired architecture as the main point cloud processing network, we cannot fuse camera and LiDAR imagery at the lower levels like in other settings <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> because of the sparsity of LiDAR.
We propose a cascade architecture with a CNN-based camera network for 2D pose estimation. In addition, we add an auxiliary segmentation branch in the point network to introduce stronger supervision to each point via multi-task learning. This gives us an advantage in the ”in-the-wild” settings, as shown by the results on the Waymo Open Dataset (Table <a href="#S4.T1" title="Table 1 ‣ Labeling: ‣ 4.1 Data and Evaluation Metrics ‣ 4 Experiments ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and Table <a href="#S4.T3" title="Table 3 ‣ Labeling: ‣ 4.1 Data and Evaluation Metrics ‣ 4 Experiments ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). In the rest of the paper, we show that pose estimation performance benefits from all these designs.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The main contributions of this paper are as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We propose a multi-modal framework which fuses RGB camera images and LiDAR point clouds to exploit the texture information and geometry information for 3D pose estimation in challenging AV scenarios.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We train 3D pose estimation models by weak supervision from pure 2D labels, which makes the labeling stage much less expensive.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We introduce an auxiliary segmentation branch into the point network to improve 3D pose estimation performance via multi-task learning.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">We review related work in Section <a href="#S2" title="2 Related Work ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, and follow it up with details about our approach in Section <a href="#S3" title="3 Method ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Section <a href="#S4" title="4 Experiments ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> discusses detailed experiments with results on two large datasets, followed by ablation studies and performance analysis (refer to supplementary for additional results).
Finally, we conclude in Section <a href="#S5" title="5 Conclusions ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> with a discussion of avenues for improvement and future directions.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In recent years, many methods have been introduced for 3D HPE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, although hardly any work has addressed the AV scenario. Most take RGB or RGB-D images as inputs, and operate in monocular, multiview or video settings.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2112.12141/assets/model_overview_new.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="216" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S2.F2.3.2" class="ltx_text" style="font-size:90%;">Model overview: the model is a cascade of camera network and point network. The camera network takes the 2D camera image as input and predicts the 2D keypoint heatmap. This 2D heatmap is augmented with the point cloud using modality fusion (Figure <a href="#S3.F3" title="Figure 3 ‣ 3.2 Modality Fusion of LiDAR and Camera ‣ 3 Method ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) and is fed into the point network. The regression branch of the point network predicts 3D keypoint coordinates as output. The auxiliary segmentation branch generates pointwise predictions which are only used for training. The model is trained on pseudo 3D labels and pointwise labels generated from 2D keypoint labels (Figure <a href="#S3.F4" title="Figure 4 ‣ 3.2 Modality Fusion of LiDAR and Camera ‣ 3 Method ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>).</span></figcaption>
</figure>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Monocular 3D HPE approaches like Tome <em id="S2.p2.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> take the simplest of inputs (monocular RGB images) and predict 3D keypoints using a multi-stage method. This classical approach of “lifting” 3D keypoints from 2D images has been recently done using deep learning<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, and in the past using a database of 3D skeletons <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. Recent criticisms of this approach have focused on over-reliance on the underlying 2D estimator, and of generalization problems<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>. Extending this approach temporally <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> also has been attempted, but still underperforms approaches which use depth information (see <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> table 11).</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Depth based approaches also come in different flavors. Some, like Zimmermann <em id="S2.p3.1.1" class="ltx_emph ltx_font_italic">et al.</em>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> use a VoxelNet based method on RGB-D images with 3D labels.
Others might only use point clouds <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, add temporal consistency formulations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, use a split and recombine approach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>, or generate large amounts of synthetic data followed by supervised learning strategy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. Semi-supervised approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> have also been recently attempted to deal with the long tail and ”in-the-wild” scenarios.</p>
</div>
<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Weakly-supervised 3D Human Pose Estimation:</h5>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">Besides the above fully- and semi-supervised methods which rely on at least a certain amount of 3D annotations, there are also weakly-supervised methods that use pure 2D annotations. Tripathi <em id="S2.SS0.SSS0.Px1.p1.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> introduced a self-supervised method with teacher-student strategy on RGB sequences. Chen <em id="S2.SS0.SSS0.Px1.p1.1.2" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> introduced a weakly-supervised method with cycle GAN<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>-like structure on pure 2D labels. Other weakly-supervised methods include <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. All the above methods are RGB-based, and do not involve the use of point clouds, while our method utilizes point clouds to help to improve the prediction accuracy.</p>
</div>
<div id="S2.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p2.1" class="ltx_p">Fürst <em id="S2.SS0.SSS0.Px1.p2.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> proposed an end-to-end system for 3D detection and HPE for RGB and LiDAR in AV with pure 2D keypoint annotations. However, their work only includes evaluations for 2D HPE and projected 3D HPE, while our approach is evaluated with real 3D annotations.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Point Cloud-Based Approaches:</h5>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">Point cloud-based approaches differ from HPE on traditional depth sensors in their ability to handle sparse 3D data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.
PointNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> is a popular network for point cloud-based classification and segmentation, improved with hierarchical structures in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> and utilized for 3D object detection on RGB-D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, and hand pose estimation  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.
Finally, Zhang <em id="S2.SS0.SSS0.Px2.p1.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> proposed a weakly supervised point cloud-based method for 3D human pose estimation. However, their method requires 3D annotations and is only evaluated in indoor RGB-D datasets, while our method works on uncontrolled AV scenarios with pure 2D annotations.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Problem Formulation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.11" class="ltx_p">The 3D pose estimation problem can be described as follows. For each human subject in consideration, there are two modalities of data available: the point cloud and a camera image of the person. The point cloud <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="\mathbf{P}=\begin{bmatrix}\mathbf{p}_{1},\cdots,\mathbf{p}_{i},\cdots,\mathbf{p}_{N}\end{bmatrix}\in\mathbb{R}^{N\times d}" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.2" xref="S3.SS1.p1.1.m1.1.2.cmml"><mi id="S3.SS1.p1.1.m1.1.2.2" xref="S3.SS1.p1.1.m1.1.2.2.cmml">𝐏</mi><mo id="S3.SS1.p1.1.m1.1.2.3" xref="S3.SS1.p1.1.m1.1.2.3.cmml">=</mo><mrow id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.2.cmml"><mo id="S3.SS1.p1.1.m1.1.1.3.1" xref="S3.SS1.p1.1.m1.1.1.2.1.cmml">[</mo><mtable id="S3.SS1.p1.1.m1.1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.1.cmml"><mtr id="S3.SS1.p1.1.m1.1.1.1.1a" xref="S3.SS1.p1.1.m1.1.1.1.1.cmml"><mtd id="S3.SS1.p1.1.m1.1.1.1.1b" xref="S3.SS1.p1.1.m1.1.1.1.1.cmml"><mrow id="S3.SS1.p1.1.m1.1.1.1.1.5.5.5.5.5" xref="S3.SS1.p1.1.m1.1.1.1.1.5.5.5.5.6.cmml"><msub id="S3.SS1.p1.1.m1.1.1.1.1.3.3.3.3.3.1" xref="S3.SS1.p1.1.m1.1.1.1.1.3.3.3.3.3.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.1.1.3.3.3.3.3.1.2" xref="S3.SS1.p1.1.m1.1.1.1.1.3.3.3.3.3.1.2.cmml">𝐩</mi><mn id="S3.SS1.p1.1.m1.1.1.1.1.3.3.3.3.3.1.3" xref="S3.SS1.p1.1.m1.1.1.1.1.3.3.3.3.3.1.3.cmml">1</mn></msub><mo id="S3.SS1.p1.1.m1.1.1.1.1.5.5.5.5.5.4" xref="S3.SS1.p1.1.m1.1.1.1.1.5.5.5.5.6.cmml">,</mo><mi mathvariant="normal" id="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.1.1.cmml">⋯</mi><mo id="S3.SS1.p1.1.m1.1.1.1.1.5.5.5.5.5.5" xref="S3.SS1.p1.1.m1.1.1.1.1.5.5.5.5.6.cmml">,</mo><msub id="S3.SS1.p1.1.m1.1.1.1.1.4.4.4.4.4.2" xref="S3.SS1.p1.1.m1.1.1.1.1.4.4.4.4.4.2.cmml"><mi id="S3.SS1.p1.1.m1.1.1.1.1.4.4.4.4.4.2.2" xref="S3.SS1.p1.1.m1.1.1.1.1.4.4.4.4.4.2.2.cmml">𝐩</mi><mi id="S3.SS1.p1.1.m1.1.1.1.1.4.4.4.4.4.2.3" xref="S3.SS1.p1.1.m1.1.1.1.1.4.4.4.4.4.2.3.cmml">i</mi></msub><mo id="S3.SS1.p1.1.m1.1.1.1.1.5.5.5.5.5.6" xref="S3.SS1.p1.1.m1.1.1.1.1.5.5.5.5.6.cmml">,</mo><mi mathvariant="normal" id="S3.SS1.p1.1.m1.1.1.1.1.2.2.2.2.2" xref="S3.SS1.p1.1.m1.1.1.1.1.2.2.2.2.2.cmml">⋯</mi><mo id="S3.SS1.p1.1.m1.1.1.1.1.5.5.5.5.5.7" xref="S3.SS1.p1.1.m1.1.1.1.1.5.5.5.5.6.cmml">,</mo><msub id="S3.SS1.p1.1.m1.1.1.1.1.5.5.5.5.5.3" xref="S3.SS1.p1.1.m1.1.1.1.1.5.5.5.5.5.3.cmml"><mi id="S3.SS1.p1.1.m1.1.1.1.1.5.5.5.5.5.3.2" xref="S3.SS1.p1.1.m1.1.1.1.1.5.5.5.5.5.3.2.cmml">𝐩</mi><mi id="S3.SS1.p1.1.m1.1.1.1.1.5.5.5.5.5.3.3" xref="S3.SS1.p1.1.m1.1.1.1.1.5.5.5.5.5.3.3.cmml">N</mi></msub></mrow></mtd></mtr></mtable><mo id="S3.SS1.p1.1.m1.1.1.3.2" xref="S3.SS1.p1.1.m1.1.1.2.1.cmml">]</mo></mrow><mo id="S3.SS1.p1.1.m1.1.2.4" xref="S3.SS1.p1.1.m1.1.2.4.cmml">∈</mo><msup id="S3.SS1.p1.1.m1.1.2.5" xref="S3.SS1.p1.1.m1.1.2.5.cmml"><mi id="S3.SS1.p1.1.m1.1.2.5.2" xref="S3.SS1.p1.1.m1.1.2.5.2.cmml">ℝ</mi><mrow id="S3.SS1.p1.1.m1.1.2.5.3" xref="S3.SS1.p1.1.m1.1.2.5.3.cmml"><mi id="S3.SS1.p1.1.m1.1.2.5.3.2" xref="S3.SS1.p1.1.m1.1.2.5.3.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.1.m1.1.2.5.3.1" xref="S3.SS1.p1.1.m1.1.2.5.3.1.cmml">×</mo><mi id="S3.SS1.p1.1.m1.1.2.5.3.3" xref="S3.SS1.p1.1.m1.1.2.5.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.2"><and id="S3.SS1.p1.1.m1.1.2a.cmml" xref="S3.SS1.p1.1.m1.1.2"></and><apply id="S3.SS1.p1.1.m1.1.2b.cmml" xref="S3.SS1.p1.1.m1.1.2"><eq id="S3.SS1.p1.1.m1.1.2.3.cmml" xref="S3.SS1.p1.1.m1.1.2.3"></eq><ci id="S3.SS1.p1.1.m1.1.2.2.cmml" xref="S3.SS1.p1.1.m1.1.2.2">𝐏</ci><apply id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.3"><csymbol cd="latexml" id="S3.SS1.p1.1.m1.1.1.2.1.cmml" xref="S3.SS1.p1.1.m1.1.1.3.1">matrix</csymbol><matrix id="S3.SS1.p1.1.m1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1"><matrixrow id="S3.SS1.p1.1.m1.1.1.1.1a.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1"><list id="S3.SS1.p1.1.m1.1.1.1.1.5.5.5.5.6.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.5.5.5.5.5"><apply id="S3.SS1.p1.1.m1.1.1.1.1.3.3.3.3.3.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.3.3.3.3.3.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.1.3.3.3.3.3.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.3.3.3.3.3.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.1.1.3.3.3.3.3.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.3.3.3.3.3.1.2">𝐩</ci><cn type="integer" id="S3.SS1.p1.1.m1.1.1.1.1.3.3.3.3.3.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.3.3.3.3.3.1.3">1</cn></apply><ci id="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.1.1">⋯</ci><apply id="S3.SS1.p1.1.m1.1.1.1.1.4.4.4.4.4.2.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.4.4.4.4.4.2"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.1.4.4.4.4.4.2.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.4.4.4.4.4.2">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.1.1.4.4.4.4.4.2.2.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.4.4.4.4.4.2.2">𝐩</ci><ci id="S3.SS1.p1.1.m1.1.1.1.1.4.4.4.4.4.2.3.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.4.4.4.4.4.2.3">𝑖</ci></apply><ci id="S3.SS1.p1.1.m1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.2.2.2.2.2">⋯</ci><apply id="S3.SS1.p1.1.m1.1.1.1.1.5.5.5.5.5.3.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.5.5.5.5.5.3"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.1.5.5.5.5.5.3.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.5.5.5.5.5.3">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.1.1.5.5.5.5.5.3.2.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.5.5.5.5.5.3.2">𝐩</ci><ci id="S3.SS1.p1.1.m1.1.1.1.1.5.5.5.5.5.3.3.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.5.5.5.5.5.3.3">𝑁</ci></apply></list></matrixrow></matrix></apply></apply><apply id="S3.SS1.p1.1.m1.1.2c.cmml" xref="S3.SS1.p1.1.m1.1.2"><in id="S3.SS1.p1.1.m1.1.2.4.cmml" xref="S3.SS1.p1.1.m1.1.2.4"></in><share href="#S3.SS1.p1.1.m1.1.1.cmml" id="S3.SS1.p1.1.m1.1.2d.cmml" xref="S3.SS1.p1.1.m1.1.2"></share><apply id="S3.SS1.p1.1.m1.1.2.5.cmml" xref="S3.SS1.p1.1.m1.1.2.5"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.2.5.1.cmml" xref="S3.SS1.p1.1.m1.1.2.5">superscript</csymbol><ci id="S3.SS1.p1.1.m1.1.2.5.2.cmml" xref="S3.SS1.p1.1.m1.1.2.5.2">ℝ</ci><apply id="S3.SS1.p1.1.m1.1.2.5.3.cmml" xref="S3.SS1.p1.1.m1.1.2.5.3"><times id="S3.SS1.p1.1.m1.1.2.5.3.1.cmml" xref="S3.SS1.p1.1.m1.1.2.5.3.1"></times><ci id="S3.SS1.p1.1.m1.1.2.5.3.2.cmml" xref="S3.SS1.p1.1.m1.1.2.5.3.2">𝑁</ci><ci id="S3.SS1.p1.1.m1.1.2.5.3.3.cmml" xref="S3.SS1.p1.1.m1.1.2.5.3.3">𝑑</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\mathbf{P}=\begin{bmatrix}\mathbf{p}_{1},\cdots,\mathbf{p}_{i},\cdots,\mathbf{p}_{N}\end{bmatrix}\in\mathbb{R}^{N\times d}</annotation></semantics></math>, consists of <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">N</annotation></semantics></math> LiDAR points from a single scan with <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mi id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">d</annotation></semantics></math>-dimensional features. In this work, <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="d=3" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><mrow id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml"><mi id="S3.SS1.p1.4.m4.1.1.2" xref="S3.SS1.p1.4.m4.1.1.2.cmml">d</mi><mo id="S3.SS1.p1.4.m4.1.1.1" xref="S3.SS1.p1.4.m4.1.1.1.cmml">=</mo><mn id="S3.SS1.p1.4.m4.1.1.3" xref="S3.SS1.p1.4.m4.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><apply id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1"><eq id="S3.SS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1.1"></eq><ci id="S3.SS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2">𝑑</ci><cn type="integer" id="S3.SS1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">d=3</annotation></semantics></math>. The camera image is an <math id="S3.SS1.p1.5.m5.1" class="ltx_Math" alttext="H\times W\times 3" display="inline"><semantics id="S3.SS1.p1.5.m5.1a"><mrow id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml"><mi id="S3.SS1.p1.5.m5.1.1.2" xref="S3.SS1.p1.5.m5.1.1.2.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.5.m5.1.1.1" xref="S3.SS1.p1.5.m5.1.1.1.cmml">×</mo><mi id="S3.SS1.p1.5.m5.1.1.3" xref="S3.SS1.p1.5.m5.1.1.3.cmml">W</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.5.m5.1.1.1a" xref="S3.SS1.p1.5.m5.1.1.1.cmml">×</mo><mn id="S3.SS1.p1.5.m5.1.1.4" xref="S3.SS1.p1.5.m5.1.1.4.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><apply id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1"><times id="S3.SS1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1.1"></times><ci id="S3.SS1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1.2">𝐻</ci><ci id="S3.SS1.p1.5.m5.1.1.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3">𝑊</ci><cn type="integer" id="S3.SS1.p1.5.m5.1.1.4.cmml" xref="S3.SS1.p1.5.m5.1.1.4">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">H\times W\times 3</annotation></semantics></math> RGB image. Assuming we have the extrinsics and intrinsics of the LiDAR and camera, for each point <math id="S3.SS1.p1.6.m6.1" class="ltx_Math" alttext="\mathbf{p}_{i}" display="inline"><semantics id="S3.SS1.p1.6.m6.1a"><msub id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml"><mi id="S3.SS1.p1.6.m6.1.1.2" xref="S3.SS1.p1.6.m6.1.1.2.cmml">𝐩</mi><mi id="S3.SS1.p1.6.m6.1.1.3" xref="S3.SS1.p1.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><apply id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m6.1.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.p1.6.m6.1.1.2.cmml" xref="S3.SS1.p1.6.m6.1.1.2">𝐩</ci><ci id="S3.SS1.p1.6.m6.1.1.3.cmml" xref="S3.SS1.p1.6.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">\mathbf{p}_{i}</annotation></semantics></math>, its 3D world coordinates <math id="S3.SS1.p1.7.m7.1" class="ltx_Math" alttext="\mathbf{x}_{i}^{(3)}" display="inline"><semantics id="S3.SS1.p1.7.m7.1a"><msubsup id="S3.SS1.p1.7.m7.1.2" xref="S3.SS1.p1.7.m7.1.2.cmml"><mi id="S3.SS1.p1.7.m7.1.2.2.2" xref="S3.SS1.p1.7.m7.1.2.2.2.cmml">𝐱</mi><mi id="S3.SS1.p1.7.m7.1.2.2.3" xref="S3.SS1.p1.7.m7.1.2.2.3.cmml">i</mi><mrow id="S3.SS1.p1.7.m7.1.1.1.3" xref="S3.SS1.p1.7.m7.1.2.cmml"><mo stretchy="false" id="S3.SS1.p1.7.m7.1.1.1.3.1" xref="S3.SS1.p1.7.m7.1.2.cmml">(</mo><mn id="S3.SS1.p1.7.m7.1.1.1.1" xref="S3.SS1.p1.7.m7.1.1.1.1.cmml">3</mn><mo stretchy="false" id="S3.SS1.p1.7.m7.1.1.1.3.2" xref="S3.SS1.p1.7.m7.1.2.cmml">)</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.1b"><apply id="S3.SS1.p1.7.m7.1.2.cmml" xref="S3.SS1.p1.7.m7.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m7.1.2.1.cmml" xref="S3.SS1.p1.7.m7.1.2">superscript</csymbol><apply id="S3.SS1.p1.7.m7.1.2.2.cmml" xref="S3.SS1.p1.7.m7.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m7.1.2.2.1.cmml" xref="S3.SS1.p1.7.m7.1.2">subscript</csymbol><ci id="S3.SS1.p1.7.m7.1.2.2.2.cmml" xref="S3.SS1.p1.7.m7.1.2.2.2">𝐱</ci><ci id="S3.SS1.p1.7.m7.1.2.2.3.cmml" xref="S3.SS1.p1.7.m7.1.2.2.3">𝑖</ci></apply><cn type="integer" id="S3.SS1.p1.7.m7.1.1.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.1c">\mathbf{x}_{i}^{(3)}</annotation></semantics></math> in the point cloud coordinate system and 2D coordinates <math id="S3.SS1.p1.8.m8.1" class="ltx_Math" alttext="\mathbf{x}_{i}^{(2)}" display="inline"><semantics id="S3.SS1.p1.8.m8.1a"><msubsup id="S3.SS1.p1.8.m8.1.2" xref="S3.SS1.p1.8.m8.1.2.cmml"><mi id="S3.SS1.p1.8.m8.1.2.2.2" xref="S3.SS1.p1.8.m8.1.2.2.2.cmml">𝐱</mi><mi id="S3.SS1.p1.8.m8.1.2.2.3" xref="S3.SS1.p1.8.m8.1.2.2.3.cmml">i</mi><mrow id="S3.SS1.p1.8.m8.1.1.1.3" xref="S3.SS1.p1.8.m8.1.2.cmml"><mo stretchy="false" id="S3.SS1.p1.8.m8.1.1.1.3.1" xref="S3.SS1.p1.8.m8.1.2.cmml">(</mo><mn id="S3.SS1.p1.8.m8.1.1.1.1" xref="S3.SS1.p1.8.m8.1.1.1.1.cmml">2</mn><mo stretchy="false" id="S3.SS1.p1.8.m8.1.1.1.3.2" xref="S3.SS1.p1.8.m8.1.2.cmml">)</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m8.1b"><apply id="S3.SS1.p1.8.m8.1.2.cmml" xref="S3.SS1.p1.8.m8.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.8.m8.1.2.1.cmml" xref="S3.SS1.p1.8.m8.1.2">superscript</csymbol><apply id="S3.SS1.p1.8.m8.1.2.2.cmml" xref="S3.SS1.p1.8.m8.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.8.m8.1.2.2.1.cmml" xref="S3.SS1.p1.8.m8.1.2">subscript</csymbol><ci id="S3.SS1.p1.8.m8.1.2.2.2.cmml" xref="S3.SS1.p1.8.m8.1.2.2.2">𝐱</ci><ci id="S3.SS1.p1.8.m8.1.2.2.3.cmml" xref="S3.SS1.p1.8.m8.1.2.2.3">𝑖</ci></apply><cn type="integer" id="S3.SS1.p1.8.m8.1.1.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m8.1c">\mathbf{x}_{i}^{(2)}</annotation></semantics></math> in the image coordinate system are known. Given these inputs, the goal is to predict 3D coordinates of <math id="S3.SS1.p1.9.m9.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS1.p1.9.m9.1a"><mi id="S3.SS1.p1.9.m9.1.1" xref="S3.SS1.p1.9.m9.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.9.m9.1b"><ci id="S3.SS1.p1.9.m9.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.9.m9.1c">K</annotation></semantics></math> pose keypoints <math id="S3.SS1.p1.10.m10.2" class="ltx_Math" alttext="\{\mathbf{y}_{k}^{(3)}\}_{k=1}^{K}\in\mathbb{R}^{K\times 3}" display="inline"><semantics id="S3.SS1.p1.10.m10.2a"><mrow id="S3.SS1.p1.10.m10.2.2" xref="S3.SS1.p1.10.m10.2.2.cmml"><msubsup id="S3.SS1.p1.10.m10.2.2.1" xref="S3.SS1.p1.10.m10.2.2.1.cmml"><mrow id="S3.SS1.p1.10.m10.2.2.1.1.1.1" xref="S3.SS1.p1.10.m10.2.2.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS1.p1.10.m10.2.2.1.1.1.1.2" xref="S3.SS1.p1.10.m10.2.2.1.1.1.2.cmml">{</mo><msubsup id="S3.SS1.p1.10.m10.2.2.1.1.1.1.1" xref="S3.SS1.p1.10.m10.2.2.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.10.m10.2.2.1.1.1.1.1.2.2" xref="S3.SS1.p1.10.m10.2.2.1.1.1.1.1.2.2.cmml">𝐲</mi><mi id="S3.SS1.p1.10.m10.2.2.1.1.1.1.1.2.3" xref="S3.SS1.p1.10.m10.2.2.1.1.1.1.1.2.3.cmml">k</mi><mrow id="S3.SS1.p1.10.m10.1.1.1.3" xref="S3.SS1.p1.10.m10.2.2.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p1.10.m10.1.1.1.3.1" xref="S3.SS1.p1.10.m10.2.2.1.1.1.1.1.cmml">(</mo><mn id="S3.SS1.p1.10.m10.1.1.1.1" xref="S3.SS1.p1.10.m10.1.1.1.1.cmml">3</mn><mo stretchy="false" id="S3.SS1.p1.10.m10.1.1.1.3.2" xref="S3.SS1.p1.10.m10.2.2.1.1.1.1.1.cmml">)</mo></mrow></msubsup><mo stretchy="false" id="S3.SS1.p1.10.m10.2.2.1.1.1.1.3" xref="S3.SS1.p1.10.m10.2.2.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS1.p1.10.m10.2.2.1.1.3" xref="S3.SS1.p1.10.m10.2.2.1.1.3.cmml"><mi id="S3.SS1.p1.10.m10.2.2.1.1.3.2" xref="S3.SS1.p1.10.m10.2.2.1.1.3.2.cmml">k</mi><mo id="S3.SS1.p1.10.m10.2.2.1.1.3.1" xref="S3.SS1.p1.10.m10.2.2.1.1.3.1.cmml">=</mo><mn id="S3.SS1.p1.10.m10.2.2.1.1.3.3" xref="S3.SS1.p1.10.m10.2.2.1.1.3.3.cmml">1</mn></mrow><mi id="S3.SS1.p1.10.m10.2.2.1.3" xref="S3.SS1.p1.10.m10.2.2.1.3.cmml">K</mi></msubsup><mo id="S3.SS1.p1.10.m10.2.2.2" xref="S3.SS1.p1.10.m10.2.2.2.cmml">∈</mo><msup id="S3.SS1.p1.10.m10.2.2.3" xref="S3.SS1.p1.10.m10.2.2.3.cmml"><mi id="S3.SS1.p1.10.m10.2.2.3.2" xref="S3.SS1.p1.10.m10.2.2.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p1.10.m10.2.2.3.3" xref="S3.SS1.p1.10.m10.2.2.3.3.cmml"><mi id="S3.SS1.p1.10.m10.2.2.3.3.2" xref="S3.SS1.p1.10.m10.2.2.3.3.2.cmml">K</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.10.m10.2.2.3.3.1" xref="S3.SS1.p1.10.m10.2.2.3.3.1.cmml">×</mo><mn id="S3.SS1.p1.10.m10.2.2.3.3.3" xref="S3.SS1.p1.10.m10.2.2.3.3.3.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.10.m10.2b"><apply id="S3.SS1.p1.10.m10.2.2.cmml" xref="S3.SS1.p1.10.m10.2.2"><in id="S3.SS1.p1.10.m10.2.2.2.cmml" xref="S3.SS1.p1.10.m10.2.2.2"></in><apply id="S3.SS1.p1.10.m10.2.2.1.cmml" xref="S3.SS1.p1.10.m10.2.2.1"><csymbol cd="ambiguous" id="S3.SS1.p1.10.m10.2.2.1.2.cmml" xref="S3.SS1.p1.10.m10.2.2.1">superscript</csymbol><apply id="S3.SS1.p1.10.m10.2.2.1.1.cmml" xref="S3.SS1.p1.10.m10.2.2.1"><csymbol cd="ambiguous" id="S3.SS1.p1.10.m10.2.2.1.1.2.cmml" xref="S3.SS1.p1.10.m10.2.2.1">subscript</csymbol><set id="S3.SS1.p1.10.m10.2.2.1.1.1.2.cmml" xref="S3.SS1.p1.10.m10.2.2.1.1.1.1"><apply id="S3.SS1.p1.10.m10.2.2.1.1.1.1.1.cmml" xref="S3.SS1.p1.10.m10.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.10.m10.2.2.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.10.m10.2.2.1.1.1.1.1">superscript</csymbol><apply id="S3.SS1.p1.10.m10.2.2.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.10.m10.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.10.m10.2.2.1.1.1.1.1.2.1.cmml" xref="S3.SS1.p1.10.m10.2.2.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.10.m10.2.2.1.1.1.1.1.2.2.cmml" xref="S3.SS1.p1.10.m10.2.2.1.1.1.1.1.2.2">𝐲</ci><ci id="S3.SS1.p1.10.m10.2.2.1.1.1.1.1.2.3.cmml" xref="S3.SS1.p1.10.m10.2.2.1.1.1.1.1.2.3">𝑘</ci></apply><cn type="integer" id="S3.SS1.p1.10.m10.1.1.1.1.cmml" xref="S3.SS1.p1.10.m10.1.1.1.1">3</cn></apply></set><apply id="S3.SS1.p1.10.m10.2.2.1.1.3.cmml" xref="S3.SS1.p1.10.m10.2.2.1.1.3"><eq id="S3.SS1.p1.10.m10.2.2.1.1.3.1.cmml" xref="S3.SS1.p1.10.m10.2.2.1.1.3.1"></eq><ci id="S3.SS1.p1.10.m10.2.2.1.1.3.2.cmml" xref="S3.SS1.p1.10.m10.2.2.1.1.3.2">𝑘</ci><cn type="integer" id="S3.SS1.p1.10.m10.2.2.1.1.3.3.cmml" xref="S3.SS1.p1.10.m10.2.2.1.1.3.3">1</cn></apply></apply><ci id="S3.SS1.p1.10.m10.2.2.1.3.cmml" xref="S3.SS1.p1.10.m10.2.2.1.3">𝐾</ci></apply><apply id="S3.SS1.p1.10.m10.2.2.3.cmml" xref="S3.SS1.p1.10.m10.2.2.3"><csymbol cd="ambiguous" id="S3.SS1.p1.10.m10.2.2.3.1.cmml" xref="S3.SS1.p1.10.m10.2.2.3">superscript</csymbol><ci id="S3.SS1.p1.10.m10.2.2.3.2.cmml" xref="S3.SS1.p1.10.m10.2.2.3.2">ℝ</ci><apply id="S3.SS1.p1.10.m10.2.2.3.3.cmml" xref="S3.SS1.p1.10.m10.2.2.3.3"><times id="S3.SS1.p1.10.m10.2.2.3.3.1.cmml" xref="S3.SS1.p1.10.m10.2.2.3.3.1"></times><ci id="S3.SS1.p1.10.m10.2.2.3.3.2.cmml" xref="S3.SS1.p1.10.m10.2.2.3.3.2">𝐾</ci><cn type="integer" id="S3.SS1.p1.10.m10.2.2.3.3.3.cmml" xref="S3.SS1.p1.10.m10.2.2.3.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.10.m10.2c">\{\mathbf{y}_{k}^{(3)}\}_{k=1}^{K}\in\mathbb{R}^{K\times 3}</annotation></semantics></math> of the corresponding person. Note that LiDAR point clouds are usually sparse and lie on the surface of the object, while ground truth keypoints are defined inside the human body. Therefore, we cannot choose a subset of <math id="S3.SS1.p1.11.m11.1" class="ltx_Math" alttext="\mathbf{P}" display="inline"><semantics id="S3.SS1.p1.11.m11.1a"><mi id="S3.SS1.p1.11.m11.1.1" xref="S3.SS1.p1.11.m11.1.1.cmml">𝐏</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.11.m11.1b"><ci id="S3.SS1.p1.11.m11.1.1.cmml" xref="S3.SS1.p1.11.m11.1.1">𝐏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.11.m11.1c">\mathbf{P}</annotation></semantics></math> as the 3D pose of the person and approach 3D HPE in AV as a classification problem.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">An overview of the proposed approach is shown in Figure <a href="#S2.F2" title="Figure 2 ‣ 2 Related Work ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Our model is a cascade of a camera network and a point network. The camera network takes a 2D image as input and predicts a 2D keypoints heatmap<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. This heatmap is used to augment the point cloud using modality fusion and fed into the point network. Finally, the regression branch of the point network predicts the 3D coordinates of <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">K</annotation></semantics></math> keypoints. An auxiliary segmentation branch generates pointwise predictions which are only used for training. The model is trained on pseudo 3D labels and pointwise labels generated from 2D labels.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Modality Fusion of LiDAR and Camera</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.8" class="ltx_p">We introduce a 2D camera network with modality fusion to transfer texture information from RGB images to point clouds. Our camera network follows the architecture proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> which consists of a downscale module and an upscale module. The downscale module is a ResNet-50 network and the upscale module consists of three deconvolutional layers. A <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="1\times 1" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mn id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p1.1.m1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><times id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">1</cn><cn type="integer" id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">1\times 1</annotation></semantics></math> convolutional layer with sigmoid activation follows the upscale module and
produces the output heatmap. The network takes an RGB image with size <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="H\times W\times 3" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mrow id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p1.2.m2.1.1.1" xref="S3.SS2.p1.2.m2.1.1.1.cmml">×</mo><mi id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml">W</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p1.2.m2.1.1.1a" xref="S3.SS2.p1.2.m2.1.1.1.cmml">×</mo><mn id="S3.SS2.p1.2.m2.1.1.4" xref="S3.SS2.p1.2.m2.1.1.4.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><times id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1"></times><ci id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">𝐻</ci><ci id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3">𝑊</ci><cn type="integer" id="S3.SS2.p1.2.m2.1.1.4.cmml" xref="S3.SS2.p1.2.m2.1.1.4">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">H\times W\times 3</annotation></semantics></math> as input and generates a keypoint heatmap <math id="S3.SS2.p1.3.m3.7" class="ltx_Math" alttext="\mathbf{H}=\{\mathbf{h}_{m,n}\}_{m=1,n=1}^{H^{\prime},W^{\prime}}" display="inline"><semantics id="S3.SS2.p1.3.m3.7a"><mrow id="S3.SS2.p1.3.m3.7.7" xref="S3.SS2.p1.3.m3.7.7.cmml"><mi id="S3.SS2.p1.3.m3.7.7.3" xref="S3.SS2.p1.3.m3.7.7.3.cmml">𝐇</mi><mo id="S3.SS2.p1.3.m3.7.7.2" xref="S3.SS2.p1.3.m3.7.7.2.cmml">=</mo><msubsup id="S3.SS2.p1.3.m3.7.7.1" xref="S3.SS2.p1.3.m3.7.7.1.cmml"><mrow id="S3.SS2.p1.3.m3.7.7.1.1.1.1" xref="S3.SS2.p1.3.m3.7.7.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS2.p1.3.m3.7.7.1.1.1.1.2" xref="S3.SS2.p1.3.m3.7.7.1.1.1.2.cmml">{</mo><msub id="S3.SS2.p1.3.m3.7.7.1.1.1.1.1" xref="S3.SS2.p1.3.m3.7.7.1.1.1.1.1.cmml"><mi id="S3.SS2.p1.3.m3.7.7.1.1.1.1.1.2" xref="S3.SS2.p1.3.m3.7.7.1.1.1.1.1.2.cmml">𝐡</mi><mrow id="S3.SS2.p1.3.m3.2.2.2.4" xref="S3.SS2.p1.3.m3.2.2.2.3.cmml"><mi id="S3.SS2.p1.3.m3.1.1.1.1" xref="S3.SS2.p1.3.m3.1.1.1.1.cmml">m</mi><mo id="S3.SS2.p1.3.m3.2.2.2.4.1" xref="S3.SS2.p1.3.m3.2.2.2.3.cmml">,</mo><mi id="S3.SS2.p1.3.m3.2.2.2.2" xref="S3.SS2.p1.3.m3.2.2.2.2.cmml">n</mi></mrow></msub><mo stretchy="false" id="S3.SS2.p1.3.m3.7.7.1.1.1.1.3" xref="S3.SS2.p1.3.m3.7.7.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS2.p1.3.m3.4.4.2.2" xref="S3.SS2.p1.3.m3.4.4.2.3.cmml"><mrow id="S3.SS2.p1.3.m3.3.3.1.1.1" xref="S3.SS2.p1.3.m3.3.3.1.1.1.cmml"><mi id="S3.SS2.p1.3.m3.3.3.1.1.1.2" xref="S3.SS2.p1.3.m3.3.3.1.1.1.2.cmml">m</mi><mo id="S3.SS2.p1.3.m3.3.3.1.1.1.1" xref="S3.SS2.p1.3.m3.3.3.1.1.1.1.cmml">=</mo><mn id="S3.SS2.p1.3.m3.3.3.1.1.1.3" xref="S3.SS2.p1.3.m3.3.3.1.1.1.3.cmml">1</mn></mrow><mo id="S3.SS2.p1.3.m3.4.4.2.2.3" xref="S3.SS2.p1.3.m3.4.4.2.3a.cmml">,</mo><mrow id="S3.SS2.p1.3.m3.4.4.2.2.2" xref="S3.SS2.p1.3.m3.4.4.2.2.2.cmml"><mi id="S3.SS2.p1.3.m3.4.4.2.2.2.2" xref="S3.SS2.p1.3.m3.4.4.2.2.2.2.cmml">n</mi><mo id="S3.SS2.p1.3.m3.4.4.2.2.2.1" xref="S3.SS2.p1.3.m3.4.4.2.2.2.1.cmml">=</mo><mn id="S3.SS2.p1.3.m3.4.4.2.2.2.3" xref="S3.SS2.p1.3.m3.4.4.2.2.2.3.cmml">1</mn></mrow></mrow><mrow id="S3.SS2.p1.3.m3.6.6.2.2" xref="S3.SS2.p1.3.m3.6.6.2.3.cmml"><msup id="S3.SS2.p1.3.m3.5.5.1.1.1" xref="S3.SS2.p1.3.m3.5.5.1.1.1.cmml"><mi id="S3.SS2.p1.3.m3.5.5.1.1.1.2" xref="S3.SS2.p1.3.m3.5.5.1.1.1.2.cmml">H</mi><mo id="S3.SS2.p1.3.m3.5.5.1.1.1.3" xref="S3.SS2.p1.3.m3.5.5.1.1.1.3.cmml">′</mo></msup><mo id="S3.SS2.p1.3.m3.6.6.2.2.3" xref="S3.SS2.p1.3.m3.6.6.2.3.cmml">,</mo><msup id="S3.SS2.p1.3.m3.6.6.2.2.2" xref="S3.SS2.p1.3.m3.6.6.2.2.2.cmml"><mi id="S3.SS2.p1.3.m3.6.6.2.2.2.2" xref="S3.SS2.p1.3.m3.6.6.2.2.2.2.cmml">W</mi><mo id="S3.SS2.p1.3.m3.6.6.2.2.2.3" xref="S3.SS2.p1.3.m3.6.6.2.2.2.3.cmml">′</mo></msup></mrow></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.7b"><apply id="S3.SS2.p1.3.m3.7.7.cmml" xref="S3.SS2.p1.3.m3.7.7"><eq id="S3.SS2.p1.3.m3.7.7.2.cmml" xref="S3.SS2.p1.3.m3.7.7.2"></eq><ci id="S3.SS2.p1.3.m3.7.7.3.cmml" xref="S3.SS2.p1.3.m3.7.7.3">𝐇</ci><apply id="S3.SS2.p1.3.m3.7.7.1.cmml" xref="S3.SS2.p1.3.m3.7.7.1"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.7.7.1.2.cmml" xref="S3.SS2.p1.3.m3.7.7.1">superscript</csymbol><apply id="S3.SS2.p1.3.m3.7.7.1.1.cmml" xref="S3.SS2.p1.3.m3.7.7.1"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.7.7.1.1.2.cmml" xref="S3.SS2.p1.3.m3.7.7.1">subscript</csymbol><set id="S3.SS2.p1.3.m3.7.7.1.1.1.2.cmml" xref="S3.SS2.p1.3.m3.7.7.1.1.1.1"><apply id="S3.SS2.p1.3.m3.7.7.1.1.1.1.1.cmml" xref="S3.SS2.p1.3.m3.7.7.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.7.7.1.1.1.1.1.1.cmml" xref="S3.SS2.p1.3.m3.7.7.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p1.3.m3.7.7.1.1.1.1.1.2.cmml" xref="S3.SS2.p1.3.m3.7.7.1.1.1.1.1.2">𝐡</ci><list id="S3.SS2.p1.3.m3.2.2.2.3.cmml" xref="S3.SS2.p1.3.m3.2.2.2.4"><ci id="S3.SS2.p1.3.m3.1.1.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1.1.1">𝑚</ci><ci id="S3.SS2.p1.3.m3.2.2.2.2.cmml" xref="S3.SS2.p1.3.m3.2.2.2.2">𝑛</ci></list></apply></set><apply id="S3.SS2.p1.3.m3.4.4.2.3.cmml" xref="S3.SS2.p1.3.m3.4.4.2.2"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.4.4.2.3a.cmml" xref="S3.SS2.p1.3.m3.4.4.2.2.3">formulae-sequence</csymbol><apply id="S3.SS2.p1.3.m3.3.3.1.1.1.cmml" xref="S3.SS2.p1.3.m3.3.3.1.1.1"><eq id="S3.SS2.p1.3.m3.3.3.1.1.1.1.cmml" xref="S3.SS2.p1.3.m3.3.3.1.1.1.1"></eq><ci id="S3.SS2.p1.3.m3.3.3.1.1.1.2.cmml" xref="S3.SS2.p1.3.m3.3.3.1.1.1.2">𝑚</ci><cn type="integer" id="S3.SS2.p1.3.m3.3.3.1.1.1.3.cmml" xref="S3.SS2.p1.3.m3.3.3.1.1.1.3">1</cn></apply><apply id="S3.SS2.p1.3.m3.4.4.2.2.2.cmml" xref="S3.SS2.p1.3.m3.4.4.2.2.2"><eq id="S3.SS2.p1.3.m3.4.4.2.2.2.1.cmml" xref="S3.SS2.p1.3.m3.4.4.2.2.2.1"></eq><ci id="S3.SS2.p1.3.m3.4.4.2.2.2.2.cmml" xref="S3.SS2.p1.3.m3.4.4.2.2.2.2">𝑛</ci><cn type="integer" id="S3.SS2.p1.3.m3.4.4.2.2.2.3.cmml" xref="S3.SS2.p1.3.m3.4.4.2.2.2.3">1</cn></apply></apply></apply><list id="S3.SS2.p1.3.m3.6.6.2.3.cmml" xref="S3.SS2.p1.3.m3.6.6.2.2"><apply id="S3.SS2.p1.3.m3.5.5.1.1.1.cmml" xref="S3.SS2.p1.3.m3.5.5.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.5.5.1.1.1.1.cmml" xref="S3.SS2.p1.3.m3.5.5.1.1.1">superscript</csymbol><ci id="S3.SS2.p1.3.m3.5.5.1.1.1.2.cmml" xref="S3.SS2.p1.3.m3.5.5.1.1.1.2">𝐻</ci><ci id="S3.SS2.p1.3.m3.5.5.1.1.1.3.cmml" xref="S3.SS2.p1.3.m3.5.5.1.1.1.3">′</ci></apply><apply id="S3.SS2.p1.3.m3.6.6.2.2.2.cmml" xref="S3.SS2.p1.3.m3.6.6.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.6.6.2.2.2.1.cmml" xref="S3.SS2.p1.3.m3.6.6.2.2.2">superscript</csymbol><ci id="S3.SS2.p1.3.m3.6.6.2.2.2.2.cmml" xref="S3.SS2.p1.3.m3.6.6.2.2.2.2">𝑊</ci><ci id="S3.SS2.p1.3.m3.6.6.2.2.2.3.cmml" xref="S3.SS2.p1.3.m3.6.6.2.2.2.3">′</ci></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.7c">\mathbf{H}=\{\mathbf{h}_{m,n}\}_{m=1,n=1}^{H^{\prime},W^{\prime}}</annotation></semantics></math> with size <math id="S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="H^{\prime}\times W^{\prime}\times K" display="inline"><semantics id="S3.SS2.p1.4.m4.1a"><mrow id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml"><msup id="S3.SS2.p1.4.m4.1.1.2" xref="S3.SS2.p1.4.m4.1.1.2.cmml"><mi id="S3.SS2.p1.4.m4.1.1.2.2" xref="S3.SS2.p1.4.m4.1.1.2.2.cmml">H</mi><mo id="S3.SS2.p1.4.m4.1.1.2.3" xref="S3.SS2.p1.4.m4.1.1.2.3.cmml">′</mo></msup><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p1.4.m4.1.1.1" xref="S3.SS2.p1.4.m4.1.1.1.cmml">×</mo><msup id="S3.SS2.p1.4.m4.1.1.3" xref="S3.SS2.p1.4.m4.1.1.3.cmml"><mi id="S3.SS2.p1.4.m4.1.1.3.2" xref="S3.SS2.p1.4.m4.1.1.3.2.cmml">W</mi><mo id="S3.SS2.p1.4.m4.1.1.3.3" xref="S3.SS2.p1.4.m4.1.1.3.3.cmml">′</mo></msup><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p1.4.m4.1.1.1a" xref="S3.SS2.p1.4.m4.1.1.1.cmml">×</mo><mi id="S3.SS2.p1.4.m4.1.1.4" xref="S3.SS2.p1.4.m4.1.1.4.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><apply id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1"><times id="S3.SS2.p1.4.m4.1.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1.1"></times><apply id="S3.SS2.p1.4.m4.1.1.2.cmml" xref="S3.SS2.p1.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.4.m4.1.1.2.1.cmml" xref="S3.SS2.p1.4.m4.1.1.2">superscript</csymbol><ci id="S3.SS2.p1.4.m4.1.1.2.2.cmml" xref="S3.SS2.p1.4.m4.1.1.2.2">𝐻</ci><ci id="S3.SS2.p1.4.m4.1.1.2.3.cmml" xref="S3.SS2.p1.4.m4.1.1.2.3">′</ci></apply><apply id="S3.SS2.p1.4.m4.1.1.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.4.m4.1.1.3.1.cmml" xref="S3.SS2.p1.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS2.p1.4.m4.1.1.3.2.cmml" xref="S3.SS2.p1.4.m4.1.1.3.2">𝑊</ci><ci id="S3.SS2.p1.4.m4.1.1.3.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3.3">′</ci></apply><ci id="S3.SS2.p1.4.m4.1.1.4.cmml" xref="S3.SS2.p1.4.m4.1.1.4">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">H^{\prime}\times W^{\prime}\times K</annotation></semantics></math>, where <math id="S3.SS2.p1.5.m5.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS2.p1.5.m5.1a"><mi id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><ci id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">K</annotation></semantics></math> is the number of keypoints. Each pixel <math id="S3.SS2.p1.6.m6.2" class="ltx_Math" alttext="{h}_{m,n}" display="inline"><semantics id="S3.SS2.p1.6.m6.2a"><msub id="S3.SS2.p1.6.m6.2.3" xref="S3.SS2.p1.6.m6.2.3.cmml"><mi id="S3.SS2.p1.6.m6.2.3.2" xref="S3.SS2.p1.6.m6.2.3.2.cmml">h</mi><mrow id="S3.SS2.p1.6.m6.2.2.2.4" xref="S3.SS2.p1.6.m6.2.2.2.3.cmml"><mi id="S3.SS2.p1.6.m6.1.1.1.1" xref="S3.SS2.p1.6.m6.1.1.1.1.cmml">m</mi><mo id="S3.SS2.p1.6.m6.2.2.2.4.1" xref="S3.SS2.p1.6.m6.2.2.2.3.cmml">,</mo><mi id="S3.SS2.p1.6.m6.2.2.2.2" xref="S3.SS2.p1.6.m6.2.2.2.2.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m6.2b"><apply id="S3.SS2.p1.6.m6.2.3.cmml" xref="S3.SS2.p1.6.m6.2.3"><csymbol cd="ambiguous" id="S3.SS2.p1.6.m6.2.3.1.cmml" xref="S3.SS2.p1.6.m6.2.3">subscript</csymbol><ci id="S3.SS2.p1.6.m6.2.3.2.cmml" xref="S3.SS2.p1.6.m6.2.3.2">ℎ</ci><list id="S3.SS2.p1.6.m6.2.2.2.3.cmml" xref="S3.SS2.p1.6.m6.2.2.2.4"><ci id="S3.SS2.p1.6.m6.1.1.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1.1.1">𝑚</ci><ci id="S3.SS2.p1.6.m6.2.2.2.2.cmml" xref="S3.SS2.p1.6.m6.2.2.2.2">𝑛</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m6.2c">{h}_{m,n}</annotation></semantics></math> in the heatmap is a <math id="S3.SS2.p1.7.m7.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS2.p1.7.m7.1a"><mi id="S3.SS2.p1.7.m7.1.1" xref="S3.SS2.p1.7.m7.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.7.m7.1b"><ci id="S3.SS2.p1.7.m7.1.1.cmml" xref="S3.SS2.p1.7.m7.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.7.m7.1c">K</annotation></semantics></math> dimensional vector, indicating the likelihood of the corresponding image pixel belonging to each of the <math id="S3.SS2.p1.8.m8.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS2.p1.8.m8.1a"><mi id="S3.SS2.p1.8.m8.1.1" xref="S3.SS2.p1.8.m8.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.8.m8.1b"><ci id="S3.SS2.p1.8.m8.1.1.cmml" xref="S3.SS2.p1.8.m8.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.8.m8.1c">K</annotation></semantics></math> keypoints.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2112.12141/assets/modality_fusion_new.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="359" height="311" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.3.2" class="ltx_text" style="font-size:90%;">Modality fusion: the 2D heatmap from the camera network is first smoothed by Gaussian kernel, then sampled by 2D point cloud projections on the camera image. The sampled heatmap slices are considered as camera features and are concatenated with point coordinates of the point cloud as augmented input to the point network. See Sec <a href="#S3.SS2" title="3.2 Modality Fusion of LiDAR and Camera ‣ 3 Method ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a> for details.</span></figcaption>
</figure>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.10" class="ltx_p">The heatmap <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="\mathbf{H}" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">𝐇</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">𝐇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">\mathbf{H}</annotation></semantics></math> is consequently sampled at points corresponding to the 2D projections on the camera image of 3D LiDAR points, to generate camera features <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="\mathbf{p}_{i}^{\text{cam}}" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><msubsup id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2.2" xref="S3.SS2.p2.2.m2.1.1.2.2.cmml">𝐩</mi><mi id="S3.SS2.p2.2.m2.1.1.2.3" xref="S3.SS2.p2.2.m2.1.1.2.3.cmml">i</mi><mtext id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3a.cmml">cam</mtext></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">superscript</csymbol><apply id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.2.1.cmml" xref="S3.SS2.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p2.2.m2.1.1.2.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2.2">𝐩</ci><ci id="S3.SS2.p2.2.m2.1.1.2.3.cmml" xref="S3.SS2.p2.2.m2.1.1.2.3">𝑖</ci></apply><ci id="S3.SS2.p2.2.m2.1.1.3a.cmml" xref="S3.SS2.p2.2.m2.1.1.3"><mtext mathsize="70%" id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3">cam</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">\mathbf{p}_{i}^{\text{cam}}</annotation></semantics></math> as shown in Figure <a href="#S3.F3" title="Figure 3 ‣ 3.2 Modality Fusion of LiDAR and Camera ‣ 3 Method ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. The camera feature for point <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><mi id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><ci id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">i</annotation></semantics></math> is computed as <math id="S3.SS2.p2.4.m4.4" class="ltx_Math" alttext="\mathbf{p}_{i}^{\text{cam}}=\mathbf{h}_{m(i),n(i)}" display="inline"><semantics id="S3.SS2.p2.4.m4.4a"><mrow id="S3.SS2.p2.4.m4.4.5" xref="S3.SS2.p2.4.m4.4.5.cmml"><msubsup id="S3.SS2.p2.4.m4.4.5.2" xref="S3.SS2.p2.4.m4.4.5.2.cmml"><mi id="S3.SS2.p2.4.m4.4.5.2.2.2" xref="S3.SS2.p2.4.m4.4.5.2.2.2.cmml">𝐩</mi><mi id="S3.SS2.p2.4.m4.4.5.2.2.3" xref="S3.SS2.p2.4.m4.4.5.2.2.3.cmml">i</mi><mtext id="S3.SS2.p2.4.m4.4.5.2.3" xref="S3.SS2.p2.4.m4.4.5.2.3a.cmml">cam</mtext></msubsup><mo id="S3.SS2.p2.4.m4.4.5.1" xref="S3.SS2.p2.4.m4.4.5.1.cmml">=</mo><msub id="S3.SS2.p2.4.m4.4.5.3" xref="S3.SS2.p2.4.m4.4.5.3.cmml"><mi id="S3.SS2.p2.4.m4.4.5.3.2" xref="S3.SS2.p2.4.m4.4.5.3.2.cmml">𝐡</mi><mrow id="S3.SS2.p2.4.m4.4.4.4.4" xref="S3.SS2.p2.4.m4.4.4.4.5.cmml"><mrow id="S3.SS2.p2.4.m4.3.3.3.3.1" xref="S3.SS2.p2.4.m4.3.3.3.3.1.cmml"><mi id="S3.SS2.p2.4.m4.3.3.3.3.1.2" xref="S3.SS2.p2.4.m4.3.3.3.3.1.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.4.m4.3.3.3.3.1.1" xref="S3.SS2.p2.4.m4.3.3.3.3.1.1.cmml">​</mo><mrow id="S3.SS2.p2.4.m4.3.3.3.3.1.3.2" xref="S3.SS2.p2.4.m4.3.3.3.3.1.cmml"><mo stretchy="false" id="S3.SS2.p2.4.m4.3.3.3.3.1.3.2.1" xref="S3.SS2.p2.4.m4.3.3.3.3.1.cmml">(</mo><mi id="S3.SS2.p2.4.m4.1.1.1.1" xref="S3.SS2.p2.4.m4.1.1.1.1.cmml">i</mi><mo stretchy="false" id="S3.SS2.p2.4.m4.3.3.3.3.1.3.2.2" xref="S3.SS2.p2.4.m4.3.3.3.3.1.cmml">)</mo></mrow></mrow><mo id="S3.SS2.p2.4.m4.4.4.4.4.3" xref="S3.SS2.p2.4.m4.4.4.4.5.cmml">,</mo><mrow id="S3.SS2.p2.4.m4.4.4.4.4.2" xref="S3.SS2.p2.4.m4.4.4.4.4.2.cmml"><mi id="S3.SS2.p2.4.m4.4.4.4.4.2.2" xref="S3.SS2.p2.4.m4.4.4.4.4.2.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.4.m4.4.4.4.4.2.1" xref="S3.SS2.p2.4.m4.4.4.4.4.2.1.cmml">​</mo><mrow id="S3.SS2.p2.4.m4.4.4.4.4.2.3.2" xref="S3.SS2.p2.4.m4.4.4.4.4.2.cmml"><mo stretchy="false" id="S3.SS2.p2.4.m4.4.4.4.4.2.3.2.1" xref="S3.SS2.p2.4.m4.4.4.4.4.2.cmml">(</mo><mi id="S3.SS2.p2.4.m4.2.2.2.2" xref="S3.SS2.p2.4.m4.2.2.2.2.cmml">i</mi><mo stretchy="false" id="S3.SS2.p2.4.m4.4.4.4.4.2.3.2.2" xref="S3.SS2.p2.4.m4.4.4.4.4.2.cmml">)</mo></mrow></mrow></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.4b"><apply id="S3.SS2.p2.4.m4.4.5.cmml" xref="S3.SS2.p2.4.m4.4.5"><eq id="S3.SS2.p2.4.m4.4.5.1.cmml" xref="S3.SS2.p2.4.m4.4.5.1"></eq><apply id="S3.SS2.p2.4.m4.4.5.2.cmml" xref="S3.SS2.p2.4.m4.4.5.2"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.4.5.2.1.cmml" xref="S3.SS2.p2.4.m4.4.5.2">superscript</csymbol><apply id="S3.SS2.p2.4.m4.4.5.2.2.cmml" xref="S3.SS2.p2.4.m4.4.5.2"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.4.5.2.2.1.cmml" xref="S3.SS2.p2.4.m4.4.5.2">subscript</csymbol><ci id="S3.SS2.p2.4.m4.4.5.2.2.2.cmml" xref="S3.SS2.p2.4.m4.4.5.2.2.2">𝐩</ci><ci id="S3.SS2.p2.4.m4.4.5.2.2.3.cmml" xref="S3.SS2.p2.4.m4.4.5.2.2.3">𝑖</ci></apply><ci id="S3.SS2.p2.4.m4.4.5.2.3a.cmml" xref="S3.SS2.p2.4.m4.4.5.2.3"><mtext mathsize="70%" id="S3.SS2.p2.4.m4.4.5.2.3.cmml" xref="S3.SS2.p2.4.m4.4.5.2.3">cam</mtext></ci></apply><apply id="S3.SS2.p2.4.m4.4.5.3.cmml" xref="S3.SS2.p2.4.m4.4.5.3"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.4.5.3.1.cmml" xref="S3.SS2.p2.4.m4.4.5.3">subscript</csymbol><ci id="S3.SS2.p2.4.m4.4.5.3.2.cmml" xref="S3.SS2.p2.4.m4.4.5.3.2">𝐡</ci><list id="S3.SS2.p2.4.m4.4.4.4.5.cmml" xref="S3.SS2.p2.4.m4.4.4.4.4"><apply id="S3.SS2.p2.4.m4.3.3.3.3.1.cmml" xref="S3.SS2.p2.4.m4.3.3.3.3.1"><times id="S3.SS2.p2.4.m4.3.3.3.3.1.1.cmml" xref="S3.SS2.p2.4.m4.3.3.3.3.1.1"></times><ci id="S3.SS2.p2.4.m4.3.3.3.3.1.2.cmml" xref="S3.SS2.p2.4.m4.3.3.3.3.1.2">𝑚</ci><ci id="S3.SS2.p2.4.m4.1.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1.1.1">𝑖</ci></apply><apply id="S3.SS2.p2.4.m4.4.4.4.4.2.cmml" xref="S3.SS2.p2.4.m4.4.4.4.4.2"><times id="S3.SS2.p2.4.m4.4.4.4.4.2.1.cmml" xref="S3.SS2.p2.4.m4.4.4.4.4.2.1"></times><ci id="S3.SS2.p2.4.m4.4.4.4.4.2.2.cmml" xref="S3.SS2.p2.4.m4.4.4.4.4.2.2">𝑛</ci><ci id="S3.SS2.p2.4.m4.2.2.2.2.cmml" xref="S3.SS2.p2.4.m4.2.2.2.2">𝑖</ci></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.4c">\mathbf{p}_{i}^{\text{cam}}=\mathbf{h}_{m(i),n(i)}</annotation></semantics></math>,
which is a slice of <math id="S3.SS2.p2.5.m5.1" class="ltx_Math" alttext="\mathbf{H}" display="inline"><semantics id="S3.SS2.p2.5.m5.1a"><mi id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml">𝐇</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><ci id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1">𝐇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">\mathbf{H}</annotation></semantics></math> at location <math id="S3.SS2.p2.6.m6.4" class="ltx_Math" alttext="(m(i),n(i))" display="inline"><semantics id="S3.SS2.p2.6.m6.4a"><mrow id="S3.SS2.p2.6.m6.4.4.2" xref="S3.SS2.p2.6.m6.4.4.3.cmml"><mo stretchy="false" id="S3.SS2.p2.6.m6.4.4.2.3" xref="S3.SS2.p2.6.m6.4.4.3.cmml">(</mo><mrow id="S3.SS2.p2.6.m6.3.3.1.1" xref="S3.SS2.p2.6.m6.3.3.1.1.cmml"><mi id="S3.SS2.p2.6.m6.3.3.1.1.2" xref="S3.SS2.p2.6.m6.3.3.1.1.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.6.m6.3.3.1.1.1" xref="S3.SS2.p2.6.m6.3.3.1.1.1.cmml">​</mo><mrow id="S3.SS2.p2.6.m6.3.3.1.1.3.2" xref="S3.SS2.p2.6.m6.3.3.1.1.cmml"><mo stretchy="false" id="S3.SS2.p2.6.m6.3.3.1.1.3.2.1" xref="S3.SS2.p2.6.m6.3.3.1.1.cmml">(</mo><mi id="S3.SS2.p2.6.m6.1.1" xref="S3.SS2.p2.6.m6.1.1.cmml">i</mi><mo stretchy="false" id="S3.SS2.p2.6.m6.3.3.1.1.3.2.2" xref="S3.SS2.p2.6.m6.3.3.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS2.p2.6.m6.4.4.2.4" xref="S3.SS2.p2.6.m6.4.4.3.cmml">,</mo><mrow id="S3.SS2.p2.6.m6.4.4.2.2" xref="S3.SS2.p2.6.m6.4.4.2.2.cmml"><mi id="S3.SS2.p2.6.m6.4.4.2.2.2" xref="S3.SS2.p2.6.m6.4.4.2.2.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.6.m6.4.4.2.2.1" xref="S3.SS2.p2.6.m6.4.4.2.2.1.cmml">​</mo><mrow id="S3.SS2.p2.6.m6.4.4.2.2.3.2" xref="S3.SS2.p2.6.m6.4.4.2.2.cmml"><mo stretchy="false" id="S3.SS2.p2.6.m6.4.4.2.2.3.2.1" xref="S3.SS2.p2.6.m6.4.4.2.2.cmml">(</mo><mi id="S3.SS2.p2.6.m6.2.2" xref="S3.SS2.p2.6.m6.2.2.cmml">i</mi><mo stretchy="false" id="S3.SS2.p2.6.m6.4.4.2.2.3.2.2" xref="S3.SS2.p2.6.m6.4.4.2.2.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.SS2.p2.6.m6.4.4.2.5" xref="S3.SS2.p2.6.m6.4.4.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m6.4b"><interval closure="open" id="S3.SS2.p2.6.m6.4.4.3.cmml" xref="S3.SS2.p2.6.m6.4.4.2"><apply id="S3.SS2.p2.6.m6.3.3.1.1.cmml" xref="S3.SS2.p2.6.m6.3.3.1.1"><times id="S3.SS2.p2.6.m6.3.3.1.1.1.cmml" xref="S3.SS2.p2.6.m6.3.3.1.1.1"></times><ci id="S3.SS2.p2.6.m6.3.3.1.1.2.cmml" xref="S3.SS2.p2.6.m6.3.3.1.1.2">𝑚</ci><ci id="S3.SS2.p2.6.m6.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1">𝑖</ci></apply><apply id="S3.SS2.p2.6.m6.4.4.2.2.cmml" xref="S3.SS2.p2.6.m6.4.4.2.2"><times id="S3.SS2.p2.6.m6.4.4.2.2.1.cmml" xref="S3.SS2.p2.6.m6.4.4.2.2.1"></times><ci id="S3.SS2.p2.6.m6.4.4.2.2.2.cmml" xref="S3.SS2.p2.6.m6.4.4.2.2.2">𝑛</ci><ci id="S3.SS2.p2.6.m6.2.2.cmml" xref="S3.SS2.p2.6.m6.2.2">𝑖</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m6.4c">(m(i),n(i))</annotation></semantics></math>. Here <math id="S3.SS2.p2.7.m7.2" class="ltx_Math" alttext="m(i)=\mathrm{round}(\frac{W^{\prime}}{W}x_{1i})" display="inline"><semantics id="S3.SS2.p2.7.m7.2a"><mrow id="S3.SS2.p2.7.m7.2.2" xref="S3.SS2.p2.7.m7.2.2.cmml"><mrow id="S3.SS2.p2.7.m7.2.2.3" xref="S3.SS2.p2.7.m7.2.2.3.cmml"><mi id="S3.SS2.p2.7.m7.2.2.3.2" xref="S3.SS2.p2.7.m7.2.2.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.7.m7.2.2.3.1" xref="S3.SS2.p2.7.m7.2.2.3.1.cmml">​</mo><mrow id="S3.SS2.p2.7.m7.2.2.3.3.2" xref="S3.SS2.p2.7.m7.2.2.3.cmml"><mo stretchy="false" id="S3.SS2.p2.7.m7.2.2.3.3.2.1" xref="S3.SS2.p2.7.m7.2.2.3.cmml">(</mo><mi id="S3.SS2.p2.7.m7.1.1" xref="S3.SS2.p2.7.m7.1.1.cmml">i</mi><mo stretchy="false" id="S3.SS2.p2.7.m7.2.2.3.3.2.2" xref="S3.SS2.p2.7.m7.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.SS2.p2.7.m7.2.2.2" xref="S3.SS2.p2.7.m7.2.2.2.cmml">=</mo><mrow id="S3.SS2.p2.7.m7.2.2.1" xref="S3.SS2.p2.7.m7.2.2.1.cmml"><mi id="S3.SS2.p2.7.m7.2.2.1.3" xref="S3.SS2.p2.7.m7.2.2.1.3.cmml">round</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.7.m7.2.2.1.2" xref="S3.SS2.p2.7.m7.2.2.1.2.cmml">​</mo><mrow id="S3.SS2.p2.7.m7.2.2.1.1.1" xref="S3.SS2.p2.7.m7.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p2.7.m7.2.2.1.1.1.2" xref="S3.SS2.p2.7.m7.2.2.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p2.7.m7.2.2.1.1.1.1" xref="S3.SS2.p2.7.m7.2.2.1.1.1.1.cmml"><mfrac id="S3.SS2.p2.7.m7.2.2.1.1.1.1.2" xref="S3.SS2.p2.7.m7.2.2.1.1.1.1.2.cmml"><msup id="S3.SS2.p2.7.m7.2.2.1.1.1.1.2.2" xref="S3.SS2.p2.7.m7.2.2.1.1.1.1.2.2.cmml"><mi id="S3.SS2.p2.7.m7.2.2.1.1.1.1.2.2.2" xref="S3.SS2.p2.7.m7.2.2.1.1.1.1.2.2.2.cmml">W</mi><mo id="S3.SS2.p2.7.m7.2.2.1.1.1.1.2.2.3" xref="S3.SS2.p2.7.m7.2.2.1.1.1.1.2.2.3.cmml">′</mo></msup><mi id="S3.SS2.p2.7.m7.2.2.1.1.1.1.2.3" xref="S3.SS2.p2.7.m7.2.2.1.1.1.1.2.3.cmml">W</mi></mfrac><mo lspace="0em" rspace="0em" id="S3.SS2.p2.7.m7.2.2.1.1.1.1.1" xref="S3.SS2.p2.7.m7.2.2.1.1.1.1.1.cmml">​</mo><msub id="S3.SS2.p2.7.m7.2.2.1.1.1.1.3" xref="S3.SS2.p2.7.m7.2.2.1.1.1.1.3.cmml"><mi id="S3.SS2.p2.7.m7.2.2.1.1.1.1.3.2" xref="S3.SS2.p2.7.m7.2.2.1.1.1.1.3.2.cmml">x</mi><mrow id="S3.SS2.p2.7.m7.2.2.1.1.1.1.3.3" xref="S3.SS2.p2.7.m7.2.2.1.1.1.1.3.3.cmml"><mn id="S3.SS2.p2.7.m7.2.2.1.1.1.1.3.3.2" xref="S3.SS2.p2.7.m7.2.2.1.1.1.1.3.3.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S3.SS2.p2.7.m7.2.2.1.1.1.1.3.3.1" xref="S3.SS2.p2.7.m7.2.2.1.1.1.1.3.3.1.cmml">​</mo><mi id="S3.SS2.p2.7.m7.2.2.1.1.1.1.3.3.3" xref="S3.SS2.p2.7.m7.2.2.1.1.1.1.3.3.3.cmml">i</mi></mrow></msub></mrow><mo stretchy="false" id="S3.SS2.p2.7.m7.2.2.1.1.1.3" xref="S3.SS2.p2.7.m7.2.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.7.m7.2b"><apply id="S3.SS2.p2.7.m7.2.2.cmml" xref="S3.SS2.p2.7.m7.2.2"><eq id="S3.SS2.p2.7.m7.2.2.2.cmml" xref="S3.SS2.p2.7.m7.2.2.2"></eq><apply id="S3.SS2.p2.7.m7.2.2.3.cmml" xref="S3.SS2.p2.7.m7.2.2.3"><times id="S3.SS2.p2.7.m7.2.2.3.1.cmml" xref="S3.SS2.p2.7.m7.2.2.3.1"></times><ci id="S3.SS2.p2.7.m7.2.2.3.2.cmml" xref="S3.SS2.p2.7.m7.2.2.3.2">𝑚</ci><ci id="S3.SS2.p2.7.m7.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1">𝑖</ci></apply><apply id="S3.SS2.p2.7.m7.2.2.1.cmml" xref="S3.SS2.p2.7.m7.2.2.1"><times id="S3.SS2.p2.7.m7.2.2.1.2.cmml" xref="S3.SS2.p2.7.m7.2.2.1.2"></times><ci id="S3.SS2.p2.7.m7.2.2.1.3.cmml" xref="S3.SS2.p2.7.m7.2.2.1.3">round</ci><apply id="S3.SS2.p2.7.m7.2.2.1.1.1.1.cmml" xref="S3.SS2.p2.7.m7.2.2.1.1.1"><times id="S3.SS2.p2.7.m7.2.2.1.1.1.1.1.cmml" xref="S3.SS2.p2.7.m7.2.2.1.1.1.1.1"></times><apply id="S3.SS2.p2.7.m7.2.2.1.1.1.1.2.cmml" xref="S3.SS2.p2.7.m7.2.2.1.1.1.1.2"><divide id="S3.SS2.p2.7.m7.2.2.1.1.1.1.2.1.cmml" xref="S3.SS2.p2.7.m7.2.2.1.1.1.1.2"></divide><apply id="S3.SS2.p2.7.m7.2.2.1.1.1.1.2.2.cmml" xref="S3.SS2.p2.7.m7.2.2.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p2.7.m7.2.2.1.1.1.1.2.2.1.cmml" xref="S3.SS2.p2.7.m7.2.2.1.1.1.1.2.2">superscript</csymbol><ci id="S3.SS2.p2.7.m7.2.2.1.1.1.1.2.2.2.cmml" xref="S3.SS2.p2.7.m7.2.2.1.1.1.1.2.2.2">𝑊</ci><ci id="S3.SS2.p2.7.m7.2.2.1.1.1.1.2.2.3.cmml" xref="S3.SS2.p2.7.m7.2.2.1.1.1.1.2.2.3">′</ci></apply><ci id="S3.SS2.p2.7.m7.2.2.1.1.1.1.2.3.cmml" xref="S3.SS2.p2.7.m7.2.2.1.1.1.1.2.3">𝑊</ci></apply><apply id="S3.SS2.p2.7.m7.2.2.1.1.1.1.3.cmml" xref="S3.SS2.p2.7.m7.2.2.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.7.m7.2.2.1.1.1.1.3.1.cmml" xref="S3.SS2.p2.7.m7.2.2.1.1.1.1.3">subscript</csymbol><ci id="S3.SS2.p2.7.m7.2.2.1.1.1.1.3.2.cmml" xref="S3.SS2.p2.7.m7.2.2.1.1.1.1.3.2">𝑥</ci><apply id="S3.SS2.p2.7.m7.2.2.1.1.1.1.3.3.cmml" xref="S3.SS2.p2.7.m7.2.2.1.1.1.1.3.3"><times id="S3.SS2.p2.7.m7.2.2.1.1.1.1.3.3.1.cmml" xref="S3.SS2.p2.7.m7.2.2.1.1.1.1.3.3.1"></times><cn type="integer" id="S3.SS2.p2.7.m7.2.2.1.1.1.1.3.3.2.cmml" xref="S3.SS2.p2.7.m7.2.2.1.1.1.1.3.3.2">1</cn><ci id="S3.SS2.p2.7.m7.2.2.1.1.1.1.3.3.3.cmml" xref="S3.SS2.p2.7.m7.2.2.1.1.1.1.3.3.3">𝑖</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.7.m7.2c">m(i)=\mathrm{round}(\frac{W^{\prime}}{W}x_{1i})</annotation></semantics></math> and <math id="S3.SS2.p2.8.m8.2" class="ltx_Math" alttext="n(i)=\mathrm{round}(\frac{H^{\prime}}{H}x_{2i})" display="inline"><semantics id="S3.SS2.p2.8.m8.2a"><mrow id="S3.SS2.p2.8.m8.2.2" xref="S3.SS2.p2.8.m8.2.2.cmml"><mrow id="S3.SS2.p2.8.m8.2.2.3" xref="S3.SS2.p2.8.m8.2.2.3.cmml"><mi id="S3.SS2.p2.8.m8.2.2.3.2" xref="S3.SS2.p2.8.m8.2.2.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.8.m8.2.2.3.1" xref="S3.SS2.p2.8.m8.2.2.3.1.cmml">​</mo><mrow id="S3.SS2.p2.8.m8.2.2.3.3.2" xref="S3.SS2.p2.8.m8.2.2.3.cmml"><mo stretchy="false" id="S3.SS2.p2.8.m8.2.2.3.3.2.1" xref="S3.SS2.p2.8.m8.2.2.3.cmml">(</mo><mi id="S3.SS2.p2.8.m8.1.1" xref="S3.SS2.p2.8.m8.1.1.cmml">i</mi><mo stretchy="false" id="S3.SS2.p2.8.m8.2.2.3.3.2.2" xref="S3.SS2.p2.8.m8.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.SS2.p2.8.m8.2.2.2" xref="S3.SS2.p2.8.m8.2.2.2.cmml">=</mo><mrow id="S3.SS2.p2.8.m8.2.2.1" xref="S3.SS2.p2.8.m8.2.2.1.cmml"><mi id="S3.SS2.p2.8.m8.2.2.1.3" xref="S3.SS2.p2.8.m8.2.2.1.3.cmml">round</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.8.m8.2.2.1.2" xref="S3.SS2.p2.8.m8.2.2.1.2.cmml">​</mo><mrow id="S3.SS2.p2.8.m8.2.2.1.1.1" xref="S3.SS2.p2.8.m8.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p2.8.m8.2.2.1.1.1.2" xref="S3.SS2.p2.8.m8.2.2.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p2.8.m8.2.2.1.1.1.1" xref="S3.SS2.p2.8.m8.2.2.1.1.1.1.cmml"><mfrac id="S3.SS2.p2.8.m8.2.2.1.1.1.1.2" xref="S3.SS2.p2.8.m8.2.2.1.1.1.1.2.cmml"><msup id="S3.SS2.p2.8.m8.2.2.1.1.1.1.2.2" xref="S3.SS2.p2.8.m8.2.2.1.1.1.1.2.2.cmml"><mi id="S3.SS2.p2.8.m8.2.2.1.1.1.1.2.2.2" xref="S3.SS2.p2.8.m8.2.2.1.1.1.1.2.2.2.cmml">H</mi><mo id="S3.SS2.p2.8.m8.2.2.1.1.1.1.2.2.3" xref="S3.SS2.p2.8.m8.2.2.1.1.1.1.2.2.3.cmml">′</mo></msup><mi id="S3.SS2.p2.8.m8.2.2.1.1.1.1.2.3" xref="S3.SS2.p2.8.m8.2.2.1.1.1.1.2.3.cmml">H</mi></mfrac><mo lspace="0em" rspace="0em" id="S3.SS2.p2.8.m8.2.2.1.1.1.1.1" xref="S3.SS2.p2.8.m8.2.2.1.1.1.1.1.cmml">​</mo><msub id="S3.SS2.p2.8.m8.2.2.1.1.1.1.3" xref="S3.SS2.p2.8.m8.2.2.1.1.1.1.3.cmml"><mi id="S3.SS2.p2.8.m8.2.2.1.1.1.1.3.2" xref="S3.SS2.p2.8.m8.2.2.1.1.1.1.3.2.cmml">x</mi><mrow id="S3.SS2.p2.8.m8.2.2.1.1.1.1.3.3" xref="S3.SS2.p2.8.m8.2.2.1.1.1.1.3.3.cmml"><mn id="S3.SS2.p2.8.m8.2.2.1.1.1.1.3.3.2" xref="S3.SS2.p2.8.m8.2.2.1.1.1.1.3.3.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.SS2.p2.8.m8.2.2.1.1.1.1.3.3.1" xref="S3.SS2.p2.8.m8.2.2.1.1.1.1.3.3.1.cmml">​</mo><mi id="S3.SS2.p2.8.m8.2.2.1.1.1.1.3.3.3" xref="S3.SS2.p2.8.m8.2.2.1.1.1.1.3.3.3.cmml">i</mi></mrow></msub></mrow><mo stretchy="false" id="S3.SS2.p2.8.m8.2.2.1.1.1.3" xref="S3.SS2.p2.8.m8.2.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.8.m8.2b"><apply id="S3.SS2.p2.8.m8.2.2.cmml" xref="S3.SS2.p2.8.m8.2.2"><eq id="S3.SS2.p2.8.m8.2.2.2.cmml" xref="S3.SS2.p2.8.m8.2.2.2"></eq><apply id="S3.SS2.p2.8.m8.2.2.3.cmml" xref="S3.SS2.p2.8.m8.2.2.3"><times id="S3.SS2.p2.8.m8.2.2.3.1.cmml" xref="S3.SS2.p2.8.m8.2.2.3.1"></times><ci id="S3.SS2.p2.8.m8.2.2.3.2.cmml" xref="S3.SS2.p2.8.m8.2.2.3.2">𝑛</ci><ci id="S3.SS2.p2.8.m8.1.1.cmml" xref="S3.SS2.p2.8.m8.1.1">𝑖</ci></apply><apply id="S3.SS2.p2.8.m8.2.2.1.cmml" xref="S3.SS2.p2.8.m8.2.2.1"><times id="S3.SS2.p2.8.m8.2.2.1.2.cmml" xref="S3.SS2.p2.8.m8.2.2.1.2"></times><ci id="S3.SS2.p2.8.m8.2.2.1.3.cmml" xref="S3.SS2.p2.8.m8.2.2.1.3">round</ci><apply id="S3.SS2.p2.8.m8.2.2.1.1.1.1.cmml" xref="S3.SS2.p2.8.m8.2.2.1.1.1"><times id="S3.SS2.p2.8.m8.2.2.1.1.1.1.1.cmml" xref="S3.SS2.p2.8.m8.2.2.1.1.1.1.1"></times><apply id="S3.SS2.p2.8.m8.2.2.1.1.1.1.2.cmml" xref="S3.SS2.p2.8.m8.2.2.1.1.1.1.2"><divide id="S3.SS2.p2.8.m8.2.2.1.1.1.1.2.1.cmml" xref="S3.SS2.p2.8.m8.2.2.1.1.1.1.2"></divide><apply id="S3.SS2.p2.8.m8.2.2.1.1.1.1.2.2.cmml" xref="S3.SS2.p2.8.m8.2.2.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p2.8.m8.2.2.1.1.1.1.2.2.1.cmml" xref="S3.SS2.p2.8.m8.2.2.1.1.1.1.2.2">superscript</csymbol><ci id="S3.SS2.p2.8.m8.2.2.1.1.1.1.2.2.2.cmml" xref="S3.SS2.p2.8.m8.2.2.1.1.1.1.2.2.2">𝐻</ci><ci id="S3.SS2.p2.8.m8.2.2.1.1.1.1.2.2.3.cmml" xref="S3.SS2.p2.8.m8.2.2.1.1.1.1.2.2.3">′</ci></apply><ci id="S3.SS2.p2.8.m8.2.2.1.1.1.1.2.3.cmml" xref="S3.SS2.p2.8.m8.2.2.1.1.1.1.2.3">𝐻</ci></apply><apply id="S3.SS2.p2.8.m8.2.2.1.1.1.1.3.cmml" xref="S3.SS2.p2.8.m8.2.2.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.8.m8.2.2.1.1.1.1.3.1.cmml" xref="S3.SS2.p2.8.m8.2.2.1.1.1.1.3">subscript</csymbol><ci id="S3.SS2.p2.8.m8.2.2.1.1.1.1.3.2.cmml" xref="S3.SS2.p2.8.m8.2.2.1.1.1.1.3.2">𝑥</ci><apply id="S3.SS2.p2.8.m8.2.2.1.1.1.1.3.3.cmml" xref="S3.SS2.p2.8.m8.2.2.1.1.1.1.3.3"><times id="S3.SS2.p2.8.m8.2.2.1.1.1.1.3.3.1.cmml" xref="S3.SS2.p2.8.m8.2.2.1.1.1.1.3.3.1"></times><cn type="integer" id="S3.SS2.p2.8.m8.2.2.1.1.1.1.3.3.2.cmml" xref="S3.SS2.p2.8.m8.2.2.1.1.1.1.3.3.2">2</cn><ci id="S3.SS2.p2.8.m8.2.2.1.1.1.1.3.3.3.cmml" xref="S3.SS2.p2.8.m8.2.2.1.1.1.1.3.3.3">𝑖</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.8.m8.2c">n(i)=\mathrm{round}(\frac{H^{\prime}}{H}x_{2i})</annotation></semantics></math>, where <math id="S3.SS2.p2.9.m9.3" class="ltx_Math" alttext="\mathbf{x}_{i}^{(2)}=(x_{1i},x_{2i})" display="inline"><semantics id="S3.SS2.p2.9.m9.3a"><mrow id="S3.SS2.p2.9.m9.3.3" xref="S3.SS2.p2.9.m9.3.3.cmml"><msubsup id="S3.SS2.p2.9.m9.3.3.4" xref="S3.SS2.p2.9.m9.3.3.4.cmml"><mi id="S3.SS2.p2.9.m9.3.3.4.2.2" xref="S3.SS2.p2.9.m9.3.3.4.2.2.cmml">𝐱</mi><mi id="S3.SS2.p2.9.m9.3.3.4.2.3" xref="S3.SS2.p2.9.m9.3.3.4.2.3.cmml">i</mi><mrow id="S3.SS2.p2.9.m9.1.1.1.3" xref="S3.SS2.p2.9.m9.3.3.4.cmml"><mo stretchy="false" id="S3.SS2.p2.9.m9.1.1.1.3.1" xref="S3.SS2.p2.9.m9.3.3.4.cmml">(</mo><mn id="S3.SS2.p2.9.m9.1.1.1.1" xref="S3.SS2.p2.9.m9.1.1.1.1.cmml">2</mn><mo stretchy="false" id="S3.SS2.p2.9.m9.1.1.1.3.2" xref="S3.SS2.p2.9.m9.3.3.4.cmml">)</mo></mrow></msubsup><mo id="S3.SS2.p2.9.m9.3.3.3" xref="S3.SS2.p2.9.m9.3.3.3.cmml">=</mo><mrow id="S3.SS2.p2.9.m9.3.3.2.2" xref="S3.SS2.p2.9.m9.3.3.2.3.cmml"><mo stretchy="false" id="S3.SS2.p2.9.m9.3.3.2.2.3" xref="S3.SS2.p2.9.m9.3.3.2.3.cmml">(</mo><msub id="S3.SS2.p2.9.m9.2.2.1.1.1" xref="S3.SS2.p2.9.m9.2.2.1.1.1.cmml"><mi id="S3.SS2.p2.9.m9.2.2.1.1.1.2" xref="S3.SS2.p2.9.m9.2.2.1.1.1.2.cmml">x</mi><mrow id="S3.SS2.p2.9.m9.2.2.1.1.1.3" xref="S3.SS2.p2.9.m9.2.2.1.1.1.3.cmml"><mn id="S3.SS2.p2.9.m9.2.2.1.1.1.3.2" xref="S3.SS2.p2.9.m9.2.2.1.1.1.3.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S3.SS2.p2.9.m9.2.2.1.1.1.3.1" xref="S3.SS2.p2.9.m9.2.2.1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p2.9.m9.2.2.1.1.1.3.3" xref="S3.SS2.p2.9.m9.2.2.1.1.1.3.3.cmml">i</mi></mrow></msub><mo id="S3.SS2.p2.9.m9.3.3.2.2.4" xref="S3.SS2.p2.9.m9.3.3.2.3.cmml">,</mo><msub id="S3.SS2.p2.9.m9.3.3.2.2.2" xref="S3.SS2.p2.9.m9.3.3.2.2.2.cmml"><mi id="S3.SS2.p2.9.m9.3.3.2.2.2.2" xref="S3.SS2.p2.9.m9.3.3.2.2.2.2.cmml">x</mi><mrow id="S3.SS2.p2.9.m9.3.3.2.2.2.3" xref="S3.SS2.p2.9.m9.3.3.2.2.2.3.cmml"><mn id="S3.SS2.p2.9.m9.3.3.2.2.2.3.2" xref="S3.SS2.p2.9.m9.3.3.2.2.2.3.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.SS2.p2.9.m9.3.3.2.2.2.3.1" xref="S3.SS2.p2.9.m9.3.3.2.2.2.3.1.cmml">​</mo><mi id="S3.SS2.p2.9.m9.3.3.2.2.2.3.3" xref="S3.SS2.p2.9.m9.3.3.2.2.2.3.3.cmml">i</mi></mrow></msub><mo stretchy="false" id="S3.SS2.p2.9.m9.3.3.2.2.5" xref="S3.SS2.p2.9.m9.3.3.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.9.m9.3b"><apply id="S3.SS2.p2.9.m9.3.3.cmml" xref="S3.SS2.p2.9.m9.3.3"><eq id="S3.SS2.p2.9.m9.3.3.3.cmml" xref="S3.SS2.p2.9.m9.3.3.3"></eq><apply id="S3.SS2.p2.9.m9.3.3.4.cmml" xref="S3.SS2.p2.9.m9.3.3.4"><csymbol cd="ambiguous" id="S3.SS2.p2.9.m9.3.3.4.1.cmml" xref="S3.SS2.p2.9.m9.3.3.4">superscript</csymbol><apply id="S3.SS2.p2.9.m9.3.3.4.2.cmml" xref="S3.SS2.p2.9.m9.3.3.4"><csymbol cd="ambiguous" id="S3.SS2.p2.9.m9.3.3.4.2.1.cmml" xref="S3.SS2.p2.9.m9.3.3.4">subscript</csymbol><ci id="S3.SS2.p2.9.m9.3.3.4.2.2.cmml" xref="S3.SS2.p2.9.m9.3.3.4.2.2">𝐱</ci><ci id="S3.SS2.p2.9.m9.3.3.4.2.3.cmml" xref="S3.SS2.p2.9.m9.3.3.4.2.3">𝑖</ci></apply><cn type="integer" id="S3.SS2.p2.9.m9.1.1.1.1.cmml" xref="S3.SS2.p2.9.m9.1.1.1.1">2</cn></apply><interval closure="open" id="S3.SS2.p2.9.m9.3.3.2.3.cmml" xref="S3.SS2.p2.9.m9.3.3.2.2"><apply id="S3.SS2.p2.9.m9.2.2.1.1.1.cmml" xref="S3.SS2.p2.9.m9.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.9.m9.2.2.1.1.1.1.cmml" xref="S3.SS2.p2.9.m9.2.2.1.1.1">subscript</csymbol><ci id="S3.SS2.p2.9.m9.2.2.1.1.1.2.cmml" xref="S3.SS2.p2.9.m9.2.2.1.1.1.2">𝑥</ci><apply id="S3.SS2.p2.9.m9.2.2.1.1.1.3.cmml" xref="S3.SS2.p2.9.m9.2.2.1.1.1.3"><times id="S3.SS2.p2.9.m9.2.2.1.1.1.3.1.cmml" xref="S3.SS2.p2.9.m9.2.2.1.1.1.3.1"></times><cn type="integer" id="S3.SS2.p2.9.m9.2.2.1.1.1.3.2.cmml" xref="S3.SS2.p2.9.m9.2.2.1.1.1.3.2">1</cn><ci id="S3.SS2.p2.9.m9.2.2.1.1.1.3.3.cmml" xref="S3.SS2.p2.9.m9.2.2.1.1.1.3.3">𝑖</ci></apply></apply><apply id="S3.SS2.p2.9.m9.3.3.2.2.2.cmml" xref="S3.SS2.p2.9.m9.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p2.9.m9.3.3.2.2.2.1.cmml" xref="S3.SS2.p2.9.m9.3.3.2.2.2">subscript</csymbol><ci id="S3.SS2.p2.9.m9.3.3.2.2.2.2.cmml" xref="S3.SS2.p2.9.m9.3.3.2.2.2.2">𝑥</ci><apply id="S3.SS2.p2.9.m9.3.3.2.2.2.3.cmml" xref="S3.SS2.p2.9.m9.3.3.2.2.2.3"><times id="S3.SS2.p2.9.m9.3.3.2.2.2.3.1.cmml" xref="S3.SS2.p2.9.m9.3.3.2.2.2.3.1"></times><cn type="integer" id="S3.SS2.p2.9.m9.3.3.2.2.2.3.2.cmml" xref="S3.SS2.p2.9.m9.3.3.2.2.2.3.2">2</cn><ci id="S3.SS2.p2.9.m9.3.3.2.2.2.3.3.cmml" xref="S3.SS2.p2.9.m9.3.3.2.2.2.3.3">𝑖</ci></apply></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.9.m9.3c">\mathbf{x}_{i}^{(2)}=(x_{1i},x_{2i})</annotation></semantics></math> are the 2D image coordinates of point <math id="S3.SS2.p2.10.m10.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS2.p2.10.m10.1a"><mi id="S3.SS2.p2.10.m10.1.1" xref="S3.SS2.p2.10.m10.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.10.m10.1b"><ci id="S3.SS2.p2.10.m10.1.1.cmml" xref="S3.SS2.p2.10.m10.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.10.m10.1c">i</annotation></semantics></math>. In practice, we observe that heatmaps from the camera network are usually very peaky, which contains little information at locations not close to any keypoints. Hence, we apply Gaussian smoothing to enlarge the receptive field at these locations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, so the corresponding point can utilize the information from a larger neighborhood on the image.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.3" class="ltx_p">Finally, camera features <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="\mathbf{p}_{i}^{\text{cam}}" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><msubsup id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml"><mi id="S3.SS2.p3.1.m1.1.1.2.2" xref="S3.SS2.p3.1.m1.1.1.2.2.cmml">𝐩</mi><mi id="S3.SS2.p3.1.m1.1.1.2.3" xref="S3.SS2.p3.1.m1.1.1.2.3.cmml">i</mi><mtext id="S3.SS2.p3.1.m1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.3a.cmml">cam</mtext></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">superscript</csymbol><apply id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.1.1.2.1.cmml" xref="S3.SS2.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p3.1.m1.1.1.2.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2.2">𝐩</ci><ci id="S3.SS2.p3.1.m1.1.1.2.3.cmml" xref="S3.SS2.p3.1.m1.1.1.2.3">𝑖</ci></apply><ci id="S3.SS2.p3.1.m1.1.1.3a.cmml" xref="S3.SS2.p3.1.m1.1.1.3"><mtext mathsize="70%" id="S3.SS2.p3.1.m1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3">cam</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">\mathbf{p}_{i}^{\text{cam}}</annotation></semantics></math> are concatenated with the original point feature <math id="S3.SS2.p3.2.m2.1" class="ltx_Math" alttext="\mathbf{p}_{i}" display="inline"><semantics id="S3.SS2.p3.2.m2.1a"><msub id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml"><mi id="S3.SS2.p3.2.m2.1.1.2" xref="S3.SS2.p3.2.m2.1.1.2.cmml">𝐩</mi><mi id="S3.SS2.p3.2.m2.1.1.3" xref="S3.SS2.p3.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><apply id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p3.2.m2.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1.2">𝐩</ci><ci id="S3.SS2.p3.2.m2.1.1.3.cmml" xref="S3.SS2.p3.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">\mathbf{p}_{i}</annotation></semantics></math> to generate the augmented point cloud <math id="S3.SS2.p3.3.m3.1" class="ltx_Math" alttext="\mathbf{P}^{\text{aug}}\in\mathbb{R}^{N\times(d+K)}" display="inline"><semantics id="S3.SS2.p3.3.m3.1a"><mrow id="S3.SS2.p3.3.m3.1.2" xref="S3.SS2.p3.3.m3.1.2.cmml"><msup id="S3.SS2.p3.3.m3.1.2.2" xref="S3.SS2.p3.3.m3.1.2.2.cmml"><mi id="S3.SS2.p3.3.m3.1.2.2.2" xref="S3.SS2.p3.3.m3.1.2.2.2.cmml">𝐏</mi><mtext id="S3.SS2.p3.3.m3.1.2.2.3" xref="S3.SS2.p3.3.m3.1.2.2.3a.cmml">aug</mtext></msup><mo id="S3.SS2.p3.3.m3.1.2.1" xref="S3.SS2.p3.3.m3.1.2.1.cmml">∈</mo><msup id="S3.SS2.p3.3.m3.1.2.3" xref="S3.SS2.p3.3.m3.1.2.3.cmml"><mi id="S3.SS2.p3.3.m3.1.2.3.2" xref="S3.SS2.p3.3.m3.1.2.3.2.cmml">ℝ</mi><mrow id="S3.SS2.p3.3.m3.1.1.1" xref="S3.SS2.p3.3.m3.1.1.1.cmml"><mi id="S3.SS2.p3.3.m3.1.1.1.3" xref="S3.SS2.p3.3.m3.1.1.1.3.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p3.3.m3.1.1.1.2" xref="S3.SS2.p3.3.m3.1.1.1.2.cmml">×</mo><mrow id="S3.SS2.p3.3.m3.1.1.1.1.1" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p3.3.m3.1.1.1.1.1.2" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p3.3.m3.1.1.1.1.1.1" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p3.3.m3.1.1.1.1.1.1.2" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.2.cmml">d</mi><mo id="S3.SS2.p3.3.m3.1.1.1.1.1.1.1" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.1.cmml">+</mo><mi id="S3.SS2.p3.3.m3.1.1.1.1.1.1.3" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.3.cmml">K</mi></mrow><mo stretchy="false" id="S3.SS2.p3.3.m3.1.1.1.1.1.3" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><apply id="S3.SS2.p3.3.m3.1.2.cmml" xref="S3.SS2.p3.3.m3.1.2"><in id="S3.SS2.p3.3.m3.1.2.1.cmml" xref="S3.SS2.p3.3.m3.1.2.1"></in><apply id="S3.SS2.p3.3.m3.1.2.2.cmml" xref="S3.SS2.p3.3.m3.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p3.3.m3.1.2.2.1.cmml" xref="S3.SS2.p3.3.m3.1.2.2">superscript</csymbol><ci id="S3.SS2.p3.3.m3.1.2.2.2.cmml" xref="S3.SS2.p3.3.m3.1.2.2.2">𝐏</ci><ci id="S3.SS2.p3.3.m3.1.2.2.3a.cmml" xref="S3.SS2.p3.3.m3.1.2.2.3"><mtext mathsize="70%" id="S3.SS2.p3.3.m3.1.2.2.3.cmml" xref="S3.SS2.p3.3.m3.1.2.2.3">aug</mtext></ci></apply><apply id="S3.SS2.p3.3.m3.1.2.3.cmml" xref="S3.SS2.p3.3.m3.1.2.3"><csymbol cd="ambiguous" id="S3.SS2.p3.3.m3.1.2.3.1.cmml" xref="S3.SS2.p3.3.m3.1.2.3">superscript</csymbol><ci id="S3.SS2.p3.3.m3.1.2.3.2.cmml" xref="S3.SS2.p3.3.m3.1.2.3.2">ℝ</ci><apply id="S3.SS2.p3.3.m3.1.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1.1"><times id="S3.SS2.p3.3.m3.1.1.1.2.cmml" xref="S3.SS2.p3.3.m3.1.1.1.2"></times><ci id="S3.SS2.p3.3.m3.1.1.1.3.cmml" xref="S3.SS2.p3.3.m3.1.1.1.3">𝑁</ci><apply id="S3.SS2.p3.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1.1.1.1"><plus id="S3.SS2.p3.3.m3.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.1"></plus><ci id="S3.SS2.p3.3.m3.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.2">𝑑</ci><ci id="S3.SS2.p3.3.m3.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.3">𝐾</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">\mathbf{P}^{\text{aug}}\in\mathbb{R}^{N\times(d+K)}</annotation></semantics></math>, which serves as the input of the following point network. This augmentation directly incorporates texture information from RGB images into the point cloud, which helps the LiDAR based point network with information useful for more accurate keypoint predictions. Similar concatenation can be found in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>, where voxel representations are concatenated with heatmaps before feeding into a VoxelNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">The proposed cascade modality fusion architecture achieves improvements because heatmap predictions from the camera network carry complementary texture related semantic cues that are not present in LiDAR point features. Therefore, augmenting lower-level LiDAR point features with higher-level camera features provides the point network both low- and high- level point cloud information. By introducing modality fusion, we achieve <math id="S3.SS2.p4.1.m1.1" class="ltx_Math" alttext="\sim 6\%" display="inline"><semantics id="S3.SS2.p4.1.m1.1a"><mrow id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml"><mi id="S3.SS2.p4.1.m1.1.1.2" xref="S3.SS2.p4.1.m1.1.1.2.cmml"></mi><mo id="S3.SS2.p4.1.m1.1.1.1" xref="S3.SS2.p4.1.m1.1.1.1.cmml">∼</mo><mrow id="S3.SS2.p4.1.m1.1.1.3" xref="S3.SS2.p4.1.m1.1.1.3.cmml"><mn id="S3.SS2.p4.1.m1.1.1.3.2" xref="S3.SS2.p4.1.m1.1.1.3.2.cmml">6</mn><mo id="S3.SS2.p4.1.m1.1.1.3.1" xref="S3.SS2.p4.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><apply id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1"><csymbol cd="latexml" id="S3.SS2.p4.1.m1.1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S3.SS2.p4.1.m1.1.1.2.cmml" xref="S3.SS2.p4.1.m1.1.1.2">absent</csymbol><apply id="S3.SS2.p4.1.m1.1.1.3.cmml" xref="S3.SS2.p4.1.m1.1.1.3"><csymbol cd="latexml" id="S3.SS2.p4.1.m1.1.1.3.1.cmml" xref="S3.SS2.p4.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="S3.SS2.p4.1.m1.1.1.3.2.cmml" xref="S3.SS2.p4.1.m1.1.1.3.2">6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">\sim 6\%</annotation></semantics></math> relative improvement on the Waymo Open Dataset compared to the LiDAR-only baseline (Table <a href="#S4.T3" title="Table 3 ‣ Labeling: ‣ 4.1 Data and Evaluation Metrics ‣ 4 Experiments ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> in Section <a href="#S4" title="4 Experiments ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>).</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2112.12141/assets/pseudo_label_generation.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="359" height="233" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.7.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.8.2" class="ltx_text" style="font-size:90%;">Pseudo label generation: a pseudo 3D keypoint label (<span id="S3.F4.8.2.1" class="ltx_text" style="color:#FF0000;">red</span> triangle) is computed as the weighed average of 3D coordinates of neighboring points (<span id="S3.F4.8.2.2" class="ltx_text" style="color:#0000FF;">blue</span> triangles and dots) to the keypoint label in 2D space (<span id="S3.F4.8.2.3" class="ltx_text" style="color:#FF0000;">red</span> dot). Similarly, to generate pointwise labels, positive labels are assigned to neighboring points (<span id="S3.F4.8.2.4" class="ltx_text" style="color:#0000FF;">blue</span> dots) of a ground truth keypoint (<span id="S3.F4.8.2.5" class="ltx_text" style="color:#FF0000;">red</span> dot) in 2D space (best viewed in color). See Sec <a href="#S3.SS4.SSS1" title="3.4.1 Label Generation ‣ 3.4 Weakly-Supervised Model Training ‣ 3 Method ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4.1</span></a> for details.</span></figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Auxiliary Pointwise Segmentation Branch</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.2" class="ltx_p">Our point network is the primary component of the proposed method, which directly generates 3D keypoint prediction from augmented point clouds. The regression branch predicts a <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="3K" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mrow id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mn id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S3.SS3.p1.1.m1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><times id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2">3</cn><ci id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">3K</annotation></semantics></math>-dimensional output vector corresponding to the 3D coordinates of <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mi id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><ci id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">K</annotation></semantics></math> keypoints.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.3" class="ltx_p">Even though rich camera information from the camera network is provided to the point network by modality fusion, the model’s designated output is still a fixed set of keypoints. It is difficult for a global regression loss to guide the point network to effectively utilize the camera information for each point. Therefore, to provide more direct supervision to every individual point, we propose an auxiliary segmentation branch after the feature encoder in the point network, inspired by the architecture of a segmentation PointNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. For each LiDAR point, the segmentation branch predicts the pose keypoint it is closest to. In other words, the segmentation branch generates <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="N\times K" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mrow id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mi id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p2.1.m1.1.1.1" xref="S3.SS3.p2.1.m1.1.1.1.cmml">×</mo><mi id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><times id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1"></times><ci id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">𝑁</ci><ci id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">N\times K</annotation></semantics></math> confidence scores for assigning <math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><mi id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><ci id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">N</annotation></semantics></math> LiDAR points to <math id="S3.SS3.p2.3.m3.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS3.p2.3.m3.1a"><mi id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><ci id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">K</annotation></semantics></math> pose keypoints (a point with high score means that it is close to the corresponding keypoint). Here, the keypoint type for each point corresponds to the type of its nearest keypoint.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">This additional point-wise loss helps the point network to digest more information from the camera network. By adding the auxiliary segmentation branch and loss, we achieve <math id="S3.SS3.p3.1.m1.1" class="ltx_Math" alttext="\sim 1.8\%" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><mrow id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml"><mi id="S3.SS3.p3.1.m1.1.1.2" xref="S3.SS3.p3.1.m1.1.1.2.cmml"></mi><mo id="S3.SS3.p3.1.m1.1.1.1" xref="S3.SS3.p3.1.m1.1.1.1.cmml">∼</mo><mrow id="S3.SS3.p3.1.m1.1.1.3" xref="S3.SS3.p3.1.m1.1.1.3.cmml"><mn id="S3.SS3.p3.1.m1.1.1.3.2" xref="S3.SS3.p3.1.m1.1.1.3.2.cmml">1.8</mn><mo id="S3.SS3.p3.1.m1.1.1.3.1" xref="S3.SS3.p3.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><apply id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1"><csymbol cd="latexml" id="S3.SS3.p3.1.m1.1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S3.SS3.p3.1.m1.1.1.2.cmml" xref="S3.SS3.p3.1.m1.1.1.2">absent</csymbol><apply id="S3.SS3.p3.1.m1.1.1.3.cmml" xref="S3.SS3.p3.1.m1.1.1.3"><csymbol cd="latexml" id="S3.SS3.p3.1.m1.1.1.3.1.cmml" xref="S3.SS3.p3.1.m1.1.1.3.1">percent</csymbol><cn type="float" id="S3.SS3.p3.1.m1.1.1.3.2.cmml" xref="S3.SS3.p3.1.m1.1.1.3.2">1.8</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">\sim 1.8\%</annotation></semantics></math> relative improvement on the Waymo Open Dataset compared to the modality-fusion architecture without the segmentation branch (Table <a href="#S4.T3" title="Table 3 ‣ Labeling: ‣ 4.1 Data and Evaluation Metrics ‣ 4 Experiments ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> in Section <a href="#S4" title="4 Experiments ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>).</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Weakly-Supervised Model Training</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">Training the proposed point network with two branches needs two sets of labels: For the main regression branch, ground truth 3D keypoint coordinates are required; for the segmentation branch, pointwise keypoint type labels are needed. In the proposed method, we introduce a label generation method to enable model training on pure 2D labels for both tasks.</p>
</div>
<section id="S3.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1 </span>Label Generation</h4>

<div id="S3.SS4.SSS1.p1" class="ltx_para">
<p id="S3.SS4.SSS1.p1.3" class="ltx_p">As stated in Section <a href="#S3.SS1" title="3.1 Problem Formulation ‣ 3 Method ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>, we know the 3D coordinates of input points <math id="S3.SS4.SSS1.p1.1.m1.2" class="ltx_Math" alttext="\{\mathbf{x}_{i}^{(3)}\}_{i=1}^{N}" display="inline"><semantics id="S3.SS4.SSS1.p1.1.m1.2a"><msubsup id="S3.SS4.SSS1.p1.1.m1.2.2" xref="S3.SS4.SSS1.p1.1.m1.2.2.cmml"><mrow id="S3.SS4.SSS1.p1.1.m1.2.2.1.1.1" xref="S3.SS4.SSS1.p1.1.m1.2.2.1.1.2.cmml"><mo stretchy="false" id="S3.SS4.SSS1.p1.1.m1.2.2.1.1.1.2" xref="S3.SS4.SSS1.p1.1.m1.2.2.1.1.2.cmml">{</mo><msubsup id="S3.SS4.SSS1.p1.1.m1.2.2.1.1.1.1" xref="S3.SS4.SSS1.p1.1.m1.2.2.1.1.1.1.cmml"><mi id="S3.SS4.SSS1.p1.1.m1.2.2.1.1.1.1.2.2" xref="S3.SS4.SSS1.p1.1.m1.2.2.1.1.1.1.2.2.cmml">𝐱</mi><mi id="S3.SS4.SSS1.p1.1.m1.2.2.1.1.1.1.2.3" xref="S3.SS4.SSS1.p1.1.m1.2.2.1.1.1.1.2.3.cmml">i</mi><mrow id="S3.SS4.SSS1.p1.1.m1.1.1.1.3" xref="S3.SS4.SSS1.p1.1.m1.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS4.SSS1.p1.1.m1.1.1.1.3.1" xref="S3.SS4.SSS1.p1.1.m1.2.2.1.1.1.1.cmml">(</mo><mn id="S3.SS4.SSS1.p1.1.m1.1.1.1.1" xref="S3.SS4.SSS1.p1.1.m1.1.1.1.1.cmml">3</mn><mo stretchy="false" id="S3.SS4.SSS1.p1.1.m1.1.1.1.3.2" xref="S3.SS4.SSS1.p1.1.m1.2.2.1.1.1.1.cmml">)</mo></mrow></msubsup><mo stretchy="false" id="S3.SS4.SSS1.p1.1.m1.2.2.1.1.1.3" xref="S3.SS4.SSS1.p1.1.m1.2.2.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS4.SSS1.p1.1.m1.2.2.1.3" xref="S3.SS4.SSS1.p1.1.m1.2.2.1.3.cmml"><mi id="S3.SS4.SSS1.p1.1.m1.2.2.1.3.2" xref="S3.SS4.SSS1.p1.1.m1.2.2.1.3.2.cmml">i</mi><mo id="S3.SS4.SSS1.p1.1.m1.2.2.1.3.1" xref="S3.SS4.SSS1.p1.1.m1.2.2.1.3.1.cmml">=</mo><mn id="S3.SS4.SSS1.p1.1.m1.2.2.1.3.3" xref="S3.SS4.SSS1.p1.1.m1.2.2.1.3.3.cmml">1</mn></mrow><mi id="S3.SS4.SSS1.p1.1.m1.2.2.3" xref="S3.SS4.SSS1.p1.1.m1.2.2.3.cmml">N</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p1.1.m1.2b"><apply id="S3.SS4.SSS1.p1.1.m1.2.2.cmml" xref="S3.SS4.SSS1.p1.1.m1.2.2"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p1.1.m1.2.2.2.cmml" xref="S3.SS4.SSS1.p1.1.m1.2.2">superscript</csymbol><apply id="S3.SS4.SSS1.p1.1.m1.2.2.1.cmml" xref="S3.SS4.SSS1.p1.1.m1.2.2"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p1.1.m1.2.2.1.2.cmml" xref="S3.SS4.SSS1.p1.1.m1.2.2">subscript</csymbol><set id="S3.SS4.SSS1.p1.1.m1.2.2.1.1.2.cmml" xref="S3.SS4.SSS1.p1.1.m1.2.2.1.1.1"><apply id="S3.SS4.SSS1.p1.1.m1.2.2.1.1.1.1.cmml" xref="S3.SS4.SSS1.p1.1.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p1.1.m1.2.2.1.1.1.1.1.cmml" xref="S3.SS4.SSS1.p1.1.m1.2.2.1.1.1.1">superscript</csymbol><apply id="S3.SS4.SSS1.p1.1.m1.2.2.1.1.1.1.2.cmml" xref="S3.SS4.SSS1.p1.1.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p1.1.m1.2.2.1.1.1.1.2.1.cmml" xref="S3.SS4.SSS1.p1.1.m1.2.2.1.1.1.1">subscript</csymbol><ci id="S3.SS4.SSS1.p1.1.m1.2.2.1.1.1.1.2.2.cmml" xref="S3.SS4.SSS1.p1.1.m1.2.2.1.1.1.1.2.2">𝐱</ci><ci id="S3.SS4.SSS1.p1.1.m1.2.2.1.1.1.1.2.3.cmml" xref="S3.SS4.SSS1.p1.1.m1.2.2.1.1.1.1.2.3">𝑖</ci></apply><cn type="integer" id="S3.SS4.SSS1.p1.1.m1.1.1.1.1.cmml" xref="S3.SS4.SSS1.p1.1.m1.1.1.1.1">3</cn></apply></set><apply id="S3.SS4.SSS1.p1.1.m1.2.2.1.3.cmml" xref="S3.SS4.SSS1.p1.1.m1.2.2.1.3"><eq id="S3.SS4.SSS1.p1.1.m1.2.2.1.3.1.cmml" xref="S3.SS4.SSS1.p1.1.m1.2.2.1.3.1"></eq><ci id="S3.SS4.SSS1.p1.1.m1.2.2.1.3.2.cmml" xref="S3.SS4.SSS1.p1.1.m1.2.2.1.3.2">𝑖</ci><cn type="integer" id="S3.SS4.SSS1.p1.1.m1.2.2.1.3.3.cmml" xref="S3.SS4.SSS1.p1.1.m1.2.2.1.3.3">1</cn></apply></apply><ci id="S3.SS4.SSS1.p1.1.m1.2.2.3.cmml" xref="S3.SS4.SSS1.p1.1.m1.2.2.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p1.1.m1.2c">\{\mathbf{x}_{i}^{(3)}\}_{i=1}^{N}</annotation></semantics></math>, their corresponding 2D image coordinates <math id="S3.SS4.SSS1.p1.2.m2.2" class="ltx_Math" alttext="\{\mathbf{x}_{i}^{(2)}\}_{i=1}^{N}" display="inline"><semantics id="S3.SS4.SSS1.p1.2.m2.2a"><msubsup id="S3.SS4.SSS1.p1.2.m2.2.2" xref="S3.SS4.SSS1.p1.2.m2.2.2.cmml"><mrow id="S3.SS4.SSS1.p1.2.m2.2.2.1.1.1" xref="S3.SS4.SSS1.p1.2.m2.2.2.1.1.2.cmml"><mo stretchy="false" id="S3.SS4.SSS1.p1.2.m2.2.2.1.1.1.2" xref="S3.SS4.SSS1.p1.2.m2.2.2.1.1.2.cmml">{</mo><msubsup id="S3.SS4.SSS1.p1.2.m2.2.2.1.1.1.1" xref="S3.SS4.SSS1.p1.2.m2.2.2.1.1.1.1.cmml"><mi id="S3.SS4.SSS1.p1.2.m2.2.2.1.1.1.1.2.2" xref="S3.SS4.SSS1.p1.2.m2.2.2.1.1.1.1.2.2.cmml">𝐱</mi><mi id="S3.SS4.SSS1.p1.2.m2.2.2.1.1.1.1.2.3" xref="S3.SS4.SSS1.p1.2.m2.2.2.1.1.1.1.2.3.cmml">i</mi><mrow id="S3.SS4.SSS1.p1.2.m2.1.1.1.3" xref="S3.SS4.SSS1.p1.2.m2.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS4.SSS1.p1.2.m2.1.1.1.3.1" xref="S3.SS4.SSS1.p1.2.m2.2.2.1.1.1.1.cmml">(</mo><mn id="S3.SS4.SSS1.p1.2.m2.1.1.1.1" xref="S3.SS4.SSS1.p1.2.m2.1.1.1.1.cmml">2</mn><mo stretchy="false" id="S3.SS4.SSS1.p1.2.m2.1.1.1.3.2" xref="S3.SS4.SSS1.p1.2.m2.2.2.1.1.1.1.cmml">)</mo></mrow></msubsup><mo stretchy="false" id="S3.SS4.SSS1.p1.2.m2.2.2.1.1.1.3" xref="S3.SS4.SSS1.p1.2.m2.2.2.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS4.SSS1.p1.2.m2.2.2.1.3" xref="S3.SS4.SSS1.p1.2.m2.2.2.1.3.cmml"><mi id="S3.SS4.SSS1.p1.2.m2.2.2.1.3.2" xref="S3.SS4.SSS1.p1.2.m2.2.2.1.3.2.cmml">i</mi><mo id="S3.SS4.SSS1.p1.2.m2.2.2.1.3.1" xref="S3.SS4.SSS1.p1.2.m2.2.2.1.3.1.cmml">=</mo><mn id="S3.SS4.SSS1.p1.2.m2.2.2.1.3.3" xref="S3.SS4.SSS1.p1.2.m2.2.2.1.3.3.cmml">1</mn></mrow><mi id="S3.SS4.SSS1.p1.2.m2.2.2.3" xref="S3.SS4.SSS1.p1.2.m2.2.2.3.cmml">N</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p1.2.m2.2b"><apply id="S3.SS4.SSS1.p1.2.m2.2.2.cmml" xref="S3.SS4.SSS1.p1.2.m2.2.2"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p1.2.m2.2.2.2.cmml" xref="S3.SS4.SSS1.p1.2.m2.2.2">superscript</csymbol><apply id="S3.SS4.SSS1.p1.2.m2.2.2.1.cmml" xref="S3.SS4.SSS1.p1.2.m2.2.2"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p1.2.m2.2.2.1.2.cmml" xref="S3.SS4.SSS1.p1.2.m2.2.2">subscript</csymbol><set id="S3.SS4.SSS1.p1.2.m2.2.2.1.1.2.cmml" xref="S3.SS4.SSS1.p1.2.m2.2.2.1.1.1"><apply id="S3.SS4.SSS1.p1.2.m2.2.2.1.1.1.1.cmml" xref="S3.SS4.SSS1.p1.2.m2.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p1.2.m2.2.2.1.1.1.1.1.cmml" xref="S3.SS4.SSS1.p1.2.m2.2.2.1.1.1.1">superscript</csymbol><apply id="S3.SS4.SSS1.p1.2.m2.2.2.1.1.1.1.2.cmml" xref="S3.SS4.SSS1.p1.2.m2.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p1.2.m2.2.2.1.1.1.1.2.1.cmml" xref="S3.SS4.SSS1.p1.2.m2.2.2.1.1.1.1">subscript</csymbol><ci id="S3.SS4.SSS1.p1.2.m2.2.2.1.1.1.1.2.2.cmml" xref="S3.SS4.SSS1.p1.2.m2.2.2.1.1.1.1.2.2">𝐱</ci><ci id="S3.SS4.SSS1.p1.2.m2.2.2.1.1.1.1.2.3.cmml" xref="S3.SS4.SSS1.p1.2.m2.2.2.1.1.1.1.2.3">𝑖</ci></apply><cn type="integer" id="S3.SS4.SSS1.p1.2.m2.1.1.1.1.cmml" xref="S3.SS4.SSS1.p1.2.m2.1.1.1.1">2</cn></apply></set><apply id="S3.SS4.SSS1.p1.2.m2.2.2.1.3.cmml" xref="S3.SS4.SSS1.p1.2.m2.2.2.1.3"><eq id="S3.SS4.SSS1.p1.2.m2.2.2.1.3.1.cmml" xref="S3.SS4.SSS1.p1.2.m2.2.2.1.3.1"></eq><ci id="S3.SS4.SSS1.p1.2.m2.2.2.1.3.2.cmml" xref="S3.SS4.SSS1.p1.2.m2.2.2.1.3.2">𝑖</ci><cn type="integer" id="S3.SS4.SSS1.p1.2.m2.2.2.1.3.3.cmml" xref="S3.SS4.SSS1.p1.2.m2.2.2.1.3.3">1</cn></apply></apply><ci id="S3.SS4.SSS1.p1.2.m2.2.2.3.cmml" xref="S3.SS4.SSS1.p1.2.m2.2.2.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p1.2.m2.2c">\{\mathbf{x}_{i}^{(2)}\}_{i=1}^{N}</annotation></semantics></math>, and 2D ground truth keypoints <math id="S3.SS4.SSS1.p1.3.m3.2" class="ltx_Math" alttext="\{\mathbf{y}_{k}^{(2)}\}_{k=1}^{K}" display="inline"><semantics id="S3.SS4.SSS1.p1.3.m3.2a"><msubsup id="S3.SS4.SSS1.p1.3.m3.2.2" xref="S3.SS4.SSS1.p1.3.m3.2.2.cmml"><mrow id="S3.SS4.SSS1.p1.3.m3.2.2.1.1.1" xref="S3.SS4.SSS1.p1.3.m3.2.2.1.1.2.cmml"><mo stretchy="false" id="S3.SS4.SSS1.p1.3.m3.2.2.1.1.1.2" xref="S3.SS4.SSS1.p1.3.m3.2.2.1.1.2.cmml">{</mo><msubsup id="S3.SS4.SSS1.p1.3.m3.2.2.1.1.1.1" xref="S3.SS4.SSS1.p1.3.m3.2.2.1.1.1.1.cmml"><mi id="S3.SS4.SSS1.p1.3.m3.2.2.1.1.1.1.2.2" xref="S3.SS4.SSS1.p1.3.m3.2.2.1.1.1.1.2.2.cmml">𝐲</mi><mi id="S3.SS4.SSS1.p1.3.m3.2.2.1.1.1.1.2.3" xref="S3.SS4.SSS1.p1.3.m3.2.2.1.1.1.1.2.3.cmml">k</mi><mrow id="S3.SS4.SSS1.p1.3.m3.1.1.1.3" xref="S3.SS4.SSS1.p1.3.m3.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS4.SSS1.p1.3.m3.1.1.1.3.1" xref="S3.SS4.SSS1.p1.3.m3.2.2.1.1.1.1.cmml">(</mo><mn id="S3.SS4.SSS1.p1.3.m3.1.1.1.1" xref="S3.SS4.SSS1.p1.3.m3.1.1.1.1.cmml">2</mn><mo stretchy="false" id="S3.SS4.SSS1.p1.3.m3.1.1.1.3.2" xref="S3.SS4.SSS1.p1.3.m3.2.2.1.1.1.1.cmml">)</mo></mrow></msubsup><mo stretchy="false" id="S3.SS4.SSS1.p1.3.m3.2.2.1.1.1.3" xref="S3.SS4.SSS1.p1.3.m3.2.2.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS4.SSS1.p1.3.m3.2.2.1.3" xref="S3.SS4.SSS1.p1.3.m3.2.2.1.3.cmml"><mi id="S3.SS4.SSS1.p1.3.m3.2.2.1.3.2" xref="S3.SS4.SSS1.p1.3.m3.2.2.1.3.2.cmml">k</mi><mo id="S3.SS4.SSS1.p1.3.m3.2.2.1.3.1" xref="S3.SS4.SSS1.p1.3.m3.2.2.1.3.1.cmml">=</mo><mn id="S3.SS4.SSS1.p1.3.m3.2.2.1.3.3" xref="S3.SS4.SSS1.p1.3.m3.2.2.1.3.3.cmml">1</mn></mrow><mi id="S3.SS4.SSS1.p1.3.m3.2.2.3" xref="S3.SS4.SSS1.p1.3.m3.2.2.3.cmml">K</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p1.3.m3.2b"><apply id="S3.SS4.SSS1.p1.3.m3.2.2.cmml" xref="S3.SS4.SSS1.p1.3.m3.2.2"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p1.3.m3.2.2.2.cmml" xref="S3.SS4.SSS1.p1.3.m3.2.2">superscript</csymbol><apply id="S3.SS4.SSS1.p1.3.m3.2.2.1.cmml" xref="S3.SS4.SSS1.p1.3.m3.2.2"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p1.3.m3.2.2.1.2.cmml" xref="S3.SS4.SSS1.p1.3.m3.2.2">subscript</csymbol><set id="S3.SS4.SSS1.p1.3.m3.2.2.1.1.2.cmml" xref="S3.SS4.SSS1.p1.3.m3.2.2.1.1.1"><apply id="S3.SS4.SSS1.p1.3.m3.2.2.1.1.1.1.cmml" xref="S3.SS4.SSS1.p1.3.m3.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p1.3.m3.2.2.1.1.1.1.1.cmml" xref="S3.SS4.SSS1.p1.3.m3.2.2.1.1.1.1">superscript</csymbol><apply id="S3.SS4.SSS1.p1.3.m3.2.2.1.1.1.1.2.cmml" xref="S3.SS4.SSS1.p1.3.m3.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p1.3.m3.2.2.1.1.1.1.2.1.cmml" xref="S3.SS4.SSS1.p1.3.m3.2.2.1.1.1.1">subscript</csymbol><ci id="S3.SS4.SSS1.p1.3.m3.2.2.1.1.1.1.2.2.cmml" xref="S3.SS4.SSS1.p1.3.m3.2.2.1.1.1.1.2.2">𝐲</ci><ci id="S3.SS4.SSS1.p1.3.m3.2.2.1.1.1.1.2.3.cmml" xref="S3.SS4.SSS1.p1.3.m3.2.2.1.1.1.1.2.3">𝑘</ci></apply><cn type="integer" id="S3.SS4.SSS1.p1.3.m3.1.1.1.1.cmml" xref="S3.SS4.SSS1.p1.3.m3.1.1.1.1">2</cn></apply></set><apply id="S3.SS4.SSS1.p1.3.m3.2.2.1.3.cmml" xref="S3.SS4.SSS1.p1.3.m3.2.2.1.3"><eq id="S3.SS4.SSS1.p1.3.m3.2.2.1.3.1.cmml" xref="S3.SS4.SSS1.p1.3.m3.2.2.1.3.1"></eq><ci id="S3.SS4.SSS1.p1.3.m3.2.2.1.3.2.cmml" xref="S3.SS4.SSS1.p1.3.m3.2.2.1.3.2">𝑘</ci><cn type="integer" id="S3.SS4.SSS1.p1.3.m3.2.2.1.3.3.cmml" xref="S3.SS4.SSS1.p1.3.m3.2.2.1.3.3">1</cn></apply></apply><ci id="S3.SS4.SSS1.p1.3.m3.2.2.3.cmml" xref="S3.SS4.SSS1.p1.3.m3.2.2.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p1.3.m3.2c">\{\mathbf{y}_{k}^{(2)}\}_{k=1}^{K}</annotation></semantics></math>. The correspondence is pre-computed by projecting 3D points onto the camera image coordinates according to the camera model. Since the projection is not a one-to-one mapping, directly back-projecting 2D labels to 3D space is impossible.</p>
</div>
<div id="S3.SS4.SSS1.p2" class="ltx_para">
<p id="S3.SS4.SSS1.p2.1" class="ltx_p">To generate 3D keypoint labels from the 2D labels and the point cloud, we make the following assumptions:</p>
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">the point cloud is dense enough so that there is at least one point in the neighborhood of each keypoint in 2D space;</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">the human surface is smooth enough so that the depth does not rapidly change in the neighborhood of a keypoint;</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">point cloud to camera registration is reliable.</p>
</div>
</li>
</ol>
<p id="S3.SS4.SSS1.p2.2" class="ltx_p">Though point clouds will be downsampled to a fixed size before being fed into the point network, pseudo 3D labels are generated based on the point cloud before downsampling. Therefore, the above assumptions hold in most cases. Also, since LiDAR and camera are usually attached to the same rigid object (the vehicle) and are frequently calibrated, it is reasonable to assume that the registration is reliable.</p>
</div>
<div id="S3.SS4.SSS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS4.SSS1.p3.6" class="ltx_p"><span id="S3.SS4.SSS1.p3.6.1" class="ltx_text ltx_font_bold">3D Keypoint Coordinates Label Generation:</span>
Based on our assumption, for each point in the point cloud, its accurate 2D projection on the camera image is known. Therefore, for a ground truth keypoint in 2D coordinates, we can first find its neighboring points in 2D space. Then, based on our assumptions, the depths of these points will be close enough to the true depth of the keypoint. As Figure <a href="#S3.F4" title="Figure 4 ‣ 3.2 Modality Fusion of LiDAR and Camera ‣ 3 Method ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we use the average 3D coordinates of these neighboring points to approximate the coordinates of the keypoint,</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.12" class="ltx_Math" alttext="\tilde{\mathbf{y}}_{k}^{(3)}=\sum_{i=1}^{N}\alpha_{ik}\mathbf{x}_{i}^{(3)},\;\alpha_{ik}=\frac{\exp\left(-T\|\mathbf{x}_{i}^{(2)}-\mathbf{y}_{k}^{(2)}\|_{2}^{2}\right)}{\sum_{j=1}^{N}\exp\left(-T\|\mathbf{x}_{j}^{(2)}-\mathbf{y}_{k}^{(2)}\|_{2}^{2}\right)}" display="block"><semantics id="S3.E1.m1.12a"><mrow id="S3.E1.m1.12.12.2" xref="S3.E1.m1.12.12.3.cmml"><mrow id="S3.E1.m1.11.11.1.1" xref="S3.E1.m1.11.11.1.1.cmml"><msubsup id="S3.E1.m1.11.11.1.1.2" xref="S3.E1.m1.11.11.1.1.2.cmml"><mover accent="true" id="S3.E1.m1.11.11.1.1.2.2.2" xref="S3.E1.m1.11.11.1.1.2.2.2.cmml"><mi id="S3.E1.m1.11.11.1.1.2.2.2.2" xref="S3.E1.m1.11.11.1.1.2.2.2.2.cmml">𝐲</mi><mo id="S3.E1.m1.11.11.1.1.2.2.2.1" xref="S3.E1.m1.11.11.1.1.2.2.2.1.cmml">~</mo></mover><mi id="S3.E1.m1.11.11.1.1.2.2.3" xref="S3.E1.m1.11.11.1.1.2.2.3.cmml">k</mi><mrow id="S3.E1.m1.1.1.1.3" xref="S3.E1.m1.11.11.1.1.2.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.3.1" xref="S3.E1.m1.11.11.1.1.2.cmml">(</mo><mn id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml">3</mn><mo stretchy="false" id="S3.E1.m1.1.1.1.3.2" xref="S3.E1.m1.11.11.1.1.2.cmml">)</mo></mrow></msubsup><mo rspace="0.111em" id="S3.E1.m1.11.11.1.1.1" xref="S3.E1.m1.11.11.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.11.11.1.1.3" xref="S3.E1.m1.11.11.1.1.3.cmml"><munderover id="S3.E1.m1.11.11.1.1.3.1" xref="S3.E1.m1.11.11.1.1.3.1.cmml"><mo movablelimits="false" id="S3.E1.m1.11.11.1.1.3.1.2.2" xref="S3.E1.m1.11.11.1.1.3.1.2.2.cmml">∑</mo><mrow id="S3.E1.m1.11.11.1.1.3.1.2.3" xref="S3.E1.m1.11.11.1.1.3.1.2.3.cmml"><mi id="S3.E1.m1.11.11.1.1.3.1.2.3.2" xref="S3.E1.m1.11.11.1.1.3.1.2.3.2.cmml">i</mi><mo id="S3.E1.m1.11.11.1.1.3.1.2.3.1" xref="S3.E1.m1.11.11.1.1.3.1.2.3.1.cmml">=</mo><mn id="S3.E1.m1.11.11.1.1.3.1.2.3.3" xref="S3.E1.m1.11.11.1.1.3.1.2.3.3.cmml">1</mn></mrow><mi id="S3.E1.m1.11.11.1.1.3.1.3" xref="S3.E1.m1.11.11.1.1.3.1.3.cmml">N</mi></munderover><mrow id="S3.E1.m1.11.11.1.1.3.2" xref="S3.E1.m1.11.11.1.1.3.2.cmml"><msub id="S3.E1.m1.11.11.1.1.3.2.2" xref="S3.E1.m1.11.11.1.1.3.2.2.cmml"><mi id="S3.E1.m1.11.11.1.1.3.2.2.2" xref="S3.E1.m1.11.11.1.1.3.2.2.2.cmml">α</mi><mrow id="S3.E1.m1.11.11.1.1.3.2.2.3" xref="S3.E1.m1.11.11.1.1.3.2.2.3.cmml"><mi id="S3.E1.m1.11.11.1.1.3.2.2.3.2" xref="S3.E1.m1.11.11.1.1.3.2.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.11.11.1.1.3.2.2.3.1" xref="S3.E1.m1.11.11.1.1.3.2.2.3.1.cmml">​</mo><mi id="S3.E1.m1.11.11.1.1.3.2.2.3.3" xref="S3.E1.m1.11.11.1.1.3.2.2.3.3.cmml">k</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.11.11.1.1.3.2.1" xref="S3.E1.m1.11.11.1.1.3.2.1.cmml">​</mo><msubsup id="S3.E1.m1.11.11.1.1.3.2.3" xref="S3.E1.m1.11.11.1.1.3.2.3.cmml"><mi id="S3.E1.m1.11.11.1.1.3.2.3.2.2" xref="S3.E1.m1.11.11.1.1.3.2.3.2.2.cmml">𝐱</mi><mi id="S3.E1.m1.11.11.1.1.3.2.3.2.3" xref="S3.E1.m1.11.11.1.1.3.2.3.2.3.cmml">i</mi><mrow id="S3.E1.m1.2.2.1.3" xref="S3.E1.m1.11.11.1.1.3.2.3.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.1.3.1" xref="S3.E1.m1.11.11.1.1.3.2.3.cmml">(</mo><mn id="S3.E1.m1.2.2.1.1" xref="S3.E1.m1.2.2.1.1.cmml">3</mn><mo stretchy="false" id="S3.E1.m1.2.2.1.3.2" xref="S3.E1.m1.11.11.1.1.3.2.3.cmml">)</mo></mrow></msubsup></mrow></mrow></mrow><mo rspace="0.447em" id="S3.E1.m1.12.12.2.3" xref="S3.E1.m1.12.12.3a.cmml">,</mo><mrow id="S3.E1.m1.12.12.2.2" xref="S3.E1.m1.12.12.2.2.cmml"><msub id="S3.E1.m1.12.12.2.2.2" xref="S3.E1.m1.12.12.2.2.2.cmml"><mi id="S3.E1.m1.12.12.2.2.2.2" xref="S3.E1.m1.12.12.2.2.2.2.cmml">α</mi><mrow id="S3.E1.m1.12.12.2.2.2.3" xref="S3.E1.m1.12.12.2.2.2.3.cmml"><mi id="S3.E1.m1.12.12.2.2.2.3.2" xref="S3.E1.m1.12.12.2.2.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.12.12.2.2.2.3.1" xref="S3.E1.m1.12.12.2.2.2.3.1.cmml">​</mo><mi id="S3.E1.m1.12.12.2.2.2.3.3" xref="S3.E1.m1.12.12.2.2.2.3.3.cmml">k</mi></mrow></msub><mo id="S3.E1.m1.12.12.2.2.1" xref="S3.E1.m1.12.12.2.2.1.cmml">=</mo><mfrac id="S3.E1.m1.10.10" xref="S3.E1.m1.10.10.cmml"><mrow id="S3.E1.m1.6.6.4.4" xref="S3.E1.m1.6.6.4.5.cmml"><mi id="S3.E1.m1.5.5.3.3" xref="S3.E1.m1.5.5.3.3.cmml">exp</mi><mo id="S3.E1.m1.6.6.4.4a" xref="S3.E1.m1.6.6.4.5.cmml">⁡</mo><mrow id="S3.E1.m1.6.6.4.4.1" xref="S3.E1.m1.6.6.4.5.cmml"><mo id="S3.E1.m1.6.6.4.4.1.2" xref="S3.E1.m1.6.6.4.5.cmml">(</mo><mrow id="S3.E1.m1.6.6.4.4.1.1" xref="S3.E1.m1.6.6.4.4.1.1.cmml"><mo id="S3.E1.m1.6.6.4.4.1.1a" xref="S3.E1.m1.6.6.4.4.1.1.cmml">−</mo><mrow id="S3.E1.m1.6.6.4.4.1.1.1" xref="S3.E1.m1.6.6.4.4.1.1.1.cmml"><mi id="S3.E1.m1.6.6.4.4.1.1.1.3" xref="S3.E1.m1.6.6.4.4.1.1.1.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.6.6.4.4.1.1.1.2" xref="S3.E1.m1.6.6.4.4.1.1.1.2.cmml">​</mo><msubsup id="S3.E1.m1.6.6.4.4.1.1.1.1" xref="S3.E1.m1.6.6.4.4.1.1.1.1.cmml"><mrow id="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1" xref="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.2" xref="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1" xref="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.cmml"><msubsup id="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.2.2.2.cmml">𝐱</mi><mi id="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.2.2.3" xref="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.2.2.3.cmml">i</mi><mrow id="S3.E1.m1.3.3.1.1.1.3" xref="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E1.m1.3.3.1.1.1.3.1" xref="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.2.cmml">(</mo><mn id="S3.E1.m1.3.3.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.cmml">2</mn><mo stretchy="false" id="S3.E1.m1.3.3.1.1.1.3.2" xref="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></msubsup><mo id="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.1.cmml">−</mo><msubsup id="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.3.2.2" xref="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.3.2.2.cmml">𝐲</mi><mi id="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.3.2.3" xref="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.3.2.3.cmml">k</mi><mrow id="S3.E1.m1.4.4.2.2.1.3" xref="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.3.cmml"><mo stretchy="false" id="S3.E1.m1.4.4.2.2.1.3.1" xref="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.3.cmml">(</mo><mn id="S3.E1.m1.4.4.2.2.1.1" xref="S3.E1.m1.4.4.2.2.1.1.cmml">2</mn><mo stretchy="false" id="S3.E1.m1.4.4.2.2.1.3.2" xref="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.3.cmml">)</mo></mrow></msubsup></mrow><mo stretchy="false" id="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.3" xref="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S3.E1.m1.6.6.4.4.1.1.1.1.1.3" xref="S3.E1.m1.6.6.4.4.1.1.1.1.1.3.cmml">2</mn><mn id="S3.E1.m1.6.6.4.4.1.1.1.1.3" xref="S3.E1.m1.6.6.4.4.1.1.1.1.3.cmml">2</mn></msubsup></mrow></mrow><mo id="S3.E1.m1.6.6.4.4.1.3" xref="S3.E1.m1.6.6.4.5.cmml">)</mo></mrow></mrow><mrow id="S3.E1.m1.10.10.8" xref="S3.E1.m1.10.10.8.cmml"><msubsup id="S3.E1.m1.10.10.8.5" xref="S3.E1.m1.10.10.8.5.cmml"><mo id="S3.E1.m1.10.10.8.5.2.2" xref="S3.E1.m1.10.10.8.5.2.2.cmml">∑</mo><mrow id="S3.E1.m1.10.10.8.5.2.3" xref="S3.E1.m1.10.10.8.5.2.3.cmml"><mi id="S3.E1.m1.10.10.8.5.2.3.2" xref="S3.E1.m1.10.10.8.5.2.3.2.cmml">j</mi><mo id="S3.E1.m1.10.10.8.5.2.3.1" xref="S3.E1.m1.10.10.8.5.2.3.1.cmml">=</mo><mn id="S3.E1.m1.10.10.8.5.2.3.3" xref="S3.E1.m1.10.10.8.5.2.3.3.cmml">1</mn></mrow><mi id="S3.E1.m1.10.10.8.5.3" xref="S3.E1.m1.10.10.8.5.3.cmml">N</mi></msubsup><mrow id="S3.E1.m1.10.10.8.4.1" xref="S3.E1.m1.10.10.8.4.2.cmml"><mi id="S3.E1.m1.9.9.7.3" xref="S3.E1.m1.9.9.7.3.cmml">exp</mi><mo id="S3.E1.m1.10.10.8.4.1a" xref="S3.E1.m1.10.10.8.4.2.cmml">⁡</mo><mrow id="S3.E1.m1.10.10.8.4.1.1" xref="S3.E1.m1.10.10.8.4.2.cmml"><mo id="S3.E1.m1.10.10.8.4.1.1.2" xref="S3.E1.m1.10.10.8.4.2.cmml">(</mo><mrow id="S3.E1.m1.10.10.8.4.1.1.1" xref="S3.E1.m1.10.10.8.4.1.1.1.cmml"><mo id="S3.E1.m1.10.10.8.4.1.1.1a" xref="S3.E1.m1.10.10.8.4.1.1.1.cmml">−</mo><mrow id="S3.E1.m1.10.10.8.4.1.1.1.1" xref="S3.E1.m1.10.10.8.4.1.1.1.1.cmml"><mi id="S3.E1.m1.10.10.8.4.1.1.1.1.3" xref="S3.E1.m1.10.10.8.4.1.1.1.1.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.10.10.8.4.1.1.1.1.2" xref="S3.E1.m1.10.10.8.4.1.1.1.1.2.cmml">​</mo><msubsup id="S3.E1.m1.10.10.8.4.1.1.1.1.1" xref="S3.E1.m1.10.10.8.4.1.1.1.1.1.cmml"><mrow id="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1" xref="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.cmml"><msubsup id="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.2.2.2.cmml">𝐱</mi><mi id="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.2.2.3" xref="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.2.2.3.cmml">j</mi><mrow id="S3.E1.m1.7.7.5.1.1.3" xref="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E1.m1.7.7.5.1.1.3.1" xref="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.2.cmml">(</mo><mn id="S3.E1.m1.7.7.5.1.1.1" xref="S3.E1.m1.7.7.5.1.1.1.cmml">2</mn><mo stretchy="false" id="S3.E1.m1.7.7.5.1.1.3.2" xref="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></msubsup><mo id="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.1.cmml">−</mo><msubsup id="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.3.2.2" xref="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.3.2.2.cmml">𝐲</mi><mi id="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.3.2.3" xref="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.3.2.3.cmml">k</mi><mrow id="S3.E1.m1.8.8.6.2.1.3" xref="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.3.cmml"><mo stretchy="false" id="S3.E1.m1.8.8.6.2.1.3.1" xref="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.3.cmml">(</mo><mn id="S3.E1.m1.8.8.6.2.1.1" xref="S3.E1.m1.8.8.6.2.1.1.cmml">2</mn><mo stretchy="false" id="S3.E1.m1.8.8.6.2.1.3.2" xref="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.3.cmml">)</mo></mrow></msubsup></mrow><mo stretchy="false" id="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.3" xref="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.3.cmml">2</mn><mn id="S3.E1.m1.10.10.8.4.1.1.1.1.1.3" xref="S3.E1.m1.10.10.8.4.1.1.1.1.1.3.cmml">2</mn></msubsup></mrow></mrow><mo id="S3.E1.m1.10.10.8.4.1.1.3" xref="S3.E1.m1.10.10.8.4.2.cmml">)</mo></mrow></mrow></mrow></mfrac></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.12b"><apply id="S3.E1.m1.12.12.3.cmml" xref="S3.E1.m1.12.12.2"><csymbol cd="ambiguous" id="S3.E1.m1.12.12.3a.cmml" xref="S3.E1.m1.12.12.2.3">formulae-sequence</csymbol><apply id="S3.E1.m1.11.11.1.1.cmml" xref="S3.E1.m1.11.11.1.1"><eq id="S3.E1.m1.11.11.1.1.1.cmml" xref="S3.E1.m1.11.11.1.1.1"></eq><apply id="S3.E1.m1.11.11.1.1.2.cmml" xref="S3.E1.m1.11.11.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.11.11.1.1.2.1.cmml" xref="S3.E1.m1.11.11.1.1.2">superscript</csymbol><apply id="S3.E1.m1.11.11.1.1.2.2.cmml" xref="S3.E1.m1.11.11.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.11.11.1.1.2.2.1.cmml" xref="S3.E1.m1.11.11.1.1.2">subscript</csymbol><apply id="S3.E1.m1.11.11.1.1.2.2.2.cmml" xref="S3.E1.m1.11.11.1.1.2.2.2"><ci id="S3.E1.m1.11.11.1.1.2.2.2.1.cmml" xref="S3.E1.m1.11.11.1.1.2.2.2.1">~</ci><ci id="S3.E1.m1.11.11.1.1.2.2.2.2.cmml" xref="S3.E1.m1.11.11.1.1.2.2.2.2">𝐲</ci></apply><ci id="S3.E1.m1.11.11.1.1.2.2.3.cmml" xref="S3.E1.m1.11.11.1.1.2.2.3">𝑘</ci></apply><cn type="integer" id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1">3</cn></apply><apply id="S3.E1.m1.11.11.1.1.3.cmml" xref="S3.E1.m1.11.11.1.1.3"><apply id="S3.E1.m1.11.11.1.1.3.1.cmml" xref="S3.E1.m1.11.11.1.1.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.11.11.1.1.3.1.1.cmml" xref="S3.E1.m1.11.11.1.1.3.1">superscript</csymbol><apply id="S3.E1.m1.11.11.1.1.3.1.2.cmml" xref="S3.E1.m1.11.11.1.1.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.11.11.1.1.3.1.2.1.cmml" xref="S3.E1.m1.11.11.1.1.3.1">subscript</csymbol><sum id="S3.E1.m1.11.11.1.1.3.1.2.2.cmml" xref="S3.E1.m1.11.11.1.1.3.1.2.2"></sum><apply id="S3.E1.m1.11.11.1.1.3.1.2.3.cmml" xref="S3.E1.m1.11.11.1.1.3.1.2.3"><eq id="S3.E1.m1.11.11.1.1.3.1.2.3.1.cmml" xref="S3.E1.m1.11.11.1.1.3.1.2.3.1"></eq><ci id="S3.E1.m1.11.11.1.1.3.1.2.3.2.cmml" xref="S3.E1.m1.11.11.1.1.3.1.2.3.2">𝑖</ci><cn type="integer" id="S3.E1.m1.11.11.1.1.3.1.2.3.3.cmml" xref="S3.E1.m1.11.11.1.1.3.1.2.3.3">1</cn></apply></apply><ci id="S3.E1.m1.11.11.1.1.3.1.3.cmml" xref="S3.E1.m1.11.11.1.1.3.1.3">𝑁</ci></apply><apply id="S3.E1.m1.11.11.1.1.3.2.cmml" xref="S3.E1.m1.11.11.1.1.3.2"><times id="S3.E1.m1.11.11.1.1.3.2.1.cmml" xref="S3.E1.m1.11.11.1.1.3.2.1"></times><apply id="S3.E1.m1.11.11.1.1.3.2.2.cmml" xref="S3.E1.m1.11.11.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.11.11.1.1.3.2.2.1.cmml" xref="S3.E1.m1.11.11.1.1.3.2.2">subscript</csymbol><ci id="S3.E1.m1.11.11.1.1.3.2.2.2.cmml" xref="S3.E1.m1.11.11.1.1.3.2.2.2">𝛼</ci><apply id="S3.E1.m1.11.11.1.1.3.2.2.3.cmml" xref="S3.E1.m1.11.11.1.1.3.2.2.3"><times id="S3.E1.m1.11.11.1.1.3.2.2.3.1.cmml" xref="S3.E1.m1.11.11.1.1.3.2.2.3.1"></times><ci id="S3.E1.m1.11.11.1.1.3.2.2.3.2.cmml" xref="S3.E1.m1.11.11.1.1.3.2.2.3.2">𝑖</ci><ci id="S3.E1.m1.11.11.1.1.3.2.2.3.3.cmml" xref="S3.E1.m1.11.11.1.1.3.2.2.3.3">𝑘</ci></apply></apply><apply id="S3.E1.m1.11.11.1.1.3.2.3.cmml" xref="S3.E1.m1.11.11.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.11.11.1.1.3.2.3.1.cmml" xref="S3.E1.m1.11.11.1.1.3.2.3">superscript</csymbol><apply id="S3.E1.m1.11.11.1.1.3.2.3.2.cmml" xref="S3.E1.m1.11.11.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.11.11.1.1.3.2.3.2.1.cmml" xref="S3.E1.m1.11.11.1.1.3.2.3">subscript</csymbol><ci id="S3.E1.m1.11.11.1.1.3.2.3.2.2.cmml" xref="S3.E1.m1.11.11.1.1.3.2.3.2.2">𝐱</ci><ci id="S3.E1.m1.11.11.1.1.3.2.3.2.3.cmml" xref="S3.E1.m1.11.11.1.1.3.2.3.2.3">𝑖</ci></apply><cn type="integer" id="S3.E1.m1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1.1">3</cn></apply></apply></apply></apply><apply id="S3.E1.m1.12.12.2.2.cmml" xref="S3.E1.m1.12.12.2.2"><eq id="S3.E1.m1.12.12.2.2.1.cmml" xref="S3.E1.m1.12.12.2.2.1"></eq><apply id="S3.E1.m1.12.12.2.2.2.cmml" xref="S3.E1.m1.12.12.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.12.12.2.2.2.1.cmml" xref="S3.E1.m1.12.12.2.2.2">subscript</csymbol><ci id="S3.E1.m1.12.12.2.2.2.2.cmml" xref="S3.E1.m1.12.12.2.2.2.2">𝛼</ci><apply id="S3.E1.m1.12.12.2.2.2.3.cmml" xref="S3.E1.m1.12.12.2.2.2.3"><times id="S3.E1.m1.12.12.2.2.2.3.1.cmml" xref="S3.E1.m1.12.12.2.2.2.3.1"></times><ci id="S3.E1.m1.12.12.2.2.2.3.2.cmml" xref="S3.E1.m1.12.12.2.2.2.3.2">𝑖</ci><ci id="S3.E1.m1.12.12.2.2.2.3.3.cmml" xref="S3.E1.m1.12.12.2.2.2.3.3">𝑘</ci></apply></apply><apply id="S3.E1.m1.10.10.cmml" xref="S3.E1.m1.10.10"><divide id="S3.E1.m1.10.10.9.cmml" xref="S3.E1.m1.10.10"></divide><apply id="S3.E1.m1.6.6.4.5.cmml" xref="S3.E1.m1.6.6.4.4"><exp id="S3.E1.m1.5.5.3.3.cmml" xref="S3.E1.m1.5.5.3.3"></exp><apply id="S3.E1.m1.6.6.4.4.1.1.cmml" xref="S3.E1.m1.6.6.4.4.1.1"><minus id="S3.E1.m1.6.6.4.4.1.1.2.cmml" xref="S3.E1.m1.6.6.4.4.1.1"></minus><apply id="S3.E1.m1.6.6.4.4.1.1.1.cmml" xref="S3.E1.m1.6.6.4.4.1.1.1"><times id="S3.E1.m1.6.6.4.4.1.1.1.2.cmml" xref="S3.E1.m1.6.6.4.4.1.1.1.2"></times><ci id="S3.E1.m1.6.6.4.4.1.1.1.3.cmml" xref="S3.E1.m1.6.6.4.4.1.1.1.3">𝑇</ci><apply id="S3.E1.m1.6.6.4.4.1.1.1.1.cmml" xref="S3.E1.m1.6.6.4.4.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.4.4.1.1.1.1.2.cmml" xref="S3.E1.m1.6.6.4.4.1.1.1.1">superscript</csymbol><apply id="S3.E1.m1.6.6.4.4.1.1.1.1.1.cmml" xref="S3.E1.m1.6.6.4.4.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.4.4.1.1.1.1.1.2.cmml" xref="S3.E1.m1.6.6.4.4.1.1.1.1">subscript</csymbol><apply id="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1"><minus id="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.1"></minus><apply id="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.2.2.2">𝐱</ci><ci id="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.2.2.3">𝑖</ci></apply><cn type="integer" id="S3.E1.m1.3.3.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1">2</cn></apply><apply id="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.3">superscript</csymbol><apply id="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.3.2.2">𝐲</ci><ci id="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E1.m1.6.6.4.4.1.1.1.1.1.1.1.1.3.2.3">𝑘</ci></apply><cn type="integer" id="S3.E1.m1.4.4.2.2.1.1.cmml" xref="S3.E1.m1.4.4.2.2.1.1">2</cn></apply></apply></apply><cn type="integer" id="S3.E1.m1.6.6.4.4.1.1.1.1.1.3.cmml" xref="S3.E1.m1.6.6.4.4.1.1.1.1.1.3">2</cn></apply><cn type="integer" id="S3.E1.m1.6.6.4.4.1.1.1.1.3.cmml" xref="S3.E1.m1.6.6.4.4.1.1.1.1.3">2</cn></apply></apply></apply></apply><apply id="S3.E1.m1.10.10.8.cmml" xref="S3.E1.m1.10.10.8"><apply id="S3.E1.m1.10.10.8.5.cmml" xref="S3.E1.m1.10.10.8.5"><csymbol cd="ambiguous" id="S3.E1.m1.10.10.8.5.1.cmml" xref="S3.E1.m1.10.10.8.5">superscript</csymbol><apply id="S3.E1.m1.10.10.8.5.2.cmml" xref="S3.E1.m1.10.10.8.5"><csymbol cd="ambiguous" id="S3.E1.m1.10.10.8.5.2.1.cmml" xref="S3.E1.m1.10.10.8.5">subscript</csymbol><sum id="S3.E1.m1.10.10.8.5.2.2.cmml" xref="S3.E1.m1.10.10.8.5.2.2"></sum><apply id="S3.E1.m1.10.10.8.5.2.3.cmml" xref="S3.E1.m1.10.10.8.5.2.3"><eq id="S3.E1.m1.10.10.8.5.2.3.1.cmml" xref="S3.E1.m1.10.10.8.5.2.3.1"></eq><ci id="S3.E1.m1.10.10.8.5.2.3.2.cmml" xref="S3.E1.m1.10.10.8.5.2.3.2">𝑗</ci><cn type="integer" id="S3.E1.m1.10.10.8.5.2.3.3.cmml" xref="S3.E1.m1.10.10.8.5.2.3.3">1</cn></apply></apply><ci id="S3.E1.m1.10.10.8.5.3.cmml" xref="S3.E1.m1.10.10.8.5.3">𝑁</ci></apply><apply id="S3.E1.m1.10.10.8.4.2.cmml" xref="S3.E1.m1.10.10.8.4.1"><exp id="S3.E1.m1.9.9.7.3.cmml" xref="S3.E1.m1.9.9.7.3"></exp><apply id="S3.E1.m1.10.10.8.4.1.1.1.cmml" xref="S3.E1.m1.10.10.8.4.1.1.1"><minus id="S3.E1.m1.10.10.8.4.1.1.1.2.cmml" xref="S3.E1.m1.10.10.8.4.1.1.1"></minus><apply id="S3.E1.m1.10.10.8.4.1.1.1.1.cmml" xref="S3.E1.m1.10.10.8.4.1.1.1.1"><times id="S3.E1.m1.10.10.8.4.1.1.1.1.2.cmml" xref="S3.E1.m1.10.10.8.4.1.1.1.1.2"></times><ci id="S3.E1.m1.10.10.8.4.1.1.1.1.3.cmml" xref="S3.E1.m1.10.10.8.4.1.1.1.1.3">𝑇</ci><apply id="S3.E1.m1.10.10.8.4.1.1.1.1.1.cmml" xref="S3.E1.m1.10.10.8.4.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.10.10.8.4.1.1.1.1.1.2.cmml" xref="S3.E1.m1.10.10.8.4.1.1.1.1.1">superscript</csymbol><apply id="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.cmml" xref="S3.E1.m1.10.10.8.4.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.10.10.8.4.1.1.1.1.1">subscript</csymbol><apply id="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1"><minus id="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.1"></minus><apply id="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.2.2.2">𝐱</ci><ci id="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.2.2.3">𝑗</ci></apply><cn type="integer" id="S3.E1.m1.7.7.5.1.1.1.cmml" xref="S3.E1.m1.7.7.5.1.1.1">2</cn></apply><apply id="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.3">superscript</csymbol><apply id="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.3.2.2">𝐲</ci><ci id="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.1.1.1.3.2.3">𝑘</ci></apply><cn type="integer" id="S3.E1.m1.8.8.6.2.1.1.cmml" xref="S3.E1.m1.8.8.6.2.1.1">2</cn></apply></apply></apply><cn type="integer" id="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.10.10.8.4.1.1.1.1.1.1.3">2</cn></apply><cn type="integer" id="S3.E1.m1.10.10.8.4.1.1.1.1.1.3.cmml" xref="S3.E1.m1.10.10.8.4.1.1.1.1.1.3">2</cn></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.12c">\tilde{\mathbf{y}}_{k}^{(3)}=\sum_{i=1}^{N}\alpha_{ik}\mathbf{x}_{i}^{(3)},\;\alpha_{ik}=\frac{\exp\left(-T\|\mathbf{x}_{i}^{(2)}-\mathbf{y}_{k}^{(2)}\|_{2}^{2}\right)}{\sum_{j=1}^{N}\exp\left(-T\|\mathbf{x}_{j}^{(2)}-\mathbf{y}_{k}^{(2)}\|_{2}^{2}\right)}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS4.SSS1.p3.5" class="ltx_p">Here <math id="S3.SS4.SSS1.p3.1.m1.1" class="ltx_Math" alttext="\alpha_{ik}" display="inline"><semantics id="S3.SS4.SSS1.p3.1.m1.1a"><msub id="S3.SS4.SSS1.p3.1.m1.1.1" xref="S3.SS4.SSS1.p3.1.m1.1.1.cmml"><mi id="S3.SS4.SSS1.p3.1.m1.1.1.2" xref="S3.SS4.SSS1.p3.1.m1.1.1.2.cmml">α</mi><mrow id="S3.SS4.SSS1.p3.1.m1.1.1.3" xref="S3.SS4.SSS1.p3.1.m1.1.1.3.cmml"><mi id="S3.SS4.SSS1.p3.1.m1.1.1.3.2" xref="S3.SS4.SSS1.p3.1.m1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS4.SSS1.p3.1.m1.1.1.3.1" xref="S3.SS4.SSS1.p3.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS4.SSS1.p3.1.m1.1.1.3.3" xref="S3.SS4.SSS1.p3.1.m1.1.1.3.3.cmml">k</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p3.1.m1.1b"><apply id="S3.SS4.SSS1.p3.1.m1.1.1.cmml" xref="S3.SS4.SSS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p3.1.m1.1.1.1.cmml" xref="S3.SS4.SSS1.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.SSS1.p3.1.m1.1.1.2.cmml" xref="S3.SS4.SSS1.p3.1.m1.1.1.2">𝛼</ci><apply id="S3.SS4.SSS1.p3.1.m1.1.1.3.cmml" xref="S3.SS4.SSS1.p3.1.m1.1.1.3"><times id="S3.SS4.SSS1.p3.1.m1.1.1.3.1.cmml" xref="S3.SS4.SSS1.p3.1.m1.1.1.3.1"></times><ci id="S3.SS4.SSS1.p3.1.m1.1.1.3.2.cmml" xref="S3.SS4.SSS1.p3.1.m1.1.1.3.2">𝑖</ci><ci id="S3.SS4.SSS1.p3.1.m1.1.1.3.3.cmml" xref="S3.SS4.SSS1.p3.1.m1.1.1.3.3">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p3.1.m1.1c">\alpha_{ik}</annotation></semantics></math> weights the contribution of point <math id="S3.SS4.SSS1.p3.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS4.SSS1.p3.2.m2.1a"><mi id="S3.SS4.SSS1.p3.2.m2.1.1" xref="S3.SS4.SSS1.p3.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p3.2.m2.1b"><ci id="S3.SS4.SSS1.p3.2.m2.1.1.cmml" xref="S3.SS4.SSS1.p3.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p3.2.m2.1c">i</annotation></semantics></math> to the pseudo keypoint <math id="S3.SS4.SSS1.p3.3.m3.1" class="ltx_Math" alttext="\hat{\mathbf{y}}_{k}^{(3)}" display="inline"><semantics id="S3.SS4.SSS1.p3.3.m3.1a"><msubsup id="S3.SS4.SSS1.p3.3.m3.1.2" xref="S3.SS4.SSS1.p3.3.m3.1.2.cmml"><mover accent="true" id="S3.SS4.SSS1.p3.3.m3.1.2.2.2" xref="S3.SS4.SSS1.p3.3.m3.1.2.2.2.cmml"><mi id="S3.SS4.SSS1.p3.3.m3.1.2.2.2.2" xref="S3.SS4.SSS1.p3.3.m3.1.2.2.2.2.cmml">𝐲</mi><mo id="S3.SS4.SSS1.p3.3.m3.1.2.2.2.1" xref="S3.SS4.SSS1.p3.3.m3.1.2.2.2.1.cmml">^</mo></mover><mi id="S3.SS4.SSS1.p3.3.m3.1.2.2.3" xref="S3.SS4.SSS1.p3.3.m3.1.2.2.3.cmml">k</mi><mrow id="S3.SS4.SSS1.p3.3.m3.1.1.1.3" xref="S3.SS4.SSS1.p3.3.m3.1.2.cmml"><mo stretchy="false" id="S3.SS4.SSS1.p3.3.m3.1.1.1.3.1" xref="S3.SS4.SSS1.p3.3.m3.1.2.cmml">(</mo><mn id="S3.SS4.SSS1.p3.3.m3.1.1.1.1" xref="S3.SS4.SSS1.p3.3.m3.1.1.1.1.cmml">3</mn><mo stretchy="false" id="S3.SS4.SSS1.p3.3.m3.1.1.1.3.2" xref="S3.SS4.SSS1.p3.3.m3.1.2.cmml">)</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p3.3.m3.1b"><apply id="S3.SS4.SSS1.p3.3.m3.1.2.cmml" xref="S3.SS4.SSS1.p3.3.m3.1.2"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p3.3.m3.1.2.1.cmml" xref="S3.SS4.SSS1.p3.3.m3.1.2">superscript</csymbol><apply id="S3.SS4.SSS1.p3.3.m3.1.2.2.cmml" xref="S3.SS4.SSS1.p3.3.m3.1.2"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p3.3.m3.1.2.2.1.cmml" xref="S3.SS4.SSS1.p3.3.m3.1.2">subscript</csymbol><apply id="S3.SS4.SSS1.p3.3.m3.1.2.2.2.cmml" xref="S3.SS4.SSS1.p3.3.m3.1.2.2.2"><ci id="S3.SS4.SSS1.p3.3.m3.1.2.2.2.1.cmml" xref="S3.SS4.SSS1.p3.3.m3.1.2.2.2.1">^</ci><ci id="S3.SS4.SSS1.p3.3.m3.1.2.2.2.2.cmml" xref="S3.SS4.SSS1.p3.3.m3.1.2.2.2.2">𝐲</ci></apply><ci id="S3.SS4.SSS1.p3.3.m3.1.2.2.3.cmml" xref="S3.SS4.SSS1.p3.3.m3.1.2.2.3">𝑘</ci></apply><cn type="integer" id="S3.SS4.SSS1.p3.3.m3.1.1.1.1.cmml" xref="S3.SS4.SSS1.p3.3.m3.1.1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p3.3.m3.1c">\hat{\mathbf{y}}_{k}^{(3)}</annotation></semantics></math> based on their distances to the ground truth keypoint <math id="S3.SS4.SSS1.p3.4.m4.1" class="ltx_Math" alttext="\mathbf{y}_{k}^{(2)}" display="inline"><semantics id="S3.SS4.SSS1.p3.4.m4.1a"><msubsup id="S3.SS4.SSS1.p3.4.m4.1.2" xref="S3.SS4.SSS1.p3.4.m4.1.2.cmml"><mi id="S3.SS4.SSS1.p3.4.m4.1.2.2.2" xref="S3.SS4.SSS1.p3.4.m4.1.2.2.2.cmml">𝐲</mi><mi id="S3.SS4.SSS1.p3.4.m4.1.2.2.3" xref="S3.SS4.SSS1.p3.4.m4.1.2.2.3.cmml">k</mi><mrow id="S3.SS4.SSS1.p3.4.m4.1.1.1.3" xref="S3.SS4.SSS1.p3.4.m4.1.2.cmml"><mo stretchy="false" id="S3.SS4.SSS1.p3.4.m4.1.1.1.3.1" xref="S3.SS4.SSS1.p3.4.m4.1.2.cmml">(</mo><mn id="S3.SS4.SSS1.p3.4.m4.1.1.1.1" xref="S3.SS4.SSS1.p3.4.m4.1.1.1.1.cmml">2</mn><mo stretchy="false" id="S3.SS4.SSS1.p3.4.m4.1.1.1.3.2" xref="S3.SS4.SSS1.p3.4.m4.1.2.cmml">)</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p3.4.m4.1b"><apply id="S3.SS4.SSS1.p3.4.m4.1.2.cmml" xref="S3.SS4.SSS1.p3.4.m4.1.2"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p3.4.m4.1.2.1.cmml" xref="S3.SS4.SSS1.p3.4.m4.1.2">superscript</csymbol><apply id="S3.SS4.SSS1.p3.4.m4.1.2.2.cmml" xref="S3.SS4.SSS1.p3.4.m4.1.2"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p3.4.m4.1.2.2.1.cmml" xref="S3.SS4.SSS1.p3.4.m4.1.2">subscript</csymbol><ci id="S3.SS4.SSS1.p3.4.m4.1.2.2.2.cmml" xref="S3.SS4.SSS1.p3.4.m4.1.2.2.2">𝐲</ci><ci id="S3.SS4.SSS1.p3.4.m4.1.2.2.3.cmml" xref="S3.SS4.SSS1.p3.4.m4.1.2.2.3">𝑘</ci></apply><cn type="integer" id="S3.SS4.SSS1.p3.4.m4.1.1.1.1.cmml" xref="S3.SS4.SSS1.p3.4.m4.1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p3.4.m4.1c">\mathbf{y}_{k}^{(2)}</annotation></semantics></math> in 2D space, <math id="S3.SS4.SSS1.p3.5.m5.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS4.SSS1.p3.5.m5.1a"><mi id="S3.SS4.SSS1.p3.5.m5.1.1" xref="S3.SS4.SSS1.p3.5.m5.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p3.5.m5.1b"><ci id="S3.SS4.SSS1.p3.5.m5.1.1.cmml" xref="S3.SS4.SSS1.p3.5.m5.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p3.5.m5.1c">T</annotation></semantics></math> is the temperature that controls the softmax operation.</p>
</div>
<div id="S3.SS4.SSS1.p4" class="ltx_para">
<p id="S3.SS4.SSS1.p4.2" class="ltx_p">In case the pseudo 3D labels are not accurate, we also compute the reliability of the 3D approximation for each keypoint as <math id="S3.SS4.SSS1.p4.1.m1.4" class="ltx_Math" alttext="r_{k}=\exp\left(-T_{r}\min_{i}\|\mathbf{x}_{i}^{(2)}-\mathbf{y}_{k}^{(2)}\|_{2}^{2}\right)" display="inline"><semantics id="S3.SS4.SSS1.p4.1.m1.4a"><mrow id="S3.SS4.SSS1.p4.1.m1.4.4" xref="S3.SS4.SSS1.p4.1.m1.4.4.cmml"><msub id="S3.SS4.SSS1.p4.1.m1.4.4.3" xref="S3.SS4.SSS1.p4.1.m1.4.4.3.cmml"><mi id="S3.SS4.SSS1.p4.1.m1.4.4.3.2" xref="S3.SS4.SSS1.p4.1.m1.4.4.3.2.cmml">r</mi><mi id="S3.SS4.SSS1.p4.1.m1.4.4.3.3" xref="S3.SS4.SSS1.p4.1.m1.4.4.3.3.cmml">k</mi></msub><mo id="S3.SS4.SSS1.p4.1.m1.4.4.2" xref="S3.SS4.SSS1.p4.1.m1.4.4.2.cmml">=</mo><mrow id="S3.SS4.SSS1.p4.1.m1.4.4.1.1" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.2.cmml"><mi id="S3.SS4.SSS1.p4.1.m1.3.3" xref="S3.SS4.SSS1.p4.1.m1.3.3.cmml">exp</mi><mo id="S3.SS4.SSS1.p4.1.m1.4.4.1.1a" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.2.cmml">⁡</mo><mrow id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.2.cmml"><mo id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.2" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.2.cmml">(</mo><mrow id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.cmml"><mo id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1a" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.cmml">−</mo><mrow id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.cmml"><msub id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.3" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.3.cmml"><mi id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.3.2" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.3.2.cmml">T</mi><mi id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.3.3" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.3.3.cmml">r</mi></msub><mo lspace="0.167em" rspace="0em" id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.2" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.cmml"><msub id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.2" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.2.cmml"><mi id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.2.2" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.2.2.cmml">min</mi><mi id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.2.3" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1a" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.cmml">⁡</mo><msubsup id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.cmml"><mrow id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.2" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.cmml"><msubsup id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.2.2" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml">𝐱</mi><mi id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.2.3" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml">i</mi><mrow id="S3.SS4.SSS1.p4.1.m1.1.1.1.3" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS4.SSS1.p4.1.m1.1.1.1.3.1" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.cmml">(</mo><mn id="S3.SS4.SSS1.p4.1.m1.1.1.1.1" xref="S3.SS4.SSS1.p4.1.m1.1.1.1.1.cmml">2</mn><mo stretchy="false" id="S3.SS4.SSS1.p4.1.m1.1.1.1.3.2" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></msubsup><mo id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.cmml">−</mo><msubsup id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.2.2" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.2.2.cmml">𝐲</mi><mi id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.2.3" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.2.3.cmml">k</mi><mrow id="S3.SS4.SSS1.p4.1.m1.2.2.1.3" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mo stretchy="false" id="S3.SS4.SSS1.p4.1.m1.2.2.1.3.1" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.cmml">(</mo><mn id="S3.SS4.SSS1.p4.1.m1.2.2.1.1" xref="S3.SS4.SSS1.p4.1.m1.2.2.1.1.cmml">2</mn><mo stretchy="false" id="S3.SS4.SSS1.p4.1.m1.2.2.1.3.2" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.cmml">)</mo></mrow></msubsup></mrow><mo stretchy="false" id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.3" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.3" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.3.cmml">2</mn><mn id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.3" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.3.cmml">2</mn></msubsup></mrow></mrow></mrow><mo id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.3" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p4.1.m1.4b"><apply id="S3.SS4.SSS1.p4.1.m1.4.4.cmml" xref="S3.SS4.SSS1.p4.1.m1.4.4"><eq id="S3.SS4.SSS1.p4.1.m1.4.4.2.cmml" xref="S3.SS4.SSS1.p4.1.m1.4.4.2"></eq><apply id="S3.SS4.SSS1.p4.1.m1.4.4.3.cmml" xref="S3.SS4.SSS1.p4.1.m1.4.4.3"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p4.1.m1.4.4.3.1.cmml" xref="S3.SS4.SSS1.p4.1.m1.4.4.3">subscript</csymbol><ci id="S3.SS4.SSS1.p4.1.m1.4.4.3.2.cmml" xref="S3.SS4.SSS1.p4.1.m1.4.4.3.2">𝑟</ci><ci id="S3.SS4.SSS1.p4.1.m1.4.4.3.3.cmml" xref="S3.SS4.SSS1.p4.1.m1.4.4.3.3">𝑘</ci></apply><apply id="S3.SS4.SSS1.p4.1.m1.4.4.1.2.cmml" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1"><exp id="S3.SS4.SSS1.p4.1.m1.3.3.cmml" xref="S3.SS4.SSS1.p4.1.m1.3.3"></exp><apply id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.cmml" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1"><minus id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.2.cmml" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1"></minus><apply id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.cmml" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1"><times id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.2.cmml" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.2"></times><apply id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.3.cmml" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.3.1.cmml" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.3">subscript</csymbol><ci id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.3.2.cmml" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.3.2">𝑇</ci><ci id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.3.3.cmml" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.3.3">𝑟</ci></apply><apply id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.cmml" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1"><apply id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.2.cmml" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.2.1.cmml" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.2">subscript</csymbol><min id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.2.2.cmml" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.2.2"></min><ci id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.2.3.cmml" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.cmml" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.2.cmml" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.cmml" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1">subscript</csymbol><apply id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1"><minus id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1"></minus><apply id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.2.2">𝐱</ci><ci id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.2.3">𝑖</ci></apply><cn type="integer" id="S3.SS4.SSS1.p4.1.m1.1.1.1.1.cmml" xref="S3.SS4.SSS1.p4.1.m1.1.1.1.1">2</cn></apply><apply id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3">superscript</csymbol><apply id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.2.2">𝐲</ci><ci id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.2.3">𝑘</ci></apply><cn type="integer" id="S3.SS4.SSS1.p4.1.m1.2.2.1.1.cmml" xref="S3.SS4.SSS1.p4.1.m1.2.2.1.1">2</cn></apply></apply></apply><cn type="integer" id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.1.3">2</cn></apply><cn type="integer" id="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.3.cmml" xref="S3.SS4.SSS1.p4.1.m1.4.4.1.1.1.1.1.1.1.3">2</cn></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p4.1.m1.4c">r_{k}=\exp\left(-T_{r}\min_{i}\|\mathbf{x}_{i}^{(2)}-\mathbf{y}_{k}^{(2)}\|_{2}^{2}\right)</annotation></semantics></math>, where <math id="S3.SS4.SSS1.p4.2.m2.1" class="ltx_Math" alttext="T_{r}" display="inline"><semantics id="S3.SS4.SSS1.p4.2.m2.1a"><msub id="S3.SS4.SSS1.p4.2.m2.1.1" xref="S3.SS4.SSS1.p4.2.m2.1.1.cmml"><mi id="S3.SS4.SSS1.p4.2.m2.1.1.2" xref="S3.SS4.SSS1.p4.2.m2.1.1.2.cmml">T</mi><mi id="S3.SS4.SSS1.p4.2.m2.1.1.3" xref="S3.SS4.SSS1.p4.2.m2.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p4.2.m2.1b"><apply id="S3.SS4.SSS1.p4.2.m2.1.1.cmml" xref="S3.SS4.SSS1.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p4.2.m2.1.1.1.cmml" xref="S3.SS4.SSS1.p4.2.m2.1.1">subscript</csymbol><ci id="S3.SS4.SSS1.p4.2.m2.1.1.2.cmml" xref="S3.SS4.SSS1.p4.2.m2.1.1.2">𝑇</ci><ci id="S3.SS4.SSS1.p4.2.m2.1.1.3.cmml" xref="S3.SS4.SSS1.p4.2.m2.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p4.2.m2.1c">T_{r}</annotation></semantics></math> is the temperature factor, to weight the losses on different keypoints during training.</p>
</div>
<div id="S3.SS4.SSS1.p5" class="ltx_para ltx_noindent">
<p id="S3.SS4.SSS1.p5.3" class="ltx_p"><span id="S3.SS4.SSS1.p5.3.1" class="ltx_text ltx_font_bold">Pointwise Keypoint Type Label Generation:</span>
To generate pointwise type labels for the segmentation task, we simply assign all neighboring points of a keypoint in 2D space to the corresponding keypoint type, shown in Figure <a href="#S3.F4" title="Figure 4 ‣ 3.2 Modality Fusion of LiDAR and Camera ‣ 3 Method ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. The type label <math id="S3.SS4.SSS1.p5.1.m1.1" class="ltx_Math" alttext="l_{ik}" display="inline"><semantics id="S3.SS4.SSS1.p5.1.m1.1a"><msub id="S3.SS4.SSS1.p5.1.m1.1.1" xref="S3.SS4.SSS1.p5.1.m1.1.1.cmml"><mi id="S3.SS4.SSS1.p5.1.m1.1.1.2" xref="S3.SS4.SSS1.p5.1.m1.1.1.2.cmml">l</mi><mrow id="S3.SS4.SSS1.p5.1.m1.1.1.3" xref="S3.SS4.SSS1.p5.1.m1.1.1.3.cmml"><mi id="S3.SS4.SSS1.p5.1.m1.1.1.3.2" xref="S3.SS4.SSS1.p5.1.m1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS4.SSS1.p5.1.m1.1.1.3.1" xref="S3.SS4.SSS1.p5.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS4.SSS1.p5.1.m1.1.1.3.3" xref="S3.SS4.SSS1.p5.1.m1.1.1.3.3.cmml">k</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p5.1.m1.1b"><apply id="S3.SS4.SSS1.p5.1.m1.1.1.cmml" xref="S3.SS4.SSS1.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p5.1.m1.1.1.1.cmml" xref="S3.SS4.SSS1.p5.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.SSS1.p5.1.m1.1.1.2.cmml" xref="S3.SS4.SSS1.p5.1.m1.1.1.2">𝑙</ci><apply id="S3.SS4.SSS1.p5.1.m1.1.1.3.cmml" xref="S3.SS4.SSS1.p5.1.m1.1.1.3"><times id="S3.SS4.SSS1.p5.1.m1.1.1.3.1.cmml" xref="S3.SS4.SSS1.p5.1.m1.1.1.3.1"></times><ci id="S3.SS4.SSS1.p5.1.m1.1.1.3.2.cmml" xref="S3.SS4.SSS1.p5.1.m1.1.1.3.2">𝑖</ci><ci id="S3.SS4.SSS1.p5.1.m1.1.1.3.3.cmml" xref="S3.SS4.SSS1.p5.1.m1.1.1.3.3">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p5.1.m1.1c">l_{ik}</annotation></semantics></math> for point <math id="S3.SS4.SSS1.p5.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS4.SSS1.p5.2.m2.1a"><mi id="S3.SS4.SSS1.p5.2.m2.1.1" xref="S3.SS4.SSS1.p5.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p5.2.m2.1b"><ci id="S3.SS4.SSS1.p5.2.m2.1.1.cmml" xref="S3.SS4.SSS1.p5.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p5.2.m2.1c">i</annotation></semantics></math> with respect to keypoint <math id="S3.SS4.SSS1.p5.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS4.SSS1.p5.3.m3.1a"><mi id="S3.SS4.SSS1.p5.3.m3.1.1" xref="S3.SS4.SSS1.p5.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p5.3.m3.1b"><ci id="S3.SS4.SSS1.p5.3.m3.1.1.cmml" xref="S3.SS4.SSS1.p5.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p5.3.m3.1c">k</annotation></semantics></math> is generated by</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.4" class="ltx_Math" alttext="l_{ik}=\begin{cases}1&amp;\text{if }\|\mathbf{x}_{i}^{(2)}-\mathbf{y}_{k}^{(2)}\|_{2}\leq r,\\
0&amp;\text{otherwise}\end{cases}" display="block"><semantics id="S3.E2.m1.4a"><mrow id="S3.E2.m1.4.5" xref="S3.E2.m1.4.5.cmml"><msub id="S3.E2.m1.4.5.2" xref="S3.E2.m1.4.5.2.cmml"><mi id="S3.E2.m1.4.5.2.2" xref="S3.E2.m1.4.5.2.2.cmml">l</mi><mrow id="S3.E2.m1.4.5.2.3" xref="S3.E2.m1.4.5.2.3.cmml"><mi id="S3.E2.m1.4.5.2.3.2" xref="S3.E2.m1.4.5.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.5.2.3.1" xref="S3.E2.m1.4.5.2.3.1.cmml">​</mo><mi id="S3.E2.m1.4.5.2.3.3" xref="S3.E2.m1.4.5.2.3.3.cmml">k</mi></mrow></msub><mo id="S3.E2.m1.4.5.1" xref="S3.E2.m1.4.5.1.cmml">=</mo><mrow id="S3.E2.m1.4.4" xref="S3.E2.m1.4.5.3.1.cmml"><mo id="S3.E2.m1.4.4.5" xref="S3.E2.m1.4.5.3.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S3.E2.m1.4.4.4" xref="S3.E2.m1.4.5.3.1.cmml"><mtr id="S3.E2.m1.4.4.4a" xref="S3.E2.m1.4.5.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E2.m1.4.4.4b" xref="S3.E2.m1.4.5.3.1.cmml"><mn id="S3.E2.m1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.cmml">1</mn></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E2.m1.4.4.4c" xref="S3.E2.m1.4.5.3.1.cmml"><mrow id="S3.E2.m1.2.2.2.2.2.1.3" xref="S3.E2.m1.2.2.2.2.2.1.3.1.cmml"><mrow id="S3.E2.m1.2.2.2.2.2.1.3.1" xref="S3.E2.m1.2.2.2.2.2.1.3.1.cmml"><mrow id="S3.E2.m1.2.2.2.2.2.1.3.1.1" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1.cmml"><mtext id="S3.E2.m1.2.2.2.2.2.1.3.1.1.3" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1.3a.cmml">if </mtext><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.2.2.2.1.3.1.1.2" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1.2.cmml">​</mo><msub id="S3.E2.m1.2.2.2.2.2.1.3.1.1.1" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.cmml"><mrow id="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.2" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.2.1.cmml">‖</mo><mrow id="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.cmml"><msubsup id="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.2" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.2.2.2" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.2.2.2.cmml">𝐱</mi><mi id="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.2.2.3" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.2.2.3.cmml">i</mi><mrow id="S3.E2.m1.2.2.2.2.2.1.1.1.3" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E2.m1.2.2.2.2.2.1.1.1.3.1" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.2.cmml">(</mo><mn id="S3.E2.m1.2.2.2.2.2.1.1.1.1" xref="S3.E2.m1.2.2.2.2.2.1.1.1.1.cmml">2</mn><mo stretchy="false" id="S3.E2.m1.2.2.2.2.2.1.1.1.3.2" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.2.cmml">)</mo></mrow></msubsup><mo id="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.1" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.1.cmml">−</mo><msubsup id="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.3" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.3.2.2" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.3.2.2.cmml">𝐲</mi><mi id="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.3.2.3" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.3.2.3.cmml">k</mi><mrow id="S3.E2.m1.2.2.2.2.2.1.2.1.3" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.3.cmml"><mo stretchy="false" id="S3.E2.m1.2.2.2.2.2.1.2.1.3.1" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.3.cmml">(</mo><mn id="S3.E2.m1.2.2.2.2.2.1.2.1.1" xref="S3.E2.m1.2.2.2.2.2.1.2.1.1.cmml">2</mn><mo stretchy="false" id="S3.E2.m1.2.2.2.2.2.1.2.1.3.2" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.3.cmml">)</mo></mrow></msubsup></mrow><mo stretchy="false" id="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.3" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.3" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.3.cmml">2</mn></msub></mrow><mo id="S3.E2.m1.2.2.2.2.2.1.3.1.2" xref="S3.E2.m1.2.2.2.2.2.1.3.1.2.cmml">≤</mo><mi id="S3.E2.m1.2.2.2.2.2.1.3.1.3" xref="S3.E2.m1.2.2.2.2.2.1.3.1.3.cmml">r</mi></mrow><mo id="S3.E2.m1.2.2.2.2.2.1.3.2" xref="S3.E2.m1.2.2.2.2.2.1.3.1.cmml">,</mo></mrow></mtd></mtr><mtr id="S3.E2.m1.4.4.4d" xref="S3.E2.m1.4.5.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E2.m1.4.4.4e" xref="S3.E2.m1.4.5.3.1.cmml"><mn id="S3.E2.m1.3.3.3.3.1.1" xref="S3.E2.m1.3.3.3.3.1.1.cmml">0</mn></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E2.m1.4.4.4f" xref="S3.E2.m1.4.5.3.1.cmml"><mtext id="S3.E2.m1.4.4.4.4.2.1" xref="S3.E2.m1.4.4.4.4.2.1a.cmml">otherwise</mtext></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.4b"><apply id="S3.E2.m1.4.5.cmml" xref="S3.E2.m1.4.5"><eq id="S3.E2.m1.4.5.1.cmml" xref="S3.E2.m1.4.5.1"></eq><apply id="S3.E2.m1.4.5.2.cmml" xref="S3.E2.m1.4.5.2"><csymbol cd="ambiguous" id="S3.E2.m1.4.5.2.1.cmml" xref="S3.E2.m1.4.5.2">subscript</csymbol><ci id="S3.E2.m1.4.5.2.2.cmml" xref="S3.E2.m1.4.5.2.2">𝑙</ci><apply id="S3.E2.m1.4.5.2.3.cmml" xref="S3.E2.m1.4.5.2.3"><times id="S3.E2.m1.4.5.2.3.1.cmml" xref="S3.E2.m1.4.5.2.3.1"></times><ci id="S3.E2.m1.4.5.2.3.2.cmml" xref="S3.E2.m1.4.5.2.3.2">𝑖</ci><ci id="S3.E2.m1.4.5.2.3.3.cmml" xref="S3.E2.m1.4.5.2.3.3">𝑘</ci></apply></apply><apply id="S3.E2.m1.4.5.3.1.cmml" xref="S3.E2.m1.4.4"><csymbol cd="latexml" id="S3.E2.m1.4.5.3.1.1.cmml" xref="S3.E2.m1.4.4.5">cases</csymbol><cn type="integer" id="S3.E2.m1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1">1</cn><apply id="S3.E2.m1.2.2.2.2.2.1.3.1.cmml" xref="S3.E2.m1.2.2.2.2.2.1.3"><leq id="S3.E2.m1.2.2.2.2.2.1.3.1.2.cmml" xref="S3.E2.m1.2.2.2.2.2.1.3.1.2"></leq><apply id="S3.E2.m1.2.2.2.2.2.1.3.1.1.cmml" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1"><times id="S3.E2.m1.2.2.2.2.2.1.3.1.1.2.cmml" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1.2"></times><ci id="S3.E2.m1.2.2.2.2.2.1.3.1.1.3a.cmml" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1.3"><mtext id="S3.E2.m1.2.2.2.2.2.1.3.1.1.3.cmml" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1.3">if </mtext></ci><apply id="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.cmml" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.2.cmml" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1.1">subscript</csymbol><apply id="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.2.1.cmml" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.2">norm</csymbol><apply id="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1"><minus id="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.1"></minus><apply id="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.2.2.2">𝐱</ci><ci id="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.2.2.3">𝑖</ci></apply><cn type="integer" id="S3.E2.m1.2.2.2.2.2.1.1.1.1.cmml" xref="S3.E2.m1.2.2.2.2.2.1.1.1.1">2</cn></apply><apply id="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.3">superscript</csymbol><apply id="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.3.2.2">𝐲</ci><ci id="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.1.1.1.3.2.3">𝑘</ci></apply><cn type="integer" id="S3.E2.m1.2.2.2.2.2.1.2.1.1.cmml" xref="S3.E2.m1.2.2.2.2.2.1.2.1.1">2</cn></apply></apply></apply><cn type="integer" id="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.3.cmml" xref="S3.E2.m1.2.2.2.2.2.1.3.1.1.1.3">2</cn></apply></apply><ci id="S3.E2.m1.2.2.2.2.2.1.3.1.3.cmml" xref="S3.E2.m1.2.2.2.2.2.1.3.1.3">𝑟</ci></apply><cn type="integer" id="S3.E2.m1.3.3.3.3.1.1.cmml" xref="S3.E2.m1.3.3.3.3.1.1">0</cn><ci id="S3.E2.m1.4.4.4.4.2.1a.cmml" xref="S3.E2.m1.4.4.4.4.2.1"><mtext id="S3.E2.m1.4.4.4.4.2.1.cmml" xref="S3.E2.m1.4.4.4.4.2.1">otherwise</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.4c">l_{ik}=\begin{cases}1&amp;\text{if }\|\mathbf{x}_{i}^{(2)}-\mathbf{y}_{k}^{(2)}\|_{2}\leq r,\\
0&amp;\text{otherwise}\end{cases}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS4.SSS1.p5.4" class="ltx_p">where <math id="S3.SS4.SSS1.p5.4.m1.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S3.SS4.SSS1.p5.4.m1.1a"><mi id="S3.SS4.SSS1.p5.4.m1.1.1" xref="S3.SS4.SSS1.p5.4.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p5.4.m1.1b"><ci id="S3.SS4.SSS1.p5.4.m1.1.1.cmml" xref="S3.SS4.SSS1.p5.4.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p5.4.m1.1c">r</annotation></semantics></math> is the neighboring radius for positive samples.</p>
</div>
<div id="S3.SS4.SSS1.p6" class="ltx_para">
<p id="S3.SS4.SSS1.p6.1" class="ltx_p">With the generated pseudo 3D labels to train the 3D keypoint model, we achieve <math id="S3.SS4.SSS1.p6.1.m1.1" class="ltx_Math" alttext="\sim 22\%" display="inline"><semantics id="S3.SS4.SSS1.p6.1.m1.1a"><mrow id="S3.SS4.SSS1.p6.1.m1.1.1" xref="S3.SS4.SSS1.p6.1.m1.1.1.cmml"><mi id="S3.SS4.SSS1.p6.1.m1.1.1.2" xref="S3.SS4.SSS1.p6.1.m1.1.1.2.cmml"></mi><mo id="S3.SS4.SSS1.p6.1.m1.1.1.1" xref="S3.SS4.SSS1.p6.1.m1.1.1.1.cmml">∼</mo><mrow id="S3.SS4.SSS1.p6.1.m1.1.1.3" xref="S3.SS4.SSS1.p6.1.m1.1.1.3.cmml"><mn id="S3.SS4.SSS1.p6.1.m1.1.1.3.2" xref="S3.SS4.SSS1.p6.1.m1.1.1.3.2.cmml">22</mn><mo id="S3.SS4.SSS1.p6.1.m1.1.1.3.1" xref="S3.SS4.SSS1.p6.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p6.1.m1.1b"><apply id="S3.SS4.SSS1.p6.1.m1.1.1.cmml" xref="S3.SS4.SSS1.p6.1.m1.1.1"><csymbol cd="latexml" id="S3.SS4.SSS1.p6.1.m1.1.1.1.cmml" xref="S3.SS4.SSS1.p6.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S3.SS4.SSS1.p6.1.m1.1.1.2.cmml" xref="S3.SS4.SSS1.p6.1.m1.1.1.2">absent</csymbol><apply id="S3.SS4.SSS1.p6.1.m1.1.1.3.cmml" xref="S3.SS4.SSS1.p6.1.m1.1.1.3"><csymbol cd="latexml" id="S3.SS4.SSS1.p6.1.m1.1.1.3.1.cmml" xref="S3.SS4.SSS1.p6.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="S3.SS4.SSS1.p6.1.m1.1.1.3.2.cmml" xref="S3.SS4.SSS1.p6.1.m1.1.1.3.2">22</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p6.1.m1.1c">\sim 22\%</annotation></semantics></math> relative improvement on the Waymo Open Dataset compared to the baseline of predicting 2D keypoints with 2D labels and lifting to 3D (Table <a href="#S4.T3" title="Table 3 ‣ Labeling: ‣ 4.1 Data and Evaluation Metrics ‣ 4 Experiments ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> in Section <a href="#S4" title="4 Experiments ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>).</p>
</div>
</section>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Training Losses</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.3" class="ltx_p"><span id="S3.SS5.p1.3.1" class="ltx_text ltx_font_bold">Point Network:</span>
The training loss for the regression branch is a Huber loss <math id="S3.SS5.p1.1.m1.1" class="ltx_Math" alttext="L_{\text{reg}}" display="inline"><semantics id="S3.SS5.p1.1.m1.1a"><msub id="S3.SS5.p1.1.m1.1.1" xref="S3.SS5.p1.1.m1.1.1.cmml"><mi id="S3.SS5.p1.1.m1.1.1.2" xref="S3.SS5.p1.1.m1.1.1.2.cmml">L</mi><mtext id="S3.SS5.p1.1.m1.1.1.3" xref="S3.SS5.p1.1.m1.1.1.3a.cmml">reg</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.1.m1.1b"><apply id="S3.SS5.p1.1.m1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS5.p1.1.m1.1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS5.p1.1.m1.1.1.2.cmml" xref="S3.SS5.p1.1.m1.1.1.2">𝐿</ci><ci id="S3.SS5.p1.1.m1.1.1.3a.cmml" xref="S3.SS5.p1.1.m1.1.1.3"><mtext mathsize="70%" id="S3.SS5.p1.1.m1.1.1.3.cmml" xref="S3.SS5.p1.1.m1.1.1.3">reg</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.1.m1.1c">L_{\text{reg}}</annotation></semantics></math> on the generated pseudo 3D labels, weighted by the reliability <math id="S3.SS5.p1.2.m2.1" class="ltx_Math" alttext="r_{k}" display="inline"><semantics id="S3.SS5.p1.2.m2.1a"><msub id="S3.SS5.p1.2.m2.1.1" xref="S3.SS5.p1.2.m2.1.1.cmml"><mi id="S3.SS5.p1.2.m2.1.1.2" xref="S3.SS5.p1.2.m2.1.1.2.cmml">r</mi><mi id="S3.SS5.p1.2.m2.1.1.3" xref="S3.SS5.p1.2.m2.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.2.m2.1b"><apply id="S3.SS5.p1.2.m2.1.1.cmml" xref="S3.SS5.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS5.p1.2.m2.1.1.1.cmml" xref="S3.SS5.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS5.p1.2.m2.1.1.2.cmml" xref="S3.SS5.p1.2.m2.1.1.2">𝑟</ci><ci id="S3.SS5.p1.2.m2.1.1.3.cmml" xref="S3.SS5.p1.2.m2.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.2.m2.1c">r_{k}</annotation></semantics></math>. The loss for the segmentation branch is a cross-entropy loss <math id="S3.SS5.p1.3.m3.1" class="ltx_Math" alttext="L_{\text{seg}}" display="inline"><semantics id="S3.SS5.p1.3.m3.1a"><msub id="S3.SS5.p1.3.m3.1.1" xref="S3.SS5.p1.3.m3.1.1.cmml"><mi id="S3.SS5.p1.3.m3.1.1.2" xref="S3.SS5.p1.3.m3.1.1.2.cmml">L</mi><mtext id="S3.SS5.p1.3.m3.1.1.3" xref="S3.SS5.p1.3.m3.1.1.3a.cmml">seg</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.3.m3.1b"><apply id="S3.SS5.p1.3.m3.1.1.cmml" xref="S3.SS5.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS5.p1.3.m3.1.1.1.cmml" xref="S3.SS5.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS5.p1.3.m3.1.1.2.cmml" xref="S3.SS5.p1.3.m3.1.1.2">𝐿</ci><ci id="S3.SS5.p1.3.m3.1.1.3a.cmml" xref="S3.SS5.p1.3.m3.1.1.3"><mtext mathsize="70%" id="S3.SS5.p1.3.m3.1.1.3.cmml" xref="S3.SS5.p1.3.m3.1.1.3">seg</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.3.m3.1c">L_{\text{seg}}</annotation></semantics></math> on the pseudo pointwise labels weighted by different positive/negative sample weights. The overall loss for the point network is</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.1" class="ltx_Math" alttext="L=L_{\text{reg}}+\lambda L_{\text{seg}}" display="block"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml"><mi id="S3.E3.m1.1.1.2" xref="S3.E3.m1.1.1.2.cmml">L</mi><mo id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.cmml">=</mo><mrow id="S3.E3.m1.1.1.3" xref="S3.E3.m1.1.1.3.cmml"><msub id="S3.E3.m1.1.1.3.2" xref="S3.E3.m1.1.1.3.2.cmml"><mi id="S3.E3.m1.1.1.3.2.2" xref="S3.E3.m1.1.1.3.2.2.cmml">L</mi><mtext id="S3.E3.m1.1.1.3.2.3" xref="S3.E3.m1.1.1.3.2.3a.cmml">reg</mtext></msub><mo id="S3.E3.m1.1.1.3.1" xref="S3.E3.m1.1.1.3.1.cmml">+</mo><mrow id="S3.E3.m1.1.1.3.3" xref="S3.E3.m1.1.1.3.3.cmml"><mi id="S3.E3.m1.1.1.3.3.2" xref="S3.E3.m1.1.1.3.3.2.cmml">λ</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.3.3.1" xref="S3.E3.m1.1.1.3.3.1.cmml">​</mo><msub id="S3.E3.m1.1.1.3.3.3" xref="S3.E3.m1.1.1.3.3.3.cmml"><mi id="S3.E3.m1.1.1.3.3.3.2" xref="S3.E3.m1.1.1.3.3.3.2.cmml">L</mi><mtext id="S3.E3.m1.1.1.3.3.3.3" xref="S3.E3.m1.1.1.3.3.3.3a.cmml">seg</mtext></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1"><eq id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"></eq><ci id="S3.E3.m1.1.1.2.cmml" xref="S3.E3.m1.1.1.2">𝐿</ci><apply id="S3.E3.m1.1.1.3.cmml" xref="S3.E3.m1.1.1.3"><plus id="S3.E3.m1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.3.1"></plus><apply id="S3.E3.m1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.2.1.cmml" xref="S3.E3.m1.1.1.3.2">subscript</csymbol><ci id="S3.E3.m1.1.1.3.2.2.cmml" xref="S3.E3.m1.1.1.3.2.2">𝐿</ci><ci id="S3.E3.m1.1.1.3.2.3a.cmml" xref="S3.E3.m1.1.1.3.2.3"><mtext mathsize="70%" id="S3.E3.m1.1.1.3.2.3.cmml" xref="S3.E3.m1.1.1.3.2.3">reg</mtext></ci></apply><apply id="S3.E3.m1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.3.3"><times id="S3.E3.m1.1.1.3.3.1.cmml" xref="S3.E3.m1.1.1.3.3.1"></times><ci id="S3.E3.m1.1.1.3.3.2.cmml" xref="S3.E3.m1.1.1.3.3.2">𝜆</ci><apply id="S3.E3.m1.1.1.3.3.3.cmml" xref="S3.E3.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.3.3.1.cmml" xref="S3.E3.m1.1.1.3.3.3">subscript</csymbol><ci id="S3.E3.m1.1.1.3.3.3.2.cmml" xref="S3.E3.m1.1.1.3.3.3.2">𝐿</ci><ci id="S3.E3.m1.1.1.3.3.3.3a.cmml" xref="S3.E3.m1.1.1.3.3.3.3"><mtext mathsize="70%" id="S3.E3.m1.1.1.3.3.3.3.cmml" xref="S3.E3.m1.1.1.3.3.3.3">seg</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">L=L_{\text{reg}}+\lambda L_{\text{seg}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS5.p1.4" class="ltx_p">where <math id="S3.SS5.p1.4.m1.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S3.SS5.p1.4.m1.1a"><mi id="S3.SS5.p1.4.m1.1.1" xref="S3.SS5.p1.4.m1.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.4.m1.1b"><ci id="S3.SS5.p1.4.m1.1.1.cmml" xref="S3.SS5.p1.4.m1.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.4.m1.1c">\lambda</annotation></semantics></math> is used to weight the auxiliary segmentation loss.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para ltx_noindent">
<p id="S3.SS5.p2.1" class="ltx_p"><span id="S3.SS5.p2.1.1" class="ltx_text ltx_font_bold">Camera Network:</span> Similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, the camera network is trained on a mean-squared-error loss with ground truth 2D heatmap. We train the camera network independently, then freeze it during point network training.</p>
</div>
<div id="S3.SS5.p3" class="ltx_para">
<p id="S3.SS5.p3.1" class="ltx_p">Note that we only train and evaluate on visible keypoints. During training, keypoint losses are only applied on visible keypoints, which means we will not generate pseudo labels for occluded keypoints. In Section <a href="#S4" title="4 Experiments ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we show that even trained on visible keypoints only, the model is able to predict reasonable keypoints for occluded body parts. For more details of training losses, please refer to the supplementary material.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Data and Evaluation Metrics</h3>

<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Training Data:</h5>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p1.1" class="ltx_p">We collect an internal dataset with RGB images and LiDAR point clouds similar to the Waymo Open Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. It consists of a total number of 197,381 pedestrians. These pedestrians are labeled with 2D keypoint labels of 13 keypoint types (<em id="S4.SS1.SSS0.Px1.p1.1.1" class="ltx_emph ltx_font_italic">nose</em>, <em id="S4.SS1.SSS0.Px1.p1.1.2" class="ltx_emph ltx_font_italic">left/right shoulders</em>, <em id="S4.SS1.SSS0.Px1.p1.1.3" class="ltx_emph ltx_font_italic">left/right elbows</em>, <em id="S4.SS1.SSS0.Px1.p1.1.4" class="ltx_emph ltx_font_italic">left/right wrists</em>, <em id="S4.SS1.SSS0.Px1.p1.1.5" class="ltx_emph ltx_font_italic">left/right hips</em>, <em id="S4.SS1.SSS0.Px1.p1.1.6" class="ltx_emph ltx_font_italic">left/right knees</em> and <em id="S4.SS1.SSS0.Px1.p1.1.7" class="ltx_emph ltx_font_italic">left/right ankles</em>) in the camera image. These samples are split into a training set with 155,182 pedestrians and a test set with 42,199 pedestrians. The training set with pure 2D labels is used to train the proposed model.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">3D Evaluation Data and Metrics:</h5>

<div id="S4.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p1.1" class="ltx_p">The Waymo Open Dataset serves as our 3D evaluation set. It is composed of sensor data collected by Waymo cars under a variety of conditions. It contains 1,950 segments of 20s each, with sensor data including point clouds from LiDAR and RGB images captured by cameras. For 3D evaluation, we labeled 986 pedestrians with 3D keypoint coordinates of 13 keypoint types (same as our internal dataset) on LiDAR point clouds. We are looking to release these labels for evaluation once obtained related approvals.</p>
</div>
<div id="S4.SS1.SSS0.Px2.p2" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p2.1" class="ltx_p">Evaluation results are reported in the OKS (Object Keypoint Similarity) accuracy (OKS/ACC) metric, which is similar to the OKS/AP metric introduced in COCO keypoint challenge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> (please refer to the supplementary material for more details), and MPJPE (Mean Per Joint Position Error) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> in 3D coordinates.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">2D Evaluation Data and Metrics:</h5>

<div id="S4.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px3.p1.1" class="ltx_p">The test set of our internal dataset serves as the 2D evaluation set. Evaluation results are reported in the OKS/ACC metric in 2D coordinates, after the 3D predictions are projected to 2D space by the corresponding lidar to camera projections.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Labeling:</h5>

<div id="S4.SS1.SSS0.Px4.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px4.p1.1" class="ltx_p">For 2D/3D keypoint labeling on the Waymo Open Dataset and the Internal Dataset, we adopt a definition of keypoints similar to the COCO Challenge. Each keypoint is labeled by multiple annotators, whose results are aggregated to determine the final label. For 2D labeling, we only label 2D coordinates of keypoints that are visible in the camera image. For occluded keypoints, we label them as invisible. 3D labeling is similar, where we only label keypoints that are visible from the point clouds. Since we pair each LiDAR with its closest camera in location,
the occlusion status of keypoints is mostly consistent between 2D and 3D.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<div id="S4.T1.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:105.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(31.2pt,-7.6pt) scale(1.167910558327,1.167910558327) ;">
<table id="S4.T1.3.3" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.3.3.4.1" class="ltx_tr">
<th id="S4.T1.3.3.4.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T1.3.3.4.1.1.1" class="ltx_text">Methods</span></th>
<th id="S4.T1.3.3.4.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2">Waymo Open Dataset</th>
<th id="S4.T1.3.3.4.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Internal Dataset</th>
</tr>
<tr id="S4.T1.3.3.3" class="ltx_tr">
<td id="S4.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">OKS@3D<math id="S4.T1.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T1.1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T1.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">MPJPE<math id="S4.T1.2.2.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T1.2.2.2.2.m1.1a"><mo stretchy="false" id="S4.T1.2.2.2.2.m1.1.1" xref="S4.T1.2.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.2.m1.1b"><ci id="S4.T1.2.2.2.2.m1.1.1.cmml" xref="S4.T1.2.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.2.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T1.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">OKS@2D<math id="S4.T1.3.3.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.3.3.3.3.m1.1a"><mo stretchy="false" id="S4.T1.3.3.3.3.m1.1.1" xref="S4.T1.3.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.3.m1.1b"><ci id="S4.T1.3.3.3.3.m1.1.1.cmml" xref="S4.T1.3.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.3.m1.1c">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T1.3.3.5.2" class="ltx_tr">
<td id="S4.T1.3.3.5.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">camera-only <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>
</td>
<td id="S4.T1.3.3.5.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">51.74%</td>
<td id="S4.T1.3.3.5.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">13.90cm</td>
<td id="S4.T1.3.3.5.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">78.19%</td>
</tr>
<tr id="S4.T1.3.3.6.3" class="ltx_tr">
<td id="S4.T1.3.3.6.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">LiDAR-only</td>
<td id="S4.T1.3.3.6.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">59.58%</td>
<td id="S4.T1.3.3.6.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10.80cm</td>
<td id="S4.T1.3.3.6.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">77.53%</td>
</tr>
<tr id="S4.T1.3.3.7.4" class="ltx_tr">
<td id="S4.T1.3.3.7.4.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">multi-modal</td>
<td id="S4.T1.3.3.7.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.3.3.7.4.2.1" class="ltx_text ltx_font_bold">63.14%</span></td>
<td id="S4.T1.3.3.7.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.3.3.7.4.3.1" class="ltx_text ltx_font_bold">10.32cm</span></td>
<td id="S4.T1.3.3.7.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.3.3.7.4.4.1" class="ltx_text ltx_font_bold">82.94%</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.5.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.6.2" class="ltx_text" style="font-size:90%;">Comparison of camera-only, LiDAR-only, and multi-modal models. As described in Section <a href="#S4.SS1" title="4.1 Data and Evaluation Metrics ‣ 4 Experiments ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>, OKS@3D stands for OKS/ACC in 3D evaluation, OKS@2D stands for OKS/ACC in 2D evaluation, and MPJPE is another evaluation metric in 3D. These metrics are used throughout the experiments. The proposed multi-modal model achieves the best results on both datasets.</span></figcaption>
</figure>
<figure id="S4.T2" class="ltx_table">
<div id="S4.T2.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:303.5pt;height:147.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-33.2pt,16.2pt) scale(0.820541668493661,0.820541668493661) ;">
<table id="S4.T2.2.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.2.1.1.1" class="ltx_tr">
<td id="S4.T2.2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T2.2.1.1.1.1.1" class="ltx_text">parts</span></td>
<td id="S4.T2.2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">camera-only</td>
<td id="S4.T2.2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">LiDAR-only</td>
<td id="S4.T2.2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">multi-modal</td>
</tr>
<tr id="S4.T2.2.1.2.2" class="ltx_tr">
<td id="S4.T2.2.1.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">OKS@3D</td>
<td id="S4.T2.2.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">OKS@2D</td>
<td id="S4.T2.2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">OKS@3D</td>
<td id="S4.T2.2.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">OKS@2D</td>
<td id="S4.T2.2.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">OKS@3D</td>
<td id="S4.T2.2.1.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">OKS@2D</td>
</tr>
<tr id="S4.T2.2.1.3.3" class="ltx_tr">
<td id="S4.T2.2.1.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">nose</td>
<td id="S4.T2.2.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">24.50%</td>
<td id="S4.T2.2.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.1.3.3.3.1" class="ltx_text ltx_font_bold">75.10%</span></td>
<td id="S4.T2.2.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">23.83%</td>
<td id="S4.T2.2.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">56.27%</td>
<td id="S4.T2.2.1.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.1.3.3.6.1" class="ltx_text ltx_font_bold">29.74%</span></td>
<td id="S4.T2.2.1.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">72.17%</td>
</tr>
<tr id="S4.T2.2.1.4.4" class="ltx_tr">
<td id="S4.T2.2.1.4.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">shoulder</td>
<td id="S4.T2.2.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">65.41%</td>
<td id="S4.T2.2.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">83.38%</td>
<td id="S4.T2.2.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.1.4.4.4.1" class="ltx_text ltx_font_bold">77.04%</span></td>
<td id="S4.T2.2.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">85.68%</td>
<td id="S4.T2.2.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">76.93%</td>
<td id="S4.T2.2.1.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.1.4.4.7.1" class="ltx_text ltx_font_bold">87.89%</span></td>
</tr>
<tr id="S4.T2.2.1.5.5" class="ltx_tr">
<td id="S4.T2.2.1.5.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">elbow</td>
<td id="S4.T2.2.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">65.61%</td>
<td id="S4.T2.2.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">82.63%</td>
<td id="S4.T2.2.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">66.61%</td>
<td id="S4.T2.2.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">78.72%</td>
<td id="S4.T2.2.1.5.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.1.5.5.6.1" class="ltx_text ltx_font_bold">72.49%</span></td>
<td id="S4.T2.2.1.5.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.1.5.5.7.1" class="ltx_text ltx_font_bold">84.82%</span></td>
</tr>
<tr id="S4.T2.2.1.6.6" class="ltx_tr">
<td id="S4.T2.2.1.6.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">wrist</td>
<td id="S4.T2.2.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">45.99%</td>
<td id="S4.T2.2.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">79.03%</td>
<td id="S4.T2.2.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">30.37%</td>
<td id="S4.T2.2.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">64.10%</td>
<td id="S4.T2.2.1.6.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.1.6.6.6.1" class="ltx_text ltx_font_bold">46.97%</span></td>
<td id="S4.T2.2.1.6.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.1.6.6.7.1" class="ltx_text ltx_font_bold">79.17%</span></td>
</tr>
<tr id="S4.T2.2.1.7.7" class="ltx_tr">
<td id="S4.T2.2.1.7.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">hip</td>
<td id="S4.T2.2.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">57.69%</td>
<td id="S4.T2.2.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">87.97%</td>
<td id="S4.T2.2.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.1.7.7.4.1" class="ltx_text ltx_font_bold">79.42%</span></td>
<td id="S4.T2.2.1.7.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">90.33%</td>
<td id="S4.T2.2.1.7.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">74.76%</td>
<td id="S4.T2.2.1.7.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.1.7.7.7.1" class="ltx_text ltx_font_bold">92.37%</span></td>
</tr>
<tr id="S4.T2.2.1.8.8" class="ltx_tr">
<td id="S4.T2.2.1.8.8.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">knee</td>
<td id="S4.T2.2.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">65.40%</td>
<td id="S4.T2.2.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">85.91%</td>
<td id="S4.T2.2.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">77.48%</td>
<td id="S4.T2.2.1.8.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">86.82%</td>
<td id="S4.T2.2.1.8.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.1.8.8.6.1" class="ltx_text ltx_font_bold">78.04%</span></td>
<td id="S4.T2.2.1.8.8.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.1.8.8.7.1" class="ltx_text ltx_font_bold">90.05%</span></td>
</tr>
<tr id="S4.T2.2.1.9.9" class="ltx_tr">
<td id="S4.T2.2.1.9.9.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">ankle</td>
<td id="S4.T2.2.1.9.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">62.68%</td>
<td id="S4.T2.2.1.9.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">84.17%</td>
<td id="S4.T2.2.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">69.06%</td>
<td id="S4.T2.2.1.9.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">85.63%</td>
<td id="S4.T2.2.1.9.9.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.1.9.9.6.1" class="ltx_text ltx_font_bold">72.30%</span></td>
<td id="S4.T2.2.1.9.9.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.1.9.9.7.1" class="ltx_text ltx_font_bold">88.72%</span></td>
</tr>
<tr id="S4.T2.2.1.10.10" class="ltx_tr">
<td id="S4.T2.2.1.10.10.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_tt">overall</td>
<td id="S4.T2.2.1.10.10.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">51.74%</td>
<td id="S4.T2.2.1.10.10.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">78.19%</td>
<td id="S4.T2.2.1.10.10.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">59.58%</td>
<td id="S4.T2.2.1.10.10.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">77.53%</td>
<td id="S4.T2.2.1.10.10.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt"><span id="S4.T2.2.1.10.10.6.1" class="ltx_text ltx_font_bold">63.14%</span></td>
<td id="S4.T2.2.1.10.10.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt"><span id="S4.T2.2.1.10.10.7.1" class="ltx_text ltx_font_bold">82.94%</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.3.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.4.2" class="ltx_text" style="font-size:90%;">Per-keypoint comparison of camera-only, LiDAR-only, and multi-modal models. OKS@3D is on the Waymo Open Dataset and OKS@2D is on the Internal Dataset. Note that the per-keypoint OKS is computed on each keypoint separately (please refer to supplementary for details). The proposed multi-modal model achieves the best results on most of the keypoint types.</span></figcaption>
</figure>
<figure id="S4.T3" class="ltx_table">
<div id="S4.T3.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:260.2pt;height:81.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-43.0pt,13.4pt) scale(0.751767606580182,0.751767606580182) ;">
<table id="S4.T3.3.3" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.3.3.4.1" class="ltx_tr">
<td id="S4.T3.3.3.4.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="3">Configurations</td>
<td id="S4.T3.3.3.4.1.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="2">Waymo Open Dataset</td>
<td id="S4.T3.3.3.4.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Internal Dataset</td>
</tr>
<tr id="S4.T3.3.3.3" class="ltx_tr">
<td id="S4.T3.3.3.3.4" class="ltx_td ltx_align_center ltx_border_l ltx_border_t">Reg. Loss</td>
<td id="S4.T3.3.3.3.5" class="ltx_td ltx_align_center ltx_border_t">Seg. Loss</td>
<td id="S4.T3.3.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Camera</td>
<td id="S4.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">OKS@3D<math id="S4.T3.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T3.1.1.1.1.m1.1.1" xref="S4.T3.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T3.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">MPJPE<math id="S4.T3.2.2.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T3.2.2.2.2.m1.1a"><mo stretchy="false" id="S4.T3.2.2.2.2.m1.1.1" xref="S4.T3.2.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.2.m1.1b"><ci id="S4.T3.2.2.2.2.m1.1.1.cmml" xref="S4.T3.2.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.2.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T3.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">OKS@2D<math id="S4.T3.3.3.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.3.3.3.3.m1.1a"><mo stretchy="false" id="S4.T3.3.3.3.3.m1.1.1" xref="S4.T3.3.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.3.m1.1b"><ci id="S4.T3.3.3.3.3.m1.1.1.cmml" xref="S4.T3.3.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.3.m1.1c">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T3.3.3.5.2" class="ltx_tr">
<td id="S4.T3.3.3.5.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_t">✓</td>
<td id="S4.T3.3.3.5.2.2" class="ltx_td ltx_border_t"></td>
<td id="S4.T3.3.3.5.2.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T3.3.3.5.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">59.10%</td>
<td id="S4.T3.3.3.5.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10.93cm</td>
<td id="S4.T3.3.3.5.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">77.52%</td>
</tr>
<tr id="S4.T3.3.3.6.3" class="ltx_tr">
<td id="S4.T3.3.3.6.3.1" class="ltx_td ltx_align_center ltx_border_l">✓</td>
<td id="S4.T3.3.3.6.3.2" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T3.3.3.6.3.3" class="ltx_td ltx_border_r"></td>
<td id="S4.T3.3.3.6.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">59.58%</td>
<td id="S4.T3.3.3.6.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10.80cm</td>
<td id="S4.T3.3.3.6.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">77.53%</td>
</tr>
<tr id="S4.T3.3.3.7.4" class="ltx_tr">
<td id="S4.T3.3.3.7.4.1" class="ltx_td ltx_align_center ltx_border_l">✓</td>
<td id="S4.T3.3.3.7.4.2" class="ltx_td"></td>
<td id="S4.T3.3.3.7.4.3" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S4.T3.3.3.7.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">62.03%</td>
<td id="S4.T3.3.3.7.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10.53cm</td>
<td id="S4.T3.3.3.7.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">82.51%</td>
</tr>
<tr id="S4.T3.3.3.8.5" class="ltx_tr">
<td id="S4.T3.3.3.8.5.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l">✓</td>
<td id="S4.T3.3.3.8.5.2" class="ltx_td ltx_align_center ltx_border_b">✓</td>
<td id="S4.T3.3.3.8.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">✓</td>
<td id="S4.T3.3.3.8.5.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T3.3.3.8.5.4.1" class="ltx_text ltx_font_bold">63.14%</span></td>
<td id="S4.T3.3.3.8.5.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T3.3.3.8.5.5.1" class="ltx_text ltx_font_bold">10.32cm</span></td>
<td id="S4.T3.3.3.8.5.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T3.3.3.8.5.6.1" class="ltx_text ltx_font_bold">82.94%</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.5.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S4.T3.6.2" class="ltx_text" style="font-size:90%;">Ablation studies on different model architectures. The best performance is achieved by using multi-modal architecture with auxiliary segmentation loss.</span></figcaption>
</figure>
</section>
<section id="S4.SS1.SSS0.Px5" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Implementation Details:</h5>

<div id="S4.SS1.SSS0.Px5.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px5.p1.1" class="ltx_p">For the Waymo Open Dataset and the Internal Dataset, we resize all camera images to <math id="S4.SS1.SSS0.Px5.p1.1.m1.1" class="ltx_Math" alttext="256\times 256" display="inline"><semantics id="S4.SS1.SSS0.Px5.p1.1.m1.1a"><mrow id="S4.SS1.SSS0.Px5.p1.1.m1.1.1" xref="S4.SS1.SSS0.Px5.p1.1.m1.1.1.cmml"><mn id="S4.SS1.SSS0.Px5.p1.1.m1.1.1.2" xref="S4.SS1.SSS0.Px5.p1.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.SSS0.Px5.p1.1.m1.1.1.1" xref="S4.SS1.SSS0.Px5.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS1.SSS0.Px5.p1.1.m1.1.1.3" xref="S4.SS1.SSS0.Px5.p1.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px5.p1.1.m1.1b"><apply id="S4.SS1.SSS0.Px5.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px5.p1.1.m1.1.1"><times id="S4.SS1.SSS0.Px5.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS0.Px5.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS1.SSS0.Px5.p1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS0.Px5.p1.1.m1.1.1.2">256</cn><cn type="integer" id="S4.SS1.SSS0.Px5.p1.1.m1.1.1.3.cmml" xref="S4.SS1.SSS0.Px5.p1.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px5.p1.1.m1.1c">256\times 256</annotation></semantics></math>, and randomly sub-sample the input point cloud to a fixed size of 256 points (we did not observe obvious performance gain for larger number of points). Please refer to the supplementary material for more training details.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Performance Analysis</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">To show the effectiveness of the proposed method, we compare with the following models.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p"><span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_bold">Camera-only model:</span> we use the same camera network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> as the proposed method to predict 2D keypoints. Then 2D-to-3D keypoint lifting is implemented by the 2D-to-3D pseudo label generation method introduced in Section <a href="#S3.SS4.SSS1" title="3.4.1 Label Generation ‣ 3.4 Weakly-Supervised Model Training ‣ 3 Method ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4.1</span></a>, the same way as we generate training labels.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p"><span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_bold">LiDAR-only model:</span> we use the proposed point network to predict 3D keypoints without the modality fusion, i.e. only use 3D coordinates of the point clouds as features.</p>
</div>
<figure id="S4.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F5.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2112.12141/assets/baseline.png" id="S4.F5.sf1.g1" class="ltx_graphics ltx_img_square" width="685" height="685" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F5.sf1.3.2" class="ltx_text" style="font-size:90%;">No Camera (LiDAR-Only)</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F5.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2112.12141/assets/i48.png" id="S4.F5.sf2.g1" class="ltx_graphics ltx_img_square" width="685" height="685" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F5.sf2.3.2" class="ltx_text" style="font-size:90%;">Inception 48x48</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F5.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2112.12141/assets/i64.png" id="S4.F5.sf3.g1" class="ltx_graphics ltx_img_square" width="685" height="685" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S4.F5.sf3.3.2" class="ltx_text" style="font-size:90%;">Inception 64x64</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F5.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2112.12141/assets/r50.png" id="S4.F5.sf4.g1" class="ltx_graphics ltx_img_square" width="685" height="685" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S4.F5.sf4.3.2" class="ltx_text" style="font-size:90%;">ResNet50 256x256</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.3.2" class="ltx_text" style="font-size:90%;">3D predictions with different camera image sizes and camera network backbones from the Waymo Open Dataset (best viewed in color). ResNet50 with 256x256 image size predicts the most accurate keypoints.</span></figcaption>
</figure>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">Experimental results on two datasets are shown in Table <a href="#S4.T1" title="Table 1 ‣ Labeling: ‣ 4.1 Data and Evaluation Metrics ‣ 4 Experiments ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Table <a href="#S4.T2" title="Table 2 ‣ Labeling: ‣ 4.1 Data and Evaluation Metrics ‣ 4 Experiments ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> further shows per-keypoint results.
These results
show that our method outperforms all baselines in the corresponding datasets. We also have the following observations.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.1" class="ltx_p"><span id="S4.SS2.p5.1.1" class="ltx_text ltx_font_bold">Training on pseudo labels is effective.</span> LiDAR-only baseline and the proposed method both outperform camera-only baseline on 3D metrics on the Waymo Open Dataset. Since the camera-only baseline is also trained on 2D labels and utilizes point clouds to lift the predictions to 3D space, the results indicate that it is more effective to directly train a 3D human pose model on pseudo labels generated from 2D ground truth.</p>
</div>
<div id="S4.SS2.p6" class="ltx_para">
<p id="S4.SS2.p6.1" class="ltx_p"><span id="S4.SS2.p6.1.1" class="ltx_text ltx_font_bold">Camera image improves 3D prediction.</span> The proposed method performs better than LiDAR-only baseline on 3D metrics, which demonstrates that the information from 2D camera images helps 3D pose estimation. Table <a href="#S4.T2" title="Table 2 ‣ Labeling: ‣ 4.1 Data and Evaluation Metrics ‣ 4 Experiments ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows that the proposed method outperforms baselines on almost all body parts. Compared to the LiDAR-only baseline, the margins are larger for difficult body parts like <em id="S4.SS2.p6.1.2" class="ltx_emph ltx_font_italic">elbows</em> or <em id="S4.SS2.p6.1.3" class="ltx_emph ltx_font_italic">wrists</em>, which shows that texture information from camera images is especially helpful for keypoints that are hard to localize.</p>
</div>
<div id="S4.SS2.p7" class="ltx_para">
<p id="S4.SS2.p7.1" class="ltx_p"><span id="S4.SS2.p7.1.1" class="ltx_text ltx_font_bold">Point cloud improves 2D prediction.</span> LiDAR-only baseline has comparable performance with the camera-only baseline for 2D pose estimation on 2D metrics on the Internal Dataset. The proposed method surpasses the camera-only baseline, even if the models are not directly trained for 2D pose estimation. It shows that the depth information from 3D LiDAR point clouds also improves 2D pose estimation performance.</p>
</div>
<div id="S4.SS2.p8" class="ltx_para">
<p id="S4.SS2.p8.1" class="ltx_p"><span id="S4.SS2.p8.1.1" class="ltx_text ltx_font_bold">Modality fusion benefits from both modalities.</span> The proposed method achieves the best performance on all metrics for both datasets. It proves that camera images and LiDAR point clouds provide complementary information, and modality fusion combines these sources of information to improve the overall performance.</p>
</div>
<figure id="S4.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S4.F6.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2112.12141/assets/occlusion_2d_1.png" id="S4.F6.sf1.g1" class="ltx_graphics ltx_img_square" width="685" height="685" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S4.F6.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2112.12141/assets/occlusion_1.png" id="S4.F6.sf2.g1" class="ltx_graphics ltx_img_portrait" width="685" height="892" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S4.F6.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2112.12141/assets/occlusion_2d_3.png" id="S4.F6.sf3.g1" class="ltx_graphics ltx_img_square" width="685" height="685" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S4.F6.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2112.12141/assets/occlusion_3.png" id="S4.F6.sf4.g1" class="ltx_graphics ltx_img_square" width="685" height="673" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S4.F6.sf5" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2112.12141/assets/new_example_2.png" id="S4.F6.sf5.g1" class="ltx_graphics ltx_img_square" width="685" height="685" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.sf5.2.1.1" class="ltx_text" style="font-size:90%;">(e)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S4.F6.sf6" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2112.12141/assets/new_example_2_3d.png" id="S4.F6.sf6.g1" class="ltx_graphics ltx_img_square" width="685" height="725" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.sf6.2.1.1" class="ltx_text" style="font-size:90%;">(f)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S4.F6.sf7" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2112.12141/assets/occlusion_2d_4.png" id="S4.F6.sf7.g1" class="ltx_graphics ltx_img_square" width="685" height="685" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.sf7.2.1.1" class="ltx_text" style="font-size:90%;">(g)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S4.F6.sf8" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2112.12141/assets/occlusion_4.png" id="S4.F6.sf8.g1" class="ltx_graphics ltx_img_square" width="685" height="847" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.sf8.2.1.1" class="ltx_text" style="font-size:90%;">(h)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S4.F6.sf9" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2112.12141/assets/new_example_3.png" id="S4.F6.sf9.g1" class="ltx_graphics ltx_img_square" width="685" height="685" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.sf9.2.1.1" class="ltx_text" style="font-size:90%;">(i)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S4.F6.sf10" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2112.12141/assets/new_example_3_3d.png" id="S4.F6.sf10.g1" class="ltx_graphics ltx_img_square" width="685" height="678" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.sf10.2.1.1" class="ltx_text" style="font-size:90%;">(j)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S4.F6.sf11" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2112.12141/assets/occlusion_2d_5.png" id="S4.F6.sf11.g1" class="ltx_graphics ltx_img_square" width="685" height="685" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.sf11.2.1.1" class="ltx_text" style="font-size:90%;">(k)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S4.F6.sf12" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2112.12141/assets/occlusion_5.png" id="S4.F6.sf12.g1" class="ltx_graphics ltx_img_portrait" width="685" height="891" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.sf12.2.1.1" class="ltx_text" style="font-size:90%;">(l)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.3.2" class="ltx_text" style="font-size:90%;">Results on the Waymo Open Dataset. <a href="#S4.F6.sf2" title="Figure 6(b) ‣ Figure 6 ‣ 4.2 Performance Analysis ‣ 4 Experiments ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(b)</span></a>, <a href="#S4.F6.sf4" title="Figure 6(d) ‣ Figure 6 ‣ 4.2 Performance Analysis ‣ 4 Experiments ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(d)</span></a>, <a href="#S4.F6.sf8" title="Figure 6(h) ‣ Figure 6 ‣ 4.2 Performance Analysis ‣ 4 Experiments ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(h)</span></a>, <a href="#S4.F6.sf12" title="Figure 6(l) ‣ Figure 6 ‣ 4.2 Performance Analysis ‣ 4 Experiments ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(l)</span></a> are 3D predictions. <a href="#S4.F6.sf1" title="Figure 6(a) ‣ Figure 6 ‣ 4.2 Performance Analysis ‣ 4 Experiments ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(a)</span></a>, <a href="#S4.F6.sf3" title="Figure 6(c) ‣ Figure 6 ‣ 4.2 Performance Analysis ‣ 4 Experiments ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(c)</span></a>, <a href="#S4.F6.sf7" title="Figure 6(g) ‣ Figure 6 ‣ 4.2 Performance Analysis ‣ 4 Experiments ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(g)</span></a>, <a href="#S4.F6.sf11" title="Figure 6(k) ‣ Figure 6 ‣ 4.2 Performance Analysis ‣ 4 Experiments ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(k)</span></a> show the corresponding 2D projections overlaid on camera images (3D predictions may not be shown under the same viewpoint as the camera images. Best viewed in color). More results can be found in supplementary.</span></figcaption>
</figure>
<div id="S4.SS2.p9" class="ltx_para">
<p id="S4.SS2.p9.1" class="ltx_p">Figure <a href="#S4.F6" title="Figure 6 ‣ 4.2 Performance Analysis ‣ 4 Experiments ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows some qualitative results of the proposed method on the Waymo Open Dataset. In these examples, pedestrians are either occluded (<a href="#S4.F6.sf1" title="Figure 6(a) ‣ Figure 6 ‣ 4.2 Performance Analysis ‣ 4 Experiments ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(a)</span></a>), in an irregular pose (<a href="#S4.F6.sf3" title="Figure 6(c) ‣ Figure 6 ‣ 4.2 Performance Analysis ‣ 4 Experiments ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(c)</span></a>, <a href="#S4.F6.sf9" title="Figure 6(i) ‣ Figure 6 ‣ 4.2 Performance Analysis ‣ 4 Experiments ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(i)</span></a>), or carrying a large object (<a href="#S4.F6.sf7" title="Figure 6(g) ‣ Figure 6 ‣ 4.2 Performance Analysis ‣ 4 Experiments ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(g)</span></a>, <a href="#S4.F6.sf5" title="Figure 6(e) ‣ Figure 6 ‣ 4.2 Performance Analysis ‣ 4 Experiments ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(e)</span></a>). The proposed method accurately predicts the visible human keypoints and provides reasonable guesses for the occluded keypoints. Figure <a href="#S4.F6.sf11" title="Figure 6(k) ‣ Figure 6 ‣ 4.2 Performance Analysis ‣ 4 Experiments ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(k)</span></a> is a failure case where the camera image is blurred because of the sensor motion. It causes an inaccurate prediction of the left wrist. More qualitative results can be found in the supplementary.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Ablation Studies</h3>

<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Ablation Study on Model Architecture</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.1" class="ltx_p">We conduct ablation studies to further demonstrate the effectiveness of our key designs: the auxiliary segmentation branch and the modality fusion with camera network. The results are shown in Table <a href="#S4.T3" title="Table 3 ‣ Labeling: ‣ 4.1 Data and Evaluation Metrics ‣ 4 Experiments ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, where <em id="S4.SS3.SSS1.p1.1.1" class="ltx_emph ltx_font_italic">Reg. Loss</em> means using regression loss (the primary loss) to train the point network, <em id="S4.SS3.SSS1.p1.1.2" class="ltx_emph ltx_font_italic">Seg. Loss</em> means auxiliary segmentation branch being added (see Section <a href="#S3.SS5" title="3.5 Training Losses ‣ 3 Method ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.5</span></a>), and <em id="S4.SS3.SSS1.p1.1.3" class="ltx_emph ltx_font_italic">Camera</em> means using modality fusion with camera features. The results show that, by adding key features to the model, the performance improves consistently on all datasets. We also observe that the segmentation branch and modality fusion provide complementary improvements.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<div id="S4.T4.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:347.8pt;height:163.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-17.9pt,8.4pt) scale(0.906443299850528,0.906443299850528) ;">
<table id="S4.T4.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T4.2.1.1.1" class="ltx_tr">
<th id="S4.T4.2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" rowspan="10"><span id="S4.T4.2.1.1.1.1.1" class="ltx_text">
<span id="S4.T4.2.1.1.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.9pt;height:97.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:97.7pt;transform:translate(-44.42pt,-43.44pt) rotate(-90deg) ;">
<span id="S4.T4.2.1.1.1.1.1.1.1" class="ltx_p">Per-keypoint MPJPEs</span>
</span></span></span></th>
<th id="S4.T4.2.1.1.1.2" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t"></th>
<td id="S4.T4.2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="4">Camera Network</td>
</tr>
<tr id="S4.T4.2.1.2.2" class="ltx_tr">
<th id="S4.T4.2.1.2.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t"></th>
<td id="S4.T4.2.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">No camera</td>
<td id="S4.T4.2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Inception 48x48</td>
<td id="S4.T4.2.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Inception 64x64</td>
<td id="S4.T4.2.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">ResNet50 256x256</td>
</tr>
<tr id="S4.T4.2.1.3.3" class="ltx_tr">
<th id="S4.T4.2.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><em id="S4.T4.2.1.3.3.1.1" class="ltx_emph ltx_font_italic">all</em></th>
<td id="S4.T4.2.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.1080</td>
<td id="S4.T4.2.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.2.1.3.3.3.1" class="ltx_text ltx_font_bold">0.1026</span></td>
<td id="S4.T4.2.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.1028</td>
<td id="S4.T4.2.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.1032</td>
</tr>
<tr id="S4.T4.2.1.4.4" class="ltx_tr">
<th id="S4.T4.2.1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><em id="S4.T4.2.1.4.4.1.1" class="ltx_emph ltx_font_italic">elbow</em></th>
<td id="S4.T4.2.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.1006</td>
<td id="S4.T4.2.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.0940</td>
<td id="S4.T4.2.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.0931</td>
<td id="S4.T4.2.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.2.1.4.4.5.1" class="ltx_text ltx_font_bold">0.0891</span></td>
</tr>
<tr id="S4.T4.2.1.5.5" class="ltx_tr">
<th id="S4.T4.2.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><em id="S4.T4.2.1.5.5.1.1" class="ltx_emph ltx_font_italic">wrist</em></th>
<td id="S4.T4.2.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.1652</td>
<td id="S4.T4.2.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.1501</td>
<td id="S4.T4.2.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.1473</td>
<td id="S4.T4.2.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.2.1.5.5.5.1" class="ltx_text ltx_font_bold">0.1320</span></td>
</tr>
<tr id="S4.T4.2.1.6.6" class="ltx_tr">
<th id="S4.T4.2.1.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><em id="S4.T4.2.1.6.6.1.1" class="ltx_emph ltx_font_italic">hip</em></th>
<td id="S4.T4.2.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.2.1.6.6.2.1" class="ltx_text ltx_font_bold">0.1081</span></td>
<td id="S4.T4.2.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.1113</td>
<td id="S4.T4.2.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.1113</td>
<td id="S4.T4.2.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.1205</td>
</tr>
<tr id="S4.T4.2.1.7.7" class="ltx_tr">
<th id="S4.T4.2.1.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><em id="S4.T4.2.1.7.7.1.1" class="ltx_emph ltx_font_italic">knee</em></th>
<td id="S4.T4.2.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.0944</td>
<td id="S4.T4.2.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.2.1.7.7.3.1" class="ltx_text ltx_font_bold">0.0896</span></td>
<td id="S4.T4.2.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.0910</td>
<td id="S4.T4.2.1.7.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.0925</td>
</tr>
<tr id="S4.T4.2.1.8.8" class="ltx_tr">
<th id="S4.T4.2.1.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><em id="S4.T4.2.1.8.8.1.1" class="ltx_emph ltx_font_italic">ankle</em></th>
<td id="S4.T4.2.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.1163</td>
<td id="S4.T4.2.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.2.1.8.8.3.1" class="ltx_text ltx_font_bold">0.1100</span></td>
<td id="S4.T4.2.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.1102</td>
<td id="S4.T4.2.1.8.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.1107</td>
</tr>
<tr id="S4.T4.2.1.9.9" class="ltx_tr">
<th id="S4.T4.2.1.9.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><em id="S4.T4.2.1.9.9.1.1" class="ltx_emph ltx_font_italic">nose</em></th>
<td id="S4.T4.2.1.9.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.0814</td>
<td id="S4.T4.2.1.9.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.0762</td>
<td id="S4.T4.2.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.2.1.9.9.4.1" class="ltx_text ltx_font_bold">0.0760</span></td>
<td id="S4.T4.2.1.9.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.0837</td>
</tr>
<tr id="S4.T4.2.1.10.10" class="ltx_tr">
<th id="S4.T4.2.1.10.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t"><em id="S4.T4.2.1.10.10.1.1" class="ltx_emph ltx_font_italic">shoulder</em></th>
<td id="S4.T4.2.1.10.10.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.0850</td>
<td id="S4.T4.2.1.10.10.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T4.2.1.10.10.3.1" class="ltx_text ltx_font_bold">0.0814</span></td>
<td id="S4.T4.2.1.10.10.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.0830</td>
<td id="S4.T4.2.1.10.10.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.0872</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.5.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S4.T4.6.2" class="ltx_text" style="font-size:90%;">Per-keypoint performance with different camera networks and image sizes on the Waymo Open Dataset. ResNet50 with 256x256 image size performs the best on challenging keypoints like <em id="S4.T4.6.2.1" class="ltx_emph ltx_font_italic">elbow</em> and <em id="S4.T4.6.2.2" class="ltx_emph ltx_font_italic">wrist</em> with large margins, but slightly worse than smaller image sizes on other keypoint types.</span></figcaption>
</figure>
<figure id="S4.T5" class="ltx_table">
<div id="S4.T5.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:281.9pt;height:146pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-32.8pt,17.0pt) scale(0.811273304677456,0.811273304677456) ;">
<table id="S4.T5.3.3" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T5.3.3.4.1" class="ltx_tr">
<td id="S4.T5.3.3.4.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T5.3.3.4.1.1.1" class="ltx_text">Camera Network</span></td>
<td id="S4.T5.3.3.4.1.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="2">Config</td>
<td id="S4.T5.3.3.4.1.3" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="2">Waymo Open Dataset</td>
<td id="S4.T5.3.3.4.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Internal Dataset</td>
</tr>
<tr id="S4.T5.3.3.3" class="ltx_tr">
<td id="S4.T5.3.3.3.4" class="ltx_td ltx_align_center ltx_border_t">Reg.</td>
<td id="S4.T5.3.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Seg.</td>
<td id="S4.T5.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">OKS@3D<math id="S4.T5.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T5.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T5.1.1.1.1.m1.1.1" xref="S4.T5.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T5.1.1.1.1.m1.1b"><ci id="S4.T5.1.1.1.1.m1.1.1.cmml" xref="S4.T5.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T5.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">MPJPE<math id="S4.T5.2.2.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T5.2.2.2.2.m1.1a"><mo stretchy="false" id="S4.T5.2.2.2.2.m1.1.1" xref="S4.T5.2.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T5.2.2.2.2.m1.1b"><ci id="S4.T5.2.2.2.2.m1.1.1.cmml" xref="S4.T5.2.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.2.2.2.2.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T5.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">OKS@2D<math id="S4.T5.3.3.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T5.3.3.3.3.m1.1a"><mo stretchy="false" id="S4.T5.3.3.3.3.m1.1.1" xref="S4.T5.3.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T5.3.3.3.3.m1.1b"><ci id="S4.T5.3.3.3.3.m1.1.1.cmml" xref="S4.T5.3.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.3.3.3.3.m1.1c">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T5.3.3.5.2" class="ltx_tr">
<td id="S4.T5.3.3.5.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T5.3.3.5.2.1.1" class="ltx_text">No Camera</span></td>
<td id="S4.T5.3.3.5.2.2" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S4.T5.3.3.5.2.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T5.3.3.5.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">59.10%</td>
<td id="S4.T5.3.3.5.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10.93cm</td>
<td id="S4.T5.3.3.5.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">77.52%</td>
</tr>
<tr id="S4.T5.3.3.6.3" class="ltx_tr">
<td id="S4.T5.3.3.6.3.1" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T5.3.3.6.3.2" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S4.T5.3.3.6.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">59.58%</td>
<td id="S4.T5.3.3.6.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10.80cm</td>
<td id="S4.T5.3.3.6.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">77.53%</td>
</tr>
<tr id="S4.T5.3.3.7.4" class="ltx_tr">
<td id="S4.T5.3.3.7.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T5.3.3.7.4.1.1" class="ltx_text">Inception 48x48</span></td>
<td id="S4.T5.3.3.7.4.2" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S4.T5.3.3.7.4.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T5.3.3.7.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">61.12%</td>
<td id="S4.T5.3.3.7.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10.51cm</td>
<td id="S4.T5.3.3.7.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">78.72%</td>
</tr>
<tr id="S4.T5.3.3.8.5" class="ltx_tr">
<td id="S4.T5.3.3.8.5.1" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T5.3.3.8.5.2" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S4.T5.3.3.8.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">62.22%</td>
<td id="S4.T5.3.3.8.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.3.3.8.5.4.1" class="ltx_text ltx_font_bold">10.26cm</span></td>
<td id="S4.T5.3.3.8.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">79.55%</td>
</tr>
<tr id="S4.T5.3.3.9.6" class="ltx_tr">
<td id="S4.T5.3.3.9.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T5.3.3.9.6.1.1" class="ltx_text">Inception 64x64</span></td>
<td id="S4.T5.3.3.9.6.2" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S4.T5.3.3.9.6.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T5.3.3.9.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">61.05%</td>
<td id="S4.T5.3.3.9.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10.46cm</td>
<td id="S4.T5.3.3.9.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">78.95%</td>
</tr>
<tr id="S4.T5.3.3.10.7" class="ltx_tr">
<td id="S4.T5.3.3.10.7.1" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T5.3.3.10.7.2" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S4.T5.3.3.10.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">62.52%</td>
<td id="S4.T5.3.3.10.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10.28cm</td>
<td id="S4.T5.3.3.10.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">79.44%</td>
</tr>
<tr id="S4.T5.3.3.11.8" class="ltx_tr">
<td id="S4.T5.3.3.11.8.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T5.3.3.11.8.1.1" class="ltx_text">ResNet50 256x256</span></td>
<td id="S4.T5.3.3.11.8.2" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S4.T5.3.3.11.8.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T5.3.3.11.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">62.03%</td>
<td id="S4.T5.3.3.11.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10.53cm</td>
<td id="S4.T5.3.3.11.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">82.51%</td>
</tr>
<tr id="S4.T5.3.3.12.9" class="ltx_tr">
<td id="S4.T5.3.3.12.9.1" class="ltx_td ltx_align_center ltx_border_b">✓</td>
<td id="S4.T5.3.3.12.9.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">✓</td>
<td id="S4.T5.3.3.12.9.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T5.3.3.12.9.3.1" class="ltx_text ltx_font_bold">63.14%</span></td>
<td id="S4.T5.3.3.12.9.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">10.32cm</td>
<td id="S4.T5.3.3.12.9.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T5.3.3.12.9.5.1" class="ltx_text ltx_font_bold">82.94%</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T5.5.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="S4.T5.6.2" class="ltx_text" style="font-size:90%;">Ablation studies on the different camera image sizes and camera network backbones. ResNet50 with 256x256 image size achieves the best performance in general.</span></figcaption>
</figure>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Ablation Study on Camera Image Size and Camera Network Backbone</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.1" class="ltx_p">To study the effectiveness of modality fusion, experiments are conducted with different camera image sizes and camera network backbones with results in Table <a href="#S4.T5" title="Table 5 ‣ 4.3.1 Ablation Study on Model Architecture ‣ 4.3 Ablation Studies ‣ 4 Experiments ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. Here <em id="S4.SS3.SSS2.p1.1.1" class="ltx_emph ltx_font_italic">Inception 48x48</em> uses an Inception<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>-inspired convolutional network backbone with a 48x48 image size; <em id="S4.SS3.SSS2.p1.1.2" class="ltx_emph ltx_font_italic">Inception 64x64</em> is similar to <em id="S4.SS3.SSS2.p1.1.3" class="ltx_emph ltx_font_italic">Inception 48x48</em> but with a 64x64 image size; <em id="S4.SS3.SSS2.p1.1.4" class="ltx_emph ltx_font_italic">ResNet50 256x256</em> is the ResNet50 backbone used in the proposed method with a 256x256 image size. From the results in Table <a href="#S4.T5" title="Table 5 ‣ 4.3.1 Ablation Study on Model Architecture ‣ 4.3 Ablation Studies ‣ 4 Experiments ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we observe that, even with smaller camera patch size and shallower backbone, the model still benefits from the additional camera modality. This observation is consistent with or without the auxiliary segmentation branch. With larger camera patch size and deeper backbone network, the overall performance is better.</p>
</div>
<div id="S4.SS3.SSS2.p2" class="ltx_para">
<p id="S4.SS3.SSS2.p2.1" class="ltx_p">We further studied the effect of different image sizes and network backbones on per-keypoint prediction errors in Table <a href="#S4.T4" title="Table 4 ‣ 4.3.1 Ablation Study on Model Architecture ‣ 4.3 Ablation Studies ‣ 4 Experiments ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. These experiments are all with the proposed auxiliary segmentation branch. The results show that 1) Despite the choice of image size and backbone, addtional camera images generally bring considerable improvements on <em id="S4.SS3.SSS2.p2.1.1" class="ltx_emph ltx_font_italic">elbow</em>, <em id="S4.SS3.SSS2.p2.1.2" class="ltx_emph ltx_font_italic">wrist</em>, <em id="S4.SS3.SSS2.p2.1.3" class="ltx_emph ltx_font_italic">knee</em> and <em id="S4.SS3.SSS2.p2.1.4" class="ltx_emph ltx_font_italic">ankle</em>. This is because merely based on sparse and noisy LiDAR point clouds, accurately localizing these limb keypoints is difficult. Additional texture information from camera images makes the localization relatively easier. 2) Larger image size has better performance on most difficult keypoints like <em id="S4.SS3.SSS2.p2.1.5" class="ltx_emph ltx_font_italic">elbow</em> and <em id="S4.SS3.SSS2.p2.1.6" class="ltx_emph ltx_font_italic">wrist</em>. Surprisingly, it performs slightly worse than smaller patch sizes on other keypoints.</p>
</div>
<div id="S4.SS3.SSS2.p3" class="ltx_para">
<p id="S4.SS3.SSS2.p3.1" class="ltx_p">Figure <a href="#S4.F5" title="Figure 5 ‣ 4.2 Performance Analysis ‣ 4 Experiments ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows visualizations of 3D keypoint predictions on a pedestrian riding a scooter from the Waymo Open Dataset. It is a challenging case because of the objects (backpack, scooter) attached to the pedestrian and the irregular pose. The LiDAR-only model fails to predict accurate keypoints in Figure <a href="#S4.F5.sf1" title="Figure 5(a) ‣ Figure 5 ‣ 4.2 Performance Analysis ‣ 4 Experiments ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5(a)</span></a>. By introducing modality fusion, improvements are observed on keypoints that are difficult to localize from sparse point clouds like those on the limbs (<em id="S4.SS3.SSS2.p3.1.1" class="ltx_emph ltx_font_italic">elbow</em>, <em id="S4.SS3.SSS2.p3.1.2" class="ltx_emph ltx_font_italic">wrist</em>, <em id="S4.SS3.SSS2.p3.1.3" class="ltx_emph ltx_font_italic">knee</em> and <em id="S4.SS3.SSS2.p3.1.4" class="ltx_emph ltx_font_italic">ankle</em>). The camera network used in the proposed method (ResNet50 on 256x256 images) predicts the most accurate keypoints (Figure <a href="#S4.F5.sf4" title="Figure 5(d) ‣ Figure 5 ‣ 4.2 Performance Analysis ‣ 4 Experiments ‣ Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5(d)</span></a>).</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">LiDAR based 3D HPE in AV differs from other applications for a variety of reasons including 3D resolution and range, absence of dense depth maps, and variation in test conditions. In this paper, we propose a multi-modal 3D HPE model with 2D weak supervision for autonomous driving. The model leverages both RGB camera images and LiDAR point clouds to tackle the challenges of 3D human pose estimation in unconstrained scenarios. Instead of using expensive 3D labels, the proposed model is trained on pure 2D labels. An auxiliary segmentation branch is added to introduce stronger supervision to the point network. Results on the Waymo Open Dataset (with evaluation labels to be released) and our internal dataset, and additional ablation studies showing the effectiveness of the proposed method.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Ijaz Akhter and Michael J. Black.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Pose-conditioned joint angle limits for 3d human pose reconstruction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, pages
1446–1455, 2015.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Anurag Arnab, Carl Doersch, and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Exploiting temporal context for 3d human pose estimation in the wild.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</span><span id="bib.bib2.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
C. Chen and D. Ramanan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">3d human pose estimation = 2d pose estimation + matching.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, pages
5759–5767, 2017.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Ching-Hang Chen, Ambrish Tyagi, Amit Agrawal, Dylan Drover, M. V. Rohith,
Stefan Stojanov, and James M. Rehg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Unsupervised 3d pose estimation with geometric self-supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</span><span id="bib.bib4.4.2" class="ltx_text" style="font-size:90%;">, pages
5707–5717, 2019.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Yu Cheng, Bo Yang, Bo Wang, and Robby T. Tan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">3d human pose estimation using spatio-temporal networks with explicit
occlusion training.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">AAAI</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Srijan Das, Saurav Sharma, Rui Dai, Francois Bremond, and Monique Thonnat.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Vpn: Learning video-pose embedding for activities of daily living,
2020.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Yang Feiyu, Song Zhan, Xiao Zhenzhong, Mo Yaoyang, Chen Yu, Pan Zhe, Zhang Min,
Zhang Yao, Qian Beibei, and Jin Wu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Error compensation heatmap decoding for human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Access</span><span id="bib.bib7.4.2" class="ltx_text" style="font-size:90%;">, 9:114514–114522, 2021.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Michael Fürst, Shriya T. P. Gupta, René Schuster, Oliver
Wasenmüller, and Didier Stricker.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">HPERL: 3d human pose estimation from RGB and lidar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Computing Research Repository</span><span id="bib.bib8.4.2" class="ltx_text" style="font-size:90%;">, abs/2010.08221, 2020.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Liuhao Ge, Yujun Cai, Junwu Weng, and Junsong Yuan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Hand pointnet: 3d hand pose estimation using point sets.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Liuhao Ge, Zhou Ren, and Junsong Yuan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Point-to-point regression pointnet for 3d hand pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Eur. Conf. Comput. Vis. (ECCV)</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Yulan Guo, Hanyun Wang, Qingyong Hu, Hao Liu, Li Liu, and Mohammed Bennamoun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Deep learning for 3d point clouds: A survey.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span id="bib.bib11.4.2" class="ltx_text" style="font-size:90%;">,
43(12):4338–4364, 2021.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Mir Rayat Imtiaz Hossain and J. Little.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Exploiting temporal information for 3d human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Eur. Conf. Comput. Vis. (ECCV)</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Human3.6m: Large scale datasets and predictive methods for 3d human
sensing in natural environments.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span id="bib.bib13.4.2" class="ltx_text" style="font-size:90%;">,
36(7):1325–1339, jul 2014.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Muhammed Kocabas, Salih Karagoz, and Emre Akbas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Self-supervised learning of 3d human pose using multi-view geometry.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</span><span id="bib.bib14.4.2" class="ltx_text" style="font-size:90%;">, pages
1077–1086, 2019.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Shichao Li, Lei Ke, Kevin Pratama, Yu-Wing Tai, Chi-Keung Tang, and Kwang-Ting
Cheng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Cascaded deep monocular 3d human pose estimation with evolutionary
training data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, June 2020.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C. Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Microsoft coco: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Eur. Conf. Comput. Vis. (ECCV)</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, pages 740–755, 2014.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Jingyuan Liu, Hongbo Fu, and Chiew-Lan Tai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Posetween: Pose-driven tween animation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 33rd Annual ACM Symposium on User
Interface Software and Technology</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, UIST ’20, page 791–804, New York, NY,
USA, 2020. Association for Computing Machinery.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Julieta Martinez, Rayat Hossain, Javier Romero, and James J. Little.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">A simple yet effective baseline for 3d human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. Conf. Comput. Vis. (ICCV)</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Georgios Pavlakos, Xiaowei Zhou, and Kostas Daniilidis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Ordinal depth supervision for 3D human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Dario Pavllo, Christoph Feichtenhofer, David Grangier, and Michael Auli.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">3d human pose estimation in video with temporal convolutions and
semi-supervised training.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Charles R. Qi, Wei Liu, Chenxia Wu, Hao Su, and Leonidas J. Guibas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Frustum pointnets for 3d object detection from rgb-d data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, June 2018.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Pointnet: Deep learning on point sets for 3d classification and
segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1612.00593</span><span id="bib.bib22.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Charles R Qi, Li Yi, Hao Su, and Leonidas J Guibas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Pointnet++: Deep hierarchical feature learning on point sets in a
metric space.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1706.02413</span><span id="bib.bib23.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Varun Ramakrishna, Takeo Kanade, and Yaser Sheikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Reconstructing 3d human pose from 2d image landmarks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In Andrew Fitzgibbon, Svetlana Lazebnik, Pietro Perona, Yoichi Sato,
and Cordelia Schmid, editors, </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Eur. Conf. Comput. Vis. (ECCV)</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, pages
573–586, Berlin, Heidelberg, 2012. Springer Berlin Heidelberg.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Helge Rhodin, Mathieu Salzmann, and Pascal Fua.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Unsupervised geometry-aware representation learning for 3d human pose
estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Eur. Conf. Comput. Vis. (ECCV)</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
H. Rhodin, Jörg Spörri, Isinsu Katircioglu, V. Constantin, F. Meyer, E.
Müller, M. Salzmann, and P. Fua.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Learning monocular 3d human pose estimation from multi-view images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</span><span id="bib.bib26.4.2" class="ltx_text" style="font-size:90%;">, pages
8437–8446, 2018.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai
Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, Vijay
Vasudevan, Wei Han, Jiquan Ngiam, Hang Zhao, Aleksei Timofeev, Scott
Ettinger, Maxim Krivokon, Amy Gao, Aditya Joshi, Yu Zhang, Jonathon Shlens,
Zhifeng Chen, and Dragomir Anguelov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Scalability in perception for autonomous driving: Waymo open dataset.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, June 2020.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
C. Szegedy, Wei Liu, Yangqing Jia, P. Sermanet, S. Reed, D.
Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Going deeper with convolutions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, pages 1–9,
2015.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Bugra Tekin, Pablo Marquez-Neila, Mathieu Salzmann, and Pascal Fua.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Learning to fuse 2d and 3d image cues for monocular body pose
estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. Conf. Comput. Vis. (ICCV)</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, pages 3961–3970, 10 2017.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Denis Tome, Chris Russell, and Lourdes Agapito.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Lifting from the deep: Convolutional 3d pose estimation from a single
image.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, July 2017.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Shashank Tripathi, Siddhant Ranade, Ambrish Tyagi, and Amit Agrawal.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Posenet3d: Unsupervised 3d human shape and pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">2020.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Bastian Wandt and Bodo Rosenhahn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Repnet: Weakly supervised training of an adversarial reprojection
network for 3d human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">, June 2019.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Chunyu Wang, Yizhou Wang, Zhouchen Lin, Alan L. Yuille, and Wen Gao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Robust estimation of 3d human poses from a single image.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:90%;">, pages
2369–2376, 2014.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Chung-Yi Weng, Brian Curless, and Ira Kemelmacher-Shlizerman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Photo wake-up: 3d character animation from a single photo, 2018.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Nora S. Willett, Hijung Valentina Shin, Zeyu Jin, Wilmot Li, and Adam
Finkelstein.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Pose2Pose: Pose selection and transfer for 2D character
animation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">25th International Conference on Intelligent User Interfaces
(IUI 2020)</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:90%;">, page 12, Mar. 2020.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Bin Xiao, Haiping Wu, and Yichen Wei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Simple baselines for human pose estimation and tracking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Eur. Conf. Comput. Vis. (ECCV)</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Wei Yang, Wanli Ouyang, Xiaolong Wang, Jimmy S. J. Ren, Hongsheng Li, and
Xiaogang Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">3d human pose estimation in the wild by adversarial learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</span><span id="bib.bib37.4.2" class="ltx_text" style="font-size:90%;">, pages
5255–5264, 2018.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Jiaming Ying and Xu Zhao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Rgb-d fusion for point-cloud-based 3d human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Int. Conf. Image Process. (ICIP)</span><span id="bib.bib38.5.3" class="ltx_text" style="font-size:90%;">, pages 3108–3112,
2021.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Ailing Zeng, X. Sun, F. Huang, Minhao Liu, Qiang Xu, and Stephen Lin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">Srnet: Improving generalization in 3d human pose estimation with a
split-and-recombine approach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Eur. Conf. Comput. Vis. (ECCV)</span><span id="bib.bib39.4.2" class="ltx_text" style="font-size:90%;">, abs/2007.09389, 2020.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Haotian Zhang, Cristobal Sciutto, Maneesh Agrawala, and Kayvon Fatahalian.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Vid2player: Controllable video sprites that behave and appear like
professional tennis players.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ACM Trans. Graph.</span><span id="bib.bib40.4.2" class="ltx_text" style="font-size:90%;">, 40(3), may 2021.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Z. Zhang, L. Hu, X. Deng, and S. Xia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">Weakly supervised adversarial learning for 3d human pose estimation
from point clouds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Visualization and Computer Graphics</span><span id="bib.bib41.4.2" class="ltx_text" style="font-size:90%;">,
26(5):1851–1859, 2020.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Zhe Zhang, Chunyu Wang, Weichao Qiu, Wenhu Qin, and Wenjun Zeng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Adafuse: Adaptive multiview fusion for accurate human pose estimation
in the wild.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. J. Comput. Vis. (IJCV)</span><span id="bib.bib42.4.2" class="ltx_text" style="font-size:90%;">, pages 1–16, 2020.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Ce Zheng, Wenhan Wu, Taojiannan Yang, Sijie Zhu, Chen Chen, Ruixu Liu, Ju Shen,
Nasser Kehtarnavaz, and Mubarak Shah.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Deep learning-based human pose estimation: A survey.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Computing Research Repository</span><span id="bib.bib43.4.2" class="ltx_text" style="font-size:90%;">, abs/2012.13392, 2020.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Xingyi Zhou, Qixing Huang, Xiao Sun, Xiangyang Xue, and Yichen Wei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Towards 3d human pose estimation in the wild: A weakly-supervised
approach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib44.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. Conf. Comput. Vis. (ICCV)</span><span id="bib.bib44.5.3" class="ltx_text" style="font-size:90%;">, Oct 2017.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G. Derpanis, and
Kostas Daniilidis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">Sparseness meets deepness: 3d human pose estimation from monocular
video.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib45.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</span><span id="bib.bib45.5.3" class="ltx_text" style="font-size:90%;">, pages 4966–4975, 2016.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Yin Zhou and Oncel Tuzel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">Voxelnet: End-to-end learning for point cloud based 3d object
detection, 2017.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">Unpaired image-to-image translation using cycle-consistent
adversarial networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib47.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. Conf. Comput. Vis. (ICCV)</span><span id="bib.bib47.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
Christian Zimmermann, Tim Welschehold, Christian Dornhege, Wolfram Burgard, and
Thomas Brox.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">3d human pose estimation in rgbd images for robotic task learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib48.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE International Conference on Robotics and Automation
(ICRA)</span><span id="bib.bib48.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2112.12140" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2112.12141" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2112.12141">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2112.12141" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2112.12143" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 15:59:26 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
