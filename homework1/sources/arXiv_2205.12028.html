<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2205.12028] Effect of Gender, Pose and Camera Distance on Human Body Dimensions Estimation</title><meta property="og:description" content="Human Body Dimensions Estimation (HBDE) is a task that an intelligent agent can
perform to attempt to determine human body information from images (2D) or point
clouds or meshes (3D). More specifically, if we define th…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Effect of Gender, Pose and Camera Distance on Human Body Dimensions Estimation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Effect of Gender, Pose and Camera Distance on Human Body Dimensions Estimation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2205.12028">

<!--Generated on Mon Mar 11 15:00:43 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Human Body Dimensions Estimation Human Body Measurements Deep
Learning.">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Department of Artificial Intelligence and Human Interfaces, Paris Lodron University of Salzburg, Austria
<br class="ltx_break"><sup id="id1.1" class="ltx_sup">1</sup><span id="id1.2" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>yansel.gonzalez-tejeda@stud.sbg.ac.at</span></span></span>
<br class="ltx_break"><sup id="id1.3" class="ltx_sup">2</sup><span id="id1.4" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>helmut@cs.sbg.ac.at</span></span></span>
</span></span></span>
<h1 class="ltx_title ltx_title_document">Effect of Gender, Pose and Camera Distance on Human Body
Dimensions Estimation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yansel González Tejeda<sup id="id1.1.id1" class="ltx_sup">1</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0003-1002-3815" title="ORCID identifier" class="ltx_ref">0000-0003-1002-3815</a></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Helmut A. Mayer<sup id="id2.1.id1" class="ltx_sup">2</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-2428-0962" title="ORCID identifier" class="ltx_ref">0000-0002-2428-0962</a></span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id3.id1" class="ltx_p">Human Body Dimensions Estimation (HBDE) is a task that an intelligent agent can
perform to attempt to determine human body information from images (2D) or point
clouds or meshes (3D). More specifically, if we define the HBDE problem as inferring human body measurements from images, then HBDE is a
difficult, inverse, multi-task regression problem that can be tackled with machine
learning techniques, particularly convolutional neural networks (CNN). Despite the
community’s tremendous effort to advance human shape analysis, there is a lack of
systematic experiments to assess CNNs estimation of human body dimensions from
images. Our contribution lies in assessing a CNN estimation performance in a series of
controlled experiments. To that end, we augment our recently published neural
anthropometer dataset by rendering images with different camera distance. We
evaluate the network inference absolute and relative mean error between the
estimated and actual HBDs. We train and evaluate the CNN in four scenarios: (1) training with subjects of a specific gender, (2) in a specific pose, (3) sparse camera distance and (4) dense camera distance. Not only our experiments demonstrate that the network can
perform the task successfully, but also reveal a number of relevant facts that contribute to better understand the task of HBDE.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Human Body Dimensions Estimation Human Body Measurements Deep
Learning.
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Human Body Dimensions Estimation (HBDE) is a task that an intelligent can
perform to attempt to determine human body information from images (2D) or point
clouds or meshes (3D). For instance, estimating the height and the shoulder width
of a person from a picture or a 3D mesh. Being humans in the center of society, one would expect that intelligent agents
should be able to perceive the shape of a person and reason about it
from an
anthropometric perspective, i.e., be capable of accurately estimating her human
body
measurements.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">This problem can be characterized by specifying the intelligent
agent’s perceptual input. If the HBDE problem is
circumscribed to inferring human body measurements from images, then HBDE is,
theoretically, a difficult, inverse, multi-task regression problem.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Practically, HBDE from images is a compelling problem, as well. HBDE plays an
important role in several areas ranging from digital
sizing<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>,
thought ergonomics<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
and computational forensics<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, to virtual
try-on<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> and
even fashion design and
intelligent automatic door systems<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. Moreover, since
accurately estimating a
person’s body measurements would decrease the probability that the person returns
clothes acquired online, HBDE has gained attention as an important step toward a more individual-oriented clothes manufacture.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Inverse problems such as HBDE can be tackled with convolutional neural networks
(CNN).
However, most studies in the field of HBDE have only focused on investigating to
what
extent CNNs can predict body measurements. A number of factors can affect this
prediction, but researchers have not treated them in depth. What is not yet clear is the impact of the person’s gender, pose, and camera
distance
with respect to the subject, on the estimation performance.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In this paper, we investigate these dependencies with a series of experiments. Despite the tremendous effort from researchers to attempt to better understand
HBDE, there is lack of this kind of experiment in the literature. We believe that
our contribution will shed light on how a CNN estimate
HBDs. Upon publication, we will make our code publicly available for research
purposes.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Code under <a target="_blank" href="https://github.com/neoglez/gpcamdis_hbde" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://github.com/neoglez/gpcamdis˙hbde</a></span></span></span></p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>The Problem of Human Body Dimensions Estimation</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">As stated above, CNNs can be
employed to approach the HBDE problem. However, supervised learning
methods demand large amounts of data. Unfortunately, this kind of data is
extremely difficult to collect. For the network input,
several persons must be photographed with the same camera under equal lighting
conditions. Further, in order to study the effect of pose, the subjects must adopt
several poses; and to study the effect of camera distance, they would have to be
again photographed.
The supervision signal is even more challenging and costly: these same subjects
must be
accurately measured with identical methods to acquire their body
dimensions. This is <span id="S2.p1.1.1" class="ltx_text ltx_font_bold">the data scarcity problem in HBDE</span>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">A possible solution is to generate realistic 3D human meshes and
calculate HBDs from these meshes. But the HBD calculation is by no means a trivial
task. Properly defining HBDs
suffers from two issues: inconsistency and uncertainty.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">HBDs definitions differ depending on their intended purpose. To just mention one
example, health studies measure waist circumference at the midpoint between the inferior margin of the
last rib and the iliac crest <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. However, while investigating the height of the waist for clothing pattern design, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> found seven different waist definitions and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> directly enunciated that not all body measurements defined by 3D scanning technologies are valid for clothing pattern. This
multiplicity of definitions complicates consistent conceptualization for machine
learning.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Furthermore, HBDs are defined based on skeletal joins and/or body landmarks. These
reference criteria are highly uncertain and depend on the person
performing the measurement. A single HBD may exhibit important variability due to
observer or instrument error. Also, researchers and practitioners base their
analysis on HBD by presenting a figure of a thin subject with the measurements
depicted by
segments without further elucidation.
This approach hinders the HBDs calculation reproducibility.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.3" class="ltx_p">Formally, the HBDE problem has been defined by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> as a deep
regression problem. Given an image <math id="S2.p5.1.m1.1" class="ltx_Math" alttext="\mathcal{I}" display="inline"><semantics id="S2.p5.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S2.p5.1.m1.1.1" xref="S2.p5.1.m1.1.1.cmml">ℐ</mi><annotation-xml encoding="MathML-Content" id="S2.p5.1.m1.1b"><ci id="S2.p5.1.m1.1.1.cmml" xref="S2.p5.1.m1.1.1">ℐ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p5.1.m1.1c">\mathcal{I}</annotation></semantics></math> from a 3D
human body with HBD <math id="S2.p5.2.m2.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S2.p5.2.m2.1a"><mi id="S2.p5.2.m2.1.1" xref="S2.p5.2.m2.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S2.p5.2.m2.1b"><ci id="S2.p5.2.m2.1.1.cmml" xref="S2.p5.2.m2.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p5.2.m2.1c">D</annotation></semantics></math>, the goal is to return a set <math id="S2.p5.3.m3.1" class="ltx_Math" alttext="\hat{D}" display="inline"><semantics id="S2.p5.3.m3.1a"><mover accent="true" id="S2.p5.3.m3.1.1" xref="S2.p5.3.m3.1.1.cmml"><mi id="S2.p5.3.m3.1.1.2" xref="S2.p5.3.m3.1.1.2.cmml">D</mi><mo id="S2.p5.3.m3.1.1.1" xref="S2.p5.3.m3.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S2.p5.3.m3.1b"><apply id="S2.p5.3.m3.1.1.cmml" xref="S2.p5.3.m3.1.1"><ci id="S2.p5.3.m3.1.1.1.cmml" xref="S2.p5.3.m3.1.1.1">^</ci><ci id="S2.p5.3.m3.1.1.2.cmml" xref="S2.p5.3.m3.1.1.2">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p5.3.m3.1c">\hat{D}</annotation></semantics></math> of estimated human body
dimensions, that is</p>
</div>
<div id="S2.p6" class="ltx_para">
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.2" class="ltx_Math" alttext="\hat{D}=\mathcal{M}(\mathcal{I}(D))." display="block"><semantics id="S2.E1.m1.2a"><mrow id="S2.E1.m1.2.2.1" xref="S2.E1.m1.2.2.1.1.cmml"><mrow id="S2.E1.m1.2.2.1.1" xref="S2.E1.m1.2.2.1.1.cmml"><mover accent="true" id="S2.E1.m1.2.2.1.1.3" xref="S2.E1.m1.2.2.1.1.3.cmml"><mi id="S2.E1.m1.2.2.1.1.3.2" xref="S2.E1.m1.2.2.1.1.3.2.cmml">D</mi><mo id="S2.E1.m1.2.2.1.1.3.1" xref="S2.E1.m1.2.2.1.1.3.1.cmml">^</mo></mover><mo id="S2.E1.m1.2.2.1.1.2" xref="S2.E1.m1.2.2.1.1.2.cmml">=</mo><mrow id="S2.E1.m1.2.2.1.1.1" xref="S2.E1.m1.2.2.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.2.2.1.1.1.3" xref="S2.E1.m1.2.2.1.1.1.3.cmml">ℳ</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.1.1.1.2" xref="S2.E1.m1.2.2.1.1.1.2.cmml">​</mo><mrow id="S2.E1.m1.2.2.1.1.1.1.1" xref="S2.E1.m1.2.2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E1.m1.2.2.1.1.1.1.1.2" xref="S2.E1.m1.2.2.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.2.2.1.1.1.1.1.1" xref="S2.E1.m1.2.2.1.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.2.2.1.1.1.1.1.1.2" xref="S2.E1.m1.2.2.1.1.1.1.1.1.2.cmml">ℐ</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.1.1.1.1.1.1.1" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.cmml">​</mo><mrow id="S2.E1.m1.2.2.1.1.1.1.1.1.3.2" xref="S2.E1.m1.2.2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E1.m1.2.2.1.1.1.1.1.1.3.2.1" xref="S2.E1.m1.2.2.1.1.1.1.1.1.cmml">(</mo><mi id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">D</mi><mo stretchy="false" id="S2.E1.m1.2.2.1.1.1.1.1.1.3.2.2" xref="S2.E1.m1.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S2.E1.m1.2.2.1.1.1.1.1.3" xref="S2.E1.m1.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S2.E1.m1.2.2.1.2" xref="S2.E1.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.2b"><apply id="S2.E1.m1.2.2.1.1.cmml" xref="S2.E1.m1.2.2.1"><eq id="S2.E1.m1.2.2.1.1.2.cmml" xref="S2.E1.m1.2.2.1.1.2"></eq><apply id="S2.E1.m1.2.2.1.1.3.cmml" xref="S2.E1.m1.2.2.1.1.3"><ci id="S2.E1.m1.2.2.1.1.3.1.cmml" xref="S2.E1.m1.2.2.1.1.3.1">^</ci><ci id="S2.E1.m1.2.2.1.1.3.2.cmml" xref="S2.E1.m1.2.2.1.1.3.2">𝐷</ci></apply><apply id="S2.E1.m1.2.2.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.1"><times id="S2.E1.m1.2.2.1.1.1.2.cmml" xref="S2.E1.m1.2.2.1.1.1.2"></times><ci id="S2.E1.m1.2.2.1.1.1.3.cmml" xref="S2.E1.m1.2.2.1.1.1.3">ℳ</ci><apply id="S2.E1.m1.2.2.1.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1"><times id="S2.E1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1"></times><ci id="S2.E1.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.2">ℐ</ci><ci id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">𝐷</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.2c">\hat{D}=\mathcal{M}(\mathcal{I}(D)).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.p7" class="ltx_para">
<p id="S2.p7.1" class="ltx_p">The dataset is assumed to be drawn from a
generating distribution and the deep neural network <math id="S2.p7.1.m1.1" class="ltx_Math" alttext="\mathcal{M}" display="inline"><semantics id="S2.p7.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S2.p7.1.m1.1.1" xref="S2.p7.1.m1.1.1.cmml">ℳ</mi><annotation-xml encoding="MathML-Content" id="S2.p7.1.m1.1b"><ci id="S2.p7.1.m1.1.1.cmml" xref="S2.p7.1.m1.1.1">ℳ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p7.1.m1.1c">\mathcal{M}</annotation></semantics></math> minimizes the prediction error.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Related Work</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Obviously, human body dimensions are determined by human shape. In the field of Human Shape estimation (HSE), shape has been ambiguously presented either as a parametric model
acting as proxy to a 3D mesh or directly as a triangular mesh. In a community
effort to be more precise, the task of shape estimation has been currently
sharper defined as human mesh recovery, estimation or reconstruction.
Additionally, pose estimation has been established as
inferring the location of
skeleton joints, albeit these not being anatomically correct. In the last five
years, the body of work in these two fields has exploded. Since human mesh and
pose
estimation are barely indirectly related to our work, we will not discuss them
here. In contrast, we focus on end-to-end adults HBDE from images, i.e., the model input
are
images of adult subjects and the output are human body measurements.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">Undoubtedly, anthropometry has contributed most to human shape analysis.
Important surveys such as CAESAR (1999)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, ANSUR I and II
(2017)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and NHANES(1999-2021)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, have collected
HBDs.
However, they did not take images of the subjects. This makes unclear how
the
CNN input could be obtained. Recently, other datasets have been released for
specific tasks, e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> propose a dataset with images and
seven HBDs for estimation in the automotive context.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">Of all these compendiums, CAESAR is probably the most convenient data in
terms of realism. It contains rigorously recorded human body dimensions and 3D
scans,
from
which realistic images could be synthesized. The project costed six million USD
(see <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> executive summary). Consequently, this data is
highly expensive. Alternatively, we employ a generative model derived from real
humans, capable of producing thousands of 3D meshes from which we can
calculate and visualize the HBDs.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">Certainly, height is the HBD that has been investigated the
most<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>,
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>,
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.
Very early work<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> investigated the effect of gender and
inverted pictures when humans estimate height from images. They quantified
estimation
performance using Pearson’s Correlation Coefficient and established that the
estimated
and ground truth height where highly correlated. This fact has been confirmed recently by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, which also concluded that humans estimate height inaccurately. Other HBDs have been explored, e.g., waist<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> but, in general, they have received significantly less attention.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">Strongly related to our work are studies using or generating synthetic data and
calculating or manually collecting HBDs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>,
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. None of these works
investigated the effect of gender, pose or camera distance
in the estimation performance. Here, we explore these interactions.</p>
</div>
<div id="S3.p6" class="ltx_para">
<p id="S3.p6.1" class="ltx_p">Recently, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> proposed a baseline for HBDE
given height and weight. They claimed that linear regression estimates accurately HBDs when the inputs are height and weight. Like we, this method use ground truth
derived from the SMPL model<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. Despite their input being different
to ours, we will use this work for comparison.</p>
</div>
<div id="S3.p7" class="ltx_para">
<p id="S3.p7.1" class="ltx_p">A <span id="S3.p7.1.1" class="ltx_text ltx_font_italic">neural anthropometer</span> (NeuralAnthro) was introduced by
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.
The CNN was trained on grayscale synthetic images of moderate complexity, i.e., no
background, limited
human poses, and fix camera perspective and distance. In this work, we go further
and increase the image complexity, making the input more challenging to the intelligent agent conducting HBDE.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Material and Methods</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We now detail the dataset and CNN (model) of the supervised learning approach that
governs our experiments.</p>
</div>
<figure id="S4.F1" class="ltx_figure"><img src="/html/2205.12028/assets/OurCuratedDataset.png" id="S4.F1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="214" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Our curated dataset. We augment the NeuralAnthro dataset, containing
images of female (left) and male (right) subjects in pose zero (arms
stretched to the sides) and pose one (arms lowered) taken with a camera at
a
fix distance, by rendering photos with sparse and dense camera distances
(center). Note that the subjects appear nearer or farther in the images.
All instances are <math id="S4.F1.2.m1.1" class="ltx_Math" alttext="200\times 200" display="inline"><semantics id="S4.F1.2.m1.1b"><mrow id="S4.F1.2.m1.1.1" xref="S4.F1.2.m1.1.1.cmml"><mn id="S4.F1.2.m1.1.1.2" xref="S4.F1.2.m1.1.1.2.cmml">200</mn><mo lspace="0.222em" rspace="0.222em" id="S4.F1.2.m1.1.1.1" xref="S4.F1.2.m1.1.1.1.cmml">×</mo><mn id="S4.F1.2.m1.1.1.3" xref="S4.F1.2.m1.1.1.3.cmml">200</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F1.2.m1.1c"><apply id="S4.F1.2.m1.1.1.cmml" xref="S4.F1.2.m1.1.1"><times id="S4.F1.2.m1.1.1.1.cmml" xref="S4.F1.2.m1.1.1.1"></times><cn type="integer" id="S4.F1.2.m1.1.1.2.cmml" xref="S4.F1.2.m1.1.1.2">200</cn><cn type="integer" id="S4.F1.2.m1.1.1.3.cmml" xref="S4.F1.2.m1.1.1.3">200</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F1.2.m1.1d">200\times 200</annotation></semantics></math> pixels grayscale images displaying a
single subject.</figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Dataset</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We start with the NeuralAnthro synthetic dataset. The
reason to use a synthetic dataset is the cost and effort that collecting ”real”
data would imply. Since coherent pose variability is more difficult to find in
real datasets, another important aspect is the possibility to vary the subject
posture to experiment with different poses. While we did not collect our data from
physical humans, we use the SMPL model,
which is derived from real humans. SMPL is the most employed model in academia and
industry for its realism and simplicity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.</p>
</div>
<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Input</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.2" class="ltx_p">Figure <a href="#S4.F1" title="Figure 1 ‣ 4 Material and Methods ‣ Effect of Gender, Pose and Camera Distance on Human Body Dimensions Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> depicts our dataset. We obtained the 3D meshes, <math id="S4.SS1.SSS1.p1.1.m1.1" class="ltx_Math" alttext="6000" display="inline"><semantics id="S4.SS1.SSS1.p1.1.m1.1a"><mn id="S4.SS1.SSS1.p1.1.m1.1.1" xref="S4.SS1.SSS1.p1.1.m1.1.1.cmml">6000</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.1.m1.1b"><cn type="integer" id="S4.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1">6000</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.1.m1.1c">6000</annotation></semantics></math> female
and
male
subjects in pose zero and pose one (total <math id="S4.SS1.SSS1.p1.2.m2.1" class="ltx_Math" alttext="12000" display="inline"><semantics id="S4.SS1.SSS1.p1.2.m2.1a"><mn id="S4.SS1.SSS1.p1.2.m2.1.1" xref="S4.SS1.SSS1.p1.2.m2.1.1.cmml">12000</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.2.m2.1b"><cn type="integer" id="S4.SS1.SSS1.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS1.p1.2.m2.1.1">12000</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.2.m2.1c">12000</annotation></semantics></math> meshes), from the neural
anthropometer dataset<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.</p>
</div>
<div id="S4.SS1.SSS1.p2" class="ltx_para">
<p id="S4.SS1.SSS1.p2.1" class="ltx_p">Using the current standard method to
employ a render engine to produce the mesh corresponding images, we
simulate the cinematographic technique of <span id="S4.SS1.SSS1.p2.1.1" class="ltx_text ltx_font_italic">tracking back</span> (sparsely and
densely varying the camera distance to the mesh), as follows.</p>
</div>
<div id="S4.SS1.SSS1.p3" class="ltx_para">
<p id="S4.SS1.SSS1.p3.3" class="ltx_p"><span id="S4.SS1.SSS1.p3.3.1" class="ltx_text ltx_font_bold">Sparse camera distance</span>: back tracking by placing the camera at
distances <math id="S4.SS1.SSS1.p3.1.m1.1" class="ltx_Math" alttext="4\;m" display="inline"><semantics id="S4.SS1.SSS1.p3.1.m1.1a"><mrow id="S4.SS1.SSS1.p3.1.m1.1.1" xref="S4.SS1.SSS1.p3.1.m1.1.1.cmml"><mn id="S4.SS1.SSS1.p3.1.m1.1.1.2" xref="S4.SS1.SSS1.p3.1.m1.1.1.2.cmml">4</mn><mo lspace="0.280em" rspace="0em" id="S4.SS1.SSS1.p3.1.m1.1.1.1" xref="S4.SS1.SSS1.p3.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS1.SSS1.p3.1.m1.1.1.3" xref="S4.SS1.SSS1.p3.1.m1.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p3.1.m1.1b"><apply id="S4.SS1.SSS1.p3.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p3.1.m1.1.1"><times id="S4.SS1.SSS1.p3.1.m1.1.1.1.cmml" xref="S4.SS1.SSS1.p3.1.m1.1.1.1"></times><cn type="integer" id="S4.SS1.SSS1.p3.1.m1.1.1.2.cmml" xref="S4.SS1.SSS1.p3.1.m1.1.1.2">4</cn><ci id="S4.SS1.SSS1.p3.1.m1.1.1.3.cmml" xref="S4.SS1.SSS1.p3.1.m1.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p3.1.m1.1c">4\;m</annotation></semantics></math>, <math id="S4.SS1.SSS1.p3.2.m2.1" class="ltx_Math" alttext="5\;m" display="inline"><semantics id="S4.SS1.SSS1.p3.2.m2.1a"><mrow id="S4.SS1.SSS1.p3.2.m2.1.1" xref="S4.SS1.SSS1.p3.2.m2.1.1.cmml"><mn id="S4.SS1.SSS1.p3.2.m2.1.1.2" xref="S4.SS1.SSS1.p3.2.m2.1.1.2.cmml">5</mn><mo lspace="0.280em" rspace="0em" id="S4.SS1.SSS1.p3.2.m2.1.1.1" xref="S4.SS1.SSS1.p3.2.m2.1.1.1.cmml">​</mo><mi id="S4.SS1.SSS1.p3.2.m2.1.1.3" xref="S4.SS1.SSS1.p3.2.m2.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p3.2.m2.1b"><apply id="S4.SS1.SSS1.p3.2.m2.1.1.cmml" xref="S4.SS1.SSS1.p3.2.m2.1.1"><times id="S4.SS1.SSS1.p3.2.m2.1.1.1.cmml" xref="S4.SS1.SSS1.p3.2.m2.1.1.1"></times><cn type="integer" id="S4.SS1.SSS1.p3.2.m2.1.1.2.cmml" xref="S4.SS1.SSS1.p3.2.m2.1.1.2">5</cn><ci id="S4.SS1.SSS1.p3.2.m2.1.1.3.cmml" xref="S4.SS1.SSS1.p3.2.m2.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p3.2.m2.1c">5\;m</annotation></semantics></math> and <math id="S4.SS1.SSS1.p3.3.m3.1" class="ltx_Math" alttext="6\;m" display="inline"><semantics id="S4.SS1.SSS1.p3.3.m3.1a"><mrow id="S4.SS1.SSS1.p3.3.m3.1.1" xref="S4.SS1.SSS1.p3.3.m3.1.1.cmml"><mn id="S4.SS1.SSS1.p3.3.m3.1.1.2" xref="S4.SS1.SSS1.p3.3.m3.1.1.2.cmml">6</mn><mo lspace="0.280em" rspace="0em" id="S4.SS1.SSS1.p3.3.m3.1.1.1" xref="S4.SS1.SSS1.p3.3.m3.1.1.1.cmml">​</mo><mi id="S4.SS1.SSS1.p3.3.m3.1.1.3" xref="S4.SS1.SSS1.p3.3.m3.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p3.3.m3.1b"><apply id="S4.SS1.SSS1.p3.3.m3.1.1.cmml" xref="S4.SS1.SSS1.p3.3.m3.1.1"><times id="S4.SS1.SSS1.p3.3.m3.1.1.1.cmml" xref="S4.SS1.SSS1.p3.3.m3.1.1.1"></times><cn type="integer" id="S4.SS1.SSS1.p3.3.m3.1.1.2.cmml" xref="S4.SS1.SSS1.p3.3.m3.1.1.2">6</cn><ci id="S4.SS1.SSS1.p3.3.m3.1.1.3.cmml" xref="S4.SS1.SSS1.p3.3.m3.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p3.3.m3.1c">6\;m</annotation></semantics></math>.</p>
</div>
<div id="S4.SS1.SSS1.p4" class="ltx_para">
<p id="S4.SS1.SSS1.p4.2" class="ltx_p"><span id="S4.SS1.SSS1.p4.2.1" class="ltx_text ltx_font_bold">Dense camera distance</span>: back tracking by randomly placing the
camera between distances <math id="S4.SS1.SSS1.p4.1.m1.2" class="ltx_Math" alttext="4,2\;m" display="inline"><semantics id="S4.SS1.SSS1.p4.1.m1.2a"><mrow id="S4.SS1.SSS1.p4.1.m1.2.2.1" xref="S4.SS1.SSS1.p4.1.m1.2.2.2.cmml"><mn id="S4.SS1.SSS1.p4.1.m1.1.1" xref="S4.SS1.SSS1.p4.1.m1.1.1.cmml">4</mn><mo id="S4.SS1.SSS1.p4.1.m1.2.2.1.2" xref="S4.SS1.SSS1.p4.1.m1.2.2.2.cmml">,</mo><mrow id="S4.SS1.SSS1.p4.1.m1.2.2.1.1" xref="S4.SS1.SSS1.p4.1.m1.2.2.1.1.cmml"><mn id="S4.SS1.SSS1.p4.1.m1.2.2.1.1.2" xref="S4.SS1.SSS1.p4.1.m1.2.2.1.1.2.cmml">2</mn><mo lspace="0.280em" rspace="0em" id="S4.SS1.SSS1.p4.1.m1.2.2.1.1.1" xref="S4.SS1.SSS1.p4.1.m1.2.2.1.1.1.cmml">​</mo><mi id="S4.SS1.SSS1.p4.1.m1.2.2.1.1.3" xref="S4.SS1.SSS1.p4.1.m1.2.2.1.1.3.cmml">m</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p4.1.m1.2b"><list id="S4.SS1.SSS1.p4.1.m1.2.2.2.cmml" xref="S4.SS1.SSS1.p4.1.m1.2.2.1"><cn type="integer" id="S4.SS1.SSS1.p4.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p4.1.m1.1.1">4</cn><apply id="S4.SS1.SSS1.p4.1.m1.2.2.1.1.cmml" xref="S4.SS1.SSS1.p4.1.m1.2.2.1.1"><times id="S4.SS1.SSS1.p4.1.m1.2.2.1.1.1.cmml" xref="S4.SS1.SSS1.p4.1.m1.2.2.1.1.1"></times><cn type="integer" id="S4.SS1.SSS1.p4.1.m1.2.2.1.1.2.cmml" xref="S4.SS1.SSS1.p4.1.m1.2.2.1.1.2">2</cn><ci id="S4.SS1.SSS1.p4.1.m1.2.2.1.1.3.cmml" xref="S4.SS1.SSS1.p4.1.m1.2.2.1.1.3">𝑚</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p4.1.m1.2c">4,2\;m</annotation></semantics></math> and <math id="S4.SS1.SSS1.p4.2.m2.2" class="ltx_Math" alttext="7,2\;m" display="inline"><semantics id="S4.SS1.SSS1.p4.2.m2.2a"><mrow id="S4.SS1.SSS1.p4.2.m2.2.2.1" xref="S4.SS1.SSS1.p4.2.m2.2.2.2.cmml"><mn id="S4.SS1.SSS1.p4.2.m2.1.1" xref="S4.SS1.SSS1.p4.2.m2.1.1.cmml">7</mn><mo id="S4.SS1.SSS1.p4.2.m2.2.2.1.2" xref="S4.SS1.SSS1.p4.2.m2.2.2.2.cmml">,</mo><mrow id="S4.SS1.SSS1.p4.2.m2.2.2.1.1" xref="S4.SS1.SSS1.p4.2.m2.2.2.1.1.cmml"><mn id="S4.SS1.SSS1.p4.2.m2.2.2.1.1.2" xref="S4.SS1.SSS1.p4.2.m2.2.2.1.1.2.cmml">2</mn><mo lspace="0.280em" rspace="0em" id="S4.SS1.SSS1.p4.2.m2.2.2.1.1.1" xref="S4.SS1.SSS1.p4.2.m2.2.2.1.1.1.cmml">​</mo><mi id="S4.SS1.SSS1.p4.2.m2.2.2.1.1.3" xref="S4.SS1.SSS1.p4.2.m2.2.2.1.1.3.cmml">m</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p4.2.m2.2b"><list id="S4.SS1.SSS1.p4.2.m2.2.2.2.cmml" xref="S4.SS1.SSS1.p4.2.m2.2.2.1"><cn type="integer" id="S4.SS1.SSS1.p4.2.m2.1.1.cmml" xref="S4.SS1.SSS1.p4.2.m2.1.1">7</cn><apply id="S4.SS1.SSS1.p4.2.m2.2.2.1.1.cmml" xref="S4.SS1.SSS1.p4.2.m2.2.2.1.1"><times id="S4.SS1.SSS1.p4.2.m2.2.2.1.1.1.cmml" xref="S4.SS1.SSS1.p4.2.m2.2.2.1.1.1"></times><cn type="integer" id="S4.SS1.SSS1.p4.2.m2.2.2.1.1.2.cmml" xref="S4.SS1.SSS1.p4.2.m2.2.2.1.1.2">2</cn><ci id="S4.SS1.SSS1.p4.2.m2.2.2.1.1.3.cmml" xref="S4.SS1.SSS1.p4.2.m2.2.2.1.1.3">𝑚</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p4.2.m2.2c">7,2\;m</annotation></semantics></math>.</p>
</div>
<div id="S4.SS1.SSS1.p5" class="ltx_para">
<p id="S4.SS1.SSS1.p5.2" class="ltx_p">In total, we synthesize <math id="S4.SS1.SSS1.p5.1.m1.1" class="ltx_Math" alttext="72000" display="inline"><semantics id="S4.SS1.SSS1.p5.1.m1.1a"><mn id="S4.SS1.SSS1.p5.1.m1.1.1" xref="S4.SS1.SSS1.p5.1.m1.1.1.cmml">72000</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p5.1.m1.1b"><cn type="integer" id="S4.SS1.SSS1.p5.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p5.1.m1.1.1">72000</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p5.1.m1.1c">72000</annotation></semantics></math> pictures from the <math id="S4.SS1.SSS1.p5.2.m2.1" class="ltx_Math" alttext="12000" display="inline"><semantics id="S4.SS1.SSS1.p5.2.m2.1a"><mn id="S4.SS1.SSS1.p5.2.m2.1.1" xref="S4.SS1.SSS1.p5.2.m2.1.1.cmml">12000</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p5.2.m2.1b"><cn type="integer" id="S4.SS1.SSS1.p5.2.m2.1.1.cmml" xref="S4.SS1.SSS1.p5.2.m2.1.1">12000</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p5.2.m2.1c">12000</annotation></semantics></math> meshes. The images
correspond to meshes of a specific gender and a definite pose, taken at specific
camera distance with respect to the subject.</p>
</div>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Supervision Signal</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.p1.1" class="ltx_p">While the data scarcity problem is the major challenge in HBDE, another problem
is measurement inconsistency. There is no consensus regarding the
correct manner to define a
specific measurement, let alone several of them. The problem arises even when HBDs are
automatically computed by 3D scanning
technologies<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, making manually corrections
unavoidable. The united method introduced by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> with Sharmeam
(<span id="S4.SS1.SSS2.p1.1.1" class="ltx_text ltx_font_bold">Sh</span>oulder width, right and left
<span id="S4.SS1.SSS2.p1.1.2" class="ltx_text ltx_font_bold">arm</span>s length and inse<span id="S4.SS1.SSS2.p1.1.3" class="ltx_text ltx_font_bold">am</span>) and Calvis (<span id="S4.SS1.SSS2.p1.1.4" class="ltx_text ltx_font_bold">C</span>hest,
w<span id="S4.SS1.SSS2.p1.1.5" class="ltx_text ltx_font_bold">a</span>ist and pe<span id="S4.SS1.SSS2.p1.1.6" class="ltx_text ltx_font_bold">lvis</span> circumference plus height) allows us to resolve
the
inconsistency issue because it provides a proper method to calculate eight HBDs.
Additionally, it agrees, to a large extent,
with
anthropometry and tailoring.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Neural Anthropometer</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.4" class="ltx_p">The NeuralAnthro is a small, easily deployable CNN that we use to conduct our
experiments. We use the same experimental setting as in the original
paper<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, i.e., we
train for <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mn id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><cn type="integer" id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">20</annotation></semantics></math> epochs and use mini-batches of size <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mn id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><cn type="integer" id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">100</annotation></semantics></math>. We report results
based on 5-fold cross-validation. We minimize the mean squared error between the
actual and the estimated HBDs using stochastic gradient descent with a momentum
<math id="S4.SS2.p1.3.m3.1" class="ltx_Math" alttext="0.9" display="inline"><semantics id="S4.SS2.p1.3.m3.1a"><mn id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml">0.9</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><cn type="float" id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1">0.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">0.9</annotation></semantics></math>; the learning rate is set to <math id="S4.SS2.p1.4.m4.1" class="ltx_Math" alttext="0.01" display="inline"><semantics id="S4.SS2.p1.4.m4.1a"><mn id="S4.SS2.p1.4.m4.1.1" xref="S4.SS2.p1.4.m4.1.1.cmml">0.01</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.4.m4.1b"><cn type="float" id="S4.SS2.p1.4.m4.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1">0.01</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.4.m4.1c">0.01</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results and Discussion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">For the presentation of the results we use the following abbreviations: shoulder
width (SW), right
arm length (RAL), left arm length (LAL), inseam (a.k.a. crotch height ) (I), chest
circumference (CC),
waist circumference (WC), pelvis circumference (PC) and height (H). Average MAD
(AMAD) and Average RPE (ARPE) are both represented by a capital A. The figures we
present are interesting in several ways. Due to space restrictions we can not
discuss exhaustively all their aspects. Therefore, we examine the most salient
results.</p>
</div>
<figure id="S5.F2" class="ltx_figure"><img src="/html/2205.12028/assets/x1.png" id="S5.F2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="235" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Effect of gender on HBDE. Left: we display Mean
Absolute Error (MAE) in <math id="S5.F2.2.m1.1" class="ltx_Math" alttext="mm" display="inline"><semantics id="S5.F2.2.m1.1b"><mrow id="S5.F2.2.m1.1.1" xref="S5.F2.2.m1.1.1.cmml"><mi id="S5.F2.2.m1.1.1.2" xref="S5.F2.2.m1.1.1.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S5.F2.2.m1.1.1.1" xref="S5.F2.2.m1.1.1.1.cmml">​</mo><mi id="S5.F2.2.m1.1.1.3" xref="S5.F2.2.m1.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.F2.2.m1.1c"><apply id="S5.F2.2.m1.1.1.cmml" xref="S5.F2.2.m1.1.1"><times id="S5.F2.2.m1.1.1.1.cmml" xref="S5.F2.2.m1.1.1.1"></times><ci id="S5.F2.2.m1.1.1.2.cmml" xref="S5.F2.2.m1.1.1.2">𝑚</ci><ci id="S5.F2.2.m1.1.1.3.cmml" xref="S5.F2.2.m1.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F2.2.m1.1d">mm</annotation></semantics></math>; right: Relative Percentage Error
(RPE). </figcaption>
</figure>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Effect of Gender</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">We start our discussion by evaluating the network performance when the input are
images from humans of a specific gender in pose zero or one. We define training
with two gender as
<span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_italic">unisex training</span> and <span id="S5.SS1.p1.1.2" class="ltx_text ltx_font_italic">gender training</span> when the input are subjects
of a
specific gender.
Figure <a href="#S5.F2" title="Figure 2 ‣ 5 Results and Discussion ‣ Effect of Gender, Pose and Camera Distance on Human Body Dimensions Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the results.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.3" class="ltx_p">Like <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, we observe that height estimation is more
accurate in unisex training, compared to gender training (RPE <math id="S5.SS1.p2.1.m1.1" class="ltx_Math" alttext="1.58" display="inline"><semantics id="S5.SS1.p2.1.m1.1a"><mn id="S5.SS1.p2.1.m1.1.1" xref="S5.SS1.p2.1.m1.1.1.cmml">1.58</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.1b"><cn type="float" id="S5.SS1.p2.1.m1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1">1.58</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.1c">1.58</annotation></semantics></math> unisex
training reported in
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> vs. gender training
<math id="S5.SS1.p2.2.m2.1" class="ltx_Math" alttext="2.85" display="inline"><semantics id="S5.SS1.p2.2.m2.1a"><mn id="S5.SS1.p2.2.m2.1.1" xref="S5.SS1.p2.2.m2.1.1.cmml">2.85</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.2.m2.1b"><cn type="float" id="S5.SS1.p2.2.m2.1.1.cmml" xref="S5.SS1.p2.2.m2.1.1">2.85</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.2.m2.1c">2.85</annotation></semantics></math> female and <math id="S5.SS1.p2.3.m3.1" class="ltx_Math" alttext="2.95" display="inline"><semantics id="S5.SS1.p2.3.m3.1a"><mn id="S5.SS1.p2.3.m3.1.1" xref="S5.SS1.p2.3.m3.1.1.cmml">2.95</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.3.m3.1b"><cn type="float" id="S5.SS1.p2.3.m3.1.1.cmml" xref="S5.SS1.p2.3.m3.1.1">2.95</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.3.m3.1c">2.95</annotation></semantics></math> male).</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.4" class="ltx_p">For the network, it is considerably more difficult to estimate female gender
training SW than male gender training
SW. Although female gender training SW MAE is lower than male gender training
(<math id="S5.SS1.p3.1.m1.1" class="ltx_Math" alttext="12.63mm" display="inline"><semantics id="S5.SS1.p3.1.m1.1a"><mrow id="S5.SS1.p3.1.m1.1.1" xref="S5.SS1.p3.1.m1.1.1.cmml"><mn id="S5.SS1.p3.1.m1.1.1.2" xref="S5.SS1.p3.1.m1.1.1.2.cmml">12.63</mn><mo lspace="0em" rspace="0em" id="S5.SS1.p3.1.m1.1.1.1" xref="S5.SS1.p3.1.m1.1.1.1.cmml">​</mo><mi id="S5.SS1.p3.1.m1.1.1.3" xref="S5.SS1.p3.1.m1.1.1.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p3.1.m1.1.1.1a" xref="S5.SS1.p3.1.m1.1.1.1.cmml">​</mo><mi id="S5.SS1.p3.1.m1.1.1.4" xref="S5.SS1.p3.1.m1.1.1.4.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.1b"><apply id="S5.SS1.p3.1.m1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1"><times id="S5.SS1.p3.1.m1.1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1.1"></times><cn type="float" id="S5.SS1.p3.1.m1.1.1.2.cmml" xref="S5.SS1.p3.1.m1.1.1.2">12.63</cn><ci id="S5.SS1.p3.1.m1.1.1.3.cmml" xref="S5.SS1.p3.1.m1.1.1.3">𝑚</ci><ci id="S5.SS1.p3.1.m1.1.1.4.cmml" xref="S5.SS1.p3.1.m1.1.1.4">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.1c">12.63mm</annotation></semantics></math> vs. <math id="S5.SS1.p3.2.m2.1" class="ltx_Math" alttext="16.07mm" display="inline"><semantics id="S5.SS1.p3.2.m2.1a"><mrow id="S5.SS1.p3.2.m2.1.1" xref="S5.SS1.p3.2.m2.1.1.cmml"><mn id="S5.SS1.p3.2.m2.1.1.2" xref="S5.SS1.p3.2.m2.1.1.2.cmml">16.07</mn><mo lspace="0em" rspace="0em" id="S5.SS1.p3.2.m2.1.1.1" xref="S5.SS1.p3.2.m2.1.1.1.cmml">​</mo><mi id="S5.SS1.p3.2.m2.1.1.3" xref="S5.SS1.p3.2.m2.1.1.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p3.2.m2.1.1.1a" xref="S5.SS1.p3.2.m2.1.1.1.cmml">​</mo><mi id="S5.SS1.p3.2.m2.1.1.4" xref="S5.SS1.p3.2.m2.1.1.4.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.2.m2.1b"><apply id="S5.SS1.p3.2.m2.1.1.cmml" xref="S5.SS1.p3.2.m2.1.1"><times id="S5.SS1.p3.2.m2.1.1.1.cmml" xref="S5.SS1.p3.2.m2.1.1.1"></times><cn type="float" id="S5.SS1.p3.2.m2.1.1.2.cmml" xref="S5.SS1.p3.2.m2.1.1.2">16.07</cn><ci id="S5.SS1.p3.2.m2.1.1.3.cmml" xref="S5.SS1.p3.2.m2.1.1.3">𝑚</ci><ci id="S5.SS1.p3.2.m2.1.1.4.cmml" xref="S5.SS1.p3.2.m2.1.1.4">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.2.m2.1c">16.07mm</annotation></semantics></math>), the inverse relation can be observed, when considering
RPE (<math id="S5.SS1.p3.3.m3.1" class="ltx_Math" alttext="7.37" display="inline"><semantics id="S5.SS1.p3.3.m3.1a"><mn id="S5.SS1.p3.3.m3.1.1" xref="S5.SS1.p3.3.m3.1.1.cmml">7.37</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.3.m3.1b"><cn type="float" id="S5.SS1.p3.3.m3.1.1.cmml" xref="S5.SS1.p3.3.m3.1.1">7.37</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.3.m3.1c">7.37</annotation></semantics></math> vs. <math id="S5.SS1.p3.4.m4.1" class="ltx_Math" alttext="3.93" display="inline"><semantics id="S5.SS1.p3.4.m4.1a"><mn id="S5.SS1.p3.4.m4.1.1" xref="S5.SS1.p3.4.m4.1.1.cmml">3.93</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.4.m4.1b"><cn type="float" id="S5.SS1.p3.4.m4.1.1.cmml" xref="S5.SS1.p3.4.m4.1.1">3.93</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.4.m4.1c">3.93</annotation></semantics></math>).</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.4" class="ltx_p">Curiously, regarding the effect of gender, the CNN and humans appear to estimate height differently. Unlike <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>’s results, Fig.<a href="#S5.F2" title="Figure 2 ‣ 5 Results and Discussion ‣ Effect of Gender, Pose and Camera Distance on Human Body Dimensions Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows that the female height estimation error (RPE <math id="S5.SS1.p4.1.m1.1" class="ltx_Math" alttext="2.85" display="inline"><semantics id="S5.SS1.p4.1.m1.1a"><mn id="S5.SS1.p4.1.m1.1.1" xref="S5.SS1.p4.1.m1.1.1.cmml">2.85</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.1.m1.1b"><cn type="float" id="S5.SS1.p4.1.m1.1.1.cmml" xref="S5.SS1.p4.1.m1.1.1">2.85</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.1.m1.1c">2.85</annotation></semantics></math>) is lower than male (RPE <math id="S5.SS1.p4.2.m2.1" class="ltx_Math" alttext="2.95" display="inline"><semantics id="S5.SS1.p4.2.m2.1a"><mn id="S5.SS1.p4.2.m2.1.1" xref="S5.SS1.p4.2.m2.1.1.cmml">2.95</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.2.m2.1b"><cn type="float" id="S5.SS1.p4.2.m2.1.1.cmml" xref="S5.SS1.p4.2.m2.1.1">2.95</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.2.m2.1c">2.95</annotation></semantics></math>). Perhaps it is not surprisingly, that this relation holds for inseam as well (RPE <math id="S5.SS1.p4.3.m3.1" class="ltx_Math" alttext="3.86" display="inline"><semantics id="S5.SS1.p4.3.m3.1a"><mn id="S5.SS1.p4.3.m3.1.1" xref="S5.SS1.p4.3.m3.1.1.cmml">3.86</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.3.m3.1b"><cn type="float" id="S5.SS1.p4.3.m3.1.1.cmml" xref="S5.SS1.p4.3.m3.1.1">3.86</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.3.m3.1c">3.86</annotation></semantics></math> vs. <math id="S5.SS1.p4.4.m4.1" class="ltx_Math" alttext="4.35" display="inline"><semantics id="S5.SS1.p4.4.m4.1a"><mn id="S5.SS1.p4.4.m4.1.1" xref="S5.SS1.p4.4.m4.1.1.cmml">4.35</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.4.m4.1b"><cn type="float" id="S5.SS1.p4.4.m4.1.1.cmml" xref="S5.SS1.p4.4.m4.1.1">4.35</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.4.m4.1c">4.35</annotation></semantics></math>).
With the exceptions of these two HBDs, the RPE of estimating other HBDs is larger for female as for male subjects.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Effect of Pose</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.2" class="ltx_p">Figure <a href="#S5.F3" title="Figure 3 ‣ 5.2 Effect of Pose ‣ 5 Results and Discussion ‣ Effect of Gender, Pose and Camera Distance on Human Body Dimensions Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> presents the breakdown of the estimation error when we
train the network individually with images of humans in pose zero and pose one
(<span id="S5.SS2.p1.2.1" class="ltx_text ltx_font_italic">multi-pose training</span>). Surprisingly, the network estimated shoulder width
more poorly when the subject was in pose one as in pose zero (RPE <math id="S5.SS2.p1.1.m1.1" class="ltx_Math" alttext="6.4" display="inline"><semantics id="S5.SS2.p1.1.m1.1a"><mn id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml">6.4</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><cn type="float" id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1">6.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">6.4</annotation></semantics></math> vs.
<math id="S5.SS2.p1.2.m2.1" class="ltx_Math" alttext="6.0" display="inline"><semantics id="S5.SS2.p1.2.m2.1a"><mn id="S5.SS2.p1.2.m2.1.1" xref="S5.SS2.p1.2.m2.1.1.cmml">6.0</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.2.m2.1b"><cn type="float" id="S5.SS2.p1.2.m2.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1">6.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.2.m2.1c">6.0</annotation></semantics></math>). One would expect that estimating SW would be easier when
the subject is in pose one, because the arms are lowered, and, therefore, the
shoulder joints could be easier recognized.</p>
</div>
<figure id="S5.F3" class="ltx_figure"><img src="/html/2205.12028/assets/x2.png" id="S5.F3.g1" class="ltx_graphics ltx_img_landscape" width="461" height="235" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Effect of pose on HBDE. Left: we display Mean
Absolute Error (MAE) in <math id="S5.F3.2.m1.1" class="ltx_Math" alttext="mm" display="inline"><semantics id="S5.F3.2.m1.1b"><mrow id="S5.F3.2.m1.1.1" xref="S5.F3.2.m1.1.1.cmml"><mi id="S5.F3.2.m1.1.1.2" xref="S5.F3.2.m1.1.1.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S5.F3.2.m1.1.1.1" xref="S5.F3.2.m1.1.1.1.cmml">​</mo><mi id="S5.F3.2.m1.1.1.3" xref="S5.F3.2.m1.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.F3.2.m1.1c"><apply id="S5.F3.2.m1.1.1.cmml" xref="S5.F3.2.m1.1.1"><times id="S5.F3.2.m1.1.1.1.cmml" xref="S5.F3.2.m1.1.1.1"></times><ci id="S5.F3.2.m1.1.1.2.cmml" xref="S5.F3.2.m1.1.1.2">𝑚</ci><ci id="S5.F3.2.m1.1.1.3.cmml" xref="S5.F3.2.m1.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F3.2.m1.1d">mm</annotation></semantics></math>; right: Relative Percentage Error
(RPE). </figcaption>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Effect of Camera Distance</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.8" class="ltx_p">The most interesting finding was that the network is able to accurately estimate
all HBDs independently of the camera distance to the person (ARPE <math id="S5.SS3.p1.1.m1.1" class="ltx_Math" alttext="3.04" display="inline"><semantics id="S5.SS3.p1.1.m1.1a"><mn id="S5.SS3.p1.1.m1.1.1" xref="S5.SS3.p1.1.m1.1.1.cmml">3.04</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.1.m1.1b"><cn type="float" id="S5.SS3.p1.1.m1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1">3.04</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.1.m1.1c">3.04</annotation></semantics></math>, <math id="S5.SS3.p1.2.m2.1" class="ltx_Math" alttext="3.03" display="inline"><semantics id="S5.SS3.p1.2.m2.1a"><mn id="S5.SS3.p1.2.m2.1.1" xref="S5.SS3.p1.2.m2.1.1.cmml">3.03</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.2.m2.1b"><cn type="float" id="S5.SS3.p1.2.m2.1.1.cmml" xref="S5.SS3.p1.2.m2.1.1">3.03</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.2.m2.1c">3.03</annotation></semantics></math>,
<math id="S5.SS3.p1.3.m3.1" class="ltx_Math" alttext="2.96" display="inline"><semantics id="S5.SS3.p1.3.m3.1a"><mn id="S5.SS3.p1.3.m3.1.1" xref="S5.SS3.p1.3.m3.1.1.cmml">2.96</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.3.m3.1b"><cn type="float" id="S5.SS3.p1.3.m3.1.1.cmml" xref="S5.SS3.p1.3.m3.1.1">2.96</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.3.m3.1c">2.96</annotation></semantics></math>, <math id="S5.SS3.p1.4.m4.1" class="ltx_Math" alttext="3.57" display="inline"><semantics id="S5.SS3.p1.4.m4.1a"><mn id="S5.SS3.p1.4.m4.1.1" xref="S5.SS3.p1.4.m4.1.1.cmml">3.57</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.4.m4.1b"><cn type="float" id="S5.SS3.p1.4.m4.1.1.cmml" xref="S5.SS3.p1.4.m4.1.1">3.57</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.4.m4.1c">3.57</annotation></semantics></math> and <math id="S5.SS3.p1.5.m5.1" class="ltx_Math" alttext="3.11" display="inline"><semantics id="S5.SS3.p1.5.m5.1a"><mn id="S5.SS3.p1.5.m5.1.1" xref="S5.SS3.p1.5.m5.1.1.cmml">3.11</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.5.m5.1b"><cn type="float" id="S5.SS3.p1.5.m5.1.1.cmml" xref="S5.SS3.p1.5.m5.1.1">3.11</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.5.m5.1c">3.11</annotation></semantics></math>), when training with sparse camera distance <math id="S5.SS3.p1.6.m6.1" class="ltx_Math" alttext="4\;m" display="inline"><semantics id="S5.SS3.p1.6.m6.1a"><mrow id="S5.SS3.p1.6.m6.1.1" xref="S5.SS3.p1.6.m6.1.1.cmml"><mn id="S5.SS3.p1.6.m6.1.1.2" xref="S5.SS3.p1.6.m6.1.1.2.cmml">4</mn><mo lspace="0.280em" rspace="0em" id="S5.SS3.p1.6.m6.1.1.1" xref="S5.SS3.p1.6.m6.1.1.1.cmml">​</mo><mi id="S5.SS3.p1.6.m6.1.1.3" xref="S5.SS3.p1.6.m6.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.6.m6.1b"><apply id="S5.SS3.p1.6.m6.1.1.cmml" xref="S5.SS3.p1.6.m6.1.1"><times id="S5.SS3.p1.6.m6.1.1.1.cmml" xref="S5.SS3.p1.6.m6.1.1.1"></times><cn type="integer" id="S5.SS3.p1.6.m6.1.1.2.cmml" xref="S5.SS3.p1.6.m6.1.1.2">4</cn><ci id="S5.SS3.p1.6.m6.1.1.3.cmml" xref="S5.SS3.p1.6.m6.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.6.m6.1c">4\;m</annotation></semantics></math>, <math id="S5.SS3.p1.7.m7.1" class="ltx_Math" alttext="5\;m" display="inline"><semantics id="S5.SS3.p1.7.m7.1a"><mrow id="S5.SS3.p1.7.m7.1.1" xref="S5.SS3.p1.7.m7.1.1.cmml"><mn id="S5.SS3.p1.7.m7.1.1.2" xref="S5.SS3.p1.7.m7.1.1.2.cmml">5</mn><mo lspace="0.280em" rspace="0em" id="S5.SS3.p1.7.m7.1.1.1" xref="S5.SS3.p1.7.m7.1.1.1.cmml">​</mo><mi id="S5.SS3.p1.7.m7.1.1.3" xref="S5.SS3.p1.7.m7.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.7.m7.1b"><apply id="S5.SS3.p1.7.m7.1.1.cmml" xref="S5.SS3.p1.7.m7.1.1"><times id="S5.SS3.p1.7.m7.1.1.1.cmml" xref="S5.SS3.p1.7.m7.1.1.1"></times><cn type="integer" id="S5.SS3.p1.7.m7.1.1.2.cmml" xref="S5.SS3.p1.7.m7.1.1.2">5</cn><ci id="S5.SS3.p1.7.m7.1.1.3.cmml" xref="S5.SS3.p1.7.m7.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.7.m7.1c">5\;m</annotation></semantics></math> and <math id="S5.SS3.p1.8.m8.1" class="ltx_Math" alttext="6\;m" display="inline"><semantics id="S5.SS3.p1.8.m8.1a"><mrow id="S5.SS3.p1.8.m8.1.1" xref="S5.SS3.p1.8.m8.1.1.cmml"><mn id="S5.SS3.p1.8.m8.1.1.2" xref="S5.SS3.p1.8.m8.1.1.2.cmml">6</mn><mo lspace="0.280em" rspace="0em" id="S5.SS3.p1.8.m8.1.1.1" xref="S5.SS3.p1.8.m8.1.1.1.cmml">​</mo><mi id="S5.SS3.p1.8.m8.1.1.3" xref="S5.SS3.p1.8.m8.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.8.m8.1b"><apply id="S5.SS3.p1.8.m8.1.1.cmml" xref="S5.SS3.p1.8.m8.1.1"><times id="S5.SS3.p1.8.m8.1.1.1.cmml" xref="S5.SS3.p1.8.m8.1.1.1"></times><cn type="integer" id="S5.SS3.p1.8.m8.1.1.2.cmml" xref="S5.SS3.p1.8.m8.1.1.2">6</cn><ci id="S5.SS3.p1.8.m8.1.1.3.cmml" xref="S5.SS3.p1.8.m8.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.8.m8.1c">6\;m</annotation></semantics></math> and randomly chosen
camera distance respectively. This fact
challenges intuition, e.g., contradicts current research claiming that the network
can only correctly estimate height if the evaluation is performed for a particular
camera
distance<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. But this finding is in accordance to when
humans estimating
height as reported in preliminary work<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.</p>
</div>
<figure id="S5.F4" class="ltx_figure"><img src="/html/2205.12028/assets/x3.png" id="S5.F4.g1" class="ltx_graphics ltx_img_landscape" width="461" height="245" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Effect of camera distance on HBDE. We placed the camera at distances
<math id="S5.F4.7.m1.1" class="ltx_Math" alttext="-4\;m" display="inline"><semantics id="S5.F4.7.m1.1b"><mrow id="S5.F4.7.m1.1.1" xref="S5.F4.7.m1.1.1.cmml"><mo id="S5.F4.7.m1.1.1b" xref="S5.F4.7.m1.1.1.cmml">−</mo><mrow id="S5.F4.7.m1.1.1.2" xref="S5.F4.7.m1.1.1.2.cmml"><mn id="S5.F4.7.m1.1.1.2.2" xref="S5.F4.7.m1.1.1.2.2.cmml">4</mn><mo lspace="0.280em" rspace="0em" id="S5.F4.7.m1.1.1.2.1" xref="S5.F4.7.m1.1.1.2.1.cmml">​</mo><mi id="S5.F4.7.m1.1.1.2.3" xref="S5.F4.7.m1.1.1.2.3.cmml">m</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.F4.7.m1.1c"><apply id="S5.F4.7.m1.1.1.cmml" xref="S5.F4.7.m1.1.1"><minus id="S5.F4.7.m1.1.1.1.cmml" xref="S5.F4.7.m1.1.1"></minus><apply id="S5.F4.7.m1.1.1.2.cmml" xref="S5.F4.7.m1.1.1.2"><times id="S5.F4.7.m1.1.1.2.1.cmml" xref="S5.F4.7.m1.1.1.2.1"></times><cn type="integer" id="S5.F4.7.m1.1.1.2.2.cmml" xref="S5.F4.7.m1.1.1.2.2">4</cn><ci id="S5.F4.7.m1.1.1.2.3.cmml" xref="S5.F4.7.m1.1.1.2.3">𝑚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.7.m1.1d">-4\;m</annotation></semantics></math>, <math id="S5.F4.8.m2.1" class="ltx_Math" alttext="-5\;m" display="inline"><semantics id="S5.F4.8.m2.1b"><mrow id="S5.F4.8.m2.1.1" xref="S5.F4.8.m2.1.1.cmml"><mo id="S5.F4.8.m2.1.1b" xref="S5.F4.8.m2.1.1.cmml">−</mo><mrow id="S5.F4.8.m2.1.1.2" xref="S5.F4.8.m2.1.1.2.cmml"><mn id="S5.F4.8.m2.1.1.2.2" xref="S5.F4.8.m2.1.1.2.2.cmml">5</mn><mo lspace="0.280em" rspace="0em" id="S5.F4.8.m2.1.1.2.1" xref="S5.F4.8.m2.1.1.2.1.cmml">​</mo><mi id="S5.F4.8.m2.1.1.2.3" xref="S5.F4.8.m2.1.1.2.3.cmml">m</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.F4.8.m2.1c"><apply id="S5.F4.8.m2.1.1.cmml" xref="S5.F4.8.m2.1.1"><minus id="S5.F4.8.m2.1.1.1.cmml" xref="S5.F4.8.m2.1.1"></minus><apply id="S5.F4.8.m2.1.1.2.cmml" xref="S5.F4.8.m2.1.1.2"><times id="S5.F4.8.m2.1.1.2.1.cmml" xref="S5.F4.8.m2.1.1.2.1"></times><cn type="integer" id="S5.F4.8.m2.1.1.2.2.cmml" xref="S5.F4.8.m2.1.1.2.2">5</cn><ci id="S5.F4.8.m2.1.1.2.3.cmml" xref="S5.F4.8.m2.1.1.2.3">𝑚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.8.m2.1d">-5\;m</annotation></semantics></math> and <math id="S5.F4.9.m3.1" class="ltx_Math" alttext="-6\;m" display="inline"><semantics id="S5.F4.9.m3.1b"><mrow id="S5.F4.9.m3.1.1" xref="S5.F4.9.m3.1.1.cmml"><mo id="S5.F4.9.m3.1.1b" xref="S5.F4.9.m3.1.1.cmml">−</mo><mrow id="S5.F4.9.m3.1.1.2" xref="S5.F4.9.m3.1.1.2.cmml"><mn id="S5.F4.9.m3.1.1.2.2" xref="S5.F4.9.m3.1.1.2.2.cmml">6</mn><mo lspace="0.280em" rspace="0em" id="S5.F4.9.m3.1.1.2.1" xref="S5.F4.9.m3.1.1.2.1.cmml">​</mo><mi id="S5.F4.9.m3.1.1.2.3" xref="S5.F4.9.m3.1.1.2.3.cmml">m</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.F4.9.m3.1c"><apply id="S5.F4.9.m3.1.1.cmml" xref="S5.F4.9.m3.1.1"><minus id="S5.F4.9.m3.1.1.1.cmml" xref="S5.F4.9.m3.1.1"></minus><apply id="S5.F4.9.m3.1.1.2.cmml" xref="S5.F4.9.m3.1.1.2"><times id="S5.F4.9.m3.1.1.2.1.cmml" xref="S5.F4.9.m3.1.1.2.1"></times><cn type="integer" id="S5.F4.9.m3.1.1.2.2.cmml" xref="S5.F4.9.m3.1.1.2.2">6</cn><ci id="S5.F4.9.m3.1.1.2.3.cmml" xref="S5.F4.9.m3.1.1.2.3">𝑚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.9.m3.1d">-6\;m</annotation></semantics></math>, and randomly distances sampled from
<math id="S5.F4.10.m4.2" class="ltx_Math" alttext="-4,2\;m" display="inline"><semantics id="S5.F4.10.m4.2b"><mrow id="S5.F4.10.m4.2.2.2" xref="S5.F4.10.m4.2.2.3.cmml"><mrow id="S5.F4.10.m4.1.1.1.1" xref="S5.F4.10.m4.1.1.1.1.cmml"><mo id="S5.F4.10.m4.1.1.1.1b" xref="S5.F4.10.m4.1.1.1.1.cmml">−</mo><mn id="S5.F4.10.m4.1.1.1.1.2" xref="S5.F4.10.m4.1.1.1.1.2.cmml">4</mn></mrow><mo id="S5.F4.10.m4.2.2.2.3" xref="S5.F4.10.m4.2.2.3.cmml">,</mo><mrow id="S5.F4.10.m4.2.2.2.2" xref="S5.F4.10.m4.2.2.2.2.cmml"><mn id="S5.F4.10.m4.2.2.2.2.2" xref="S5.F4.10.m4.2.2.2.2.2.cmml">2</mn><mo lspace="0.280em" rspace="0em" id="S5.F4.10.m4.2.2.2.2.1" xref="S5.F4.10.m4.2.2.2.2.1.cmml">​</mo><mi id="S5.F4.10.m4.2.2.2.2.3" xref="S5.F4.10.m4.2.2.2.2.3.cmml">m</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.F4.10.m4.2c"><list id="S5.F4.10.m4.2.2.3.cmml" xref="S5.F4.10.m4.2.2.2"><apply id="S5.F4.10.m4.1.1.1.1.cmml" xref="S5.F4.10.m4.1.1.1.1"><minus id="S5.F4.10.m4.1.1.1.1.1.cmml" xref="S5.F4.10.m4.1.1.1.1"></minus><cn type="integer" id="S5.F4.10.m4.1.1.1.1.2.cmml" xref="S5.F4.10.m4.1.1.1.1.2">4</cn></apply><apply id="S5.F4.10.m4.2.2.2.2.cmml" xref="S5.F4.10.m4.2.2.2.2"><times id="S5.F4.10.m4.2.2.2.2.1.cmml" xref="S5.F4.10.m4.2.2.2.2.1"></times><cn type="integer" id="S5.F4.10.m4.2.2.2.2.2.cmml" xref="S5.F4.10.m4.2.2.2.2.2">2</cn><ci id="S5.F4.10.m4.2.2.2.2.3.cmml" xref="S5.F4.10.m4.2.2.2.2.3">𝑚</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.10.m4.2d">-4,2\;m</annotation></semantics></math> to <math id="S5.F4.11.m5.2" class="ltx_Math" alttext="-7,2\;m" display="inline"><semantics id="S5.F4.11.m5.2b"><mrow id="S5.F4.11.m5.2.2.2" xref="S5.F4.11.m5.2.2.3.cmml"><mrow id="S5.F4.11.m5.1.1.1.1" xref="S5.F4.11.m5.1.1.1.1.cmml"><mo id="S5.F4.11.m5.1.1.1.1b" xref="S5.F4.11.m5.1.1.1.1.cmml">−</mo><mn id="S5.F4.11.m5.1.1.1.1.2" xref="S5.F4.11.m5.1.1.1.1.2.cmml">7</mn></mrow><mo id="S5.F4.11.m5.2.2.2.3" xref="S5.F4.11.m5.2.2.3.cmml">,</mo><mrow id="S5.F4.11.m5.2.2.2.2" xref="S5.F4.11.m5.2.2.2.2.cmml"><mn id="S5.F4.11.m5.2.2.2.2.2" xref="S5.F4.11.m5.2.2.2.2.2.cmml">2</mn><mo lspace="0.280em" rspace="0em" id="S5.F4.11.m5.2.2.2.2.1" xref="S5.F4.11.m5.2.2.2.2.1.cmml">​</mo><mi id="S5.F4.11.m5.2.2.2.2.3" xref="S5.F4.11.m5.2.2.2.2.3.cmml">m</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.F4.11.m5.2c"><list id="S5.F4.11.m5.2.2.3.cmml" xref="S5.F4.11.m5.2.2.2"><apply id="S5.F4.11.m5.1.1.1.1.cmml" xref="S5.F4.11.m5.1.1.1.1"><minus id="S5.F4.11.m5.1.1.1.1.1.cmml" xref="S5.F4.11.m5.1.1.1.1"></minus><cn type="integer" id="S5.F4.11.m5.1.1.1.1.2.cmml" xref="S5.F4.11.m5.1.1.1.1.2">7</cn></apply><apply id="S5.F4.11.m5.2.2.2.2.cmml" xref="S5.F4.11.m5.2.2.2.2"><times id="S5.F4.11.m5.2.2.2.2.1.cmml" xref="S5.F4.11.m5.2.2.2.2.1"></times><cn type="integer" id="S5.F4.11.m5.2.2.2.2.2.cmml" xref="S5.F4.11.m5.2.2.2.2.2">2</cn><ci id="S5.F4.11.m5.2.2.2.2.3.cmml" xref="S5.F4.11.m5.2.2.2.2.3">𝑚</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.11.m5.2d">-7,2\;m</annotation></semantics></math> with respect to the subject. Top: Relative
Percentage Error (RPE); bottom: Mean Absolute Error (MAE) in <math id="S5.F4.12.m6.1" class="ltx_Math" alttext="mm" display="inline"><semantics id="S5.F4.12.m6.1b"><mrow id="S5.F4.12.m6.1.1" xref="S5.F4.12.m6.1.1.cmml"><mi id="S5.F4.12.m6.1.1.2" xref="S5.F4.12.m6.1.1.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S5.F4.12.m6.1.1.1" xref="S5.F4.12.m6.1.1.1.cmml">​</mo><mi id="S5.F4.12.m6.1.1.3" xref="S5.F4.12.m6.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.F4.12.m6.1c"><apply id="S5.F4.12.m6.1.1.cmml" xref="S5.F4.12.m6.1.1"><times id="S5.F4.12.m6.1.1.1.cmml" xref="S5.F4.12.m6.1.1.1"></times><ci id="S5.F4.12.m6.1.1.2.cmml" xref="S5.F4.12.m6.1.1.2">𝑚</ci><ci id="S5.F4.12.m6.1.1.3.cmml" xref="S5.F4.12.m6.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.12.m6.1d">mm</annotation></semantics></math>.</figcaption>
</figure>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Quantitative Comparison to Related Work</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">Although we did not aim to present a method that outperform SOTA estimation
methods, we discuss comparative quantitative results for completeness. Basically,
we compare to
NeuralAnthro’s<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> original results, the best baseline results
(Baseline I = 2) on ANSUR data in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, a recently
published study on height estimation from real images by humans<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, and the
ANSUR II
allowable error.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p">We have been eminently cautious in comparing our results in the task
of human body dimension estimation. Several reasons hinder a fair comparison and
constitute a major obstacle to advance the field.</p>
</div>
<div id="S5.SS4.p3" class="ltx_para">
<p id="S5.SS4.p3.2" class="ltx_p">First, in the literature, Mean Absolute Error (MAE) and Mean Absolute Difference
(MAD) refer to the same error quantity. Also, Relative Percentage Error (RPE)
has not been consistently reported. RPE is important
because human body dimensions
are not in the same scale. For
instance, probably <math id="S5.SS4.p3.1.m1.1" class="ltx_Math" alttext="40mm" display="inline"><semantics id="S5.SS4.p3.1.m1.1a"><mrow id="S5.SS4.p3.1.m1.1.1" xref="S5.SS4.p3.1.m1.1.1.cmml"><mn id="S5.SS4.p3.1.m1.1.1.2" xref="S5.SS4.p3.1.m1.1.1.2.cmml">40</mn><mo lspace="0em" rspace="0em" id="S5.SS4.p3.1.m1.1.1.1" xref="S5.SS4.p3.1.m1.1.1.1.cmml">​</mo><mi id="S5.SS4.p3.1.m1.1.1.3" xref="S5.SS4.p3.1.m1.1.1.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S5.SS4.p3.1.m1.1.1.1a" xref="S5.SS4.p3.1.m1.1.1.1.cmml">​</mo><mi id="S5.SS4.p3.1.m1.1.1.4" xref="S5.SS4.p3.1.m1.1.1.4.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p3.1.m1.1b"><apply id="S5.SS4.p3.1.m1.1.1.cmml" xref="S5.SS4.p3.1.m1.1.1"><times id="S5.SS4.p3.1.m1.1.1.1.cmml" xref="S5.SS4.p3.1.m1.1.1.1"></times><cn type="integer" id="S5.SS4.p3.1.m1.1.1.2.cmml" xref="S5.SS4.p3.1.m1.1.1.2">40</cn><ci id="S5.SS4.p3.1.m1.1.1.3.cmml" xref="S5.SS4.p3.1.m1.1.1.3">𝑚</ci><ci id="S5.SS4.p3.1.m1.1.1.4.cmml" xref="S5.SS4.p3.1.m1.1.1.4">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p3.1.m1.1c">40mm</annotation></semantics></math> MAE, say, account for a lower height
estimation error (better performance) as for head circumference error (worst
performance). Besides MAD and RPE,
estimation performance has been reported by Mean<math id="S5.SS4.p3.2.m2.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.SS4.p3.2.m2.1a"><mo id="S5.SS4.p3.2.m2.1.1" xref="S5.SS4.p3.2.m2.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.p3.2.m2.1b"><csymbol cd="latexml" id="S5.SS4.p3.2.m2.1.1.cmml" xref="S5.SS4.p3.2.m2.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p3.2.m2.1c">\pm</annotation></semantics></math>Std. Dev and
<span id="S5.SS4.p3.2.1" class="ltx_text ltx_font_italic">success rate<cite class="ltx_cite ltx_citemacro_cite"><span id="S5.SS4.p3.2.1.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib31" title="" class="ltx_ref">31</a><span id="S5.SS4.p3.2.1.2.2" class="ltx_text ltx_font_upright">]</span></cite></span>, seemingly <span id="S5.SS4.p3.2.2" class="ltx_text ltx_font_italic">Expert
ratio</span>. This
inconsistency in reporting results complicates significantly the comparison with
other
research. Second, most method’s input are 3D, therefore, inadequate for comparison
to 2D methods.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison to four related methods. We compare estimation performance
in
terms of MAD error in <math id="S5.T1.2.m1.1" class="ltx_Math" alttext="mm" display="inline"><semantics id="S5.T1.2.m1.1b"><mrow id="S5.T1.2.m1.1.1" xref="S5.T1.2.m1.1.1.cmml"><mi id="S5.T1.2.m1.1.1.2" xref="S5.T1.2.m1.1.1.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S5.T1.2.m1.1.1.1" xref="S5.T1.2.m1.1.1.1.cmml">​</mo><mi id="S5.T1.2.m1.1.1.3" xref="S5.T1.2.m1.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.2.m1.1c"><apply id="S5.T1.2.m1.1.1.cmml" xref="S5.T1.2.m1.1.1"><times id="S5.T1.2.m1.1.1.1.cmml" xref="S5.T1.2.m1.1.1.1"></times><ci id="S5.T1.2.m1.1.1.2.cmml" xref="S5.T1.2.m1.1.1.2">𝑚</ci><ci id="S5.T1.2.m1.1.1.3.cmml" xref="S5.T1.2.m1.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.2.m1.1d">mm</annotation></semantics></math>. We do not present HBDs that are
not comparable (na: not
applicable). Minimal errors are bold and we emphasized ANSUR II
allowable error. Additionally, we enclosed in parenthesis our
experiment setting that achieved best estimation
results.</figcaption>
<table id="S5.T1.11" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T1.11.10.1" class="ltx_tr">
<th id="S5.T1.11.10.1.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" style="width:76.8pt;">
<span id="S5.T1.11.10.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.10.1.1.1.1" class="ltx_p">Method</span>
</span>
</th>
<th id="S5.T1.11.10.1.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_r ltx_border_t" style="width:25.0pt;">
<span id="S5.T1.11.10.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.10.1.2.1.1" class="ltx_p">SW</span>
</span>
</th>
<th id="S5.T1.11.10.1.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_r ltx_border_t" style="width:25.0pt;">
<span id="S5.T1.11.10.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.10.1.3.1.1" class="ltx_p">RAL</span>
</span>
</th>
<th id="S5.T1.11.10.1.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_r ltx_border_t" style="width:25.0pt;">
<span id="S5.T1.11.10.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.10.1.4.1.1" class="ltx_p">LAL</span>
</span>
</th>
<th id="S5.T1.11.10.1.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_r ltx_border_t" style="width:25.0pt;">
<span id="S5.T1.11.10.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.10.1.5.1.1" class="ltx_p">I</span>
</span>
</th>
<th id="S5.T1.11.10.1.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_r ltx_border_t" style="width:25.0pt;">
<span id="S5.T1.11.10.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.10.1.6.1.1" class="ltx_p">CC</span>
</span>
</th>
<th id="S5.T1.11.10.1.7" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_r ltx_border_t" style="width:56.9pt;">
<span id="S5.T1.11.10.1.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.10.1.7.1.1" class="ltx_p">WC</span>
</span>
</th>
<th id="S5.T1.11.10.1.8" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_r ltx_border_t" style="width:25.0pt;">
<span id="S5.T1.11.10.1.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.10.1.8.1.1" class="ltx_p">PC</span>
</span>
</th>
<th id="S5.T1.11.10.1.9" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_r ltx_border_t" style="width:25.0pt;">
<span id="S5.T1.11.10.1.9.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.10.1.9.1.1" class="ltx_p">H</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T1.11.11.1" class="ltx_tr">
<td id="S5.T1.11.11.1.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:76.8pt;padding-bottom:6.0pt;">
<span id="S5.T1.11.11.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.11.1.1.1.1" class="ltx_p">Baseline (I = 2) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite></span>
</span>
</td>
<td id="S5.T1.11.11.1.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:25.0pt;padding-bottom:6.0pt;">
<span id="S5.T1.11.11.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.11.1.2.1.1" class="ltx_p">na</span>
</span>
</td>
<td id="S5.T1.11.11.1.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:25.0pt;padding-bottom:6.0pt;">
<span id="S5.T1.11.11.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.11.1.3.1.1" class="ltx_p">na</span>
</span>
</td>
<td id="S5.T1.11.11.1.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:25.0pt;padding-bottom:6.0pt;">
<span id="S5.T1.11.11.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.11.1.4.1.1" class="ltx_p">na</span>
</span>
</td>
<td id="S5.T1.11.11.1.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:25.0pt;padding-bottom:6.0pt;">
<span id="S5.T1.11.11.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.11.1.5.1.1" class="ltx_p">na</span>
</span>
</td>
<td id="S5.T1.11.11.1.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:25.0pt;padding-bottom:6.0pt;">
<span id="S5.T1.11.11.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.11.1.6.1.1" class="ltx_p">29.1</span>
</span>
</td>
<td id="S5.T1.11.11.1.7" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:56.9pt;padding-bottom:6.0pt;">
<span id="S5.T1.11.11.1.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.11.1.7.1.1" class="ltx_p">37.9</span>
</span>
</td>
<td id="S5.T1.11.11.1.8" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:25.0pt;padding-bottom:6.0pt;">
<span id="S5.T1.11.11.1.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.11.1.8.1.1" class="ltx_p"><span id="S5.T1.11.11.1.8.1.1.1" class="ltx_text ltx_font_bold">21.6</span></span>
</span>
</td>
<td id="S5.T1.11.11.1.9" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:25.0pt;padding-bottom:6.0pt;">
<span id="S5.T1.11.11.1.9.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.11.1.9.1.1" class="ltx_p">na</span>
</span>
</td>
</tr>
<tr id="S5.T1.11.12.2" class="ltx_tr">
<td id="S5.T1.11.12.2.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r" style="width:76.8pt;padding-bottom:6.0pt;">
<span id="S5.T1.11.12.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.12.2.1.1.1" class="ltx_p">NeuralAnthro <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite></span>
</span>
</td>
<td id="S5.T1.11.12.2.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" style="width:25.0pt;padding-bottom:6.0pt;">
<span id="S5.T1.11.12.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.12.2.2.1.1" class="ltx_p">12.54</span>
</span>
</td>
<td id="S5.T1.11.12.2.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" style="width:25.0pt;padding-bottom:6.0pt;">
<span id="S5.T1.11.12.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.12.2.3.1.1" class="ltx_p"><span id="S5.T1.11.12.2.3.1.1.1" class="ltx_text ltx_font_bold">12.98</span></span>
</span>
</td>
<td id="S5.T1.11.12.2.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" style="width:25.0pt;padding-bottom:6.0pt;">
<span id="S5.T1.11.12.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.12.2.4.1.1" class="ltx_p">13.48</span>
</span>
</td>
<td id="S5.T1.11.12.2.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" style="width:25.0pt;padding-bottom:6.0pt;">
<span id="S5.T1.11.12.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.12.2.5.1.1" class="ltx_p"><span id="S5.T1.11.12.2.5.1.1.1" class="ltx_text ltx_font_bold">12.17</span></span>
</span>
</td>
<td id="S5.T1.11.12.2.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" style="width:25.0pt;padding-bottom:6.0pt;">
<span id="S5.T1.11.12.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.12.2.6.1.1" class="ltx_p"><span id="S5.T1.11.12.2.6.1.1.1" class="ltx_text ltx_font_bold">25.22</span></span>
</span>
</td>
<td id="S5.T1.11.12.2.7" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" style="width:56.9pt;padding-bottom:6.0pt;">
<span id="S5.T1.11.12.2.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.12.2.7.1.1" class="ltx_p">27.53</span>
</span>
</td>
<td id="S5.T1.11.12.2.8" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" style="width:25.0pt;padding-bottom:6.0pt;">
<span id="S5.T1.11.12.2.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.12.2.8.1.1" class="ltx_p">25.85</span>
</span>
</td>
<td id="S5.T1.11.12.2.9" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" style="width:25.0pt;padding-bottom:6.0pt;">
<span id="S5.T1.11.12.2.9.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.12.2.9.1.1" class="ltx_p">27.34</span>
</span>
</td>
</tr>
<tr id="S5.T1.11.9" class="ltx_tr">
<td id="S5.T1.11.9.10" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r" style="width:76.8pt;padding-bottom:10.0pt;">
<span id="S5.T1.11.9.10.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.9.10.1.1" class="ltx_p">Our experiments</span>
</span>
</td>
<td id="S5.T1.3.1.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" style="width:25.0pt;padding-bottom:10.0pt;">
<span id="S5.T1.3.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.3.1.1.1.1" class="ltx_p"><span id="S5.T1.3.1.1.1.1.1" class="ltx_text ltx_font_bold">11.93</span> (<math id="S5.T1.3.1.1.1.1.m1.1" class="ltx_Math" alttext="6m" display="inline"><semantics id="S5.T1.3.1.1.1.1.m1.1a"><mrow id="S5.T1.3.1.1.1.1.m1.1.1" xref="S5.T1.3.1.1.1.1.m1.1.1.cmml"><mn id="S5.T1.3.1.1.1.1.m1.1.1.2" xref="S5.T1.3.1.1.1.1.m1.1.1.2.cmml">6</mn><mo lspace="0em" rspace="0em" id="S5.T1.3.1.1.1.1.m1.1.1.1" xref="S5.T1.3.1.1.1.1.m1.1.1.1.cmml">​</mo><mi id="S5.T1.3.1.1.1.1.m1.1.1.3" xref="S5.T1.3.1.1.1.1.m1.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.3.1.1.1.1.m1.1b"><apply id="S5.T1.3.1.1.1.1.m1.1.1.cmml" xref="S5.T1.3.1.1.1.1.m1.1.1"><times id="S5.T1.3.1.1.1.1.m1.1.1.1.cmml" xref="S5.T1.3.1.1.1.1.m1.1.1.1"></times><cn type="integer" id="S5.T1.3.1.1.1.1.m1.1.1.2.cmml" xref="S5.T1.3.1.1.1.1.m1.1.1.2">6</cn><ci id="S5.T1.3.1.1.1.1.m1.1.1.3.cmml" xref="S5.T1.3.1.1.1.1.m1.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.3.1.1.1.1.m1.1c">6m</annotation></semantics></math>)</span>
</span>
</td>
<td id="S5.T1.4.2.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" style="width:25.0pt;padding-bottom:10.0pt;">
<span id="S5.T1.4.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.4.2.2.1.1" class="ltx_p">13.30 (<math id="S5.T1.4.2.2.1.1.m1.1" class="ltx_Math" alttext="6m" display="inline"><semantics id="S5.T1.4.2.2.1.1.m1.1a"><mrow id="S5.T1.4.2.2.1.1.m1.1.1" xref="S5.T1.4.2.2.1.1.m1.1.1.cmml"><mn id="S5.T1.4.2.2.1.1.m1.1.1.2" xref="S5.T1.4.2.2.1.1.m1.1.1.2.cmml">6</mn><mo lspace="0em" rspace="0em" id="S5.T1.4.2.2.1.1.m1.1.1.1" xref="S5.T1.4.2.2.1.1.m1.1.1.1.cmml">​</mo><mi id="S5.T1.4.2.2.1.1.m1.1.1.3" xref="S5.T1.4.2.2.1.1.m1.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.4.2.2.1.1.m1.1b"><apply id="S5.T1.4.2.2.1.1.m1.1.1.cmml" xref="S5.T1.4.2.2.1.1.m1.1.1"><times id="S5.T1.4.2.2.1.1.m1.1.1.1.cmml" xref="S5.T1.4.2.2.1.1.m1.1.1.1"></times><cn type="integer" id="S5.T1.4.2.2.1.1.m1.1.1.2.cmml" xref="S5.T1.4.2.2.1.1.m1.1.1.2">6</cn><ci id="S5.T1.4.2.2.1.1.m1.1.1.3.cmml" xref="S5.T1.4.2.2.1.1.m1.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.4.2.2.1.1.m1.1c">6m</annotation></semantics></math>)</span>
</span>
</td>
<td id="S5.T1.5.3.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" style="width:25.0pt;padding-bottom:10.0pt;">
<span id="S5.T1.5.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.5.3.3.1.1" class="ltx_p"><span id="S5.T1.5.3.3.1.1.1" class="ltx_text ltx_font_bold">12.9</span> (<math id="S5.T1.5.3.3.1.1.m1.1" class="ltx_Math" alttext="6m" display="inline"><semantics id="S5.T1.5.3.3.1.1.m1.1a"><mrow id="S5.T1.5.3.3.1.1.m1.1.1" xref="S5.T1.5.3.3.1.1.m1.1.1.cmml"><mn id="S5.T1.5.3.3.1.1.m1.1.1.2" xref="S5.T1.5.3.3.1.1.m1.1.1.2.cmml">6</mn><mo lspace="0em" rspace="0em" id="S5.T1.5.3.3.1.1.m1.1.1.1" xref="S5.T1.5.3.3.1.1.m1.1.1.1.cmml">​</mo><mi id="S5.T1.5.3.3.1.1.m1.1.1.3" xref="S5.T1.5.3.3.1.1.m1.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.5.3.3.1.1.m1.1b"><apply id="S5.T1.5.3.3.1.1.m1.1.1.cmml" xref="S5.T1.5.3.3.1.1.m1.1.1"><times id="S5.T1.5.3.3.1.1.m1.1.1.1.cmml" xref="S5.T1.5.3.3.1.1.m1.1.1.1"></times><cn type="integer" id="S5.T1.5.3.3.1.1.m1.1.1.2.cmml" xref="S5.T1.5.3.3.1.1.m1.1.1.2">6</cn><ci id="S5.T1.5.3.3.1.1.m1.1.1.3.cmml" xref="S5.T1.5.3.3.1.1.m1.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.5.3.3.1.1.m1.1c">6m</annotation></semantics></math>)</span>
</span>
</td>
<td id="S5.T1.6.4.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" style="width:25.0pt;padding-bottom:10.0pt;">
<span id="S5.T1.6.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.6.4.4.1.1" class="ltx_p">22.05 (<math id="S5.T1.6.4.4.1.1.m1.1" class="ltx_Math" alttext="6m" display="inline"><semantics id="S5.T1.6.4.4.1.1.m1.1a"><mrow id="S5.T1.6.4.4.1.1.m1.1.1" xref="S5.T1.6.4.4.1.1.m1.1.1.cmml"><mn id="S5.T1.6.4.4.1.1.m1.1.1.2" xref="S5.T1.6.4.4.1.1.m1.1.1.2.cmml">6</mn><mo lspace="0em" rspace="0em" id="S5.T1.6.4.4.1.1.m1.1.1.1" xref="S5.T1.6.4.4.1.1.m1.1.1.1.cmml">​</mo><mi id="S5.T1.6.4.4.1.1.m1.1.1.3" xref="S5.T1.6.4.4.1.1.m1.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.6.4.4.1.1.m1.1b"><apply id="S5.T1.6.4.4.1.1.m1.1.1.cmml" xref="S5.T1.6.4.4.1.1.m1.1.1"><times id="S5.T1.6.4.4.1.1.m1.1.1.1.cmml" xref="S5.T1.6.4.4.1.1.m1.1.1.1"></times><cn type="integer" id="S5.T1.6.4.4.1.1.m1.1.1.2.cmml" xref="S5.T1.6.4.4.1.1.m1.1.1.2">6</cn><ci id="S5.T1.6.4.4.1.1.m1.1.1.3.cmml" xref="S5.T1.6.4.4.1.1.m1.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.6.4.4.1.1.m1.1c">6m</annotation></semantics></math>)</span>
</span>
</td>
<td id="S5.T1.7.5.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" style="width:25.0pt;padding-bottom:10.0pt;">
<span id="S5.T1.7.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.7.5.5.1.1" class="ltx_p">25.93 (<math id="S5.T1.7.5.5.1.1.m1.1" class="ltx_Math" alttext="6m" display="inline"><semantics id="S5.T1.7.5.5.1.1.m1.1a"><mrow id="S5.T1.7.5.5.1.1.m1.1.1" xref="S5.T1.7.5.5.1.1.m1.1.1.cmml"><mn id="S5.T1.7.5.5.1.1.m1.1.1.2" xref="S5.T1.7.5.5.1.1.m1.1.1.2.cmml">6</mn><mo lspace="0em" rspace="0em" id="S5.T1.7.5.5.1.1.m1.1.1.1" xref="S5.T1.7.5.5.1.1.m1.1.1.1.cmml">​</mo><mi id="S5.T1.7.5.5.1.1.m1.1.1.3" xref="S5.T1.7.5.5.1.1.m1.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.7.5.5.1.1.m1.1b"><apply id="S5.T1.7.5.5.1.1.m1.1.1.cmml" xref="S5.T1.7.5.5.1.1.m1.1.1"><times id="S5.T1.7.5.5.1.1.m1.1.1.1.cmml" xref="S5.T1.7.5.5.1.1.m1.1.1.1"></times><cn type="integer" id="S5.T1.7.5.5.1.1.m1.1.1.2.cmml" xref="S5.T1.7.5.5.1.1.m1.1.1.2">6</cn><ci id="S5.T1.7.5.5.1.1.m1.1.1.3.cmml" xref="S5.T1.7.5.5.1.1.m1.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.7.5.5.1.1.m1.1c">6m</annotation></semantics></math>)</span>
</span>
</td>
<td id="S5.T1.9.7.7" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" style="width:56.9pt;padding-bottom:10.0pt;">
<span id="S5.T1.9.7.7.2" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.9.7.7.2.2" class="ltx_p"><span id="S5.T1.9.7.7.2.2.1" class="ltx_text ltx_font_bold">27.39</span>
(<math id="S5.T1.8.6.6.1.1.m1.2" class="ltx_Math" alttext="-4.2;-7.2" display="inline"><semantics id="S5.T1.8.6.6.1.1.m1.2a"><mrow id="S5.T1.8.6.6.1.1.m1.2.2.2" xref="S5.T1.8.6.6.1.1.m1.2.2.3.cmml"><mrow id="S5.T1.8.6.6.1.1.m1.1.1.1.1" xref="S5.T1.8.6.6.1.1.m1.1.1.1.1.cmml"><mo id="S5.T1.8.6.6.1.1.m1.1.1.1.1a" xref="S5.T1.8.6.6.1.1.m1.1.1.1.1.cmml">−</mo><mn id="S5.T1.8.6.6.1.1.m1.1.1.1.1.2" xref="S5.T1.8.6.6.1.1.m1.1.1.1.1.2.cmml">4.2</mn></mrow><mo id="S5.T1.8.6.6.1.1.m1.2.2.2.3" xref="S5.T1.8.6.6.1.1.m1.2.2.3.cmml">;</mo><mrow id="S5.T1.8.6.6.1.1.m1.2.2.2.2" xref="S5.T1.8.6.6.1.1.m1.2.2.2.2.cmml"><mo id="S5.T1.8.6.6.1.1.m1.2.2.2.2a" xref="S5.T1.8.6.6.1.1.m1.2.2.2.2.cmml">−</mo><mn id="S5.T1.8.6.6.1.1.m1.2.2.2.2.2" xref="S5.T1.8.6.6.1.1.m1.2.2.2.2.2.cmml">7.2</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.8.6.6.1.1.m1.2b"><list id="S5.T1.8.6.6.1.1.m1.2.2.3.cmml" xref="S5.T1.8.6.6.1.1.m1.2.2.2"><apply id="S5.T1.8.6.6.1.1.m1.1.1.1.1.cmml" xref="S5.T1.8.6.6.1.1.m1.1.1.1.1"><minus id="S5.T1.8.6.6.1.1.m1.1.1.1.1.1.cmml" xref="S5.T1.8.6.6.1.1.m1.1.1.1.1"></minus><cn type="float" id="S5.T1.8.6.6.1.1.m1.1.1.1.1.2.cmml" xref="S5.T1.8.6.6.1.1.m1.1.1.1.1.2">4.2</cn></apply><apply id="S5.T1.8.6.6.1.1.m1.2.2.2.2.cmml" xref="S5.T1.8.6.6.1.1.m1.2.2.2.2"><minus id="S5.T1.8.6.6.1.1.m1.2.2.2.2.1.cmml" xref="S5.T1.8.6.6.1.1.m1.2.2.2.2"></minus><cn type="float" id="S5.T1.8.6.6.1.1.m1.2.2.2.2.2.cmml" xref="S5.T1.8.6.6.1.1.m1.2.2.2.2.2">7.2</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.8.6.6.1.1.m1.2c">-4.2;-7.2</annotation></semantics></math>)<math id="S5.T1.9.7.7.2.2.m2.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S5.T1.9.7.7.2.2.m2.1a"><mi id="S5.T1.9.7.7.2.2.m2.1.1" xref="S5.T1.9.7.7.2.2.m2.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S5.T1.9.7.7.2.2.m2.1b"><ci id="S5.T1.9.7.7.2.2.m2.1.1.cmml" xref="S5.T1.9.7.7.2.2.m2.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.9.7.7.2.2.m2.1c">m</annotation></semantics></math></span>
</span>
</td>
<td id="S5.T1.10.8.8" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" style="width:25.0pt;padding-bottom:10.0pt;">
<span id="S5.T1.10.8.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.10.8.8.1.1" class="ltx_p">23.28
(<math id="S5.T1.10.8.8.1.1.m1.1" class="ltx_Math" alttext="6m" display="inline"><semantics id="S5.T1.10.8.8.1.1.m1.1a"><mrow id="S5.T1.10.8.8.1.1.m1.1.1" xref="S5.T1.10.8.8.1.1.m1.1.1.cmml"><mn id="S5.T1.10.8.8.1.1.m1.1.1.2" xref="S5.T1.10.8.8.1.1.m1.1.1.2.cmml">6</mn><mo lspace="0em" rspace="0em" id="S5.T1.10.8.8.1.1.m1.1.1.1" xref="S5.T1.10.8.8.1.1.m1.1.1.1.cmml">​</mo><mi id="S5.T1.10.8.8.1.1.m1.1.1.3" xref="S5.T1.10.8.8.1.1.m1.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.10.8.8.1.1.m1.1b"><apply id="S5.T1.10.8.8.1.1.m1.1.1.cmml" xref="S5.T1.10.8.8.1.1.m1.1.1"><times id="S5.T1.10.8.8.1.1.m1.1.1.1.cmml" xref="S5.T1.10.8.8.1.1.m1.1.1.1"></times><cn type="integer" id="S5.T1.10.8.8.1.1.m1.1.1.2.cmml" xref="S5.T1.10.8.8.1.1.m1.1.1.2">6</cn><ci id="S5.T1.10.8.8.1.1.m1.1.1.3.cmml" xref="S5.T1.10.8.8.1.1.m1.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.10.8.8.1.1.m1.1c">6m</annotation></semantics></math>)</span>
</span>
</td>
<td id="S5.T1.11.9.9" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" style="width:25.0pt;padding-bottom:10.0pt;">
<span id="S5.T1.11.9.9.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.9.9.1.1" class="ltx_p"><span id="S5.T1.11.9.9.1.1.1" class="ltx_text ltx_font_bold">24.75</span> (<math id="S5.T1.11.9.9.1.1.m1.1" class="ltx_Math" alttext="6m" display="inline"><semantics id="S5.T1.11.9.9.1.1.m1.1a"><mrow id="S5.T1.11.9.9.1.1.m1.1.1" xref="S5.T1.11.9.9.1.1.m1.1.1.cmml"><mn id="S5.T1.11.9.9.1.1.m1.1.1.2" xref="S5.T1.11.9.9.1.1.m1.1.1.2.cmml">6</mn><mo lspace="0em" rspace="0em" id="S5.T1.11.9.9.1.1.m1.1.1.1" xref="S5.T1.11.9.9.1.1.m1.1.1.1.cmml">​</mo><mi id="S5.T1.11.9.9.1.1.m1.1.1.3" xref="S5.T1.11.9.9.1.1.m1.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.11.9.9.1.1.m1.1b"><apply id="S5.T1.11.9.9.1.1.m1.1.1.cmml" xref="S5.T1.11.9.9.1.1.m1.1.1"><times id="S5.T1.11.9.9.1.1.m1.1.1.1.cmml" xref="S5.T1.11.9.9.1.1.m1.1.1.1"></times><cn type="integer" id="S5.T1.11.9.9.1.1.m1.1.1.2.cmml" xref="S5.T1.11.9.9.1.1.m1.1.1.2">6</cn><ci id="S5.T1.11.9.9.1.1.m1.1.1.3.cmml" xref="S5.T1.11.9.9.1.1.m1.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.11.9.9.1.1.m1.1c">6m</annotation></semantics></math>)</span>
</span>
</td>
</tr>
<tr id="S5.T1.11.13.3" class="ltx_tr">
<td id="S5.T1.11.13.3.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r" style="width:76.8pt;padding-bottom:10.0pt;">
<span id="S5.T1.11.13.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.13.3.1.1.1" class="ltx_p">Humans observing <span id="S5.T1.11.13.3.1.1.1.1" class="ltx_text ltx_font_bold">real images</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite></span>
</span>
</td>
<td id="S5.T1.11.13.3.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" style="width:25.0pt;padding-bottom:10.0pt;">
<span id="S5.T1.11.13.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.13.3.2.1.1" class="ltx_p">na</span>
</span>
</td>
<td id="S5.T1.11.13.3.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" style="width:25.0pt;padding-bottom:10.0pt;">
<span id="S5.T1.11.13.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.13.3.3.1.1" class="ltx_p">na</span>
</span>
</td>
<td id="S5.T1.11.13.3.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" style="width:25.0pt;padding-bottom:10.0pt;">
<span id="S5.T1.11.13.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.13.3.4.1.1" class="ltx_p">na</span>
</span>
</td>
<td id="S5.T1.11.13.3.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" style="width:25.0pt;padding-bottom:10.0pt;">
<span id="S5.T1.11.13.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.13.3.5.1.1" class="ltx_p">na</span>
</span>
</td>
<td id="S5.T1.11.13.3.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" style="width:25.0pt;padding-bottom:10.0pt;">
<span id="S5.T1.11.13.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.13.3.6.1.1" class="ltx_p">na</span>
</span>
</td>
<td id="S5.T1.11.13.3.7" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" style="width:56.9pt;padding-bottom:10.0pt;">
<span id="S5.T1.11.13.3.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.13.3.7.1.1" class="ltx_p">na</span>
</span>
</td>
<td id="S5.T1.11.13.3.8" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" style="width:25.0pt;padding-bottom:10.0pt;">
<span id="S5.T1.11.13.3.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.13.3.8.1.1" class="ltx_p">na</span>
</span>
</td>
<td id="S5.T1.11.13.3.9" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" style="width:25.0pt;padding-bottom:10.0pt;">
<span id="S5.T1.11.13.3.9.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.13.3.9.1.1" class="ltx_p">64.0</span>
</span>
</td>
</tr>
<tr id="S5.T1.11.14.4" class="ltx_tr">
<td id="S5.T1.11.14.4.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_l ltx_border_r" style="width:76.8pt;">
<span id="S5.T1.11.14.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.14.4.1.1.1" class="ltx_p">Humans, ANSUR (Allowable error ANSUR II)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite></span>
</span>
</td>
<td id="S5.T1.11.14.4.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r" style="width:25.0pt;">
<span id="S5.T1.11.14.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.14.4.2.1.1" class="ltx_p">na</span>
</span>
</td>
<td id="S5.T1.11.14.4.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r" style="width:25.0pt;">
<span id="S5.T1.11.14.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.14.4.3.1.1" class="ltx_p">na</span>
</span>
</td>
<td id="S5.T1.11.14.4.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r" style="width:25.0pt;">
<span id="S5.T1.11.14.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.14.4.4.1.1" class="ltx_p">na</span>
</span>
</td>
<td id="S5.T1.11.14.4.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r" style="width:25.0pt;">
<span id="S5.T1.11.14.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.14.4.5.1.1" class="ltx_p"><em id="S5.T1.11.14.4.5.1.1.1" class="ltx_emph ltx_font_bold ltx_font_italic">10.0</em></span>
</span>
</td>
<td id="S5.T1.11.14.4.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r" style="width:25.0pt;">
<span id="S5.T1.11.14.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.14.4.6.1.1" class="ltx_p"><em id="S5.T1.11.14.4.6.1.1.1" class="ltx_emph ltx_font_bold ltx_font_italic">14.0</em></span>
</span>
</td>
<td id="S5.T1.11.14.4.7" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r" style="width:56.9pt;">
<span id="S5.T1.11.14.4.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.14.4.7.1.1" class="ltx_p"><em id="S5.T1.11.14.4.7.1.1.1" class="ltx_emph ltx_font_bold ltx_font_italic">12.0</em></span>
</span>
</td>
<td id="S5.T1.11.14.4.8" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r" style="width:25.0pt;">
<span id="S5.T1.11.14.4.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.14.4.8.1.1" class="ltx_p"><em id="S5.T1.11.14.4.8.1.1.1" class="ltx_emph ltx_font_bold ltx_font_italic">12.0</em></span>
</span>
</td>
<td id="S5.T1.11.14.4.9" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r" style="width:25.0pt;">
<span id="S5.T1.11.14.4.9.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.11.14.4.9.1.1" class="ltx_p"><em id="S5.T1.11.14.4.9.1.1.1" class="ltx_emph ltx_font_bold ltx_font_italic">6.0</em></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS4.p4" class="ltx_para">
<p id="S5.SS4.p4.1" class="ltx_p">Third, we require
that methods’ result has been reported persistently in the literature. Neither we
compare to results reported online
that are not longer available, nor to results that has been used for comparison
but we were not able to locate in
the original cited paper. Last, in the literature, different
datasets have been used
for comparing. This might render previous and this comparison
counterproductive. For example, see ANSUR<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> App. G
for an extensive account on comparability limitations.</p>
</div>
<div id="S5.SS4.p5" class="ltx_para">
<p id="S5.SS4.p5.1" class="ltx_p">Table <a href="#S5.T1" title="Table 1 ‣ 5.4 Quantitative Comparison to Related Work ‣ 5 Results and Discussion ‣ Effect of Gender, Pose and Camera Distance on Human Body Dimensions Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the comparison. The input to Baseline is not
images (like ours) but height and weight. However, that research does establish
a
conceptual baseline: HBDE methods should estimate body
measurements with higher accuracy compared to regression. This statement should
not be categorically interpreted. Methods requiring images as input without any
other information are more challenging and, therefore, might exhibit less
accuracy. As it can be seen, NeuralAntro estimates more precisely RAL, I and CC
as the regression baseline, when applicable, and all of our experiment settings.
This might happen because NeuralAnthro was trained and evaluated with fixed camera distance.
The network probably found more difficult learning when trained with three different camera distances. Nevertheless, being SW the most difficult
HBD to estimate, our experiment with one camera distance at <math id="S5.SS4.p5.1.m1.1" class="ltx_Math" alttext="6m" display="inline"><semantics id="S5.SS4.p5.1.m1.1a"><mrow id="S5.SS4.p5.1.m1.1.1" xref="S5.SS4.p5.1.m1.1.1.cmml"><mn id="S5.SS4.p5.1.m1.1.1.2" xref="S5.SS4.p5.1.m1.1.1.2.cmml">6</mn><mo lspace="0em" rspace="0em" id="S5.SS4.p5.1.m1.1.1.1" xref="S5.SS4.p5.1.m1.1.1.1.cmml">​</mo><mi id="S5.SS4.p5.1.m1.1.1.3" xref="S5.SS4.p5.1.m1.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p5.1.m1.1b"><apply id="S5.SS4.p5.1.m1.1.1.cmml" xref="S5.SS4.p5.1.m1.1.1"><times id="S5.SS4.p5.1.m1.1.1.1.cmml" xref="S5.SS4.p5.1.m1.1.1.1"></times><cn type="integer" id="S5.SS4.p5.1.m1.1.1.2.cmml" xref="S5.SS4.p5.1.m1.1.1.2">6</cn><ci id="S5.SS4.p5.1.m1.1.1.3.cmml" xref="S5.SS4.p5.1.m1.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p5.1.m1.1c">6m</annotation></semantics></math> manifests the
best estimation performance. Moreover, our experiment setting with randomly selected camera distances shows
the best WC estimation performance.</p>
</div>
<div id="S5.SS4.p6" class="ltx_para">
<p id="S5.SS4.p6.1" class="ltx_p">As the authors indicate in
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, height estimation by humans exhibits poor
performance. The cause is, probably, that the persons estimated the HBD from real
images, which is the input with highest complexity, compare to synthetic
controlled data.</p>
</div>
<div id="S5.SS4.p7" class="ltx_para">
<p id="S5.SS4.p7.1" class="ltx_p">Estimation error of all HBD lies over the ANSUR II allowable error, but the fact that the NeuralAnthro is a small CNN could indicate, that by incrementing the size of the network, the estimation performance could be improved as well.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions and Future Work</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this paper, we assessed the performance of a neural network employed to estimate human body measurements from images. To that end, we augmented our recently published dataset containing images of female and male subjects in two
poses, with images of these subjects synthesized using different camera distances
with respect to the subjects. We trained a CNN with two
genders, two different poses and sparse and dense camera distances. After training we evaluated the network performance in terms of MAE and RPE.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">The CNN estimated HBDs of male subjects more accurately than those of females. The shoulder width predictions exhibit a surprising pose dependency. The width
is estimated more correctly for subjects with arms spread out to the side
(compared to subjects with lowered arms, where the contours of the shoulders
are more pronounced). In contrast to our expectations, network performance
decreases only slightly when perceiving humans from a range of (camera)
distances instead of a fixed distance; given that the person is completely visible in the image. In general, shoulder
width is the most difficult HBD to estimate.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Future Work</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">An important question that needs to be answered is why the estimation is, in
general, highly accurate (errors are reported in mm). Exploring to what extent synthetic data is representative of the real HBDs would contribute to understand this phenomenon. Increasing the level of realism of the images would probably have the strongest effect in HBDE. Also, investigating the minimum amount of data for conducting HBDs with reasonable accuracy, would help determining bounds to collect a plausible real dataset, therefore, alleviating the data scarcity problem.</p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
The anthropometric survey of us army personnel,
<a target="_blank" href="https://www.openlab.psu.edu/ansur2/" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://www.openlab.psu.edu/ansur2/</a>

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Nhanes questionnaires, datasets, and related documentation,
<a target="_blank" href="https://wwwn.cdc.gov/nchs/nhanes/continuousnhanes/default.aspx" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://wwwn.cdc.gov/nchs/nhanes/continuousnhanes/default.aspx</a>

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Ashmawi, S., Alharbi, M., Almaghrabi, A., Alhothali, A.: Fitme: Body
measurement estimations using machine learning method. Procedia Computer
Science <span id="bib.bib3.1.1" class="ltx_text ltx_font_bold">163</span>, 209–217 (2019). https://doi.org/10.1016/j.procs.2019.12.102,
<a target="_blank" href="https://www.sciencedirect.com/science/article/pii/S1877050919321416" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://www.sciencedirect.com/science/article/pii/S1877050919321416</a>,
16th Learning and Technology Conference 2019Artificial Intelligence and
Machine Learning: Embedding the Intelligence

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Bartol, K., Bojanić, D., Petković, T., Peharec, S., Pribanić, T.: Linear
regression vs. deep learning: A simple yet effective baseline for human body
measurement. Sensors <span id="bib.bib4.1.1" class="ltx_text ltx_font_bold">22</span>(5) (2022). https://doi.org/10.3390/s22051885,
<a target="_blank" href="https://www.mdpi.com/1424-8220/22/5/1885" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://www.mdpi.com/1424-8220/22/5/1885</a>

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
BenAbdelkader, C., Yacoob, Y.: Statistical body height estimation from a single
image. In: 8th IEEE International Conference on Automatic Face and Gesture
Recognition (FG 2008), Amsterdam, The Netherlands, 17-19 September 2008.
pp. 1–7 (2008). https://doi.org/10.1109/AFGR.2008.4813453

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Brito, M.F., Ramos, A.L., Carneiro, P., Gonçalves, M.A.: Ergonomic
Analysis in Lean Manufacturing and Industry 4.0—A Systematic Review, pp.
95–127. Springer International Publishing (2019).
https://doi.org/10.1007/978-3-030-13515-7_4

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Canhada, S.L., Luft, V.C., Giatti, L., Duncan, B.B., Chor, D., Fonseca,
M.d.J.M.d., Matos, S.M.A., Molina, M.d.C.B., Barreto, S.M., Levy, R.B.,
et al.: Ultra-processed foods, incident overweight and obesity, and
longitudinal changes in weight and waist circumference: the brazilian
longitudinal study of adult health (elsa-brasil). Public Health Nutrition
<span id="bib.bib7.1.1" class="ltx_text ltx_font_bold">23</span>(6), 1076–1086 (2020). https://doi.org/10.1017/S1368980019002854

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Dey, R., Nangia, M., Ross, K.W., Liu, Y.: Estimating Heights from Photo
Collections: A Data-Driven Approach, p. 227–238. Association for Computing
Machinery, New York, NY, USA (2014),
<a target="_blank" href="https://doi.org/10.1145/2660460.2660466" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1145/2660460.2660466</a>

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Dibra, E., Jain, H., Öztireli, C., Ziegler, R., Gross, M.: Hs-nets:
Estimating human body shape from silhouettes with convolutional neural
networks. In: 2016 Fourth International Conference on 3D Vision (3DV). pp.
108–117. IEEE (2016). https://doi.org/10.1109/3DV.2016.19

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Dibra, E., Öztireli, A.C., Ziegler, R., Gross, M.H.: Shape from selfies:
Human body shape estimation using cca regression forests. In: ECCV (2016)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Gill, S., Ahmed, M., Parker, C., Hayes, S.: Not all body scanning measurements
are valid: Perspectives from pattern practice (Oct 2017).
https://doi.org/10.15221/17.043, 8th International Conference and Exhibition on 3D Body
Scanning and Processing Technologies ; Conference date: 10-10-2017 Through
11-10-2017

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Gill, S., Parker, C.J., Hayes, S., Brownbridge, K., Wren, P., Panchenko, A.:
The true height of the waist: Explorations of automated body scanner waist
definitions of the tc2 scanner. In: Proceedings of the 5th International
Conference on 3D Body Scanning Technologies, Lugano, Switzerland, 21-22
October 2014. Hometrica Consulting - Dr. Nicola D’Apuzzo (Oct 2014).
https://doi.org/10.15221/14.055

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Gonzalez Tejeda, Y., Mayer, H.A.: A neural anthropometer learning from body
dimensions computed on human 3d meshes. In: 2021 IEEE Symposium Series on
Computational Intelligence (SSCI). pp. 1–8 (2021).
https://doi.org/10.1109/SSCI50451.2021.9660069

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Guan, Y.P., et al.: Unsupervised human height estimation from a single image.
Journal of Biomedical Science and Engineering <span id="bib.bib14.1.1" class="ltx_text ltx_font_bold">2</span>(06),  425 (2009)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Günel, S., Rhodin, H., Fua, P.: What face and body shapes can tell us about
height. In: ICCV Workshops. pp. 1819–1827 (2019).
https://doi.org/10.1109/ICCVW.2019.00226

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Kato, K., Higashiyama, A.: Estimation of height for persons in pictures.
Perception &amp; psychophysics <span id="bib.bib16.1.1" class="ltx_text ltx_font_bold">60</span>(8), 1318–1328 (1998)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Kiru, M.U., Belaton, B., Mohamad, S.M.S., Usman, G.M., Kazaure, A.A.:
Intelligent automatic door system based on supervised learning. In: 2020 IEEE
Conference on Open Systems (ICOS). pp. 43–47. IEEE (2020).
https://doi.org/10.1109/ICOS50156.2020.9293673

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., Black, M.J.: Smpl: A skinned
multi-person linear model. ACM Trans. Graph. <span id="bib.bib18.1.1" class="ltx_text ltx_font_bold">34</span>, 248:1–248:16
(2015)

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Martynov, K., Garimella, K., West, R.: Human biases in body measurement
estimation (Oct 2020),
<a target="_blank" href="https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-020-00250-x" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-020-00250-x</a>

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Michael, E., Antje, C., Silke, S.: Online shopping featuring "my customized
avatar" - generating customized avatars for a sustainable shopping experience
in e-commerce. In: 12th International Conference and Exhibition on 3D Body
Scanning and Processing Technologies. (2021). https://doi.org/10.15221/21

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Momeni-k, M., Diamantas, S.C., Ruggiero, F., Siciliano, B.: Height estimation
from a single camera view. In: VISAPP (1). pp. 358–364 (2012)

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Paquette, S.: Anthropometric survey (ANSUR) II pilot study: methods and summary
statistics. Anthrotch, US Army Natick Soldier Research, Development and
Engineering Center (2009)

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Pini, S., D’Eusanio, A., Borghi, G., Vezzani, R., Cucchiara, R.: Baracca: a
multimodal dataset for anthropometric measurements in automotive. In: 2020
IEEE International Joint Conference on Biometrics (IJCB). pp. 1–7. IEEE
(2020)

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Robinette, K.M., Daanen, H., Paquet, E.: The caesar project: a 3-d surface
anthropometry survey. In: Second International Conference on 3-D Digital
Imaging and Modeling (Cat. No. PR00062). pp. 380–386. IEEE (1999)

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Shi, F., Wang, J., Shi, J., Wu, Z., Wang, Q., Tang, Z., He, K., Shi, Y., Shen,
D.: Review of artificial intelligence techniques in imaging data acquisition,
segmentation, and diagnosis for covid-19. IEEE reviews in biomedical
engineering <span id="bib.bib25.1.1" class="ltx_text ltx_font_bold">14</span>, 4–15 (2020)

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Škorvánková, D., Riečickỳ, A., Madaras, M.: Automatic
estimation of anthropometric human body measurements. arXiv preprint
arXiv:2112.11992 (2021)

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Sriharsha, K.V., Alphonse, P.J.A.: Anthropometric based real height
estimation using multi layer peceptron ann architecture in surveillance
areas. In: 2019 10th International Conference on Computing, Communication and
Networking Technologies (ICCCNT). pp. 1–6 (2019)

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Tanasa, D., Cojocea, E.: Esenca - a fit predictor built for the 21st century.
In: Proc. of 3DBODY.TECH 2021. pp. 83–90. IEEE (2021). https://doi.org/10.15221/21

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Thakkar, N., Farid, H.: On the feasibility of 3d model-based forensic height
and weight estimation. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. pp. 953–961 (2021).
https://doi.org/10.1109/CVPRW53098.2021.00106

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Yan, S., Kämäräinen, J.K.: Learning anthropometry from rendered
humans. arXiv preprint arXiv:2101.02515 (2021)

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Yan, S., Wirta, J., Kämäräinen, J.K.: Anthropometric clothing
measurements from 3d body scans. Machine Vision and Applications
<span id="bib.bib31.1.1" class="ltx_text ltx_font_bold">31</span>, 1–11 (2020)

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Yan, S., Wirta, J., Kämäräinen, J.K.: Silhouette body measurement
benchmarks. In: 2020 25th International Conference on Pattern Recognition
(ICPR). pp. 7804–7809 (2021). https://doi.org/10.1109/ICPR48806.2021.9412708

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2205.12027" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2205.12028" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2205.12028">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2205.12028" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2205.12029" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Mar 11 15:00:43 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
